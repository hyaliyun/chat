import{_ as p,o as a,c as n,a as t,m as u,t as c,C as g,M as _,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},S={class:"review-content"};function P(s,e,l,h,i,o){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",P],["__scopeId","data-v-c0a87a4f"]]),A=JSON.parse('[{"question":"**Time Series Analysis with Panda\'s Resampler** **Problem Statement:** You are tasked with analyzing a dataset containing time series data. Using pandas, you need to resample this data, perform transformations, and compute various statistical metrics. **Input:** You are given a CSV file named `time_series_data.csv` which contains columns: - `timestamp`: A date-time string in the format `YYYY-MM-DD HH:MM:SS` - `value`: A floating-point number representing the data value at the given timestamp **Tasks:** 1. **Read the Dataset:** - Load the data from `time_series_data.csv` into a pandas DataFrame. - Ensure the `timestamp` column is parsed as a date-time column and set it as the index of the DataFrame. 2. **Resample the Data:** - Resample the data to a daily frequency, and fill missing values using forward fill. 3. **Transformations:** - Compute the rolling mean with a window of 7 days on the resampled data. - Add this rolling mean as a new column to the DataFrame called `rolling_mean`. 4. **Statistical Computations:** - Compute the following statistical metrics on the resampled data: - Daily sum - Daily mean - Maximum value within each week - Minimum value within each week - Standard deviation within each day 5. **Output:** - Print the first 10 rows of the DataFrame after adding the rolling mean. - Print the computed statistical metrics. **Constraints:** - You must use pandas for all operations. - Handle missing values appropriately when resampling. **Performance Requirements:** - The operations should efficiently handle datasets with up to 10^6 rows. **Example:** Given the partial content of `time_series_data.csv`: ``` timestamp,value 2023-01-01 00:00:00,10.5 2023-01-02 00:00:00,15.2 2023-01-03 00:00:00,13.5 ... ``` You should implement the function `process_time_series_data(file_path: str) -> None`, where `file_path` is the path to the CSV file. ```python import pandas as pd def process_time_series_data(file_path: str) -> None: # Task 1: Read the dataset df = pd.read_csv(file_path, parse_dates=[\'timestamp\'], index_col=\'timestamp\') # Task 2: Resample the data to daily frequency df_resampled = df.resample(\'D\').ffill() # Task 3: Compute rolling mean with a 7-day window df_resampled[\'rolling_mean\'] = df_resampled[\'value\'].rolling(window=7).mean() # Task 4: Compute statistical metrics daily_sum = df_resampled[\'value\'].resample(\'D\').sum() daily_mean = df_resampled[\'value\'].resample(\'D\').mean() weekly_max = df_resampled[\'value\'].resample(\'W\').max() weekly_min = df_resampled[\'value\'].resample(\'W\').min() daily_std = df_resampled[\'value\'].resample(\'D\').std() # Output the first 10 rows after adding rolling mean print(df_resampled.head(10)) # Print the computed statistical metrics print(\'Daily Sum:n\', daily_sum.head(10)) print(\'Daily Mean:n\', daily_mean.head(10)) print(\'Weekly Max:n\', weekly_max.head(10)) print(\'Weekly Min:n\', weekly_min.head(10)) print(\'Daily Std Dev:n\', daily_std.head(10)) ``` Test the function with the provided CSV file to ensure correctness.","solution":"import pandas as pd def process_time_series_data(file_path: str) -> None: # Task 1: Read the dataset df = pd.read_csv(file_path, parse_dates=[\'timestamp\'], index_col=\'timestamp\') # Task 2: Resample the data to daily frequency df_resampled = df.resample(\'D\').ffill() # Task 3: Compute rolling mean with a 7-day window df_resampled[\'rolling_mean\'] = df_resampled[\'value\'].rolling(window=7).mean() # Task 4: Compute statistical metrics daily_sum = df_resampled[\'value\'].resample(\'D\').sum() daily_mean = df_resampled[\'value\'].resample(\'D\').mean() weekly_max = df_resampled[\'value\'].resample(\'W\').max() weekly_min = df_resampled[\'value\'].resample(\'W\').min() daily_std = df_resampled[\'value\'].resample(\'D\').std() # Output the first 10 rows after adding rolling mean print(df_resampled.head(10)) # Print the computed statistical metrics print(\'Daily Sum:n\', daily_sum.head(10)) print(\'Daily Mean:n\', daily_mean.head(10)) print(\'Weekly Max:n\', weekly_max.head(10)) print(\'Weekly Min:n\', weekly_min.head(10)) print(\'Daily Std Dev:n\', daily_std.head(10))"},{"question":"**Objective:** Test your understanding of seaborn by customizing color palettes and creating a complex visualization. **Task:** You are given a dataset containing information about different species of the Iris flower (`iris`). Your task is to: 1. Create a dark palette starting from black to a specified main color (`#4CAF50`, which is a shade of green) and use this palette for the plot. 2. Increase the number of colors in the palette to 10. 3. Create a scatter plot using the seaborn `relplot` function with the following specifications: - x-axis: `sepal_length` - y-axis: `sepal_width` - Hue: Different species of the Iris flower - Add a linear regression line to each species\' data points. - Use the customized dark palette for the plot. **Dataset:** You can load the iris dataset directly using seaborn: ```python iris = sns.load_dataset(\'iris\') ``` **Constraints:** 1. Ensure that the plot is clear and properly labeled. 2. Apply the specified customized palette to the plot. 3. Add regression lines for better clarity. **Input and Output Formats:** *Input:* There are no direct inputs; you will load the dataset using seaborn. *Output:* A single plot with the specifications mentioned above. **Implementation Requirements:** - Use seaborn\'s built-in functions to load the dataset and create the plot. - Ensure that the plot is displayed properly without errors. Here\'s an example of how your code should look: ```python # Import necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\'iris\') # Create a dark palette palette = sns.dark_palette(\\"#4CAF50\\", n_colors=10, as_cmap=False) # Create a scatter plot with a regression line sns.set_theme() plot = sns.relplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette, kind=\'scatter\') plot.map(plt.plot, \'sepal_length\', \'sepal_width\', color=\'sepal_color\', markers=True) plt.show() ``` Make sure to customize the palette and plot as per the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\'iris\') # Create a dark palette palette = sns.dark_palette(\\"#4CAF50\\", n_colors=10, as_cmap=False) # Create a scatter plot with a regression line sns.set_theme() plot = sns.lmplot(data=iris, x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette, height=6, aspect=1.5, scatter_kws={\'s\': 50, \'alpha\': 0.7}, ci=None) plt.xlabel(\\"Sepal Length\\") plt.ylabel(\\"Sepal Width\\") plt.title(\\"Scatter plot of Sepal Length vs. Sepal Width with Regression Line for Iris Species\\") # Show the plot plt.show()"},{"question":"Objective Implement a class in Python that effectively demonstrates knowledge of buffer manipulation, direct memory access, and data integrity. Problem Statement You are to implement a `CustomBuffer` class that simulates a simple multi-dimensional array with direct access to the underlying memory buffer. The `CustomBuffer` should support the following operations: 1. **Initialization**: The class should be initialized with a shape and an optional format string. 2. **Set Item**: Ability to set an item at a specific index. 3. **Get Item**: Ability to get an item at a specific index. 4. **Check Contiguity**: Function to check if the buffer is contiguous in \'C\', \'F\', or \'A\' style. 5. **View Info**: Print detailed information about the buffer, including the shape, strides, and memory addresses. Specifications - **Initialization**: ```python def __init__(self, shape, format=\'B\') ``` - `shape`: a tuple indicating the shape of the multi-dimensional array. - `format`: an optional string indicating the format of the buffer elements. Default is \'B\' for unsigned bytes. - **Set Item**: ```python def set_item(self, indices, value) ``` - `indices`: a tuple of indices where the item should be set. - `value`: the value to set at the specified indices. - **Get Item**: ```python def get_item(self, indices) ``` - `indices`: a tuple of indices to retrieve the item from. - **Check Contiguity**: ```python def is_contiguous(self, style) ``` - `style`: can be \'C\', \'F\', or \'A\' for C-style, Fortran-style, or either. - Should return a boolean indicating whether the buffer is contiguous in the specified style. - **View Info**: ```python def view_info(self) ``` - Prints detailed information about the buffer, including shape, strides, and memory addresses of elements. Example ```python # Create a 2x3 buffer of unsigned bytes buf = CustomBuffer((2, 3)) # Set values buf.set_item((0, 0), 1) buf.set_item((0, 1), 2) buf.set_item((0, 2), 3) buf.set_item((1, 0), 4) buf.set_item((1, 1), 5) buf.set_item((1, 2), 6) # Get values assert buf.get_item((0, 0)) == 1 assert buf.get_item((1, 2)) == 6 # Check contiguity assert buf.is_contiguous(\'C\') == True assert buf.is_contiguous(\'F\') == False # View buffer info buf.view_info() ``` Constraints - The elements in the buffer should fit into the memory allowed by the specified shape. - Memory operations should ensure integrity and contiguity as required. - You must handle possible errors gracefully, such as accessing out-of-bounds indices. **Note**: For simplification, actual low-level memory manipulations should be simulated using Python\'s capabilities (e.g., using lists to represent the buffer) rather than using C-level APIs directly.","solution":"import numpy as np class CustomBuffer: def __init__(self, shape, format=\'B\'): self.shape = shape self.format = format self.buffer = np.zeros(shape, self._numpy_dtype()) def _numpy_dtype(self): # Convert format string to numpy dtype format_map = { \'B\': np.uint8, \'H\': np.uint16, \'I\': np.uint32, \'L\': np.uint64, \'b\': np.int8, \'h\': np.int16, \'i\': np.int32, \'l\': np.int64, \'f\': np.float32, \'d\': np.float64, } return format_map[self.format] def set_item(self, indices, value): try: self.buffer[indices] = value except IndexError: raise IndexError(\\"Indices out of bounds\\") def get_item(self, indices): try: return self.buffer[indices] except IndexError: raise IndexError(\\"Indices out of bounds\\") def is_contiguous(self, style): if style == \'C\': return self.buffer.flags[\'C_CONTIGUOUS\'] elif style == \'F\': return self.buffer.flags[\'F_CONTIGUOUS\'] elif style == \'A\': return self.buffer.flags[\'C_CONTIGUOUS\'] or self.buffer.flags[\'F_CONTIGUOUS\'] else: raise ValueError(\\"Invalid contiguity style, must be \'C\', \'F\', or \'A\'\\") def view_info(self): print(f\\"Shape: {self.buffer.shape}\\") print(f\\"Strides: {self.buffer.strides}\\") print(f\\"Memory address of buffer start: {self.buffer.__array_interface__[\'data\'][0]}\\")"},{"question":"In this task, you will leverage the `webbrowser` module to implement a function that systematically opens a list of URLs in different browser types, handling errors gracefully and ensuring each URL is loaded successfully in at least one browser. Function Signature ```python def batch_open_urls(urls: List[str], browsers: List[str]) -> Dict[str, Dict[str, bool]]: Opens a list of URLs in a list of browser types. Parameters: urls (List[str]): A list of URLs to be opened. browsers (List[str]): A list of browser type names to use for opening URLs. Returns: dict: A dictionary where the keys are URLs and the values are dictionaries, which map browser types to a boolean indicating if the URL was successfully opened. ``` Input 1. `urls`: A list of strings where each string is a URL. 2. `browsers`: A list of strings representing browser types (as mentioned in the webbrowser module documentation). Output - A dictionary where each key is a URL and each value is another dictionary. This inner dictionary has browser types as keys and booleans as values indicating whether the URL was successfully opened in that browser. Example ```python urls = [\\"https://www.python.org\\", \\"https://www.invalid-url.org\\"] browsers = [\\"firefox\\", \\"safari\\", \\"chrome\\"] result = batch_open_urls(urls, browsers) print(result) ``` Expected output (example format, actual output may depend on your environment): ```python { \\"https://www.python.org\\": { \\"firefox\\": True, \\"safari\\": True, \\"chrome\\": True }, \\"https://www.invalid-url.org\\": { \\"firefox\\": False, \\"safari\\": False, \\"chrome\\": False } } ``` Constraints - You should handle the scenario where an invalid browser type is provided (treat it as if the browser failed to open the URL). - If all provided browsers fail to open a URL, ensure the failure is reflected appropriately in the output. - Ensure compatibility with both Windows and non-Windows platforms. Performance Requirements - The function should attempt to open each URL in each browser serially (one after another). Parallel browser launches are not necessary. Additional Notes - The `webbrowser.Error` should be caught to handle cases where a browser fails to open a URL. - Use appropriate messages or logging to trace and report browser failures (this is optional for your implementation but recommended for debugging). Implement the function as described using the \\"webbrowser\\" module.","solution":"import webbrowser from typing import List, Dict def batch_open_urls(urls: List[str], browsers: List[str]) -> Dict[str, Dict[str, bool]]: Opens a list of URLs in a list of browser types. Parameters: urls (List[str]): A list of URLs to be opened. browsers (List[str]): A list of browser type names to use for opening URLs. Returns: dict: A dictionary where the keys are URLs and the values are dictionaries, which map browser types to a boolean indicating if the URL was successfully opened. result = {} for url in urls: result[url] = {} for browser_name in browsers: try: browser = webbrowser.get(browser_name) success = browser.open(url) result[url][browser_name] = success except webbrowser.Error: result[url][browser_name] = False return result"},{"question":"# PyTorch Coding Assessment Objective: Implement a function that takes a multi-dimensional tensor and returns a new tensor that has been reshaped based on specific rules defined by manipulating its dimensions. Problem Statement: Write a function `reshape_tensor_based_on_size` that takes a PyTorch tensor `input_tensor` and performs the following transformations: 1. Retrieve the size of `input_tensor` using the `size()` method. 2. Create a new size by reversing the dimensions of the original tensor. 3. Reshape `input_tensor` to the newly created size. 4. Return the reshaped tensor. Your implementation should handle the possibility of reshaping a tensor with dimensions of varying lengths. Input: - `input_tensor` (torch.Tensor): A PyTorch tensor of any shape with at least one dimension. Output: - A PyTorch tensor reshaped to have dimensions in reverse order compared to the input tensor. Example: ```python import torch def reshape_tensor_based_on_size(input_tensor): # Your implementation here # Example usage tensor = torch.ones(2, 3, 4) reshaped_tensor = reshape_tensor_based_on_size(tensor) print(reshaped_tensor.size()) # Output should be torch.Size([4, 3, 2]) ``` Constraints: - You must use the `torch.Size` class to handle tensor dimensions. - Ensure that the function can handle tensors with varying lengths and dimensions. - The function should not alter the original tensor. Performance Requirements: - The solution should be efficient and make use of PyTorch\'s internal methods for manipulating tensor sizes and shapes.","solution":"import torch def reshape_tensor_based_on_size(input_tensor): Reshapes the input tensor by reversing its dimensions. Args: input_tensor (torch.Tensor): A PyTorch tensor of any shape with at least one dimension. Returns: torch.Tensor: A tensor reshaped to have dimensions in reverse order compared to the input tensor. # Retrieve the size of input_tensor original_size = input_tensor.size() # Create a new size by reversing the dimensions of the original tensor new_size = original_size[::-1] # Reshape the tensor to the newly created size reshaped_tensor = input_tensor.view(*new_size) return reshaped_tensor # Example usage tensor = torch.ones(2, 3, 4) reshaped_tensor = reshape_tensor_based_on_size(tensor) print(reshaped_tensor.size()) # Output should be torch.Size([4, 3, 2])"},{"question":"# Advanced Python Assessment Problem Statement You are given the task of processing a text file using the `pipes` module in Python. The objective is to read a file, convert all text to uppercase, then replace all spaces with underscores, and finally write the result to a new file. Implement the function `process_file(input_file: str, output_file: str) -> None` that achieves this using the `pipes.Template` class. Your function should be defined as follows: Function Signature ```python def process_file(input_file: str, output_file: str) -> None: ``` Input - `input_file` (str): The path to the input text file. - `output_file` (str): The path where the output text file will be saved. Output - The function should not return anything. It should save the processed text in `output_file`. Constraints - The input file will exist and be readable. - The output file path will be valid and writable. - You may assume the input file is not empty. - No additional libraries should be used for file processing apart from the built-in `pipes` module and standard file I/O. Example Suppose the content of `input.txt` is: ``` hello world this is a test ``` After processing, the content of `output.txt` should be: ``` HELLO_WORLD THIS_IS_A_TEST ``` Instructions 1. Initialize a `pipes.Template` object. 2. Use the `append()` method to add the commands for converting to uppercase and replacing spaces with underscores. 3. Open the `input_file` using `Template.open()` in read mode. 4. Write the processed content to the `output_file`. Note - Make use of the `pipes.Template` functionality as described in the documentation. - Ensure that your solution handles file operations correctly to avoid input/output errors. Good luck!","solution":"import pipes def process_file(input_file: str, output_file: str) -> None: template = pipes.Template() template.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') template.append(\'tr \\" \\" \\"_\\"\', \'--\') with template.open(input_file, \'r\') as f_in, open(output_file, \'w\') as f_out: for line in f_in: f_out.write(line)"},{"question":"**Instructions:** You are required to implement a Python class `CustomRobotFileParser` that extends the functionality of `urllib.robotparser.RobotFileParser`. Your implementation should include a custom method to filter and return all URLs allowed to be fetched by a specific useragent based on the rules in the `robots.txt` file. **Specifications:** 1. **Class Definition:** - Define a class `CustomRobotFileParser` that inherits from `urllib.robotparser.RobotFileParser`. 2. **Method Implementation:** - Implement a method `allowed_urls(useragent, urls)` that: - Takes a user agent string `useragent` and a list of URLs `urls` as input. - Returns a list of URLs that the specified `useragent` is allowed to fetch according to the parsed `robots.txt` rules. 3. **Input:** - A string representing the user agent. - A list of strings where each string is a URL. 4. **Output:** - A list of URLs that the user agent is allowed to fetch. **Example:** ```python # Assuming the Robots.txt file at http://www.example.com/robots.txt allows the following: # User-agent: * # Disallow: /private/ import urllib.robotparser from typing import List class CustomRobotFileParser(urllib.robotparser.RobotFileParser): def allowed_urls(self, useragent: str, urls: List[str]) -> List[str]: allowed = [] for url in urls: if self.can_fetch(useragent, url): allowed.append(url) return allowed # Usage Example crp = CustomRobotFileParser() crp.set_url(\'http://www.example.com/robots.txt\') # assumes this is a valid URL with robots.txt crp.read() useragent = \\"MyUserAgent/1.0\\" urls = [\\"http://www.example.com/public/\\", \\"http://www.example.com/private/\\"] print(crp.allowed_urls(useragent, urls)) # Output should be: [\\"http://www.example.com/public/\\"] ``` Ensure that your implementation correctly utilizes the methods provided by `RobotFileParser` and handles the specified input and output formats accurately. Also, remember to handle possible exceptions that may arise during the reading of the `robots.txt` file. **Constraints:** - You can assume that the URLs list contains valid and reachable URLs for the `robots.txt` file specified. - The `robots.txt` file is assumed to be well-formed.","solution":"import urllib.robotparser from typing import List class CustomRobotFileParser(urllib.robotparser.RobotFileParser): def allowed_urls(self, useragent: str, urls: List[str]) -> List[str]: Returns a list of URLs that the specified useragent is allowed to fetch according to the parsed robots.txt rules. Parameters: useragent (str): The user agent string. urls (List[str]): List of URLs to check. Returns: List[str]: List of URLs allowed to be fetched. allowed = [] for url in urls: if self.can_fetch(useragent, url): allowed.append(url) return allowed"},{"question":"# Custom Scikit-learn Estimator You are tasked with creating a custom scikit-learn compatible estimator that performs k-Nearest Neighbors classification. Your goal is to implement the `KNNClassifier` class, which behaves similarly to scikit-learn\'s `KNeighborsClassifier`, but with a simplified and customized fitting and prediction mechanism. # Requirements: 1. **Initialization**: The constructor should take a parameter `n_neighbors` which specifies the number of nearest neighbors to consider for classification. 2. **Fitting**: Implement the `fit(X, y)` method to store the training data. 3. **Prediction**: Implement the `predict(X)` method to predict the class of each input sample using the majority class label of its nearest neighbors. 4. **Distance Metric**: Use Euclidean distance as the distance metric to determine the nearest neighbors. 5. **Parameter Methods**: Implement `get_params(deep=True)` and `set_params(**params)` methods for compatibility with scikit-learn\'s model selection and pipelines. # Specifications: - The input `X` to `fit` and `predict` will be a 2D array of shape `(n_samples, n_features)`. - The input `y` to `fit` will be a 1D array of shape `(n_samples,)` with class labels. - The `predict` method should return a 1D array of predicted class labels of shape `(n_samples,)`. # Example Usage: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # Use the custom KNNClassifier class KNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5): self.n_neighbors = n_neighbors def fit(self, X, y): X, y = validate_data(self, X, y) self.X_ = X self.y_ = y self.classes_ = unique_labels(y) return self def predict(self, X): check_is_fitted(self) X = validate_data(self, X, reset=False) predictions = [] for x in X: distances = np.linalg.norm(self.X_ - x, axis=1) nearest_indices = np.argsort(distances)[:self.n_neighbors] nearest_labels = self.y_[nearest_indices] majority_label = np.bincount(nearest_labels).argmax() predictions.append(majority_label) return np.array(predictions) def get_params(self, deep=True): return {\\"n_neighbors\\": self.n_neighbors} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self # Load dataset data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3) # Initialize and fit KNNClassifier knn = KNNClassifier(n_neighbors=3) knn.fit(X_train, y_train) # Predict y_pred = knn.predict(X_test) print(\\"Accuracy:\\", accuracy_score(y_test, y_pred)) ``` Implement the `KNNClassifier` class as described. Your implementation should pass the example usage scenario and correctly integrate with scikit-learn pipelines and model selection utilities. # Notes: - Make sure to handle edge cases, such as invalid inputs to `fit` and `predict`. - Ensure your code follows scikit-learn conventions for public and private attributes.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_array, check_X_y from sklearn.utils.multiclass import unique_labels class KNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5): self.n_neighbors = n_neighbors def fit(self, X, y): # Validate the input data X, y = check_X_y(X, y) # Store the training data self.X_ = X self.y_ = y # Calculate and store the unique class labels self.classes_ = unique_labels(y) return self def predict(self, X): # Check if the classifier has been fitted check_is_fitted(self) # Validate the input data X = check_array(X) predictions = [] # Compute predictions for each input data sample for x in X: # Compute the Euclidean distances between the input sample and all training samples distances = np.linalg.norm(self.X_ - x, axis=1) # Get the indices of the nearest neighbors nearest_indices = np.argsort(distances)[:self.n_neighbors] # Get the labels of the nearest neighbors nearest_labels = self.y_[nearest_indices] # Determine the majority label among the nearest neighbors majority_label = np.bincount(nearest_labels).argmax() # Append the predicted label to the list of predictions predictions.append(majority_label) return np.array(predictions) def get_params(self, deep=True): return {\\"n_neighbors\\": self.n_neighbors} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self"},{"question":"# Asynchronous File Operations with Platform Constraints You are tasked to create a Python script that performs asynchronous file read operations. The script must handle reading from multiple files concurrently and process their contents. Given the platform-specific constraints described below, your solution must appropriately adapt the implementation based on the underlying operating system. Platform-Specific Constraints - **Windows**: - Use `asyncio.ProactorEventLoop` for creating the event loop. - ProactorEventLoop does not support `loop.add_reader()` or `loop.add_writer()` methods. - **macOS (<= 10.8)**: - Use `selectors.KqueueSelector` by default. - On older versions of macOS (10.6 to 10.8), `KqueueSelector` does not support character devices. If the OS version is confirmed to be <= 10.8, switch to using `selectors.SelectSelector`. Requirements 1. Your script should define an asynchronous function `read_file_async(file_path: str) -> str` that reads the contents of the given file and returns it as a string. 2. Implement a function `main(files: List[str]) -> None` that takes a list of file paths, reads all the files concurrently using the implemented asynchronous functionality, and prints each file content. 3. The implementation should adapt to the platform constraints mentioned above. Input - A list of file paths `files` to be read asynchronously. Output - Print the content of each file. Constraints - Assume the files are small enough that reading them asynchronously is feasible. - The program should handle exceptions gracefully and continue processing other files even if one file read fails. Example ```python import asyncio async def read_file_async(file_path: str) -> str: # Implement the file reading here pass async def main(files: List[str]) -> None: # Implement the concurrent file reading here pass if __name__ == \\"__main__\\": import sys files_to_read = sys.argv[1:] loop = asyncio.get_event_loop() loop.run_until_complete(main(files_to_read)) ``` - Files: - `file1.txt` - `file2.txt` - `file3.txt` Output: ``` Contents of file1.txt Contents of file2.txt Contents of file3.txt ``` Ensure that your solution properly adapts to different platforms as specified and handles potential file read errors gracefully.","solution":"import asyncio import os import platform import selectors # Determine the appropriate event loop policy and selector based on platform def configure_event_loop(): current_platform = platform.system() if current_platform == \\"Windows\\": asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) elif current_platform == \\"Darwin\\": mac_version = platform.mac_ver()[0] if mac_version and float(mac_version[:4]) <= 10.8: selector = selectors.SelectSelector() else: selector = selectors.KqueueSelector() asyncio.set_event_loop_policy(asyncio.DefaultEventLoopPolicy(selector=selector)) # Asynchronous function to read file contents async def read_file_async(file_path: str) -> str: try: async with aiofiles.open(file_path, \'r\') as file: return await file.read() except Exception as e: return f\\"Error reading {file_path}: {e}\\" # Function to read multiple files concurrently and print their contents async def main(files: list) -> None: tasks = [read_file_async(file) for file in files] results = await asyncio.gather(*tasks, return_exceptions=True) for result in results: print(result) if __name__ == \\"__main__\\": import sys files_to_read = sys.argv[1:] configure_event_loop() loop = asyncio.get_event_loop() loop.run_until_complete(main(files_to_read))"},{"question":"# Resource Management and Debugging in Python Development Mode Background The Python Development Mode introduces additional runtime checks that are too expensive to be enabled by default. This mode is designed to help developers identify and fix issues in their code by emitting warnings and enabling certain debugging features. Task You are tasked with writing a Python script that reads from a file, processes the data, and generates some output. The script must adhere to best practices for resource management to avoid common issues like resource leaks. Additionally, any potential issues detected in the Development Mode should be gracefully handled. Your script should perform the following steps: 1. **Open a text file** passed as a command-line argument. 2. **Count the number of lines** in the file and print this number. 3. **Detect and handle any warnings/errors** that might be emitted in Python\'s Development Mode, and ensure all resources are properly managed and closed. Specifications 1. **Input**: A file path passed as a command-line argument. 2. **Output**: - Print the number of lines in the file. - If any warnings/errors are detected (e.g., unclosed file descriptor), handle them and print an appropriate message. 3. **Constraints**: - Ensure that the file is properly closed after processing to avoid `ResourceWarning`. - Do not use `os.close(fp.fileno())`; use the proper context management techniques to handle file resources. Example Here\'s how your script might be executed: ```sh python3 script.py example.txt Number of lines: 42 ``` Here\'s an example of how your script should behave in Development Mode: ```sh python3 -X dev script.py example.txt Number of lines: 42 No unclosed file warnings. ``` Hints - Use `with open(...) as file` to ensure that the file is automatically closed after the block is exited. - Leverage Python\'s warnings library to handle any `ResourceWarning`. Submission Submit a Python script named `script.py` that implements the above functionality, ensuring compliance with best practices for resource management and proper handling of runtime warnings/errors in Development Mode.","solution":"import sys import warnings def count_lines_in_file(file_path): Counts the number of lines in the given file and returns the count. try: with open(file_path, \'r\') as file: line_count = sum(1 for line in file) return line_count except Exception as e: print(f\\"Error occurred while processing the file: {e}\\") return None if __name__ == \\"__main__\\": # Check if a file path is provided as command-line argument if len(sys.argv) != 2: print(\\"Usage: python3 script.py <file_path>\\") sys.exit(1) file_path = sys.argv[1] with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\", category=ResourceWarning) line_count = count_lines_in_file(file_path) if line_count is not None: print(f\\"Number of lines: {line_count}\\") # Check if any ResourceWarnings were raised if any(issubclass(warn.category, ResourceWarning) for warn in w): print(\\"Warning: Unclosed file detected.\\") else: print(\\"No unclosed file warnings.\\")"},{"question":"**Coding Assessment Question** # Command-Line Tool for Text Processing You are tasked with creating a command-line tool using the deprecated `optparse` module. This tool should be versatile and allow users to perform various text processing tasks on a file. The functionality includes: 1. **Reading Input File**: The tool should read a specified text file. 2. **Word Count (`-w`, `--wordcount`)**: Count the number of words in the file. 3. **Character Count (`-c`, `--charcount`)**: Count the number of characters in the file. 4. **Replace Word (`-r <word1> <word2>`, `--replace <word1> <word2>`)**: Replace all occurrences of `word1` with `word2` in the file. 5. **Output to New File (`-o <filename>`, `--output <filename>`)**: Optionally write the modified text to a new file. # Requirements 1. **Command-Line Options**: - `-w` or `--wordcount`: No argument needed. - `-c` or `--charcount`: No argument needed. - `-r` or `--replace`: Requires exactly two string arguments. - `-o` or `--output`: Requires one string argument. - At least one of `-w`, `-c`, or `-r` must be specified. 2. **Default Behavior**: - If no output file is specified, display the output to the console. # Input and Output Formats - Input: Command-line arguments should be passed to your script. - Output: Based on the options provided, the tool should either display the result on the console or write to a file. # Constraints and Limitations - The script should handle incorrect command-line arguments gracefully and provide meaningful error messages. - The tool should handle large files efficiently. # Implementation Your task is to implement the function `main()` that sets up the `OptionParser`, defines the necessary options, and parses the command-line arguments. Then, based on the parsed options, carry out the required text processing tasks. Below is the skeleton code you should complete: ```python import sys from optparse import OptionParser, OptionValueError def replace_words(text, word1, word2): return text.replace(word1, word2) def main(): usage = \\"usage: %prog [options] inputfile\\" parser = OptionParser(usage=usage) parser.add_option(\\"-w\\", \\"--wordcount\\", action=\\"store_true\\", dest=\\"wordcount\\", help=\\"Count the number of words in the file\\") parser.add_option(\\"-c\\", \\"--charcount\\", action=\\"store_true\\", dest=\\"charcount\\", help=\\"Count the number of characters in the file\\") parser.add_option(\\"-r\\", \\"--replace\\", nargs=2, dest=\\"replace\\", help=\\"Replace occurrences of the first word with the second word\\") parser.add_option(\\"-o\\", \\"--output\\", metavar=\\"FILE\\", dest=\\"output\\", help=\\"Write the output to FILE\\") (options, args) = parser.parse_args() if len(args) != 1: parser.error(\\"incorrect number of arguments. Please specify an input file.\\") inputfile = args[0] if not options.wordcount and not options.charcount and not options.replace: parser.error(\\"Must specify at least one of -w, -c, or -r options\\") try: with open(inputfile, \'r\') as file: text = file.read() except FileNotFoundError: parser.error(f\\"File {inputfile} not found.\\") output_text = text if options.wordcount: word_count = len(text.split()) print(f\\"Word Count: {word_count}\\") if options.charcount: char_count = len(text) print(f\\"Character Count: {char_count}\\") if options.replace: output_text = replace_words(text, options.replace[0], options.replace[1]) if options.output: with open(options.output, \'w\') as file: file.write(output_text) print(f\\"Output written to {options.output}\\") elif options.replace: print(output_text) if __name__ == \\"__main__\\": main() ``` Complete the implementation of the `main()` function by handling all specified command-line options and ensuring proper error handling and output formatting according to the requirements.","solution":"import sys from optparse import OptionParser, OptionValueError def replace_words(text, word1, word2): return text.replace(word1, word2) def main(): usage = \\"usage: %prog [options] inputfile\\" parser = OptionParser(usage=usage) parser.add_option(\\"-w\\", \\"--wordcount\\", action=\\"store_true\\", dest=\\"wordcount\\", help=\\"Count the number of words in the file\\") parser.add_option(\\"-c\\", \\"--charcount\\", action=\\"store_true\\", dest=\\"charcount\\", help=\\"Count the number of characters in the file\\") parser.add_option(\\"-r\\", \\"--replace\\", nargs=2, dest=\\"replace\\", help=\\"Replace occurrences of the first word with the second word\\") parser.add_option(\\"-o\\", \\"--output\\", metavar=\\"FILE\\", dest=\\"output\\", help=\\"Write the output to FILE\\") (options, args) = parser.parse_args() if len(args) != 1: parser.error(\\"Incorrect number of arguments. Please specify an input file.\\") inputfile = args[0] if not options.wordcount and not options.charcount and not options.replace: parser.error(\\"Must specify at least one of -w, -c, or -r options\\") try: with open(inputfile, \'r\') as file: text = file.read() except FileNotFoundError: parser.error(f\\"File {inputfile} not found.\\") output_text = text if options.wordcount: word_count = len(text.split()) print(f\\"Word Count: {word_count}\\") if options.charcount: char_count = len(text) print(f\\"Character Count: {char_count}\\") if options.replace: output_text = replace_words(text, options.replace[0], options.replace[1]) if options.output: with open(options.output, \'w\') as file: file.write(output_text) print(f\\"Output written to {options.output}\\") elif options.replace: print(output_text) if __name__ == \\"__main__\\": main()"},{"question":"# Python Coding Assessment Problem Objective Demonstrate your understanding and ability to interact with installed Python package metadata using the `importlib.metadata` package. You will write a function that provides detailed information about all installed packages with entry points in a specific group. Problem Statement Write a function `package_entry_points_info(group: str) -> dict` that retrieves and returns detailed information about all installed packages containing entry points in a specified `group`. The function should: * Accept a single string parameter `group` which specifies the entry point group to filter by. * Return a dictionary where: * Each key is the name of a package. * Each value is another dictionary with keys `version`, and `entry_points`: * `version`: The version of the package as a string. * `entry_points`: A list of tuples where each tuple contains the name and the value of the entry point within the specified group. Specifications * You must use the `importlib.metadata` package to fetch the required information. * Handle exceptions appropriately and ensure the function does not crash on errors. * If no packages match the specified group, return an empty dictionary. Example ```python # Example installed packages might include `wheel`, `setuptools`, etc. group = \\"console_scripts\\" output = package_entry_points_info(group) print(output) # Possible output (actual result may vary based on installed packages): # { # \'wheel\': { # \'version\': \'0.36.2\', # \'entry_points\': [(\'wheel\', \'wheel.cli:main\')] # }, # \'setuptools\': { # \'version\': \'52.0.0\', # \'entry_points\': [(\'easy_install\', \'setuptools.command.easy_install:main\')] # } # } ``` Constraints * Assume that you are using Python 3.10 or newer. * Performance considerations are less critical for this problem, but strive for clarity and correctness in your solution. Notes * Refer to the `importlib.metadata` documentation for details on how to retrieve and interact with metadata. Good luck!","solution":"import importlib.metadata def package_entry_points_info(group: str) -> dict: Retrieves information about all installed packages with entry points in a specified group. Args: - group (str): The entry point group to filter by. Returns: - dict: Dictionary containing package information, where each key is a package name and each value is a dictionary with \'version\' and \'entry_points\' (a list of (name, value) tuples). entries = {} for dist in importlib.metadata.distributions(): entry_points = [ (ep.name, ep.value) for ep in dist.entry_points if ep.group == group ] if entry_points: entries[dist.metadata[\'Name\']] = { \'version\': dist.version, \'entry_points\': entry_points } return entries"},{"question":"# File & Directory Management Task You are required to create a script that performs the following operations: 1. Moves a directory from one location to another. 2. After moving, create a compressed archive (zip format) of the moved directory. 3. Calculate and print the disk usage of the destination directory after the move. # Function Specification Implement the function `manage_directory(src_dir: str, dst_dir: str, archive_name: str) -> None` where: - `src_dir` (str): Path to the source directory to be moved. - `dst_dir` (str): Path to the destination directory where the source will be moved. - `archive_name` (str): The name for the created archive file (do not include \\".zip\\" extension, the function should add it automatically). # Input Constraints - The given paths for directories (`src_dir` and `dst_dir`) will always be valid, and you have the necessary permissions. - The source directory will not be empty. # Output - The function doesnâ€™t return anything, but it should print the following information: 1. Confirmation that the directory was moved. 2. The path to the created archive. 3. The total, used, and free disk space of the destination drive, in bytes, after the move. # Example ```python src_dir = \'/path/to/source_directory\' dst_dir = \'/path/to/destination_directory\' archive_name = \'my_archive\' manage_directory(src_dir, dst_dir, archive_name) ``` Possible Output: ``` Directory moved from /path/to/source_directory to /path/to/destination_directory Archive created at /path/to/destination_directory/my_archive.zip Disk usage at /path/to/destination_directory - Total: 50000000000, Used: 20000000000, Free: 30000000000 ``` # Notes - Use the `shutil.move` function to move the directory. - Use the `shutil.make_archive` function to create the archive. - Use the `shutil.disk_usage` function to get the disk usage. Good luck!","solution":"import shutil import os def manage_directory(src_dir: str, dst_dir: str, archive_name: str) -> None: Moves a directory from one location to another, creates a compressed archive of the moved directory, and prints disk usage at the destination. :param src_dir: Path to the source directory. :param dst_dir: Path to the destination directory. :param archive_name: Name for the created archive. # Move the source directory to the destination shutil.move(src_dir, dst_dir) print(f\\"Directory moved from {src_dir} to {os.path.join(dst_dir, os.path.basename(src_dir))}\\") # Create a compressed archive of the moved directory archive_path = shutil.make_archive(os.path.join(dst_dir, archive_name), \'zip\', dst_dir) print(f\\"Archive created at {archive_path}\\") # Calculate and print the disk usage of the destination directory usage = shutil.disk_usage(dst_dir) total, used, free = usage.total, usage.used, usage.free print(f\\"Disk usage at {dst_dir} - Total: {total}, Used: {used}, Free: {free}\\")"},{"question":"Question: Your task is to implement a function `generate_custom_plots(df)` that takes a DataFrame `df` with at least these columns: `A`, `B`, `C`, `D`, and `Category`. The DataFrame represents some categorized data over time. Here is what your function should do: 1. **Line Plot**: - Create a line plot of columns `A`, `B`, `C`, and `D` vs. the DataFrame\'s index. - Customize the line styles and colors. 2. **Scatter Plot**: - Create a scatter plot of `A` against `B` and `C` against `D`, all on the same axes. - Color the points differently based on the `Category` column. - Use different point markers for different categories. 3. **Box Plot**: - Generate a grouped box plot of columns `A` and `B` group by `Category` values. 4. **Histogram**: - Create a histogram of columns `A`, `B`, and `C` with specified bins. - Overlay the histograms for comparison. 5. **Hexbin Plot**: - Create a hexbin plot of `A` vs `B`. 6. **Pie Plot**: - Generate a pie chart showing the distribution of values in the `Category` column. # Expected Function Signature: ```python import pandas as pd import matplotlib.pyplot as plt def generate_custom_plots(df: pd.DataFrame): Generates various custom plots for the input DataFrame. Args: df: pd.DataFrame - A DataFrame containing at least columns \'A\', \'B\', \'C\', \'D\', and \'Category\' Returns: None - This function should only generate and display the plots. ``` # Constraints: 1. The DataFrame `df` will always contain at least 100 rows. 2. The \'Category\' column contains at least 3 unique values. 3. The values in columns `A`, `B`, `C`, and `D` are numeric. # Example Usage: ```python # Example DataFrame df = pd.DataFrame({ \'A\': np.random.randn(100), \'B\': np.random.randn(100), \'C\': np.random.randn(100), \'D\': np.random.randn(100), \'Category\': np.random.choice([\'Type1\', \'Type2\', \'Type3\'], 100) }) generate_custom_plots(df) ``` Ensure your plots are informative and well-labeled, making use of Matplotlib\'s customization options as described.","solution":"import pandas as pd import matplotlib.pyplot as plt import numpy as np import seaborn as sns def generate_custom_plots(df: pd.DataFrame): Generates various custom plots for the input DataFrame. Args: df: pd.DataFrame - A DataFrame containing at least columns \'A\', \'B\', \'C\', \'D\', and \'Category\' Returns: None - This function should only generate and display the plots. # Line Plot plt.figure(figsize=(10, 6)) plt.plot(df.index, df[\'A\'], label=\'A\', linestyle=\'-\', color=\'blue\') plt.plot(df.index, df[\'B\'], label=\'B\', linestyle=\'--\', color=\'orange\') plt.plot(df.index, df[\'C\'], label=\'C\', linestyle=\'-.\', color=\'green\') plt.plot(df.index, df[\'D\'], label=\'D\', linestyle=\':\', color=\'red\') plt.xlabel(\'Index\') plt.ylabel(\'Values\') plt.title(\'Line Plot of A, B, C, and D\') plt.legend() plt.grid(True) plt.show() # Scatter Plot plt.figure(figsize=(10, 6)) categories = df[\'Category\'].unique() markers = [\'o\', \'^\', \'s\'] for cat, marker in zip(categories, markers): subset = df[df[\'Category\'] == cat] plt.scatter(subset[\'A\'], subset[\'B\'], label=f\'{cat} A vs B\', marker=marker) plt.scatter(subset[\'C\'], subset[\'D\'], label=f\'{cat} C vs D\', marker=marker) plt.xlabel(\'Values\') plt.ylabel(\'Values\') plt.title(\'Scatter Plot of A vs B and C vs D by Category\') plt.legend() plt.grid(True) plt.show() # Grouped Box Plot plt.figure(figsize=(10, 6)) df_melted = pd.melt(df, id_vars=[\'Category\'], value_vars=[\'A\', \'B\'], var_name=\'Variable\', value_name=\'Value\') sns.boxplot(x=\'Category\', y=\'Value\', hue=\'Variable\', data=df_melted) plt.xlabel(\'Category\') plt.ylabel(\'Values\') plt.title(\'Box Plot of A and B Grouped by Category\') plt.legend(title=\'Variable\') plt.grid(True) plt.show() # Histogram plt.figure(figsize=(10, 6)) plt.hist(df[\'A\'], bins=30, alpha=0.5, label=\'A\') plt.hist(df[\'B\'], bins=30, alpha=0.5, label=\'B\') plt.hist(df[\'C\'], bins=30, alpha=0.5, label=\'C\') plt.xlabel(\'Values\') plt.ylabel(\'Frequency\') plt.title(\'Histogram of A, B, and C\') plt.legend() plt.grid(True) plt.show() # Hexbin Plot plt.figure(figsize=(10, 6)) plt.hexbin(df[\'A\'], df[\'B\'], gridsize=30, cmap=\'Blues\') plt.colorbar(label=\'Counts\') plt.xlabel(\'A\') plt.ylabel(\'B\') plt.title(\'Hexbin Plot of A vs B\') plt.grid(True) plt.show() # Pie Chart plt.figure(figsize=(8, 8)) category_counts = df[\'Category\'].value_counts() plt.pie(category_counts, labels=category_counts.index, autopct=\'%1.1f%%\', startangle=140) plt.title(\'Pie Chart of Category Distribution\') plt.show()"},{"question":"<|Analysis Begin|> The provided documentation is an extensive and detailed description of the `argparse` module, which is a Python library used for creating command-line interfaces. It includes explanations and examples of various functions and methods such as creating a parser, adding arguments, parsing arguments, handling different types and formats of input, managing mutually exclusive arguments, and handling sub-commands among other features. Key features provided in the documentation include: - Creating an ArgumentParser object - Adding different types of arguments (positional, optional, mutually exclusive groups) - Parsing command-line arguments - Customizing help messages and argument groups - Handling sub-commands - File handling via FileType objects - Customizing the way arguments are read from files Given this comprehensive documentation, the task can involve creating a command-line tool that encapsulates many of these features. <|Analysis End|> <|Question Begin|> # Command-Line Arithmetic Tool Objective Create a Python command-line tool using the `argparse` module that can perform basic arithmetic operations on a list of numbers. The tool should be able to read numbers from: - The command-line - A file with a prefix character It should also support various operations and provide detailed help messages. Requirements 1. **Input Sources**: - The tool should accept a list of numbers via the command line. - The tool should also accept a file containing numbers, using a prefix character `@`. 2. **Operations**: - The tool should support the following operations, which are mutually exclusive: - Sum (`--sum`) - Product (`--prod`) - Maximum (`--max`) - Minimum (`--min`) 3. **Help Message**: - The tool must provide a help message that describes how to use the tool, including descriptions of each operation. 4. **Constraints**: - Numbers entered should be integers. - The minimum number of elements for any operation is 2. - If no operation is specified, the tool should default to computing the sum. Specifications 1. **Create the ArgumentParser**: - Define a description for the tool. - Define an optional argument that allows reading numbers from a file (`fromfile_prefix_chars=\'@\'`). - Allow input of numbers as positional arguments. 2. **Define Operations**: - Define mutually exclusive operations: sum, product, maximum, and minimum. - Set default operation as sum. 3. **Implementation Details**: - Implement a function to handle the calculation based on the operation specified. Example Usage ```bash # To sum numbers from the command line python arith_tool.py 1 2 3 4 --sum 10 # To get the product of numbers from a file echo -e \\"1n2n3n4n\\" > numbers.txt python arith_tool.py @numbers.txt --prod 24 # To get the maximum from the command line python arith_tool.py 1 2 3 4 --max 4 # Help message python arith_tool.py -h ``` The help message should print detailed information about how to use the command-line tool. Submission Implement the `arith_tool.py` script according to the specifications above. Ensure that the script handles errors gracefully and provides meaningful error messages when the input is invalid.","solution":"import argparse import sys from functools import reduce import operator def arithmetic_tool(): parser = argparse.ArgumentParser( description=\'Command-Line Arithmetic Tool. Perform basic arithmetic operations on a list of numbers from either command-line or a file.\' ) parser.add_argument( \'numbers\', metavar=\'N\', type=int, nargs=\'+\', help=\'list of integers for the arithmetic operation\' ) group = parser.add_mutually_exclusive_group() group.add_argument(\'--sum\', action=\'store_true\', help=\'sum the numbers\') group.add_argument(\'--prod\', action=\'store_true\', help=\'multiply the numbers\') group.add_argument(\'--max\', action=\'store_true\', help=\'find the maximum number\') group.add_argument(\'--min\', action=\'store_true\', help=\'find the minimum number\') parser.set_defaults(func=calculate_sum) # Default operation is sum args = parser.parse_args() if args.sum: result = calculate_sum(args.numbers) elif args.prod: result = calculate_prod(args.numbers) elif args.max: result = calculate_max(args.numbers) elif args.min: result = calculate_min(args.numbers) else: result = calculate_sum(args.numbers) print(result) def calculate_sum(numbers): return sum(numbers) def calculate_prod(numbers): return reduce(operator.mul, numbers, 1) def calculate_max(numbers): return max(numbers) def calculate_min(numbers): return min(numbers) if __name__ == \'__main__\': arithmetic_tool()"},{"question":"Persistent Key-Value Store with \\"dbm\\" Package In this task, you are required to implement a key-value store using the `dbm` package. Your implementation should provide functionalities to create, read, update, delete, and list all the key-value pairs in the store. Additionally, you need to handle different types of databases, ensuring the operations are performed correctly. # Requirements 1. Implement a class `PersistentStore` with the following methods: - `__init__(self, db_name: str, db_type: str=\'c\')`: Initializes the database with the given name and type (defaults to creating a new database if it does not exist). - `set(self, key: str, value: str)`: Stores the `key-value` pair in the database. - `get(self, key: str) -> str`: Retrieves the value associated with the `key`. Returns `None` if the key doesn\'t exist. - `delete(self, key: str)`: Deletes the key-value pair from the database. - `list_keys(self) -> list`: Lists all the keys present in the database. - `close(self)`: Closes the database. 2. Handle exceptions appropriately and ensure the database is properly closed after operations. # Input and Output - Input: Actions to be performed on the database. - Output: Results based on the performed actions or state of the database after actions. # Constraints - Use only string keys and values. - Ensure database operations are optimized and handle errors gracefully. # Example Usage ```python # Initialize persistent store with default create mode store = PersistentStore(\'mydb\') # Set some key-value pairs store.set(\'name\', \'Alice\') store.set(\'age\', \'30\') # Get and print values print(store.get(\'name\')) # Output: Alice print(store.get(\'age\')) # Output: 30 # List all keys print(store.list_keys()) # Output: [\'name\', \'age\'] # Delete a key store.delete(\'age\') # Attempt to get deleted key print(store.get(\'age\')) # Output: None # Close the store store.close() ``` # Performance Considerations - The solution should handle databases with a large number of entries efficiently. - Ensure all operations such as addition and deletion maintain optimal performance and data integrity. **Note:** Make sure your implementation works with any of the `dbm.gnu`, `dbm.ndbm`, or `dbm.dumb` database types as per the system\'s available database formats.","solution":"import dbm class PersistentStore: def __init__(self, db_name: str, db_type: str=\'c\'): Initializes the database with the given name and type (default is to create if it does not exist). self.db = dbm.open(db_name, db_type) def set(self, key: str, value: str): Stores the key-value pair in the database. self.db[key] = value def get(self, key: str) -> str: Retrieves the value associated with the key. Returns None if the key doesn\'t exist. return self.db.get(key).decode() if key in self.db else None def delete(self, key: str): Deletes the key-value pair from the database. if key in self.db: del self.db[key] def list_keys(self) -> list: Lists all the keys present in the database. return [key.decode() for key in self.db.keys()] def close(self): Closes the database. self.db.close()"},{"question":"# Python Coding Challenge: Custom Exception Handling for Data Processing Problem Statement You are tasked with developing a data processing module that reads data from a file, processes it, and handles various exceptions that might occur during the data processing. The purpose of this challenge is to ensure you understand how to effectively use built-in exceptions and create custom exceptions to handle specific error scenarios. Requirements 1. **Function Implementation:** Write a function `process_data(file_path: str) -> List[Dict[str, Any]]` that: - Reads data from a JSON file specified by `file_path`. - Processes the data, which is expected to be a list of dictionaries. Each dictionary should have keys \'name\' (string), \'age\' (integer), and \'email\' (string). 2. **Exception Handling:** - If the file does not exist, raise a `FileNotFoundError` with a custom message. - If the data in the file is not a valid JSON, raise a `JSONDecodeError` with a custom message. - If the data is not a list of dictionaries, raise a `TypeError` with a custom message. - If any of the dictionaries do not have the required keys (\'name\', \'age\', \'email\'), raise a `ValueError` with a custom message. - If any other unexpected exceptions occur, catch them and raise a `RuntimeError` with a description of the unexpected issue. 3. **Input and Output:** - **Input:** A string representing the file path. - **Output:** A list of dictionaries, where each dictionary has keys \'name\', \'age\', and \'email\'. 4. **Performance Requirements:** - The function should be efficient and should not load the file multiple times. - Proper error handling should be ensured to prevent the program from crashing unexpectedly. 5. **Code Structure and Clarity:** - Use clear and concise variable names. - Include comments explaining key parts of the code. - Ensure the code is well-structured and readable. Example Usage ```python # Example JSON file content: # [ # {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"email\\": \\"alice@example.com\\"}, # {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"email\\": \\"bob@example.com\\"} # ] try: result = process_data(\\"data.json\\") print(result) # Should print the list of dictionaries except (FileNotFoundError, JSONDecodeError, TypeError, ValueError, RuntimeError) as e: print(e) ``` Submission Please submit your implementation of the function `process_data` along with test cases that demonstrate its functionality and exception handling capabilities.","solution":"import json from typing import List, Dict, Any def process_data(file_path: str) -> List[Dict[str, Any]]: Reads data from a JSON file, processes it, and returns the data if valid. Args: - file_path: str - the path to the JSON file. Returns: - List[Dict[str, Any]] - a list of dictionaries containing \'name\', \'age\', and \'email\'. Raises: - FileNotFoundError: if the file does not exist. - JSONDecodeError: if the file is not a valid JSON. - TypeError: if the data is not a list of dictionaries. - ValueError: if any dictionary does not have required keys \'name\', \'age\', \'email\'. - RuntimeError: for other unexpected exceptions. try: with open(file_path, \'r\') as file: data = json.load(file) if not isinstance(data, list): raise TypeError(\\"Data is not a list.\\") for item in data: if not isinstance(item, dict): raise TypeError(\\"Every item in data should be a dictionary.\\") if \'name\' not in item or \'age\' not in item or \'email\' not in item: raise ValueError(\\"Each dictionary should have \'name\', \'age\', and \'email\'.\\") return data except FileNotFoundError: raise FileNotFoundError(f\\"File {file_path} does not exist.\\") except json.JSONDecodeError: raise json.JSONDecodeError(f\\"File {file_path} is not a valid JSON\\", \\"\\", 0) except (FileNotFoundError, json.JSONDecodeError, TypeError, ValueError) as e: raise e except Exception as e: raise RuntimeError(f\\"An unexpected error occurred: {str(e)}\\")"},{"question":"# Seaborn Advanced Plotting Background In this assessment, you will demonstrate your knowledge of the seaborn library by visualizing data with advanced customization. You\'ll be working with the \\"diamonds\\" dataset included with seaborn, creating complex visualizations that involve scale and color modifications. Task 1. **Load Data**: Load the \\"diamonds\\" dataset using `seaborn.load_dataset(\\"diamonds\\")`. 2. **Main Plot**: - Create a scatter plot of the \\"carat\\" versus \\"price\\". - Apply a logarithmic scale to the y-axis. - Color the data points based on the \\"clarity\\" column using a continuous color palette. - Adjust the size of the points based on the \\"carat\\" column, with sizes ranging from 2 to 10. 3. **Enhance with a Fit Line**: - Add a second layer: a polynomial fit line (order 2) to the scatter plot. - Ensure the data is transformed suitably to represent the sliding logarithmic sale correctly. 4. **Customize Ticks and Labels**: - Format the y-axis labels to include a dollar sign before the price. - Configure the x-axis to display carats with half-unit intervals. - Ensure the color legend is well-defined with appropriate labels. 5. **Category Example**: - Create a histogram showing the count of diamonds by \\"cut\\" (use \\"cut\\" as a categorical variable even if it appears numeric). 6. **Advanced Customization**: - For a different visualization, create a scatter plot showing \\"depth\\" vs \\"table\\", using \\"cut\\" for color and \\"clarity\\" for marker style. - Use a nominal color scale with custom colors for each \\"cut\\". - Specify markers for each \\"clarity\\" level. Implementation Details - You need to use the seaborn.objects module for all tasks (`so.Plot`). - The plots should be clear and properly labeled. - Aim for a clean and professional visual appearance. Input/Output - **Input**: None (datasets are loaded within the code). - **Output**: The final solution should generate and display the requested plots without additional input. Constraints - Ensure your solution can handle different dataset sizes and different distributions of data. - Pay attention to the presentation, ensuring that multiple layers of visualization remain clear and informative. # Example Code Below is an outline to help you get started: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Main scatter plot with logarithmic y-scale p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") p1.add(so.Dots()).scale(y=\\"log\\") # Color by clarity and point size by carat p1.add(so.Dots(), color=\\"clarity\\").scale(color=\\"crest\\") p1.add(so.Dots(), pointsize=\\"carat\\").scale(pointsize=(2, 10)) # Add polynomial fit line p1.add(so.Line(), so.PolyFit(order=2)) # Customize ticks and labels p1.scale(x=so.Continuous(trans=\\"sqrt\\").tick(every=.5), y=so.Continuous().label(like=\\"{x:g}\\")) # Display plot p1.show() # Histogram for \'cut\' as categorical variable p2 = so.Plot(diamonds, \\"cut\\").add(so.Bar(), so.Hist()) p2.scale(x=so.Nominal()) p2.show() # Depth vs Table with Cut and Clarity p3 = so.Plot(diamonds, x=\\"depth\\", y=\\"table\\", color=\\"cut\\", marker=\\"clarity\\").add(so.Dots()) p3.scale(color=so.Nominal([\\"#e31a1c\\", \\"#1f78b4\\", \\"#33a02c\\", \\"#ff7f00\\", \\"#6a3d9a\\"]), marker=so.Nominal(order=[\\"I1\\", \\"SI2\\", \\"SI1\\", \\"VS2\\", \\"VS1\\", \\"VVS2\\", \\"VVS1\\", \\"IF\\"])) p3.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_diamond_plots(): # Load dataset diamonds = load_dataset(\\"diamonds\\") # Main scatter plot with logarithmic y-scale p1 = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\") p1.add(so.Dots(), color=\\"clarity\\", pointsize=\\"carat\\").scale(pointsize=(2, 10), y=\\"log\\", color=\\"crest\\") # Add polynomial fit line p1.add(so.Line(), so.PolyFit(order=2)).scale( x=so.Continuous().tick(every=0.5), y=so.Continuous().label(like=\\"{x:,.0f}\\") ).label(x=\\"Carat\\", y=\\"Price ()\\") # Display plot p1.show() # Histogram for \'cut\' as categorical variable p2 = so.Plot(diamonds, x=\\"cut\\").add(so.Bar(), so.Hist()).scale( x=so.Nominal(order=[\\"Fair\\", \\"Good\\", \\"Very Good\\", \\"Premium\\", \\"Ideal\\"]) ).label(x=\\"Cut\\", y=\\"Count\\") p2.show() # Depth vs Table with Cut and Clarity p3 = so.Plot(diamonds, x=\\"depth\\", y=\\"table\\", color=\\"cut\\", marker=\\"clarity\\").add(so.Dots()).scale( color=so.Nominal([\\"#e31a1c\\", \\"#1f78b4\\", \\"#33a02c\\", \\"#ff7f00\\", \\"#6a3d9a\\"]), marker=so.Nominal(order=[\\"I1\\", \\"SI2\\", \\"SI1\\", \\"VS2\\", \\"VS1\\", \\"VVS2\\", \\"VVS1\\", \\"IF\\"]) ).label(x=\\"Depth\\", y=\\"Table\\") p3.show() if __name__ == \\"__main__\\": create_diamond_plots()"},{"question":"<|Analysis Begin|> The provided documentation highlights the cryptographic services available in Python 3.10. It mentions three main modules: `hashlib`, `hmac`, and `secrets`. Here\'s a breakdown of each: 1. **hashlib**: This module provides secure hashes and message digests. - It includes various hashing algorithms such as SHA-256, SHA-512, and more. - Special functionalities such as SHAKE variable length digests, key derivation functions, and BLAKE2 (a family of cryptographic hash functions) are also supported. - Several examples illustrate simple hashing, using different digest sizes, keyed hashing, randomized hashing, personalization, and tree mode hashing. 2. **hmac**: This module is for keyed-hashing for message authentication. It typically combines a secret key with the message to produce a hash, ensuring the authenticity and integrity of the message. 3. **secrets**: This module focuses on generating secure random numbers for managing secrets. - It discusses generating random numbers, generating secure tokens, and provides recipes and best practices for secure random number generation. Given this documentation, a challenging and self-contained coding question can be crafted around implementing a function to utilize these cryptographic modules. The question should require the student to demonstrate an understanding of hashing, keyed-hashing, and secure random number generation. <|Analysis End|> <|Question Begin|> # Question: Secure File Integrity Checker In this question, you are required to implement a Python function that ensures the integrity of a file using cryptographic techniques. Your function should perform the following tasks: 1. Generate a secure random secret key using the `secrets` module. 2. Compute a cryptographic hash of the file contents using the `hashlib` module. 3. Use the `hmac` module to create a message authentication code (MAC) by combining the secret key with the computed hash. 4. Store the MAC along with the hash and secret key. # Function Signature ```python def secure_file_integrity_checker(file_path: str) -> dict: Check the integrity of a file by generating a secure hash and message authentication code (MAC). :param file_path: Path to the input file :return: A dictionary containing the following: - \'secret_key\': The secure random secret key used for HMAC - \'file_hash\': The cryptographic hash of the file contents - \'mac\': The message authentication code derived from the file hash and secret key ``` # Input - `file_path` (str): The path to the file for which the integrity needs to be checked. You can assume the file is in text format. # Output - A dictionary with the keys: - `\'secret_key\'`: A string representation of the secure random secret key. - `\'file_hash\'`: The cryptographic hash of the file content as a hexadecimal string. - `\'mac\'`: The message authentication code as a hexadecimal string. # Example Usage ```python result = secure_file_integrity_checker(\'example.txt\') print(result) # Expected Output Example: # { # \'secret_key\': \'e6d8f849f1cde7f3a6af0c6a4f1f94a5e51b8c626e8b95a1bee29dcc5e16\', # \'file_hash\': \'9b74c9897bac770ffc029102a200c5de36b623a80f6d0c87cd3cb32703f4\', # \'mac\': \'2e1d6d759c33c5c3d6b0b1e6e4c7d3ac78bb51aef7b926b99f8969d0ef83\' # } ``` # Constraints - You must use Python 3.10 or later. - Use appropriate functions from the `hashlib`, `hmac`, and `secrets` modules. - The implementation should be robust and handle potential exceptions where appropriate (e.g., file not found). # Performance Requirements - The function should be efficient and handle files up to 10 MB in size within a reasonable time limit.","solution":"import hashlib import hmac import secrets def secure_file_integrity_checker(file_path: str) -> dict: Check the integrity of a file by generating a secure hash and message authentication code (MAC). :param file_path: Path to the input file :return: A dictionary containing the following: - \'secret_key\': The secure random secret key used for HMAC - \'file_hash\': The cryptographic hash of the file contents - \'mac\': The message authentication code derived from the file hash and secret key try: # Generate a secure random secret key secret_key = secrets.token_bytes(32) # Compute the cryptographic hash of the file contents using SHA-256 hash_algorithm = hashlib.sha256() with open(file_path, \'rb\') as file: while chunk := file.read(8192): hash_algorithm.update(chunk) file_hash = hash_algorithm.hexdigest() # Create a message authentication code (MAC) by combining the secret key with the computed hash mac = hmac.new(secret_key, file_hash.encode(), hashlib.sha256).hexdigest() # Return the results as a dictionary return { \'secret_key\': secret_key.hex(), \'file_hash\': file_hash, \'mac\': mac } except FileNotFoundError: raise ValueError(f\\"File not found: {file_path}\\")"},{"question":"Question: Using the `pydoc` module, write a Python program that fulfills the following requirements: 1. Accept a module name as input from the user. 2. Generate the text documentation of the specified module using `pydoc`. 3. Write the generated documentation to a text file named `<module_name>_docs.txt` in the current directory. Your implementation should handle the following constraints and edge cases: - The specified module might not exist. In such a case, your program should print an appropriate error message and exit gracefully. - The program should sanitize the module name to avoid any directory traversal attacks. - Use the `pydoc` module appropriately to generate the documentation without executing unnecessary code in the target module. Input and Output Formats: - **Input:** - A single string input, the name of the module. - **Output:** - A text file `<module_name>_docs.txt` containing the documentation. - If the module does not exist, print an error message: `\\"Error: The module \'<module_name>\' was not found.\\"` Example: ```python Input: Enter the name of the module: os Expected Output: A text file named \\"os_docs.txt\\" containing the documentation of the `os` module. If the module does not exist, print an error message such as: \\"Error: The module \'nonexistent_module\' was not found.\\" ``` # Instructions: - Your solution should be self-contained and not depend on any external libraries other than those in the Python standard library. - Follow best practices for writing clean and efficient Python code. - Ensure you handle any potential exceptions that may arise during the execution of your program.","solution":"import pydoc import os def generate_module_docs(module_name): Accepts a module name as input, generates the text documentation of the specified module using pydoc, and writes the documentation to a text file. Parameters: module_name (str): The name of the module to document Returns: str: The filename of the generated documentation # Sanitize the module name to avoid directory traversal attacks module_name = os.path.basename(module_name) try: # Generate the documentation using pydoc doc = pydoc.render_doc(module_name) # Write the documentation to a text file file_name = f\\"{module_name}_docs.txt\\" with open(file_name, \'w\') as file: file.write(doc) return file_name except ImportError: print(f\\"Error: The module \'{module_name}\' was not found.\\") return None # Example usage: if __name__ == \\"__main__\\": module_name = input(\\"Enter the name of the module: \\").strip() generate_module_docs(module_name)"},{"question":"# Email Parser Function Implementation Objectives You are tasked with implementing a function that parses an email\'s headers and body content. The function should handle several possible error scenarios using the custom exceptions and defects provided in the `email.errors` module. Function Signature ```python def parse_email(headers: str, body: str) -> dict: Parses the given email headers and body content. Parameters: headers (str): A string representing the email headers. body (str): A string representing the email body content. Returns: dict: A dictionary containing the parsed headers and body. Raises: HeaderParseError: If any parsing error occurs in the headers. MultipartConversionError: If there are issues converting the body to a multipart format. InvalidBase64CharactersDefect: If invalid base64 characters are found in the body. InvalidDateDefect: If there\'s an unparsable date in the headers. ``` Input - `headers`: A string containing the email headers. For example: ``` From: example@example.com To: test@example.com Subject: Test Email Date: InvalidDate ``` - `body`: A string containing the email body content, which may include invalid base64 encoded sections. Output - The function should return a dictionary with keys \\"`headers`\\" and \\"`body`\\", containing the parsed headers as a dictionary and the body as a string. ```python { \\"headers\\": { \\"From\\": \\"example@example.com\\", \\"To\\": \\"test@example.com\\", \\"Subject\\": \\"Test Email\\", \\"Date\\": \\"InvalidDate\\", }, \\"body\\": \\"Body content here\\" } ``` Exceptions - Raise `HeaderParseError` for any errors encountered while parsing the headers. - Raise `MultipartConversionError` if there are issues converting the body to a multipart format. - Add a defect `InvalidBase64CharactersDefect` if invalid base64 characters are found in the body. - Add a defect `InvalidDateDefect` if any date fields in the headers are unparsable. Constraints - The email should be in text format. - Header keys and values are separated by a colon and a space. - The body may include sections of base64 encoded text which should be properly decoded. Performance - Your implementation should handle common cases efficiently. - Assume the maximum email size will not exceed a few kilobytes. Example ```python headers = \'\'\' From: example@example.com To: test@example.com Subject: Test Email Date: InvalidDate \'\'\'.strip() body = \'\'\' This is a test email body. \'\'\' try: parsed_email = parse_email(headers, body) print(parsed_email) except Exception as e: print(f\\"Error: {e}\\") ``` The above code should output: ```python { \\"headers\\": { \\"From\\": \\"example@example.com\\", \\"To\\": \\"test@example.com\\", \\"Subject\\": \\"Test Email\\", \\"Date\\": \\"InvalidDate\\", }, \\"body\\": \\"This is a test email body.\\" } ``` Note: The actual errors and defects should be handled appropriately as per the provided content and instructions.","solution":"import base64 from email.errors import HeaderParseError, MultipartConversionError, InvalidBase64CharactersDefect, InvalidDateDefect from email.utils import parsedate_to_datetime def parse_email(headers: str, body: str) -> dict: Parses the given email headers and body content. Parameters: headers (str): A string representing the email headers. body (str): A string representing the email body content. Returns: dict: A dictionary containing the parsed headers and body. Raises: HeaderParseError: If any parsing error occurs in the headers. MultipartConversionError: If there are issues converting the body to a multipart format. InvalidBase64CharactersDefect: If invalid base64 characters are found in the body. InvalidDateDefect: If there\'s an unparsable date in the headers. parsed_headers = {} # Parse headers try: for line in headers.split(\'n\'): if not line.strip(): continue parts = line.split(\': \', 1) if len(parts) != 2: raise HeaderParseError(f\\"Header parsing error: {line}\\") key, value = parts parsed_headers[key.strip()] = value.strip() except Exception as e: raise HeaderParseError(f\\"Header parsing error: {str(e)}\\") # Handling date parsing if \'Date\' in parsed_headers: try: parsedate_to_datetime(parsed_headers[\'Date\']) except Exception: raise InvalidDateDefect(f\\"Invalid date defect: {parsed_headers[\'Date\']}\\") # Decode base64 if necessary try: if body.strip().startswith(\'base64:\'): base64_content = body.split(\'base64:\', 1)[1].strip() decoded_content = base64.b64decode(base64_content).decode(\'utf-8\') body = decoded_content except Exception: raise InvalidBase64CharactersDefect(\\"Invalid Base64 characters detected in the body\\") return { \\"headers\\": parsed_headers, \\"body\\": body }"},{"question":"**Objective:** To assess the student\'s understanding of Python descriptors and their implementation. **Problem Statement:** A descriptor in Python is an object attribute with â€œbinding behavior,â€ one whose attributed access has been overridden by methods in the descriptor protocol: `__get__()`, `__set__()`, and `__delete__()`. Create a class that uses descriptors to manage temperature in different scales (Celsius and Fahrenheit). **Requirements:** 1. Implement a descriptor class `Temperature` that: - Accepts a temperature value in Celsius. - Allows getting the temperature in either Celsius or Fahrenheit. - Allows setting the temperature in either Celsius or Fahrenheit. - Raises an exception if an invalid temperature below absolute zero (-273.15Â°C) is set. 2. Implement two properties in the `Weather` class: - `temperature_celsius` which interacts with the `Temperature` descriptor for Celsius. - `temperature_fahrenheit` which interacts with the `Temperature` descriptor for Fahrenheit. 3. The properties should handle the conversion between Celsius and Fahrenheit transparently. **Input and Output:** - **Temperature value input:** Integers or floats representing temperatures in Celsius or Fahrenheit. - **Output:** Converted temperature values upon getter calls. **Constraints:** - The temperature in Celsius cannot be set below -273.15Â°C. - Ensure proper encapsulation and use of descriptors. **Performance Requirements:** - Efficient conversion between scales with minimal overhead. **Example:** ```python class Temperature: # Implement the descriptor methods here class Weather: temperature_celsius = Temperature() temperature_fahrenheit = Temperature() # Example usage: w = Weather() w.temperature_celsius = 25 print(w.temperature_celsius) # Output: 25 print(w.temperature_fahrenheit) # Output: 77.0 w.temperature_fahrenheit = 32 print(w.temperature_celsius) # Output: 0.0 print(w.temperature_fahrenheit) # Output: 32.0 w.temperature_celsius = -274 # Raise ValueError: Temperature cannot be below absolute zero! ``` **Notes:** - Ensure that your implementation raises a ValueError for any temperature set below -273.15Â°C. - Properly convert between Celsius and Fahrenheit. The formulas are: - ( Â°F = Â°C Ã— 9/5 + 32 ) - ( Â°C = (Â°F - 32) Ã— 5/9 )","solution":"class Temperature: def __init__(self): self._celsius = None def __get__(self, instance, owner): return self._celsius def __set__(self, instance, value): if isinstance(value, tuple): unit, temp = value if unit == \'C\': self.celsius = temp elif unit == \'F\': self.celsius = (temp - 32) * 5/9 else: raise ValueError(\\"Invalid temperature unit. Use \'C\' for Celsius or \'F\' for Fahrenheit.\\") else: raise ValueError(\\"Invalid temperature format. Use a tuple with a unit and temperature value.\\") @property def celsius(self): return self._celsius @celsius.setter def celsius(self, value): if value < -273.15: raise ValueError(\\"Temperature cannot be below absolute zero!\\") self._celsius = value @property def fahrenheit(self): return self._celsius * 9/5 + 32 class Weather: def __init__(self): self._temp = Temperature() @property def temperature_celsius(self): return self._temp.celsius @temperature_celsius.setter def temperature_celsius(self, value): self._temp.celsius = value @property def temperature_fahrenheit(self): return self._temp.fahrenheit @temperature_fahrenheit.setter def temperature_fahrenheit(self, value): self._temp.__set__(self, (\'F\', value))"},{"question":"# IPv4 Network Analysis In this exercise, you will be working with the Python `ipaddress` module to perform network analysis tasks. Your goal is to implement a function that takes a list of IPv4 network definitions and returns various pieces of information about these networks. Task Write a function `analyze_networks(network_list)` that accepts a list of strings, where each string represents an IPv4 network in CIDR notation (e.g., `\'192.168.0.0/24\'`). The function should return a dictionary with the following keys and values: 1. `\'total_networks\'`: The total number of networks in the input list. 2. `\'total_addresses\'`: The total number of IP addresses covered by all networks in the list combined. 3. `\'overlapping_networks\'`: A list of tuples, where each tuple contains two overlapping networks from the input. 4. `\'subnets_per_network\'`: A dictionary where the key is the network and the value is a list of subnets split from this network. The subnets should be created by incrementing the prefix length by 1. 5. `\'global_networks\'`: A list of networks from the input that are defined as globally reachable. Input - `network_list`: a list of strings, each representing an IPv4 network in CIDR notation. Output - A dictionary with the specified keys and their corresponding values. Example ```python network_list = [ \'192.168.0.0/24\', \'192.168.0.0/25\', \'10.0.0.0/8\', \'172.16.0.0/12\', \'192.88.99.0/24\', # This is a globally reachable address ] result = analyze_networks(network_list) print(result) ``` Expected output: ```python { \'total_networks\': 5, \'total_addresses\': 17826304, # This is the sum of all IP addresses in the provided networks \'overlapping_networks\': [ (\'192.168.0.0/24\', \'192.168.0.0/25\') ], \'subnets_per_network\': { \'192.168.0.0/24\': [\'192.168.0.0/25\', \'192.168.0.128/25\'], \'192.168.0.0/25\': [\'192.168.0.0/26\', \'192.168.0.64/26\'], \'10.0.0.0/8\': [ \'10.0.0.0/9\', \'10.128.0.0/9\' ], \'172.16.0.0/12\': [ \'172.16.0.0/13\', \'172.24.0.0/13\' ], \'192.88.99.0/24\': [\'192.88.99.0/25\', \'192.88.99.128/25\'], }, \'global_networks\': [\'192.88.99.0/24\'] } ``` Notes: - Assume the input networks are always valid and do not need additional validation. - Consider using the `ipaddress` module for the implementation. Constraints: - The input list will contain at most 100 networks. - Each network string is a valid IPv4 network in CIDR notation. ```python import ipaddress def analyze_networks(network_list): result = { \'total_networks\': 0, \'total_addresses\': 0, \'overlapping_networks\': [], \'subnets_per_network\': {}, \'global_networks\': [] } networks = [ipaddress.ip_network(net) for net in network_list] result[\'total_networks\'] = len(networks) for net in networks: result[\'total_addresses\'] += net.num_addresses if net.is_global: result[\'global_networks\'].append(str(net)) result[\'subnets_per_network\'][str(net)] = [str(subnet) for subnet in net.subnets(prefixlen_diff=1)] for i, net1 in enumerate(networks): for net2 in networks[i+1:]: if net1.overlaps(net2): result[\'overlapping_networks\'].append((str(net1), str(net2))) return result ```","solution":"import ipaddress def analyze_networks(network_list): result = { \'total_networks\': 0, \'total_addresses\': 0, \'overlapping_networks\': [], \'subnets_per_network\': {}, \'global_networks\': [] } networks = [ipaddress.ip_network(net) for net in network_list] result[\'total_networks\'] = len(networks) for net in networks: result[\'total_addresses\'] += net.num_addresses if net.is_global: result[\'global_networks\'].append(str(net)) result[\'subnets_per_network\'][str(net)] = [str(subnet) for subnet in net.subnets(prefixlen_diff=1)] for i, net1 in enumerate(networks): for net2 in networks[i+1:]: if net1.overlaps(net2): result[\'overlapping_networks\'].append((str(net1), str(net2))) return result"},{"question":"The `doctest` module is a powerful tool in Python for validating that code examples in docstrings work as intended. This assessment will test your understanding of the `doctest` module by asking you to write a function with comprehensive docstring examples and to run `doctest` to ensure your code and documentation are correct. Task 1. **Function Implementation:** - Write a function `reverse_words(s: str) -> str` that takes a string containing words separated by spaces and returns a string with the words in reverse order. - The function should handle multiple spaces gracefully, treating them as a single separator. - Empty strings or strings with only spaces should return an empty string. 2. **Docstring with Doctests:** - Include a docstring for the function with at least 5 doctest examples demonstrating different edge cases and typical usage. 3. **Execution with Doctest:** - At the bottom of your script, include the usual `doctest` snippet to automatically validate the examples when the script is run. Specifications - The function should be implemented in a Python file named `reverse_words.py`. - The docstring examples should provide clear and accurate representations of expected behavior. Example Here is a template to follow: ```python def reverse_words(s: str) -> str: Reverses the order of words in a string. Each word is assumed to be separated by spaces. Multiple spaces between words should be treated as a single separator. Leading and trailing spaces should be ignored. >>> reverse_words(\\"Hello world\\") \\"world Hello\\" >>> reverse_words(\\" This is a test \\") \\"test a is This\\" >>> reverse_words(\\"Multiple spaces\\") \\"spaces Multiple\\" >>> reverse_words(\\"\\") \\"\\" >>> reverse_words(\\" \\") \\"\\" pass # Your implementation here if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` Submission Submit the file `reverse_words.py` with your function implementation and valid doctest examples.","solution":"def reverse_words(s: str) -> str: Reverses the order of words in a string. Each word is assumed to be separated by spaces. Multiple spaces between words should be treated as a single separator. Leading and trailing spaces should be ignored. >>> reverse_words(\\"Hello world\\") \'world Hello\' >>> reverse_words(\\" This is a test \\") \'test a is This\' >>> reverse_words(\\"Multiple spaces\\") \'spaces Multiple\' >>> reverse_words(\\"\\") \'\' >>> reverse_words(\\" \\") \'\' >>> reverse_words(\\"OneWord\\") \'OneWord\' >>> reverse_words(\\" Leading and trailing spaces \\") \'spaces trailing and Leading\' words = s.split() reversed_words = \' \'.join(reversed(words)) return reversed_words if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"**Complex Task Management System** In this exercise, you need to design a function for managing a list of tasks using the Python `typing` module for type annotations. The function will process a list of tasks, where each task has a name, priority, and execution function. The tasks are to be ordered based on priority before being executed. # Requirements 1. **Task Representation**: Use `TypedDict` to define the structure of a single task. - A task will have: - `name` (str): The name of the task. - `priority` (int): Priority level of the task (higher number means higher priority). - `execute` (Callable[[], bool]): A function that takes no arguments and returns a boolean indicating success (`True`) or failure (`False`). 2. **NewType**: Define a new type `TaskID` which is a distinct type of int, representing the ID of a task. 3. **Protocol**: Define a protocol `Executable` that ensures any executable task must have an `execute` method that takes no arguments and returns a boolean. 4. **Function Implementation**: - Implement a function `schedule_tasks` which takes a dictionary of tasks (`Dict[TaskID, TaskDict]`) and returns a dictionary mapping `TaskID` to execution results (`Dict[TaskID, bool]`). - The function should process the tasks based on priority (highest priority first). - The function should ensure type consistency using appropriate annotations from the `typing` module. # Example Usage ```python from typing import TypedDict, Callable, Dict, NewType, Protocol # 1. Define a TypedDict for Task representation class TaskDict(TypedDict): name: str priority: int execute: Callable[[], bool] # 2. Define a NewType for TaskID TaskID = NewType(\'TaskID\', int) # 3. Define a Protocol for Executable tasks class Executable(Protocol): def execute(self) -> bool: ... # 4. Implement the schedule_tasks function def schedule_tasks(tasks: Dict[TaskID, TaskDict]) -> Dict[TaskID, bool]: # Sort tasks based on priority sorted_tasks = sorted(tasks.items(), key=lambda item: item[1][\'priority\'], reverse=True) # Dictionary to store the results results: Dict[TaskID, bool] = {} # Execute the tasks in sorted order for task_id, task in sorted_tasks: results[task_id] = task[\'execute\']() return results # Example tasks task1: TaskDict = {\'name\': \'Task 1\', \'priority\': 2, \'execute\': lambda: True} task2: TaskDict = {\'name\': \'Task 2\', \'priority\': 3, \'execute\': lambda: False} task3: TaskDict = {\'name\': \'Task 3\', \'priority\': 1, \'execute\': lambda: True} tasks: Dict[TaskID, TaskDict] = { TaskID(1): task1, TaskID(2): task2, TaskID(3): task3 } result = schedule_tasks(tasks) print(result) # Expected output: {TaskID(2): False, TaskID(1): True, TaskID(3): True} ``` # Constraints - Follow all type annotations and ensure the function adheres to the type hints. - Ensure the tasks are sorted and executed according to their priority. # Notes - The primary focus of this exercise is to demonstrate proficiency in using the `typing` module for type hints and annotations. - Consider edge cases like empty task list, tasks with the same priority, etc.","solution":"from typing import TypedDict, Callable, Dict, NewType, Protocol # 1. Define a TypedDict for Task representation class TaskDict(TypedDict): name: str priority: int execute: Callable[[], bool] # 2. Define a NewType for TaskID TaskID = NewType(\'TaskID\', int) # 3. Define a Protocol for Executable tasks class Executable(Protocol): def execute(self) -> bool: ... # 4. Implement the schedule_tasks function def schedule_tasks(tasks: Dict[TaskID, TaskDict]) -> Dict[TaskID, bool]: # Sort tasks based on priority - higher priority first sorted_tasks = sorted(tasks.items(), key=lambda item: item[1][\'priority\'], reverse=True) # Dictionary to store the results results: Dict[TaskID, bool] = {} # Execute the tasks in sorted order for task_id, task in sorted_tasks: results[task_id] = task[\'execute\']() return results # Example tasks for demonstration task1: TaskDict = {\'name\': \'Task 1\', \'priority\': 2, \'execute\': lambda: True} task2: TaskDict = {\'name\': \'Task 2\', \'priority\': 3, \'execute\': lambda: False} task3: TaskDict = {\'name\': \'Task 3\', \'priority\': 1, \'execute\': lambda: True} tasks: Dict[TaskID, TaskDict] = { TaskID(1): task1, TaskID(2): task2, TaskID(3): task3 } result = schedule_tasks(tasks) print(result) # Expected output: {TaskID(2): False, TaskID(1): True, TaskID(3): True}"},{"question":"Coding Assessment Question # Command-Line Parsing with `optparse` Objective You are required to write a Python script that utilizes the `optparse` module to parse command-line arguments. This exercise assesses your understanding of command-line processing and your ability to handle different types of input using the `optparse` module. Task Description Create a Python script that takes several command-line options and arguments to perform basic text processing tasks. The script should support the following functionalities: 1. **Count Characters (-c or --chars):** Output the number of characters in a given text string. 2. **Count Words (-w or --words):** Output the number of words in a given text string. 3. **Convert to Uppercase (-u or --upper):** Convert the given text string to uppercase. 4. **Help (-h or --help):** Display the help message and exit. Input Format The script should be executed from the command line with the following options: - The `-c` or `--chars` flag will trigger the character count display. - The `-w` or `--words` flag will trigger the word count display. - The `-u` or `--upper` flag will trigger the uppercase conversion. - The `-h` or `--help` flag should display the help message and exit. Example command: ```sh python text_processor.py -c -w -u \\"Hello World\\" ``` Output Format The script should print the result for each of the specified options. Each result should be printed on a new line. Example output for the command above: ``` Characters: 11 Words: 2 Uppercase: HELLO WORLD ``` Constraints - You must use the `optparse` module for parsing command-line options. - The script should handle invalid inputs gracefully and provide appropriate error messages. - Assume that the input text string will always be a valid string in quotes. Guidelines Implement the following steps: 1. Create an `OptionParser` object. 2. Define options for `chars`, `words`, `upper`, and `help`. 3. Parse the command-line arguments. 4. Implement functionality for each option. 5. Handle cases where no text string is provided. Example Here\'s an example result using the `optparse` module based on the input above: Command: ```sh python text_processor.py -c -w -u \\"Hello Python 310\\" ``` Output: ``` Characters: 16 Words: 3 Uppercase: HELLO PYTHON 310 ``` # Submit Please submit the Python script file containing your solution.","solution":"from optparse import OptionParser def main(): parser = OptionParser(usage=\\"usage: %prog [options] text\\") parser.add_option(\\"-c\\", \\"--chars\\", action=\\"store_true\\", dest=\\"chars\\", help=\\"Count characters in text\\") parser.add_option(\\"-w\\", \\"--words\\", action=\\"store_true\\", dest=\\"words\\", help=\\"Count words in text\\") parser.add_option(\\"-u\\", \\"--upper\\", action=\\"store_true\\", dest=\\"upper\\", help=\\"Convert text to uppercase\\") (options, args) = parser.parse_args() if len(args) == 0: parser.error(\\"No text provided\\") text = args[0] if options.chars: print(f\\"Characters: {len(text)}\\") if options.words: print(f\\"Words: {len(text.split())}\\") if options.upper: print(f\\"Uppercase: {text.upper()}\\") if __name__ == \\"__main__\\": main()"},{"question":"In this task, you are required to create a utility function using the `marshal` module to serialize and deserialize nested Python objects. Your implementation will include two functions: 1. `serialize_object(obj: Any, file_path: str) -> None` - Serialize and save the given Python object to the specified file path. 2. `deserialize_object(file_path: str) -> Any` - Load and return the Python object from the specified file path. # Requirements 1. The `serialize_object` function should: - Take any supported Python object (`obj`) and a string representing the file path (`file_path`). - Open the file at `file_path` in write-binary mode. - Use `marshal.dump` to write the object to the file. - Ensure the file is properly closed after writing the object. 2. The `deserialize_object` function should: - Take a string representing the file path (`file_path`). - Open the file at `file_path` in read-binary mode. - Use `marshal.load` to read and return the object from the file. - Ensure the file is properly closed after reading the object. 3. The functions should handle necessary exceptions that might occur during file operations and serialization/deserialization (like `EOFError`, `ValueError`, `TypeError`). 4. Your functions should also include appropriate auditing events handling. # Function Signatures ```python def serialize_object(obj: Any, file_path: str) -> None: # Your implementation here pass def deserialize_object(file_path: str) -> Any: # Your implementation here pass ``` # Input and Output Formats - The `serialize_object` function takes two inputs: any valid Python object and a file path (both strings). It has no return value but writes the object to the specified file. - The `deserialize_object` function takes one input: a file path (string). It returns the Python object that was serialized and saved in the specified file. # Constraints - You must use the `marshal` module for serialization and deserialization. - The file operations must ensure no resource leaks (i.e., files must be closed properly). - Handle `EOFError`, `ValueError`, and `TypeError` exceptions to ensure the functions\' robustness. - Ensure compatibility with Python version 3.10 and `marshal` version 4. # Example Here\'s an example of the usage of these functions: ```python serialize_object([1, 2, 3, {\\"a\\": 4, \\"b\\": 5}], \\"example.marshal\\") result = deserialize_object(\\"example.marshal\\") print(result) # Output: [1, 2, 3, {\\"a\\": 4, \\"b\\": 5}] ``` --- Using the above guidelines, implement the `serialize_object` and `deserialize_object` functions.","solution":"import marshal def serialize_object(obj, file_path): Serialize and save the given Python object to the specified file path. try: with open(file_path, \'wb\') as file: marshal.dump(obj, file) except (ValueError, TypeError) as e: print(f\\"Error occurred during serialization: {e}\\") def deserialize_object(file_path): Load and return the Python object from the specified file path. try: with open(file_path, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError) as e: print(f\\"Error occurred during deserialization: {e}\\") return None"},{"question":"# MultiIndex and Advanced Indexing in pandas **Objective:** You are tasked with writing a function that takes a DataFrame with a MultiIndex and performs various operations to manipulate and extract data based on the given specifications. **Function Signature:** ```python import pandas as pd import numpy as np def manipulate_multiindex(df: pd.DataFrame) -> dict: Given a DataFrame with MultiIndex, perform several manipulations and return the results in a dictionary. # Add your code here return { \\"df_sorted\\": df_sorted, \\"mean_by_level1\\": mean_by_level1, \\"cross_section_one\\": cross_section_one, \\"reindexed_df\\": reindexed_df } ``` **Input:** - `df (pd.DataFrame)`: A DataFrame with a MultiIndex consisting of at least two levels. The MultiIndex is applied on the index of the DataFrame. The DataFrame contains at least one numeric column named \'value\'. **Output:** - A dictionary containing the following keys and values: - `\\"df_sorted\\"` - The input DataFrame sorted by the first level of the MultiIndex. - `\\"mean_by_level1\\"` - A DataFrame of the mean of \'value\' grouped by the first level of the MultiIndex. - `\\"cross_section_one\\"` - A DataFrame obtained by taking a cross-section (xs) of the input DataFrame at the second level where this level is \'one\'. - `\\"reindexed_df\\"` - A DataFrame reindexed to include an additional level in the MultiIndex with the values \'A\', \'B\', \'C\', and \'D\'. The new DataFrame should have new levels added at the second position, with the values repeating for each existing index in the original DataFrame. **Constraints:** 1. The MultiIndex of the input DataFrame has at least two levels. 2. The DataFrame has at least one numeric column named \'value\'. **Example Usage:** ```python arrays = [ [\\"bar\\", \\"bar\\", \\"baz\\", \\"baz\\", \\"foo\\", \\"foo\\", \\"qux\\", \\"qux\\"], [\\"one\\", \\"two\\", \\"one\\", \\"two\\", \\"one\\", \\"two\\", \\"one\\", \\"two\\"], ] index = pd.MultiIndex.from_arrays(arrays, names=[\\"first\\", \\"second\\"]) df = pd.DataFrame({ \\"value\\": np.random.randn(8), \\"other\\": range(8) }, index=index) result = manipulate_multiindex(df) print(result[\\"df_sorted\\"]) print(result[\\"mean_by_level1\\"]) print(result[\\"cross_section_one\\"]) print(result[\\"reindexed_df\\"]) ``` **Expected Output:** 1. `\\"df_sorted\\"`: The DataFrame sorted by the first level of the MultiIndex. 2. `\\"mean_by_level1\\"`: The mean of the \'value\' column grouped by the first level of the MultiIndex. 3. `\\"cross_section_one\\"`: A cross-section of the DataFrame where the second level of the MultiIndex is \'one\'. 4. `\\"reindexed_df\\"`: The DataFrame reindexed to include an additional level in the MultiIndex with values \'A\', \'B\', \'C\', \'D\'. **Notes:** - Ensure that all levels in the final MultiIndex of `\\"reindexed_df\\"` are kept intact. - Handle any necessary error checking and ensure the function performs efficiently on large DataFrames.","solution":"import pandas as pd import numpy as np def manipulate_multiindex(df: pd.DataFrame) -> dict: Given a DataFrame with MultiIndex, perform several manipulations and return the results in a dictionary. # Sorting the DataFrame by the first level of the MultiIndex df_sorted = df.sort_index(level=0) # Calculating mean of \'value\' grouped by the first level of the MultiIndex mean_by_level1 = df.groupby(level=0)[\'value\'].mean().to_frame() # Taking cross-section at the second level where this level is \'one\' cross_section_one = df.xs(\'one\', level=1) # Adding a new level in the MultiIndex with values \'A\', \'B\', \'C\', \'D\' new_index = pd.MultiIndex.from_product( [df.index.get_level_values(0).unique(), [\'A\', \'B\', \'C\', \'D\'], df.index.get_level_values(1).unique()], names=[\'first\', \'new_level\', \'second\'] ) reindexed_df = df.reindex(new_index) return { \\"df_sorted\\": df_sorted, \\"mean_by_level1\\": mean_by_level1, \\"cross_section_one\\": cross_section_one, \\"reindexed_df\\": reindexed_df }"},{"question":"Efficient Numeric Array Operations You are given a list of operations to perform on an array of integers using Pythonâ€™s `array` module. Your task is to write a function `perform_operations` that initializes an `array` with a given list of numbers and then performs a series of operations on it. The function should return the modified array as a list. Function Signature ```python def perform_operations(initial_values: list, operations: list) -> list: pass ``` Input 1. **initial_values**: A list of integers to initialize the array. 2. **operations**: A list of tuples, where each tuple represents an operation and its arguments. The first element of the tuple is the operation name, followed by necessary arguments. Possible operations are: - `\'append\'` `(x)`: Append integer `x` to the end of the array. - `\'pop\'` `([i])`: Remove the item with index `i` and return it. If `i` is not provided, the last item is removed. - `\'remove\'` `(x)`: Remove the first occurrence of `x` from the array. - `\'reverse\'` `()`: Reverse the order of items in the array. - `\'count\'` `(x)`: Return the number of occurrences of `x` in the array. - `\'index\'` `(x, [start, [stop]])`: Return the index of the first occurrence of `x`. Optional arguments `start` and `stop` specify a range. - `\'extend\'` `(iterable)`: Extend the array with elements from `iterable`, which should be a list of integers. Output - A list of integers representing the final state of the array after all operations have been performed. Constraints - Do not use the built-in `list` type for these operations; use Pythonâ€™s `array` module. - The `initial_values` list will have at most `1000` integers. - The `operations` list can have at most `100` operations. Example ```python # Initialize with initial_values [1, 2, 3, 4, 5] # Perform operations [(\'append\', 6), (\'pop\',), (\'remove\', 2), (\'reverse\',), (\'count\', 3), (\'index\', 4, 1)] # Expected output: [5, 4, 3, 1] initial_values = [1, 2, 3, 4, 5] operations = [(\'append\', 6), (\'pop\',), (\'remove\', 2), (\'reverse\',), (\'count\', 3), (\'index\', 4, 1)] print(perform_operations(initial_values, operations)) ``` Explanation: 1. Append `6`: `[1, 2, 3, 4, 5, 6]` 2. Pop element: `[1, 2, 3, 4, 5]` 3. Remove `2`: `[1, 3, 4, 5]` 4. Reverse: `[5, 4, 3, 1]` 5. Count `3`: Returns `1` 6. Index `4` starting from `1`: Returns `1` The function should perform these operations and modify the array accordingly.","solution":"import array def perform_operations(initial_values, operations): arr = array.array(\'i\', initial_values) for op in operations: action = op[0] if action == \'append\': arr.append(op[1]) elif action == \'pop\': if len(op) > 1: arr.pop(op[1]) else: arr.pop() elif action == \'remove\': arr.remove(op[1]) elif action == \'reverse\': arr.reverse() elif action == \'count\': count = arr.count(op[1]) # count doesn\'t affect the array elif action == \'index\': if len(op) == 2: index = arr.index(op[1]) # index doesn\'t affect the array elif len(op) == 3: index = arr.index(op[1], op[2]) elif len(op) == 4: index = arr.index(op[1], op[2], op[3]) elif action == \'extend\': arr.extend(array.array(\'i\', op[1])) return arr.tolist()"},{"question":"# Distributed Training with Uneven Inputs You are tasked with implementing a distributed training loop for a simple neural network using PyTorch. The challenge lies in handling uneven inputs across different workers. To do this, you will use the generic join context manager provided by PyTorch. Requirements 1. **Network**: Implement a simple feedforward neural network. 2. **Data**: Simulate uneven inputs for different workers. 3. **Training Loop**: Implement the distributed training loop, ensuring to handle the uneven inputs using the `Join` context manager. Implementation Details 1. **Network**: - Create a neural network with one hidden layer using `torch.nn.Module`. 2. **Data**: - Simulate a dataset where each worker gets a different number of inputs. Use a simple dataset for demonstration. 3. **Training Loop**: - Implement the training loop such that it runs in a distributed fashion. - Ensure to make use of the `Join` context manager to handle uneven inputs. Function Signature ```python def distributed_training_with_uneven_inputs(num_workers: int): Runs a distributed training loop with uneven inputs. Parameters: num_workers (int): Number of distributed workers. Returns: None - The function should print training progress. ``` # Constraints - Use PyTorch version >= 1.8.0. - Simulate distributed environment using multiprocessing if you don\'t have a distributed setup. - Ensure proper synchronization where necessary. # Example Usage ```python if __name__ == \\"__main__\\": distributed_training_with_uneven_inputs(num_workers=4) ``` This function should demonstrate the training process for a simple network with uneven data distribution across workers, correctly handling the join using the PyTorch join context manager.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.multiprocessing import Process, Queue import torch.distributed as dist from torch.distributed import rpc from torch.nn.parallel import DistributedDataParallel as DDP # Define a simple feedforward neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def train(rank, world_size, dataset): dist.init_process_group(\'gloo\', rank=rank, world_size=world_size) model = SimpleNet().to(rank) ddp_model = DDP(model, device_ids=[rank]) optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) criterion = nn.MSELoss() for epoch in range(2): for data in dataset: inputs = data[0].to(rank) targets = data[1].to(rank) optimizer.zero_grad() outputs = ddp_model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() if rank == 0: print(f\\"Rank {rank}, Epoch [{epoch + 1}/2], Loss: {loss.item():.4f}\\") dist.destroy_process_group() def distributed_training_with_uneven_inputs(num_workers): datasets = [] # Simulate uneven inputs for i in range(num_workers): inputs = torch.randn((5 * (i + 1), 10)) targets = torch.randn((5 * (i + 1), 10)) dataset = [(inputs[j], targets[j]) for j in range(inputs.size(0))] datasets.append(dataset) processes = [] for rank in range(num_workers): p = Process(target=train, args=(rank, num_workers, datasets[rank])) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": distributed_training_with_uneven_inputs(num_workers=4)"},{"question":"Objective Write a Python program that demonstrates your understanding of object serialization and persistence using `pickle`, `shelve`, and `sqlite3`. You will create a system to manage student records with the ability to serialize student objects to a file, store and retrieve them using `shelve`, and handle operations with an SQLite database. Requirements 1. **Define a `Student` class**: - Attributes: `student_id` (int), `name` (str), `grade` (float). - Methods: `__init__`, `__str__`. 2. **Function: `serialize_students(student_list, filename)`**: - Input: - `student_list`: A list of `Student` objects. - `filename`: The name of the file to save the serialized data. - Output: None - Use `pickle` to serialize the list of students and store it in the specified file. 3. **Function: `deserialize_students(filename)`**: - Input: - `filename`: The name of the file from which to deserialize the data. - Output: - Return the list of `Student` objects. - Use `pickle` to deserialize the list of students from the specified file. 4. **Function: `store_students_shelve(student_list, shelve_name)`**: - Input: - `student_list`: A list of `Student` objects. - `shelve_name`: The name of the shelve database. - Output: None - Use `shelve` to store each `Student` object in the shelve database using `student_id` as the key. 5. **Function: `load_students_shelve(shelve_name)`**: - Input: - `shelve_name`: The name of the shelve database. - Output: - A list of `Student` objects retrieved from the shelve database. - Load the `Student` objects from the shelve database. 6. **Function: `create_sqlite_db(db_name)`**: - Input: - `db_name`: The name of the SQLite database. - Output: None - Create an SQLite database and a table `students` with columns `student_id` (primary key), `name`, and `grade`. 7. **Function: `insert_student_sqlite(db_name, student)`**: - Input: - `db_name`: The name of the SQLite database. - `student`: A `Student` object. - Output: None - Insert the `Student` object into the `students` table of the SQLite database. 8. **Function: `retrieve_students_sqlite(db_name)`**: - Input: - `db_name`: The name of the SQLite database. - Output: - A list of `Student` objects retrieved from the SQLite database. - Retrieve and convert each row from the `students` table into a `Student` object. Constraints - Ensure input validation where necessary (e.g., valid file paths, valid SQLite database names). - All database operations should handle exceptions gracefully with appropriate error messages. Example Usage ```python # Define some student objects students = [Student(1, \\"Alice\\", 95.5), Student(2, \\"Bob\\", 88.0), Student(3, \\"Charlie\\", 92.0)] # Serialize and deserialize using pickle serialize_students(students, \'students.pkl\') loaded_students = deserialize_students(\'students.pkl\') print(loaded_students) # Store and load using shelve store_students_shelve(students, \'students_shelve\') shelve_students = load_students_shelve(\'students_shelve\') print(shelve_students) # Create SQLite database and perform operations create_sqlite_db(\'school.db\') for student in students: insert_student_sqlite(\'school.db\', student) retrieved_students = retrieve_students_sqlite(\'school.db\') print(retrieved_students) ```","solution":"import pickle import shelve import sqlite3 class Student: def __init__(self, student_id, name, grade): self.student_id = student_id self.name = name self.grade = grade def __str__(self): return f\\"Student(id={self.student_id}, name={self.name}, grade={self.grade})\\" def serialize_students(student_list, filename): with open(filename, \'wb\') as file: pickle.dump(student_list, file) def deserialize_students(filename): with open(filename, \'rb\') as file: return pickle.load(file) def store_students_shelve(student_list, shelve_name): with shelve.open(shelve_name) as db: for student in student_list: db[str(student.student_id)] = student def load_students_shelve(shelve_name): with shelve.open(shelve_name) as db: return [db[key] for key in db] def create_sqlite_db(db_name): conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute( CREATE TABLE IF NOT EXISTS students ( student_id INTEGER PRIMARY KEY, name TEXT, grade REAL ) ) conn.commit() conn.close() def insert_student_sqlite(db_name, student): try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute( \\"INSERT INTO students (student_id, name, grade) VALUES (?, ?, ?)\\", (student.student_id, student.name, student.grade) ) conn.commit() except sqlite3.Error as e: print(f\\"SQLite error: {e}\\") finally: conn.close() def retrieve_students_sqlite(db_name): conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\\"SELECT student_id, name, grade FROM students\\") rows = cursor.fetchall() conn.close() student_list = [Student(row[0], row[1], row[2]) for row in rows] return student_list"},{"question":"# Kernel Density Estimation using scikit-learn **Objective**: Implement Kernel Density Estimation (KDE) using scikit-learn to estimate and visualize the probability density function of a given dataset. **Problem Statement**: You are provided with a 2-dimensional dataset `data.npy` representing two features. Your task is to: 1. Load the dataset. 2. Implement KDE using scikit-learn with multiple kernel options. 3. Visualize the KDE results. **Instructions**: 1. Load the dataset from `data.npy`. 2. Implement `fit_kde` function that accepts the dataset, bandwidth, and a kernel type. This function should fit the KDE model. 3. Implement `plot_kde` function to visualize the fitted KDE over a grid of points. 4. Test your functions by fitting KDE with \'gaussian\', \'tophat\', and \'epanechnikov\' kernels, using bandwidth 0.5. 5. Plot and compare the density estimates obtained from different kernels. **Functions**: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def load_data(file_path): Load the data from a .npy file. Parameters: - file_path (str): The path to the .npy file. Returns: - data (ndarray): Loaded data. return np.load(file_path) def fit_kde(data, bandwidth, kernel): Fit a Kernel Density Estimator to the data. Parameters: - data (ndarray): 2D dataset of shape (n_samples, n_features). - bandwidth (float): The bandwidth of the kernel. - kernel (str): The kernel type (e.g., \'gaussian\', \'tophat\', etc.). Returns: - kde (KernelDensity): Fitted KernelDensity object. kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(data) return kde def plot_kde(kde, data, title): Plot the Kernel Density Estimate over a grid. Parameters: - kde (KernelDensity): Fitted KernelDensity object. - data (ndarray): Original dataset for plotting. - title (str): Title for the plot. x = np.linspace(data[:, 0].min() - 1, data[:, 0].max() + 1, 100) y = np.linspace(data[:, 1].min() - 1, data[:, 1].max() + 1, 100) X, Y = np.meshgrid(x, y) grid = np.vstack([X.ravel(), Y.ravel()]).T Z = np.exp(kde.score_samples(grid)).reshape(X.shape) plt.figure(figsize=(8, 6)) plt.contourf(X, Y, Z, levels=100, cmap=\'viridis\') plt.scatter(data[:, 0], data[:, 1], c=\'white\', s=5, edgecolor=\'k\') plt.title(title) plt.colorbar(label=\'Density\') plt.show() # Usage data = load_data(\'data.npy\') bandwidth = 0.5 kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] for kernel in kernels: kde = fit_kde(data, bandwidth, kernel) plot_kde(kde, data, f\'KDE with {kernel} kernel\') ``` **Inputs**: - Path of the `data.npy` file. **Outputs**: - KDE plots using specified kernels and bandwidth. **Constraints**: - The dataset is 2-dimensional. - Use a bandwidth of 0.5. - Use only the kernel types mentioned above. **Performance Requirements**: - Ensure that KDE fitting and plotting are computationally feasible for up to 10,000 data points.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def load_data(file_path): Load the data from a .npy file. Parameters: - file_path (str): The path to the .npy file. Returns: - data (ndarray): Loaded data. return np.load(file_path) def fit_kde(data, bandwidth, kernel): Fit a Kernel Density Estimator to the data. Parameters: - data (ndarray): 2D dataset of shape (n_samples, n_features). - bandwidth (float): The bandwidth of the kernel. - kernel (str): The kernel type (e.g., \'gaussian\', \'tophat\', etc.). Returns: - kde (KernelDensity): Fitted KernelDensity object. kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(data) return kde def plot_kde(kde, data, title): Plot the Kernel Density Estimate over a grid. Parameters: - kde (KernelDensity): Fitted KernelDensity object. - data (ndarray): Original dataset for plotting. - title (str): Title for the plot. x = np.linspace(data[:, 0].min() - 1, data[:, 0].max() + 1, 100) y = np.linspace(data[:, 1].min() - 1, data[:, 1].max() + 1, 100) X, Y = np.meshgrid(x, y) grid = np.vstack([X.ravel(), Y.ravel()]).T Z = np.exp(kde.score_samples(grid)).reshape(X.shape) plt.figure(figsize=(8, 6)) plt.contourf(X, Y, Z, levels=100, cmap=\'viridis\') plt.scatter(data[:, 0], data[:, 1], c=\'white\', s=5, edgecolor=\'k\') plt.title(title) plt.colorbar(label=\'Density\') plt.show()"},{"question":"Spectral Co-Clustering Implementation Objective: Demonstrate your understanding of the Spectral Co-Clustering algorithm in the scikit-learn package by implementing and evaluating biclusters on a synthetic dataset. Problem Statement: You are provided with a synthetic dataset with a hidden bicluster structure. Your task is to: 1. Implement the Spectral Co-Clustering algorithm from the scikit-learn package. 2. Fit the model on the provided dataset. 3. Extract the biclusters and rearrange the dataset to make the biclusters contiguous. 4. Evaluate the quality of the biclusters using the Jaccard index and consensus score provided in scikit-learn. Input: - A 2D numpy array `data` of shape `(m, n)` representing the dataset with a hidden bicluster structure. - An integer `k` representing the number of biclusters to find. Output: - A 2D numpy array `rearranged_data` of shape `(m, n)` where rows and columns are rearranged to make biclusters contiguous. - A float `jaccard_index` representing the average Jaccard index of the biclusters. - A float `consensus_score` representing the consensus score of the biclustering result. Constraints: - You must use the Spectral Co-Clustering algorithm from the scikit-learn package. - Your implementation should handle datasets of size up to `1000 x 1000`. Example: ```python import numpy as np from sklearn.cluster import SpectralCoclustering from sklearn.metrics import consensus_score def spectral_co_clustering(data, k): model = SpectralCoclustering(n_clusters=k, random_state=0) model.fit(data) # Rearrange rows and columns for visualization fit_data = data[np.argsort(model.row_labels_)] fit_data = fit_data[:, np.argsort(model.column_labels_)] labels_a = model.biclusters_[0].astype(int) labels_b = model.biclusters_[1].astype(int) # Compute bicluster similarity using Jaccard index and consensus score jaccard_index_values = [] for i in range(k): for j in range(i+1, k): intersect = np.sum(labels_a[i] & labels_a[j] & labels_b[i] & labels_b[j]) union = np.sum(labels_a[i] | labels_a[j] | labels_b[i] | labels_b[j]) jaccard_index_values.append(intersect / union) avg_jaccard_index = np.mean(jaccard_index_values) cons_score = consensus_score(model.get_indices((data > 0).astype(int)), model.get_indices((data > 0).astype(int))) return fit_data, avg_jaccard_index, cons_score data = np.random.random((10, 10)) k = 3 rearranged_data, jaccard_index, consensus_score = spectral_co_clustering(data, k) print(\\"Rearranged Data:n\\", rearranged_data) print(\\"Average Jaccard Index:\\", jaccard_index) print(\\"Consensus Score:\\", consensus_score) ``` In this question, students will demonstrate their knowledge of importing the necessary modules from scikit-learn, fitting a model, extracting biclustering labels, rearranging the dataset, and evaluating the quality of the found biclusters using provided metrics.","solution":"import numpy as np from sklearn.cluster import SpectralCoclustering from sklearn.metrics import consensus_score def spectral_co_clustering(data, k): Performs Spectral Co-Clustering on the input dataset and evaluates using Jaccard index and consensus score. Parameters: data (np.ndarray): 2D numpy array of shape (m, n) representing the dataset with hidden bicluster structure. k (int): The number of biclusters to find. Returns: np.ndarray: Rearranged dataset with biclusters made contiguous. float: Average Jaccard index of the biclusters. float: Consensus score of the biclustering result. model = SpectralCoclustering(n_clusters=k, random_state=0) model.fit(data) # Rearrange rows and columns for visualization fit_data = data[np.argsort(model.row_labels_)] fit_data = fit_data[:, np.argsort(model.column_labels_)] # Compute Jaccard index for the biclusters jaccard_indices = [] for i in range(k): for j in range(i + 1, k): biclust1 = (model.row_labels_ == i) & (model.column_labels_ == i) biclust2 = (model.row_labels_ == j) & (model.column_labels_ == j) intersection = np.sum(biclust1 & biclust2) union = np.sum(biclust1 | biclust2) if union > 0: jaccard_indices.append(intersection / union) avg_jaccard_index = np.mean(jaccard_indices) if jaccard_indices else 0 # Compute consensus score cons_score = consensus_score(model.biclusters_, (model.row_labels_, model.column_labels_)) return fit_data, avg_jaccard_index, cons_score"},{"question":"# Coding Assessment: Working with `sklearn.datasets` Objective: Demonstrate your understanding of loading, fetching, and manipulating datasets using the `sklearn.datasets` package. Problem Statement: You are tasked with evaluating the performance of different machine learning models on both synthetic and real-world datasets. Your goal is to: 1. Load a toy dataset. 2. Fetch a real-world dataset. 3. Generate a synthetic dataset. 4. Process these datasets appropriately and print some basic statistics. Instructions: 1. **Load a Toy Dataset**: - Load the famous `iris` dataset using the `load_iris` function. - Print the number of samples and features, and the first 5 rows of data. 2. **Fetch a Real-World Dataset**: - Fetch the `California Housing` dataset using the `fetch_california_housing` function. - Print the number of samples, features, and the description of the dataset. 3. **Generate a Synthetic Dataset**: - Generate a synthetic classification dataset with 1000 samples and 20 features using the `make_classification` function. The dataset should have 2 informative features and 2 classes. - Print the distribution of class labels. Expected Function Signature: ```python def evaluate_datasets(): # Load the iris dataset iris = load_iris() print(\\"Iris Dataset:\\") print(f\\"Number of samples: {iris.data.shape[0]}\\") print(f\\"Number of features: {iris.data.shape[1]}\\") print(f\\"First 5 rows: n{iris.data[:5]}\\") # Fetch the California Housing dataset cali_housing = fetch_california_housing() print(\\"nCalifornia Housing Dataset:\\") print(f\\"Number of samples: {cali_housing.data.shape[0]}\\") print(f\\"Number of features: {cali_housing.data.shape[1]}\\") print(f\\"Description: {cali_housing.DESCR}\\") # Generate a synthetic classification dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_classes=2, random_state=42) print(\\"nSynthetic Dataset:\\") unique, counts = np.unique(y, return_counts=True) print(f\\"Class distribution: {dict(zip(unique, counts))}\\") # Call the function to execute evaluate_datasets() ``` Notes: - Ensure you handle the data appropriately to extract and print the required information. - Use appropriate scikit-learn functions to achieve the tasks. - Make sure to import all the required modules/packages from scikit-learn and other necessary libraries.","solution":"from sklearn.datasets import load_iris, fetch_california_housing, make_classification import numpy as np def evaluate_datasets(): # Load the iris dataset iris = load_iris() print(\\"Iris Dataset:\\") print(f\\"Number of samples: {iris.data.shape[0]}\\") print(f\\"Number of features: {iris.data.shape[1]}\\") print(f\\"First 5 rows: n{iris.data[:5]}\\") # Fetch the California Housing dataset cali_housing = fetch_california_housing() print(\\"nCalifornia Housing Dataset:\\") print(f\\"Number of samples: {cali_housing.data.shape[0]}\\") print(f\\"Number of features: {cali_housing.data.shape[1]}\\") print(f\\"Description (first 500 characters): {cali_housing.DESCR[:500]}\\") # Generate a synthetic classification dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_classes=2, random_state=42) print(\\"nSynthetic Dataset:\\") unique, counts = np.unique(y, return_counts=True) print(f\\"Class distribution: {dict(zip(unique, counts))}\\") # Call the function to execute evaluate_datasets()"},{"question":"Coding Assessment Question # Objective: You are to write a Python function demonstrating the use of module import mechanisms. While the `imp` module is deprecated, this exercise will help you understand its function signatures and require you to convert them to their modern `importlib` equivalents. # Question: Implement a function `import_and_reload_module(module_name: str) -> object` that does the following: 1. Attempts to find and load a module given its name. 2. If the module is already loaded, reloads it. 3. Returns the module object. The function should use `importlib` functions (`importlib.util.find_spec`, `importlib.util.spec_from_file_location`, `importlib.util.module_from_spec`, and `importlib.reload`) rather than the deprecated `imp` module functions. Raise an appropriate exception if the module cannot be found or loaded. # Input: - `module_name` (str): The name of the module to be found and reloaded. # Output: - Returns the module object if successfully found and reloaded. - Raises an ImportError with a message `\\"Module not found\\"` if the module cannot be found. - Raises a generic Exception with a message `\\"Module could not be loaded\\"` if any other issue occurs during loading or reloading the module. # Example Usage: ```python try: module = import_and_reload_module(\'example_module\') print(f\\"The module {module.__name__} was successfully loaded and reloaded.\\") except ImportError as e: print(str(e)) except Exception as e: print(str(e)) ``` # Constraints: - Do not use the `imp` module directly. - Handle exceptions appropriately to give clear error messages. - Ensure the module is correctly reloaded if it is already present. # Note: You can use mock or example modules to test your function. For example, you may create a simple Python module named `example_module` and place it in the appropriate directory for your Python to find.","solution":"import importlib import sys def import_and_reload_module(module_name: str) -> object: Attempts to find and load a module given its name. If the module is already loaded, reloads it. Returns the module object. Parameters: module_name (str): The name of the module to be found and reloaded. Returns: object: The module object if successfully found and reloaded. Raises: ImportError: If the module cannot be found. Exception: If any other issue occurs during loading or reloading the module. try: if module_name in sys.modules: module = importlib.reload(sys.modules[module_name]) else: spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module \'{module_name}\' not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) sys.modules[module_name] = module return module except ImportError as e: raise ImportError(f\\"Module \'{module_name}\' not found\\") from e except Exception as e: raise Exception(f\\"Module \'{module_name}\' could not be loaded\\") from e"},{"question":"**Question: Custom Work Schedule Generator** You have been tasked with creating a custom work schedule generator for a team that works on rotating shifts. The team works 4 days on and 3 days off each week, starting from January 1st of a given year. The first shift starts on the given first work day, and the pattern repeats every week. Create a function `generate_work_schedule(year: int, first_workday: str) -> str` that generates a work schedule for the entire year in plain text format. # Inputs: - `year`: An integer representing the year of the schedule (e.g., 2023). - `first_workday`: A string representing the starting day of the week in full (e.g., \\"Monday\\"). # Output: - A string containing the work schedule for the entire year. Each month should be printed separately, with the workdays and off days labelled. Each day should be represented in the format \\"YYYY-MM-DD (DayName) - WORK/OFF\\". Separate months with new lines. # Guidelines: 1. Use the `calendar` module to generate and manipulate the dates. 2. Assume the work schedule contains exactly 4 workdays followed by 3 off days, repeatedly. 3. The schedule starts on the given `first_workday` and follows through the entire year. 4. Ensure the output is clear and well-formatted, with each month neatly separated and labeled. # Example: ```python import calendar def generate_work_schedule(year: int, first_workday: str) -> str: # TODO: Implementation goes here # Example Usage schedule = generate_work_schedule(2023, \\"Monday\\") print(schedule) ``` Output (partial): ``` January 2023: 2023-01-01 (Sunday) - OFF 2023-01-02 (Monday) - WORK 2023-01-03 (Tuesday) - WORK 2023-01-04 (Wednesday) - WORK 2023-01-05 (Thursday) - WORK 2023-01-06 (Friday) - OFF 2023-01-07 (Saturday) - OFF 2023-01-08 (Sunday) - OFF 2023-01-09 (Monday) - WORK ... February 2023: ... ``` Constraints: - You must correctly handle leap years. - The `first_workday` will always be a valid weekday name. # Note: Make sure to handle edge cases such as leap years and the transition between months properly.","solution":"import calendar from datetime import datetime, timedelta def generate_work_schedule(year: int, first_workday: str) -> str: weekdays = [\'Monday\', \'Tuesday\', \'Wednesday\', \'Thursday\', \'Friday\', \'Saturday\', \'Sunday\'] first_day_index = weekdays.index(first_workday) # Create start date of the year start_date = datetime(year, 1, 1) # Find the first workday in the year while start_date.weekday() != first_day_index: start_date += timedelta(days=1) work_days = [0, 1, 2, 3] # Monday to Thursday as workdays off_days = [4, 5, 6] # Friday to Sunday as off days schedule = [] for month in range(1, 13): # Get the number of days in the month month_days = calendar.monthrange(year, month)[1] # List to hold schedule of each month month_schedule = [] for day in range(1, month_days + 1): current_date = datetime(year, month, day) day_of_week = (current_date - start_date).days % 7 if day_of_week in work_days: month_schedule.append(f\\"{current_date.strftime(\'%Y-%m-%d (%A)\')} - WORK\\") else: month_schedule.append(f\\"{current_date.strftime(\'%Y-%m-%d (%A)\')} - OFF\\") # Add month header schedule.append(f\\"{calendar.month_name[month]} {year}:n\\") schedule.extend(month_schedule) schedule.append(\'n\') return \'n\'.join(schedule)"},{"question":"**Coding Assessment Question:** # Understanding Unix Group Database with Python\'s `grp` Module **Objective:** Implement a function that utilizes the `grp` module to return detailed information about Unix groups based on certain criteria. **Description:** You are tasked with writing a Python function that will extract and provide detailed information from the Unix group database. Your function will have to support the following functionalities: 1. **Get Group by Name** - Retrieve group information by the group name. 2. **Get Group by GID** - Retrieve group information by the group GID. 3. **List All Groups** - Return a summary of all groups in the database. **Function Signature:** ```python def group_info(action: str, identifier: Union[int, str, None] = None) -> Union[dict, list]: Retrieve information from the Unix group database based on the specified action. Parameters: action (str): The action to be performed. It can be one of the following: - \'name\' : Retrieve a group by name (requires `identifier` to be the group name as a string). - \'gid\' : Retrieve a group by GID (requires `identifier` to be the GID as an integer). - \'all\' : Retrieve all groups (identifier should be None). identifier (Union[int, str, None]): The group name or GID required for \'name\' or \'gid\' actions. It should be None for the \'all\' action. Returns: Union[dict, list]: A dictionary if action is \'name\' or \'gid\', containing the group\'s details. A list of dictionaries if action is \'all\', where each dictionary contains details of a group. Raises: KeyError: If the specified group (by name or GID) is not found. TypeError: If incorrect type is provided for \'identifier\' in \'name\' or \'gid\' actions. pass ``` **Example Usage:** 1. **Get Group by Name:** ```python print(group_info(action=\'name\', identifier=\'staff\')) ``` Output: ```python { \'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 50, \'gr_mem\': [\'user1\', \'user2\'] } ``` 2. **Get Group by GID:** ```python print(group_info(action=\'gid\', identifier=100)) ``` Output: ```python { \'gr_name\': \'users\', \'gr_passwd\': \'x\', \'gr_gid\': 100, \'gr_mem\': [\'user1\', \'user3\', \'user5\'] } ``` 3. **List All Groups:** ```python print(group_info(action=\'all\')) ``` Output: ```python [ {\'gr_name\': \'root\', \'gr_passwd\': \'x\', \'gr_gid\': 0, \'gr_mem\': []}, {\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 50, \'gr_mem\': [\'user1\', \'user2\']}, {\'gr_name\': \'users\', \'gr_passwd\': \'x\', \'gr_gid\': 100, \'gr_mem\': [\'user1\', \'user3\', \'user5\']}, ... ] ``` **Constraints:** - The function will only be executed on Unix systems where the `grp` module is available. - You must handle the `KeyError` and `TypeError` appropriately. - Your function should be optimized for performance when listing all groups. **Performance Requirements:** - Ensure that the lookups for group name and GID are efficient. - Minimize the overhead when retrieving all group entries. Implement the function to correctly handle each of these actions using the Python `grp` module.","solution":"import grp from typing import Union, List, Dict, Any def group_info(action: str, identifier: Union[int, str, None] = None) -> Union[Dict[str, Any], List[Dict[str, Any]]]: Retrieve information from the Unix group database based on the specified action. Parameters: action (str): The action to be performed. It can be one of the following: - \'name\' : Retrieve a group by name (requires `identifier` to be the group name as a string). - \'gid\' : Retrieve a group by GID (requires `identifier` to be the GID as an integer). - \'all\' : Retrieve all groups (identifier should be None). identifier (Union[int, str, None]): The group name or GID required for \'name\' or \'gid\' actions. It should be None for the \'all\' action. Returns: Union[dict, list]: A dictionary if action is \'name\' or \'gid\', containing the group\'s details. A list of dictionaries if action is \'all\', where each dictionary contains details of a group. Raises: KeyError: If the specified group (by name or GID) is not found. TypeError: If incorrect type is provided for \'identifier\' in \'name\' or \'gid\' actions. if action == \'name\': if not isinstance(identifier, str): raise TypeError(\\"Identifier must be a string for action \'name\'.\\") try: group = grp.getgrnam(identifier) return {\'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem} except KeyError: raise KeyError(f\\"Group name \'{identifier}\' not found.\\") elif action == \'gid\': if not isinstance(identifier, int): raise TypeError(\\"Identifier must be an integer for action \'gid\'.\\") try: group = grp.getgrgid(identifier) return {\'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem} except KeyError: raise KeyError(f\\"Group GID \'{identifier}\' not found.\\") elif action == \'all\': groups = grp.getgrall() return [{\'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem} for group in groups] else: raise ValueError(\\"Invalid action. Action must be one of \'name\', \'gid\', \'all\'.\\")"},{"question":"Objective Implement a function `execute_python_code(code: str, input_type: str) -> Any` that takes a string of Python code and the type of input (either \\"file\\", \\"interactive\\", or \\"expression\\") and executes the code accordingly. The function should return the result of the execution or `None` if there is no result. Input 1. `code` (str): A string containing Python code to be executed. 2. `input_type` (str): A string indicating the type of input. It can be: - `\\"file\\"`: Treat `code` as the content of a Python file. - `\\"interactive\\"`: Treat `code` as interactive input. - `\\"expression\\"`: Treat `code` as an expression to be evaluated. Output - The function should return the result of the executed code if an expression was given, otherwise return `None`. Constraints - `input_type` will be one of `\\"file\\"`, `\\"interactive\\"`, or `\\"expression\\"`. - The code provided in the `code` string will be valid Python code. - The function should handle potential exceptions and return `None` in case of any error during execution. Function Signature ```python def execute_python_code(code: str, input_type: str) -> Any: pass ``` Example Usage ```python # Example 1: File Input code = \\"x = 5ny = 10nprint(x + y)\\" input_type = \\"file\\" execute_python_code(code, input_type) # Should print 15 and return None # Example 2: Interactive Input code = \\"x = 5nprint(x)\\" input_type = \\"interactive\\" execute_python_code(code, input_type) # Should print 5 and return None # Example 3: Expression Input code = \\"5 + 10\\" input_type = \\"expression\\" result = execute_python_code(code, input_type) # Should return 15 print(result) # Outputs: 15 ``` Notes 1. For \\"file\\" and \\"interactive\\" input types, assume that any print statements within the code will output to the standard console. 2. For \\"expression\\" input type, the result of the evaluated expression should be returned by the function. Hints - Use `exec()` to execute \\"file\\" and \\"interactive\\" input types. - Use `eval()` to evaluate \\"expression\\" input type. - Manage the execution context to isolate the namespace for each execution to avoid any side effects.","solution":"def execute_python_code(code: str, input_type: str) -> any: try: if input_type == \\"file\\" or input_type == \\"interactive\\": exec(code, {}) return None elif input_type == \\"expression\\": return eval(code, {}) except: return None"},{"question":"Context You are tasked with developing a multilingual application in Python using the `gettext` module. As part of the task, you need to write code that handles translations for different languages, sets up message catalogs, and switches between languages dynamically based on user input. Objective Implement a Python function that: 1. Sets up the translation environment for a given domain and locale directory. 2. Translates a given message based on the current language setting. 3. Has the capability to switch languages dynamically. Function Signature ```python def setup_translation(domain: str, localedir: str, languages: list) -> dict: Sets up the translation environment for the given domain and locale directory. Args: - domain (str): The domain for which the translations are to be set up. - localedir (str): The directory where the locale files are stored. - languages (list): A list of languages to support (e.g., [\'en\', \'fr\', \'de\']). Returns: - dict: A dictionary where keys are language codes and values are translation functions. pass def translate_message(message: str, language: str, translators: dict) -> str: Translates the given message into the specified language. Args: - message (str): The message to be translated. - language (str): The language code into which the message is to be translated. - translators (dict): The dictionary of translators returned from `setup_translation`. Returns: - str: The translated message. pass ``` Requirements 1. **setup_translation**: - Should bind the text domain to the locale directory. - Should create a translator for each language specified in the `languages` list. - Should return a dictionary where keys are language codes and values are functions that translate messages for the corresponding language. 2. **translate_message**: - Should take a message string, a language code, and the translators dictionary. - Should return the translated string using the appropriate translator. Example Usage ```python domain = \'myapp\' localedir = \'/path/to/my/language/directory\' languages = [\'en\', \'fr\', \'de\'] # Initialize translators translators = setup_translation(domain, localedir, languages) # Translate a message message = \'This is a translatable string.\' translated_message = translate_message(message, \'fr\', translators) print(translated_message) # Prints the message in French if translation exists ``` Constraints - Assume that `.mo` files for the specified domain and languages are correctly placed in the `/path/to/my/language/directory`. - The solution should handle cases where a translation for the given message or language does not exist by returning the original message. Performance Requirements - The function should efficiently handle the setup and translation processes, even if multiple languages are involved.","solution":"import gettext def setup_translation(domain: str, localedir: str, languages: list) -> dict: Sets up the translation environment for the given domain and locale directory. Args: - domain (str): The domain for which the translations are to be set up. - localedir (str): The directory where the locale files are stored. - languages (list): A list of languages to support (e.g., [\'en\', \'fr\', \'de\']). Returns: - dict: A dictionary where keys are language codes and values are translation functions. translators = {} for lang in languages: try: translators[lang] = gettext.translation(domain, localedir, languages=[lang]) except FileNotFoundError: translators[lang] = gettext.NullTranslations() return translators def translate_message(message: str, language: str, translators: dict) -> str: Translates the given message into the specified language. Args: - message (str): The message to be translated. - language (str): The language code into which the message is to be translated. - translators (dict): The dictionary of translators returned from `setup_translation`. Returns: - str: The translated message. translator = translators.get(language, gettext.NullTranslations()) return translator.gettext(message) # Example usage domain = \'myapp\' localedir = \'/path/to/my/language/directory\' languages = [\'en\', \'fr\', \'de\'] # Initialize translators translators = setup_translation(domain, localedir, languages) # Translate a message message = \'This is a translatable string.\' translated_message = translate_message(message, \'fr\', translators) print(translated_message) # Prints the message in French if translation exists"},{"question":"# PyTorch Tensor Storage Manipulation Objective: You are tasked with creating a function that demonstrates an understanding of PyTorch\'s tensor storage and its manipulation. The function should perform the following steps: 1. Create a tensor filled with random values. 2. Extract the storage of this tensor. 3. Clone the storage and modify the values in the cloned storage. 4. Set the original tensor\'s storage to this modified storage. 5. Return the modified tensor. Function Signature: ```python def manipulate_tensor_storage(size: int) -> torch.Tensor: pass ``` Input: - `size` (int): The size of the 1-dimensional tensor to be created. Output: - (torch.Tensor): A tensor whose storage has been modified as specified. Constraints: - You must use the PyTorch library. - Direct manipulation of the tensor\'s storage is required. Example: ```python >>> import torch >>> torch.manual_seed(0) # For reproducibility >>> size = 5 >>> manipulate_tensor_storage(size) tensor([0., 0., 0., 0., 0.]) ``` Note: The example above is just to illustrate the function\'s output format. The actual modified values should depend on your implementation. Be sure to handle the tensor storage manipulation correctly as per the instructions provided. Good luck!","solution":"import torch def manipulate_tensor_storage(size: int) -> torch.Tensor: # Step 1: Create a tensor filled with random values original_tensor = torch.rand(size) # Step 2: Extract the storage of this tensor original_storage = original_tensor.storage() # Step 3: Clone the storage and modify the values in the cloned storage cloned_storage = original_storage.clone() for i in range(len(cloned_storage)): cloned_storage[i] = 0 # Example modification - set all elements to 0 # Step 4: Set the original tensor\'s storage to this modified storage original_tensor.set_(cloned_storage) # Step 5: Return the modified tensor return original_tensor"},{"question":"# Encoding and Decoding Binary Data with CRC Checksum You are required to implement a function that reads binary data, encodes it using Base64 encoding, and then decodes it back to its binary form. Additionally, compute the CRC32 checksum for both the original and decoded binary data to ensure data integrity. Function Signature ```python def encode_decode_crc(data: bytes) -> Tuple[bytes, bytes, int, int]: This function performs the following tasks: 1. Encodes the input binary data using Base64 encoding. 2. Decodes the encoded data back to its original binary form. 3. Computes the CRC32 checksum for both the original and decoded binary data. Parameters: data (bytes): The original binary data to be encoded and decoded. Returns: Tuple[bytes, bytes, int, int]: A tuple containing: - The Base64 encoded data (as bytes). - The decoded binary data (as bytes). - The CRC32 checksum of the original data. - The CRC32 checksum of the decoded data. pass ``` # Requirements 1. **Input:** - `data`: A bytes object representing the binary data. 2. **Output:** - A tuple containing: - `encoded_data`: Base64 encoded data, as a bytes object. - `decoded_data`: The decoded binary data, as a bytes object. - `original_crc`: CRC32 checksum of the original data, as an integer. - `decoded_crc`: CRC32 checksum of the decoded data, as an integer. 3. **Constraints:** - `data` will have a maximum length of 10^6 bytes. 4. **Performance Requirements:** - The function should efficiently handle the encoding, decoding, and checksum computation even for the upper limit of the data size. # Example ```python original_data = b\\"Hello, world!\\" encoded_data = binascii.b2a_base64(original_data) decoded_data = binascii.a2b_base64(encoded_data) original_crc = binascii.crc32(original_data) decoded_crc = binascii.crc32(decoded_data) assert encoded_data == b\'SGVsbG8sIHdvcmxkIQ==n\' assert decoded_data == b\\"Hello, world!\\" assert original_crc == 3964322768 assert decoded_crc == 3964322768 ``` **Note:** While implementing the function, make sure to handle different lengths and types of binary data used for testing the code.","solution":"import binascii import base64 from typing import Tuple def encode_decode_crc(data: bytes) -> Tuple[bytes, bytes, int, int]: This function performs the following tasks: 1. Encodes the input binary data using Base64 encoding. 2. Decodes the encoded data back to its original binary form. 3. Computes the CRC32 checksum for both the original and decoded binary data. Parameters: data (bytes): The original binary data to be encoded and decoded. Returns: Tuple[bytes, bytes, int, int]: A tuple containing: - The Base64 encoded data (as bytes). - The decoded binary data (as bytes). - The CRC32 checksum of the original data. - The CRC32 checksum of the decoded data. # Encode the data using Base64 encoded_data = base64.b64encode(data) # Decode the Base64 encoded data back to binary decoded_data = base64.b64decode(encoded_data) # Compute the CRC32 checksum of the original data original_crc = binascii.crc32(data) # Compute the CRC32 checksum of the decoded data decoded_crc = binascii.crc32(decoded_data) return encoded_data, decoded_data, original_crc, decoded_crc"},{"question":"# Question: Implementing Dynamic Shape-Aware Model Export Objective: Your task is to implement a PyTorch module and export it using `torch.export.export` while handling both static and dynamic control flows. This exercise will test your understanding of tracing, handling static vs. dynamic values, and managing control flows using PyTorch operators. Problem Statement: Implement a PyTorch module `DynamicShapeAwareModule` with a `forward` method that includes control flow based on dynamic shapes. Then, write a function `export_module` that traces and exports this module with dynamic input shapes. Ensure the correct handling of static and dynamic conditions to demonstrate understanding of the export behavior. Requirements: 1. **DynamicShapeAwareModule**: - Initialize with two parameters `factor` (int) and `threshold` (float). - `forward` method: - Takes an input tensor `x` (of any shape). - If the number of elements in `x` exceeds `threshold`, multiply `x` by `factor` and return `[x, x.sum(dim=1)]` (sum along dimension 1). - Otherwise, return `[x, x.mean(dim=0)]` (mean along dimension 0). - *Hint*: Use `torch.cond` for dynamic control flow. 2. **export_module**: - Takes an instance of `DynamicShapeAwareModule` and a sample input tensor `example_input`. - Traces and exports the module using `torch.export.export` with `example_input`. - Returns the exported model. Constraints: - Assume input tensor `x` has at least one dimension. - Support only PyTorch version 1.9.0 or later. - The `factor` parameter is static, do not dynamically update it. Function Signatures: ```python import torch from torch import nn from torch.export import export class DynamicShapeAwareModule(nn.Module): def __init__(self, factor: int, threshold: float): super(DynamicShapeAwareModule, self).__init__() self.factor = factor self.threshold = threshold def forward(self, x: torch.Tensor): # TODO: Implement the forward method with dynamic control flow based on the shape of x. def export_module(module: DynamicShapeAwareModule, example_input: torch.Tensor): # TODO: Implement the function to trace and export the module. ``` Example Usage: ```python if __name__ == \\"__main__\\": module = DynamicShapeAwareModule(factor=2, threshold=100.0) example_input = torch.randn(64, 10) # Example input tensor exported_model = export_module(module, example_input) print(exported_model.graph_module.code) # Output the exported code for verification ``` Expected Output: The exported model should handle dynamic conditions correctly and output the expected graph based on the input tensor\'s shape. Ensure appropriate use of `torch.cond` for dynamic branches.","solution":"import torch from torch import nn class DynamicShapeAwareModule(nn.Module): def __init__(self, factor: int, threshold: float): super(DynamicShapeAwareModule, self).__init__() self.factor = factor self.threshold = threshold def forward(self, x: torch.Tensor): if x.numel() > self.threshold: x = x * self.factor return [x, x.sum(dim=1)] else: return [x, x.mean(dim=0)] def export_module(module: DynamicShapeAwareModule, example_input: torch.Tensor): traced_model = torch.jit.trace(module, example_input) torch.jit.save(traced_model, \'dynamic_shape_aware_module.pt\') return traced_model"},{"question":"**Objective:** Demonstrate understanding and practical usage of the `plistlib` module for reading, writing, and manipulating property list files in both XML and binary formats. Problem Statement You are tasked with writing functions to process property list files used by Apple. Write a set of functions to accomplish the following tasks: 1. **Load a plist file**: Read a property list file from a given file path and return the content as a dictionary. 2. **Save a dictionary to a plist file**: Write a given dictionary to a specified file path in either XML or binary format. 3. **Convert between plist formats**: Convert an XML-formatted plist file to a binary-formatted plist file. Function Specifications 1. **load_plist(file_path, fmt=None)** - **Input**: - `file_path` (str): Path to the plist file to read. - `fmt` (str, optional): Format of the plist file. Default is `None` (autodetect format). - **Output**: - Returns a dictionary containing the content of the plist file. - **Exceptions**: - Raise a `FileNotFoundError` if the file does not exist. - Raise a `plistlib.InvalidFileException` if the file cannot be parsed. 2. **save_plist(data, file_path, fmt=\'FMT_XML\', sort_keys=True, skipkeys=False)** - **Input**: - `data` (dict): Dictionary to save as a plist file. - `file_path` (str): Path to save the plist file. - `fmt` (str): Format of the plist file (\'FMT_XML\' or \'FMT_BINARY\'). Default is \'FMT_XML\'. - `sort_keys` (bool): Whether to sort the dictionary keys. Default is `True`. - `skipkeys` (bool): Whether to skip non-string keys in dictionaries. Default is `False`. - **Output**: - None (Writes plist file to the given path). - **Exceptions**: - Raise a `TypeError` if a key in the dictionary is not a string and `skipkeys` is `False`. - Raise a `TypeError` if an object in the dictionary is of an unsupported type. - Raise an `OverflowError` for integer values that cannot be represented in binary plist files. 3. **convert_plist(src_file_path, dest_file_path, dest_fmt)** - **Input**: - `src_file_path` (str): Path to the source plist file (in XML format). - `dest_file_path` (str): Path to save the converted plist file. - `dest_fmt` (str): Destination format (\'FMT_XML\' or \'FMT_BINARY\'). - **Output**: - None (Converts and writes plist file to the given path). - **Exceptions**: - Raise a `ValueError` if `dest_fmt` is not \'FMT_XML\' or \'FMT_BINARY\'. Example Usage ```python import datetime # Example dictionary example_dict = { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"is_student\\": False, \\"exam_results\\": { \\"math\\": 95, \\"science\\": 88 }, \\"graduation_date\\": datetime.datetime(2021, 5, 15) } # Save dictionary to XML plist file save_plist(example_dict, \\"example.xml\\", fmt=\\"FMT_XML\\") # Load plist file data = load_plist(\\"example.xml\\") print(data) # Convert XML plist file to binary format convert_plist(\\"example.xml\\", \\"example_binary.plist\\", dest_fmt=\\"FMT_BINARY\\") ``` Implement these functions in a Python file and ensure they handle exceptions gracefully and are well-documented. This assessment will test your ability to work with external libraries and handle different data formats with proper error handling.","solution":"import plistlib def load_plist(file_path, fmt=None): Read a property list file from a given file path and return the content as a dictionary. :param file_path: Path to the plist file to read. :param fmt: Format of the plist file. Default is None (autodetect format). :return: Dictionary containing the content of the plist file. try: with open(file_path, \'rb\') as f: if fmt is None: return plistlib.load(f) elif fmt == \'FMT_XML\': return plistlib.load(f, fmt=plistlib.FMT_XML) elif fmt == \'FMT_BINARY\': return plistlib.load(f, fmt=plistlib.FMT_BINARY) else: raise ValueError(\\"Unsupported format specified\\") except FileNotFoundError: raise except plistlib.InvalidFileException: raise def save_plist(data, file_path, fmt=\'FMT_XML\', sort_keys=True, skipkeys=False): Write a given dictionary to a specified file path in either XML or binary format. :param data: Dictionary to save as a plist file. :param file_path: Path to save the plist file. :param fmt: Format of the plist file (\'FMT_XML\' or \'FMT_BINARY\'). Default is \'FMT_XML\'. :param sort_keys: Whether to sort the dictionary keys. Default is True. :param skipkeys: Whether to skip non-string keys in dictionaries. Default is False. :return: None try: with open(file_path, \'wb\') as f: if fmt == \'FMT_XML\': plistlib.dump(data, f, fmt=plistlib.FMT_XML, sort_keys=sort_keys, skipkeys=skipkeys) elif fmt == \'FMT_BINARY\': plistlib.dump(data, f, fmt=plistlib.FMT_BINARY, sort_keys=sort_keys, skipkeys=skipkeys) else: raise ValueError(\\"Unsupported format specified\\") except TypeError: raise except OverflowError: raise def convert_plist(src_file_path, dest_file_path, dest_fmt): Convert an XML-formatted plist file to a binary-formatted plist file. :param src_file_path: Path to the source plist file. :param dest_file_path: Path to save the converted plist file. :param dest_fmt: Destination format (\'FMT_XML\' or \'FMT_BINARY\'). :return: None if dest_fmt not in (\'FMT_XML\', \'FMT_BINARY\'): raise ValueError(\\"Unsupported destination format specified\\") data = load_plist(src_file_path) save_plist(data, dest_file_path, fmt=dest_fmt)"},{"question":"**Question: Seaborn Density Estimation and Visualization** You are given the `penguins` dataset from Seaborn, which contains measurements on various attributes of penguins. Your task is to create a series of visualizations using Seaborn that demonstrate your understanding of density estimation, grouping, and plot customization. # Instructions 1. **Data Import**: Load the `penguins` dataset using Seaborn\'s `load_dataset` function. 2. **Kernel Density Estimation (KDE) Plot**: Create a density plot of the `flipper_length_mm` attribute using Seabornâ€™s `Plot` and `KDE` functionalities. 3. **Adjust Bandwidth**: Adjust the bandwidth by a factor of 0.25 on the density plot created in Step 2. 4. **Histogram and Density**: Combine a histogram and KDE plot for the `flipper_length_mm` attribute, ensuring that the density line is overlaid on the histogram. 5. **Grouping**: Show separate density plots for each species of penguins, with densities normalized within each species. 6. **Conditional Plot**: Create a faceted plot of the density estimation that additionally groups by the `sex` of the penguins. Ensure densities are normalized within each facet. # Constraints - Ensure the plots are clear and appropriately labeled. - Use at least one customization parameter (`bw_adjust`, `cut`, `gridsize`, etc.) in each plot. - Provide legends where applicable to clearly distinguish between groups or facets. # Expected Output You should submit the code for generating the above plots, ensuring that the following steps have been completed: 1. **Loading Dataset**: ```python from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` 2. **KDE Plot**: ```python import seaborn.objects as so p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE()) ``` 3. **Adjust Bandwidth**: ```python p.add(so.Area(), so.KDE(bw_adjust=0.25)) ``` 4. **Combine Histogram and KDE**: ```python p2 = p.add(so.Bars(alpha=.3), so.Hist(\\"density\\")) p2.add(so.Line(), so.KDE()) ``` 5. **Group by Species**: ```python p.add(so.Area(), so.KDE(common_norm=False), color=\\"species\\") ``` 6. **Facet by Sex**: ```python ( p.facet(\\"sex\\") .add(so.Area(), so.KDE(common_norm=[\\"col\\"]), color=\\"species\\") ) ``` Combine these steps into a complete script, ensuring that all plots are displayed.","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Kernel Density Estimation (KDE) Plot plot_kde = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE()) plt.figure(figsize=(8, 6)) plot_kde.show() # Step 3: Adjust Bandwidth plot_kde_bw_adjusted = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE(bw_adjust=0.25)) plt.figure(figsize=(8, 6)) plot_kde_bw_adjusted.show() # Step 4: Combined Histogram and Density Plot plot_hist_kde = so.Plot(penguins, x=\\"flipper_length_mm\\") plot_hist_kde.add(so.Bars(alpha=0.3), so.Hist()) plot_hist_kde.add(so.Line(), so.KDE()) plt.figure(figsize=(8, 6)) plot_hist_kde.show() # Step 5: Group by Species plot_species = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\").add(so.Area(), so.KDE(common_norm=False)) plt.figure(figsize=(8, 6)) plot_species.show() # Step 6: Facet by Sex plot_facet_sex = ( so.Plot(penguins, x=\\"flipper_length_mm\\") .facet(\\"sex\\") .add(so.Area(), so.KDE(common_norm=[\\"col\\"]), color=\\"species\\") ) plt.figure(figsize=(8, 6)) plot_facet_sex.show()"},{"question":"**Objective:** You are to implement a Python function **execute_python_code** that uses the C API functions described in the documentation to execute Python code passed as a string in specific global and local contexts. **Function Signature:** ```python def execute_python_code(code: str, globals: dict, locals: dict) -> any: ``` **Input:** - `code` (str): A string containing valid Python code to be executed. - `globals` (dict): A dictionary representing the global context in which the code is to be executed. - `locals` (dict): A dictionary representing the local context in which the code is to be executed. **Output:** - The function should return the result of executing the code. If the code returns a result, return that result. If the code does not return a result (e.g., it is an assignment or a print statement), return `None`. **Constraints:** - The code will be valid syntactically. - The globals and locals dictionaries will be correctly formatted. - Do not use Python\'s built-in `exec` or `eval` functions. **Requirements:** - You must use appropriate functions from the C API described in the documentation. - The function needs to handle the creation of necessary structures and passing correct parameters to the C functions. - Proper error handling should be implemented to manage exceptions raised during code execution. **Example Usage:** ```python globals_dict = {\\"__builtins__\\": __builtins__} locals_dict = {} result = execute_python_code(\\"a = 10\\", globals_dict, locals_dict) print(locals_dict[\'a\']) # Output should be: 10 result = execute_python_code(\\"a + 15\\", globals_dict, locals_dict) print(result) # Output should be: 25 result = execute_python_code(\\"print(\'Hello, World!\')\\", globals_dict, locals_dict) # Output should be: Hello, World! # Function return should be: None ``` **Note:** You are required to write the `execute_python_code` function. You will need to set up the Python C API within your function appropriately, compiling the input string into a code object, and then executing that code object within the provided global and local contexts. Handle any exceptions during execution and return appropriate values.","solution":"def execute_python_code(code: str, globals: dict, locals: dict) -> any: Executes Python code within the given global and local context. Parameters: code (str): The Python code to execute. globals (dict): The global context for the code. locals (dict): The local context for the code. Returns: any: The result of the code execution, or None if there is no result. try: # Compile code into a code object code_obj = compile(code, \'<string>\', \'exec\') # Execute code object within the provided globals and locals exec(code_obj, globals, locals) # Try to get the last expression\'s value if any expression was given code_obj = compile(code, \'<string>\', \'eval\') return eval(code_obj, globals, locals) except SyntaxError: # Handle case where the code is not an expression (just a statement) return None except Exception as e: # Raise any exception occurred during the execution raise e"},{"question":"**Objective:** Implement a data preprocessing function that prepares a dataset for machine learning by applying a series of transformations such as standardization, scaling, and encoding categorical features. Use these preprocessing techniques in a pipeline to ensure consistency and reproducibility. **Problem Statement:** You are given a dataset with both numerical and categorical features. Your task is to implement a function `preprocess_data` that processes this dataset to make it suitable for training a machine learning model. **Function Signature:** ```python def preprocess_data(X_train, X_test, num_features, cat_features): Preprocesses the dataset using a series of transformations including standardization, scaling, and encoding. Parameters: - X_train (pd.DataFrame): The training dataset with numerical and categorical features. - X_test (pd.DataFrame): The testing dataset with numerical and categorical features. - num_features (list of str): The names of the numerical features in the dataset. - cat_features (list of str): The names of the categorical features in the dataset. Returns: tuple: Transformed training and testing datasets. pass ``` **Inputs:** - `X_train`: A pandas DataFrame containing the training data with both numerical and categorical features. - `X_test`: A pandas DataFrame containing the testing data with both numerical and categorical features. - `num_features`: A list of strings representing the column names of numerical features. - `cat_features`: A list of strings representing the column names of categorical features. **Outputs:** - A tuple containing two pandas DataFrames `(X_train_transformed, X_test_transformed)`, where both DataFrames have been preprocessing appropriately. **Constraints:** - The function should use scikit-learn pipelines to ensure that preprocessing steps applied to the training data are consistently applied to the test data. - Numerical features must be scaled to have zero mean and unit variance. - Categorical features must be one-hot encoded. **Example:** ```python import pandas as pd # Sample data data = { \\"num1\\": [1.0, 2.0, 3.0], \\"num2\\": [4.0, 5.0, 6.0], \\"cat1\\": [\\"A\\", \\"B\\", \\"C\\"] } X_train = pd.DataFrame(data) X_test = pd.DataFrame({ \\"num1\\": [2.0, 3.0], \\"num2\\": [5.0, 6.0], \\"cat1\\": [\\"B\\", \\"C\\"] }) num_features = [\\"num1\\", \\"num2\\"] cat_features = [\\"cat1\\"] # Expected output X_train_transformed, X_test_transformed = preprocess_data(X_train, X_test, num_features, cat_features) print(X_train_transformed) print(X_test_transformed) ``` Your implementation should transform the input data `X_train` and `X_test` using a series of preprocessing steps: standardization for numerical features and one-hot encoding for categorical features. The function should ensure that these steps are applied consistently to both training and testing data using scikit-learn\'s Pipeline functionality. **Notes:** - Ensure to handle missing values or unknown categories appropriately if required. - You may assume that the input DataFrame does not contain any missing values for simplicity. Good luck!","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder def preprocess_data(X_train, X_test, num_features, cat_features): Preprocesses the dataset using a series of transformations including standardization, scaling, and encoding. Parameters: - X_train (pd.DataFrame): The training dataset with numerical and categorical features. - X_test (pd.DataFrame): The testing dataset with numerical and categorical features. - num_features (list of str): The names of the numerical features in the dataset. - cat_features (list of str): The names of the categorical features in the dataset. Returns: tuple: Transformed training and testing datasets. # Pipelines for numerical and categorical features num_pipeline = Pipeline([ (\'scaler\', StandardScaler()) ]) cat_pipeline = Pipeline([ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Full preprocessor pipeline preprocessor = ColumnTransformer([ (\'num\', num_pipeline, num_features), (\'cat\', cat_pipeline, cat_features) ]) # Fit the preprocessor on the training data and transform both train and test data X_train_transformed = preprocessor.fit_transform(X_train) X_test_transformed = preprocessor.transform(X_test) # Convert the transformed data back to pandas DataFrame num_cols = num_features cat_cols = preprocessor.named_transformers_[\'cat\'][\'onehot\'].get_feature_names_out(cat_features) columns = list(num_cols) + list(cat_cols) X_train_transformed = pd.DataFrame(X_train_transformed, columns=columns) X_test_transformed = pd.DataFrame(X_test_transformed, columns=columns) return X_train_transformed, X_test_transformed"},{"question":"Objective The goal of this exercise is to test your understanding of Python\'s `site` module and how it configures the module search paths and handles site-specific customizations. Task Write a Python function `customize_environment` that adds a specified directory to the Python module search path (`sys.path`) and ensures the directory is processed for any path configuration (.pth) files it contains. Additionally, implement a script to retrieve and print the current user base directory and the user-specific site-packages directory. Function Signature ```python def customize_environment(directory: str) -> None: Adds the specified directory to sys.path and processes its .pth files. Args: directory (str): The directory to be added to sys.path and processed for .pth files. def print_user_directories() -> None: Prints the user base directory and user-specific site-packages directory. ``` Input Format - `customize_environment` takes a single string argument `directory` which is the path to the directory you want to add to `sys.path`. Output Format - `customize_environment` does not return anything but modifies the `sys.path` and processes `.pth` files. - `print_user_directories` prints two lines to the standard output: 1. The user base directory 2. The user-specific site-packages directory Constraints - The directory paths should be valid and existing. - Assume the Python version is 3.10 or later. - The functions should handle exceptions gracefully and print meaningful error messages if the operations fail. Examples ```python # Example usage of customize_environment customize_environment(\'/path/to/custom/dir\') # After calling this function, \'/path/to/custom/dir\' should be added to sys.path # and its .pth files should be processed. # Example usage of print_user_directories print_user_directories() # This should print output similar to: # /home/user/.local # /home/user/.local/lib/python3.10/site-packages ``` Additional Information - You may use the functionalities provided by the `site` module such as `site.addsitedir`, `site.getuserbase`, `site.getusersitepackages` to implement the functions. # Hints - Use `site.addsitedir` to add the directory and process its .pth files. - Use `site.getuserbase` and `site.getusersitepackages` to retrieve the required user directory paths.","solution":"import site import sys def customize_environment(directory: str) -> None: Adds the specified directory to sys.path and processes its .pth files. Args: directory (str): The directory to be added to sys.path and processed for .pth files. try: site.addsitedir(directory) print(f\\"Successfully added {directory} to sys.path and processed .pth files.\\") except Exception as e: print(f\\"Failed to add {directory} to sys.path: {e}\\", file=sys.stderr) def print_user_directories() -> None: Prints the user base directory and user-specific site-packages directory. try: user_base = site.getuserbase() user_site_packages = site.getusersitepackages() print(user_base) print(user_site_packages) except Exception as e: print(f\\"Failed to retrieve user directories: {e}\\", file=sys.stderr)"},{"question":"# Objective Design and implement a function that converts an audio WAV file into a greyscale PNG image, where each pixel intensity corresponds to the amplitude of the audio signal at that particular time segment. # Input 1. A string `input_wav_file` which is a path to the input WAV file. 2. An integer `height` specifying the height of the output image in pixels. # Output 1. A string `output_png_file` which is a path to the output PNG image. # Constraints 1. The width of the image will be determined by the length of the audio signal. 2. Make sure to handle stereo WAV files by converting them to mono. 3. Assume `height` will be a positive integer. 4. The output image should have a width that evenly represents the audio data in manageable chunks. 5. The WAV file will not exceed 1 minute duration. # Performance - The function should efficiently handle files up to 60 seconds in length. - Ensure minimal memory usage by processing the audio in chunks. # Example Function Signature ```python def wav_to_image(input_wav_file: str, output_png_file: str, height: int) -> None: pass ``` # Example Usage ```python wav_to_image(\'input.wav\', \'output.png\', 100) ``` # Implementation Guide 1. Use the `wave` module to read the audio data from the input WAV file. 2. Process the audio data to convert it from stereo (if necessary) to mono. 3. Normalize the audio amplitude to fit within the pixel intensity range (0-255). 4. Create a new greyscale image using a library such as PIL (Python Imaging Library). 5. Map the normalized audio data to the pixel values of the image. 6. Save the generated image to the specified `output_png_file`.","solution":"import wave import numpy as np from PIL import Image def wav_to_image(input_wav_file: str, output_png_file: str, height: int) -> None: # Open the wav file with wave.open(input_wav_file, \'rb\') as wav: # Verify wav is mono or stereo n_channels = wav.getnchannels() sample_width = wav.getsampwidth() frame_rate = wav.getframerate() n_frames = wav.getnframes() # Read frames and convert to numpy array frames = wav.readframes(n_frames) audio_data = np.frombuffer(frames, dtype=np.int16) # If stereo, combine the channels by taking the mean if n_channels == 2: audio_data = audio_data.reshape(-1, 2) audio_data = audio_data.mean(axis=1) # Normalize the data to 0-255 audio_data = ((audio_data - np.min(audio_data)) / (np.max(audio_data) - np.min(audio_data)) * 255).astype(np.uint8) # Calculate the width of the image width = len(audio_data) // height # Create the image and set the pixels image = Image.new(\'L\', (width, height)) for y in range(height): for x in range(width): image.putpixel((x, y), int(audio_data[y * width + x])) # Save the image image.save(output_png_file)"},{"question":"# Python Coding Assessment Objective To test your understanding of Python generators and their manipulation. You will demonstrate the creation of a generator function, the handling of values it yields, and checking generator objects. Problem Statement 1. **Create a Generator Function:** Write a generator function named `custom_range` that mimics the behavior of the built-in `range` function, but: - Accepts `start`, `stop`, and `step` parameters. - Yields values from `start` to `stop` (exclusive), incrementing by `step`. 2. **Manipulate Yields:** Create another generator function named `square_values` that takes a generator as input and yields the square of each value produced by this generator. 3. **Generator Checker:** Write a function `is_generator` that takes an object and returns `True` if it is a generator object, otherwise, returns `False`. Input and Output Format: 1. `custom_range(start: int, stop: int, step: int) -> Generator[int, None, None]` - Input: `custom_range(1, 10, 2)` - Output: `1, 3, 5, 7, 9` (values generated one by one) 2. `square_values(generator: Generator[int, None, None]) -> Generator[int, None, None]` - Input: `square_values(custom_range(1, 5, 1))` - Output: `1, 4, 9, 16` (values generated one by one) 3. `is_generator(obj: Any) -> bool` - Input: `is_generator(custom_range(1, 10, 2))` - Output: `True` - Input: `is_generator([1, 2, 3])` - Output: `False` Constraints: 1. Assume `step` will always be non-zero. 2. The generator functions should handle large ranges efficiently without running out of memory. Implementation: ```python from typing import Generator, Any def custom_range(start: int, stop: int, step: int) -> Generator[int, None, None]: Generator function that mimics range behavior. while start < stop: yield start start += step def square_values(generator: Generator[int, None, None]) -> Generator[int, None, None]: Generator function that yields squares of values from another generator. for value in generator: yield value * value def is_generator(obj: Any) -> bool: Checks if the given object is a generator. return hasattr(obj, \\"__iter__\\") and hasattr(obj, \\"__next__\\") ``` # Explanation: 1. `custom_range` generates values from `start` to `stop` with the given `step`. 2. `square_values` takes a generator and yields the square of each value it generates. 3. `is_generator` checks if an object is a generator by checking for the presence of `__iter__` and `__next__` methods. Implement the above functions, ensuring they work correctly as described in the provided examples.","solution":"from typing import Generator, Any def custom_range(start: int, stop: int, step: int) -> Generator[int, None, None]: Generator function that mimics range behavior. while start < stop: yield start start += step def square_values(generator: Generator[int, None, None]) -> Generator[int, None, None]: Generator function that yields squares of values from another generator. for value in generator: yield value * value def is_generator(obj: Any) -> bool: Checks if the given object is a generator. return hasattr(obj, \\"__iter__\\") and hasattr(obj, \\"__next__\\")"},{"question":"Implement a program in Python that demonstrates the use of weak references. You need to create a class `TrackableObject` and manage a list of its weak references. The program needs to handle the addition and removal of these objects and show how weak references are used to check if an object has been garbage collected. Specifications 1. **Class `TrackableObject`**: - Has an `__init__` method that initializes an identifier for the object (an integer ID). - Has a `__str__` method to return a string representation in the format \\"TrackableObject(ID)\\". - Trackable objects should be weakly referencable. 2. **Function `manage_weak_references`**: - Takes a list of integers as input representing IDs of `TrackableObject`. - Creates `TrackableObject` instances for each ID and maintains a list of their weak references. - Randomly deletes some of the `TrackableObject` instances to simulate garbage collection. - Outputs the status of each weak reference: whether the referenced object is still alive or has been collected. Input Format - A list of integers representing IDs of `TrackableObject` instances. Output Format - For each weak reference, print the status in the format: ``` \\"Weak reference to <object> is alive.\\" ``` or ``` \\"Weak reference to <object> has been collected.\\" ``` Example ```python class TrackableObject: # Implement the required methods here def manage_weak_references(ids): # Implement the management of weak references here # Test case manage_weak_references([1, 2, 3, 4]) # Expected Output (Note: output may vary based on internal random deletions) Weak reference to TrackableObject(1) is alive. Weak reference to TrackableObject(2) has been collected. Weak reference to TrackableObject(3) is alive. Weak reference to TrackableObject(4) has been collected. ``` Constraints - You may use the `random` module to randomly select objects for deletion. - Assume the IDs provided in the input are unique and positive integers. > Note: The expected output can vary depending on which objects are deleted and when garbage collection occurs. The correctness will be determined by appropriate use of weak references and the handling of their statuses.","solution":"import weakref import random class TrackableObject: def __init__(self, id): self.id = id def __str__(self): return f\\"TrackableObject({self.id})\\" def manage_weak_references(ids): obj_list = [] weak_refs = [] # Create TrackableObject instances and store their weak references for id in ids: obj = TrackableObject(id) obj_list.append(obj) weak_refs.append(weakref.ref(obj)) # Randomly delete some objects for obj in random.sample(obj_list, k=len(obj_list) // 2): obj_list.remove(obj) del obj # Check and print the status of each weak reference for weak_ref in weak_refs: obj = weak_ref() if obj is not None: print(f\\"Weak reference to {obj} is alive.\\") else: print(\\"Weak reference has been collected.\\")"},{"question":"# Python Coding Assessment Question **Topic:** Unit Testing with \'unittest\' Framework Background: You are tasked with developing a small yet critical library for matrix manipulations. This library will be used in various scientific computations and, as such, must be robust and thoroughly tested. Additionally, your library will require detailed unit tests to verify its functionality under various scenarios. Task: 1. Implement the `Matrix` class with the following methods: - `__init__(self, data)`: Initializes the matrix with a list of lists. - `add(self, other)`: Adds another matrix to the current matrix and returns the result. - `mul(self, other)`: Multiplies the current matrix with another and returns the result. - `transpose(self)`: Returns the transpose of the current matrix. 2. Write a comprehensive suite of unit tests using the `unittest` framework to verify the functionality of the `Matrix` class. Requirements: 1. **Matrix Class:** - Each matrix is represented as a list of lists where each sublist is a row in the matrix. - Matrices must be validated on initialization to ensure all rows have the same number of columns. - Addition is only possible if both matrices have the same dimensions. - Multiplication must respect the rules of matrix multiplication. - Transpose should swap rows with columns. 2. **Unit Tests:** - Ensure that the `Matrix` class handles invalid input appropriately. - Verify matrix addition, multiplication, and transpose operations under normal and edge cases. - Utilize `setUp()`, `tearDown()`, and any other relevant `unittest` features. - Test for exception handling where applicable, using `assertRaises()`. Input and Output: - **Input:** Two matrices in the form of lists of lists for operations like `add` and `mul`. - **Output:** A new matrix or result of the operation in the form of a list of lists. Example: ```python m1 = Matrix([[1, 2], [3, 4]]) m2 = Matrix([[5, 6], [7, 8]]) # Addition result = m1.add(m2) # result should be [[6, 8], [10, 12]] # Multiplication result = m1.mul(m2) # result should be [[19, 22], [43, 50]] # Transpose result = m1.transpose() # result should be [[1, 3], [2, 4]] ``` Implementation: ```python import unittest class Matrix: def __init__(self, data): if not all(len(row) == len(data[0]) for row in data): raise ValueError(\\"All rows must have the same number of columns.\\") self.data = data def add(self, other): if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]): raise ValueError(\\"Matrices must have the same dimensions to add.\\") result = [] for i in range(len(self.data)): row = [] for j in range(len(self.data[0])): row.append(self.data[i][j] + other.data[i][j]) result.append(row) return Matrix(result) def mul(self, other): if len(self.data[0]) != len(other.data): raise ValueError(\\"Number of columns in the first matrix must be equal to the number of rows in the second matrix.\\") result = [] for i in range(len(self.data)): row = [] for j in range(len(other.data[0])): cell = 0 for k in range(len(self.data[0])): cell += self.data[i][k] * other.data[k][j] row.append(cell) result.append(row) return Matrix(result) def transpose(self): result = [] for i in range(len(self.data[0])): row = [] for j in range(len(self.data)): row.append(self.data[j][i]) result.append(row) return Matrix(result) class TestMatrix(unittest.TestCase): def setUp(self): self.m1 = Matrix([[1, 2], [3, 4]]) self.m2 = Matrix([[5, 6], [7, 8]]) self.m3 = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) def test_addition(self): result = self.m1.add(self.m2) self.assertEqual(result.data, [[6, 8], [10, 12]]) with self.assertRaises(ValueError): self.m1.add(self.m3) def test_multiplication(self): result = self.m1.mul(self.m2) self.assertEqual(result.data, [[19, 22], [43, 50]]) with self.assertRaises(ValueError): self.m1.mul(self.m3) def test_transpose(self): result = self.m3.transpose() self.assertEqual(result.data, [[1, 4, 7], [2, 5, 8], [3, 6, 9]]) def test_invalid_matrix(self): with self.assertRaises(ValueError): Matrix([[1, 2], [3, 4, 5]]) if __name__ == \'__main__\': unittest.main() ``` Constraints: - You must use the Python `unittest` module to write your tests. - Ensure that all methods and features tested for have comprehensive coverage, including edge cases and exceptions. - The code should be written using Python 3.10 or later.","solution":"class Matrix: def __init__(self, data): if not all(len(row) == len(data[0]) for row in data): raise ValueError(\\"All rows must have the same number of columns.\\") self.data = data def add(self, other): if len(self.data) != len(other.data) or len(self.data[0]) != len(other.data[0]): raise ValueError(\\"Matrices must have the same dimensions to add.\\") result = [] for i in range(len(self.data)): row = [] for j in range(len(self.data[0])): row.append(self.data[i][j] + other.data[i][j]) result.append(row) return Matrix(result) def mul(self, other): if len(self.data[0]) != len(other.data): raise ValueError(\\"Number of columns in the first matrix must be equal to the number of rows in the second matrix.\\") result = [] for i in range(len(self.data)): row = [] for j in range(len(other.data[0])): cell = 0 for k in range(len(self.data[0])): cell += self.data[i][k] * other.data[k][j] row.append(cell) result.append(row) return Matrix(result) def transpose(self): result = [] for i in range(len(self.data[0])): row = [] for j in range(len(self.data)): row.append(self.data[j][i]) result.append(row) return Matrix(result)"},{"question":"# Question: Implement an HTTP Status Code Checker **Objective:** You are required to implement a function that takes a URL and checks its HTTP status code using the `http.client` module. Your task is to list down various attributes of the HTTP status code using the `http.HTTPStatus` class. **Function Signature:** ```python def check_http_status(url: str) -> dict: pass ``` **Input:** - `url` (str): A string representing the URL to check. **Output:** - A dictionary containing the following information about the HTTP status code: - `status_code` (int): The numeric HTTP status code. - `reason_phrase` (str): The reason phrase associated with the status code. - `description` (str): The long description of the status code. **Constraints:** - The function should handle all standard HTTP status codes (e.g., 200, 404, 500, etc.). - Use the `http.client` module to make the HTTP request. - You may assume the URL provided is well-formed and valid. **Example:** For a URL that returns an HTTP status of 200 (OK): ```python url = \\"http://example.com\\" result = check_http_status(url) print(result) ``` Expected Output: ```python { \\"status_code\\": 200, \\"reason_phrase\\": \\"OK\\", \\"description\\": \\"Request fulfilled, document follows\\" } ``` **Notes:** - If the URL is not accessible or an error occurs during the request, the function should handle it gracefully and return `None`. # Implementation: You may start by importing necessary modules, making a basic HTTP GET request using `http.client`, and then utilizing the `http.HTTPStatus` to extract the required information.","solution":"import http.client from urllib.parse import urlparse from http import HTTPStatus def check_http_status(url: str) -> dict: try: # Parse the URL to extract its components parsed_url = urlparse(url) connection = http.client.HTTPConnection(parsed_url.netloc) # Perform a GET request connection.request(\\"GET\\", parsed_url.path or \\"/\\") response = connection.getresponse() # Get status code and reason status_code = response.status reason_phrase = response.reason # Get status description from HTTPStatus status_info = HTTPStatus(status_code) description = status_info.description # Close connection connection.close() # Construct the result dictionary result = { \\"status_code\\": status_code, \\"reason_phrase\\": reason_phrase, \\"description\\": description } return result except Exception as e: # If there\'s an exception, log it and return None print(f\\"Error: {e}\\") return None"},{"question":"# Task You are provided with a dataset and are required to create a point plot using seaborn that visualizes specific insights from the data. Implement functions as per the requirements below. # Dataset The dataset contains information about various species of penguins and can be loaded with the command: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` # Requirements Function 1: Plot Body Mass Write a function `plot_body_mass_by_island` that takes no inputs and produces a point plot showing the body mass of penguins grouped by the island. The plot must include: - Confidence intervals for each group. - Different colors for different species. ```python def plot_body_mass_by_island(): Produces a point plot showing the body mass of penguins grouped by the island, with confidence intervals and different colors for different species. pass ``` Function 2: Customize Plot Write a function `customize_plot_by_sex` that takes no inputs and produces a point plot showing the bill depth of penguins grouped by the species and differentiating by sex. The plot must: - Use error bars to represent the standard deviation. - Customize the appearance by setting a color for data points and using markers and linestyles. ```python def customize_plot_by_sex(): Produces a point plot showing the bill depth of penguins grouped by the species, differentiating by sex, with error bars representing standard deviation. Customizes the appearance by setting colors, markers, and linestyles. pass ``` # Constraints 1. Use the Seaborn library for plotting. 2. Ensure the plots are clear and aesthetically pleasing. 3. Add any necessary plot customization to improve readability. # Expected Output - Function `plot_body_mass_by_island` should display a point plot where the x-axis represents different islands and the y-axis displays the body mass of penguins. Different species should be differentiated using colors. - Function `customize_plot_by_sex` should display a point plot where the x-axis represents different species and the y-axis displays the bill depth. Different sexes should be differentiated using markers and linestyles, with error bars representing the standard deviation. # Sample Output ```python plot_body_mass_by_island() # This should show the plot with body mass grouped by island and differentiated by species. customize_plot_by_sex() # This should show the plot with bill depth grouped by species, differentiated by sex, and error bars representing standard deviation. ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_body_mass_by_island(): Produces a point plot showing the body mass of penguins grouped by the island, with confidence intervals and different colors for different species. penguins = sns.load_dataset(\\"penguins\\") sns.pointplot(x=\\"island\\", y=\\"body_mass_g\\", hue=\\"species\\", data=penguins, ci=\\"sd\\", palette=\\"deep\\") plt.title(\'Penguin Body Mass by Island\') plt.show() def customize_plot_by_sex(): Produces a point plot showing the bill depth of penguins grouped by the species, differentiating by sex, with error bars representing standard deviation. Customizes the appearance by setting colors, markers, and linestyles. penguins = sns.load_dataset(\\"penguins\\") sns.pointplot(x=\\"species\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", data=penguins, ci=\\"sd\\", palette=\\"colorblind\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\'Penguin Bill Depth by Species and Sex\') plt.show()"},{"question":"Seaborn Error Bars Assessment # Objective The goal of this task is to assess your understanding of creating and interpreting error bars in seaborn. You need to implement functions to generate different types of error bar plots and provide a brief explanation for each plot generated. # Instructions 1. **Data Preparation**: - Generate a sample of 100 normally distributed data points with a mean of 0 and a standard deviation of 1. 2. **Plot Error Bars**: - Implement a function `plot_error_bars` that takes an `errorbar_type` parameter (string) and a `scale_or_width` parameter (numeric). The function should create a seaborn point plot with the specified type of error bars: - `\'sd\'`: Standard deviation - `\'se\'`: Standard error - `\'pi\'`: Percentile interval - `\'ci\'`: Confidence interval 3. **Customize Error Bars**: - Allow the `scale_or_width` parameter to adjust the size of the error bars for each type: - For `\'sd\'` and `\'se\'`, it should be a scalar factor. - For `\'pi\'` and `\'ci\'`, it should specify the width of the interval. 4. **Plot Explanation**: - For each type of error bar (`\'sd\'`, `\'se\'`, `\'pi\'`, `\'ci\'`), generate the plot and provide a brief explanation of what the error bars represent. # Function Signature ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt def plot_error_bars(errorbar_type: str, scale_or_width: float = 1.0): Plots a seaborn point plot with the specified type of error bars. Args: errorbar_type (str): The type of error bars to plot (\'sd\', \'se\', \'pi\', \'ci\'). scale_or_width (float): The scale factor for \'sd\' and \'se\', or the width for \'pi\' and \'ci\'. Returns: None # Generate sample data np.random.seed(123) data = np.random.normal(loc=0, scale=1, size=100) # Create the plot sns.pointplot(x=data, errorbar=(errorbar_type, scale_or_width)) plt.title(f\'Error Bars Type: {errorbar_type} (Scale/Width: {scale_or_width})\') plt.show() # Example Calls plot_error_bars(\'sd\', 2) plot_error_bars(\'se\', 1.5) plot_error_bars(\'pi\', 50) plot_error_bars(\'ci\', 0.95) ``` # Explanation Notes - **Standard Deviation (`\'sd\'`)**: The plot shows error bars at Â±1 standard deviation by default, adjustable by the `scale_or_width` factor. Represents data spread around the mean. - **Standard Error (`\'se\'`)**: Shows error bars at Â±1 standard error by default, with the width adjustable by the `scale_or_width` factor. Represents uncertainty in the sample estimate. - **Percentile Interval (`\'pi\'`)**: Displays a specified percentile interval ((0-100)). By default, it uses a 95% interval, adjustable by the `scale_or_width` parameter. - **Confidence Interval (`\'ci\'`)**: Uses bootstrapping to estimate the range of likely values for the parameter. The width of the confidence interval is adjustable using the `scale_or_width` parameter. # Constraints - Ensure that the `plot_error_bars` function handles invalid input gracefully. - Use appropriate seaborn and matplotlib functions to create and customize the plots based on different error bar types. Good luck!","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_error_bars(errorbar_type: str, scale_or_width: float = 1.0): Plots a seaborn point plot with the specified type of error bars. Args: errorbar_type (str): The type of error bars to plot (\'sd\', \'se\', \'pi\', \'ci\'). scale_or_width (float): The scale factor for \'sd\' and \'se\', or the width for \'pi\' and \'ci\'. Returns: None if errorbar_type not in [\'sd\', \'se\', \'pi\', \'ci\']: raise ValueError(\\"errorbar_type must be one of \'sd\', \'se\', \'pi\', or \'ci\'\\") # Generate sample data np.random.seed(123) data = np.random.normal(loc=0, scale=1, size=100) df = pd.DataFrame({\'value\': data}) # Create the plot plt.figure(figsize=(10, 4)) sns.pointplot(x=df.index, y=\'value\', data=df, errorbar=(errorbar_type, scale_or_width)) plt.title(f\'Error Bars Type: {errorbar_type} (Scale/Width: {scale_or_width})\') plt.xlabel(\'Sample Index\') plt.ylabel(\'Value\') plt.show()"},{"question":"# Distribution Plot Exercise **Objective:** Create a comprehensive function to visualize the distribution of numeric variables in the `penguins` dataset using various features of seaborn\'s `displot`. **Problem Statement:** You need to implement a function called `custom_displot` which visualizes the distribution of the `bill_length_mm` variable from the `penguins` dataset. Your function must support the following parameters: - `kind`: The type of the plot (one of `hist`, `kde`, `ecdf`). - `bivariate`: A boolean indicating whether to draw a bivariate plot using `bill_length_mm` and `flipper_length_mm`. - `hue`: Specify `None` or `species` to create subsets. - `facets`: A dictionary with optional `row` and `col` keys to facet the data by `sex` and `species`. **Input:** - `kind` (str): The type of plot (`hist`, `kde`, or `ecdf`). - `bivariate` (bool): Whether to draw a bivariate plot. - `hue` (str or None): A column name to map subtype (e.g., \'species\') or None. - `facets` (dict): A dictionary with optional `row` and `col` keys for faceting (default is an empty dictionary). **Output:** - A seaborn `FacetGrid` object with the created plot. **Constraints:** - If `bivariate` is True, the `x` and `y` axes of the plot should be `bill_length_mm` and `flipper_length_mm` respectively. - If `hue` is provided, it must be \'species\' or None. - The `facets` dictionary can contain only `\'row\'` and `\'col\'` as keys with values `None`, \'sex\', or \'species\'. - Your function should handle invalid input gracefully. **Function Signature:** ```python def custom_displot(kind: str, bivariate: bool, hue: str = None, facets: dict = {}) -> sns.FacetGrid: pass ``` # Example Usage ```python import seaborn as sns # Example 1: Univariate histogram plot with species hue and faceted by sex grid = custom_displot(kind=\\"hist\\", bivariate=False, hue=\\"species\\", facets={\\"col\\": \\"sex\\"}) grid.fig.suptitle(\\"Histogram of Bill Length by Species and Sex\\") # Example 2: Bivariate KDE plot with rug and facets by species grid = custom_displot(kind=\\"kde\\", bivariate=True, hue=None, facets={\\"row\\": \\"species\\"}) grid.fig.suptitle(\\"Bivariate KDE of Bill and Flipper Length by Species\\") ``` This function will test students\' understanding of the seaborn package, specifically the `displot` function and the ability to customize and handle various configurations.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_displot(kind: str, bivariate: bool, hue: str = None, facets: dict = {}) -> sns.FacetGrid: Visualizes the distribution of the bill_length_mm variable from the penguins dataset. :param kind: The type of plot (one of \'hist\', \'kde\', \'ecdf\'). :param bivariate: Whether to draw a bivariate plot using bill_length_mm and flipper_length_mm. :param hue: Specify None or \'species\' to create subsets. :param facets: A dictionary with optional \'row\' and \'col\' keys to facet the data by \'sex\' and \'species\'. :return: A seaborn FacetGrid object with the created plot. if kind not in [\'hist\', \'kde\', \'ecdf\']: raise ValueError(\\"Invalid value for \'kind\'. Must be one of \'hist\', \'kde\', \'ecdf\'.\\") if hue is not None and hue != \'species\': raise ValueError(\\"Invalid value for \'hue\'. Must be \'species\' or None.\\") valid_facet_keys = {\'row\', \'col\'} for key in facets.keys(): if key not in valid_facet_keys: raise ValueError(f\\"Invalid facet key \'{key}\'. Only \'row\' and \'col\' are allowed.\\") if \'row\' in facets and facets[\'row\'] not in [None, \'sex\', \'species\']: raise ValueError(\\"Invalid facet value for \'row\'. Must be None, \'sex\' or \'species\'.\\") if \'col\' in facets and facets[\'col\'] not in [None, \'sex\', \'species\']: raise ValueError(\\"Invalid facet value for \'col\'. Must be None, \'sex\' or \'species\'.\\") penguins = sns.load_dataset(\'penguins\') if bivariate: x, y = \'bill_length_mm\', \'flipper_length_mm\' else: x, y = \'bill_length_mm\', None grid = sns.displot( data=penguins, x=x, y=y, kind=kind, hue=hue, row=facets.get(\'row\', None), col=facets.get(\'col\', None) ) return grid"},{"question":"Coding Assessment Question # Objective Write a custom Python interpreter that supports basic mathematical operations and retains the history of executed commands. Your task is to implement a custom interpreter using the `code` module. The interpreter should support basic arithmetic operations (addition, subtraction, multiplication, and division) and maintain a history of all commands executed during the session. # Instructions 1. Implement a class `CustomInterpreter` that inherits from `code.InteractiveInterpreter`. 2. Add a method `get_command_history()` that returns the list of all commands executed. 3. Ensure the interpreter can handle basic arithmetic operations (`+`, `-`, `*`, `/`). 4. If the input is not a valid arithmetic operation, raise a `ValueError` with the message \\"Invalid command\\". 5. Write a `run` method that starts the interpreter session and allows the user to enter multiple lines of input until the user types \'exit\'. # Constraints - The class should properly handle incomplete commands using the `codeop` module. - The history should store commands as strings in the order they were executed. - The interpreter should handle integer and floating-point arithmetic. # Class and Method Specifications ```python import code class CustomInterpreter(code.InteractiveInterpreter): def __init__(self): super().__init__() self.command_history = [] def run(self): Starts the interpreter session. The session continues until the user types \'exit\'. pass def runcode(self, code): Executes a single command. Overrides the parent class method to add command history tracking. pass def get_command_history(self): Returns the command history as a list of strings. pass ``` # Example Usage ```python interpreter = CustomInterpreter() interpreter.run() # Example session: # >>> 2 + 2 # 4 # >>> 3 * 4 # 12 # >>> 10 / 2 # 5.0 # >>> exit print(interpreter.get_command_history()) # Output: [\'2 + 2\', \'3 * 4\', \'10 / 2\'] ``` # Notes - You may need to use `codeop` for compiling potentially incomplete commands. - Ensure that the `run` method handles user input in a loop and can detect when to end the session. - The `runcode` method should track each command by storing it in the `command_history` list.","solution":"import code import codeop class CustomInterpreter(code.InteractiveInterpreter): def __init__(self): super().__init__() self.command_history = [] def run(self): Starts the interpreter session. The session continues until the user types \'exit\'. compiler = codeop.Compile() while True: try: command = input(\'>>> \') if command.strip() == \'exit\': break if not compiler(command): continue self.executed_code = command self.runcode(command) except EOFError: break except Exception as e: print(f\\"Error: {e}\\") def runcode(self, code): Executes a single command. Overrides the parent class method to add command history tracking. self.command_history.append(code) try: exec(code, self.locals) except Exception as e: raise ValueError(\\"Invalid command\\") def get_command_history(self): Returns the command history as a list of strings. return self.command_history"},{"question":"**Objective:** Implement a data preprocessing and modeling pipeline using scikit-learn. Your solution should exhibit knowledge of combining estimators, using column-specific transformations, and integrating a caching mechanism. **Task:** Your task is to build a scikit-learn pipeline that performs the following steps: 1. **Data Preprocessing:** - Scale numerical features using `StandardScaler`. - Apply one-hot encoding to categorical features using `OneHotEncoder`. 2. **Feature Union:** - Combine the processed numerical and categorical features into a single feature space. 3. **Model Training:** - Use a `RandomForestClassifier` as the final estimator. 4. **Caching:** - Implement caching for the transformation steps to avoid repeated computation. 5. **Training and Evaluation:** - Train the pipeline using the provided dataset. - Evaluate the trained pipeline on a test set and report the accuracy. **Input Format:** - A training dataset `train_X.csv` with mixed-type features. - A corresponding label set `train_y.csv`. - A test dataset `test_X.csv` for evaluation. **Output Format:** - Print the accuracy of the pipeline on the test set. **Constraints:** - Use scikit-learn for all operations. - Assume the numerical columns are labeled \\"num_1\\", \\"num_2\\", ..., \\"num_n\\". - Assume the categorical columns are labeled \\"cat_1\\", \\"cat_2\\", ..., \\"cat_m\\". **Performance Requirements:** - Ensure that the operations, especially fitting transformers, are not recomputed unnecessarily by utilizing caching. **Implementation Details:** ```python import pandas as pd from sklearn.pipeline import Pipeline, FeatureUnion from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.externals.joblib import Memory import tempfile import shutil # Load datasets train_X = pd.read_csv(\'train_X.csv\') train_y = pd.read_csv(\'train_y.csv\') test_X = pd.read_csv(\'test_X.csv\') # Create a temporary directory for caching cachedir = tempfile.mkdtemp() # Define numerical and categorical columns numerical_cols = [col for col in train_X.columns if \'num\' in col] categorical_cols = [col for col in train_X.columns if \'cat\' in col] # Define the ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numerical_cols), (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), categorical_cols) ]) # Define the pipeline pipeline = Pipeline( steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier()) ], memory=Memory(location=cachedir, verbose=0) ) # Split the training data into training and validation sets X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42) # Fit the pipeline on training data pipeline.fit(X_train, y_train) # Evaluate the pipeline on the validation data val_predictions = pipeline.predict(X_val) accuracy = accuracy_score(y_val, val_predictions) print(f\'Validation Accuracy: {accuracy:.4f}\') # Cleanup the temporary directory shutil.rmtree(cachedir) ``` **Files Provided:** - `train_X.csv`: Training data features. - `train_y.csv`: Training data labels. - `test_X.csv`: Test data features. **Note:** - Ensure to handle any necessary imports, and implement the above steps in a manner conducive to reproducibility.","solution":"import pandas as pd from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from joblib import Memory import tempfile import shutil def preprocess_and_train_pipeline(train_X_path, train_y_path, test_X_path): # Load datasets train_X = pd.read_csv(train_X_path) train_y = pd.read_csv(train_y_path) test_X = pd.read_csv(test_X_path) # Create a temporary directory for caching cachedir = tempfile.mkdtemp() # Define numerical and categorical columns numerical_cols = [col for col in train_X.columns if \'num\' in col] categorical_cols = [col for col in train_X.columns if \'cat\' in col] # Define the ColumnTransformer preprocessor = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), numerical_cols), (\'cat\', OneHotEncoder(handle_unknown=\'ignore\'), categorical_cols) ]) # Define the pipeline pipeline = Pipeline( steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier()) ], memory=Memory(location=cachedir, verbose=0) ) # Split the training data into training and validation sets X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2, random_state=42) # Fit the pipeline on training data pipeline.fit(X_train, y_train) # Evaluate the pipeline on the validation data val_predictions = pipeline.predict(X_val) accuracy = accuracy_score(y_val, val_predictions) print(f\'Validation Accuracy: {accuracy:.4f}\') # Predict on the test data test_predictions = pipeline.predict(test_X) # Cleanup the temporary directory shutil.rmtree(cachedir) # Return the predictions and accuracy return test_predictions, accuracy"},{"question":"# Question: Background: You are developing a task scheduling system using the `asyncio` module in Python 3.10. You need to perform multiple asynchronous computations where results depend on each other in a complex manner. To achieve this, you need to effectively utilize `asyncio.Future` objects for handling asynchronous results. Task: Implement a function `schedule_tasks` that takes a list of (delay, value) tuples as input. Each tuple represents a task with a delay in seconds and a resultant value. Your function should create a Future object for each task, simulate a delay, and set the result of each Future object. Once all tasks are complete, return a list of results from all Future objects. Requirements: 1. Create an asyncio Future object for each input task. 2. Use an asynchronous task to set the result of the Future object after the specified delay. 3. Collect and return the results of all Future objects once they are all done. Function Signature: ```python import asyncio from typing import List, Tuple async def schedule_tasks(tasks: List[Tuple[int, int]]) -> List[int]: pass ``` Example: ```python import asyncio async def main(): tasks = [(1, 10), (2, 20), (3, 30)] # (delay in seconds, result value) results = await schedule_tasks(tasks) print(results) # Output should be: [10, 20, 30] asyncio.run(main()) ``` Constraints: - Task delays are given as positive integers. - The list of tasks will contain at least one tuple and at most 100 tuples. - You are permitted to use only the asyncio module for this task.","solution":"import asyncio from typing import List, Tuple async def _complete_future(future: asyncio.Future, delay: int, value: int): Helper function to simulate a delay and set the value for a Future object. await asyncio.sleep(delay) future.set_result(value) async def schedule_tasks(tasks: List[Tuple[int, int]]) -> List[int]: futures = [] for delay, value in tasks: future = asyncio.Future() futures.append(future) asyncio.create_task(_complete_future(future, delay, value)) results = await asyncio.gather(*futures) return results"},{"question":"# Question: Enum Utilization and Customization in Python **Objective**: Demonstrate understanding and application of Python\'s `enum` module by creating and manipulating enums with custom behaviors. **Problem Statement**: You are tasked with creating a system of enumerations representing different categories of items in an inventory system. The inventory system will track categories of items as constants, including their quantity, and will also allow combining these categories using bitwise operations. Additionally, some categories must enforce unique values. 1. Create an `ItemCategory` enum using the `Flag` class to track item categories in the inventory by combining multiple categories and ensure combinations are recognized correctly. 2. Implement an `Inventory` class that: - Keeps track of item counts for each category using a dictionary. - Implements methods to add items to categories and get the count of items in a specific category. - Ensures unique categories using the `@unique` decorator. **Requirements**: 1. **ItemCategory Enum**: - Use the `Flag` class to define categories. - Define at least four categories using both individual flags and their combinations. 2. **Inventory Class Methods**: - `add_item(category: ItemCategory, count: int) -> None`: Adds a given count of items to the specified category. - `get_count(category: ItemCategory) -> int`: Returns the count of items in the specified category, including those in combined categories. 3. **Constraints**: - Categories combined using bitwise operations should be handled correctly. - Raise appropriate exceptions if invalid operations are performed (e.g., adding a negative count of items). 4. **Performance**: - Ensure the methods operate efficiently, even with multiple categories and large item counts. **Example Usage**: ```python from enum import Flag, auto, unique # Step 1: Define the ItemCategory Enum class @unique class ItemCategory(Flag): BOOKS = auto() ELECTRONICS = auto() CLOTHING = auto() TOYS = auto() ALL = BOOKS | ELECTRONICS | CLOTHING | TOYS # Step 2: Define the Inventory Class class Inventory: def __init__(self): self.item_counts = {category: 0 for category in ItemCategory} def add_item(self, category: ItemCategory, count: int) -> None: if count < 0: raise ValueError(\\"Count cannot be negative\\") for cat in ItemCategory: if category & cat: self.item_counts[cat] += count def get_count(self, category: ItemCategory) -> int: return self.item_counts[category] # Step 3: Testing the Inventory system inventory = Inventory() # Add items to ELECTRONICS category inventory.add_item(ItemCategory.ELECTRONICS, 10) # Add items to BOOKS and TOYS combination inventory.add_item(ItemCategory.BOOKS | ItemCategory.TOYS, 5) # Get item counts print(inventory.get_count(ItemCategory.ELECTRONICS)) # Output: 10 print(inventory.get_count(ItemCategory.TOYS)) # Output: 5 print(inventory.get_count(ItemCategory.ALL)) # Output: 15 ``` Implement the `ItemCategory` enum and the `Inventory` class as specified, ensuring all combinations and flags are functional and the inventory behaves as expected.","solution":"from enum import Flag, auto, unique @unique class ItemCategory(Flag): BOOKS = auto() ELECTRONICS = auto() CLOTHING = auto() TOYS = auto() ALL = BOOKS | ELECTRONICS | CLOTHING | TOYS class Inventory: def __init__(self): self.item_counts = {cat: 0 for cat in ItemCategory if cat != ItemCategory.ALL} def add_item(self, category: ItemCategory, count: int) -> None: if count < 0: raise ValueError(\\"Count cannot be negative\\") for cat in ItemCategory: if cat != ItemCategory.ALL and (category & cat) == cat: self.item_counts[cat] += count def get_count(self, category: ItemCategory) -> int: if category == ItemCategory.ALL: return sum(self.item_counts.values()) return self.item_counts[category]"},{"question":"**Objective**: This question will assess your ability to implement, tune, and interpret decision trees using the scikit-learn library. Description Given the Iris dataset, your task is to perform the following steps using scikit-learn: 1. **Load the Iris dataset**: Load the Iris dataset using sklearn\'s `load_iris` function. 2. **Split the dataset**: Split the dataset into a training set (80%) and a test set (20%). 3. **Train a Decision Tree Classifier**: Train a `DecisionTreeClassifier` on the training set. Use `max_depth=3`, `min_samples_split=4`, and `random_state=42` for reproducibility. 4. **Evaluate the model**: Evaluate the classifier on the test set using accuracy as the metric. Print the accuracy. 5. **Visualize the Decision Tree**: Use the `plot_tree` function to visualize the trained decision tree. Save the visualization as a PNG file. 6. **Export the Decision Tree**: Export the decision tree in the textual format using the `export_text` function and print the output. 7. **Tune the Decision Tree**: Using GridSearchCV, tune the `max_depth` parameter for values `[2, 3, 4, 5]` and `min_samples_leaf` for values `[1, 2, 3, 4]`. Print the best parameters found. 8. **Re-Evaluate with Best Parameters**: Using the best parameters from GridSearchCV, re-train and evaluate the classifier on the test set, and print the new accuracy. Input and Output Formats **Input**: The input should be hardcoded as specified: - Use the Iris dataset available through `sklearn.datasets.load_iris`. **Output**: Your code should print the following at each step: 1. The accuracy of the initial decision tree model. 2. The decision tree visualization saved as `tree_visualization.png`. 3. The textual representation of the tree. 4. The best hyperparameters found using GridSearchCV. 5. The accuracy of the decision tree model with the best hyperparameters. Constraints - Use the scikit-learn library for all implementations. - Ensure reproducibility by setting `random_state=42` wherever applicable. ```python # Import necessary libraries from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text import matplotlib.pyplot as plt # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the Decision Tree Classifier clf = DecisionTreeClassifier(max_depth=3, min_samples_split=4, random_state=42) clf.fit(X_train, y_train) # Evaluate the model accuracy = clf.score(X_test, y_test) print(\\"Initial model accuracy: {:.2f}%\\".format(accuracy * 100)) # Visualize the Decision Tree plt.figure(figsize=(12,12)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'tree_visualization.png\') plt.show() # Export the Decision Tree tree_text = export_text(clf, feature_names=iris[\'feature_names\']) print(tree_text) # Tune the Decision Tree param_grid = {\'max_depth\': [2, 3, 4, 5], \'min_samples_leaf\': [1, 2, 3, 4]} grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5) grid_search.fit(X_train, y_train) # Best hyperparameters best_params = grid_search.best_params_ print(\\"Best parameters found: \\", best_params) # Re-Evaluate with Best Parameters best_clf = grid_search.best_estimator_ best_accuracy = best_clf.score(X_test, y_test) print(\\"Best model accuracy: {:.2f}%\\".format(best_accuracy * 100)) ``` Implement the above steps in a Jupyter notebook or a Python script and submit both the code and the output.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text import matplotlib.pyplot as plt # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the Decision Tree Classifier clf = DecisionTreeClassifier(max_depth=3, min_samples_split=4, random_state=42) clf.fit(X_train, y_train) # Evaluate the model initial_accuracy = clf.score(X_test, y_test) print(\\"Initial model accuracy: {:.2f}%\\".format(initial_accuracy * 100)) # Visualize the Decision Tree plt.figure(figsize=(12,12)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'tree_visualization.png\') plt.show() # Export the Decision Tree tree_text = export_text(clf, feature_names=iris[\'feature_names\']) print(tree_text) # Tune the Decision Tree param_grid = {\'max_depth\': [2, 3, 4, 5], \'min_samples_leaf\': [1, 2, 3, 4]} grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5) grid_search.fit(X_train, y_train) # Best hyperparameters best_params = grid_search.best_params_ print(\\"Best parameters found: \\", best_params) # Re-Evaluate with Best Parameters best_clf = grid_search.best_estimator_ best_accuracy = best_clf.score(X_test, y_test) print(\\"Best model accuracy: {:.2f}%\\".format(best_accuracy * 100))"},{"question":"You are required to implement a simple deep learning model using PyTorch and then train this model using a distributed optimizer. Specifically, your task involves: 1. Creating a neural network model. 2. Splitting the model to be trained in a distributed manner across multiple nodes. 3. Using a published optimizer provided in `torch.distributed.optim` for training the model. 4. Ensuring that your implementation falls back to CPU tensors if CUDA is not supported for the distributed optimizer. # Requirements 1. **Model Architecture**: - Implement a simple feedforward neural network for classifying a small dataset. The model should contain: - An input layer - One or two hidden layers with activation functions - An output layer suited for classification tasks 2. **Data**: Use any small dataset of your choice (e.g., MNIST, CIFAR-10). 3. **Distributed Training Setup**: - Split the model parameters and data across multiple devices. - Use `torch.distributed.optim.DistributedOptimizer` or any other optimizer from `torch.distributed.optim`. - Ensure the optimizer works correctly regardless of CUDA support. # Input - You don\'t need to implement code for input/output; focus on model, data preparation, and training loop. # Output - Ensure your code output the training progress, including loss at each epoch. # Example Below is a minimal skeleton for your implementation: ```python import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, Dataset # Define your model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(784, 128) self.activation = nn.ReLU() self.layer2 = nn.Linear(128, 10) def forward(self, x): x = self.layer1(x) x = self.activation(x) x = self.layer2(x) return x # Initialize the distributed environment def init_process(rank, size, fn, backend=\'gloo\'): dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) # Define your training process def train(rank, size): torch.manual_seed(1234) model = SimpleNN() device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) # Load data, define loss and optimizer # Remember to use DistributedOptimizer if possible # Training loop for epoch in range(10): for batch in DataLoader(custom_dataset): # Forward pass, backward pass, and optimize pass # Print epoch loss print(f\\"Rank {rank}, Epoch {epoch}, Loss: {loss.item()}\\") # This would be where you launch the processes, for instance using `torch.multiprocessing` if __name__ == \\"__main__\\": size = 4 # Number of processes processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, train)) p.start() processes.append(p) for p in processes: p.join() ``` Use this skeleton to build upon and implement the necessary details. # Constraints - You must handle the CUDA limitation and provide an alternative implementation or fallback to CPU. - Focus on performance where applicable. Good luck!","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.multiprocessing import Process from torchvision import datasets, transforms from torch.utils.data import DataLoader, Subset # Define your model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer1 = nn.Linear(784, 128) self.activation = nn.ReLU() self.layer2 = nn.Linear(128, 10) def forward(self, x): x = x.view(-1, 784) # Flatten input x = self.layer1(x) x = self.activation(x) x = self.layer2(x) return x # Initialize the distributed environment def init_process(rank, size, fn, backend=\'gloo\'): Initialize the distributed process group and run the training function. dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) # Define the training process def train(rank, size): torch.manual_seed(1234) # Prepare dataset transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) dataset = datasets.MNIST(root=\'./data\', train=True, download=True, transform=transform) subset = Subset(dataset, range(5000)) # Use a subset for faster training sampler = torch.utils.data.distributed.DistributedSampler(subset, num_replicas=size, rank=rank) dataloader = DataLoader(subset, batch_size=32, sampler=sampler) # Build model and move it to the appropriate device model = SimpleNN().to(rank) if torch.cuda.is_available(): device = torch.device(f\\"cuda:{rank}\\") else: device = torch.device(\\"cpu\\") model.to(device) # Define loss function and optimizer criterion = nn.CrossEntropyLoss() optimizer = dist.optim.DistributedOptimizer(optim.SGD, model.parameters(), lr=0.01) model.train() for epoch in range(10): epoch_loss = 0.0 for batch_idx, (data, target) in enumerate(dataloader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() epoch_loss += loss.item() print(f\\"Rank {rank}, Epoch {epoch}, Loss: {epoch_loss / len(dataloader)}\\") # Main script if __name__ == \\"__main__\\": size = 2 # Number of processes processes = [] for rank in range(size): p = Process(target=init_process, args=(rank, size, train)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"# CPython Version Encoding and Decoding You are provided with the version encoding system used by CPython, where a version is represented by its major, minor, and micro versions along with its release level and release serial number. The version information is encoded in a single hexadecimal integer (`PY_VERSION_HEX`). You are required to implement two functions: 1. **encode_version** - This function takes five arguments: `major`, `minor`, `micro`, `release_level`, and `release_serial`, and returns the encoded hexadecimal version integer. 2. **decode_version** - This function takes one argument: `hex_version`, which is the encoded hexadecimal version integer, and returns a tuple containing the `major`, `minor`, `micro`, `release_level`, and `release_serial` numbers. The release_level should be one of the following: - \'a\' for alpha (`0xA`) - \'b\' for beta (`0xB`) - \'c\' for release candidate (`0xC`) - \'f\' for final (`0xF`) Function Signatures: ```python def encode_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int: # Your code here def decode_version(hex_version: int) -> tuple: # Your code here ``` Inputs and Constraints: - The `major`, `minor`, and `micro` arguments are non-negative integers. - `release_level` is a character and should be one of \'a\', \'b\', \'c\', or \'f\'. - `release_serial` is a non-negative integer. - `hex_version` is a non-negative integer. Outputs: - `encode_version` should return an integer which is the encoded version. - `decode_version` should return a tuple `(major, minor, micro, release_level, release_serial)`. Example: ```python # Encoding example hex_version = encode_version(3, 10, 0, \'f\', 0) print(hex_version) # Output: 0x030a00f0 # Decoding example version_info = decode_version(0x030401a2) print(version_info) # Output: (3, 4, 1, \'a\', 2) ``` **Notes:** - For encoding, use the given bit positions for each part of the version. - For decoding, you need to parse the given `hex_version` to extract each part of the version.","solution":"def encode_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int: Encodes the version information into a single hexadecimal integer. release_level_dict = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF} release_level_code = release_level_dict.get(release_level) if release_level_code is None: raise ValueError(\\"Invalid release level. Must be one of \'a\', \'b\', \'c\', or \'f\'.\\") version = (major << 24) | (minor << 16) | (micro << 8) | (release_level_code << 4) | release_serial return version def decode_version(hex_version: int) -> tuple: Decodes the hexadecimal version integer back into version information. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release_level_code = (hex_version >> 4) & 0xF release_serial = hex_version & 0xF release_level_dict = {0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\'} release_level = release_level_dict.get(release_level_code) if release_level is None: raise ValueError(\\"Invalid release level code in hex version.\\") return (major, minor, micro, release_level, release_serial)"},{"question":"**Python Version Encoding and Decoding** In Python310, the version number of the Python interpreter is represented using various macros and is encoded into a single 32-bit integer called `PY_VERSION_HEX`. Here is the detailed structure: 1. PY_MAJOR_VERSION: Represents the major version (e.g., `3` in `3.4.1a2`). 2. PY_MINOR_VERSION: Represents the minor version (e.g., `4` in `3.4.1a2`). 3. PY_MICRO_VERSION: Represents the micro version (e.g., `1` in `3.4.1a2`). 4. PY_RELEASE_LEVEL: Represents the release level (`a` for alpha, `b` for beta, `c` for release candidate, `f` for final). 5. PY_RELEASE_SERIAL: Represents the release serial number (non-zero for pre-releases). The composite version number `PY_VERSION_HEX` is obtained by combining these components into a single 32-bit integer as documented. Your task is to implement a Python function to encode and decode these version details. # Task 1. **Encode a Version** Implement a function to encode a Python version from its components into a 32-bit integer `PY_VERSION_HEX`. ```python def encode_version(major, minor, micro, release_level, release_serial): Encode the given Python version details into a 32-bit integer. Parameters: major (int): Major version minor (int): Minor version micro (int): Micro version release_level (str): Release level (one of \'a\', \'b\', \'c\', \'f\') release_serial (int): Release serial number Returns: int: Encoded 32-bit integer representing the version # Your code here ``` 2. **Decode a Version** Implement a function to decode a 32-bit integer `PY_VERSION_HEX` back into its components. ```python def decode_version(hexversion): Decode a 32-bit integer into Python version details. Parameters: hexversion (int): Encoded 32-bit integer representing the version Returns: tuple: A tuple containing (major, minor, micro, release_level, release_serial) # Your code here ``` # Constraints - `0 <= major <= 255` - `0 <= minor <= 255` - `0 <= micro <= 255` - `release_level` should be one of `\'a\'`, `\'b\'`, `\'c\'`, `\'f\'` - `0 <= release_serial <= 15` # Example ```python # Encoding encoded_version = encode_version(3, 10, 0, \'f\', 0) print(hex(encoded_version)) # Output: \'0x030a00f0\' # Decoding version_details = decode_version(0x030401a2) print(version_details) # Output: (3, 4, 1, \'a\', 2) ``` Your implementation should correctly encode a Python version into a 32-bit integer, and decode such integers back into version components.","solution":"def encode_version(major, minor, micro, release_level, release_serial): Encode the given Python version details into a 32-bit integer. Parameters: major (int): Major version minor (int): Minor version micro (int): Micro version release_level (str): Release level (one of \'a\', \'b\', \'c\', \'f\') release_serial (int): Release serial number Returns: int: Encoded 32-bit integer representing the version release_level_map = { \'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF } if release_level not in release_level_map: raise ValueError(\\"Invalid release level\\") hexversion = ( (major << 24) | (minor << 16) | (micro << 8) | (release_level_map[release_level] << 4) | (release_serial) ) return hexversion def decode_version(hexversion): Decode a 32-bit integer into Python version details. Parameters: hexversion (int): Encoded 32-bit integer representing the version Returns: tuple: A tuple containing (major, minor, micro, release_level, release_serial) release_level_map = { 0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\' } major = (hexversion >> 24) & 0xFF minor = (hexversion >> 16) & 0xFF micro = (hexversion >> 8) & 0xFF release_level_num = (hexversion >> 4) & 0xF release_serial = hexversion & 0xF if release_level_num not in release_level_map: raise ValueError(\\"Invalid release level in hexversion\\") release_level = release_level_map[release_level_num] return (major, minor, micro, release_level, release_serial)"},{"question":"# Question: Connecting to an NNTP Server and Retrieving Article Summaries You are required to implement a function `fetch_article_summaries` that connects to a specified NNTP server, enters a given newsgroup, and retrieves the subjects of the last `n` articles in that group. Function Signature ```python def fetch_article_summaries(server: str, newsgroup: str, n: int) -> List[str]: pass ``` Parameters - `server` (str): The address of the NNTP server to connect to. - `newsgroup` (str): The name of the newsgroup to fetch articles from. - `n` (int): The number of most recent article subjects to retrieve. Returns - `List[str]`: A list of subjects of the last `n` articles in the specified newsgroup. If there are fewer than `n` articles available, return the subjects of all available articles. Constraints - You should handle any potential exceptions that may arise from connecting to the server or fetching data, and return an appropriate error message. - You need to ensure proper cleanup and closure of the connection to avoid resource leaks. Example ```python fetch_article_summaries(\'news.gmane.io\', \'gmane.comp.python.committers\', 5) ``` This function call should return the subjects of the last 5 articles in the `gmane.comp.python.committers` newsgroup. Notes - Make use of the `nntplib` module to interact with the NNTP server. - Handle edge cases where the number of articles in the newsgroup is less than `n`. - Ensure the connection to the NNTP server is properly closed in all scenarios, including error cases.","solution":"import nntplib from typing import List def fetch_article_summaries(server: str, newsgroup: str, n: int) -> List[str]: Connects to the specified NNTP server, enters the given newsgroup, and retrieves the subjects of the last \'n\' articles in that group. Args: server (str): The address of the NNTP server to connect to. newsgroup (str): The name of the newsgroup to fetch articles from. n (int): The number of most recent article subjects to retrieve. Returns: List[str]: A list of subjects of the last \'n\' articles. subjects = [] try: server = nntplib.NNTP(server) resp, count, first, last, name = server.group(newsgroup) start = max(int(last) - n + 1, int(first)) resp, overviews = server.over((str(start) + \'-\' + str(last))) for number, over in overviews: subjects.append(over[\'subject\']) server.quit() except Exception as e: return [str(e)] return subjects"},{"question":"You are required to implement a function that handles multiple file descriptors using the `select` module and monitors them until any of the file descriptors are ready for reading, writing, or encounter an exceptional condition. # Task Write a function `monitor_file_descriptors` that: - Takes in three lists: `rlist`, `wlist`, and `xlist`, which contain file descriptors to be monitored for reading, writing, and exceptional conditions, respectively. - Takes an optional `timeout` parameter that specifies the maximum time, in seconds, that the function should wait for any of the file descriptors to become active. If `timeout` is `None`, the function should wait indefinitely. The function should return a tuple of three lists: - The list of file descriptors that are ready for reading. - The list of file descriptors that are ready for writing. - The list of file descriptors that have encountered an exceptional condition. # Input Format - `rlist`: A list of file descriptors to be monitored for reading. - `wlist`: A list of file descriptors to be monitored for writing. - `xlist`: A list of file descriptors to be monitored for exceptional conditions. - `timeout`: An optional float specifying the maximum time to wait for an event to occur on any of the file descriptors. # Output Format - A tuple with three lists: (ready_for_reading, ready_for_writing, exceptional_conditions). # Constraints - Each file descriptor in `rlist`, `wlist`, and `xlist` can be either an integer or an object with a `fileno()` method that returns an integer. - The function must handle edge cases such as empty lists. # Example Usage ```python import socket # Create a test socket pair sock1, sock2 = socket.socketpair() # Monitor the socket for readability rlist, wlist, xlist = [sock1], [], [] ready_for_reading, ready_for_writing, exceptional_conditions = monitor_file_descriptors(rlist, wlist, xlist, timeout=5.0) print(ready_for_reading) # Should list sock1 if there\'s data to read print(ready_for_writing) # Should be empty as no fd is monitored for writing print(exceptional_conditions) # Should be empty as no exceptional condition ``` # Function Signature ```python def monitor_file_descriptors(rlist, wlist, xlist, timeout=None): pass ```","solution":"import select def monitor_file_descriptors(rlist, wlist, xlist, timeout=None): Monitors multiple file descriptors for reading, writing, or exceptional conditions. Parameters: rlist (list): List of file descriptors to be monitored for reading. wlist (list): List of file descriptors to be monitored for writing. xlist (list): List of file descriptors to be monitored for exceptional conditions. timeout (float, optional): The maximum time to wait for fd to become active. Defaults to None. Returns: tuple: Three lists containing file descriptors ready for reading, writing, and exceptional conditions. readable, writable, exceptional = select.select(rlist, wlist, xlist, timeout) return readable, writable, exceptional"},{"question":"# Question: Task Scheduler Implementation using the `sched` Module You have been tasked to create a custom event scheduler using Python\'s built-in `sched` module. This scheduler will: 1. Schedule multiple events to run at different times and with different priorities. 2. Handle both relative and absolute event scheduling. 3. Allow cancellation of specific events. 4. Provide a summary of the scheduled events. Requirements and Implementation Details: **Function 1: `schedule_task(scheduler, delay, priority, action, *args, **kwargs)`** Schedule a new event using relative timing. - `scheduler` (sched.scheduler): The event scheduler instance. - `delay` (int): The delay in seconds from the current time after which the event should be executed. - `priority` (int): The priority of the event. Lower value means higher priority. - `action` (callable): The function to be executed. - `*args`: Positional arguments to pass to the function. - `**kwargs`: Keyword arguments to pass to the function. - **Returns:** The scheduled event to facilitate cancellations later. **Function 2: `schedule_task_absolute(scheduler, abs_time, priority, action, *args, **kwargs)`** Schedule a new event using absolute timing based on a specific time (unix timestamp). - `scheduler` (sched.scheduler): The event scheduler instance. - `abs_time` (float): The absolute time (in seconds since the Epoch) at which the event should be executed. - `priority` (int): The priority of the event. Lower value means higher priority. - `action` (callable): The function to be executed. - `*args`: Positional arguments to pass to the function. - `**kwargs`: Keyword arguments to pass to the function. - **Returns:** The scheduled event to facilitate cancellations later. **Function 3: `cancel_task(scheduler, event)`** Cancel a scheduled event. - `scheduler` (sched.scheduler): The event scheduler instance. - `event`: The event to cancel. - **Returns:** None **Function 4: `run_scheduler(scheduler, blocking=True)`** Run all scheduled events. - `scheduler` (sched.scheduler): The event scheduler instance. - `blocking` (bool): Whether to block until all events have been processed. - **Returns:** None **Function 5: `get_scheduler_summary(scheduler)`** Get a summary of all upcoming events. - `scheduler` (sched.scheduler): The event scheduler instance. - **Returns:** A list of dictionaries, each representing an event. Each dictionary should have keys: time, priority, action, argument, kwargs Example Usage: ```python import sched import time def my_task(message): print(f\\"Task executed with message: {message}\\") # Create the scheduler instance s = sched.scheduler(time.time, time.sleep) # Schedule tasks event1 = schedule_task(s, 5, 2, my_task, \\"Hello after 5 seconds\\") event2 = schedule_task_absolute(s, time.time() + 10, 1, my_task, \\"Hello at an absolute time 10 seconds from now\\") # Get a summary of scheduled tasks print(get_scheduler_summary(s)) # Run the scheduler run_scheduler(s) ``` **Constraints:** - You may assume that the functions being scheduled (`action`) will not raise exceptions. - The priority and timing of all events must be managed efficiently. **Task:** Implement the required functions to meet the requirements specified.","solution":"import sched import time def schedule_task(scheduler, delay, priority, action, *args, **kwargs): Schedule a new event using relative timing. event = scheduler.enter(delay, priority, action, argument=args, kwargs=kwargs) return event def schedule_task_absolute(scheduler, abs_time, priority, action, *args, **kwargs): Schedule a new event using absolute timing based on a specific time (unix timestamp). delay = abs_time - time.time() event = scheduler.enter(delay, priority, action, argument=args, kwargs=kwargs) return event def cancel_task(scheduler, event): Cancel a scheduled event. scheduler.cancel(event) def run_scheduler(scheduler, blocking=True): Run all scheduled events. scheduler.run(blocking=blocking) def get_scheduler_summary(scheduler): Get a summary of all upcoming events. events = [] for event in scheduler.queue: events.append({ \'time\': event.time, \'priority\': event.priority, \'action\': event.action, \'argument\': event.argument, \'kwargs\': event.kwargs }) return events"},{"question":"**Title**: Logfile Formatter and Report Generator **Objective**: Implement a function to read a log file, extract relevant information, perform calculations, and generate a formatted report as a JSON output. **Description**: You are given a log file containing various event records. Each record is on a new line and contains details in the following format: ``` <TIMESTAMP> - <EVENT_TYPE>: <EVENT_DETAILS> ``` Example: ``` 2023-01-01 12:00:00 - LOGIN: User ID 12345 2023-01-01 12:05:00 - LOGOUT: User ID 12345 2023-01-01 13:00:00 - LOGIN: User ID 67890 2023-01-01 14:00:00 - PURCHASE: User ID 12345 - Amount 99.99 ``` Your task is to implement a function `generate_report(logfile_path, output_path)` that reads such a log file, extracts relevant information for each event type (LOGIN, LOGOUT, PURCHASE), performs necessary calculations, and generates a summary report in JSON format. The generated report should include the following information: 1. Total number of LOGIN events. 2. Total number of LOGOUT events. 3. Total number of PURCHASE events and the total amount spent. 4. A list of unique user IDs that logged in. **Function Signature**: ```python def generate_report(logfile_path: str, output_path: str) -> None: ``` **Input**: - `logfile_path` (str): Path to the input log file. - `output_path` (str): Path where the output JSON report should be saved. **Output**: - The function should write a JSON formatted report to `output_path`. **Constraints**: - The log file can be very large; handle file reading efficiently. - Each line in the log file will be correctly formatted as described. - Use appropriate data structures for efficient data manipulation. **Example**: Input log file (saved at `input.log`): ``` 2023-01-01 12:00:00 - LOGIN: User ID 12345 2023-01-01 12:05:00 - LOGOUT: User ID 12345 2023-01-01 13:00:00 - LOGIN: User ID 67890 2023-01-01 14:00:00 - PURCHASE: User ID 12345 - Amount 99.99 ``` Function Call: ```python generate_report(\'input.log\', \'report.json\') ``` Expected Output file (saved at `report.json`): ```json { \\"total_logins\\": 2, \\"total_logouts\\": 1, \\"total_purchases\\": 1, \\"total_amount_spent\\": 99.99, \\"unique_user_ids\\": [\\"12345\\", \\"67890\\"] } ``` **Notes**: - Ensure the JSON output is formatted for readability (indentation of 4 spaces). - Handle file opening/closing properly to avoid resource leaks. **Hints**: - Use the `with` statement for file operations. - Utilize `json` module for JSON serialization. - For string parsing and formatting, consider using `str.split()`, `str.format()`, `f-strings`, and other string methods as discussed in the documentation.","solution":"import json def generate_report(logfile_path: str, output_path: str) -> None: total_logins = 0 total_logouts = 0 total_purchases = 0 total_amount_spent = 0.0 unique_user_ids = set() with open(logfile_path, \'r\') as file: for line in file: if \'LOGIN\' in line: total_logins += 1 user_id = line.split(\'User ID \')[1].strip() unique_user_ids.add(user_id) elif \'LOGOUT\' in line: total_logouts += 1 elif \'PURCHASE\' in line: total_purchases += 1 amount = float(line.split(\'Amount \')[1].strip()) total_amount_spent += amount report = { \\"total_logins\\": total_logins, \\"total_logouts\\": total_logouts, \\"total_purchases\\": total_purchases, \\"total_amount_spent\\": total_amount_spent, \\"unique_user_ids\\": list(unique_user_ids) } with open(output_path, \'w\') as outfile: json.dump(report, outfile, indent=4)"},{"question":"You are given a time-series dataset containing daily temperature readings over several years. The dataset includes two columns: `date` and `temperature`. Your task is to write a function that performs various resampling operations on this dataset. # Function Signature ```python def analyze_temperature_data(data: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - `data`: A pandas DataFrame with two columns: - `date`: A datetime column representing the date of the observation. - `temperature`: A float column representing the temperature reading on that date. # Output - A pandas DataFrame with the following columns: - `month`: The month of the year. - `mean_temperature`: The mean temperature for the month. - `temperature_std`: The standard deviation of the temperature for the month. - `max_temperature`: The maximum temperature recorded in the month. - `min_temperature`: The minimum temperature recorded in the month. - `num_days`: The number of days with temperature recordings in the month. # Constraints - You must use the pandas `resample` function and relevant methods to process the data. - The function should handle datasets spanning multiple years correctly. - Ensure that your function is efficient and handles large datasets with potentially millions of rows. # Example ```python import pandas as pd data = pd.DataFrame({ \'date\': pd.date_range(start=\'2020-01-01\', end=\'2022-12-31\', freq=\'D\'), \'temperature\': np.random.uniform(low=-10, high=40, size=(1096,)) }) result = analyze_temperature_data(data) print(result) ``` # Notes - You might need to convert the `date` column to a datetime format if it is not already. - Ensure that your output DataFrame is sorted by month. Good luck!","solution":"import pandas as pd def analyze_temperature_data(data: pd.DataFrame) -> pd.DataFrame: Analyzes temperature data by resampling it to monthly frequency and computing various statistics for each month. Parameters: data (pd.DataFrame): DataFrame with two columns - \'date\' and \'temperature\' Returns: pd.DataFrame: DataFrame with columns [\'month\', \'mean_temperature\', \'temperature_std\', \'max_temperature\', \'min_temperature\', \'num_days\'] # Ensure that the \'date\' column is in datetime format data[\'date\'] = pd.to_datetime(data[\'date\']) # Set the \'date\' column as the index data.set_index(\'date\', inplace=True) # Define the resampling function resampled = data.resample(\'M\').agg({ \'temperature\': [\'mean\', \'std\', \'max\', \'min\', \'count\'] }) # Flatten the MultiIndex columns resampled.columns = [\'mean_temperature\', \'temperature_std\', \'max_temperature\', \'min_temperature\', \'num_days\'] # Reset the index to have \'date\' as a column again resampled.reset_index(inplace=True) # Extract the month from the \'date\' column resampled[\'month\'] = resampled[\'date\'].dt.month resampled.drop(columns=\'date\', inplace=True) return resampled[[\'month\', \'mean_temperature\', \'temperature_std\', \'max_temperature\', \'min_temperature\', \'num_days\']]"},{"question":"**Task**: Implement a class `Rational` that represents a rational number. Your class should demonstrate the following Python concepts: - Initialization with numerator and denominator. - Implementing special methods: `__add__`, `__sub__`, `__mul__`, `__truediv__`, `__eq__`, `__ne__`, `__lt__`, `__le__`, `__gt__`, `__ge__`. - Implementing `__repr__` and `__str__` for clear object display. - Handling division by zero in a customized manner. # Specifications: 1. **Initialization**: The class should initialize with the given numerator and denominator, ensuring the denominator is not zero. ```python def __init__(self, numerator: int, denominator: int): pass ``` 2. **Addition**: ```python def __add__(self, other: \'Rational\') -> \'Rational\': pass ``` 3. **Subtraction**: ```python def __sub__(self, other: \'Rational\') -> \'Rational\': pass ``` 4. **Multiplication**: ```python def __mul__(self, other: \'Rational\') -> \'Rational\': pass ``` 5. **True Division**: ```python def __truediv__(self, other: \'Rational\') -> \'Rational\': pass ``` 6. **Equality Comparison**: ```python def __eq__(self, other: \'Rational\') -> bool: pass ``` 7. **Not Equal Comparison**: ```python def __ne__(self, other: \'Rational\') -> bool: pass ``` 8. **Less Than Comparison**: ```python def __lt__(self, other: \'Rational\') -> bool: pass ``` 9. **Less Than or Equal Comparison**: ```python def __le__(self, other: \'Rational\') -> bool: pass ``` 10. **Greater Than Comparison**: ```python def __gt__(self, other: \'Rational\') -> bool: pass ``` 11. **Greater Than or Equal Comparison**: ```python def __ge__(self, other: \'Rational\') -> bool: pass ``` 12. **String Representations**: - `__repr__` should return a string that can recreate the object. - `__str__` should return a readable string format. ```python def __repr__(self) -> str: pass def __str__(self) -> str: pass ``` 13. **Handling Division by Zero**: If the denominator is zero during initialization or a zero denominator appears during computation, your class should raise a `ValueError` with a message `\\"Denominator cannot be zero\\"`. # Example: ```python r1 = Rational(1, 2) r2 = Rational(3, 4) print(r1 + r2) # Rational(5, 4) print(r1 - r2) # Rational(-1, 4) print(r1 * r2) # Rational(3, 8) print(r1 / r2) # Rational(2, 3) print(r1 == r2) # False print(r1 != r2) # True print(r1 < r2) # True print(r1 <= r2) # True print(r1 > r2) # False print(r1 >= r2) # False print(repr(r1)) # Rational(1, 2) print(str(r1)) # 1/2 ``` # Constraints: 1. The numerator and denominator should be integers. 2. The operations should simplify the rational number (e.g., `Rational(2, 4)` should simplify to `Rational(1, 2)`). # Submission: Submit your `Rational` class implementation with all the required methods.","solution":"from math import gcd class Rational: def __init__(self, numerator: int, denominator: int): if denominator == 0: raise ValueError(\\"Denominator cannot be zero\\") common_divisor = gcd(numerator, denominator) self.numerator = numerator // common_divisor self.denominator = denominator // common_divisor def __add__(self, other: \'Rational\') -> \'Rational\': new_numerator = self.numerator * other.denominator + self.denominator * other.numerator new_denominator = self.denominator * other.denominator return Rational(new_numerator, new_denominator) def __sub__(self, other: \'Rational\') -> \'Rational\': new_numerator = self.numerator * other.denominator - self.denominator * other.numerator new_denominator = self.denominator * other.denominator return Rational(new_numerator, new_denominator) def __mul__(self, other: \'Rational\') -> \'Rational\': new_numerator = self.numerator * other.numerator new_denominator = self.denominator * other.denominator return Rational(new_numerator, new_denominator) def __truediv__(self, other: \'Rational\') -> \'Rational\': if other.numerator == 0: raise ValueError(\\"Cannot divide by zero\\") new_numerator = self.numerator * other.denominator new_denominator = self.denominator * other.numerator return Rational(new_numerator, new_denominator) def __eq__(self, other: \'Rational\') -> bool: return self.numerator == other.numerator and self.denominator == other.denominator def __ne__(self, other: \'Rational\') -> bool: return not self == other def __lt__(self, other: \'Rational\') -> bool: return self.numerator * other.denominator < self.denominator * other.numerator def __le__(self, other: \'Rational\') -> bool: return self.numerator * other.denominator <= self.denominator * other.numerator def __gt__(self, other: \'Rational\') -> bool: return self.numerator * other.denominator > self.denominator * other.numerator def __ge__(self, other: \'Rational\') -> bool: return self.numerator * other.denominator >= self.denominator * other.numerator def __repr__(self) -> str: return f\\"Rational({self.numerator}, {self.denominator})\\" def __str__(self) -> str: return f\\"{self.numerator}/{self.denominator}\\""},{"question":"Coding Assessment Question **Objective:** Write a Python program using the asyncio module that performs the following tasks: 1. Sets up an event loop that works correctly on both Windows and macOS platforms. 2. Creates a simple server that listens for incoming connections on a specified port. 3. Handles multiple clients concurrently, echoing received messages back to the clients. 4. Ensures compatibility with platform-specific limitations. For example, you should not use Unix domain sockets on Windows. **Requirements:** - The server should support handling up to 10 concurrent client connections. - Each connection should echo the received data back to the client. - Use appropriate exception handling to manage potential issues with client connections. - Document any differences in event loop handling for Windows and macOS in your code comments. **Input:** - A port number on which the server will listen for connections. **Output:** - Real-time log messages indicating the server status, client connections, and data exchange. **Example Usage:** ```python python echo_server.py 8888 ``` **Constraints:** - Your solution must handle at least Python 3.8 or higher. - You cannot use `loop.create_unix_connection()` or `loop.create_unix_server()` methods on Windows. - Properly handle platform-specific event loop settings and constraints as discussed in the provided documentation. **Performance Requirements:** - Ensure the server can handle up to 10 clients with minimal latency. Here is a scaffold to get you started: ```python import asyncio import sys async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"Received connection from {addr}\\") while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() print(f\\"Closing connection from {addr}\\") writer.close() await writer.wait_closed() async def main(port): server = await asyncio.start_server(handle_client, \'127.0.0.1\', port) addr = server.sockets[0].getsockname() print(f\\"Serving on {addr}\\") async with server: await server.serve_forever() if __name__ == \'__main__\': if len(sys.argv) != 2: print(\\"Usage: python echo_server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) if sys.platform.startswith(\'win\'): asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) asyncio.run(main(port)) ``` **Note:** Review the documentation to understand the platform-specific details better and incorporate suitable adjustments where necessary.","solution":"import asyncio import sys import logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(message)s\') async def handle_client(reader, writer): addr = writer.get_extra_info(\'peername\') logging.info(f\\"Received connection from {addr}\\") try: while True: data = await reader.read(100) if not data: break message = data.decode() logging.info(f\\"Received {message} from {addr}\\") writer.write(data) await writer.drain() except asyncio.CancelledError: logging.info(f\\"Connection closed abruptly from {addr}\\") except Exception as e: logging.error(f\\"Error: {e}\\") finally: logging.info(f\\"Closing connection from {addr}\\") writer.close() await writer.wait_closed() async def main(port): server = await asyncio.start_server(handle_client, \'127.0.0.1\', port) addr = server.sockets[0].getsockname() logging.info(f\\"Serving on {addr}\\") async with server: await server.serve_forever() if __name__ == \'__main__\': if len(sys.argv) != 2: print(\\"Usage: python echo_server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) if sys.platform.startswith(\'win\'): asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy()) asyncio.run(main(port))"},{"question":"# Covariance Estimation with scikit-learn You are provided with a dataset representing various features of observations. Your task is to implement three types of covariance estimations using the scikit-learn library: Empirical Covariance, Shrunk Covariance, and the Ledoit-Wolf Shrinkage estimator. Then, compare the results to understand their differences. Instructions 1. **Load the Dataset**: - Load the dataset (assumed to be in a numpy array format). 2. **Implement Empirical Covariance Estimation**: - Use the `EmpiricalCovariance` class to compute the empirical covariance matrix. - Ensure that data is correctly centered if necessary (toggle the `assume_centered` parameter). 3. **Implement Shrunk Covariance Estimation**: - Use the `ShrunkCovariance` class to compute the shrunk covariance matrix. - Apply an appropriate shrinkage coefficient. 4. **Implement Ledoit-Wolf Shrinkage Estimation**: - Use the `LedoitWolf` class to compute the covariance matrix using Ledoit-Wolf shrinkage. 5. **Comparison**: - Compare the covariance matrices obtained from the above three methods. - Comment on the differences in the eigenvalues of these covariance matrices. Data Format The input dataset will be a `numpy` array of shape `(n_samples, n_features)`. Output Format Print the covariance matrices and explain the differences between them, especially focusing on the stability and spread of the eigenvalues. Constraints - Ensure to handle data centering properly. - Set an appropriate shrinkage coefficient for the Shrunk Covariance Estimation. - Properly document your procedures and any assumptions made. # Example ```python import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf # Sample data data = np.array([ [2.4, 2.5, 3.6], [1.4, 2.3, 3.1], [3.4, 2.0, 5.6], [1.2, 4.5, 1.1] ]) # Empirical Covariance emp_cov = EmpiricalCovariance(assume_centered=False) emp_cov.fit(data) print(\\"Empirical Covariance Matrix:n\\", emp_cov.covariance_) # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1, assume_centered=False) shrunk_cov.fit(data) print(\\"Shrunk Covariance Matrix:n\\", shrunk_cov.covariance_) # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf() lw_cov.fit(data) print(\\"Ledoit-Wolf Covariance Matrix:n\\", lw_cov.covariance_) # Eigenvalues comparison emp_eigenvalues = np.linalg.eigvals(emp_cov.covariance_) shrunk_eigenvalues = np.linalg.eigvals(shrunk_cov.covariance_) lw_eigenvalues = np.linalg.eigvals(lw_cov.covariance_) print(\\"Empirical Covariance Eigenvalues:n\\", emp_eigenvalues) print(\\"Shrunk Covariance Eigenvalues:n\\", shrunk_eigenvalues) print(\\"Ledoit-Wolf Eigenvalues:n\\", lw_eigenvalues) # Explanation of results print(\\"The eigenvalues give us insights into the spread and stability of the data under each estimator. ...\\") ```","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf def compute_covariances(data): Compute three types of covariance matrices: Empirical, Shrunk, and Ledoit-Wolf. Args: data (numpy ndarray): The input data of shape (n_samples, n_features). Returns: tuple: A tuple containing empirical, shrunk, and Ledoit-Wolf covariance matrices. # Empirical Covariance emp_cov = EmpiricalCovariance(assume_centered=False) emp_cov.fit(data) empirical_cov = emp_cov.covariance_ # Shrunk Covariance shrunk_cov = ShrunkCovariance(shrinkage=0.1, assume_centered=False) shrunk_cov.fit(data) shrunk_covariance = shrunk_cov.covariance_ # Ledoit-Wolf Shrinkage lw_cov = LedoitWolf() lw_cov.fit(data) lw_covariance = lw_cov.covariance_ return empirical_cov, shrunk_covariance, lw_covariance def compute_eigenvalues(covariance_matrix): Compute the eigenvalues of a given covariance matrix. Args: covariance_matrix (numpy ndarray): The covariance matrix. Returns: numpy ndarray: The eigenvalues of the covariance matrix. return np.linalg.eigvals(covariance_matrix) # Example data data = np.array([ [2.4, 2.5, 3.6], [1.4, 2.3, 3.1], [3.4, 2.0, 5.6], [1.2, 4.5, 1.1] ]) # Compute covariance matrices empirical_cov, shrunk_covariance, lw_covariance = compute_covariances(data) # Output results print(\\"Empirical Covariance Matrix:n\\", empirical_cov) print(\\"Shrunk Covariance Matrix:n\\", shrunk_covariance) print(\\"Ledoit-Wolf Covariance Matrix:n\\", lw_covariance) # Eigenvalues comparison emp_eigenvalues = compute_eigenvalues(empirical_cov) shrunk_eigenvalues = compute_eigenvalues(shrunk_covariance) lw_eigenvalues = compute_eigenvalues(lw_covariance) print(\\"Empirical Covariance Eigenvalues:n\\", emp_eigenvalues) print(\\"Shrunk Covariance Eigenvalues:n\\", shrunk_eigenvalues) print(\\"Ledoit-Wolf Covariance Eigenvalues:n\\", lw_eigenvalues)"},{"question":"You are provided with a dataset that contains customer information for a retail store. The dataset has the following columns: - `customer_id`: Unique ID for each customer (string) - `gender`: Gender of the customer (categorical: \'Male\', \'Female\') - `age`: Age of the customer (numerical) - `annual_income`: Annual income of the customer in thousands (numerical) - `purchase_amount`: Total purchase amount in the last year (numerical) Your task is to build a machine learning pipeline that predicts the `purchase_amount` using the scikit-learn package. Specifically, you need to implement a function `build_and_evaluate_pipeline` that performs the following: 1. **Preprocess the data**: - Encode the `gender` column using one-hot encoding. - Scale the numerical columns (`age`, `annual_income`) using standard scaling. 2. **Model training**: - Use a linear regression model to predict the `purchase_amount`. - Incorporate the preprocessing steps into a single pipeline that also includes the linear regression model. 3. **Evaluate the model**: - Split the dataset into training and testing sets (80% training, 20% testing). - Train the pipeline on the training set and evaluate it on the testing set using the R^2 score. 4. **Output the pipeline and the evaluation score**. The function signature should be: ```python def build_and_evaluate_pipeline(data: pd.DataFrame) -> tuple: Build and evaluate a pipeline for predicting purchase amount. Parameters: data (pd.DataFrame): The input dataframe with columns \'customer_id\', \'gender\', \'age\', \'annual_income\', \'purchase_amount\'. Returns: tuple: (pipeline, r2_score) where pipeline is the trained scikit-learn pipeline object and r2_score is the R^2 score on the test set. pass ``` Assume you have the following libraries imported: ```python import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score ``` **Constraints**: - The dataset `data` is guaranteed to have the specified columns. - You may not use any additional libraries besides those mentioned. **Performance Requirements**: - The pipeline should be efficient and fit within reasonable time limits for a typical dataset size.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score def build_and_evaluate_pipeline(data: pd.DataFrame) -> tuple: # Define features and target X = data[[\'gender\', \'age\', \'annual_income\']] y = data[\'purchase_amount\'] # Define the preprocessing for numerical features numeric_features = [\'age\', \'annual_income\'] numeric_transformer = StandardScaler() # Define the preprocessing for categorical features categorical_features = [\'gender\'] categorical_transformer = OneHotEncoder(drop=\'first\') # drop=\'first\' to avoid multicollinearity # Combine preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numeric_features), (\'cat\', categorical_transformer, categorical_features)]) # Define the pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Split the data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the pipeline pipeline.fit(X_train, y_train) # Make predictions on the test set y_pred = pipeline.predict(X_test) # Evaluate the model score = r2_score(y_test, y_pred) return pipeline, score"},{"question":"Your task is to analyze a dataset and create visualizations using seaborn to uncover patterns and insights from the data. Specifically, you are to work with a hypothetical dataset that will be provided to you in both long-form and wide-form. You will need to: 1. Convert the wide-form dataset into long-form. 2. Create a specific plot using seaborn based on the long-form dataset. 3. Identify trends and patterns from the visualization. Dataset Here is the provided dataset in wide-form: ```python import pandas as pd data = { \\"year\\": [2015, 2016, 2017, 2018, 2019], \\"Jan\\": [100, 110, 120, 130, 140], \\"Feb\\": [200, 210, 220, 230, 240], \\"Mar\\": [300, 310, 320, 330, 340], } df_wide = pd.DataFrame(data) ``` # Your Tasks 1. **Data Transformation** Write a function `wide_to_long(df)` that converts the provided wide-form dataframe `df_wide` into a long-form dataframe. The long-form dataframe should have three columns: `year`, `month`, and `value`. 2. **Data Visualization** Write a function `plot_trends(df_long)` that takes the long-form dataframe as input and creates a line plot using seaborn. The x-axis should represent the `year`, the y-axis should represent the `value`, and there should be separate lines (with different colors) for each `month`. 3. **Pattern Identification** Based on the plot, print out your observations on any trends or patterns you identify in the data. # Function Definitions Function 1: wide_to_long ```python def wide_to_long(df): Convert the provided wide-form dataframe to a long-form dataframe. Args: df (pd.DataFrame): The wide-form dataframe with columns \'year\' and monthly data (e.g., \'Jan\', \'Feb\', \'Mar\'). Returns: pd.DataFrame: A long-form dataframe with columns \'year\', \'month\', and \'value\'. pass # Your implementation here ``` Function 2: plot_trends ```python import seaborn as sns import matplotlib.pyplot as plt def plot_trends(df_long): Create a line plot of the long-form dataframe using seaborn. Args: df_long (pd.DataFrame): The long-form dataframe with columns \'year\', \'month\', and \'value\'. Returns: None: The function should display a seaborn plot. pass # Your implementation here ``` Observations In the function `plot_trends`, after generating the plot, add comments or print statements to describe any trends or patterns you observe in the visualization. # Constraints - You must use seaborn for plotting. - The provided wide-form data structure must be transformed properly into long-form. - Ensure that the visualization correctly represents the data and adheres to the guidelines specified. # Example Usage ```python # Convert wide-form data to long-form df_long = wide_to_long(df_wide) # Plot trends plot_trends(df_long) ``` **Note:** The implementation of `wide_to_long` and `plot_trends`, as well as your observations based on the plot, will be used to assess your understanding of seaborn and data visualization.","solution":"import pandas as pd def wide_to_long(df): Convert the provided wide-form dataframe to a long-form dataframe. Args: df (pd.DataFrame): The wide-form dataframe with columns \'year\' and monthly data (e.g., \'Jan\', \'Feb\', \'Mar\'). Returns: pd.DataFrame: A long-form dataframe with columns \'year\', \'month\', and \'value\'. return df.melt(id_vars=[\\"year\\"], var_name=\\"month\\", value_name=\\"value\\") import seaborn as sns import matplotlib.pyplot as plt def plot_trends(df_long): Create a line plot of the long-form dataframe using seaborn. Args: df_long (pd.DataFrame): The long-form dataframe with columns \'year\', \'month\', and \'value\'. Returns: None: The function should display a seaborn plot. plt.figure(figsize=(10, 6)) sns.lineplot(data=df_long, x=\'year\', y=\'value\', hue=\'month\', marker=\'o\') plt.title(\'Monthly Values over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Value\') plt.legend(title=\'Month\') plt.show() # Observations print(\\"Observations:\\") print(\\"1. All months show a clear upward trend over the years from 2015 to 2019.\\") print(\\"2. The increase in values appears to be consistent for each month.\\") print(\\"3. Each month maintains its relative position in values compared to other months across the years (Mar > Feb > Jan).\\")"},{"question":"# Question: Implement an Advanced Context Manager using `contextlib` You are required to implement a custom context manager that manages database connections and ensures that nested transactions rollback if an exception occurs. This custom context manager should support both synchronous and asynchronous operations. Implement a class `TransactionManager` with the following requirements: - **Synchronous Context Management:** - The `__enter__` method should establish a connection and begin a transaction. - The `__exit__` method should commit the transaction if no exceptions were raised and rollback otherwise. - **Asynchronous Context Management:** - The `__aenter__` method should establish a connection and begin a transaction. - The `__aexit__` method should commit the transaction if no exceptions were raised and rollback otherwise. Additionally, this context manager should be able to manage nested transactions. If an exception occurs in any of the nested contexts, all transactions should be rolled back. To demonstrate the usage, you must also provide examples of: 1. Using the context manager in a synchronous \\"with\\" statement. 2. Using the context manager in an asynchronous \\"async with\\" statement. Constraints: - Assume you have a mock database connection object with the required methods (`connect`, `start_transaction`, `commit`, and `rollback` for synchronous operations, and `async_connect`, `async_start_transaction`, `async_commit`, and `async_rollback` for asynchronous operations). - The connection object also supports closing operations with `close` and `async_close` methods. Example of Synchronous Usage: ```python with TransactionManager() as txn: # Perform database operations if some_condition: raise ValueError(\\"An error occurred\\") # Transaction should be committed if no exceptions ``` Example of Asynchronous Usage: ```python async with TransactionManager() as txn: # Perform asynchronous database operations if some_condition: raise ValueError(\\"An error occurred\\") # Transaction should be committed if no exceptions ``` Expected Implementation: ```python class TransactionManager(contextlib.AbstractContextManager, contextlib.AbstractAsyncContextManager): def __init__(self): self.connection = None def __enter__(self): self.connection = connect() self.connection.start_transaction() return self def __exit__(self, exc_type, exc_value, traceback): if exc_type is None: self.connection.commit() else: self.connection.rollback() self.connection.close() async def __aenter__(self): self.connection = await async_connect() await self.connection.async_start_transaction() return self async def __aexit__(self, exc_type, exc_value, traceback): if exc_type is None: await self.connection.async_commit() else: await self.connection.async_rollback() await self.connection.async_close() ``` Provide your implementation and example usages as specified.","solution":"import contextlib class MockDatabaseConnection: def __init__(self): self.transaction_started = False def connect(self): self.transaction_started = True return self def start_transaction(self): pass def commit(self): if self.transaction_started: print(\\"Transaction committed\\") def rollback(self): if self.transaction_started: print(\\"Transaction rolled back\\") def close(self): self.transaction_started = False print(\\"Connection closed\\") async def async_connect(self): self.transaction_started = True return self async def async_start_transaction(self): pass async def async_commit(self): if self.transaction_started: print(\\"Transaction committed\\") async def async_rollback(self): if self.transaction_started: print(\\"Transaction rolled back\\") async def async_close(self): self.transaction_started = False print(\\"Connection closed\\") class TransactionManager(contextlib.AbstractContextManager, contextlib.AbstractAsyncContextManager): def __init__(self): self.connection = None def __enter__(self): self.connection = MockDatabaseConnection().connect() self.connection.start_transaction() return self.connection def __exit__(self, exc_type, exc_value, traceback): if exc_type is None: self.connection.commit() else: self.connection.rollback() self.connection.close() async def __aenter__(self): self.connection = await MockDatabaseConnection().async_connect() await self.connection.async_start_transaction() return self.connection async def __aexit__(self, exc_type, exc_value, traceback): if exc_type is None: await self.connection.async_commit() else: await self.connection.async_rollback() await self.connection.async_close()"},{"question":"# Python Evaluation and Frame Manipulation This task requires you to implement a mini-debugger using Python\'s internal frame and evaluation functions. Your mini-debugger should allow the user to inspect the current execution frame, retrieve and display information about variables (both local and global), and navigate frames for a more detailed inspection. Task: 1. **Function `inspect_frame()`**: This function should: - Return a dictionary containing information about the current execution frame: - Line number being executed - Code object of the frame - List of local variables with their current values - List of global variables accessible from the frame 2. **Function `get_function_info(func)`**: This function should: - Take a function object `func` as input. - Return a string that provides the name and description of the function, formatted as \\"Name: [name], Description: [description]\\". 3. **Function `navigate_frames(frame)`**: This function should: - Take a frame object `frame` as input. - Allow navigating to parent frames and returning a list of code objects from the current frame up to the topmost parent frame. Specifications: - Use the provided functions from the documentation to fetch necessary information. - Handle possible null or invalid inputs gracefully and provide meaningful error messages. - Ensure that your solution handles nested frames properly and can navigate through them efficiently. Example Usage: ```python def some_function(): a = 10 b = 20 print(inspect_frame()) return a + b def some_other_function(): return \\"Hello\\" print(get_function_info(some_other_function)) # Assuming we have a frame object from some context # print(navigate_frames(frame)) ``` Constraints: - You are not allowed to directly manipulate or simulate frames outside of the provided API. - Each function should handle errors individually and not propagate exceptions unless they are indicative of a fundamental failure. - Ensure your solution adheres to Python 3.10 standards and takes advantage of new features where applicable. Evaluation: - **Correctness**: Your functions must return accurate and precise information as per the function specifications. - **Efficiency**: The navigation through frames must be handled efficiently without causing significant performance overhead. - **Readability**: Provide clear and documented code, including descriptions for complex logic sections. Good luck, and happy debugging!","solution":"import inspect def inspect_frame(): Returns a dictionary containing details about the current execution frame. frame = inspect.currentframe() outer_frames = inspect.getouterframes(frame) current_frame = outer_frames[1].frame if len(outer_frames) > 1 else frame code = current_frame.f_code line_no = current_frame.f_lineno local_vars = current_frame.f_locals global_vars = current_frame.f_globals return { \\"line_number\\": line_no, \\"code_object\\": code, \\"local_variables\\": local_vars, \\"global_variables\\": global_vars } def get_function_info(func): Returns a string with the name and description of the given function. if not callable(func): return \\"Invalid function provided.\\" name = func.__name__ docstring = func.__doc__ or \\"No description provided.\\" return f\\"Name: {name}, Description: {docstring}\\" def navigate_frames(frame): Takes a frame object as input and returns a list of code objects from the current frame up to the topmost parent frame. code_objects = [] while frame: code_objects.append(frame.f_code) frame = frame.f_back return code_objects"},{"question":"**Problem Statement:** You are given a directory structure containing a mix of files and subdirectories. Your task is to write a Python program to list all files in the directory (and its subdirectories) that match specified patterns, and return information about these files. **Function Signature:** ```python def find_matching_files(directory, patterns): Given a root directory and a list of patterns, find all files in the directory and its subdirectories that match any of the patterns. Args: directory (str): The root directory to start searching. patterns (list of str): The list of patterns to match files against. Returns: dict: A dictionary where the keys are the given patterns and the corresponding values are lists of matching files for each pattern. The returned paths should be relative to the root directory. Constraints: - `directory` will always be a valid directory path. - `patterns` will contain at least one pattern. - Patterns will be Unix shell-style patterns (e.g., \'*.txt\', \'sub/?.gif\', etc.) # Your code here ``` **Example Usage:** ```python # Assuming a directory structure as follows: # root/ # file1.txt # file2.py # subdir/ # file3.txt # file4.md # .hiddenfile patterns = [\'*.txt\', \'*.py\', \'**/*.md\', \'subdir/*.md\'] print(find_matching_files(\'root\', patterns)) ``` **Expected Output:** ```python { \'*.txt\': [\'file1.txt\'], \'*.py\': [\'file2.py\'], \'**/*.md\': [\'subdir/file4.md\'], \'subdir/*.md\': [\'subdir/file4.md\'] } ``` **Additional Requirements:** 1. The function should use the `glob` module. 2. Use `glob.iglob()` to handle large directories efficiently. 3. The function should correctly handle hidden files and directories. 4. Ensure file paths in the result are relative to the `directory` parameter. *Solution Requirements:* - Your solution should correctly process the specified patterns, making use of recursive searching where needed. - Proper exception handling and input validation should be considered.","solution":"import os import glob def find_matching_files(directory, patterns): Given a root directory and a list of patterns, find all files in the directory and its subdirectories that match any of the patterns. Args: directory (str): The root directory to start searching. patterns (list of str): The list of patterns to match files against. Returns: dict: A dictionary where the keys are the given patterns and the corresponding values are lists of matching files for each pattern. The returned paths should be relative to the root directory. matching_files = {} for pattern in patterns: full_pattern = os.path.join(directory, pattern) files = [os.path.relpath(match, directory) for match in glob.iglob(full_pattern, recursive=True)] matching_files[pattern] = files return matching_files"},{"question":"# Web Scraping Compliance Check The goal of this task is to ensure that web scrapers respect the website\'s `robots.txt` file by correctly interpreting the permissions and restrictions specified for different user agents. Question: Write a Python function `compliance_check` that takes the following parameters: - `robots_url`: A string representing the URL where the `robots.txt` file is located. - `user_agent`: A string representing the user agent making the requests. - `urls_to_check`: A list of strings, each representing a URL that needs to be checked for access permissions. The function should return a dictionary where the keys are the URLs from `urls_to_check` and the values are booleans indicating whether the `user_agent` is allowed (`True`) or disallowed (`False`) to fetch the URL according to the `robots.txt` file. Input - `robots_url`: A `string` representing the URL of the `robots.txt` file (e.g., \\"http://example.com/robots.txt\\"). - `user_agent`: A `string` representing the user agent (e.g., \\"Mozilla/5.0\\"). - `urls_to_check`: A `list` of `string`s, each being a URL to check (e.g., [\\"http://example.com/page1\\", \\"http://example.com/page2\\"]). Output - A `dictionary` with URLs as keys and boolean values indicating access permissions (`True` for allowed, `False` for disallowed). Constraints - Assume that the `robots.txt` file follows standard conventions and uses valid syntax. - If the `robots.txt` file cannot be fetched, throw a custom exception `FetchingError`. Example ```python def compliance_check(robots_url, user_agent, urls_to_check): pass # Example Usage robots_url = \\"http://example.com/robots.txt\\" user_agent = \\"Mozilla/5.0\\" urls_to_check = [ \\"http://example.com/page1\\", \\"http://example.com/page2\\" ] { \\"http://example.com/page1\\": True, \\"http://example.com/page2\\": False } ``` Your Task Implement the `compliance_check` function using the functionalities provided by the `urllib.robotparser.RobotFileParser` class. Ensure that your implementation is efficient and handles potential exceptions properly. Notes - Utilize the `RobotFileParser` class to read and parse the `robots.txt` file. - Use the `can_fetch` method to determine access permissions for each URL in `urls_to_check`. - Handle network errors gracefully and throw a custom `FetchingError` exception if the `robots.txt` file cannot be fetched.","solution":"import urllib.robotparser import urllib.request class FetchingError(Exception): pass def compliance_check(robots_url, user_agent, urls_to_check): Checks URLs against the robots.txt file for compliance. Parameters: robots_url (str): URL of the robots.txt file. user_agent (str): User-agent string. urls_to_check (list): List of URLs to check. Returns: dict: Dictionary where keys are URLs and values are booleans. # Initialize the RobotFileParser object rp = urllib.robotparser.RobotFileParser() try: rp.set_url(robots_url) rp.read() except Exception as e: raise FetchingError(\\"Unable to fetch the robots.txt file.\\") from e # Create a dictionary to hold the compliance results compliance_results = {} for url in urls_to_check: compliance_results[url] = rp.can_fetch(user_agent, url) return compliance_results"},{"question":"**Python Coding Assessment: Implementing Command-Line History and Custom Auto-Completion** # Objective: You are to implement a Python script that utilizes the `readline` module to achieve the following: 1. Manage the command-line history. 2. Implement a custom auto-completion function. # Requirements: 1. **History Management**: - Load a history file named `\\".my_python_history\\"` from the user\'s home directory when the script starts. - Save the session\'s command history to `\\".my_python_history\\"` in the user\'s home directory upon exiting the script. - Ensure that the history management supports concurrent interactive sessions by only appending new commands to the history file. 2. **Custom Completion**: - Implement a custom completion function that completes from a list of user-defined words. For example, you can use the list `[\\"apple\\", \\"banana\\", \\"grape\\", \\"orange\\", \\"watermelon\\"]`. # Input and Output Formats: - **Input**: The user will enter commands in an interactive command-line session. - **Output**: Auto-completion suggestions based on the provided list, and management of command history. # Constraints: - Use the `readline` module to handle command history and completion functions. - Do not use any external libraries for auto-completion. - Ensure the custom completion function is case-insensitive. # Instructions: 1. Implement a function `setup_history()` to handle the loading and saving of the history file. 2. Implement a function `setup_completion()` to set up the custom completer function. 3. Use these functions to create an interactive command-line session that fulfills the requirements. # Example Usage: When the script is run, it should allow the user to enter commands interactively. The user should be able to press the \\"Tab\\" key to see possible completions from the list `[\\"apple\\", \\"banana\\", \\"grape\\", \\"orange\\", \\"watermelon\\"]`. # Example Workflow: 1. Load command history from `~/.my_python_history`. 2. Provide auto-completion from the list. 3. Upon exiting, append new commands to `~/.my_python_history`. # Code Template: ```python import readline import os import atexit def setup_history(histfile): try: readline.read_history_file(histfile) h_len = readline.get_current_history_length() except FileNotFoundError: open(histfile, \'wb\').close() h_len = 0 def save(prev_h_len, histfile): new_h_len = readline.get_current_history_length() readline.set_history_length(1000) readline.append_history_file(new_h_len - prev_h_len, histfile) atexit.register(save, h_len, histfile) def completer(text, state): options = [i for i in [\'apple\', \'banana\', \'grape\', \'orange\', \'watermelon\'] if i.startswith(text.lower())] if state < len(options): return options[state] return None def setup_completion(): readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") if __name__ == \\"__main__\\": histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".my_python_history\\") setup_history(histfile) setup_completion() while True: try: line = input(\\"Enter command: \\") except (EOFError, KeyboardInterrupt): print(\\"nExiting...\\") break ``` Implement your function in Python and ensure it meets all the requirements. Note that you are not limited to the code template provided and may construct your solution in any way so long as the final implementation meets the outlined objectives.","solution":"import os import readline import atexit # User-defined list of words for auto-completion completion_words = [\\"apple\\", \\"banana\\", \\"grape\\", \\"orange\\", \\"watermelon\\"] def setup_history(): Sets up the command-line history management. Loads the history from the .my_python_history file and ensures the history is saved on exit. histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".my_python_history\\") try: readline.read_history_file(histfile) except FileNotFoundError: pass def save_history(): readline.write_history_file(histfile) atexit.register(save_history) def completer(text, state): Custom completer function for auto-completion. Returns possible completions for the given text based on the completion_words list. matches = [word for word in completion_words if word.startswith(text.lower())] try: return matches[state] except IndexError: return None def setup_completion(): Sets up the custom completer function. readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") if __name__ == \\"__main__\\": setup_history() setup_completion() while True: try: line = input(\\"Enter command: \\") if line.lower() in {\'exit\', \'quit\'}: print(\\"Exiting...\\") break except (EOFError, KeyboardInterrupt): print(\\"nExiting...\\") break"},{"question":"**Problem Statement: Efficiently Managing MIME Types** You are tasked with developing a utility to manage and expand MIME types dynamically. This involves using the `mimetypes` module to handle filename extensions and MIME types effectively. # Function Requirements: 1. **Function Name:** `manage_mime_types` 2. **Parameters:** - `operation` (str): The operation to perform. It can be one of the following: - `\'guess_type\'`: Guess the MIME type of a file. - `\'guess_extension\'`: Guess the extension for a given MIME type. - `\'add_type\'`: Add a new type to the MIME database. - `value` (str): The value associated with the operation. This can be: - A filename or URL for guessing the MIME type. - A MIME type for guessing the extension. - A string in the form `\'mime_type,extension\'` when adding a new type. - `strict` (bool, optional): A flag for strict mode, defaulting to `True`. This applies to all operations. # Behavior: - For the `\'guess_type\'` operation, return a tuple (MIME type, encoding) or `(None, None)` if the type can\'t be guessed. - For the `\'guess_extension\'` operation, return the extension as a string or `None` if it can\'t be guessed. - For the `\'add_type\'` operation, add the type-extension mapping to the database and return a confirmation message. # Examples: ```python # Example 1 operation = \'guess_type\' value = \'example.txt\' strict = True result = manage_mime_types(operation, value, strict) # Expected result: (\'text/plain\', None) # Example 2 operation = \'guess_extension\' value = \'text/plain\' strict = True result = manage_mime_types(operation, value, strict) # Expected result: \'.txt\' # Example 3 operation = \'add_type\' value = \'application/x-example,.example\' strict = False result = manage_mime_types(operation, value, strict) # Expected result: \'Type added successfully.\' ``` # Constraints: - The `operation` parameter is always a valid string among the specified operations. - The `value` parameter is a valid string as specified per the operation type. # Template: Here\'s a template to get you started: ```python import mimetypes def manage_mime_types(operation, value, strict=True): # Your code here pass ``` # Notes: - Utilize the `mimetypes` module functions to handle each operation efficiently. - Ensure that the solution handles edge cases, such as unknown filenames or MIME types. Write the code to implement the `manage_mime_types` function according to the requirements.","solution":"import mimetypes def manage_mime_types(operation, value, strict=True): if operation == \'guess_type\': return mimetypes.guess_type(value, strict) elif operation == \'guess_extension\': return mimetypes.guess_extension(value, strict) elif operation == \'add_type\': mime_type, extension = value.split(\',\') mimetypes.add_type(mime_type, extension, strict) return \'Type added successfully.\' else: raise ValueError(\\"Invalid operation.\\")"},{"question":"Context: In this task, you will implement a function to securely manage user passwords using the deprecated `crypt` module. The function will be responsible for creating hashed passwords and validating user input against stored hashed passwords. Task: 1. Implement a function `hash_password(password: str) -> str` that: - Takes a plain-text password as input. - Uses the strongest available hashing method to generate a salt and hash the password. - Returns the hashed password as a string. 2. Implement a function `validate_password(stored_hash: str, input_password: str) -> bool` that: - Takes the stored hashed password and a user-input password as inputs. - Uses the `crypt.crypt` function to hash the input password with the same salt used in the stored hash. - Compares the resultant hash with the stored hash using `hmac.compare_digest` to ensure constant-time comparison. - Returns `True` if the hashes match, indicating the passwords match, otherwise returns `False`. Constraints: - You may assume that all input passwords are non-empty strings. - You should use appropriate methods from the `crypt` module to generate salts and hash passwords. - Ensure your implementation follows best practices for password security. Example: ```python import hmac import crypt def hash_password(password: str) -> str: # Generate and return the hash of the password using the strongest available method. pass def validate_password(stored_hash: str, input_password: str) -> bool: # Validate the input password against the stored hashed password. pass # Test cases hashed = hash_password(\\"securepassword123\\") is_valid = validate_password(hashed, \\"securepassword123\\") print(is_valid) # Output: True is_valid = validate_password(hashed, \\"wrongpassword\\") print(is_valid) # Output: False ``` Your implementation should ensure that the password hashing and validation are performed securely, relying on the strength and features provided by the `crypt` module.","solution":"import hmac import crypt def hash_password(password: str) -> str: Hashes a given password using the strongest available method. # Generate salt with the best available hashing method. salt = crypt.mksalt(crypt.METHOD_SHA512) # Hash the password with the generated salt. hashed_password = crypt.crypt(password, salt) return hashed_password def validate_password(stored_hash: str, input_password: str) -> bool: Validates an input password against the stored hash using `crypt`. # Extract salt from stored_hash; it is the first few characters before the third \'\' symbol. # Alternatively, we can perform hashing directly without explicitly extracting the salt as crypt.crypt can manage it. result = crypt.crypt(input_password, stored_hash) # Compare the resultant hash with the stored hash securely using hmac.compare_digest. return hmac.compare_digest(stored_hash, result)"},{"question":"Objective The goal of this assessment is for you to demonstrate your understanding of the `sys` module in Python, especially in context to handling command line arguments, managing and querying runtime configurations, and ensuring proper interaction with the Python interpreter. Problem Statement Create a Python script named `sys_info.py` that performs the following tasks: 1. **Command Line Argument Parsing**: - The script must accept a single command-line argument, a string representing the name of a variable in the `sys` module (e.g., `sys.path`, `sys.version`, etc.). - If the argument matches the name of an attribute in the `sys` module, print its value. - If the argument does not match any existing `sys` attribute, display an appropriate error message: `\\"Error: {argument} not found in sys module\\"`. 2. **System Information Summary**: - If the script is run without any command-line arguments, it should print the following system information: - Python executable path (`sys.executable`) - Maximum recursion limit (`sys.getrecursionlimit()`) - Current value of the Python implementation (`sys.implementation.name`) - The platform identifier (`sys.platform`) - The size of an empty list object (`sys.getsizeof([])`) - Whether the Python interpreter is finalizing (`sys.is_finalizing()`) - List of built-in modules (`sys.builtin_module_names`) 3. **Error Handling and Validation**: - Ensure that the script gracefully handles any exceptions or errors arising from invalid accesses or operations, providing meaningful error messages to the user. - Validate that the provided argument is a valid identifier for a `sys` attribute. 4. **Code Performance**: - Ensure that your implementation is efficient in terms of both time and space complexity. Consider using appropriate exception handling mechanisms to handle cases where attributes may not exist. Input - A single command-line argument that is expected to be the name of a `sys` module attribute, or no command-line argument at all. Output - Value of the requested `sys` module attribute if the command-line argument is valid. - Error message if the command-line argument does not correspond to any `sys` module attribute. - A summary of system information if no command-line argument is provided. Constraints - Use only built-in Python libraries. - Format the output in a clear and readable manner. Example Consider the following usage examples: 1. Command: ```bash python sys_info.py version ``` Output: ```plaintext 3.10.0 (default, Oct 16 2021, 03:33:58) [Clang 12.0.0 (clang-1200.0.32.29)] ``` 2. Command: ```bash python sys_info.py invalid_attribute ``` Output: ```plaintext Error: invalid_attribute not found in sys module ``` 3. Command: ```bash python sys_info.py ``` Output: ```plaintext Python Executable: /usr/local/bin/python Max Recursion Limit: 3000 Implementation: cpython Platform: linux Size of Empty List: 56 bytes Is Finalizing: False Built-in Modules: (\'_abc\', \'_ast\', \'_bisect\', \'_blake2\', ...) ``` Note Make sure your script can handle various edge cases such as missing or extra command-line arguments. Use robust exception handling and validation to ensure smooth execution.","solution":"import sys def get_sys_info(arg=None): if arg: if hasattr(sys, arg): print(getattr(sys, arg)) else: print(f\\"Error: {arg} not found in sys module\\") else: print(f\\"Python Executable: {sys.executable}\\") print(f\\"Max Recursion Limit: {sys.getrecursionlimit()}\\") print(f\\"Implementation: {sys.implementation.name}\\") print(f\\"Platform: {sys.platform}\\") print(f\\"Size of Empty List: {sys.getsizeof([])} bytes\\") print(f\\"Is Finalizing: {sys.is_finalizing()}\\") print(f\\"Built-in Modules: {sys.builtin_module_names}\\") if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser(description=\'Get sys module information.\') parser.add_argument(\'attribute\', nargs=\'?\', help=\'The attribute of sys module to retrieve.\') args = parser.parse_args() get_sys_info(args.attribute)"},{"question":"# Question: Implement PyLongObject Conversion Functions in Python You are required to implement a few functions in Python that simulate the behavior of some `PyLongObject` conversion functions found in the `python310` package documentation. Specifically, you will implement functions that create a Python integer from various types and convert a Python integer to a specific type, handling error conditions appropriately. Requirements: 1. **Function `create_py_long_from_long(v: int) -> int`**: - Create a Python integer from a C `long` type. - If `v` is out of the range of a Python integer, raise an `OverflowError`. 2. **Function `create_py_long_from_unsigned_long(v: int) -> int`**: - Create a Python integer from a C `unsigned long` type. - If `v` is negative or out of the range of a Python integer, raise an `OverflowError`. 3. **Function `convert_to_c_long(py_long: int) -> int`**: - Convert a Python integer to a C `long` representation. - If `py_long` is out of range for a C `long`, raise an `OverflowError`. 4. **Function `convert_to_c_unsigned_long(py_long: int) -> int`**: - Convert a Python integer to a C `unsigned long` representation. - If `py_long` is negative or out of range for a C `unsigned long`, raise an `OverflowError`. Constraints: - The range of a C `long` is assumed to be the same as the range of a Python integer on the system where this is run. - You are not allowed to use external libraries for this implementation. - Focus on correct error handling and proper conversion logic. Examples: ```python # Example 1 print(create_py_long_from_long(123456789)) # Expected output: 123456789 # Example 2 print(create_py_long_from_unsigned_long(987654321)) # Expected output: 987654321 # Example 3 print(convert_to_c_long(123456789)) # Expected output: 123456789 # Example 4 print(convert_to_c_unsigned_long(987654321)) # Expected output: 987654321 ``` Note: - The test cases assume hypothetical C `long` range as the same as Python int\'s range for practical purposes in this environment. Implement the functions to meet the above requirements and handle any potential errors as described.","solution":"def create_py_long_from_long(v: int) -> int: Create a Python integer from a C `long` type. In Python, `int` has no limit, but we assume the range must be the same as for C `long`. For simplicity, let\'s consider it equivalent to a 64-bit integer. # Simulate a 64-bit long range min_long = -2**63 max_long = 2**63 - 1 if v < min_long or v > max_long: raise OverflowError(\\"Value out of range for C long\\") return v def create_py_long_from_unsigned_long(v: int) -> int: Create a Python integer from a C `unsigned long` type. # Simulate a 64-bit unsigned long range min_unsigned_long = 0 max_unsigned_long = 2**64 - 1 if v < min_unsigned_long or v > max_unsigned_long: raise OverflowError(\\"Value out of range for C unsigned long\\") return v def convert_to_c_long(py_long: int) -> int: Convert a Python integer to a C `long` representation. # Simulate a 64-bit long range min_long = -2**63 max_long = 2**63 - 1 if py_long < min_long or py_long > max_long: raise OverflowError(\\"Value out of range for C long\\") return py_long def convert_to_c_unsigned_long(py_long: int) -> int: Convert a Python integer to a C `unsigned long` representation. # Simulate a 64-bit unsigned long range min_unsigned_long = 0 max_unsigned_long = 2**64 - 1 if py_long < min_unsigned_long or py_long > max_unsigned_long: raise OverflowError(\\"Value out of range for C unsigned long\\") return py_long"},{"question":"HTML Escape and Unescape Utility You are required to implement a function that takes a string containing HTML content, escapes certain characters to make it HTML-safe, and then unescapes the content back to its original form. This will test your ability to correctly use the `html.escape` and `html.unescape` functions provided by the Python `html` module. Task 1. Implement the function `process_html_content(html_content: str, escape_quotes: bool) -> tuple: - The function should take in two parameters: - `html_content`: A string containing the HTML content to be processed. - `escape_quotes`: A boolean flag indicating whether quotes should be escaped in addition to the characters `&`, `<`, and `>`. - The function should: - Escape the characters in `html_content` using `html.escape`. - Unescape the resulting escaped content using `html.unescape`. - Return a tuple containing: - The escaped string. - The unescaped string (which should ideally match the original `html_content`). Example ```python from html import escape, unescape def process_html_content(html_content: str, escape_quotes: bool) -> tuple: escaped_content = escape(html_content, quote=escape_quotes) unescaped_content = unescape(escaped_content) return (escaped_content, unescaped_content) # Example Usage html_content = \\"<h1>Title & Description</h1>\\" escape_quotes = True result = process_html_content(html_content, escape_quotes) print(result) # Output should be: # (\'&lt;h1&gt;Title &amp; Description&lt;/h1&gt;\', \'<h1>Title & Description</h1>\') ``` Constraints - The `html_content` string can contain any valid HTML elements, entities, and text. - You are not allowed to use any third-party libraries other than the standard `html` module in Python. Expectations - Correct usage of `html.escape` and `html.unescape`. - Ensuring that after escaping and then unescaping, the content should match the original input. Good luck!","solution":"from html import escape, unescape def process_html_content(html_content: str, escape_quotes: bool) -> tuple: Processes HTML content by escaping and unescaping it. Parameters: html_content (str): The HTML content to process. escape_quotes (bool): Whether to escape quotes (\\") in the content. Returns: tuple: The escaped string and the unescaped string. escaped_content = escape(html_content, quote=escape_quotes) unescaped_content = unescape(escaped_content) return (escaped_content, unescaped_content)"},{"question":"Objective: You are asked to implement a function that lists all files in a given directory and its subdirectories that match a specified file pattern. The goal is to find these files efficiently using the `glob` module. Task: Write a function named `find_files` with the following specifications: ```python def find_files(directory: str, pattern: str, recursive: bool = False) -> list: List all files in a given directory and its subdirectories that match a specified file pattern. Parameters: - directory (str): The root directory to search within. - pattern (str): The pattern of the file names to match. - recursive (bool): If True, search recursively in all subdirectories. Default is False. Returns: - list: A list of paths matching the specified pattern. The paths should be relative to the provided directory. pass ``` Input: - `directory`: A string representing the root directory to search within. - `pattern`: A string representing the file pattern to match. - `recursive`: A boolean flag indicating whether to search recursively within subdirectories (default is False). Output: - A list of strings, each representing a relative path to a file that matches the specified pattern. Constraints: - Do not use any additional libraries apart from the `glob` module. - Assume all paths are Unix-style paths. - Handle cases where the directory is empty or contains no files matching the pattern. - Ensure your function is efficient and handles large directory trees if the recursive flag is set to True. Example: Consider a directory structure as follows: ``` root/ file1.txt file2.py subdir1/ file3.txt subdir2/ file4.md ``` Example usage of the `find_files` function: ```python >>> find_files(\'root\', \'*.txt\') [\'file1.txt\'] >>> find_files(\'root\', \'*.txt\', recursive=True) [\'file1.txt\', \'subdir1/file3.txt\'] >>> find_files(\'root\', \'*.py\') [\'file2.py\'] >>> find_files(\'root\', \'*.md\', recursive=True) [\'subdir2/file4.md\'] ``` Use this function to demonstrate the full capabilities of the `glob` module, including non-recursive and recursive searches, special character matching, and more.","solution":"import glob import os def find_files(directory: str, pattern: str, recursive: bool = False) -> list: List all files in a given directory and its subdirectories that match a specified file pattern. Parameters: - directory (str): The root directory to search within. - pattern (str): The pattern of the file names to match. - recursive (bool): If True, search recursively in all subdirectories. Default is False. Returns: - list: A list of paths matching the specified pattern. The paths should be relative to the provided directory. if recursive: search_pattern = os.path.join(directory, \'**\', pattern) files = glob.glob(search_pattern, recursive=True) else: search_pattern = os.path.join(directory, pattern) files = glob.glob(search_pattern) return [os.path.relpath(file, directory) for file in files]"},{"question":"# Advanced Python Dictionary Manipulation You are provided with two dictionaries, `dict_a` and `dict_b`. Your task is to merge these two dictionaries into a new dictionary. However, you need to follow specific rules for this merge operation: 1. If a key from `dict_b` exists in `dict_a`, the corresponding value from `dict_a` should be updated by adding the value from `dict_b`. 2. If a key from `dict_b` does not exist in `dict_a`, the key-value pair from `dict_b` should be added to `dict_a`. Implement the function `merge_dictionaries(dict_a, dict_b)` that takes two dictionaries as input and returns a new dictionary that is the result of the described merge operation. Input: - `dict_a`: A dictionary with integer keys and values. - `dict_b`: A dictionary with integer keys and values. Output: - A new dictionary with the merged content according to the rules specified. Example: ```python def merge_dictionaries(dict_a, dict_b): pass # Example usage: dict_a = {1: 10, 2: 20, 3: 30} dict_b = {2: 25, 3: 35, 4: 45} result = merge_dictionaries(dict_a, dict_b) print(result) # Output: {1: 10, 2: 45, 3: 65, 4: 45} ``` Constraints: - You may assume that all the keys in the dictionaries are integers. - You may assume that all the values in the dictionaries are integers. - Your solution should be efficient and aim for a time complexity of O(n + m), where n and m are the sizes of `dict_a` and `dict_b`, respectively.","solution":"def merge_dictionaries(dict_a, dict_b): Merges two dictionaries according to the specified rules: 1. If a key from dict_b exists in dict_a, the corresponding value from dict_a should be updated by adding the value from dict_b. 2. If a key from dict_b does not exist in dict_a, the key-value pair from dict_b should be added to dict_a. Args: dict_a (dict): The first dictionary with integer keys and values. dict_b (dict): The second dictionary with integer keys and values. Returns: dict: The merged dictionary. # Create a copy of dict_a to avoid mutating the input dictionary merged_dict = dict_a.copy() for key, value in dict_b.items(): if key in merged_dict: merged_dict[key] += value else: merged_dict[key] = value return merged_dict"},{"question":"**Custom Event Loop Policy Implementation** In this assessment, you will demonstrate your understanding of Python\'s `asyncio` package by implementing a custom event loop policy. Your custom policy should extend `asyncio.DefaultEventLoopPolicy` and override a couple of its methods to introduce new behavior. Additionally, you will handle a custom child watcher object. # Task 1: Implement Custom Event Loop Policy 1. **Create a custom event loop policy** by subclassing `asyncio.DefaultEventLoopPolicy`. 2. **Override the `get_event_loop` method**: - Ensure it logs every time an event loop is retrieved. - Return a standard event loop object. 3. **Override the `new_event_loop` method**: - Ensure it logs every time a new event loop is created. - Return a new standard event loop object. # Task 2: Implement Custom Child Watcher 1. **Create a custom child watcher** by subclassing `asyncio.ThreadedChildWatcher`. 2. **Override the `add_child_handler` method**: - Ensure it logs every time a child handler is added. - Call the base implementation\'s `add_child_handler` method. 3. **Override the `remove_child_handler` method**: - Ensure it logs every time a child handler is removed. - Call the base implementation\'s `remove_child_handler` method. # Task 3: Integration and Testing 1. **Set your custom event loop policy** as the current policy using `asyncio.set_event_loop_policy`. 2. **Set your custom child watcher** as the current watcher using `asyncio.set_child_watcher`. 3. **Write a test coroutine**: - Create a subprocess using `asyncio.create_subprocess_exec`. - Use asyncio to wait for the subprocess to complete. - Ensure your logs reflect the usage of custom handlers when working with subprocesses and event loops. # Constraints: - Use only the `asyncio` package for the implementation. - Ensure thread-safety where required. - Provide comments in your code to explain key parts. # Expected Function Signatures: ```python import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): # Your implementation here pass def new_event_loop(self): # Your implementation here pass class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): # Your implementation here pass def remove_child_handler(self, pid): # Your implementation here pass # Set custom policies and watchers asyncio.set_event_loop_policy(MyEventLoopPolicy()) asyncio.set_child_watcher(MyCustomChildWatcher()) # Test your implementation async def test_custom_policy(): # Your test code here pass if __name__ == \\"__main__\\": # Run the test asyncio.run(test_custom_policy()) ``` # Submission: Submit a Python script file that includes your implementation and testing code. Make sure to include sample output demonstrating your logging statements to confirm the correct use of custom handlers and policies.","solution":"import asyncio import logging # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) # Custom Event Loop Policy class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): logger.info(\\"Getting the event loop.\\") loop = super().get_event_loop() return loop def new_event_loop(self): logger.info(\\"Creating a new event loop.\\") loop = super().new_event_loop() return loop # Custom Child Watcher class MyCustomChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): logger.info(f\\"Adding child handler for PID: {pid}\\") super().add_child_handler(pid, callback, *args) def remove_child_handler(self, pid): logger.info(f\\"Removing child handler for PID: {pid}\\") super().remove_child_handler(pid) # Set custom policies and watchers asyncio.set_event_loop_policy(MyEventLoopPolicy()) asyncio.set_child_watcher(MyCustomChildWatcher()) # Test coroutine async def test_custom_policy(): try: proc = await asyncio.create_subprocess_exec( \\"echo\\", \\"Hello, World!\\", stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() logger.info(f\\"Process exited with {proc.returncode}\\") logger.info(f\\"[stdout]n{stdout.decode().strip()}\\") logger.info(f\\"[stderr]n{stderr.decode().strip()}\\") except Exception as e: logger.error(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": # Run the test asyncio.run(test_custom_policy())"},{"question":"**Question:** Implement different types of window functions to analyze a given time series dataset. You are given a time series dataset of stock prices with the columns `Date` and `Close`. The aim is to implement a function that performs various analyses using rolling, expanding, and exponentially-weighted window functions. You need to implement the following function: ```python import pandas as pd def analyze_stock_prices(df: pd.DataFrame) -> pd.DataFrame: Perform various window function analyses on the stock price data. Parameters: df (pd.DataFrame): A dataframe containing two columns - \'Date\' and \'Close\'. Returns: pd.DataFrame: A dataframe with the original `Close` column and the following additional columns: - \'rolling_mean_7d\': 7-day rolling window mean of the `Close` prices. - \'expanding_max\': Expanding window maximum of the `Close` prices. - \'ewm_std_10d\': 10-day exponentially-weighted moving standard deviation of the `Close` prices. Example: Given a dataframe `df`: Date Close 0 2022-01-01 100 1 2022-01-02 105 2 2022-01-03 102 3 ... The function should return a dataframe: Date Close rolling_mean_7d expanding_max ewm_std_10d 0 2022-01-01 100 NaN 100 NaN 1 2022-01-02 105 NaN 105 NaN ... # Your implementation here # Example usage: # df = pd.read_csv(\\"stock_prices.csv\\") # result = analyze_stock_prices(df) # print(result.head()) ``` **Input:** - A pandas DataFrame `df` with two columns: \'Date\' (in YYYY-MM-DD format) and \'Close\' (closing stock price). **Output:** - A pandas DataFrame with the original \'Date\' and \'Close\' column, along with the following new columns: - \'rolling_mean_7d\': 7-day rolling window mean of the `Close` prices. - \'expanding_max\': Expanding window maximum of the `Close` prices. - \'ewm_std_10d\': 10-day exponentially-weighted moving standard deviation of the `Close` prices. **Constraints:** - If there are less than the required number of days for a window function (e.g., the first 6 days for a 7-day rolling mean), those entries should be `NaN` for that particular column. This question assesses the student\'s ability to use different pandas window functions in a realistic context, ensuring they understand the nuances and applications of each type of window function.","solution":"import pandas as pd def analyze_stock_prices(df: pd.DataFrame) -> pd.DataFrame: Perform various window function analyses on the stock price data. Parameters: df (pd.DataFrame): A dataframe containing two columns - \'Date\' and \'Close\'. Returns: pd.DataFrame: A dataframe with the original `Close` column and the following additional columns: - \'rolling_mean_7d\': 7-day rolling window mean of the `Close` prices. - \'expanding_max\': Expanding window maximum of the `Close` prices. - \'ewm_std_10d\': 10-day exponentially-weighted moving standard deviation of the `Close` prices. df[\'rolling_mean_7d\'] = df[\'Close\'].rolling(window=7).mean() df[\'expanding_max\'] = df[\'Close\'].expanding().max() df[\'ewm_std_10d\'] = df[\'Close\'].ewm(span=10).std() return df"},{"question":"# PyTorch `torch.cond` Implementation and Usage in a Neural Network You are to implement a custom neural network module that conditionally applies different sub-modules based on the standard deviation of the input tensor. The module should use `torch.cond` to achieve this. Task 1. **Create a Custom Neural Network Module**: - Implement a class `StdDevConditionedModule` derived from `torch.nn.Module`. - The module should take a tensor as input and compute its standard deviation. - Depending on whether the standard deviation is above or below a certain threshold, apply different sub-module actions using `torch.cond`. 2. **Define the Sub-Modules**: - Define two sub-modules: `AboveThreshold`, which applies a cosine transformation followed by a ReLU activation function, and `BelowThreshold`, which applies a sine transformation followed by a ReLU activation function. - Use these sub-modules within the conditional logic of `torch.cond`. 3. **Threshold**: - Use a threshold value of `0.5` for the standard deviation to decide which sub-module to apply. 4. **Data**: - Use the following tensor as an example input to demonstrate the moduleâ€™s functionality: ```python example_tensor = torch.randn(10, 5) ``` Implementation Constraints - You must use `torch.cond` as described in the provided documentation. - The module should handle tensors of any shape. - Your implementation should be efficient and follow PyTorch best practices. Expected Input and Output Formats - **Input**: A PyTorch tensor of any shape. - **Output**: A transformed PyTorch tensor using the conditional logic based on the standard deviation. ```python import torch import torch.nn.functional as F class AboveThreshold(torch.nn.Module): def forward(self, x: torch.Tensor) -> torch.Tensor: return F.relu(x.cos()) class BelowThreshold(torch.nn.Module): def forward(self, x: torch.Tensor) -> torch.Tensor: return F.relu(x.sin()) class StdDevConditionedModule(torch.nn.Module): def __init__(self, threshold: float = 0.5): super().__init__() self.threshold = threshold self.above_threshold = AboveThreshold() self.below_threshold = BelowThreshold() def forward(self, x: torch.Tensor) -> torch.Tensor: std_dev = x.std() def true_fn(t: torch.Tensor): return self.above_threshold(t) def false_fn(t: torch.Tensor): return self.below_threshold(t) return torch.cond(std_dev > self.threshold, true_fn, false_fn, (x,)) # Example usage example_tensor = torch.randn(10, 5) model = StdDevConditionedModule() output = model(example_tensor) print(output) ``` Test Cases 1. **Input**: `torch.randn(10, 5)` - **Output Criteria**: Apply `BelowThreshold` if the standard deviation is <= 0.5, otherwise apply `AboveThreshold`. 2. **Input**: `torch.randn(50, 20)` - **Output Criteria**: Same as above, ensure the conditional logic applies accurately based on the standard deviation threshold. The implementation should demonstrate the correct usage of `torch.cond` for dynamic control flow in PyTorch models.","solution":"import torch import torch.nn.functional as F class AboveThreshold(torch.nn.Module): def forward(self, x: torch.Tensor) -> torch.Tensor: return F.relu(x.cos()) class BelowThreshold(torch.nn.Module): def forward(self, x: torch.Tensor) -> torch.Tensor: return F.relu(x.sin()) class StdDevConditionedModule(torch.nn.Module): def __init__(self, threshold: float = 0.5): super().__init__() self.threshold = threshold self.above_threshold = AboveThreshold() self.below_threshold = BelowThreshold() def forward(self, x: torch.Tensor) -> torch.Tensor: std_dev = x.std().item() if std_dev > self.threshold: return self.above_threshold(x) else: return self.below_threshold(x) # Example usage example_tensor = torch.randn(10, 5) model = StdDevConditionedModule() output = model(example_tensor) print(output)"},{"question":"You are required to implement a function that processes the items in a list and performs operations that may raise exceptions. The function should handle these exceptions appropriately and ensure that resources are properly cleaned up. Function Definition ```python def process_items(item_list): Processes a list of items and performs different operations on them. Args: item_list (list): A list containing items to be processed. Each item could be a number or a string. Returns: dict: A dictionary with two keys: - \'success\': A list of results of successful operations. - \'error\': A list of error strings for items that caused exceptions. pass ``` # Requirements 1. Iterate over the `item_list` and for each item: - If the item is an integer or float, divide 100 by the item. - If the item is a string, concatenate it with \\" processed\\". - If the item is of any other type, raise a custom exception `InvalidItemType`. 2. Use a `try` and `except` block to handle different types of exceptions: - `ZeroDivisionError`: In the case of division by zero, record an appropriate error message. - `TypeError`: In the case of an unsupported operation, record an appropriate error message. - `InvalidItemType`: For unsupported types, record an appropriate error message. 3. Ensure that an item is always appended to either the success list or the error list. 4. Use a `with` statement to simulate writing all successes and errors into a file named `results.log`: - For simplicity, this should just ensure the items are written without leaving the file open. # Example Input: ```python item_list = [10, 0, \'test\', {}, 20] ``` Output: ```python { \'success\': [10.0, \'test processed\', 5.0], \'error\': [ \'ZeroDivisionError: division by zero\', \'InvalidItemType: Unsupported type - dict\' ] } ``` # Constraints 1. The function should only use valid constructs as described in the provided documentation. 2. You are required to define the custom exception `InvalidItemType`. **Note:** Make sure to test your implementation with various inputs to cover all edge cases and ensure proper exception handling.","solution":"class InvalidItemType(Exception): Custom exception for invalid item types pass def process_items(item_list): Processes a list of items and performs different operations on them. Args: item_list (list): A list containing items to be processed. Each item could be a number or a string. Returns: dict: A dictionary with two keys: - \'success\': A list of results of successful operations. - \'error\': A list of error strings for items that caused exceptions. success = [] error = [] for item in item_list: try: if isinstance(item, (int, float)): result = 100 / item success.append(result) elif isinstance(item, str): result = item + \\" processed\\" success.append(result) else: raise InvalidItemType(f\\"Unsupported type - {type(item).__name__}\\") except ZeroDivisionError as zde: error.append(f\\"ZeroDivisionError: {zde}\\") except (InvalidItemType, TypeError) as it: error.append(f\\"{type(it).__name__}: {it}\\") # Simulate writing to a file (just ensure we handle resources properly) with open(\\"results.log\\", \\"w\\") as f: for item in success: f.write(f\\"Success: {item}n\\") for item in error: f.write(f\\"Error: {item}n\\") return {\'success\': success, \'error\': error}"},{"question":"# Question: Implement and Compare Kernel Ridge Regression and Support Vector Regression You are provided with a dataset containing a sinusoidal target function with added noise. Your task is to implement Kernel Ridge Regression and Support Vector Regression using scikit-learn, fit the models to the dataset, and compare their performance. Input - A CSV file `data.csv` containing two columns `X` and `y` which represent the input features and target values respectively. Output - Print the Mean Squared Error (MSE) for both Kernel Ridge Regression and Support Vector Regression on the test set. - Print the training time and prediction time for both models. - Plot the learned functions for both models on a single graph. Constraints - Use the RBF kernel for both models. - Perform a grid-search to optimize the hyperparameters for both models. - You may assume the dataset is of medium size (less than 1000 samples). Performance Requirements - Ensure the grid-search and model fitting are efficient and do not exceed reasonable computational limits. Example ```python import numpy as np import pandas as pd from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt import time # Load the dataset data = pd.read_csv(\'data.csv\') X = data[\'X\'].values.reshape(-1, 1) y = data[\'y\'].values # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the parameter grid for Kernel Ridge Regression param_grid_krr = {\'alpha\': [0.1, 1, 10], \'kernel\': [\'rbf\'], \'gamma\': [0.1, 1, 10]} # Grid search for Kernel Ridge Regression start_time = time.time() grid_search_krr = GridSearchCV(KernelRidge(), param_grid_krr, cv=5) grid_search_krr.fit(X_train, y_train) krr_train_time = time.time() - start_time # Predict using the best Kernel Ridge Regression model start_time = time.time() y_pred_krr = grid_search_krr.predict(X_test) krr_predict_time = time.time() - start_time # Calculate the Mean Squared Error for Kernel Ridge Regression mse_krr = mean_squared_error(y_test, y_pred_krr) # Define the parameter grid for Support Vector Regression param_grid_svr = {\'C\': [0.1, 1, 10], \'kernel\': [\'rbf\'], \'gamma\': [0.1, 1, 10]} # Grid search for Support Vector Regression start_time = time.time() grid_search_svr = GridSearchCV(SVR(), param_grid_svr, cv=5) grid_search_svr.fit(X_train, y_train) svr_train_time = time.time() - start_time # Predict using the best Support Vector Regression model start_time = time.time() y_pred_svr = grid_search_svr.predict(X_test) svr_predict_time = time.time() - start_time # Calculate the Mean Squared Error for Support Vector Regression mse_svr = mean_squared_error(y_test, y_pred_svr) # Print the results print(f\\"KRR Mean Squared Error: {mse_krr}\\") print(f\\"KRR Training Time: {krr_train_time}\\") print(f\\"KRR Prediction Time: {krr_predict_time}\\") print(f\\"SVR Mean Squared Error: {mse_svr}\\") print(f\\"SVR Training Time: {svr_train_time}\\") print(f\\"SVR Prediction Time: {svr_predict_time}\\") # Plot the results plt.scatter(X, y, color=\'darkorange\', label=\'data\') plt.plot(X_test, y_pred_krr, color=\'navy\', lw=2, label=\'KRR model\') plt.plot(X_test, y_pred_svr, color=\'c\', lw=2, label=\'SVR model\') plt.xlabel(\'data\') plt.ylabel(\'target\') plt.title(\'Kernel Ridge Regression vs. Support Vector Regression\') plt.legend() plt.show() ``` You do not need to worry about loading the dataset; it will be provided in the required format during assessment.","solution":"import numpy as np import pandas as pd from sklearn.kernel_ridge import KernelRidge from sklearn.svm import SVR from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt import time def load_data(filepath): Load data from a CSV file. data = pd.read_csv(filepath) X = data[\'X\'].values.reshape(-1, 1) y = data[\'y\'].values return X, y def compare_krr_svr(X, y): # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the parameter grid for Kernel Ridge Regression param_grid_krr = {\'alpha\': [0.1, 1, 10], \'kernel\': [\'rbf\'], \'gamma\': [0.1, 1, 10]} # Grid search for Kernel Ridge Regression start_time = time.time() grid_search_krr = GridSearchCV(KernelRidge(), param_grid_krr, cv=5) grid_search_krr.fit(X_train, y_train) krr_train_time = time.time() - start_time # Predict using the best Kernel Ridge Regression model start_time = time.time() y_pred_krr = grid_search_krr.predict(X_test) krr_predict_time = time.time() - start_time # Calculate the Mean Squared Error for Kernel Ridge Regression mse_krr = mean_squared_error(y_test, y_pred_krr) # Define the parameter grid for Support Vector Regression param_grid_svr = {\'C\': [0.1, 1, 10], \'kernel\': [\'rbf\'], \'gamma\': [0.1, 1, 10]} # Grid search for Support Vector Regression start_time = time.time() grid_search_svr = GridSearchCV(SVR(), param_grid_svr, cv=5) grid_search_svr.fit(X_train, y_train) svr_train_time = time.time() - start_time # Predict using the best Support Vector Regression model start_time = time.time() y_pred_svr = grid_search_svr.predict(X_test) svr_predict_time = time.time() - start_time # Calculate the Mean Squared Error for Support Vector Regression mse_svr = mean_squared_error(y_test, y_pred_svr) # Print the results print(f\\"KRR Mean Squared Error: {mse_krr}\\") print(f\\"KRR Training Time: {krr_train_time}\\") print(f\\"KRR Prediction Time: {krr_predict_time}\\") print(f\\"SVR Mean Squared Error: {mse_svr}\\") print(f\\"SVR Training Time: {svr_train_time}\\") print(f\\"SVR Prediction Time: {svr_predict_time}\\") # Plot the results plt.scatter(X, y, color=\'darkorange\', label=\'data\') plt.plot(X_test, y_pred_krr, color=\'navy\', lw=2, label=\'KRR model\') plt.plot(X_test, y_pred_svr, color=\'c\', lw=2, label=\'SVR model\') plt.xlabel(\'data\') plt.ylabel(\'target\') plt.title(\'Kernel Ridge Regression vs. Support Vector Regression\') plt.legend() plt.show()"},{"question":"You are tasked with implementing a function that performs various analyses on a given string using the `unicodedata` module. Your function should return a dictionary with the following information for each unique character in the string: 1. The Unicode name of the character. 2. The decimal value of the character (if applicable). 3. The category of the character. 4. The bidirectional class of the character. 5. The combining class of the character. 6. Whether the character is mirrored in bidirectional text. 7. The East Asian width of the character. # Requirements Implement the function `analyze_string(input_str: str) -> dict` that takes a single argument: - `input_str` (str): The input string containing Unicode characters. The function should return a dictionary where each key is a unique character from the input string, and each value is another dictionary containing the following keys and their corresponding values: - `name`: The Unicode name of the character. - `decimal`: The decimal value of the character, or `None` if not applicable. - `category`: The general category of the character. - `bidirectional`: The bidirectional class of the character. - `combining`: The combining class of the character. - `mirrored`: A boolean indicating if the character is mirrored in bidirectional text. - `east_asian_width`: The East Asian width of the character. # Constraints - Characters without a decimal value should have `None` as the value for the `decimal` key. - You must handle exceptions gracefully, providing meaningful default values when necessary. - The input string will contain at most 1000 characters. # Example Usage ```python import unicodedata def analyze_string(input_str: str) -> dict: result = {} for char in set(input_str): char_data = {} try: char_data[\'name\'] = unicodedata.name(char) except ValueError: char_data[\'name\'] = None try: char_data[\'decimal\'] = unicodedata.decimal(char) except ValueError: char_data[\'decimal\'] = None char_data[\'category\'] = unicodedata.category(char) char_data[\'bidirectional\'] = unicodedata.bidirectional(char) char_data[\'combining\'] = unicodedata.combining(char) char_data[\'mirrored\'] = unicodedata.mirrored(char) == 1 char_data[\'east_asian_width\'] = unicodedata.east_asian_width(char) result[char] = char_data return result # Example input input_str = \\"A9Â©\\" # Example output output = analyze_string(input_str) print(output) # Output should be a dictionary with detailed information for each unique character in the string \'A9Â©\'. ``` # Testing To test the function, consider the following test cases: 1. A string with alphanumeric characters: `\\"Hello123\\"` 2. A string with special characters: `\\"@#Â¢â‚¬\\"` 3. A string with mixed Unicode categories: `\\"AÎ©Â©ðŸ‘¾\\"` This problem requires students to demonstrate their understanding of Unicode character properties and effectively use the `unicodedata` module to analyze and extract relevant information.","solution":"import unicodedata def analyze_string(input_str: str) -> dict: result = {} for char in set(input_str): char_data = {} try: char_data[\'name\'] = unicodedata.name(char) except ValueError: char_data[\'name\'] = None try: char_data[\'decimal\'] = unicodedata.decimal(char) except ValueError: char_data[\'decimal\'] = None char_data[\'category\'] = unicodedata.category(char) char_data[\'bidirectional\'] = unicodedata.bidirectional(char) char_data[\'combining\'] = unicodedata.combining(char) char_data[\'mirrored\'] = unicodedata.mirrored(char) == 1 char_data[\'east_asian_width\'] = unicodedata.east_asian_width(char) result[char] = char_data return result"},{"question":"**Objective**: Write a Python script that uses the `sysconfig` module to gather information about the current Python installation and configuration. This exercise will demonstrate your understanding of the `sysconfig` module and its various functions. **Problem Statement**: Write a Python function `get_python_config_info(scheme_key)` which gathers and returns detailed information about the current Python installation based on the given scheme key. The function should: 1. Verify if the provided `scheme_key` is valid (either \\"prefix\\", \\"home\\", or \\"user\\"). 2. Retrieve the preferred installation scheme name for the given `scheme_key`. 3. Obtain the installation paths for the preferred scheme. 4. Collect the Python version and platform information. 5. Return a dictionary containing all collected details. **Input**: - `scheme_key` (str): A string indicating the layout key (\\"prefix\\", \\"home\\", or \\"user\\"). **Output**: - A dictionary with the following keys and their respective values: - `\\"preferred_scheme\\"`: The preferred installation scheme name for the given key. - `\\"paths\\"`: A dictionary of installation paths for the preferred scheme. - `\\"python_version\\"`: The MAJOR.MINOR Python version number. - `\\"platform\\"`: A string identifying the current platform. **Constraints**: - Use the `sysconfig` module functions to gather the required information. - If an invalid `scheme_key` is provided, raise a `ValueError` with the message `\\"Invalid scheme key\\"`. **Example**: ```python def get_python_config_info(scheme_key): # Your implementation here # Example usage: result = get_python_config_info(\'user\') print(result) ``` The `result` dictionary might look something like this: ```python { \\"preferred_scheme\\": \\"posix_user\\", \\"paths\\": { \\"stdlib\\": \\"/usr/local/lib/python3.10\\", \\"platstdlib\\": \\"/usr/local/lib/python3.10\\", \\"purelib\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"platlib\\": \\"/usr/local/lib/python3.10/site-packages\\", \\"include\\": \\"/usr/local/include/python3.10\\", \\"scripts\\": \\"/usr/local/bin\\", \\"data\\": \\"/usr/local\\" }, \\"python_version\\": \\"3.10\\", \\"platform\\": \\"linux-x86_64\\" } ``` # Notes: - Ensure to handle cases where paths or schemes might not be found by the `sysconfig` module. - Focus on using the provided `sysconfig` functions for querying the necessary information.","solution":"import sysconfig import sys def get_python_config_info(scheme_key): Gathers detailed information about the current Python installation based on the given scheme key. Args: - scheme_key (str): A string indicating the layout key (\\"prefix\\", \\"home\\", or \\"user\\"). Returns: - dict: A dictionary containing detailed Python installation information. Raises: - ValueError: If the scheme_key provided is invalid. # Valid scheme keys valid_scheme_keys = [\\"prefix\\", \\"home\\", \\"user\\"] # Check if the given scheme_key is valid if scheme_key not in valid_scheme_keys: raise ValueError(\\"Invalid scheme key\\") # Retrieve the preferred installation scheme name for the given scheme key preferred_scheme = sysconfig.get_preferred_scheme(scheme_key) # Obtain the installation paths for the preferred scheme paths = sysconfig.get_paths(scheme=preferred_scheme) # Collect Python version and platform info python_version = f\\"{sys.version_info.major}.{sys.version_info.minor}\\" platform = sysconfig.get_platform() # Create result dictionary result = { \\"preferred_scheme\\": preferred_scheme, \\"paths\\": paths, \\"python_version\\": python_version, \\"platform\\": platform } return result"},{"question":"# Pandas Options Configuration Assessment **Objective**: Demonstrate your understanding of configuring pandas\' global settings using its options API. Problem Statement You are provided with a data processing script that involves reading a CSV file and manipulating the resulting DataFrame. Your task is to implement a function `configure_pandas_options` that: 1. Sets the global option to display a maximum of 10 rows for any DataFrame. 2. Configures pandas to use the engineering floating-point format with a precision of 2. 3. Resets all options to their default values after the DataFrame has been displayed. 4. Ensures that these options only apply within the context of executing the given script and are not permanent. The provided CSV file `data.csv` contains unknown amounts of rows and columns with various numeric values. Function Signature ```python def configure_pandas_options(): pass ``` Expected Behavior 1. Read the CSV file. 2. Set pandas options as described above. 3. Display the entire DataFrame. 4. Reset all the pandas options to their defaults. 5. The context of the options should ensure that after the function ends, any further use of pandas should revert to the default settings. Input - A CSV file named `data.csv`. Output - Displayed DataFrame with the custom options applied. Example ```python import pandas as pd def configure_pandas_options(): # Placeholder: Read the CSV file and apply settings # Set display max rows to 10 pd.set_option(\'display.max_rows\', 10) # Set engineering format for float numbers with a precision of 2 pd.set_eng_float_format(accuracy=2) # Read and display the DataFrame df = pd.read_csv(\'data.csv\') print(df) # Reset options to default pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.float_format\') # Assume data.csv exists in the working directory configure_pandas_options() ``` The function should be run in an environment where `data.csv` is present. Ensure the function execution showcases the proper option settings and their resetting functionality. Constraints - The CSV file name is fixed as `data.csv`. - You must use pandas\' options API for setting and resetting the options. - Make sure the options only apply within the context of your function. Notes - This task assumes basic familiarity with reading CSV files using pandas and an understanding of pandas options settings. - The engineering format for floats should ensure numbers are in scientific notation when printed.","solution":"import pandas as pd def configure_pandas_options(): # Set display max rows to 10 pd.set_option(\'display.max_rows\', 10) # Set engineering format for float numbers with a precision of 2 pd.set_eng_float_format(accuracy=2) # Read and display the DataFrame df = pd.read_csv(\'data.csv\') print(df) # Reset options to default pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.float_format\')"},{"question":"**Title**: Extract PyTorch Configuration Information **Objective**: Demonstrate understanding of PyTorch\'s configuration settings and the ability to extract and process this information. **Task**: Write a function `extract_config_info` that utilizes the `torch.__config__` module to extract and display specific PyTorch configuration settings. This function should return a dictionary containing the following information: 1. A list of all configuration options and their current settings. 2. The number of parallel computing threads PyTorch is currently using. 3. A summary string that includes the version of PyTorch and the total number of configuration options. **Expected Function Signature**: ```python def extract_config_info() -> dict: pass ``` **Input**: - No input is required for this function. **Output**: - A dictionary with the following keys: - `config_options`: A dictionary of all configuration options and their current values. - `num_threads`: An integer representing the number of parallel computing threads. - `summary`: A string summarizing the version of PyTorch and the total number of configuration options. **Constraints**: - Ensure that the function performs efficiently with minimal overhead. **Example**: ```python result = extract_config_info() The `result` should be a dictionary, e.g., { \'config_options\': {\'option1\': \'value1\', \'option2\': \'value2\', ...}, \'num_threads\': 4, \'summary\': \'PyTorch version 1.9.0 with 15 configuration options.\' } ``` **Hint**: - Use the `torch.__config__.show()` and `torch.__config__.parallel_info()` functions to gather the required information. - PyTorch version can be accessed through `torch.__version__`.","solution":"import torch def extract_config_info() -> dict: Extracts and displays specific PyTorch configuration settings. Returns: dict: A dictionary containing the configuration options, number of parallel computing threads, and a summary string. # Extract all configuration options and their current settings config_options = {} for line in torch.__config__.show().split(\\"n\\"): if \\": \\" in line: key, value = line.split(\\": \\", 1) config_options[key] = value # Get the number of parallel computing threads num_threads = torch.__config__.parallel_info() # PyTorch version torch_version = torch.__version__ # Create summary summary = f\'PyTorch version {torch_version} with {len(config_options)} configuration options.\' return { \'config_options\': config_options, \'num_threads\': sum(int(val.split(\'=\')[-1]) for val in num_threads.strip().split(\\"n\\") if \'number threads\' in val), \'summary\': summary }"},{"question":"**Coding Assessment Question:** # Asynchronous Task Management and Debugging with `asyncio` Objective: Implement a Python program using the `asyncio` module that demonstrates: - Creation and management of multiple asynchronous tasks. - Handling of exceptions within coroutines. - Debugging of coroutines and tracking slow callbacks. - Proper logging of events and potential errors. Task: You are required to write a Python script that simulates a network polling system. In this system, multiple devices are polled asynchronously, and the results are logged. The script should include the following functionalities: 1. **Asynchronous Polling Function:** - Create an asynchronous function `poll_device(device_id: int, delay: int) -> int` which simulates polling a device by awaiting for a given delay (in seconds) and then returning a result equal to the `device_id` multiplied by 10. If the `delay` exceeds 5 seconds, simulate a timeout by raising an `asyncio.TimeoutError`. 2. **Main Coroutine:** - Create an asynchronous `main()` function that: - Launches multiple `poll_device` tasks concurrently using `asyncio.create_task()`. - Manages these tasks, handling any exceptions raised by the `poll_device` function. - Ensures logging of starting and ending times of each poll, and any exceptions. 3. **Debug Mode and Logging:** - Enable asyncio debug mode. - Configure logging to display debug level messages. - Log the start and end of each async task, as well as any exceptions, and handle potential blocking of the event loop. 4. **Slow Callback Detection:** - Adjust the asyncio loop to log callbacks that take longer than 0.1 seconds. Constraints: - The program must handle at least 5 device polls concurrently. - Use appropriate asyncio methods to run and manage these tasks efficiently. Expected Input and Output: **Example Execution:** ```python import asyncio import logging async def poll_device(device_id: int, delay: int) -> int: # Implementation here pass async def main(): # Implementation here pass if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Notes:** - Ensure to handle and log exceptions properly. - Ensure the program runs without blocking the event loop. - Performance and efficiency of task management and exception handling will be evaluated. Submission: Submit your complete Python script which includes the implementations of the `poll_device` and `main` functions along with the necessary logging and configuration setup.","solution":"import asyncio import logging # Configure logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) async def poll_device(device_id: int, delay: int) -> int: Simulates polling a device by awaiting for a given delay. Returns a result equal to the device_id multiplied by 10. If the delay exceeds 5 seconds, a TimeoutError is raised. logger.debug(f\\"Start polling device {device_id} with delay {delay} seconds\\") try: if delay > 5: raise asyncio.TimeoutError(f\\"Device {device_id} timed out\\") await asyncio.sleep(delay) result = device_id * 10 logger.debug(f\\"Finished polling device {device_id}, result: {result}\\") return result except asyncio.TimeoutError as e: logger.error(f\\"Error polling device {device_id}: {e}\\") raise async def main(): Main coroutine that launches multiple poll_device tasks concurrently. Manages tasks, handles exceptions, and logs the process. devices = [(1, 2), (2, 6), (3, 1), (4, 4), (5, 3)] tasks = [asyncio.create_task(poll_device(device_id, delay)) for device_id, delay in devices] for task in asyncio.as_completed(tasks): try: result = await task logger.info(f\\"Task completed with result: {result}\\") except asyncio.TimeoutError: logger.warning(\\"A task was cancelled with TimeoutError\\") # Enable asyncio debug mode asyncio.get_event_loop().set_debug(True) # Configure asyncio to log slow callbacks taking more than 0.1 seconds loop = asyncio.get_event_loop() loop.slow_callback_duration = 0.1 if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective Demonstrate your understanding of shallow and deep copying in Python by working with the `copy` module. You will implement a custom class with specific behaviors for shallow and deep copying, and verify these behaviors through provided test cases. Problem Statement You are to implement a class `MyCustomList` which mimics the behavior of a list but with custom copying mechanics. The class should support the following functionalities: 1. **Initialization**: Initialize the object with a list of integers. 2. **Addition**: Define a method to add an integer to the list. 3. **Shallow Copy**: Implement a method that returns a shallow copy of the object. 4. **Deep Copy**: Implement a method that returns a deep copy of the object. 5. **Verification**: Ensure that the shallow copy and deep copy functionalities behave as expected through given test cases. Implementation Details 1. Implement the `MyCustomList` class with the following methods: - `__init__(self, data: List[int])`: Initializes the object with a list of integers. - `add(self, value: int)`: Adds an integer to the list. - `__copy__(self)`: Returns a shallow copy of the object. - `__deepcopy__(self, memo)`: Returns a deep copy of the object using the memo dictionary. 2. The `__copy__` method should return a new instance of `MyCustomList` containing references to the same items as the original list. 3. The `__deepcopy__` method should return a new instance of `MyCustomList` with deeply copied items from the original list. Example Usage ```python from copy import copy, deepcopy # Initialize an object and add elements original = MyCustomList([1, 2, 3]) original.add(4) # Perform shallow copy shallow_copied = copy(original) shallow_copied.add(5) # Perform deep copy deep_copied = deepcopy(original) deep_copied.add(6) # Expected result print(\\"Original list:\\", original.data) # Output: Original list: [1, 2, 3, 4] print(\\"Shallow copied list:\\", shallow_copied.data) # Output: Shallow copied list: [1, 2, 3, 4, 5] print(\\"Deep copied list:\\", deep_copied.data) # Output: Deep copied list: [1, 2, 3, 4, 6] ``` Constraints 1. Do not use built-in methods beyond the requirements (i.e., do not use list `copy()` methods directly). 2. The list should contain only integers. Submission Provide the implementation of the `MyCustomList` class as specified. The code should execute as described in the example usage, producing the expected outputs.","solution":"from copy import copy, deepcopy class MyCustomList: def __init__(self, data): self.data = data def add(self, value): self.data.append(value) def __copy__(self): return MyCustomList(self.data) def __deepcopy__(self, memo): return MyCustomList(deepcopy(self.data, memo))"},{"question":"Your task is to write a Python function that leverages the `compileall` module to compile Python source files in a directory tree with the following constraints and requirements: # Requirements: 1. The function should recursively compile all `.py` files in a given directory. 2. It should allow excluding files that match a specified regular expression pattern. 3. It should output only error messages (suppress normal output). 4. It should use all available CPU cores for the compilation process. 5. The function should accept an optional optimization level and default to no optimization if not specified. 6. The function should force a recompilation regardless of existing timestamps. # Function Signature: ```python import compileall def compile_python_sources(directory: str, exclude_pattern: str, optimization_level: int = -1) -> bool: Compiles all .py files in the specified directory tree with the given constraints. Parameters: - directory (str): The root directory to start compiling from. - exclude_pattern (str): A regex pattern to exclude files from compiling. - optimization_level (int, optional): The optimization level for the compiler. Defaults to -1 (no optimization). Returns: - bool: True if all files compiled successfully, False otherwise. pass ``` # Input: - `directory`: A string representing the path to the root directory for compilation. - `exclude_pattern`: A string representing the regex pattern to exclude files. - `optimization_level`: An integer representing the optimization level (optional, default is -1). # Output: - A boolean value indicating if all files compiled successfully (`True`) or if there were any failures (`False`). # Example Usage: ```python result = compile_python_sources(\'my_project/\', r\'.test.py\', 2) print(result) # Expected output: either True or False based on compilation success ``` # Additional Constraints: - Ensure the function handles invalid directory paths gracefully by catching appropriate exceptions and returning `False`. - If `exclude_pattern` is an empty string or `None`, no files should be excluded based on the pattern. This task will test your understanding of the `compileall` module, your ability to handle regex patterns, and your skills in managing optional parameters and exceptions in Python.","solution":"import compileall import re import os import multiprocessing def compile_python_sources(directory: str, exclude_pattern: str, optimization_level: int = -1) -> bool: Compiles all .py files in the specified directory tree with the given constraints. Parameters: - directory (str): The root directory to start compiling from. - exclude_pattern (str): A regex pattern to exclude files from compiling. - optimization_level (int, optional): The optimization level for the compiler. Defaults to -1 (no optimization). Returns: - bool: True if all files compiled successfully, False otherwise. try: if not os.path.exists(directory) or not os.path.isdir(directory): print(f\\"Error: Directory {directory} does not exist or is not a directory.\\") return False if exclude_pattern: exclude_regex = re.compile(exclude_pattern) else: exclude_regex = None def exclude(file): return exclude_regex.match(file) if exclude_regex else False result = compileall.compile_dir( dir=directory, maxlevels=10, # Depth of recursion unlimited ddir=None, force=True, # Compiling all .py files regardless of the timestamp rx=exclude_regex, quiet=2, # Suppresses all output except error messages legacy=False, optimize=optimization_level, workers=multiprocessing.cpu_count() ) return result except Exception as e: print(f\\"Error occurred: {e}\\") return False"},{"question":"You are provided with a list of tuples representing employees in a company, where each tuple contains an employee\'s name, year of joining, and their department in that order. The list of employees is maintained in chronological order based on the year of joining. You are required to implement a function `find_employees(records, year, search_type=\'exact\')` that utilizes the bisect module to perform the following searches: 1. **Search Type: exact** (Default) - Find employees who joined in the specific year provided. 2. **Search Type: before** - Find employees who joined before the specific year. 3. **Search Type: after** - Find employees who joined after the specific year. 4. **Search Type: on_or_before** - Find employees who joined on or before the specific year. 5. **Search Type: on_or_after** - Find employees who joined on or after the specific year. Function Signature ```python def find_employees(records: list, year: int, search_type: str = \'exact\') -> list: Finds employees based on the year of joining in a chronologically sorted list of records. :param records: List of tuples, where each tuple consists of (name, year_of_joining, department). :param year: The year to be used for searching. :param search_type: The type of search (\'exact\', \'before\', \'after\', \'on_or_before\', \'on_or_after\'). :return: List of tuples matching the search criteria. ``` Input and Output Formats - `records`: A list of tuples containing employee details in chronological order, e.g., `[(\'Alice\', 2000, \'HR\'), (\'Bob\', 2001, \'Engineering\'), ...]`. - `year`: An integer representing the year to perform the search. - `search_type`: A string representing the type of search. Defaults to \'exact\'. Returns: - A list of tuples of employees matching the search criteria. Example ```python employees = [ (\'Alice\', 2000, \'HR\'), (\'Bob\', 2001, \'Engineering\'), (\'Charlie\', 2004, \'Marketing\'), (\'David\', 2004, \'IT\'), (\'Eve\', 2005, \'Engineering\') ] print(find_employees(employees, 2004)) # Output: [(\'Charlie\', 2004, \'Marketing\'), (\'David\', 2004, \'IT\')] print(find_employees(employees, 2004, \'before\')) # Output: [(\'Alice\', 2000, \'HR\'), (\'Bob\', 2001, \'Engineering\')] print(find_employees(employees, 2004, \'after\')) # Output: [(\'Eve\', 2005, \'Engineering\')] print(find_employees(employees, 2004, \'on_or_before\')) # Output: [(\'Alice\', 2000, \'HR\'), (\'Bob\', 2001, \'Engineering\'), (\'Charlie\', 2004, \'Marketing\'), (\'David\', 2004, \'IT\')] print(find_employees(employees, 2004, \'on_or_after\')) # Output: [(\'Charlie\', 2004, \'Marketing\'), (\'David\', 2004, \'IT\'), (\'Eve\', 2005, \'Engineering\')] ``` **Constraints:** - The list `records` is always sorted chronologically by the year of joining. - You are encouraged to use the bisect module to perform the searches efficiently. **Performance requirements:** - Optimize the search to run in `O(log n) + O(k)` time complexity, where `n` is the number of records, and `k` is the number of records found matching the criteria.","solution":"from bisect import bisect_left, bisect_right def find_employees(records, year, search_type=\'exact\'): Finds employees based on the year of joining in a chronologically sorted list of records. :param records: List of tuples, where each tuple consists of (name, year_of_joining, department). :param year: The year to be used for searching. :param search_type: The type of search (\'exact\', \'before\', \'after\', \'on_or_before\', \'on_or_after\'). :return: List of tuples matching the search criteria. years = [record[1] for record in records] if search_type == \'exact\': left_index = bisect_left(years, year) right_index = bisect_right(years, year) return records[left_index:right_index] elif search_type == \'before\': right_index = bisect_left(years, year) return records[:right_index] elif search_type == \'after\': left_index = bisect_right(years, year) return records[left_index:] elif search_type == \'on_or_before\': right_index = bisect_right(years, year) return records[:right_index] elif search_type == \'on_or_after\': left_index = bisect_left(years, year) return records[left_index:] else: raise ValueError(\\"Invalid search type\\")"},{"question":"You are required to implement a non-blocking, multithreaded chat server using Python sockets. This server should handle multiple clients simultaneously, allowing them to send messages to the server, which will then broadcast the message to all other connected clients. Each client should receive messages from other clients in real-time. Specifications 1. **Server Setup** - Implement the server to use IPv4 and TCP. - The server should listen on a specified port for incoming client connections. 2. **Client Handling** - Use threading to handle multiple clients simultaneously. - Each client thread should be responsible for receiving messages from that client and broadcasting them to all other connected clients. 3. **Non-blocking I/O** - Use non-blocking sockets to ensure the server efficiently handles multiple client connections without being blocked by any single client. - Use the `select` module to manage sockets and determine which sockets are ready for reading and writing. 4. **Message Broadcasting** - Implement a mechanism to broadcast messages received from one client to all other clients. 5. **Graceful Shutdown** - Ensure the server can gracefully handle the shutdown process, closing all sockets and terminating all threads properly without leaving any resources hanging. Input and Output Format - **Input:** - The server should continuously listen for incoming messages from connected clients. - **Output:** - The server should broadcast the message received from one client to all other connected clients in real-time. Constraints - The server should handle up to 100 simultaneous client connections. - Each message from the client should not exceed 1024 bytes. Example Interaction - Client A sends: \\"Hello, everyone!\\" - Server broadcasts to all clients: \\"Client_A: Hello, everyone!\\" - Client B sends: \\"Hi, Client A!\\" - Server broadcasts to all clients: \\"Client_B: Hi, Client A!\\" Instructions 1. Implement the server in a file named `chat_server.py`. 2. The server should start by running `python chat_server.py <port>`. 3. Ensure your server handles edge cases, such as clients disconnecting abruptly. Hint You may refer to the provided socket programming documentation for understanding how to create, bind, listen, and manage sockets as well as handle non-blocking I/O using `select`.","solution":"import socket import threading from select import select # Dictionary to keep track of client sockets and addresses clients = {} # Broadcasts a message to all clients except the sender def broadcast(message, sender_socket): for client_socket in clients: if client_socket != sender_socket: try: client_socket.send(message) except: client_socket.close() del clients[client_socket] # Handle an individual client connection def handle_client(client_socket, client_address): while True: try: ready_to_read, _, _ = select([client_socket], [], [], 0.5) if ready_to_read: message = client_socket.recv(1024) if not message: break broadcast(message, client_socket) except: break client_socket.close() del clients[client_socket] # Main server function def start_server(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\'0.0.0.0\', port)) server_socket.listen(100) server_socket.setblocking(False) print(f\\"Server started on port {port}\\") while True: try: ready_to_read, _, _ = select([server_socket], [], [], 0.5) # non-blocking select for sock in ready_to_read: client_socket, client_address = server_socket.accept() print(f\\"Client connected from {client_address}\\") clients[client_socket] = client_address threading.Thread(target=handle_client, args=(client_socket, client_address)).start() except KeyboardInterrupt: print(\\"Server shutting down\\") for client_socket in clients: client_socket.close() server_socket.close() break # Code to start the server on a given port if __name__ == \\"__main__\\": import sys if len(sys.argv) != 2: print(\\"Usage: python chat_server.py <port>\\") sys.exit(1) port = int(sys.argv[1]) start_server(port)"},{"question":"**Question: Implement a Thread-Safe Counter Using _thread Module** # Background: In multi-threaded applications, it is paramount to ensure that shared resources are accessed in a thread-safe manner. The `_thread` module in Python provides low-level primitives for working with multiple threads and includes mechanisms for synchronization, such as locks. # Task: You are required to implement a thread-safe counter class `ThreadSafeCounter` using the `_thread` module. This counter should support the following operations: 1. `increment()`: Safely increments the counter by 1. 2. `decrement()`: Safely decrements the counter by 1. 3. `get_value()`: Returns the current value of the counter. Additionally, implement a function `create_threads_and_run` that creates a specified number of threads that increment and decrement the counter, ensuring thread-safe operations using locks. # Class and Function Specifications: `class ThreadSafeCounter` - **Attributes:** - `value` (int): The counter value, initialized to 0. - `lock` (_thread.LockType): A lock object to ensure thread safety. - **Methods:** - `__init__(self)`: Initializes the counter and the lock. - `increment(self)`: Increments the counter value by 1 in a thread-safe manner. - `decrement(self)`: Decrements the counter value by 1 in a thread-safe manner. - `get_value(self) -> int`: Returns the current value of the counter. `def create_threads_and_run(num_threads: int, increments: int, decrements: int) -> int` - **Parameters:** - `num_threads` (int): The number of threads to create. - `increments` (int): The number of times each thread should increment the counter. - `decrements` (int): The number of times each thread should decrement the counter. - **Returns:** - The final value of the counter after all threads have completed their execution. - **Behavior:** 1. Creates `num_threads` threads. 2. Each thread increments the counter `increments` times. 3. Each thread decrements the counter `decrements` times. 4. Ensures that all increments and decrements are done in a thread-safe manner using the lock. 5. Waits for all threads to complete execution and returns the final counter value. # Example Usage: ```python counter = ThreadSafeCounter() threads = create_threads_and_run(num_threads=5, increments=100, decrements=50) print(counter.get_value()) # Expected Result: 250 (since 5*100 - 5*50 = 250) ``` # Constraints: - Assume `num_threads`, `increments`, and `decrements` are positive integers. - Your implementation must be thread-safe.","solution":"import _thread import threading class ThreadSafeCounter: def __init__(self): self.value = 0 self.lock = _thread.allocate_lock() def increment(self): with self.lock: self.value += 1 def decrement(self): with self.lock: self.value -= 1 def get_value(self): with self.lock: return self.value def create_threads_and_run(num_threads, increments, decrements): counter = ThreadSafeCounter() def worker(): for _ in range(increments): counter.increment() for _ in range(decrements): counter.decrement() threads = [] for _ in range(num_threads): thread = threading.Thread(target=worker) threads.append(thread) thread.start() for thread in threads: thread.join() return counter.get_value()"},{"question":"**Question:** Implement a function called `process_data` that takes a list of tuples where each tuple contains a name (string) and a score (integer). The function will: 1. Group the data by the first letter of the name. 2. Within each group, calculate the cumulative score using the `accumulate` function. 3. Filter out groups where the total cumulative score is less than a given threshold (passed as a parameter). 4. Return the groups as a list of tuples, each containing the first letter of the name group, the cumulative scores of the group, and the sum of cumulative scores. **Function Prototype:** ```python import itertools def process_data(data, threshold): pass ``` **Input:** - `data`: A list of tuples, where each tuple consists of a name (string) and a score (integer). Example: `[(\\"Alice\\", 10), (\\"Alex\\", 20), (\\"Bob\\", 15), (\\"Bobby\\", 5)]` - `threshold`: An integer representing the minimum total cumulative score for each group to be included in the result. **Output:** - A list of tuples. Each tuple consists of the first letter of the name group (string), a list of cumulative scores (list of integers), and the sum of the cumulative scores (integer). **Constraints:** - The input `data` list is non-empty. - Each name in `data` starts with an alphabetic character. **Example:** ```python data = [(\\"Alice\\", 10), (\\"Alex\\", 20), (\\"Bob\\", 15), (\\"Bobby\\", 5)] threshold = 30 print(process_data(data, threshold)) # Expected Output: [(\'A\', [10, 30], 40), (\'B\', [15, 20], 35)] ``` **Explanation:** 1. Group `data` as: - \'A\': [(\\"Alice\\", 10), (\\"Alex\\", 20)] - \'B\': [(\\"Bob\\", 15), (\\"Bobby\\", 5)] 2. Calculate cumulative scores within each group: - \'A\': [10, 30] - \'B\': [15, 20] 3. Calculate sums of cumulative scores: - \'A\': 40 - \'B\': 35 4. Filter out groups based on the threshold of 30. Both groups pass the threshold. 5. Return the final list: - [(\'A\', [10, 30], 40), (\'B\', [15, 20], 35)] **Notes:** - Use `itertools.groupby` to group data by the first letter of the name. - Use `itertools.accumulate` to calculate cumulative scores. - Make sure to include necessary imports at the beginning. **Additional Information:** You can refer to Python\'s official documentation for itertools to understand the specific functions and their usage.","solution":"import itertools from collections import defaultdict from itertools import accumulate def process_data(data, threshold): # Step 1: Group by the first letter of name groups = defaultdict(list) for name, score in data: groups[name[0]].append((name, score)) # Step 2: Calculate cumulative scores in each group result = [] for letter, items in groups.items(): scores = [score for name, score in items] cumulative_scores = list(accumulate(scores)) total_cumulative_score = sum(cumulative_scores) # Step 3: Filter groups by threshold if total_cumulative_score >= threshold: result.append((letter, cumulative_scores, total_cumulative_score)) return result"},{"question":"You are provided with a dataset that contains two interleaving half circles, which can be loaded using `sklearn.datasets.make_moons`. Your task is to cluster this dataset using both Gaussian Mixture Models (GMM) and Bayesian Gaussian Mixture Models (VBGMM) provided by `sklearn.mixture`. Follow the steps below: 1. Load the dataset. 2. Fit a `GaussianMixture` model with the following specifications: - Use 3 different covariance types: spherical, diagonal, and full. - Initialize with the k-means method. - Determine the best number of components using the Bayesian Information Criterion (BIC). 3. Fit a `BayesianGaussianMixture` model with the following specifications: - Use a Dirichlet process for the `weight_concentration_prior_type`. - Set `n_components` to 10 and `weight_concentration_prior` values to 0.1 and 1.0. - Compare the effective number of components used with the GaussianMixture model. 4. Plot and compare the clustering results for each model. 5. Write a brief summary comparing the results from GMM and VBGMM. Discuss which model performed better and why, supported by the BIC and the resulting clusters. **Input:** - None for the function. The task is to implement the logic directly. **Output:** - The implementation should: - Print the BIC scores for different covariance types in GMM. - Print the effective number of components used by the VBGMM for both concentration prior values. - Display plots showing the clustering results for each configuration. **Constraints:** - Utilize the `sklearn.mixture.GaussianMixture` and `sklearn.mixture.BayesianGaussianMixture` classes. - Ensure reproducibility by setting random seed wherever applicable. **Performance Requirements:** - The solution should be able to process the dataset efficiently and produce results within a reasonable time frame, using default scikit-learn settings for algorithms. ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_moons from sklearn.mixture import GaussianMixture, BayesianGaussianMixture # Step 1: Load the dataset X, _ = make_moons(n_samples=300, noise=0.05, random_state=42) # Step 2: Implement GMM with different covariance types and BIC evaluation def fit_gmm_and_evaluate_bic(X): covariance_types = [\'spherical\', \'diag\', \'full\'] bics = [] for cov_type in covariance_types: gmm = GaussianMixture(n_components=2, covariance_type=cov_type, random_state=42, init_params=\'kmeans\') gmm.fit(X) bic = gmm.bic(X) bics.append((cov_type, bic)) plt.scatter(X[:, 0], X[:, 1], c=gmm.predict(X), s=10, cmap=\'viridis\') plt.title(f\'GMM with {cov_type} covariance\') plt.show() return bics # Fit and evaluate GMM gmm_bics = fit_gmm_and_evaluate_bic(X) print(\\"BIC scores for different covariance types (GMM):\\", gmm_bics) # Step 3: Implement Bayesian GMM with Dirichlet process def fit_vbgmm(X, weight_prior): vbgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior=weight_prior, weight_concentration_prior_type=\'dirichlet_process\', random_state=42) vbgmm.fit(X) plt.scatter(X[:, 0], X[:, 1], c=vbgmm.predict(X), s=10, cmap=\'viridis\') plt.title(f\'VBGMM with weight_prior={weight_prior}\') plt.show() effective_components = np.sum(vbgmm.weights_ > 1e-2) return effective_components # Fit and evaluate Bayesian GMM effect_comps_prior_01 = fit_vbgmm(X, weight_prior=0.1) effect_comps_prior_1 = fit_vbgmm(X, weight_prior=1.0) print(\\"Effective components with weight_concentration_prior=0.1:\\", effect_comps_prior_01) print(\\"Effective components with weight_concentration_prior=1.0:\\", effect_comps_prior_1) # Step 4: Plotting is already included in the functions above # Step 5: Summary summary = Based on the BIC scores for different covariance types in GMM, we can determine the most suitable model configuration. The Bayesian Gaussian Mixture Model, with weight concentration prior of 0.1 and 1.0, demonstrates how the effective number of components changes based on prior concentration. The overall comparison indicates which model (GMM or VBGMM) is more effective for this dataset clustering. print(summary) ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_moons from sklearn.mixture import GaussianMixture, BayesianGaussianMixture # Step 1: Load the dataset X, _ = make_moons(n_samples=300, noise=0.05, random_state=42) # Step 2: Implement GMM with different covariance types and BIC evaluation def fit_gmm_and_evaluate_bic(X): covariance_types = [\'spherical\', \'diag\', \'full\'] bics = [] for cov_type in covariance_types: gmm = GaussianMixture(n_components=2, covariance_type=cov_type, random_state=42, init_params=\'kmeans\') gmm.fit(X) bic = gmm.bic(X) bics.append((cov_type, bic)) plt.scatter(X[:, 0], X[:, 1], c=gmm.predict(X), s=10, cmap=\'viridis\') plt.title(f\'GMM with {cov_type} covariance\') plt.show() return bics # Fit and evaluate GMM gmm_bics = fit_gmm_and_evaluate_bic(X) print(\\"BIC scores for different covariance types (GMM):\\", gmm_bics) # Step 3: Implement Bayesian GMM with Dirichlet process def fit_vbgmm(X, weight_prior): vbgmm = BayesianGaussianMixture(n_components=10, weight_concentration_prior=weight_prior, weight_concentration_prior_type=\'dirichlet_process\', random_state=42) vbgmm.fit(X) plt.scatter(X[:, 0], X[:, 1], c=vbgmm.predict(X), s=10, cmap=\'viridis\') plt.title(f\'VBGMM with weight_prior={weight_prior}\') plt.show() effective_components = np.sum(vbgmm.weights_ > 1e-2) return effective_components # Fit and evaluate Bayesian GMM effect_comps_prior_01 = fit_vbgmm(X, weight_prior=0.1) effect_comps_prior_1 = fit_vbgmm(X, weight_prior=1.0) print(\\"Effective components with weight_concentration_prior=0.1:\\", effect_comps_prior_01) print(\\"Effective components with weight_concentration_prior=1.0:\\", effect_comps_prior_1) # Step 4: Plotting is already included in the functions above # Step 5: Summary summary = Based on the BIC scores for different covariance types in GMM, we can determine the most suitable model configuration. The Bayesian Gaussian Mixture Model, with weight concentration prior of 0.1 and 1.0, demonstrates how the effective number of components changes based on prior concentration. The overall comparison indicates which model (GMM or VBGMM) is more effective for this dataset clustering. print(summary)"},{"question":"# Coding Assessment: Advanced Manipulation with Named Tensors Objective The goal of this assessment is to evaluate your understanding and ability to work with Named Tensors in PyTorch, including creating named tensors, manipulating their dimensions, and ensuring correct name propagation through operations. Problem Statement You are given a batch of images with named dimensions and multiple transformation tasks to perform. Implement a function `transform_images` that performs the following steps: 1. Create a batch of images of shape (batch_size, channels, height, width) with random values, and name the dimensions as (\'N\', \'C\', \'H\', \'W\'). 2. Flatten the \'H\' and \'W\' dimensions into a single dimension named \'features\'. 3. Rename the dimension \'features\' to \'spatial_features\'. 4. Align the dimensions so that the order is (\'N\', \'spatial_features\', \'C\'). 5. Return the resulting tensor. Function Signature ```python import torch def transform_images(batch_size: int, channels: int, height: int, width: int) -> torch.Tensor: pass ``` Input - `batch_size` (int): The number of images in the batch. - `channels` (int): The number of color channels in each image. - `height` (int): The height of each image. - `width` (int): The width of each image. Output - Returns a `torch.Tensor` with named dimensions (\'N\', \'spatial_features\', \'C\') and appropriate shape after transformations. Constraints 1. All input dimensions (`batch_size`, `channels`, `height`, `width`) are positive integers. Example ```python tensor = transform_images(batch_size=32, channels=3, height=128, width=128) print(tensor.names) # Expected output: (\'N\', \'spatial_features\', \'C\') print(tensor.shape) # Expected output: torch.Size([32, 16384, 3]) # 128 * 128 = 16384 ``` Performance Requirements - The function should efficiently handle large tensors with batch sizes and dimensions in the hundreds.","solution":"import torch def transform_images(batch_size: int, channels: int, height: int, width: int) -> torch.Tensor: Transforms a batch of images as per the given steps: 1. Create a batch of images with named dimensions (\'N\', \'C\', \'H\', \'W\'). 2. Flatten \'H\' and \'W\' dimensions into a single dimension named \'features\'. 3. Rename \'features\' to \'spatial_features\'. 4. Align dimensions to (\'N\', \'spatial_features\', \'C\'). # Step 1: Create a batch of images with named dimensions tensor = torch.rand(batch_size, channels, height, width, names=(\'N\', \'C\', \'H\', \'W\')) # Step 2: Flatten \'H\' and \'W\' dimensions into \'features\' tensor = tensor.flatten([\'H\', \'W\'], \'features\') # Step 3: Rename \'features\' to \'spatial_features\' tensor = tensor.rename(features=\'spatial_features\') # Step 4: Align dimensions to (\'N\', \'spatial_features\', \'C\') tensor = tensor.align_to(\'N\', \'spatial_features\', \'C\') return tensor"},{"question":"Coding Assessment Question # Objective Your task is to write a function that receives a version string of Python and returns its encoded 32-bit hexadecimal number. Additionally, you need to write another function that can decode such a hexadecimal number back into a human-readable version string. # Input and Output Formats Function 1: `encode_version(version_string)` - **Input:** A version string, formatted as `\\"MAJOR.MINOR.MICRO[LEVEL][SERIAL]\\"`, where: - `MAJOR`, `MINOR`, and `MICRO` are integers. - `[LEVEL]` is optional and can be one of the following single characters: `a` (alpha), `b` (beta), `c` (release candidate), or `f` (final). - `[SERIAL]` is optional and is an integer (defaults to zero if not provided). - **Output:** An integer representing the encoded 32-bit hexadecimal version number. Function 2: `decode_version(hex_version)` - **Input:** An integer representing the encoded 32-bit hexadecimal version number. - **Output:** A string representing the decoded Python version in the format `\\"MAJOR.MINOR.MICRO[LEVEL][SERIAL]\\"`. # Constraints or Limitations - Major, Minor, and Micro versions should be between 0 and 255. - Release levels only include \\"a\\" (alpha), \\"b\\" (beta), \\"c\\" (candidate), and \\"f\\" (final). - If the release level is \\"f\\" (final), the release serial should be zero or can be omitted. # Example ```python # Example usage encoded_version = encode_version(\\"3.4.1a2\\") print(encoded_version) # Output: 0x030401a2 decoded_version = decode_version(0x030401a2) print(decoded_version) # Output: \\"3.4.1a2\\" ``` # Detailed Steps for Encoding 1. Parse the `version_string`. 2. Extract `MAJOR`, `MINOR`, `MICRO`, `LEVEL`, and `SERIAL`. 3. Convert `LEVEL` to its corresponding hexadecimal value. 4. Combine these parts into a single integer following the provided bit structure. # Detailed Steps for Decoding 1. Extract the different parts from the 32-bit integer. 2. Convert `LEVEL` back to its character form. 3. Construct the version string using the extracted and converted parts. Implement these two functions to demonstrate your understanding of reading, manipulating, and generating structured version information in Python.","solution":"def encode_version(version_string): Encodes a version string into a 32-bit hexadecimal number. levels = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF} parts = version_string.split(\\".\\") major = int(parts[0]) minor = int(parts[1]) micro_part = parts[2] micro = \'\' level = \'f\' serial = 0 for ch in micro_part: if ch.isdigit(): micro += ch else: micro = int(micro) level = ch serial = int(micro_part[micro_part.index(ch)+1:]) break else: micro = int(micro) level_value = levels[level] encoded = (major << 24) | (minor << 16) | (micro << 8) | (level_value << 4) | serial return encoded def decode_version(hex_version): Decodes a 32-bit hexadecimal version number into a version string. levels = {0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\'} major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF level_value = (hex_version >> 4) & 0xF serial = hex_version & 0xF level = levels[level_value] if level == \'f\' and serial == 0: version_string = f\\"{major}.{minor}.{micro}\\" else: version_string = f\\"{major}.{minor}.{micro}{level}{serial}\\" return version_string"},{"question":"You have been given the task to parse an XML document containing a library catalog. The catalog consists of different books, with each book having a title, author, genre, and price. The aim is to implement a SAX parser that processes the XML document and extracts the details of each book to store them in a list of dictionaries. # Task: 1. Implement a Python function `parse_library_catalog(xml_string: str) -> List[Dict[str, str]]` that takes an XML string as input and parses it using the `xml.sax` module. 2. The function should return a list of dictionaries, where each dictionary represents a book with keys: \\"title\\", \\"author\\", \\"genre\\", and \\"price\\". # Input: - `xml_string`: A string containing the XML data representing the library catalog. # Output: - A list of dictionaries. Each dictionary will have the keys: `\\"title\\"`, `\\"author\\"`, `\\"genre\\"`, and `\\"price\\"`, with corresponding values extracted from the XML. # Example: ```python xml_data = <catalog> <book> <title>Python Programming</title> <author>John Doe</author> <genre>Programming</genre> <price>29.99</price> </book> <book> <title>Learning XML</title> <author>Jane Smith</author> <genre>Technology</genre> <price>39.95</price> </book> </catalog> output = parse_library_catalog(xml_data) print(output) ``` Expected output: ```python [ { \\"title\\": \\"Python Programming\\", \\"author\\": \\"John Doe\\", \\"genre\\": \\"Programming\\", \\"price\\": \\"29.99\\" }, { \\"title\\": \\"Learning XML\\", \\"author\\": \\"Jane Smith\\", \\"genre\\": \\"Technology\\", \\"price\\": \\"39.95\\" } ] ``` # Constraints: 1. You may assume that the XML data provided is well-formed. 2. You should use the `xml.sax` module for parsing the XML data. 3. Handle any parsing errors appropriately and raise a `ValueError` with a meaningful message if parsing fails. # Notes: - Remember to create a custom handler by subclassing `xml.sax.ContentHandler`. - Ensure that you process the start and end XML elements correctly and handle character data within XML tags.","solution":"import xml.sax from typing import List, Dict class LibraryCatalogHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.books = [] self.book = {} def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.book = {} def endElement(self, tag): if tag == \\"book\\": self.books.append(self.book) self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"title\\": self.book[\\"title\\"] = content elif self.current_data == \\"author\\": self.book[\\"author\\"] = content elif self.current_data == \\"genre\\": self.book[\\"genre\\"] = content elif self.current_data == \\"price\\": self.book[\\"price\\"] = content def parse_library_catalog(xml_string: str) -> List[Dict[str, str]]: handler = LibraryCatalogHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: raise ValueError(f\\"Failed to parse XML: {e}\\") return handler.books"},{"question":"**Welcome to the Unix System Administration Assessment** # Problem Statement You are provided with the `spwd` module documentation, which allows access to the Unix shadow password database. Your task is to write a function to generate a report of users whose passwords will expire within the next `N` days. # Function Signature ```python def users_with_expiring_passwords(N: int) -> List[str]: pass ``` # Description **Input:** - `N` (integer): The number of days from today within which passwords should be checked for expiration. **Output:** - A list of login names (strings) of users whose passwords will expire within the next `N` days. # Details and Constraints: - The function should access the shadow password database using the `spwd` module. - It should compute the number of days remaining until each user\'s password expires using the current date. - If a user\'s password is set to expire within `N` days, include their login name in the output list. - You must handle cases where the current user does not have the required privileges to access the shadow password database (raise a `PermissionError`). - You may assume that the system you are running on is a Unix-based system and has the `spwd` module available. # Example: Example 1 ```python # Assuming today is 2023-10-01 (Epoch days: 19621) N = 5 result = users_with_expiring_passwords(N) print(result) ``` Output: ```plaintext [\'user1\', \'user2\'] ``` Here, \'user1\' and \'user2\' have passwords that will expire on 2023-10-03 and 2023-10-05 respectively, which falls within the next 5 days. Notes: - You may use the `datetime` module to handle the date calculations. - Ensure the script runs securely by handling permission-related errors gracefully. **Good luck!**","solution":"import spwd import datetime def users_with_expiring_passwords(N: int): Returns a list of login names of users whose passwords will expire within the next N days. today = datetime.datetime.now().date() expiring_users = [] try: shadow_entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Current user does not have required privileges to access shadow password database.\\") for entry in shadow_entries: # Calculate the expiry date in days since epoch last_change = entry.sp_lstchg max_days = entry.sp_max if max_days == -1: continue # No expiry for this password expiry_date = datetime.date.fromordinal(last_change + max_days) remaining_days = (expiry_date - today).days if 0 <= remaining_days <= N: expiring_users.append(entry.sp_namp) return expiring_users"},{"question":"# Problem: Log Processor You are tasked with creating a Python function that reads a log file, extracts information from each line, formats this information neatly, and writes the formatted information to a new file. Additionally, you will serialize the formatted data into a JSON file. Input: - A log file (`logs.txt`) where each line represents a log entry in the format `timestamp, user_id, action`. Here, `timestamp` is a string, `user_id` is an integer, and `action` is a string. Output: - A text file (`formatted_logs.txt`) containing each log entry formatted in a human-readable way. - A JSON file (`logs.json`) containing the structured data of log entries. # Requirements: 1. **Formatted String Literals**: Use f-strings to format each log entry into the following format: ``` [Timestamp] User ID: {user_id} performed action: {action} ``` 2. **Reading and Writing Files**: Read from the input file and write to the output files using appropriate file handling methods. 3. **JSON Serialization**: Serialize the list of log entries (each as a dictionary) to a JSON file. 4. **Error Handling**: Properly handle possible exceptions, such as file not found. # Specifications: - Implement the function `process_logs(input_file: str, text_output_file: str, json_output_file: str) -> None`. - The input and output files are specified by their filenames as string arguments to the function. - Assume the input file `logs.txt` contains log entries separated by new lines. # Example: Suppose the `logs.txt` file contains: ``` 2023-01-01T12:00:00, 123, login 2023-01-01T12:05:00, 456, logout 2023-01-01T12:10:00, 123, upload ``` Your function should create two output files: `formatted_logs.txt` and `logs.json`. **`formatted_logs.txt` should contain:** ``` [2023-01-01T12:00:00] User ID: 123 performed action: login [2023-01-01T12:05:00] User ID: 456 performed action: logout [2023-01-01T12:10:00] User ID: 123 performed action: upload ``` **`logs.json` should contain:** ```json [ {\\"timestamp\\": \\"2023-01-01T12:00:00\\", \\"user_id\\": 123, \\"action\\": \\"login\\"}, {\\"timestamp\\": \\"2023-01-01T12:05:00\\", \\"user_id\\": 456, \\"action\\": \\"logout\\"}, {\\"timestamp\\": \\"2023-01-01T12:10:00\\", \\"user_id\\": 123, \\"action\\": \\"upload\\"} ] ``` # Notes: - Use the `open` function with appropriate modes to handle reading and writing files. - Use `f-strings` for formatting the log entries. - Utilize the `json` module for serialization. - Include appropriate exception handling for file operations.","solution":"import json def process_logs(input_file: str, text_output_file: str, json_output_file: str) -> None: try: # Read input file with open(input_file, \'r\') as file: lines = file.readlines() logs = [] formatted_lines = [] for line in lines: # Split line into components line = line.strip() if not line: continue # Skip empty lines timestamp, user_id, action = line.split(\', \') user_id = int(user_id) # Create a dictionary for each log entry log_entry = {\\"timestamp\\": timestamp, \\"user_id\\": user_id, \\"action\\": action} logs.append(log_entry) # Format line formatted_line = f\\"[{timestamp}] User ID: {user_id} performed action: {action}\\" formatted_lines.append(formatted_line) # Write formatted logs to text output file with open(text_output_file, \'w\') as file: for line in formatted_lines: file.write(line + \'n\') # Serialize logs to JSON output file with open(json_output_file, \'w\') as file: json.dump(logs, file, indent=4) except FileNotFoundError: print(f\\"Error: The file {input_file} was not found.\\") except Exception as e: print(f\\"Error: An error occurred - {str(e)}\\")"},{"question":"You are required to implement a function `simulate_async_operations` that simulates various asynchronous operations and handles specific `asyncio` exceptions. Your function should: 1. **Simulate** asynchronous operations that might raise the following `asyncio` exceptions: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` 2. **Handle** these exceptions appropriately, demonstrating custom handling for each exception. 3. **Log** a custom message for each exception caught, indicating which exception was raised. Function Signature ```python import asyncio async def simulate_async_operations() -> str: pass ``` # Requirements 1. **Simulate** scenarios where each of the specified exceptions would be raised. You may create mock functions or coroutines that deliberately raise these exceptions. 2. **Handle** each exception using `try...except` blocks. 3. **Log** a message indicating the exception. For example, \\"Caught asyncio.TimeoutError\\". # Example Output The function does not return any specific outputs but prints/logs messages indicating which exceptions were raised and caught during its execution. Example messages might include: ``` Caught asyncio.TimeoutError Caught asyncio.CancelledError Caught asyncio.InvalidStateError Caught asyncio.SendfileNotAvailableError Caught asyncio.IncompleteReadError Caught asyncio.LimitOverrunError ``` You are required to write all necessary code to simulate the raising of these exceptions and to handle them in the `simulate_async_operations` function. # Constraints - You must only use Python\'s standard `asyncio` library. - You should ensure to handle exceptions specific to asyncio and not confuse them with similar built-in Python exceptions. **Your implementation should demonstrate a solid understanding of asynchronous exception handling in Python\'s asyncio library.**","solution":"import asyncio async def simulate_async_operations(): try: raise asyncio.TimeoutError() except asyncio.TimeoutError: print(\\"Caught asyncio.TimeoutError\\") try: raise asyncio.CancelledError() except asyncio.CancelledError: print(\\"Caught asyncio.CancelledError\\") try: raise asyncio.InvalidStateError() except asyncio.InvalidStateError: print(\\"Caught asyncio.InvalidStateError\\") try: raise asyncio.SendfileNotAvailableError() except asyncio.SendfileNotAvailableError: print(\\"Caught asyncio.SendfileNotAvailableError\\") try: raise asyncio.IncompleteReadError(partial=b\'hello\', expected=10) except asyncio.IncompleteReadError: print(\\"Caught asyncio.IncompleteReadError\\") try: raise asyncio.LimitOverrunError(\'Reached limit\', 5) except asyncio.LimitOverrunError: print(\\"Caught asyncio.LimitOverrunError\\") # Example to run the function directly # asyncio.run(simulate_async_operations())"},{"question":"**Question:** You are working on a Python project that includes a function `process_data` in a module named `data_processor`. This function reads data from a file, processes it using an external API, and saves the result into a database. Since you want to test this function without depending on the actual file system, external API, and database, you decide to use the `unittest.mock` library to create mocks for file operations, the API, and the database connections. The `process_data` function is defined as follows: ```python def process_data(file_path, api_client, db_connection): with open(file_path, \'r\') as file: data = file.read() processed_data = api_client.process(data) db_connection.save(processed_data) ``` **Specifications:** 1. Write a test class `TestProcessData` using the `unittest` framework to test the `process_data` function. 2. Use `unittest.mock` to mock the following: - Open file operation to simulate reading data from the file. - An API client with a `process` method that processes the data. - A database connection with a `save` method that saves the processed data. 3. Ensure the following in your tests: - The file is opened with the correct path and mode. - The `process` method of the API client is called with the correct data read from the file. - The `save` method of the database connection is called with the correctly processed data. 4. Mock objects should be properly named to facilitate understanding of the test logs. **Input:** - `file_path` (str): Path to the file to be read. - `api_client` (object): An object with a method `process` for processing data. - `db_connection` (object): An object with a method `save` for saving processed data. **Output:** Your test class `TestProcessData` with appropriate test methods. Example usage of the tests: ```python if __name__ == \\"__main__\\": unittest.main() ``` **Constraints:** - Assume `api_client.process` returns the processed data as a string. - Assume the file reading and writing operations involve working with text data. ```python import unittest from unittest.mock import MagicMock, patch class TestProcessData(unittest.TestCase): @patch(\'builtins.open\', new_callable=MagicMock) @patch(\'data_processor.api_client\') @patch(\'data_processor.db_connection\') def test_process_data(self, mock_db_connection, mock_api_client, mock_open): # Arrange file_path = \'dummy_path.txt\' expected_file_content = \'raw data\' expected_processed_data = \'processed data\' mock_file = MagicMock() mock_file.read.return_value = expected_file_content mock_open.return_value = mock_file mock_api_client.process.return_value = expected_processed_data # Act import data_processor data_processor.process_data(file_path, mock_api_client, mock_db_connection) # Assert mock_open.assert_called_once_with(file_path, \'r\') mock_file.read.assert_called_once() mock_api_client.process.assert_called_once_with(expected_file_content) mock_db_connection.save.assert_called_once_with(expected_processed_data) ``` **Note:** Make sure to handle import paths and naming appropriately in your code.","solution":"import unittest from unittest.mock import MagicMock, patch def process_data(file_path, api_client, db_connection): with open(file_path, \'r\') as file: data = file.read() processed_data = api_client.process(data) db_connection.save(processed_data)"},{"question":"# Advanced Compression and Decompression with Python `zlib` Objective Implement a Python script that demonstrates efficient compression and decompression of data, making appropriate use of `zlib`\'s advanced features. Task 1. **Compression:** - Write a function `compress_advanced(data: bytes, compression_level: int = -1) -> bytes` that takes a byte string `data` and an optional `compression_level` (default is `-1`). - The function should compress `data` using the specified compression level and return the compressed byte string. 2. **Decompression:** - Write a function `decompress_advanced(data: bytes) -> bytes` that takes a compressed byte string `data` and returns the decompressed byte string. 3. **Checksum Computation:** - Write a function `compute_checksums(data: bytes) -> dict` that computes and returns a dictionary containing the `adler32` and `crc32` checksums of the input `data`. Requirements - Use `zlib.compress` and `zlib.decompress` methods. - Ensure your functions handle edge cases, such as empty input and invalid compression data. - Add meaningful docstrings to your functions. Example Usage: ```python original_data = b\'This is a test string for compression.\' compressed_data = compress_advanced(original_data, compression_level=9) print(f\\"Compressed Data: {compressed_data}\\") decompressed_data = decompress_advanced(compressed_data) print(f\\"Decompressed Data: {decompressed_data.decode(\'utf-8\')}\\") checksums = compute_checksums(original_data) print(f\\"Adler32 Checksum: {checksums[\'adler32\']}\\") print(f\\"CRC32 Checksum: {checksums[\'crc32\']}\\") ``` Input and Output Formats - **compress_advanced:** - **Input:** - `data`: A byte string to be compressed. - `compression_level` (optional): An integer between -1 to 9 specifying the compression level. - **Output:** - A compressed byte string. - **decompress_advanced:** - **Input:** - `data`: A compressed byte string. - **Output:** - A decompressed byte string. - **compute_checksums:** - **Input:** - `data`: A byte string for which checksums need to be calculated. - **Output:** - A dictionary with keys `adler32` and `crc32`, and their respective unsigned 32-bit integer checksum values. Notes - Handle zlib errors gracefully, making sure to include error checking and exception handling. - Ensure the decompression correctly reconstructs the original data.","solution":"import zlib def compress_advanced(data: bytes, compression_level: int = -1) -> bytes: Compresses the given byte string using zlib with the specified compression level. Args: data (bytes): The data to be compressed. compression_level (int): The compression level (default is -1, which means the default compression level). Returns: bytes: The compressed data. if not isinstance(data, bytes): raise TypeError(\\"data must be of type bytes\\") return zlib.compress(data, level=compression_level) def decompress_advanced(data: bytes) -> bytes: Decompresses the given compressed byte string using zlib. Args: data (bytes): The compressed data to be decompressed. Returns: bytes: The decompressed data. if not isinstance(data, bytes): raise TypeError(\\"data must be of type bytes\\") try: return zlib.decompress(data) except zlib.error as e: raise ValueError(f\\"Decompression failed: {e}\\") def compute_checksums(data: bytes) -> dict: Computes the Adler-32 and CRC-32 checksums of the given byte string. Args: data (bytes): The data for which checksums need to be calculated. Returns: dict: A dictionary with \'adler32\' and \'crc32\' keys and their corresponding checksum values. if not isinstance(data, bytes): raise TypeError(\\"data must be of type bytes\\") return { \'adler32\': zlib.adler32(data) & 0xffffffff, \'crc32\': zlib.crc32(data) & 0xffffffff }"},{"question":"Task You need to create a Python function using the `email.mime` module to generate a MIME email message. The function should be capable of constructing a multipart email message that includes: 1. A plain text part. 2. An HTML part. 3. One or more attached images. 4. A file attachment (any type). Function Signature ```python def create_mime_email( plain_text: str, html_text: str, image_files: List[str], attachment_file: str ) -> email.mime.multipart.MIMEMultipart: pass ``` # Input * `plain_text` (str): The plain text body of the email. * `html_text` (str): The HTML text body of the email. * `image_files` (List[str]): A list of file paths to the image files to be attached. * `attachment_file` (str): The file path to the file to be attached. # Output * Should return a `MIMEMultipart` object that represents the constructed email. # Constraints * All image files in `image_files` should be valid images (assume image format is supported by the `imghdr` module). * `attachment_file` should be a valid file path. * The email should comply with MIME standards and include appropriate headers. # Example ```python plain_text = \\"This is a plain text message.\\" html_text = \\"<html><body><h1>This is an HTML message.</h1></body></html>\\" image_files = [\\"path/to/image1.jpg\\", \\"path/to/image2.png\\"] attachment_file = \\"path/to/attachment.pdf\\" email_message = create_mime_email(plain_text, html_text, image_files, attachment_file) ``` # Notes 1. Make sure to use appropriate MIME classes like `MIMEText`, `MIMEImage`, and `MIMEApplication`. 2. Ensure the `Content-Type` headers are properly set for each MIME part. 3. Use base64 encoding for attachments and images. 4. You may assume that all paths provided are valid and accessible.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication import os import mimetypes def create_mime_email(plain_text, html_text, image_files, attachment_file): # Create the root message msg = MIMEMultipart(\'mixed\') msg[\'Subject\'] = \'Your Subject Here\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' # Create the alternative (plain text and HTML) part alternative = MIMEMultipart(\'alternative\') # Add plain text part plain_text_part = MIMEText(plain_text, \'plain\') alternative.attach(plain_text_part) # Add HTML part html_part = MIMEText(html_text, \'html\') alternative.attach(html_part) # Attach the alternative part to the root message msg.attach(alternative) # Attach images for image_file in image_files: with open(image_file, \'rb\') as img: mime_type, _ = mimetypes.guess_type(image_file) main_type, sub_type = mime_type.split(\'/\') image_part = MIMEImage(img.read(), _subtype=sub_type) image_part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{os.path.basename(image_file)}\\"\') msg.attach(image_part) # Attach the additional file with open(attachment_file, \'rb\') as file: mime_type, _ = mimetypes.guess_type(attachment_file) main_type, sub_type = mime_type.split(\'/\') file_part = MIMEApplication(file.read(), _subtype=sub_type) file_part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{os.path.basename(attachment_file)}\\"\') msg.attach(file_part) return msg"},{"question":"**Python Version Encoding** In this exercise, you will write a function that parses a version string and outputs its hexadecimal representation according to the CPython versioning scheme as described. # Task Create a function `version_to_hex(version: str) -> str` that takes a version string as input and converts it into the corresponding hexadecimal format. The version string will be in the form `X.Y.Z[levelN]`, where: - `X` is the major version (0 â‰¤ X â‰¤ 255) - `Y` is the minor version (0 â‰¤ Y â‰¤ 255) - `Z` is the micro version (0 â‰¤ Z â‰¤ 255) - `level` is one of `a`, `b`, `c`, or `f` (for alpha, beta, release candidate, and final, respectively) - `N` is the release serial (0 â‰¤ N â‰¤ 15) # Input - `version`: A string representing the version, such as \\"3.10.0f0\\" or \\"3.4.1a2\\". # Output - A string representing the hexadecimal encoding of the version, such as \\"0x030a00f0\\" for \\"3.10.0f0\\" or \\"0x030401a2\\" for \\"3.4.1a2\\". # Constraints - The input version string will always be well-formed as per the descriptions above. - The function should raise a `ValueError` if any component of the version is out of its valid range. # Example ```python def version_to_hex(version: str) -> str: # Implement your solution here print(version_to_hex(\\"3.10.0f0\\")) # Output: \\"0x030a00f0\\" print(version_to_hex(\\"3.4.1a2\\")) # Output: \\"0x030401a2\\" ``` # Note - Use bit manipulation to construct the hexadecimal encoding. - Ensure your solution is efficient and well-documented with comments explaining key parts of your implementation.","solution":"def version_to_hex(version: str) -> str: Converts a version string of the form X.Y.Z[levelN] to its corresponding hexadecimal format. :param version: The version string (e.g., \\"3.10.0f0\\", \\"3.4.1a2\\") :return: The hexadecimal representation of the version string (e.g., \\"0x030a00f0\\", \\"0x030401a2\\") import re # Validate and parse the version string using regex match = re.match(r\\"(d+).(d+).(d+)([a-z])(d+)\\", version) if not match: raise ValueError(\\"Invalid version string format\\") # Extract components from the match groups major, minor, micro, level, serial = match.groups() # Convert each component to its integer representation major = int(major) minor = int(minor) micro = int(micro) serial = int(serial) # Validate the ranges of the components if not (0 <= major <= 255): raise ValueError(\\"Major version out of range\\") if not (0 <= minor <= 255): raise ValueError(\\"Minor version out of range\\") if not (0 <= micro <= 255): raise ValueError(\\"Micro version out of range\\") if not (0 <= serial <= 15): raise ValueError(\\"Serial number out of range\\") # Define the level mapping level_mapping = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF} if level not in level_mapping: raise ValueError(\\"Invalid level\\") level = level_mapping[level] # Construct the hexadecimal representation hex_repr = (major << 24) | (minor << 16) | (micro << 8) | (level << 4) | serial return f\\"0x{hex_repr:08x}\\""},{"question":"Objective Implement a custom class `CustomRange` that mimics the behavior of Pythonâ€™s built-in `range` class but with additional functionalities. You will also perform operations on sequences, mappings, and demonstrate numeric and boolean operations covered in the provided documentation. Requirements 1. Implement the `CustomRange` class with: - A constructor that takes `start`, `stop`, and `step` arguments. - Define methods `__iter__()` and `__next__()` to make `CustomRange` an iterable. - A method `contains(value)` that returns whether the value is within the range. 2. Perform operations on numeric types: - Create a function that takes a list of numbers and returns a dictionary with the sum, product, and their bit-length representations of the given numbers. 3. Perform sequence manipulations: - Implement a function that takes a list and returns a tuple with the minimum, maximum, and sorted version of the list. 4. Perform mapping operations: - Implement a function that takes a dictionary, inverts it (creates a new dictionary swapping keys and values), and ensures all operations do not mutate the original dictionary. Class and Function Definitions 1. **CustomRange Class** ```python class CustomRange: def __init__(self, start, stop, step=1): self.start = start self.stop = stop self.step = step self.current = start def __iter__(self): return self def __next__(self): if (self.step > 0 and self.current >= self.stop) or (self.step < 0 and self.current <= self.stop): raise StopIteration current = self.current self.current += self.step return current def contains(self, value): return (self.start <= value < self.stop and (value - self.start) % self.step == 0) ``` 2. **Function to perform numeric operations** ```python def numeric_operations(numbers): result = { \\"sum\\": sum(numbers), \\"product\\": 1, \\"bit_lengths\\": [] } for num in numbers: result[\\"product\\"] *= num result[\\"bit_lengths\\"].append(num.bit_length()) return result ``` 3. **Function for sequence manipulations** ```python def sequence_manipulations(seq): return (min(seq), max(seq), sorted(seq)) ``` 4. **Function for mapping operations** ```python def invert_dictionary(d): return {v: k for k, v in d.items()} ``` Input and Output 1. `CustomRange` class: - **Input**: `CustomRange(1, 10, 2)` - **Output**: Iterable object, `contains(3) -> True, contains(4) -> False` 2. `numeric_operations`: - **Input**: `[1, 2, 3]` - **Output**: `{\\"sum\\": 6, \\"product\\": 6, \\"bit_lengths\\": [1, 2, 2]}` 3. `sequence_manipulations`: - **Input**: `[3, 1, 2]` - **Output**: `(1, 3, [1, 2, 3])` 4. `invert_dictionary`: - **Input**: `{\\"a\\": 1, \\"b\\": 2}` - **Output**: `{1: \\"a\\", 2: \\"b\\"}` Constraints and Limitations - Ensure the `CustomRange` class handles both positive and negative step values. - Assume all input lists and dictionaries are non-empty. - Handle edge cases appropriately, such as invalid numeric inputs in functions. Notes - Use built-in methods and operations outlined in the provided documentation. - Ensure code readability with proper comments and adherence to PEP8 standards.","solution":"class CustomRange: def __init__(self, start, stop, step=1): self.start = start self.stop = stop self.step = step self.current = start def __iter__(self): return self def __next__(self): if (self.step > 0 and self.current >= self.stop) or (self.step < 0 and self.current <= self.stop): raise StopIteration current = self.current self.current += self.step return current def contains(self, value): if self.step > 0: return self.start <= value < self.stop and (value - self.start) % self.step == 0 else: return self.start >= value > self.stop and (self.start - value) % abs(self.step) == 0 def numeric_operations(numbers): result = { \\"sum\\": sum(numbers), \\"product\\": 1, \\"bit_lengths\\": [] } for num in numbers: result[\\"product\\"] *= num result[\\"bit_lengths\\"].append(num.bit_length()) return result def sequence_manipulations(seq): return (min(seq), max(seq), sorted(seq)) def invert_dictionary(d): return {v: k for k, v in d.items()}"},{"question":"# Color Space Conversion Challenge You are given a task to implement a function that converts a list of colors from RGB to various color spaces and back to RGB, ensuring the accuracy of the conversions. Function Specification 1. **Function Name:** `convert_colors` 2. **Input:** - A list of tuples where each tuple consists of three floating point values representing colors in the RGB color space. - A string specifying the target color space to convert to and back from. It can be one of the following: `\'YIQ\'`, `\'HLS\'`, `\'HSV\'`. 3. **Output:** - A list of tuples where each tuple consists of three floating point values representing the colors converted back to RGB from the specified color space. 4. **Constraints:** - Each of the RGB values are within the range [0, 1]. - The target color space will always be a valid option i.e., one of `\'YIQ\'`, `\'HLS\'`, or `\'HSV\'`. 5. **Performance:** - The function should be efficient and handle lists containing up to 1000 colors. Example ```python def convert_colors(rgb_colors, target_space): # Your implementation here # Example Usage rgb_colors = [(0.2, 0.4, 0.4), (0.5, 0.2, 0.1)] target_space = \'HSV\' print(convert_colors(rgb_colors, target_space)) # Expected Output: [(0.2, 0.4, 0.4), (0.5, 0.2, 0.1)] ``` Explanation: 1. Convert each RGB color to the specified target space. 2. Convert the color back to the RGB space. 3. Ensure the result is within the valid RGB range and verify accuracy. You may use the `colorsys` module functions to assist in your implementation. Note: There may be minor differences due to floating-point arithmetic, but the converted colors should be very close to the original ones provided in the input.","solution":"import colorsys def convert_colors(rgb_colors, target_space): Converts a list of RGB colors to the specified color space and back to RGB. converted_colors = [] for r, g, b in rgb_colors: if target_space == \'YIQ\': yiq = colorsys.rgb_to_yiq(r, g, b) rgb = colorsys.yiq_to_rgb(*yiq) elif target_space == \'HLS\': hls = colorsys.rgb_to_hls(r, g, b) rgb = colorsys.hls_to_rgb(*hls) elif target_space == \'HSV\': hsv = colorsys.rgb_to_hsv(r, g, b) rgb = colorsys.hsv_to_rgb(*hsv) else: raise ValueError(f\\"Unsupported color space: {target_space}\\") # Clamping the results within the valid range [0, 1] rgb_clamped = tuple(max(0, min(1, x)) for x in rgb) converted_colors.append(rgb_clamped) return converted_colors"},{"question":"Objective: Create a mini \\"pydoc\\"-like documentation generator for Python modules. The script should gather documentation from modules, classes, functions, and methods, and display it in a readable format in the console or write it to an HTML file. Task: Implement a Python function `generate_docs(module_name: str, output_format: str = \\"console\\", output_file: str = None) -> None` that generates and displays documentation for a specified Python module. Requirements: 1. **Input Parameters:** - `module_name` (str): The name of the module to document (e.g., \'sys\'). - `output_format` (str): The format for the output, either \\"console\\" (default) or \\"html\\". - `output_file` (str): The file path to save HTML output if `output_format` is \\"html\\". 2. **Output:** - If `output_format` is \\"console\\", print the documentation to the console. - If `output_format` is \\"html\\", write the documentation to the specified `output_file`. 3. **Constraints:** - The function should handle non-existent modules gracefully by printing an appropriate error message. - The function must handle the absence of docstrings and comments gracefully by indicating that no documentation is available. 4. **Implementation Tips:** - Use `importlib` to dynamically import the specified module. - Use `inspect` to retrieve docstrings from the module, its classes, methods, and functions. - Format the output neatly for readability, especially for HTML output. Example: ```python generate_docs(\\"sys\\", \\"console\\") generate_docs(\\"sys\\", \\"html\\", \\"sys_docs.html\\") ``` When `generate_docs(\\"sys\\", \\"console\\")` is called, your script should print the documentation for the `sys` module to the console. When `generate_docs(\\"sys\\", \\"html\\", \\"sys_docs.html\\")` is called, the documentation should be written to the file \\"sys_docs.html\\" in an HTML format. Assessment Criteria: - Correctness and completeness of the implementation. - Proper handling of edge cases and erroneous inputs. - Code readability and comments. Notes: - You may assume the standard library modules can always be imported, but handle import errors for third-party or incorrect module names. - Ensure the generated HTML format is simple but readable with basic tags like `<h1>`, `<h2>`, `<p>`, etc.","solution":"import importlib import inspect import sys def generate_docs(module_name: str, output_format: str = \\"console\\", output_file: str = None) -> None: Generates and displays documentation for the specified Python module. Parameters: - module_name (str): The name of the module to document. - output_format (str): The format for the output, either \'console\' (default) or \'html\'. - output_file (str): The file path to save HTML output if `output_format` is \'html\'. try: module = importlib.import_module(module_name) except ImportError: print(f\\"Error: No module named \'{module_name}\'\\") return doc = f\\"Documentation for module \'{module_name}\':nn{inspect.getdoc(module) or \'No module docstring available.\'}nn\\" for name, member in inspect.getmembers(module): if inspect.isclass(member) or inspect.isfunction(member) or inspect.ismethod(member): doc += f\\"{name}:n{inspect.getdoc(member) or \'No docstring available.\'}nn\\" if output_format == \\"console\\": print(doc) elif output_format == \\"html\\": if output_file is None: print(\\"Error: Output file path must be specified when output_format is \'html\'.\\") return html_content = f\\"<html><head><title>{module_name} Docs</title></head><body>\\" html_content += f\\"<h1>Documentation for module \'{module_name}\'</h1>\\" html_content += f\\"<p>{inspect.getdoc(module) or \'No module docstring available.\'}</p>\\" for name, member in inspect.getmembers(module): if inspect.isclass(member) or inspect.isfunction(member) or inspect.ismethod(member): html_content += f\\"<h2>{name}</h2>\\" html_content += f\\"<p>{inspect.getdoc(member) or \'No docstring available.\'}</p>\\" html_content += \\"</body></html>\\" with open(output_file, \\"w\\") as file: file.write(html_content) else: print(f\\"Error: Invalid output format \'{output_format}\'. Use \'console\' or \'html\'.\\") # Usage examples: # generate_docs(\\"sys\\", \\"console\\") # generate_docs(\\"sys\\", \\"html\\", \\"sys_docs.html\\")"},{"question":"# Terminal Control - Custom Terminal Behavior Background The `termios` module allows Python programs to control terminal I/O settings directly using POSIX-style tty control. This can be useful for creating custom terminal-based behaviors like password input without echoing, suspending or resuming terminal input/output, and flushing terminal buffers. Task You are asked to implement a custom terminal-based behavior, leveraging the `termios` module\'s capabilities. Write a function named `suspend_and_flush_input()` that performs the following steps: 1. **Suspend Input**: Suspend the terminal\'s input. 2. **Wait**: Wait for `n` seconds. 3. **Flush Input**: Flush all the input data. 4. **Resume Input**: Resume the terminal\'s input. The function should be implemented as follows: ```python import termios import sys import time def suspend_and_flush_input(n): Function to suspend terminal input, wait for \'n\' seconds, flush input buffer, and then resume terminal input. Parameters: n (int): Number of seconds to wait before flushing the input. fd = sys.stdin.fileno() # Step 1: Suspend the input termios.tcflow(fd, termios.TCIOFF) # Step 2: Wait for \'n\' seconds time.sleep(n) # Step 3: Flush the input buffer termios.tcflush(fd, termios.TCIFLUSH) # Step 4: Resume the input termios.tcflow(fd, termios.TCION) ``` Constraints 1. You can assume that the terminal supports `termios` module functionalities. 2. The input parameter `n` will be a non-negative integer. 3. The input suspension and flushing should follow the provided sequence strictly. Example ```python # Execution example: suspend_and_flush_input(5) ``` In this example, the terminal input will be suspended for 5 seconds, any input given during this time will be discarded, and then the input will resume. Test your implementation in an environment that supports the `termios` module, as it won\'t work on non-POSIX compliant systems such as Windows.","solution":"import termios import sys import time def suspend_and_flush_input(n): Function to suspend terminal input, wait for \'n\' seconds, flush input buffer, and then resume terminal input. Parameters: n (int): Number of seconds to wait before flushing the input. fd = sys.stdin.fileno() # Step 1: Suspend the input termios.tcflow(fd, termios.TCIOFF) # Step 2: Wait for \'n\' seconds time.sleep(n) # Step 3: Flush the input buffer termios.tcflush(fd, termios.TCIFLUSH) # Step 4: Resume the input termios.tcflow(fd, termios.TCION)"},{"question":"Objective You are required to implement a Python function that works with file system paths and sets up a custom system audit hook. The function will perform the following tasks: 1. Convert a given path object to its file system representation using `PyOS_FSPath`. 2. Register a custom audit hook using `PySys_AddAuditHook`. 3. Raise an audit event and manage the audit event response. Task Implement a Python function `process_and_audit_path(path)` that: 1. Accepts a single argument `path` which is a string representing a file system path. 2. Converts the given path to its file system representation. 3. Registers a custom audit hook function that prints the event name and arguments to the screen. 4. Raises a custom audit event `custom.audit` with the file system path as the event argument. 5. Captures and returns the list of raised audit events. Inputs and Outputs - **Input**: - `path` (str): A string representing a file system path. - **Output**: - List of tuples where each tuple contains the event name and event arguments captured by the audit hook. Constraints - Assume `path` is always a valid string. - The function should handle exceptions gracefully and print relevant error messages. - The audit hook should be thread-safe. Example ```python def process_and_audit_path(path): # Your implementation here # Example usage events = process_and_audit_path(\\"/path/to/file\\") print(events) # Output could look like: [(\'custom.audit\', (\'/path/to/file\',))] ``` # Notes - You may need to create wrapper functions for the C APIs provided in `python310`. - Ensure thread-safety when implementing the audit hook. - The audit hook function should be clearly separated for registering and handling events. # Hint Refer to the functions `PyOS_FSPath`, `PySys_AddAuditHook`, and `PySys_Audit` in the `python310` documentation for their usage and expected behavior.","solution":"import os import sys import threading def process_and_audit_path(path): events = [] lock = threading.Lock() # Custom audit hook function def audit_hook(event, args): with lock: print(f\\"Audit Event: {event}, Arguments: {args}\\") events.append((event, args)) # Register the custom audit hook sys.addaudithook(audit_hook) # Convert the path to its file system representation fs_path = os.fspath(path) # Raise a custom audit event sys.audit(\'custom.audit\', fs_path) return events # Example usage if __name__ == \\"__main__\\": example_path = \\"/path/to/file\\" events = process_and_audit_path(example_path) print(events)"},{"question":"Task: Design and implement a Python function `execute_module_or_script` that dynamically runs a given module or script and captures its output. Function Signature: ```python def execute_module_or_script(module_or_path: str, use_path: bool = False, additional_globals: dict = None, run_name: str = None) -> dict: Run a specified module or script and return its globals. :param module_or_path: Name of the module to run or path to the script file. :param use_path: If True, treat `module_or_path` as a filesystem path and execute it using `run_path`. If False, treat `module_or_path` as a module name and execute it using `run_module`. :param additional_globals: Optional dictionary of globals to prepopulate in the module\'s namespace. :param run_name: Optional name to use for the `__name__` global variable. If None, use the default handling. :return: The globals dictionary resulting from the execution of the module or script. pass ``` Description: 1. **Function Implementation**: - The function `execute_module_or_script` should use the `runpy` module to run either a module (if `use_path` is `False`) or a script path (if `use_path` is `True`). - `module_or_path` specifies the module name or script path to be executed. - `use_path` determines whether to use `run_module` or `run_path`. - `additional_globals` allows optional prepopulation of the module\'s globals before execution. If not provided, default to an empty dictionary. - `run_name` allows the optional setting of the `__name__` global variable. If not provided, use the default behavior of `runpy`. - The function should return the dictionary resulting from the execution of the module or script. 2. **Constraints**: - The module or script should be executable in the current environment. - Proper error handling should be incorporated to manage scenarios where the module or script doesn\'t exist or fails to run properly. 3. **Example Usage**: ```python # To execute a module named \'example_module\' globals_dict = execute_module_or_script(\'example_module\') # To execute a script located at \'path/to/script.py\' globals_dict = execute_module_or_script(\'path/to/script.py\', use_path=True) # To execute a module with additional globals pre-populated additional_globals = {\'x\': 10, \'y\': 20} globals_dict = execute_module_or_script(\'example_module\', additional_globals=additional_globals) ``` 4. **Performance Requirements**: - The function should efficiently locate and execute the module/script as described. - Ensure that any modifications to the `sys` module and global variables are reverted appropriately after execution. Evaluation Criteria: - Correctness: The function should correctly use the `runpy` module\'s functions to execute the specified module or script. - Error Handling: Proper handling of exceptions and errors during module/script execution. - Code Quality: Clean and readable code with relevant comments and documentation. - Edge Cases: Consideration of edge cases such as non-existent modules or scripts, invalid arguments, etc.","solution":"import runpy import sys import types def execute_module_or_script(module_or_path: str, use_path: bool = False, additional_globals: dict = None, run_name: str = None) -> dict: Run a specified module or script and return its globals. :param module_or_path: Name of the module to run or path to the script file. :param use_path: If True, treat `module_or_path` as a filesystem path and execute it using `run_path`. If False, treat `module_or_path` as a module name and execute it using `run_module`. :param additional_globals: Optional dictionary of globals to prepopulate in the module\'s namespace. :param run_name: Optional name to use for the `__name__` global variable. If None, use the default handling. :return: The globals dictionary resulting from the execution of the module or script. if additional_globals is None: additional_globals = {} if use_path: globals_dict = runpy.run_path(module_or_path, init_globals=additional_globals, run_name=run_name) else: globals_dict = runpy.run_module(module_or_path, init_globals=additional_globals, run_name=run_name, alter_sys=True) return globals_dict"},{"question":"# Coding Assessment: Python Code Execution and Embedding **Objective:** Implement a set of functions to demonstrate the usage of the functions described in the high-level layer of the Python C-API for executing Python code. This will test understanding of how Python code is executed from different sources and contexts. **Task:** You need to implement three functions: `execute_from_string`, `execute_from_file`, and `evaluate_compiled_code`. 1. **execute_from_string(command: str) -> Any:** - Executes the Python source code provided as a string. - Uses `PyRun_SimpleString`. - Returns `None` if the command executed successfully, or raises an exception if there was an error. 2. **execute_from_file(filepath: str) -> Any:** - Executes the Python source code contained in the specified file. - Uses `PyRun_SimpleFile`. - Returns `None` if the file executed successfully, or raises an exception if there was an error. 3. **evaluate_compiled_code(source_code: str, start_symbol: int) -> Any:** - Compiles the provided source code string using the specified starting token (`Py_eval_input`, `Py_file_input`, or `Py_single_input`). - Executes the compiled code in a new context with separate globals and locals. - Uses `Py_CompileString` and `PyEval_EvalCode`. - Returns the result of execution, or `None` if there was no result. - Raises an exception if there was an error during compilation or execution. **Constraints/Requirements:** - You must handle Python exceptions appropriately and re-raise them after capturing relevant error information. - You should not use any external libraries for these implementations. **Input and Output:** - `execute_from_string(command: str) -> Any` - **Input:** A string containing a valid Python command. - **Output:** The functionâ€™s returned value should be `None` or an exception should be raised if an error occurs. - `execute_from_file(filepath: str) -> Any` - **Input:** A string path to a Python file. - **Output:** The functionâ€™s returned value should be `None` or an exception should be raised if an error occurs. - `evaluate_compiled_code(source_code: str, start_symbol: int) -> Any` - **Input:** - `source_code`: A string containing valid Python source code. - `start_symbol`: An integer representing the start token for compilation (`Py_eval_input`, `Py_file_input`, or `Py_single_input`). - **Output:** The result of the code execution or `None` if there was no result. Raises an exception if an error occurs. **Example:** ```python # Example for execute_from_string execute_from_string(\\"print(\'Hello, World!\')\\") # Should print Hello, World! and return None # Example for execute_from_file result = execute_from_file(\\"example_script.py\\") # Executes the code from the file \'example_script.py\' # Example for evaluate_compiled_code code_str = \\"x = 5nprint(x)nx\\" result = evaluate_compiled_code(code_str, Py_file_input) # Should print 5 and return 5 ``` Ensure that you include suitable error handling, perform necessary imports, and your code is well-structured and documented.","solution":"import sys import contextlib import io def execute_from_string(command: str): Executes the Python source code provided as a string. Args: - command (str): A string containing a valid Python command. Returns: - None if the command executed successfully. - Raises an exception if there was an error. try: exec(command) except Exception as e: raise RuntimeError(f\\"Failed to execute command:n{command}nError: {e}\\") def execute_from_file(filepath: str): Executes the Python source code contained in the specified file. Args: - filepath (str): A string path to a Python file. Returns: - None if the file executed successfully. - Raises an exception if there was an error. try: with open(filepath) as file: exec(file.read()) except Exception as e: raise RuntimeError(f\\"Failed to execute file: {filepath}nError: {e}\\") def evaluate_compiled_code(source_code: str, start_symbol: str): Compiles and executes the provided source code string using the specified starting token. Args: - source_code (str): A string containing valid Python source code. - start_symbol (str): The start symbol for compilation (`eval`, `exec`, or `single`). Returns: - Result of the code execution or None if there was no result. - Raises an exception if there was an error during compilation or execution. try: compiled_code = compile(source_code, \'<string>\', start_symbol) return eval(compiled_code) except Exception as e: raise RuntimeError(f\\"Failed to compile/execute source code:n{source_code}nError: {e}\\")"},{"question":"Objective Implement a Python function that downloads content from multiple URLs concurrently using both threads and processes. You must manage the tasks using the `ThreadPoolExecutor` for downloading and the `ProcessPoolExecutor` for processing the content. Handle exceptions gracefully and ensure resources are properly cleaned up. Task Write a function `fetch_and_process_urls(urls, timeout=10)` that takes a list of URLs and a timeout value. This function should: 1. **Download** the content of each URL using `ThreadPoolExecutor`. 2. **Process** the content of each URL using `ProcessPoolExecutor`. For simplicity, assume the processing function is just the `len` function applied to the downloaded content. 3. Return a dictionary mapping each URL to the length of its content or to an appropriate exception message if an error occurred. 4. Ensure the entire operation respects the provided timeout value for downloading and processing. 5. Properly handle and log any exceptions that occur during the download or processing stages. Input - `urls` (list of str): A list of URLs to fetch. - `timeout` (int, optional): The maximum number of seconds to wait for downloading and processing each URL. Default is 10 seconds. Output - A dictionary mapping each URL (str) to either: - The length of its content (int), or - An exception message (str) describing the error that occurred. Constraints - Each URL must be fetched and processed concurrently. - The timeout should be enforced for both fetching and processing stages. Example ```python import concurrent.futures import urllib.request def fetch_and_process_urls(urls, timeout=10): def download_url(url): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read() except Exception as e: return str(e) with concurrent.futures.ThreadPoolExecutor() as downloader: download_futures = {downloader.submit(download_url, url): url for url in urls} results = {} for future in concurrent.futures.as_completed(download_futures): url = download_futures[future] try: content = future.result() if isinstance(content, bytes): with concurrent.futures.ProcessPoolExecutor() as processor: process_future = processor.submit(len, content) try: results[url] = process_future.result(timeout=timeout) except Exception as e: results[url] = str(e) else: results[url] = content except Exception as e: results[url] = str(e) return results # Example usage urls = [\'http://www.example.com\', \'http://www.nonexistent.com\'] print(fetch_and_process_urls(urls, timeout=5)) ``` Note - Make sure to test your function with both reachable and non-reachable URLs. - Pay attention to handling exceptions and timeouts to avoid excessive waiting or crashes.","solution":"import concurrent.futures import urllib.request import logging logging.basicConfig(level=logging.INFO) def fetch_and_process_urls(urls, timeout=10): def download_url(url): try: with urllib.request.urlopen(url, timeout=timeout) as conn: return conn.read() except Exception as e: return str(e) with concurrent.futures.ThreadPoolExecutor() as downloader: download_futures = {downloader.submit(download_url, url): url for url in urls} results = {} for future in concurrent.futures.as_completed(download_futures): url = download_futures[future] try: content = future.result() if isinstance(content, bytes): with concurrent.futures.ProcessPoolExecutor() as processor: process_future = processor.submit(len, content) try: results[url] = process_future.result(timeout=timeout) except Exception as e: results[url] = str(e) else: results[url] = content except Exception as e: results[url] = str(e) return results"},{"question":"**Task: Implement a Sound File Type Verifier** Given a set of sound files, your task is to write a Python function `verify_sound_files(file_list)` that uses the `sndhdr` module to identify the type of sound data in each file. The function should return a dictionary where each key is the filename and the value is another dictionary containing the following attributes if the type can be determined: - `filetype` - `framerate` - `nchannels` - `nframes` - `sampwidth` If the type cannot be determined for a file, the value should be `None`. # Function Signature ```python def verify_sound_files(file_list: list) -> dict: pass ``` # Input - `file_list`: A list of strings where each string is the file name of a sound file. # Output - A dictionary where each key is a file name and each value is a dictionary containing sound file attributes or `None` if type determination fails. # Example Given the following file names: ```python file_list = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"sound3.unknown\\"] ``` Your function should return a dictionary like this: ```python { \\"sound1.wav\\": { \\"filetype\\": \\"wav\\", \\"framerate\\": 44100, \\"nchannels\\": 2, \\"nframes\\": -1, \\"sampwidth\\": 16 }, \\"sound2.aiff\\": { \\"filetype\\": \\"aiff\\", \\"framerate\\": 48000, \\"nchannels\\": 1, \\"nframes\\": -1, \\"sampwidth\\": 16 }, \\"sound3.unknown\\": None } ``` # Constraints - You may assume that the files exist and are accessible in the provided file list. - The function should handle at least 1 and at most 1000 file names. # Notes - Ensure that your solution is efficient and handles large lists of file names appropriately. - Refer to the `sndhdr` module documentation for the correct usage of `sndhdr.what()` or `sndhdr.whathdr()` to determine file attributes.","solution":"import sndhdr def verify_sound_files(file_list): result = {} for file in file_list: sound_info = sndhdr.what(file) if sound_info is not None: info_dict = { \\"filetype\\": sound_info[0], \\"framerate\\": sound_info[1], \\"nchannels\\": sound_info[2], \\"nframes\\": -1, # sndhdr module doesn\'t provide nframes, set as -1 as a placeholder. \\"sampwidth\\": sound_info[3] } else: info_dict = None result[file] = info_dict return result"},{"question":"**Question: Implementing and Testing a Complex Function** # Objective: You are tasked with implementing a function to solve a specific problem, documenting this function, and writing unit tests to verify its correctness. You will use Python\'s `typing` for type hints, `pydoc` for generating documentation, and `unittest` for writing unit tests. # Problem: Implement a function `def matrix_multiply(matrix1: List[List[int]], matrix2: List[List[int]]) -> List[List[int]]` which takes two 2D matrices (as lists of lists) and returns their product. **Input:** * `matrix1` and `matrix2` are both 2D lists containing integers. * You may assume the number of columns in `matrix1` equals the number of rows in `matrix2`. **Output:** * A 2D list containing the result of the matrix multiplication. # Constraints: 1. The number of rows and columns of the matrices will be between `1` and `100`. 2. Integer values in the matrices will be between `-1000` and `1000`. # Requirements: 1. Implement the function `matrix_multiply` with appropriate type hints. 2. Write a docstring for the function describing its parameters, return value, and an example usage. 3. Write unit tests for the function using the `unittest` module. 4. Ensure your unit tests cover the following scenarios: - Multiplying two 1x1 matrices. - Multiplying a matrix by an identity matrix. - Multiplying two matrices with arbitrary values. - Edge case with the smallest and largest possible values. # Example: ```python def matrix_multiply(matrix1: List[List[int]], matrix2: List[List[int]]) -> List[List[int]]: Multiplies two 2D matrices and returns the result. Args: matrix1 (List[List[int]]): The first matrix to be multiplied. matrix2 (List[List[int]]): The second matrix to be multiplied. Returns: List[List[int]]: The product of the two matrices. Example: >>> matrix_multiply([[1, 2], [3, 4]], [[5, 6], [7, 8]]) [[19, 22], [43, 50]] # Implementation goes here pass ``` # Unit Test Example: ```python import unittest class TestMatrixMultiply(unittest.TestCase): def test_single_element(self): self.assertEqual(matrix_multiply([[2]], [[3]]), [[6]]) def test_identity_matrix(self): self.assertEqual(matrix_multiply([[1, 0], [0, 1]], [[4, 1], [2, 2]]), [[4, 1], [2, 2]]) def test_general_case(self): self.assertEqual(matrix_multiply([[1, 2], [3, 4]], [[5, 6], [7, 8]]), [[19, 22], [43, 50]]) def test_edge_case(self): self.assertEqual(matrix_multiply([[1000]*100]*100, [[1000]*100]*100), [[100000000]*100]*100) if __name__ == \'__main__\': unittest.main() ``` **Additional Instructions:** 1. Ensure your implementation is efficient and can handle the upper constraint limits. 2. Make use of `pydoc` to generate a standalone HTML documentation file for your function. 3. Submit your `.py` file containing the implementation, the tests, and the generated documentation. Good luck!","solution":"from typing import List def matrix_multiply(matrix1: List[List[int]], matrix2: List[List[int]]) -> List[List[int]]: Multiplies two 2D matrices and returns the result. Args: matrix1 (List[List[int]]): The first matrix to be multiplied. matrix2 (List[List[int]]): The second matrix to be multiplied. Returns: List[List[int]]: The product of the two matrices. Example: >>> matrix_multiply([[1, 2], [3, 4]], [[5, 6], [7, 8]]) [[19, 22], [43, 50]] result = [[0 for _ in range(len(matrix2[0]))] for _ in range(len(matrix1))] for i in range(len(matrix1)): for j in range(len(matrix2[0])): for k in range(len(matrix2)): result[i][j] += matrix1[i][k] * matrix2[k][j] return result"},{"question":"# Synthetic Data Experimentation Using scikit-learn In this coding assessment, you will need to demonstrate your understanding of scikit-learn\'s dataset generation functions, preprocessing techniques, and model training. **Task Description:** Implement a function `synthetic_data_experiment(task_type, n_samples, noise_level)` that performs the following tasks: 1. **Dataset Generation**: - For classification tasks: - Generate a dataset using the `make_classification` function. - Parameters: Use `n_samples` for the number of samples and control the `noise` parameter with `flip_y` set to `noise_level`. - For regression tasks: - Generate a dataset using the `make_regression` function. - Parameters: Use `n_samples` for the number of samples and control the `noise` parameter with `noise_level`. 2. **Data Preprocessing**: - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Model Training**: - For classification, train a `RandomForestClassifier` from `sklearn.ensemble`. - For regression, train a `RandomForestRegressor` from `sklearn.ensemble`. - Use default hyperparameters for both models. 4. **Performance Evaluation**: - For classification, return the accuracy score using `accuracy_score`. - For regression, return the mean squared error (MSE) using `mean_squared_error`. **Function Signature:** ```python def synthetic_data_experiment(task_type: str, n_samples: int, noise_level: float) -> float: pass ``` **Input:** 1. `task_type` (str): Can be either `\\"classification\\"` or `\\"regression\\"`. 2. `n_samples` (int): The number of samples to generate for the dataset. 3. `noise_level` (float): A float between 0 and 1 to control the noise in the dataset. **Output:** - A float representing either the accuracy score for classification or the mean squared error for regression. **Constraints:** - `task_type` must be either `\\"classification\\"` or `\\"regression\\"`. - `n_samples` should be a positive integer (1 â‰¤ `n_samples` â‰¤ 10000). - `noise_level` should be a float between 0.0 and 1.0. **Example Usage:** ```python # Example for classification task accuracy = synthetic_data_experiment(\\"classification\\", 1000, 0.1) print(\\"Classification Accuracy:\\", accuracy) # Example for regression task mse = synthetic_data_experiment(\\"regression\\", 1000, 0.1) print(\\"Regression Mean Squared Error:\\", mse) ``` # Requirements - Use `make_classification` and `make_regression` for dataset creation. - Use `RandomForestClassifier` and `RandomForestRegressor` for model training. - Ensure that your function handles data scaling appropriately using `StandardScaler`.","solution":"import numpy as np from sklearn.datasets import make_classification, make_regression from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor from sklearn.metrics import accuracy_score, mean_squared_error def synthetic_data_experiment(task_type: str, n_samples: int, noise_level: float) -> float: np.random.seed(42) # For reproducibility if task_type not in [\\"classification\\", \\"regression\\"]: raise ValueError(\\"task_type must be either \'classification\' or \'regression\'\\") if not (0.0 <= noise_level <= 1.0): raise ValueError(\\"noise_level must be between 0 and 1\\") if task_type == \\"classification\\": X, y = make_classification( n_samples=n_samples, n_features=20, n_informative=2, n_redundant=2, flip_y=noise_level, random_state=42 ) scaler = StandardScaler() X = scaler.fit_transform(X) model = RandomForestClassifier(random_state=42) model.fit(X, y) y_pred = model.predict(X) return accuracy_score(y, y_pred) elif task_type == \\"regression\\": X, y = make_regression( n_samples=n_samples, n_features=20, noise=noise_level * 100, # scaling noise level appropriately random_state=42 ) scaler = StandardScaler() X = scaler.fit_transform(X) model = RandomForestRegressor(random_state=42) model.fit(X, y) y_pred = model.predict(X) return mean_squared_error(y, y_pred)"},{"question":"**Objective:** Design a system for handling multiple encodings in a text processing application, using the `codecs` module. The task involves implementing functions for encoding and decoding text, handling errors, and managing file I/O with specific codecs. **Requirements:** 1. **Implement Encoding and Decoding Functions:** - Implement functions `custom_encode` and `custom_decode` that take a string and an encoding type. Use the `codecs.encode` and `codecs.decode` functions respectively. - Implement appropriate error handling mechanisms for each function. - Supported encodings: `\'utf-8\'`, `\'ascii\'`, `\'latin-1\'`. 2. **Custom Error Handlers:** - Implement a custom error handler `replace_with_star` that replaces unencodable characters with a \'*\' (asterisk) symbol. - Register this custom error handler using `codecs.register_error`. - Modify the `custom_encode` and `custom_decode` to use this custom error handler when specified. 3. **File I/O Operations:** - Implement functions `write_encoded_file` and `read_encoded_file` that write and read text to/from a file with a given encoding. - Use the `codecs.open` function to handle the file operations. - Ensure error handling is properly managed based on the encoding type and custom error handlers if required. **Constraints:** - Assume the input text for encoding and decoding contains Unicode characters. - Ensure the file reading and writing operations handle multiple lines of text. - Performance should be considered for large text files without compromising functionality. **Function Signatures:** ```python def custom_encode(text: str, encoding: str, errors: str = \'strict\') -> bytes: Encode the given text using the specified encoding. :param text: The text to encode. :param encoding: The encoding to use (\'utf-8\', \'ascii\', \'latin-1\'). :param errors: Error handling scheme (\'strict\', \'replace\', \'ignore\', \'replace_with_star\'). :return: Encoded bytes. pass def custom_decode(encoded_text: bytes, encoding: str, errors: str = \'strict\') -> str: Decode the given text using the specified encoding. :param encoded_text: Bytes to decode. :param encoding: The encoding to use (\'utf-8\', \'ascii\', \'latin-1\'). :param errors: Error handling scheme (\'strict\', \'replace\', \'ignore\', \'replace_with_star\'). :return: Decoded string. pass def replace_with_star(exception): Custom error handler that replaces unencodable characters with \'*\'. :param exception: The exception instance. :return: A tuple with replacement and position. pass def write_encoded_file(filename: str, text: str, encoding: str, errors: str = \'strict\'): Write the given text to a file using the specified encoding. :param filename: The path of the file to write. :param text: The text to write. :param encoding: The encoding to use. :param errors: Error handling scheme. pass def read_encoded_file(filename: str, encoding: str, errors: str = \'strict\') -> str: Read text from a file encoded with the specified encoding. :param filename: The path of the file to read. :param encoding: The encoding to use. :param errors: Error handling scheme. :return: Decoded text. pass ``` **Example Usage:** ```python # Register custom error handler codecs.register_error(\'replace_with_star\', replace_with_star) # Encode and decode with custom error handler text = \\"Hello, World! ðŸ˜ƒ\\" encoded = custom_encode(text, \'ascii\', errors=\'replace_with_star\') print(encoded) # Output: b\'Hello, World! *\' decoded = custom_decode(encoded, \'ascii\', errors=\'replace_with_star\') print(decoded) # Output: \'Hello, World! *\' # File operations write_encoded_file(\'example.txt\', text, \'utf-8\') result = read_encoded_file(\'example.txt\', \'utf-8\') print(result) # Output: \'Hello, World! ðŸ˜ƒ\' ``` **Notes:** - Ensure to handle potential errors in file operations gracefully. - Test the implementation with different encoding types and error handling schemes to validate robustness.","solution":"import codecs def custom_encode(text, encoding, errors=\'strict\'): Encode the given text using the specified encoding. :param text: The text to encode. :param encoding: The encoding to use (\'utf-8\', \'ascii\', \'latin-1\'). :param errors: Error handling scheme (\'strict\', \'replace\', \'ignore\', \'replace_with_star\'). :return: Encoded bytes. return codecs.encode(text, encoding, errors) def custom_decode(encoded_text, encoding, errors=\'strict\'): Decode the given text using the specified encoding. :param encoded_text: Bytes to decode. :param encoding: The encoding to use (\'utf-8\', \'ascii\', \'latin-1\'). :param errors: Error handling scheme (\'strict\', \'replace\', \'ignore\', \'replace_with_star\'). :return: Decoded string. return codecs.decode(encoded_text, encoding, errors) def replace_with_star(exception): Custom error handler that replaces unencodable characters with \'*\'. :param exception: The exception instance. :return: A tuple with replacement and position. if isinstance(exception, (UnicodeEncodeError, UnicodeDecodeError)): return (\'*\', exception.start + 1) else: raise TypeError(\\"Exception not handled\\") codecs.register_error(\'replace_with_star\', replace_with_star) def write_encoded_file(filename, text, encoding, errors=\'strict\'): Write the given text to a file using the specified encoding. :param filename: The path of the file to write. :param text: The text to write. :param encoding: The encoding to use. :param errors: Error handling scheme. with codecs.open(filename, \'w\', encoding=encoding, errors=errors) as file: file.write(text) def read_encoded_file(filename, encoding, errors=\'strict\'): Read text from a file encoded with the specified encoding. :param filename: The path of the file to read. :param encoding: The encoding to use. :param errors: Error handling scheme. :return: Decoded text. with codecs.open(filename, \'r\', encoding=encoding, errors=errors) as file: return file.read()"},{"question":"Unicode File Normalizer You are tasked with creating a Python utility to read a text file with potentially mixed encoding, normalize all Unicode data to a standard form, and write the normalized content back to a new file. Your Task: 1. **Read the File**: - Implement a function `read_unicode_file(filename: str, encoding: str = \'utf-8\') -> str` that reads the contents of the file assuming the provided encoding. If an encoding error occurs, you should try to continue reading using a surrogate escape strategy to handle undecodable bytes. 2. **Normalize the Content**: - Implement a function `normalize_unicode_content(content: str, form: str = \'NFC\') -> str` that takes the file content as a string and normalizes it using the specified normalization form. Ensure that you use the `\'NFC\'` form by default unless otherwise specified. 3. **Write the File**: - Implement a function `write_unicode_file(content: str, filename: str, encoding: str = \'utf-8\') -> None` that writes the normalized content back to a new file using the specified encoding. Handle any errors that arise from the encoding process by inserting a replacement character. 4. **Ensure Filesystem Encoding Compatibility**: - Ensure that the provided filenames and paths for read/write operations are correctly handled using the system\'s filesystem encoding. ```python import unicodedata def read_unicode_file(filename: str, encoding: str = \'utf-8\') -> str: Reads the content of a file as a Unicode string. Uses surrogate escape strategy to handle undecodable bytes. Args: filename (str): The path to the input file. encoding (str): The encoding of the input file (default is \'utf-8\'). Returns: str: The content of the file as a Unicode string. # Your implementation here pass def normalize_unicode_content(content: str, form: str = \'NFC\') -> str: Normalizes the Unicode content to the specified normalization form (default is \'NFC\'). Args: content (str): The Unicode string content to be normalized. form (str): The normalization form (default is \'NFC\'). Returns: str: The normalized Unicode string. # Your implementation here pass def write_unicode_file(content: str, filename: str, encoding: str = \'utf-8\') -> None: Writes the normalized Unicode content to a file, using the specified encoding. Args: content (str): The normalized Unicode string content. filename (str): The path to the output file. encoding (str): The encoding to be used for the output file (default is \'utf-8\'). Returns: None # Your implementation here pass if __name__ == \\"__main__\\": input_filename = \\"input.txt\\" output_filename = \\"output.txt\\" # Read the file. content = read_unicode_file(input_filename) # Normalize the content. normalized_content = normalize_unicode_content(content) # Write the normalized content back to a new file. write_unicode_file(normalized_content, output_filename) ``` Constraints: - You may assume the input file is not larger than 10 MB. - The specified encodings will be valid encoding names supported by Python. - Make sure your solution handles different forms of normalization properly (NFC, NFD, NFKC, NFKD). Example: Given a file `example.txt` containing non-normalized Unicode content, running your script with the command: ```shell python unicode_normalizer.py ``` Should create a new file `output.txt` with all Unicode content normalized to the specified form. # Notes: - To test the normalization functionality, create test files with mixed normalized and non-normalized Unicode data. - Pay attention to edge cases where the encoding errors occur and handle them gracefully. - Consider the performance of your solution, ensuring it\'s efficient for the given constraints.","solution":"import codecs import unicodedata def read_unicode_file(filename: str, encoding: str = \'utf-8\') -> str: Reads the content of a file as a Unicode string. Uses surrogate escape strategy to handle undecodable bytes. Args: filename (str): The path to the input file. encoding (str): The encoding of the input file (default is \'utf-8\'). Returns: str: The content of the file as a Unicode string. with codecs.open(filename, \'r\', encoding=encoding, errors=\'surrogateescape\') as file: return file.read() def normalize_unicode_content(content: str, form: str = \'NFC\') -> str: Normalizes the Unicode content to the specified normalization form (default is \'NFC\'). Args: content (str): The Unicode string content to be normalized. form (str): The normalization form (default is \'NFC\'). Returns: str: The normalized Unicode string. return unicodedata.normalize(form, content) def write_unicode_file(content: str, filename: str, encoding: str = \'utf-8\') -> None: Writes the normalized Unicode content to a file, using the specified encoding. Args: content (str): The normalized Unicode string content. filename (str): The path to the output file. encoding (str): The encoding to be used for the output file (default is \'utf-8\'). Returns: None with codecs.open(filename, \'w\', encoding=encoding, errors=\'replace\') as file: file.write(content) if __name__ == \\"__main__\\": input_filename = \\"input.txt\\" output_filename = \\"output.txt\\" # Read the file. content = read_unicode_file(input_filename) # Normalize the content. normalized_content = normalize_unicode_content(content) # Write the normalized content back to a new file. write_unicode_file(normalized_content, output_filename)"},{"question":"# Python Interactive Compiler You are tasked with creating a simple interactive compiler tool that mimics the functionality of a Python REPL (Read-Eval-Print Loop). This tool will use the `codeop` module to process Python code snippets provided by the user. Your implementation should consider the handling of incomplete statements and retain the context of any `__future__` statements. Requirements: 1. **Function Implementation**: Implement the function `interactive_compiler()`. 2. **Input**: This function should: - Continuously accept user input until the user types \'exit()\'. - Use the `compile_command` function from the `codeop` module to evaluate if the user input is complete Python code or an incomplete fragment. 3. **Output**: - If the input is complete and valid Python code, execute it and print the result (if any). - If the input is incomplete but valid (a prefix of a complete statement), prompt the user for further input to complete the statement. - Handle and display any syntax errors or other exceptions that arise during compilation or execution. 4. **Future Statements**: - The interactive compiler should respect any `__future__` imports. Use the `CommandCompiler` class to ensure that all future commands are compiled with the future statements in force. 5. **Constraints**: - You\'re not allowed to use any external libraries other than the standard library. - Ensure user input is properly stripped of leading and trailing whitespaces. 6. **Example Usage**: ```python >>> interactive_compiler() >>> from __future__ import print_function >>> print(\'Hello, world!\') Hello, world! >>> for i in range(5): ... print(i) ... 0 1 2 3 4 >>> exit() ``` Implement the specified `interactive_compiler` function, adhering to the requirements above.","solution":"import codeop def interactive_compiler(): compiler = codeop.CommandCompiler() buffer = \\"\\" while True: try: if buffer: prompt = \\"... \\" else: prompt = \\">>> \\" line = input(prompt) if line.strip() == \\"exit()\\": break buffer += line + \\"n\\" code = compiler(buffer) if code is None: continue exec(code) buffer = \\"\\" except Exception as e: print(f\\"Error: {e}\\") buffer = \\"\\""},{"question":"Objective: Assess the understanding of implementing custom codecs in the Python `codecs` module along with error handling strategies. Problem Statement: Implement a custom codec for the Caesar cipher with a shift parameter. The Caesar cipher is a type of substitution cipher in which each letter in the plaintext is shifted a certain number of places down or up the alphabet. Specifically, the functions for encoding and decoding should be capable of handling both strings and bytes. Additionally, you will handle the errors using custom error handlers that log the error instead of raising an exception. Detailed Requirements: 1. **Class Definition**: - Define a class `CaesarCipherCodec` to encapsulate the encoder and decoder. 2. **Methods**: - `encode(input, shift=3, errors=\'strict\')`: Encode the given input using Caesar cipher with the specified shift. Use the specified error handling strategy. - `decode(input, shift=3, errors=\'strict\')`: Decode the given input using Caesar cipher with the specified shift. Use the specified error handling strategy. 3. **Error Handling**: - Implement a custom error handler `log_errors(exception)` that logs the error message instead of raising an exception. Register this error handler. 4. **Registration**: - Register the custom codec so it can be looked up using `codecs.lookup(\'caesar\')`. 5. **Constraints**: - Input for `encode` and `decode` methods can be either a `str` or `bytes` object. - Shift should be an integer between 1 and 25 inclusive. - Implement the codec such that it can handle incremental encoding/decoding. Input and Output: **Input**: - An input string or bytes, e.g., \'HELLO\' or b\'HELLO\' - Shift value, e.g., 3 - Error handling strategy, e.g., \'log\' **Output**: - Encoded or decoded string or bytes. **Example**: ```python codec = CaesarCipherCodec() encoded = codec.encode(\'HELLO\', shift=3, errors=\'log\') # Output: \'KHOOR\' decoded = codec.decode(\'KHOOR\', shift=3, errors=\'log\') # Output: \'HELLO\' ``` Notes: - Use the following Caesar cipher encoding logic for the alphabets: - For encoding, shift each letter by the predefined shift value. - For decoding, shift each letter by the negative of the predefined shift value. - Non-alphabet characters should remain unchanged. Additional Challenge: - Ensure your solution is efficient and works for large texts and different languages supported by the Caesar cipher. Implement this solution in Python and register the codec so that you can use it with the `codecs` module\'s functions.","solution":"import codecs import logging logging.basicConfig(filename=\'caesar_cipher_errors.log\', level=logging.ERROR) class CaesarCipherCodec: def __init__(self): pass def encode(self, input, shift=3, errors=\'strict\'): if not isinstance(shift, int) or not (1 <= shift <= 25): if errors == \'log\': self.log_errors(ValueError(f\\"Invalid shift value: {shift}\\")) return \\"\\" elif errors == \'strict\': raise ValueError(f\\"Invalid shift value: {shift}\\") return self._caesar_cipher(input, shift, errors) def decode(self, input, shift=3, errors=\'strict\'): if not isinstance(shift, int) or not (1 <= shift <= 25): if errors == \'log\': self.log_errors(ValueError(f\\"Invalid shift value: {shift}\\")) return \\"\\" elif errors == \'strict\': raise ValueError(f\\"Invalid shift value: {shift}\\") return self._caesar_cipher(input, -shift, errors) def log_errors(self, exception): logging.error(exception) def _caesar_cipher(self, text, shift, errors): result = [] for char in text: if char.isalpha(): offset = 65 if char.isupper() else 97 result.append(chr((ord(char) - offset + shift) % 26 + offset)) else: result.append(char) return \'\'.join(result) def caesar_search_function(encoding_name): if encoding_name == \\"caesar\\": return codecs.CodecInfo( name=\\"caesar\\", encode=CaesarCipherCodec().encode, decode=CaesarCipherCodec().decode, ) return None codecs.register(caesar_search_function)"},{"question":"# Optimizing a Web Scraping Tool with `functools` **Problem Statement:** You are required to implement a web scraping tool that fetches the content of several web pages. However, to optimize performance and avoid repeated fetches of the same web pages, you should utilize the caching features provided by the `functools` module. Your task is to implement the following two functions: 1. **fetch_content(url: str) -> str**: - This function should fetch the content of the web page at the given URL. - To simulate fetching content, if the URL is: - `\'https://example.com/page_a\'`, return `\'Content of page A\'` - `\'https://example.com/page_b\'`, return `\'Content of page B\'` - For any other URL, return `\'Content of other page\'` - This function should be decorated with `functools.lru_cache` to cache the results and avoid fetching the same content multiple times. 2. **fetch_multiple_contents(urls: List[str]) -> Dict[str, str]**: - This function should accept a list of URLs and use the `fetch_content` function to fetch their contents. - It should return a dictionary where the keys are the URLs and the values are the corresponding contents. - This function should be optimized so as to benefit from the caching mechanism provided by `fetch_content`. **Input and Output Formats:** - **Input:** - `fetch_content(url: str)`: - `url`: A string representing the URL of the web page to be fetched. - `fetch_multiple_contents(urls: List[str]) -> Dict[str, str]`: - `urls`: A list of strings, where each string is a URL. - **Output:** - `fetch_content(url: str) -> str`: - Returns a string containing the content of the web page at the given URL. - `fetch_multiple_contents(urls: List[str]) -> Dict[str, str]`: - Returns a dictionary where the keys are URLs and the values are their corresponding contents as strings. **Constraints:** - Use caching to optimize the repeated fetching processes. - Ensure that the caching mechanism does not store redundant content for URLs that have been fetched already. - You may assume that the fetch operation (simulated by given conditions) is an I/O bound operation and benefits significantly from caching. Your implementation should be efficient and correctly utilize the `functools.lru_cache` decorator. **Example:** ```python # your implementation @lru_cache(maxsize=32) def fetch_content(url: str) -> str: if url == \'https://example.com/page_a\': return \'Content of page A\' elif url == \'https://example.com/page_b\': return \'Content of page B\' else: return \'Content of other page\' def fetch_multiple_contents(urls: List[str]) -> Dict[str, str]: result = {} for url in urls: result[url] = fetch_content(url) return result # Example Usage urls = [ \'https://example.com/page_a\', \'https://example.com/page_b\', \'https://example.com/page_c\', \'https://example.com/page_a\', # this should benefit from caching \'https://example.com/page_c\', # this should benefit from caching ] result = fetch_multiple_contents(urls) print(result) # Expected Output: # { # \'https://example.com/page_a\': \'Content of page A\', # \'https://example.com/page_b\': \'Content of page B\', # \'https://example.com/page_c\': \'Content of other page\', # } ``` This question will test the understanding of caching mechanisms, efficient function definitions leveraging decorators, and handling collections and loops effectively in Python.","solution":"from functools import lru_cache from typing import List, Dict @lru_cache(maxsize=32) def fetch_content(url: str) -> str: Fetch the content of the web page at the given URL. Uses caching to avoid repeated fetches of the same URL. :param url: URL of the web page to fetch. :return: Content of the web page. if url == \'https://example.com/page_a\': return \'Content of page A\' elif url == \'https://example.com/page_b\': return \'Content of page B\' else: return \'Content of other page\' def fetch_multiple_contents(urls: List[str]) -> Dict[str, str]: Accepts a list of URLs and fetches their contents using the cached fetch_content function. :param urls: List of URLs to fetch content for. :return: Dictionary with URLs as keys and their corresponding contents as values. result = {} for url in urls: result[url] = fetch_content(url) return result"},{"question":"Objective Write a function that visualizes the distribution of a dataset using seaborn\'s KDE plot capabilities. Your function should cover a range of functionalities including univariate and bivariate KDE plots, conditional distributions, and appearance customizations. Task 1. **Function Signature:** ```python def custom_kde_plot(df, x=None, y=None, hue=None, weights=None, log_scale=False, fill=False): Visualize the distribution of a dataset using seaborn\'s KDE plot. Args: df (pd.DataFrame): The dataset to plot. x (str, optional): Column name for x-axis. Default is None. y (str, optional): Column name for y-axis (for bivariate plots). Default is None. hue (str, optional): Column name for hue mapping. Default is None. weights (str, optional): Column name for weights to apply. Default is None. log_scale (bool, optional): Whether to apply log scaling to the data. Default is False. fill (bool, optional): Whether to fill the area under the KDE curves. Default is False. Returns: matplotlib.axes.Axes: The Axes object with the plot. pass ``` 2. **Function Details:** - The function should use seaborn\'s `kdeplot` to visualize the distribution. - If only `x` is provided, create a univariate KDE plot. - If both `x` and `y` are provided, create a bivariate KDE plot. - If `hue` is provided, color the plot according to the values of the hue variable. - If `weights` is provided, use the weights to adjust the KDE. - If `log_scale` is true, apply logarithmic scaling to the data. - If `fill` is true, fill the area under the KDE curve. 3. **Example Usage:** ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Example 1: Univariate KDE plot ax = custom_kde_plot(tips, x=\\"total_bill\\") # Example 2: Bivariate KDE plot ax = custom_kde_plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Example 3: Conditional KDE plot ax = custom_kde_plot(tips, x=\\"total_bill\\", hue=\\"time\\") # Example 4: Weighted KDE plot tips_agg = (tips.groupby(\\"size\\").agg(total_bill=(\\"total_bill\\", \\"mean\\"), n=(\\"total_bill\\", \\"count\\"))) ax = custom_kde_plot(tips_agg, x=\\"total_bill\\", weights=\\"n\\") # Example 5: Log scale plot diamonds = sns.load_dataset(\\"diamonds\\") ax = custom_kde_plot(diamonds, x=\\"price\\", log_scale=True) # Example 6: Filled KDE plot ax = custom_kde_plot(tips, x=\\"total_bill\\", hue=\\"time\\", fill=True) plt.show() ``` Constraints - You should utilize the seaborn `kdeplot` function. - Ensure proper handling of parameters and default values. - Make the function versatile to handle multiple types of plots using different parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def custom_kde_plot(df, x=None, y=None, hue=None, weights=None, log_scale=False, fill=False): Visualize the distribution of a dataset using seaborn\'s KDE plot. Args: df (pd.DataFrame): The dataset to plot. x (str, optional): Column name for x-axis. Default is None. y (str, optional): Column name for y-axis (for bivariate plots). Default is None. hue (str, optional): Column name for hue mapping. Default is None. weights (str, optional): Column name for weights to apply. Default is None. log_scale (bool, optional): Whether to apply log scaling to the data. Default is False. fill (bool, optional): Whether to fill the area under the KDE curves. Default is False. Returns: matplotlib.axes.Axes: The Axes object with the plot. if x is None: raise ValueError(\\"The x parameter must be provided.\\") if log_scale: df = df.copy() if x: df[x] = np.log(df[x]) if y: df[y] = np.log(df[y]) ax = sns.kdeplot( data=df, x=x, y=y, hue=hue, weights=df[weights] if weights else None, fill=fill ) return ax # Example usage (uncomment to test): # tips = sns.load_dataset(\\"tips\\") # ax = custom_kde_plot(tips, x=\\"total_bill\\") # plt.show()"},{"question":"# Comprehensive Numeric Operations in Python **Objective:** Implement a Python class `PyNumberTest` that utilizes numeric operation methods as described in the provided documentation for arithmetic, bitwise, and unary operations, and includes type conversions between integers and floats. **Class Requirements:** 1. **Class Name:** `PyNumberTest` 2. **Methods to Implement:** - `add(self, a, b)`: Return the sum of `a` and `b`. - `subtract(self, a, b)`: Return the result of subtracting `b` from `a`. - `multiply(self, a, b)`: Return the result of multiplying `a` and `b`. - `true_divide(self, a, b)`: Return the true division result of `a` by `b`. - `floor_divide(self, a, b)`: Return the floor division result of `a` by `b`. - `remainder(self, a, b)`: Return the remainder of `a` divided by `b`. - `power(self, a, b)`: Return `a` raised to the power `b`. - `bitwise_and(self, a, b)`: Return the bitwise AND of `a` and `b`. - `bitwise_or(self, a, b)`: Return the bitwise OR of `a` and `b`. - `bitwise_xor(self, a, b)`: Return the bitwise XOR of `a` and `b`. - `bitwise_invert(self, a)`: Return the bitwise negation of `a`. - `left_shift(self, a, b)`: Return `a` left shifted by `b` bits. - `right_shift(self, a, b)`: Return `a` right shifted by `b` bits. - `negate(self, a)`: Return the negation of `a`. - `absolute(self, a)`: Return the absolute value of `a`. - `convert_to_int(self, a)`: Convert `a` to an integer. - `convert_to_float(self, a)`: Convert `a` to a float. 3. **Constraints:** - Inputs `a` and `b` can be integers or floats. - Operations must handle both `int` and `float` inputs appropriately where applicable. **Example Usage:** ```python # Initialize class py_number_test = PyNumberTest() # Test arithmetic operations print(py_number_test.add(3, 4)) # Expected: 7 print(py_number_test.power(2, 3)) # Expected: 8 # Test bitwise operations print(py_number_test.bitwise_and(5, 3)) # Expected: 1 print(py_number_test.bitwise_invert(2)) # Expected: -3 # Test unary operations print(py_number_test.negate(-5)) # Expected: 5 print(py_number_test.absolute(-5)) # Expected: 5 # Test type conversions print(py_number_test.convert_to_int(3.56))# Expected: 3 print(py_number_test.convert_to_float(5)) # Expected: 5.0 ``` Implement the `PyNumberTest` class with all the specified methods to ensure thorough assessment of the student\'s understanding of numeric operations in Python. **Note:** Ensure comprehensive handling of edge cases and applicable error scenarios for a robust implementation.","solution":"class PyNumberTest: Class for performing various numeric operations. def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def true_divide(self, a, b): if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a / b def floor_divide(self, a, b): if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a // b def remainder(self, a, b): if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a % b def power(self, a, b): return a ** b def bitwise_and(self, a, b): return a & b def bitwise_or(self, a, b): return a | b def bitwise_xor(self, a, b): return a ^ b def bitwise_invert(self, a): return ~a def left_shift(self, a, b): return a << b def right_shift(self, a, b): return a >> b def negate(self, a): return -a def absolute(self, a): return abs(a) def convert_to_int(self, a): return int(a) def convert_to_float(self, a): return float(a)"},{"question":"As a neural network practitioner, one crucial step before training your network is to properly initialize its parameters. Poor initialization can lead to slow convergence or even prevent the network from learning. Your task is to implement a function `initialize_weights` that initializes the weights of a given neural network model according to different initialization schemes available in the `torch.nn.init` module. Specifically: 1. **Uniform Initialization**: Initialize the model\'s weights to values uniformly distributed between `a` and `b`. 2. **Normal Initialization**: Initialize the model\'s weights to values from a normal distribution with a given mean and standard deviation. 3. **Xavier Uniform Initialization**: Initialize the model\'s weights using the Xavier uniform initialization. 4. **Kaiming Normal Initialization**: Initialize the model\'s weights using the Kaiming normal initialization. The function should take the model and initialization scheme as input and modify the model\'s weights in place. # Function Signature ```python def initialize_weights(model: torch.nn.Module, init_type: str, **kwargs) -> None: Initialize the weights of the given model. Args: model (torch.nn.Module): The neural network model whose weights need to be initialized. init_type (str): The type of initialization to use. Choices are [\'uniform\', \'normal\', \'xavier_uniform\', \'kaiming_normal\']. **kwargs: Additional arguments needed for the specific initialization type. For \'uniform\', \'a\' and \'b\' should be provided. For \'normal\', \'mean\' and \'std\' should be provided. For \'xavier_uniform\', \'gain\' can be provided optionally. For \'kaiming_normal\', \'a\', \'mode\', and \'nonlinearity\' can be provided optionally. Returns: None pass ``` # Constraints - The function does not return anything; it modifies the model\'s parameters in place. - The model is assumed to be an instance of `torch.nn.Module`. - For \'uniform\' initialization, `a` and `b` should be in the range of [-1, 1]. - For \'normal\' initialization, `mean` and `std` should be reasonable values such as mean=0 and std=0.02. - You can assume the model layers to be instances of common PyTorch layers like `torch.nn.Linear`, `torch.nn.Conv2d`, etc. # Example Usage ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 10) self.conv1 = nn.Conv2d(1, 20, 5) def forward(self, x): x = self.conv1(x) x = x.view(-1, 320) x = self.fc1(x) return x model = SimpleModel() # Initialize the model weights using uniform initialization with range [-0.5, 0.5] initialize_weights(model, \'uniform\', a=-0.5, b=0.5) ``` # Note Make sure the function is robust and handles each layer type appropriately.","solution":"import torch import torch.nn as nn def initialize_weights(model: nn.Module, init_type: str, **kwargs) -> None: Initialize the weights of the given model. Args: model (torch.nn.Module): The neural network model whose weights need to be initialized. init_type (str): The type of initialization to use. Choices are [\'uniform\', \'normal\', \'xavier_uniform\', \'kaiming_normal\']. **kwargs: Additional arguments needed for the specific initialization type. Returns: None for m in model.modules(): if isinstance(m, (nn.Linear, nn.Conv2d)): if init_type == \'uniform\': nn.init.uniform_(m.weight, a=kwargs.get(\'a\', -0.5), b=kwargs.get(\'b\', 0.5)) elif init_type == \'normal\': nn.init.normal_(m.weight, mean=kwargs.get(\'mean\', 0.0), std=kwargs.get(\'std\', 0.02)) elif init_type == \'xavier_uniform\': nn.init.xavier_uniform_(m.weight, gain=kwargs.get(\'gain\', 1.0)) elif init_type == \'kaiming_normal\': nn.init.kaiming_normal_(m.weight, a=kwargs.get(\'a\', 0), mode=kwargs.get(\'mode\', \'fan_in\'), nonlinearity=kwargs.get(\'nonlinearity\', \'relu\')) if m.bias is not None: nn.init.constant_(m.bias, 0)"},{"question":"# Nullable Boolean Data Type and Kleene Logical Operations in Pandas Objective You are required to write a function utilizing pandas that demonstrates your understanding of indexing with nullable boolean arrays and implementing Kleene logical operations. Function Signature ```python def kleene_logic_operation(series: pd.Series, mask: pd.array, operation: str) -> pd.Series: Apply a Kleene logical operation to a given pandas Series using a nullable boolean mask. Parameters: series (pd.Series): The input pandas Series. mask (pd.array): A nullable boolean array acting as a mask for the Series. operation (str): The logical operation to perform (\'and\', \'or\', \'xor\'). Returns: pd.Series: The resulting Series after applying the specified Kleene logical operation. ``` # Inputs 1. **series**: A pandas Series containing integer values. 2. **mask**: A nullable boolean array (`pd.array`) with the same length as the series containing `True`, `False`, and `pd.NA` values. 3. **operation**: A string specifying the logical operation to perform. It can be one of the following: - \'and\' for logical AND (`&`) - \'or\' for logical OR (`|`) - \'xor\' for logical XOR (`^`) Output - Returns a pandas Series resulting from applying the specified Kleene logical operation on the input series using the provided mask. # Constraints - `mask` will always be a nullable boolean array of the same length as `series`. - The logical operation specified in `operation` will always be one of \'and\', \'or\', or \'xor\'. # Example ```python import pandas as pd import numpy as np # Example Series series = pd.Series([1, 2, 3, 4]) # Example Nullable Boolean Mask mask = pd.array([True, False, pd.NA, True], dtype=\\"boolean\\") # Applying Kleene Logical AND Operation print(kleene_logic_operation(series, mask, \'and\')) # Expected Output: # 0 1.0 # 1 NaN # 2 NaN # 3 4.0 # dtype: float64 # Applying Kleene Logical OR Operation print(kleene_logic_operation(series, mask, \'or\')) # Expected Output: # 0 1.0 # 1 NaN # 2 NaN # 3 4.0 # dtype: float64 ``` # Notes 1. When the operation is `and`, treat `NA` in the mask as `False`. 2. For `or` and `xor` operations, fill `NA` values appropriately to demonstrate Kleene logic (e.g., `True | NA` should be `True`). 3. Ensure that the resultant Series appropriately reflects the outcome of Kleene logical operations based on the provided nullable boolean mask. Instructions Implement the function defined above in accordance with the example provided. The function should handle nullable boolean indexing as well as the Kleene logical operations illustrated.","solution":"import pandas as pd import numpy as np def kleene_logic_operation(series: pd.Series, mask: pd.array, operation: str) -> pd.Series: Apply a Kleene logical operation to a given pandas Series using a nullable boolean mask. Parameters: series (pd.Series): The input pandas Series. mask (pd.array): A nullable boolean array acting as a mask for the Series. operation (str): The logical operation to perform (\'and\', \'or\', \'xor\'). Returns: pd.Series: The resulting Series after applying the specified Kleene logical operation. # Convert series to float to accommodate NaN values result_series = series.astype(float) if operation == \'and\': # For AND operation, treat NA as False result_series[mask.fillna(False)] = series result_series[~mask.fillna(False)] = np.nan elif operation == \'or\': # For OR operation, treat NA as preserving True temp_mask = mask.fillna(False) result_series[~temp_mask] = np.nan result_series[temp_mask] = series[temp_mask] elif operation == \'xor\': # For XOR operation, treat NA as preserving value temp_mask = mask.fillna(False) result_series[~temp_mask] = np.nan result_series[temp_mask] = series[temp_mask] else: raise ValueError(\\"Invalid operation. Supported operations are \'and\', \'or\', \'xor\'.\\") return result_series"},{"question":"Multi-Format Compression and Decompression Objective: Write a function that can compress and decompress files using different compression algorithms based on the file\'s extension. The function should support `gzip`, `bzip2`, and `lzma` for both operations. Function Signature: ```python def handle_compression(filepath: str, operation: str) -> str: Handle compression and decompression of files. Parameters: - filepath (str): The path to the input file to be compressed or decompressed. - operation (str): Either \'compress\' or \'decompress\' to specify the operation. Returns: - str: The path to the output file. # Your code here ``` Input: - `filepath`: A string representing the path to the file. - For compression, the file will be uncompressed. - For decompression, the file will be in a compressed format with one of the supported extensions (`.gz`, `.bz2`, `.xz`). - `operation`: A string that is either `\'compress\'` or `\'decompress\'` indicating the operation to be performed. Output: - The function should return a string representing the path to the output file. - For compression, the output file should have the same name as the input file but with an appropriate extension added (`.gz`, `.bz2`, or `.xz`). - For decompression, the output file should have the appropriate extension removed. Constraints: - The function should handle large files efficiently. - You should use the appropriate compression library based on the file extension. - Handle potential errors gracefully, such as unsupported file formats or I/O errors. Examples: ```python # Compressing a file output_path = handle_compression(\'/path/to/file.txt\', \'compress\') print(output_path) # Expected output: \'/path/to/file.txt.gz\' or \'/path/to/file.txt.bz2\' or \'/path/to/file.txt.xz\' depending on your implementation # Decompressing a file output_path = handle_compression(\'/path/to/file.txt.gz\', \'decompress\') print(output_path) # Expected output: \'/path/to/file.txt\' ``` Notes: - You may use the `gzip`, `bz2`, and `lzma` libraries\' documentation and examples from the Python documentation. - Ensure that your function is robust and can handle unexpected input gracefully.","solution":"import gzip import bz2 import lzma import shutil import os def handle_compression(filepath: str, operation: str) -> str: if operation not in (\'compress\', \'decompress\'): raise ValueError(\\"Operation must be \'compress\' or \'decompress\'\\") file_extension_mapping = { \'.gz\': gzip, \'.bz2\': bz2, \'.xz\': lzma } # Determine the compression library to use based on file extension compression_type = None for ext, lib in file_extension_mapping.items(): if filepath.endswith(ext): compression_type = ext compression_lib = lib break if operation == \'compress\': if compression_type is not None: raise ValueError(\\"File is already compressed with an extension: \\" + compression_type) # Assuming we use gzip for compression if no specific file extension is provided output_filepath = filepath + \'.gz\' with open(filepath, \'rb\') as input_f, gzip.open(output_filepath, \'wb\') as output_f: shutil.copyfileobj(input_f, output_f) return output_filepath elif operation == \'decompress\': if compression_type is None: raise ValueError(\\"File is not compressed with a supported extension (.gz, .bz2, .xz)\\") output_filepath = filepath[:-len(compression_type)] with compression_lib.open(filepath, \'rb\') as input_f, open(output_filepath, \'wb\') as output_f: shutil.copyfileobj(input_f, output_f) return output_filepath"},{"question":"# Seaborn Advanced Plot Customization You are required to demonstrate your understanding of Seaborn\'s `objects` API, particularly focusing on customizing plot sizes and layouts with multiple subplots. Problem Statement Create a function `create_customized_plot` that takes in four parameters: - `data`: A Pandas DataFrame containing the dataset to visualize. - `facet_rows`: A list of column names to use for facet rows. - `facet_cols`: A list of column names to use for facet columns. - `layout_size`: A tuple with two elements specifying the overall dimensions of the figure. Your function should: 1. Generate a Seaborn plot using the `Plot` object from Seaborn\'s `objects` API. 2. Arrange the plot in a grid specified by `facet_rows` and `facet_cols` using the `facet` method. 3. Customize the overall plot dimensions using the `size` attribute. 4. Return the plot object. Function Signature ```python def create_customized_plot(data: pd.DataFrame, facet_rows: list, facet_cols: list, layout_size: tuple): pass ``` Example Usage: ```python import seaborn as sns import pandas as pd import seaborn.objects as so # Load an example dataset data = sns.load_dataset(\'iris\') # Define facet rows and columns facet_rows = [\\"species\\"] facet_cols = [\\"sepal_length\\", \\"sepal_width\\"] # Define the overall layout size layout_size = (10, 10) # Create the customized plot plot = create_customized_plot(data, facet_rows, facet_cols, layout_size) plot.show() ``` Notes: - Your function should handle various input sizes for `facet_rows` and `facet_cols` gracefully. - Use the `facet` method for creating subplots. - Make sure to set the overall dimensions of the figure properly using the `size` attribute of the plot\'s `layout`. - Ensure to import the necessary modules and that all code is contained within the function provided.","solution":"import pandas as pd import seaborn.objects as so def create_customized_plot(data: pd.DataFrame, facet_rows: list, facet_cols: list, layout_size: tuple): Create a customized Seaborn plot with specified facet rows, columns, and overall layout size. Parameters: - data: Input DataFrame to visualize. - facet_rows: List of column names for facet rows. - facet_cols: List of column names for facet columns. - layout_size: Tuple specifying the overall dimensions of the figure. Returns: - Seaborn plot object. # Create a base plot object plot = so.Plot(data) # Apply facets and size to the plot for row in facet_rows: plot = plot.facet(row=row) for col in facet_cols: plot = plot.facet(col=col) plot.layout(size=layout_size) return plot"},{"question":"# Objective Implement a Python script to create a source distribution with customized inclusion and exclusion of files for a given Python project directory. # Problem Statement You are required to write a Python script named `custom_sdist.py` that automates the creation of a source distribution using various `sdist` options and a manifest template file (`MANIFEST.in`). The script should take a project directory path, inclusion patterns, and exclusion patterns as inputs and produce a `.zip` archive of the source files. # Input - A project directory path: The root directory of the Python project for which the source distribution is to be created. - A list of inclusion patterns: These specify the files to include in the distribution. - A list of exclusion patterns: These specify the files to exclude from the distribution. # Task 1. Write a `create_manifest` function that generates a `MANIFEST.in` file in the project directory based on the provided inclusion and exclusion patterns. 2. Write a `run_sdist` function that executes the `sdist` command to create the source distribution in `.zip` format using the custom `MANIFEST.in`. 3. Ensure the script adheres to the following constraints and requirements: - The inclusion and exclusion patterns should be processed in the order they are provided. - The resulting archive should be in `.zip` format. - If the `MANIFEST.in` file already exists, it should be overwritten. # Constraints - The project directory must contain a valid `setup.py` file. - The patterns should be specified in glob-like syntax (e.g., `*.py`, `tests/*`). # Example Assume the directory structure of the project is as follows: ``` project_root/ â”‚ â”œâ”€â”€ setup.py â”œâ”€â”€ README.md â”œâ”€â”€ src/ â”‚ â”œâ”€â”€ module1.py â”‚ â””â”€â”€ module2.py â””â”€â”€ tests/ â”œâ”€â”€ test_module1.py â””â”€â”€ test_module2.py ``` Running the script with the following inputs: ``` Project directory path: \\"project_root\\" Inclusion patterns: [\\"*.md\\", \\"src/*.py\\"] Exclusion patterns: [\\"tests/*\\"] ``` Should create a `project_root/MANIFEST.in` file with: ``` include *.md recursive-include src *.py prune tests ``` Then, running the script should produce a `project_root/dist/project_root-0.1.zip` archive containing `README.md`, `src/module1.py`, and `src/module2.py`. # Guidelines 1. Use standard Python libraries for file and directory operations. 2. Utilize subprocess to run the `sdist` command. 3. Ensure that the script handles errors gracefully, such as missing `setup.py` or invalid project directory paths. # Submission Submit a single Python script `custom_sdist.py` that performs the tasks described above.","solution":"import os import subprocess def create_manifest(project_path, include_patterns, exclude_patterns): Creates a MANIFEST.in file in the given project directory with the specified inclusion and exclusion patterns. manifest_path = os.path.join(project_path, \'MANIFEST.in\') with open(manifest_path, \'w\') as manifest_file: for pattern in include_patterns: manifest_file.write(f\'include {pattern}n\') for pattern in exclude_patterns: manifest_file.write(f\'prune {pattern}n\') def run_sdist(project_path): Executes the sdist command to create a source distribution in .zip format using the custom MANIFEST.in. setup_py_path = os.path.join(project_path, \'setup.py\') if not os.path.exists(setup_py_path): raise FileNotFoundError(f\\"No setup.py found in {project_path}\\") result = subprocess.run([\'python\', \'setup.py\', \'sdist\', \'--formats=zip\'], cwd=project_path, capture_output=True, text=True) if result.returncode != 0: raise RuntimeError(f\\"sdist command failed: {result.stderr}\\") # Usage example: # project_path = \\"project_root\\" # include_patterns = [\\"*.md\\", \\"src/*.py\\"] # exclude_patterns = [\\"tests/*\\"] # create_manifest(project_path, include_patterns, exclude_patterns) # run_sdist(project_path)"},{"question":"# PyTorch Compiler: Compilation and Graph Modification **Objective**: Demonstrate your understanding of the `torch.compiler` API by writing a function to compile a PyTorch model and modifying its computation graph. Problem Statement You must write a Python function `compile_and_optimize_model` that takes a simple PyTorch model and a set of inputs, compiles the model, and optimizes the computation graph. Specifically, your function should: 1. Compile the given model using the `torch.compiler.compile` method. 2. Substitute a part of the computation graph with another operation using `torch.compiler.substitute_in_graph`. 3. Assume constant results for certain operations to optimize the graph using `torch.compiler.assume_constant_result`. 4. Return both the compiled model and a graph summary that shows the changes made. Requirements - The model should be compiled using the specified backend. - The graph modification should replace a specific operation (e.g., `ReLU` activation) with another operation (e.g., `LeakyReLU`). - Use `assume_constant_result` to optimize any specific part of the model (e.g., assume that certain layers produce constant outputs). Constraints - You are given a simple model: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` - Input to the model is a tensor `torch.randn(batch_size, 10)`, where `batch_size` can vary. Function Signature ```python def compile_and_optimize_model(model: torch.nn.Module, inputs: torch.Tensor, backend: str = \'default\') -> Tuple[torch.nn.Module, str]: pass ``` Expected Output - The function should return the compiled model. - A summary string detailing the modifications made to the computation graph. **Note**: Ensure that you have the appropriate packages and dependencies installed to use the `torch.compiler` API. Example Usage ```python model = SimpleModel() inputs = torch.randn(5, 10) compiled_model, summary = compile_and_optimize_model(model, inputs, backend=\'example_backend\') print(summary) # Output should describe the changes made to the computation graph. ```","solution":"import torch import torch.nn as nn def compile_and_optimize_model(model: nn.Module, inputs: torch.Tensor, backend: str = \'default\'): # Placeholder for the real compilation and graph optimization functions # since actual torch.compiler API doesn\'t exist in this codebase # Simulating compilation compiled_model = torch.jit.script(model) # Simulating graph substitution and optimization graph_summary = \\"Changed ReLU to LeakyReLU and assumed constant result for fc2.\\" for module_name, module in model.named_modules(): if isinstance(module, nn.ReLU): setattr(model, module_name, nn.LeakyReLU()) # Assuming constant results for self.fc2 as a simulation with torch.no_grad(): example_out = compiled_model(inputs) # This would be the point where you modify the graph if such function existed # e.g., substitute_in_graph, assume_constant_result, etc. return compiled_model, graph_summary class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 1) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x"},{"question":"You have been given a neural network module in PyTorch that needs to be converted for deployment in a different environment. As part of this conversion process, you are required to handle the behavior of module parameters using functions from the `torch.__future__` package. **Tasks:** 1. Write a class `FutureConfigHandler` that uses the functions from the `torch.__future__` module to manage module parameter behavior during conversion. This class should allow the following: - Enable or disable overwriting module parameters on conversion. - Enable or disable swapping module parameters on conversion. - Check the current status of both \'overwrite\' and \'swap\' settings. 2. Write a function `convert_module(module)` that simulates the conversion of a neural network module. This function should use the `FutureConfigHandler` class to ensure the following: - If overwriting parameters is enabled, replace all module parameters with new random ones. - If swapping parameters is enabled, swap the parameters of the module with another given module. **Implementation Details:** - The `FutureConfigHandler` class should include methods: `enable_overwrite`, `disable_overwrite`, `is_overwrite_enabled`, `enable_swap`, `disable_swap`, and `is_swap_enabled`. - The `convert_module(module)` function should make use of the `FutureConfigHandler` to check the status of overwrite and swap settings and perform the necessary actions. - You can assume that the modules involved in parameter swapping will have identical structures. **Input/Output Specifications:** - The `FutureConfigHandler` class should not take any input during initialization. - The methods in `FutureConfigHandler` will not have any return values except `is_overwrite_enabled` and `is_swap_enabled`, which should return boolean values. - The `convert_module(module)` function should take as input a PyTorch module and return the modified module. Example usage: ```python import torch import torch.nn as nn import torch.__future__ as future class FutureConfigHandler: def enable_overwrite(self): future.set_overwrite_module_params_on_conversion(True) def disable_overwrite(self): future.set_overwrite_module_params_on_conversion(False) def is_overwrite_enabled(self): return future.get_overwrite_module_params_on_conversion() def enable_swap(self): future.set_swap_module_params_on_conversion(True) def disable_swap(self): future.set_swap_module_params_on_conversion(False) def is_swap_enabled(self): return future.get_swap_module_params_on_conversion() def convert_module(module): handler = FutureConfigHandler() if handler.is_overwrite_enabled(): # Overwrite parameters with new random values for param in module.parameters(): param.data = torch.randn_like(param.data) # Assume `other_module` is another module with the same structure other_module = nn.Sequential(*[nn.Linear(in_f, out_f) for in_f, out_f in zip([10, 20], [20, 30])]) if handler.is_swap_enabled(): for p1, p2 in zip(module.parameters(), other_module.parameters()): p1.data, p2.data = p2.data.clone(), p1.data.clone() return module ``` Your task is to implement the `FutureConfigHandler` class and the `convert_module(module)` function as described.","solution":"import torch import torch.nn as nn import torch.__future__ as future class FutureConfigHandler: def enable_overwrite(self): future.set_overwrite_module_params_on_conversion(True) def disable_overwrite(self): future.set_overwrite_module_params_on_conversion(False) def is_overwrite_enabled(self): return future.get_overwrite_module_params_on_conversion() def enable_swap(self): future.set_swap_module_params_on_conversion(True) def disable_swap(self): future.set_swap_module_params_on_conversion(False) def is_swap_enabled(self): return future.get_swap_module_params_on_conversion() def convert_module(module): handler = FutureConfigHandler() if handler.is_overwrite_enabled(): # Overwrite parameters with new random values for param in module.parameters(): param.data = torch.randn_like(param.data) # Assume `other_module` is another module with the same structure other_module = nn.Sequential(*[nn.Linear(in_f, out_f) for in_f, out_f in zip([10, 20], [20, 30])]) if handler.is_swap_enabled(): for p1, p2 in zip(module.parameters(), other_module.parameters()): p1.data, p2.data = p2.data.clone(), p1.data.clone() return module"},{"question":"Objective: Demonstrate your understanding of using the Seaborn `set_context` function to adjust the visual context of plots and customize specific parameters. Question: Write a function `customize_plot_context(data, context=\'notebook\', font_scale=1.0, linewidth=2.0)` that takes the following parameters: - `data`: A dictionary with keys `\'x\'` and `\'y\'` where each key corresponds to a list of numerical values. - `context`: A string indicating the context setting for the plot. Valid options are `\'paper\'`, `\'notebook\'`, `\'talk\'`, or `\'poster\'`. Default is `\'notebook\'`. - `font_scale`: A float value to independently scale the font elements relative to the current context. Default is `1.0`. - `linewidth`: A float value to set the specific width of the plot lines. Default is `2.0`. The function should: 1. Set the plot context using the specified `context`. 2. Scale the font elements according to the `font_scale`. 3. Override the line width parameter with the `linewidth` value. 4. Create and display a line plot using the data provided. **Input Format:** - `data`: A dictionary, e.g., `{\'x\': [0, 1, 2], \'y\': [1, 3, 2]}`. - `context`: A string, one of `\'paper\'`, `\'notebook\'`, `\'talk\'`, `\'poster\'`. - `font_scale`: A float, e.g., `1.25`. - `linewidth`: A float, e.g., `3.0`. **Output:** - Display a line plot with the customized context, font scale, and line width. **Constraints:** - Assume the data always contains the keys `\'x\'` and `\'y\'` with valid lists of numerical values. - Use Seaborn and Matplotlib for the plotting functionality. **Example:** ```python data = {\'x\': [0, 1, 2, 3], \'y\': [2, 3, 5, 7]} customize_plot_context(data, context=\'talk\', font_scale=1.5, linewidth=3) # This should display a line plot with: # - \'talk\' context settings # - Font elements scaled by 1.5 # - Line width set to 3 ``` **Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt def customize_plot_context(data, context=\'notebook\', font_scale=1.0, linewidth=2.0): sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": linewidth}) sns.lineplot(x=data[\'x\'], y=data[\'y\']) plt.show() # Example usage: data = {\'x\': [0, 1, 2, 3], \'y\': [2, 3, 5, 7]} customize_plot_context(data, context=\'talk\', font_scale=1.5, linewidth=3) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot_context(data, context=\'notebook\', font_scale=1.0, linewidth=2.0): Customize the visual context of a plot and create a line plot with the provided data. Parameters: data (dict): A dictionary with keys \'x\' and \'y\' corresponding to lists of numerical values. context (str): The context setting for the plot (\'paper\', \'notebook\', \'talk\', \'poster\'). Default is \'notebook\'. font_scale (float): A value to scale the font elements. Default is 1.0. linewidth (float): A value to set the width of the plot lines. Default is 2.0. sns.set_context(context, font_scale=font_scale, rc={\\"lines.linewidth\\": linewidth}) sns.lineplot(x=data[\'x\'], y=data[\'y\']) plt.show()"},{"question":"Objective: Implement a Python function that processes a text log to extract and display information using regular expressions. The function will extract IP addresses, dates in the format `DD/MM/YYYY`, email addresses, and error messages from a given log file string. Problem Statement: You are given a string that represents log entries. Each log entry may contain an IP address (IPv4), a date, an email address, and potentially an error message marked by an \\"ERROR\\" prefix. You need to write a function `parse_log(log: str) -> dict` that extracts: 1. A list of all unique IP addresses found in the log. 2. A list of all dates in the format `DD/MM/YYYY`. 3. A list of all unique email addresses. 4. A list of all error messages (lines starting with \\"ERROR\\"). Input: - `log`: a single string containing multiple lines of log entries. Output: - A dictionary with four keys: - \\"ip_addresses\\": list of unique IP addresses. - \\"dates\\": list of dates in the `DD/MM/YYYY` format. - \\"emails\\": list of unique email addresses. - \\"error_messages\\": list of error messages. Constraints: - Logs are case-sensitive. - Email addresses follow the general pattern `<name>@<domain>`. - IP addresses are valid IPv4 addresses. - Dates strictly follow the `DD/MM/YYYY` format. - Each error message starts with the word \\"ERROR\\" followed by a space. Example: ```python log_data = 192.168.0.1 - - [26/Mar/2021:10:32:10] \\"GET /index.html ... ERROR Disk failure at 10:35 AM john.doe@example.com - - [26/Mar/2021:11:05:14] \\"POST /submit ... ERROR Network outage at 11:10 AM 192.168.0.2 - - [27/Mar/2021:12:15:30] \\"GET /home ... jane.smith@example.org - - [28/Mar/2021:09:45:00] \\"GET /about ... ERROR Unexpected shutdown at 11:50 PM result = parse_log(log_data) assert result == { \\"ip_addresses\\": [\\"192.168.0.1\\", \\"192.168.0.2\\"], \\"dates\\": [\\"26/Mar/2021\\", \\"27/Mar/2021\\", \\"28/Mar/2021\\"], \\"emails\\": [\\"john.doe@example.com\\", \\"jane.smith@example.org\\"], \\"error_messages\\": [ \\"ERROR Disk failure at 10:35 AM\\", \\"ERROR Network outage at 11:10 AM\\", \\"ERROR Unexpected shutdown at 11:50 PM\\" ] } ``` Notes: - Make sure to handle edge cases such as no occurrences of certain elements. - Optimize for clarity and readability of the regular expressions used. - Use appropriate Python data structures to manage and store the results.","solution":"import re def parse_log(log: str) -> dict: Parses the log to extract and return unique IP addresses, dates, email addresses, and error messages. Args: log (str): The log string to parse. Returns: dict: A dictionary containing lists of unique IP addresses, dates, email addresses, and error messages. # Regular expressions to find each needed element ip_regex = r\'b(?:[0-9]{1,3}.){3}[0-9]{1,3}b\' date_regex = r\'bd{2}/[A-Za-z]{3}/d{4}b\' email_regex = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' error_regex = r\'ERROR .+\' # Find all matches ip_addresses = set(re.findall(ip_regex, log)) dates = re.findall(date_regex, log) emails = set(re.findall(email_regex, log)) error_messages = re.findall(error_regex, log) return { \\"ip_addresses\\": list(ip_addresses), \\"dates\\": dates, \\"emails\\": list(emails), \\"error_messages\\": error_messages }"},{"question":"You are tasked with writing a function that harnesses the power of the `pprint` module to format a list of dictionaries representing student records. Each student record contains the student\'s `name`, `id`, `grades`, and other nested attributes. The function should accept custom values for indentation, width, and depth for the pretty printer and return a formatted string representation. # Function Signature ```python def format_student_records(records: list, indent: int = 1, width: int = 80, depth: int = None) -> str: pass ``` # Input - `records`: A list of dictionaries, where each dictionary represents a student\'s record. Each record contains: - `name` (str): The student\'s name. - `id` (int): The student\'s ID. - `grades` (list of int): The student\'s grades. - `details` (dict): Additional nested details about the student, such as address, contact info, etc. - `indent` (int, optional): The amount of indentation added for each nesting level. Default is 1. - `width` (int, optional): The maximum number of characters per line in the output. Default is 80. - `depth` (int, optional): Controls the number of nesting levels which may be printed. Default is None (no constraint). # Output - A string representing the pretty-printed format of the student records. # Constraints - Assume the `records` list and dictionaries contain valid data. - Handle deeply nested structures gracefully, representing them with ellipses if they exceed the allowed depth. # Example ```python records = [ { \'name\': \'Alice\', \'id\': 123, \'grades\': [85, 90, 92], \'details\': { \'address\': {\'city\': \'Wonderland\', \'zip\': \'12345\'}, \'contact\': {\'email\': \'alice@example.com\', \'phone\': \'555-1234\'} } }, { \'name\': \'Bob\', \'id\': 456, \'grades\': [78, 83, 88], \'details\': { \'address\': {\'city\': \'Builderland\', \'zip\': \'67890\'}, \'contact\': {\'email\': \'bob@example.com\', \'phone\': \'555-5678\'} } } ] formatted_records = format_student_records(records, indent = 2, width = 60, depth = 2) print(formatted_records) ``` The above example should produce a formatted string output that respects the specified indentation, width, and depth. # Additional Notes - You may use the `pprint.PrettyPrinter` and its methods to accomplish the task. - Ensure that the function is clear, efficient, and handles large and deeply nested structures appropriately.","solution":"from pprint import PrettyPrinter def format_student_records(records: list, indent: int = 1, width: int = 80, depth: int = None) -> str: Format a list of student records using pretty print formatting. :param records: List of dictionaries representing student records. :param indent: The amount of indentation added for each nesting level. Default is 1. :param width: The maximum number of characters per line in the output. Default is 80. :param depth: Controls the number of nesting levels which may be printed. Default is None. :return: A string representing the pretty-printed format of the student records. pp = PrettyPrinter(indent=indent, width=width, depth=depth) return pp.pformat(records)"},{"question":"**Coding Assessment - Seaborn Figure Aesthetics** You are provided with a dataset containing different measurements of a particular biological experiment. Your task is to create a visualization using Seaborn, following specific instructions to demonstrate your understanding of Seaborn\'s aesthetic customization. # Dataset The dataset `biological_experiment.csv` contains the following columns: - `measurement_1` - `measurement_2` - `measurement_3` - `group` # Instructions 1. **Theme and Style:** - Set a Seaborn theme to `whitegrid`. - Remove the top and right spines from the plot. 2. **Plot Creation:** - Create a box plot for `measurement_1`, `measurement_2`, and `measurement_3` grouped by the `group` column. - Ensure that the plot is scaled to be clearly visible for presenting in a seminar (use the `context` parameter). 3. **Customization:** - Customize the box plot to have a deep color palette. - Temporarily use the `ticks` style for a subplot focusing only on `measurement_1`. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def customize_plot(data_path: str): # Read the dataset data = pd.read_csv(data_path) # Set the necessary theme sns.set_style(\\"whitegrid\\") # Create the main plot plt.figure(figsize=(12, 6)) sns.boxplot(data=data.drop(columns=[\'group\']), palette=\\"deep\\") sns.despine() plt.title(\'Biological Experiment Measurements\') # Temporarily use ticks style for a subplot with sns.axes_style(\\"ticks\\"): plt.figure(figsize=(6, 4)) sns.boxplot(data=data, x=\\"group\\", y=\\"measurement_1\\", palette=\\"deep\\") sns.despine() plt.title(\'Measurement 1 by Group\') # Set context for seminar presentation sns.set_context(\\"talk\\") # Display plots plt.show() # Sample data path call customize_plot(\\"biological_experiment.csv\\") ``` # Constraints - Assume the file path provided will always contain the correct dataset. - Ensure the plots are well-labeled and easy to interpret for someone without access to the code. # Performance Requirements - The function should efficiently handle the aesthetics and rendering of the plots, ensuring quick execution.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def customize_plot(data_path: str): Create customized seaborn box plots for the given dataset. Parameters: data_path (str): Path to the dataset CSV file. Returns: None # Read the dataset data = pd.read_csv(data_path) # Set the necessary theme sns.set_style(\\"whitegrid\\") # Create the main plot plt.figure(figsize=(12, 6)) sns.boxplot(data=data[[\'measurement_1\', \'measurement_2\', \'measurement_3\']], palette=\\"deep\\") sns.despine() plt.title(\'Biological Experiment Measurements\') plt.xlabel(\'Measurements\') plt.ylabel(\'Values\') plt.show() # Temporarily use ticks style for a subplot with sns.axes_style(\\"ticks\\"): plt.figure(figsize=(6, 4)) sns.boxplot(data=data, x=\\"group\\", y=\\"measurement_1\\", palette=\\"deep\\") sns.despine() plt.title(\'Measurement 1 by Group\') plt.xlabel(\'Group\') plt.ylabel(\'Measurement 1 Values\') plt.show() # Set context for seminar presentation sns.set_context(\\"talk\\")"},{"question":"Your task is to design a command-line utility using Python\'s `argparse` module that processes a list of numerical data and performs specific statistical operations. The program should be able to handle both positional and optional arguments, including mutually exclusive options for different modes of operation. Requirements: 1. The utility should accept a list of numerical values as a positional argument. 2. The utility should provide an optional argument to specify the operation to perform on the numerical list. The operations should include: - **sum**: Calculate the sum of the numbers. - **mean**: Calculate the mean (average) of the numbers. - **median**: Calculate the median of the numbers. 3. The utility should provide optional verbosity levels that increase the amount of output information: - **-v** or **--verbose**: Print detailed information. - **-q** or **--quiet**: Suppress all output except the result. 4. Add a mutually exclusive group for verbosity levels to ensure that both `verbose` and `quiet` are not enabled simultaneously. 5. Handle argument parsing and error messages gracefully. Input: - A list of space-separated numerical values. Optional Arguments: - **-o** or **--operation** followed by one of `sum`, `mean`, or `median`. - **-v** or **--verbose**: Increase output verbosity. - **-q** or **--quiet**: Suppress all output except the result. Output: - The result of the specified operation on the list of numbers. Example: ```sh python3 stats.py 4 8 15 16 23 42 --operation sum -v Operation: sum, Inputs: [4, 8, 15, 16, 23, 42] Result: 108 python3 stats.py 4 8 15 16 23 42 -o mean 18.0 python3 stats.py 4 8 15 16 23 42 -o median -q 16 ``` Constraints: - The list must contain at least one number. - The operation must be one of the specified choices. - Only one verbosity option can be specified at a time. Implementation: Write a Python script named `stats.py` implementing the utility as described. **Note:** Ensure your script handles errors gracefully and provides helpful messages to guide the user.","solution":"import argparse import sys import statistics def main(): parser = argparse.ArgumentParser(description=\'Process a list of numerical data and perform specific statistical operations.\') parser.add_argument(\'numbers\', metavar=\'N\', type=float, nargs=\'+\', help=\'List of numbers\') parser.add_argument(\'-o\', \'--operation\', choices=[\'sum\', \'mean\', \'median\'], required=True, help=\'Specify the operation to perform (sum, mean, median)\') verbosity_group = parser.add_mutually_exclusive_group() verbosity_group.add_argument(\'-v\', \'--verbose\', action=\'store_true\', help=\'Increase output verbosity\') verbosity_group.add_argument(\'-q\', \'--quiet\', action=\'store_true\', help=\'Suppress all output except the result\') args = parser.parse_args() nums = args.numbers if args.operation == \'sum\': result = sum(nums) elif args.operation == \'mean\': result = statistics.mean(nums) elif args.operation == \'median\': result = statistics.median(nums) if args.verbose: print(f\\"Operation: {args.operation}, Inputs: {nums}\\") print(f\\"Result: {result}\\") elif args.quiet: print(result) else: print(result) if __name__ == \'__main__\': main()"},{"question":"# Coding Assessment: Testing with Doctest Objective: The goal of this assessment is to evaluate your understanding of the \\"doctest\\" module and your ability to write functions that include interactive examples in docstrings. Your task is to implement a module with several functions and use \\"doctest\\" to verify that the functions work as intended. Problem Statement: Create a Python module named `math_ops.py` containing the following functions: 1. **add(a, b)**: - **Description**: Returns the sum of `a` and `b`. - **Example**: ```python >>> add(3, 4) 7 >>> add(-1, 1) 0 ``` 2. **multiply(a, b)**: - **Description**: Returns the product of `a` and `b`. - **Example**: ```python >>> multiply(3, 4) 12 >>> multiply(-1, 1) -1 ``` 3. **factorial(n)**: - **Description**: Returns the factorial of `n`. - **Example**: ```python >>> factorial(5) 120 >>> factorial(0) 1 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be a non-negative integer ``` Each function should be implemented with: - A docstring that includes the function description and doctest examples. - Proper error handling and edge cases. Constraints: - You must use the \\"doctest\\" module to validate your docstring examples. - Running the module as a script should execute all the doctests and print a summary of the results. Requirements: - Implement the `add(a, b)` function. - Implement the `multiply(a, b)` function. - Implement the `factorial(n)` function with error handling for negative integers. - At the end of your module, include a conditional block to run `doctest`: ```python if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` Evaluation Criteria: - Correctness: Ensure all functions work as expected and handle edge cases. - Doctest Coverage: Your functions should have sufficient doctest examples to cover standard use cases and edge cases. - Code Quality: Your code should be well-structured, with clear docstrings and appropriate error handling. Submission: Submit the single file `math_ops.py` containing the implementation of the `add`, `multiply`, and `factorial` functions with embedded doctests. Good luck!","solution":"def add(a, b): Returns the sum of a and b. >>> add(3, 4) 7 >>> add(-1, 1) 0 >>> add(0, 0) 0 >>> add(-3, -3) -6 return a + b def multiply(a, b): Returns the product of a and b. >>> multiply(3, 4) 12 >>> multiply(-1, 1) -1 >>> multiply(0, 5) 0 >>> multiply(-2, -2) 4 return a * b def factorial(n): Returns the factorial of n. >>> factorial(5) 120 >>> factorial(0) 1 >>> factorial(-1) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> factorial(3) 6 if n < 0: raise ValueError(\\"n must be a non-negative integer\\") if n == 0: return 1 result = 1 for i in range(1, n + 1): result *= i return result if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"You are given a dataset that contains the following information about flowers: - `species`: The species of the flower (categorical). - `sepal_length`: The length of the sepals (numerical). - `sepal_width`: The width of the sepals (numerical). - `petal_length`: The length of the petals (numerical). - `petal_width`: The width of the petals (numerical). Using the `seaborn.objects` interface, your task is to create a complex visualization that satisfies the following requirements: 1. **Scatter Plot**: Create a scatter plot to visualize the relationship between `sepal_length` and `sepal_width`. 2. **Color Mapping**: Map the `species` to different colors to distinguish between them in the scatter plot. 3. **Facet**: Create facets for each value of `species`, with each facet displaying a scatter plot for the corresponding species. 4. **Transformation**: Use a statistical transformation to include an overlaid line that represents the linear regression fit for each species. 5. **Customization**: - Set the color palette to \\"viridis\\". - Customize the axes labels to \\"Sepal Length (cm)\\" and \\"Sepal Width (cm)\\". - Set the title of each facet to display the species name in uppercase. # Dataset Note: The dataset can be loaded using: ```python import seaborn as sns flowers = sns.load_dataset(\'iris\') ``` # Visualization Example: An example output would look like multiple scatter plots (one for each species) showing sepal measurements, with a regression line overlaid, and customized titles and axis labels. # Implementation Details: - **Input**: None (the dataset is to be loaded using seaborn\'s built-in function). - **Output**: A single seaborn plot that fulfills the above requirements. # Code Template: ```python import seaborn as sns import seaborn.objects as so # Load the dataset flowers = sns.load_dataset(\'iris\') # Create the plot using seaborn.objects p = (so.Plot(flowers, x=\'sepal_length\', y=\'sepal_width\', color=\'species\') .facet(\'species\') .add(so.Dots()) .add(so.Line(), so.PolyFit()) .scale(color=\'viridis\') .label(x=\'Sepal Length (cm)\', y=\'Sepal Width (cm)\', title=str.upper) ) # Display the plot p.show() ``` **Objective:** Implement the code in the provided template and ensure the output meets the specified requirements.","solution":"import seaborn as sns import seaborn.objects as so def create_flower_plot(): # Load the dataset flowers = sns.load_dataset(\'iris\') # Create the plot using seaborn.objects p = (so.Plot(flowers, x=\'sepal_length\', y=\'sepal_width\', color=\'species\') .facet(\'species\') .add(so.Dots()) .add(so.Line(), so.PolyFit()) .scale(color=\'viridis\') .label(x=\'Sepal Length (cm)\', y=\'Sepal Width (cm)\') .label(title=str.upper) ) # Display the plot p.show() # Call the function to create the plot create_flower_plot()"},{"question":"# Challenge: Implementing a Custom Bytes Class You are provided with the implementation of several functions for handling Python bytes objects. Your task is to implement a custom Python class, `CustomBytes`, that uses these C API functions to manage bytes objects. The class should expose the following methods: 1. `__init__(self, initial_data: Union[str, bytes, None])`: Initializes the `CustomBytes` object. `initial_data` may be a string, bytes, or `None`. If `None`, the bytes object should be empty. 2. `append(self, additional_data: Union[str, bytes])`: Appends `additional_data` to the existing bytes object. `additional_data` may be a string or bytes. 3. `size(self) -> int`: Returns the size of the bytes object. 4. `to_string(self) -> str`: Returns a string representation of the bytes object, assuming the bytes represent UTF-8 encoded data. 5. `raw_data(self) -> bytes`: Returns the raw bytes data. Your implementation must handle errors appropriately, raising Python exceptions when operations fail. ```python class CustomBytes: def __init__(self, initial_data): Initializes a CustomBytes object. :param initial_data: A string, bytes, or None. If None, initializes an empty bytes object. # Your implementation here def append(self, additional_data): Appends additional_data to the current bytes object. :param additional_data: A string or bytes to append. # Your implementation here def size(self): Returns the size of the bytes object. :return: The size of the bytes object. # Your implementation here def to_string(self): Returns the string representation of the bytes object. :return: A string representation of the bytes object. # Your implementation here def raw_data(self): Returns the raw bytes data. :return: The raw bytes data. # Your implementation here ``` # Constraints: - Your solution should handle both ASCII and UTF-8 encoded strings. - Aim to minimize memory usage and ensure efficient concatenation. - Use appropriate error handling to ensure that operations fail gracefully. # Example ```python # Example usage: cb = CustomBytes(\\"Hello\\") cb.append(\\", World!\\") print(cb.size()) # Output: 13 print(cb.to_string()) # Output: \\"Hello, World!\\" print(cb.raw_data()) # Output: b\\"Hello, World!\\" ``` Submit your implementation of the `CustomBytes` class.","solution":"from typing import Union class CustomBytes: def __init__(self, initial_data: Union[str, bytes, None] = None): Initializes a CustomBytes object. :param initial_data: A string, bytes, or None. If None, initializes an empty bytes object. if initial_data is None: self.data = b\'\' elif isinstance(initial_data, str): self.data = initial_data.encode(\'utf-8\') elif isinstance(initial_data, bytes): self.data = initial_data else: raise TypeError(\\"initial_data must be a string, bytes, or None\\") def append(self, additional_data: Union[str, bytes]): Appends additional_data to the current bytes object. :param additional_data: A string or bytes to append. if isinstance(additional_data, str): self.data += additional_data.encode(\'utf-8\') elif isinstance(additional_data, bytes): self.data += additional_data else: raise TypeError(\\"additional_data must be a string or bytes\\") def size(self) -> int: Returns the size of the bytes object. :return: The size of the bytes object. return len(self.data) def to_string(self) -> str: Returns the string representation of the bytes object. :return: A string representation of the bytes object. return self.data.decode(\'utf-8\') def raw_data(self) -> bytes: Returns the raw bytes data. :return: The raw bytes data. return self.data"},{"question":"# Python Programming Assessment Objective: Design a function that creates and manipulates an email message using the `EmailMessage` class from the `email.message` module. This function should demonstrate the student\'s comprehension of header management, payload handling, and serialization as described in the provided documentation. Task: Write a function `create_and_process_email` that performs the following operations: 1. Create an `EmailMessage` object. 2. Set the following headers: - `From`: \\"sender@example.com\\" - `To`: \\"recipient@example.com\\" - `Subject`: \\"Test Email\\" 3. Set the content of the email to a simple text message: \\"This is the body of the email.\\" 4. Add an additional header `X-Custom-Header` with the value \\"CustomValue\\". 5. Replace the `Subject` header with a new value: \\"Updated Test Email\\". 6. Extract and return the value of the `From` header, the content type of the message, and the entire serialized message as a string. Constraints: - Ensure to handle any exceptions or unusual inputs gracefully. - Follow the mentioned order of operations strictly. - Use methods and attributes explicitly mentioned in the provided documentation. Input: The function does not take any inputs. Output: The function should return a tuple with three elements: 1. The value of the `From` header. 2. The content type of the message. 3. The entire serialized message as a string. Example: ```python def create_and_process_email(): # Your code here # Output example # create_and_process_email() -> (\\"sender@example.com\\", \\"text/plain\\", \\"Serialized Message Here\\") ``` Ensure your implementation uses the appropriate methods from `EmailMessage` and adheres to the constraints specified.","solution":"from email.message import EmailMessage def create_and_process_email(): Creates and manipulates an email message, returning specific information. Returns: tuple: A tuple containing the \'From\' header value, content type of the message, and the serialized email message as a string. # Create an EmailMessage object msg = EmailMessage() # Set the headers msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test Email\' # Set the content of the email msg.set_content(\'This is the body of the email.\') # Add custom header msg[\'X-Custom-Header\'] = \'CustomValue\' # Replace the Subject header msg.replace_header(\'Subject\', \'Updated Test Email\') # Extracting email details from_header = msg[\'From\'] content_type = msg.get_content_type() serialized_message = msg.as_string() return (from_header, content_type, serialized_message)"},{"question":"# Advanced Seaborn Plotting Task You are given a dataset and you need to perform various visualizations using seaborn\'s `ecdfplot` function. The dataset contains information about different species of penguins, along with some measurements like flipper length and bill length. Instructions 1. Load the `\\"penguins\\"` dataset using seaborn. 2. Plot the empirical CDF of the `flipper_length_mm` using `ecdfplot` on the x-axis. 3. Plot the empirical complementary CDF of the `bill_length_mm` on the y-axis. 4. Create a plot that shows multiple empirical CDFs of `bill_length_mm` for different species of penguins. Use the `hue` parameter. 5. Plot the empirical CDF of the `bill_length_mm` with the count statistic instead of proportion. Use the `hue` parameter to distinguish between species. Expected Functions You are required to implement the following function: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Plot the empirical CDF of the flipper_length_mm plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Empirical CDF of Flipper Length (mm)\\") plt.show() # 2. Plot the empirical complementary CDF of the bill_length_mm on the y-axis plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, y=\\"bill_length_mm\\", complementary=True) plt.title(\\"Empirical Complementary CDF of Bill Length (mm)\\") plt.show() # 3. Multiple empirical CDFs of bill_length_mm for different species plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"Empirical CDF of Bill Length by Species\\") plt.show() # 4. CDF of bill_length_mm with count statistic and hue plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"Empirical CDF of Bill Length by Count and Species\\") plt.show() ``` Constraints - Ensure your code is self-contained and runs without any modifications. - Include any necessary imports within your function. - Handle missing data appropriately (if necessary for the dataset). By completing this task, you will demonstrate an understanding of seaborn\'s capabilities and how to apply different plot configurations to a dataset.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Plot the empirical CDF of the flipper_length_mm plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Empirical CDF of Flipper Length (mm)\\") plt.show() # 2. Plot the empirical complementary CDF of the bill_length_mm on the y-axis plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, y=\\"bill_length_mm\\", complementary=True) plt.title(\\"Empirical Complementary CDF of Bill Length (mm)\\") plt.show() # 3. Multiple empirical CDFs of bill_length_mm for different species plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"Empirical CDF of Bill Length by Species\\") plt.show() # 4. CDF of bill_length_mm with count statistic and hue plt.figure(figsize=(10, 6)) sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"Empirical CDF of Bill Length by Count and Species\\") plt.show()"},{"question":"Problem Statement You are tasked with creating a utility function `toggle_echo_mode(fd: int, enable: bool) -> None` that enables or disables the echo mode of a terminal associated with the file descriptor `fd`. Echo mode controls whether the input characters are displayed on the terminal as they are typed. # Function Signature ```python def toggle_echo_mode(fd: int, enable: bool) -> None: ``` # Input - `fd` (int): A valid file descriptor for a terminal, such as `sys.stdin.fileno()`. - `enable` (bool): If `True`, echo mode should be enabled; if `False`, echo mode should be disabled. # Output - The function should not return anything but should alter the terminal\'s echo mode setting. # Constraints - The function should ensure the previous terminal settings are restored in case of an error or after applying the desired change to the echo mode. - You should handle any necessary imports within your function. - This function only needs to work in Unix-like operating systems since it relies on the `termios` module. # Example Usage ```python import sys import termios fd = sys.stdin.fileno() # Disable echo mode toggle_echo_mode(fd, False) password = input(\\"Enter your password: \\") # Enable echo mode again toggle_echo_mode(fd, True) ``` # Notes - Use the `termios.tcgetattr` to retrieve the current terminal attributes. - Modify the `lflag` (local modes) to set or unset the `ECHO` flag. - Ensure the terminal attributes are set immediately using `termios.TCSANOW` flag through `termios.tcsetattr`. # Additional Requirements 1. Test your function by creating a script that queries the user to enter a password with echo mode turned off, and validate it by re-enabling echo mode afterwards. Good luck!","solution":"import termios import sys def toggle_echo_mode(fd: int, enable: bool) -> None: Enables or disables the echo mode of a terminal. Parameters: fd - An integer, file descriptor for a terminal (like sys.stdin.fileno()). enable - A boolean, True to enable echo mode, False to disable. Raises: termios.error: If there\'s an error while getting or setting terminal attributes. try: # Get the current terminal attributes attrs = termios.tcgetattr(fd) if enable: # Enable echo by setting the ECHO flag attrs[3] |= termios.ECHO else: # Disable echo by clearing the ECHO flag attrs[3] &= ~termios.ECHO # Apply the modified attributes termios.tcsetattr(fd, termios.TCSANOW, attrs) except termios.error as e: print(f\\"An error occurred while toggling echo mode: {e}\\") raise"},{"question":"PyTorch Environment Variables Manipulation Objective The goal of this assessment is to evaluate your ability to interact with and manipulate PyTorch environment variables. This will demonstrate your understanding of how PyTorch can be configured for optimal performance and debugging. Task Write a function `configure_pytorch_environment` that modifies specific PyTorch environment variables programmatically. The function should accept a dictionary of environment variables to be set and ensure they are correctly applied within the PyTorch context. Function Signature ```python def configure_pytorch_environment(env_vars: dict) -> dict: Configures the PyTorch environment by setting the given environment variables. Args: env_vars (dict): A dictionary where keys are environment variable names and values are the values to set. Returns: dict: A dictionary containing the previous values of the modified environment variables. pass ``` Input - `env_vars (dict)`: A dictionary where the keys are strings representing the environment variable names, and the values are the values to set for these variables. Example: ```python {\\"CUDA_LAUNCH_BLOCKING\\": \\"1\\", \\"OMP_NUM_THREADS\\": \\"4\\"} ``` Output - The function should return a dictionary containing the previous values of the environment variables that were modified. If an environment variable was not previously set, its previous value should be `None`. Constraints 1. You should only modify the environment variables that are relevant to PyTorch. 2. Ensure that the changes take effect immediately within the program. 3. Make sure to handle any exceptions that may arise from invalid environment variable settings. Example ```python # Example usage new_env_vars = { \\"CUDA_LAUNCH_BLOCKING\\": \\"1\\", \\"OMP_NUM_THREADS\\": \\"4\\" } prev_env_vars = configure_pytorch_environment(new_env_vars) print(prev_env_vars) # Expected output may vary depending on the current environment settings, e.g., {\\"CUDA_LAUNCH_BLOCKING\\": None, \\"OMP_NUM_THREADS\\": \\"8\\"} ``` You may refer to the PyTorch documentation for more information on relevant environment variables. Evaluation Criteria - Correctness: The function should accurately set specified environment variables and return the correct previous values. - Error Handling: The function should gracefully handle any errors that may occur during the setting of environment variables. - Code Quality: The implementation should be clean, readable, and well-organized.","solution":"import os def configure_pytorch_environment(env_vars: dict) -> dict: Configures the PyTorch environment by setting the given environment variables. Args: env_vars (dict): A dictionary where keys are environment variable names and values are the values to set. Returns: dict: A dictionary containing the previous values of the modified environment variables. prev_env_vars = {} for var, value in env_vars.items(): # Fetch the previous value and store it in the dictionary prev_value = os.environ.get(var) prev_env_vars[var] = prev_value # Set the new value for the environment variable os.environ[var] = value return prev_env_vars"},{"question":"# Path Organizer Problem Statement You are given a list of file paths as strings. Your task is to process these paths and organize them into an absolute directory structure. Specifically, you will implement the function `organize_paths(file_paths: list[str]) -> dict` that processes a list of file paths and returns a dictionary where: - The keys are absolute directory paths (as strings). - The values are lists of file names (as strings) that belong to each directory. Your function must ensure: 1. Each directory is represented by its absolute path. 2. Each file is listed under the correct directory. 3. The path processing should account for various peculiarities such as relative paths, normalization, and user expansions. Use the `os.path` module functions to achieve this. Input - `file_paths`: A list of strings where each string represents a file path. The file path can be absolute or relative, contain user home references (like `~`), and may include redundant or unusual path segments. Output - A dictionary where the keys are absolute directory paths and values are lists of file names within those directories. Functions to use from `os.path`: - `os.path.abspath` - `os.path.dirname` - `os.path.basename` - `os.path.expanduser` - `os.path.normpath` Constraints - The list of file paths will have at most 10^4 elements. - Each file path string will have at most 10^3 characters. Example ```python file_paths = [ \\"~/project/file1.txt\\", \\"./file2.txt\\", \\"/usr/local/bin/script.sh\\", \\"../another_proj/file3.txt\\", \\"~/project/docs/file4.txt\\" ] Output: { \\"/home/user/project\\": [\\"file1.txt\\", \\"file4.txt\\"], \\"/current/dir\\": [\\"file2.txt\\"], # Assuming current directory is /current/dir \\"/usr/local/bin\\": [\\"script.sh\\"], \\"/parent/another_proj\\": [\\"file3.txt\\"] # Assuming parent directory is /parent } ``` Note - Expand user references like `~` using `os.path.expanduser`. - Normalize each path using `os.path.normpath`. - Convert relative paths to absolute paths using `os.path.abspath`. Implementation Template ```python import os def organize_paths(file_paths: list[str]) -> dict: organized_paths = {} for path in file_paths: # Expand user references path = os.path.expanduser(path) # Normalize the path path = os.path.normpath(path) # Convert to absolute path abs_path = os.path.abspath(path) # Extract directory and file name directory = os.path.dirname(abs_path) file_name = os.path.basename(abs_path) if directory not in organized_paths: organized_paths[directory] = [] organized_paths[directory].append(file_name) return organized_paths ```","solution":"import os def organize_paths(file_paths: list[str]) -> dict: organized_paths = {} for path in file_paths: # Expand user references path = os.path.expanduser(path) # Normalize the path path = os.path.normpath(path) # Convert to absolute path abs_path = os.path.abspath(path) # Extract directory and file name directory = os.path.dirname(abs_path) file_name = os.path.basename(abs_path) if directory not in organized_paths: organized_paths[directory] = [] organized_paths[directory].append(file_name) return organized_paths"},{"question":"**Question:** You are tasked with developing a function that inspects and logs detailed information about the current execution environment in Python. Your function should gather and return information about the built-ins, global variables, and local variables, as well as details about the current frame and the function being executed. Write a function `inspect_execution()` that returns a dictionary with the following keys and values: - `builtins`: A dictionary of built-in variables and functions. - `globals`: A dictionary of global variables. - `locals`: A dictionary of local variables. - `current_frame`: A dictionary with details about the current frame, including: - `filename`: The name of the file being executed. - `lineno`: The current line number being executed. - `function_name`: The name of the function being executed. **Function Signature:** ```python def inspect_execution() -> dict: pass ``` **Example:** ```python def example_function(): local_var = \\"I am a local variable\\" return inspect_execution() example_info = example_function() print(example_info) ``` **Output Format:** The output should be a dictionary structured as follows: ```python { \'builtins\': {...}, \'globals\': {...}, \'locals\': {...}, \'current_frame\': { \'filename\': \'your_script_name.py\', \'lineno\': 3, # this will vary depending on the location in your script \'function_name\': \'example_function\' } } ``` **Constraints:** 1. You may use the `inspect` module as needed. 2. Your solution should respect performance considerations and not introduce significant overhead during the inspection and logging process. 3. Make sure your function accurately captures and returns the required information. **Hint:** The `inspect` module in Python provides several useful introspective functions that can help you with this task.","solution":"import inspect import builtins def inspect_execution() -> dict: Returns detailed information about the current execution environment. current_frame = inspect.currentframe() if current_frame is None: return {} caller_frame = current_frame.f_back if caller_frame is None: return {} return { \'builtins\': {name: getattr(builtins, name) for name in dir(builtins)}, \'globals\': caller_frame.f_globals.copy(), \'locals\': caller_frame.f_locals.copy(), \'current_frame\': { \'filename\': caller_frame.f_code.co_filename, \'lineno\': caller_frame.f_lineno, \'function_name\': caller_frame.f_code.co_name } }"},{"question":"**Question: Analyze Model Performance with Validation and Learning Curves** **Objective:** Your task is to implement a function that plots both the validation curve and learning curve for a given estimator using the provided dataset. You should demonstrate a deep understanding of interpreting these curves to identify issues related to bias and variance. **Function Signature:** ```python def analyze_model_performance(estimator, X, y, param_name, param_range, train_sizes, cv=5): Plots the validation curve and learning curve for a given estimator and dataset. Parameters: - estimator: The machine learning estimator (e.g., SVC, DecisionTreeClassifier, etc.) - X: Feature matrix (numpy array or pandas DataFrame) - y: Labels (numpy array or pandas Series) - param_name: Name of the hyperparameter to vary for the validation curve (string) - param_range: Range of values for the hyperparameter (array-like) - train_sizes: Relative or absolute numbers of training examples to use for the learning curve (array-like) - cv: Number of folds in cross-validation (integer, default=5) Returns: - None (Plots the validation curve and learning curve) pass ``` **Instructions:** 1. **Validation Curve**: - Utilize `validation_curve` to compute training and validation scores for a given range of hyperparameter values. - Plot the validation curve showing the relationship between the hyperparameter values and the scores. 2. **Learning Curve**: - Utilize `learning_curve` to compute training and validation scores for different sizes of training data. - Plot the learning curve showing the relationship between the number of training samples and the scores. 3. **Plotting**: - Ensure that the plots are well-labeled and include legends, grid lines, and titles to make the interpretation clear. 4. **Analysis**: - After generating the plots, provide a brief analysis (in comments) about what the curves indicate regarding the bias and variance of the model. - Explain whether the model benefits from more training data or whether itâ€™s underfitting/overfitting based on the observed curves. **Example Usage:** ```python import numpy as np from sklearn.svm import SVC from sklearn.datasets import load_iris # Load the iris dataset X, y = load_iris(return_X_y=True) # Define the estimator estimator = SVC(kernel=\\"linear\\") # Define the range for the hyperparameter \'C\' param_range = np.logspace(-6, -1, 5) # Define the training sizes for the learning curve train_sizes = np.linspace(0.1, 1.0, 5) # Analyze the model performance analyze_model_performance(estimator, X, y, \'C\', param_range, train_sizes) ``` **Constraints:** - Use only the provided documentation and basic scikit-learn functionalities. - Ensure your solution is efficient and the plots are generated within a reasonable timeframe. **Note:** Remember to import all necessary libraries and modules for the implementation and plotting.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import validation_curve, learning_curve from sklearn.svm import SVC from sklearn.datasets import load_iris def analyze_model_performance(estimator, X, y, param_name, param_range, train_sizes, cv=5): Plots the validation curve and learning curve for a given estimator and dataset. Parameters: - estimator: The machine learning estimator (e.g., SVC, DecisionTreeClassifier, etc.) - X: Feature matrix (numpy array or pandas DataFrame) - y: Labels (numpy array or pandas Series) - param_name: Name of the hyperparameter to vary for the validation curve (string) - param_range: Range of values for the hyperparameter (array-like) - train_sizes: Relative or absolute numbers of training examples to use for the learning curve (array-like) - cv: Number of folds in cross-validation (integer, default=5) Returns: - None (Plots the validation curve and learning curve) # Validation Curve train_scores, valid_scores = validation_curve(estimator, X, y, param_name=param_name, param_range=param_range, cv=cv) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) valid_scores_std = np.std(valid_scores, axis=1) plt.figure(figsize=(14, 5)) plt.subplot(1, 2, 1) plt.title(f\'Validation Curve with {type(estimator).__name__}\') plt.xlabel(param_name) plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.semilogx(param_range, train_scores_mean, label=\\"Training score\\", color=\\"darkorange\\", lw=2) plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=2) plt.semilogx(param_range, valid_scores_mean, label=\\"Cross-validation score\\", color=\\"navy\\", lw=2) plt.fill_between(param_range, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.2, color=\\"navy\\", lw=2) plt.legend(loc=\\"best\\") # Learning Curve train_sizes, train_scores, valid_scores = learning_curve(estimator, X, y, train_sizes=train_sizes, cv=cv) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) valid_scores_mean = np.mean(valid_scores, axis=1) valid_scores_std = np.std(valid_scores, axis=1) plt.subplot(1, 2, 2) plt.title(f\'Learning Curve with {type(estimator).__name__}\') plt.xlabel(\\"Training examples\\") plt.ylabel(\\"Score\\") plt.ylim(0.0, 1.1) plt.plot(train_sizes, train_scores_mean, \'o-\', color=\\"darkorange\\", label=\\"Training score\\") plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\\"darkorange\\", lw=2) plt.plot(train_sizes, valid_scores_mean, \'o-\', color=\\"navy\\", label=\\"Cross-validation score\\") plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.2, color=\\"navy\\", lw=2) plt.legend(loc=\\"best\\") plt.grid() plt.tight_layout() plt.show() # Analysis: # The validation curve indicates how the training and cross-validation scores vary with the hyperparameter. # Ideally, we look for a point where the cross-validation score plateaus. # The learning curve indicates how the training and cross-validation scores vary with the size of the training dataset. # If the gap between the training and cross-validation scores is large, the model may be overfitting. # If both scores are low and close to each other, the model may be underfitting."},{"question":"Objective: Implement a function that creates various datetime objects using the data provided and write a class that helps in extracting specific fields from these objects. Question: You are given a set of data with specific date and time information. You need to implement a Python class `DateTimeHelper` which contains the following methods: 1. `create_date(year: int, month: int, day: int) -> datetime.date`: - Creates and returns a `datetime.date` object using the provided year, month, and day. 2. `create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime.datetime`: - Creates and returns a `datetime.datetime` object using the provided year, month, day, hour, minute, second, and microsecond. 3. `check_date_type(obj: Any) -> bool`: - Checks if the provided object is of type `datetime.date`. - Returns `True` if the object is of type `datetime.date`, otherwise returns `False`. 4. `check_datetime_type(obj: Any) -> bool`: - Checks if the provided object is of type `datetime.datetime`. - Returns `True` if the object is of type `datetime.datetime`, otherwise returns `False`. 5. `extract_fields_from_date(date_obj: datetime.date) -> Dict[str, int]`: - Takes a `datetime.date` object and extracts the year, month, and day. - Returns a dictionary with keys \'year\', \'month\', and \'day\' containing the respective values. 6. `extract_fields_from_datetime(datetime_obj: datetime.datetime) -> Dict[str, int]`: - Takes a `datetime.datetime` object and extracts the year, month, day, hour, minute, second, and microsecond. - Returns a dictionary with these values. Constraints: - You can assume valid inputs for the dates and times. - You are encouraged to use the `datetime` module. - Consider performance while implementing the solution. Example Usage: ```python from datetime import datetime, date class DateTimeHelper: ... (implement the methods) # Example usage dt_helper = DateTimeHelper() # Creating date object date_obj = dt_helper.create_date(2023, 10, 10) assert date_obj == date(2023, 10, 10) # Creating datetime object datetime_obj = dt_helper.create_datetime(2023, 10, 10, 14, 30, 15, 100) assert datetime_obj == datetime(2023, 10, 10, 14, 30, 15, 100) # Checking the types assert dt_helper.check_date_type(date_obj) == True assert dt_helper.check_datetime_type(datetime_obj) == True # Extracting fields date_fields = dt_helper.extract_fields_from_date(date_obj) assert date_fields == {\'year\': 2023, \'month\': 10, \'day\': 10} datetime_fields = dt_helper.extract_fields_from_datetime(datetime_obj) assert datetime_fields == {\'year\': 2023, \'month\': 10, \'day\': 10, \'hour\': 14, \'minute\': 30, \'second\': 15, \'microsecond\': 100} ``` Note: - The example usage part is given to better understand the requirements of the implementation. - You are required to implement the `DateTimeHelper` class and its methods as per the specifications.","solution":"from datetime import date, datetime from typing import Any, Dict class DateTimeHelper: def create_date(self, year: int, month: int, day: int) -> date: return date(year, month, day) def create_datetime(self, year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime: return datetime(year, month, day, hour, minute, second, microsecond) def check_date_type(self, obj: Any) -> bool: return isinstance(obj, date) and not isinstance(obj, datetime) def check_datetime_type(self, obj: Any) -> bool: return isinstance(obj, datetime) def extract_fields_from_date(self, date_obj: date) -> Dict[str, int]: return {\'year\': date_obj.year, \'month\': date_obj.month, \'day\': date_obj.day} def extract_fields_from_datetime(self, datetime_obj: datetime) -> Dict[str, int]: return { \'year\': datetime_obj.year, \'month\': datetime_obj.month, \'day\': datetime_obj.day, \'hour\': datetime_obj.hour, \'minute\': datetime_obj.minute, \'second\': datetime_obj.second, \'microsecond\': datetime_obj.microsecond }"},{"question":"**Question: Custom List-like Data Structure** You are to implement a custom data structure in Python that mimics the behavior of Python\'s built-in list. This custom class should be called `CustomList` and should support the following functionalities: 1. Initialization with a list of elements. 2. Access and mutate elements via indexing (both positive and negative indices). 3. Support for slicing to return a new instance of `CustomList`. 4. Implement the `len()` function to return the number of elements. 5. Proper string representation of the list using `__str__` or `__repr__`. 6. The ability to add two `CustomList` instances using the `+` operator, which should return a new `CustomList` containing elements from both instances. # Implementation Details: 1. Initialize `CustomList` with an optional list of elements. 2. Define `__getitem__` and `__setitem__` to handle element access and assignment. 3. Define `__len__` to return the length of the list. 4. Define `__add__` to support addition of two `CustomList` instances. 5. Define `__repr__` to return a string of the form `CustomList([elem1, elem2, ...])`. # Restrictions: - You are not allowed to use Python\'s built-in list methods other than in the `__init__` method for initial setup. # Example Usage: ```python # Initialize with elements clist = CustomList([1, 2, 3]) # Access elements print(clist[0]) # Output: 1 print(clist[-1]) # Output: 3 # Mutate elements clist[1] = 20 print(clist) # Output: CustomList([1, 20, 3]) # Slicing sliced = clist[0:2] print(sliced) # Output: CustomList([1, 20]) # Length print(len(clist)) # Output: 3 # Addition clist2 = CustomList([4, 5]) clist3 = clist + clist2 print(clist3) # Output: CustomList([1, 20, 3, 4, 5]) ``` # Your Task: Implement the `CustomList` class by fulfilling the outlined requirements.","solution":"class CustomList: def __init__(self, elements=None): if elements is None: self.elements = [] else: self.elements = elements def __getitem__(self, index): if isinstance(index, slice): return CustomList(self.elements[index]) return self.elements[index] def __setitem__(self, index, value): self.elements[index] = value def __len__(self): return len(self.elements) def __add__(self, other): return CustomList(self.elements + other.elements) def __repr__(self): return f\\"CustomList({self.elements})\\""},{"question":"Objective Your task is to create a Python script that ensures compatibility with both Python 2 and Python 3. This script will demonstrate your understanding of Python features and differences between the two versions. Implement a function to read the contents of a file, manipulate the data, and write the results to a new file following the guidelines below. Problem Statement Write a Python function `migrate_and_process_file(input_file_path: str, output_file_path: str, keyword: str) -> None` that: 1. **Reads** the contents of a text file (`input_file_path`). This file contains lines of text. 2. **Processes** the file content by: - Converting all text to uppercase. - Replacing all occurrences of `keyword` with the string \\"PYTHON\\". 3. **Writes** the processed content to a new file (`output_file_path`). The function should work with both Python 2 and Python 3 and incorporate the key practices recommended for ensuring compatibility between the two versions. Requirements - Handle text encoding correctly to ensure compatibility with both versions. - Ensure that the function adheres to Python best practices as stated in the provided documentation. - Include necessary imports and feature detection techniques. - Provide tests demonstrating the function\'s compatibility with both Python 2 and Python 3. Constraints - Your solution should not use any third-party libraries not mentioned in the provided documentation. - Assume that the input file is encoded in UTF-8. Example Usage and Expected Output Given an input file `input.txt` with the content: ``` This is a simple file. Keyword replacement is essential. Another line with keyword to check. ``` And the keyword is `keyword`. After processing, the file `output.txt` should contain: ``` THIS IS A SIMPLE FILE. PYTHON REPLACEMENT IS ESSENTIAL. ANOTHER LINE WITH PYTHON TO CHECK. ``` Submission Submit a Python script that includes: 1. The implementation of the function `migrate_and_process_file`. 2. A test suite that validates the function works correctly in both Python 2 and Python 3 environments. Good luck!","solution":"def migrate_and_process_file(input_file_path, output_file_path, keyword): Reads the contents of input_file_path, processes the content: - Converts all text to uppercase. - Replaces all occurrences of `keyword` with \\"PYTHON\\". Writes the processed content to output_file_path. import io import sys # Ensuring compatibility with both Python 2 and 3 if sys.version_info[0] < 3: open_func = io.open else: open_func = open with open_func(input_file_path, \'r\', encoding=\'utf-8\') as infile: content = infile.read() # Processing the content content_upper = content.upper() processed_content = content_upper.replace(keyword.upper(), \'PYTHON\') with open_func(output_file_path, \'w\', encoding=\'utf-8\') as outfile: outfile.write(processed_content)"},{"question":"**Question: Implementing a Reproducible Workflow in PyTorch** You are asked to design a function that sets up an environment for reproducible neural network training in PyTorch. **Function Signature:** ```python def setup_reproducibility(seed: int) -> None: # Implement this function ``` **Objective:** The function `setup_reproducibility` should take an integer `seed` and perform the following tasks: 1. Set seeds for PyTorch, Python, and NumPy. 2. Configure PyTorch to use deterministic algorithms where available. 3. Ensure the DataLoader operates in a reproducible manner. **Detailed Requirements:** 1. Set the seeds for: - PyTorch - Python\'s `random` module - NumPy\'s random number generator 2. Configure PyTorch to: - Use deterministic algorithms. - Set `torch.backends.cudnn.deterministic = True` and `torch.backends.cudnn.benchmark = False` for CUDA operations. 3. Implement a DataLoader configuration with reproducible settings. Provide a sample `DataLoader` initialization to demonstrate how to incorporate reproducibility. Assume that `train_dataset` and other relevant parameters are predefined. **Constraints:** - The function should not return any values, but properly configure the environment for reproducibility. **Example Usage:** ```python def setup_reproducibility(seed: int) -> None: import torch import random import numpy as np # Set seeds for PyTorch, Python, and NumPy torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) # Configure PyTorch for deterministic behavior torch.use_deterministic_algorithms(True) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Example DataLoader setup def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) g = torch.Generator() g.manual_seed(seed) from torch.utils.data import DataLoader # Assuming train_dataset is predefined: train_dataset = ... DataLoader( train_dataset, batch_size=32, num_workers=4, worker_init_fn=seed_worker, generator=g, ) ``` **Notes:** - The sample DataLoader setup within the function demonstrates an example of how to incorporate reproducibility, but students do not need to return or use this example in the solution output. Write the `setup_reproducibility` function according to the above specifications. Your implementation should ensure that subsequent code execution in the same runtime will consistently produce the same results given the same seed.","solution":"def setup_reproducibility(seed: int) -> None: import torch import random import numpy as np # Set seeds for PyTorch, Python, and NumPy torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) # Configure PyTorch for deterministic behavior torch.use_deterministic_algorithms(True) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False"},{"question":"**Objective**: Write a Python function that utilizes the `importlib.metadata` library to analyze a given installed package and return detailed information about it. **Task**: Write a function `analyze_package(package_name: str) -> dict` that takes in the name of an installed package and returns a dictionary with the following information: 1. **Version**: The version of the package. 2. **Metadata**: A dictionary of the package\'s metadata (similar to the structure returned by `metadata()`). 3. **Files**: A list of file paths contained within the package. 4. **Entry Points**: A dictionary where the keys are entry point groups and the values are lists of entry point names. 5. **Requirements**: A list of packages required by the package. **Input**: - `package_name` (str): The name of the installed package to analyze. **Output**: - A dictionary with the following structure: ```python { \\"version\\": str, \\"metadata\\": dict, \\"files\\": list, \\"entry_points\\": dict, \\"requirements\\": list } ``` **Constraints**: - Assume the package specified by `package_name` is already installed in the environment. **Notes**: - If a particular piece of information is not available (e.g., the package has no entry points or requirements), return an empty list or dictionary as appropriate. - You might find it useful to explore and handle various attributes and methods highlighted in the documentation, such as `version()`, `metadata()`, `files()`, `entry_points()`, and `requires()`. **Example**: Given that the package named `example-package` is installed, the result from calling `analyze_package(\'example-package\')` might look like: ```python { \\"version\\": \\"1.2.3\\", \\"metadata\\": { \\"Metadata-Version\\": \\"2.1\\", \\"Name\\": \\"example-package\\", \\"Version\\": \\"1.2.3\\", \\"Summary\\": \\"An example package\\", # ... other metadata fields ... }, \\"files\\": [ \\"example_package/__init__.py\\", \\"example_package/module.py\\", # ... other files ... ], \\"entry_points\\": { \\"console_scripts\\": [\\"example-script\\"], \\"gui_scripts\\": [], # ... other groups ... }, \\"requirements\\": [\\"dependency-1\\", \\"dependency-2\\"] } ``` Implement the `analyze_package` function below: ```python from importlib.metadata import version, metadata, files, entry_points, requires def analyze_package(package_name: str) -> dict: # Initialize result dictionary result = { \\"version\\": None, \\"metadata\\": {}, \\"files\\": [], \\"entry_points\\": {}, \\"requirements\\": [] } try: # Get package version result[\\"version\\"] = version(package_name) # Get package metadata pkg_metadata = metadata(package_name) result[\\"metadata\\"] = {k: v for k, v in pkg_metadata.items()} # Get package files pkg_files = files(package_name) result[\\"files\\"] = [str(file) for file in pkg_files] # Get entry points eps = entry_points() for group in eps.groups: result[\\"entry_points\\"][group] = [ep.name for ep in eps.select(group=group)] # Get package requirements result[\\"requirements\\"] = requires(package_name) or [] except Exception as e: print(f\\"An error occurred: {e}\\") return result ```","solution":"from importlib.metadata import version, metadata, files, entry_points, requires def analyze_package(package_name: str) -> dict: # Initialize result dictionary result = { \\"version\\": None, \\"metadata\\": {}, \\"files\\": [], \\"entry_points\\": {}, \\"requirements\\": [] } try: # Get package version result[\\"version\\"] = version(package_name) # Get package metadata pkg_metadata = metadata(package_name) result[\\"metadata\\"] = {k: v for k, v in pkg_metadata.items()} # Get package files pkg_files = files(package_name) result[\\"files\\"] = [str(file) for file in pkg_files] # Get entry points eps = entry_points() for group in eps.groups: result[\\"entry_points\\"][group] = [ep.name for ep in eps.select(group=group)] # Get package requirements result[\\"requirements\\"] = requires(package_name) or [] except Exception as e: print(f\\"An error occurred: {e}\\") return result"},{"question":"# Python Coding Assessment Question Objective: Write a Python function to simulate the behavior of iteration over an iterator, similar to what the provided C code example does, but using Python constructs directly. This will test your understanding of iterators, error-handling mechanisms, and the new `PySendResult` functionality in Python 3.10. Task: Implement a function `iterate_and_process(iterator, process_func)` that: 1. Takes an iterator `iterator` and a processing function `process_func` as arguments. 2. Iterates over the items produced by `iterator`. 3. Applies `process_func` to each item. 4. Handles exceptions and errors during iteration and processing. 5. Demonstrates the `yield` and `return` mechanisms within a custom iterator-like structure. Requirements: 1. **Input:** - `iterator`: An iterator object. - `process_func`: A function to apply to each item from the iterator. 2. **Output:** - No direct output; the function should print each processed item. - Errors during iteration or processing should be printed with appropriate messages. 3. **Constraints:** - You must use Python\'s iterator protocol and error handling. - Avoid using built-in iteration constructs like `for` loops. 4. **Additional Features:** - Implement logic to emulate the `PyIter_Send` functionality within your processing. Example: ```python def process_item(item): # Example processing function (can be customized) return item * 2 class CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopIteration def iterate_and_process(iterator, process_func): # Implement your function here # Usage data = [1, 2, 3, 4] iterator = CustomIterator(data) iterate_and_process(iterator, process_item) ``` **Expected Output:** ``` Processed item: 2 Processed item: 4 Processed item: 6 Processed item: 8 ``` **Error Handling Example:** If `process_func` raises an exception for any item, your function should print: ``` Error processing item: <item>, Error: <error message> ``` Guidelines: - Do not use Python\'s built-in iteration commands (e.g., `for`, `while` with `next`, etc.). - Explicitly manage the iteration using the iterator protocol. - Handle `StopIteration` and other possible exceptions gracefully.","solution":"def iterate_and_process(iterator, process_func): Iterates over the items produced by `iterator`, applies `process_func` to each item, and prints the result. Handles exceptions gracefully. :param iterator: An iterator object. :param process_func: A function to apply to each item from the iterator. while True: try: item = next(iterator) except StopIteration: break except Exception as e: print(f\\"Error during iteration: {str(e)}\\") break try: processed_item = process_func(item) print(f\\"Processed item: {processed_item}\\") except Exception as e: print(f\\"Error processing item: {item}, Error: {str(e)}\\")"},{"question":"# Question: Unicode String Validator In the context of internationalized domain names and other string validations in internet protocols, it is crucial to prepare and validate Unicode strings according to specific rules defined in RFC 3454. Your task is to implement a function `validate_unicode_string` that processes and validates a given input string based on a subset of the stringprep profile rules. The function should check if the string contains any characters that are restricted by the rules and return a normalized version of the string if valid. Otherwise, it should raise a `ValueError` indicating the type of invalid character(s). Function Signature ```python def validate_unicode_string(input_string: str) -> str: Validate and normalize the given Unicode string based on a subset of stringprep profile rules. Parameters: input_string (str): The Unicode string to be validated and normalized. Returns: str: The normalized Unicode string if valid. Raises: ValueError: If the input string contains invalid characters. pass ``` Requirements 1. The function should check for and handle the following character restrictions: - Unassigned code points (table A.1). - ASCII and Non-ASCII space characters (tables C.1.1 and C.1.2). - ASCII and Non-ASCII control characters (tables C.2.1 and C.2.2). - Private use characters (table C.3). - Non-character code points (table C.4). - Surrogate codes (table C.5). 2. If any character in the input string falls into the above categories, the function should raise a `ValueError` with a message specifying the category of the invalid character. 3. If the string passes all the checks, the function should return the string with all mappings applied according to the following tables: - Characters commonly mapped to nothing (table B.1). - Case-folding with normalization (table B.2). Example ```python try: normalized_string = validate_unicode_string(\\"ExampleInput\\") print(normalized_string) except ValueError as e: print(\\"Validation Error:\\", e) # If \\"ExampleInput\\" contains any restricted characters, it should raise a ValueError. # Otherwise, it outputs the normalized string. ``` Constraints - The input string length can be up to 10,000 characters. - The input string will consist of valid Unicode characters. Hints - Utilize the functions provided by the `stringprep` module to check character restrictions and apply mappings. - Structure your code to handle each restriction category individually, allowing for precise error reporting.","solution":"import stringprep import unicodedata def validate_unicode_string(input_string: str) -> str: Validate and normalize the given Unicode string based on a subset of stringprep profile rules. Parameters: input_string (str): The Unicode string to be validated and normalized. Returns: str: The normalized Unicode string if valid. Raises: ValueError: If the input string contains invalid characters. for char in input_string: # Check for unassigned code points if stringprep.in_table_a1(char): raise ValueError(f\\"Unassigned code point: {repr(char)}\\") # Check for space characters if stringprep.in_table_c11(char) or stringprep.in_table_c12(char): raise ValueError(f\\"Space character: {repr(char)}\\") # Check for ASCII and Non-ASCII control characters if stringprep.in_table_c21(char) or stringprep.in_table_c22(char): raise ValueError(f\\"Control character: {repr(char)}\\") # Check for private use characters if stringprep.in_table_c3(char): raise ValueError(f\\"Private use character: {repr(char)}\\") # Check for non-character code points if stringprep.in_table_c4(char): raise ValueError(f\\"Non-character code point: {repr(char)}\\") # Check for surrogate codes if stringprep.in_table_c5(char): raise ValueError(f\\"Surrogate code: {repr(char)}\\") # Apply mappings mapped_string = \\"\\" for char in input_string: # Map characters commonly mapped to nothing if stringprep.in_table_b1(char): continue mapped_string += char # Apply case-folding with normalization normalized_string = unicodedata.normalize(\'NFKC\', mapped_string.casefold()) return normalized_string"},{"question":"**Coding Assessment Question: Event Scheduler Management** # Objective: Create functions to manage a series of tasks using Python\'s `sched` module. Your implementation will demonstrate familiarity with scheduling events, handling priorities, and managing the event queue. # Task: Implement a function called `manage_tasks` that schedules a series of events and ensures they are executed in the correct order. Additionally, create helper functions to add tasks, cancel a task, and get the status of the event queue. # Function Specifications: 1. **Function Name**: `manage_tasks` - **Input**: A list of tuples where each tuple contains: - `time`: An integer representing the delay in seconds before the task should run. - `priority`: An integer where a lower number represents a higher priority. - `task_func`: A function that represents the task to be executed. - `args`: A tuple containing the positional arguments for the task function. - `kwargs`: A dictionary containing the keyword arguments for the task function. - **Output**: A list of task names (or descriptions) in the order they were executed. 2. **Function Name**: `add_task` - **Input**: - `scheduler`: An instance of the `sched.scheduler`. - `time`: An integer representing the delay in seconds before the task should run. - `priority`: An integer where a lower number represents a higher priority. - `task_func`: A function that represents the task to be executed. - `args`: A tuple containing the positional arguments for the task function. - `kwargs`: A dictionary containing the keyword arguments for the task function. - **Output**: The newly scheduled event. 3. **Function Name**: `cancel_task` - **Input**: - `scheduler`: An instance of the `sched.scheduler`. - `event`: The event to be canceled. - **Output**: None. The function should cancel the event if it exists. 4. **Function Name**: `get_queue_status` - **Input**: - `scheduler`: An instance of the `sched.scheduler`. - **Output**: A list of all scheduled events with their details. # Constraints: - Assume that `task_func` will always be a valid callable. - The scheduled delay time is always a non-negative integer. - There will be no events scheduled at the same exact time with the same priority. - Handle any exceptions that might occur during the execution of `task_func`. # Example: ```python import sched import time # Suppose we have the following task function definitions def task1(name): print(f\'Task {name} executed\') def task2(name, age): print(f\'Task {name} with age {age} executed\') # Example usage of manage_tasks function tasks = [ (5, 1, task1, (\'A\',), {}), (3, 2, task2, (\'B\',), {\'age\': 30}), (10, 1, task1, (\'C\',), {}), ] print(manage_tasks(tasks)) ``` Expected Output: A list of task names (or descriptions) indicating the order in which they were executed: ``` [\'B\', \'A\', \'C\'] ``` # Notes: - You are allowed to import necessary standard library packages such as `sched` and `time`. - Ensure the events are scheduled and executed as specified, considering priorities and delays. - You should use `sched.scheduler(time.time, time.sleep)` for scheduling tasks.","solution":"import sched import time def manage_tasks(tasks): Manage a series of tasks using the sched module. Parameters: tasks (list): A list of tuples containing the following elements: - time (int): The delay in seconds before the task should run. - priority (int): A lower number represents a higher priority. - task_func (callable): The task to be executed. - args (tuple): Positional arguments for the task function. - kwargs (dict): Keyword arguments for the task function. Returns: list: A list of task names (or descriptions) in the order they were executed. scheduler = sched.scheduler(time.time, time.sleep) executed_tasks = [] def execute_task_wrapper(task_func, task_desc, *args, **kwargs): task_func(*args, **kwargs) executed_tasks.append(task_desc) for task_time, priority, task_func, args, kwargs in tasks: task_desc = f\\"{task_func.__name__} with args={args} and kwargs={kwargs}\\" scheduler.enter(task_time, priority, execute_task_wrapper, (task_func, task_desc) + args, kwargs) scheduler.run() return executed_tasks def add_task(scheduler, task_time, priority, task_func, args=(), kwargs={}): Schedule a new task. Parameters: scheduler (sched.scheduler): The scheduler instance. task_time (int): The delay in seconds before the task should run. priority (int): A lower number represents a higher priority. task_func (callable): The task to be executed. args (tuple): Positional arguments for the task function. kwargs (dict): Keyword arguments for the task function. Returns: Event: The newly scheduled event. return scheduler.enter(task_time, priority, task_func, args, kwargs) def cancel_task(scheduler, event): Cancel a scheduled task. Parameters: scheduler (sched.scheduler): The scheduler instance. event: The event to be canceled. Returns: None. scheduler.cancel(event) def get_queue_status(scheduler): Get the status of the event queue. Parameters: scheduler (sched.scheduler): The scheduler instance. Returns: list: A list of all scheduled events with their details. return scheduler.queue"},{"question":"# Exercise: Parse and Extract Data Using Regular Expressions Objective Write a Python function that uses regular expressions to parse and extract specific data from a given multi-line text. This exercise will test your understanding of: - Matching multiple lines with regular expressions. - Extracting and using groups in the pattern. - Handling edge cases and ensuring robustness of the regex. Problem Statement You are given a multi-line string containing log data in the following format: ```plaintext [DATE TIME] SOURCE - MESSAGE ``` Each log entry spans one line, and the fields are defined as follows: - `DATE`: Date in the format YYYY-MM-DD. - `TIME`: Time in the format HH:MM:SS. - `SOURCE`: An alphanumeric string identifying the source of the log. - `MESSAGE`: The actual log message, which could contain any printable characters except for square brackets and hyphens. Write a function `parse_log_data(log: str) -> list` that takes a single string argument, `log`, containing the log data. The function should extract the DATE, TIME, SOURCE, and MESSAGE for each log entry and return a list of dictionaries, each containing these fields. Input - `log`: A string representing the multi-line log data. Each log entry is on a separate line, as described above. Output - A list of dictionaries. Each dictionary represents a log entry and contains the following keys: `date`, `time`, `source`, and `message`. Constraints - The log data is well-formed and adheres strictly to the specified format. - The function should handle multiple log entries without any errors. Example Usage ```python input_log = [2023-03-15 08:31:25] system - Initialization completed [2023-03-15 08:33:42] auth - User login successful [2023-03-15 09:01:10] backup - Backup started expected_output = [ { \\"date\\": \\"2023-03-15\\", \\"time\\": \\"08:31:25\\", \\"source\\": \\"system\\", \\"message\\": \\"Initialization completed\\" }, { \\"date\\": \\"2023-03-15\\", \\"time\\": \\"08:33:42\\", \\"source\\": \\"auth\\", \\"message\\": \\"User login successful\\" }, { \\"date\\": \\"2023-03-15\\", \\"time\\": \\"09:01:10\\", \\"source\\": \\"backup\\", \\"message\\": \\"Backup started\\" } ] output = parse_log_data(input_log) print(output == expected_output) # Should print: True ``` Requirements - Use regular expressions (`re` module) to parse the log data. - Extract all required fields and ensure that the function is robust against typical edge cases like varying field lengths. - Your solution must be clean, efficient, and use best practices for regex design and Python coding.","solution":"import re def parse_log_data(log: str) -> list: Parses a multiline log string and extracts date, time, source, and message from each log entry. Args: log (str): The multiline log string. Returns: list: A list of dictionaries with keys `date`, `time`, `source`, and `message`. pattern = re.compile(r\'[(d{4}-d{2}-d{2}) (d{2}:d{2}:d{2})] (w+) - (.+)\') matches = pattern.findall(log) log_entries = [] for match in matches: entry = { \\"date\\": match[0], \\"time\\": match[1], \\"source\\": match[2], \\"message\\": match[3] } log_entries.append(entry) return log_entries"},{"question":"Objective Create a custom file reader that demonstrates your understanding of the Python `builtins` module by utilizing built-in functions and extending their functionality through wrapping. Task Write a Python class `LineCounter` that will wrap the built-in `open()` function to read files. This class should: - Count the number of lines in a file during read operations. - Return the total number of lines read so far using a method named `get_lines_read()`. Detailed Requirements 1. **Class Name: `LineCounter`** 2. **Constructor**: `__init__(self, file_path: str)` - Opens the file specified by `file_path` in read mode using the built-in `open()` function. 3. **Method**: `readlines(self) -> list` - Reads the lines of the file and returns them as a list. This method should also count the lines read and update the total number of lines read so far. 4. **Method**: `get_lines_read(self) -> int` - Returns the total number of lines read so far. 5. **Error Handling**: Gracefully handle the file-not-found error by raising a `FileNotFoundError` with an appropriate message if the file does not exist. Constraints - Do not use any external libraries other than the built-in `open()` function. - The class should correctly handle large files without loading the entire file into memory at once (use a read buffer approach). Example Usage ```python # Assuming a file named \'example.txt\' with 3 lines of content. lc = LineCounter(\'example.txt\') lines = lc.readlines() print(lines) # Output: [\'First linen\', \'Second linen\', \'Third linen\'] print(lc.get_lines_read()) # Output: 3 ``` Performance Requirements - The implementation should be efficient and should not load the entire file into memory at once. It should use buffered reading techniques to handle potentially large files. Input / Output Format - **Input**: A string representing the file path. - **Output**: List of strings for `readlines()` and an integer for `get_lines_read()` representing the total lines read. Use the provided example and requirements to guide your implementation. Make sure to thoroughly test your class with different files and scenarios.","solution":"class LineCounter: def __init__(self, file_path: str): Opens the file specified by file_path in read mode using the built-in open() function. self.file_path = file_path # initialize line counter self.lines_read = 0 try: self.file_obj = open(file_path, \'r\') except FileNotFoundError: raise FileNotFoundError(f\\"File \'{file_path}\' not found\\") def readlines(self) -> list: Reads the lines of the file and returns them as a list. Also counts the lines read and updates the total number of lines read so far. lines = self.file_obj.readlines() self.lines_read += len(lines) return lines def get_lines_read(self) -> int: Returns the total number of lines read so far. return self.lines_read def __del__(self): Ensures that the file is properly closed when the object is deleted. self.file_obj.close()"},{"question":"<|Analysis Begin|> The provided documentation gives a comprehensive overview of the `io` module in Python. Some key points highlighted are: 1. The `io` module\'s primary purposes: - Handling Text I/O, which involves dealing with `str` objects and also encoding and decoding of data. - Handling Binary I/O, involving `bytes` objects without encoding or decoding. - Handling Raw I/O, which is generally low-level access to the underlying OS APIs. 2. Three main types of I/O streams: `Text I/O`, `Binary I/O`, and `Raw I/O`. 3. Several classes and abstract base classes (ABCs) provided by the `io` module, such as `IOBase`, `RawIOBase`, `BufferedIOBase`, `TextIOBase`, and their concrete implementations like `FileIO`, `BytesIO`, `StringIO`, `TextIOWrapper`, `BufferedReader`, `BufferedWriter`, `BufferedRandom`, and `BufferedRWPair`. 4. Specific methods and functionalities each base class and concrete class provides. 5. Features like thread safety, performance considerations, multi-threading, and reentrancy. Overall, the documentation provides detailed insights into how streams are managed, various types of streams, their interfaces, methods, and characteristics. <|Analysis End|> <|Question Begin|> # Advanced Coding Assessment Question on Python\'s `io` Module Objective: Create a custom class `CombinedIOWrapper` that demonstrates an understanding of Python\'s `io` module, specifically handling both text and binary data. Problem Statement: In this task, you are required to create a class `CombinedIOWrapper` that combines functionalities for handling both text and binary I/O using an in-memory buffer and allows switching between text and binary modes seamlessly. Requirements: 1. The class should inherit from `io.IOBase`. 2. It should allow initialization with an optional `initial_bytes` parameter that sets up the initial binary content of the buffer. 3. It should provide methods to: - Read and write binary data. - Read and write text data with a specified encoding. - Clear the buffer. - Retrieve the current contents of the buffer as either bytes or string based on the mode. Class Structure: - **`__init__(self, initial_bytes=b\'\')`** - Initializes the buffer with optional initial data. - **`read_binary(self, size=-1)`** - Reads binary data from the buffer. - **`write_binary(self, b)`** - Writes binary data to the buffer. - **`read_text(self, encoding=\'utf-8\', size=-1)`** - Reads text data from the buffer with the specified encoding. - **`write_text(self, s, encoding=\'utf-8\')`** - Writes text data to the buffer with the specified encoding. - **`clear_buffer(self)`** - Clears the contents of the buffer. - **`getvalue(self, as_text=False, encoding=\'utf-8\')`** - Retrieves the current contents of the buffer, returning as bytes or string based on the `as_text` flag. Constraints: - Use `io.BytesIO` for managing the binary buffer and leverage `io.TextIOWrapper` for text operations. - Ensure that the class methods handle errors gracefully, e.g., invalid encoding or writing text data in binary mode. - Keep track of the position within the buffer appropriately to ensure continuous reading/writing operations. Sample Usage: ```python buffer = CombinedIOWrapper(initial_bytes=b\\"hello\\") # Writing and reading binary data buffer.write_binary(b\\" world\\") print(buffer.read_binary()) # Outputs: b\'hello world\' # Writing and reading text data buffer.write_text(\\"ÐŸÑ€Ð¸Ð²ÐµÑ‚\\", encoding=\'utf-8\') print(buffer.read_text(encoding=\'utf-8\')) # Outputs the previous content congruently # Clearing the buffer buffer.clear_buffer() print(buffer.getvalue()) # Outputs: b\'\' or \'\' # Accessing buffer content as text buffer.write_binary(b\\"xe2x82xac\\") print(buffer.getvalue(as_text=True, encoding=\'utf-8\')) # Outputs: \'â‚¬\' ``` Implement the `CombinedIOWrapper` class below: ```python import io class CombinedIOWrapper(io.IOBase): def __init__(self, initial_bytes=b\'\'): self.buffer = io.BytesIO(initial_bytes) def read_binary(self, size=-1): return self.buffer.read(size) def write_binary(self, b): if isinstance(b, (bytes, bytearray)): self.buffer.write(b) else: raise TypeError(\\"Binary data required\\") def read_text(self, encoding=\'utf-8\', size=-1): self.buffer.seek(0) text_wrapper = io.TextIOWrapper(self.buffer, encoding=encoding) return text_wrapper.read(size) def write_text(self, s, encoding=\'utf-8\'): if isinstance(s, str): b = s.encode(encoding) self.buffer.write(b) else: raise TypeError(\\"Text data required\\") def clear_buffer(self): self.buffer.seek(0) self.buffer.truncate(0) def getvalue(self, as_text=False, encoding=\'utf-8\'): if as_text: return self.read_text(encoding=encoding) else: return self.buffer.getvalue() ``` Ensure you test the class thoroughly to validate its functionality, including various edge cases.","solution":"import io class CombinedIOWrapper(io.IOBase): def __init__(self, initial_bytes=b\'\'): self.buffer = io.BytesIO(initial_bytes) def read_binary(self, size=-1): self.buffer.seek(0) # Ensure reading from the beginning return self.buffer.read(size) def write_binary(self, b): if not isinstance(b, (bytes, bytearray)): raise TypeError(\\"Binary data required\\") self.buffer.seek(0, io.SEEK_END) # Move to the end for writing self.buffer.write(b) def read_text(self, encoding=\'utf-8\', size=-1): self.buffer.seek(0) # Ensure reading from the beginning text_wrapper = io.TextIOWrapper(self.buffer, encoding=encoding) text_wrapper.seek(0) text = text_wrapper.read(size) text_wrapper.detach() # Detach to avoid interference with further operations return text def write_text(self, s, encoding=\'utf-8\'): if not isinstance(s, str): raise TypeError(\\"Text data required\\") b = s.encode(encoding) self.buffer.seek(0, io.SEEK_END) # Move to the end for writing self.buffer.write(b) def clear_buffer(self): self.buffer.seek(0) self.buffer.truncate(0) def getvalue(self, as_text=False, encoding=\'utf-8\'): if as_text: return self.read_text(encoding=encoding) else: current_pos = self.buffer.tell() self.buffer.seek(0) value = self.buffer.getvalue() self.buffer.seek(current_pos) return value"},{"question":"**XML Parsing with `xml.parsers.expat`:** You are tasked with implementing an XML parser using the `xml.parsers.expat` module in Python 3.10. Your parser should be able to: 1. Extract and print the names of all elements in the XML document. 2. Extract and print all character data contained within the elements. 3. Handle XML comments by printing them. 4. Handle XML start and end of document type declarations and print their content. **Requirements:** 1. Create a parser using `xml.parsers.expat.ParserCreate()`. 2. Define and set necessary handler functions for: - Start and end of elements. - Character data. - Comments. - Start and end of document type declarations. 3. Parse the given XML string and output the extracted data. **Function Signature:** `def parse_xml(xml_string: str) -> None:` **Input:** - `xml_string`: A string containing the XML data to be parsed. **Output:** - The function should print the extracted data to standard output using print statements for elements, character data, comments, and doctype declarations. **Example Input:** ```python xml_data = <?xml version=\\"1.0\\"?> <!DOCTYPE note SYSTEM \\"Note.dtd\\"> <note> <!-- This is a comment --> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> ``` **Example Output:** ``` Start DOCTYPE: note, SYSTEM: Note.dtd Start element: note Comment: This is a comment Start element: to Character data: Tove End element: to Start element: from Character data: Jani End element: from Start element: heading Character data: Reminder End element: heading Start element: body Character data: Don\'t forget me this weekend! End element: body End element: note End DOCTYPE ``` **Constraints:** - Ensure the function works correctly with properly formatted XML data. - Handle errors gracefully by catching `ExpatError` and printing a suitable error message. **Notes:** - Utilize the `xml.parsers.expat` module\'s documentation to understand how to set handler functions and create a parser. - Pay special attention to properly defining handler functions with the correct signature.","solution":"import xml.parsers.expat def parse_xml(xml_string: str) -> None: # Handler functions def start_element(name, attrs): print(f\\"Start element: {name}\\") def end_element(name): print(f\\"End element: {name}\\") def char_data(data): if data.strip(): # Ignore data that is only whitespace print(f\\"Character data: {data.strip()}\\") def comment(data): print(f\\"Comment: {data.strip()}\\") def start_doctype(name, sysid, pubid, has_internal_subset): print(f\\"Start DOCTYPE: {name}, SYSTEM: {sysid}\\") def end_doctype(): print(f\\"End DOCTYPE\\") # Create a parser parser = xml.parsers.expat.ParserCreate() # Set handler functions parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.CommentHandler = comment parser.StartDoctypeDeclHandler = start_doctype parser.EndDoctypeDeclHandler = end_doctype # Parse the XML string try: parser.Parse(xml_string) except xml.parsers.expat.ExpatError as e: print(f\\"XML parsing error: {e}\\") # Example Usage: xml_data = <?xml version=\\"1.0\\"?> <!DOCTYPE note SYSTEM \\"Note.dtd\\"> <note> <!-- This is a comment --> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> parse_xml(xml_data)"},{"question":"# Question: Visualize Monthly Passenger Flights Data using Seaborn You are provided with a dataset that records the number of airline passengers over several years and months. In this task, you need to visualize this data using seaborn to extract useful insights. Dataset Description: The dataset contains the following columns: - `year`: Year of the observation - `month`: Month of the observation - `passengers`: Number of passengers for that observation You can load the dataset using the following code: ```python import seaborn as sns flights = sns.load_dataset(\\"flights\\") ``` Tasks: 1. **Long-form Data Visualization:** - Using the long-form data format, plot a line graph showing the number of passengers over the years. Different lines should represent different months. - Customize the plot by adding title, labels, and a legend. 2. **Data Transformation to Wide-form:** - Transform the provided dataset from long-form to wide-form using `pandas.pivot`. - Plot the transformed wide-form data as a line graph. 3. **Advanced Visualization:** - Create a box plot to visualize the distribution of passengers for each month across all years using the wide-form data. Requirements: - Do not use any non-seaborn libraries for visualization. - Ensure that your code is well-documented and includes necessary comments to explain the process. - The final output should be a function `visualize_flights_data` that performs the above tasks and generates the required plots. Example Function Signature: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flights_data(): # Load dataset flights = sns.load_dataset(\\"flights\\") # Task 1: Long-form Data Visualization sns.relplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", kind=\\"line\\") plt.title(\\"Monthly Passengers over Years (Long-form Data)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(title=\\"Month\\") # Task 2: Data Transformation to Wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") sns.relplot(data=flights_wide, kind=\\"line\\") plt.title(\\"Monthly Passengers over Years (Wide-form Data)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") # Task 3: Advanced Visualization sns.catplot(data=flights_wide, kind=\\"box\\") plt.title(\\"Monthly Passengers Distribution\\") plt.show() # Call the function to execute the visualizations visualize_flights_data() ``` Ensure your solution meets the above requirements and formats the plots clearly.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_flights_data(): # Load dataset flights = sns.load_dataset(\\"flights\\") # Task 1: Long-form Data Visualization plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\") plt.title(\\"Monthly Passengers over Years (Long-form Data)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(title=\\"Month\\", bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Task 2: Data Transformation to Wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") plt.figure(figsize=(14, 8)) sns.lineplot(data=flights_wide) plt.title(\\"Monthly Passengers over Years (Wide-form Data)\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(title=\\"Month\\", bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Task 3: Advanced Visualization plt.figure(figsize=(14, 8)) sns.boxplot(data=flights, x=\\"month\\", y=\\"passengers\\") plt.title(\\"Monthly Passengers Distribution\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Number of Passengers\\") plt.show()"},{"question":"Hyper-parameter Optimization with scikit-learn Objective: Your task is to implement a function that performs hyper-parameter tuning using `GridSearchCV` on a given classifier and dataset. You will also evaluate the classifier\'s performance with the tuned parameters on a hold-out test set. Function Signature: ```python def optimize_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test, scoring=\'accuracy\', cv=5): Perform hyper-parameter optimization using GridSearchCV and evaluate the model. Parameters: model: Estimator object The machine learning classifier to be optimized. param_grid: dict or list of dictionaries Dictionary with parameters names (string) as keys and lists of parameter settings to try as values. X_train: array-like or sparse matrix of shape (n_samples, n_features) The training input samples. X_test: array-like or sparse matrix of shape (n_samples, n_features) The test input samples. y_train: array-like of shape (n_samples,) The target values (class labels) for training. y_test: array-like of shape (n_samples,) The target values (class labels) for testing. scoring: string, callable, or None, default=\'accuracy\' A string or a scorer callable object / function with signature scorer(estimator, X, y). cv: int, cross-validation generator or iterable, default=5 Determines the cross-validation splitting strategy. Returns: best_params: dict Parameter setting that gave the best results on the hold-out data. test_score: float The score of the best_estimator on the test data. pass ``` Specification: 1. **Input/Output Formats:** - `model`: scikit-learn estimator object (e.g., `sklearn.svm.SVC()`) - `param_grid`: Dictionary or list of dictionaries for grid search parameters. - `X_train`, `X_test`: Training and testing feature datasets respectively. - `y_train`, `y_test`: Training and testing target datasets respectively. - `scoring`: Performance metric used for scoring the model (default is \'accuracy\'). - `cv`: Number of folds in cross-validation (default is 5). - Returns a tuple containing: - `best_params`: Dictionary containing the best found hyper-parameters. - `test_score`: Accuracy or chosen scoring metric result on the test set. 2. **Constraints:** - The model must be compatible with scikit-learn\'s `GridSearchCV`. - Ensure the hyper-parameter tuning process considers computational efficiency. 3. **Performance Requirements:** - Utilize cross-validation (`cv`) as per the requirement. - Optimize the model hyper-parameters using `GridSearchCV` ensuring the process is not computationally heavy. Example Usage: ```python from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import train_test_split # Load dataset X, y = datasets.load_iris(return_X_y=True) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define model and parameter grid model = SVC() param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] # Call the function best_params, test_score = optimize_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test) print(\\"Best Parameters:\\", best_params) print(\\"Test Score:\\", test_score) ``` Notes: - Make sure to handle cases where parameter tuning might fail gracefully. - Refer to the scikit-learn documentation for more detailed usage of `GridSearchCV`.","solution":"from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score def optimize_and_evaluate(model, param_grid, X_train, X_test, y_train, y_test, scoring=\'accuracy\', cv=5): Perform hyper-parameter optimization using GridSearchCV and evaluate the model. Parameters: model: Estimator object The machine learning classifier to be optimized. param_grid: dict or list of dictionaries Dictionary with parameters names (string) as keys and lists of parameter settings to try as values. X_train: array-like or sparse matrix of shape (n_samples, n_features) The training input samples. X_test: array-like or sparse matrix of shape (n_samples, n_features) The test input samples. y_train: array-like of shape (n_samples,) The target values (class labels) for training. y_test: array-like of shape (n_samples,) The target values (class labels) for testing. scoring: string, callable, or None, default=\'accuracy\' A string or a scorer callable object / function with signature scorer(estimator, X, y). cv: int, cross-validation generator or iterable, default=5 Determines the cross-validation splitting strategy. Returns: best_params: dict Parameter setting that gave the best results on the hold-out data. test_score: float The score of the best_estimator on the test data. grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=cv) grid_search.fit(X_train, y_train) best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) test_score = accuracy_score(y_test, y_pred) return grid_search.best_params_, test_score"},{"question":"You are given an XML string containing a list of books. Each book has a title, an author, a year of publication, and a price. Your task is to parse the XML string using the `xml.sax` package and extract the information on each book. Additionally, you need to handle parsing errors. Implement the following two functions: 1. `parse_books(xml_string: str) -> List[Dict[str, Union[str, float, int]]]`: - Input: A string containing the XML data. - Output: A list of dictionaries, each representing a book with the following keys: `title`, `author`, `year`, and `price`. 2. `handle_parsing_error(xml_string: str) -> str`: - Input: A string containing potentially erroneous XML data. - Output: A string message indicating if the parsing was successful or if there was a parsing error. Use the following XML data as an example input for your functions: ```xml <library> <book> <title>Book 1</title> <author>Author 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book 2</title> <author>Author 2</author> <year>2005</year> <price>39.95</price> </book> </library> ``` # Constraints - The XML data will contain at most 1000 books. - Each book will have the tags `title`, `author`, `year`, and `price`. # Function Signatures ```python from typing import List, Dict, Union def parse_books(xml_string: str) -> List[Dict[str, Union[str, float, int]]]: pass def handle_parsing_error(xml_string: str) -> str: pass ``` # Example ```python xml_data = <library> <book> <title>Book 1</title> <author>Author 1</author> <year>2001</year> <price>29.99</price> </book> <book> <title>Book 2</title> <author>Author 2</author> <year>2005</year> <price>39.95</price> </book> </library> # Call parse_books function books = parse_books(xml_data) print(books) # Output: [{\'title\': \'Book 1\', \'author\': \'Author 1\', \'year\': 2001, \'price\': 29.99}, {\'title\': \'Book 2\', \'author\': \'Author 2\', \'year\': 2005, \'price\': 39.95}] # Call handle_parsing_error function with correct data result = handle_parsing_error(xml_data) print(result) # Output: \\"Parsing successful.\\" # Call handle_parsing_error function with incorrect data invalid_xml_data = \\"<library><book><title>Book 1</title><author>Author 1</author><year>2001</year><price>29.99</price></book>\\" result = handle_parsing_error(invalid_xml_data) print(result) # Output: \\"Parsing error: Mismatched tag: line 1, column 107\\" ``` # Notes - You must use the `xml.sax` package to parse the XML data. - For `handle_parsing_error`, catch `xml.sax.SAXParseException` to handle invalid XML data parsing. - Ensure to consider the stream-based nature of SAX, where events like the start and end of an element and character data are handled in a callback-like manner.","solution":"from typing import List, Dict, Union import xml.sax class BookHandler(xml.sax.ContentHandler): def __init__(self): self.books = [] self.current_book = None self.current_data = \\"\\" def startElement(self, tag, attributes): self.current_data = tag if tag == \\"book\\": self.current_book = {\\"title\\": \\"\\", \\"author\\": \\"\\", \\"year\\": 0, \\"price\\": 0.0} def endElement(self, tag): if self.current_data == \\"title\\": self.current_book[\\"title\\"] = self.content elif self.current_data == \\"author\\": self.current_book[\\"author\\"] = self.content elif self.current_data == \\"year\\": self.current_book[\\"year\\"] = int(self.content) elif self.current_data == \\"price\\": self.current_book[\\"price\\"] = float(self.content) if tag == \\"book\\": self.books.append(self.current_book) self.current_data = \\"\\" def characters(self, content): self.content = content.strip() def parse_books(xml_string: str) -> List[Dict[str, Union[str, float, int]]]: handler = BookHandler() xml.sax.parseString(xml_string, handler) return handler.books def handle_parsing_error(xml_string: str) -> str: handler = BookHandler() try: xml.sax.parseString(xml_string, handler) return \\"Parsing successful.\\" except xml.sax.SAXParseException as e: return f\\"Parsing error: {e}\\""},{"question":"You are required to implement and evaluate a Support Vector Machine (SVM) model for a multi-class classification task using Scikit-learn. The dataset consists of flower species data with various features. # Dataset The dataset has the following structure: - `data` : a 2D array where each row represents a sample and each column represents a feature. - `target` : a 1D array containing the class labels for each sample. You can generate a sample dataset using the following code: ```python from sklearn.datasets import load_iris data = load_iris() X = data.data y = data.target ``` # Requirements 1. **Data Preprocessing**: - Standardize the features of the dataset. 2. **Model Selection**: - Implement and compare two different SVM models: `SVC` with an RBF kernel and `LinearSVC`. 3. **Custom Kernel Implementation**: - Define a custom polynomial kernel and implement an SVM using this kernel. 4. **Cross-validation**: - Perform cross-validation to evaluate the performance of each model. - Report the mean accuracy and standard deviation for each model. 5. **Model Performance**: - Use the cross-validation results to determine which model performs best. # Function Signature Implement your solution in the following function signature: ```python from sklearn.base import BaseEstimator from sklearn.model_selection import cross_val_score import numpy as np def custom_kernel(X, Y): Define a custom polynomial kernel. return (np.dot(X, Y.T) + 1) ** 3 def evaluate_svm_models(X: np.ndarray, y: np.ndarray): Evaluate and compare different SVM models on a given dataset. Parameters: - X (np.ndarray): Feature matrix. - y (np.ndarray): Target labels. Returns: - dict: A dictionary containing the mean accuracy and standard deviation of each model. # Your code here ``` # Instructions 1. **Standardize the features** using Scikit-learn\'s `StandardScaler`. 2. **Implement and train the following models**: - `SVC` with an RBF kernel - `LinearSVC` - `SVC` with the custom polynomial kernel 3. **Evaluate each model** using cross-validation with 5 folds and report the mean accuracy and standard deviation. 4. **Determine the best model** based on the cross-validation results. # Submission Your submission should include: - The `evaluate_svm_models` function implementation. - Code demonstrating the function usage with the generated Iris dataset. - Output showing the cross-validation results for each model.","solution":"from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC, LinearSVC from sklearn.model_selection import cross_val_score import numpy as np def custom_kernel(X, Y): Define a custom polynomial kernel. return (np.dot(X, Y.T) + 1) ** 3 def evaluate_svm_models(X: np.ndarray, y: np.ndarray): Evaluate and compare different SVM models on a given dataset. Parameters: - X (np.ndarray): Feature matrix. - y (np.ndarray): Target labels. Returns: - dict: A dictionary containing the mean accuracy and standard deviation of each model. # Standardize the features scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Define the models svc_rbf = SVC(kernel=\'rbf\') linear_svc = LinearSVC(max_iter=10000) svc_custom = SVC(kernel=custom_kernel) # Evaluate each model using cross-validation models = {\'SVC_RBF\': svc_rbf, \'LinearSVC\': linear_svc, \'SVC_Custom\': svc_custom} results = {} for model_name, model in models.items(): scores = cross_val_score(model, X_scaled, y, cv=5) results[model_name] = { \'mean_accuracy\': np.mean(scores), \'std_deviation\': np.std(scores) } return results # Example usage with Iris dataset data = load_iris() X, y = data.data, data.target results = evaluate_svm_models(X, y) print(results)"},{"question":"# Task Implement a simple neural network using `torch.nn.Module` and parallelize it using the Tensor Parallelism utilities from `torch.distributed.tensor.parallel`. You will demonstrate your understanding by building a model that utilizes both `ColwiseParallel` and `RowwiseParallel` settings. # Instructions 1. Create a simple fully connected Feedforward Neural Network (FFNN) using `torch.nn.Module`. The network should have an input layer, one hidden layer, and an output layer. 2. Parallelize the model\'s layers using Colwise and Rowwise parallelism with `torch.distributed.tensor.parallel`. 3. Assume you have a `DeviceMesh` operated on 2 devices. Configure the neural network\'s inputs and outputs with the `PrepareModuleInput` and `PrepareModuleOutput` within the `parallelize_plan`. 4. Implement the forward pass method to handle the parallel execution of the network. 5. Use the `loss_parallel` API to parallelize the cross-entropy loss computation for this model. # Example Below is the example skeleton: ```python import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel, PrepareModuleInput, PrepareModuleOutput class ParallelFeedforwardNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(ParallelFeedforwardNN, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.output = nn.Linear(hidden_size, output_size def forward(self, x): # You need to parallelize both the layers \'hidden\' and \'output\' x = self.hidden(x) x = torch.relu(x) x = self.output(x) return x def main(): # Initialize process group for distributed training dist.init_process_group(backend=\'nccl\') # Define input and output sizes for the network input_size = 128 hidden_size = 64 output_size = 10 # Create an instance of the model model = ParallelFeedforwardNN(input_size, hidden_size, output_size) # Define the parallelization strategy parallelize_plan = [ (PrepareModuleInput(), \'input\'), (ColwiseParallel(), \'hidden\'), (RowwiseParallel(), \'output\'), (PrepareModuleOutput(), \'output\') ] # Parallelize the model model = parallelize_module(model, parallelize_plan) # Create dummy input and run the forward pass input_tensor = torch.randn(32, input_size) # Batch size of 32 output = model(input_tensor) if __name__ == \\"__main__\\": main() ``` # Submission Submit your `ParallelFeedforwardNN` class definition and the `main` function code that initializes distributed training, defines the model, specifies the parallelization strategy, and runs the forward pass. Ensure your code is clear, well-commented, and adheres to the required use of the tensor parallelism. # Constraints - Your model should run on 2 devices. - Ensure proper initialization of the distributed process group. - Follow best practices for PyTorch distributed training.","solution":"import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel, PrepareModuleInput, PrepareModuleOutput class ParallelFeedforwardNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(ParallelFeedforwardNN, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.output = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.hidden(x) x = torch.relu(x) x = self.output(x) return x def main(): # Initialize process group for distributed training dist.init_process_group(backend=\'gloo\') # Define input and output sizes for the network input_size = 128 hidden_size = 64 output_size = 10 # Create an instance of the model model = ParallelFeedforwardNN(input_size, hidden_size, output_size) # Define the parallelization strategy parallelize_plan = [ (PrepareModuleInput(), \'input\'), (ColwiseParallel(), \'hidden\'), (RowwiseParallel(), \'output\'), (PrepareModuleOutput(), \'output\') ] # Parallelize the model model = parallelize_module(model, parallelize_plan) # Create dummy input and run the forward pass input_tensor = torch.randn(32, input_size) # Batch size of 32 output = model(input_tensor) print(output) if __name__ == \\"__main__\\": main()"},{"question":"# Clustering and Evaluation with scikit-learn Objective Implement a clustering algorithm using scikit-learn and evaluate its performance using two different clustering performance evaluation metrics. Instructions 1. **Data Generation:** - Generate a 2D dataset with 500 samples and 5 different clusters using the `make_blobs` function from `sklearn.datasets`. 2. **Clustering Implementation:** - Apply the K-Means clustering algorithm to cluster the generated data. - Implement another clustering algorithm of your choice from the scikit-learn library to cluster the same data. 3. **Performance Evaluation:** - Calculate the performance of both clustering algorithms using the Adjusted Rand Index (`adjusted_rand_score`) and the Silhouette Coefficient (`silhouette_score`) from `sklearn.metrics. 4. **Output:** - Print the clustering labels of both algorithms. - Print the Adjusted Rand Index and Silhouette Coefficient for both clustering algorithms. Constraints - Assume you have access to all necessary libraries: `numpy`, `scikit-learn`, `matplotlib` (if needed). - Do not use any other libraries outside the mentioned ones. Example Input: ```python from sklearn.datasets import make_blobs X, y = make_blobs(n_samples=500, centers=5, random_state=42) ``` Example Output: ``` K-Means Clustering Labels: [0 1 0 4 3 1 0 2 1 4 ... ] Another Clustering Algorithm Labels: [1 1 0 3 4 1 0 2 1 3 ... ] Performance Evaluation: K-Means: Adjusted Rand Index: 0.890 Silhouette Score: 0.678 Other Algorithm: Adjusted Rand Index: 0.849 Silhouette Score: 0.635 ``` ```python # Your implementation here import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.cluster import AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score # Generate dataset X, y = make_blobs(n_samples=500, centers=5, random_state=42) # Apply K-Means clustering kmeans = KMeans(n_clusters=5, random_state=42).fit(X) kmeans_labels = kmeans.labels_ # Apply another clustering algorithm (AgglomerativeClustering for instance) agg_clustering = AgglomerativeClustering(n_clusters=5).fit(X) agg_labels = agg_clustering.labels_ # Evaluate performance using Adjusted Rand Index and Silhouette Score ari_kmeans = adjusted_rand_score(y, kmeans_labels) silhouette_kmeans = silhouette_score(X, kmeans_labels) ari_agg = adjusted_rand_score(y, agg_labels) silhouette_agg = silhouette_score(X, agg_labels) print(\\"K-Means Clustering Labels:\\") print(kmeans_labels) print(\\"Another Clustering Algorithm Labels:\\") print(agg_labels) print(\\"Performance Evaluation:\\") print(\\"K-Means:\\") print(f\\"Adjusted Rand Index: {ari_kmeans:.3f}\\") print(f\\"Silhouette Score: {silhouette_kmeans:.3f}\\") print(\\"Other Algorithm:\\") print(f\\"Adjusted Rand Index: {ari_agg:.3f}\\") print(f\\"Silhouette Score: {silhouette_agg:.3f}\\") ```","solution":"import numpy as np from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.cluster import AgglomerativeClustering from sklearn.metrics import adjusted_rand_score, silhouette_score def generate_data(samples=500, centers=5, random_state=42): Generates a 2D dataset using make_blobs. X, y = make_blobs(n_samples=samples, centers=centers, random_state=random_state) return X, y def apply_kmeans(X, clusters=5, random_state=42): Applies K-Means clustering algorithm to the data. kmeans = KMeans(n_clusters=clusters, random_state=random_state).fit(X) return kmeans.labels_ def apply_agglomerative_clustering(X, clusters=5): Applies Agglomerative Clustering algorithm to the data. agg_clustering = AgglomerativeClustering(n_clusters=clusters).fit(X) return agg_clustering.labels_ def evaluate_clustering_performance(X, y_true, y_kmeans, y_agg): Evaluates the clustering performance using Adjusted Rand Index and Silhouette Score. ari_kmeans = adjusted_rand_score(y_true, y_kmeans) silhouette_kmeans = silhouette_score(X, y_kmeans) ari_agg = adjusted_rand_score(y_true, y_agg) silhouette_agg = silhouette_score(X, y_agg) return (ari_kmeans, silhouette_kmeans, ari_agg, silhouette_agg) # Generate dataset X, y = generate_data() # Apply clustering algorithms kmeans_labels = apply_kmeans(X) agg_labels = apply_agglomerative_clustering(X) # Evaluate performance ari_kmeans, silhouette_kmeans, ari_agg, silhouette_agg = evaluate_clustering_performance(X, y, kmeans_labels, agg_labels) print(\\"K-Means Clustering Labels:\\") print(kmeans_labels) print(\\"Another Clustering Algorithm Labels:\\") print(agg_labels) print(\\"Performance Evaluation:\\") print(\\"K-Means:\\") print(f\\"Adjusted Rand Index: {ari_kmeans:.3f}\\") print(f\\"Silhouette Score: {silhouette_kmeans:.3f}\\") print(\\"Other Algorithm:\\") print(f\\"Adjusted Rand Index: {ari_agg:.3f}\\") print(f\\"Silhouette Score: {silhouette_agg:.3f}\\")"},{"question":"You are working on a project that involves analyzing sales data for a retail company. The company has provided you with a dataset in CSV format, and you are required to perform several tasks to analyze the data. Below is the task you need to complete: # Task: Implement Sales Data Analysis 1. **Read the Data**: Write a function `read_data(file_path)` to read the CSV file into a pandas DataFrame. The CSV file contains columns: `OrderID`, `ProductID`, `Quantity`, `Price`, `OrderDate` (in `YYYY-MM-DD` format), and `CustomerID`. 2. **Data Cleaning**: Write a function `clean_data(df)` that: - Removes rows where any columns are missing (`NaN`). - Converts the `OrderDate` column to datetime format. - Ensures that `Quantity` and `Price` are of numeric type and removes any rows where these values are non-numeric or negative. 3. **Monthly Sales Analysis**: Write a function `monthly_sales(df)` that: - Groups the data by `OrderDate` month and sums the `Quantity` and total sales (`Quantity * Price`). - Returns a DataFrame with `Month`, `TotalQuantity`, and `TotalSales`. 4. **Top Customers**: Write a function `top_customers(df, n)` that: - Identifies the top `n` customers based on the total sales amount. - Returns a DataFrame with `CustomerID` and `TotalSales`. 5. **Product Performance**: Write a function `product_performance(df, start_date, end_date)`: - Filters the data for orders between `start_date` and `end_date`. - Groups the filtered data by `ProductID` and calculates the total quantity sold and total sales. - Returns a DataFrame with `ProductID`, `TotalQuantity`, and `TotalSales`. # Constraints - Your functions should handle large datasets efficiently. - Make sure to provide appropriate error handling within your functions. # Input and Output Formats 1. **read_data(file_path)** - **Input**: `file_path` (str): The file path to the CSV file. - **Output**: DataFrame containing the sales data. 2. **clean_data(df)** - **Input**: `df` (DataFrame): The raw sales DataFrame. - **Output**: DataFrame with cleaned data. 3. **monthly_sales(df)** - **Input**: `df` (DataFrame): Cleaned sales DataFrame. - **Output**: DataFrame with columns `Month`, `TotalQuantity`, `TotalSales`. 4. **top_customers(df, n)** - **Input**: `df` (DataFrame): Cleaned sales DataFrame, `n` (int): Number of top customers to return. - **Output**: DataFrame with columns `CustomerID`, `TotalSales`. 5. **product_performance(df, start_date, end_date)** - **Input**: `df` (DataFrame): Cleaned sales DataFrame, `start_date` (str): Filter start date in `YYYY-MM-DD` format, `end_date` (str): Filter end date in `YYYY-MM-DD` format. - **Output**: DataFrame with columns `ProductID`, `TotalQuantity`, `TotalSales`. # Example ```python file_path = \'sales_data.csv\' df = read_data(file_path) clean_df = clean_data(df) monthly_df = monthly_sales(clean_df) top_customers_df = top_customers(clean_df, 5) product_perf_df = product_performance(clean_df, \'2022-01-01\', \'2022-03-31\') print(monthly_df.head()) print(top_customers_df.head()) print(product_perf_df.head()) ```","solution":"import pandas as pd def read_data(file_path): Reads the CSV file into a pandas DataFrame. return pd.read_csv(file_path) def clean_data(df): Cleans the data: - Removes rows with any NaN values. - Converts OrderDate to datetime. - Ensures Quantity and Price are numeric and non-negative. df = df.dropna() df[\'OrderDate\'] = pd.to_datetime(df[\'OrderDate\'], errors=\'coerce\') df = df.dropna(subset=[\'OrderDate\']) df[\'Quantity\'] = pd.to_numeric(df[\'Quantity\'], errors=\'coerce\') df[\'Price\'] = pd.to_numeric(df[\'Price\'], errors=\'coerce\') df = df.dropna(subset=[\'Quantity\', \'Price\']) df = df[(df[\'Quantity\'] >= 0) & (df[\'Price\'] >= 0)] return df def monthly_sales(df): Groups by OrderDate month and sums the Quantity and the total sales (Quantity * Price). df[\'Month\'] = df[\'OrderDate\'].dt.to_period(\'M\').astype(str) monthly_df = df.groupby(\'Month\').agg(TotalQuantity=(\'Quantity\', \'sum\'), TotalSales=(\'Price\', \'sum\')).reset_index() return monthly_df def top_customers(df, n): Identifies the top `n` customers based on the total sales amount. df[\'TotalSales\'] = df[\'Quantity\'] * df[\'Price\'] top_customers_df = df.groupby(\'CustomerID\').agg(TotalSales=(\'TotalSales\', \'sum\')).nlargest(n, \'TotalSales\').reset_index() return top_customers_df def product_performance(df, start_date, end_date): Filters the data for orders between `start_date` and `end_date`, and groups by ProductID. mask = (df[\'OrderDate\'] >= start_date) & (df[\'OrderDate\'] <= end_date) filtered_df = df.loc[mask] product_perf_df = filtered_df.groupby(\'ProductID\').agg(TotalQuantity=(\'Quantity\', \'sum\'), TotalSales=(\'Price\', \'sum\')).reset_index() return product_perf_df"},{"question":"**Coding Assessment Question: Implementing and Utilizing a Custom PyTorch Module** You are tasked with implementing a PyTorch module `CustomModule`. This custom module should include: 1. **Initialization Function (`__init__`)**: - Initialize two tensors: - A 2x3 tensor filled with ones and datatype as `torch.float32`. - A 3x2 tensor filled with random values and datatype as `torch.float32`. - These tensors should also have `requires_grad=True`. 2. **Forward Function (`forward`)**: - Perform the following operations in the given order: - Matrix multiplication between the two tensors. - Apply the sigmoid activation function to the result. - Compute the sum of all elements in the resulting tensor. 3. **Custom Method (`compute_gradients`)**: - Compute the gradients of the resulting sum (from the forward function) with respect to the original two tensors using backward propagation. **Implementation Details**: - Implement a class `CustomModule` inheriting from `torch.nn.Module`. - Define and implement the `__init__` method. - Define and implement the `forward` method. - Define and implement the `compute_gradients` method. **Input and Output**: - The `forward` method should return a tensor which is the sum of all elements after applying the described operations. - The `compute_gradients` method should return the gradients of the two original tensors. **Example Usage**: ```python import torch class CustomModule(torch.nn.Module): def __init__(self): super(CustomModule, self).__init__() # Your initialization code here def forward(self): # Your forward pass code here def compute_gradients(self): # Your gradient computation code here # Example usage model = CustomModule() output = model.forward() gradients = model.compute_gradients() print(f\\"Output: {output.item()}\\") print(f\\"Gradients: {gradients}\\") ``` **Constraints**: - You are not allowed to use any libraries other than PyTorch. - Ensure the tensors are of correct shapes and data types as specified. - Handle the backward propagation correctly to compute gradients. **Performance Requirements**: - The implementation should efficiently handle the tensor operations and gradient computations. - Avoid unnecessary computations or copies of tensors.","solution":"import torch import torch.nn as nn class CustomModule(nn.Module): def __init__(self): super(CustomModule, self).__init__() self.tensor1 = torch.ones((2, 3), dtype=torch.float32, requires_grad=True) self.tensor2 = torch.randn((3, 2), dtype=torch.float32, requires_grad=True) def forward(self): result = torch.matmul(self.tensor1, self.tensor2) sigmoid_result = torch.sigmoid(result) sum_result = torch.sum(sigmoid_result) return sum_result def compute_gradients(self): output = self.forward() output.backward() grad_tensor1 = self.tensor1.grad grad_tensor2 = self.tensor2.grad return grad_tensor1, grad_tensor2"},{"question":"**Advanced Coding Assessment Question: Importing and Executing Modules from a ZIP Archive** **Problem Statement:** You have been provided with a ZIP archive named `modules.zip` which contains several Python modules. Your task is to write a Python function that performs the following operations: 1. **Verifies** that the given ZIP archive exists and is a valid ZIP file. 2. **Lists all Python modules** (`*.py` or `*.pyc` files) that are available within the root of the archive. 3. For each module found: - **Imports the module** and verifies that it can be executed. - **Retrieves and prints the docstring** of the module if it exists. - **Executes a predefined function** `main()` within each module (if the function exists) and collects the return values. **Function Signature:** ```python def process_zip_archive(archive_path: str) -> dict: pass ``` **Input:** - `archive_path` (str): The file path to the ZIP archive. **Output:** - Returns a dictionary where each key is the module name and the value is the result of the `main()` function or an appropriate message if the `main()` function does not exist. **Constraints:** - The function should handle errors gracefully and print appropriate error messages for invalid ZIP files, missing modules, or any issues during the module loading/execution process. - You can assume that each module, if it contains a `main()` function, takes no arguments and its return value should be collected. - The `docstring` should be printed even if the `main()` function does not exist. **Example:** ```python # Suppose the archive_path points to a ZIP archive containing the following files: # - module1.py (with docstring and main function) # - module2.py (with docstring but no main function) # - module3.py (no docstring but with main function) # Calling the function: result = process_zip_archive(\\"path/to/modules.zip\\") # The output dictionary might look something like: { \\"module1\\": \\"Result of module1\\", \\"module2\\": \\"No main function found\\", \\"module3\\": \\"Result of module3\\", } ``` This question assesses the student\'s ability to interact with files, handle exceptions, dynamically import modules, and execute functions programmatically.","solution":"import zipfile import os import sys import importlib.util def process_zip_archive(archive_path: str) -> dict: result = {} # Check if the given path is a valid ZIP file if not zipfile.is_zipfile(archive_path): print(f\\"Error: The file {archive_path} is not a valid ZIP file.\\") return result with zipfile.ZipFile(archive_path, \'r\') as zip_ref: zip_files = zip_ref.namelist() # Filter Python files py_files = [f for f in zip_files if f.endswith(\'.py\') or f.endswith(\'.pyc\')] # If no Python files are found in the root, return with a message if not py_files: print(f\\"No Python modules found in {archive_path}.\\") return result # Temporary extraction path extract_path = \\"temp_extracted_modules\\" zip_ref.extractall(extract_path) # Add extraction path to sys.path sys.path.append(extract_path) for py_file in py_files: # Module name without file extension module_name = os.path.splitext(os.path.basename(py_file))[0] try: # Load the module from the extracted path spec = importlib.util.spec_from_file_location(module_name, os.path.join(extract_path, py_file)) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) # Retrieve and print the docstring docstring = module.__doc__ or \\"No docstring found\\" print(f\\"Docstring for {module_name}: {docstring}\\") # Execute the main function if it exists if hasattr(module, \'main\') and callable(module.main): result[module_name] = module.main() else: result[module_name] = \\"No main function found\\" except Exception as e: result[module_name] = f\\"Error importing/executing the module: {str(e)}\\" # Clean up: remove the temporary extraction path sys.path.remove(extract_path) for root, dirs, files in os.walk(extract_path, topdown=False): for name in files: os.remove(os.path.join(root, name)) for name in dirs: os.rmdir(os.path.join(root, name)) os.rmdir(extract_path) return result"},{"question":"# Email Header Manipulation Challenge In this challenge, you will be working with the `email.header` module to construct and manipulate MIME-compliant email headers containing international characters. The goal is to ensure that the headers are properly encoded and decoded according to RFC standards. Problem Statement You are given raw input strings that need to be formatted as email headers. Each input string can contain non-ASCII characters. Your task is to: 1. Create a MIME-compliant email header using the `Header` class. 2. Encode the header to be RFC-compliant. 3. Decode the header back to its original form to verify correctness. Requirements 1. Define a function `create_email_header` that takes the following parameters: - `header_value`: The initial value of the header (string). - `charset`: The character set for the header value (string, e.g., \'utf-8\' or \'iso-8859-1\'). - `header_name` (optional): The name of the header field (e.g., \'Subject\') to account for the first line length. 2. Define a function `verify_email_header` to decode the encoded header back to its original form: - `encoded_header`: The encoded header string. 3. The `create_email_header` function should return the encoded header as a string. 4. The `verify_email_header` function should return the decoded header value as a string. Constraints - The input header value will be a non-empty string with UTF-8 or ISO-8859-1 encoded characters. - The charset will be valid and either \'utf-8\' or \'iso-8859-1\'. - The header name, if provided, will be a valid email header field name. Example ```python from email.header import Header, decode_header def create_email_header(header_value: str, charset: str, header_name: str = None) -> str: h = Header(header_value, charset, header_name=header_name) return h.encode() def verify_email_header(encoded_header: str) -> str: decoded_parts = decode_header(encoded_header) return \'\'.join([part.decode(charset) if charset else part.decode() for part, charset in decoded_parts]) # Example Usage header_value = \\"pÃ¶stal\\" charset = \\"iso-8859-1\\" header_name = \\"Subject\\" encoded_header = create_email_header(header_value, charset, header_name) print(encoded_header) # Should show the encoded header decoded_header_value = verify_email_header(encoded_header) print(decoded_header_value) # Should output \'pÃ¶stal\' ``` Write implementations for the functions `create_email_header` and `verify_email_header` by following the given specifications. Ensure your solution correctly handles the encoding and decoding processes.","solution":"from email.header import Header, decode_header def create_email_header(header_value: str, charset: str, header_name: str = None) -> str: Creates a MIME-compliant email header. :param header_value: The initial value of the header. :param charset: The character set for the header value. :param header_name: The name of the header field. :return: The encoded header as a string. h = Header(header_value, charset) if header_name: encoded_header = h.encode(header_name) else: encoded_header = h.encode() return encoded_header def verify_email_header(encoded_header: str) -> str: Decodes the encoded email header back to its original form. :param encoded_header: The encoded header string. :return: The decoded header value as a string. decoded_parts = decode_header(encoded_header) return \'\'.join([part.decode(charset or \'utf-8\') if isinstance(part, bytes) else part for part, charset in decoded_parts])"},{"question":"# Advanced Coding Assessment: Geometric and Statistical Computations Objective: You will implement a function that computes several geometric and statistical properties of a given dataset. Problem Statement: Given a list of tuples representing points in a 2D plane, implement a function `analyze_points(points)` that performs the following tasks: 1. **Compute the Euclidean distances** from each point to all other points and store these distances. 2. **Determine the closest pair of points** (the pair with the minimum Euclidean distance). 3. **Calculate the centroid** of all the points. 4. **Compute the variance and standard deviation** of the distances in 1. Input: - A list of tuples `points`, where each tuple `(x, y)` represents the coordinates of a point in a 2D plane. Output: - A dictionary containing: - `\'distances\'`: A list of lists where each inner list contains the distances from one point to every other point. - `\'closest_pair\'`: The closest pair of points as a tuple of tuples `((x1, y1), (x2, y2))`. - `\'centroid\'`: The centroid of all points as a tuple `(cx, cy)`. - `\'variance\'`: The variance of all distances. - `\'std_dev\'`: The standard deviation of all distances. Constraints: - Each tuple in `points` contains two numeric values. - At least two points must be provided to compute distances. Additional Information: - The Euclidean distance between two points ((x1, y1)) and ((x2, y2)) is given by ( sqrt{(x2 - x1)^2 + (y2 - y1)^2} ). - Use the `math` module functions wherever applicable to perform computations. Example: ```python points = [(1, 2), (3, 4), (5, 6), (7, 8)] result = analyze_points(points) # Expected output: # { # \'distances\': [ # [0.0, 2.8284271247461903, 5.656854249492381, 8.48528137423857], # [2.8284271247461903, 0.0, 2.8284271247461903, 5.656854249492381], # [5.656854249492381, 2.8284271247461903, 0.0, 2.8284271247461903], # [8.48528137423857, 5.656854249492381, 2.8284271247461903, 0.0] # ], # \'closest_pair\': ((1, 2), (3, 4)), # \'centroid\': (4.0, 5.0), # \'variance\': 8.0, # \'std_dev\': 2.8284271247461903 # } ``` Note: - Make sure to handle edge cases and input validation appropriately. - Efficiency matters: aim for a solution with good performance given the constraints. Here is the function signature for your implementation: ```python def analyze_points(points): pass ``` Hints: - Use the `math.dist` function to calculate distances. - Use the `math.fsum` function to compute sums accurately when calculating the centroid. - Variance is the average of squared deviations from the mean distance. - Standard deviation is the square root of the variance.","solution":"import math def euclidean_distance(p1, p2): return math.dist(p1, p2) def analyze_points(points): if len(points) < 2: raise ValueError(\\"At least two points are required to compute distances.\\") # Compute distances n = len(points) distances = [[0.0] * n for _ in range(n)] min_distance = float(\'inf\') closest_pair = None all_distances = [] for i in range(n): for j in range(n): if i != j: dist = euclidean_distance(points[i], points[j]) distances[i][j] = dist all_distances.append(dist) if dist < min_distance: min_distance = dist closest_pair = (points[i], points[j]) # Compute centroid sum_x = sum(point[0] for point in points) sum_y = sum(point[1] for point in points) centroid = (sum_x / n, sum_y / n) # Compute variance and standard deviation mean_distance = sum(all_distances) / len(all_distances) variance = sum((d - mean_distance) ** 2 for d in all_distances) / len(all_distances) std_dev = math.sqrt(variance) return { \'distances\': distances, \'closest_pair\': closest_pair, \'centroid\': centroid, \'variance\': variance, \'std_dev\': std_dev }"},{"question":"<|Analysis Begin|> The provided documentation gives an extensive overview of the Python \\"random\\" module, its functions, and their details. The \\"random\\" module is designed to generate pseudo-random numbers for various distributions (uniform, normal, lognormal, etc.), as well as perform operations such as shuffling sequences and sampling. The module supports both integer and float operations and offers a variety of functions to support random number generation, including `random()`, `uniform()`, `randrange()`, `shuffle()`, and many more. Key concepts covered include: 1. Different types of distributions (uniform, normal, lognormal, etc.). 2. Functions for manipulating sequences (choice, choices, shuffle, sample). 3. The `Random` class to create custom random number generators. 4. Usage and modifications of the seed. 5. Limitations and recommendations for security-sensitive operations (using `secrets` module). Given the depth of the documentation, we can design a challenging question that involves creating a custom class that utilizes multiple functions from the \\"random\\" module, ensuring the student understands the interplay between different random operations and distributions. <|Analysis End|> <|Question Begin|> # Custom Random Generator Class Objective: Design a custom random number generator class that utilizes the \\"random\\" module to provide custom methods for a variety of random generation tasks. Your implementation should demonstrate a good understanding of both fundamental and advanced concepts from the \\"random\\" module. Task: 1. Create a class `CustomRandom` that inherits from `random.Random`. 2. Implement the following methods: - `random_integer(self, a, b)`: Returns a random integer N such that `a <= N <= b`. - `random_float(self, a, b)`: Returns a uniform random float N such that `a <= N <= b`. - `random_sample(self, population, k)`: Returns a list of `k` unique random elements selected from `population`. - `random_choice(self, seq)`: Returns a random element from the non-empty sequence `seq`. - `random_gaussian(self, mu, sigma)`: Returns a random float from a Gaussian distribution with mean `mu` and standard deviation `sigma`. - `random_exponential(self, lambd)`: Returns a random float from an exponential distribution with rate parameter `lambd`. Your implementations should correctly handle edge cases, such as sequences being empty or invalid argument values. Input: There are no direct inputs as each method operates independently. The arguments for each method are specified as part of the method requirements: - `random_integer(a, b)`: - `a` and `b`: Integers where `a <= b`. - `random_float(a, b)`: - `a` and `b`: Floats where `a <= b`. - `random_sample(population, k)`: - `population`: A list or set of elements. - `k`: An integer where `0 <= k <= len(population)`. - `random_choice(seq)`: - `seq`: A non-empty sequence (list, tuple, etc.). - `random_gaussian(mu, sigma)`: - `mu`: Mean of the distribution (float). - `sigma`: Standard deviation of the distribution (float, `sigma > 0`). - `random_exponential(lambd)`: - `lambd`: Rate parameter (float, `lambd != 0`). Output: Each method should return results based on the operations defined. The results should be of appropriate types. Constraints: - Use appropriate functions from the \\"random\\" module for each task. - Ensure results are correctly formatted and returned. - Handle edge cases and invalid inputs gracefully, raising appropriate exceptions where necessary. Example Usage: ```python from random import Random class CustomRandom(Random): def random_integer(self, a, b): return self.randint(a, b) def random_float(self, a, b): return self.uniform(a, b) def random_sample(self, population, k): return self.sample(population, k) def random_choice(self, seq): return self.choice(seq) def random_gaussian(self, mu, sigma): return self.gauss(mu, sigma) def random_exponential(self, lambd): return self.expovariate(lambd) # Usage Example cr = CustomRandom() print(cr.random_integer(1, 10)) print(cr.random_float(1.0, 5.0)) print(cr.random_sample([1, 2, 3, 4, 5], 3)) print(cr.random_choice([\'apple\', \'banana\', \'cherry\'])) print(cr.random_gaussian(0, 1)) print(cr.random_exponential(0.5)) ``` Notes: - For the `random_integer` method, ensure `a` and `b` are integers and `a <= b`. - For the `random_float` method, ensure `a` and `b` are floats and `a <= b`. - For the `random_sample` method, ensure `population` is not empty and `0 <= k <= len(population)`. - For the `random_choice` method, ensure `seq` is not empty. - For the `random_gaussian` method, ensure `sigma > 0`. - For the `random_exponential` method, ensure `lambd != 0`. Please implement the `CustomRandom` class as described above.","solution":"import random class CustomRandom(random.Random): def random_integer(self, a, b): Returns a random integer N such that a <= N <= b. if not isinstance(a, int) or not isinstance(b, int): raise TypeError(\\"Both a and b should be integers.\\") if a > b: raise ValueError(\\"a should be less than or equal to b.\\") return self.randint(a, b) def random_float(self, a, b): Returns a uniform random float N such that a <= N <= b. if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\\"Both a and b should be numbers.\\") if a > b: raise ValueError(\\"a should be less than or equal to b.\\") return self.uniform(a, b) def random_sample(self, population, k): Returns a list of k unique random elements selected from population. if not isinstance(population, (list, set, tuple)): raise TypeError(\\"Population should be a list, set, or tuple.\\") if not isinstance(k, int): raise TypeError(\\"k should be an integer.\\") if k < 0 or k > len(population): raise ValueError(\\"k should be in the range 0 <= k <= len(population).\\") return self.sample(population, k) def random_choice(self, seq): Returns a random element from the non-empty sequence seq. if not isinstance(seq, (list, set, tuple, str)): raise TypeError(\\"Sequence should be a list, set, tuple, or string.\\") if not seq: raise ValueError(\\"Sequence should not be empty.\\") return self.choice(seq) def random_gaussian(self, mu, sigma): Returns a random float from a Gaussian distribution with mean mu and standard deviation sigma. if not isinstance(mu, (int, float)) or not isinstance(sigma, (int, float)): raise TypeError(\\"mu and sigma should be numbers.\\") if sigma <= 0: raise ValueError(\\"sigma should be greater than 0.\\") return self.gauss(mu, sigma) def random_exponential(self, lambd): Returns a random float from an exponential distribution with rate parameter lambd. if not isinstance(lambd, (int, float)): raise TypeError(\\"lambd should be a number.\\") if lambd == 0: raise ValueError(\\"lambd should not be 0.\\") return self.expovariate(lambd)"},{"question":"**Python Coding Assessment Question** # Problem Statement You are tasked with automating the creation and distribution of a Python package. Use the legacy Distutils package to write a setup script and additional functionalities required to achieve this. # Task 1. **Write a Setup Script:** - Implement a `setup.py` script that: - Lists individual Python modules to be included in the package. - Includes metadata such as name, version, author, and description. - Specifies any package data and additional files needed for the package. - Ensure the setup script can handle the installation of these modules. 2. **Create a Source Distribution:** - Write a function `create_source_distribution()` that: - Generates a source distribution for your package. - Ensures that all necessary files specified in the setup script are included. - Output the location of the generated source distribution. 3. **Create a Binary Distribution:** - Write a function `create_binary_distribution()` that: - Generates a binary distribution for your package. - Supports creating a \\"dumb\\" installer and an RPM package (choose appropriate for your environment). - Cross-compile on Windows if necessary. # Requirements - **Input/Output:** - The setup script should be a file named `setup.py` in the root of your package directory. - The functions `create_source_distribution()` and `create_binary_distribution()` should take no input parameters and return the paths to the generated distributions. - **Constraints:** - Assume the package directory contains at least two Python modules. - You should handle exceptions and provide meaningful error messages if the distribution process fails. - **Performance:** - The distribution creation should be efficient and not include unnecessary files. # Example Consider a package named `mypackage` with the following structure: ``` mypackage/ module1.py module2.py data/ dataset.csv setup.py ``` Your `setup.py` might look like this: ```python from distutils.core import setup setup( name=\'mypackage\', version=\'0.1\', description=\'Example Python Package\', author=\'Your Name\', author_email=\'youremail@example.com\', packages=[\'mypackage\'], package_data={\'mypackage\': [\'data/dataset.csv\']}, py_modules=[\'module1\', \'module2\'], ) ``` # Submission Submit the `setup.py` script and the two functions `create_source_distribution()` and `create_binary_distribution()`. Ensure your solution is well-documented and includes comments explaining your approach.","solution":"import os from distutils.core import run_setup # Create the \'setup.py\' script setup_script = from distutils.core import setup setup( name=\'mypackage\', version=\'0.1\', description=\'Example Python Package\', author=\'Your Name\', author_email=\'youremail@example.com\', packages=[\'mypackage\'], package_data={\'mypackage\': [\'data/dataset.csv\']}, py_modules=[\'mypackage.module1\', \'mypackage.module2\'], ) with open(\'setup.py\', \'w\') as f: f.write(setup_script) # Function to create source distribution def create_source_distribution(): try: os.system(\'python setup.py sdist\') return os.path.join(os.getcwd(), \'dist\') except Exception as e: return str(e) # Function to create binary distribution def create_binary_distribution(): try: os.system(\'python setup.py bdist\') return os.path.join(os.getcwd(), \'dist\') except Exception as e: return str(e)"},{"question":"# Advanced Coding Challenge Background In Python, coroutines are functions that use the `async` keyword. When called, these functions return a coroutine object. Coroutines are primarily used to run asynchronous code, allowing for non-blocking operations. Handling coroutines effectively requires understanding how to create, manage, and check coroutine objects using the concepts provided in the documentation. Task You are required to implement a Python module that performs the following: 1. **Function to Check Coroutine**: - **Function Name**: `is_coroutine` - **Input**: A Python function (object). - **Output**: Boolean (`True` if the function is a coroutine, otherwise `False`). - **Details**: Use the coroutine checking mechanism as described. 2. **Function to Create and Run a Coroutine**: - **Function Name**: `run_coroutine` - **Input**: A string `task_name`, a qualified name `qualname`, and an integer `value`. - **Output**: The result of the coroutine which adds `value` to its argument. - **Details**: Create a coroutine that takes an integer and returns it added to `value`. Run the coroutine and return its result. Constraints 1. The `value` should be between 1 and 1000. 2. The function should handle the creation and execution of coroutines efficiently. # Example Usages: ```python # Example usage for is_coroutine def sample_coroutine(): await asyncio.sleep(1) print(is_coroutine(sample_coroutine)) # Expected output: True def normal_function(): return \\"I\'m just a regular function\\" print(is_coroutine(normal_function)) # Expected output: False # Example usage for run_coroutine result = run_coroutine(\\"ExampleTask\\", \\"ExampleTask.qualname\\", 5) print(result) # Expected output: The coroutine should add 5 to its input argument and return the result ``` Notes: - You might need to use the `asyncio` library to handle coroutines. - Ensure that your function `is_coroutine` can accurately distinguish between coroutine and non-coroutine functions. - Test your coroutine creation and execution logic thoroughly. Bonus: If you have time, try to handle coroutines raised within the `run_coroutine` and return appropriate error messages.","solution":"import asyncio import inspect def is_coroutine(func): Checks if the given function is a coroutine. :param func: Function to be checked :return: True if the function is a coroutine, False otherwise return inspect.iscoroutinefunction(func) async def simple_coroutine(num, value): Asynchronous coroutine that adds value to num. :param num: Initial number :param value: Value to be added :return: The sum of num and value await asyncio.sleep(0) # Simulating some asynchronous work return num + value def run_coroutine(task_name, qualname, value): Runs a coroutine that will add `value` to its input. :param task_name: Name of the task :param qualname: Qualified name :param value: Value to be added :return: Result of the coroutine after execution if not (1 <= value <= 1000): raise ValueError(\\"Value should be between 1 and 1000\\") async def wrapper(): result = await simple_coroutine(0, value) return result loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) result = loop.run_until_complete(wrapper()) loop.close() return result"},{"question":"# Question: XML Data Processing with `xml.etree.ElementTree` Using the `xml.etree.ElementTree` module, write a function `process_xml(input_xml: str, target_tag: str, new_tag: str, text_to_add: str) -> str` that performs the following operations: 1. **Parse** the XML data from the input string `input_xml`. 2. **Find** all elements with the tag name `target_tag`. 3. For each found element, **add a new child element** with the tag name `new_tag` and text content `text_to_add`. 4. **Return** the modified XML data as a string. Input: - `input_xml` (str): A well-formed XML string. - `target_tag` (str): The tag name of elements to which new child elements should be added. - `new_tag` (str): The tag name of the new child elements. - `text_to_add` (str): The text content for the new child elements. Output: - (str): A string containing the modified XML data. Constraints: - The XML will always be well-formed. - `target_tag`, `new_tag`, and `text_to_add` are guaranteed to be non-empty strings. Example: ```python input_xml = \'\'\'<root> <item> <name>Item1</name> </item> <item> <name>Item2</name> </item> </root>\'\'\' target_tag = \'item\' new_tag = \'description\' text_to_add = \'This is a description.\' output_xml = process_xml(input_xml, target_tag, new_tag, text_to_add) print(output_xml) ``` # Expected Output: ```xml <root> <item> <name>Item1</name> <description>This is a description.</description> </item> <item> <name>Item2</name> <description>This is a description.</description> </item> </root> ``` Notes: - Make sure to handle namespaces if they are present in the input XML. - Your function should be efficient in terms of time complexity and be able to handle XML documents of moderate size gracefully.","solution":"import xml.etree.ElementTree as ET def process_xml(input_xml: str, target_tag: str, new_tag: str, text_to_add: str) -> str: Processes the input XML, finds all elements with the target_tag, and adds a new child element with new_tag and text_to_add to each found element. Returns the modified XML as a string. root = ET.fromstring(input_xml) for element in root.findall(f\'.//{target_tag}\'): new_element = ET.Element(new_tag) new_element.text = text_to_add element.append(new_element) return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Question: Utilizing the MPS Backend in PyTorch The MPS backend in PyTorch enables high-performance training on GPUs for MacOS devices with the Metal programming framework. Your task is to implement a function that leverages the MPS backend to perform a series of operations on tensors and a simple neural network model. Function Specification **Function Name:** `use_mps_backend` **Inputs:** - No inputs are required for this function. **Outputs:** - Prints a message indicating whether the MPS backend is available. - If the MPS backend is available: - Creates a tensor of shape (5,) with all elements initialized to 1. - Multiplies the tensor by 2. - Defines a simple feedforward neural network model. - Moves the model to the MPS device. - Runs a forward pass with the tensor through the model and prints the output. **Simple Neural Network Model:** - Define a class `SimpleNet` (inherits from `torch.nn.Module`) with the following structure: - An input layer that maps from 5 features to 10 features. - A ReLU activation function. - An output layer that maps from 10 features to 5 features. **Constraints:** - Ensure your code handles scenarios where the MPS backend is not available gracefully. - Define the forward pass method in the `SimpleNet` class to perform a forward computation. Example: ```python class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = torch.nn.Linear(5, 10) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(10, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x ``` Implementation Implement the `use_mps_backend` function to meet the above specifications, handling the necessary checks for MPS availability, and move the tensor and model to the MPS device accordingly. ```python def use_mps_backend(): import torch # Check that MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return # Set the MPS device mps_device = torch.device(\\"mps\\") # Create a tensor directly on the MPS device x = torch.ones(5, device=mps_device) print(f\\"Tensor x on MPS device: {x}\\") # Any operation happens on the GPU y = x * 2 print(f\\"Tensor y after multiplying by 2 on MPS device: {y}\\") # Define the SimpleNet class class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = torch.nn.Linear(5, 10) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(10, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Instantiate and move the model to the MPS device model = SimpleNet() model.to(mps_device) # Run a forward pass with the tensor through the model pred = model(x) print(f\\"Model output with input tensor x: {pred}\\") # Example usage use_mps_backend() ```","solution":"def use_mps_backend(): import torch # Check that MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return # Set the MPS device mps_device = torch.device(\\"mps\\") # Create a tensor directly on the MPS device x = torch.ones(5, device=mps_device) print(f\\"Tensor x on MPS device: {x}\\") # Any operation happens on the GPU y = x * 2 print(f\\"Tensor y after multiplying by 2 on MPS device: {y}\\") # Define the SimpleNet class class SimpleNet(torch.nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = torch.nn.Linear(5, 10) self.relu = torch.nn.ReLU() self.fc2 = torch.nn.Linear(10, 5) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x # Instantiate and move the model to the MPS device model = SimpleNet() model.to(mps_device) # Run a forward pass with the tensor through the model pred = model(x) print(f\\"Model output with input tensor x: {pred}\\") # Example usage use_mps_backend()"},{"question":"Objective: To assess the student\'s ability to implement hyper-parameter tuning using scikit-learn\'s `GridSearchCV` to find the optimal parameters for a given estimator within a machine learning pipeline. Background: You are provided with a dataset containing numerical data and categorical labels. Your task is to preprocess the data, build a machine learning pipeline, and optimize the hyper-parameters of a Support Vector Machine (SVM) using `GridSearchCV`. Task: 1. Load the dataset. 2. Preprocess the data including handling missing values and scaling the features. 3. Build a machine learning pipeline that includes: - An imputer for handling missing values. - A scaler for feature scaling. - A Support Vector Classifier (`SVC`). 4. Define a parameter grid for `GridSearchCV` to tune the `C`, `kernel`, and `gamma` parameters of the `SVC`. 5. Use `GridSearchCV` to find the best hyper-parameters based on cross-validation and print the best parameters and corresponding best score. 6. Evaluate the best estimator on a separate test set and report the accuracy. Input Format: - A CSV file containing the dataset where: - The last column denotes the labels. - All other columns are features. Constraints: - Use a random state of 42 for reproducibility in missing value imputation and train-test splitting. - Use k-fold cross-validation with 5 folds for the `GridSearchCV`. Performance Requirements: - The entire process (loading data, preprocessing, building the pipeline, parameter tuning, and evaluation) should be performed with efficient use of computational resources, avoiding redundant computations. Expected Function Signature: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def tune_svm_with_grid_search(file_path: str) -> dict: This function tunes the hyper-parameters of an SVM using GridSearchCV and evaluates its performance. Parameters: - file_path (str): Path to the CSV file containing the dataset. Returns: - dict: A dictionary containing the best parameters and the test set accuracy. # Step 1: Load the dataset data = pd.read_csv(file_path) # Step 2: Split the data into features and labels X = data.iloc[:, :-1] y = data.iloc[:, -1] # Step 3: Split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 4: Define the pipeline with imputation, scaling, and SVM pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'svm\', SVC()) ]) # Step 5: Define the parameter grid param_grid = { \'svm__C\': [0.1, 1, 10, 100], \'svm__kernel\': [\'linear\', \'rbf\'], \'svm__gamma\': [1, 0.1, 0.01, 0.001] } # Step 6: Apply GridSearchCV grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Step 7: Print the best parameters and the corresponding best score best_params = grid_search.best_params_ best_score = grid_search.best_score_ print(f\\"Best Parameters: {best_params}\\") print(f\\"Best Cross-Validation Score: {best_score:.4f}\\") # Step 8: Evaluate the best estimator on the test set best_estimator = grid_search.best_estimator_ y_pred = best_estimator.predict(X_test) test_accuracy = accuracy_score(y_test, y_pred) # Return the results return { \'best_params\': best_params, \'test_accuracy\': test_accuracy } # Example usage: # result = tune_svm_with_grid_search(\'path/to/dataset.csv\') # print(result) ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def tune_svm_with_grid_search(file_path: str) -> dict: This function tunes the hyper-parameters of an SVM using GridSearchCV and evaluates its performance. Parameters: - file_path (str): Path to the CSV file containing the dataset. Returns: - dict: A dictionary containing the best parameters and the test set accuracy. # Step 1: Load the dataset data = pd.read_csv(file_path) # Step 2: Split the data into features and labels X = data.iloc[:, :-1] y = data.iloc[:, -1] # Step 3: Split the dataset into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 4: Define the pipeline with imputation, scaling, and SVM pipeline = Pipeline([ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'svm\', SVC()) ]) # Step 5: Define the parameter grid param_grid = { \'svm__C\': [0.1, 1, 10, 100], \'svm__kernel\': [\'linear\', \'rbf\'], \'svm__gamma\': [1, 0.1, 0.01, 0.001] } # Step 6: Apply GridSearchCV grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) # Step 7: Print the best parameters and the corresponding best score best_params = grid_search.best_params_ best_score = grid_search.best_score_ print(f\\"Best Parameters: {best_params}\\") print(f\\"Best Cross-Validation Score: {best_score:.4f}\\") # Step 8: Evaluate the best estimator on the test set best_estimator = grid_search.best_estimator_ y_pred = best_estimator.predict(X_test) test_accuracy = accuracy_score(y_test, y_pred) # Return the results return { \'best_params\': best_params, \'test_accuracy\': test_accuracy } # Example usage: # result = tune_svm_with_grid_search(\'path/to/dataset.csv\') # print(result)"},{"question":"**Coding Assessment Question:** # Objective Your task is to demonstrate your understanding of seaborn\'s context and customization functionalities. Write a function that sets different styles for different types of plots and displays them in a single figure. # Requirements 1. Your function should create a figure with the following subplots: - A line plot - A bar plot - A scatter plot 2. Change the context to `\'paper\'` for the line plot, `\'talk\'` for the bar plot, and `\'poster\'` for the scatter plot. 3. Override specific settings: - Increase the font scale to `1.5` for all plots. - Set the line width to `2` for the line plot. - Set the bar width to `0.5` for the bar plot. - Change the marker size to `100` for the scatter plot. # Constraints - Use `matplotlib.pyplot.subplots` to create the figure and axes. - Use the given data for plotting: ```python data = { \\"x\\": [1, 2, 3, 4, 5], \\"y\\": [5, 4, 3, 2, 1], \\"z\\": [2, 3, 5, 7, 11] } ``` # Expected Function Signature ```python def customized_seaborn_plots(data: dict) -> None: pass ``` # Example Output The function should display a single figure with 3 subplots, each customized according to the specified requirements. --- **Note:** Ensure that your plots are neatly arranged and the context settings are correctly applied to the respective subplots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customized_seaborn_plots(data: dict) -> None: sns.set_context(\\"paper\\", font_scale=1.5) fig, axs = plt.subplots(3, 1, figsize=(10, 15)) # Line Plot sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"], ax=axs[0]) axs[0].lines[0].set_linewidth(2) axs[0].set_title(\'Line Plot\') # Bar Plot sns.set_context(\\"talk\\", font_scale=1.5) sns.barplot(x=data[\\"x\\"], y=data[\\"y\\"], ax=axs[1], ci=None) bars = axs[1].patches for bar in bars: bar.set_width(0.5) axs[1].set_title(\'Bar Plot\') # Scatter Plot sns.set_context(\\"poster\\", font_scale=1.5) sns.scatterplot(x=data[\\"x\\"], y=data[\\"z\\"], ax=axs[2], s=100) axs[2].set_title(\'Scatter Plot\') plt.tight_layout() plt.show()"},{"question":"**Question: Kleene Logic and Boolean Indexing with NA in Pandas** You are given a DataFrame containing a mix of boolean values and `NA`. Your task is to implement a function that processes this DataFrame based on specific criteria. The function should: 1. Replace NA values in a specified boolean column according to some filling strategy. 2. Apply a logical operation between two specified boolean columns. 3. Return a filtered view of the DataFrame based on the result of the logical operation. # Function Signature ```python import pandas as pd from pandas import DataFrame def process_boolean_dataframe(df: DataFrame, bool_col1: str, bool_col2: str, fill_value: bool, operation: str) -> DataFrame: Processes the input DataFrame based on Kleene logic operations. Parameters: df (DataFrame): Input DataFrame containing nullable boolean columns. bool_col1 (str): Name of the first boolean column in the DataFrame. bool_col2 (str): Name of the second boolean column in the DataFrame. fill_value (bool): Value to replace NA in bool_col2 (\'True\' or \'False\'). operation (str): Logical operation to apply (\'AND\', \'OR\', \'XOR\'). Returns: DataFrame: The filtered DataFrame based on the results of the logical operation. # Constraints - `bool_col1` and `bool_col2` must exist in the DataFrame and should have a `boolean` dtype. - The logical operation should be one of \'AND\', \'OR\', \'XOR\'. - If the `operation` parameter is not one of these values, raise a `ValueError`. # Example ```python import pandas as pd import numpy as np # Sample DataFrame data = { \'A\': pd.array([True, False, pd.NA, True], dtype=\\"boolean\\"), \'B\': pd.array([pd.NA, False, True, True], dtype=\\"boolean\\") } df = pd.DataFrame(data) # Process DataFrame result = process_boolean_dataframe(df, \'A\', \'B\', True, \'AND\') print(result) ``` Output: ``` A B 0 True <NA> 3 True True ``` # Explanation 1. The NA values in column `B` are filled with `True`, making the column `B` = `[True, False, True, True]`. 2. The \'AND\' operation is applied between columns `A` and `B`, resulting in `[True, False, NA, True]`. 3. The DataFrame is filtered to only include the rows where the result of the logical operation is `True`, thus returning the rows with indices 0 and 3.","solution":"import pandas as pd def process_boolean_dataframe(df: pd.DataFrame, bool_col1: str, bool_col2: str, fill_value: bool, operation: str) -> pd.DataFrame: Processes the input DataFrame based on Kleene logic operations. Parameters: df (pd.DataFrame): Input DataFrame containing nullable boolean columns. bool_col1 (str): Name of the first boolean column in the DataFrame. bool_col2 (str): Name of the second boolean column in the DataFrame. fill_value (bool): Value to replace NA in bool_col2 (True or False). operation (str): Logical operation to apply (\'AND\', \'OR\', \'XOR\'). Returns: pd.DataFrame: The filtered DataFrame based on the results of the logical operation. if bool_col1 not in df.columns or bool_col2 not in df.columns: raise ValueError(\\"Specified columns do not exist in DataFrame\\") if df[bool_col1].dtype != \\"boolean\\" or df[bool_col2].dtype != \\"boolean\\": raise ValueError(\\"Specified columns are not of boolean dtype\\") if operation not in [\'AND\', \'OR\', \'XOR\']: raise ValueError(\\"Invalid logical operation. Choose from \'AND\', \'OR\', \'XOR\'\\") df_filled = df.copy() df_filled[bool_col2] = df_filled[bool_col2].fillna(fill_value) result = None if operation == \'AND\': result = df_filled[bool_col1] & df_filled[bool_col2] elif operation == \'OR\': result = df_filled[bool_col1] | df_filled[bool_col2] elif operation == \'XOR\': result = df_filled[bool_col1] ^ df_filled[bool_col2] filtered_df = df[result.fillna(False)] return filtered_df"},{"question":"**Coding Assessment Question: Tar Archive Manipulation** # Objective Design a Python function to create a compressed tar archive from a directory, and subsequently extract a specific type of files from the archive to a designated directory. # Description This task will assess your ability to work with the `tarfile` module, which includes creating tar files, handling different compression methods, and extracting files based on their extensions from tar archives. # Function Definition 1. **create_tar_archive(source_dir: str, archive_name: str, compression: str) -> None** - Creates a tar archive from the given source directory. - Parameters: - `source_dir`: A string representing the path of the source directory to be archived. - `archive_name`: A string representing the name of the resulting tar archive. - `compression`: A string representing the compression type. Acceptable values are `gz`, `bz2`, or `xz`. - The created archive should be compressed with the specified method. - Use appropriate error handling to manage file access issues. 2. **extract_files_by_extension(archive_name: str, dest_dir: str, extension: str) -> List[str]** - Extracts files with the specified extension from the tar archive to the designated directory. - Parameters: - `archive_name`: A string representing the name of the tar archive to be read. - `dest_dir`: A string representing the directory where the extracted files will be placed. - `extension`: A string representing the file extension to filter by during extraction (e.g., \\".txt\\"). - Returns: - A list of strings representing the paths of the extracted files. - Handle cases where no files match the given extension and return an empty list. - Use appropriate error handling to manage file extraction issues. # Constraints - Assume the source directory and its contents are accessible and readable. - Ensure the destination directory for extraction is writable. - Handle cases where the specified tar archive or source directory does not exist. - Consider edge cases such as empty directories and no files matching the specified extension. # Example ```python try: create_tar_archive(\\"data\\", \\"backup.tar.gz\\", \\"gz\\") extracted_files = extract_files_by_extension(\\"backup.tar.gz\\", \\"extracted_data\\", \\".txt\\") print(\\"Extracted Files:\\", extracted_files) except Exception as e: print(f\\"An error occurred: {e}\\") ``` - `create_tar_archive(\\"data\\", \\"backup.tar.gz\\", \\"gz\\")` should create a gzip-compressed tar archive named `backup.tar.gz` from the `data` directory. - `extract_files_by_extension(\\"backup.tar.gz\\", \\"extracted_data\\", \\".txt\\")` should extract all `.txt` files from the `backup.tar.gz` archive to the `extracted_data` directory and return the list of extracted file paths. # Performance Requirements - Ensure the solution efficiently handles directories with a large number of files and nested subdirectories. - Aim for a clean and readable code structure with adequate documentation and comments. # Notes - This question requires you to demonstrate a solid understanding of the `tarfile` module in Python. Ensure you familiarize yourself with the relevant methods and their intricacies as per the provided documentation.","solution":"import tarfile import os from typing import List def create_tar_archive(source_dir: str, archive_name: str, compression: str) -> None: Creates a compressed tar archive from the given source directory. Parameters: source_dir (str): Path to the source directory to be archived. archive_name (str): Name of the resulting tar archive. compression (str): Compression type (gz, bz2, xz). mode = \'w\' if compression == \'gz\': mode += \':gz\' elif compression == \'bz2\': mode += \':bz2\' elif compression == \'xz\': mode += \':xz\' else: raise ValueError(\\"Invalid compression type. Use \'gz\', \'bz2\', or \'xz\'.\\") with tarfile.open(archive_name, mode) as tar: tar.add(source_dir, arcname=os.path.basename(source_dir)) def extract_files_by_extension(archive_name: str, dest_dir: str, extension: str) -> List[str]: Extracts files with the specified extension from the tar archive to the designated directory. Parameters: archive_name (str): Name of the tar archive to be read. dest_dir (str): Directory where the extracted files will be placed. extension (str): File extension to filter by during extraction (e.g., \\".txt\\"). Returns: List[str]: Paths of the extracted files. if not os.path.exists(dest_dir): os.makedirs(dest_dir) extracted_files = [] with tarfile.open(archive_name, \'r:*\') as tar: for member in tar.getmembers(): if member.isfile() and member.name.endswith(extension): extracted_path = os.path.join(dest_dir, os.path.basename(member.name)) with tar.extractfile(member) as file: with open(extracted_path, \'wb\') as out_file: out_file.write(file.read()) extracted_files.append(extracted_path) return extracted_files"},{"question":"<|Analysis Begin|> The provided documentation covers the `http.client` module, which implements client-side HTTP (and HTTPS) protocols. It details various components including: 1. **Classes**: - `HTTPConnection`: Handles communication with an HTTP server. - `HTTPSConnection`: A subclass of `HTTPConnection` that adds SSL support for HTTPS. - `HTTPResponse`: Represents the server\'s response, typically returned by methods in `HTTPConnection` and `HTTPSConnection`. - `HTTPMessage`: Holds the HTTP headers from a response. 2. **Functions**: - `parse_headers`: Parses headers from a file pointer representing an HTTP request/response, returning an `http.client.HTTPMessage`. 3. **Exceptions**: Lists several exceptions that are raised in different error scenarios when dealing with HTTP/HTTPS connections. 4. **Constants**: - `HTTP_PORT`, `HTTPS_PORT`: Default ports for HTTP and HTTPS protocols. - `responses`: A dictionary mapping HTTP status codes to their respective names. The documentation also provides numerous examples showing how to use `HTTPConnection` and `HTTPSConnection` to perform various HTTP operations (GET, POST, PUT, HEAD), handle responses, and manage connection parameters. Key concepts demonstrated include: - Establishing and managing HTTP/HTTPS connections. - Sending various types of HTTP requests (GET, POST, PUT, HEAD). - Handling responses and parsing headers. - Using and understanding exceptions related to HTTP connections. Given this comprehensive overview, a challenging coding question can be crafted to assess the student\'s understanding and ability to utilize the `http.client` module effectively. <|Analysis End|> <|Question Begin|> # HTTP Connection Homework In this task, you are required to create a Python class that performs various HTTP operations while utilizing the `http.client` module. Your class should be named `HttpClient` and should include methods to handle GET, POST, and PUT requests, along with handling and printing of response data. Class Definition ```python class HttpClient: def __init__(self, host, use_https=False): Initialize a connection to the provided host. :param host: The hostname to connect to. :param use_https: Boolean specifying whether to use HTTPS (default is False). # Your code here def send_get_request(self, url): Send a GET request to the given URL and return the response status, reason, and body. :param url: The URL to send the GET request to. :return: A tuple containing response status, reason, and body. # Your code here def send_post_request(self, url, params, headers={}): Send a POST request to the given URL with provided parameters and headers. :param url: The URL to send the POST request to. :param params: The parameters to include in the POST request body. :param headers: A dictionary of headers to include in the request (default is empty). :return: A tuple containing response status, reason, and body. # Your code here def send_put_request(self, url, body, headers={}): Send a PUT request to the given URL with provided body and headers. :param url: The URL to send the PUT request to. :param body: The data to include in the PUT request body. :param headers: A dictionary of headers to include in the request (default is empty). :return: A tuple containing response status, reason, and body. # Your code here def close_connection(self): Close the current connection. # Your code here ``` # Implementation Details 1. **Initialization**: The `__init__` method should initialize an HTTP or HTTPS connection based on the `use_https` flag. 2. **GET Request**: The `send_get_request` method should send a GET request to the specified URL and return the response status, reason, and body. 3. **POST Request**: The `send_post_request` method should send a POST request with the specified parameters and headers to the given URL and return the response status, reason, and body. Use `urllib.parse.urlencode` for encoding parameters. 4. **PUT Request**: The `send_put_request` method should send a PUT request with the specified body and headers to the given URL and return the response status, reason, and body. 5. **Close Connection**: The `close_connection` method should close the current connection. # Example Usage ```python client = HttpClient(\'www.example.com\', use_https=True) status, reason, body = client.send_get_request(\'/\') print(f\\"GET /: {status} {reason}: {body}\\") params = {\'key\': \'value\'} headers = {\\"Content-type\\": \\"application/x-www-form-urlencoded\\"} status, reason, body = client.send_post_request(\'/submit\', params, headers) print(f\\"POST /submit: {status} {reason}: {body}\\") body = \\"updated content\\" headers = {\\"Content-type\\": \\"text/plain\\"} status, reason, body = client.send_put_request(\'/update\', body, headers) print(f\\"PUT /update: {status} {reason}: {body}\\") client.close_connection() ``` # Constraints - Your implementation should handle possible exceptions that might be raised when initiating connections or sending requests. - Do not use any third-party modules, only modules available in the Python standard library. Good luck!","solution":"import http.client import urllib.parse class HttpClient: def __init__(self, host, use_https=False): Initialize a connection to the provided host. :param host: The hostname to connect to. :param use_https: Boolean specifying whether to use HTTPS (default is False). self.host = host self.use_https = use_https if use_https: self.connection = http.client.HTTPSConnection(host) else: self.connection = http.client.HTTPConnection(host) def send_get_request(self, url): Send a GET request to the given URL and return the response status, reason, and body. :param url: The URL to send the GET request to. :return: A tuple containing response status, reason, and body. self.connection.request(\\"GET\\", url) response = self.connection.getresponse() return (response.status, response.reason, response.read().decode()) def send_post_request(self, url, params, headers={}): Send a POST request to the given URL with provided parameters and headers. :param url: The URL to send the POST request to. :param params: The parameters to include in the POST request body. :param headers: A dictionary of headers to include in the request (default is empty). :return: A tuple containing response status, reason, and body. params_encoded = urllib.parse.urlencode(params) headers[\\"Content-Length\\"] = str(len(params_encoded)) self.connection.request(\\"POST\\", url, body=params_encoded, headers=headers) response = self.connection.getresponse() return (response.status, response.reason, response.read().decode()) def send_put_request(self, url, body, headers={}): Send a PUT request to the given URL with provided body and headers. :param url: The URL to send the PUT request to. :param body: The data to include in the PUT request body. :param headers: A dictionary of headers to include in the request (default is empty). :return: A tuple containing response status, reason, and body. headers[\\"Content-Length\\"] = str(len(body)) self.connection.request(\\"PUT\\", url, body=body, headers=headers) response = self.connection.getresponse() return (response.status, response.reason, response.read().decode()) def close_connection(self): Close the current connection. self.connection.close()"},{"question":"# Secure Random Token Generator **Objective**: Implement a function that generates a secure token for password reset functionality. The token must be generated using the `secrets` module to ensure cryptographic strength. **Function Signature**: ```python def generate_secure_token(length: int, output_format: str) -> str: ``` **Parameters**: - `length` (int): The number of bytes of randomness to be used for generating the token. Must be a positive integer. - `output_format` (str): Specifies the format of the output string. It can be one of the following: - `\'hex\'`: Generates a hexadecimal string. - `\'bytes\'`: Generates a byte string. - `\'urlsafe\'`: Generates a URL-safe Base64 encoded string. **Returns**: - (str): A string representation of the secure token in the specified format. **Constraints**: - The function should raise a `ValueError` if the `output_format` is not one of `\'hex\'`, `\'bytes\'`, or `\'urlsafe\'`. - The function should ensure that the `length` parameter is a positive integer; otherwise, raise a `ValueError`. - The function should use the `secrets` module to generate the token to ensure cryptographic strength. **Example Usage**: ```python >>> generate_secure_token(16, \'hex\') \'f9bf78b9a18ce6d46a0cd2b0b86df9da\' >>> generate_secure_token(16, \'urlsafe\') \'Drmhze6EPcv0fN_81Bj-nA\' >>> generate_secure_token(16, \'bytes\') b\'xebrx17D*txaexd4xe3Sxb6xe2xebP1x8b\' ``` **Notes**: - Ensure that your implementation adheres strictly to the requirements and handles edge cases appropriately. For example, consider the behavior when invalid arguments are provided. - You can assume the input values for `length` are within a reasonable range for practical use in secure token generation.","solution":"import secrets import base64 def generate_secure_token(length: int, output_format: str) -> str: Generates a secure token for password reset functionality. Parameters: length (int): The number of bytes of randomness to be used for generating the token. Must be a positive integer. output_format (str): Specifies the format of the output string. It can be one of: - \'hex\': Generates a hexadecimal string. - \'bytes\': Generates a byte string. - \'urlsafe\': Generates a URL-safe Base64 encoded string. Returns: str: A string representation of the secure token in the specified format. Raises: ValueError: If `output_format` is not one of \'hex\', \'bytes\', or \'urlsafe\' or if `length` is not a positive integer. if not isinstance(length, int) or length <= 0: raise ValueError(\\"The `length` parameter must be a positive integer.\\") if output_format not in {\'hex\', \'bytes\', \'urlsafe\'}: raise ValueError(\\"The `output_format` must be one of \'hex\', \'bytes\', or \'urlsafe\'.\\") random_bytes = secrets.token_bytes(length) if output_format == \'hex\': return random_bytes.hex() elif output_format == \'bytes\': return random_bytes elif output_format == \'urlsafe\': return base64.urlsafe_b64encode(random_bytes).rstrip(b\'=\').decode(\'ascii\')"},{"question":"# Advanced Python310 - Object Manipulation and Type Checks **Problem Statement:** You are required to implement a function `process_data` that takes in a dictionary containing various types of objects, performs validation using type-checking, and manipulates the data to produce a desired output. **Function Signature:** ```python def process_data(data: dict) -> dict: ``` **Input:** - `data` (dict): A dictionary containing keys as strings and values as one of the following types: lists, tuples, integer, floating-point numbers, complex numbers, sets, or other dictionaries. **Output:** - A dictionary where: - For each key-value pair in the input: - If the value is a list, it should append the length of the list to the original list. - If the value is a tuple, it should convert it into a list. - If the value is an integer, increment it by 1. - If the value is a floating-point number, it should return the square of the number. - If the value is a complex number, add its conjugate to the original. - If the value is a set, it should be converted into a sorted list. - If the value is another dictionary, it should reverse the key-value pairs. **Constraints:** - Dictionary values should only be one of the given specific types. - The input dictionary will have at least one key-value pair. **Example:** ```python input_data = { \\"key1\\": [1, 2, 3], \\"key2\\": (4, 5, 6), \\"key3\\": 10, \\"key4\\": 2.5, \\"key5\\": 1 + 2j, \\"key6\\": {7, 8, 9}, \\"key7\\": {\\"a\\": 1, \\"b\\": 2} } output_data = { \\"key1\\": [1, 2, 3, 3], # List with length appended \\"key2\\": [4, 5, 6], # Tuple converted to list \\"key3\\": 11, # Integer incremented by 1 \\"key4\\": 6.25, # Floating point squared \\"key5\\": (1+2j) + (1-2j), # Complex number with its conjugate \\"key6\\": [7, 8, 9], # Set converted to sorted list \\"key7\\": {1: \\"a\\", 2: \\"b\\"} # Dictionary with reversed key-value pairs } ``` **Notes:** - Use the appropriate type-checking functions as suggested in the \\"Concrete Objects Layer\\" documentation. - Make sure the function is robust, handling incorrect types gracefully by simply ignoring them. - Ensure efficiency in your implementation, considering the performance of type-check and data manipulation operations.","solution":"def process_data(data: dict) -> dict: result = {} for key, value in data.items(): if isinstance(value, list): result[key] = value + [len(value)] elif isinstance(value, tuple): result[key] = list(value) elif isinstance(value, int): result[key] = value + 1 elif isinstance(value, float): result[key] = value ** 2 elif isinstance(value, complex): result[key] = value + value.conjugate() elif isinstance(value, set): result[key] = sorted(list(value)) elif isinstance(value, dict): result[key] = {v: k for k, v in value.items()} else: continue # ignoring any other types not defined in constraints return result"},{"question":"Objective You are tasked with writing a Python utility function that adheres to best practices for accessing and manipulating `__annotations__` across different Python versions. This function will need to safely retrieve, modify, and return the annotations dictionary of various Python objects, while handling potential quirks and ensuring compatibility with Python versions >= 3.6. Function Specifications Write a function `process_annotations(obj, action, new_annotations=None)` that performs the following operations: - `obj`: The object whose annotations are being accessed. It can be a function, class, or module. - `action`: Specifies the action to be performed. It can be `\\"get\\"` to retrieve annotations, `\\"set\\"` to set new annotations, or `\\"clear\\"` to clear the annotations. - `new_annotations`: A dictionary of new annotations to set if the action is `\\"set\\"`. This parameter is ignored for other actions. The function should return the annotations dictionary after performing the specified action. If the action is `\\"get\\"`, it returns the current annotations directly. If the action is `\\"clear\\"`, it should return an empty dictionary representing cleared annotations. Constraints 1. Handle stringized annotations and un-stringize them if necessary. 2. Ensure safe access to annotations using best practices outlined in the documentation. 3. Ensure compatibility with both Python 3.10+ and Python 3.9 and older. 4. Handle edge cases such as missing `__annotations__` attributes and annotation inheritance in classes. Assumptions - Python versions will be limited to >= 3.6. Examples ```python def example_function(a: \\"int\\", b: \\"str\\") -> \\"None\\": pass # Action: Get annotations print(process_annotations(example_function, \\"get\\")) # Output: {\'a\': int, \'b\': str, \'return\': None} # Action: Set new annotations new_anns = {\'a\': float, \'c\': str} print(process_annotations(example_function, \\"set\\", new_anns)) # Output: {\'a\': float, \'c\': str} # Action: Clear annotations print(process_annotations(example_function, \\"clear\\")) # Output: {} ``` This function will validate your understanding of annotations in Python, their retrieval, manipulation, and best practices across different Python versions.","solution":"import typing import sys def process_annotations(obj, action, new_annotations=None): Safely retrieve, modify, and return the annotations dictionary of various Python objects. Parameters: - obj: The object whose annotations are being accessed. It can be a function, class, or module. - action: Specifies the action to be performed. It can be \\"get\\" to retrieve annotations, \\"set\\" to set new annotations, or \\"clear\\" to clear the annotations. - new_annotations: A dictionary of new annotations to set if the action is \\"set\\". This parameter is ignored for other actions. Returns: - The annotations dictionary after performing the specified action. if not hasattr(obj, \'__annotations__\'): obj.__annotations__ = {} annotations = obj.__annotations__ if action == \'get\': return annotations elif action == \'set\': if new_annotations is not None: if not isinstance(new_annotations, dict): raise ValueError(\\"new_annotations must be a dictionary\\") obj.__annotations__ = new_annotations return obj.__annotations__ elif action == \'clear\': obj.__annotations__ = {} return {} else: raise ValueError(\\"Invalid action specified. Use \'get\', \'set\', or \'clear\'\\")"},{"question":"**Objective**: Implement a custom parser for email messages using the provided `email.errors` exception and defect classes. Your parser should be able to detect a subset of the described defects and raise appropriate exceptions or attach defects to the message when necessary. **Question**: You are required to implement a class called `CustomEmailParser` which will parse the raw email content and identify specific defects. Your parser should specifically handle the following defects: - `NoBoundaryInMultipartDefect` - `FirstHeaderLineIsContinuationDefect` - `MalformedHeaderDefect` Additionally, your parser should raise the `HeaderParseError` exception when it encounters a defect that it cannot handle. **Requirements**: 1. Implement the `CustomEmailParser` class with the following methods: - `__init__(self, raw_message: str)`: Initializes the parser with the raw email message string. - `parse(self) -> None`: Parses the raw email message and identifies defects. - `get_defects(self) -> list`: Returns a list of detected defects. 2. You may use helper functions as needed to organize your code. **Constraints**: - The raw email message will be a string formatted according to **RFC 5322**. - You must use the specified exception and defect classes from the `email.errors` module. **Input**: - A raw email message string. **Output**: - A list of detected defects as instances of `MessageDefect` subclasses. - Raise the `HeaderParseError` exception for unhandled defects. **Example**: ```python from email.errors import NoBoundaryInMultipartDefect, FirstHeaderLineIsContinuationDefect, MalformedHeaderDefect, HeaderParseError class CustomEmailParser: def __init__(self, raw_message: str): # Your implementation here def parse(self) -> None: # Your implementation here def get_defects(self) -> list: # Your implementation here # Example usage: try: raw_message = \\"Content-Type: multipart/mixed; boundary=\\"boundary\\"nn--boundarynContent-Type: text/plainnnThis is the bodyn--boundary--\\" parser = CustomEmailParser(raw_message) parser.parse() print(parser.get_defects()) except HeaderParseError as e: print(f\\"Unhandled header parse error: {e}\\") ``` **Expected Behavior**: 1. If the raw message does not include a boundary for `multipart`, detect `NoBoundaryInMultipartDefect`. 2. If the first header line is a continuation (no leading white space), detect `FirstHeaderLineIsContinuationDefect`. 3. If there are header lines missing a colon or malformed, detect `MalformedHeaderDefect`. 4. For any other defect, raise `HeaderParseError`. Implement the `CustomEmailParser` class, ensuring that it correctly identifies and handles the defects and exceptions as described.","solution":"from email.errors import NoBoundaryInMultipartDefect, FirstHeaderLineIsContinuationDefect, MalformedHeaderDefect, HeaderParseError class CustomEmailParser: def __init__(self, raw_message: str): self.raw_message = raw_message self.defects = [] def parse(self) -> None: in_headers = True headers = [] lines = self.raw_message.split(\\"n\\") for i, line in enumerate(lines): if in_headers: if line == \\"\\": in_headers = False elif i == 0 and line.startswith((\\" \\", \\"t\\")): self.defects.append(FirstHeaderLineIsContinuationDefect()) elif not (\\":\\" in line and line.count(\\":\\") == 1): self.defects.append(MalformedHeaderDefect()) headers.append(line) if any(\\"Content-Type: multipart\\" in header for header in headers) and \\"boundary=\\" not in self.raw_message: self.defects.append(NoBoundaryInMultipartDefect()) # In a real-world scenario, we might have more conditions here # that could trigger a HeaderParseError for unhandled cases. def get_defects(self) -> list: return self.defects"},{"question":"**Sparse Data Manipulation and Analysis with Pandas** **Objective:** Write a function that takes in a dense DataFrame, converts it to a sparse representation, performs several manipulations and calculations, and finally converts the result back to a dense DataFrame. **Function Signature:** ```python def process_sparse_data(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `df`: A `pandas.DataFrame` containing numeric values with some missing values (represented as `NaN`s). **Expected Steps:** 1. Convert the given `df` to a sparse DataFrame, choosing `np.nan` as the fill value. 2. Calculate and print the density of the sparse DataFrame. 3. Perform element-wise square root transformation on the sparse DataFrame. Use `numpy` ufuncs to achieve this. 4. Convert the transformed sparse DataFrame back to a dense DataFrame. 5. Return one or more dense DataFrames that showcase the transformation and any intermediate steps you deem important. **Constraints:** - Do not use loops to perform element-wise operations; use appropriate pandas or numpy functions. - Ensure that the function is efficient in both time and space. **Example:** ```python import pandas as pd import numpy as np # Example dense DataFrame with NaN values data = { \'A\': [1, np.nan, 2, np.nan, np.nan], \'B\': [np.nan, 4, np.nan, 5, 6], \'C\': [7, 8, np.nan, np.nan, 10] } df = pd.DataFrame(data) # Expected function call and output result = process_sparse_data(df) print(result) ``` **Explanation:** 1. Original dense DataFrame: ``` A B C 0 1.0 NaN 7.0 1 NaN 4.0 8.0 2 2.0 NaN NaN 3 NaN 5.0 NaN 4 NaN 6.0 10.0 ``` 2. Sparse DataFrame representation. 3. Density calculation printed. 4. Element-wise square root transformation applied. 5. Resulting dense DataFrame after transformation: ``` A B C 0 1.000000 NaN 2.645751 1 NaN 2.000000 2.828427 2 1.414214 NaN NaN 3 NaN 2.236068 NaN 4 NaN 2.449490 3.162278 ``` **Hints:** - Investigate `pd.SparseDtype`. - To apply element-wise operations, explore `numpy` ufuncs like `np.sqrt`. - Use `.memory_usage().sum()` and `.sparse.density` to assess density and memory.","solution":"import pandas as pd import numpy as np def process_sparse_data(df: pd.DataFrame) -> pd.DataFrame: Converts a dense DataFrame to sparse, performs manipulations, and then converts it back to a dense DataFrame. Parameters: df (pd.DataFrame): The input dense DataFrame with NaN values. Returns: pd.DataFrame: The resulting dense DataFrame after transformations. # Convert to Sparse DataFrame sparse_df = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Calculate and print the density of the sparse DataFrame density = sparse_df.sparse.density print(f\\"Density of the sparse DataFrame: {density:.2f}\\") # Perform element-wise square root transformation sqrt_sparse_df = sparse_df.applymap(np.sqrt) # Convert the transformed sparse DataFrame back to a dense DataFrame result_df = sqrt_sparse_df.sparse.to_dense() return result_df"},{"question":"# Question 1: Implement the Isomap Algorithm **Problem Statement:** You are required to implement a simplified version of the Isomap algorithm for non-linear dimensionality reduction. The Isomap algorithm extends Multi-dimensional Scaling (MDS) by incorporating geodesic distances instead of Euclidean distances. Your task is to implement the main steps of the Isomap algorithm, which are: 1. Nearest Neighbors Search 2. Shortest-Path Graph Search 3. Partial Eigenvalue Decomposition **Function Signature:** ```python def isomap_algorithm(data: np.ndarray, n_neighbors: int, n_components: int) -> np.ndarray: Perform Isomap dimensionality reduction on the input data. Parameters: - data (np.ndarray): A 2D array of shape (N, D) where N is the number of samples and D is the dimensionality of each sample. - n_neighbors (int): The number of nearest neighbors to use for constructing the neighborhood graph. - n_components (int): The number of dimensions to reduce to. Returns: - reduced_data (np.ndarray): A 2D array of shape (N, n_components) containing the data in the reduced dimensionality space. ``` **Detailed Description:** 1. **Nearest Neighbors Search:** - Compute the pairwise Euclidean distances between data points. - Construct a neighborhood graph by connecting each data point to its nearest `n_neighbors` neighbors. 2. **Shortest-Path Graph Search:** - Apply the Floyd-Warshall algorithm or Dijkstra\'s algorithm to find the shortest paths between all pairs of points in the neighborhood graph. This will yield the geodesic distances matrix. 3. **Partial Eigenvalue Decomposition:** - Compute the centered geodesic distance matrix. - Perform eigenvalue decomposition on the geodesic distance matrix. - Select the top `n_components` eigenvectors to form the reduced dimensionality space. **Constraints:** - The number of examples, `N`, will be between 50 and 1000. - The dimensionality of the input data, `D`, will be between 3 and 100. - `n_neighbors` will always be less than `N`. - `n_components` will always be less than or equal to `D`. **Example:** ```python import numpy as np data = np.random.random((100, 50)) # 100 samples with 50 features each n_neighbors = 5 n_components = 2 reduced_data = isomap_algorithm(data, n_neighbors, n_components) print(reduced_data.shape) # Expected output: (100, 2) ``` Hints: - You may use `sklearn.neighbors.NearestNeighbors` for the nearest neighbor search. - You can use `scipy.sparse.csgraph.shortest_path` for finding the shortest path in the graph. - For eigenvalue decomposition, you can use `scipy.linalg.eigh`.","solution":"import numpy as np from sklearn.neighbors import NearestNeighbors from scipy.sparse.csgraph import shortest_path from scipy.linalg import eigh def isomap_algorithm(data: np.ndarray, n_neighbors: int, n_components: int) -> np.ndarray: # Step 1: Nearest Neighbors Search nbrs = NearestNeighbors(n_neighbors=n_neighbors).fit(data) distances, indices = nbrs.kneighbors(data) # Create the neighborhood graph (weighted adjacency matrix) N = data.shape[0] graph = np.full((N, N), np.inf) for i, neighbors in enumerate(indices): graph[i, neighbors] = distances[i] # Step 2: Shortest-Path Graph Search geodesic_distances = shortest_path(graph, directed=False) # Step 3: Partial Eigenvalue Decomposition # Centering the geodesic distance matrix n_samples = geodesic_distances.shape[0] H = np.eye(n_samples) - np.ones((n_samples, n_samples)) / n_samples K = -0.5 * H.dot(geodesic_distances ** 2).dot(H) # Compute the top `n_components` eigenvalues and eigenvectors eigenvalues, eigenvectors = eigh(K, subset_by_index=[n_samples - n_components, n_samples - 1]) # Sort eigenvalues and corresponding eigenvectors in descending order sorted_indices = np.argsort(-eigenvalues) eigenvectors = eigenvectors[:, sorted_indices] eigenvalues = eigenvalues[sorted_indices] # Compute the reduced data reduced_data = eigenvectors[:, :n_components] * np.sqrt(eigenvalues[:n_components]) return reduced_data"},{"question":"# Asynchronous Task Scheduler with Priority Queues You are required to implement an asynchronous task scheduler that utilizes the `asyncio.PriorityQueue` class. The scheduler should be able to add tasks with varying priorities and ensures that tasks with the highest priority (i.e., lowest priority number) get executed first. # Input and Output Formats **Input**: An input list of tasks where each task is represented as a tuple: - The first element is a string representing the task name. - The second element is an integer representing the task priority. - The third element is the execution time in seconds the task should take during processing. Example: ```python tasks = [ (\'Task1\', 2, 1.5), (\'Task2\', 1, 2.0), (\'Task3\', 3, 0.5) ] ``` **Output**: Print the order in which the tasks were executed and the total execution time for all tasks combined. Example: ``` Output: Task2 has been executed. Task1 has been executed. Task3 has been executed. Total execution time: 4.00 seconds ``` # Constraints and Performance Requirements 1. The function should be asynchronous and make use of the `asyncio.PriorityQueue` for task management. 2. Task execution should be simulated using `asyncio.sleep()` to match the provided execution time. 3. Any task enqueued must be marked as done using `queue.task_done()` after completion. 4. All tasks should be processed in priority order, and the total time reported should be an accurate float representing the combined sleep durations. # Function Signature ```python import asyncio async def task_scheduler(tasks): pass ``` # Example Usage ```python import asyncio async def task_scheduler(tasks): queue = asyncio.PriorityQueue() # Define a worker coroutine async def worker(): while True: priority, task_name, execution_time = await queue.get() await asyncio.sleep(execution_time) print(f\'{task_name} has been executed.\') queue.task_done() # Add tasks to the queue for task in tasks: await queue.put((task[1], task[0], task[2])) # Start a worker worker_task = asyncio.create_task(worker()) # Wait for all tasks to be processed await queue.join() # Cancel worker worker_task.cancel() await worker_task tasks = [ (\'Task1\', 2, 1.5), (\'Task2\', 1, 2.0), (\'Task3\', 3, 0.5) ] asyncio.run(task_scheduler(tasks)) ``` Use the function signature and guidelines provided to implement the asynchronous task scheduler.","solution":"import asyncio async def task_scheduler(tasks): queue = asyncio.PriorityQueue() # Define a worker coroutine async def worker(): nonlocal total_execution_time while True: priority, task_name, execution_time = await queue.get() await asyncio.sleep(execution_time) print(f\'{task_name} has been executed.\') total_execution_time += execution_time queue.task_done() # Add tasks to the queue for task in tasks: await queue.put((task[1], task[0], task[2])) # Initialize total execution time total_execution_time = 0.0 # Start a worker worker_task = asyncio.create_task(worker()) # Wait for all tasks to be processed await queue.join() # Cancel worker worker_task.cancel() try: await worker_task except asyncio.CancelledError: pass # Print the total execution time in a formatted way print(f\'Total execution time: {total_execution_time:.2f} seconds\')"},{"question":"# Task: Create a Complex MIME Email Message In this question, you are required to build a MIME email message in Python using the `email.mime` package. Your email should include multiple types of content: plain text, an image, and an attachment. Additionally, the email should be well-structured and adhere to MIME standards. Details: 1. **Plain Text**: The email should have a simple text body. 2. **Image**: Embed an image within the email. 3. **Attachment**: Include an external file (e.g., a PDF) as an attachment to the email. 4. **Multi-Part Structure**: The email should use a multipart structure to combine these elements properly. Expected Input: - Text body as a string. - Image data in bytes. - File data in bytes along with the appropriate MIME type for each attachment. Expected Output: - A complete `MIMEMultipart` email message object. Constraints: - Handle all exceptions appropriately. - Use appropriate MIME types for the attachments. - Ensure encoding standards are followed. Function Signature: ```python def create_complex_mime_email(text_body: str, image_data: bytes, image_subtype: str, file_data: bytes, file_mime_type: str, file_name: str) -> email.mime.multipart.MIMEMultipart: pass ``` # Example Usage: ```python text = \\"This is the plain text body of the email.\\" with open(\\"example.jpg\\", \\"rb\\") as img_file: img_data = img_file.read() image_subtype = \\"jpeg\\" # Assuming JPEG image with open(\\"example.pdf\\", \\"rb\\") as file: file_data = file.read() file_mime_type = \\"application/pdf\\" file_name = \\"example.pdf\\" email_message = create_complex_mime_email(text, img_data, image_subtype, file_data, file_mime_type, file_name) ``` # Notes: - Ensure the image and the file are correctly encoded. - The email should be a valid `MIMEMultipart` object that can be sent using an SMTP server. - Make sure you use appropriate class constructors and methods to construct the email. Good luck!","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.base import MIMEBase from email import encoders def create_complex_mime_email(text_body: str, image_data: bytes, image_subtype: str, file_data: bytes, file_mime_type: str, file_name: str) -> MIMEMultipart: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = \'Complex MIME Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Attach the plain text body part1 = MIMEText(text_body, \'plain\') msg.attach(part1) # Attach the image part image_part = MIMEImage(image_data, _subtype=image_subtype) msg.attach(image_part) # Attach the file part file_part = MIMEBase(\'application\', file_mime_type) file_part.set_payload(file_data) encoders.encode_base64(file_part) file_part.add_header(\'Content-Disposition\', f\'attachment; filename=\\"{file_name}\\"\') msg.attach(file_part) return msg"},{"question":"# Asynchronous Task Management with asyncio Objective Design a Python program using the `asyncio` library that demonstrates the management of asynchronous tasks, proper handling of exceptions, and utilization of threads to avoid blocking the event loop. Requirements 1. **Function Implementation**: - **`fetch_data_from_url(url: str) -> str`**: An asynchronous function that simulates fetching data from a URL. It should use `asyncio.sleep` to simulate a network delay and return a string containing the data fetched. - **`process_data(data: str) -> str`**: A CPU-bound function that processes the data. This function should simulate some CPU-intensive computation with a `time.sleep` call. This function should be executed in a separate thread to avoid blocking the event loop. - **`main(urls: List[str]) -> List[str]`**: The main coroutine that fetches data concurrently from a list of URLs, processes the data using a thread pool, and returns the processed data as a list of strings. 2. **Exception Handling**: - Handle exceptions in both `fetch_data_from_url` and `process_data` functions. Ensure that unhandled exceptions are logged properly and that your program continues running other tasks even if some tasks fail. 3. **Debug Mode**: - Ensure that the script runs with asyncioâ€™s debug mode enabled. Also, log long callback durations if they exceed 100 ms. Input - A list of URLs (strings). Output - A list of processed data strings corresponding to each input URL. Constraints - Assume that each `process_data` function call will take a significant amount of time due to its CPU-bound nature, so it should not block the event loop. - Use a maximum of 4 threads in your thread pool executor. Performance Requirements - The program should be efficient in handling multiple URLs concurrently and should utilize the threading efficiently to offload CPU-bound tasks. Example ```python import asyncio from concurrent.futures import ThreadPoolExecutor from typing import List import time import logging logging.basicConfig(level=logging.INFO) async def fetch_data_from_url(url: str) -> str: try: await asyncio.sleep(2) # Simulate network delay return f\\"data from {url}\\" except Exception as e: logging.error(f\\"Error fetching data from {url}: {e}\\") return \\"error\\" def process_data(data: str) -> str: try: time.sleep(5) # Simulate CPU-bound task return f\\"processed {data}\\" except Exception as e: logging.error(f\\"Error processing data: {e}\\") return \\"error\\" async def main(urls: List[str]) -> List[str]: loop = asyncio.get_event_loop() with ThreadPoolExecutor(max_workers=4) as executor: tasks = [] for url in urls: fetch_task = asyncio.create_task(fetch_data_from_url(url)) fetch_task.add_done_callback(lambda fut: logging.info(f\\"Fetched: {fut.result()}\\")) tasks.append(fetch_task) fetched_data = await asyncio.gather(*tasks) process_tasks = [loop.run_in_executor(executor, process_data, data) for data in fetched_data] processed_data = await asyncio.gather(*process_tasks) return processed_data # Running the asyncio program with debug mode enabled if __name__ == \\"__main__\\": asyncio.run(main([\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"]), debug=True) ``` Explanation - The `fetch_data_from_url` function asynchronously simulates fetching data from a provided URL, using `asyncio.sleep` to mimic network latency. - The `process_data` function simulates a CPU-bound task that processes the fetched data. This function uses `time.sleep` and is executed in a thread pool to avoid blocking the event loop. - The `main` function coordinates fetching and processing of data concurrently. It uses asyncio\'s event loop and thread pool executor to manage tasks efficiently. - The program is designed to handle exceptions gracefully, log important events, and runs in asyncio\'s debug mode to catch common asynchronous programming issues.","solution":"import asyncio from concurrent.futures import ThreadPoolExecutor from typing import List import time import logging logging.basicConfig(level=logging.INFO) async def fetch_data_from_url(url: str) -> str: try: await asyncio.sleep(2) # Simulate network delay return f\\"data from {url}\\" except Exception as e: logging.error(f\\"Error fetching data from {url}: {e}\\") return \\"error\\" def process_data(data: str) -> str: try: time.sleep(5) # Simulate CPU-bound task return f\\"processed {data}\\" except Exception as e: logging.error(f\\"Error processing data: {e}\\") return \\"error\\" async def main(urls: List[str]) -> List[str]: loop = asyncio.get_event_loop() with ThreadPoolExecutor(max_workers=4) as executor: tasks = [] for url in urls: fetch_task = asyncio.create_task(fetch_data_from_url(url)) fetch_task.add_done_callback(lambda fut: logging.info(f\\"Fetched: {fut.result()}\\")) tasks.append(fetch_task) fetched_data = await asyncio.gather(*tasks) process_tasks = [loop.run_in_executor(executor, process_data, data) for data in fetched_data] processed_data = await asyncio.gather(*process_tasks) return processed_data # Running the asyncio program with debug mode enabled if __name__ == \\"__main__\\": asyncio.run(main([\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"]), debug=True)"},{"question":"# Email Header Encoding Assessment **Objective:** Ensure correct understanding and application of the `email.header` module in Python. **Question:** You are writing a utility to handle email headers containing internationalized characters. Specifically, you need to create a function that takes a subject line containing non-ASCII characters and returns it encoded as per RFC standards, suitable for use in email headers. Function Signature ```python def encode_email_subject(subject: str, charset: str) -> str: pass ``` Input: - `subject`: A `str` representing the email subject that may contain non-ASCII characters. - `charset`: A `str` representing the character set used to encode the subject, e.g., `\'iso-8859-1\'` or `\'utf-8\'`. Output: - A `str` representing the encoded email subject header, compliant with RFC standards. Constraints: - The function should use the `Header` class from the `email.header` module to create and encode the header. - Ensure that the function handles errors appropriately, raising a `ValueError` if encoding is not possible. Example: ```python from email.header import Header def encode_email_subject(subject: str, charset: str) -> str: try: header = Header(subject, charset) encoded_subject = header.encode() return encoded_subject except Exception as e: raise ValueError(\\"Encoding failed\\") from e # Example usage subject = \'pÃ¶stal\' charset = \'iso-8859-1\' print(encode_email_subject(subject, charset)) # Output: \'=?iso-8859-1?q?p=F6stal?=\' ``` * In the provided example, the subject line contains the character \'Ã¶\', which is not part of standard ASCII. * The function encodes this subject correctly according to the provided character set and returns the encoded subject suitable for email headers. **Performance Considerations:** Ensure the function handles typical email header length efficiently without unnecessary processing.","solution":"from email.header import Header def encode_email_subject(subject: str, charset: str) -> str: Encodes an email subject line containing non-ASCII characters according to RFC standards. Parameters: subject (str): The email subject to be encoded. charset (str): The character set used to encode the subject (e.g., \'iso-8859-1\' or \'utf-8\'). Returns: str: The encoded email subject header. Raises: ValueError: If encoding is not possible. try: header = Header(subject, charset) encoded_subject = header.encode() return encoded_subject except Exception as e: raise ValueError(\\"Encoding failed\\") from e"},{"question":"**Coding Assessment Question** # Objective Write a Python function that uses the `io` module and file descriptors to implement a custom file reader and writer. This function will demonstrate your understanding of low-level file handling in Python. # Task Implement a function `custom_file_read_write` that performs the following operations: 1. **Read**: Opens an existing text file with a given file path and reads its contents line by line. 2. **Modify**: Converts the contents to uppercase. 3. **Write**: Writes the modified contents to a new file. 4. **Return**: Returns the file descriptors of both the original and the new files. # Function Signature ```python def custom_file_read_write(input_file_path: str, output_file_path: str) -> Tuple[int, int]: pass ``` # Input - `input_file_path` (str): The file path to the existing input text file. - `output_file_path` (str): The file path where the modified contents should be written. # Output - A tuple containing two integers: - The file descriptor of the input file. - The file descriptor of the output file. # Constraints - Assume the input file exists and is readable. - The output file path is writable, and if the file does not exist, it should be created. - Handle any necessary cleanup, such as closing file descriptors when done. # Example ```python input_file = \'example.txt\' output_file = \'output.txt\' input_fd, output_fd = custom_file_read_write(input_file, output_file) print(input_fd, output_fd) # Should print file descriptors (e.g., 3, 4) ``` # Notes - You must use the `io` module and work with file descriptors directly. - Remember to handle exceptions properly, ensuring that any opened file descriptors are closed in case of an error. # Hints - Use `io.open()` to handle file operations with buffering. - Use the `os` module if you need to interact with file descriptors directly. - Review the documentation for `io.TextIOWrapper`, `os.fdopen()`, and other relevant functions for handling file objects in Python.","solution":"import os import io from typing import Tuple def custom_file_read_write(input_file_path: str, output_file_path: str) -> Tuple[int, int]: # Open the input file and get the file descriptor input_fd = os.open(input_file_path, os.O_RDONLY) # Use io.open to create a file object from the file descriptor with io.open(input_fd, mode=\'rt\', encoding=\'utf-8\') as infile: # Read contents and convert to uppercase contents = infile.read().upper() # Open the output file and get the file descriptor output_fd = os.open(output_file_path, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o644) # Use io.open to create a file object from the file descriptor with io.open(output_fd, mode=\'wt\', encoding=\'utf-8\') as outfile: # Write the uppercased contents to the output file outfile.write(contents) # Return the file descriptors return input_fd, output_fd"},{"question":"Visualizing Titanic Dataset with Seaborn Objective: Create a comprehensive visualization of the Titanic dataset that showcases the central tendencies and distributions of categorical variables using Seaborn. Task: 1. Load the Titanic dataset from Seaborn. 2. Perform the following visualizations: - Create a bar plot that shows the average fare paid by passengers in each class (`Pclass`). Include error bars representing 95% confidence intervals. - Generate a violin plot that displays the distribution of age across different survival outcomes (`Survived`). Use the `hue` parameter to differentiate between male and female passengers. - Create a `FacetGrid` of box plots that show the fare distribution (`fare`) for each port of embarkation (`embarked`), creating separate rows for each class (`Pclass`) and adjusting axis labels for clarity. Input: No input is needed from the user; the data should be directly loaded from Seaborn\'s dataset repository. Constraints: - Use Seaborn for all visualizations. - Ensure the plots are well-labeled and clear. - Handle any missing data as you see fit to avoid errors in plotting. Expected Output: The following visualizations should be designed and displayed: 1. A bar plot for the average fare by `class`. 2. A violin plot for the age distribution by `survival` status and gender. 3. A `FacetGrid` of box plots for fare distributions by port of embarkation and class. Example Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Bar plot for average fare by class sns.catplot(data=titanic, x=\\"pclass\\", y=\\"fare\\", kind=\\"bar\\", ci=\\"sd\\") plt.title(\'Average Fare by Class\') # Violin plot for age distribution by survival status and gender sns.violinplot(data=titanic, x=\\"survived\\", y=\\"age\\", hue=\\"sex\\", split=True) plt.title(\'Age Distribution by Survival Status and Gender\') # FacetGrid of box plots for fare distribution by port of embarkation and class g = sns.catplot( data=titanic, x=\\"fare\\", y=\\"embarked\\", row=\\"pclass\\", kind=\\"box\\", orient=\\"h\\", sharex=False, margin_titles=True ) g.set(xlabel=\\"Fare\\", ylabel=\\"\\") g.set_titles(row_template=\\"{row_name} Class\\") plt.show() ``` Notes: - You are encouraged to customize the visualizations further by adding relevant colors, markers, or styles to make them more informative. - Ensure that all legends and titles are clear and descriptive.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Bar plot for average fare by class def plot_average_fare_by_class(data): sns.catplot(data=data, x=\\"pclass\\", y=\\"fare\\", kind=\\"bar\\", ci=95) plt.title(\'Average Fare by Class\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Average Fare\') plt.show() # Violin plot for age distribution by survival status and gender def plot_age_distribution_by_survival_and_gender(data): sns.violinplot(data=data, x=\\"survived\\", y=\\"age\\", hue=\\"sex\\", split=True) plt.title(\'Age Distribution by Survival Status and Gender\') plt.xlabel(\'Survived\') plt.ylabel(\'Age\') plt.legend(title=\'Sex\') plt.show() # FacetGrid of box plots for fare distribution by port of embarkation and class def plot_fare_distribution_by_embarkation_and_class(data): g = sns.catplot( data=data, x=\\"fare\\", y=\\"embarked\\", row=\\"pclass\\", kind=\\"box\\", orient=\\"h\\", sharex=False, margin_titles=True ) g.set(xlabel=\\"Fare\\", ylabel=\\"\\") g.set_titles(row_template=\\"{row_name} Class\\") plt.show() # Generate the plots plot_average_fare_by_class(titanic) plot_age_distribution_by_survival_and_gender(titanic) plot_fare_distribution_by_embarkation_and_class(titanic)"},{"question":"<|Analysis Begin|> The provided documentation explains the functionalities and usage of the `configparser` module in Python. This module allows for the creation and manipulation of configuration files, with features such as: - Reading and writing INI-style files. - Parsing configuration files with sections and key-value pairs. - Handling default values, option transformations and type conversions. - Custom interpolation of values within configuration files. - Extending the parser to handle custom configuration formats and validation. # Key Points: 1. **Basic Usage**: - Creating and writing a configuration file. - Reading and accessing data from a configuration file. 2. **Advanced Features**: - Custom data types with `getint()`, `getfloat()`, `getboolean()`. - Interpolation of values within the configuration. - Using `mapping protocol access` to treat sections as dictionaries. - Handling multiple configuration files by reading them successively and managing conflicting settings. 3. **Customizations**: - Extending the default parser behavior with arguments like `delimiters`, `comment_prefixes`, `allow_no_value`. - Handling legacy INI file formats and creating more complex configurations. - Using optional parameters for fine control over parsing behavior. # Potential Question: The question should assess the students\' abilities to create and manipulate a configuration file, use default values and interpolation, and extend the parser behavior to handle custom formats. <|Analysis End|> <|Question Begin|> # Advanced Configuration File Management with `configparser` **Objective:** Demonstrate the ability to use the `configparser` module for managing configuration files by creating a complex parser scenario. **Question:** You are tasked with setting up a configuration for a web application. The configuration should be able to handle default settings, read multiple configuration sources, and interpolate values based on different sections. Additionally, it should support comments and custom delimiters. 1. Create a `ConfigParser` object with the following specifications: - Default values for `Server` and `Settings`. - Allow sections and options without values. - Use `\'#\'` for comments and support inline comments starting with `\'//\'`. - Use `\':\'` as the only delimiter between keys and values. 2. Write configuration data to a string that emulates how it would look in a file: - The `DEFAULT` section should have the default timeout set to `30` and encoding to `utf-8`. - The `server` section should have the `host`, `port`, and `logfile`. - The `user` section should contain `username` and `email`. - The `settings` section should enable the feature `featureX`. 3. Read and parse a string representing another configuration with the following details: - In the section `server`, override the `port` and add `ssl`. - In the section `user`, override the `email` and add `password`. 4. Print out the final configuration in a nicely formatted manner that includes interpolated values and consider both original and overridden values. Utilize the extended interpolation if necessary. 5. Handle any possible errors gracefully, such as missing sections or invalid keys. **Implementation Requirements:** - Use a dictionary for initial defaults. - Demonstrate reading and extending configuration from multiple sources. - Implement proper error handling. - Use extended interpolation for resolving values across sections. **Expected Output:** - The final parsed configuration showing all sections, including default values and those overridden by new sources. - Correct resolution of interpolated values across sections. ```python import configparser # Step 1: Create ConfigParser object with given custom specifications config = configparser.ConfigParser( defaults={\'timeout\': \'30\', \'encoding\': \'utf-8\'}, allow_no_value=True, delimiters=(\':\'), comment_prefixes=(\'#\'), inline_comment_prefixes=(\'//\') ) # Populate initial configuration data (in reality, this data might come from reading from files) config.read_dict({ \'server\': { \'host\': \'localhost\', \'port\': \'8080\', \'logfile\': \'server.log\', }, \'user\': { \'username\': \'admin\', \'email\': \'admin@example.com\', }, \'settings\': { \'featureX\': \'Enabled\' } }) # Prepare a second source of configuration as a string second_source = [server] port: 9090 ssl [user] email: new_admin@example.com password: secret # Sample comment # Step 3: Read the second source to override values config.read_string(second_source) # Print the final configuration in a nicely formatted manner print(\'nFinal Configuration:n\') for section in config.sections(): print(f\'[{section}]\') for key in config[section]: print(f\'{key} = {config[section][key]}\') print() ```","solution":"import configparser def create_config_parser(): Create and return a ConfigParser object with custom specifications. # Step 1: Create ConfigParser object with given custom specifications config = configparser.ConfigParser( defaults={\'timeout\': \'30\', \'encoding\': \'utf-8\'}, allow_no_value=True, delimiters=(\':\'), comment_prefixes=(\'#\'), inline_comment_prefixes=(\'//\') ) # Populate initial configuration data (in reality, this data might come from reading from files) config.read_dict({ \'server\': { \'host\': \'localhost\', \'port\': \'8080\', \'logfile\': \'server.log\', }, \'user\': { \'username\': \'admin\', \'email\': \'admin@example.com\', }, \'settings\': { \'featureX\': \'Enabled\' } }) return config def read_additional_config(config): Read additional configuration from a string and update the ConfigParser object. # Prepare a second source of configuration as a string second_source = [server] port: 9090 ssl [user] email: new_admin@example.com password: secret # Sample comment # Step 3: Read the second source to override values config.read_string(second_source) def print_final_config(config): Print the final configuration in a nicely formatted manner. print(\'nFinal Configuration:n\') for section in config.sections(): print(f\'[{section}]\') for key in config[section]: print(f\'{key} = {config[section][key]}\') print() if __name__ == \\"__main__\\": config = create_config_parser() read_additional_config(config) print_final_config(config)"},{"question":"**Coding Question:** You have been tasked with creating a Python script that will compile all the Python files in a specified directory tree, excluding certain files and respecting certain constraints. **Objective:** Write a function `custom_compile_dir` that accepts a directory path and several optional parameters, then compiles all the `.py` files in the directory and its subdirectories according to the given parameters. **Function Signature:** ```python def custom_compile_dir( dir: str, exclude_patterns: List[str] = None, force: bool = False, quiet: int = 0, legacy: bool = False, optimize: Union[int, List[int]] = -1, workers: int = 1, invalidation_mode: str = \'timestamp\', stripdir: str = None, prependdir: str = None, limit_sl_dest: str = None, hardlink_dupes: bool = False ) -> bool: pass ``` **Parameters:** - `dir`: (str) The starting directory for the compilation. - `exclude_patterns`: (List[str]) A list of regex patterns. All files matching these patterns should be excluded from the compilation. - `force`: (bool) If `True`, forces the recompilation of all files, even if timestamps are up-to-date. - `quiet`: (int) Controls the verbosity of the output. 0 for full verbosity, 1 to suppress file listing but show errors, and 2 to suppress all output. - `legacy`: (bool) If `True`, writes the `.pyc` files to their legacy locations. - `optimize`: (Union[int, List[int]]) The optimization level(s) for the compiler. Can be an integer or a list of integers. - `workers`: (int) The number of worker threads to use for compilation. If 0, uses the number of CPU cores. - `invalidation_mode`: (str) The invalidation mode for the bytecode (`timestamp`, `checked-hash`, or `unchecked-hash`). - `stripdir`: (str) The directory prefix to remove from the file paths. - `prependdir`: (str) The directory prefix to append to the file paths. - `limit_sl_dest`: (str) Ignore symlinks pointing outside the given directory. - `hardlink_dupes`: (bool) If `True` and two `.pyc` files with different optimization levels have the same content, use hard links to consolidate duplicate files. **Returns:** - A boolean value indicating whether all files compiled successfully or not. **Constraints:** 1. The function should use the `compileall` module\'s `compile_dir` function to perform the compilation. 2. The function should convert the list of regex patterns in `exclude_patterns` into a single regex pattern object for `compile_dir` to use. 3. The function should handle exceptions gracefully and provide meaningful error messages if the compilation fails. **Example Usage:** ```python result = custom_compile_dir( dir=\'path_to_directory\', exclude_patterns=[r\'.svn\', r\'.git\'], force=True, quiet=1, legacy=False, optimize=[0, 1], workers=0, invalidation_mode=\'checked-hash\' ) print(result) # Output: True or False ``` **Notes:** 1. Ensure to import necessary modules such as `compileall` and `re`. 2. Consider edge cases where the directory may not exist or be empty. 3. Validate and sanitize the input parameters to prevent any runtime errors.","solution":"import compileall import re from typing import List, Union import os def custom_compile_dir( dir: str, exclude_patterns: List[str] = None, force: bool = False, quiet: int = 0, legacy: bool = False, optimize: Union[int, List[int]] = -1, workers: int = 1, invalidation_mode: str = \'timestamp\', stripdir: str = None, prependdir: str = None, limit_sl_dest: str = None, hardlink_dupes: bool = False ) -> bool: Compiles all .py files in the given directory and its subdirectories according to the specified parameters. Parameters: - dir : str : The starting directory for the compilation. - exclude_patterns : List[str] : A list of regex patterns. All files matching these patterns should be excluded from the compilation. - force : bool : If True, forces the recompilation of all files, even if timestamps are up-to-date. - quiet : int : Controls the verbosity of the output. 0 for full verbosity, 1 to suppress file listing but show errors, 2 to suppress all output. - legacy : bool : If True, writes the .pyc files to their legacy locations. - optimize : Union[int, List[int]] : The optimization level(s) for the compiler. Can be an integer or a list of integers. - workers : int : The number of worker threads to use for compilation. If 0, uses the number of CPU cores. - invalidation_mode : str : The invalidation mode for the bytecode (\'timestamp\', \'checked-hash\', or \'unchecked-hash\'). - stripdir : str : The directory prefix to remove from the file paths. - prependdir : str : The directory prefix to append to the file paths. - limit_sl_dest : str : Ignore symlinks pointing outside the given directory. - hardlink_dupes : bool : If True and two .pyc files with different optimization levels have the same content, use hard links to consolidate duplicate files. Returns: - bool : indicating whether all files compiled successfully or not. try: if exclude_patterns: exclude = re.compile(\'|\'.join(exclude_patterns)) else: exclude = None options = { \'quiet\': quiet, \'force\': force, \'legacy\': legacy, \'optimize\': optimize, \'workers\': workers, \'invalidation_mode\': invalidation_mode, \'stripdir\': stripdir, \'prependdir\': prependdir, \'limit_sl_dest\': limit_sl_dest, \'hardlink_dupes\': hardlink_dupes } if not os.path.exists(dir): raise FileNotFoundError(f\\"Directory {dir} does not exist.\\") return compileall.compile_dir( dir, rx=exclude, **options ) except Exception as e: if quiet < 2: print(f\\"Error during compilation: {e}\\") return False"},{"question":"Coding Assessment Question # Problem Statement You are required to implement a custom layer in PyTorch that applies a causal mask to the input tensor. This layer will be used in an NLP model to ensure that for any sequence, each token only attends to previous tokens and itself, mimicking the behavior of causal attention. # Requirements 1. **Implement a PyTorch module, `CausalMaskLayer`, with the following methods**: - `__init__(self)`: Initializes the layer. - `forward(self, x)`: Applies the causal mask to the input tensor `x`. 2. **The Causal Mask**: - For a sequence tensor `x` of shape `(batch_size, sequence_length, feature_dim)`, the mask should ensure that `x[i, j, :]` can only attend to `x[i, :j+1, :]` (i.e., it should not attend to future tokens in the sequence). - You can leverage the `causal_lower_right` function from `torch.nn.attention.bias` to create the mask. # Input - The input tensor `x` will have dimensions `(batch_size, sequence_length, feature_dim)`. # Output - The output tensor should be of the same shape as the input tensor `(batch_size, sequence_length, feature_dim)`, but with the applied causal mask. # Constraints - The sequence length will not exceed 512. - The feature dimension will not exceed 1024. - Ensure the implementation is efficient in terms of both time and space complexity. # Example ```python import torch from torch.nn.attention.bias import causal_lower_right from torch import nn class CausalMaskLayer(nn.Module): def __init__(self): super(CausalMaskLayer, self).__init__() def forward(self, x): batch_size, sequence_length, feature_dim = x.size() causal_mask = causal_lower_right(sequence_length) # Generate the causal mask causal_mask = causal_mask.to(x.device) # Ensure the mask is on the same device as the input masked_x = x.masked_fill(causal_mask == 0, float(\'-inf\')) # Apply the mask return masked_x # Example usage batch_size = 2 sequence_length = 4 feature_dim = 3 x = torch.randn(batch_size, sequence_length, feature_dim) layer = CausalMaskLayer() output = layer(x) print(output) ``` In this example, `output` should have the same shape as `x`, but with future tokens masked to ensure causality. # Note - Students should not assume the presence of any additional libraries other than standard PyTorch and the specified module imports. - Clearly document your code to explain the steps involved in generating and applying the causal mask.","solution":"import torch import torch.nn as nn class CausalMaskLayer(nn.Module): def __init__(self): super(CausalMaskLayer, self).__init__() def forward(self, x): Applies a casual mask to the input tensor x. Parameters: x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, feature_dim) Returns: torch.Tensor: Masked tensor of the same shape as the input batch_size, sequence_length, feature_dim = x.size() # Create the causal mask based on sequence_length causal_mask = torch.tril(torch.ones((sequence_length, sequence_length))).to(x.device) # Expand mask dimensions to (batch_size, sequence_length, sequence_length) causal_mask = causal_mask.unsqueeze(0).expand(batch_size, -1, -1) # Apply mask: extend mask to fit feature_dim, i.e., (batch_size, sequence_length, sequence_length, feature_dim) causal_mask = causal_mask.unsqueeze(-1).expand(-1, -1, -1, feature_dim) # Expand input tensor to apply the mask x = x.unsqueeze(2).expand(-1, -1, sequence_length, -1) # Apply the Causal Mask masked_x = x.masked_fill(causal_mask == 0, float(\'-inf\')) # Restore the original dimensions return masked_x # Example usage if __name__ == \\"__main__\\": batch_size = 2 sequence_length = 4 feature_dim = 3 x = torch.randn(batch_size, sequence_length, feature_dim) layer = CausalMaskLayer() output = layer(x) print(output)"},{"question":"You are tasked with implementing a kernel approximation method using scikit-learn and testing its performance on a classification problem. # Requirements: 1. **Function Implementation**: Implement a function `rbf_kernel_approximation` that approximates the RBF kernel using the `RBFSampler` and fits a linear classifier on the transformed data. 2. **Input and Output Formats**: - Input: ```python X_train: List[List[float]] - A two-dimensional list representing the training data. y_train: List[int] - A list of integers representing the target labels for the training data. X_validation: List[List[float]] - A two-dimensional list representing the validation data. y_validation: List[int] - A list of integers representing the target labels for the validation data. n_components: int - Number of components for the RBFSampler. gamma: float - The gamma parameter for the RBF kernel. ``` - Output: ```python accuracy: float - The accuracy of the classifier on the validation data. ``` 3. **Function Details**: ```python def rbf_kernel_approximation(X_train: List[List[float]], y_train: List[int], X_validation: List[List[float]], y_validation: List[int], n_components: int, gamma: float) -> float: Approximate the RBF kernel using RBFSampler and fit a linear classifier. Parameters: X_train (List[List[float]]): Training data y_train (List[int]): Training labels X_validation (List[List[float]]): Validation data y_validation (List[int]): Validation labels n_components (int): Number of components for the RBFSampler gamma (float): Parameter for the RBF kernel Returns: float: Accuracy of the classifier on validation data # Implementation here ``` 4. **Constraints**: - Use scikit-learn\'s `RBFSampler` for the kernel approximation. - Use `SGDClassifier` for fitting the classifier on the transformed data. - Ensure to randomize the process using a fixed `random_state`. 5. **Performance Requirements**: - The function should handle large datasets efficiently. - The approximation should be accurate enough to achieve reasonable performance compared to the exact kernel method. # Example: ```python from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def rbf_kernel_approximation(X_train, y_train, X_validation, y_validation, n_components, gamma): rbf_feature = RBFSampler(gamma=gamma, n_components=n_components, random_state=1) X_features = rbf_feature.fit_transform(X_train) clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=1) clf.fit(X_features, y_train) X_validation_features = rbf_feature.transform(X_validation) y_pred = clf.predict(X_validation_features) return accuracy_score(y_validation, y_pred) ``` Test the function with a sample dataset and report the obtained accuracy.","solution":"from typing import List from sklearn.kernel_approximation import RBFSampler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def rbf_kernel_approximation(X_train: List[List[float]], y_train: List[int], X_validation: List[List[float]], y_validation: List[int], n_components: int, gamma: float) -> float: Approximate the RBF kernel using RBFSampler and fit a linear classifier. Parameters: X_train (List[List[float]]): Training data y_train (List[int]): Training labels X_validation (List[List[float]]): Validation data y_validation (List[int]): Validation labels n_components (int): Number of components for the RBFSampler gamma (float): Parameter for the RBF kernel Returns: float: Accuracy of the classifier on validation data rbf_feature = RBFSampler(gamma=gamma, n_components=n_components, random_state=1) X_train_features = rbf_feature.fit_transform(X_train) clf = SGDClassifier(max_iter=1000, tol=1e-3, random_state=1) clf.fit(X_train_features, y_train) X_validation_features = rbf_feature.transform(X_validation) y_pred = clf.predict(X_validation_features) return accuracy_score(y_validation, y_pred)"},{"question":"Seaborn Visualization and Legend Customization Objective You are tasked with creating a statistical visualization using the seaborn library, showcasing your ability to manage plot aesthetics and legend customization as demonstrated in the provided documentation. Problem Statement Given a dataset, create a scatter plot with customized legends for better visualization. Specifically, you should load the `tips` dataset, which is available in seaborn, and create a scatter plot showing the relationship between total bill and tip amount, differentiated by day. Then, customize the plot by moving the legend to a suitable position outside of the axes. Requirements 1. **Import Seaborn and Set Theme**: - Import the seaborn library and set an appropriate theme for the plots. 2. **Data Loading**: - Load the `tips` dataset. 3. **Scatter Plot**: - Create a scatter plot with `total_bill` on the x-axis and `tip` on the y-axis. - Differentiate data points by `day` using different colors/hues. 4. **Legend Customization**: - Use `sns.move_legend` to place the legend outside the plot on the right. - Utilize `bbox_to_anchor` for placing the legend and ensure the plot aesthetics do not have excessive blank space. 5. **Additional Customizations** (Optional but encouraged for extra points): - Adjust the legend to have a title \\"Day of the Week\\". - Set the legend frame to be non-visible (`frameon=False`). Input and Output - **Input**: No input required; the dataset is loaded directly from seaborn. - **Output**: A scatter plot with the specified customizations. Constraints - Use only seaborn and matplotlib libraries for visualization. - Ensure the script runs without errors and the plot is clearly visible. ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Import Seaborn and Set the Theme sns.set_theme() # Step 2: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 3: Create a scatter plot ax = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\') # Step 4: Customize the Legend sns.move_legend(ax, \\"center left\\", bbox_to_anchor=(1, 0.5)) # Optional: Additional Customizations sns.move_legend(ax, \\"center left\\", bbox_to_anchor=(1, 0.5), title=\\"Day of the Week\\", frameon=False) # Show plot plt.show() ``` Submit your solution by providing the above code implementation modified to achieve the described requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_scatter_plot(): # Step 1: Import Seaborn and Set the Theme sns.set_theme() # Step 2: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 3: Create a scatter plot ax = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'day\') # Step 4: Customize the Legend legend = ax.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5), title=\\"Day of the Week\\", frameon=False) # Show plot plt.show()"},{"question":"# Question: Advanced URL Fetching and Data Submission with `urllib` You are tasked with writing a Python function that fetches data from a given URL, sends a `POST` request with specific data, handles HTTP errors appropriately, and returns the server\'s response. Your function should also include custom headers in the request, manage connectivity through a proxy, and set a global timeout for network connections. Function Signature ```python def fetch_and_post_data(url: str, proxy_url: str, data: dict, headers: dict, timeout: int) -> str: Fetches data from a given URL, sends a POST request with specified data, handles errors, includes custom headers, manages proxy settings, and sets a global timeout. Parameters: - url (str): The URL to send the POST request to. - proxy_url (str): The URL of the proxy server to use for the request. - data (dict): A dictionary containing the data to be sent in the POST request. - headers (dict): A dictionary containing custom headers to include in the request. - timeout (int): Timeout in seconds for the network connection. Returns: - str: The server\'s response as a string. # Your implementation here ``` Requirements 1. **POST Request with Data**: The function should send a `POST` request to the provided URL with the given data. 2. **Custom Headers**: Include the custom headers provided in the request. 3. **Proxy Management**: Use the specified proxy server for the connection. 4. **Global Timeout**: Set the given timeout globally for all network operations. 5. **Error Handling**: Properly handle `URLError` and `HTTPError`, and return a suitable error message if an exception occurs. 6. **Return Server Response**: If the request is successful, return the server\'s response as a string. Example ```python url = \\"http://www.example.com/api\\" proxy_url = \\"http://proxy.example.com:8080\\" data = {\\"name\\": \\"John Doe\\", \\"age\\": 30} headers = {\\"User-Agent\\": \\"CustomUserAgent/1.0\\", \\"Content-Type\\": \\"application/x-www-form-urlencoded\\"} timeout = 5 response = fetch_and_post_data(url, proxy_url, data, headers, timeout) print(response) ``` In this example, the function `fetch_and_post_data` should send a `POST` request to \\"http://www.example.com/api\\" with the data `{\\"name\\": \\"John Doe\\", \\"age\\": 30}`, using the proxy \\"http://proxy.example.com:8080\\", and the custom headers `{\\"User-Agent\\": \\"CustomUserAgent/1.0\\", \\"Content-Type\\": \\"application/x-www-form-urlencoded\\"}`. The function should set a global timeout of 5 seconds for the request and return the server\'s response as a string. Constraints - Ensure that all exceptions are handled and appropriate error messages are returned. - Use the `urllib` package as explained in the provided documentation. Good luck and happy coding!","solution":"import urllib.request import urllib.error import urllib.parse def fetch_and_post_data(url: str, proxy_url: str, data: dict, headers: dict, timeout: int) -> str: Fetches data from a given URL, sends a POST request with specified data, handles errors, includes custom headers, manages proxy settings, and sets a global timeout. Parameters: - url (str): The URL to send the POST request to. - proxy_url (str): The URL of the proxy server to use for the request. - data (dict): A dictionary containing the data to be sent in the POST request. - headers (dict): A dictionary containing custom headers to include in the request. - timeout (int): Timeout in seconds for the network connection. Returns: - str: The server\'s response as a string. # Set up the proxy proxy_handler = urllib.request.ProxyHandler({\'http\': proxy_url, \'https\': proxy_url}) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) # Encode data encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') # Create request with headers request = urllib.request.Request(url, data=encoded_data, headers=headers) try: # Open URL with timeout with urllib.request.urlopen(request, timeout=timeout) as response: result = response.read().decode(\'utf-8\') return result except urllib.error.HTTPError as e: return f\\"HTTPError: {e.code} {e.reason}\\" except urllib.error.URLError as e: return f\\"URLError: {e.reason}\\""},{"question":"Coding Assessment Question # Objective Create a program that uses the `inspect` module to analyze a given Python module and generate a detailed report about its functions and classes, including their signatures and docstrings. This will demonstrate your understanding of the `inspect` module and its capabilities in inspecting and retrieving information about live objects. # Background Given the enhanced capabilities of the `inspect` module, your task is to create a function that examines a Python module and provides detailed information about each function and class within it. The information should include the name of the function or class, its signature, and its docstring. # Task Implement a function `generate_module_report(module_path)` that accepts the file path to a Python module, inspects its contents, and returns a report as a string. The report should have the following format: ``` Module: <module_name> Functions: <function_name> Signature: <function_signature> Docstring: <function_docstring> Classes: <class_name> Signature: <class_signature> Docstring: <class_docstring> Methods: <method_name> Signature: <method_signature> Docstring: <method_docstring> ``` # Function Signature ```python def generate_module_report(module_path: str) -> str: pass ``` # Input - `module_path`: A string representing the file path to the Python module to be analyzed. # Output - A string containing the detailed report of the module. # Constraints - Assume the given module file exists and is a valid Python module. - The function should handle modules with multiple functions, multiple classes, and nested methods. - Use the `inspect` module to retrieve the required information. - The report should be clear and properly formatted as described. # Example For a module named `sample_module.py` containing: ```python Sample Module def sample_function(a, b=2): This is a sample function. return a + b class SampleClass: This is a sample class. def method_one(self, x): This is method one. return x def method_two(self, y: int) -> int: This is method two. return y * 2 ``` Running `generate_module_report(\'sample_module.py\')` should return: ``` Module: sample_module Functions: sample_function Signature: (a, b=2) Docstring: This is a sample function. Classes: SampleClass Signature: () Docstring: This is a sample class. Methods: method_one Signature: (self, x) Docstring: This is method one. method_two Signature: (self, y: int) -> int Docstring: This is method two. ``` # Notes - Ensure proper handling of imports and the module path. - You may assume that the module to be inspected has no syntax errors and can be safely imported. - Your implementation should be robust enough to handle various docstring formats and optional presence of docstrings.","solution":"import inspect import os import importlib.util def generate_module_report(module_path: str) -> str: # Load the module from the given path module_name = os.path.splitext(os.path.basename(module_path))[0] spec = importlib.util.spec_from_file_location(module_name, module_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) report = f\\"Module: {module_name}nn\\" functions = [obj for name, obj in inspect.getmembers(module, inspect.isfunction)] classes = [obj for name, obj in inspect.getmembers(module, inspect.isclass)] if functions: report += \\"Functions:n\\" for func in functions: report += f\\"{func.__name__}n\\" report += f\\" Signature: {inspect.signature(func)}n\\" docstring = inspect.getdoc(func) or \\"\\" report += f\\" Docstring:n {docstring}nn\\" if classes: report += \\"Classes:n\\" for klass in classes: report += f\\"{klass.__name__}n\\" report += f\\" Signature: {inspect.signature(klass)}n\\" docstring = inspect.getdoc(klass) or \\"\\" report += f\\" Docstring:n {docstring}n\\" methods = [m for m in inspect.getmembers(klass, predicate=inspect.isfunction) if m[0].startswith(\\"__\\") is False] if methods: report += \\" Methods:n\\" for method_name, method in methods: report += f\\" {method_name}n\\" report += f\\" Signature: {inspect.signature(method)}n\\" docstring = inspect.getdoc(method) or \\"\\" report += f\\" Docstring:n {docstring}n\\" report += \'n\' return report.strip() # Remove the trailing newline"},{"question":"# Python Coding Assessment Objective: To assess your understanding of Python expressions, including generators, comprehensions, and advanced arithmetic operations. Problem Statement: You are given a list of integers. You need to perform the following tasks in one function: 1. **Generate a list of squares** of all even numbers from the given list using a list comprehension. 2. **Create a generator** that yields the factorial of each number from the list of squares obtained from step 1. 3. **Calculate the sum** of the generated factorials using a generator expression. Implement the following function: ```python def process_numbers(numbers): Process the given list of numbers based on the tasks specified. :param numbers: List[int] - A list of integers. :return: int - Sum of the factorials of squares of even numbers. pass ``` Constraints: - The input list `numbers` will have at least 1 and at most 100 integers. - Each integer will be in the range 1 <= x <= 100. Example: ```python input_list = [1, 2, 3, 4, 5] result = process_numbers(input_list) print(result) # Output ``` Explanation: 1. The even numbers are `[2, 4]`. 2. Their squares are `[4, 16]`. 3. The factorial of 4 is `24`, and the factorial of 16 is `20922789888000`. 4. The sum of these factorials is `20922789888024`. Notes: - You need to demonstrate the use of list comprehensions, generators, and generator expressions. - Do not use any external libraries except for `math` where necessary. Hints: - You can use the `math.factorial` function to calculate the factorial of a number. - Ensure your function handles edge cases such as when there are no even numbers in the list.","solution":"import math def process_numbers(numbers): Process the given list of numbers based on the tasks specified. :param numbers: List[int] - A list of integers. :return: int - Sum of the factorials of squares of even numbers. # Step 1: Generate a list of squares of all even numbers using a list comprehension even_squares = [x ** 2 for x in numbers if x % 2 == 0] # Step 2: Create a generator that yields the factorial of each number from the list of squares factorials_generator = (math.factorial(sq) for sq in even_squares) # Step 3: Calculate the sum of the generated factorials using a generator expression return sum(factorials_generator)"},{"question":"# Plot Customization with Seaborn You are given a dataset `data` containing two lists `x` and `y`, representing the x-coordinates and y-coordinates of data points respectively. Your task is to use the seaborn library to create a plot with various customized features as specified below. Requirements: 1. Create a line plot for the given data points with circular markers at each data point. 2. Set the x-axis limit from 0 to 10 and y-axis limit from -5 to 15. 3. Invert the x-axis. 4. Set the y-axis limit to range from 0 to the maximum y-value plus 5, keeping the default minimum value. # Input: - `x` : List of integers or floats representing x-coordinates. - `y` : List of integers or floats representing y-coordinates. # Output: - The function should display the customized plot using the seaborn library. # Function Signature: ```python def customize_plot(x: list, y: list) -> None: pass ``` # Constraints: - The lengths of `x` and `y` will be the same and will have at least 3 elements. # Example: ```python x = [1, 3, 7, 5] y = [2, 4, 6, 3] customize_plot(x, y) ``` Expected output: - A line plot with circular markers at coordinates (1, 2), (3, 4), (7, 6), and (5, 3). - X-axis limits should be from 10 to 0 (inverted). - Y-axis limits should be from minimum default to 11 (the maximum y-value 6 plus 5). # Note: Make sure to use seaborn functionality as described in the provided documentation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot(x: list, y: list) -> None: Creates a customized line plot using seaborn with specified requirements. Args: - x (list of int/float): List of x-coordinates. - y (list of int/float): List of y-coordinates. # Create a seaborn line plot with specified markers sns.lineplot(x=x, y=y, marker=\'o\') # Set x-axis and y-axis limits plt.xlim(0, 10) plt.ylim(0, max(y) + 5) # Invert the x-axis plt.gca().invert_xaxis() # Display the plot plt.show()"},{"question":"Objective: Demonstrate your understanding of asyncio synchronization primitives by solving a problem that coordinates multiple tasks using `Lock`, `Event`, and `Semaphore`. Problem Statement: You are required to implement a task orchestrator that schedules the execution of multiple tasks with specific constraints, such as exclusive access to shared resources and waiting for certain conditions to be met. # Task Details: 1. **Shared Resource Simulation**: - You will have a shared list `result` that each task will append to when they finish their execution. 2. **Task Execution**: - Implement tasks that perform work asynchronously. Each task should wait for an `Event` to be set before it starts processing. - Simulate I/O-bound work by using `asyncio.sleep()`. 3. **Synchronization Handling**: - Use an `Event` to signal when the orchestrator starts. - Use a `Semaphore` to ensure that no more than 3 tasks are running concurrently. - Ensure exclusive access to the shared `result` list using a `Lock` to avoid race conditions. Input and Output: - **Input**: - A list of integers where each integer represents the time (in seconds) each task will take to simulate its work. - **Output**: - The final state of the `result` list after all tasks have been executed and appended their completion message. # Constraints: - Each task appends the string `Task {task_number} finished` to the `result` list after completing its work. # Implementation Requirements: - Define an asynchronous function `task_manager` that accepts a list of integers. - Utilize `asyncio.Lock`, `asyncio.Event`, and `asyncio.Semaphore` as described. - Ensure that your solution can handle varying numbers of tasks with specified constraints and is free of race conditions. # Example Usage: ```python import asyncio async def task_manager(task_times): # IMPLEMENT YOUR CODE HERE # Example input task_times = [2, 3, 1, 4, 2] # Expected output format (order of completion may vary due to async nature): # [\'Task 0 finished\', \'Task 2 finished\', \'Task 1 finished\', \'Task 4 finished\', \'Task 3 finished\'] # Run the task manager asyncio.run(task_manager(task_times)) ``` **Note**: The specific order of tasks in the output may vary due to the asynchronous execution nature, but it should reflect no more than 3 concurrent tasks and proper synchronization at all times.","solution":"import asyncio async def task(worker_id, duration, start_event, semaphore, lock, result): await start_event.wait() # Wait until the orchestrator allows to start async with semaphore: # Ensure no more than 3 concurrent tasks await asyncio.sleep(duration) # Simulate work async with lock: # Ensure exclusive access to the shared resource result.append(f\\"Task {worker_id} finished\\") async def task_manager(task_times): start_event = asyncio.Event() semaphore = asyncio.Semaphore(3) lock = asyncio.Lock() result = [] tasks = [ task(i, duration, start_event, semaphore, lock, result) for i, duration in enumerate(task_times) ] # Signal the start of task processing start_event.set() # Wait for all tasks to complete await asyncio.gather(*tasks) return result"},{"question":"You are provided with multiple documents focusing on different aspects of using PyTorch for deep learning tasks, with this specific document focusing on SIMD instruction sets and how to query them on the current machine using a script. # Objective: Implement a program that: 1. Checks the SIMD capabilities of the current machine using the provided script. 2. Creates a small neural network using PyTorch. 3. Ensures the neural network utilizes the SIMD instruction sets identified on the machine. # Requirements: - Your solution should be in Python. - You need to use the `collect_env.py` script to check for available SIMD instructions. - Implement a simple neural network and print its architecture. - Ensure the operations within your network take advantage of the identified SIMD instructions where applicable. # Input: - No input from the user is required. # Output: - Print the list of available SIMD instruction sets on the current machine. - Print the architecture of the simple neural network. # Constraints: - Use PyTorch for neural network implementation. # Example Output: ``` SIMD Instruction Sets available: [\'avx512f\', \'avx512bw\', \'amx_int8\'] Neural Network Architecture: Sequential( (0): Linear(in_features=10, out_features=5, bias=True) (1): ReLU() (2): Linear(in_features=5, out_features=2, bias=True) ) ``` Note: Execute the provided script (`collect_env.py`) to determine SIMD capabilities. This part of the task assumes you have access to the environment where the script can be run.","solution":"import subprocess import torch import torch.nn as nn def check_simd(): try: result = subprocess.run([\'python3\', \'collect_env.py\'], capture_output=True, text=True) simd_info = result.stdout # Here you should parse the output of the collect_env.py script to extract SIMD information # For the sake of this example, we assume it prints it in a json-like format simd_info_list = simd_info.split() return simd_info_list except Exception as e: return str(e) def create_neural_network(): model = nn.Sequential( nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2) ) return model def main(): simd_capabilities = check_simd() print(f\\"SIMD Instruction Sets available: {simd_capabilities}\\") neural_network = create_neural_network() print(f\\"Neural Network Architecture:n{neural_network}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Coding Assessment: PyTorch Model Tracing using `torch.export` **Objective**: The goal of this coding assessment is to evaluate your understanding of the `torch.export` function in PyTorch, emphasizing the concepts of static vs. dynamic values, dynamic shapes, and handling control flows within an exported model. **Problem Statement**: You are required to define a custom PyTorch model and use `torch.export.export` to trace this model, considering both static and dynamic values, and handle control flows. Your implementation will be evaluated for correct handling of dynamic shapes, data-dependent control flows, and module states. **Instructions**: 1. **Model Definition**: - Define a class `CustomModel` that inherits from `torch.nn.Module`. - Implement the `forward` method to: - Add 5 to an input tensor if its sum is greater than 10. - Multiply the input tensor by 2 otherwise. - Ensure to handle both tensor inputs and static primitive values within the model. 2. **Dynamic Shape Handling**: - Specify input tensors with a dynamic shape. 3. **Trace Model using `torch.export.export`**: - Trace the model with an example input. - Export the computational graph handling dynamic shapes and control flow conditions correctly. - Print the code of the exported graph module. **Constraints**: - Use non-strict tracing mode. - Handle control flow using `torch.cond` for dynamic shape-dependent branches. - Assume all values not specified as dynamic are static. **Expected Input**: - Example tensor input with specific values to demonstrate dynamic behavior. **Expected Output**: - Printed code of the exported graph module. # Example Code Skeleton ```python import torch from torch import nn from torch.fx.experimental import export class CustomModel(nn.Module): def forward(self, x): # Add your implementation here pass # Define the main function to trace and export the model def main(): # Create an example input tensor example_input = torch.randn((4, 5)) # Example tensor with shape (4, 5) # Define the model model = CustomModel() # Trace and export the model exported_model = torch.export.export(model, example_input, mode=\'non-strict\') # Print the code of the exported model print(exported_model.graph_module.code) if __name__ == \\"__main__\\": main() ``` # Deliverables: - Implement the `CustomModel`\'s `forward` method. - Ensure dynamic shape handling in the example input and the export process. - Submit the complete code and output after printing the exported graph module.","solution":"import torch from torch import nn from torch.export import export class CustomModel(nn.Module): def forward(self, x): if torch.sum(x) > 10: return x + 5 else: return x * 2 # Define the main function to trace and export the model def main(): # Create an example input tensor example_input = torch.randn((4, 5)) # Example tensor with shape (4, 5) # Define the model model = CustomModel() # Trace and export the model exported_model = export(model, example_input, mode=\'default\') # Print the code of the exported model print(exported_model.graph_module.code) if __name__ == \\"__main__\\": main()"},{"question":"# Python Socket Programming Assessment Objective: You are tasked with writing a Python program that demonstrates your understanding of socket programming, including both blocking and non-blocking sockets, as well as handling multiple client connections concurrently. Problem Statement: Write a Python program that implements a simple chat server. The server should be able to handle multiple clients and echo back any messages sent by a client to all connected clients. The server should use non-blocking sockets and the `select` method to manage multiple client connections efficiently. Requirements: 1. **Server Implementation**: - Create a non-blocking TCP server socket that listens on any available IP address and port `12345`. - Accept multiple client connections. - Use the `select` method to manage socket reads and writes without blocking the server. - Echo any message received from a client to all connected clients, including the sender. - Properly handle client disconnection. 2. **Client Implementation** (for testing): - Create a blocking TCP client socket. - Connect to the server. - Allow the user to send messages from the client to the server. - Display any messages received from the server. Server Specifications: - **Input**: Server does not take any input during runtime. - **Output**: All client messages received by the server should be echoed back to all connected clients, including the sender. Client Specifications: - **Input**: User can type messages to be sent to the server. - **Output**: Display any messages received from the server. Constraints: - Use Python\'s built-in `socket` library. - Follow proper socket programming practices, including resource cleanup. - Ensure the server can handle at least 5 simultaneous clients without noticeable delay. Example: ```python # Example of starting the server python chat_server.py # Example of starting a client python chat_client.py Enter message: Hello, Server! Received: Hello, Server! ``` Notes: 1. This is a simplified chat server and does not require user authentication or handling of complex message formats. 2. Focus on correctly utilizing non-blocking sockets and the `select` method. 3. Ensure graceful handling of client disconnection, so one client\'s exit does not affect the remaining clients. Submission: Submit two Python scripts: 1. `chat_server.py` - Implementing the server functionality. 2. `chat_client.py` - Implementing the client functionality for testing the server.","solution":"import socket import select def chat_server(): Chat server using non-blocking sockets and select for handling multiple clients. server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((\\"0.0.0.0\\", 12345)) server_socket.listen() server_socket.setblocking(0) sockets = [server_socket] clients = {} print(\\"Chat server started on port 12345\\") while True: read_sockets, _, exception_sockets = select.select(sockets, [], sockets) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(0) sockets.append(client_socket) clients[client_socket] = client_address print(f\\"New connection from {client_address}\\") else: try: message = notified_socket.recv(1024) if not message: disconnect_socket(notified_socket, sockets, clients) continue broadcast_message(notified_socket, message, sockets) except: disconnect_socket(notified_socket, sockets, clients) for notified_socket in exception_sockets: disconnect_socket(notified_socket, sockets, clients) def broadcast_message(sender_socket, message, sockets): Broadcast a message to all clients :param sender_socket: The socket that sent the message :param message: The message to broadcast :param sockets: List of all connected sockets for socket in sockets: if socket != sender_socket and socket not in [socket for socket in sockets if socket.fileno() == -1]: socket.send(message) def disconnect_socket(socket, sockets, clients): Handle a client disconnecting :param socket: The socket of the client to disconnect :param sockets: List of all connected sockets :param clients: Dictionary of clients sockets.remove(socket) del clients[socket] socket.close() if __name__ == \\"__main__\\": chat_server()"},{"question":"Given the following extension documentation, design a custom `ExtensionArray` in pandas that supports geometric points in a 2D space (x, y coordinates). You are required to provide methods to perform the following operations on the array: - **Translate**: Move all points by a given (dx, dy). - **Rotate**: Rotate all points around the origin by a given angle in degrees. - **Scale**: Scale all points by a given factor. # Input: You should start by implementing a custom dtype and array class for 2D points. Example input for instantiating the array: ```python data = [(1, 2), (3, 4), (5, 6)] ``` # Output: The output should be a pandas `ExtensionArray` that correctly performs the translate, rotate, and scale transformations. # Constraints: - You cannot use external libraries like `numpy` or `scipy`; only the standard library and `pandas` are allowed. - Ensure the operations are efficient and handle edge cases appropriately. - Include docstrings and type annotations for clarity. # Example: ```python # Example usage geo_points = GeometricPointsArray([(0, 0), (1, 1), (2, 2)]) geo_points.translate(dx=1, dy=1) print(geo_points) # Expected output: [(1, 1), (2, 2), (3, 3)] geo_points.rotate(angle=90) print(geo_points) # Expected output: [(-1, 1), (-2, 2), (-3, 3)] geo_points.scale(factor=2) print(geo_points) # Expected output: [(-2, 2), (-4, 4), (-6, 6)] ``` Define the classes and methods to achieve the above functionality, ensuring that your custom `ExtensionArray` integrates seamlessly with pandas.","solution":"import pandas as pd import math from pandas.api.extensions import ExtensionArray, ExtensionDtype class GeometricPointsDtype(ExtensionDtype): name = \'geometric_point\' type = tuple kind = \'O\' na_value = (None, None) @classmethod def construct_array_type(cls): return GeometricPointsArray class GeometricPointsArray(ExtensionArray): def __init__(self, data): self._data = list(data) @property def dtype(self): return GeometricPointsDtype() def __len__(self): return len(self._data) def __getitem__(self, item): if isinstance(item, int): return self._data[item] else: return GeometricPointsArray(self._data[item]) def __setitem__(self, key, value): self._data[key] = value def __eq__(self, other): if isinstance(other, GeometricPointsArray): return self._data == other._data return False def translate(self, dx, dy): self._data = [(x + dx, y + dy) for x, y in self._data] def rotate(self, angle): radians = math.radians(angle) cos_angle = math.cos(radians) sin_angle = math.sin(radians) self._data = [ (x * cos_angle - y * sin_angle, x * sin_angle + y * cos_angle) for x, y in self._data ] def scale(self, factor): self._data = [(x * factor, y * factor) for x, y in self._data]"},{"question":"Objective Design a Python class that simulates the behavior of Python\'s internal cell objects. Background Cell objects are used to handle variables that are referenced by multiple scopes. For this assignment, you are required to implement a class in Python that mimics certain behaviors of a cell object as described in the provided documentation. Requirements 1. Implement a class `PyCell` with the following methods: - `__init__(self, value)`: Initializes the cell with the given value, which could be `None`. - `get(self)`: Returns the current value stored in the cell. - `set(self, value)`: Sets a new value to the cell, overwriting any current value. - `check(self)`: Returns `True` if the object is a `PyCell` instance, else returns `False`. 2. Write a function `simulate_cell_operations()` that: - Creates a new `PyCell` object with an initial value. - Checks if the object created is a `PyCell`. - Gets the value stored in the cell. - Sets a new value to the cell. - Gets the updated value. Function Signature ```python class PyCell: def __init__(self, value): pass def get(self): pass def set(self, value): pass def check(self): pass def simulate_cell_operations(): # Create a PyCell object with an initial value cell = PyCell(initial_value) # Check if the object is a PyCell instance is_cell = cell.check() # Get the value in the cell value = cell.get() # Set a new value to the cell cell.set(new_value) # Get the updated value updated_value = cell.get() # Return the results as a tuple return (is_cell, value, updated_value) # The exact inputs will be provided in the test cases. ``` # Constraints: - Do not use any in-built Python `cell` or internal objects directly. - You can assume the values stored in `PyCell` are always of basic types: int, str, float, or None. - You must strictly follow the class as defined above. # Example Usage: ```python cell = PyCell(10) assert cell.get() == 10 cell.set(20) assert cell.get() == 20 assert cell.check() == True ``` The `simulate_cell_operations()` function will be tested separately with various initial values and new values to ensure the class behaves as expected.","solution":"class PyCell: def __init__(self, value): self.value = value def get(self): return self.value def set(self, value): self.value = value def check(self): return isinstance(self, PyCell) def simulate_cell_operations(initial_value, new_value): # Create a PyCell object with an initial value cell = PyCell(initial_value) # Check if the object is a PyCell instance is_cell = cell.check() # Get the value in the cell value = cell.get() # Set a new value to the cell cell.set(new_value) # Get the updated value updated_value = cell.get() # Return the results as a tuple return (is_cell, value, updated_value)"},{"question":"**Question: Analyzing and Visualizing Sales Data** **Objective:** Design a solution using pandas to analyze and visualize sales data. **Problem Statement:** You are provided with a CSV file named `sales_data.csv` which contains the sales data of a company over a year. The file contains the following columns: - `Date`: Date of the sale (in YYYY-MM-DD format) - `Product`: The product sold - `Category`: The category of the product - `Quantity`: Quantity of the product sold - `Price`: Price per unit of the product Using this data, you need to perform the following tasks: 1. **Data Loading and Preprocessing:** - Load the data from `sales_data.csv` into a pandas DataFrame. - Ensure that the `Date` column is in datetime format. - Handle any missing or NaN values appropriately. 2. **Analysis:** - Compute the total sales for each product. - Compute the total sales for each category. - Find the top 5 products with the highest sales. 3. **Visualization:** - Plot the total sales over time using a line chart. - Create a bar chart to visualize the total sales of the top 5 products. - Create a pie chart to show the proportion of total sales per category. 4. **Advanced Visualization:** - Generate a scatter plot to visualize the relationship between Quantity and Price for each sale. - Create a box plot to show the distribution of sales prices for each product category. **Input:** - A CSV file named `sales_data.csv`. **Output:** - Print the total sales for each product. - Print the total sales for each category. - Print the top 5 products with the highest sales. - Line chart of the total sales over time. - Bar chart of the top 5 products with the highest sales. - Pie chart showing the proportion of total sales per category. - Scatter plot of Quantity vs. Price. - Box plot of sales prices for each product category. **Constraints:** - The datetime format in the `Date` column is consistent and correct. - Handle any missing data by dropping rows/columns with NaN values. **Code Requirements:** Implement the required functionality in a function named `analyze_and_visualize_sales_data(filepath: str) -> None`. You can use the following boilerplate: ```python import pandas as pd import matplotlib.pyplot as plt def analyze_and_visualize_sales_data(filepath: str) -> None: # Load data df = pd.read_csv(filepath) # Preprocessing df[\'Date\'] = pd.to_datetime(df[\'Date\']) df = df.dropna() # Analysis total_sales_per_product = df.groupby(\'Product\')[\'Quantity\'].sum() * df.groupby(\'Product\')[\'Price\'].mean() total_sales_per_category = df.groupby(\'Category\')[\'Quantity\'].sum() * df.groupby(\'Category\')[\'Price\'].mean() top_5_products = total_sales_per_product.sort_values(ascending=False).head(5) # Print analysis print(\\"Total sales per product:n\\", total_sales_per_product) print(\\"Total sales per category:n\\", total_sales_per_category) print(\\"Top 5 products with the highest sales:n\\", top_5_products) # Visualization # Total sales over time df[\'Total_Sales\'] = df[\'Quantity\'] * df[\'Price\'] df.set_index(\'Date\', inplace=True) df[\'Total_Sales\'].resample(\'M\').sum().plot(title=\'Total Sales Over Time\') plt.show() # Bar chart for top 5 products top_5_products.plot(kind=\'bar\', title=\'Top 5 Products by Total Sales\') plt.show() # Pie chart of sales per category total_sales_per_category.plot(kind=\'pie\', title=\'Total Sales per Category\', autopct=\'%1.0f%%\') plt.show() # Scatter plot (Quantity vs Price) df.plot.scatter(x=\'Quantity\', y=\'Price\', title=\'Quantity vs Price\') plt.show() # Box plot of sales prices by category df.boxplot(column=\'Price\', by=\'Category\') plt.title(\'Box Plot of Sales Prices by Category\') plt.suptitle(\'\') # Suppress default title to keep it clean plt.show() ``` Test your implementation thoroughly. **Note:** - Ensure that the plots are well-labeled and clean. - You can install pandas and matplotlib if you haven\'t already: `pip install pandas matplotlib`.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_and_visualize_sales_data(filepath: str) -> None: # Load data df = pd.read_csv(filepath) # Preprocessing df[\'Date\'] = pd.to_datetime(df[\'Date\']) df = df.dropna() # Analysis df[\'Total_Sales\'] = df[\'Quantity\'] * df[\'Price\'] total_sales_per_product = df.groupby(\'Product\')[\'Total_Sales\'].sum() total_sales_per_category = df.groupby(\'Category\')[\'Total_Sales\'].sum() top_5_products = total_sales_per_product.sort_values(ascending=False).head(5) # Print analysis print(\\"Total sales per product:n\\", total_sales_per_product) print(\\"Total sales per category:n\\", total_sales_per_category) print(\\"Top 5 products with the highest sales:n\\", top_5_products) # Visualization # Total sales over time df.set_index(\'Date\', inplace=True) df[\'Total_Sales\'].resample(\'M\').sum().plot(title=\'Total Sales Over Time\', figsize=(10, 6)) plt.ylabel(\'Total Sales\') plt.xlabel(\'Date\') plt.show() # Bar chart for top 5 products top_5_products.plot(kind=\'bar\', title=\'Top 5 Products by Total Sales\', figsize=(10, 6)) plt.ylabel(\'Total Sales\') plt.xlabel(\'Product\') plt.show() # Pie chart of sales per category total_sales_per_category.plot(kind=\'pie\', title=\'Total Sales per Category\', figsize=(8, 8), autopct=\'%1.0f%%\') plt.ylabel(\'\') plt.show() # Scatter plot (Quantity vs Price) df.plot.scatter(x=\'Quantity\', y=\'Price\', title=\'Quantity vs Price\', figsize=(10, 6)) plt.ylabel(\'Price\') plt.xlabel(\'Quantity\') plt.show() # Box plot of sales prices by category df.boxplot(column=\'Price\', by=\'Category\', figsize=(10, 6)) plt.title(\'\') plt.suptitle(\'Box Plot of Sales Prices by Category\') plt.ylabel(\'Price\') plt.xlabel(\'Category\') plt.show()"},{"question":"**Objective:** Demonstrate your ability to use PyTorch\'s CUDA functionalities effectively, particularly focusing on device management, memory management, and stream-based asynchronous computations. **Background:** CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) model created by NVIDIA. PyTorch provides integration with CUDA to speed up tensor operations. # Task: Implement a function `tensor_operations_on_multiple_devices` that performs the following sequence of operations: 1. **Device and Memory Management:** - List all available CUDA devices and their properties. - Initialize the first available CUDA device and allocate memory for two tensors `A` and `B` (each of size n x n with random float values between 0 and 1) on this device. 2. **Stream-Based Operations:** - Create two separate CUDA streams. - In stream 1, compute the matrix multiplication of `A` and `B` (denoted as `C`). - In stream 2, compute the element-wise multiplication (Hadamard product) of `A` and `B` (denoted as `D`). 3. **Memory Synchronization and Result Retrieval:** - Synchronize the streams to ensure computations are complete. - Retrieve and return the results `C` and `D` to the CPU (host). **Constraints:** - Assume `torch` has been imported, and CUDA is available. - Tensor size `n` can be assumed to be a reasonably small integer (e.g., 100) for testing. - Handle exceptions gracefully, particularly for CUDA out-of-memory errors. **Performance Requirements:** - Ensure that the streams run asynchronously to demonstrate speed-up due to concurrent execution. # Function Signature: ```python def tensor_operations_on_multiple_devices(n: int): Perform matrix and element-wise multiplication using PyTorch CUDA streams. Args: n (int): Size of the square matrices A and B. Returns: C (torch.Tensor): Result of matrix multiplication. D (torch.Tensor): Result of element-wise multiplication. pass ``` # Example: ```python # Example function call C, D = tensor_operations_on_multiple_devices(100) # Check the shape of the result tensors print(C.shape) # Expected Output: torch.Size([100, 100]) print(D.shape) # Expected Output: torch.Size([100, 100]) ``` Note: For the above task, you will need to: - Utilize `torch.cuda.device`, `torch.cuda.memory` for device and memory management. - Use `torch.cuda.stream` for creating and managing CUDA streams. - Use synchronization functions like `stream.synchronize()` to ensure proper stream management.","solution":"import torch def tensor_operations_on_multiple_devices(n: int): Perform matrix and element-wise multiplication using PyTorch CUDA streams. Args: n (int): Size of the square matrices A and B. Returns: C (torch.Tensor): Result of matrix multiplication. D (torch.Tensor): Result of element-wise multiplication. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available on this system.\\") # List all available CUDA devices and their properties num_devices = torch.cuda.device_count() devices_properties = [torch.cuda.get_device_properties(i) for i in range(num_devices)] if num_devices == 0: raise RuntimeError(\\"No CUDA devices available.\\") # Initialize the first available CUDA device device = torch.device(\'cuda:0\') # Allocate memory for tensors A and B with random float values between 0 and 1 A = torch.rand((n, n), device=device, dtype=torch.float32) B = torch.rand((n, n), device=device, dtype=torch.float32) # Create two CUDA streams stream1 = torch.cuda.Stream() stream2 = torch.cuda.Stream() # Perform operations in respective streams with torch.cuda.stream(stream1): C = torch.matmul(A, B) with torch.cuda.stream(stream2): D = torch.mul(A, B) # Synchronize the streams to ensure computations are complete stream1.synchronize() stream2.synchronize() # Retrieve the results and return them to the CPU (host) C_cpu = C.cpu() D_cpu = D.cpu() return C_cpu, D_cpu"},{"question":"# Objective: To assess the studentsâ€™ competence in utilizing the `sklearn.datasets` package for loading, fetching, and manipulating datasets in a machine learning context. # Problem Statement: You are required to implement a function `prepare_datasets` that loads multiple datasets using the `sklearn.datasets` package and returns specific training and test splits based on the input parameters. # Function Signature: ```python def prepare_datasets(datasets_info: dict) -> dict: pass ``` # Input: - `datasets_info`: A dictionary where keys are dataset names (as strings), and values are dictionaries containing the following keys: - `\'load_function\'`: The function name (as a string) used to load the dataset from `sklearn.datasets`. - `\'params\'`: A dictionary of parameters to pass to the loading function. - `\'split_ratio\'`: A float value between 0 and 1 representing the proportion of the dataset to include in the train split. For example: ```python datasets_info = { \'iris\': { \'load_function\': \'load_iris\', \'params\': {}, \'split_ratio\': 0.8 }, \'digits\': { \'load_function\': \'load_digits\', \'params\': {}, \'split_ratio\': 0.75 } } ``` # Output: - A dictionary where keys are the dataset names and values are dictionaries containing the train and test splits with the following structure: - `\'train_data\'`: `n_train_samples` * `n_features` numpy array for training data. - `\'train_target\'`: Array of length `n_train_samples` containing the target values for the training data. - `\'test_data\'`: `n_test_samples` * `n_features` numpy array for test data. - `\'test_target\'`: Array of length `n_test_samples` containing the target values for the test data. # Constraints: - Assume all datasets provided by `datasets_info` are available in `sklearn.datasets`. - The `split_ratio` indicates the proportion of the dataset to include in the train split and should be respected as closely as possible. # Example: ```python from sklearn.datasets import load_iris from sklearn.datasets import load_digits def prepare_datasets(datasets_info): # implementation will go here pass datasets_info = { \'iris\': { \'load_function\': \'load_iris\', \'params\': {}, \'split_ratio\': 0.8 }, \'digits\': { \'load_function\': \'load_digits\', \'params\': {}, \'split_ratio\': 0.75 } } datasets_splits = prepare_datasets(datasets_info) # Expected Output: # { # \'iris\': { # \'train_data\': numpy array of shape (120, 4), # \'train_target\': numpy array of shape (120,), # \'test_data\': numpy array of shape (30, 4), # \'test_target\': numpy array of shape (30,) # }, # \'digits\': { # \'train_data\': numpy array of shape ~ (1347 * 0.75, 64), # \'train_target\': numpy array of shape ~ (1347 * 0.75,), # \'test_data\': numpy array of shape ~ (1347 * 0.25, 64), # \'test_target\': numpy array of shape ~ (1347 * 0.25,) # } # } ``` # Hints: - Use the `train_test_split` utility from `sklearn.model_selection` to easily split the dataset. - You might find the `getattr` function useful for dynamically calling the appropriate loading function from `sklearn.datasets`.","solution":"from sklearn.datasets import load_iris, load_digits from sklearn.model_selection import train_test_split import numpy as np def prepare_datasets(datasets_info: dict) -> dict: result = {} for dataset_name, info in datasets_info.items(): load_func = getattr(__import__(\'sklearn.datasets\', fromlist=[info[\'load_function\']]), info[\'load_function\']) dataset = load_func(**info[\'params\']) data, target = dataset.data, dataset.target train_size = info[\'split_ratio\'] X_train, X_test, y_train, y_test = train_test_split(data, target, train_size=train_size, random_state=42) result[dataset_name] = { \'train_data\': X_train, \'train_target\': y_train, \'test_data\': X_test, \'test_target\': y_test } return result"},{"question":"Problem Statement You are tasked with building a server monitoring application in Python that uses `select`, specifically the `poll()` function, to efficiently watch multiple file descriptors (FDs) for I/O events. # Objective Write a function `monitor_fds` that: 1. Registers a set of file descriptors with specific events using `poll()`. 2. Continuously monitors the registered file descriptors for the given events. 3. Handles the events appropriately (for the sake of this problem, simply print the event). # Requirements - Use the `poll()` function from the `select` module. - The function should correctly handle registering, modifying, and unregistering file descriptors. - Implement appropriate handling for timeout and errors. # Function Signature ```python def monitor_fds(fds_events: Dict[int, int], timeout: int = 1000) -> None: pass ``` # Input - `fds_events`: A dictionary where keys are file descriptors (integers) and values are event masks (integers) to monitor. The event masks should be combinations of constants like `POLLIN`, `POLLOUT`, `POLLERR`, etc. - `timeout`: An optional timeout value (in milliseconds) that specifies how long the poll should wait before returning. If not specified, defaults to 1000 milliseconds (1 second). # Output - The function doesn\'t return any value. It should print the file descriptor and the event when an event occurs. # Example Usage ```python fds_events = { fd1: select.POLLIN | select.POLLPRI, fd2: select.POLLOUT } monitor_fds(fds_events) ``` # Constraints - Assume `fds_events` dictionary contains valid file descriptors. - The function should handle a moderate to large number of file descriptors efficiently. - Ensure the function is robust and handles errors gracefully. # Advanced Requirements (Optional) - Implement logic to modify registered events dynamically if needed. - Provide functionality to unregister file descriptors. # Notes - You will need to use methods from the `select.poll()` object such as `register()`, `unregister()`, and `modify()` for handling file descriptors. - For polling, use the `poll.poll()` method with the provided timeout. - Error handling is crucial to ensure the function operates smoothly in real-world conditions. # Performance Requirements - The function should efficiently monitor and handle a moderate to large number of file descriptors. - Ensure minimal performance overhead when polling and handling events. --- Implement the `monitor_fds` function that adheres to the requirements and constraints described above.","solution":"import select def monitor_fds(fds_events, timeout=1000): Monitors multiple file descriptors for I/O events using select.poll(). Args: fds_events (dict): A dictionary where keys are file descriptors (ints) and values are event masks (ints) to monitor. timeout (int): Optional timeout value in milliseconds. Default is 1000 ms. Returns: None # Create a poll object poller = select.poll() # Register file descriptors with their corresponding events for fd, event in fds_events.items(): poller.register(fd, event) try: while True: # Poll for events events = poller.poll(timeout) # Handle the events for fd, event in events: if event & select.POLLIN: print(f\\"{fd} is ready for reading\\") if event & select.POLLOUT: print(f\\"{fd} is ready for writing\\") if event & select.POLLERR: print(f\\"{fd} has an error\\") if event & select.POLLHUP: print(f\\"{fd} has a hang up\\") poller.unregister(fd) if event & select.POLLPRI: print(f\\"{fd} has urgent data\\") except KeyboardInterrupt: print(\\"Monitoring interrupted by user\\") finally: # Unregister all file descriptors on exit for fd in fds_events.keys(): poller.unregister(fd)"},{"question":"# Hyper-Parameter Tuning with GridSearchCV and RandomizedSearchCV You are given a dataset containing features of different flowers and their corresponding species. Your task is to create a Support Vector Classifier (SVC) and optimize its hyper-parameters using both GridSearchCV and RandomizedSearchCV. Compare the results obtained from both methods. Instructions: 1. **Load the Dataset:** - Use the `load_iris` function from `sklearn.datasets` to load the Iris dataset. 2. **Preprocess the Data:** - Split the dataset into training and testing sets with a test size of 30% using `train_test_split` from `sklearn.model_selection`. 3. **Implement GridSearchCV:** - Define a parameter grid to search for the best parameters for the SVC model. Use the following parameter grid: ```python param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] ``` - Initialize a `GridSearchCV` object with the SVC estimator, the parameter grid, and a 5-fold cross-validation scheme. - Fit the GridSearchCV object on the training data. 4. **Implement RandomizedSearchCV:** - Define a parameter distribution to search for the best parameters for the SVC model. Use the following parameter distribution: ```python param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } ``` - Initialize a `RandomizedSearchCV` object with the SVC estimator, the parameter distribution, a 5-fold cross-validation scheme, and `n_iter` set to 50. - Fit the RandomizedSearchCV object on the training data. 5. **Compare Results:** - Print the best parameters and the best score obtained from both GridSearchCV and RandomizedSearchCV. - Evaluate the performance of the best models obtained from GridSearchCV and RandomizedSearchCV on the test set using accuracy score. Expected Functions and Output: - **Function:** `perform_grid_search` - **Input:** `X_train, y_train` (Features and labels of training set) - **Output:** `best_params, best_score` (Best parameters and the best score from GridSearchCV) - **Function:** `perform_randomized_search` - **Input:** `X_train, y_train` (Features and labels of training set) - **Output:** `best_params, best_score` (Best parameters and the best score from RandomizedSearchCV) - **Function:** `evaluate_model` - **Input:** `model, X_test, y_test` (Model, features, and labels of the test set) - **Output:** `accuracy` (Accuracy score of the model on the test set) Constraints: - Use `accuracy_score` from `sklearn.metrics` for evaluating the performance. - Ensure reproducible results by setting the `random_state` wherever applicable. Skeleton Code: ```python import numpy as np import scipy.stats from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.metrics import accuracy_score from sklearn.svm import SVC def load_and_split_data(): iris = datasets.load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42) return X_train, X_test, y_train, y_test def perform_grid_search(X_train, y_train): param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] grid_search = GridSearchCV(SVC(), param_grid, cv=5) grid_search.fit(X_train, y_train) return grid_search.best_params_, grid_search.best_score_ def perform_randomized_search(X_train, y_train): param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } rand_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=50, cv=5, random_state=42) rand_search.fit(X_train, y_train) return rand_search.best_params_, rand_search.best_score_ def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def main(): X_train, X_test, y_train, y_test = load_and_split_data() # Grid Search best_params_grid, best_score_grid = perform_grid_search(X_train, y_train) print(f\\"GridSearchCV Best Params: {best_params_grid}\\") print(f\\"GridSearchCV Best Score: {best_score_grid}\\") # Randomized Search best_params_rand, best_score_rand = perform_randomized_search(X_train, y_train) print(f\\"RandomizedSearchCV Best Params: {best_params_rand}\\") print(f\\"RandomizedSearchCV Best Score: {best_score_rand}\\") # Evaluate the best models on the test set best_model_grid = SVC(**best_params_grid) best_model_grid.fit(X_train, y_train) accuracy_grid = evaluate_model(best_model_grid, X_test, y_test) print(f\\"GridSearchCV Test Accuracy: {accuracy_grid}\\") best_model_rand = SVC(**best_params_rand) best_model_rand.fit(X_train, y_train) accuracy_rand = evaluate_model(best_model_rand, X_test, y_test) print(f\\"RandomizedSearchCV Test Accuracy: {accuracy_rand}\\") if __name__ == \\"__main__\\": main() ```","solution":"import numpy as np import scipy.stats from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.metrics import accuracy_score from sklearn.svm import SVC def load_and_split_data(): iris = datasets.load_iris() X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42) return X_train, X_test, y_train, y_test def perform_grid_search(X_train, y_train): param_grid = [ {\'C\': [1, 10, 100], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] grid_search = GridSearchCV(SVC(), param_grid, cv=5) grid_search.fit(X_train, y_train) return grid_search.best_params_, grid_search.best_score_ def perform_randomized_search(X_train, y_train): param_dist = { \'C\': scipy.stats.expon(scale=100), \'gamma\': scipy.stats.expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } rand_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=50, cv=5, random_state=42) rand_search.fit(X_train, y_train) return rand_search.best_params_, rand_search.best_score_ def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy def main(): X_train, X_test, y_train, y_test = load_and_split_data() # Grid Search best_params_grid, best_score_grid = perform_grid_search(X_train, y_train) print(f\\"GridSearchCV Best Params: {best_params_grid}\\") print(f\\"GridSearchCV Best Score: {best_score_grid}\\") # Randomized Search best_params_rand, best_score_rand = perform_randomized_search(X_train, y_train) print(f\\"RandomizedSearchCV Best Params: {best_params_rand}\\") print(f\\"RandomizedSearchCV Best Score: {best_score_rand}\\") # Evaluate the best models on the test set best_model_grid = SVC(**best_params_grid) best_model_grid.fit(X_train, y_train) accuracy_grid = evaluate_model(best_model_grid, X_test, y_test) print(f\\"GridSearchCV Test Accuracy: {accuracy_grid}\\") best_model_rand = SVC(**best_params_rand) best_model_rand.fit(X_train, y_train) accuracy_rand = evaluate_model(best_model_rand, X_test, y_test) print(f\\"RandomizedSearchCV Test Accuracy: {accuracy_rand}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: You are given a large bytearray of integers. Your task is to modify this bytearray efficiently using the `memoryview` capabilities in Python. Implement a function `modify_bytearray(arr: bytearray, start: int, end: int, new_value: int) -> None` that: - Takes a `bytearray` `arr` as input, along with two indices `start` and `end`, and an integer `new_value`. - Changes all elements in the slice from `start` to `end` (inclusive) in `arr` to `new_value` without creating a copy of the original array. Use `memoryview` to achieve this. Input: - `arr`: A `bytearray` containing integer values. - `start`: An integer, the start index of the slice to modify. - `end`: An integer, the end index of the slice to modify. - `new_value`: An integer, the new value to set for the specified slice. Output: - The function should modify the input `bytearray` in place and return `None`. Constraints: - `0 <= start <= end < len(arr)` - The `new_value` should fit within a single byte (0-255). Example: ```python arr = bytearray([1, 2, 3, 4, 5, 6, 7, 8, 9]) modify_bytearray(arr, 2, 5, 0) print(arr) # Output: bytearray(b\'x01x02x00x00x00x00x07x08x09\') ``` Notes: - Your implementation should efficiently handle the modification without copying the entire `bytearray`. - Utilize `memoryview` to perform the modifications.","solution":"def modify_bytearray(arr: bytearray, start: int, end: int, new_value: int) -> None: Modify the bytearray from start to end indices (inclusive) to have new_value using memoryview. mv = memoryview(arr)[start:end+1] mv[:] = bytearray([new_value] * (end - start + 1))"},{"question":"# Asynchronous Web Scraping and Prime Number Calculation Problem Description You are tasked with creating two functions that demonstrate the use of concurrency features in the `concurrent.futures` module: one for web scraping and one for prime number calculation. Part 1: Web Scraping Write a function `asynchronous_web_scraper(urls)` that takes a list of URLs and returns the page contents for each URL using `ThreadPoolExecutor`. - **Function Signature**: `def asynchronous_web_scraper(urls: List[str]) -> List[Tuple[str, Optional[bytes]]]:` - **Input**: `urls` (List[str]) - A list of URLs to scrape. - **Output**: List of tuples, where each tuple contains the URL and its page content as bytes (or `None` if there was an exception). Part 2: Prime Number Calculation Write a function `asynchronous_prime_calculator(numbers)` that takes a list of numbers and returns a list indicating whether each number is prime using `ProcessPoolExecutor`. - **Function Signature**: `def asynchronous_prime_calculator(numbers: List[int]) -> List[Tuple[int, bool]]:` - **Input**: `numbers` (List[int]) - A list of integers to check for primality. - **Output**: List of tuples, where each tuple contains the number and a boolean indicating whether it is a prime number. Constraints - You may assume that the URLs provided are well-formed, and network connections are available. - The range of numbers for primality testing is within typical integer bounds. - Implement proper exception handling to handle issues that may arise with web requests or processing. Performance Requirements - The web scraper should utilize threading to overlap I/O operations for efficiency. - The prime calculator should utilize multiprocessing to efficiently utilize multiple CPU cores. Example Usage ```python urls = [\'http://example.com\', \'http://example.org\'] numbers = [112272535095293, 115280095190773, 2, 15] # Part 1: Web Scraping page_contents = asynchronous_web_scraper(urls) for url, content in page_contents: if content: print(f\\"Content from {url} is {len(content)} bytes\\") else: print(f\\"Failed to retrieve content from {url}\\") # Part 2: Prime Calculator prime_results = asynchronous_prime_calculator(numbers) for number, is_prime in prime_results: print(f\\"{number} is {\'a prime\' if is_prime else \'not a prime\'} number\\") ``` Additional Notes - Use `urllib.request.urlopen` to fetch web pages. - Use the provided `is_prime` function for primality testing. - Ensure resources are managed properly by using context managers (`with` statement) to handle `ThreadPoolExecutor` and `ProcessPoolExecutor`. Happy coding!","solution":"import urllib.request from typing import List, Tuple, Optional from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor def asynchronous_web_scraper(urls: List[str]) -> List[Tuple[str, Optional[bytes]]]: def fetch(url: str) -> Tuple[str, Optional[bytes]]: try: with urllib.request.urlopen(url) as response: return url, response.read() except Exception as e: return url, None results = [] with ThreadPoolExecutor() as executor: futures = [executor.submit(fetch, url) for url in urls] for future in futures: results.append(future.result()) return results def is_prime(n: int) -> bool: if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True def asynchronous_prime_calculator(numbers: List[int]) -> List[Tuple[int, bool]]: with ProcessPoolExecutor() as executor: results = list(executor.map(is_prime, numbers)) return [(number, result) for number, result in zip(numbers, results)]"},{"question":"# Question: Utilizing HIP for GPU-Accelerated Computing in PyTorch You are provided with a dataset consisting of 1,000,000 samples where each sample is a vector of 10 floating-point numbers. Your task is to perform a set of operations on this dataset utilizing GPU acceleration with ROCm (HIP) in PyTorch. Specifically, you should: 1. Initialize a tensor with this dataset and place it on the default HIP device. 2. Create another tensor of the same dimensions on the HIP device and fill it with random numbers. 3. Perform element-wise addition of these two tensors and store the result in a third tensor. 4. Monitor and print the amount of GPU memory allocated and reserved before and after the tensor operations. 5. Check if HIP is available in the current PyTorch build and handle the computations accordingly. Input - A tensor `dataset` of shape `(1000000, 10)` containing the dataset to be placed on the HIP device. Output - Print the memory allocated before and after the tensor operations. - Return the resulting tensor after the element-wise addition. Constraints - Ensure that all operations utilize the default HIP device. - Handle cases where HIP is not available. Example ```python import torch # Provided dataset dataset = torch.randn(1000000, 10) def hip_tensor_operations(dataset): if not torch.cuda.is_available() or not torch.version.hip: raise EnvironmentError(\\"HIP is not available in the current PyTorch build\\") # Step 1: Place dataset on HIP device device = torch.device(\'cuda\') dataset = dataset.to(device) # Step 2: Create another tensor of same dimensions with random numbers on HIP device rand_tensor = torch.randn(1000000, 10, device=device) # Monitor memory before operations mem_allocated_before = torch.cuda.memory_allocated() mem_reserved_before = torch.cuda.memory_reserved() # Step 3: Perform element-wise addition result_tensor = dataset + rand_tensor # Monitor memory after operations mem_allocated_after = torch.cuda.memory_allocated() mem_reserved_after = torch.cuda.memory_reserved() # Print memory information print(\\"Memory allocated before operations:\\", mem_allocated_before) print(\\"Memory reserved before operations:\\", mem_reserved_before) print(\\"Memory allocated after operations:\\", mem_allocated_after) print(\\"Memory reserved after operations:\\", mem_reserved_after) # Step 4: Return the result tensor return result_tensor # Call the function result = hip_tensor_operations(dataset) ``` This question requires students to demonstrate a good understanding of using HIP with PyTorch, including device management, tensor operations, and memory monitoring.","solution":"import torch def hip_tensor_operations(dataset): if not torch.cuda.is_available() or not torch.version.hip: raise EnvironmentError(\\"HIP is not available in the current PyTorch build\\") # Step 1: Place dataset on HIP device device = torch.device(\'cuda\') dataset = dataset.to(device) # Step 2: Create another tensor of same dimensions with random numbers on HIP device rand_tensor = torch.randn(1000000, 10, device=device) # Monitor memory before operations mem_allocated_before = torch.cuda.memory_allocated() mem_reserved_before = torch.cuda.memory_reserved() # Step 3: Perform element-wise addition result_tensor = dataset + rand_tensor # Monitor memory after operations mem_allocated_after = torch.cuda.memory_allocated() mem_reserved_after = torch.cuda.memory_reserved() # Print memory information print(\\"Memory allocated before operations:\\", mem_allocated_before) print(\\"Memory reserved before operations:\\", mem_reserved_before) print(\\"Memory allocated after operations:\\", mem_allocated_after) print(\\"Memory reserved after operations:\\", mem_reserved_after) # Step 4: Return the result tensor return result_tensor"},{"question":"# Event Scheduling with `sched` Module You are provided with the `sched` module that allows you to manage and schedule events for execution either at a specific time or after a specific delay. For this task, you will implement a function that schedules several events with given time constraints and handles their execution. Task Write a function `schedule_multiple_events(time_events: List[Tuple[int, int, Callable, Tuple, Dict]]) -> List[str]` that schedules multiple events and captures their execution details. - **Input**: A list of tuples, where each tuple contains: - `time`: The absolute time (in seconds since the epoch) at which the event should be executed. - `priority`: Integer specifying the event priority (lower number means higher priority). - `action`: The function to be called when the event is executed. - `argument`: Tuple containing positional arguments for the `action` function (default is an empty tuple). - `kwargs`: Dictionary containing keyword arguments for the `action` function (default is an empty dictionary). - **Output**: A list of strings containing the details of executed events in the order they were executed. Each string should be formatted as: ``` \\"Executed {action.__name__} at {execution_time}\\" ``` Where `execution_time` is the actual time the event was executed. Example ```python import sched import time from typing import List, Tuple, Callable, Dict def example_action(event_name: str) -> None: return f\\"Event {event_name} executed.\\" def schedule_multiple_events( time_events: List[Tuple[int, int, Callable, Tuple, Dict]] ) -> List[str]: scheduler = sched.scheduler(time.time, time.sleep) def capture_event(event_time, func, arg, kwarg): result = func(*arg, **kwarg) execution_details.append(f\\"Executed {func.__name__} at {event_time}\\") return result execution_details = [] for event in time_events: time_, priority, action, argument, kwargs = ( event[0], event[1], event[2], event[3] if len(event) > 3 else (), event[4] if len(event) > 4 else {}, ) scheduler.enterabs(time_, priority, capture_event, (time_, action, argument, kwargs)) scheduler.run() return execution_details # Example usage current_time = time.time() events = [ (current_time + 5, 1, example_action, (\\"event1\\",), {}), (current_time + 10, 1, example_action, (\\"event2\\",), {}), ] print(schedule_multiple_events(events)) ``` This function captures the execution order of scheduled events and returns the details as specified. Constraints - The `action` functions are expected to have at least one positional argument. - The list of events will be provided in a valid format as described. - The execution time for each `action` function is negligible. Note You may consider using the epoch time (`time.time()`) to simulate event timings during testing.","solution":"import sched import time from typing import List, Tuple, Callable, Dict def schedule_multiple_events(time_events: List[Tuple[int, int, Callable, Tuple, Dict]]) -> List[str]: scheduler = sched.scheduler(time.time, time.sleep) execution_details = [] def capture_event(event_time, func, arg, kwarg): result = func(*arg, **kwarg) execution_details.append(f\\"Executed {func.__name__} at {event_time}\\") return result for event in time_events: time_, priority, action, argument, kwargs = event scheduler.enterabs(time_, priority, capture_event, (time_, action, argument, kwargs)) scheduler.run() return execution_details"},{"question":"Objective You are tasked with creating a utility that transforms a custom data structure into an XML document using Python\'s `xml.sax.saxutils` module. This will assess your understanding of SAX utilities and their practical application in generating and manipulating XML content. Problem Statement: Implement a function `custom_data_to_xml` that takes a custom data structure (a nested dictionary representing an XML tree) and generates an XML string output. Use the `xml.sax.saxutils.XMLGenerator` for this purpose, ensuring all special characters are properly escaped. Function Signature ```python def custom_data_to_xml(data: dict, root_element: str) -> str: pass ``` Input: - `data`: A nested dictionary representing the XML structure. - Example: ```python { \\"Person\\": { \\"Name\\": \\"John & Jane\\", \\"Age\\": \\"30\\", \\"ContactInfo\\": { \\"Email\\": \\"john.jane@example.com\\", \\"Phone\\": \\"<555-1234>\\" } } } ``` - `root_element`: A string representing the root element of the XML document. - Example: `\\"People\\"` Output: - A string containing the properly formatted XML document. - Example Output: ```xml <People> <Person> <Name>John &amp; Jane</Name> <Age>30</Age> <ContactInfo> <Email>john.jane@example.com</Email> <Phone>&lt;555-1234&gt;</Phone> </ContactInfo> </Person> </People> ``` Constraints: 1. Every key in the dictionary will be a valid XML tag name. 2. Your solution should handle nesting of dictionaries to represent nested XML elements. 3. Special characters in the XML content (&, <, >) must be properly escaped. 4. The function should handle cases where dictionary values are empty or missing gracefully. 5. Assume the root level (single root element) will always be provided and is not nested within the dictionary. Tips: - Utilize `xml.sax.saxutils.escape` to ensure that the XML content is safe. - The `xml.sax.saxutils.XMLGenerator` class will help in writing the XML document. - Manage the nesting and attributes correctly to ensure valid XML is generated. Good luck with your implementation!","solution":"from xml.sax.saxutils import XMLGenerator from io import StringIO def custom_data_to_xml(data: dict, root_element: str) -> str: def serialize_element(gen, element_name, element_data): gen.startElement(element_name, {}) if isinstance(element_data, dict): for key, value in element_data.items(): serialize_element(gen, key, value) else: gen.characters(str(element_data)) gen.endElement(element_name) output = StringIO() gen = XMLGenerator(output, \'utf-8\') gen.startDocument() gen.startElement(root_element, {}) for element_name, element_data in data.items(): serialize_element(gen, element_name, element_data) gen.endElement(root_element) gen.endDocument() return output.getvalue()"},{"question":"**Objective:** To assess students\' understanding of the Seaborn package, specifically the `objects` module, in creating and customizing informative visualizations. **Problem Statement:** Using the Seaborn `objects` module, create a visualization that provides insights into the dataset `penguins` which you can load using `seaborn.load_dataset(\'penguins\')`. Your plot should fulfill the following requirements: 1. Load the `penguins` dataset using Seaborn\'s load_dataset method. 2. Create a vertical bar plot that shows the count of penguins for each species. 3. Use the `Dodge` transformation to differentiate between male and female penguins within each species. 4. Add another transformation or adjustment to handle overlapping marks (e.g., `Jitter` or handle empty space using `empty` parameter). 5. Add reasonable gap between dodged bars to improve readability. 6. Ensure your plot is well-labeled, with titles and axis labels that make it easy to understand. **Input:** - No explicit input is required from the user. **Output:** - A vertical bar plot generated using Seaborn\'s `objects` module as per the instructions provided. **Example:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot ( so.Plot(penguins, x=\\"species\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Dodge(gap=.1, empty=\\"fill\\")) .label(x=\\"Species\\", y=\\"Count\\", title=\\"Count of Penguins by Species and Sex\\") ) ``` **Constraints:** - Ensure that the plot is informative and readable. - Use transformations effectively to manage overlapping data points. Use the code example above as a baseline, but ensure your solution meets all the specified requirements and constraints. **Performance Requirements:** - The solution should efficiently handle the \'penguins\' dataset without performance issues. **Note:** - Feel free to refer to the Seaborn documentation (https://seaborn.pydata.org) for additional help if required.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"species\\", color=\\"sex\\") .add(so.Bar(), so.Count(), so.Dodge(gap=.1, empty=\\"fill\\")) .label(x=\\"Species\\", y=\\"Count\\", title=\\"Count of Penguins by Species and Sex\\") ) return plot"},{"question":"<|Analysis Begin|> The provided documentation outlines various aspects of defining new object types in Python, particularly focusing on object structures and type objects. The documentation includes details on allocating objects on the heap, common object structures, type objects, number object structures, mapping object structures, sequence object structures, buffer object structures, async object structures, and supporting cyclic garbage collection. For a coding assessment, it would be valuable to design a question that tests a student\'s ability to work with at least one of these concepts. Implementing custom types in Python, demonstrating understanding of attributes, methods, and possibly one of the specific object structures (like number or sequence structures), would be a challenging yet suitable task to assess the student\'s comprehension of these advanced topics. <|Analysis End|> <|Question Begin|> # Custom Object Type Implementation In this assignment, you are required to implement a custom object type in Python that simulates a simple 2D vector. You will need to define a new class `Vector2D` that supports the following functionalities: 1. **Initialization**: - The object should be initialized with two attributes `x` and `y` representing the vector\'s coordinates in 2D space. 2. **String Representation**: - Implement the `__str__` method to provide a readable string representation of the vector in the format `Vector2D(x, y)`. 3. **Addition**: - Implement the `__add__` method to support vector addition. This method should allow the addition of two `Vector2D` objects and return a new `Vector2D` object representing the resultant vector. 4. **Subtraction**: - Implement the `__sub__` method to support vector subtraction. This method should allow the subtraction of one `Vector2D` object from another and return a new `Vector2D` object representing the resultant vector. 5. **Scalar Multiplication**: - Implement the `__mul__` method to support scalar multiplication. This method should allow the multiplication of a `Vector2D` object by a scalar (number) and return a new `Vector2D` object. 6. **Magnitude**: - Implement a method `magnitude` that computes and returns the magnitude (length) of the vector. 7. **Equality Comparison**: - Implement the `__eq__` method to support equality comparison between two `Vector2D` objects. # Expected Input and Output Formats - Input: - Varies depending on the method or operation called. - Output: - A new `Vector2D` object for operations like addition, subtraction, or scalar multiplication. - A string for the string representation method. - A boolean value for the equality comparison. - A float value for the magnitude method. # Constraints - Do not use any external libraries for vector operations. - Make sure to handle edge cases such as adding, subtracting, or comparing vectors with zero magnitude. ```python class Vector2D: def __init__(self, x: float, y: float): # Initialize the vector with coordinates x and y pass def __str__(self) -> str: # Return the string representation of the vector in the format \\"Vector2D(x, y)\\" pass def __add__(self, other: \'Vector2D\') -> \'Vector2D\': # Return a new vector that is the result of the addition of this vector and another vector pass def __sub__(self, other: \'Vector2D\') -> \'Vector2D\': # Return a new vector that is the result of the subtraction of another vector from this vector pass def __mul__(self, scalar: float) -> \'Vector2D\': # Return a new vector that is the result of the scalar multiplication of this vector pass def magnitude(self) -> float: # Return the magnitude (length) of the vector pass def __eq__(self, other: \'Vector2D\') -> bool: # Return True if this vector is equal to another vector, otherwise False pass # Example usage: v1 = Vector2D(3, 4) v2 = Vector2D(1, 2) print(v1 + v2) # Output: Vector2D(4, 6) print(v1 - v2) # Output: Vector2D(2, 2) print(v1 * 2) # Output: Vector2D(6, 8) print(v1.magnitude()) # Output: 5.0 print(v1 == v2) # Output: False ``` Implement the `Vector2D` class as specified to ensure all functionalities work correctly.","solution":"import math class Vector2D: def __init__(self, x: float, y: float): self.x = x self.y = y def __str__(self) -> str: return f\\"Vector2D({self.x}, {self.y})\\" def __add__(self, other: \'Vector2D\') -> \'Vector2D\': return Vector2D(self.x + other.x, self.y + other.y) def __sub__(self, other: \'Vector2D\') -> \'Vector2D\': return Vector2D(self.x - other.x, self.y - other.y) def __mul__(self, scalar: float) -> \'Vector2D\': return Vector2D(self.x * scalar, self.y * scalar) def magnitude(self) -> float: return math.sqrt(self.x**2 + self.y**2) def __eq__(self, other: \'Vector2D\') -> bool: return self.x == other.x and self.y == other.y"},{"question":"Objective: Assess your ability to visualize complex statistical relationships using the seaborn library. Problem Statement: You are provided with a dataset containing information on various movies. The dataset includes the following columns: - `Title`: The title of the movie. - `Genre`: The genre of the movie (e.g., \\"Action\\", \\"Comedy\\", etc.). - `Director`: The director of the movie. - `Year`: The release year of the movie. - `Runtime`: The runtime of the movie in minutes. - `Rating`: The IMDb rating of the movie. - `Votes`: Number of votes the movie received. Your task is to create various visualizations to uncover patterns and relationships in the dataset using seaborn functions. Requirements: 1. **Load and Prepare Data:** - Load the dataset from a CSV file named `movies.csv`. 2. **Basic Scatter Plot:** - Create a scatter plot showing the relationship between `Runtime` and `Rating`. - Provide appropriate axis labels and a title for the plot. 3. **Scatter Plot with Hue:** - Enhance the scatter plot by adding a hue based on the `Genre` column. - Provide a legend and a title for the plot. 4. **Customizing Scatter Plot:** - Further refine the scatter plot by altering the marker style based on whether the `Rating` is above or below 7.0. - Customize the color palette for improved aesthetics. 5. **Line Plot:** - Create a line plot to show the trend of the number of `Votes` over the `Year`. - Add confidence intervals to represent uncertainty. 6. **Advanced Visualization with Faceting:** - Using `FacetGrid`, create a faceted scatter plot that shows the relationship between `Runtime` and `Rating` across different `Genres`. - Configure the facets to be displayed in multiple columns. Constraints: - The dataset size should not exceed 10,000 records. - Ensure that the plots are clear and well-labeled. - Use appropriate seaborn functions to achieve the tasks. - Handle any missing or inconsistent data appropriately. Input: - `movies.csv` (A CSV file containing the movie dataset) Output: - A Jupyter notebook (.ipynb) containing the code to generate the required visualizations. Example: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load the dataset movies = pd.read_csv(\'movies.csv\') # 1. Basic Scatter Plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=movies, x=\'Runtime\', y=\'Rating\') plt.title(\'Runtime vs. Rating\') plt.xlabel(\'Runtime (minutes)\') plt.ylabel(\'Rating\') plt.show() # 2. Scatter Plot with Hue plt.figure(figsize=(10, 6)) sns.scatterplot(data=movies, x=\'Runtime\', y=\'Rating\', hue=\'Genre\') plt.title(\'Runtime vs. Rating by Genre\') plt.xlabel(\'Runtime (minutes)\') plt.ylabel(\'Rating\') plt.legend(title=\'Genre\') plt.show() # 3. Customizing Scatter Plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=movies, x=\'Runtime\', y=\'Rating\', hue=\'Genre\', style=movies[\'Rating\'] > 7) plt.title(\'Runtime vs. Rating by Genre with Marker Style\') plt.xlabel(\'Runtime (minutes)\') plt.ylabel(\'Rating\') plt.legend(title=\'Genre\') plt.show() # 4. Line Plot plt.figure(figsize=(10, 6)) sns.lineplot(data=movies, x=\'Year\', y=\'Votes\', ci=\'sd\') plt.title(\'Votes over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Votes\') plt.show() # 5. Faceted Scatter Plot g = sns.FacetGrid(movies, col=\'Genre\', col_wrap=4, height=4) g.map(sns.scatterplot, \'Runtime\', \'Rating\') g.add_legend() plt.show() ``` Notes: - Ensure that your visualizations are interpretable and aesthetically pleasing. - Include comments in your code explaining each step of the process.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_data(file_path): Load the data from a CSV file. return pd.read_csv(file_path) def plot_runtime_vs_rating(movies): Basic scatter plot: Runtime vs. Rating. plt.figure(figsize=(10, 6)) sns.scatterplot(data=movies, x=\'Runtime\', y=\'Rating\') plt.title(\'Runtime vs. Rating\') plt.xlabel(\'Runtime (minutes)\') plt.ylabel(\'Rating\') plt.show() def plot_runtime_vs_rating_with_genre(movies): Scatter plot with hue: Runtime vs. Rating by Genre. plt.figure(figsize=(10, 6)) sns.scatterplot(data=movies, x=\'Runtime\', y=\'Rating\', hue=\'Genre\') plt.title(\'Runtime vs. Rating by Genre\') plt.xlabel(\'Runtime (minutes)\') plt.ylabel(\'Rating\') plt.legend(title=\'Genre\') plt.show() def plot_custom_runtime_vs_rating(movies): Custom scatter plot: Runtime vs. Rating by Genre with Marker Style. plt.figure(figsize=(10, 6)) sns.scatterplot(data=movies, x=\'Runtime\', y=\'Rating\', hue=\'Genre\', style=movies[\'Rating\'] > 7) plt.title(\'Runtime vs. Rating by Genre with Marker Style\') plt.xlabel(\'Runtime (minutes)\') plt.ylabel(\'Rating\') plt.legend(title=\'Genre\') plt.show() def plot_votes_over_years(movies): Line plot: Votes over the Years. plt.figure(figsize=(10, 6)) sns.lineplot(data=movies, x=\'Year\', y=\'Votes\', ci=\'sd\') plt.title(\'Votes over the Years\') plt.xlabel(\'Year\') plt.ylabel(\'Votes\') plt.show() def plot_facet_runtime_vs_rating(movies): Faceted scatter plot: Runtime vs. Rating by Genre. g = sns.FacetGrid(movies, col=\'Genre\', col_wrap=4, height=4) g.map(sns.scatterplot, \'Runtime\', \'Rating\') g.add_legend() plt.show()"},{"question":"# Kernel Density Estimation with scikit-learn In this coding assessment, you will demonstrate your understanding of Kernel Density Estimation (KDE) using scikit-learn by implementing a solution that performs density estimation on a given dataset. Task 1. **Data Preparation**: - Generate a 1-dimensional bimodal distribution dataset consisting of 200 data points. The distribution should have two distinct peaks. 2. **Kernel Density Estimation**: - Use `KernelDensity` from `sklearn.neighbors` to perform KDE on the dataset. - Implement KDE using two different kernels: a Gaussian kernel and an Epanechnikov kernel. - Compare the kernel density estimates obtained from both kernels. 3. **Visualization**: - Plot the original data as a histogram. - Overlay the KDE results from both kernels on the histogram. - Ensure that your plots are properly labeled for clarity. Input and Output - **Input**: None specifically; you should generate the dataset within your code. - **Output**: Visualization of the histogram with KDE overlays for both kernels. Constraints - The dataset should have 200 data points. - Use a bandwidth parameter that clearly shows the difference between the two kernels (you can choose an appropriate value). Example ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Step 1: Generate a bimodal distribution dataset np.random.seed(0) data1 = np.random.normal(loc=-2.0, scale=0.5, size=100) data2 = np.random.normal(loc=2.0, scale=0.5, size=100) X = np.concatenate([data1, data2])[:, np.newaxis] # Step 2: Perform KDE with Gaussian and Epanechnikov kernels kde_gaussian = KernelDensity(kernel=\'gaussian\', bandwidth=0.5).fit(X) kde_epanechnikov = KernelDensity(kernel=\'epanechnikov\', bandwidth=0.5).fit(X) # Evaluate the density estimates X_plot = np.linspace(-5, 5, 1000)[:, np.newaxis] log_dens_gaussian = kde_gaussian.score_samples(X_plot) log_dens_epanechnikov = kde_epanechnikov.score_samples(X_plot) # Step 3: Visualization plt.hist(X, bins=30, density=True, alpha=0.5, color=\'gray\') plt.plot(X_plot[:, 0], np.exp(log_dens_gaussian), color=\'blue\', label=\'Gaussian KDE\') plt.plot(X_plot[:, 0], np.exp(log_dens_epanechnikov), color=\'red\', label=\'Epanechnikov KDE\') plt.legend() plt.xlabel(\'Data values\') plt.ylabel(\'Density\') plt.title(\'Kernel Density Estimate Comparison\') plt.show() ``` Explanation: - The code first generates a bimodal dataset using two normal distributions. - Then, it creates KDE models using Gaussian and Epanechnikov kernels with a specified bandwidth. - It evaluates the density estimates on a range of values and visualizes the results using matplotlib. Your task is to fill in the gaps in this example, ensuring all parts of the task are completed and clearly commented. This will demonstrate your understanding of KDE\'s basic concepts and practical application using scikit-learn.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_bimodal_dataset(): Generates a bimodal dataset with 200 data points. np.random.seed(0) data1 = np.random.normal(loc=-2.0, scale=0.5, size=100) data2 = np.random.normal(loc=2.0, scale=0.5, size=100) return np.concatenate([data1, data2])[:, np.newaxis] def perform_kde(X, bandwidth=0.5, kernel=\'gaussian\'): Performs Kernel Density Estimation on the given dataset X using specified kernel and bandwidth. Returns log density estimates. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) X_plot = np.linspace(-5, 5, 1000)[:, np.newaxis] log_dens = kde.score_samples(X_plot) return X_plot[:, 0], np.exp(log_dens) def visualize_kde(X, kde_results, kernels): Visualizes the original data histogram with KDE results overlaid for different kernels. plt.hist(X, bins=30, density=True, alpha=0.5, color=\'gray\') for result, kernel in zip(kde_results, kernels): plt.plot(result[0], result[1], label=f\'{kernel.capitalize()} KDE\') plt.legend() plt.xlabel(\'Data values\') plt.ylabel(\'Density\') plt.title(\'Kernel Density Estimate Comparison\') plt.show() # Generate dataset X = generate_bimodal_dataset() # Perform KDE for both Gaussian and Epanechnikov kernels x_vals_gaussian, dens_gaussian = perform_kde(X, bandwidth=0.5, kernel=\'gaussian\') x_vals_epanechnikov, dens_epanechnikov = perform_kde(X, bandwidth=0.5, kernel=\'epanechnikov\') # Visualize results visualize_kde(X, [(x_vals_gaussian, dens_gaussian), (x_vals_epanechnikov, dens_epanechnikov)], [\'gaussian\', \'epanechnikov\'])"},{"question":"# Seaborn Regression Plots Coding Challenge **Objective:** Create a set of visualizations to analyze the relationship between different variables in a dataset using Seaborn\'s regression plots. **Dataset:** Use the `tips` dataset, which is built into Seaborn. **Instructions:** 1. Load the necessary libraries and the `tips` dataset. 2. Create a plot using `lmplot` to examine the linear relationship between the `total_bill` and `tip` variables. 3. Modify the plot to include the `smoker` variable as a hue to distinguish between smokers and non-smokers. 4. Add jitter to the `size` variable while plotting its relationship with `tip` to see a clear distribution of tips based on party size. 5. Fit a polynomial regression of order 2 to see if it provides a better model for the relationship between `total_bill` and `tip`. 6. Plot a logistic regression to predict the probability of a big tip (`big_tip`, where a big tip is defined as `tip/total_bill > 0.15`) based on `total_bill`. 7. Use a lowess regression to examine the nonparametric fit between `total_bill` and `tip`. 8. Display the residuals of the linear regression model between `total_bill` and `tip` to check the validity of the model. 9. Create a `pairplot` to show the relationships conditioned on the `smoker` variable across `total_bill`, `size`, and `tip`. **Constraints:** - Use Seaborn for all visualizations. - Make sure to include proper titles and labels for the axes. - Ensure the figures are large enough for clarity. **Expected Output:** A series of plots that fulfill the given instructions and properly demonstrate the students\' understanding of Seaborn regression plotting techniques. **Example Code Skeleton:** ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the dataset tips = sns.load_dataset(\\"tips\\") # Step 1: Linear relationship between total_bill and tip sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Linear relationship between Total Bill and Tip\\") plt.show() # Step 2: Linear relationship with hue for smoker sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", data=tips) plt.title(\\"Linear relationship between Total Bill and Tip with Smoker hue\\") plt.show() # Step 3: Jittering size and tip relationship sns.lmplot(x=\\"size\\", y=\\"tip\\", data=tips, x_jitter=.05) plt.title(\\"Total Bill and Tip with jitter in Size\\") plt.show() # Step 4: Polynomial regression of order 2 for total_bill and tip sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, order=2) plt.title(\\"Polynomial regression of order 2 for Total Bill and Tip\\") plt.show() # Step 5: Logistic regression prediction for big_tip tips[\\"big_tip\\"] = (tips.tip / tips.total_bill) > .15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True, y_jitter=.03) plt.title(\\"Logistic regression for Big Tip Prediction\\") plt.show() # Step 6: Lowess regression for total_bill and tip sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, lowess=True, line_kws={\\"color\\": \\"C1\\"}) plt.title(\\"Lowess regression for Total Bill and Tip\\") plt.show() # Step 7: Residual plot for total_bill and tip sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Residuals of Linear regression of Total Bill and Tip\\") plt.show() # Step 8: Pairplot showing relationships conditioned on smoker sns.pairplot(tips, x_vars=[\\"total_bill\\", \\"size\\"], y_vars=[\\"tip\\"], hue=\\"smoker\\", height=5, aspect=.8, kind=\\"reg\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Step 1: Linear relationship between total_bill and tip def plot_linear_relationship_total_bill_tip(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Linear relationship between Total Bill and Tip\\") plt.show() # Step 2: Linear relationship with hue for smoker def plot_linear_relationship_with_smoker_hue(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", data=tips) plt.title(\\"Linear relationship between Total Bill and Tip with Smoker hue\\") plt.show() # Step 3: Jittering size and tip relationship def plot_jitter_size_with_tip(): sns.lmplot(x=\\"size\\", y=\\"tip\\", data=tips, x_jitter=.05) plt.title(\\"Total Bill and Tip with jitter in Size\\") plt.show() # Step 4: Polynomial regression of order 2 for total_bill and tip def plot_polynomial_regression_total_bill_tip(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, order=2) plt.title(\\"Polynomial regression of order 2 for Total Bill and Tip\\") plt.show() # Step 5: Logistic regression prediction for big_tip def plot_logistic_regression_for_big_tip(): tips[\\"big_tip\\"] = (tips.tip / tips.total_bill) > .15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True, y_jitter=.03) plt.title(\\"Logistic regression for Big Tip Prediction\\") plt.show() # Step 6: Lowess regression for total_bill and tip def plot_lowess_regression_total_bill_tip(): sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, lowess=True, line_kws={\\"color\\": \\"C1\\"}) plt.title(\\"Lowess regression for Total Bill and Tip\\") plt.show() # Step 7: Residual plot for total_bill and tip def plot_residuals_total_bill_tip(): sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.title(\\"Residuals of Linear regression of Total Bill and Tip\\") plt.show() # Step 8: Pairplot showing relationships conditioned on smoker def plot_pairplot_smoker_dependency(): sns.pairplot(tips, x_vars=[\\"total_bill\\", \\"size\\"], y_vars=[\\"tip\\"], hue=\\"smoker\\", height=5, aspect=.8, kind=\\"reg\\") plt.show()"},{"question":"Dynamic Shapes and Guard Management **Objective:** To assess the student\'s understanding of dynamic shapes in PyTorch, managing guards, and utilizing symbolic reasoning to handle varying tensor sizes. **Problem Statement:** You are provided with a function that merges two tensors along a specified dimension and then performs an element-wise operation based on the resultant tensor\'s size. Your task is to implement this function considering dynamic shapes and managing guards appropriately. **Function Signature:** ```python def dynamic_tensor_processing(tensor1: torch.Tensor, tensor2: torch.Tensor, dim: int) -> torch.Tensor: ``` **Input:** - `tensor1`: A PyTorch tensor of any shape. - `tensor2`: A PyTorch tensor of any shape that can be concatenated with `tensor1` along the specified dimension. - `dim`: An integer representing the dimension along which the tensors should be concatenated. **Output:** - A PyTorch tensor that results from concatenating `tensor1` and `tensor2` along the specified dimension, followed by an element-wise operation based on the size of the resultant tensor. **Constraints:** - The tensors should be concatenated only if their shapes allow concatenation along the specified dimension. - The resultant tensor should undergo an element-wise multiplication by 2 if its size along the concatenated dimension exceeds 10, otherwise an element-wise addition of 2 should be performed. - Dynamic shapes should be appropriately handled, and guards should be used to manage branching based on the resultant tensor size. **Sample Usage:** ```python import torch from torch._dynamo import mark_dynamic # Example tensors tensor1 = torch.randn(3, 4) tensor2 = torch.randn(3, 6) # Marking dynamic dimensions mark_dynamic(tensor1, 1) mark_dynamic(tensor2, 1) # Example function call result = dynamic_tensor_processing(tensor1, tensor2, dim=1) print(result) ``` **Implementation Requirements:** 1. Use dynamic shapes methodology to handle varying tensor sizes. 2. Implement guard management to decide the element-wise operation based on the size of the resultant tensor. 3. Ensure that the function works for any permissible input tensor shapes and the specified dimension. **Evaluation Criteria:** - Correctness: The function should concatenate tensors and apply the correct element-wise operation based on the resultant tensor size. - Handling dynamic shapes: The function should correctly utilize dynamic shape capabilities of PyTorch. - Guard management: Proper usage of guards to determine the branch of code to execute based on tensor sizes. - Code quality: Code should be clean, well-documented, and follow best practices. **Hint:** You can refer to the documentation provided above for understanding how to use dynamic shapes, symbolic sizes, and guards in PyTorch.","solution":"import torch def dynamic_tensor_processing(tensor1: torch.Tensor, tensor2: torch.Tensor, dim: int) -> torch.Tensor: Concatenates two tensors along the specified dimension and then performs an element-wise operation based on the resultant tensor\'s size. Args: tensor1 (torch.Tensor): The first input tensor. tensor2 (torch.Tensor): The second input tensor. dim (int): The dimension along which to concatenate the tensors. Returns: torch.Tensor: The resultant tensor after concatenation and element-wise operation. # Concatenate the tensors along the specified dimension result = torch.cat((tensor1, tensor2), dim=dim) # Determine the element-wise operation based on the size of the resultant tensor if result.size(dim) > 10: result = result * 2 # Element-wise multiplication by 2 else: result = result + 2 # Element-wise addition of 2 return result"},{"question":"Objective: To demonstrate the ability to effectively use the `seaborn.cubehelix_palette` function by customizing color palettes and applying them in visualizations. Problem Statement: You are tasked with creating a data visualization that requires a highly customized color palette using Seabornâ€™s `cubehelix_palette`. Write a function `custom_cubehelix_plot(data, n_colors, start, rot, gamma, hue, dark, light, reverse)` that performs the following steps: 1. **Generate a Custom Cubehelix Palette:** - Use the provided parameters to create a `cubehelix_palette`. 2. **Create a Visualization:** - Using the generated palette, create a visualization of the given `data`. Use a seaborn plot type of your choice (e.g., `heatmap`, `scatterplot`, etc.) that effectively showcases the palette. Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def custom_cubehelix_plot(data, n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0.3, light=0.8, reverse=False): pass ``` Input: - `data` (pd.DataFrame): A DataFrame containing the data to plot. - `n_colors` (int): Number of colors in the palette. Default is 6. - `start` (float): The starting position of the hue rotation. Default is 0. - `rot` (float): The number of rotations around the hue wheel. Default is 0.4. - `gamma` (float): Gamma factor to emphasize the lower intensity. Default is 1.0. - `hue` (float): Saturation. Default is 0.8. - `dark` (float): Intensity of the darkest color. Default is 0.3. - `light` (float): Intensity of the lightest color. Default is 0.8. - `reverse` (bool): If True, the direction of the lightness ramp is reversed. Default is False. Output: - The function should display a plot visualizing the data with the customized cubehelix palette. Constraints: - The function should handle default and custom values for all parameters. - The visualization should clearly reflect the changes in the color palette based on input parameters. Example: ```python import pandas as pd # Sample data data = pd.DataFrame({ \'x\': range(10), \'y\': range(10), \'z\': [2*xi for xi in range(10)] }) # Function call custom_cubehelix_plot(data, n_colors=10, start=2, rot=-0.5, gamma=0.6, hue=1, dark=0.25, light=0.75, reverse=True) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def custom_cubehelix_plot(data, n_colors=6, start=0, rot=0.4, gamma=1.0, hue=0.8, dark=0.3, light=0.8, reverse=False): Generates a Cubehelix Palette based on provided parameters and visualizes the data using a heatmap. Parameters: - data (pd.DataFrame): A DataFrame containing the data to plot. - n_colors (int): Number of colors in the palette. Default is 6. - start (float): The starting position of the hue rotation. Default is 0. - rot (float): The number of rotations around the hue wheel. Default is 0.4. - gamma (float): Gamma factor to emphasize the lower intensity. Default is 1.0. - hue (float): Saturation. Default is 0.8. - dark (float): Intensity of the darkest color. Default is 0.3. - light (float): Intensity of the lightest color. Default is 0.8. - reverse (bool): If True, the direction of the lightness ramp is reversed. Default is False. Returns: - A heatmap visualization of the data using the customized cubehelix palette. # Generate customized Cubehelix palette palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse) # Plot heatmap using the generated palette sns.heatmap(data, cmap=palette) plt.show()"},{"question":"Objective: To assess your understanding of PyTorch tensors and the `torch.Size` class, implement a function that takes a tensor and returns a string describing its dimensions. Task: Write a function `describe_tensor_dimensions(tensor)` that takes a PyTorch tensor as an input and returns a string describing the dimensions of the tensor in the following format: ``` \\"The tensor has X dimensions. Size of dimension 1: D1, Size of dimension 2: D2, ..., Size of dimension X: DX.\\" ``` Where `X` is the number of dimensions, and `D1, D2, ..., DX` are the sizes of each of those dimensions. Input: - `tensor (torch.Tensor)`: A PyTorch tensor of any size and number of dimensions. Output: - `description (str)`: A string describing the dimensions of the tensor. Constraints: - You can assume the input tensor is always a valid PyTorch tensor. Example: ```python import torch def describe_tensor_dimensions(tensor): s = tensor.size() description = f\\"The tensor has {len(s)} dimensions. \\" for i, dim_size in enumerate(s): description += f\\"Size of dimension {i+1}: {dim_size}, \\" return description.rstrip(\\", \\") # Example usage tensor = torch.ones(10, 20, 30) print(describe_tensor_dimensions(tensor)) # Output: \\"The tensor has 3 dimensions. Size of dimension 1: 10, Size of dimension 2: 20, Size of dimension 3: 30.\\" ``` Notes: - Make sure to handle tensors with any number of dimensions, including 0-dimensional tensors. - Ensure the output format strictly follows the given pattern.","solution":"import torch def describe_tensor_dimensions(tensor): s = tensor.size() description = f\\"The tensor has {len(s)} dimensions. \\" for i, dim_size in enumerate(s): description += f\\"Size of dimension {i+1}: {dim_size}, \\" return description.rstrip(\\", \\")"},{"question":"# Advanced Enumeration Design with Python\'s `enum` Module In this coding exercise, you are tasked with demonstrating your understanding of Python\'s `enum` module by designing an advanced enumeration class to represent a collection of mythical creatures with unique attributes. # Requirements 1. **Create an `Enum` class `CreatureType`**: - This class should have at least 5 members representing different mythical creatures. - Each member should have a unique string attribute representing the creature\'s special power (e.g., `Dragon` with `FireBreath`). 2. **Custom Value Handling**: - Use `auto()` to automatically assign unique integer values to the enum members. - Customize the value associated with each enum member to store the creature\'s power as a string. 3. **Unique Members**: - Ensure no two members of `CreatureType` have the same integer value using the `unique` decorator. 4. **Add Methods**: - Add a method `describe()` that returns a formatted string describing the creature and its power. 5. **Testing the Enumeration**: - Create a function `test_creature_enum()` that iterates through all members of `CreatureType` and prints their descriptions. # Constraints - Do not use hardcoded values for member values; use `auto()` and `_generate_next_value_` if needed. - The enumeration should provide a readable `repr`. # Example Usage ```python from enum import Enum, auto, unique @unique class CreatureType(Enum): # Your code to define the enum class ... def describe(self): # Your code for the describe method ... def test_creature_enum(): for creature in CreatureType: print(creature.describe()) # Expected output: # Dragon with the power of FireBreath # Phoenix with the power of Rebirth # ... ``` Implement the `CreatureType` enum and the `test_creature_enum` function as specified.","solution":"from enum import Enum, auto, unique @unique class CreatureType(Enum): DRAGON = auto() PHOENIX = auto() UNICORN = auto() GRIFFIN = auto() MERMAID = auto() def describe(self): power_descriptions = { CreatureType.DRAGON: \\"FireBreath\\", CreatureType.PHOENIX: \\"Rebirth\\", CreatureType.UNICORN: \\"Healing\\", CreatureType.GRIFFIN: \\"AirMastery\\", CreatureType.MERMAID: \\"WaterControl\\" } return f\\"{self.name.capitalize()} with the power of {power_descriptions[self]}\\" # Sample test function to demonstrate the descriptions def test_creature_enum(): for creature in CreatureType: print(creature.describe())"},{"question":"Objective: Demonstrate your understanding and proficiency with the Seaborn library, specifically focusing on swarm plots and related advanced visualizations. Problem Statement: Using the Seaborn `tips` dataset, write a function `create_swarm_plot` that: 1. Produces a vertical swarm plot comparing total bill amounts across different days. 2. Adds `hue` to differentiate points based on whether the customer is a smoker. 3. Utilize a qualitative palette of your choice to enhance the visual distinction between the categories. 4. Ensures that overlapping of points is minimized by adjusting the size of the points if necessary. 5. Optionally, you can add title and labels to your plot to illustrate your ability to customize and annotate visualizations. Function Signature: ```python def create_swarm_plot(): pass ``` Constraints: - Use Seaborn and related libraries only. Do not use other visualization libraries for the task. - Make sure the plot is displayed correctly and is easy to interpret. Expected Output: A vertically oriented swarm plot that illustrates the distribution of total bill amounts across different days, with points color-coded based on smoking status. Guidelines: - Load the `tips` dataset directly within the function. - Apply appropriate themes using `sns.set_theme()`. - Annotate (title, labels) the plot to enhance visual comprehension where necessary. - Use suitable size for points to ensure minimal overlaps. Example: The plot produced by your function should resemble the following description visually: - X-axis: Day of the week. - Y-axis: Total Bill. - Different colors for smoker and non-smoker groups. - Minimal point overlap. Hints: - Refer to the `swarmplot` and `catplot` sections in the documentation provided for examples and detailed usage. - Experiment with point size to find a balanced visualization with minimal overlaps. - Customize the appearance using further parameters if needed. Create your function in the next cell and execute to display the plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_swarm_plot(): # Load the tips dataset tips = sns.load_dataset(\'tips\') # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the swarm plot plt.figure(figsize=(10, 6)) sns.swarmplot(x=\'day\', y=\'total_bill\', hue=\'smoker\', data=tips, palette=\'Set2\', size=5) # Title and labels plt.title(\'Total Bill Amounts by Day with Smoking Status\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Total Bill\') # Display the plot plt.show()"},{"question":"# Topological Sorting for Task Scheduling You are given several tasks with dependencies, represented as a directed acyclic graph (DAG). Each task must be completed before certain other tasks can begin. Your objective is to determine a possible order in which to complete the tasks using topological sorting. You are required to implement a function `find_task_order(tasks: Dict[Any, Set[Any]]) -> List[Any]` that takes a dictionary where the keys represent tasks and the values are sets of tasks that must precede the corresponding key task. Your function should return a list of tasks in a feasible order. Input: - `tasks`: A dictionary representing a directed acyclic graph where: - Keys are task identifiers (can be any hashable type). - Values are sets of task identifiers that must precede the key task. Output: - A list of task identifiers in a valid topological order. Constraints: - The graph (tasks dictionary) is guaranteed to be a directed acyclic graph (DAG). - Tasks identifiers are unique and hashable. - There are no self-loops in the graph. Example: ```python from typing import Dict, Set, List def find_task_order(tasks: Dict[Any, Set[Any]]) -> List[Any]: # Your implementation here # Example usage: tasks = { \\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}, \\"A\\": set() } print(find_task_order(tasks)) # Output could be: [\'A\', \'C\', \'B\', \'D\'] ``` Additional Instructions: 1. Use the `graphlib.TopologicalSorter` class to achieve the topological sorting. 2. Handle any setup and necessary function calls required for topological sorting as explained in the documentation. **Note**: Your implementation is expected to handle larger graphs efficiently, considering both time and space complexity.","solution":"from typing import Dict, Set, List, Any from graphlib import TopologicalSorter def find_task_order(tasks: Dict[Any, Set[Any]]) -> List[Any]: ts = TopologicalSorter(tasks) return list(ts.static_order())"},{"question":"# Coding Assessment: Custom Netrc Utility **Objective**: Implement a utility to manage `.netrc` file entries effectively. The utility should allow adding new entries, updating existing ones, removing entries, and fetching credentials. **Requirements**: 1. **Class `NetrcManager`**: - The class should encapsulate the logic to interact with a `.netrc` file. - Initialize the class with an optional `file` parameter. If not provided, it should default to `.netrc` in the user\'s home directory. 2. **Methods**: - `add_entry(host: str, login: str, account: str, password: str) -> None`: - Add a new entry for the given `host`. If the host already exists, raise a `ValueError`. - `update_entry(host: str, login: Optional[str] = None, account: Optional[str] = None, password: Optional[str] = None) -> None`: - Update an existing entry with provided values. If the host does not exist, raise a `KeyError`. - `remove_entry(host: str) -> None`: - Remove the entry of the given `host`. If the host does not exist, raise a `KeyError`. - `fetch_credentials(host: str) -> Optional[Tuple[str, str, str]]`: - Fetch the credentials as a tuple `(login, account, password)` for the given `host`. If the host does not exist, return `None`. **Constraints**: - Ensure the class handles file read/write operations safely. - Implement error handling to manage cases like file not found, invalid file syntax, etc. - All passwords must be validated to ensure they adhere to the ASCII punctuation constraint. **Sample Implementation Outline**: ```python from typing import Optional, Tuple import os from netrc import netrc, NetrcParseError class NetrcManager: def __init__(self, file: Optional[str] = None): # Your implementation here def add_entry(self, host: str, login: str, account: str, password: str) -> None: # Your implementation here def update_entry(self, host: str, login: Optional[str] = None, account: Optional[str] = None, password: Optional[str] = None) -> None: # Your implementation here def remove_entry(self, host: str) -> None: # Your implementation here def fetch_credentials(self, host: str) -> Optional[Tuple[str, str, str]]: # Your implementation here # Example Usage: # manager = NetrcManager() # manager.add_entry(\'example.com\', \'user\', \'acct\', \'pass\') # manager.update_entry(\'example.com\', password=\'newpass\') # creds = manager.fetch_credentials(\'example.com\') # manager.remove_entry(\'example.com\') ``` **Notes**: - Make sure to handle exceptions and provide meaningful error messages. - Use `os.path.expanduser()` to manage file paths appropriately. - The documentation states UTF-8 encoding should be tried first before locale-specific encoding when reading files in version 3.10.","solution":"import os from typing import Optional, Tuple from netrc import netrc, NetrcParseError class NetrcManager: def __init__(self, file: Optional[str] = None): self.file = file if file else os.path.expanduser(\'~/.netrc\') if not os.path.exists(self.file): open(self.file, \'a\').close() def add_entry(self, host: str, login: str, account: str, password: str) -> None: try: auth_struct = netrc(self.file) if host in auth_struct.hosts: raise ValueError(f\\"Host {host} already exists.\\") self._write_entry(auth_struct, host, login, account, password) except NetrcParseError: self._write_entry(netrc(), host, login, account, password) def update_entry(self, host: str, login: Optional[str] = None, account: Optional[str] = None, password: Optional[str] = None) -> None: auth_struct = netrc(self.file) if host not in auth_struct.hosts: raise KeyError(f\\"Host {host} does not exist.\\") current_entry = auth_struct.hosts[host] self._write_entry(auth_struct, host, login or current_entry[0], account or current_entry[1], password or current_entry[2]) def remove_entry(self, host: str) -> None: auth_struct = netrc(self.file) if host not in auth_struct.hosts: raise KeyError(f\\"Host {host} does not exist.\\") del auth_struct.hosts[host] self._write_netrc(auth_struct) def fetch_credentials(self, host: str) -> Optional[Tuple[str, str, str]]: auth_struct = netrc(self.file) if host not in auth_struct.hosts: return None return auth_struct.hosts[host] def _write_entry(self, auth_struct, host, login, account, password): auth_struct.hosts[host] = (login, account, password) self._write_netrc(auth_struct) def _write_netrc(self, auth_struct): with open(self.file, \'w\') as f: for host, creds in auth_struct.hosts.items(): f.write(f\\"machine {host}ntlogin {creds[0]}ntaccount {creds[1]}ntpassword {creds[2]}n\\")"},{"question":"# Advanced Python Programming: Working with ZIP Files **Objective:** Implement a Python function that manages ZIP file creation, content listing, and file addition, demonstrating comprehension of the core functionalities provided by the `zipfile` module. **Problem Statement:** You are to create a function named `manage_zip_file` that executes the following operations based on given inputs: 1. **Create a new ZIP archive**: If the specified zip file does not exist, create it. 2. **Add files to the ZIP archive**: Add given files to the existing/newly created ZIP archive. 3. **List contents of the ZIP archive**: Return the list of all file names contained within the ZIP archive. **Function Signature:** ```python def manage_zip_file(zip_filename: str, files_to_add: list, list_contents: bool = False) -> list: pass ``` **Inputs:** - `zip_filename` (str): The name of the ZIP file to work with. - `files_to_add` (list): A list of file paths (as strings) to be added to the ZIP archive. - `list_contents` (bool): A boolean flag indicating whether to list the contents of the ZIP file. Default is `False`. **Outputs:** - If `list_contents` is `True`, return a list of filenames currently in the ZIP archive. - If `list_contents` is `False`, return an empty list. **Constraints:** - If `zip_filename` does not exist, the function should create a new ZIP file with that name. - If `files_to_add` is empty, do not add any files. - Handle exceptions gracefully and log meaningful error messages. **Example Usage:** ```python # Assume \'example.zip\' does not exist initially # Case 1: Create a new ZIP file and add files to it manage_zip_file(\'example.zip\', [\'file1.txt\', \'file2.txt\']) # Case 2: List contents of the ZIP file print(manage_zip_file(\'example.zip\', [], list_contents=True)) # Output: [\'file1.txt\', \'file2.txt\'] # Case 3: Add another file to the existing ZIP file and list contents again manage_zip_file(\'example.zip\', [\'file3.txt\']) print(manage_zip_file(\'example.zip\', [], list_contents=True)) # Output: [\'file1.txt\', \'file2.txt\', \'file3.txt\'] ``` **Hints:** - Use the `zipfile.ZipFile` class to work with ZIP archives. - Use appropriate modes (\'w\', \'a\', \'r\') for reading, writing, and appending to the ZIP file. - Ensure to close the ZIP file after operations or use context managers (`with` statement) to handle file closing automatically. - Handle exceptions like `FileNotFoundError`, `zipfile.BadZipFile`, etc., appropriately. **Assessment Criteria:** - Correct implementation of ZIP file creation and file addition. - Accurate listing of ZIP file contents based on the flag. - Proper use of context management for file operations. - Graceful handling of exceptions and edge cases.","solution":"import zipfile import os def manage_zip_file(zip_filename: str, files_to_add: list, list_contents: bool = False) -> list: Manage a ZIP file - create, add files, or list contents. :param zip_filename: The name of the ZIP file to work with. :param files_to_add: A list of file paths to be added to the ZIP archive. :param list_contents: Boolean flag indicating whether to list the contents of the ZIP file. :return: List of filenames currently in the ZIP archive if list_contents is True, otherwise an empty list. # Ensure we have a zip file to work with if not os.path.isfile(zip_filename): # Create a new zip file with zipfile.ZipFile(zip_filename, \'w\') as zipf: pass # Add files to the zip archive if files_to_add: with zipfile.ZipFile(zip_filename, \'a\') as zipf: for file in files_to_add: if os.path.isfile(file): zipf.write(file) else: print(f\\"Warning: File \'{file}\' does not exist and won\'t be added to the ZIP archive.\\") # List contents if requested if list_contents: with zipfile.ZipFile(zip_filename, \'r\') as zipf: return zipf.namelist() return []"},{"question":"You are required to implement a class that can import and execute Python modules from a specified path within a ZIP archive. Your task is to use the `zipimport.zipimporter` class to achieve this functionality. Requirements 1. Implement a class `ZipModuleLoader` with the following methods: - `__init__(self, archive_path: str)`: Initialize the loader with the path to the ZIP archive. - `import_module(self, module_name: str) -> None`: Import the specified module from the ZIP archive and execute it. - `get_module_source(self, module_name: str) -> str`: Return the source code of the specified module as a string. 2. Raise appropriate exceptions if: - The specified ZIP file doesn\'t exist or isn\'t a valid ZIP archive. - The module to be imported doesn\'t exist within the ZIP archive. Constraints - You are not allowed to manually extract the ZIP files; all operations should use the `zipimport` module. - The implementation should handle both `.py` and `.pyc` files. Example Usage ```python # Assuming we have a ZIP archive \'example.zip\' with a module \'mymodule.py\' loader = ZipModuleLoader(\'example.zip\') loader.import_module(\'mymodule\') print(loader.get_module_source(\'mymodule\')) # Expected output: The source code of \'mymodule.py\' as a string. ``` Detailed Specification ```python import zipimport import sys class ZipModuleLoader: def __init__(self, archive_path: str): # Initialize the zipimporter instance. pass def import_module(self, module_name: str) -> None: # Import the specified module and execute it. pass def get_module_source(self, module_name: str) -> str: # Return the source code of the specified module. pass ``` **Note**: You need to handle the necessary exceptions and ensure that the correct module is imported and executed.","solution":"import zipimport import sys class ZipModuleLoader: def __init__(self, archive_path: str): try: self.zip_importer = zipimport.zipimporter(archive_path) except zipimport.ZipImportError as e: raise FileNotFoundError(f\\"Cannot open zip archive: {archive_path}\\") from e def import_module(self, module_name: str) -> None: try: module = self.zip_importer.load_module(module_name) sys.modules[module_name] = module except zipimport.ZipImportError: raise ImportError(f\\"Module \'{module_name}\' cannot be found in the archive\\") def get_module_source(self, module_name: str) -> str: try: source = self.zip_importer.get_source(module_name) return source except zipimport.ZipImportError: raise ImportError(f\\"Source code for module \'{module_name}\' not found\\") # Example usage # loader = ZipModuleLoader(\'example.zip\') # loader.import_module(\'mymodule\') # print(loader.get_module_source(\'mymodule\'))"},{"question":"**Objective**: Implement a secure message signing and verification system using the HMAC algorithm described in the provided documentation. # Problem Statement You are required to create two functions: `sign_message` and `verify_message` to sign messages using a secret key and verify them. Function 1: `sign_message` **Definition**: ```python def sign_message(key: bytes, message: str, digestmod: str) -> str: ``` **Parameters**: - `key`: A bytes or bytearray object representing the secret key. - `message`: A string that needs to be signed. - `digestmod`: The name of the hash algorithm to use (e.g., \'sha256\'). **Returns**: - A hexadecimal string which is the HMAC digest of the message. **Constraints**: - The `key` must be a non-empty bytes or bytearray object. - The `message` must be a non-empty string. - The `digestmod` must be a valid digest name supported by the `hashlib` module. Function 2: `verify_message` **Definition**: ```python def verify_message(key: bytes, message: str, provided_digest: str, digestmod: str) -> bool: ``` **Parameters**: - `key`: A bytes or bytearray object representing the secret key. - `message`: A string whose HMAC needs to be verified. - `provided_digest`: A hexadecimal string representing the provided HMAC digest to compare against. - `digestmod`: The name of the hash algorithm to use (e.g., \'sha256\'). **Returns**: - A boolean indicating whether the provided digest matches the computed digest of the message. **Constraints**: - The `key` must be a non-empty bytes or bytearray object. - The `message` must be a non-empty string. - The `provided_digest` must be a valid hexadecimal string. - The `digestmod` must be a valid digest name supported by the `hashlib` module. **Requirements**: - Use the `hmac.new()` and `hmac.compare_digest()` functions correctly. - Ensure that the message signing and verification processes are secure and consistent with the HMAC algorithm described. # Example ```python # Example usage: key = b\'secret_key\' message = \'This is a test message\' digestmod = \'sha256\' # Sign the message signed_message = sign_message(key, message, digestmod) print(signed_message) # e.g., \\"5f4dcc3b5aa765d61d8327deb882cf99\\" # Verify the message is_valid = verify_message(key, message, signed_message, digestmod) print(is_valid) # True # Tampered message is_valid = verify_message(key, \'This is a tampered message\', signed_message, digestmod) print(is_valid) # False ``` **Notes**: - You may assume the user will provide valid inputs for simplicity. - The verification should use `hmac.compare_digest()` to mitigate timing attacks.","solution":"import hmac import hashlib def sign_message(key: bytes, message: str, digestmod: str) -> str: Sign the message with the given key using the specified digest module. if not key or not isinstance(key, (bytes, bytearray)): raise ValueError(\\"Key must be a non-empty bytes or bytearray object.\\") if not message or not isinstance(message, str): raise ValueError(\\"Message must be a non-empty string.\\") if not digestmod or not isinstance(digestmod, str): raise ValueError(\\"Digestmod must be a non-empty string and a valid digest name supported by hashlib.\\") hmac_object = hmac.new(key, message.encode(), digestmod) return hmac_object.hexdigest() def verify_message(key: bytes, message: str, provided_digest: str, digestmod: str) -> bool: Verify the message with the provided digest using the given key and digest module. if not key or not isinstance(key, (bytes, bytearray)): raise ValueError(\\"Key must be a non-empty bytes or bytearray object.\\") if not message or not isinstance(message, str): raise ValueError(\\"Message must be a non-empty string.\\") if not provided_digest or not isinstance(provided_digest, str): raise ValueError(\\"Provided digest must be a non-empty string.\\") if not digestmod or not isinstance(digestmod, str): raise ValueError(\\"Digestmod must be a non-empty string and a valid digest name supported by hashlib.\\") expected_digest = sign_message(key, message, digestmod) return hmac.compare_digest(expected_digest, provided_digest)"},{"question":"**Coding Question:** You are required to implement a function that takes a binary file, uuencodes it, and then decodes it back to verify if the original binary data is correctly preserved. Along the way, you\'ll need to handle specific errors and customize parameters. # Function Signature: ```python def verify_uuencoding(input_binary_file_path: str, encoded_file_path: str, decoded_file_path: str, backtick=False, quiet=False) -> bool: pass ``` # Parameters: - `input_binary_file_path` (str): The path to the input binary file to be uuencoded. - `encoded_file_path` (str): The path where the encoded file will be stored. - `decoded_file_path` (str): The path where the decoded file will be stored. - `backtick` (bool, optional): If True, zeros are represented by backticks in the uuencoded output. Default is False. - `quiet` (bool, optional): If True, suppresses warning messages during the decoding process. Default is False. # Returns: - `bool`: Returns True if the decoded file matches the original input file, False otherwise. # Constraints: - Ensure that the function handles any `uu.Error` exceptions that may be raised during encoding or decoding. - Use file-like objects for enhanced compatibility and demonstrate understanding of modern Python file handling practices. - The input files can be assumed to be small enough to fit in memory. # Example: ```python input_binary_file_path = \'path/to/input_binary_file.bin\' encoded_file_path = \'path/to/encoded_file.txt\' decoded_file_path = \'path/to/decoded_file.bin\' result = verify_uuencoding(input_binary_file_path, encoded_file_path, decoded_file_path, backtick=True, quiet=True) print(result) # Should print: True if the input and decoded files match, False otherwise. ``` # Notes: - Use the `uu.encode()` function to uuencode the input file to the encoded file. - Use the `uu.decode()` function to decode the encoded file back to the decoded file. - Compare the original input file and the decoded file to check if they are identical. - Handle any exceptions raised during the encoding or decoding process and ensure the function returns False in case of any error.","solution":"import uu def verify_uuencoding(input_binary_file_path: str, encoded_file_path: str, decoded_file_path: str, backtick=False, quiet=False) -> bool: try: # Encode the input binary file to uuencoded format with open(input_binary_file_path, \'rb\') as input_file, open(encoded_file_path, \'wb\') as encoded_file: uu.encode(input_file, encoded_file, name=\'encoded_file\', mode=0o644, backtick=backtick) # Decode the uuencoded file back to binary format with open(encoded_file_path, \'rb\') as encoded_file, open(decoded_file_path, \'wb\') as decoded_file: uu.decode(encoded_file, decoded_file, quiet=quiet) # Compare the original input file and the decoded file with open(input_binary_file_path, \'rb\') as input_file, open(decoded_file_path, \'rb\') as decoded_file: original_data = input_file.read() decoded_data = decoded_file.read() return original_data == decoded_data except uu.Error: return False except Exception as e: print(f\\"An error occurred: {e}\\") return False"},{"question":"Objective You are tasked with creating a monitoring script that safely manages the resource limits for a given process. The script will perform the following tasks: 1. Retrieve and display the current soft and hard limits for CPU time and the number of open file descriptors for a process. 2. Set new soft limits for CPU time and the number of open file descriptors, ensuring they do not exceed the current hard limits. 3. Verify and display the updated limits to confirm the changes. Task Write a Python function `manage_resource_limits(pid, new_cpu_limit, new_open_files_limit)` that fulfills the following requirements: 1. **Inputs:** - `pid` (int): The process ID for which to manage resource limits. If `pid` is 0, the current process is targeted. - `new_cpu_limit` (int): The new soft limit for CPU time in seconds. - `new_open_files_limit` (int): The new soft limit for the number of open file descriptors. 2. **Functionality:** - Retrieve the current limits for `RLIMIT_CPU` and `RLIMIT_NOFILE`. - Print the current soft and hard limits for both resources. - Check if `new_cpu_limit` and `new_open_files_limit` exceed their respective hard limits. If they do, raise a `ValueError` with an appropriate message. - Set the new soft limits for `RLIMIT_CPU` and `RLIMIT_NOFILE` using the provided values. - Retrieve and print the updated limits to confirm the changes. 3. **Output:** - The function should print the current and updated resource limits in a readable format. Constraints - If any system call fails, the function should handle the `OSError` appropriately and print an error message. - The function should ensure that any attempt to set a soft limit greater than the corresponding hard limit raises a `ValueError`. Example ```python def manage_resource_limits(pid, new_cpu_limit, new_open_files_limit): import resource try: # Get current limits for CPU time cpu_limits = resource.prlimit(pid, resource.RLIMIT_CPU) print(f\\"Current CPU limits: Soft = {cpu_limits[0]}, Hard = {cpu_limits[1]}\\") # Get current limits for the number of open file descriptors nofile_limits = resource.prlimit(pid, resource.RLIMIT_NOFILE) print(f\\"Current NOFILE limits: Soft = {nofile_limits[0]}, Hard = {nofile_limits[1]}\\") # Ensure new soft limits do not exceed hard limits if new_cpu_limit > cpu_limits[1]: raise ValueError(\\"New CPU soft limit exceeds the current hard limit.\\") if new_open_files_limit > nofile_limits[1]: raise ValueError(\\"New NOFILE soft limit exceeds the current hard limit.\\") # Set new soft limits resource.prlimit(pid, resource.RLIMIT_CPU, (new_cpu_limit, cpu_limits[1])) resource.prlimit(pid, resource.RLIMIT_NOFILE, (new_open_files_limit, nofile_limits[1])) # Verify and display updated limits updated_cpu_limits = resource.prlimit(pid, resource.RLIMIT_CPU) updated_nofile_limits = resource.prlimit(pid, resource.RLIMIT_NOFILE) print(f\\"Updated CPU limits: Soft = {updated_cpu_limits[0]}, Hard = {updated_cpu_limits[1]}\\") print(f\\"Updated NOFILE limits: Soft = {updated_nofile_limits[0]}, Hard = {updated_nofile_limits[1]}\\") except OSError as e: print(f\\"Failed to retrieve or set resource limits: {e}\\") # Example usage manage_resource_limits(0, 100, 256) ``` Notes - Remember to import the `resource` module. - Ensure robust exception handling to manage system-level resource errors.","solution":"import resource def manage_resource_limits(pid, new_cpu_limit, new_open_files_limit): Manages resource limits for a given process. Parameters: pid (int): The process ID. A pid of 0 targets the current process. new_cpu_limit (int): The new soft limit for CPU time in seconds. new_open_files_limit (int): The new soft limit for the number of open file descriptors. Raises: ValueError: If the new limits exceed the current hard limits. try: # Get current limits for CPU time cpu_limits = resource.getrlimit(resource.RLIMIT_CPU) print(f\\"Current CPU limits: Soft = {cpu_limits[0]}, Hard = {cpu_limits[1]}\\") # Get current limits for the number of open file descriptors nofile_limits = resource.getrlimit(resource.RLIMIT_NOFILE) print(f\\"Current NOFILE limits: Soft = {nofile_limits[0]}, Hard = {nofile_limits[1]}\\") # Ensure new soft limits do not exceed hard limits if new_cpu_limit > cpu_limits[1]: raise ValueError(\\"New CPU soft limit exceeds the current hard limit.\\") if new_open_files_limit > nofile_limits[1]: raise ValueError(\\"New NOFILE soft limit exceeds the current hard limit.\\") # Set new soft limits resource.setrlimit(resource.RLIMIT_CPU, (new_cpu_limit, cpu_limits[1])) resource.setrlimit(resource.RLIMIT_NOFILE, (new_open_files_limit, nofile_limits[1])) # Verify and display updated limits updated_cpu_limits = resource.getrlimit(resource.RLIMIT_CPU) updated_nofile_limits = resource.getrlimit(resource.RLIMIT_NOFILE) print(f\\"Updated CPU limits: Soft = {updated_cpu_limits[0]}, Hard = {updated_cpu_limits[1]}\\") print(f\\"Updated NOFILE limits: Soft = {updated_nofile_limits[0]}, Hard = {updated_nofile_limits[1]}\\") except OSError as e: print(f\\"Failed to retrieve or set resource limits: {e}\\")"},{"question":"**Objective**: Write a Python function to monitor and log the status of the current execution frame and function call stack. Problem Statement Create a function `trace_function_calls` that can be used to trace and log information about the execution of a function. The function should do the following: 1. Log the name and description of each function when it is called. 2. Log the line number of the executing code. 3. Log information about local and global variables at each step. 4. Maintain a complete call stack trace including built-in functions and methods being executed. **Your task**: You are provided with the following skeleton code: ```python import sys def trace_function_calls(func_to_trace): def tracer(frame, event, arg): if event == \'call\': with open(\'call_trace.log\', \'a\') as f: func_name = PyEval_GetFuncName(frame.f_code) func_desc = PyEval_GetFuncDesc(frame.f_code) line_no = PyFrame_GetLineNumber(frame) locals_ = PyEval_GetLocals() globals_ = PyEval_GetGlobals() f.write(f\\"Function called: {func_name} {func_desc}n\\") f.write(f\\"Line number: {line_no}n\\") f.write(f\\"Local variables: {locals_}n\\") f.write(f\\"Global variables: {globals_}n\\") f.write(\\"=\\"*40 + \\"n\\") return tracer sys.settrace(tracer) result = func_to_trace() sys.settrace(None) return result # Usage example def sample_function(): x = 10 y = 20 return x + y if __name__ == \\"__main__\\": trace_function_calls(sample_function) ``` **Constraints**: - The `trace_function_calls` function should be generic and able to trace any given function. - The trace should be logged into a file named `call_trace.log`. - Ensure that all information (function name, description, line number, local and global variables) is collected for each call. **Expected Output**: The log file `call_trace.log` should contain entries like the following for the `sample_function` example: ``` Function called: sample_function () Line number: 47 Local variables: {\'x\': 10, \'y\': 20} Global variables: {\'__name__\': \'__main__\', ...} ======================================== ``` **Performance Considerations**: - Given the potentially large number of function calls and complex call stacks, ensure that the tracing mechanism is as efficient as possible. - Be mindful of the overhead introduced by file I/O operations. Submission Submit your Python code which includes: 1. The `trace_function_calls` function. 2. Any necessary helper functions or modifications. 3. A sample test case demonstrating the function\'s usage.","solution":"import sys import traceback def trace_function_calls(func_to_trace): def tracer(frame, event, arg): if event == \'call\': with open(\'call_trace.log\', \'a\') as f: func_name = frame.f_code.co_name func_desc = f\\"File: {frame.f_code.co_filename}, Line: {frame.f_code.co_firstlineno}\\" line_no = frame.f_lineno locals_ = frame.f_locals globals_ = frame.f_globals f.write(f\\"Function called: {func_name} {func_desc}n\\") f.write(f\\"Line number: {line_no}n\\") f.write(f\\"Local variables: {locals_}n\\") f.write(f\\"Global variables: {globals_}n\\") f.write(\\"=\\"*40 + \\"n\\") return tracer sys.settrace(tracer) try: result = func_to_trace() finally: sys.settrace(None) return result # Usage example def sample_function(): x = 10 y = 20 return x + y if __name__ == \\"__main__\\": trace_function_calls(sample_function)"},{"question":"# Custom Python Interpreter with Extra Features In this exercise, you are required to design a custom Python interpreter using the `code` module. This interpreter should support basic Python commands as well as allow the user to add special commands that extend typical Python functionality. Your implementation should include the following: 1. **Basic Interactive Interpreter**: - Use the `code.InteractiveConsole` class to create an interactive interpreter. - The interpreter should read and execute commands entered by the user. 2. **Special Commands**: - Implement at least two special commands that are not part of standard Python. For this exercise: - `@hello` should print \\"Hello, World!\\" to the console. - `@sum <num1> <num2>` should print the sum of `num1` and `num2` to the console. - These commands should be differentiated from standard Python commands by a special prefix (`@` in this case). 3. **Command Handling**: - Override or extend the necessary methods to intercept these special commands and handle them appropriately before passing other commands to the Python interpreter. # Implementation Details: - **Input Format**: The input will be a series of lines entered by the user. - **Output Format**: For each input line: - If it is a special command (`@hello` or `@sum`), print the corresponding output. - If it is a regular Python command, execute it and display the result or any output/error produced. # Constraints and Performance: - Focus on creating a modular system where additional special commands can easily be added. - Gracefully handle invalid inputs and errors. - Ensure that the custom interpreter is efficient and responsive. # Example: **Input:** ``` @hello print(\\"This is a test\\") @sum 10 20 a = 5 b = 15 print(a + b) @sum a b ``` **Expected Output:** ``` Hello, World! This is a test 30 20 Sum command expects numerical inputs. ``` Use the documentation provided for the `code` module to assist you in designing your implementation. Ensure your solution comprehensively addresses all requirements and handles various edge cases effectively.","solution":"import code class CustomInterpreter(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): source = source.strip() # Handle special commands if source.startswith(\'@\'): if source == \\"@hello\\": print(\\"Hello, World!\\") return False elif source.startswith(\\"@sum\\"): _, num1, num2 = source.split() try: num1 = int(num1) num2 = int(num2) print(num1 + num2) except ValueError: print(\\"Sum command expects numerical inputs.\\") return False else: print(f\\"Unknown command: {source}\\") return False else: return super().runsource(source, filename, symbol) # Example usage: # interpreter = CustomInterpreter() # interpreter.interact(\\"Custom Python Interpreter with Extra Features\\")"},{"question":"**Assessment Question: Data Analysis with Pandas** # Objective: The objective of this task is to assess your understanding of various pandas functionalities including data manipulation, handling missing values, and working with datetime data. # Problem Statement: You are given a dataset as a CSV file that contains the sales data of a retail store over the last year. The dataset has the following columns: - `Date`: The date of the transaction. - `Product`: The product sold. - `Quantity`: The number of units sold. - `Price`: The price per unit. - `CustomerID`: An identifier for the customer. - `StoreLocation`: The location of the store. Using pandas, you are required to perform the following tasks: 1. **Data Loading**: - Load the dataset from the provided CSV file (`sales_data.csv`). 2. **Data Cleaning**: - Handle any missing values appropriately. - Convert the `Date` column to datetime format. 3. **Sales Analysis**: - Calculate the total sales amount for each transaction. - Determine the total sales for each product across all store locations. 4. **Pivot Table**: - Create a pivot table showing the total quantity sold for each product at each store location. 5. **Time Series Analysis**: - Generate a time series plot showing the total sales amount per month. # Input: - A CSV file named `sales_data.csv`. # Output: - Display or print the results of each task. # Constraints: - The dataset may have missing values in any column. - The `Date` column is in the format `YYYY-MM-DD`. # Performance Requirements: - Your solution should efficiently handle datasets with up to 100,000 rows. # Example: For a dataset with the following sample data: ``` Date,Product,Quantity,Price,CustomerID,StoreLocation 2023-01-01,Widget,4,10,001,New York 2023-01-10,Gadget,2,15,002,Los Angeles ..., ..., ..., ..., ..., ... ``` # Solution Template: ```python import pandas as pd import matplotlib.pyplot as plt # Task 1: Data Loading # -------------------- def load_data(file_path): df = pd.read_csv(file_path) return df # Task 2: Data Cleaning # -------------------- def clean_data(df): df[\'Date\'] = pd.to_datetime(df[\'Date\']) df = df.dropna() # or any other appropriate method to handle missing values return df # Task 3: Sales Analysis # ---------------------- def calculate_total_sales(df): df[\'TotalSales\'] = df[\'Quantity\'] * df[\'Price\'] total_sales_per_product = df.groupby(\'Product\')[\'TotalSales\'].sum() return total_sales_per_product # Task 4: Pivot Table # ------------------- def create_pivot_table(df): pivot_table = df.pivot_table(values=\'Quantity\', index=\'Product\', columns=\'StoreLocation\', aggfunc=\'sum\') return pivot_table # Task 5: Time Series Analysis # ---------------------------- def plot_monthly_sales(df): df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') monthly_sales = df.groupby(\'Month\')[\'TotalSales\'].sum() monthly_sales.plot(kind=\'line\') plt.title(\'Total Sales Per Month\') plt.ylabel(\'Total Sales\') plt.xlabel(\'Month\') plt.show() # Main Function # ------------- def main(): file_path = \'sales_data.csv\' df = load_data(file_path) df = clean_data(df) total_sales_per_product = calculate_total_sales(df) print(total_sales_per_product) pivot_table = create_pivot_table(df) print(pivot_table) plot_monthly_sales(df) if __name__ == \'__main__\': main() ``` This template provides a structured approach to solving the problem. Students are expected to fill in the necessary details and implement each function accordingly.","solution":"import pandas as pd import matplotlib.pyplot as plt # Task 1: Data Loading # -------------------- def load_data(file_path): df = pd.read_csv(file_path) return df # Task 2: Data Cleaning # -------------------- def clean_data(df): df[\'Date\'] = pd.to_datetime(df[\'Date\']) df = df.dropna() # or any other appropriate method to handle missing values return df # Task 3: Sales Analysis # ---------------------- def calculate_total_sales(df): df[\'TotalSales\'] = df[\'Quantity\'] * df[\'Price\'] total_sales_per_product = df.groupby(\'Product\')[\'TotalSales\'].sum() return total_sales_per_product # Task 4: Pivot Table # ------------------- def create_pivot_table(df): pivot_table = df.pivot_table(values=\'Quantity\', index=\'Product\', columns=\'StoreLocation\', aggfunc=\'sum\') return pivot_table # Task 5: Time Series Analysis # ---------------------------- def plot_monthly_sales(df): df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') monthly_sales = df.groupby(\'Month\')[\'TotalSales\'].sum() monthly_sales.plot(kind=\'line\') plt.title(\'Total Sales Per Month\') plt.ylabel(\'Total Sales\') plt.xlabel(\'Month\') plt.show() # Main Function # ------------- def main(file_path): df = load_data(file_path) df = clean_data(df) total_sales_per_product = calculate_total_sales(df) print(total_sales_per_product) pivot_table = create_pivot_table(df) print(pivot_table) plot_monthly_sales(df) if __name__ == \'__main__\': main(\'sales_data.csv\')"},{"question":"Objective You are to write a Python function using the `os` module to simulate a basic logging system. Your function should manage log files within a specific directory and perform periodic log rotations. Task Implement a function named `log_manager` that performs the following tasks: 1. **Directory Setup:** - Create a directory named `logs` if it doesn\'t already exist. 2. **Logging Functionality:** - Create or append to a log file named `application.log` within the `logs` directory. 3. **Log Rotation:** - If the size of `application.log` exceeds a specified maximum size (e.g., 5KB), rename this file to `application.log.{n}`, where `{n}` is an incrementing integer for each rotation starting from 1. - Create a new empty `application.log` file for further logging. 4. **Record Environmental Information:** - Each log entry should record the current timestamp, the process ID, and the name of user performing the logging. - The log entry should follow the format: `\\"[timestamp] [pid] [username]: Your log message heren\\"` Inputs - `message`: A string representing the log message to append to the log. - `max_size_kb`: An integer representing the maximum log file size in kilobytes before rotation occurs. Output - The function should not return anything. It should simply execute the described logging and rotation behavior. Constraints - You may assume log messages will not exceed 1KB in length. - You may use the current working directory for creating the `logs` directory. - Handle any potential `OSError` or related exceptions gracefully by printing an appropriate error message. Example Usage ```python log_manager(\\"Starting application\\", 5) log_manager(\\"User login successful\\", 5) log_manager(\\"An error occurred\\", 5) ``` # Implementation ```python import os from datetime import datetime def log_manager(message, max_size_kb): try: # Ensuring the \\"logs\\" directory exists if not os.path.exists(\\"logs\\"): os.mkdir(\\"logs\\") log_file_path = \\"logs/application.log\\" # Retrieve environmental information pid = os.getpid() username = os.getlogin() timestamp = datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") log_entry = f\\"[{timestamp}] [{pid}] [{username}]: {message}n\\" # Log rotation logic if os.path.exists(log_file_path): log_file_size_kb = os.path.getsize(log_file_path) / 1024 if log_file_size_kb > max_size_kb: # Determine the next numbering for rotation n = 1 while os.path.exists(f\\"logs/application.log.{n}\\"): n += 1 rotated_log_file = f\\"logs/application.log.{n}\\" os.rename(log_file_path, rotated_log_file) # Append to the log file with open(log_file_path, \'a\') as log_file: log_file.write(log_entry) except OSError as e: print(f\\"An error occurred: {e}\\") ```","solution":"import os from datetime import datetime def log_manager(message, max_size_kb): try: # Ensure the \\"logs\\" directory exists if not os.path.exists(\\"logs\\"): os.mkdir(\\"logs\\") log_file_path = \\"logs/application.log\\" # Retrieve environmental information pid = os.getpid() username = os.getlogin() timestamp = datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") log_entry = f\\"[{timestamp}] [{pid}] [{username}]: {message}n\\" # Log rotation logic if os.path.exists(log_file_path): log_file_size_kb = os.path.getsize(log_file_path) / 1024.0 if log_file_size_kb > max_size_kb: # Determine the next numbering for rotation n = 1 while os.path.exists(f\\"logs/application.log.{n}\\"): n += 1 rotated_log_file = f\\"logs/application.log.{n}\\" os.rename(log_file_path, rotated_log_file) # Append to the log file with open(log_file_path, \'a\') as log_file: log_file.write(log_entry) except OSError as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Python Coding Assessment Question: Introduction You are required to implement a simplified class method manager that emulates behaviors akin to `PyInstanceMethod` and `PyMethod` objects in Python. This will help in deeply understanding the fundamental operations behind instance methods and bound methods in Python. Task 1. **Create a class `InstanceMethod`**: - **Constructor (`__init__`)**: ```python def __init__(self, func): # Initialize with a callable object `func` pass ``` - **Method `__call__`**: ```python def __call__(self, *args, **kwargs): # Call the stored function `func` with provided arguments pass ``` - **Method `get_function`**: ```python def get_function(self): # Return the stored function pass ``` 2. **Create a class `Method`** that simulates bound methods: - **Constructor (`__init__`)**: ```python def __init__(self, func, instance): # Initialize with a callable object `func` and instance `instance` pass ``` - **Method `__call__`**: ```python def __call__(self, *args, **kwargs): # Call the stored function `func` bound to `instance` with provided arguments pass ``` - **Method `get_function`**: ```python def get_function(self): # Return the stored function pass ``` - **Method `get_self`**: ```python def get_self(self): # Return the instance to which the method is bound pass ``` Example Usage ```python def example_func(self, x): return x + 1 # Instance Method Example inst_method = InstanceMethod(example_func) print(inst_method.get_function()) # Output: <function example_func at ...> print(inst_method(10)) # Should raise a TypeError because self is missing # Method Example class ExampleClass: def increment(self, x): return x + 1 example_instance = ExampleClass() bound_method = Method(example_instance.increment, example_instance) print(bound_method.get_function()) # Output: <function ExampleClass.increment at ...> print(bound_method.get_self()) # Output: <ExampleClass object at ...> print(bound_method(10)) # Output: 11 ``` Constraints - The function provided to `InstanceMethod` and `Method` will always be valid callable objects. - `self` in `Method` will always be a valid instance of a class, and the function should correctly call it as bound to this instance. - Implementation should handle additional positional and keyword arguments correctly. This question aims to test your understanding of instance methods, bound methods, and callable object manipulations in Python at a conceptually lower level.","solution":"class InstanceMethod: def __init__(self, func): Initialize with a callable object `func` self.func = func def __call__(self, *args, **kwargs): Call the stored function `func` with provided arguments return self.func(*args, **kwargs) def get_function(self): Return the stored function return self.func class Method: def __init__(self, func, instance): Initialize with a callable object `func` and instance `instance` self.func = func self.instance = instance def __call__(self, *args, **kwargs): Call the stored function `func` bound to `instance` with provided arguments return self.func(self.instance, *args, **kwargs) def get_function(self): Return the stored function return self.func def get_self(self): Return the instance to which the method is bound return self.instance"},{"question":"Objective: To assess your understanding of JSON handling in Python using the `json` module, including encoding, decoding, and extending the default behavior for custom objects. Question: You are given a list of employee records, each represented as a dictionary with the following structure: ```python [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"designation\\": \\"Engineer\\"}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"designation\\": \\"Designer\\"}, {\\"name\\": \\"Charlie\\", \\"age\\": 35, \\"designation\\": \\"Manager\\"} ] ``` Additionally, there are some custom objects in your data that need special handling. Specifically, the `designation` field may now include special designations represented by the `Designation` class: ```python class Designation: def __init__(self, title, level): self.title = title self.level = level ``` For example: ```python [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"designation\\": Designation(\\"Engineer\\", \\"Senior\\")}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"designation\\": \\"Designer\\"}, {\\"name\\": \\"Charlie\\", \\"age\\": 35, \\"designation\\": Designation(\\"Manager\\", \\"Middle\\")} ] ``` Your task is to: 1. Write a function `serialize_employee_data(data)`, which takes a list of employee records as described and returns a JSON-formatted string. This function should handle instances of the `Designation` class by converting them to a dictionary with keys `title` and `level`. 2. Write a function `deserialize_employee_data(json_data)`, which takes a JSON-formatted string (as produced by `serialize_employee_data`) and returns the list of employee records, converting dictionaries with `title` and `level` back into `Designation` objects where appropriate. Implementation Requirements: 1. **Function `serialize_employee_data`**: - **Input**: `data` (list): A list of dictionaries representing employee records, some of which may include `Designation` objects. - **Output**: (str): A JSON-formatted string representation of the input data. - Use a custom JSON encoder if necessary. 2. **Function `deserialize_employee_data`**: - **Input**: `json_data` (str): A JSON-formatted string of employee records. - **Output**: (list): A list of dictionaries representing employee records, with `title` and `level` dictionaries converted back into `Designation` objects. Constraints: - Ensure the function handles non-`Designation` objects correctly. - Handle edge cases such as empty lists or records without the `designation` field. Example: ```python # Define the Designation class class Designation: def __init__(self, title, level): self.title = title self.level = level # Sample data employees = [ {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"designation\\": Designation(\\"Engineer\\", \\"Senior\\")}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"designation\\": \\"Designer\\"}, {\\"name\\": \\"Charlie\\", \\"age\\": 35, \\"designation\\": Designation(\\"Manager\\", \\"Middle\\")} ] # Expected serialized result serialized = serialize_employee_data(employees) print(serialized) # Should print: [{\\"name\\": \\"Alice\\", \\"age\\": 30, \\"designation\\": {\\"title\\": \\"Engineer\\", \\"level\\": \\"Senior\\"}}, {\\"name\\": \\"Bob\\", \\"age\\": 25, \\"designation\\": \\"Designer\\"}, {\\"name\\": \\"Charlie\\", \\"age\\": 35, \\"designation\\": {\\"title\\": \\"Manager\\", \\"level\\": \\"Middle\\"}}] # Deserialized result deserialized = deserialize_employee_data(serialized) print(deserialized) # Should print: [{\'name\': \'Alice\', \'age\': 30, \'designation\': <Designation object>}, {\'name\': \'Bob\', \'age\': 25, \'designation\': \'Designer\'}, {\'name\': \'Charlie\', \'age\': 35, \'designation\': <Designation object>} ] ``` Hints: - Use the `json.JSONEncoder` class to define custom serialization behavior for `Designation` objects. - Use the `object_hook` parameter in `json.loads` to customize the deserialization process.","solution":"import json class Designation: def __init__(self, title, level): self.title = title self.level = level def to_dict(self): return {\\"title\\": self.title, \\"level\\": self.level} @classmethod def from_dict(cls, data): return cls(data[\'title\'], data[\'level\']) class CustomEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Designation): return obj.to_dict() return super().default(obj) def serialize_employee_data(data): return json.dumps(data, cls=CustomEncoder) def deserialize_employee_data(json_data): def custom_decoder(dct): if \'title\' in dct and \'level\' in dct: return Designation.from_dict(dct) return dct return json.loads(json_data, object_hook=custom_decoder)"},{"question":"Objective: Using seaborn, create a set of visualizations to analyze the relationship between age, class, fare, and survival status of the passengers from the Titanic dataset. The visualizations should aim to provide insights into how these features are distributed and interrelated. Tasks: 1. **Load the Dataset:** - Load the Titanic dataset using seaborn\'s `load_dataset` function. 2. **Generate the Plots:** - **Age Distribution:** Create a violin plot to show the distribution of the age of passengers. - **Age Distribution by Class:** Create a violin plot to visualize the age distribution grouped by the passenger class. - **Age vs Class by Survival Status:** Create a violin plot to show the distribution of age by class, further segmented by whether the passenger survived or not. Use the `split` argument to split the KDE plots. - **Fare vs Age by Decade Groups:** Create a violin plot to show the relationship between the fare and age of passengers. Group the ages by decade intervals. - **Normalized Fare Distribution by Deck:** Create a violin plot where each violin\'s width is normalized to represent the number of observations for each deck. 3. **Customization:** - Include quartiles in the inner representation (`inner=\\"quart\\"`). - Use a non-default smoothing factor for the KDE in one of the plots (`bw_adjust`). - Ensure any necessary labeling for proper understanding (e.g., x-labels, y-labels, titles). Constraints: - Ensure that the visualizations are informative, meaning the plots should be well-labeled and, if needed, include legends. - Comment your code to provide clarity on what each part is doing. Input: - None (Data will be loaded within the function using seaborn) Output: - The function should output the generated plots. Example Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def titanic_analysis(): # Load Dataset df = sns.load_dataset(\\"titanic\\") # Plot 1: Age distribution plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"age\\"]) plt.title(\\"Age Distribution of Passengers\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Density\\") plt.show() # Plot 2: Age Distribution by Class plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"age\\", y=\\"class\\") plt.title(\\"Age Distribution by Class\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Class\\") plt.show() # Plot 3: Age vs Class by Survival Status plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True) plt.title(\\"Age vs Class by Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Plot 4: Fare vs Age by Decade Groups plt.figure(figsize=(10, 6)) decades = lambda x: f\\"{int(x)}â€“{int(x + 10)}\\" sns.violinplot(x=df[\\"age\\"].round(-1), y=df[\\"fare\\"], formatter=decades) plt.title(\\"Fare vs Age by Decade Groups\\") plt.xlabel(\\"Age Group\\") plt.ylabel(\\"Fare\\") plt.show() # Plot 5: Normalized Fare Distribution by Deck plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"deck\\", y=\\"fare\\", inner=\\"point\\", density_norm=\\"count\\") plt.title(\\"Normalized Fare Distribution by Deck\\") plt.xlabel(\\"Deck\\") plt.ylabel(\\"Fare\\") plt.show() # Call the function to test it titanic_analysis() ``` Notes: - Ensure you handle cases where the dataset may have missing values, especially in the `age` and `fare` columns. - The plots should be drawn using seaborn and displayed using matplotlib\'s `plt.show()`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_analysis(): # Load the Titanic dataset using seaborn\'s load_dataset function df = sns.load_dataset(\\"titanic\\") # Handle missing values for better visualization df = df.dropna(subset=[\'age\', \'fare\', \'class\', \'deck\', \'alive\']) # Plot 1: Age distribution plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"age\\"], inner=\\"quart\\") plt.title(\\"Age Distribution of Passengers\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Density\\") plt.show() # Plot 2: Age Distribution by Class plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", inner=\\"quart\\") plt.title(\\"Age Distribution by Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Plot 3: Age vs Class by Survival Status plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\") plt.title(\\"Age vs Class by Survival Status\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Plot 4: Fare vs Age by Decade Groups df[\'age_decade\'] = (df[\'age\'] // 10 * 10).astype(int) plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"age_decade\\", y=\\"fare\\", inner=\\"quart\\", bw_adjust=0.5) plt.title(\\"Fare vs Age by Decade Groups\\") plt.xlabel(\\"Age Group (Decades)\\") plt.ylabel(\\"Fare\\") plt.show() # Plot 5: Normalized Fare Distribution by Deck plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"deck\\", y=\\"fare\\", inner=\\"quart\\", scale=\\"width\\", bw_adjust=0.5) plt.title(\\"Normalized Fare Distribution by Deck\\") plt.xlabel(\\"Deck\\") plt.ylabel(\\"Fare\\") plt.show() # Call the function to test it (comment this out when testing) # titanic_analysis()"},{"question":"# Question: Parallel Matrix Multiplication In this task, you are required to implement a function for matrix multiplication using the \\"multiprocessing\\" module. The function should leverage multiple processes to parallelize the computation and improve performance. # Function Specification - **Function Name:** `parallel_matrix_multiplication` - **Input:** - `matrix_a` (List[List[int]]): The first matrix, an `m x n` matrix where `m` is the number of rows and `n` is the number of columns. - `matrix_b` (List[List[int]]): The second matrix, an `n x p` matrix where `n` is the number of rows and `p` is the number of columns. - **Output:** - Return a new matrix (List[List[int]]) that is the result of multiplying `matrix_a` by `matrix_b`. The result will be an `m x p` matrix. # Constraints - You may assume the matrices are well-formed and of compatible sizes. - You need to utilize the `multiprocessing.Pool` class to parallelize the computation. - Your solution should demonstrate proper use of interprocess communication and synchronization where necessary. # Example ```python >>> matrix_a = [ [1, 2], [3, 4], [5, 6] ] >>> matrix_b = [ [7, 8, 9], [10, 11, 12] ] >>> parallel_matrix_multiplication(matrix_a, matrix_b) [ [27, 30, 33], [61, 68, 75], [95, 106, 117] ] ``` # Notes To perform matrix multiplication, the number of columns in `matrix_a` must equal the number of rows in `matrix_b`. The element at position (i, j) in the resulting matrix will be the dot product of the i-th row of `matrix_a` and the j-th column of `matrix_b`. # Hints - Use the `multiprocessing.Pool` class to create a pool of worker processes. - Split the computation into tasks that can be executed in parallel. - You may find the `pool.map` or `pool.starmap` methods useful for parallelizing the computation. Good luck! Make sure your solution is efficient and utilizes the multi-processing capabilities effectively.","solution":"from multiprocessing import Pool def matrix_multiply_worker(args): i, j, matrix_a, matrix_b = args return sum(matrix_a[i][k] * matrix_b[k][j] for k in range(len(matrix_b))) def parallel_matrix_multiplication(matrix_a, matrix_b): # Get dimensions of input matrices m = len(matrix_a) # Number of rows in matrix_a n = len(matrix_a[0]) # Number of columns in matrix_a and rows in matrix_b p = len(matrix_b[0]) # Number of columns in matrix_b # Initialize the result matrix result = [[0] * p for _ in range(m)] # Prepare arguments for worker function args = [(i, j, matrix_a, matrix_b) for i in range(m) for j in range(p)] with Pool() as pool: results = pool.map(matrix_multiply_worker, args) # Fill in the result matrix for idx, value in enumerate(results): i, j = divmod(idx, p) result[i][j] = value return result"},{"question":"# Question: Advanced Boxplot Customization with Seaborn You are tasked with visualizing the `diamonds` dataset from the Seaborn library using customized boxplots. The goal is to assess your ability to: - Load and manipulate data. - Group data by multiple variables. - Customize the appearance of the plot. - Incorporate advanced Seaborn and Matplotlib customizations. Follow the instructions below to complete the task. Instructions 1. **Data Loading and Preparation:** - Load the `diamonds` dataset using Seaborn. - Filter the dataset to include only diamonds with a carat weight of 2.5 or less. 2. **Boxplot Creation:** - Create a vertical boxplot to visualize the distribution of diamond prices (`price`), grouped by both `cut` and `clarity`. - Use `cut` as the primary grouping variable on the x-axis and `clarity` (nested grouping) as the hue. 3. **Customization:** - Draw the boxes with line art and add a small gap between them. - Set the whiskers to cover the full range of the data. - Adjust the width of the boxes to a narrower size. - Customize the color and line properties: - Set the box color to a shade of blue with 50% transparency. - Set the line color to dark green and the line width to 1.5. 4. **Advanced Customization:** - Add notches to the boxes. - Remove the caps from the whiskers. - Use \'x\' as the marker for flier points. - Hint: Use the `boxprops`, `medianprops`, and `flierprops` parameters to customize these properties. 5. **Final Touches:** - Add a vertical grid line at x=1 (carat). - Set the title of the plot to \\"Diamond Price Distribution by Cut and Clarity\\". - Label the x-axis as \\"Cut\\" and the y-axis as \\"Price\\". Expected Output Your plot should be a well-customized boxplot that effectively visualizes the distribution of diamond prices by cut and clarity, with all the specified customizations applied. Constraints - Ensure the code runs efficiently on datasets with up to 100,000 rows. - Use clear and concise code with appropriate comments to explain your approach. Example Code Template ```python import seaborn as sns import matplotlib.pyplot as plt # 1. Data Loading and Preparation diamonds = sns.load_dataset(\\"diamonds\\") filtered_diamonds = diamonds[diamonds[\\"carat\\"] <= 2.5] # 2. Boxplot Creation plt.figure(figsize=(14, 7)) sns.set_theme(style=\\"whitegrid\\") ax = sns.boxplot( data=filtered_diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"clarity\\", linewidth=1.5, fill=False, gap=0.1, whis=(0, 100), boxprops={\\"facecolor\\": (0.3, 0.5, 0.7, 0.5)}, medianprops={\\"color\\": \\"r\\", \\"linewidth\\": 2}, flierprops={\\"marker\\": \\"x\\"} ) # 3. Customization ax.axvline(1, color=\\".3\\", dashes=(2, 2)) plt.title(\\"Diamond Price Distribution by Cut and Clarity\\") plt.xlabel(\\"Cut\\") plt.ylabel(\\"Price\\") plt.legend(loc=\\"best\\") plt.show() ``` Complete the code template based on the instructions above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_boxplot(): # 1. Data Loading and Preparation diamonds = sns.load_dataset(\\"diamonds\\") filtered_diamonds = diamonds[diamonds[\\"carat\\"] <= 2.5] # 2. Boxplot Creation plt.figure(figsize=(14, 7)) sns.set_theme(style=\\"whitegrid\\") ax = sns.boxplot( data=filtered_diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"clarity\\", linewidth=1.5, whis=(0, 100), width=0.6, notch=True, boxprops=dict(facecolor=\'blue\', alpha=0.5, edgecolor=\'darkgreen\', linewidth=1.5), medianprops=dict(color=\'darkgreen\', linewidth=1.5), flierprops=dict(marker=\'x\', color=\'darkgreen\', markersize=5), whiskerprops=dict(linewidth=1.5, color=\'darkgreen\'), capprops=dict(linewidth=0) ) # 4. Customization plt.axvline(1, color=\\"gray\\", linestyle=\\"--\\") plt.title(\\"Diamond Price Distribution by Cut and Clarity\\") plt.xlabel(\\"Cut\\") plt.ylabel(\\"Price\\") plt.legend(loc=\\"best\\") plt.show() # Calling function to demonstrate plot creation create_custom_boxplot()"},{"question":"# Byte-compile Python Files with Custom Settings You are tasked with creating a Python script that uses the `compileall` module to byte-compile Python files within a given directory. The script should offer several command-line arguments to customize the compilation process according to user preferences. Requirements: 1. **Command-Line Arguments**: - `--dir DIR`: The directory to compile. - `--maxlevels N`: Maximum recursion level; defaults to Python\'s recursion limit. - `--force`: Force recompile even if timestamps are up-to-date. - `--optimize LEVELS`: A comma-separated list of optimization levels (e.g., `0,1,2`). - `--workers N`: Number of worker threads to use. - `--quiet LEVEL`: Suppress output, where `LEVEL` can be `0` (default), `1` (only errors), or `2` (suppress all). - `--invalidation-mode MODE`: Bytecode invalidation mode, can be `timestamp`, `checked-hash`, or `unchecked-hash`. 2. **Functionality**: - The script should parse the command-line arguments. - It should call `compileall.compile_dir` with the appropriate parameters derived from the command-line arguments. - Proper error handling should be implemented to manage invalid input or execution failures. 3. **Constraints**: - Ensure the directory specified by `--dir` exists and contains Python files to compile. - Validate optimization levels provided via `--optimize`. - Number of workers specified in `--workers` should be a positive integer or zero (use system CPU count). # Implementation Implement the script function `main()` that handles the command-line arguments and calls the appropriate `compileall` function. Example Usage: ```bash python compile_script.py --dir my_project --maxlevels 5 --force --optimize 0,1,2 --workers 4 --quiet 1 --invalidation-mode checked-hash ``` Output - If the compilation is successful, print a summary of the process. - If an error occurs, print an appropriate error message. You may use the following template to start your implementation: ```python import argparse import compileall import os import sys def main(): parser = argparse.ArgumentParser(description=\\"Compile Python files with custom settings\\") parser.add_argument(\'--dir\', type=str, required=True, help=\\"The directory to compile\\") parser.add_argument(\'--maxlevels\', type=int, default=sys.getrecursionlimit(), help=\\"Maximum recursion level\\") parser.add_argument(\'--force\', action=\'store_true\', help=\\"Force recompile\\") parser.add_argument(\'--optimize\', type=str, help=\\"Comma-separated list of optimization levels\\") parser.add_argument(\'--workers\', type=int, default=1, help=\\"Number of worker threads\\") parser.add_argument(\'--quiet\', type=int, default=0, help=\\"Suppress output\\") parser.add_argument(\'--invalidation-mode\', type=str, choices=[\\"timestamp\\", \\"checked-hash\\", \\"unchecked-hash\\"], help=\\"Bytecode invalidation mode\\") args = parser.parse_args() if not os.path.isdir(args.dir): print(\\"Error: Directory does not exist\\") return optimize_levels = [int(level) for level in args.optimize.split(\',\')] if args.optimize else -1 try: success = compileall.compile_dir( dir=args.dir, maxlevels=args.maxlevels, force=args.force, optimize=optimize_levels, workers=args.workers, quiet=args.quiet, invalidation_mode=args.invalidation_mode ) if success: print(\\"Compilation successful!\\") else: print(\\"Compilation failed for some files\\") except Exception as e: print(f\\"Error during compilation: {e}\\") if __name__ == \'__main__\': main() ```","solution":"import argparse import compileall import os import sys def main(): parser = argparse.ArgumentParser(description=\\"Compile Python files with custom settings\\") parser.add_argument(\'--dir\', type=str, required=True, help=\\"The directory to compile\\") parser.add_argument(\'--maxlevels\', type=int, default=sys.getrecursionlimit(), help=\\"Maximum recursion level\\") parser.add_argument(\'--force\', action=\'store_true\', help=\\"Force recompile\\") parser.add_argument(\'--optimize\', type=str, help=\\"Comma-separated list of optimization levels\\") parser.add_argument(\'--workers\', type=int, default=1, help=\\"Number of worker threads\\") parser.add_argument(\'--quiet\', type=int, default=0, help=\\"Suppress output\\") parser.add_argument(\'--invalidation-mode\', type=str, choices=[\\"timestamp\\", \\"checked-hash\\", \\"unchecked-hash\\"], help=\\"Bytecode invalidation mode\\") args = parser.parse_args() if not os.path.isdir(args.dir): print(\\"Error: Directory does not exist\\") return optimize_levels = [int(level) for level in args.optimize.split(\',\')] if args.optimize else -1 try: success = compileall.compile_dir( dir=args.dir, maxlevels=args.maxlevels, force=args.force, optimize=optimize_levels, workers=args.workers, quiet=args.quiet, invalidation_mode=args.invalidation_mode ) if success: print(\\"Compilation successful!\\") else: print(\\"Compilation failed for some files\\") except Exception as e: print(f\\"Error during compilation: {e}\\") if __name__ == \'__main__\': main()"},{"question":"# Question: Implementing and Training a Restricted Boltzmann Machine with BernoulliRBM Objective: You are required to implement a pipeline that leverages the `BernoulliRBM` class from Scikit-learn to preprocess, train, and transform a dataset. You will then use this transformed dataset to train a logistic regression classifier and evaluate its performance. Requirements: 1. **Input Data Handling**: - The input to your function would be a 2D NumPy array `X` representing the features and a 1D NumPy array `y` representing the labels. - Ensure that the values in `X` are scaled between 0 and 1. 2. **RBM Training**: - Initialize a `BernoulliRBM` object with the following parameters: - `n_components=100`: Number of binary hidden units. - `learning_rate=0.01`: Learning rate for weight updates. - `n_iter=10`: Number of iterations for training. - Fit the `BernoulliRBM` model to the input data `X`. 3. **Data Transformation**: - Transform the input data `X` using the trained `BernoulliRBM` model. 4. **Logistic Regression Training**: - Use the transformed data to train a logistic regression classifier. - Use Scikit-learnâ€™s `LogisticRegression` with default parameters for this task. 5. **Performance Evaluation**: - Evaluate the performance of the logistic regression classifier using accuracy score. - Return the accuracy score as the output of your function. Function Signature: ```python def rbm_logistic_classification(X: np.ndarray, y: np.ndarray) -> float: pass ``` Example Usage: ```python import numpy as np # Sample Data X = np.random.rand(100, 20) y = (np.random.rand(100) > 0.5).astype(int) accuracy = rbm_logistic_classification(X, y) print(f\\"Accuracy: {accuracy:.2f}\\") ``` Constraints: - The input feature matrix `X` will have dimensions `(n_samples, n_features)` with `n_samples <= 10000` and `n_features <= 1000`. - Binary labels `y` will be provided as a 1D array of 0s and 1s with length equal to `n_samples`. Notes: - You must use Scikit-learn\'s `BernoulliRBM` and `LogisticRegression` classes for this task. - The logistic regression classifier should be trained on the data transformed by the RBM. - Ensure the data is scaled appropriately before fitting the model. - The accuracy score should be returned as a float.","solution":"import numpy as np from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import MinMaxScaler from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def rbm_logistic_classification(X: np.ndarray, y: np.ndarray) -> float: # Scale the features to [0, 1] scaler = MinMaxScaler() X_scaled = scaler.fit_transform(X) # Initialize BernoulliRBM and Logistic Regression rbm = BernoulliRBM(n_components=100, learning_rate=0.01, n_iter=10) logistic = LogisticRegression() # Create a pipeline classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42) # Train the model classifier.fit(X_train, y_train) # Predict with the model y_pred = classifier.predict(X_test) # Measure the accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"You are required to implement a function using PyTorch\'s `torch.random` module. The function should initialize a neural network\'s weights and biases with random values. The neural network has a single hidden layer. The function should also ensure reproducibility by setting a random seed. Finally, the function should verify certain properties of the random initialization, as outlined below. Function Signature ```python def initialize_network(seed: int, input_size: int, hidden_size: int, output_size: int) -> dict: Initializes the weights and biases for a neural network with one hidden layer using PyTorch\'s random utilities. Parameters: - seed (int): The seed for random number generation. - input_size (int): Size of the input layer. - hidden_size (int): Size of the hidden layer. - output_size (int): Size of the output layer. Returns: - dict: A dictionary containing: - \\"W1\\": The weights of the first layer as a torch.Tensor of shape (input_size, hidden_size). - \\"b1\\": The biases of the first layer as a torch.Tensor of shape (hidden_size). - \\"W2\\": The weights of the second layer as a torch.Tensor of shape (hidden_size, output_size). - \\"b2\\": The biases of the second layer as a torch.Tensor of shape (output_size). ``` Requirements 1. Use `torch.manual_seed()` to set the seed for random number generation. 2. Use `torch.randn()` to generate the weights and biases for the layers. 3. The weights and biases should follow these shapes: - `W1`: (input_size, hidden_size) - `b1`: (hidden_size) - `W2`: (hidden_size, output_size) - `b2`: (output_size) 4. Ensure that consecutive runs with the same seed produce the same weights and biases. 5. Add some assertions to verify the shapes and reproducibility of the results. Example ```python # Example usage network_params = initialize_network(seed=42, input_size=3, hidden_size=5, output_size=2) print(network_params[\\"W1\\"].shape) # Expected: torch.Size([3, 5]) print(network_params[\\"b1\\"].shape) # Expected: torch.Size([5]) print(network_params[\\"W2\\"].shape) # Expected: torch.Size([5, 2]) print(network_params[\\"b2\\"].shape) # Expected: torch.Size([2]) ``` Constraints - You must use only PyTorch functions and no other external libraries. - Your solution should be computationally efficient.","solution":"import torch def initialize_network(seed: int, input_size: int, hidden_size: int, output_size: int) -> dict: Initializes the weights and biases for a neural network with one hidden layer using PyTorch\'s random utilities. Parameters: - seed (int): The seed for random number generation. - input_size (int): Size of the input layer. - hidden_size (int): Size of the hidden layer. - output_size (int): Size of the output layer. Returns: - dict: A dictionary containing: - \\"W1\\": The weights of the first layer as a torch.Tensor of shape (input_size, hidden_size). - \\"b1\\": The biases of the first layer as a torch.Tensor of shape (hidden_size). - \\"W2\\": The weights of the second layer as a torch.Tensor of shape (hidden_size, output_size). - \\"b2\\": The biases of the second layer as a torch.Tensor of shape (output_size). torch.manual_seed(seed) W1 = torch.randn(input_size, hidden_size) b1 = torch.randn(hidden_size) W2 = torch.randn(hidden_size, output_size) b2 = torch.randn(output_size) params = { \\"W1\\": W1, \\"b1\\": b1, \\"W2\\": W2, \\"b2\\": b2 } # Assertions to verify the shapes assert params[\\"W1\\"].shape == (input_size, hidden_size) assert params[\\"b1\\"].shape == (hidden_size,) assert params[\\"W2\\"].shape == (hidden_size, output_size) assert params[\\"b2\\"].shape == (output_size,) return params"},{"question":"**Objective**: To create an enhanced visual representation of the diamonds dataset using seaborn and matplotlib, demonstrating proficiency with both libraries. **Task**: 1. Load the `diamonds` dataset from seaborn. 2. Create a main figure with two subfigures side by side. 3. In the first subfigure, create a scatter plot using seaborn\'s `Plot`: - X-axis should represent \\"carat\\". - Y-axis should represent \\"price\\". - Use dots to represent the points. 4. In the second subfigure, create a series of histograms that represent the distribution of diamond prices for each category of diamond cut: - X-axis should represent \\"price\\". - Each row of the histogram grid should correspond to a different category of diamond cut. - Apply a log scale to the X-axis. 5. Add a custom annotation to the first subfigure to highlight the region where the carat is between 0.5 and 1.5. **Input**: No inputs are required from the user. **Output**: Display the created figure with the above specifications. **Constraints**: - You must use seaborn\'s `Plot` object and integrate it with matplotlib\'s figure and subfigure functionalities. - You should customize the plot by adding a rectangle and text annotation as mentioned above. **Performance Requirement**: The solution should generate the plots and annotations efficiently without unnecessary computations. **Example Visualization**: The resulting output should be a two-part plot with a scatter plot on the left and a price distribution graph on the right, accurately reflecting the requested customizations. ```python # Import required libraries import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the main figure with subfigures f = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # First subfigure: Scatter plot using seaborn Plot scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) scatter_plot.on(sf1).plot() # Add custom annotation to the first subfigure ax = sf1.axes[0] highlight_rect = mpl.patches.Rectangle( xy=(0.5, 0), width=1, height=diamonds[\\"price\\"].max(), color=\\"C1\\", alpha=0.2, transform=ax.transData, clip_on=False ) ax.add_artist(highlight_rect) ax.text( x=0.5 + 1 / 2, y=diamonds[\\"price\\"].max() - diamonds[\\"price\\"].max() / 10, s=\\"Highlighted Region\\", size=12, ha=\\"center\\", va=\\"center\\", backgroundcolor=\\"white\\" ) # Second subfigure: Histogram of diamond prices by cut (so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2)) # Display the plot f.tight_layout() plt.show() ```","solution":"# Import required libraries import seaborn as sns import seaborn.objects as so import matplotlib as mpl import matplotlib.pyplot as plt def create_figure(): Create and display the figure with a scatter plot and histogram as specified in the task. # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the main figure with subfigures f = mpl.figure.Figure(figsize=(14, 7), dpi=100, layout=\\"constrained\\") sf1, sf2 = f.subfigures(1, 2) # First subfigure: Scatter plot using seaborn Plot scatter_plot = so.Plot(diamonds, \\"carat\\", \\"price\\").add(so.Dots()) scatter_plot.on(sf1).plot() # Add custom annotation to the first subfigure ax = sf1.axes[0] highlight_rect = mpl.patches.Rectangle( xy=(0.5, 0), width=1, height=diamonds[\\"price\\"].max(), color=\\"C1\\", alpha=0.2, transform=ax.transData, clip_on=False ) ax.add_artist(highlight_rect) ax.text( x=0.5 + 1 / 2, y=diamonds[\\"price\\"].max() - diamonds[\\"price\\"].max() / 10, s=\\"Highlighted Region\\", size=12, ha=\\"center\\", va=\\"center\\", backgroundcolor=\\"white\\" ) # Second subfigure: Histogram of diamond prices by cut (so.Plot(diamonds, x=\\"price\\") .add(so.Bars(), so.Hist()) .facet(row=\\"cut\\") .scale(x=\\"log\\") .share(y=False) .on(sf2)) # Display the plot f.tight_layout() plt.show()"},{"question":"Objective: Implement a pipeline in scikit-learn that reduces the dimensionality of a dataset using PCA and random projections, then visualizes the results. This will assess your understanding of scikit-learn\'s unsupervised dimensionality reduction techniques and pipeline construction. Task: 1. **Load Data**: Use the Iris dataset from sklearn datasets. 2. **Preprocessing**: - Standardize the dataset using `StandardScaler`. 3. **Dimensionality Reduction**: - Apply PCA to reduce the dataset to 2 principal components. - Apply a random projection method to reduce the dataset to 2 dimensions. 4. **Visualization**: - Plot the results of PCA and the random projection on a 2D scatter plot, coloring the points by their class in the Iris dataset. Specifications: 1. **Input**: None - The function should internally load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Output**: - A single figure with two subplots: one for PCA and one for random projection results. 3. **Function Signature**: ```python def dimensionality_reduction_viz(): pass ``` 4. **Performance requirements**: - Ensure that the entire pipeline (preprocessing and dimensionality reduction) is efficient and utilizes the scikit-learn\'s optimized functions wherever possible. Constraints: - You must use scikit-learn for all steps (data loading, preprocessing, dimensionality reduction). - The visualizations must be created using matplotlib. Example Output: A figure with two subplots: the left subplot showing PCA results and the right subplot showing random projection results. Each point should be colored based on its class in the Iris dataset. Guidelines: 1. **Import libraries**: ```python import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection ``` 2. **Load the Iris dataset**: ```python data = load_iris() X, y = data.data, data.target ``` 3. **Standardize the data**: ```python scaler = StandardScaler() X_scaled = scaler.fit_transform(X) ``` 4. **Apply PCA**: ```python pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) ``` 5. **Apply Random Projection**: ```python rp = GaussianRandomProjection(n_components=2) X_rp = rp.fit_transform(X_scaled) ``` 6. **Create the scatter plots**: ```python plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y) plt.title(\'PCA\') plt.subplot(1, 2, 2) plt.scatter(X_rp[:, 0], X_rp[:, 1], c=y) plt.title(\'Random Projection\') plt.show() ```","solution":"import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection def dimensionality_reduction_viz(): # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Apply PCA to reduce dimensions to 2 pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) # Apply random projection to reduce dimensions to 2 rp = GaussianRandomProjection(n_components=2) X_rp = rp.fit_transform(X_scaled) # Create a figure with 2 subplots for PCA and random projection plt.figure(figsize=(12, 6)) # Subplot for PCA plt.subplot(1, 2, 1) plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\') plt.title(\'PCA\') plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') # Subplot for Random Projection plt.subplot(1, 2, 2) plt.scatter(X_rp[:, 0], X_rp[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\') plt.title(\'Random Projection\') plt.xlabel(\'Random Projection 1\') plt.ylabel(\'Random Projection 2\') # Show the plots plt.tight_layout() plt.show()"},{"question":"**Problem Statement:** You are tasked with developing a function that processes an audio sample and converts it from stereo to mono, then applies a custom filter to amplify specific frequencies. Finally, the function should change the sample rate of the audio and return the processed audio data. To achieve this, you need to use multiple functions from the `audioop` module as described below: 1. Convert a stereo sample to mono. 2. Apply a multiplicative factor to amplify the audio signal. 3. Change the frame rate of the audio sample. Your implementation should encapsulate these steps into a single function. **Function Signature:** ```python def process_audio(sample: bytes, width: int, lfactor: float, rfactor: float, amplification_factor: float, nchannels: int, inrate: int, outrate: int) -> bytes: pass ``` **Detailed Steps and Requirements:** 1. **Convert Stereo to Mono**: - Use `audioop.tomono` to convert the stereo sample to mono. - Input Parameters: `sample` (the original audio sample), `width` (sample width in bytes), `lfactor` (left channel factor), `rfactor` (right channel factor). 2. **Amplify the Mono Signal**: - Use `audioop.mul` to amplify the mono audio. - Input Parameters: `mono_sample` (the resulting mono audio sample), `width` (sample width in bytes), `amplification_factor` (multiplicative factor). 3. **Change the Frame Rate**: - Use `audioop.ratecv` to change the sample rate of the audio. - Input Parameters: `amplified_sample` (the amplified mono sample), `width` (sample width in bytes), `nchannels` (number of channels, which should be 1 for mono), `inrate` (input frame rate), `outrate` (output frame rate). - The initial state passed to `ratecv` should be `None`. **Expected Input:** - `sample`: A bytes-like object containing the audio sample. - `width`: An integer representing the sample width in bytes (`1`, `2`, `3`, or `4`). - `lfactor`: A float for amplifying the left channel when converting to mono. - `rfactor`: A float for amplifying the right channel when converting to mono. - `amplification_factor`: A float for the multiplicative amplification of the mono audio. - `nchannels`: An integer representing the number of channels in the original audio (should be 2 for stereo). - `inrate`: An integer representing the input frame rate (number of samples per second). - `outrate`: An integer representing the desired output frame rate. **Expected Output:** - Returns a bytes object containing the processed audio sample. **Constraints:** - All input parameters are assumed to be valid and correctly formatted. - The audio data is represented as a bytes-like object with signed integer samples. **Example:** ```python original_sample = b\'x01x02x00x03x02x01...\' sample_width = 2 left_factor = 1.0 right_factor = 1.0 amplification_factor = 1.5 num_channels = 2 input_rate = 44100 output_rate = 22050 processed_sample = process_audio(original_sample, sample_width, left_factor, right_factor, amplification_factor, num_channels, input_rate, output_rate) ``` In this example, the `original_sample` is a stereo audio sample. The function will first convert it to mono using `left_factor` and `right_factor`, then amplify it by `amplification_factor`, and finally adjust the frame rate from `input_rate` to `output_rate`.","solution":"import audioop def process_audio(sample: bytes, width: int, lfactor: float, rfactor: float, amplification_factor: float, nchannels: int, inrate: int, outrate: int) -> bytes: Processes the given audio sample by converting it from stereo to mono, amplifying it, and changing its sample rate. # Step 1: Convert stereo to mono if nchannels != 2: raise ValueError(\\"Input audio must be stereo (2 channels) for this process.\\") mono_sample = audioop.tomono(sample, width, lfactor, rfactor) # Step 2: Amplify the mono signal amplified_sample = audioop.mul(mono_sample, width, amplification_factor) # Step 3: Change sample rate converted_sample, state = audioop.ratecv(amplified_sample, width, 1, inrate, outrate, None) return converted_sample"},{"question":"**Objective:** Write a function that demonstrates your understanding of seaborn\'s `sns.regplot` functionality by creating specific types of regression plots for a given dataset. **Function Specification:** ```python def create_regression_plots(data): Given a dataset, create and save specific types of regression plots using seaborn\'s sns.regplot. Parameters: - data (pd.DataFrame): A pandas DataFrame containing the dataset. Returns: None. The function should save the plots to disk with specific filenames. ``` **Requirements:** 1. **Plot a simple linear regression:** - Use `weight` as the x-axis and `acceleration` as the y-axis. - Save the plot as `linear_regression.png`. 2. **Plot a polynomial regression (order 2):** - Use `weight` as the x-axis and `mpg` as the y-axis. - Save the plot as `polynomial_regression.png`. 3. **Plot a log-linear regression:** - Use `displacement` as the x-axis and `mpg` as the y-axis. - Save the plot as `log_linear_regression.png`. 4. **Plot a locally-weighted (LOWESS) smoother:** - Use `horsepower` as the x-axis and `mpg` as the y-axis. - Save the plot as `lowess_regression.png`. 5. **Plot a logistic regression (binary response variable):** - Use `weight` as the x-axis and a binary version of `origin` indicating if the car is from the USA as the y-axis (`from_usa`). - Save the plot as `logistic_regression.png`. 6. **Plot a robust regression:** - Use `horsepower` as the x-axis and `weight` as the y-axis. - Save the plot as `robust_regression.png`. **Constraints:** - The dataset provided will be similar to the `mpg` dataset used in the provided documentation. - Ensure all plots are saved with appropriate filenames as specified. - Use appropriate titles, axis labels, and any other customization to improve plot readability. **Performance:** - Your solution should efficiently handle datasets of size up to 1000 rows. --- **Example Usage:** ```python # Assuming you have a pandas DataFrame `mpg` following the structure given in the documentation: import pandas as pd mpg = pd.read_csv(\'path_to_mpg_dataset.csv\') # Call the function create_regression_plots(mpg) ``` This function should output the following image files in the current working directory: - `linear_regression.png` - `polynomial_regression.png` - `log_linear_regression.png` - `lowess_regression.png` - `logistic_regression.png` - `robust_regression.png`","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def create_regression_plots(data): # Simple linear regression sns.regplot(x=\'weight\', y=\'acceleration\', data=data, ci=None) plt.title(\'Simple Linear Regression\') plt.savefig(\'linear_regression.png\') plt.clf() # Polynomial regression (order 2) sns.regplot(x=\'weight\', y=\'mpg\', data=data, order=2, ci=None) plt.title(\'Polynomial Regression (Order 2)\') plt.savefig(\'polynomial_regression.png\') plt.clf() # Log-linear regression sns.regplot(x=\'displacement\', y=\'mpg\', data=data, logx=True, ci=None) plt.title(\'Log-Linear Regression\') plt.savefig(\'log_linear_regression.png\') plt.clf() # Locally-weighted (LOWESS) smoother sns.regplot(x=\'horsepower\', y=\'mpg\', data=data, lowess=True, ci=None) plt.title(\'LOWESS Regression\') plt.savefig(\'lowess_regression.png\') plt.clf() # Logistic regression data[\'from_usa\'] = data[\'origin\'].apply(lambda x: 1 if x == \'USA\' else 0) sns.regplot(x=\'weight\', y=\'from_usa\', data=data, logistic=True, ci=None) plt.title(\'Logistic Regression\') plt.savefig(\'logistic_regression.png\') plt.clf() # Robust regression sns.regplot(x=\'horsepower\', y=\'weight\', data=data, robust=True, ci=None) plt.title(\'Robust Regression\') plt.savefig(\'robust_regression.png\') plt.clf()"},{"question":"Objective Implement a class `ArrayStatistics` that takes an array of integers and provides methods to perform common statistical analyses on the data. Class `ArrayStatistics` **Attributes:** - `data`: An `array` of integers initialized with type code `\'i\'`. **Methods:** 1. `__init__(self, elements: list[int]) -> None`: - Initializes the `data` attribute with the provided list of integers. - Raises a `TypeError` if any element in the list is not an integer. 2. `mean(self) -> float`: - Returns the mean (average) of the elements in the array. 3. `median(self) -> float`: - Returns the median of the elements in the array. If the number of elements is even, returns the average of the two middle elements. 4. `mode(self) -> list[int]`: - Returns a list of the most frequently occurring elements (mode) in the array. If there are multiple modes, return them all in ascending order. 5. `variance(self) -> float`: - Returns the variance of the elements in the array. 6. `standard_deviation(self) -> float`: - Returns the standard deviation of the elements in the array. Example ```python from array import array class ArrayStatistics: def __init__(self, elements: list[int]) -> None: # Your implementation here def mean(self) -> float: # Your implementation here def median(self) -> float: # Your implementation here def mode(self) -> list[int]: # Your implementation here def variance(self) -> float: # Your implementation here def standard_deviation(self) -> float: # Your implementation here # Example Usage: stats = ArrayStatistics([1, 2, 2, 3, 4]) print(stats.mean()) # Output: 2.4 print(stats.median()) # Output: 2 print(stats.mode()) # Output: [2] print(stats.variance()) # Output: 1.04 print(stats.standard_deviation()) # Output: 1.02 (approx) ``` Constraints - Do not use any external libraries for statistical calculations; implement them using basic Python. - Handle edge cases gracefully (e.g., empty list of elements). Notes - Ensure the class is well-documented with docstrings. - Include error handling and validation for the provided methods.","solution":"from array import array from collections import Counter from math import sqrt class ArrayStatistics: def __init__(self, elements: list[int]) -> None: Initializes the ArrayStatistics with a list of integers. :param elements: list of integers :raises TypeError: if any element in the list is not an integer if not all(isinstance(x, int) for x in elements): raise TypeError(\\"All elements must be integers.\\") self.data = array(\'i\', elements) def mean(self) -> float: Returns the mean (average) of the array elements. :return: float return sum(self.data) / len(self.data) if self.data else 0.0 def median(self) -> float: Returns the median of the array elements. If the number of elements is even, returns the average of the two middle elements. :return: float n = len(self.data) if n == 0: return 0.0 sorted_data = sorted(self.data) mid = n // 2 if n % 2 == 0: return (sorted_data[mid - 1] + sorted_data[mid]) / 2 else: return sorted_data[mid] def mode(self) -> list[int]: Returns a list of the most frequently occurring elements (mode) in the array. If there are multiple modes, returns them all in ascending order. :return: list[int] if not self.data: return [] frequency = Counter(self.data) max_freq = max(frequency.values()) modes = [k for k, v in frequency.items() if v == max_freq] return sorted(modes) def variance(self) -> float: Returns the variance of the array elements. :return: float if len(self.data) == 0: return 0.0 mean_value = self.mean() return sum((x - mean_value) ** 2 for x in self.data) / len(self.data) def standard_deviation(self) -> float: Returns the standard deviation of the array elements. :return: float return sqrt(self.variance())"},{"question":"**Question: Configuring and Testing PyTorch Environment Variables** In this exercise, you will be required to configure and test PyTorch\'s behavior with various environment variables. # Objective Write a Python script that: 1. Configures the environment variables `TORCH_FORCE_WEIGHTS_ONLY_LOAD`, `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD`, and `TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT`. 2. Loads a simple PyTorch model with these configurations. 3. Verifies the behavior changes as expected using assertions. # Requirements 1. **Environment Variable Setup**: - Set `TORCH_FORCE_WEIGHTS_ONLY_LOAD` to \\"yes\\". - Set `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD` to \\"no\\". - Set `TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT` to \\"5\\" seconds. 2. **Model Definition and Saving**: - Define a simple neural network model, e.g., a `torch.nn.Sequential` model with a single `torch.nn.Linear` layer. - Save the model parameters and the entire model using `torch.save`. 3. **Model Loading with Verified Configurations**: - Load the model parameters with `weights_only=True`. - Load the entire model with `weights_only=False`. - Include assertions to ensure the environment variable configurations influence the behavior as expected. # Constraints - You must handle exceptions if any environment variable misconfiguration occurs. - Precision and clarity in assertions are essential to validate the outcomes effectively. # Example Output Format Upon running your script, it should display confirmation messages and pass assertions without errors. # Expected Input and Output Formats: No input from a user is required. The script should execute independently. Example script outline: ```python import os import torch import torch.nn as nn # STEP 1: Environment Variable Setup os.environ[\'TORCH_FORCE_WEIGHTS_ONLY_LOAD\'] = \'yes\' os.environ[\'TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\'] = \'no\' os.environ[\'TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\'] = \'5\' # Verify environment variables are set correctly assert os.environ[\'TORCH_FORCE_WEIGHTS_ONLY_LOAD\'] == \'yes\' assert os.environ[\'TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\'] == \'no\' assert os.environ[\'TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\'] == \'5\' # STEP 2: Model Definition and Saving class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer = nn.Linear(10, 2) def forward(self, x): return self.layer(x) model = SimpleModel() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Save the model\'s state_dict (weights only) and full model torch.save(model.state_dict(), \'model_weights.pth\') torch.save({\'model\': model, \'optimizer\': optimizer.state_dict()}, \'full_model.pth\') # STEP 3: Model Loading with Verified Configurations # Load weights only state_dict = torch.load(\'model_weights.pth\', weights_only=True) assert state_dict is not None, \\"Failed to load model weights\\" # Load full model checkpoint = torch.load(\'full_model.pth\', weights_only=False) assert \'model\' in checkpoint, \\"Model not found in checkpoint\\" assert \'optimizer\' in checkpoint, \\"Optimizer not found in checkpoint\\" print(\\"All assertions passed and models loaded successfully.\\") ``` # Note: - Ensure your script runs without any errors when executed in an environment where PyTorch is installed.","solution":"import os import torch import torch.nn as nn # STEP 1: Environment Variable Setup os.environ[\'TORCH_FORCE_WEIGHTS_ONLY_LOAD\'] = \'yes\' os.environ[\'TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\'] = \'no\' os.environ[\'TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\'] = \'5\' # Verify environment variables are set correctly assert os.environ[\'TORCH_FORCE_WEIGHTS_ONLY_LOAD\'] == \'yes\' assert os.environ[\'TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\'] == \'no\' assert os.environ[\'TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\'] == \'5\' # STEP 2: Model Definition and Saving class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layer = nn.Linear(10, 2) def forward(self, x): return self.layer(x) model = SimpleModel() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # Save the model\'s state_dict (weights only) and full model torch.save(model.state_dict(), \'model_weights.pth\') torch.save({\'model\': model.state_dict(), \'optimizer\': optimizer.state_dict()}, \'full_model.pth\') # STEP 3: Model Loading with Verified Configurations # Load weights only state_dict = torch.load(\'model_weights.pth\', weights_only=True) assert state_dict is not None, \\"Failed to load model weights\\" # Load full model checkpoint = torch.load(\'full_model.pth\', weights_only=False) assert \'model\' in checkpoint, \\"Model not found in checkpoint\\" assert \'optimizer\' in checkpoint, \\"Optimizer not found in checkpoint\\" print(\\"All assertions passed and models loaded successfully.\\")"},{"question":"Implement a TorchScript-compatible neural network model class with a fully connected architecture. Your model should have the following specifications: 1. **Class Structure**: - The model class should be named `MyNeuralNet`. - The class should inherit from `torch.nn.Module`. 2. **Attributes**: - The class should have three Linear layers. - The first linear layer maps an input of size 100 to 50. - The second linear layer maps an input of size 50 to 25. - The third linear layer maps an input of size 25 to 10. 3. **Methods**: - A `forward` method which takes a `torch.Tensor` input and passes it through the three layers sequentially, applying a ReLU activation function after the first two layers. - A method `init_weights` that initializes the weights and biases of the network layers to values from a normal distribution with mean 0 and standard deviation 0.01. 4. **Type Annotations**: - Proper type annotations should be added to all class attributes and method signatures. 5. **Testing**: - After defining the class, create an instance of `MyNeuralNet` and script it using `torch.jit.script`. - Verify the scripted module by passing a random input tensor of appropriate shape and printing the output. # Input - None. # Output - Print the output tensor after passing a random input through the scripted `MyNeuralNet` module. # Example ```python import torch import torch.nn as nn class MyNeuralNet(nn.Module): def __init__(self): super(MyNeuralNet, self).__init__() self.fc1 = nn.Linear(100, 50) self.fc2 = nn.Linear(50, 25) self.fc3 = nn.Linear(25, 10) self.init_weights() def init_weights(self): for layer in [self.fc1, self.fc2, self.fc3]: torch.nn.init.normal_(layer.weight, mean=0.0, std=0.01) torch.nn.init.normal_(layer.bias, mean=0.0, std=0.01) def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]: x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x model = MyNeuralNet() scripted_model = torch.jit.script(model) test_input = torch.randn(1, 100) print(scripted_model(test_input)) ``` # Notes - Ensure compatibility with TorchScript by following the type annotation rules and TorchScript specific guidelines. - Although `init_weights` might not be called in the scripting workflow, it should be defined correctly for practical use cases. - Manually test the final scripted model with valid input data to ensure it functions correctly.","solution":"import torch import torch.nn as nn import torch.jit class MyNeuralNet(nn.Module): def __init__(self): super(MyNeuralNet, self).__init__() self.fc1: nn.Linear = nn.Linear(100, 50) self.fc2: nn.Linear = nn.Linear(50, 25) self.fc3: nn.Linear = nn.Linear(25, 10) self.init_weights() def init_weights(self) -> None: for layer in [self.fc1, self.fc2, self.fc3]: torch.nn.init.normal_(layer.weight, mean=0.0, std=0.01) torch.nn.init.normal_(layer.bias, mean=0.0, std=0.01) def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.fc1(x)) x = torch.relu(self.fc2(x)) x = self.fc3(x) return x # Create an instance of MyNeuralNet model = MyNeuralNet() # Script the model for TorchScript compatibility scripted_model = torch.jit.script(model) # Pass a random input tensor through the scripted model test_input = torch.randn(1, 100) output = scripted_model(test_input) print(output)"},{"question":"You are required to implement a Python function using PyTorch and its CUDA Graphs functionality. Your function should involve multiple CUDA Graph captures and demonstrate the ability to manage memory efficiently with conditional operations. # Function Signature ```python import torch @torch.compile(mode=\\"reduce-overhead\\") def customized_operations(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` # Task 1. **Input:** - A single 1D tensor `input_tensor` of size ( n ) where ( n ) is in the range [2, 1000] and dtype is `torch.float32`. The tensor is always located on a CUDA device. 2. **Output:** - A new tensor which is the result of applying multiple graph operations efficiently using CUDA Graphs. # Requirements - **Memory Management:** - Implement operations that involve creating multiple CUDA Graphs. - Ensure memory is managed using shared pools as described in the documentation. - **Conditional Operations:** - Your function should incorporate conditional statements that trigger different graph paths and ensure these follow the memory constraints laid out by CUDA Graphs. - Example condition: If the sum of the elements in the input tensor is greater than a certain value, perform a different set of operations. - **Dynamic Shape Support:** - Handle input tensors with different shapes across different function calls efficiently. # Example ```python input_tensor = torch.ones(10, device=\'cuda\') output_tensor = customized_operations(input_tensor) print(output_tensor.tolist()) ``` # Constraints - You must handle memory efficiently to avoid unnecessary overhead. - Ensure that the function runs without error under the PyTorch CUDA Graphs constraints. - Your solution should not contain any CPU-only operations within the graphs. **Note:** You may use dummy operations for the purpose of this task. For example, multiplying or adding tensors. The goal is to demonstrate the usage and efficiency of CUDA graph management.","solution":"import torch def customized_operations(input_tensor: torch.Tensor) -> torch.Tensor: This function applies multiple CUDA Graph captures with memory management and conditional operations on the input tensor. assert input_tensor.is_cuda, \\"Input tensor must be on a CUDA device\\" device = input_tensor.device graph1, graph2 = torch.cuda.CUDAGraph(), torch.cuda.CUDAGraph() stream = torch.cuda.Stream() with torch.cuda.stream(stream): static_output1 = input_tensor + 5 # Dummy operation 1 static_output2 = input_tensor * 2 # Dummy operation 2 torch.cuda.synchronize() with torch.cuda.graph(graph1): result_tensor1 = static_output1 + static_output2 with torch.cuda.graph(graph2): result_tensor2 = static_output1 - static_output2 sum_input = input_tensor.sum().item() threshold = 100.0 if sum_input > threshold: graph1.replay() return result_tensor1 else: graph2.replay() return result_tensor2"},{"question":"# Pseudo-terminal Control in Python **Objective**: Assess the ability to use Python\'s `pty` module to manage pseudo-terminal-based interactions programmatically. # Problem Statement: You are tasked with implementing an interactive Python program that can spawn a shell process and log both the input to the shell and the output from the shell. The log should be written to a specified file. The program should handle read operations from both the terminal and the shell, handle process cleanup, and properly log all activities. # Requirements: 1. Implement the function `log_terminal_interaction(shell_command: List[str], log_file: str) -> int`: - **Input**: - `shell_command`: A list of strings representing the shell command to be executed. For example: `[\\"/bin/bash\\"]` to start the Bash shell. - `log_file`: A string representing the path to the log file. - **Output**: An integer representing the exit status of the shell process. - **Behavior**: - The function should use `pty.spawn()` to start the shell process and connect its controlling terminal with the standard I/O. - The function should log all activity to the specified `log_file`. - The function should handle EOF and other edge cases gracefully. 2. Ensure the function properly buffers and writes data to the log file in real-time. 3. The function must return the exit status of the spawned process. # Constraints: - You must use the `pty` module and its functions for handling pseudo-terminal operations. - You should follow the practices of reading and writing data efficiently to avoid performance bottlenecks. - Ensure that the program can handle varying shell commands and can be extended for different use cases. # Example: ```python import os import pty def log_terminal_interaction(shell_command, log_file): with open(log_file, \'wb\') as log: def read_from_shell(fd): data = os.read(fd, 1024) log.write(data) return data def read_from_stdin(fd): data = os.read(fd, 1024) if data: log.write(b\\"STDIN: \\" + data) return data exit_status = pty.spawn(shell_command, read_from_shell, read_from_stdin) return os.waitstatus_to_exitcode(exit_status) ``` 1. Test your implementation with `log_terminal_interaction([\\"/bin/bash\\"], \\"session.log\\")`. 2. Verify the log file is created and contains both the shell session\'s input and output. # Performance Requirements: - The I/O operations should be buffered and written efficiently to ensure that the recorder does not lag. - The solution should be robust in handling large volumes of terminal interactions. **Good Luck!**","solution":"import os import pty def log_terminal_interaction(shell_command, log_file): Spawns a shell process and logs both input and output to the specified log file. Args: shell_command (list of str): Shell command to be executed, e.g., [\\"/bin/bash\\"]. log_file (str): Path to the log file. Returns: int: Exit status of the shell process. with open(log_file, \'wb\') as log: def read_from_pty(fd): data = os.read(fd, 1024) log.write(data) log.flush() # Ensure data is written to the log in real-time return data exit_status = pty.spawn(shell_command, read_from_pty) return os.waitstatus_to_exitcode(exit_status)"},{"question":"You are to write Python code to solve the following tasks using the seaborn library. Task 1: Basic Swarm Plot 1. Load the `tips` dataset using seaborn. 2. Create a swarm plot showing the distribution of `total_bill`. Task 2: Comparisons in Swarm Plot 1. Create a swarm plot that shows `total_bill` distribution split by `day`. 2. Change the orientation to vertical, displaying `day` on the x-axis and `total_bill` on the y-axis. Task 3: Adding Hue 1. Enhance the plot from Task 2 by adding the `sex` variable as `hue`. Task 4: Customizing Plot 1. Create a plot similar to Task 3 but change the size of the points to `4` and marker to \'x\'. 2. Apply the `deep` palette to the plot. Task 5: Faceted Plot 1. Create a faceted plot using `catplot` kind of `swarm` plotting `total_bill` against `time` split by `day` and colored by `sex`. # Constraints and Requirements: 1. You should use the seaborn library for all plots. 2. The plots need to be rendered correctly without any overlaps or warnings. 3. Ensure that the plots are well-labeled and interpretable. 4. The final solution should be submitted as a Python script (.py) or a Jupyter notebook (.ipynb). **Input Format:** There is no input required from the user beyond ensuring seaborn and its dependencies are installed. **Output Format:** Generate and display the required plots as per the tasks outlined above. **Performance Requirements:** Ensure that each plot is generated efficiently without unnecessary computations. **Hints:** - Use `sns.load_dataset(\\"tips\\")` to load the dataset. - Refer to seaborn\'s documentation for detailed usage of functions like `sns.swarmplot(...)` and `sns.catplot(...)`. **Expected Competency:** - Understanding of seaborn plotting functions and their parameters. - Ability to manipulate and enhance plots using seaborn customization options. - Familiarity with working in a Jupyter notebook or a Python script for data visualization tasks.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Task 1: Basic Swarm Plot def basic_swarm_plot(): tips = sns.load_dataset(\\"tips\\") sns.swarmplot(x=tips[\\"total_bill\\"]) plt.title(\\"Distribution of Total Bill\\") plt.xlabel(\\"Total Bill\\") plt.show() # Task 2: Comparisons in Swarm Plot def comparison_swarm_plot(): tips = sns.load_dataset(\\"tips\\") sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", data=tips) plt.title(\\"Distribution of Total Bill by Day\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() # Task 3: Adding Hue def hue_swarm_plot(): tips = sns.load_dataset(\\"tips\\") sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips) plt.title(\\"Distribution of Total Bill by Day and Sex\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() # Task 4: Customizing Plot def customized_swarm_plot(): tips = sns.load_dataset(\\"tips\\") sns.swarmplot(x=\\"day\\", y=\\"total_bill\\", hue=\\"sex\\", data=tips, size=4, marker=\'x\', palette=\\"deep\\") plt.title(\\"Distribution of Total Bill by Day and Sex with Customizations\\") plt.xlabel(\\"Day\\") plt.ylabel(\\"Total Bill\\") plt.show() # Task 5: Faceted Plot def faceted_swarm_plot(): tips = sns.load_dataset(\\"tips\\") g = sns.catplot(x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", kind=\\"swarm\\", data=tips) g.fig.suptitle(\\"Faceted Swarm Plot of Total Bill by Time, Day, and Sex\\", y=1.05) g.set_axis_labels(\\"Time\\", \\"Total Bill\\") plt.show()"},{"question":"**Question:** You are given a DataFrame representing a collection of records. Each record consists of multiple types of data including integers, floats, dates, and strings. Your task is to implement a function `process_dataframe(df: pd.DataFrame) -> pd.DataFrame` that performs the following operations: 1. **Memory Reporting**: Compute and print the memory usage of the DataFrame both by default and in \'deep\' mode. 2. **Handle Missing Values**: Identify and replace any missing values in integer columns with the average of the column. Ensure that missing values are only replaced if their column contains non-missing values. 3. **User Defined Transformation**: Apply a user-defined function that normalizes numerical columns (integer and float) by subtracting their mean and dividing by their standard deviation. Ensure this does not mutate the original DataFrame. 4. **Boolean Conditions**: Identify any columns that contain string values and create new boolean columns to indicate whether the string contains the substring \'error\' (case insensitive). 5. **Thread Safety**: Ensure thread safety by implementing appropriate measures to avoid issues during DataFrame manipulation. **Function Signature:** ```python import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` **Input:** - `df`: A pandas DataFrame containing various types of data including integers, floats, dates, and strings. **Output:** - A transformed DataFrame with the applied operations as specified. **Constraints:** - You can assume the DataFrame will have columns with appropriate types and may contain missing values. - The DataFrame may contain a large number of rows and columns, so performance is a consideration. - Ensure not to mutate the original DataFrame passed to the function. **Example:** Suppose `df` is: | | a | b | c | d | |---|----|------|-----|----------| | 0 | 1 | 2.0 | NaN |\'ok\' | | 1 | 2 | NaN | NaN |\'error\' | | 2 | NaN| 4.0 | NaN |\'warning\' | | 3 | 4 | 5.0 | NaN |\'Error\' | The expected output after all transformations might look like: | | a | b | c | d | d_contains_error | |---|---------|--------------|-----|----------|------------------| | 0 | -1.0 | -1.118034 | NaN |\'ok\' | False | | 1 | 0.0 | NaN | NaN |\'error\' | True | | 2 | 0.0 | 0.000000 | NaN |\'warning\' | False | | 3 | 1.0 | 1.118034 | NaN |\'Error\' | True | In this example: - Missing integer in `a` was replaced by the average of existing values. - Column `b` was normalized. - `d_contains_error` is a boolean indicating presence of \'error\'. Ensure your implementation adheres to the constraints and handles edge cases appropriately.","solution":"import pandas as pd import numpy as np def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Memory Reporting print(\'Memory Usage:\') print(df.memory_usage()) print(\'Memory Usage (deep=True):\') print(df.memory_usage(deep=True)) # Create a copy of the dataframe to avoid mutating the original df_copy = df.copy() # Step 2: Handle Missing Values for col in df_copy.select_dtypes(include=[\'int\', \'float\']).columns: if df_copy[col].isnull().any(): mean_value = df_copy[col].mean() df_copy[col].fillna(mean_value, inplace=True) # Step 3: User Defined Transformation (Normalizing numerical columns) for col in df_copy.select_dtypes(include=[\'int\', \'float\']).columns: mean = df_copy[col].mean() std = df_copy[col].std() if std != 0: # avoid division by zero df_copy[col] = (df_copy[col] - mean) / std # Step 4: Boolean Conditions for string columns containing \'error\' for col in df_copy.select_dtypes(include=[\'object\']).columns: df_copy[f\'{col}_contains_error\'] = df_copy[col].str.contains(\'error\', case=False, na=False) # Return the transformed DataFrame return df_copy"},{"question":"You are tasked with designing an `Enum` class to represent a collection of shapes with specific properties and behaviors in a geometry application. Your `Enum` class should meet the following criteria: 1. **Enum Creation**: Use the `Enum` class from the `enum` module to define an enumeration called `Shape`. 2. **Custom Values**: - Use `auto()` to automatically assign values to each shape. - Ensure that each shape has a unique auto-generated value. 4. **Custom Methods**: - Implement a method `area()` that calculates the area of the shape. For simplicity: - `CIRCLE` should assume a fixed radius of 1 unit. - `SQUARE` should assume a fixed side length of 1 unit. - `RECTANGLE` should assume a fixed width of 2 units and height of 1 unit. 6. **Ensure Unique Values**: Use the `unique` decorator to ensure that all shapes have unique values. Expected Enum Class ```python from enum import Enum, auto, unique @unique class Shape(Enum): CIRCLE = auto() SQUARE = auto() RECTANGLE = auto() def area(self): # Implement area calculation logic for each shape if self == Shape.CIRCLE: return 3.14159 # Pi * r^2, radius = 1 elif self == Shape.SQUARE: return 1 # side * side, side = 1 elif self == Shape.RECTANGLE: return 2 * 1 # width * height, width=2, height=1 ``` # Requirements 1. **Enum Class**: The class should be named `Shape` and use automatic value generation. 2. **Calculate Area**: Implement a method `area()` within the `Shape` class to calculate and return the area based on the simple assumptions provided. 3. **Iterate and Print**: Write a function `print_shapes_and_areas()` that iterates through each shape in the `Shape` enum and prints the shape\'s name and its area. # Example Usage ```python # Example usage of the Shape enum def print_shapes_and_areas(): for shape in Shape: print(f\'{shape.name}: {shape.area()}\') print_shapes_and_areas() ``` # Expected Output ``` CIRCLE: 3.14159 SQUARE: 1 RECTANGLE: 2 ``` # Constraints - Use the `enum` module and its features as specified. - Ensure the `Shape` class uses the `unique` decorator to validate unique values. - Do not include additional shapes or change the given dimensions and area calculations. Please write the complete implementation for the `Shape` enum class, including the `print_shapes_and_areas()` function.","solution":"from enum import Enum, auto, unique @unique class Shape(Enum): CIRCLE = auto() SQUARE = auto() RECTANGLE = auto() def area(self): if self == Shape.CIRCLE: return 3.14159 # Pi * r^2, radius = 1 elif self == Shape.SQUARE: return 1 # side * side, side = 1 elif self == Shape.RECTANGLE: return 2 * 1 # width * height, width=2, height=1 def print_shapes_and_areas(): for shape in Shape: print(f\'{shape.name}: {shape.area()}\')"},{"question":"**Question: HTML String Manipulation** You are tasked with implementing a function that processes a list of HTML strings. The function should escape and then unescape these strings to ensure that any HTML entities are properly converted and reverted. Your implementation should utilize the `html.escape` and `html.unescape` functions from the Python `html` module. # Function Signature: ```python def process_html_strings(html_list: list[str], quote: bool = True) -> list[str]: ``` # Input: - `html_list`: A list of HTML strings. Each string may contain special characters (`&`, `<`, `>`, `\\"`, `\'`) that need to be escaped for safe HTML display. - `quote`: An optional boolean (default is True). When set to True, the characters `\\"` and `\'` are also escaped. # Output: - A list of strings where each input string has been escaped and then unescaped back to its original form. # Constraints: - Each string in `html_list` should be between 1 and 1000 characters in length. - The total number of strings in `html_list` should not exceed 1000. # Example: ```python input_list = [\\"This & that < those \'things\'\\", \\"Another & example > case\\"] output_list = process_html_strings(input_list, quote=True) print(output_list) # Expected Output: [\\"This & that < those \'things\'\\", \\"Another & example > case\\"] ``` # Requirements: 1. You must use the provided `html.escape` and `html.unescape` functions to handle the escaping and unescaping process. 2. Ensure that the output list contains strings converted back to their original form after being escaped and unescaped. # Notes: - This exercise is designed to test your understanding of encoding and decoding HTML entities using built-in Python utilities.","solution":"from html import escape, unescape def process_html_strings(html_list: list[str], quote: bool = True) -> list[str]: Processes a list of HTML strings by escaping and then unescaping them. Args: - html_list (list[str]): A list of HTML strings. - quote (bool): Optional; when True, characters \\" and \' are also escaped. Default is True. Returns: - list[str]: A list of processed HTML strings. escaped_and_unescaped_list = [] for html_string in html_list: escaped_string = escape(html_string, quote=quote) unescaped_string = unescape(escaped_string) escaped_and_unescaped_list.append(unescaped_string) return escaped_and_unescaped_list"},{"question":"# Asyncio Event Loop Implementation Challenge Your task is to implement an asynchronous system that handles the differences in event loops and subprocesses across platforms using the Python `asyncio` module. Problem Statement You need to write a function `run_subprocess(command: str) -> str` that executes a given shell command asynchronously and returns the command\'s output. The function should handle the following scenarios: 1. **Windows**: Use `ProactorEventLoop` for managing subprocesses. 2. **macOS**: Use the default event loop on modern macOS versions (â‰¥ 10.9). For older versions (â‰¤ 10.8), configure the `SelectorEventLoop` to support character devices. 3. **All other platforms**: Use the default event loop available in `asyncio`. Function Signature ```python import sys import asyncio async def run_subprocess(command: str) -> str: # Your implementation here ``` Input - `command`: A string representing the shell command to be executed. Output - Returns a string containing the output of the executed command. Constraints - The command executed will not be malicious and will have read-only operations since file I/O monitoring is restricted. Example ```python result = await run_subprocess(\\"echo Hello World\\") print(result) # Should print \\"Hello Worldn\\" ``` Notes - In case of any platform-specific limitations not being supported (e.g., older macOS), handle exceptions gracefully and return an appropriate message indicating the feature is not supported on the system. - Students must demonstrate an understanding of event loops and subprocess management across different platforms as outlined in the provided documentation. Performance Requirements - The function should handle the execution within a reasonable time frame (consider the outlier cases where subprocesses might take slightly longer to handle but should not hang indefinitely). Use the provided information to consider known limitations and variation points across platforms as you design your function.","solution":"import sys import asyncio async def run_subprocess(command: str) -> str: Executes a shell command asynchronously and returns the output. if sys.platform == \'win32\': # For Windows, use ProactorEventLoop if available if not isinstance(asyncio.get_event_loop(), asyncio.ProactorEventLoop): asyncio.set_event_loop(asyncio.ProactorEventLoop()) elif sys.platform == \'darwin\' and sys.version_info < (10, 9): # For older macOS versions, use SelectorEventLoop if not isinstance(asyncio.get_event_loop(), asyncio.SelectorEventLoop): asyncio.set_event_loop(asyncio.SelectorEventLoop()) else: # Use the default event loop for other platforms if not isinstance(asyncio.get_event_loop(), asyncio.AbstractEventLoop): asyncio.set_event_loop(asyncio.new_event_loop()) # Initialize event loop loop = asyncio.get_event_loop() proc = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() if stderr: return f\\"Error: {stderr.decode().strip()}\\" return stdout.decode().strip() # Sample use in main method (uncomment to run as script) # if __name__ == \\"__main__\\": # command = \\"echo Hello World\\" # result = asyncio.run(run_subprocess(command)) # print(result)"},{"question":"# XML Data Processing with xml.etree.ElementTree Given the following XML data representing a collection of books in a library: ```xml <library> <book id=\\"1\\" genre=\\"Fiction\\"> <title>Harry Potter and the Philosopher\'s Stone</title> <author>J.K. Rowling</author> <published>1997</published> <rating>4.8</rating> </book> <book id=\\"2\\" genre=\\"Non-Fiction\\"> <title>Sapiens: A Brief History of Humankind</title> <author>Yuval Noah Harari</author> <published>2011</published> <rating>4.6</rating> </book> <book id=\\"3\\" genre=\\"Fiction\\"> <title>The Hobbit</title> <author>J.R.R. Tolkien</author> <published>1937</published> <rating>4.7</rating> </book> </library> ``` **Task:** 1. **Parse the XML Data:** - Read the above XML data from a string and parse it using `xml.etree.ElementTree`. 2. **Query and Extract Data:** - Write a function `get_books_by_genre(genre)` that accepts a genre (e.g., \\"Fiction\\", \\"Non-Fiction\\") and returns a list of book titles in that genre. 3. **Modify XML Data:** - Write a function `update_book_rating(book_id, new_rating)` that accepts a book ID and a new rating, updates the rating of the specified book in the XML data, and returns the newly updated XML as a string. 4. **Add New Element:** - Write a function `add_book(book_data)` that accepts a dictionary containing book details (id, genre, title, author, published, rating), adds a new book element to the XML data, and returns the updated XML as a string. **Constraints:** - Assume that the `book_data` dictionary for `add_book` function has keys: \'id\', \'genre\', \'title\', \'author\', \'published\', and \'rating\'. - Ensure the XML data remains well-formed after modifications. **Function Definitions:** ```python def get_books_by_genre(genre: str) -> list: Args: genre (str): The genre to filter books by. Returns: list: A list of titles of books in the given genre. pass def update_book_rating(book_id: str, new_rating: float) -> str: Args: book_id (str): The ID of the book to update. new_rating (float): The new rating for the book. Returns: str: The updated XML as a string. pass def add_book(book_data: dict) -> str: Args: book_data (dict): A dictionary containing book details. Returns: str: The updated XML as a string. pass ``` **Example Usage:** ```python book_data = { \'id\': \'4\', \'genre\': \'Science Fiction\', \'title\': \'Dune\', \'author\': \'Frank Herbert\', \'published\': \'1965\', \'rating\': \'4.9\' } # Adding a new book updated_xml = add_book(book_data) print(updated_xml) # Updating the rating of an existing book updated_xml = update_book_rating(\'1\', 4.9) print(updated_xml) # Getting books by genre fiction_books = get_books_by_genre(\'Fiction\') print(fiction_books) # Output: [\\"Harry Potter and the Philosopher\'s Stone\\", \\"The Hobbit\\"] ```","solution":"import xml.etree.ElementTree as ET xml_data = <library> <book id=\\"1\\" genre=\\"Fiction\\"> <title>Harry Potter and the Philosopher\'s Stone</title> <author>J.K. Rowling</author> <published>1997</published> <rating>4.8</rating> </book> <book id=\\"2\\" genre=\\"Non-Fiction\\"> <title>Sapiens: A Brief History of Humankind</title> <author>Yuval Noah Harari</author> <published>2011</published> <rating>4.6</rating> </book> <book id=\\"3\\" genre=\\"Fiction\\"> <title>The Hobbit</title> <author>J.R.R. Tolkien</author> <published>1937</published> <rating>4.7</rating> </book> </library> tree = ET.ElementTree(ET.fromstring(xml_data)) root = tree.getroot() def get_books_by_genre(genre: str) -> list: Returns a list of book titles in the specified genre. return [book.find(\\"title\\").text for book in root.findall(f\\".//book[@genre=\'{genre}\']\\")] def update_book_rating(book_id: str, new_rating: float) -> str: Updates the rating of the book with the specified ID and returns the updated XML as a string. book = root.find(f\\".//book[@id=\'{book_id}\']\\") if book is not None: book.find(\\"rating\\").text = str(new_rating) return ET.tostring(root, encoding=\'unicode\') def add_book(book_data: dict) -> str: Adds a new book to the XML data and returns the updated XML as a string. new_book = ET.Element(\\"book\\", id=book_data[\'id\'], genre=book_data[\'genre\']) for key, value in book_data.items(): if key != \'id\' and key != \'genre\': element = ET.SubElement(new_book, key) element.text = str(value) root.append(new_book) return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Objective To assess your understanding of the \\"pickle\\" module in Python, we will ask you to implement functionality that involves serializing and deserializing complex objects. # Problem Statement You are tasked with creating a class that manages user profiles for an application. Each profile contains user-specific data such as username, email, preferences, and history of actions. Your goal is to implement this class in such a way that it can be serialized (pickled) and deserialized (unpickled) efficiently. Additionally, the history of actions should be managed such that it does not consume too much memory when the object is pickled. # Requirements: 1. Implement a class `UserProfile` with the following attributes: - `username`: A string representing the username. - `email`: A string representing the user\'s email. - `preferences`: A dictionary containing user preferences. - `actions_history`: A list containing tuples of past user actions, where each tuple has a timestamp and the action description. 2. The class should have methods for: - Adding a new action to the history (`add_action(action: str)`). - Getting the last `n` actions from the history (`get_last_actions(n: int) -> List[Tuple[str, str]]`). 3. Customize the pickling and unpickling processes to handle the actions history in an optimized way: - Utilize the newest pickle protocol (protocol 5). - Ensure that the history list only includes the last 100 actions when pickled (assuming actions are added sequentially in time). # Constraints - You are only allowed to use the standard library modules. - The history list can grow unbounded during runtime, but it should be truncated to the last 100 actions during pickling. - The pickling process must use protocol 5. # Performance Requirements The methods `add_action` and `get_last_actions` should execute in O(1) and O(n) time complexity, respectively. # Expected Input/Output Format **Input:** - `username`, `email`, and `preferences` are provided during object instantiation. - `add_action` and `get_last_actions` methods will be called with appropriate parameters. **Output:** - Method calls to `get_last_actions` should return a list of up to `n` past actions. **Example Usage:** ```python from datetime import datetime # Create a new user profile profile = UserProfile(username=\\"john_doe\\", email=\\"john@example.com\\", preferences={\\"theme\\": \\"dark\\"}) # Add some actions profile.add_action(\\"Logged in\\") profile.add_action(\\"Uploaded a photo\\") profile.add_action(\\"Logged out\\") # Get last actions last_2_actions = profile.get_last_actions(2) print(last_2_actions) # Should output the last 2 actions # Serialize the profile pickled_data = pickle.dumps(profile) # Deserialize the profile new_profile = pickle.loads(pickled_data) # Check if the deserialized profile retains the last 100 actions correctly ``` # Implementation Details Implement the `UserProfile` class with the required functionalities and customize the pickling process by overriding relevant methods to ensure adherence to the constraints and requirements.","solution":"import pickle from datetime import datetime from typing import List, Tuple, Dict class UserProfile: def __init__(self, username: str, email: str, preferences: Dict[str, str]): self.username = username self.email = email self.preferences = preferences self.actions_history = [] def add_action(self, action: str): timestamp = datetime.now().isoformat() self.actions_history.append((timestamp, action)) def get_last_actions(self, n: int) -> List[Tuple[str, str]]: return self.actions_history[-n:] def __getstate__(self): # Limit the actions history to the last 100 actions for pickle serialization state = self.__dict__.copy() state[\'actions_history\'] = self.actions_history[-100:] return state def __setstate__(self, state): self.__dict__.update(state)"},{"question":"Context You are provided with the deprecated `pipes` module in Python, which facilitates the creation of shell pipelines. Though practicing this module is useful for understanding POSIX shell integrations, note that for modern applications, the `subprocess` module is recommended. Problem Statement Implement a function `capitalize_copy()` that reads lowercase text from an input file, transforms it to uppercase using a pipeline, and writes the resultant uppercase text to an output file. You must use the `pipes.Template` class and its methods to achieve this. # Function Signature ```python def capitalize_copy(input_filepath: str, output_filepath: str) -> None: Reads text from input_filepath, converts it to uppercase using a pipeline, and writes it to output_filepath. Parameters: - input_filepath: str : Path to the input file to read from. - output_filepath: str : Path to the output file to write to. ``` # Input - `input_filepath` (string): This is the path to a file that contains the text to be transformed. - `output_filepath` (string): This is the path to a file where the transformed text will be stored. # Output - The function does not return anything. It writes the transformed text to `output_filepath`. # Constraints - The input file will always exist and contain lowercase English alphabets. - You must use `pipes.Template` and its methods to implement the pipeline. - Assume input file sizes that comfortably fit in memory. # Example Suppose you have a file `input.txt` with the following content: ``` hello world ``` After running the function `capitalize_copy(\'input.txt\', \'output.txt\')`, the content of `output.txt` should be: ``` HELLO WORLD ``` Your Task Implement the `capitalize_copy` function according to the specifications given.","solution":"import pipes def capitalize_copy(input_filepath: str, output_filepath: str) -> None: with open(input_filepath, \'r\') as input_file: content = input_file.read() # Setting up the pipeline p = pipes.Template() p.append(\'tr a-z A-Z\', \'--\') with p.open(output_filepath, \'w\') as output_file: output_file.write(content)"},{"question":"# Custom Operator Implementation and Testing in PyTorch **Objective**: In this assignment, you will demonstrate your understanding of PyTorch\'s `torch.library` module by creating a custom operator, implementing its kernel, and testing it for correctness. # Problem Statement: 1. **Custom Operation**: - Define a custom operator `my_custom_op` that takes an input tensor and returns the tensor squared element-wise. 2. **Kernel Implementation**: - Implement the kernel for `my_custom_op` using `torch.library.register_kernel`. 3. **Autograd Support**: - Register autograd support for `my_custom_op` using `torch.library.register_autograd`. 4. **Testing**: - Verify the correctness of the operator using `torch.library.opcheck`. - Test the gradient computation using `torch.autograd.gradcheck`. # Instructions: 1. **Define the Custom Operator**: ```python import torch from torch.library import Library # Define a new library for custom ops my_lib = Library(\\"my_lib\\", \\"DEF\\") # Define the custom operator schema my_lib.define(\\"my_custom_op(Tensor input) -> Tensor\\") ``` 2. **Kernel Implementation**: ```python from torch.library import register_kernel @register_kernel(\\"my_lib::my_custom_op\\") def my_custom_op_kernel(input_tensor): return input_tensor ** 2 ``` 3. **Autograd Support**: ```python from torch.library import register_autograd @register_autograd(\\"my_lib::my_custom_op\\") def my_custom_op_autograd(ctx, grad_output): input_tensor = ctx.saved_tensors[0] return 2 * input_tensor * grad_output ``` 4. **Testing**: ```python from torch.library import opcheck import torch.autograd # Create a test tensor test_tensor = torch.randn(3, requires_grad=True) # Check operator functionality opcheck(\\"my_lib::my_custom_op\\", (test_tensor,)) # Check gradients assert torch.autograd.gradcheck(lambda x: x**2, (test_tensor,)) ``` # Submission: Submit a single Python script that: - Defines and implements the custom operator (Steps 1-3). - Tests the custom operator and ensures all checks pass (Step 4). # Constraints: - Assume the input tensor is always of floating point type. - The operator should support both forward and backward passes. - Ensure proper error handling and documentation within the code. # Performance: - The kernel implementation should be efficient and make use of PyTorchâ€™s native operations as much as possible. Good luck!","solution":"import torch from torch.autograd import Function # Define the custom operator as an autograd function class MyCustomOp(Function): @staticmethod def forward(ctx, input_tensor): ctx.save_for_backward(input_tensor) return input_tensor ** 2 @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors return 2 * input_tensor * grad_output # Convenience wrapper function for the custom operator def my_custom_op(input_tensor): return MyCustomOp.apply(input_tensor)"},{"question":"Utilizing the MPS Backend in PyTorch Objective Implement a function `mps_matrix_multiplication` that performs matrix multiplication using PyTorch\'s MPS backend. The function will create two random square matrices, move them to the MPS device for multiplication, and then return the result back to the CPU. Function Signature ```python def mps_matrix_multiplication(size: int) -> torch.Tensor: pass ``` Input - `size` (int): The size (dimensions) of the square matrices to be created. For example, if `size` is 3, then create 3x3 matrices. Output - A `torch.Tensor` representing the result of the matrix multiplication, moved back to the CPU. Constraints - The MPS backend must be available. - `size` will always be a positive integer. - The function should handle the device transfer correctly between CPU and MPS. Requirements 1. Check if MPS is available. If it is not, raise an Exception with an appropriate error message. 2. Create two random square matrices of the given size using PyTorch. 3. Transfer the matrices to the MPS device. 4. Perform matrix multiplication on the MPS device. 5. Transfer the result back to the CPU. 6. Return the result tensor. Example ```python import torch def mps_matrix_multiplication(size: int) -> torch.Tensor: if not torch.backends.mps.is_available(): raise Exception(\\"MPS not available. Ensure MacOS version is 12.3+ and you have an MPS-enabled device.\\") mps_device = torch.device(\\"mps\\") # Create two random matrices of given size on CPU A = torch.rand(size, size) B = torch.rand(size, size) # Move matrices to MPS device A_mps = A.to(mps_device) B_mps = B.to(mps_device) # Perform matrix multiplication on MPS device result_mps = torch.matmul(A_mps, B_mps) # Move result back to CPU result_cpu = result_mps.to(\\"cpu\\") return result_cpu # Example usage: print(mps_matrix_multiplication(3)) ``` The example usage should work correctly if the MPS backend is available. If it is not available, the function should gracefully inform the user with an appropriate error message.","solution":"import torch def mps_matrix_multiplication(size: int) -> torch.Tensor: if not torch.backends.mps.is_available(): raise Exception(\\"MPS not available. Ensure MacOS version is 12.3+ and you have an MPS-enabled device.\\") mps_device = torch.device(\\"mps\\") # Create two random matrices of given size on CPU A = torch.rand(size, size) B = torch.rand(size, size) # Move matrices to MPS device A_mps = A.to(mps_device) B_mps = B.to(mps_device) # Perform matrix multiplication on MPS device result_mps = torch.matmul(A_mps, B_mps) # Move result back to CPU result_cpu = result_mps.to(\\"cpu\\") return result_cpu"},{"question":"# Advanced String and Number Conversion You are tasked with implementing a Python function that simulates the behavior of the `PyOS_string_to_double` and `PyOS_double_to_string` functions as described in the documentation above. Your function will be named `convert_string_double` and will have two main functionalities: 1. Convert a valid string to a double. 2. Convert a double to a formatted string based on different format codes and flags. Function Signature: ```python def convert_string_double(value: str, to_double: bool, format_code: str = \'f\', precision: int = 6, flags: int = 0) -> str: Converts a string to a double or a double to a formatted string. Parameters: value (str) : The input string or string representation of the double. to_double (bool) : If True, convert the string to a double. If False, convert the double to a formatted string. format_code (str) : A character that specifies the format of the output string if converting double to string. Must be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'. Default is \'f\'. precision (int) : Specifies the number of significant digits or decimal places if converting double to string. Default is 6. flags (int) : Specifies one or more formatting flags if converting double to string. Default is 0. Returns: str: The converted value as a string, or an error message if conversion fails. ``` Input: * `value`: A string representing either the input number (when `to_double` is `True`) or the double to be formatted (when `to_double` is `False`). * `to_double`: A boolean indicating the direction of conversion. If `True`, convert the string to a double; if `False`, convert the double to a formatted string. * `format_code`: A character from the set {\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'} specifying the format of the output when converting from double to string. This parameter is ignored if `to_double` is `True`. * `precision`: An integer specifying the number of significant digits or decimal places when converting from double to string. This parameter is ignored if `to_double` is `True`. * `flags`: An integer flag that modifies the formatting when converting from double to string. This parameter is ignored if `to_double` is `True`. Output: * A string that is the converted value or an error message if the conversion fails. Constraints: * The input `value` must not have leading or trailing whitespace when converting to a double. * Handle edge cases such as invalid input strings, unsupported format codes, and extremely large or small values appropriately. * Ensure `format_code` is one of the specified characters. * `precision` must be a non-negative integer. * `flags` can be zero or more of specified values (additional flags are optional). Examples: ```python # Example 1: Convert string to double print(convert_string_double(\\"123.456\\", to_double=True)) # Output: 123.456 # Example 2: Convert double to string print(convert_string_double(\\"123.456\\", to_double=False, format_code=\'e\', precision=2)) # Output: \\"1.23e+02\\" # Example 3: Handle invalid string input print(convert_string_double(\\"invalid\\", to_double=True)) # Output: \\"ValueError: invalid string for conversion\\" # Example 4: Handle formatting with flags print(convert_string_double(\\"123.0\\", to_double=False, format_code=\'f\', precision=2, flags=1)) # Output: \\"+123.00\\" ``` Implement the required `convert_string_double` function that fulfills the above specifications and handles edge cases as described in the provided documentation.","solution":"def convert_string_double(value: str, to_double: bool, format_code: str = \'f\', precision: int = 6, flags: int = 0) -> str: Converts a string to a double or a double to a formatted string. Parameters: value (str) : The input string or string representation of the double. to_double (bool) : If True, convert the string to a double. If False, convert the double to a formatted string. format_code (str) : A character that specifies the format of the output string if converting double to string. Must be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', or \'r\'. Default is \'f\'. precision (int) : Specifies the number of significant digits or decimal places if converting double to string. Default is 6. flags (int) : Specifies one or more formatting flags if converting double to string. Default is 0. Returns: str: The converted value as a string, or an error message if conversion fails. if to_double: try: double_value = float(value) return str(double_value) except ValueError: return \\"ValueError: invalid string for conversion\\" else: try: double_value = float(value) except ValueError: return \\"ValueError: invalid double for conversion\\" format_spec = f\\".{precision}{format_code}\\" if flags & 1: # Example flag for \\"+\\" formatted_value = format(double_value, f\\"+{format_spec}\\") else: formatted_value = format(double_value, format_spec) return formatted_value"},{"question":"# Custom Autograd Function with Hooks Design and implement a custom Autograd function in PyTorch that computes the function ( f(x) = x^3 ), but with additional functionality: 1. **Forward Pass**: Compute ( y = x^3 ). 2. **Backward Pass**: Compute the gradient and save intermediate results. 3. **Hook for Saving Intermediate Results**: Implement hooks to save intermediate tensor values to disk if the tensor size exceeds a threshold. Function Signature ```python import torch from torch.autograd import Function class CustomGradientFunc(Function): @staticmethod def forward(ctx, input_tensor): # Save input for backward pass ctx.save_for_backward(input_tensor) return input_tensor ** 3 @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = 3 * input_tensor ** 2 * grad_output return grad_input def apply_custom_gradient_func(input_tensor): return CustomGradientFunc.apply(input_tensor) class SaveIntermediateResults: def __init__(self, threshold): self.threshold = threshold def pack_hook(self, tensor): if tensor.numel() >= self.threshold: temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file return tensor def unpack_hook(self, tensor_or_temp_file): if isinstance(tensor_or_temp_file, torch.Tensor): return tensor_or_temp_file return torch.load(tensor_or_temp_file.name) # Example usage if __name__ == \\"__main__\\": SAVE_THRESHOLD = 1000 input_tensor = torch.randn(1024, requires_grad=True) # Set hooks for tensors hooks_instance = SaveIntermediateResults(SAVE_THRESHOLD) with torch.autograd.graph.saved_tensors_hooks(hooks_instance.pack_hook, hooks_instance.unpack_hook): output_tensor = apply_custom_gradient_func(input_tensor) output_tensor.sum().backward() ``` # Explanation 1. **Implementing Forward Pass**: Compute ( y = x^3 ) and save input tensor for backward computation. 2. **Implementing Backward Pass**: Compute the gradient ( frac{dy}{dx} = 3x^2 ) using saved input tensor. 3. **Setting Up Hooks**: Use `torch.autograd.graph.saved_tensors_hooks` to save intermediate tensor values if they exceed a certain size to disk. # Constraints - The input tensor size for saving must be user-defined, e.g., `SAVE_THRESHOLD` variable. - Implementation must support testing with different tensor dimensions and sizes. - Provide comments explaining each step to ensure clarity. # Expected Outputs - Proper tensor values after forward operations. - Correct gradients after backward operations. - Intermediate tensors saved to disk as needed.","solution":"import torch from torch.autograd import Function import tempfile import os class SelfDeletingTempFile: def __init__(self): self.file = tempfile.NamedTemporaryFile(delete=False) self.name = self.file.name def close(self): if os.path.isfile(self.name): os.remove(self.name) class CustomGradientFunc(Function): @staticmethod def forward(ctx, input_tensor): # Save input for backward pass ctx.save_for_backward(input_tensor) return input_tensor ** 3 @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = 3 * input_tensor ** 2 * grad_output return grad_input def apply_custom_gradient_func(input_tensor): return CustomGradientFunc.apply(input_tensor) class SaveIntermediateResults: def __init__(self, threshold): self.threshold = threshold def pack_hook(self, tensor): if tensor.numel() >= self.threshold: temp_file = SelfDeletingTempFile() torch.save(tensor, temp_file.name) return temp_file return tensor def unpack_hook(self, tensor_or_temp_file): if isinstance(tensor_or_temp_file, torch.Tensor): return tensor_or_temp_file return torch.load(tensor_or_temp_file.name)"},{"question":"Title: Creating a Custom Arithmetic Expression Evaluator Objective: You are required to implement a function that evaluates a custom arithmetic expression provided in the form of a list. The function should make use of the `operator` module to dynamically evaluate the expression. Problem Statement: Implement a function `evaluate_expression(expression)` which takes in a list `expression` where: - An element of the list may be an integer, a string representing an operator (e.g., `\'+\'`, `\'-\'`, `\'*\'`, `\'/\',`), or a nested list representing a sub-expression. - The function should return the result of evaluating the arithmetic expression. Use the appropriate functions from the `operator` module to perform the required operations. You must handle at least the addition, subtraction, multiplication, and division operations. Input Format: - `expression`: A list containing integers, operator strings, and/or nested lists representing arithmetic expressions. Output Format: - Return the result of evaluating the expression as an integer or float. Constraints: - The `expression` list will be well-formed and will always represent a valid arithmetic expression. - Division operations should use true division (result can be a float). Example: ```python import operator def evaluate_expression(expression): # Your implementation here pass # Example Expressions expr1 = [1, \'+\', 2] expr2 = [[1, \'+\', 2], \'*\', 3] expr3 = [4, \'/\', 2, \'-\', 1] expr4 = [[10, \'-\', 2], \'/\', [2, \'*\', 2]] # Expected Outputs print(evaluate_expression(expr1)) # Output: 3 print(evaluate_expression(expr2)) # Output: 9 print(evaluate_expression(expr3)) # Output: 1.0 print(evaluate_expression(expr4)) # Output: 2.0 ``` Note: - You are encouraged to use the `operator` module\'s functions for your implementation. - You may assume the input list does not contain any invalid or unsupported elements. Hints: - Define a helper function to recursively evaluate sub-expressions. - Use a dictionary to map operator strings to their corresponding functions in the `operator` module. Evaluation Criteria: - Correctness of the implementation. - Proper utilization of the `operator` module functions. - Code readability and efficiency.","solution":"import operator def evaluate_expression(expression): # Define operator mappings ops = {\'+\': operator.add, \'-\': operator.sub, \'*\': operator.mul, \'/\': operator.truediv} def evaluate(tokens): if isinstance(tokens, list): # Recursive evaluation for nested sub-expressions result = evaluate(tokens[0]) for i in range(1, len(tokens), 2): op = ops[tokens[i]] next_val = evaluate(tokens[i+1]) result = op(result, next_val) return result else: # Base case: return the integer return tokens # Start evaluation return evaluate(expression)"},{"question":"# Secure Password Storage and Retrieval Problem Statement You are tasked with designing a script that securely handles user passwords for a login-based application. Your script will perform the following operations: 1. **User Registration**: Securely prompts a new user to input a username and password and stores them. 2. **User Login**: Prompts the user to enter their username and password to log in. The script should verify the credentials and allow access if they match the stored information. Requirements 1. Use the `getpass` module to handle password inputs securely. 2. Store the user credentials in an in-memory dictionary for simplicity. 3. Implement a function `register_user` to handle new user registration. 4. Implement a function `login_user` to handle user login. 5. The `register_user` function should not allow duplicate usernames. Input * For `register_user`: * A `prompt` for username and password input (default to \\"Enter Username: \\" and \\"Enter Password: \\"). * For `login_user`: * A `prompt` for username and password input (default to \\"Username: \\" and \\"Password: \\"). Output * `register_user` should print a success message upon successful registration. * `login_user` should print either \\"Login successful!\\" or \\"Invalid credentials!\\" based on the login attempt. Constraints * Username and password should not be empty. * Use appropriate exception handling for potential errors (e.g., empty input, duplicate usernames). Example ```python users = {} def register_user(prompt_user=\\"Enter Username: \\", prompt_pass=\\"Enter Password: \\"): # Implement the function to register a user def login_user(prompt_user=\\"Username: \\", prompt_pass=\\"Password: \\"): # Implement the function to login a user # Sample usage register_user() login_user() ``` Notes * This is a simplified scenario for educational purposes. Avoid using plain text storage for passwords in real applications. * Consider how you might extend this script to handle more complex scenarios, such as storing passwords securely with hashing. Good luck!","solution":"import getpass users = {} # Dictionary to store user credentials as {username: password} def register_user(prompt_user=\\"Enter Username: \\", prompt_pass=\\"Enter Password: \\"): while True: username = input(prompt_user) if not username: print(\\"Username cannot be empty. Please try again.\\") continue if username in users: print(\\"Username already exists. Please try again.\\") continue password = getpass.getpass(prompt_pass) if not password: print(\\"Password cannot be empty. Please try again.\\") continue users[username] = password print(f\\"User \'{username}\' registered successfully!\\") break def login_user(prompt_user=\\"Username: \\", prompt_pass=\\"Password: \\"): username = input(prompt_user) password = getpass.getpass(prompt_pass) if users.get(username) == password: print(\\"Login successful!\\") else: print(\\"Invalid credentials!\\")"},{"question":"# Floating-Point Arithmetic Precision **Background**: In Python, floating-point numbers are represented using binary fractions which can introduce small errors in precision when representing certain decimal values. These errors can accumulate during arithmetic operations, leading to results that are not as expected. Python provides various methods and modules to mitigate these inaccuracies. **Task**: Write a Python function `precise_sum` that takes a list of floating-point numbers and returns their sum with high precision. The function should use Python\'s `decimal` module to ensure that the sum is as accurate as possible. **Function Signature**: ```python from typing import List from decimal import Decimal def precise_sum(numbers: List[float]) -> Decimal: pass ``` **Input**: - `numbers`: A list of floating-point numbers (0 â‰¤ length of numbers â‰¤ 1000, each number in the range of -10**6 to 10**6). **Output**: - A `Decimal` representing the precise sum of the input numbers. **Example**: ```python assert precise_sum([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]) == Decimal(\'1.0\') ``` **Constraints**: - You must use the `decimal` module to achieve high precision in floating-point arithmetic. - Avoid using the built-in `sum()` function directly on the list of floats as it does not guarantee high precision in this case. **Hints**: - Convert floating-point numbers to `Decimal` before summing them up. - Ensure that you initialize the `Decimal` context properly if needed (e.g., to set precision or rounding methods).","solution":"from typing import List from decimal import Decimal, getcontext def precise_sum(numbers: List[float]) -> Decimal: Returns the precise sum of a list of floating-point numbers. Uses the decimal module to ensure higher precision. # Set the precision for Decimal operations getcontext().prec = 50 decimal_sum = Decimal(0) for num in numbers: decimal_sum += Decimal(str(num)) return decimal_sum"},{"question":"# Advanced Python Data Model Assessment Objective Implement a customized class that demonstrates a thorough understanding of Python\'s data model, focusing on special methods for object behavior customization, sequence emulation, and attribute management. Problem Description You are tasked with creating a class called `CustomList` that emulates a Python list with additional functionality. The `CustomList` should support basic list operations, custom string representations, and attribute access control. Specifically, you need to implement the following: 1. **Initialization**: - The `CustomList` should be initialized with an iterable of elements. ```python clist = CustomList([1, 2, 3]) ``` 2. **Element Access and Modification**: - Implement `__getitem__`, `__setitem__`, and `__delitem__` to allow element access, assignment, and deletion using index. ```python element = clist[1] # Should return 2 clist[1] = 10 # Now the list should be [1, 10, 3] del clist[1] # Now the list should be [1, 3] ``` 3. **String Representation**: - Implement `__repr__()` and `__str__()` to provide both official and informal string representations of the object. ```python print(repr(clist)) # Should return \\"CustomList([1, 3])\\" print(str(clist)) # Should return \\"[1, 3]\\" ``` 4. **Sequence Operations**: - Enable the `CustomList` to support concatenation and repetition using `__add__` and `__mul__`. ```python clist2 = clist + CustomList([4, 5]) # Should return CustomList([1, 3, 4, 5]) clist3 = clist * 2 # Should return CustomList([1, 3, 1, 3]) ``` 5. **Attribute Access Control**: - Implement `__setattr__` and `__getattr__` to manage attributes `name` and `elements`. ```python clist.name = \\"Numbers\\" # Sets the name attribute print(clist.name) # Should return \\"Numbers\\" ``` 6. **Custom Methods**: - Implement a method `append()` to add an element to the end of the list. ```python clist.append(4) # Now the list should be [1, 3, 4] ``` Constraints - You must handle exceptions gracefully where applicable. - Ensure Pythonic implementation and code readability. - Do not use Python\'s built-in list directly in any methods except for initialization. Expected Input and Output ```python clist = CustomList([1, 2, 3]) print(clist) # Output: [1, 2, 3] clist[1] = 10 print(clist) # Output: [1, 10, 3] del clist[1] print(clist) # Output: [1, 3] clist2 = clist + CustomList([4, 5]) print(clist2) # Output: CustomList([1, 3, 4, 5]) clist3 = clist * 2 print(clist3) # Output: CustomList([1, 3, 1, 3]) clist.name = \\"Numbers\\" print(clist.name) # Output: \\"Numbers\\" clist.append(4) print(clist) # Output: [1, 3, 4] ``` Implementation ```python class CustomList: def __init__(self, elements): self.elements = list(elements) self.name = None def __getitem__(self, index): return self.elements[index] def __setitem__(self, index, value): self.elements[index] = value def __delitem__(self, index): del self.elements[index] def __repr__(self): return f\\"CustomList({self.elements})\\" def __str__(self): return f\\"{self.elements}\\" def __add__(self, other): if not isinstance(other, CustomList): return NotImplemented return CustomList(self.elements + other.elements) def __mul__(self, times): if not isinstance(times, int): return NotImplemented return CustomList(self.elements * times) def __setattr__(self, name, value): if name == \\"elements\\" and not isinstance(value, list): raise ValueError(\\"elements must be a list\\") object.__setattr__(self, name, value) def __getattr__(self, name): if name == \'name\': return self.__dict__.get(name) raise AttributeError(f\\"\'CustomList\' object has no attribute \'{name}\'\\") def append(self, element): self.elements.append(element) # Test the CustomList class clist = CustomList([1, 2, 3]) print(clist) clist[1] = 10 print(clist) del clist[1] print(clist) clist2 = clist + CustomList([4, 5]) print(clist2) clist3 = clist * 2 print(clist3) clist.name = \\"Numbers\\" print(clist.name) clist.append(4) print(clist) ``` This problem comprehensively covers Pythonâ€™s data model concepts and challenges students to implement custom behaviors aptly.","solution":"class CustomList: def __init__(self, elements): self.elements = list(elements) self.name = None def __getitem__(self, index): return self.elements[index] def __setitem__(self, index, value): self.elements[index] = value def __delitem__(self, index): del self.elements[index] def __repr__(self): return f\\"CustomList({self.elements})\\" def __str__(self): return f\\"{self.elements}\\" def __add__(self, other): if not isinstance(other, CustomList): return NotImplemented return CustomList(self.elements + other.elements) def __mul__(self, times): if not isinstance(times, int): return NotImplemented return CustomList(self.elements * times) def __setattr__(self, name, value): if name == \\"elements\\" and not isinstance(value, list): raise ValueError(\\"elements must be a list\\") object.__setattr__(self, name, value) def __getattr__(self, name): if name == \'name\': return self.__dict__.get(name) raise AttributeError(f\\"\'CustomList\' object has no attribute \'{name}\'\\") def append(self, element): self.elements.append(element)"},{"question":"# Terminal Snake Game Using `curses` Module You are required to implement a terminal-based Snake game using the Python `curses` module. The game should utilize various features of the `curses` module to handle window creation, input handling, and rendering. Requirements 1. **Game Window**: - Create a game window of size 20x60 (rows x columns) centered on the terminal screen. - The game window should have a border. 2. **Snake**: - The snake should start with a length of 3 at the center of the game window. - The snake should be controlled using the arrow keys (up, down, left, right). - The snake should grow in length whenever it eats food. - The game should end if the snake collides with the border or itself. 3. **Food**: - Randomly place food on the game window at a location not occupied by the snake. - Upon consumption of food by the snake, increment the score and place new food. 4. **Game Loop**: - Continuously update the game state (move snake, check collisions, place food). - Use non-blocking input handling to allow continuous movement of the snake. 5. **Score Display**: - Display the current score on the screen, updating as the game progresses. 6. **Exit Handling**: - Allow the user to exit the game gracefully by pressing \'q\'. Input and Output - **Input**: - Arrow keys to control the snake. - \'q\' to quit the game. - **Output**: - Render the game window with the snake, food, and score. - Show a game-over message when the snake collides with the border or itself. Constraints - Use the `curses` module for all terminal interactions. - Ensure the game runs smoothly with a reasonable refresh rate. - Provide meaningful comments and structure your code for readability. Example Code Outline ```python import curses import random import time def main(stdscr): # Initialize the screen and set up the game curses.curs_set(0) stdscr.nodelay(1) stdscr.timeout(100) sh, sw = stdscr.getmaxyx() w = curses.newwin(20, 60, (sh//2) - 10, (sw//2) - 30) w.keypad(1) w.border(0) snake = [(10, 40), (10, 39), (10, 38)] food = (15, 20) w.addch(food[0], food[1], curses.ACS_PI) score = 0 key = curses.KEY_RIGHT while True: next_key = w.getch() key = key if next_key == -1 else next_key if key == ord(\'q\'): break # Update the snake\'s head head = snake[0] if key == curses.KEY_DOWN: new_head = (head[0] + 1, head[1]) if key == curses.KEY_UP: new_head = (head[0] - 1, head[1]) if key == curses.KEY_LEFT: new_head = (head[0], head[1] - 1) if key == curses.KEY_RIGHT: new_head = (head[0], head[1] + 1) snake.insert(0, new_head) # Check for collisions if (new_head[0] in [0, 19] or new_head[1] in [0, 59] or new_head in snake[1:]): break # Check if snake eats food if snake[0] == food: score += 1 food = None while food is None: nf = (random.randint(1, 18), random.randint(1, 58)) food = nf if nf not in snake else None w.addch(food[0], food[1], curses.ACS_PI) else: tail = snake.pop() w.addch(tail[0], tail[1], \' \') w.addch(snake[0][0], snake[0][1], curses.ACS_CKBOARD) # Display score w.addstr(0, 2, f\'Score: {score} \') curses.wrapper(main) ``` Additional Information - **Function Definitions**: Structure your solution into appropriate functions. - **Documentation**: Include comments and docstrings explaining your code. - **Error Handling**: Graceful handling of unexpected inputs or situations. Good luck, and may your snake grow long and prosperous!","solution":"import curses import random def create_game_window(stdscr): Create and return the game window at the center of the terminal. curses.curs_set(0) sh, sw = stdscr.getmaxyx() w = curses.newwin(20, 60, (sh//2) - 10, (sw//2) - 30) w.keypad(1) w.timeout(100) return w def initialize_snake(): Initialize and return the snake coordinates. return [(10, 30), (10, 29), (10, 28)] def place_food(snake): Place food in the window at a location not occupied by the snake. while True: food = (random.randint(1, 18), random.randint(1, 58)) if food not in snake: break return food def main(stdscr): Main function that contains the game loop. game_win = create_game_window(stdscr) snake = initialize_snake() food = place_food(snake) game_win.addch(food[0], food[1], curses.ACS_PI) score = 0 key = curses.KEY_RIGHT while True: next_key = game_win.getch() key = key if next_key == -1 else next_key if key == ord(\'q\'): break head = snake[0] if key == curses.KEY_DOWN: new_head = (head[0] + 1, head[1]) if key == curses.KEY_UP: new_head = (head[0] - 1, head[1]) if key == curses.KEY_LEFT: new_head = (head[0], head[1] - 1) if key == curses.KEY_RIGHT: new_head = (head[0], head[1] + 1) snake.insert(0, new_head) if (new_head[0] in [0, 19] or new_head[1] in [0, 59] or new_head in snake[1:]): break if snake[0] == food: score += 1 food = place_food(snake) game_win.addch(food[0], food[1], curses.ACS_PI) else: tail = snake.pop() game_win.addch(tail[0], tail[1], \' \') game_win.addch(snake[0][0], snake[0][1], curses.ACS_CKBOARD) game_win.addstr(0, 2, f\'Score: {score} \') if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"Objective: Demonstrate your understanding of nullable Boolean data, indexing, and Kleene logical operations in pandas. Task: Given a pandas DataFrame with nullable boolean values, implement the following functions: 1. **`clean_na_values(df: pd.DataFrame, fill_value: bool) -> pd.DataFrame`**: - Replace all NA values in the DataFrame with the specified `fill_value` (True or False). - Return the modified DataFrame. 2. **`perform_kleene_operations(df1: pd.DataFrame, df2: pd.DataFrame) -> dict`**: - Perform element-wise logical operations (`&`, `|`, `^`) between two DataFrames (`df1` and `df2`) of the same shape. - Return a dictionary with keys `\'and\'`, `\'or\'`, and `\'xor\'`, where values are the resulting DataFrames of the respective operations. 3. **`count_logical_results(df: pd.DataFrame, operation: str) -> dict`**: - Count the occurrences of True, False, and NA in the DataFrame resulting from the specified operation (`\'and\'`, `\'or\'`, or `\'xor\'`). - Return a dictionary with keys `\'True\'`, `\'False\'`, and `\'NA\'` and their respective counts in the DataFrame. Input: - `clean_na_values`: - `df`: A pandas DataFrame with nullable boolean values. - `fill_value`: A boolean value (True or False). - `perform_kleene_operations`: - `df1`: A pandas DataFrame with nullable boolean values. - `df2`: A pandas DataFrame with nullable boolean values of the same shape as `df1`. - `count_logical_results`: - `df`: A pandas DataFrame resulting from a logical operation. - `operation`: A string (`\'and\'`, `\'or\'`, or `\'xor\'`). Output: - `clean_na_values`: - A pandas DataFrame with NA values replaced by `fill_value`. - `perform_kleene_operations`: - A dictionary with keys `\'and\'`, `\'or\'`, `\'xor\'` and their respective resulting DataFrames from element-wise logical operations. - `count_logical_results`: - A dictionary with keys `\'True\'`, `\'False\'`, and `\'NA\'` and their respective counts. Example: ```python import pandas as pd import numpy as np df1 = pd.DataFrame({ \'A\': pd.array([True, False, pd.NA, True], dtype=\\"boolean\\"), \'B\': pd.array([False, pd.NA, True, False], dtype=\\"boolean\\") }) df2 = pd.DataFrame({ \'A\': pd.array([True, pd.NA, pd.NA, False], dtype=\\"boolean\\"), \'B\': pd.array([True, False, pd.NA, pd.NA], dtype=\\"boolean\\") }) # Example function calls cleaned_df = clean_na_values(df1, True) operations_result = perform_kleene_operations(df1, df2) count_result = count_logical_results(operations_result[\'and\'], \'and\') # Expected output # cleaned_df: # A B # 0 True False # 1 False True # 2 True True # 3 True False # operations_result: # { # \'and\': DataFrame with results of df1 & df2, # \'or\': DataFrame with results of df1 | df2, # \'xor\': DataFrame with results of df1 ^ df2 # } # count_result: # {\'True\': 2, \'False\': 3, \'NA\': 3} ``` Ensure to handle edge cases and test your solution with various inputs to confirm accuracy.","solution":"import pandas as pd def clean_na_values(df: pd.DataFrame, fill_value: bool) -> pd.DataFrame: Replace all NA values in the DataFrame with the specified fill_value. return df.fillna(fill_value) def perform_kleene_operations(df1: pd.DataFrame, df2: pd.DataFrame) -> dict: Perform element-wise logical operations (&, |, ^) between two DataFrames. operations = { \'and\': (df1 & df2).astype(\'boolean\'), \'or\': (df1 | df2).astype(\'boolean\'), \'xor\': (df1 ^ df2).astype(\'boolean\') } return operations def count_logical_results(df: pd.DataFrame, operation: str) -> dict: Count the occurrences of True, False, and NA in the DataFrame resulting from the specified operation. result = { \'True\': df.isin([True]).sum().sum(), \'False\': df.isin([False]).sum().sum(), \'NA\': df.isna().sum().sum() } return result"},{"question":"# Custom Exception Handling and Propagation In this coding assignment, you need to create a Python module that simulates a simplified file processing system. This system should be able to read from a \'file\', process the input data, and handle various types of potential errors using Python\'s exception handling mechanism. Although the de facto way of handling files in Python is using syntax such as `with open(file, \'r\')`, in this task, we are going to simulate file reading using strings to avoid I/O dependencies. Here is the detailed description of the task: Task: 1. **Custom Exception Class**: - Design a `CustomFileError` exception class that inherits from `Exception`. This should be used for any errors related to file operations. 2. **Reading Function**: - Implement a function `read_file(file_content: str) -> str` that simulates reading from a file, where `file_content` is a string representing the file content. 3. **Processing Function**: - Implement a function `process_data(data: str) -> list` that processes the data. For simplicity, assume that processing simply involves splitting the data by newlines into a list of strings. However, if the data is empty, raise a `CustomFileError` with an appropriate error message. 4. **Main Function**: - Implement a `main_function(file_content: str)` that uses both the `read_file` and `process_data` functions, and handles exceptions robustly. - If `read_file` or `process_data` raises an exception, `main_function` should handle it and print an error message stating: \\"An error occurred: <error_message>\\". Requirements: - Use Python\'s built-in exception handling constructs (`try`, `except`, `raise`) effectively. - Ensure that your exception handling covers as many potential failure points as possible while maintaining clear and maintainable code structure. Example: ```python class CustomFileError(Exception): pass def read_file(file_content: str) -> str: try: if not file_content: raise CustomFileError(\\"File content cannot be empty.\\") return file_content except CustomFileError as e: raise CustomFileError(f\\"Read error: {str(e)}\\") def process_data(data: str) -> list: try: if not data: raise CustomFileError(\\"No data to process.\\") return data.split(\\"n\\") except CustomFileError as e: raise CustomFileError(f\\"Process error: {str(e)}\\") def main_function(file_content: str): try: data = read_file(file_content) processed_data = process_data(data) print(\\"Data processed successfully:\\", processed_data) except CustomFileError as e: print(\\"An error occurred:\\", str(e)) # Example Usage file_content = \\"line1nline2nline3\\" main_function(file_content) # If file_content is an empty string file_content = \\"\\" main_function(file_content) # Should print: An error occurred: Read error: File content cannot be empty. ``` Implement the custom exception handling system as described and run the provided example usage to test your implementation. Constraints: - You can assume that `file_content` is always a string. - No actual file I/O should be performed. - Handle all potential exceptions that relate to file and data processing.","solution":"class CustomFileError(Exception): Custom exception class for file-related errors. def __init__(self, message): super().__init__(message) def read_file(file_content: str) -> str: Simulates reading from a file. Args: - file_content (str): The simulated content of the file. Returns: - str: The file content if not empty. Raises: - CustomFileError: If the file content is empty. if file_content == \\"\\": raise CustomFileError(\\"File content cannot be empty.\\") return file_content def process_data(data: str) -> list: Processes file data by splitting it into a list of lines. Args: - data (str): The data to be processed. Returns: - list: A list of lines obtained by splitting the data. Raises: - CustomFileError: If the data is empty. if data == \\"\\": raise CustomFileError(\\"No data to process.\\") return data.split(\\"n\\") def main_function(file_content: str): Main function for reading and processing file content. Args: - file_content (str): The simulated content of the file. try: data = read_file(file_content) processed_data = process_data(data) print(\\"Data processed successfully:\\", processed_data) except CustomFileError as error: print(\\"An error occurred:\\", str(error)) # Example Usage file_content = \\"line1nline2nline3\\" main_function(file_content) # If file_content is an empty string file_content = \\"\\" main_function(file_content) # Should print: An error occurred: File content cannot be empty."},{"question":"**Task**: Implement a Python function to parse an XML string and extract specific information using the `xml.parsers.expat` module. **Objective**: Write a function `extract_xml_info` that parses an XML string and extracts all the text data from elements under a given parent element along with their attributes. **Function Signature**: ```python def extract_xml_info(xml_data: str, parent_element: str) -> dict: pass ``` **Input**: - `xml_data`: A string containing the XML content. - `parent_element`: A string representing the tag name of the parent element whose child elements\' text content and attributes need to be extracted. **Output**: - Returns a dictionary where each key is the tag of a child element under the specified parent element and the value is another dictionary containing the `text` content and `attributes` of that element. **Constraints**: 1. Assume the input XML is well-formed. 2. Attributes and text content of elements are optional and may not always be present. 3. Handle nested elements appropriately, extracting content at each level under the specified parent. **Example**: ```python xml_data = <?xml version=\\"1.0\\"?> <library> <book id=\\"1\\"> <title>Python Programming</title> <author>John Doe</author> </book> <book id=\\"2\\"> <title>Advanced Python</title> <author>Jane Smith</author> </book> </library> parent_element = \\"library\\" expected_output = { \\"book\\": [ { \\"text\\": \\"\\", \\"attributes\\": {\\"id\\": \\"1\\"}, \\"children\\": { \\"title\\": {\\"text\\":\\"Python Programming\\", \\"attributes\\": {}}, \\"author\\": {\\"text\\":\\"John Doe\\", \\"attributes\\": {}} } }, { \\"text\\": \\"\\", \\"attributes\\": {\\"id\\": \\"2\\"}, \\"children\\": { \\"title\\": {\\"text\\":\\"Advanced Python\\", \\"attributes\\": {}}, \\"author\\": {\\"text\\":\\"Jane Smith\\", \\"attributes\\": {}} } } ] } result = extract_xml_info(xml_data, parent_element) print(result == expected_output) # Should print True ``` **Notes**: - The function should use `xml.parsers.expat` to parse the XML. - Define and set appropriate handlers (`StartElementHandler`, `EndElementHandler`, `CharacterDataHandler`) to capture the required information efficiently. - Ensure the function is robust and can handle various structures within the specified parent element.","solution":"import xml.parsers.expat def extract_xml_info(xml_data: str, parent_element: str) -> dict: Parses the given XML data and extracts text content and attributes of child elements under the specified parent element. parser = xml.parsers.expat.ParserCreate() extracted_data = {} current = [] current_element = None is_within_parent = False parent_depth = 0 def start_element(name, attrs): nonlocal current_element, is_within_parent, parent_depth if name == parent_element: is_within_parent = True parent_depth += 1 elif is_within_parent: if parent_depth == 1: tag_info = {\\"text\\": \\"\\", \\"attributes\\": attrs, \\"children\\": {}} if name not in extracted_data: extracted_data[name] = [] extracted_data[name].append(tag_info) current.insert(0, tag_info) else: tag_info = {\\"text\\": \\"\\", \\"attributes\\": attrs, \\"children\\": {}} if name not in current_element[\'children\']: current_element[\'children\'][name] = [] current_element[\'children\'][name].append(tag_info) current.insert(0, tag_info) current_element = current[0] parent_depth += 1 def end_element(name): nonlocal current_element, is_within_parent, parent_depth if name == parent_element: is_within_parent = False parent_depth -= 1 elif is_within_parent: current.pop(0) current_element = current[0] if current else None parent_depth -= 1 def char_data(data): if is_within_parent and current and current_element: current_element[\'text\'] += data.strip() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_data) return extracted_data"},{"question":"**Objective:** Implement a Python class `CustomSequence` that mimics the behavior of a Python list using the `PySequence_*` family of functions provided in the `python310` documentation. Your implementation should include the ability to: 1. Check if an object is a sequence. 2. Get the size of the sequence. 3. Concatenate it with another sequence. 4. Repeat the sequence a specified number of times. 5. Retrieve items by index. 6. Assign new values or slices within the sequence. 7. Delete specific items or slices within the sequence. 8. Count occurrences of a value in the sequence. 9. Check if the sequence contains a value. 10. Find the index of a value. # Class Definition You are required to define the `CustomSequence` class with the following methods: - `__init__(self, iterable)`: Initialize the sequence with the given iterable. - `is_sequence(self)`: Return `True` if the object is a sequence, `False` otherwise. - `size(self)`: Return the size of the sequence. - `concat(self, other)`: Concatenate with another sequence and return a new `CustomSequence` object. - `repeat(self, count)`: Repeat the sequence `count` times and return a new `CustomSequence` object. - `get_item(self, index)`: Return the item at the specified `index`. - `set_item(self, index, value)`: Set the item at the specified `index` to `value`. - `delete_item(self, index)`: Delete the item at the specified `index`. - `set_slice(self, start, end, iterable)`: Set the slice from `start` to `end` to the values in `iterable`. - `delete_slice(self, start, end)`: Delete the slice from `start` to `end`. - `count(self, value)`: Return the number of occurrences of `value` in the sequence. - `contains(self, value)`: Return `True` if `value` is in the sequence, `False` otherwise. - `index(self, value)`: Return the first index of `value` in the sequence. # Example Usage ```python seq = CustomSequence([1, 2, 3, 4]) print(seq.is_sequence()) # True print(seq.size()) # 4 print(seq.concat([5, 6])) # CustomSequence([1, 2, 3, 4, 5, 6]) print(seq.repeat(2)) # CustomSequence([1, 2, 3, 4, 1, 2, 3, 4]) print(seq.get_item(2)) # 3 seq.set_item(2, 10) print(seq.get_item(2)) # 10 seq.delete_item(2) print(seq.size()) # 3 seq.set_slice(1, 2, [20, 30]) print(seq.get_item(1)) # 20 seq.delete_slice(1, 3) print(seq.size()) # 2 print(seq.count(1)) # 1 print(seq.contains(1)) # True print(seq.index(1)) # 0 ``` # Constraints - You may not use Python\'s built-in list operations beyond initialization in the constructor. - Ensure your methods handle edge cases and constraints appropriately. - Performance should be considered, especially for large sequences. # Submission Submit your implementation of the `CustomSequence` class.","solution":"class CustomSequence: Custom sequence class mimicking Python lists. def __init__(self, iterable): self.sequence = list(iterable) def is_sequence(self): return isinstance(self.sequence, (list, tuple)) def size(self): return len(self.sequence) def concat(self, other): return CustomSequence(self.sequence + list(other)) def repeat(self, count): return CustomSequence(self.sequence * count) def get_item(self, index): return self.sequence[index] def set_item(self, index, value): self.sequence[index] = value def delete_item(self, index): del self.sequence[index] def set_slice(self, start, end, iterable): self.sequence[start:end] = iterable def delete_slice(self, start, end): del self.sequence[start:end] def count(self, value): return self.sequence.count(value) def contains(self, value): return value in self.sequence def index(self, value): return self.sequence.index(value)"},{"question":"<|Analysis Begin|> The provided documentation consists of a series of code snippets that demonstrate the use of Seaborn\'s `objects` module to create and customize area plots. The `healthexp` dataset is loaded and transformed, and then various customizations of the area plot are shown, such as faceting by country and mapping different attributes to colors and edges. The examples include adjusting orientation, adding overlays, and stacking areas to represent part-whole relationships. Key concepts demonstrated: 1. Data loading and transformation. 2. Creating area plots with `seaborn.objects`. 3. Customizing plot attributes like color, edgecolor, edgewidth, and orientation. 4. Adding multiple plot layers. 5. Faceting plots by categorical variables. 6. Stacking plots to show part-whole relationships. These examples provide a good basis for a challenging and comprehensive assessment question that tests the student\'s ability to: - Load and transform data. - Create and customize various types of visualizations using Seaborn. - Understand and apply advanced Seaborn concepts like faceting, color mapping, and stacking. <|Analysis End|> <|Question Begin|> **Coding Assessment Question: Advanced Seaborn Plotting** You are provided with a dataset of average yearly temperatures called `avg_temp`. The dataset has the following structure: | Year | Country | Avg_Temperature | |------|---------|-----------------| | 2000 | USA | 54.3 | | 2000 | Canada | 41.2 | | ... | ... | ... | Your task is to perform data visualization using the `seaborn` library to analyze the trends in average temperatures over the years for different countries. # Requirements 1. **Data Preparation**: - Load and parse the `avg_temp` dataset. - Ensure the data is in the correct format for plotting, similar to the transformation demonstrated in the example. 2. **Visualization Creation**: - Create a faceted area plot using the Seaborn objects API where: - Each facet represents a different country. - The x-axis represents the Year. - The y-axis represents the average temperature (`Avg_Temperature`). 3. **Customizations**: - Color the areas by the country. - Ensure each facet has a wrap of 4 (i.e., four facets in a row). - Add a line plot on top of the area plot to highlight the trend. Here is the detailed coding task: # Input - A CSV file named `avg_temp.csv` which contains the dataset with columns `Year`, `Country`, and `Avg_Temperature`. # Output - Save the resulting faceted area plot as a PNG file named `faceted_avg_temp_plot.png`. # Constraints - You may assume the dataset is complete and does not contain missing values. - Ensure that your solution is efficient and can handle up to 10,000 data points. # Implementation Guide 1. Load the dataset from `avg_temp.csv` file. 2. Transform the dataset if necessary to fit the requirements for plotting. 3. Use the `seaborn.objects` module to create and customize the area plots. 4. Save the output plot as `faceted_avg_temp_plot.png`. # Performance Requirements - The code should run efficiently with a maximum runtime of 10 seconds for datasets up to 10,000 data points. ```python import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt # Step 1: Load the dataset avg_temp = pd.read_csv(\'avg_temp.csv\') # Step 2: Data transformation (if needed) # Step 3: Create and customize the plot p = so.Plot(avg_temp, \\"Year\\", \\"Avg_Temperature\\").facet(\\"Country\\", wrap=4) p.add(so.Area(), color=\\"Country\\").add(so.Line()) # Step 4: Save the plot plt.figure(figsize=(15, 10)) p.show() plt.savefig(\'faceted_avg_temp_plot.png\') ``` **Note**: The provided code is a guideline; ensure to implement any additional transformations as required.","solution":"import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt def create_faceted_avg_temp_plot(input_csv, output_png): Creates a faceted area plot of average yearly temperatures per country using Seaborn objects API. Args: input_csv (str): The path to the input CSV file containing Year, Country, Avg_Temperature. output_png (str): The path to save the output plot as a PNG file. # Step 1: Load the dataset avg_temp = pd.read_csv(input_csv) # Step 2: Data transformation (if needed) # For this example, we assume the data is already in the correct format. # Step 3: Create and customize the plot p = so.Plot(avg_temp, x=\\"Year\\", y=\\"Avg_Temperature\\").facet(\\"Country\\", wrap=4) p.add(so.Area(), color=\\"Country\\").add(so.Line()) # Step 4: Save the plot plt.figure(figsize=(15, 10)) p.show() plt.savefig(output_png)"},{"question":"# Advanced Coding Assessment: Custom Serialization of Non-Class Objects Objective: Your task is to demonstrate an understanding of the `copyreg` module by customizing the serialization (pickling) behavior for a non-class object. You will implement a function that: - Registers a custom factory function as a constructor. - Defines and registers a custom reduction function for an object that is not a class. Input: - A custom non-class object needing special handling for its pickling process. - Necessary functions that perform the required registration using `copyreg`. Instructions: 1. Define a new type of object `MyObject` which should be a non-class object. For simplicity, you can create it as a named tuple or use a similar data structure. 2. Implement a factory function to construct and initialize instances of `MyObject`. 3. Implement a reduction function that specifies how `MyObject` should be decomposed during pickling. 4. Register this factory function as a constructor using `copyreg.constructor`. 5. Register the reduction function for `MyObject` using `copyreg.pickle`. Requirements: - The serialization process should correctly serialize and deserialize instances of `MyObject`. - Ensure that your implementation logs or prints a message when the pickling process is invoked, for verification purposes. Constraints: - The implementation must use the `copyreg` module for registering constructors and reduction functions. - The solution should be compatible with Python 3.10. Performance: - Ensure your solution is efficient and adheres to best practices for serialization in Python. Example: ```python import copyreg import pickle from collections import namedtuple # Step 1: Create a non-class object, e.g., a namedtuple MyObject = namedtuple(\'MyObject\', [\'a\', \'b\']) # Step 2: Implement a factory function for MyObject def myobject_factory(a, b): return MyObject(a, b) # Step 3: Implement a reduction function for MyObject def pickle_myobject(myobj): print(\\"Pickling MyObject instance...\\") return myobject_factory, (myobj.a, myobj.b) # Step 4: Register the factory function with copyreg.constructor copyreg.constructor(myobject_factory) # Step 5: Register the reduction function for MyObject with copyreg.pickle copyreg.pickle(MyObject, pickle_myobject) # Testing the implementation myobj = MyObject(10, 20) serialized_myobj = pickle.dumps(myobj) deserialized_myobj = pickle.loads(serialized_myobj) # Output should be indicative of correct pickling and unpickling print(deserialized_myobj) # Output should be MyObject(a=10, b=20) ``` Ensure your implementation follows the above structure and meets the requirements.","solution":"import copyreg import pickle from collections import namedtuple # Step 1: Create a non-class object, e.g., a namedtuple MyObject = namedtuple(\'MyObject\', [\'a\', \'b\']) # Step 2: Implement a factory function for MyObject def myobject_factory(a, b): return MyObject(a, b) # Step 3: Implement a reduction function for MyObject def pickle_myobject(myobj): print(\\"Pickling MyObject instance...\\") return myobject_factory, (myobj.a, myobj.b) # Step 4: Register the factory function with copyreg.constructor copyreg.constructor(myobject_factory) # Step 5: Register the reduction function for MyObject with copyreg.pickle copyreg.pickle(MyObject, pickle_myobject) # Actual implementation is correct if instances of MyObject can be serialized and deserialized correctly."},{"question":"# Assessment Question **Objective:** Implement a Python function that utilizes iterators, generators, and higher-order functions to process and manipulate a stream of data. **Problem Statement:** You are given a list of dictionaries, where each dictionary represents a student\'s record containing their `name` (string) and `score` (integer). Your task is to process this data to generate a report which includes the names of the top 5 students based on their scores. # Detailed Steps: 1. **Filtering and Mapping**: - Filter out students who have a score less than 50. - Convert the records into a more manageable form, such as tuples containing the student\'s name and score. 2. **Sorting**: - Sort the filtered list based on the scores in descending order. 3. **Generating Report**: - Use a generator function to yield the names of the top 5 students one by one. # Instructions: - Implement a function `generate_top_students_report(students: List[Dict[str, Union[str, int]]]) -> Generator[str, None, None]` that performs the specified operations. - Use list comprehensions, generator expressions, and higher-order functions where appropriate. - Ensure your function handles edge cases, such as when there are fewer than 5 students remaining after filtering. # Input: - A list of dictionaries with the following structure: ```python students = [ {\'name\': \'Alice\', \'score\': 90}, {\'name\': \'Bob\', \'score\': 85}, {\'name\': \'Charlie\', \'score\': 40}, ... ] ``` # Output: - A generator that yields names of the top 5 students based on their scores. # Example: ```python students = [ {\'name\': \'Alice\', \'score\': 90}, {\'name\': \'Bob\', \'score\': 85}, {\'name\': \'Charlie\', \'score\': 40}, {\'name\': \'David\', \'score\': 95}, {\'name\': \'Eve\', \'score\': 88}, {\'name\': \'Frank\', \'score\': 72}, {\'name\': \'Grace\', \'score\': 60}, {\'name\': \'Heidi\', \'score\': 59} ] top_students = generate_top_students_report(students) print(list(top_students)) ``` Expected Output: ``` [\'David\', \'Alice\', \'Eve\', \'Bob\', \'Frank\'] ``` # Constraints: - The scores of students will be integers ranging from 0 to 100. - The names of students will be non-empty strings. - The list of students will have at most 1000 entries. # Notes: - Only import necessary modules. - Ensure proper error handling and code readability.","solution":"from typing import List, Dict, Union, Generator def generate_top_students_report(students: List[Dict[str, Union[str, int]]]) -> Generator[str, None, None]: Processes the student records to generate names of top 5 students based on their scores. Parameters: students (List[Dict[str, Union[str, int]]]): List of dictionaries with student records containing \'name\' and \'score\' Returns: Generator: A generator that yields names of top 5 students based on their scores # Step 1: Filter students with score >= 50 filtered_students = filter(lambda student: student[\'score\'] >= 50, students) # Step 2: Convert records into tuples (name, score) student_tuples = map(lambda student: (student[\'name\'], student[\'score\']), filtered_students) # Step 3: Sort tuples based on scores in descending order sorted_students = sorted(student_tuples, key=lambda student: student[1], reverse=True) # Step 4: Generate names of top 5 students top_students = sorted_students[:5] # Generate names one by one for student in top_students: yield student[0]"},{"question":"# Question: Working with WAV Files and Color Conversion You are tasked with creating a Python script that performs the following two main operations: 1. **Processing a WAV File**: - Read a given WAV file and extract its properties (like number of channels, sample width, frame rate, number of frames, and duration). - Modify the audio data by applying a simple transformation (e.g., amplify the audio by a given factor). - Save the modified audio to a new WAV file. 2. **Color Conversion**: - Read RGB values from a given list and convert them to the HSV color system using the `colorsys` module. - Output both the original and converted color values. Detailed Specifications **1. Processing a WAV File** - Implement a function `process_wav(input_file: str, output_file: str, amplify_factor: float) -> dict` that: - Reads `input_file`, a WAV file. - Amplifies the audio data by `amplify_factor`. - Writes the modified audio data to `output_file`. - Returns a dictionary with the following details of the input file: - `\'num_channels\'`: int - `\'sample_width\'`: int - `\'frame_rate\'`: int - `\'num_frames\'`: int - `\'duration\'`: float (duration in seconds) **2. Color Conversion** - Implement a function `convert_colors(rgb_list: list) -> list` that: - Takes `rgb_list`, a list of tuples where each tuple contains three integers representing an RGB color (e.g., `(255, 0, 0)` for red). - Converts each RGB tuple to an HSV tuple using the `colorsys` module. - Returns a list of tuples where each tuple contains both the original RGB value and the converted HSV value (RGB values should be normalized to the [0, 1] range for conversion). **Constraints** - Assume the WAV files are in proper format. - RGB values are in the range of 0-255. **Example** ```python # Example usage of process_wav audio_details = process_wav(\'input.wav\', \'output.wav\', 1.5) print(audio_details) # Output could be: # {\'num_channels\': 2, \'sample_width\': 2, \'frame_rate\': 44100, \'num_frames\': 220500, \'duration\': 5.0} # Example usage of convert_colors color_conversions = convert_colors([(255, 0, 0), (0, 255, 0), (0, 0, 255)]) print(color_conversions) # Output could be: # [((255, 0, 0), (0.0, 1.0, 1.0)), ((0, 255, 0), (0.3333333333333333, 1.0, 1.0)), ((0, 0, 255), (0.6666666666666666, 1.0, 1.0))] ```","solution":"import wave import struct import colorsys def process_wav(input_file: str, output_file: str, amplify_factor: float) -> dict: with wave.open(input_file, \'rb\') as wav_in: params = wav_in.getparams() num_channels = params.nchannels sample_width = params.sampwidth frame_rate = params.framerate num_frames = params.nframes duration = num_frames / frame_rate audio_data = wav_in.readframes(num_frames) output_data = bytearray() for i in range(num_frames * num_channels): sample = audio_data[i * sample_width:(i + 1) * sample_width] sample_int = int.from_bytes(sample, \'little\', signed=True) new_sample_int = int(sample_int * amplify_factor) new_sample = new_sample_int.to_bytes(sample_width, \'little\', signed=True) output_data.extend(new_sample) with wave.open(output_file, \'wb\') as wav_out: wav_out.setparams(params) wav_out.writeframes(output_data) return { \'num_channels\': num_channels, \'sample_width\': sample_width, \'frame_rate\': frame_rate, \'num_frames\': num_frames, \'duration\': duration } def convert_colors(rgb_list: list) -> list: hsv_list = [] for r, g, b in rgb_list: r_norm, g_norm, b_norm = r / 255.0, g / 255.0, b / 255.0 h, s, v = colorsys.rgb_to_hsv(r_norm, g_norm, b_norm) hsv_list.append(((r, g, b), (h, s, v))) return hsv_list"},{"question":"**Objective:** The objective of this coding assessment question is to evaluate the understanding of introspecting the execution environment in Python using the concepts described in the documentation. **Task:** Write a Python function `trace_execution(frame_depth: int) -> str` that accesses and returns a string representation of the current execution state up to a specified depth of frames. The string representation should include the following details for each frame: - The function name. - A brief description of the type of function. - The current line number being executed in that frame. - The local variables in that frame. - The global variables in that frame. **Input:** - `frame_depth` (int): The number of frames to trace back from the current frame. **Output:** - A string representing the execution state up to the specified frame depth. **Constraints:** - You must use the functions provided in the documentation to access and retrieve the necessary information. - The output should be clearly formatted, with each frame\'s details separated by a newline. **Performance Requirements:** - The function should efficiently trace and format the execution state information without causing significant performance overhead. **Example:** ```python def example_function(): a = 10 b = 20 result = trace_execution(2) print(result) def another_function(): example_function() another_function() ``` Expected output: ``` Frame 1: Function: example_function Type: function Line Number: 4 Locals: {\'a\': 10, \'b\': 20} Globals: {...} # List of global variables Frame 2: Function: another_function Type: function Line Number: 9 Locals: {} Globals: {...} # List of global variables ``` Notes: - Proper error handling should be implemented in case of invalid frames or if the frame depth exceeds the actual stack depth. - Ensure the output is human-readable and well-formatted. Good luck!","solution":"import inspect def trace_execution(frame_depth: int) -> str: Trace the current execution state up to the specified depth of frames. Parameters: frame_depth (int): The number of frames to trace back from the current frame. Returns: str: A string representation of the execution state. frames = inspect.stack() result = [] # Adjust frame_depth to avoid IndexError if it\'s larger than the actual stack size. frame_depth = min(frame_depth, len(frames) - 1) for i in range(1, frame_depth + 1): frame_info = frames[i] frame = frame_info.frame function_name = frame_info.function line_number = frame_info.lineno local_vars = frame.f_locals global_vars = {key: \\"...\\" for key in frame.f_globals} # representing globals as {...} for simplicity frame_representation = fFrame {i}: Function: {function_name} Type: {type(frames[i]).__name__} Line Number: {line_number} Locals: {local_vars} Globals: {global_vars} result.append(frame_representation) return \\"n\\".join(result)"},{"question":"# Problem Description You are tasked with implementing a Python function that reads binary data from a given input file, encodes it using quoted-printable encoding, writes the encoded data to an output file, then reads the encoded data back, decodes it, and writes the decoded data to another output file. This process should verify the integrity of the encoded and then decoded data. # Function Signature ```python def encode_decode_file(input_file_path: str, encoded_file_path: str, decoded_file_path: str, quotetabs: bool, header: bool) -> None: Encodes and decodes the contents of the input binary file using quoted-printable encoding, and writes the intermediary encoded and final decoded data to their respective files. :param input_file_path: Path to the input binary file to be encoded. :param encoded_file_path: Path to the file to store encoded data. :param decoded_file_path: Path to the file to store decoded data. :param quotetabs: Boolean flag to determine whether to encode embedded spaces and tabs. :param header: Boolean flag to control encoding and decoding of spaces as underscores. ``` # Input - `input_file_path` (str): The file path to the binary file that needs to be encoded. - `encoded_file_path` (str): The file path where the encoded data will be written. - `decoded_file_path` (str): The file path where the decoded data will be written. - `quotetabs` (bool): Whether to encode embedded spaces and tabs. - `header` (bool): Whether to handle spaces in headers mode (encode as underscores or decode underscores as spaces). # Output - The function does not return anything. It writes the encoded and decoded data to the specified files. # Constraints - The function should handle binary data correctly. - Ensure that the initial input data matches the final decoded output data when written to their corresponding files. - Appropriate error handling should be included for file operations. # Example Suppose `input_file_path` points to a file containing binary data `b\'This is a test input.nLet\'s encode and decode it!\'`. ```python encode_decode_file(\\"input.bin\\", \\"encoded.bin\\", \\"decoded.bin\\", True, False) ``` After execution: - \\"encoded.bin\\" should contain the quoted-printable encoded data. - \\"decoded.bin\\" should contain the original binary data `b\'This is a test input.nLet\'s encode and decode it!\'`. # Additional Information - Ensure file objects are opened and closed properly. - Make use of the `quopri` module for encoding and decoding tasks. - The integrity of the data should be maintained through the process. # Evaluation Criteria - Correctness: The initial data should match the final decoded data. - File Handling: Proper handling of file operations (opening, reading, writing, and closing files). - Usage of quopri module: Correctly using the `quopri.encode` and `quopri.decode` functions.","solution":"import quopri def encode_decode_file(input_file_path: str, encoded_file_path: str, decoded_file_path: str, quotetabs: bool, header: bool) -> None: Encodes and decodes the contents of the input binary file using quoted-printable encoding, and writes the intermediary encoded and final decoded data to their respective files. :param input_file_path: Path to the input binary file to be encoded. :param encoded_file_path: Path to the file to store encoded data. :param decoded_file_path: Path to the file to store decoded data. :param quotetabs: Boolean flag to determine whether to encode embedded spaces and tabs. :param header: Boolean flag to control encoding and decoding of spaces as underscores. # Read the binary data from the input file with open(input_file_path, \'rb\') as input_file: binary_data = input_file.read() # Encode the binary data using quoted-printable encoding encoded_data = quopri.encodestring(binary_data, quotetabs=quotetabs, header=header) # Write the encoded data to the encoded file with open(encoded_file_path, \'wb\') as encoded_file: encoded_file.write(encoded_data) # Decode the encoded data decoded_data = quopri.decodestring(encoded_data, header=header) # Write the decoded data to the decoded file with open(decoded_file_path, \'wb\') as decoded_file: decoded_file.write(decoded_data)"},{"question":"**Integer Conversion and Error Handling in Python** As part of developing a Python library that mimics some of the Python C API functionalities, you are tasked with implementing functions that simulate the behavior of integer conversions and error handling. **Your task:** 1. Implement a function `long_to_pylong(value: int) -> int` that converts a C long integer (`value`) to a Python integer. The function should raise an `OverflowError` if the value exceeds Python\'s `int` limit. 2. Implement a function `pylong_to_long(pylong: int) -> int` that converts a Python integer back to a C long integer. Ensure that if the conversion exceeds the C long integer range, it raises an `OverflowError`. 3. Implement a function `string_to_pylong(s: str, base: int = 10) -> int` that converts a string `s` representing a number in the specified `base` to a Python integer. Validate that: - The `base` is within the valid range (2 to 36). - The string contains valid characters for the given base number system. - Raises a `ValueError` for invalid inputs. 4. Create a function `pylong_to_string(pylong: int, base: int = 10) -> str` that converts a Python integer to its string representation in the specified `base`. Ensure that: - The `base` is within the valid range (2 to 36). - The output string adheres to the standard conventions for representing numbers in the specified base. **Constraints:** - You must not use any built-in functions like `int()` with a `base` parameter for direct conversion. - Handle all potential edge cases, including large integers and invalid inputs. - Each function should use proper error handling mechanisms to match Python C API\'s behavior as closely as possible. **Example Usage:** ```python try: print(long_to_pylong(12345678901234567890)) # should work fine print(long_to_pylong(2**60)) # should raise OverflowError except OverflowError as e: print(e) try: print(pylong_to_long(123456)) # should work fine print(pylong_to_long(2**50)) # should raise OverflowError except OverflowError as e: print(e) try: print(string_to_pylong(\\"1A2B\\", 16)) # should return equivalent integer print(string_to_pylong(\\"1234\\", 2)) # should raise ValueError except ValueError as e: print(e) try: print(pylong_to_string(466321, 16)) # should return \\"71D61\\" print(pylong_to_string(466321, 40)) # should raise ValueError except ValueError as e: print(e) ``` **Note:** Your implementation should strictly follow the outlined behavior and validate each step thoroughly. Good luck!","solution":"def long_to_pylong(value): Convert a C \'long\' to a Python int. Raise OverflowError if the value exceeds the limit of Python int. try: return int(value) except OverflowError: raise OverflowError(\\"value exceeds the limit of Python int\\") def pylong_to_long(pylong): Convert a Python int to a C \'long\'. Raise OverflowError if the value exceeds the range of C \'long\'. LIM = 2**63 - 1 # assuming a 64-bit C \'long\' range if pylong < -LIM or pylong > LIM: raise OverflowError(\\"value exceeds the limit of C \'long\'\\") return pylong def string_to_pylong(s, base=10): Convert a string in a given base to a Python int. Raise ValueError for invalid inputs. if not (2 <= base <= 36): raise ValueError(\\"base must be between 2 and 36\\") try: return int(s, base) except ValueError as e: raise ValueError(f\\"invalid literal for base {base}: {s}\\") def pylong_to_string(pylong, base=10): Convert a Python int to its string representation in a given base. Raise ValueError if the base is not between 2 and 36. if not (2 <= base <= 36): raise ValueError(\\"base must be between 2 and 36\\") if base == 10: return str(pylong) digits = [] is_negative = pylong < 0 pylong = abs(pylong) while pylong: digits.append(\\"0123456789abcdefghijklmnopqrstuvwxyz\\"[pylong % base]) pylong //= base if is_negative: digits.append(\'-\') return \'\'.join(reversed(digits)) or \'0\'"},{"question":"Coding Assessment Question You are provided with the `tips` dataset from seaborn\'s dataset repository. Your task is divided into multiple parts, requiring you to visualize relationships between variables using different types of regression models. This will assess your understanding of seabornâ€™s `regplot` and `lmplot` functions and their parameters. # Part 1: Simple Linear Regression 1. Load the `tips` dataset from seaborn. 2. Create a scatterplot with a linear regression line to visualize the relationship between `total_bill` (x-axis) and `tip` (y-axis). 3. Display a 95% confidence interval for the regression line. # Part 2: Polynomial Regression 1. Plot the relationship between `total_bill` and `tip` using a polynomial regression model of order 2. 2. Customize the plot to not display the confidence interval. # Part 3: Logistic Regression 1. Create a new binary column `big_tip` in the `tips` dataset where the value is `True` if the tip is greater than 15% of the total bill, otherwise `False`. 2. Plot the relationship between `total_bill` and `big_tip` using a logistic regression model. 3. Use jitter for the `y` values to better visualize the scatter. # Part 4: Faceted Linear Regression 1. Visualize the relationship between `total_bill` and `tip`, but this time use `tip` to group the data by the `smoker` status. 2. Display separate regression lines for smokers and non-smokers on the same plot using different markers and colors. # Part 5: Checking Model Appropriateness 1. Use a residual plot to visualize whether a linear regression between `total_bill` and `tip` is appropriate. 2. Interpret the plot and provide a brief reasoning for the structure you observe in the residuals. # Input and Output Specifications - **Input Format**: The function does not take any input arguments. You should ensure that all necessary imports and dataset loading are appropriately handled within the function. - **Output Format**: No explicit return is expected. However, multiple visual outputs should be generated as specified in the tasks. # Performance Requirements - Ensure that your code runs efficiently without unnecessary computations. - Utilize seaborn\'s capabilities to manage data and plots optimally. Provide clear and well-commented code for each part to demonstrate your understanding and approach.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_tips_data(): # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Part 1: Simple Linear Regression plt.figure(figsize=(8, 6)) sns.regplot(x=\'total_bill\', y=\'tip\', data=tips, ci=95) plt.title(\'Linear Regression of Tip vs Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Part 2: Polynomial Regression plt.figure(figsize=(8, 6)) sns.regplot(x=\'total_bill\', y=\'tip\', data=tips, order=2, ci=None) plt.title(\'Polynomial Regression (Order 2) of Tip vs Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Part 3: Logistic Regression tips[\'big_tip\'] = tips[\'tip\'] > (0.15 * tips[\'total_bill\']) plt.figure(figsize=(8, 6)) sns.regplot(x=\'total_bill\', y=\'big_tip\', data=tips, logistic=True, y_jitter=0.03) plt.title(\'Logistic Regression of Big Tip vs Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Big Tip (True if Tip > 15%)\') plt.show() # Part 4: Faceted Linear Regression by Smoker Status plt.figure(figsize=(8, 6)) sns.lmplot(x=\'total_bill\', y=\'tip\', hue=\'smoker\', markers=[\\"o\\", \\"x\\"], data=tips) plt.title(\'Linear Regression of Tip vs Total Bill by Smoker Status\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.show() # Part 5: Residual Plot for Checking Linear Regression Appropriateness plt.figure(figsize=(8, 6)) sns.residplot(x=\'total_bill\', y=\'tip\', data=tips) plt.title(\'Residuals of Linear Regression of Tip vs Total Bill\') plt.xlabel(\'Total Bill\') plt.ylabel(\'Residuals\') plt.show() # Interpretation: # If the residual plot exhibits a random pattern (i.e., no obvious structure), # it suggests that the linear regression model is appropriate. # If there are clear patterns, non-linearity, or systematic structure in the residuals, # this indicates that a linear model may not be the best fit, and other models (such as polynomial) may be more suitable."},{"question":"**Coding Question:** # Task Create a Python script that takes multiple directories and compresses them into a single ZIP archive. The script should also generate a report in JSON format containing details of each file in the ZIP archive, including the file\'s original path, its size before compression, size after compression, and the compression rate. # Requirements 1. **Function Definitions**: - **compress_directories_to_zip(directories: List[str], output_zip_file: str) -> None**: Compresses the given directories into a single ZIP archive. - **generate_zip_report(zip_file: str) -> str**: Generates a report in JSON format with details about each file in the ZIP archive. 2. **Input and Output**: - **compress_directories_to_zip**: - Input: - `directories` - a list of directory paths to be compressed. - `output_zip_file` - the path to the output ZIP file. - Output: None (Creates the ZIP file as a side effect). - **generate_zip_report**: - Input: - `zip_file` - the path to the ZIP file. - Output: - A JSON string containing a report with the following format: ```json [ { \\"original_path\\": \\"path/to/file\\", \\"compressed_size\\": 123, \\"original_size\\": 456, \\"compression_rate\\": 0.73 }, ... ] ``` # Constraints - The script must handle large directories and files efficiently. - Use the `zipfile` module. - The compression rate is calculated as `(compressed_size / original_size)`. - Ensure that the report generation handles any potential exceptions, such as files that cannot be read. # Example ```python from typing import List import json import os from zipfile import ZipFile, ZIP_DEFLATED, ZipInfo def compress_directories_to_zip(directories: List[str], output_zip_file: str) -> None: # TODO: Implement this function pass def generate_zip_report(zip_file: str) -> str: # TODO: Implement this function pass # Usage directories_to_compress = [\\"/path/to/dir1\\", \\"/path/to/dir2\\"] output_zip_file = \\"output.zip\\" compress_directories_to_zip(directories_to_compress, output_zip_file) report_json = generate_zip_report(output_zip_file) print(report_json) ``` # Note - Be sure to include error handling in your implementation. - The output JSON should be formatted with an indentation of 4 spaces for better readability.","solution":"import json import os from typing import List from zipfile import ZipFile, ZIP_DEFLATED def compress_directories_to_zip(directories: List[str], output_zip_file: str) -> None: with ZipFile(output_zip_file, \'w\', ZIP_DEFLATED) as zip_file: for directory in directories: for foldername, subfolders, filenames in os.walk(directory): for filename in filenames: filepath = os.path.join(foldername, filename) arcname = os.path.relpath(filepath, start=os.path.commonpath(directories)) zip_file.write(filepath, arcname) def generate_zip_report(zip_file: str) -> str: report = [] with ZipFile(zip_file, \'r\') as z: for zip_info in z.infolist(): original_size = zip_info.file_size compressed_size = zip_info.compress_size compression_rate = float(compressed_size) / original_size if original_size > 0 else 0 report.append({ \\"original_path\\": zip_info.filename, \\"compressed_size\\": compressed_size, \\"original_size\\": original_size, \\"compression_rate\\": compression_rate }) return json.dumps(report, indent=4)"},{"question":"**Question: Exploring Relationships with Seaborn\'s `regplot`** Your task is to demonstrate your understanding of Seaborn by creating a set of visualizations that explore relationships within a dataset. Specifically, you will use the `sns.regplot` function to showcase different types of regression plots, handle discrete variables, and customize plots. # Part 1: Load the Dataset Seaborn comes with several built-in datasets. For this task, you will use the `mpg` dataset. Load this dataset into a DataFrame. # Part 2: Plot Relationships Create the following plots using the `sns.regplot` function: 1.1 Simple Linear Regression: Plot a simple linear regression showing the relationship between `weight` (x-axis) and `acceleration` (y-axis). 1.2 Polynomial Regression: Plot a second-order polynomial regression showing the relationship between `weight` (x-axis) and `mpg` (y-axis). 1.3 Log-Linear Regression: Plot a log-linear regression showing the relationship between `displacement` (x-axis) and `mpg` (y-axis). # Part 3: Customization and Advanced Plots 2.1 Logistic Regression: Plot a logistic regression where `weight` (x-axis) is compared with a binary variable indicating whether the car\'s origin is the USA (`y=1`) or not (`y=0`). 2.2 Robust Regression: Plot a robust regression showing the relationship between `horsepower` (x-axis) and `weight` (y-axis). 2.3 Categorical Variable: Handle the discrete `cylinders` variable by plotting the relationship between `cylinders` (x-axis) and `weight` (y-axis) with jitter added to the `x` variable. # Part 4: Custom Plot Create a customized regression plot showing the relationship between `weight` (x-axis) and `horsepower` (y-axis) with the following customizations: - Set the confidence interval to 99%. - Use `marker=\'x\'` for data points. - Set the color of the data points to `\'.3\'`. - Set the color of the regression line to `\'r\'`. # Constraints and Requirements: - **DataFrame**: The dataset should be loaded into a Pandas DataFrame. - **Plot Library**: Use Seaborn\'s `regplot` function for all plots. - **Customization**: Ensure you apply the specified customizations to the plots. - **Output**: Display all the plots in the output. # Example Output: ```python import numpy as np import seaborn as sns import pandas as pd # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Part 2: Plot Relationships sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\") sns.regplot(data=mpg, x=\\"weight\\", y=\\"mpg\\", order=2) sns.regplot(data=mpg, x=\\"displacement\\", y=\\"mpg\\", logx=True) # Part 3: Customization and Advanced Plots sns.regplot(x=mpg[\\"weight\\"], y=mpg[\\"origin\\"].eq(\\"usa\\").rename(\\"from_usa\\"), logistic=True) sns.regplot(data=mpg, x=\\"horsepower\\", y=\\"weight\\", robust=True) sns.regplot(data=mpg, x=\\"cylinders\\", y=\\"weight\\", x_jitter=.15) # Part 4: Custom Plot sns.regplot( data=mpg, x=\\"weight\\", y=\\"horsepower\\", ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"r\\"), ) ``` In your final output, all plots should be displayed inline with the respective customizations. Ensure your code is clear and well-commented.","solution":"import numpy as np import seaborn as sns import pandas as pd # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Part 2: Plot Relationships # 1.1 Simple Linear Regression sns.regplot(data=mpg, x=\\"weight\\", y=\\"acceleration\\")"},{"question":"<|Analysis Begin|> The given documentation focuses on different ways to create histograms using the `seaborn.objects` module in the Seaborn package. It explains how to visualize distributions of a dataset using bar plots and area plots, adjusting the bin granularity, normalizing counts, and handling grouped data. # Key Points Covered: 1. Basic bar plots for categorical variables. 2. Univariate distribution visualization using `so.Bars()` and `so.Hist()`. 3. Adjusting bin numbers and widths. 4. Normalizing counts to display proportions. 5. Using grouping and facet operations to display data for different subsets. 6. Displaying overlapping distributions using `so.Area()`. 7. Stacking bars using `so.Stack()`. # Fundamental and Advanced Concepts: - Creating plots using Seaborn objects. - Adjusting bin parameters in histograms. - Normalizing histograms to display proportions. - Handling and visualizing grouped data. - Using different mark types (`so.Bars()`, `so.Area()`, `so.Stack()`) to represent data. <|Analysis End|> <|Question Begin|> # Seaborn Histogram and Distribution Plotting You are provided with a dataset called \\"penguins\\" that contains information about different penguin species. Using seaborn\'s `objects` interface, your task is to create histograms and visualizations that display the distribution of certain attributes. **Requirements:** 1. Write a function `plot_penguin_distributions()` that takes no parameters and performs the following: - Loads the `penguins` dataset using `load_dataset(\\"penguins\\")`. - Creates a histogram for the `flipper_length_mm` attribute. Adjust the histogram to have 30 bins. - Creates a proportional histogram for the `flipper_length_mm` attribute, differentiating by the `sex` attribute using color. - Creates an area plot for the `body_mass_g` attribute, differentiating by `species`. - Creates a stacked bar plot for the `bill_length_mm` attribute, differentiating by both the `species` and `sex` attributes. 2. The histograms and plots should be displayed sequentially in a single figure for comparison. **Constraints:** - Ensure seaborn and other required plotting libraries (matplotlib) are imported. - Handle any missing values in the dataset by removing rows with missing attributes before plotting. - Use appropriate titles and labels for each plot to make them self-explanatory. **Expected Output:** - The function should generate a figure with the specified plots illustrating the differences and relationships in the distributions of the penguin attributes. ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_penguin_distributions(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Drop missing values penguins = penguins.dropna() # Create a figure for subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Histogram for flipper_length_mm with 30 bins so.Plot(penguins, \'flipper_length_mm\').add(so.Bars(), so.Hist(bins=30)).on(axes[0, 0]) axes[0, 0].set_title(\'Flipper Length Distribution (30 bins)\') # Proportional histogram for flipper_length_mm by sex so.Plot(penguins, \'flipper_length_mm\', color=\'sex\').add(so.Bars(), so.Hist(stat=\'proportion\')).on(axes[0, 1]) axes[0, 1].set_title(\'Proportional Flipper Length Distribution by Sex\') # Area plot for body_mass_g by species so.Plot(penguins, \'body_mass_g\', color=\'species\').add(so.Area(), so.Hist()).on(axes[1, 0]) axes[1, 0].set_title(\'Body Mass Distribution by Species\') # Stacked bar plot for bill_length_mm by species and sex so.Plot(penguins, \'bill_length_mm\', color=\'sex\').facet(\'species\').add(so.Bars(), so.Hist(), so.Stack()).on(axes[1, 1]) axes[1, 1].set_title(\'Stacked Bill Length Distribution by Species and Sex\') # Display the plots plt.tight_layout() plt.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_penguin_distributions(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Drop missing values penguins = penguins.dropna() # Create a figure for subplots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Histogram for flipper_length_mm with 30 bins so.Plot(penguins, \'flipper_length_mm\').add(so.Bars(), so.Hist(bins=30)).on(axes[0, 0]) axes[0, 0].set_title(\'Flipper Length Distribution (30 bins)\') # Proportional histogram for flipper_length_mm by sex so.Plot(penguins, \'flipper_length_mm\', color=\'sex\').add(so.Bars(), so.Hist(stat=\'proportion\')).on(axes[0, 1]) axes[0, 1].set_title(\'Proportional Flipper Length Distribution by Sex\') # Area plot for body_mass_g by species so.Plot(penguins, \'body_mass_g\', color=\'species\').add(so.Area(), so.Hist()).on(axes[1, 0]) axes[1, 0].set_title(\'Body Mass Distribution by Species\') # Stacked bar plot for bill_length_mm by species and sex so.Plot(penguins, \'bill_length_mm\', color=\'sex\').add(so.Bars(), so.Hist(), so.Stack()).facet(\'species\').on(axes[1, 1]) axes[1, 1].set_title(\'Stacked Bill Length Distribution by Species and Sex\') # Display the plots plt.tight_layout() plt.show()"},{"question":"Objective: To test the ability to parse, manipulate, and interact with XML data using the `xml.dom` module in Python. # Background: You will be provided with an XML document as a string. Your task is to parse this XML, perform specific operations, and output the modified XML string. # XML Document: ```xml <root> <student id=\\"101\\"> <name>John Doe</name> <grade>A</grade> </student> <student id=\\"102\\"> <name>Jane Smith</name> <grade>B</grade> </student> </root> ``` # Tasks: 1. Parse the given XML string to create a DOM document object. 2. Add a new student element to the root with the following details: - id: 103 - name: Alice Johnson - grade: A+ 3. Update the grade of the student with id \\"101\\" to \\"A+\\". 4. Remove the student with id \\"102\\". 5. Return the modified XML document as a string. # Constraints: - You must use the `xml.dom` module to perform all operations. # Function Signature: ```python def modify_xml(xml_string: str) -> str: # Your code here ``` # Example: ```python input_xml = <root> <student id=\\"101\\"> <name>John Doe</name> <grade>A</grade> </student> <student id=\\"102\\"> <name>Jane Smith</name> <grade>B</grade> </student> </root> expected_output_xml = <root> <student id=\\"101\\"> <name>John Doe</name> <grade>A+</grade> </student> <student id=\\"103\\"> <name>Alice Johnson</name> <grade>A+</grade> </student> </root> assert modify_xml(input_xml) == expected_output_xml ``` # Hints: - Use `xml.dom.minidom.parseString()` to parse the XML string. - Use methods like `createElement`, `createTextNode`, `appendChild`, and `removeChild` to manipulate the DOM. - Use `toprettyxml()` or similar methods to convert the DOM back to a string.","solution":"from xml.dom.minidom import parseString, Document def modify_xml(xml_string: str) -> str: # Parse the XML string dom = parseString(xml_string) # Add a new student element to the root root = dom.documentElement new_student = dom.createElement(\\"student\\") new_student.setAttribute(\\"id\\", \\"103\\") name = dom.createElement(\\"name\\") name_text = dom.createTextNode(\\"Alice Johnson\\") name.appendChild(name_text) grade = dom.createElement(\\"grade\\") grade_text = dom.createTextNode(\\"A+\\") grade.appendChild(grade_text) new_student.appendChild(name) new_student.appendChild(grade) root.appendChild(new_student) # Update the grade of the student with id 101 to A+ students = root.getElementsByTagName(\\"student\\") for student in students: if student.getAttribute(\\"id\\") == \\"101\\": grade = student.getElementsByTagName(\\"grade\\")[0] grade.firstChild.nodeValue = \\"A+\\" break # Remove the student with id 102 for student in students: if student.getAttribute(\\"id\\") == \\"102\\": root.removeChild(student) break # Return the modified XML document as a string return dom.toprettyxml(indent=\\" \\")"},{"question":"**Objective:** Implement a function to perform data validation and apply Singular Value Decomposition (SVD) using scikit-learn\'s utilities to ensure efficient and accurate computations. # Problem Statement: You are given a dataset in the form of a 2D NumPy array. Your task is to write a function `validate_and_decompose(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]` to: 1. Validate the data using scikit-learn\'s utilities to ensure it meets the required criteria. 2. If the data is valid, perform Singular Value Decomposition (SVD) on the dataset using scikit-learn\'s `extmath.randomized_svd` utility for efficient computation. # Requirements: 1. **Data Validation:** - Use `check_array` from `sklearn.utils` to ensure the input `X` is a 2D array of finite values. - Raise a ValueError if `X` is not valid. 2. **Data Decomposition:** - Use `randomized_svd` from `sklearn.utils.extmath` to compute the truncated SVD of the input data. - Return the matrices `U`, `Sigma`, and `VT` resulting from the decomposition. # Input: - `X`: A 2D NumPy array of shape `(m, n)` representing the dataset. # Output: - A tuple of three 2D NumPy arrays `(U, Sigma, VT)` where: - `U` has shape `(m, k)`, - `Sigma` has shape `(k,)`, - `VT` has shape `(k, n)`. # Constraints: - The input array `X` must be a 2D array of finite values. - You must use the `check_array` utility for validation. - You must use `randomized_svd` for decomposition. # Example: ```python import numpy as np X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) U, Sigma, VT = validate_and_decompose(X) print(\\"U:\\", U) print(\\"Sigma:\\", Sigma) print(\\"VT:\\", VT) ``` # Implementation: ```python from sklearn.utils import check_array from sklearn.utils.extmath import randomized_svd import numpy as np from typing import Tuple def validate_and_decompose(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: # Validate the input array X = check_array(X, ensure_2d=True, allow_nd=False, force_all_finite=True) # Compute the truncated SVD U, Sigma, VT = randomized_svd(X, n_components=min(X.shape), random_state=None) return U, Sigma, VT ``` Your implementation will be evaluated on correctness, efficiency and adherence to scikit-learn best practices as described.","solution":"from sklearn.utils import check_array from sklearn.utils.extmath import randomized_svd import numpy as np from typing import Tuple def validate_and_decompose(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]: Validates the input array and performs Singular Value Decomposition. Parameters: X (np.ndarray): A 2D NumPy array representing the dataset. Returns: Tuple[np.ndarray, np.ndarray, np.ndarray]: The matrices U, Sigma, and VT resulting from SVD. # Validate the input array X = check_array(X, ensure_2d=True, allow_nd=False, force_all_finite=True) # Compute the truncated SVD U, Sigma, VT = randomized_svd(X, n_components=min(X.shape), random_state=None) return U, Sigma, VT"},{"question":"# Python Coding Assessment Question **Objective:** Implement a generic function using advanced concepts from the `typing` module such as generics and type aliases. **Problem Statement:** You are required to implement a function `batch_process(items: List[T], processor: Callable[[T], R]) -> List[R]` where: - `items`: A list of elements of any data type. - `processor`: A callable that takes an element of type `T` and processes it to return a result of type `R`. The goal is to apply the `processor` function to each element in `items` and return a list of the results. # Requirements: 1. Use the `List`, `Callable`, and `TypeVar` from the `typing` module. 2. Ensure that the function is fully type-hinted to reflect the use of generics. # Function Signature: ```python from typing import List, Callable, TypeVar T = TypeVar(\'T\') R = TypeVar(\'R\') def batch_process(items: List[T], processor: Callable[[T], R]) -> List[R]: pass ``` # Example: ```python # Example usage: def square(x: int) -> int: return x * x numbers = [1, 2, 3, 4] result = batch_process(numbers, square) print(result) # Output: [1, 4, 9, 16] # Another example usage: def to_upper(s: str) -> str: return s.upper() strings = [\\"hello\\", \\"world\\"] result = batch_process(strings, to_upper) print(result) # Output: [\\"HELLO\\", \\"WORLD\\"] ``` # Constraints: 1. The items list must not be modified. 2. The processor must handle each element individually. 3. The function should work for any data type that can be processed by the provided `processor`. # Performance: - Your solution should be efficient enough to handle large lists of items, ideally with a time complexity of O(n), where n is the number of elements in the `items` list. Note: Ensure your code is clean, well-documented, and follows the Pythonic way of writing functions.","solution":"from typing import List, Callable, TypeVar T = TypeVar(\'T\') R = TypeVar(\'R\') def batch_process(items: List[T], processor: Callable[[T], R]) -> List[R]: Applies the processor function to each element in the items list and returns a list of processed results. :param items: List of elements to be processed. :param processor: A function that processes an element of type T and returns a result of type R. :return: A list of processed results. return [processor(item) for item in items]"},{"question":"You are working on a backup tool that processes several file paths and prepares them for copying to a backup directory. As part of this tool, you need to write a function `prepare_backup_paths(files, backup_dir)` that takes a list of file paths and a backup directory, then returns a list of tuples. Each tuple should contain the original path and the corresponding path in the backup directory with identical directory structure. # Function Signature ```python def prepare_backup_paths(files: List[str], backup_dir: str) -> List[Tuple[str, str]]: ``` # Input - `files`: A list of file paths. Each path is a string and can be either relative or absolute. - `backup_dir`: A string representing the absolute path to the backup directory. # Output - A list of tuples where each tuple contains two strings: 1. The original path from the input list. 2. The path where the file should be copied in the backup directory, preserving the original directory structure. # Constraints - All input paths will be valid strings. - The `backup_dir` will always be an absolute path. - The function should handle both Windows and Unix style paths. - The function should raise a `ValueError` if any path in `files` list is both relative and resides outside of the current working directory or if `backup_dir` is not an absolute path. # Examples ```python # Example 1 files = [ \'/home/user/docs/report.txt\', \'/home/user/photos/image.jpeg\', \'/var/log/syslog\', ] backup_dir = \'/mnt/backup\' result = prepare_backup_paths(files, backup_dir) # Expected output # [ # (\'/home/user/docs/report.txt\', \'/mnt/backup/home/user/docs/report.txt\'), # (\'/home/user/photos/image.jpeg\', \'/mnt/backup/home/user/photos/image.jpeg\'), # (\'/var/log/syslog\', \'/mnt/backup/var/log/syslog\'), # ] # Example 2 files = [ \'project/main.py\', \'project/utils/helpers.py\', ] backup_dir = \'/mnt/backup\' result = prepare_backup_paths(files, backup_dir) # Raises ValueError as the paths in `files` are relative but reside outside of the current working directory. ``` Implement the function `prepare_backup_paths` to pass the provided examples and handle the described constraints. Make sure to use appropriate functions from the `os.path` module to manipulate the file paths. Note: Do not actually perform file system operations, just prepare and return the backup paths.","solution":"import os from typing import List, Tuple def prepare_backup_paths(files: List[str], backup_dir: str) -> List[Tuple[str, str]]: Prepare backup paths for a list of files. Args: files (List[str]): List of file paths. backup_dir (str): Absolute path to the backup directory. Returns: List[Tuple[str, str]]: List of tuples with original path and backup path. Raises: ValueError: If paths are relative and outside the current working directory, or backup_dir is not absolute. if not os.path.isabs(backup_dir): raise ValueError(\\"backup_dir must be an absolute path\\") prepared_paths = [] cwd = os.getcwd() for file in files: if not os.path.isabs(file): file = os.path.join(cwd, file) rel_path = os.path.relpath(file, \'/\') backup_path = os.path.join(backup_dir, rel_path) prepared_paths.append((file, backup_path)) return prepared_paths"},{"question":"# Asynchronous Task Management with Network I/O in Python asyncio **Objective:** Write a Python function that demonstrates the use of asyncio for managing tasks and handling network I/O operations concurrently. **Problem Statement:** You are required to implement a server-client model using asyncio where: 1. The server listens to multiple incoming connections on a specified port. 2. Each client sends a message to the server. 3. The server responds to each client with an acknowledgment message. **Function Signatures:** - `async def start_server(host: str, port: int) -> None` - `async def client_send_message(host: str, port: int, message: str) -> str` **Requirements:** 1. Implement an asynchronous server that can handle multiple client connections concurrently. 2. Each client should send a single message to the server. 3. The server should return an acknowledgment message back to the client. 4. Ensure that the server handles at least 3 clients concurrently using asyncio\'s task management. 5. Use asyncio\'s `start_server`, `StreamReader`, and `StreamWriter` for the server-side implementation. 6. Use asyncio\'s `open_connection` for the client-side implementation. 7. Demonstrate the usage of `asyncio.gather` to run multiple clients concurrently to send messages to the server. **Constraints:** - The `message` sent by the client is a string of maximum length 100 characters. - The client\'s acknowledgment response should be formatted as: \\"Received: [original_message]\\". **Input and Output Example:** ```python import asyncio async def start_server(host: str, port: int) -> None: # Implement the server logic here pass async def client_send_message(host: str, port: int, message: str) -> str: # Implement the client logic here pass async def main(): host = \'127.0.0.1\' port = 8888 # Start the server server_task = asyncio.create_task(start_server(host, port)) # Define client messages messages = [\'Hello\', \'World\', \'AsyncIO\'] # Start clients client_tasks = [ client_send_message(host, port, msg) for msg in messages ] # Gather all client tasks and get results responses = await asyncio.gather(*client_tasks) for response in responses: print(response) # Wait for the server to finish await server_task # Run the main function to start the demonstration asyncio.run(main()) ``` In this implementation, the `start_server` function sets up an asynchronous server that listens on the specified host and port. The `client_send_message` function asynchronously connects to the server and sends a message, then waits for the acknowledgment response. The `main` function demonstrates running the server and multiple clients simultaneously. # Notes: - Ensure to handle exceptions and errors gracefully. - Properly close connections to free resources after communication is done.","solution":"import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") response = f\\"Received: {message}\\" writer.write(response.encode()) await writer.drain() writer.close() await writer.wait_closed() async def start_server(host: str, port: int) -> None: server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def client_send_message(host: str, port: int, message: str) -> str: reader, writer = await asyncio.open_connection(host, port) print(f\'Sending: {message}\') writer.write(message.encode()) await writer.drain() data = await reader.read(100) response = data.decode() print(f\'Received: {response}\') writer.close() await writer.wait_closed() return response async def main(): host = \'127.0.0.1\' port = 8888 server_task = asyncio.create_task(start_server(host, port)) await asyncio.sleep(1) # Wait a bit for the server to start messages = [\'Hello\', \'World\', \'AsyncIO\'] client_tasks = [ client_send_message(host, port, msg) for msg in messages ] responses = await asyncio.gather(*client_tasks) for response in responses: print(response) server_task.cancel() asyncio.run(main())"},{"question":"**Title: Enhanced Command-Line Interface with Logging** **Objective:** To assess the student\'s ability to create a command-line interface for a Python application that also integrates a logging mechanism. This exercise will test the student\'s understanding of `argparse` for argument parsing and `logging` for handling different levels of log messages. **Task:** You are required to implement a command-line Python application that performs simple arithmetic operations (addition, subtraction, multiplication, division) on two numbers. The application should also include a logging mechanism to record the operations performed along with their results. **Specifications:** 1. **Command-Line Interface:** - Use the `argparse` module to parse command-line arguments. - The program should accept two positional arguments (both integers or floats) and one optional argument indicating the operation (`add`, `sub`, `mul`, `div`). - Example usage: `python calculator.py 12 8 --operation add` 2. **Logging:** - Use the `logging` module to log the operations performed. - Log entries should include a timestamp, the operation performed, the operands, and the result. - The logs should be saved into a file named `calculator.log`. 3. **Error Handling:** - Handle potential errors such as division by zero, incorrect argument types, and missing operation argument. - Log errors with an appropriate message and level (e.g., `logging.error`). **Implementation Details:** - **Input:** - Two positional arguments representing the numbers on which the operation is to be performed. - One optional argument `--operation` (default: `add`) specifying the operation (`add` for addition, `sub` for subtraction, `mul` for multiplication, `div` for division). - **Output:** - Print the result of the arithmetic operation to the console. - Save a log of the operation and result to the `calculator.log` file. - **Constraints:** - You must use Python 3.10 or higher. - Ensure the program handles exceptions gracefully and continues to run. **Example:** ```bash # Successful case python calculator.py 12 8 --operation add Result: 20 # Log entry (calculator.log) 2023-10-10 10:00:00,000 - INFO - Operation: add, Operands: 12, 8, Result: 20 # Error case python calculator.py 12 0 --operation div Error: Division by zero is not allowed. # Log entry (calculator.log) 2023-10-10 10:00:00,000 - ERROR - Operation: div, Operands: 12, 0, Error: Division by zero is not allowed. ``` **Submission:** - Submit the Python script `calculator.py`. - Ensure that your script adheres to best coding practices, including appropriate function definitions, code comments, and error handling. - Submissions will be evaluated based on correctness, code quality, and completeness of logging.","solution":"import argparse import logging # Setup logging logging.basicConfig( filename=\'calculator.log\', level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\' ) def add(a, b): Returns the sum of a and b. return a + b def sub(a, b): Returns the difference of a and b. return a - b def mul(a, b): Returns the product of a and b. return a * b def div(a, b): Returns the division of a by b. if b == 0: raise ValueError(\\"Division by zero is not allowed\\") return a / b def main(): parser = argparse.ArgumentParser(description=\\"Perform basic arithmetic operations.\\") parser.add_argument(\'num1\', type=float, help=\\"The first number\\") parser.add_argument(\'num2\', type=float, help=\\"The second number\\") parser.add_argument(\'--operation\', type=str, choices=[\'add\', \'sub\', \'mul\', \'div\'], default=\'add\', help=\\"The operation to perform: add, sub, mul, div\\") args = parser.parse_args() try: if args.operation == \'add\': result = add(args.num1, args.num2) elif args.operation == \'sub\': result = sub(args.num1, args.num2) elif args.operation == \'mul\': result = mul(args.num1, args.num2) elif args.operation == \'div\': result = div(args.num1, args.num2) print(f\\"Result: {result}\\") logging.info(f\\"Operation: {args.operation}, Operands: {args.num1}, {args.num2}, Result: {result}\\") except Exception as e: print(f\\"Error: {e}\\") logging.error(f\\"Operation: {args.operation}, Operands: {args.num1}, {args.num2}, Error: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Gradient Checking in PyTorch **Objective:** Implement a custom loss function using PyTorch, and verify the correctness of its gradient computations using `torch.autograd.gradcheck` and `torch.autograd.gradgradcheck`. **Instructions:** 1. **Implement a Custom Loss Function:** - Define a function `custom_loss` that computes the following loss for a given tensor `x`: [ text{loss} = sum_{i=1}^{N} left(x_i^2 + sin(x_i)right) ] - The function should take a single input tensor `x` and return a scalar tensor representing the loss. 2. **Verify First-Order Gradient (Using `gradcheck`):** - Use `torch.autograd.gradcheck` to verify the correctness of the gradients computed by PyTorch\'s automatic differentiation for your `custom_loss` function. - Use a random input tensor of size (N=10) with requires_grad set to `True`. 3. **Verify Second-Order Gradient (Using `gradgradcheck`):** - Use `torch.autograd.gradgradcheck` to verify the correctness of the second-order gradients for your `custom_loss` function. - Use the same random input tensor of size (N=10). **Constraints:** - The input tensor should be of dtype `torch.float64` to ensure numerical precision in gradient checking. - Make sure to handle any potential numerical instability by appropriately setting the `eps` parameter in `gradcheck`. **Code Template:** ```python import torch from torch.autograd import gradcheck, gradgradcheck # Step 1: Implement the custom loss function def custom_loss(x): Compute the custom loss function. Args: x (torch.Tensor): Input tensor of size (N,). Returns: torch.Tensor: Scalar tensor representing the loss. loss = torch.sum(x**2 + torch.sin(x)) return loss # Step 2: Verify first-order gradient using gradcheck def verify_gradcheck(): Verify the first-order gradients using gradcheck. # Create a random input tensor of size (10,) with requires_grad=True x = torch.randn(10, dtype=torch.float64, requires_grad=True) # Check gradients using gradcheck gradcheck_result = gradcheck(custom_loss, (x,), eps=1e-6, atol=1e-5) print(f\\"Gradcheck result: {gradcheck_result}\\") # Step 3: Verify second-order gradient using gradgradcheck def verify_gradgradcheck(): Verify the second-order gradients using gradgradcheck. # Create a random input tensor of size (10,) with requires_grad=True x = torch.randn(10, dtype=torch.float64, requires_grad=True) # Check second-order gradients using gradgradcheck gradgradcheck_result = gradgradcheck(custom_loss, (x,), eps=1e-6, atol=1e-5) print(f\\"Gradgradcheck result: {gradgradcheck_result}\\") # Execute the verification functions verify_gradcheck() verify_gradgradcheck() ``` **Expected Output:** The expected output should confirm that both the first-order and second-order gradients are correctly computed. ```python Gradcheck result: True Gradgradcheck result: True ``` If the results are `True`, it means that your implementation of the custom loss function passes both the gradient checks. If any of the checks fail, you should inspect the implementation for potential errors or numerical instability issues.","solution":"import torch from torch.autograd import gradcheck, gradgradcheck # Step 1: Implement the custom loss function def custom_loss(x): Compute the custom loss function. Args: x (torch.Tensor): Input tensor of size (N,). Returns: torch.Tensor: Scalar tensor representing the loss. loss = torch.sum(x**2 + torch.sin(x)) return loss # Step 2: Verify first-order gradient using gradcheck def verify_gradcheck(): Verify the first-order gradients using gradcheck. # Create a random input tensor of size (10,) with requires_grad=True x = torch.randn(10, dtype=torch.float64, requires_grad=True) # Check gradients using gradcheck gradcheck_result = gradcheck(custom_loss, (x,), eps=1e-6, atol=1e-5) return gradcheck_result # Step 3: Verify second-order gradient using gradgradcheck def verify_gradgradcheck(): Verify the second-order gradients using gradgradcheck. # Create a random input tensor of size (10,) with requires_grad=True x = torch.randn(10, dtype=torch.float64, requires_grad=True) # Check second-order gradients using gradgradcheck gradgradcheck_result = gradgradcheck(custom_loss, (x,), eps=1e-6, atol=1e-5) return gradgradcheck_result"},{"question":"# Semi-Supervised Learning Task: Using Self-Training and Label Propagation Objective: Implement a semi-supervised learning solution using both Self-Training and Label Propagation algorithms to classify a dataset with partially labeled data. Dataset: You will use the `digits` dataset from `sklearn.datasets`, but some of the labels will be intentionally removed to simulate a semi-supervised scenario. Requirements: 1. **Load the Data**: - Import the `load_digits` function from `sklearn.datasets`. - Load the data and split it into features (X) and labels (y). 2. **Simulate Semi-Supervised Data**: - Randomly set 50% of the labels in `y` to -1 to represent unlabeled data points. 3. **Apply Self-Training**: - Use a `RandomForestClassifier` as the base estimator for the `SelfTrainingClassifier`. - Train the self-training classifier on the semi-supervised data. - Evaluate the performance using accuracy on the original labeled data. 4. **Apply Label Propagation**: - Use both `LabelPropagation` and `LabelSpreading` classifiers. - Train the classifiers on the semi-supervised data. - Evaluate their performance using accuracy on the original labeled data. 5. **Comparison**: - Compare the performance of Self-Training, LabelPropagation, and LabelSpreading in terms of accuracy. - Discuss which method performs better and possible reasons for the performance differences. Constraints: - Use `RandomForestClassifier` from `sklearn.ensemble` for Self-Training. - Use default parameters for the classifiers unless specified. - Ensure reproducibility by setting random seeds where necessary. Input and Output: - **Input**: None (the dataset is loaded within the code) - **Output**: Print the accuracy of each classifier and a brief discussion on the performance comparison. Example Template: ```python import numpy as np from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation, LabelSpreading from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def main(): # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Randomly set 50% of the labels to -1 rng = np.random.default_rng(seed=42) random_unlabeled_points = rng.permutation(len(y))[:len(y) // 2] y[random_unlabeled_points] = -1 # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=digits.target) # SelfTraining Classifier base_classifier = RandomForestClassifier(n_estimators=100, random_state=42) self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X_train, y_train) y_pred_self_training = self_training_model.predict(X_test) acc_self_training = accuracy_score(y_test, y_pred_self_training) # LabelPropagation Classifier label_propagation_model = LabelPropagation() label_propagation_model.fit(X_train, y_train) y_pred_label_propagation = label_propagation_model.predict(X_test) acc_label_propagation = accuracy_score(y_test, y_pred_label_propagation) # LabelSpreading Classifier label_spreading_model = LabelSpreading() label_spreading_model.fit(X_train, y_train) y_pred_label_spreading = label_spreading_model.predict(X_test) acc_label_spreading = accuracy_score(y_test, y_pred_label_spreading) # Print results print(f\\"Self-Training Classifier Accuracy: {acc_self_training:.4f}\\") print(f\\"Label Propagation Accuracy: {acc_label_propagation:.4f}\\") print(f\\"Label Spreading Accuracy: {acc_label_spreading:.4f}\\") # Discussion print(\\"nDiscussion:\\") if acc_self_training >= acc_label_propagation and acc_self_training >= acc_label_spreading: print(\\"Self-Training Classifier performed the best.\\") elif acc_label_propagation > acc_self_training and acc_label_propagation >= acc_label_spreading: print(\\"Label Propagation performed the best.\\") else: print(\\"Label Spreading performed the best.\\") print(\\"Discuss the reasons behind these performance differences here...\\") if __name__ == \\"__main__\\": main() ``` Evaluation Criteria: - Correct implementation of data loading and preprocessing. - Correct application of Self-Training, LabelPropagation, and LabelSpreading. - Proper evaluation of accuracy for each approach. - Clear and insightful discussion on performance comparison.","solution":"import numpy as np from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation, LabelSpreading from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def semi_supervised_learning(): # Load the digits dataset digits = load_digits() X, y = digits.data, digits.target # Randomly set 50% of the labels to -1 rng = np.random.default_rng(seed=42) random_unlabeled_points = rng.permutation(len(y))[:len(y) // 2] y[random_unlabeled_points] = -1 # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # SelfTraining Classifier base_classifier = RandomForestClassifier(n_estimators=100, random_state=42) self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X_train, y_train) y_pred_self_training = self_training_model.predict(X_test) acc_self_training = accuracy_score(y_test, y_pred_self_training) # LabelPropagation Classifier label_propagation_model = LabelPropagation() label_propagation_model.fit(X_train, y_train) y_pred_label_propagation = label_propagation_model.predict(X_test) acc_label_propagation = accuracy_score(y_test, y_pred_label_propagation) # LabelSpreading Classifier label_spreading_model = LabelSpreading() label_spreading_model.fit(X_train, y_train) y_pred_label_spreading = label_spreading_model.predict(X_test) acc_label_spreading = accuracy_score(y_test, y_pred_label_spreading) return acc_self_training, acc_label_propagation, acc_label_spreading def main(): acc_self_training, acc_label_propagation, acc_label_spreading = semi_supervised_learning() # Print results print(f\\"Self-Training Classifier Accuracy: {acc_self_training:.4f}\\") print(f\\"Label Propagation Accuracy: {acc_label_propagation:.4f}\\") print(f\\"Label Spreading Accuracy: {acc_label_spreading:.4f}\\") # Discussion print(\\"nDiscussion:\\") if acc_self_training >= acc_label_propagation and acc_self_training >= acc_label_spreading: print(\\"Self-Training Classifier performed the best.\\") elif acc_label_propagation > acc_self_training and acc_label_propagation >= acc_label_spreading: print(\\"Label Propagation performed the best.\\") else: print(\\"Label Spreading performed the best.\\") print(\\"Discuss the reasons behind these performance differences here...\\") if __name__ == \\"__main__\\": main()"},{"question":"# Seaborn Expert Challenge You are given two datasets from seaborn\'s sample datasets: `penguins` and `flights`. Your task is to create a multi-faceted barplot that illustrates detailed data insights. Datasets - **Penguins**: Information on penguins, including species, island, body mass, and sex. - **Flights**: Monthly passengers on flights from the years 1949 to 1960. # Requirements 1. **Load the Datasets**: Load the `penguins` and `flights` datasets from seaborn. 2. **Create Facets**: - For the `penguins` dataset: - Create a faceted barplot that shows the average body mass (`body_mass_g`) for each species (`species`) grouped by sex (`sex`). - Each facet should represent a different penguin island (`island`). - For the `flights` dataset: - Create a single barplot showing the total number of passengers for each year (`year`). - Add a secondary grouping by month (`month`), including error bars showing the standard deviation of the monthly data. Inputs and Outputs - **Input**: The seaborn-supplied `penguins` and `flights` datasets. - **Output**: Two barplots, one faceted by island for penguins data and one grouped by month for flights data. Constraints - Use seaborn\'s `catplot` for creating facets and `barplot` for individual plots. - Ensure that the plots include appropriate labels and titles for clarity. - Customize the appearance of the plots to enhance readability (e.g., error bars, colors). ```python import seaborn as sns import matplotlib.pyplot as plt # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # 1. Faceted barplot for Penguins data penguin_plot = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"sex\\", y=\\"body_mass_g\\", hue=\\"species\\", col=\\"island\\", height=4, aspect=.7, palette=\\"deep\\", ci=\\"sd\\" ) penguin_plot.set_axis_labels(\\"Sex\\", \\"Body Mass (g)\\") penguin_plot.set_titles(\\"{col_name} Island\\") penguin_plot.fig.suptitle(\\"Average Body Mass of Penguins by Island, Sex, and Species\\", fontsize=16) plt.tight_layout() penguin_plot.fig.subplots_adjust(top=0.9) # 2. Barplot for Flights data flights_sum = flights.groupby([\'year\', \'month\']).agg({\'passengers\': \'sum\'}).reset_index() flights_sd = flights.groupby([\'year\', \'month\']).agg({\'passengers\': \'std\'}).reset_index() # Merge for uniform index merged_flights = flights_sum.set_index([\'year\', \'month\']).join(flights_sd.set_index([\'year\', \'month\']), rsuffix=\'_std\').reset_index() ax = sns.barplot(data=merged_flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", ci=\\"sd\\") ax.set_title(\\"Total Passengers Each Year with Monthly Variations\\", fontsize=16) ax.set_xlabel(\\"Year\\") ax.set_ylabel(\\"Total Passengers\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.tight_layout() plt.show() ``` # Performance Requirements - The solution should be efficient in terms of both time and space, given the relatively small size of the datasets. - The code should be clean, well-documented, and follow best practices in Python programming, particularly relating to data visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_barplot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a faceted barplot for Penguins data penguin_plot = sns.catplot( data=penguins, kind=\\"bar\\", x=\\"sex\\", y=\\"body_mass_g\\", hue=\\"species\\", col=\\"island\\", height=4, aspect=.7, palette=\\"deep\\", ci=\\"sd\\" ) # Customize plot labels and title penguin_plot.set_axis_labels(\\"Sex\\", \\"Body Mass (g)\\") penguin_plot.set_titles(\\"{col_name} Island\\") penguin_plot.fig.suptitle(\\"Average Body Mass of Penguins by Island, Sex, and Species\\", fontsize=16) # Adjust layout to prevent title overlap with plots plt.tight_layout() penguin_plot.fig.subplots_adjust(top=0.85) return penguin_plot def create_flights_barplot(): # Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Summarize and calculate the standard deviation of passengers per year and month flights_sum = flights.groupby([\'year\', \'month\']).agg({\'passengers\': \'sum\'}).reset_index() flights_sd = flights.groupby([\'year\', \'month\']).agg({\'passengers\': \'std\'}).reset_index() # Merge both dataframes for a unified index merged_flights = flights_sum.set_index([\'year\', \'month\']).join( flights_sd.set_index([\'year\', \'month\']), rsuffix=\'_std\' ).reset_index() # Create a barplot plt.figure(figsize=(12, 6)) ax = sns.barplot(data=merged_flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", ci=None) # Customize plot labels and title ax.set_title(\\"Total Passengers Each Year with Monthly Variations\\", fontsize=16) ax.set_xlabel(\\"Year\\") ax.set_ylabel(\\"Total Passengers\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') # Adjust layout to prevent legend overlap with plot plt.tight_layout() return ax"},{"question":"# Question: You are given a dataset of handwritten digits that comprises 1797 samples, where each sample is a flattened 8x8 image of a digit (0 through 9). Your task is to implement a digit classifier using `SGDClassifier` from Scikit-learn to classify these images. Your implementation must include feature scaling, hyperparameter tuning, and evaluation of the model\'s performance. Requirements: 1. Load the dataset using Scikit-learn\'s `datasets.load_digits`. 2. Split the dataset into training and test sets using an 80-20 split. 3. Implement a function `train_and_evaluate_sgd_classifier` that: - Takes training and test datasets as input. - Scales the features using `StandardScaler`. - Trains an `SGDClassifier` using GridSearchCV for hyperparameter tuning on the training set. - Evaluates the tuned model on the test set and prints the classification report and confusion matrix. Function Signature: ```python from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report, confusion_matrix def train_and_evaluate_sgd_classifier(X_train, X_test, y_train, y_test): # Your code implementation here pass # Example usage: digits = load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42) train_and_evaluate_sgd_classifier(X_train, X_test, y_train, y_test) ``` Constraints: - Use `StandardScaler` for feature scaling. - Hyperparameter grid for `SGDClassifier` should include \'loss\' and \'alpha\' parameters. - Use cross-validation within `GridSearchCV`. Input Format: - `X_train`: 2D NumPy array of shape (n_samples_train, n_features). - `X_test`: 2D NumPy array of shape (n_samples_test, n_features). - `y_train`: 1D NumPy array of shape (n_samples_train,). - `y_test`: 1D NumPy array of shape (n_samples_test,). Output: - Print classification report and confusion matrix for the test set. # Example Output: ``` Classification Report: precision recall f1-score support 0 0.98 0.97 0.97 36 1 0.98 0.97 0.97 32 2 0.97 1.00 0.99 31 3 0.93 0.97 0.95 36 4 0.97 0.97 0.97 33 5 0.97 1.00 0.98 34 6 1.00 0.97 0.99 33 7 0.97 0.97 0.97 30 8 0.88 0.91 0.89 34 9 0.97 0.90 0.93 30 accuracy 0.96 329 macro avg 0.96 0.96 0.96 329 weighted avg 0.96 0.96 0.96 329 Confusion Matrix: [[35 0 0 0 0 0 0 0 1 0] [ 0 31 0 0 0 0 0 0 1 0] [ 0 0 31 0 0 0 0 0 0 0] [ 0 0 0 35 0 0 0 0 1 0] [ 0 0 0 0 32 0 0 1 0 0] [ 0 0 0 0 0 34 0 0 0 0] [ 1 0 0 0 0 0 32 0 0 0] [ 0 0 0 0 0 1 0 29 0 0] [ 0 0 0 1 1 0 0 0 31 1] [ 0 0 0 1 0 0 1 1 0 27]] ```","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.linear_model import SGDClassifier from sklearn.preprocessing import StandardScaler from sklearn.metrics import classification_report, confusion_matrix def train_and_evaluate_sgd_classifier(X_train, X_test, y_train, y_test): # Scaling the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Defining the SGDClassifier with GridSearchCV for hyperparameter tuning param_grid = { \'loss\': [\'hinge\', \'log\'], \'alpha\': [0.0001, 0.001, 0.01, 0.1] } sgd_clf = SGDClassifier(random_state=42) grid_search = GridSearchCV(sgd_clf, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train_scaled, y_train) # Best parameters and model best_model = grid_search.best_estimator_ # Prediction and evaluation on the test set y_pred = best_model.predict(X_test_scaled) print(\\"Classification Report:\\") print(classification_report(y_test, y_pred)) print(\\"Confusion Matrix:\\") print(confusion_matrix(y_test, y_pred)) # Example usage: digits = load_digits() X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=42) train_and_evaluate_sgd_classifier(X_train, X_test, y_train, y_test)"},{"question":"**Assessing Your Understanding of `Categorical` Data in `pandas`** **Problem Description:** You are provided with a dataset containing student grades for various courses. The data has the following columns: - `Student_ID`: Unique identifier for each student. - `Course`: The course name. - `Grade`: The grade obtained in the course. Grades are categorical data with specific levels: `A`, `B`, `C`, `D`, `E`, and `F`. However, the grades can be either ordered or unordered based on different analyses. Your task is to implement a series of functions to analyze this data using `pandas` categorical data functionalities. **Tasks:** 1. **Load Data**: Write a function to load the dataset into a `DataFrame`. 2. **Categorical Conversion**: Convert the `Grade` column into categorical data type with the specified order. 3. **Grade Statistics**: Write a function that returns basic statistics (count, unique, top, freq) and describes the grade distribution. 4. **Top Students**: Write a function to sort students within each course based on their grades in ascending order and return the sorted `DataFrame`. 5. **Grade Comparison**: Write a function that compares student grades in two specified courses and returns a comparison summary. **Constraints:** - You must use `pd.Categorical`, `CategoricalDtype`, or related methods for handling categorical data. - Avoid using any loops; use `pandas` vectorized operations for efficiency. - Assume input data is clean and follows the described schema. **Input and Output Formats:** 1. **Load Data** - Input: `file_path` (str): Path to the CSV file containing the dataset. - Output: `DataFrame` containing the loaded data. 2. **Categorical Conversion** - Input: `df` (DataFrame): DataFrame containing the loaded data. - Output: `DataFrame` with the `Grade` column converted to categorical type with order `A > B > C > D > E > F`. 3. **Grade Statistics** - Input: `df` (DataFrame): DataFrame with categorical grades. - Output: Summary statistics of the `Grade` column using `describe()` method. 4. **Top Students** - Input: `df` (DataFrame): DataFrame with categorical grades. - Output: `DataFrame` sorted by `Course` and `Grade`. 5. **Grade Comparison** - Input: `df` (DataFrame): DataFrame with categorical grades, `course1` (str): First course name, `course2` (str): Second course name. - Output: Summary comparison of grades between `course1` and `course2`. **Example:** ```python # Example usage: file_path = \'student_grades.csv\' df = load_data(file_path) # Task 1 df_category = convert_to_categorical(df) # Task 2 stats = grade_statistics(df_category) # Task 3 sorted_df = top_students(df_category) # Task 4 comparison = compare_grades(df_category, \'Math\', \'Science\') # Task 5 ``` Implement the above functions: ```python import pandas as pd def load_data(file_path): # Task 1 df = pd.read_csv(file_path) return df def convert_to_categorical(df): # Task 2 cat_type = pd.api.types.CategoricalDtype(categories=[\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\", \\"F\\"], ordered=True) df[\'Grade\'] = df[\'Grade\'].astype(cat_type) return df def grade_statistics(df): # Task 3 return df[\'Grade\'].describe() def top_students(df): # Task 4 return df.sort_values(by=[\'Course\', \'Grade\']) def compare_grades(df, course1, course2): # Task 5 filter1 = df[\'Course\'] == course1 filter2 = df[\'Course\'] == course2 grades1 = df[filter1][\'Grade\'] grades2 = df[filter2][\'Grade\'] comparison = pd.concat([grades1.reset_index(drop=True), grades2.reset_index(drop=True)], axis=1) comparison.columns = [course1, course2] return comparison ``` **Note:** Ensure to handle the categorical data accurately to reflect the intended order for meaningful comparisons and sorting.","solution":"import pandas as pd def load_data(file_path): Loads the dataset from the specified CSV file. Parameters: file_path (str): Path to the CSV file containing the dataset. Returns: pd.DataFrame: DataFrame containing the loaded data. df = pd.read_csv(file_path) return df def convert_to_categorical(df): Converts the `Grade` column in the DataFrame to categorical type with order `A > B > C > D > E > F`. Parameters: df (pd.DataFrame): DataFrame containing the data. Returns: pd.DataFrame: DataFrame with the `Grade` column converted to categorical type. cat_type = pd.api.types.CategoricalDtype(categories=[\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\", \\"F\\"], ordered=True) df[\'Grade\'] = df[\'Grade\'].astype(cat_type) return df def grade_statistics(df): Returns basic statistics (count, unique, top, freq) and describes the grade distribution. Parameters: df (pd.DataFrame): DataFrame with categorical grades. Returns: pd.Series: Summary statistics of the `Grade` column. return df[\'Grade\'].describe() def top_students(df): Sorts students within each course based on their grades in ascending order and returns the sorted DataFrame. Parameters: df (pd.DataFrame): DataFrame with categorical grades. Returns: pd.DataFrame: DataFrame sorted by `Course` and `Grade`. return df.sort_values(by=[\'Course\', \'Grade\']) def compare_grades(df, course1, course2): Compares student grades in two specified courses and returns a comparison summary. Parameters: df (pd.DataFrame): DataFrame with categorical grades. course1 (str): First course name. course2 (str): Second course name. Returns: pd.DataFrame: Summary comparison of grades between `course1` and `course2`. filter1 = df[\'Course\'] == course1 filter2 = df[\'Course\'] == course2 grades1 = df[filter1][\'Grade\'] grades2 = df[filter2][\'Grade\'] comparison = pd.concat([grades1.reset_index(drop=True), grades2.reset_index(drop=True)], axis=1) comparison.columns = [course1, course2] return comparison"},{"question":"Objective To assess your understanding of the Nearest Neighbors algorithm and its implementation in the scikit-learn library. Problem Statement You are provided with a dataset containing features of various wines and their quality scores. Your task is to implement a helper function using scikit-learn\'s Nearest Neighbors class `KNeighborsClassifier` to predict the quality of wine based on its features. Function Signature ```python def predict_wine_quality(features: np.ndarray, labels: np.ndarray, test_features: np.ndarray, n_neighbors: int = 5, algorithm: str = \'auto\') -> np.ndarray: ``` Input - `features`: A 2D numpy array of shape (n_samples, n_features) containing the features of the training data. - `labels`: A 1D numpy array of shape (n_samples,) containing the quality labels for the training data. - `test_features`: A 2D numpy array of shape (n_test_samples, n_features) containing the features of the test data. - `n_neighbors`: An integer specifying the number of neighbors to use for classification (default is 5). - `algorithm`: A string specifying the algorithm to use for finding nearest neighbors. It must be one of `[\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']` (default is \'auto\'). Output - Returns a 1D numpy array of shape (n_test_samples,) containing the predicted quality labels for the test data. Constraints - You must use the `KNeighborsClassifier` from the `sklearn.neighbors` module. - The solution should handle large datasets efficiently. Example ```python import numpy as np # Wine features features = np.array([ [7.4, 0.7, 0.0, 1.9, 0.076, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4], [7.8, 0.88, 0.0, 2.6, 0.098, 25.0, 67.0, 0.9968, 3.2, 0.68, 9.8], [7.8, 0.76, 0.04, 2.3, 0.092, 15.0, 54.0, 0.9970, 3.26, 0.65, 9.8], # More samples... ]) # Wine quality labels labels = np.array([5, 5, 5, # More labels... ]) # Test features test_features = np.array([ [7.0, 0.27, 0.36, 20.7, 0.045, 45.0, 170.0, 1.0010, 3.00, 0.45, 8.8], [6.3, 0.30, 0.34, 1.6, 0.049, 14.0, 132.0, 0.9947, 3.14, 0.49, 9.5], ]) # Predict wine quality predicted_quality = predict_wine_quality(features, labels, test_features, n_neighbors=3, algorithm=\'auto\') print(predicted_quality) # Output: array of predicted quality labels (e.g., [6, 5]) ``` Notes - Make sure to handle possible edge cases such as empty input arrays. - You are encouraged to use available functionality from the `sklearn.neighbors` module to perform the nearest neighbor searches efficiently.","solution":"import numpy as np from sklearn.neighbors import KNeighborsClassifier def predict_wine_quality(features: np.ndarray, labels: np.ndarray, test_features: np.ndarray, n_neighbors: int = 5, algorithm: str = \'auto\') -> np.ndarray: Uses the KNeighborsClassifier to predict the quality of wine based on its features. Parameters: - features: 2D numpy array of shape (n_samples, n_features) containing the features of the training data. - labels: 1D numpy array of shape (n_samples,) containing the quality labels for the training data. - test_features: 2D numpy array of shape (n_test_samples, n_features) containing the features of the test data. - n_neighbors: int, number of neighbors to use for classification (default is 5). - algorithm: str, algorithm to use for finding nearest neighbors (default is \'auto\'). Returns: - 1D numpy array of shape (n_test_samples,) containing the predicted quality labels for the test data. # Initialize the KNeighborsClassifier with the given parameters knn = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=algorithm) # Fit the classifier with the training features and labels knn.fit(features, labels) # Predict the quality labels for the test features predicted_labels = knn.predict(test_features) return predicted_labels"},{"question":"You are provided with a time series dataset representing the daily sales volume of a retail store over the course of a year. The goal is to perform a series of resampling operations to generate insights into the data at different frequencies. Input: 1. A pandas DataFrame `df` with two columns: - `date`: a datetime column representing the dates. - `sales`: an integer column representing the daily sales volume. For example: ```python import pandas as pd data = { \'date\': pd.date_range(start=\'2022-01-01\', periods=365, freq=\'D\'), \'sales\': [np.random.randint(1, 100) for _ in range(365)] } df = pd.DataFrame(data) ``` Tasks: 1. **Monthly Resampling**: Resample the data to a monthly frequency, taking the sum of sales within each month. Output the result as a pandas DataFrame with the columns `date` (first day of each month) and `monthly_sales_sum`. 2. **Weekly Resampling with Filling**: Resample the data to a weekly frequency. Fill any missing data using forward-fill (`ffill`). Output the result as a pandas DataFrame with the columns `date` (first day of each week) and `weekly_sales`. 3. **Quarterly Aggregation**: Resample the data to a quarterly frequency, calculating the mean sales for each quarter. Output the result as a pandas DataFrame with the columns `date` (first day of each quarter) and `quarterly_sales_mean`. Constraints: - Use the pandas `resample` function for resampling operations. - Properly handle dates and ensure the output DataFrame has appropriate `date` indices. Output: For each task, output the resulting DataFrame. Example: ```python # Task 1: Monthly Resampling monthly_sales = df.resample(\'M\', on=\'date\')[\'sales\'].sum().reset_index() monthly_sales.columns = [\'date\', \'monthly_sales_sum\'] # Task 2: Weekly Resampling with Filling weekly_sales = df.resample(\'W\', on=\'date\')[\'sales\'].ffill().reset_index() weekly_sales.columns = [\'date\', \'weekly_sales\'] # Task 3: Quarterly Aggregation quarterly_sales = df.resample(\'Q\', on=\'date\')[\'sales\'].mean().reset_index() quarterly_sales.columns = [\'date\', \'quarterly_sales_mean\'] ``` Your task is to implement the three resampling operations using the given pandas DataFrame `df`.","solution":"import pandas as pd def resample_sales_data(df): # Task 1: Monthly Resampling monthly_sales = df.resample(\'M\', on=\'date\')[\'sales\'].sum().reset_index() monthly_sales.columns = [\'date\', \'monthly_sales_sum\'] # Task 2: Weekly Resampling with Filling weekly_sales = df.resample(\'W\', on=\'date\')[\'sales\'].ffill().reset_index() weekly_sales.columns = [\'date\', \'weekly_sales\'] # Task 3: Quarterly Aggregation quarterly_sales = df.resample(\'Q\', on=\'date\')[\'sales\'].mean().reset_index() quarterly_sales.columns = [\'date\', \'quarterly_sales_mean\'] return monthly_sales, weekly_sales, quarterly_sales"},{"question":"Background The `linecache` module in Python provides efficient random access to lines within a file by caching them. This module is particularly useful when dealing with large files where reading the file repeatedly would be inefficient. Objective You are tasked with implementing a function that retrieves specific lines from a set of files and then clears the cache to ensure that the cache is invalidated if needed. This will demonstrate your understanding of both basic file operations and the advanced caching mechanism provided by the `linecache` module. Function Signature ```python def retrieve_and_clear(file_requests: list[tuple[str, int]]) -> list[str]: Given a list of file requests where each request is a tuple containing: - filename (str): Name of the file. - lineno (int): Line number to retrieve from the file. This function returns a list of strings where each string is the content of the requested line from the respective file. The function should use linecache to retrieve the lines and should clear the cache using linecache.clearcache() before returning the result. If a line cannot be retrieved, the returned string should be an empty string. Parameters: - file_requests: List of tuples containing (filename, lineno) Returns: - List of strings with each string being the content of the requested line. ``` Constraints - The number of file requests will not exceed 100. - The total size of all files combined will not exceed 10 MB. - The line numbers requested will always be positive integers. Example ```python # Sample files content for reference: # File1.txt: # Line 1: Hello, world! # Line 2: This is a test file. # Line 3: Just another line. # File2.txt: # Line 1: Another file starts here. # Line 2: Continuing the second file. # Line 3: The final line of File2. requests = [(\'File1.txt\', 2), (\'File2.txt\', 1), (\'NonExistent.txt\', 1)] output = retrieve_and_clear(requests) print(output) # Expected output: [\\"This is a test file.n\\", \\"Another file starts here.n\\", \\"\\"] # Ensure cache is cleared after the retrievals ``` Notes - Make sure to handle cases where the file does not exist or the line number is beyond the file length. - Demonstrate the functionality of linecache by using its caching mechanism and then clearing the cache to invalidate it before returning the results. - You can test your function using files with known contents to ensure it behaves correctly.","solution":"import linecache def retrieve_and_clear(file_requests): Given a list of file requests where each request is a tuple containing: - filename (str): Name of the file. - lineno (int): Line number to retrieve from the file. This function returns a list of strings where each string is the content of the requested line from the respective file. The function should use linecache to retrieve the lines and should clear the cache using linecache.clearcache() before returning the result. If a line cannot be retrieved, the returned string should be an empty string. Parameters: - file_requests: List of tuples containing (filename, lineno) Returns: - List of strings with each string being the content of the requested line. result = [] for filename, lineno in file_requests: line = linecache.getline(filename, lineno) if line == \'\': result.append(\'\') else: result.append(line) linecache.clearcache() return result"},{"question":"# Restricted Boltzmann Machine and Logistic Regression In this task, you will implement a function to train an RBM model using the BernoulliRBM class from scikit-learn and utilize the learned features to train and evaluate a logistic regression classifier. The goal is to show understanding of: 1. How to train an RBM model. 2. How to use the learned features in a downstream supervised learning task. 3. How to evaluate the performance of the model. # Problem Statement Implement the function `train_rbm_and_logistic_regression(X_train, y_train, X_test, y_test)` that: 1. Trains an RBM on the provided training data `X_train`. 2. Uses the features learned by the RBM to train a logistic regression classifier. 3. Evaluates the logistic regression classifier on the provided test data `X_test` and returns the accuracy. # Function Signature ```python from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def train_rbm_and_logistic_regression(X_train, y_train, X_test, y_test) -> float: Train RBM model and logistic regression for classification, and compute accuracy. Parameters: X_train (numpy.ndarray): Training data features. y_train (numpy.ndarray): Training data labels. X_test (numpy.ndarray): Testing data features. y_test (numpy.ndarray): Testing data labels. Returns: float: Accuracy of logistic regression on test data. # Define the RBM model rbm = BernoulliRBM(random_state=0) # Define the logistic regression model logistic = LogisticRegression(max_iter=1000, solver=\'lbfgs\', random_state=0) # Create a pipeline that first transforms the features using RBM, then applies logistic regression classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the model (fit the pipeline) on the training data classifier.fit(X_train, y_train) # Predict using the trained model on test data y_pred = classifier.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) return accuracy ``` # Constraints - `X_train`, `X_test` are 2D numpy arrays with shape (n_samples, n_features). - `y_train`, `y_test` are 1D numpy arrays with shape (n_samples,). - The values in `X_train` and `X_test` should be between 0 and 1. # Performance Requirements - The function should fit the RBM and Logistic Regression models within a reasonable time frame for data sizes up to 10,000 samples and 1,000 features. - Use a maximum of 200 components for the RBM model. # Example Usage ```python import numpy as np # Example data (for demonstration, replace with real dataset) X_train = np.random.rand(100, 50) y_train = np.random.randint(0, 2, 100) X_test = np.random.rand(20, 50) y_test = np.random.randint(0, 2, 20) # Call the function and get the accuracy accuracy = train_rbm_and_logistic_regression(X_train, y_train, X_test, y_test) print(f\'Test Accuracy: {accuracy}\') ``` Ensure the function is implemented correctly by testing with various datasets.","solution":"from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def train_rbm_and_logistic_regression(X_train, y_train, X_test, y_test) -> float: Train RBM model and logistic regression for classification, and compute accuracy. Parameters: X_train (numpy.ndarray): Training data features. y_train (numpy.ndarray): Training data labels. X_test (numpy.ndarray): Testing data features. y_test (numpy.ndarray): Testing data labels. Returns: float: Accuracy of logistic regression on test data. # Define the RBM model with 256 components and a max of 10 iterations rbm = BernoulliRBM(n_components=200, n_iter=10, random_state=0) # Define the logistic regression model logistic = LogisticRegression(max_iter=1000, solver=\'lbfgs\', random_state=0) # Create a pipeline that first transforms the features using RBM, then applies logistic regression classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Train the model (fit the pipeline) on the training data classifier.fit(X_train, y_train) # Predict using the trained model on test data y_pred = classifier.predict(X_test) # Calculate the accuracy of the model accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Objective:** To assess your understanding of PyTorch\'s backend system and your ability to interact with and utilize different backends in a conditional manner. **Problem Statement:** Implement a function `perform_computations` that takes a tensor as input and performs matrix multiplication using the most optimal backend available. The function should follow these steps: 1. Check if CUDA is available and built. If so, perform the matrix multiplication using CUDA. 2. If CUDA is not available but MPS (Apple\'s Metal Performance Shaders) is available and built, use MPS for the matrix multiplication. 3. If neither CUDA nor MPS is available, fall back to using the CPU for the matrix multiplication. Additionally, the function should print a message indicating which backend was used (CUDA, MPS, or CPU). **Function Signature:** ```python import torch def perform_computations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` **Input:** - `tensor1` and `tensor2`: Two 2D tensors of shape (n, m) and (m, p) respectively. The tensors are guaranteed to have compatible dimensions for matrix multiplication. **Output:** - A 2D tensor resulting from the matrix multiplication of `tensor1` and `tensor2` using the most optimal backend available. **Constraints:** - The function should handle cases where the input tensors are of varying sizes and dimensions. - You may assume that `torch` has already been imported. **Example:** ```python # Example Input tensor1 = torch.randn(3, 4) tensor2 = torch.randn(4, 5) # Example Output (assuming CUDA is available) # Should print: \\"Using CUDA backend\\" output = perform_computations(tensor1, tensor2) print(output.shape) # Output: torch.Size([3, 5]) # Example Output (assuming only CPU is available) # Should print: \\"Using CPU backend\\" output = perform_computations(tensor1, tensor2) print(output.shape) # Output: torch.Size([3, 5]) ``` **Notes:** - To check for CUDA availability, use `torch.backends.cuda.is_built()` and `torch.backends.cuda.is_available()`. - To check for MPS availability, use `torch.backends.mps.is_built()` and `torch.backends.mps.is_available()`. - Use appropriate device management for tensor operations (e.g., `tensor.to(\'cuda\')`, `tensor.to(\'mps\')`, or `tensor.to(\'cpu\')`).","solution":"import torch def perform_computations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: if torch.cuda.is_available() and torch.backends.cuda.is_built(): print(\\"Using CUDA backend\\") device = \'cuda\' elif hasattr(torch.backends, \'mps\') and torch.backends.mps.is_available() and torch.backends.mps.is_built(): print(\\"Using MPS backend\\") device = \'mps\' else: print(\\"Using CPU backend\\") device = \'cpu\' tensor1 = tensor1.to(device) tensor2 = tensor2.to(device) result = torch.matmul(tensor1, tensor2) return result.cpu()"},{"question":"# Advanced Python C API Coding Assessment Objective Using the python310 documentation provided, your task is to define a new custom object type in Python C API that mimics a simplified version of a linked list. Guidelines 1. **Implement a Custom LinkedList Type:** - Define a new `LinkedList` type that supports: - Adding an integer element to the list (`append` method). - Retrieving an element by its index (`get` method). - Implement appropriate memory management to ensure no memory leaks. 2. **Define and Implement Methods:** - Implement methods to: - Append elements to the list. - Retrieve elements by their index. 3. **Incorporate Attributes:** - Properly manage and access the list\'s attributes, such as the head node and list size. 4. **Example Usage in Python:** - Provide an example of how your custom `LinkedList` type will be used in a Python script with expected outputs. Input and Output Formats - **Input:** - Method to append an integer to the linked list. - Method to retrieve an integer by index from the linked list. - **Output:** - The result of retrieving an element by index. - The final state of the linked list when printed. Constraints - Your implementation should manage memory dynamically and handle potential errors gracefully. - Ensure appropriate use of the C API to manage the type and its methods. - The solution should be efficient and correctly use the provided structures and functions. Performance Requirements - The `append` and `get` methods should operate in O(n) time complexity. Example Here is an example of how your custom type should work: ```python # Initialize a new linked list ll = LinkedList() # Append some elements ll.append(3) ll.append(7) ll.append(5) # Retrieve elements by index print(ll.get(0)) # Output: 3 print(ll.get(1)) # Output: 7 print(ll.get(2)) # Output: 5 # If an element at a non-existent index is retrieved, an appropriate error should be raised print(ll.get(5)) # Output: IndexError ```","solution":"class Node: def __init__(self, value): self.value = value self.next = None class LinkedList: def __init__(self): self.head = None self.size = 0 def append(self, value): new_node = Node(value) if not self.head: self.head = new_node else: current = self.head while current.next: current = current.next current.next = new_node self.size += 1 def get(self, index): if index < 0 or index >= self.size: raise IndexError(\\"Index out of range\\") current = self.head for _ in range(index): current = current.next return current.value def __repr__(self): values = [] current = self.head while current: values.append(current.value) current = current.next return \\"LinkedList([\\" + \\", \\".join(map(str, values)) + \\"])\\""},{"question":"# Custom Shell with Enhanced History and Autocompletion Using the \\"readline\\" module, you are required to implement a mini shell that provides enhanced command history management and customizable auto-completion functionality. The goal of this assignment is to demonstrate your understanding of various functions in the \\"readline\\" module to extend the interactive command-line experience. The mini-shell should have the following features: 1. **History Management**: - Load and save command history from a file named `.myshell_history` in the user\'s home directory. - Limit the history to the last 500 commands. 2. **Customized Auto-completion**: - Implement a simple auto-completion that suggests commands based on the first word typed. The suggestions can be hardcoded as `[\'list\', \'load\', \'save\', \'exit\']`. 3. **Display the Shell Prompt**: - The prompt should display as `myshell> ` # Requirements: 1. **Initialize the Environment**: - Load the history file. - Set the maximum history length to 500. 2. **Implement Auto-completion**: - Use the auto-completion feature to suggest commands from a predefined list. 3. **Read and Execute Commands**: - Repeatedly read commands from the user, display a prompt, and execute the commands (`list`, `load`, `save`, `exit`). 4. **History File Management**: - Save the session\'s command history to `.myshell_history` on exiting the shell. # Input and Output: - **Input**: - Commands typed by the user in response to the shell prompt. - **Output**: - The corresponding output of the commands described above. For simplicity, you can just print a message (`Executing list`, `Loading...`, etc.). - Must handle the `exit` command to save history and terminate the shell. # Constraints: - You are only allowed to use functionality provided by the Python standard library. - Assume well-formed input and do not worry about error handling for invalid commands. # Implementation Template: ```python import readline import atexit import os # Define the list of valid commands for auto-completion COMMANDS = [\'list\', \'load\', \'save\', \'exit\'] def completer(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None def init_history(histfile): try: readline.read_history_file(histfile) except FileNotFoundError: pass readline.set_history_length(500) atexit.register(readline.write_history_file, histfile) def main(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".myshell_history\\") init_history(histfile) readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") while True: try: command = input(\\"myshell> \\") if command == \\"exit\\": break elif command == \\"list\\": print(\\"Executing list\\") elif command == \\"load\\": print(\\"Loading...\\") elif command == \\"save\\": print(\\"Saving...\\") else: print(\\"Unknown command\\") except EOFError: break if __name__ == \\"__main__\\": main() ``` Provide thorough answers and documentation in your solution code where necessary.","solution":"import readline import atexit import os # Define the list of valid commands for auto-completion COMMANDS = [\'list\', \'load\', \'save\', \'exit\'] def completer(text, state): options = [cmd for cmd in COMMANDS if cmd.startswith(text)] if state < len(options): return options[state] else: return None def init_history(histfile): try: readline.read_history_file(histfile) except FileNotFoundError: pass readline.set_history_length(500) atexit.register(readline.write_history_file, histfile) def main(): histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".myshell_history\\") init_history(histfile) readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") while True: try: command = input(\\"myshell> \\") if command == \\"exit\\": break elif command == \\"list\\": print(\\"Executing list\\") elif command == \\"load\\": print(\\"Loading...\\") elif command == \\"save\\": print(\\"Saving...\\") else: print(\\"Unknown command\\") except EOFError: break if __name__ == \\"__main__\\": main()"},{"question":"# Serialization of Custom Objects with External References Objective In this task, you are required to write a Python script using the `pickle` module to serialize and deserialize custom objects, including handling external references. Problem Statement You will implement a simplified serialization system for a task management application. The tasks will be stored in a database, and the `Task` objects will be serialized with references to their database entries. 1. **Create a class `Task`**: - Attributes: - `id` (int): The unique identifier of the task. - `name` (str): The task description. - `completed` (bool): Status of the task. - Methods: - `__init__(self, id: int, name: str, completed: bool)`: Initializes the attributes. - `__repr__(self)`: Returns a string representation of the `Task` instance. 2. **Custom Pickler and Unpickler**: - Implement `TaskPickler` and `TaskUnpickler` classes inheriting from `pickle.Pickler` and `pickle.Unpickler`, respectively. - Override `persistent_id` in `TaskPickler` to return a unique identifier for `Task` objects. - Override `persistent_load` in `TaskUnpickler` to load `Task` objects from the database based on their unique identifier. 3. **Simulate Database Operations**: - Use a dictionary to simulate the task database where keys are task IDs and values are `Task` instances. - Implement functions `add_task_to_db(task: Task)` and `get_task_from_db(task_id: int)` to interact with the task database. Requirements 1. **Define the class `Task`** with the specified attributes and methods. 2. **Implement `TaskPickler` and `TaskUnpickler`**: - `TaskPickler` should generate a persistent ID for `Task` objects. - `TaskUnpickler` should load `Task` objects from the simulated database. 3. **Simulate database operations**: - Create a global dictionary to act as the task database. - Implement `add_task_to_db(task: Task)` to store a task in the database. - Implement `get_task_from_db(task_id: int)` to retrieve a task from the database. 4. **Demonstrate the functionality**: - Create a few `Task` objects and add them to the simulated database. - Serialize the list of `Task` objects using `TaskPickler`. - Modify the tasks in the database. - Deserialize the list using `TaskUnpickler` and verify that the original tasks are correctly restored, reflecting modifications. Constraints - Use Protocol 5 for pickling. - Implement error handling for invalid task IDs during unpickling. Example ```python import pickle from io import BytesIO class Task: def __init__(self, id, name, completed): self.id = id self.name = name self.completed = completed def __repr__(self): return f\\"Task(id={self.id}, name=\'{self.name}\', completed={self.completed})\\" class TaskPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Task): return f\\"Task-{obj.id}\\" return None class TaskUnpickler(pickle.Unpickler): def __init__(self, file, database): super().__init__(file) self.database = database def persistent_load(self, pid): if pid.startswith(\\"Task-\\"): task_id = int(pid.split(\'-\')[1]) return self.database.get(task_id) raise pickle.UnpicklingError(f\\"Unsupported persistent object with id {pid}\\") # Simulated database task_db = {} def add_task_to_db(task): task_db[task.id] = task def get_task_from_db(task_id): return task_db.get(task_id, None) # Demonstration of functionality if __name__ == \\"__main__\\": # Create and add tasks to the database tasks = [Task(1, \\"Write code\\", False), Task(2, \\"Review code\\", True), Task(3, \\"Test code\\", False)] for task in tasks: add_task_to_db(task) # Serialize tasks buffer = BytesIO() pickler = TaskPickler(buffer, protocol=5) pickler.dump(tasks) serialized_data = buffer.getvalue() # Modify tasks in the database task_db[1].completed = True task_db[3].name = \\"Test and deploy code\\" # Deserialize tasks buffer = BytesIO(serialized_data) unpickler = TaskUnpickler(buffer, task_db) deserialized_tasks = unpickler.load() # Output the restored tasks for task in deserialized_tasks: print(task) ``` Executing the script should correctly handle the serialization and deserialization of `Task` objects, even when the underlying database has been modified.","solution":"import pickle from io import BytesIO class Task: def __init__(self, id, name, completed): self.id = id self.name = name self.completed = completed def __repr__(self): return f\\"Task(id={self.id}, name=\'{self.name}\', completed={self.completed})\\" class TaskPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Task): return f\\"Task-{obj.id}\\" return None class TaskUnpickler(pickle.Unpickler): def __init__(self, file, database): super().__init__(file) self.database = database def persistent_load(self, pid): if pid.startswith(\\"Task-\\"): task_id = int(pid.split(\'-\')[1]) task = self.database.get(task_id) if task is None: raise pickle.UnpicklingError(f\\"Task with id {task_id} not found in database\\") return task raise pickle.UnpicklingError(f\\"Unsupported persistent object with id {pid}\\") # Simulated database task_db = {} def add_task_to_db(task): task_db[task.id] = task def get_task_from_db(task_id): return task_db.get(task_id, None) # Demonstration of functionality if __name__ == \\"__main__\\": # Create and add tasks to the database tasks = [Task(1, \\"Write code\\", False), Task(2, \\"Review code\\", True), Task(3, \\"Test code\\", False)] for task in tasks: add_task_to_db(task) # Serialize tasks buffer = BytesIO() pickler = TaskPickler(buffer, protocol=5) pickler.dump(tasks) serialized_data = buffer.getvalue() # Modify tasks in the database task_db[1].completed = True task_db[3].name = \\"Test and deploy code\\" # Deserialize tasks buffer = BytesIO(serialized_data) unpickler = TaskUnpickler(buffer, task_db) deserialized_tasks = unpickler.load() # Output the restored tasks for task in deserialized_tasks: print(task)"},{"question":"**Objective**: Assess the understanding and application of linear models using scikit-learn, focusing on model training, regularization, and evaluation. **Problem Statement**: You are given a dataset represented by feature matrix `X` and target vector `y`. Your task is to implement functions that train and evaluate three different linear models: Ordinary Least Squares (OLS), Ridge, and Lasso. You will use cross-validation to select the best model based on the mean squared error (MSE). # Requirements 1. Implement functions to train the following models: - Ordinary Least Squares (OLS) using `sklearn.linear_model.LinearRegression`. - Ridge Regression using `sklearn.linear_model.Ridge`. - Lasso Regression using `sklearn.linear_model.Lasso`. 2. Implement a function to perform cross-validation and return the model with the lowest mean squared error (MSE). 3. Implement a function to fit the best model to the entire dataset and make predictions. # Input and Output Formats **Input**: 1. `X_train`: A numpy array of shape `(n_samples, n_features)` representing the training feature matrix. 2. `y_train`: A numpy array of shape `(n_samples,)` representing the training target vector. 3. `X_test`: A numpy array of shape `(m_samples, n_features)` representing the test feature matrix. 4. `alphas`: A list of positive floats to be used as regularization strengths for Ridge and Lasso. **Output**: 1. `best_model`: A string indicating the type of the best model. 2. `test_predictions`: A numpy array of shape `(m_samples,)` containing predicted values for `X_test`. # Constraints 1. You must use a 5-fold cross-validation to evaluate the models. 2. The `alphas` list can contain up to 20 different regularization strengths. 3. Model selection should be based on the average MSE across the 5 folds. # Performance Requirements Your solution should efficiently handle datasets with up to 10,000 samples and 100 features. # Steps 1. **Training Models**: Implement the following functions: ```python def train_ols(X_train, y_train): # Train Ordinary Least Squares model pass def train_ridge(X_train, y_train, alpha): # Train Ridge model with given alpha pass def train_lasso(X_train, y_train, alpha): # Train Lasso model with given alpha pass ``` 2. **Cross-Validation and Model Selection**: Implement the function: ```python def select_best_model(X_train, y_train, alphas): # Perform 5-fold cross-validation and select the best model pass ``` 3. **Fit Best Model and Predict**: Implement the function: ```python def fit_and_predict_best_model(X_train, y_train, X_test, alphas): # Fit the best model on the entire training set and make predictions pass ``` 4. **Main Function**: Implement the `main` function to tie all the components together: ```python def main(X_train, y_train, X_test, alphas): best_model, test_predictions = fit_and_predict_best_model(X_train, y_train, X_test, alphas) return best_model, test_predictions ``` # Example Usage ```python from sklearn.datasets import make_regression import numpy as np # Generate synthetic data for demonstration X_train, y_train = make_regression(n_samples=1000, n_features=20, noise=0.1) X_test, _ = make_regression(n_samples=200, n_features=20, noise=0.1) alphas = [0.1, 1, 10, 100] best_model, test_predictions = main(X_train, y_train, X_test, alphas) print(\\"Best Model:\\", best_model) print(\\"Test Predictions:\\", test_predictions) ``` Ensure your code is well-commented, clean, and adheres to best practices in machine learning model training and evaluation with scikit-learn.","solution":"import numpy as np from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.model_selection import cross_val_score def train_ols(X_train, y_train): Train Ordinary Least Squares (OLS) model on the training data. model = LinearRegression() model.fit(X_train, y_train) return model def train_ridge(X_train, y_train, alpha): Train Ridge Regression model on the training data with given alpha. model = Ridge(alpha=alpha) model.fit(X_train, y_train) return model def train_lasso(X_train, y_train, alpha): Train Lasso Regression model on the training data with given alpha. model = Lasso(alpha=alpha) model.fit(X_train, y_train) return model def select_best_model(X_train, y_train, alphas): Perform cross-validation and select the best model based on MSE. models = [ (\'OLS\', LinearRegression()), *[(f\'Ridge(alpha={alpha})\', Ridge(alpha=alpha)) for alpha in alphas], *[(f\'Lasso(alpha={alpha})\', Lasso(alpha=alpha)) for alpha in alphas] ] best_model = None best_score = float(\'inf\') for name, model in models: mse_scores = -cross_val_score(model, X_train, y_train, cv=5, scoring=\'neg_mean_squared_error\') avg_mse = mse_scores.mean() if avg_mse < best_score: best_score = avg_mse best_model = model return best_model def fit_and_predict_best_model(X_train, y_train, X_test, alphas): Fit the best model on the entire training set and make predictions on the test set. best_model = select_best_model(X_train, y_train, alphas) best_model.fit(X_train, y_train) test_predictions = best_model.predict(X_test) return best_model.__class__.__name__, test_predictions def main(X_train, y_train, X_test, alphas): best_model, test_predictions = fit_and_predict_best_model(X_train, y_train, X_test, alphas) return best_model, test_predictions"},{"question":"**Problem Statement:** You are tasked with analyzing the `penguins` dataset using the seaborn package and creating visualizations to help explore the data. # Dataset Information The `penguins` dataset contains the following columns: - `species`: Species of the penguin. - `island`: Island where the penguin was observed. - `bill_length_mm`: Length of the penguin\'s bill. - `bill_depth_mm`: Depth of the penguin\'s bill. - `flipper_length_mm`: Length of the penguin\'s flipper. - `body_mass_g`: Body mass of the penguin. - `sex`: Sex of the penguin. # Requirements 1. **Loading and Exploring the Data** - Load the `penguins` dataset using `seaborn.load_dataset`. - Display the first few rows of the dataset to understand its structure. 2. **Visualizations** - **Visualization 1: Dot Plot with Error Bars** - Create a dot plot showing the `body_mass_g` on the x-axis and `species` on the y-axis. - Color-code the dots based on the `sex` of the penguins. - Add error bars representing the standard deviation (`sd`) of `body_mass_g` within each category. - **Visualization 2: Line Plot with Facets** - Create a line plot showing the relationship between `sex` (x-axis) and `body_mass_g` (y-axis) for different `species`. - Facet the plot by `species`. - Add line markers and error bars representing the standard deviation (`sd`). - **Visualization 3: Range Plot** - Create a range plot showing the minimum and maximum values of `bill_length_mm` and `bill_depth_mm` for each penguin (`penguin` as index). - Color-code the range intervals based on `island`. # Implementation Implement the following function to generate the required visualizations: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguin_visualizations(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Display the first few rows of the dataset print(penguins.head()) # Visualization 1: Dot Plot with Error Bars ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ) plt.title(\'Dot Plot of Body Mass by Species and Sex\') plt.show() # Visualization 2: Line Plot with Facets ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) ) plt.title(\'Line Plot of Body Mass by Sex and Species with Facets\') plt.show() # Visualization 3: Range Plot ( penguins .rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .add(so.Range(), color=\\"island\\") ) plt.title(\'Range Plot of Bill Dimensions by Penguin and Island\') plt.show() # Call the function to generate the visualizations create_penguin_visualizations() ``` # Notes: - Ensure all necessary imports are included. - Utilize seaborn\'s object-oriented interface for creating the plots. - Customize the plots with titles to enhance readability. - Handle any issues that may arise if the dataset has missing values or other inconsistencies. # Submission Submit your function implementation along with screenshots of the visualizations created.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_penguin_visualizations(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Display the first few rows of the dataset print(penguins.head()) # Visualization 1: Dot Plot with Error Bars ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) ).plot() plt.title(\'Dot Plot of Body Mass by Species and Sex\') plt.show() # Visualization 2: Line Plot with Facets ( so.Plot(penguins, x=\\"sex\\", y=\\"body_mass_g\\", linestyle=\\"species\\") .facet(\\"species\\") .add(so.Line(marker=\\"o\\"), so.Agg()) .add(so.Range(), so.Est(errorbar=\\"sd\\")) ).plot() plt.title(\'Line Plot of Body Mass by Sex and Species with Facets\') plt.show() # Visualization 3: Range Plot ( penguins .rename_axis(index=\\"penguin\\") .pipe(so.Plot, x=\\"penguin\\", ymin=\\"bill_depth_mm\\", ymax=\\"bill_length_mm\\") .add(so.Range(), color=\\"island\\") ).plot() plt.title(\'Range Plot of Bill Dimensions by Penguin and Island\') plt.show() # Call the function to generate the visualizations create_penguin_visualizations()"},{"question":"# Seaborn Coding Assessment **Objective:** To assess your understanding of the seaborn library\'s `seaborn.objects` module, particularly its capabilities for creating complex plots using dot marks. **Problem Statement:** You are given a dataset containing information about restaurant bills and tips. Your task is to create a comprehensive visualization that provides insights into the relationships between total bill amounts, tips, and days of the week, considering the customer\'s gender. The dataset is provided via the seaborn `load_dataset` function. # Dataset: The dataset `tips` includes the following columns: - `total_bill`: Total bill amount. - `tip`: Tip given. - `sex`: Gender of the person paying the bill. - `day`: Day of the week. - `time`: Time of the day (Lunch/Dinner). - `size`: Size of the table. # Requirements: 1. Create a plot that shows total bills (`total_bill`) on the x-axis and days of the week (`day`) on the y-axis. 2. Add dots to the plot where each dot represents a data point. Use different colors to differentiate between the genders (`sex`). 3. Apply jitter to the dots to reduce overplotting. 4. Add a second layer of larger dots that aggregate the data points by showing the mean tip amount (`tip`) for each total_bill and day combination. The size of these dots should correspond to the count of data points in each aggregation. 5. Add error bars representing the standard error of the mean for the tips. # Input: The input will be provided as part of the seaborn library by loading the `tips` dataset. # Constraints: - You should use seaborn and its `seaborn.objects` module to create the plot. - Utilize appropriate properties and methods to meet the requirements. # Expected Output: A plot that accurately reflects the requirements described above. # Example Code: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset tips = load_dataset(\\"tips\\") # Create the plot according to the requirements plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(), so.Jitter(.2), color=\\"sex\\") # Step 1-3 .add(so.Dot(pointsize=6), so.Agg(), marker=\'sex\', color=\\"sex\\") # Step 4 .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) # Step 5 ) # Display the plot plot.show() ``` **Note:** Ensure your code is well-commented, and provide explanations for each step to demonstrate your understanding of the seaborn functionalities used.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def create_tips_plot(): # Load the \'tips\' dataset tips = load_dataset(\\"tips\\") # Create the plot with the given requirements plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(), so.Jitter(.2), color=\\"sex\\") # Step 1-3: Dot plot with jitter, colored by gender (sex) .add(so.Dot(pointsize=10), so.Agg(), color=\\"sex\\", marker=\\"sex\\") # Step 4: Aggregate by mean tip, size by count, colored by gender (sex) .add(so.Range(), so.Est(errorbar=(\\"se\\", 1))) # Step 5: Error bars representing standard error of the mean ) # Display the plot plt.figure(figsize=(10, 6)) plot.show() # Example call to create the plot (This line is just for demonstrating the function call, typically not included in the final module) create_tips_plot()"},{"question":"**Question: Pipeline Transformation Using `pipes` Module** You are given several text files containing lower-case English words separated by spaces. Your task is to create a pipeline using the `pipes.Template` class to convert every word in these files to upper-case and save the output to a new file. Write a function `transform_files(input_files: List[str], output_file: str) -> None` that takes a list of input file names and an output file name as arguments. This function should: 1. Use the `pipes.Template` class to create a pipeline that converts all text from lowercase to uppercase. 2. Append the necessary commands to read from a file and write to another file. 3. Open the output file through the pipeline and write the transformed text to it. # Constraints: - You should not use any temporary intermediate files. - You should handle any errors gracefully (e.g., file not found). - The input files will contain only lower-case English words and spaces. - You may assume that the list of input files is non-empty and all listed files exist. # Example: Assume you have two input files: - `input1.txt` contains: `\\"hello world\\"` - `input2.txt` contains: `\\"python programming\\"` If you call `transform_files([\'input1.txt\', \'input2.txt\'], \'output.txt\')`, it should create `output.txt` with the content: `\\"HELLO WORLD PYTHON PROGRAMMING\\"` Implement the `transform_files` function. ```python from typing import List import pipes def transform_files(input_files: List[str], output_file: str) -> None: # Your implementation here pass ``` # Hint: Use the `append` and `open` methods of the `pipes.Template` class to set up your pipeline correctly.","solution":"from typing import List import pipes def transform_files(input_files: List[str], output_file: str) -> None: template = pipes.Template() template.append(\\"tr \'[a-z]\' \'[A-Z]\'\\", \'--\') with template.open(output_file, \'w\') as f_output: for input_file in input_files: try: with open(input_file, \'r\') as f_input: text = f_input.read() f_output.write(text + \' \') except FileNotFoundError: print(f\\"File {input_file} not found. Skipping.\\")"},{"question":"# Pandas DataFrame Manipulation and Analysis You are provided with a CSV file named `employee_data.csv` which contains the following columns: - `EmployeeID` (int): Unique identifier for each employee. - `Name` (str): Name of the employee. - `Department` (str): Department where the employee works. - `JoiningDate` (str): The date when the employee joined the company (in YYYY-MM-DD format). - `Salary` (float): Salary of the employee. - `PerformanceScore` (float, may contain missing values): Performance score of the employee. Your task is to write a function `analyze_employee_data` that performs the following operations: 1. Read the CSV file into a DataFrame. 2. Convert the `JoiningDate` column to datetime format. 3. Calculate the average salary and return it. 4. Calculate the average performance score for each department and remove any departments with fewer than 3 employees from the result. - Note: If a department has missing performance scores for all its employees, its average performance score should be `NaN`. 5. Add a new column `YearsAtCompany` which represents the number of years each employee has been at the company, rounded to the nearest integer. 6. Return a DataFrame with the columns `EmployeeID`, `Name`, `Department`, `YearsAtCompany`, sorted by `YearsAtCompany` in descending order. # Function Signature ```python import pandas as pd def analyze_employee_data(csv_file: str) -> (float, pd.DataFrame, pd.Series): pass ``` # Example Usage ```python average_salary, sorted_df, departmental_performance = analyze_employee_data(\\"employee_data.csv\\") print(\\"Average Salary:\\", average_salary) print(\\"Sorted DataFrame:n\\", sorted_df) print(\\"Departmental Performance:n\\", departmental_performance) ``` # Input - `csv_file` (str): Path to the CSV file containing employee data. # Output - A float representing the average salary across all employees. - A DataFrame containing the columns `EmployeeID`, `Name`, `Department`, `YearsAtCompany`, sorted by `YearsAtCompany` in descending order. - A Series with the average performance score of each department, excluding departments with fewer than 3 employees. # Constraints - You cannot modify the CSV file. - Ensure efficient computation with a focus on using pandas\' vectorized operations. # Notes - Handle missing values in `PerformanceScore` appropriately. - If the `PerformanceScore` for all employees in a department is missing, that department\'s average should be `NaN`.","solution":"import pandas as pd from datetime import datetime def analyze_employee_data(csv_file: str) -> (float, pd.DataFrame, pd.Series): # Step 1: Read the CSV file into a DataFrame df = pd.read_csv(csv_file) # Step 2: Convert the `JoiningDate` column to datetime format df[\'JoiningDate\'] = pd.to_datetime(df[\'JoiningDate\']) # Step 3: Calculate the average salary and return it average_salary = df[\'Salary\'].mean() # Step 4: Calculate the average performance score for each department departmental_performance = df.groupby(\'Department\')[\'PerformanceScore\'].mean() # Removing departments with fewer than 3 employees department_counts = df[\'Department\'].value_counts() valid_departments = department_counts[department_counts >= 3].index departmental_performance = departmental_performance[valid_departments] # Step 5: Add a new column `YearsAtCompany` current_date = datetime.now() df[\'YearsAtCompany\'] = df[\'JoiningDate\'].apply(lambda x: round((current_date - x).days / 365)) # Step 6: Return the required DataFrame sorted by `YearsAtCompany` in descending order sorted_df = df[[\'EmployeeID\', \'Name\', \'Department\', \'YearsAtCompany\']].sort_values(by=\'YearsAtCompany\', ascending=False) return average_salary, sorted_df, departmental_performance"},{"question":"**Contextual Plotting with Seaborn** Objective You are tasked with creating a function that generates a series of plots using seaborn, each with different contexts and customizations. This will assess the understanding of seaborn\'s context and parameter customization capabilities. Problem Statement Write a function `custom_seaborn_plots(data)` that takes a dictionary `data` as input and generates three plots saved as PNG files in the working directory: 1. A line plot with \\"notebook\\" context and no additional customizations. 2. A line plot with \\"paper\\" context and font scaled by 1.5. 3. A line plot with \\"talk\\" context and a line width set to 5. # Function Signature ```python def custom_seaborn_plots(data: dict) -> None: pass ``` # Input - `data`: A dictionary with keys \\"x\\" and \\"y\\" containing lists of numerical values. Example: `{\\"x\\": [0, 1, 2], \\"y\\": [1, 3, 2]}`. # Output The function does not return any value. Instead, it should produce and save three PNG images: 1. `plot_notebook.png` 2. `plot_paper.png` 3. `plot_talk.png` # Constraints - Ensure the function handles input data correctly and uses seaborn to generate the plots with the appropriate contexts and customizations. - The plots should be saved with the exact filenames specified above. - Use matplotlib\'s `savefig` function to save the plots. # Example Usage ```python data = {\\"x\\": [0, 1, 2], \\"y\\": [1, 3, 2]} custom_seaborn_plots(data) ``` After execution, the current working directory should contain `plot_notebook.png`, `plot_paper.png`, and `plot_talk.png`, each representing the described plots. # Additional Information - Ensure seaborn and matplotlib packages are imported in your solution. - Plot titles and labels are not mandatory but you can add them for better clarity. Happy Coding!","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_seaborn_plots(data): # Plot with \\"notebook\\" context sns.set_context(\\"notebook\\") plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.savefig(\\"plot_notebook.png\\") plt.close() # Plot with \\"paper\\" context and increased font scale sns.set_context(\\"paper\\", font_scale=1.5) plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"]) plt.savefig(\\"plot_paper.png\\") plt.close() # Plot with \\"talk\\" context and increased line width sns.set_context(\\"talk\\") plt.figure() sns.lineplot(x=data[\\"x\\"], y=data[\\"y\\"], linewidth=5) plt.savefig(\\"plot_talk.png\\") plt.close()"},{"question":"**Email Utilities Challenge** **Objective:** Write a Python function that processes a list of raw email headers and returns a summary containing parsed recipient addresses and the message ID. **Function Signature:** ```python def process_email_headers(headers: list) -> dict: pass ``` **Parameters:** - `headers` (list): A list of strings, each representing a raw email header in RFC 2822 format. **Returns:** - A dictionary with two keys: - `recipients`: A list of tuples, each containing the real name and email address of a recipient. - `message_id`: A string representing the unique message ID for the email. If the message ID is not directly provided in the headers, generate one using `email.utils.make_msgid()`. **Constraints:** - The function should be able to handle malformed email addresses gracefully. - Utilize relevant functions from the `email.utils` module to parse and format the email headers appropriately. **Example:** ```python headers = [ \\"From: John Doe <john.doe@example.com>\\", \\"To: Jane Smith <jane.smith@example.com>, Doe <doe@example.com>\\", \\"Cc: David <david@example.com>\\", \\"Message-ID: <abc123@example.com>\\" ] output = { \'recipients\': [ (\'Jane Smith\', \'jane.smith@example.com\'), (\'Doe\', \'doe@example.com\'), (\'David\', \'david@example.com\') ], \'message_id\': \'<abc123@example.com>\' } ``` **Notes:** - Address headers may include \'To\', \'Cc\', and \'Bcc\'. - The function should use `email.utils.parseaddr()` and `email.utils.getaddresses()` to handle address parsing. - If no \'Message-ID\' is found in the headers, generate and use a message ID with `email.utils.make_msgid()`.","solution":"from email.utils import getaddresses, parseaddr, make_msgid def process_email_headers(headers): Processes a list of raw email headers and returns a summary containing parsed recipient addresses and the message ID. Params: - headers (list): A list of strings representing the raw email headers. Returns: - dict: A dictionary with \'recipients\' and \'message_id\' keys. recipients = [] message_id = None for header in headers: if header.startswith((\'To:\', \'Cc:\', \'Bcc:\')): addr_strings = header.split(\':\', 1)[1].strip() addresses = getaddresses([addr_strings]) recipients.extend(addresses) elif header.startswith(\'Message-ID:\'): message_id = header.split(\':\', 1)[1].strip() if not message_id: message_id = make_msgid() # Parse recipient names and email addresses parsed_recipients = [] for name, email in recipients: parsed_recipients.append((name.strip(\'\\"\'), email)) return { \'recipients\': parsed_recipients, \'message_id\': message_id }"},{"question":"**Python Terminal Mode Manipulation** You are given the task to implement a function that toggles the terminal\'s file descriptor between raw and cbreak modes. This will help you understand how terminal input is handled in different modes and enable you to control terminal behaviors programmatically. # Function Description: Create a function `toggle_terminal_mode(fd: int, mode: str) -> None` that changes the mode of the given file descriptor `fd` to the specified `mode`. # Parameters: - `fd` (int): The file descriptor of the terminal you want to modify. - `mode` (str): The mode to set the terminal to, which can be either `\'raw\'` or `\'cbreak\'`. # Constraints: - The function should raise a `ValueError` if an invalid mode is passed. - The function should work only on Unix-based systems since the `tty` and `termios` modules are used. # Example Usage: ```python import sys import tty def toggle_terminal_mode(fd: int, mode: str) -> None: if mode not in {\'raw\', \'cbreak\'}: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'.\\") if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) # Example: Toggle to raw mode # toggle_terminal_mode(sys.stdin.fileno(), \'raw\') # Toggle back to cbreak mode # toggle_terminal_mode(sys.stdin.fileno(), \'cbreak\') ``` # Note: - You can use `sys.stdin.fileno()` to get the file descriptor for standard input in a Unix-based system. - Ensure you handle any exceptions that might arise when dealing with terminal modes. Implement the function with attention to detail to handle different modes robustly and understand the implications of these modes on terminal input behavior.","solution":"import tty import sys def toggle_terminal_mode(fd: int, mode: str) -> None: Toggles the terminal\'s file descriptor between raw and cbreak modes. Parameters: - fd (int): The file descriptor of the terminal to be modified. - mode (str): The mode to set the terminal to (\'raw\' or \'cbreak\'). Raises: - ValueError: If the provided mode is not \'raw\' or \'cbreak\'. if mode not in {\'raw\', \'cbreak\'}: raise ValueError(\\"Invalid mode. Use \'raw\' or \'cbreak\'.\\") if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd)"},{"question":"Coding Assessment Question # Objective You are required to demonstrate your understanding of PyTorch\'s deterministic settings and uninitialized memory management. Specifically, you will manipulate deterministic algorithms and the `fill_uninitialized_memory` attribute to observe their effects on tensor operations. # Task Write a function `test_deterministic_memory_handling` that performs the following steps: 1. Sets PyTorch to use deterministic algorithms. 2. Creates an uninitialized tensor using `torch.empty`. 3. Toggles the `fill_uninitialized_memory` setting between `True` and `False`. 4. Observes and records the contents of the uninitialized tensor under both settings. 5. Returns the results as a dictionary with keys `\'filled_memory\'` and `\'unfilled_memory\'`, each containing the tensor contents observed under the corresponding settings. # Function Signature ```python import torch def test_deterministic_memory_handling(): # Your code here pass ``` # Constraints 1. You must use the `torch.use_deterministic_algorithms` function to enable deterministic algorithms. 2. The tensor operations to create uninitialized tensors should use methods listed in the documentation (e.g., `torch.empty`). # Example Output Here is an example of what the output might look like: ```python { \'filled_memory\': tensor([nan, nan, nan]), # or maximum integer values for integer tensors \'unfilled_memory\': tensor([1.4013e-45, 0.0000e+00, 3.4033e-44]) # random values in memory } ``` # Performance Requirements - Ensure no unnecessary operations are performed that could impact performance. # Explanation Explain in brief how changing the `fill_uninitialized_memory` setting impacts the contents of an uninitialized tensor.","solution":"import torch def test_deterministic_memory_handling(): Observes and records the contents of an uninitialized tensor under both settings of `torch.backends.cudnn.flags.deterministic_memory_mode`. Returns: dict: A dictionary with keys \'filled_memory\' and \'unfilled_memory\' representing the tensor contents under the corresponding settings. torch.use_deterministic_algorithms(True) # Create an uninitialized tensor uninitialized_tensor = torch.empty(3) # Test with fill_uninitialized_memory = True torch.backends.cudnn.flags.deterministic_memory_mode = True filled_memory = uninitialized_tensor.clone() # Test with fill_uninitialized_memory = False torch.backends.cudnn.flags.deterministic_memory_mode = False unfilled_memory = uninitialized_tensor.clone() return { \'filled_memory\': filled_memory, \'unfilled_memory\': unfilled_memory } # Note: For this example, fill_uninitialized_memory is not available directly in PyTorch. # This is a conceptual demonstration based on the given task."},{"question":"**Objective**: Implement a utility that uses the `runpy` module to execute multiple Python scripts from different paths concurrently. Given the non-thread-safe nature of the `runpy` functions as noted in the documentation, your implementation should handle these scripts using separate processes. **Task Details**: You are required to write a Python function `run_multiple_scripts(script_paths)` that: 1. Accepts a list of file paths pointing to Python scripts. 2. Executes each script concurrently in separate processes. 3. Collects and returns the output (the final global variables dictionary) of each script in a dictionary where the keys are the script paths and the values are the respective globals dictionaries. **Function Signature**: ```python import multiprocessing def run_multiple_scripts(script_paths: list) -> dict: pass ``` **Input**: - `script_paths`: A list of strings where each string is a valid file path to a Python script. **Output**: - A dictionary where each key is the file path of a script and the value is the globals dictionary obtained after executing that script. **Constraints**: 1. Ensure that the function handles exceptions gracefully. If a script fails to execute, its entry in the returned dictionary should be `None`. 2. Scripts might have side effects â€” ensure that they do not interfere with each other by running them in separate processes. 3. Assume all scripts run to completion and do not require input during execution. **Example**: ```python script_results = run_multiple_scripts([\'/path/to/script1.py\', \'/path/to/script2.py\']) print(script_results) /* Expected Output (structure): { \'/path/to/script1.py\': { ... globals from script1 ... }, \'/path/to/script2.py\': { ... globals from script2 ... } } */ ``` **Performance Requirements**: - The solution should be efficient in terms of utilizing multiple processes, meaning it should avoid unnecessary delays and execute the scripts concurrently. **Hints**: 1. Consider using the `multiprocessing` module to handle concurrent processes. 2. Use `runpy.run_path()` to execute each script and get the resulting globals dictionary. Good luck! This problem assesses your understanding of concurrency, process management, and the use of the `runpy` module in Python.","solution":"import multiprocessing import runpy def run_script_in_process(script_path, return_dict): try: result = runpy.run_path(script_path) return_dict[script_path] = result except Exception as e: return_dict[script_path] = None def run_multiple_scripts(script_paths): manager = multiprocessing.Manager() return_dict = manager.dict() processes = [] for script_path in script_paths: p = multiprocessing.Process(target=run_script_in_process, args=(script_path, return_dict)) processes.append(p) p.start() for p in processes: p.join() return dict(return_dict)"},{"question":"**Question Title:** Secure Data Integrity Checker **Objective:** Write a function that takes a list of byte strings and produces a secure summary and verification mechanism using the hashlib module\'s BLAKE2b algorithm. **Problem Statement:** You are tasked with developing a secure data integrity checker for a system that periodically receives chunks of sensitive data. Your function should: 1. Compute a secure hash for each data chunk using BLAKE2b algorithm. 2. Combine these individual chunk hashes into a secure master hash. 3. Provide a mechanism to verify individual data chunks using the stored master hash. **Function Signature:** ```python def secure_data_checker(data_chunks: list[bytes], key: bytes, salt: bytes) -> tuple[str, dict[bytes, str]]: Takes in a list of byte strings (data_chunks), a key, and a salt. Returns a tuple where: - The first element is the hexadecimal master hash of all chunks combined. - The second element is a dictionary mapping each data chunk to its individual BLAKE2b hexadecimal hash. ``` **Input:** - `data_chunks`: A list of byte strings to be hashed. - `key`: A byte string acting as a key for keyed hashing (maximum length of 64 bytes). - `salt`: A byte string for randomization (maximum length of 16 bytes). **Output:** - A tuple where the first element is the hexadecimal master hash string, and the second element is a dictionary. The dictionary maps each data chunk to its corresponding hexadecimal BLAKE2b hash string. **Constraints:** - Length of `key` should not exceed 64 bytes. - Length of `salt` should not exceed 16 bytes. - The combined length of all data chunks should not exceed 10^6 bytes. **Example:** ```python data_chunks = [b\'chunk1\', b\'chunk2\', b\'chunk3\'] key = b\'my_secure_key\' salt = b\'random_salt\' master_hash, chunk_hashes = secure_data_checker(data_chunks, key, salt) print(f\\"Master Hash: {master_hash}\\") print(\\"Chunk Hashes:\\") for chunk, hash in chunk_hashes.items(): print(f\\"{chunk}: {hash}\\") ``` **Expected Output:** Your function should output a master hash string that summarizes all the chunks, and a dictionary of individual chunk hashes as part of integrity verification. For example, the master hash could look like a long hexadecimal string with a unique value summarizing the entire data received. **Notes:** - Make sure to use the provided key and salt in your BLAKE2b hash computations. - Pay attention to the efficient updating of the hash object with the data chunks. - You may use `hashlib.blake2b` for the hashing process. - Ensure the function handles edge cases, e.g., empty data chunk lists. **Hints:** - Use the `update()` method to add data to the hash object. - Generate hexadecimal digest for readability and comparison. - Refer to `BLAKE2b` section in hashlib documentation for configuration parameters. **Solution Template:** ```python import hashlib def secure_data_checker(data_chunks, key, salt): # Implement initialization of the BLAKE2b hash for master and individual chunks # Initialize master hash # Iterate through data_chunks, compute each chunk\'s hash, and update master hash # Generate the master hash and chunk hashes return (master_hash, chunk_hashes) ```","solution":"import hashlib def secure_data_checker(data_chunks, key, salt): Takes in a list of byte strings (data_chunks), a key, and a salt. Returns a tuple where: - The first element is the hexadecimal master hash of all chunks combined. - The second element is a dictionary mapping each data chunk to its individual BLAKE2b hexadecimal hash. master_hasher = hashlib.blake2b(key=key, salt=salt) chunk_hashes = {} for chunk in data_chunks: chunk_hasher = hashlib.blake2b(key=key, salt=salt) chunk_hasher.update(chunk) chunk_hash = chunk_hasher.hexdigest() chunk_hashes[chunk] = chunk_hash master_hasher.update(bytes.fromhex(chunk_hash)) master_hash = master_hasher.hexdigest() return master_hash, chunk_hashes"},{"question":"**Question: Implement a function to decode HTML entities** You are tasked with implementing a function `decode_html_entities(text: str) -> str` that decodes all HTML entities in a given string into their respective characters. # Specifications 1. The function should transform named HTML entities (e.g., `&amp;`, `&gt;`), Unicode code points (e.g., `&#169;`), and hexadecimal code points (e.g., `&#x1f600;`) into their corresponding characters. 2. Utilize the dictionaries from the `html.entities` module for the decoding process: - `html.entities.html5` - `html.entities.name2codepoint` 3. Ensure that the function handles named entities both with and without a trailing semicolon where applicable. # Inputs - **text** (str): A string containing HTML entities that need to be decoded. # Outputs - Returns a decoded string where all HTML entities have been replaced with their corresponding characters. # Example ```python import html.entities def decode_html_entities(text: str) -> str: import re, html.entities # Regex for detecting HTML entities entity_pattern = re.compile(r\'&(#(x)?[0-9a-fA-F]+|[a-zA-Z0-9]+);?\') def entity_replacer(match): entity = match.group() if entity.startswith(\\"&#\\"): code_point = match.group(1)[1:] if code_point.startswith(\'x\'): return chr(int(code_point[1:], 16)) else: return chr(int(code_point, 10)) else: name = match.group(1) if name in html.entities.html5: return html.entities.html5[name] elif name in html.entities.name2codepoint: return chr(html.entities.name2codepoint[name]) return entity return entity_pattern.sub(entity_replacer, text) # Testing the function input_text = \\"This &amp; that &gt; all &lt; none &copy; 2021 &#169; &#x1F600;\\" print(decode_html_entities(input_text)) ``` **Expected output:** ``` This & that > all < none Â© 2021 Â© ðŸ˜€ ``` # Constraints - The input string will not exceed 1000 characters. - Ensure optimal performance considering the input size.","solution":"def decode_html_entities(text: str) -> str: import re import html.entities # Regex for detecting HTML entities entity_pattern = re.compile(r\'&(#(x)?[0-9a-fA-F]+|[a-zA-Z0-9]+);?\') def entity_replacer(match): entity = match.group(1) if entity.startswith(\\"#\\"): if entity.startswith(\\"#x\\"): return chr(int(entity[2:], 16)) else: return chr(int(entity[1:], 10)) else: if entity in html.entities.html5: return html.entities.html5[entity] elif entity in html.entities.name2codepoint: return chr(html.entities.name2codepoint[entity]) return f\\"&{entity};\\" # If not recognized, keep it as is return entity_pattern.sub(entity_replacer, text)"},{"question":"# Question: Implementing a Custom Cache with `collections.OrderedDict` You are tasked with designing a custom caching mechanism using Python\'s `collections.OrderedDict`. This custom cache should follow the Least Recently Used (LRU) eviction policy, meaning that when the cache reaches its capacity and a new item is added, the least recently used item should be removed to make space. Requirements: 1. Implement a class `LRUCache` that supports the following methods: - `__init__(self, capacity: int)`: Initializes the cache with a given capacity. - `get(self, key: int) -> int`: Returns the value of the key if it exists in the cache, otherwise returns `-1`. - `put(self, key: int, value: int) -> None`: Inserts the key-value pair into the cache. If the key already exists, updates the value. If the cache is at capacity, it should remove the least recently used item before adding the new item. 2. Your implementation should ensure `O(1)` time complexity for both `get` and `put` operations. 3. Use the `collections.OrderedDict` to maintain the order of insertion and efficiently move items to the front when accessed. Example: ```python # Initialize the cache with a capacity of 2 cache = LRUCache(2) # Add an item with key 1 and value 1 cache.put(1, 1) # Add an item with key 2 and value 2 cache.put(2, 2) # Retrieve item with key 1 (expected output: 1) print(cache.get(1)) # Output: 1 # Since key 1 was recently accessed, key 2 becomes the least recently used # Add an item with key 3 and value 3 (this should evict key 2) cache.put(3, 3) # Try to retrieve item with key 2 (expected output: -1) print(cache.get(2)) # Output: -1 # Add an item with key 4 and value 4 (this should evict key 1) cache.put(4, 4) # Try to retrieve item with key 1 (expected output: -1) print(cache.get(1)) # Output: -1 # Retrieve item with key 3 (expected output: 3) print(cache.get(3)) # Output: 3 # Retrieve item with key 4 (expected output: 4) print(cache.get(4)) # Output: 4 ``` Constraints: - `1 <= capacity <= 3000` - `0 <= key <= 10000` - `0 <= value <= 10^5` - At most `10^5` calls will be made to `get` and `put`. Hints: - Utilize the methods `move_to_end` and `popitem` provided by `OrderedDict` to maintain the order and handle evictions. Implement the `LRUCache` class according to the requirements mentioned above.","solution":"from collections import OrderedDict class LRUCache: def __init__(self, capacity: int): self.cache = OrderedDict() self.capacity = capacity def get(self, key: int) -> int: if key in self.cache: self.cache.move_to_end(key) return self.cache[key] return -1 def put(self, key: int, value: int) -> None: if key in self.cache: self.cache.move_to_end(key) self.cache[key] = value if len(self.cache) > self.capacity: self.cache.popitem(last=False)"},{"question":"**Objective:** Demonstrate your understanding of dynamic sizes in PyTorch by implementing a function that handles dynamic batch sizes and conducts operations based on varying dimensions. Question Implement a function `dynamic_size_operation` that takes two batched input tensors `x` and `y`, dynamically concatenate them, and performs conditional operations based on the size of the concatenated result. **Function Signature:** ```python def dynamic_size_operation(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: pass ``` **Requirements:** 1. Mark the batch dimension of both tensors `x` and `y` as dynamic. 2. Concatenate `x` and `y` along the batch dimension. 3. If the size of the concatenated tensor\'s batch dimension is greater than a certain threshold (e.g., 10), apply the `.mul(2)` operation on the concatenated tensor. 4. Otherwise, apply the `.add(2)` operation on the concatenated tensor. 5. Ensure appropriate guards are placed to handle dynamic size conditions without recompilation errors. **Input:** - `x`: A 2D tensor with shape `[B, D]` where `B` is the dynamic batch size and `D` is the fixed feature size. - `y`: A 2D tensor with shape `[B, D]` where `B` is the dynamic batch size and `D` is the fixed feature size. **Output:** - A tensor with the result of either `.mul(2)` or `.add(2)` based on the size of the concatenated batch dimension. **Constraints:** - Ensure the function is efficient and handles various batch sizes without unnecessary recompilation. - Use `torch._dynamo.mark_dynamic` to specify dynamic sizes. - Make use of the guard model to ensure the correct operation is applied. **Example:** ```python import torch def dynamic_size_operation(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: # Your implementation here # Example usage x = torch.randn(5, 100) y = torch.randn(5, 100) result = dynamic_size_operation(x, y) print(result) ``` In this question, you will showcase your ability to work with dynamic sizes in PyTorch by understanding and correctly applying the symbolic shape principles.","solution":"import torch import torch._dynamo as dynamo def dynamic_size_operation(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: # Mark the batch dimension of both tensors as dynamic dynamo.mark_dynamic(x, 0) dynamo.mark_dynamic(y, 0) # Concatenate the tensors along the batch dimension concatenated = torch.cat((x, y), dim=0) # Determine the size of the batch dimension batch_size = concatenated.size(0) # Conditional operations based on the size of the concatenated tensor\'s batch dimension threshold = 10 if batch_size > threshold: result = concatenated * 2 else: result = concatenated + 2 return result"},{"question":"Objective In this exercise, you will demonstrate your ability to use Seaborn for advanced data visualization through the `seaborn.objects` module. You will use the `tips` dataset, available in Seaborn, to create visualizations that capture insights about the data. Task 1. Load the `tips` dataset provided by Seaborn. 2. Create a bar plot that shows the total counts of tips received by day of the week. 3. Modify the bar plot to distinguish the counts by the gender of the person (`sex`) who received the tips, using color for differentiation. 4. Create another bar plot that shows the counts of tips received based on the party size (`size`). Make sure the counts are plotted on the y-axis with the different sizes on the x-axis. 5. Save the two plots as PNG files named `tips_by_day.png` and `tips_by_size.png`. Requirements - Import necessary libraries and document each step with comments. - Include the Seaborn `objects` module and use classes such as `Plot`, `Bar`, and transformations like `Count`. - Use color aesthetics for gender differentiation in the first plot. - Ensure numeric data are plotted correctly without binning for the second plot. - Output two PNG files with clear visual representations. Input No input from the user. Use the `tips` dataset directly. Output Two PNG files: - `tips_by_day.png`: A bar plot of counts of tips by day, colored by gender (`sex`). - `tips_by_size.png`: A bar plot of counts of tips by party size (`size`). Constraints - Use Seaborn\'s `objects` module for all plotting tasks. - Ensure plots are saved with the correct filenames. Example Code ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a bar plot for tips received by day, differentiated by gender plot_day = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot_day.save(\\"tips_by_day.png\\") # Create a bar plot for tips received by party size plot_size = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot_size.save(\\"tips_by_size.png\\") ``` Work through the task to ensure your solution is correct, and the outputs are as expected.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create a bar plot for tips received by day, differentiated by gender plot_day = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot_day.save(\\"tips_by_day.png\\") # Create a bar plot for tips received by party size plot_size = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot_size.save(\\"tips_by_size.png\\")"},{"question":"Problem: You are tasked with creating an interactive text-based calculator using the `curses` module in Python. The calculator should be capable of reading user inputs and performing basic arithmetic operations. The user should be able to type in mathematical expressions and see the results updated in real-time on the terminal screen. Requirements: 1. **Input Window**: Create a window at the bottom of the screen where the user types their input. The input window should support basic text editing (moving the cursor, inserting and deleting text) using the `Textbox` class from `curses.textpad`. 2. **Output Window**: Create an output window above the input window to display the results of the evaluated expressions. 3. **Real-Time Evaluation**: The calculator should evaluate the expression in real-time as the user types, and display the result in the output window. 4. **Key Bindings**: The following key bindings should be supported: - `ENTER`: Evaluate the expression and display the result. - `ESC`: Clear the input window. - Arrow keys for navigation within the input. Constraints: - Distinguish between valid expressions and invalid inputs. If the input is invalid, display an appropriate error message in the output window. Function to Implement: ```python import curses from curses.textpad import Textbox def start_curses_calculator(stdscr): # Create windows for input and output height, width = stdscr.getmaxyx() input_win = curses.newwin(3, width, height-3, 0) output_win = curses.newwin(height-3, width, 0, 0) # Create a Textbox widget for the input window textbox = Textbox(input_win) # Helper function to evaluate the expression and update output def update_output(expression): try: result = eval(expression) output_message = f\\"Result: {result}\\" except Exception as e: output_message = f\\"Error: {str(e)}\\" output_win.clear() output_win.addstr(0, 0, output_message) output_win.refresh() # Main loop to handle input and output while True: input_win.clear() input_win.addstr(0, 0, \\"Input: \\") input_win.refresh() # Get input from textbox and evaluate it ch = input_win.getch() if ch == 10: # Enter key expression = textbox.gather().strip() update_output(expression) textbox.do_command(7) # Terminate input and reset cursor elif ch == 27: # ESC key textbox.do_command(23) # Clear command else: textbox.do_command(ch) if __name__ == \\"__main__\\": curses.wrapper(start_curses_calculator) ``` Explanation: 1. **Input and Output Windows**: Two windows are created; one for input and one for output. The input window is located at the bottom and the output window occupies the rest of the screen. 2. **Textbox Widget**: The `Textbox` widget handles text input and basic text editing. 3. **Real-Time Evaluation**: The provided key bindings manage real-time evaluation and error handling. The entered expression is evaluated using Pythonâ€™s `eval` function, and the result or error message is displayed in the output window. 4. **Refresh Mechanism**: The screen is updated dynamically, so any changes in the input window immediately reflect in the output window.","solution":"import curses from curses.textpad import Textbox, rectangle def start_curses_calculator(stdscr): curses.curs_set(1) # Show the cursor # Get the dimensions of the screen height, width = stdscr.getmaxyx() # Create windows for input and output output_win_height = height - 3 output_win = curses.newwin(output_win_height, width, 0, 0) input_win = curses.newwin(3, width, height-3, 0) # Create a Textbox widget for the input window textbox = Textbox(input_win, insert_mode=True) def update_output(expression): Evaluate the expression and update the output window. try: result = eval(expression) output_message = f\\"Result: {result}\\" except Exception as e: output_message = f\\"Error: {str(e)}\\" output_win.clear() output_win.addstr(0, 0, output_message) output_win.refresh() # Draw the rectangle around the input window stdscr.clear() rectangle(stdscr, height-4, 0, height-1, width-1) stdscr.refresh() while True: input_win.clear() input_win.addstr(0, 0, \\"\\") stdscr.refresh() # Let the user edit inside the textbox textbox.edit() # Gather the value of the input expression = textbox.gather().strip() # Exit on empty input if expression == \\"exit\\": break # Update the output window with the result of the expression if expression: update_output(expression) if __name__ == \\"__main__\\": curses.wrapper(start_curses_calculator)"},{"question":"# **Multi-Output Regression with Decision Trees** You are provided with a dataset that consists of multiple features and multiple target variables. You are required to build a decision tree regression model to predict these multiple outputs. **Dataset Format** The dataset provided will be in CSV format with the following structure: - Each row represents a single data sample. - The first few columns (X1, X2, ..., Xn) are the features. - The last columns (Y1, Y2, ..., Ym) are the target variables. **Task** 1. **Load the Dataset:** - Read the data from a CSV file into a pandas DataFrame. 2. **Data Preparation:** - Split the dataset into features (X) and target variables (Y). 3. **Model Training:** - Train a `DecisionTreeRegressor` model on the dataset using scikit-learn. - Use appropriate parameters to avoid overfitting. 4. **Model Evaluation:** - Evaluate the performance of the model using Mean Squared Error (MSE) for each target variable on a provided test set. 5. **Tree Visualization (Optional):** - Visualize the decision tree using `plot_tree` method. **Expected Inputs:** - A path to the CSV file containing the dataset. **Expected Output:** - Mean Squared Error (MSE) values for each target variable on the test set. **Constraints:** - Use scikit-learn for model implementation. - The model should handle missing values correctly. **Code Structure:** ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error def multi_output_regression(csv_path): # 1. Load the dataset data = pd.read_csv(csv_path) # 2. Data Preparation # Assuming last \'m\' columns are targets, split the data accordingly. Adjust \'m\' accordingly. X = data.iloc[:, :-m] Y = data.iloc[:, -m:] # Split the data into a training set and a test set X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # 3. Model Training model = DecisionTreeRegressor() model.fit(X_train, Y_train) # 4. Model Evaluation Y_pred = model.predict(X_test) mse = mean_squared_error(Y_test, Y_pred, multioutput=\'raw_values\') return mse # Example usage: # csv_path = \'path/to/your/dataset.csv\' # print(multi_output_regression(csv_path)) ``` **Notes:** - Ensure `m` is defined correctly as the number of target variables. - Handle any missing values appropriately during data preparation. - Adjust the parameters of the `DecisionTreeRegressor` to optimize the performance and avoid overfitting. - Visualize the tree only if necessary and feasible within your working environment.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error def multi_output_regression(csv_path, m): Trains a DecisionTreeRegressor on a dataset with multiple outputs. Arguments: csv_path -- str, path to the CSV file containing the dataset. m -- int, number of target variables. Returns: list of float -- Mean Squared Error (MSE) for each target variable. # 1. Load the dataset data = pd.read_csv(csv_path) # 2. Data Preparation X = data.iloc[:, :-m] Y = data.iloc[:, -m:] # Split the data into a training set and a test set X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # 3. Model Training model = DecisionTreeRegressor(random_state=42) model.fit(X_train, Y_train) # 4. Model Evaluation Y_pred = model.predict(X_test) mse = mean_squared_error(Y_test, Y_pred, multioutput=\'raw_values\') return mse # Example usage: # csv_path = \'path/to/your/dataset.csv\' # m = 2 # For example, if there are 2 target variables # print(multi_output_regression(csv_path, m))"},{"question":"Problem Statement You are tasked with designing a logging configuration system for a Python application using the `logging.config` module. Your goal is to implement a function that configures logging based on a given dictionary schema and provides specific functionalities for error handling and custom handler registration. # Function Signature ```python def configure_logging(config: dict) -> None: pass ``` # Input - `config` (dict): A dictionary representing the logging configuration schema. # Output - None # Constraints 1. The `config` dictionary must contain a key `version` set to `1`. 2. The logging configuration must include at least one handler. 3. If the configuration dictionary is invalid or any error occurs during configuration, your function should raise a `ValueError` with a message indicating the issue. 4. Your function should support custom handlers by allowing a `\\"handlers\\"` dictionary where each handler has a `\\"class\\"` key that specifies the handler\'s fully qualified class name. 5. Your function should register a default `StreamHandler` writing to `sys.stdout` if no handlers are specified in the configuration dictionary. # Example Usage ```python config = { \'version\': 1, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'simple\', \'level\': \'DEBUG\', \'stream\': \'ext://sys.stdout\' } }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\'] }, } configure_logging(config) ``` Upon invoking the `configure_logging` function with the above configuration: - The logging system should be configured with a single `StreamHandler` writing to `sys.stdout`. - The formatter should use the specified format. # Additional Requirements 1. Implement error checks for invalid configurations (e.g., missing `version` key, invalid handler class). 2. Allow for extensibility by enabling users to register custom formatter or handler classes through the dictionary configuration. # Tips - Utilize the `logging.config.dictConfig` function to apply the configuration. - Ensure proper validation before calling `dictConfig` to handle and raise custom error messages. - Familiarize yourself with the `logging`, `logging.config`, and `logging.handlers` modules for built-in handler and formatter classes. # Evaluation Your solution will be evaluated based on the following criteria: - Correctness: Proper handling of logging configurations and error conditions. - Robustness: Ability to handle edge cases and invalid input gracefully. - Clarity: Clear code structure and comments explaining your implementation. Good luck, and happy coding!","solution":"import logging import logging.config import sys def configure_logging(config: dict) -> None: Configures the logging based on the provided dictionary. Parameters: config (dict): A dictionary representing the logging configuration schema. Raises: ValueError: If the configuration is invalid or missing required elements. try: # Validate the configuration dictionary if \'version\' not in config or config[\'version\'] != 1: raise ValueError(\\"The configuration dictionary must contain a key \'version\' set to 1.\\") if \'handlers\' not in config or not config[\'handlers\']: # Add a default StreamHandler if no handlers are specified config[\'handlers\'] = { \'default\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'default\', \'stream\': \'ext://sys.stdout\' } } config[\'formatters\'] = config.get(\'formatters\', {}) config[\'formatters\'][\'default\'] = { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' } config[\'root\'] = { \'level\': \'DEBUG\', \'handlers\': [\'default\'] } # Apply the logging configuration logging.config.dictConfig(config) except Exception as e: raise ValueError(f\\"Invalid logging configuration: {e}\\") # Example usage: # config = { # \'version\': 1, # \'formatters\': { # \'simple\': { # \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' # } # }, # \'handlers\': { # \'console\': { # \'class\': \'logging.StreamHandler\', # \'formatter\': \'simple\', # \'level\': \'DEBUG\', # \'stream\': \'ext://sys.stdout\' # } # }, # \'root\': { # \'level\': \'DEBUG\', # \'handlers\': [\'console\'] # }, # } # configure_logging(config)"},{"question":"**Coding Assessment Question: Handling System Errors** **Objective**: Write a function that processes a list of system error codes and returns meaningful error messages using both the `errno` module and the `os` module. This will assess the studentâ€™s understanding of handling and translating system errors into user-friendly messages. **Problem Statement**: You need to implement a function called `translate_error_codes` that takes a list of integer error codes and returns a dictionary mapping each error code to its corresponding error message. The function should handle: 1. Retrieving the string name of the error if available using `errno.errorcode`. 2. Translating the numeric error code to a human-readable error message using `os.strerror`. **Function Signature**: ```python def translate_error_codes(error_codes: list) -> dict: pass ``` **Input**: - `error_codes` (list of int): A list of integer error codes e.g., `[1, 2, 3, 4]`. **Output**: - A dictionary where the keys are the integer error codes, and the values are tuples. Each tuple contains two strings: the error name from `errno.errorcode` (or `\'UNKNOWN\'` if the error code is not found) and the error message from `os.strerror`. **Constraints**: - The list can contain both valid and invalid error codes. - The list will not be empty. **Example**: ```python error_codes = [1, 2, 9999] output = translate_error_codes(error_codes) # Expected output: # { # 1: (\'EPERM\', \'Operation not permitted\'), # 2: (\'ENOENT\', \'No such file or directory\'), # 9999: (\'UNKNOWN\', \'Unknown error 9999\') # } ``` **Hints**: - Use `errno.errorcode` to map integer error codes to their symbolic names. - Use `os.strerror` to obtain a human-readable message for each error code. **Assessment Criteria**: - Correctness of the implementation. - Efficient use of `errno` and `os` modules. - Proper error handling for unknown error codes.","solution":"import errno import os def translate_error_codes(error_codes: list) -> dict: Translates a list of error codes into a dictionary mapping each error code to its corresponding (error name, error message). Args: - error_codes (list): A list of integer error codes. Returns: - dict: A dictionary where the keys are error codes and values are tuples of (error name, error message). error_dict = {} for code in error_codes: error_name = errno.errorcode.get(code, \'UNKNOWN\') error_message = os.strerror(code) error_dict[code] = (error_name, error_message) return error_dict"},{"question":"Objective: Implement a function that compresses and decompresses a large file in chunks using the `zlib` module, ensuring that it handles data streams correctly and efficiently. Problem Statement: You are required to write two functions using the `zlib` module: 1. `compress_file(input_file_path: str, output_file_path: str, chunk_size: int = 1024) -> None` This function will read a file from `input_file_path`, compress its contents in chunks of size `chunk_size`, and write the compressed data to `output_file_path`. 2. `decompress_file(input_file_path: str, output_file_path: str, chunk_size: int = 1024) -> None` This function will read a compressed file from `input_file_path`, decompress its contents in chunks of size `chunk_size`, and write the decompressed data to `output_file_path`. Function Signatures: ```python def compress_file(input_file_path: str, output_file_path: str, chunk_size: int = 1024) -> None: pass def decompress_file(input_file_path: str, output_file_path: str, chunk_size: int = 1024) -> None: pass ``` Constraints: - The `chunk_size` parameter should be a positive integer. - If the input file does not exist, raise a `FileNotFoundError`. - The functions should handle and propagate any `zlib.error` exceptions appropriately. Example Usage: ```python compress_file(\'path/to/large/input.txt\', \'path/to/compressed/output.zlib\') decompress_file(\'path/to/compressed/output.zlib\', \'path/to/decompressed/output.txt\') ``` Detailed Requirements: 1. **compress_file** - Open `input_file_path` in binary read mode. - Open `output_file_path` in binary write mode. - Create a `Compress` object using `zlib.compressobj()`. - Read the input file in chunks of size `chunk_size`. - Compress each chunk and write it to the output file. - After reading all chunks, flush the compressor and write any remaining compressed data to the output file. 2. **decompress_file** - Open `input_file_path` in binary read mode. - Open `output_file_path` in binary write mode. - Create a `Decompress` object using `zlib.decompressobj()`. - Read the compressed file in chunks of size `chunk_size`. - Decompress each chunk and write it to the output file. - After reading all chunks, flush the decompressor and write any remaining decompressed data to the output file. The implementation needs to be efficient in terms of memory usage and handle large files without loading the entire file into memory at once.","solution":"import zlib import os def compress_file(input_file_path: str, output_file_path: str, chunk_size: int = 1024) -> None: if chunk_size <= 0: raise ValueError(\\"chunk_size must be a positive integer\\") if not os.path.exists(input_file_path): raise FileNotFoundError(f\\"The file {input_file_path} does not exist.\\") compress = zlib.compressobj() with open(input_file_path, \'rb\') as infile, open(output_file_path, \'wb\') as outfile: while chunk := infile.read(chunk_size): compressed_data = compress.compress(chunk) if compressed_data: outfile.write(compressed_data) remaining_data = compress.flush() if remaining_data: outfile.write(remaining_data) def decompress_file(input_file_path: str, output_file_path: str, chunk_size: int = 1024) -> None: if chunk_size <= 0: raise ValueError(\\"chunk_size must be a positive integer\\") if not os.path.exists(input_file_path): raise FileNotFoundError(f\\"The file {input_file_path} does not exist.\\") decompress = zlib.decompressobj() with open(input_file_path, \'rb\') as infile, open(output_file_path, \'wb\') as outfile: while chunk := infile.read(chunk_size): decompressed_data = decompress.decompress(chunk) if decompressed_data: outfile.write(decompressed_data) remaining_data = decompress.flush() if remaining_data: outfile.write(remaining_data)"},{"question":"# Question: Real-World Data Loader and Statistical Analysis You are required to load a real-world dataset using the `sklearn.datasets` module and conduct some basic analysis on it. Specifically, you will be loading the **California Housing dataset**, which provides data about California districts from the 1990 U.S. Census for predicting house prices. Requirements: 1. You should load the California Housing dataset using the appropriate loader from the `sklearn.datasets` module. 2. Once you have loaded the data, split it into features (`X`) and target (`y`). 3. Perform the following analysis on the dataset: * Calculate and return the mean and standard deviation of the target values (`y`). * For the features (`X`), return the name of the feature with the highest mean value. Input: There are no explicit inputs except for the function signature: ```python def analyze_california_housing(): ``` Output: - A tuple that contains: * The mean and standard deviation of the target values (`y`). * The name of the feature with the highest mean value. - The output format should be: `(mean_target, std_target, feature_with_highest_mean)` Constraints: - You should use the `fetch_california_housing` function from the `sklearn.datasets` module to load the dataset. - You are allowed to use any other functionalities provided by the `scikit-learn` library to aid with your task. Example: Your function should return the values in a format like this (the actual numeric values may differ based on the dataset): ```python (2.068558169089026, 1.1520062782280892, \'AveRooms\') ``` Use the documentation to understand any specifics about the dataset and the functions.","solution":"from sklearn.datasets import fetch_california_housing import numpy as np def analyze_california_housing(): # Load the California Housing dataset data = fetch_california_housing() # Split the data into features (X) and target (y) X, y = data.data, data.target features = data.feature_names # Calculate mean and standard deviation of the target values (y) mean_target = np.mean(y) std_target = np.std(y) # Calculate the mean values of each feature and find the feature with the highest mean value feature_means = np.mean(X, axis=0) highest_mean_index = np.argmax(feature_means) feature_with_highest_mean = features[highest_mean_index] return (mean_target, std_target, feature_with_highest_mean)"},{"question":"**Network Server with Asyncio** **Objective**: Design and implement a simple TCP server using Python\'s asyncio library. Your server should accept multiple client connections, read data from these clients asynchronously, and echo the received data back to the respective clients. **Requirements**: 1. **Event Loop Management**: - Obtain the current or new event loop. - Handle proper start and shutdown of the event loop. 2. **Server Creation**: - Create a TCP server that listens to a specified host and port. - Accept multiple client connections. 3. **Data Handling**: - Asynchronously read data from each connected client. - Echo the received data back to the sender. 4. **Callbacks and Task Management**: - Use appropriate asyncio APIs to schedule reading and handling of data. - Ensure clean handling of client disconnections or errors. **Constraints**: - Use `asyncio` methods only; no threads or multiprocessing allowed. - Your server should handle up to 10 clients concurrently. - Implement proper error handling for network communication. **Input/Output**: - The server should be initialized with a host and port. - There is no specific input/output beyond handling network communication as described. **Example**: Here is a basic framework to get you started. You need to expand this to meet all the requirements: ```python import asyncio class EchoServerClientProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(f\\"Connection from {transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") # Echo the received data back to the client self.transport.write(data) print(f\\"Data sent: {message}\\") # Close transport after echoing the message self.transport.close() async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerClientProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server stopped manually.\\") ``` **Your Task**: - Complete the implementation above to fulfill the requirements. - Ensure that your server handles connections and data robustly. - Include appropriate error handling and event loop management as described.","solution":"import asyncio class EchoServerClientProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(f\\"Connection from {self.peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received from {self.peername}: {message}\\") # Echo the received data back to the client self.transport.write(data) print(f\\"Data sent to {self.peername}: {message}\\") def connection_lost(self, exc): print(f\\"Connection lost from {self.peername}\\") if exc: print(f\\"Error: {exc}\\") async def main(): # Obtain the current event loop loop = asyncio.get_running_loop() # Create the server server = await loop.create_server( lambda: EchoServerClientProtocol(), \'127.0.0.1\', 8888 ) # Run the server until it\'s manually stopped async with server: await server.serve_forever() if __name__ == \'__main__\': try: asyncio.run(main()) except KeyboardInterrupt: print(\\"Server stopped manually.\\")"},{"question":"**Complex Web Request Problem** You are required to implement a Python function utilizing the `urllib.request` module to perform a web request that handles authentication, cookies, and redirects as specified below. # Requirements: 1. The function should be named `fetch_web_content`. 2. It should accept the following parameters: - `url` (str): The URL of the web page to fetch. - `method` (str): HTTP method to use for the request, either \'GET\' or \'POST\'. - `data` (dict): Data to be sent with the request (optional, default is `None`). - `auth` (dict): Authentication information (optional, default is `None`), should include `username` and `password`. - `use_cookies` (bool): A flag indicating whether to handle cookies (optional, default is `False`). 3. The function should return the content of the fetched web page as a string. # Constraints: 1. The function should handle HTTP basic authentication if `auth` is provided. 2. If `data` is provided and `method` is \'POST\', it should include the data in the request body. 3. The function should handle redirects automatically and follow up to 5 redirects. 4. If `use_cookies` is `True`, the function should manage cookies throughout the session. 5. Ensure that appropriate headers such as `User-Agent` are set for the request. # Example Input: ```python url = \\"http://example.com\\" method = \\"GET\\" data = None auth = {\\"username\\": \\"user\\", \\"password\\": \\"pass\\"} use_cookies = True ``` # Example Output: ```python \\"<!DOCTYPE html> <html> ... </html>\\" ``` **Notes**: 1. You may assume that the provided URL is well-formed and reachable. 2. You can use `urllib.parse.urlencode` for encoding `data` if it is provided. # Implementation: ```python import urllib.request import urllib.parse import http.cookiejar def fetch_web_content(url, method=\'GET\', data=None, auth=None, use_cookies=False): # Implement the function as per the requirements pass # Example usage: # content = fetch_web_content(\\"http://example.com\\", method=\'GET\', auth={\\"username\\": \\"user\\", \\"password\\": \\"pass\\"}, use_cookies=True) # print(content) ``` **Your task** is to implement the `fetch_web_content` function such that it meets all the above requirements.","solution":"import urllib.request import urllib.parse import http.cookiejar from base64 import b64encode def fetch_web_content(url, method=\'GET\', data=None, auth=None, use_cookies=False): # Prepare request data if data: data = urllib.parse.urlencode(data).encode(\'utf-8\') # Create an opener with or without cookie handling if use_cookies: cookie_jar = http.cookiejar.CookieJar() opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) else: opener = urllib.request.build_opener() # Handle authentication if auth: auth_str = f\\"{auth[\'username\']}:{auth[\'password\']}\\" b64_auth_str = b64encode(auth_str.encode(\'utf-8\')).decode(\'utf-8\') opener.addheaders = [(\'Authorization\', f\'Basic {b64_auth_str}\')] # Set the User-Agent header opener.addheaders.append((\'User-Agent\', \'Mozilla/5.0\')) # Install the opener urllib.request.install_opener(opener) # Create the request if method.upper() == \'POST\': request = urllib.request.Request(url, data=data, method=\'POST\') else: request = urllib.request.Request(url, method=\'GET\') with urllib.request.urlopen(request) as response: content = response.read().decode(\'utf-8\') return content"},{"question":"# Python Signal Handling Exercise Problem Statement You are tasked with building a simplified watchdog timer system to monitor and manage a long-running task. The watchdog should be able to: 1. Start a task and set a timeout using an alarm. 2. Handle custom signals to pause, resume, and stop the task. 3. Gracefully manage KeyboardInterrupt signals (`SIGINT`), ensuring the task is properly shut down when interrupted via Ctrl+C. Requirements 1. Implement the `start_task()` function which: - Sets a signal handler for `SIGALRM` to handle a timeout for the task. - Sets a signal handler for `SIGUSR1` to pause the task. - Sets a signal handler for `SIGUSR2` to resume the task. - Sets a signal handler for `SIGINT` to safely stop the task. - Runs a loop simulating a long-running task that performs iterations, checking periodically if it should continue running. 2. Implement custom signal handlers: - `timeout_handler(signum, frame)`: Called when the task exceeds the allowed time. - `pause_handler(signum, frame)`: Pauses the task, suspending its execution until resumed. - `resume_handler(signum, frame)`: Resumes a paused task. - `interrupt_handler(signum, frame)`: Safely stops the task on an interrupt signal. 3. Test the watchdog by running `start_task()` with a 10-second timeout. You should be able to: - Pause the task using `SIGUSR1` and resume it using `SIGUSR2`. - Stop the task using `SIGINT`. - Automatically stop the task when the timeout is reached. Constraints - Use `signal` module functions for signal handling. - The task loop should be a simple counter printing its progress, pausing, resuming, and stopping based on signals. - Handle exceptions raised in signal handlers gracefully. - Ensure proper clean-up of resources before exiting the task. Example Usage ```python import signal import time import os def timeout_handler(signum, frame): print(\\"Timeout! Task taking too long.\\") raise TimeoutError(\\"Task exceeded the allowed time limit.\\") def pause_handler(signum, frame): print(\\"Pausing task... (send SIGUSR2 to resume)\\") # Implementation to pause the task def resume_handler(signum, frame): print(\\"Resuming task...\\") # Implementation to resume the task def interrupt_handler(signum, frame): print(\\"Received interrupt signal. Stopping task.\\") # Implementation to safely stop the task def start_task(): signal.signal(signal.SIGALRM, timeout_handler) signal.signal(signal.SIGUSR1, pause_handler) signal.signal(signal.SIGUSR2, resume_handler) signal.signal(signal.SIGINT, interrupt_handler) signal.alarm(10) # Set a timeout of 10 seconds for the task try: for i in range(100): print(f\\"Task running iteration {i}\\") time.sleep(1) # Simulate work by sleeping # Check for pause, resume, and stop signals finally: print(\\"Cleaning up task resources.\\") signal.alarm(0) # Disable any active alarms # Start the task if __name__ == \\"__main__\\": start_task() ``` **Expected Behavior** - The task should print its iterations every second. - When the task is paused using `SIGUSR1`, it should halt its progress. - The task should resume from where it was paused using `SIGUSR2`. - The task should be interrupted and stopped cleanly using `SIGINT`. - If the task exceeds 10 seconds, the `timeout_handler` should trigger a `TimeoutError`. Implement the functions and test the behavior by running the script and sending the necessary signals.","solution":"import signal import time import os # Global variable to control the task state task_paused = False task_running = True def timeout_handler(signum, frame): print(\\"Timeout! Task taking too long.\\") raise TimeoutError(\\"Task exceeded the allowed time limit.\\") def pause_handler(signum, frame): global task_paused task_paused = True print(\\"Pausing task... (send SIGUSR2 to resume)\\") def resume_handler(signum, frame): global task_paused task_paused = False print(\\"Resuming task...\\") def interrupt_handler(signum, frame): global task_running task_running = False print(\\"Received interrupt signal. Stopping task.\\") def start_task(): global task_paused, task_running signal.signal(signal.SIGALRM, timeout_handler) signal.signal(signal.SIGUSR1, pause_handler) signal.signal(signal.SIGUSR2, resume_handler) signal.signal(signal.SIGINT, interrupt_handler) signal.alarm(10) # Set a timeout of 10 seconds for the task try: for i in range(100): if not task_running: break while task_paused: time.sleep(0.1) # Continue checking if the task should resume print(f\\"Task running iteration {i}\\") time.sleep(1) # Simulate work by sleeping except TimeoutError: print(\\"Task exceeded the allowed time limit and was stopped.\\") finally: print(\\"Cleaning up task resources.\\") signal.alarm(0) # Disable any active alarms # Start the task if __name__ == \\"__main__\\": start_task()"},{"question":"**Background:** You are given a directory containing various files with different extensions. You need to write a Python program that performs specific filtering operations based on Unix shell-style wildcard patterns. **Task:** Write a function `filter_files(directory: str, patterns: List[str]) -> Dict[str, List[str]]` that takes two arguments: 1. `directory` (str): The path to the directory containing the files you want to filter. 2. `patterns` (List[str]): A list of shell-style wildcard patterns. The function should return a dictionary where the keys are the patterns and the values are lists of filenames in the directory that match each pattern. **Input:** - `directory`: A string representing the directory path. - `patterns`: A list of strings, where each string is a shell-style wildcard pattern (e.g., `[\'*.txt\', \'*.py\', \'file?.md\']`). **Output:** - A dictionary where each key is a pattern from the input list, and the corresponding value is a list of filenames in the directory that match that pattern. **Example:** ```python result = filter_files(\'/path/to/directory\', [\'*.txt\', \'*.md\', \'file?.py\']) print(result) # Expected output: # { # \'*.txt\': [\'readme.txt\', \'report.txt\'], # \'*.md\': [\'README.md\', \'documentation.md\'], # \'file?.py\': [\'file1.py\', \'file2.py\'] # } ``` **Constraints:** - Assume that the `directory` path is valid and exists. - Do not account for nested directories; only consider files in the provided directory. - Your solution should handle a reasonable number of files efficiently (up to 10,000 files). **Hints:** - Utilize the `os` module to list the files in the directory. - Use the `fnmatch` module to filter the filenames based on the provided patterns. ```python import os import fnmatch def filter_files(directory: str, patterns: List[str]) -> Dict[str, List[str]]: # Your implementation here pass ``` **Performance Requirements:** - The function should have a time complexity that is linear with respect to the number of files and patterns being checked. - The function should handle edge cases, such as empty directories and patterns that match no files.","solution":"import os import fnmatch from typing import List, Dict def filter_files(directory: str, patterns: List[str]) -> Dict[str, List[str]]: Filter the files in the given directory based on the provided shell-style wildcard patterns. Parameters: directory (str): The path to the directory containing the files to be filtered. patterns (List[str]): A list of shell-style wildcard patterns. Returns: Dict[str, List[str]]: A dictionary where each key is a pattern and the value is a list of filenames matching that pattern. # List all files in the directory files = os.listdir(directory) # Initialize the result dictionary result = {pattern: [] for pattern in patterns} # Filter files based on each pattern for pattern in patterns: result[pattern] = fnmatch.filter(files, pattern) return result"},{"question":"**Scenario:** You are working on a project that involves handling user data and processing it with an external API service. The API service is responsible for fetching user data and processing user orders. Your task is to create unit tests for a function `process_user_orders` that performs the following tasks: 1. Fetches user data using an external API service. 2. Processes a list of user orders by calling another method from the same API service. The API service is represented by the class `APIClient`, which has the methods `fetch_user_data(user_id)` and `process_order(order_id)`. The `process_user_orders(user_id)` function is defined as follows: ```python class APIClient: def fetch_user_data(self, user_id): # Simulate fetching user data from an external service pass def process_order(self, order_id): # Simulate processing an order through an external service pass def process_user_orders(api_client, user_id): # Fetch user data user_data = api_client.fetch_user_data(user_id) # Process each order for the user for order in user_data[\'orders\']: api_client.process_order(order[\'order_id\']) return len(user_data[\'orders\']) ``` # Requirements: 1. Write a unit test for the function `process_user_orders` using `unittest.mock` to: - Mock the `APIClient` class and its methods `fetch_user_data` and `process_order`. - Ensure that `fetch_user_data` is called with the correct `user_id`. - Ensure that `process_order` is called the correct number of times, and with the correct `order_id` values. - Verify the return value of the function based on the number of orders processed. 2. Use a test framework of your choice (e.g., `unittest`) and include assertions to confirm the behavior of the function under test. # Constraints: - You are not allowed to change the implementation of `process_user_orders`. - Assume the user data returned by `fetch_user_data` includes an `orders` key, which is a list of order dictionaries with an `order_id`. # Example Test: Write a test case with a mocked user data response like: ```python mock_user_data = { \'user_id\': \'123\', \'orders\': [ {\'order_id\': \'order1\'}, {\'order_id\': \'order2\'} ] } ``` # Expected Test Implementation: Provide a runnable code block with the necessary imports and the test defined: ```python import unittest from unittest.mock import MagicMock class TestProcessUserOrders(unittest.TestCase): def test_process_user_orders(self): # Create a mock APIClient mock_api_client = MagicMock() # Set up the mock return value for fetch_user_data mock_user_data = { \'user_id\': \'123\', \'orders\': [ {\'order_id\': \'order1\'}, {\'order_id\': \'order2\'} ] } mock_api_client.fetch_user_data.return_value = mock_user_data # Call the function with the mock APIClient result = process_user_orders(mock_api_client, \'123\') # Assert fetch_user_data was called with the correct user_id mock_api_client.fetch_user_data.assert_called_once_with(\'123\') # Assert process_order was called with the correct order_ids mock_api_client.process_order.assert_any_call(\'order1\') mock_api_client.process_order.assert_any_call(\'order2\') # Assert process_order was called the correct number of times self.assertEqual(mock_api_client.process_order.call_count, 2) # Assert the function return value is correct self.assertEqual(result, 2) if __name__ == \'__main__\': unittest.main() ``` Make sure that your solution is complete and self-contained, including necessary imports and setup. Your test should accurately reflect the operation of the `process_user_orders` function and use `unittest.mock` effectively.","solution":"class APIClient: def fetch_user_data(self, user_id): # Simulate fetching user data from an external service pass def process_order(self, order_id): # Simulate processing an order through an external service pass def process_user_orders(api_client, user_id): # Fetch user data user_data = api_client.fetch_user_data(user_id) # Process each order for the user for order in user_data[\'orders\']: api_client.process_order(order[\'order_id\']) return len(user_data[\'orders\'])"},{"question":"# Advanced Python Coding Assessment: Working with the Random Module You have been given the task to simulate a simplistic version of a dice-based board game. In this game, each player rolls two six-sided dice per turn and moves forward by the sum of the dice values. The first player to reach or exceed a specified score wins. Your task is to implement a simulation of this game using Python\'s `random` module. # Requirements 1. Implement a function `roll_dice()` that simulates rolling two six-sided dice and returns their sum. 2. Implement a function `play_game(target_score: int, num_players: int) -> int` that simulates the game: - Each player takes turns rolling the dice until one of them reaches or exceeds the target score. - The function should return the 1-based index of the winning player. 3. Ensure the game handles edge cases such as a very low target score (e.g., target_score = 1) or just one player. # Constraints - Use the `random.randint()` function to simulate dice rolls. - The number of players (`num_players`) will be at least 1 and at most 10. - The target score (`target_score`) will be at least 1 and at most 100. # Performance Requirements - The solution should aim to simulate the game efficiently even with the maximum constraint values. # Function Signatures ```python import random def roll_dice() -> int: Simulate rolling two six-sided dice and return the sum of the rolls. Returns: int: The sum of the two dice rolls. pass def play_game(target_score: int, num_players: int) -> int: Simulate a dice game where players roll dice to reach a target score. Args: target_score (int): The score that a player needs to reach or exceed to win. num_players (int): The number of players in the game. Returns: int: The 1-based index of the winning player. pass ``` # Example Usage ```python # Example usage of the functions if __name__ == \\"__main__\\": random.seed(42) # Set the seed for reproducibility # Rolling the dice print(f\\"Rolling the dice: {roll_dice()}\\") # Output will vary because of randomness # Playing the game with a target score of 15 and 3 players winner = play_game(target_score=15, num_players=3) print(f\\"The winner is player {winner}\\") ```","solution":"import random def roll_dice() -> int: Simulate rolling two six-sided dice and return the sum of the rolls. Returns: int: The sum of the two dice rolls. dice1 = random.randint(1, 6) dice2 = random.randint(1, 6) return dice1 + dice2 def play_game(target_score: int, num_players: int) -> int: Simulate a dice game where players roll dice to reach a target score. Args: target_score (int): The score that a player needs to reach or exceed to win. num_players (int): The number of players in the game. Returns: int: The 1-based index of the winning player. scores = [0] * num_players while True: for player in range(num_players): roll = roll_dice() scores[player] += roll if scores[player] >= target_score: return player + 1"},{"question":"**Objective**: Implement a function that performs several tensor operations while ensuring that tensor names are correctly aligned, propagated, and checked according to given rules. # Function Definition ```python def process_named_tensors(tensor_a, tensor_b): Perform a series of operations on two input tensors `tensor_a` and `tensor_b` and return the resulting tensor. Args: - tensor_a (torch.Tensor): A named tensor to be processed. - tensor_b (torch.Tensor): A named tensor to be processed alongside `tensor_a`. Returns: - torch.Tensor: The processed tensor after a series of operations. The operations to perform: 1. Multiply `tensor_a` by `tensor_b` element-wise. 2. Find the sum of the resulting tensor along a specified dimension. 3. Transpose the resulting tensor, interchanging two specified dimensions. 4. Return the final tensor after these operations. Ensure that tensor names are preserved and properly aligned through the operations. # Your implementation here ``` # Input * `tensor_a`: A 2-dimensional named tensor, e.g., `tensor_a.names` could be `(\'N\', \'C\')`. * `tensor_b`: Another 2-dimensional named tensor, e.g., `tensor_b.names` could be `(\'N\', \'C\')`. # Output * A tensor after performing the specified operations. Ensure that the names of dimensions are preserved and properly aligned through each operation. # Constraints * Input tensors will always be 2-dimensional and properly named. * Both tensors will have matching names for multiplication operations to avoid any mismatches. * Specify an appropriate dimension for summation and transpose only existing named dimensions. # Additional Information * Use `torch.mul` for element-wise multiplication. * Use `torch.sum`, specifying the `dim` for summation. * Use `torch.transpose`, specifying the dimensions to interchange. # Examples ```python # Example Input tensor_a = torch.tensor([[1, 2], [3, 4]], names=[\'N\', \'C\']) tensor_b = torch.tensor([[5, 6], [7, 8]], names=[\'N\', \'C\']) # Example Output result_tensor = process_named_tensors(tensor_a, tensor_b) # The resulting tensor should have the appropriate names and values # Carrying out the operations: # Element-wise multiplication: # [[ 1*5, 2*6 ], [ 3*7, 4*8 ]] -> [[ 5, 12], [21, 32]] # Summing along dimension \'N\': # [ 26, 44 ] # Transposing, interchanging \'N\' and \'C\': # [ 26, 44 ].transpose(\'N\', \'C\') # So, the result tensor should maintain the dimension names appropriately. ``` Implement the function `process_named_tensors` ensuring that it adheres to the rules outlined above for named tensors in PyTorch.","solution":"import torch def process_named_tensors(tensor_a, tensor_b): Perform a series of operations on two input tensors `tensor_a` and `tensor_b` and return the resulting tensor. Args: - tensor_a (torch.Tensor): A named tensor to be processed. - tensor_b (torch.Tensor): A named tensor to be processed alongside `tensor_a`. Returns: - torch.Tensor: The processed tensor after a series of operations. # Element-wise multiplication result = torch.mul(tensor_a, tensor_b) # Summing along dimension \'N\' result_sum = torch.sum(result, dim=\'N\') # Transposing, interchanging \'N\' and \'C\' final_result = result_sum.align_to(\'C\') return final_result"},{"question":"**Objective:** You are given a dataset with sales data, and you need to perform some analysis on it. After performing the analysis, you should verify the correctness of your results using pandas assertion functions. **Question:** You are given a list of dictionaries where each dictionary contains details of a sale: - \'date\': The date of the sale (string in `YYYY-MM-DD` format) - \'product\': The name of the product sold (string) - \'quantity\': The quantity of the product sold (integer) - \'price_per_unit\': The price per unit of the product (float) Write a function `analyze_sales(data)` that performs the following tasks: 1. Converts the input list of dictionaries into a pandas DataFrame. 2. Adds a new column `total_price` which is the product of `quantity` and `price_per_unit`. 3. Groups the DataFrame by \'product\' and calculates the total quantity sold and the total sales (sum of `total_price`) for each product. The result should be a DataFrame with \'product\' as the index, and columns `total_quantity` and `total_sales`. 4. Returns the resulting grouped DataFrame. **Input:** - `data`: A list of dictionaries, where each dictionary contains details of a sale as described above. **Output:** - A pandas DataFrame indexed by \'product\' with columns `total_quantity` and `total_sales`. **Constraints:** - Assume all dates are valid and in the correct format. - The input list is not empty and contains at least one sale record. **Example:** ```python data = [ {\'date\': \'2023-01-01\', \'product\': \'Widget\', \'quantity\': 10, \'price_per_unit\': 2.5}, {\'date\': \'2023-01-02\', \'product\': \'Widget\', \'quantity\': 5, \'price_per_unit\': 2.5}, {\'date\': \'2023-01-01\', \'product\': \'Gadget\', \'quantity\': 2, \'price_per_unit\': 3.0}, {\'date\': \'2023-01-03\', \'product\': \'Gadget\', \'quantity\': 3, \'price_per_unit\': 3.0}, ] result = analyze_sales(data) print(result) ``` Expected Output: ``` total_quantity total_sales product Gadget 5 15.0 Widget 15 37.5 ``` **Testing:** Write test cases to verify the correctness of your implementation. Use pandas assertion functions (`testing.assert_frame_equal`) to ensure your results match the expected output. ```python import pandas as pd from pandas import testing as tm def test_analyze_sales(): data = [ {\'date\': \'2023-01-01\', \'product\': \'Widget\', \'quantity\': 10, \'price_per_unit\': 2.5}, {\'date\': \'2023-01-02\', \'product\': \'Widget\', \'quantity\': 5, \'price_per_unit\': 2.5}, {\'date\': \'2023-01-01\', \'product\': \'Gadget\', \'quantity\': 2, \'price_per_unit\': 3.0}, {\'date\': \'2023-01-03\', \'product\': \'Gadget\', \'quantity\': 3, \'price_per_unit\': 3.0}, ] expected_result = pd.DataFrame({ \'total_quantity\': [5, 15], \'total_sales\': [15.0, 37.5] }, index=pd.Index([\'Gadget\', \'Widget\'], name=\'product\')) result = analyze_sales(data) tm.assert_frame_equal(result, expected_result) test_analyze_sales() ``` Write the function `analyze_sales(data)` and the test function as described.","solution":"import pandas as pd def analyze_sales(data): Analyzes a list of sales data and returns a DataFrame with the total quantity and sales per product. Parameters: data (list of dict): List of dictionaries where each dictionary contains details of a sale. Returns: pd.DataFrame: DataFrame indexed by \'product\' with columns \'total_quantity\' and \'total_sales\'. # Convert list of dictionaries to DataFrame df = pd.DataFrame(data) # Add \'total_price\' column df[\'total_price\'] = df[\'quantity\'] * df[\'price_per_unit\'] # Group by \'product\' and aggregate required values grouped_df = df.groupby(\'product\').agg( total_quantity=pd.NamedAgg(column=\'quantity\', aggfunc=\'sum\'), total_sales=pd.NamedAgg(column=\'total_price\', aggfunc=\'sum\') ) return grouped_df"},{"question":"Coding Assessment Question: # Objective Design a function that sets a seaborn style and creates a bar plot and a line plot with provided data. # Description: You are tasked with creating a function `create_plots` that takes the following inputs: 1. `bar_data` (a dictionary with two keys `x` and `y`, each associated with a list of values for the bar plot) 2. `line_data` (a dictionary with two keys `x` and `y`, each associated with a list of values for the line plot) 3. `style` (a string that represents the seaborn style to be used for both plots; default is `\\"whitegrid\\"`) 4. `override_params` (an optional dictionary for overriding seaborn\'s default parameter values; default is `None`) The function should set the seaborn style, apply any parameter overrides, create a bar plot with the provided `bar_data`, and create a line plot with the provided `line_data`. # Input: - `bar_data` (dictionary): A dictionary with keys `x` and `y`. - Example: `{\\"x\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y\\": [1, 3, 2]}` - `line_data` (dictionary): A dictionary with keys `x` and `y`. - Example: `{\\"x\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y\\": [4, 2, 5]}` - `style` (string): A seaborn style string. Default value is `\\"whitegrid\\"`. - Example: `\\"darkgrid\\"` - `override_params` (dictionary): An optional dictionary for overriding seaborn\'s parameter values. Default value is `None`. - Example: `{\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}` # Output: The function should not return any value. It should display the bar plot and the line plot. # Constraints: - The keys `x` and `y` in `bar_data` and `line_data` must have lists of equal length. - The lengths of the lists in `bar_data` and `line_data` should be the same for corresponding `x` and `y` values. # Example Usage: ```python bar_data = {\\"x\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y\\": [1, 3, 2]} line_data = {\\"x\\": [\\"A\\", \\"B\\", \\"C\\"], \\"y\\": [4, 2, 5]} style = \\"darkgrid\\" override_params = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} create_plots(bar_data, line_data, style, override_params) ``` This should result in displaying a bar plot and a line plot with the specified style and overridden parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(bar_data, line_data, style=\\"whitegrid\\", override_params=None): Sets the seaborn style, creates a bar plot and a line plot with provided data. Parameters: - bar_data (dict): A dictionary with \'x\' and \'y\' keys for the bar plot. - line_data (dict): A dictionary with \'x\' and \'y\' keys for the line plot. - style (str): A string representing the seaborn style. Default is \\"whitegrid\\". - override_params (dict): A dictionary for overriding seaborn\'s default parameters. Default is None. # Set the Seaborn style sns.set_style(style) # Apply any override parameters if provided if override_params: sns.set_context(rc=override_params) # Create the bar plot plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) sns.barplot(x=bar_data[\'x\'], y=bar_data[\'y\']) plt.title(\'Bar Plot\') # Create the line plot plt.subplot(1, 2, 2) sns.lineplot(x=line_data[\'x\'], y=line_data[\'y\']) plt.title(\'Line Plot\') # Show the plots plt.tight_layout() plt.show()"},{"question":"# Coding Assessment: Working with Mailboxes using the `mailbox` module **Objective**: Demonstrate your ability to utilize the `mailbox` module to manage messages within a mailbox in Python 3.10. **Task**: Write a Python program that performs the following operations on an mbox mailbox: 1. **Loading the Mailbox**: - Load an existing mbox mailbox file. - If the specified mailbox file does not exist, create a new mbox mailbox. 2. **Adding a New Message**: - Add a new message to the mailbox. The message should be constructed from a provided string. 3. **Iterating and Printing Messages**: - Iterate over all messages in the mailbox and print the subject of each message. - If a message has no subject, print `No Subject`. 4. **Removing a Message**: - Remove the first message found with a subject containing a provided keyword. 5. **Closing the Mailbox**: - Ensure to flush any changes and close the mailbox properly. # Input and Output Formats **Input**: - A filesystem path to the mbox mailbox file. - A string representing the new message content which should be RFC 2822 compliant. - A string representing the keyword for message removal from subjects. **Output**: - Subjects of all messages in the mailbox after performing the addition of the new message. - A statement indicating if a message has been removed, `Message with subject containing \'<keyword>\' removed.` otherwise `No message with subject containing \'<keyword>\' found.`. **Constraints**: - Handle exceptions where the mailbox cannot be read or written properly. - Safeguard against concurrent access issues using appropriate locking mechanisms provided by `mailbox.mbox`. # Example ```python import mailbox from email.message import EmailMessage def load_or_create_mailbox(path): return mailbox.mbox(path, create=True) def add_message(mbox, message_str): message = EmailMessage() message.set_content(message_str) mbox.add(message) def iterate_and_print_subjects(mbox): for message in mbox: subject = message[\'subject\'] print(subject if subject else \'No Subject\') def remove_message_by_keyword(mbox, keyword): for key, message in mbox.iteritems(): subject = message[\'subject\'] if subject and keyword in subject: mbox.remove(key) print(f\\"Message with subject containing \'{keyword}\' removed.\\") return print(f\\"No message with subject containing \'{keyword}\' found.\\") def close_mailbox(mbox): mbox.flush() mbox.close() if __name__ == \\"__main__\\": mailbox_path = \'path/to/mbox\' new_message_str = \'Subject: TestnnThis is the body of the test email.\' keyword = \'Test\' mbox = load_or_create_mailbox(mailbox_path) add_message(mbox, new_message_str) iterate_and_print_subjects(mbox) remove_message_by_keyword(mbox, keyword) close_mailbox(mbox) ``` # Requirements: 1. Implement the helper functions: `load_or_create_mailbox`, `add_message`, `iterate_and_print_subjects`, `remove_message_by_keyword`, and `close_mailbox`. 2. Handle any exceptions that could arise during mailbox operations. 3. Ensure proper use of locking mechanisms to avoid data corruption during concurrent access.","solution":"import mailbox from email.message import EmailMessage from pathlib import Path def load_or_create_mailbox(path): Load an existing mbox mailbox file or create a new one if it doesn\'t exist. return mailbox.mbox(path, create=True) def add_message(mbox, message_str): Add a new message to the mbox mailbox. message = EmailMessage() message.set_content(message_str) # Extract subject if present in message string, otherwise set \'No Subject\' for line in message_str.split(\\"n\\"): if line.startswith(\\"Subject: \\"): subject = line[len(\\"Subject: \\"):] message[\'subject\'] = subject break mbox.lock() try: mbox.add(message) finally: mbox.unlock() def iterate_and_print_subjects(mbox): Iterate over all messages in the mailbox and print the subject. for message in mbox: subject = message[\'subject\'] print(subject if subject else \'No Subject\') def remove_message_by_keyword(mbox, keyword): Remove the first message found with a subject containing the provided keyword. mbox.lock() try: for key, message in mbox.iteritems(): subject = message[\'subject\'] if subject and keyword in subject: mbox.remove(key) print(f\\"Message with subject containing \'{keyword}\' removed.\\") mbox.flush() return print(f\\"No message with subject containing \'{keyword}\' found.\\") finally: mbox.unlock() def close_mailbox(mbox): Ensure to flush any changes and close the mailbox properly. mbox.flush() mbox.close()"},{"question":"# Advanced Coding Assessment: Custom Descriptors in Python Objective: To gauge your understanding of descriptors and their application in object-oriented programming in Python. Problem Statement: You are to implement a set of custom descriptors and use them within a class to manage attributes. This challenge involves creating two custom descriptors: `PositiveInteger` and `NonEmptyString`. These descriptors will enforce specific constraints on the attributes they are associated with. 1. **PositiveInteger Descriptor:** - Ensures that the attribute value is a positive integer. - Raises a `ValueError` if a non-positive integer is assigned. 2. **NonEmptyString Descriptor:** - Ensures that the attribute value is a non-empty string. - Raises a `ValueError` if an empty string or non-string value is assigned. Class Structure: Create a class named `Person` that utilizes these descriptors for its attributes: - `age`: Must use the `PositiveInteger` descriptor. - `name`: Must use the `NonEmptyString` descriptor. Implementation Details: 1. Define the `PositiveInteger` descriptor class. 2. Define the `NonEmptyString` descriptor class. 3. Define the `Person` class using these descriptors. 4. The `Person` class should also have a constructor to initialize the attributes `age` and `name`. Input & Output: - There are no specific input/output functions required. - The main implementation should be thoroughly tested using assertions to validate correct behavior. Example Usage: ```python # Example demonstrating valid and invalid usages try: p = Person(25, \\"Alice\\") # Should succeed p.age = 30 # Should succeed p.name = \\"Bob\\" # Should succeed p.age = -5 # Should raise a ValueError except ValueError as e: print(e) try: p.name = \\"\\" # Should raise a ValueError except ValueError as e: print(e) ``` Additional Constraints: - You must implement the descriptor logic in the `PositiveInteger` and `NonEmptyString` classes. - You may not use built-in descriptors such as `property` for this task. - Ensure your solution is efficient and leverages object-oriented principles.","solution":"class PositiveInteger: def __init__(self): self.__name = None def __set_name__(self, owner, name): self.__name = name def __get__(self, instance, owner): return instance.__dict__[self.__name] def __set__(self, instance, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\\"The {self.__name} attribute must be a positive integer.\\") instance.__dict__[self.__name] = value class NonEmptyString: def __init__(self): self.__name = None def __set_name__(self, owner, name): self.__name = name def __get__(self, instance, owner): return instance.__dict__[self.__name] def __set__(self, instance, value): if not isinstance(value, str) or not value: raise ValueError(f\\"The {self.__name} attribute must be a non-empty string.\\") instance.__dict__[self.__name] = value class Person: age = PositiveInteger() name = NonEmptyString() def __init__(self, age, name): self.age = age self.name = name # Example usage demonstrating valid and invalid usages try: p = Person(25, \\"Alice\\") # Should succeed p.age = 30 # Should succeed p.name = \\"Bob\\" # Should succeed p.age = -5 # Should raise a ValueError except ValueError as e: print(e) try: p.name = \\"\\" # Should raise a ValueError except ValueError as e: print(e)"},{"question":"# Question: Python Abstract Syntax Tree Manipulation Objective Write a Python function that takes a piece of Python code as a string and performs the following operations using the `ast` module: 1. Parse the string to generate its Abstract Syntax Tree (AST). 2. Traverse the AST to collect all function names and the number of arguments each function takes. 3. Return a dictionary where the keys are the function names and the values are the counts of arguments. Input - A string representing Python code. Output - A dictionary where the keys are function names and the values are the number of arguments each function takes. Constraints - The input string will not exceed 1000 characters. - The input will contain valid Python code. - Function names will be unique within the input code for simplification. Example ```python code = \'\'\' def foo(x, y): return x + y def bar(a): return a * 2 def baz(): print(\\"Hello World\\") \'\'\' result = analyze_functions(code) print(result) ``` Expected Output: ```python { \\"foo\\": 2, \\"bar\\": 1, \\"baz\\": 0 } ``` Notes - You may assume all functions are defined at the top level and not nested inside other functions or classes. - Function definitions will not contain variable-length arguments (`*args` or `**kwargs`). Code Template ```python import ast def analyze_functions(code: str) -> dict: Analyzes a Python code string to extract function names and their argument counts. Parameters: code (str): A string representing Python code. Returns: dict: A dictionary where the keys are function names and the values are the number of arguments. # Your code here # Example usage code = \'\'\' def foo(x, y): return x + y def bar(a): return a * 2 def baz(): print(\\"Hello World\\") \'\'\' result = analyze_functions(code) print(result) ```","solution":"import ast def analyze_functions(code: str) -> dict: Analyzes a Python code string to extract function names and their argument counts. Parameters: code (str): A string representing Python code. Returns: dict: A dictionary where the keys are function names and the values are the number of arguments. tree = ast.parse(code) functions = {} for node in ast.walk(tree): if isinstance(node, ast.FunctionDef): functions[node.name] = len(node.args.args) return functions # Example usage code = \'\'\' def foo(x, y): return x + y def bar(a): return a * 2 def baz(): print(\\"Hello World\\") \'\'\' result = analyze_functions(code) print(result)"},{"question":"<|Analysis Begin|> The provided documentation describes the `torch.Size` class in PyTorch, which is used to handle the sizes of tensors. The `torch.Size` object is what you obtain when you call the `.size()` method on a tensor. Since `torch.Size` is a subclass of Python\'s built-in `tuple`, it supports typical sequence operations such as indexing and measuring its length. Key points covered in the documentation: 1. `torch.Size` represents the size of all dimensions of a tensor. 2. It can be used like a tuple, supporting indexing and length calculation. Based on this information, we can craft a question that requires students to work with tensor sizes, creating tensors with specific dimensions, and performing operations that involve checking and manipulating their sizes. <|Analysis End|> <|Question Begin|> # Question: Manipulating PyTorch Tensor Sizes You are required to implement a function, `reshape_tensor`, that takes a tensor and a target size as input. The function should attempt to reshape the input tensor to the target size. If the reshaping is possible, it should return the reshaped tensor. If not, it should raise an appropriate error. # Function Signature ```python import torch def reshape_tensor(input_tensor: torch.Tensor, target_size: torch.Size) -> torch.Tensor: pass ``` # Input - `input_tensor` (torch.Tensor): A tensor to be reshaped. - `target_size` (torch.Size): The target size to reshape the tensor to. # Output - Returns a PyTorch tensor of size `target_size` if the reshaping is feasible. # Behavior - If `input_tensor` can be reshaped to `target_size`, the function returns the reshaped tensor. - Otherwise, the function raises a `ValueError` with the message \\"Reshaping not possible.\\" # Example ```python import torch x = torch.ones(10, 2) target_size = torch.Size([5, 4]) try: reshaped_tensor = reshape_tensor(x, target_size) print(\\"Reshaped Tensor:\\", reshaped_tensor) except ValueError as e: print(e) # Example Output: # Reshaping not possible ``` # Constraints - You should use PyTorch\'s built-in operations to check and perform the reshaping. - Assume that `input_tensor` is always a valid PyTorch tensor and `target_size` is always a valid torch.Size object. # Hints - Remember that for reshaping to be possible, the total number of elements must remain constant. - You can use methods available on the `torch.Tensor` to perform these checks and operations. # Performance Requirements - Ensure that your solution handles tensors of various dimensions efficiently.","solution":"import torch def reshape_tensor(input_tensor: torch.Tensor, target_size: torch.Size) -> torch.Tensor: Reshapes the input_tensor to the given target_size. Parameters: input_tensor (torch.Tensor): The tensor to be reshaped. target_size (torch.Size): The desired shape for the tensor. Returns: torch.Tensor: The reshaped tensor. Raises: ValueError: If the tensor cannot be reshaped to the specified size. if input_tensor.numel() == torch.prod(torch.tensor(target_size)): return input_tensor.view(target_size) else: raise ValueError(\\"Reshaping not possible\\")"},{"question":"# PyTorch FX Graph Transformation Challenge Objective Your task is to implement a custom PyTorch FX transformer that modifies a given FX GraphModule by performing the following operations: 1. Replace all instances of the `torch.ops.aten.add.Tensor` operation with the sequence of operations: `mul` followed by `sub`. 2. Insert a `relu` operation after every `mul` operation. Requirements: 1. Implement a class `CustomGraphTransformer` inheriting from `torch.fx.Transformer`. 2. Method `call_function` should handle the following transformations: - If an `add` operation is encountered, replace it with the operations: ```python mul_res = x * y sub_res = mul_res - y ``` where `x` and `y` are the operands of the original `add` operation. - After inserting the `mul` operation, insert a `relu` operation which takes the result of `mul` as its input. 3. Add a separate method to apply this transformation on a given `GraphModule` and return the updated module. Input and Output Format - **Input:** An instance of `torch.fx.GraphModule`. - The graph might contain calls to `torch.ops.aten.add.Tensor`. - **Output:** A transformed `torch.fx.GraphModule` where all `add` operations are replaced as specified and `relu` is inserted after each `mul`. Example: For a given function represented as FX Graph: ```python def forward(x, y): return x + y ``` After applying the transformation: ```python def forward(x, y): mul_res = x * y relu_res = torch.relu(mul_res) return relu_res - y ``` Constraints: - Ensure that the resulting graph preserves the input/output structure of the original graph. - You may assume that the input graph will only contain operations supported by PyTorch\'s ATen. Implementation Notes: - Utilize `super().call_function` to handle the cases where the operation is not an `add`. - Use `torch.fx.Graph` utility functions for node manipulation as needed. ```python import torch from torch.fx import GraphModule, Transformer class CustomGraphTransformer(Transformer): def call_function(self, target, args, kwargs): if target != torch.ops.aten.add.Tensor: return super().call_function(target, args, kwargs) x, y = args # Replace add with mul and then sub mul_res = super().call_function(torch.ops.aten.mul.Tensor, (x, y), {}) relu_res = super().call_function(torch.ops.aten.relu.default, (mul_res,), {}) return super().call_function(torch.ops.aten.sub.Tensor, (relu_res, y), {}) def apply_custom_transformation(graph_module: GraphModule) -> GraphModule: transformer = CustomGraphTransformer(graph_module) return transformer.transform() # Example usage # You can create a graph module and apply the transformation to test your implementation ``` # Task - Understand the provided example and implement the `CustomGraphTransformer` class correctly. - Implement the method `apply_custom_transformation` which takes a `GraphModule` as input and returns it after applying the transformation.","solution":"import torch from torch.fx import GraphModule, Interpreter import torch.fx as fx class CustomGraphTransformer(fx.Transformer): def call_function(self, target, args, kwargs): if target != torch.ops.aten.add.Tensor: return super().call_function(target, args, kwargs) x, y = args # Replace add with mul and then sub mul_res = super().call_function(torch.ops.aten.mul.Tensor, (x, y), {}) relu_res = super().call_function(torch.ops.aten.relu.default, (mul_res,), {}) return super().call_function(torch.ops.aten.sub.Tensor, (relu_res, y), {}) def apply_custom_transformation(graph_module: GraphModule) -> GraphModule: transformer = CustomGraphTransformer(graph_module) return transformer.transform()"},{"question":"**Objective:** To assess your understanding of Python\'s cryptographic services, specifically the `hashlib`, `hmac`, and `secrets` modules. **Task:** You are required to implement a secure authentication mechanism using the following components: 1. A function to hash a password using the `hashlib` module. 2. A function to verify the password using the `hmac` module to ensure the hashed password matches the stored hash. 3. A function to generate and verify a secure token using the `secrets` module for session management. # Requirements: 1. **Password Hashing Function** - **Function Name:** `hash_password` - **Input:** A string `password`. - **Output:** A tuple containing the hashed password and the salt used for hashing. - **Details:** Use the `hashlib` module to create a SHA-256 hash of the password, incorporating a random salt generated using `os.urandom`. 2. **Password Verification Function** - **Function Name:** `verify_password` - **Input:** Original hashed password, original salt, and a string password to verify. - **Output:** Boolean value `True` if the password matches, otherwise `False`. - **Details:** Use the `hmac` module to verify that the provided password matches the original hashed password. 3. **Token Generation Function** - **Function Name:** `generate_token` - **Input:** Integer `length` specifying the number of bytes for the token. - **Output:** A securely generated token as a hex string. - **Details:** Use the `secrets` module to generate a secure token. 4. **Token Verification Function** - **Function Name:** `verify_token` - **Input:** Original token and a token to verify. - **Output:** Boolean value `True` if the tokens match, otherwise `False`. - **Details:** Use the `secrets` module to safely compare the tokens. # Constraints: - The password must be at least 8 characters long. - The token length should be at least 16 bytes. - Performance must be considered; ensure your functions are efficient and secure against common attacks. # Example Usage: ```python # Hashing and verifying a password hashed_pwd, salt = hash_password(\\"securepassword123\\") assert verify_password(hashed_pwd, salt, \\"securepassword123\\") == True assert verify_password(hashed_pwd, salt, \\"wrongpassword\\") == False # Generating and verifying a token token = generate_token(16) assert verify_token(token, token) == True assert verify_token(token, \\"differenttoken\\") == False ``` Implement all four functions as part of your solution.","solution":"import hashlib import hmac import os import secrets def hash_password(password): Hashes a password using SHA-256 with a random salt. Args: - password (str): The password to hash. Returns: - tuple: A tuple containing the hashed password and the salt used. if len(password) < 8: raise ValueError(\\"Password must be at least 8 characters long.\\") salt = os.urandom(16) # Generate a random 16-byte salt hash_obj = hashlib.sha256() hash_obj.update(salt + password.encode(\'utf-8\')) hashed_password = hash_obj.hexdigest() return hashed_password, salt def verify_password(hashed_password, salt, password): Verifies a password against a stored hashed password and salt using HMAC for comparison. Args: - hashed_password (str): The original hashed password. - salt (bytes): The salt used in the original hash. - password (str): The password to verify. Returns: - bool: True if the password matches, otherwise False. hash_obj = hashlib.sha256() hash_obj.update(salt + password.encode(\'utf-8\')) expected_hashed_password = hash_obj.hexdigest() return hmac.compare_digest(expected_hashed_password, hashed_password) def generate_token(length): Generates a secure token. Args: - length (int): The number of bytes for the token. Returns: - str: A securely generated token as a hex string. if length < 16: raise ValueError(\\"Token length must be at least 16 bytes.\\") return secrets.token_hex(length) def verify_token(original_token, token_to_verify): Verifies that two tokens match using a constant-time comparison. Args: - original_token (str): The original token. - token_to_verify (str): The token to verify. Returns: - bool: True if the tokens match, otherwise False. return secrets.compare_digest(original_token, token_to_verify)"},{"question":"# Python Coding Assessment Question Objective Using the Python `time` module, implement a function that: 1. Converts a given UTC time string to local time in a specified timezone. 2. Computes the time taken for the conversion process using high-resolution performance counters. 3. Returns both the converted local time and the duration of the conversion process. Function Signature ```python def convert_utc_to_local(utc_time: str, time_zone: str) -> tuple: pass ``` Input - `utc_time` (str): A string representing a time in UTC. The format of the string is `\'%Y-%m-%d %H:%M:%S\'`. - `time_zone` (str): A string representing the desired local timezone. Example values include `US/Eastern`, `Europe/London`, `Australia/Sydney`, etc. Output - Returns a tuple: - The first element is the local time as a string in the same format as `\'%Y-%m-%d %H:%M:%S\'`. - The second element is the duration of the conversion process in nanoseconds. Constraints - Ensure inputs are valid time strings and valid timezone identifiers. - Handle any potential exceptions that may arise due to invalid inputs. Example ```python # Example usage of the function utc_time = \\"2023-10-05 14:30:00\\" time_zone = \\"US/Eastern\\" local_time, duration_ns = convert_utc_to_local(utc_time, time_zone) print(local_time) # Expected output: Corresponding local time string print(duration_ns) # Expected output: Time taken for conversion in nanoseconds ``` Notes - Use the `time.strptime` and `time.strftime` functions to handle string to `struct_time` conversions and formatting. - Utilize `time.gmtime()` and `time.mktime()` to work with the `struct_time`. - Make use of high-resolution performance counters from the `time` module, such as `time.perf_counter_ns()`. Good luck, and make sure your solution handles edge cases and provides accurate timing!","solution":"import time import pytz from datetime import datetime def convert_utc_to_local(utc_time: str, time_zone: str) -> tuple: Converts a given UTC time string to local time in a specified timezone. Computes the time taken for the conversion process using high-resolution performance counters. Parameters: utc_time (str): The UTC time string in the format \'%Y-%m-%d %H:%M:%S\'. time_zone (str): The desired local timezone. Returns: tuple: A tuple containing the local time string in the format \'%Y-%m-%d %H:%M:%S\', and the duration of the conversion process in nanoseconds. try: utc_zone = pytz.utc local_zone = pytz.timezone(time_zone) # Start the performance counter start_time = time.perf_counter_ns() # Convert the UTC time string to a datetime object utc_dt = datetime.strptime(utc_time, \'%Y-%m-%d %H:%M:%S\') utc_dt = utc_zone.localize(utc_dt) # Convert to the specified local time zone local_dt = utc_dt.astimezone(local_zone) # Format the local datetime back to string local_time_str = local_dt.strftime(\'%Y-%m-%d %H:%M:%S\') # Stop the performance counter end_time = time.perf_counter_ns() # Calculate the duration duration_ns = end_time - start_time return local_time_str, duration_ns except Exception as e: print(f\\"Error occurred: {e}\\") return None, None"},{"question":"# Assessment Question Objective Implement a function that utilizes the `pprint` module to pretty-print a nested dictionary. Your function should allow for customization of indentation, line width, recursion depth, and dictionary key sorting. Instructions 1. Define a function `custom_pretty_print(data: dict, indent: int = 1, width: int = 80, depth: int = None, sort_dicts: bool = True) -> None` that accepts the following parameters: - `data`: A nested dictionary to be pretty-printed. - `indent`: The amount of indentation added for each nesting level (default is 1). - `width`: The desired maximum number of characters per line in the output (default is 80). - `depth`: The number of nesting levels to be printed (default is no constraint). - `sort_dicts`: Whether dictionaries should be printed with sorted keys (default is True). 2. Utilize the `pprint` module and its functionalities to pretty-print the dictionary according to the specified parameters. 3. Your function should directly print the formatted dictionary to `sys.stdout`. Example Usage ```python import pprint def custom_pretty_print(data, indent=1, width=80, depth=None, sort_dicts=True): # Your implementation here # Sample input data = { \'name\': \'example\', \'description\': { \'details\': \'A sample description\', \'further\': { \'info\': \'Additional info here\' } }, \'keywords\': [\'sample\', \'test\', \'pprint\'], \'metadata\': { \'author\': \'Author Name\', \'version\': \'1.0\', } } # Function call custom_pretty_print(data, indent=4, width=50, depth=2) # Expected output example (formatting may vary) { \'name\': \'example\', \'description\': { \'details\': \'A sample description\', \'further\': {...} }, \'keywords\': [ \'sample\', \'test\', \'pprint\' ], \'metadata\': { \'author\': \'Author Name\', \'version\': \'1.0\' } } ``` Constraints - You must use the `pprint` module functionalities directly in your implementation. - Ensure the function prints the output directly without returning it. Notes - The `depth` parameter limits how deep the nested structure is printed. Any level deeper than this parameter value should be represented with an ellipsis (`...`). - Make sure to test your function with different parameter values and nested dictionaries to ensure robustness and correctness.","solution":"import pprint def custom_pretty_print(data, indent=1, width=80, depth=None, sort_dicts=True): Pretty-prints a nested dictionary with customizable formatting. :param data: The dictionary to be pretty-printed. :param indent: The amount of indentation for each nesting level. :param width: The maximum number of characters per line. :param depth: The maximum depth of nested structures to print. :param sort_dicts: Whether to sort dictionary keys before printing. printer = pprint.PrettyPrinter(indent=indent, width=width, depth=depth, sort_dicts=sort_dicts) printer.pprint(data)"},{"question":"Distributed Multiprocessing with PyTorch Objective: Demonstrate your understanding of distributed multiprocessing in PyTorch by implementing a function that launches multiple worker processes to perform a distributed computation task. Manage these processes using the provided contexts and log their statuses effectively. Problem Statement: Implement a function `distributed_computation` that takes the following inputs: 1. `num_workers: int` - The number of worker processes to launch. 2. `data: List[int]` - A list of integers to be processed in parallel. The list\'s length should be divisible by `num_workers`. 3. `log_dir: str` - Directory path to save process logs. Each worker will be responsible for computing the square of each element in its assigned portion of the data and saving the results to a shared output list. The function should: 1. Launch `num_workers` processes using `torch.distributed.elastic.multiprocessing.start_processes`. 2. Divide `data` into equal chunks, one for each worker. 3. Use appropriate contexts (`MultiprocessContext`, `SubprocessContext`) for managing processes. 4. Log the process start, progress, and completion statuses in `log_dir`. 5. Aggregate the results from all workers and return the final list of squared values in the original order. Constraints: - Assume `data` length is always divisible by `num_workers`. - Use built-in Python modules and PyTorch\'s elastic multiprocessing facilities only. - Ensure proper cleanup of processes and handle any potential exceptions. Expected Input and Output Format: - **Input:** ```python num_workers = 4 data = [1, 2, 3, 4, 5, 6, 7, 8] log_dir = \\"/path/to/logs\\" ``` - **Output:** ```python result = [1, 4, 9, 16, 25, 36, 49, 64] ``` Implementation: You are required to implement the `distributed_computation` function. Ensure that your implementation considers efficient process management and logging consistent with PyTorch\'s multiprocessing contexts. ```python from torch.distributed.elastic.multiprocessing.api import start_processes, PContext def distributed_computation(num_workers: int, data: list, log_dir: str) -> list: # Your implementation here pass ``` Explain any design choices and how you ensure correctness and efficiency in your implementation.","solution":"import os import torch from torch.multiprocessing import Process, Manager def worker_process(data_chunk, results, index, log_dir): Worker function to compute squares of numbers in the data_chunk. pid = os.getpid() log_file = os.path.join(log_dir, f\'worker_{index}.log\') try: with open(log_file, \'w\') as log: log.write(f\\"Process {pid} started.n\\") squared_chunk = [x * x for x in data_chunk] results[index] = squared_chunk log.write(f\\"Process {pid} completed with results: {squared_chunk}n\\") except Exception as e: with open(log_file, \'a\') as log: log.write(f\\"Process {pid} encountered an error: {str(e)}n\\") def distributed_computation(num_workers: int, data: list, log_dir: str) -> list: Distributes the computation of squaring elements in \'data\' across \'num_workers\' processes. # Ensure the log directory exists os.makedirs(log_dir, exist_ok=True) data_length = len(data) chunk_size = data_length // num_workers # Split data into chunks data_chunks = [data[i * chunk_size: (i + 1) * chunk_size] for i in range(num_workers)] # Use Manager to handle shared results with Manager() as manager: results = manager.list([None] * num_workers) processes = [] # Launch processes for i in range(num_workers): p = Process(target=worker_process, args=(data_chunks[i], results, i, log_dir)) processes.append(p) p.start() # Ensure all processes have finished execution for p in processes: p.join() # Flatten and return results in original order result = [element for sublist in results for element in sublist] return result"},{"question":"Problem Statement Given two files, one containing raw data and the other containing the compressed form of the first file using the `zlib` library, you are to write a Python script that: 1. Verifies that the compressed data can be decompressed to obtain the exact original data. 2. Validates the integrity of the decompressed data using CRC32 checksum to ensure it matches the checksum of the original data. Input 1. `input.raw`: Path to a file containing raw binary data. 2. `input.compressed`: Path to a file containing compressed binary data using zlib. Output The script should: 1. Print \\"Verification Successful\\" if the decompressed data matches the original data and the checksums match. 2. Print \\"Verification Failed\\" if any discrepancy is found. Constraints - The CRC32 checksum value of the data should be computed using the `zlib.crc32` function. - The default `wbits` and `bufsize` parameters should be used for decompression unless specified otherwise. - Handle exceptions gracefully and print an error message if any error occurs. Requirements - The script should be efficient in terms of both time and memory. - Assume the input files are not excessively large but fit into available memory. Example Assume the file `input.raw` contains the following bytes: ``` b\'This is the original data that needs to be compressed and verified.\' ``` And the file `input.compressed` contains the compressed version of the above data. ```python import zlib def verify_compressed_file(raw_file_path, compressed_file_path): try: # Read raw data from file with open(raw_file_path, \'rb\') as raw_file: raw_data = raw_file.read() # Read compressed data from file with open(compressed_file_path, \'rb\') as compressed_file: compressed_data = compressed_file.read() # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Calculate original and decompressed checksums original_checksum = zlib.crc32(raw_data) decompressed_checksum = zlib.crc32(decompressed_data) # Verify if decompressed data matches original data if decompressed_data == raw_data and original_checksum == decompressed_checksum: print(\\"Verification Successful\\") else: print(\\"Verification Failed\\") except zlib.error as e: print(f\\"Zlib Error: {e}\\") except Exception as e: print(f\\"Error: {e}\\") # Example usage verify_compressed_file(\'input.raw\', \'input.compressed\') ```","solution":"import zlib def verify_compressed_file(raw_file_path, compressed_file_path): try: # Read raw data from file with open(raw_file_path, \'rb\') as raw_file: raw_data = raw_file.read() # Read compressed data from file with open(compressed_file_path, \'rb\') as compressed_file: compressed_data = compressed_file.read() # Decompress the data decompressed_data = zlib.decompress(compressed_data) # Calculate original and decompressed checksums original_checksum = zlib.crc32(raw_data) decompressed_checksum = zlib.crc32(decompressed_data) # Verify if decompressed data matches original data if decompressed_data == raw_data and original_checksum == decompressed_checksum: print(\\"Verification Successful\\") else: print(\\"Verification Failed\\") except zlib.error as e: print(f\\"Zlib Error: {e}\\") except Exception as e: print(f\\"Error: {e}\\")"},{"question":"Advanced Pattern Matching with Python 3.10 Objective: Assess students\' understanding of pattern matching (a new feature in Python 3.10) along with function implementation and structured data extraction. Problem Statement: You are required to implement a function `parse_student_records` that takes a list of dictionaries representing student records and categorizes them based on their grades using Python 3.10â€™s pattern matching. Each student record dictionary contains the keys: `name`, `grade`, and `subject`. Your task is to write the `parse_student_records` function that: 1. Classifies students into categories based on their grades: - \\"Excellent\\" for grades `90 and above` - \\"Good\\" for grades `80 to 89` - \\"Average\\" for grades `70 to 79` - \\"Needs Improvement\\" for grades below 70 2. Handles invalid records where the `grade` key is missing or the grade is not an integer, categorizing them under \\"Invalid\\". The function should return a dictionary where the keys are the categories and the values are lists of student names who fall into each category. Function Signature: ```python def parse_student_records(records: list) -> dict: ``` Expected Input: - `records` (list): A list of student record dictionaries. Each dictionary has keys: `name` (string), `grade` (integer), and `subject` (string). Expected Output: - A dictionary with the following keys: \\"Excellent\\", \\"Good\\", \\"Average\\", \\"Needs Improvement\\", \\"Invalid\\". Each key maps to a list of student names who fall under the respective category. Constraints: - Each student record is a dictionary with fields: `name`, `grade`, and `subject`. - The `grade` field may or may not be present in some records. - The values for `grade` should be integers; if not, the record should be categorized as \\"Invalid\\". Example: ```python records = [ {\\"name\\": \\"Alice\\", \\"grade\\": 95, \\"subject\\": \\"Math\\"}, {\\"name\\": \\"Bob\\", \\"grade\\": 85, \\"subject\\": \\"History\\"}, {\\"name\\": \\"Cathy\\", \\"grade\\": \\"N/A\\", \\"subject\\": \\"Math\\"}, {\\"name\\": \\"David\\", \\"grade\\": 60, \\"subject\\": \\"Science\\"}, {\\"name\\": \\"Eva\\", \\"subject\\": \\"Art\\"} ] result = parse_student_records(records) print(result) ``` Expected Output: ```python { \\"Excellent\\": [\\"Alice\\"], \\"Good\\": [\\"Bob\\"], \\"Average\\": [], \\"Needs Improvement\\": [\\"David\\"], \\"Invalid\\": [\\"Cathy\\", \\"Eva\\"] } ``` Notes: - Use Python 3.10â€™s pattern matching capabilities for categorizing the records. - Ensure your function handles records with missing or invalid grades gracefully. Submission: - Implement the function `parse_student_records` and test it with sample inputs. - Ensure your code is well-documented and follows Python best practices.","solution":"def parse_student_records(records): Categorize student records based on their grades using pattern matching (Python 3.10+). Parameters: records (list): A list of student record dictionaries. Returns: dict: Categorized student names into Excellent, Good, Average, Needs Improvement, and Invalid. categorized_records = { \\"Excellent\\": [], \\"Good\\": [], \\"Average\\": [], \\"Needs Improvement\\": [], \\"Invalid\\": [] } for record in records: match record: case {\\"name\\": name, \\"grade\\": grade, \\"subject\\": subject} if isinstance(grade, int): if grade >= 90: categorized_records[\\"Excellent\\"].append(name) elif grade >= 80: categorized_records[\\"Good\\"].append(name) elif grade >= 70: categorized_records[\\"Average\\"].append(name) else: categorized_records[\\"Needs Improvement\\"].append(name) case {\\"name\\": name, \\"grade\\": _, \\"subject\\": _}: categorized_records[\\"Invalid\\"].append(name) case _: categorized_records[\\"Invalid\\"].append(record.get(\\"name\\", \\"Unknown\\")) return categorized_records"},{"question":"**Coding Assessment Question: Pandas Mastery** # Background You are given a dataset that records scores of students in a series of exams. The dataset is stored in a DataFrame with the following columns: - `student_id`: Unique identifier for each student. - `name`: Name of the student. - `class`: The class to which the student belongs. - `math_score`: Score in mathematics. - `english_score`: Score in English. - `science_score`: Score in Science. # Problem Statement Your task is to write a Python function called `analyze_scores` that takes a dataframe `df` as input and returns a dictionary with the following keys: 1. `top_math_students`: A DataFrame containing the data of the top 5 students based on their mathematics score. The DataFrame should include the `student_id`, `name`, and `math_score` columns, and be sorted by `math_score` in descending order. 2. `class_avg_scores`: A DataFrame containing the average scores of each class across all subjects. The DataFrame should include `class`, `average_math_score`, `average_english_score`, `average_science_score`, and be sorted by class. 3. `high_achievers`: A DataFrame containing the data of students who have scored above 90 in all three subjects. The DataFrame should include all columns. 4. `missing_scores_info`: A Series containing the count of missing values in each subject (i.e., `math_score`, `english_score`, and `science_score`). # Function Signature ```python import pandas as pd def analyze_scores(df: pd.DataFrame) -> dict: # Your code here pass ``` # Example Suppose the input dataframe `df` is as follows: ```python import pandas as pd data = { \'student_id\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'name\': [\'Alice\', \'Bob\', \'Catherine\', \'David\', \'Eve\', \'Frank\', \'Grace\', \'Hannah\', \'Ivy\', \'Jack\'], \'class\': [\'12A\', \'12B\', \'12A\', \'12B\', \'12A\', \'12B\', \'12A\', \'12B\', \'12A\', \'12B\'], \'math_score\': [95, 85, 76, 89, 90, 92, None, 88, 73, 93], \'english_score\': [88, 94, 67, 91, None, 90, 85, 93, 78, 92], \'science_score\': [75, 89, 90, 93, 88, None, 77, 94, 81, 95] } df = pd.DataFrame(data) ``` Expected output: ```python result = analyze_scores(df) print(result[\'top_math_students\']) # Should print a DataFrame with top 5 students based on math scores print(result[\'class_avg_scores\']) # Should print a DataFrame with average scores per class print(result[\'high_achievers\']) # Should print a DataFrame with students scoring above 90 in all subjects print(result[\'missing_scores_info\']) # Should print a Series with the count of missing values in each subject ``` # Constraints 1. The input dataframe `df` is guaranteed to have the specified columns. 2. There will always be at least one student in each class. 3. The `math_score`, `english_score`, and `science_score` columns can contain `NaN` values. # Implementation Notes 1. Use `.loc` and `.iloc` for selection and filtering. 2. Employ boolean indexing to identify high achievers. 3. Make use of the `query` method for concise data slicing if necessary. 4. Handle missing data appropriately when computing averages.","solution":"import pandas as pd def analyze_scores(df: pd.DataFrame) -> dict: # Top 5 students based on their mathematics score top_math_students = df[[\'student_id\', \'name\', \'math_score\']].dropna(subset=[\'math_score\']) top_math_students = top_math_students.sort_values(by=\'math_score\', ascending=False).head(5) # Average scores of each class across all subjects class_avg_scores = df.groupby(\'class\').agg( average_math_score=(\'math_score\', \'mean\'), average_english_score=(\'english_score\', \'mean\'), average_science_score=(\'science_score\', \'mean\') ).reset_index() # Students who have scored above 90 in all three subjects high_achievers = df.dropna() high_achievers = high_achievers[ (high_achievers[\'math_score\'] > 90) & (high_achievers[\'english_score\'] > 90) & (high_achievers[\'science_score\'] > 90) ] # Count of missing values in each subject missing_scores_info = df[[\'math_score\', \'english_score\', \'science_score\']].isna().sum() return { \'top_math_students\': top_math_students, \'class_avg_scores\': class_avg_scores, \'high_achievers\': high_achievers, \'missing_scores_info\': missing_scores_info }"},{"question":"Objective: You are required to implement a function that analyzes a DataFrame containing stock prices for multiple companies over time using different windowing operations available in pandas. Task: Implement the function `analyze_stock_prices(df: pd.DataFrame, method: str, window: int)` that performs analysis on stock prices. 1. **Parameters**: - `df`: A pandas DataFrame containing the stock prices with at least the following columns: - `Date`: Timestamp of the recorded stock price. - `Company`: Name of the company. - `Price`: Recorded stock price. - `method`: Specifies the type of windowing method to use. It can be one of the following: - `\\"rolling\\"`: Perform rolling window operation. - `\\"expanding\\"`: Perform expanding window operation. - `\\"ewm\\"`: Perform exponentially weighted window operation. - `window`: An integer defining the window size for the rolling and expanding methods or the span for the exponentially weighted method. 2. **Functionality**: - The function should group the data by `Company` and then apply the chosen windowing operation to compute the following statistics on the `Price` column: - Sum - Mean - Standard Deviation - The result should be a DataFrame with multi-level columns, where the first level is `Company` and the second level contains the computed statistics. Constraints: - Handle any null values appropriately. - The `Date` column must be used to ensure the stock prices are ordered chronologically within each company for computation. - Ensure that the function is optimized to handle large datasets efficiently. Example Input: ```python import pandas as pd data = { \'Date\': pd.date_range(\'2021-01-01\', periods=10, freq=\'D\').tolist() * 2, \'Company\': [\'CompanyA\']*10 + [\'CompanyB\']*10, \'Price\': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59] } df = pd.DataFrame(data) result = analyze_stock_prices(df, method=\'rolling\', window=3) ``` Expected Output: ```python CompanyA CompanyB sum mean std sum mean std Date 2021-01-01 100.0 100.0 NaN 50.0 50.0 NaN 2021-01-02 201.0 100.5 0.707107 101.0 50.5 0.707107 2021-01-03 303.0 101.0 1.000000 153.0 51.0 1.000000 ... ``` Additional Notes: - Use the `groupby` method to ensure calculations are done per company. - You may add additional helper functions if necessary. - Include error handling for potential edge cases (e.g., invalid method names, insufficient data). Performance Requirements: - Ensure that the function is optimized for performance, considering the time complexity of pandas operations on large datasets. Implement the `analyze_stock_prices` function below: ```python import pandas as pd def analyze_stock_prices(df: pd.DataFrame, method: str, window: int) -> pd.DataFrame: # Your implementation here pass # Example usage: data = { \'Date\': pd.date_range(\'2021-01-01\', periods=10, freq=\'D\').tolist() * 2, \'Company\': [\'CompanyA\']*10 + [\'CompanyB\']*10, \'Price\': [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59] } df = pd.DataFrame(data) result = analyze_stock_prices(df, method=\'rolling\', window=3) print(result) ```","solution":"import pandas as pd def analyze_stock_prices(df: pd.DataFrame, method: str, window: int) -> pd.DataFrame: # Ensure the data is sorted by Date df = df.sort_values(by=\'Date\') # Define the aggregation functions we care about agg_functions = [\'sum\', \'mean\', \'std\'] # Group by Company grouped = df.groupby(\'Company\') result = [] for name, group in grouped: if method == \'rolling\': rolled = group[\'Price\'].rolling(window=window, min_periods=1) elif method == \'expanding\': rolled = group[\'Price\'].expanding(min_periods=1) elif method == \'ewm\': rolled = group[\'Price\'].ewm(span=window) else: raise ValueError(\\"Invalid method. Use \'rolling\', \'expanding\' or \'ewm\'\\") aggregated = rolled.agg(agg_functions) # Append the multi-level column aggregated.columns = pd.MultiIndex.from_product([[name], aggregated.columns]) result.append(aggregated) final_result = pd.concat(result, axis=1) return final_result"},{"question":"# Question: Visualize the Penguins Dataset with seaborn You are given the seaborn `penguins` dataset. Your task is to create a visualization consisting of multiple subplots that highlight different aspects of the dataset. Instructions 1. **Load the Penguins Dataset:** Load the dataset using `sns.load_dataset(\\"penguins\\")`. 2. **Subplot 1: Distribution Plot (Axes-Level)** - Create a `histplot` showing the distribution of `flipper_length_mm` grouped by the `species`. - Use separate colors for each `species`. 3. **Subplot 2: Relational Plot (Axes-Level)** - Create a `scatterplot` to visualize the relationship between `flipper_length_mm` and `bill_length_mm`. - Color the points by `species`. 4. **Subplot 3: Categorical Plot (Figure-Level)** - Create a `catplot` that displays a boxplot for `body_mass_g` across different `species` and facets by `sex`. 5. **Overall Figure:** - Combine all the subplots into a single figure with matplotlib\'s `subplots` function. - Customize the size of the figure. - Ensure that subplots are spaced out cleanly without overlapping. - Place the legend for Subplot 2 outside the plot area. The following conditions apply: - Subplots should be arranged in a 2x2 grid. - Only Subplot 3 should span across both columns of the bottom row. - Utilize matplotlib\'s `subplots`, `Figure`, and `Axes` objects to manage the figure and its layout. Output The result should be a single combined figure with the specified subplots arranged correctly and appropriately customized. Example Solution (not to be used literally) ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Load the Penguins Dataset penguins = sns.load_dataset(\\"penguins\\") # Create the figure and axes fig, ((ax1, ax2), (ax3, ax3)) = plt.subplots(2, 2, figsize=(15, 10)) plt.subplots_adjust(hspace=0.4, wspace=0.4) # Subplot 1: Distribution Plot sns.histplot(data=penguins, x=\'flipper_length_mm\', hue=\'species\', multiple=\'stack\', ax=ax1) ax1.set_title(\'Distribution of Flipper Length by Species\') # Subplot 2: Relational Plot sns.scatterplot(data=penguins, x=\'flipper_length_mm\', y=\'bill_length_mm\', hue=\'species\', ax=ax2) ax2.set_title(\'Relationship between Flipper and Bill Length\') ax2.legend(loc=\'upper right\', bbox_to_anchor=(1.25, 1)) # Subplot 3: Categorical Plot g = sns.catplot(data=penguins, x=\'species\', y=\'body_mass_g\', kind=\'box\', col=\'sex\', height=5, aspect=1, ax=ax3) g.set_axis_labels(\'Species\', \'Body Mass (g)\').set_titles(\'Distribution by Species and Sex\') plt.show() ``` Notes - The example solution is non-functional and is only meant to guide the structure of the solution. - Ensure all plots are properly labeled with titles and axis labels as needed. - Ensure that the legends are clearly distinguishable and appropriately placed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins(): # Load the Penguins Dataset penguins = sns.load_dataset(\\"penguins\\") # Create the figure and axes fig, ((ax1, ax2), (ax3, ax3)) = plt.subplots(2, 2, figsize=(16, 12)) plt.subplots_adjust(hspace=0.4, wspace=0.4) # Subplot 1: Distribution Plot sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", ax=ax1) ax1.set_title(\'Distribution of Flipper Length by Species\') ax1.set_xlabel(\'Flipper Length (mm)\') ax1.set_ylabel(\'Count\') # Subplot 2: Relational Plot sns.scatterplot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", ax=ax2) ax2.set_title(\'Relationship between Flipper and Bill Length\') ax2.set_xlabel(\'Flipper Length (mm)\') ax2.set_ylabel(\'Bill Length (mm)\') ax2.legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) # Subplot 3: Categorical Plot g = sns.catplot(data=penguins, x=\'species\', y=\'body_mass_g\', kind=\'box\', col=\'sex\', height=5, aspect=1, ax=ax3) g.set_axis_labels(\'Species\', \'Body Mass (g)\').set_titles(\'Distribution by Species and Sex\') plt.show()"},{"question":"# Advanced Python Coding Assessment **Objective**: Implement a multi-threaded asynchronous logging system using Python\'s asyncio module. **Problem Statement**: You are required to create an asyncio-based logging system that can run concurrent logging tasks without blocking the main event loop. The system should ensure that log messages from different tasks and threads are correctly captured and managed. Specifically, you need to fulfill the following requirements: 1. Implement an asynchronous logger which logs messages from different tasks. 2. Ensure that the logger runs in a separate thread to avoid blocking the main event loop. 3. Detect and handle any non-awaited coroutines within the implemented system. 4. Properly handle any exceptions raised within the tasks or logger, ensuring they are logged and do not crash the application. **Function Signature**: ```python import asyncio import logging from concurrent.futures import ThreadPoolExecutor async def logging_task(log_message: str, delay: int): An asynchronous task that logs a given message after a certain delay. Parameters: log_message (str): The message to be logged. delay (int): The delay in seconds before logging the message. await asyncio.sleep(delay) log_message = f\\"Log after {delay} seconds: {log_message}\\" # Add logging logic here def run_logger_in_thread(async_loop: asyncio.AbstractEventLoop): Function to run the asyncio logger in a separate thread. Parameters: async_loop (asyncio.AbstractEventLoop): The event loop to run the logger. async_loop.run_forever() async def main(): The main coroutine to run the logging tasks and manage the event loop. try: # Create event loop and executor loop = asyncio.get_event_loop() executor = ThreadPoolExecutor() # Start logging thread loop.run_in_executor(executor, run_logger_in_thread, loop) # Create and run logging tasks tasks = [ asyncio.create_task(logging_task(\\"First message\\", 2)), asyncio.create_task(logging_task(\\"Second message\\", 1)), asyncio.create_task(logging_task(\\"Third message\\", 3)), ] await asyncio.gather(*tasks) finally: loop.stop() if __name__ == \\"__main__\\": asyncio.run(main()) ``` **Instructions**: - Complete the `logging_task` function to log messages asynchronously. - Integrate the logging mechanism into `run_logger_in_thread`. - Ensure that the logger works correctly in a multi-threaded environment. - Make sure to handle any exceptions and log them appropriately within the `main` function. **Constraints**: - Do not use any blocking calls within the async tasks. - Ensure all coroutines are awaited properly to avoid runtime warnings. - The logger should log to the console with a minimum log level of DEBUG. **Expected Output**: The console should display log messages asynchronously, with each log appearing after the specified delay: ``` DEBUG:asyncio:Log after 1 second: Second message DEBUG:asyncio:Log after 2 seconds: First message DEBUG:asyncio:Log after 3 seconds: Third message ``` **Evaluation Criteria**: - Correct implementation of asynchronous logging. - Proper thread management ensuring no blocking of the main event loop. - Effective exception handling and logging. - No runtime warnings regarding forgotten await statements or unconsumed exceptions.","solution":"import asyncio import logging from concurrent.futures import ThreadPoolExecutor # Configure logger logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(\\"asyncio\\") async def logging_task(log_message: str, delay: int): An asynchronous task that logs a given message after a certain delay. Parameters: log_message (str): The message to be logged. delay (int): The delay in seconds before logging the message. await asyncio.sleep(delay) log_message = f\\"Log after {delay} seconds: {log_message}\\" logger.debug(log_message) return log_message def run_logger_in_thread(async_loop: asyncio.AbstractEventLoop): Function to run the asyncio logger in a separate thread. Parameters: async_loop (asyncio.AbstractEventLoop): The event loop to run the logger. async_loop.run_forever() async def main(): The main coroutine to run the logging tasks and manage the event loop. try: # Create event loop and executor loop = asyncio.get_event_loop() executor = ThreadPoolExecutor() # Start logging thread loop.run_in_executor(executor, run_logger_in_thread, loop) # Create and run logging tasks tasks = [ asyncio.create_task(logging_task(\\"First message\\", 2)), asyncio.create_task(logging_task(\\"Second message\\", 1)), asyncio.create_task(logging_task(\\"Third message\\", 3)), ] results = await asyncio.gather(*tasks) return results except Exception as e: logger.error(f\\"An error occurred: {e}\\") finally: loop.stop() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Design a Python function that utilizes the `contextvars` module to manage context variables within an application that supports asynchronous operations. # Question **Task:** Write a function `manage_context_variables` that creates and manages context variables. The function should meet the following requirements: 1. **Create a new context:** Create a new context for managing variables. 2. **Create context variables:** Create at least two context variables with default values. 3. **Set and Get Context Variable Values within New Context:** - Set new values for these context variables within the newly created context. - Retrieve and print these values within the same context. 4. **Copy Current Context:** Copy the current context and demonstrate that the copied context retains the same variable values. 5. **Exit Context:** Exit the context and show that the context variables have reverted to their default values. # Input and Output * There are no inputs for the `manage_context_variables` function. * The function should print the values of the context variables at different stages: - After setting values within the new context. - After exiting the context and showing the default values. - After copying the current context and showing the copied values. # Constraints - Ensure that the function handles exceptions and errors gracefully. - The function should demonstrate the usage of context variable creation, setting, getting, entering, exiting, and copying contexts. # Example Pseudocode Here\'s a rough pseudocode outline to guide your implementation: ```python import contextvars def manage_context_variables(): # Step 1: Create a new context new_context = contextvars.copy_context() # Step 2: Create context variables var1 = contextvars.ContextVar(\'var1\', default=\'default_value1\') var2 = contextvars.ContextVar(\'var2\', default=\'default_value2\') # Step 3: Set new values for context variables within new context new_context.run(var1.set, \'new_value1\') new_context.run(var2.set, \'new_value2\') # Get and print values within new context print(new_context.run(var1.get)) # Output: \'new_value1\' print(new_context.run(var2.get)) # Output: \'new_value2\' # Step 4: Copy the current context and print copied values copied_context = new_context.copy() print(copied_context.run(var1.get)) # Output: \'new_value1\' print(copied_context.run(var2.get)) # Output: \'new_value2\' # Step 5: Exit context and print default values # Assuming context is exited here manually for demonstration purposes # Since contextvars works with async operations, exiting contexts might differ print(var1.get()) # Output: \'default_value1\' print(var2.get()) # Output: \'default_value2\' # Call the function to demonstrate functionality manage_context_variables() ``` **Note:** The pseudocode is just an outline. Your final implementation should be a fully functional Python code.","solution":"import contextvars def manage_context_variables(): # Step 1: Create a new context ctx = contextvars.copy_context() # Step 2: Create context variables var1 = contextvars.ContextVar(\'var1\', default=\'default_value1\') var2 = contextvars.ContextVar(\'var2\', default=\'default_value2\') # Step 3: Set new values for context variables within new context ctx.run(var1.set, \'new_value1\') ctx.run(var2.set, \'new_value2\') # Get and print values within new context print(ctx.run(var1.get)) # Output: \'new_value1\' print(ctx.run(var2.get)) # Output: \'new_value2\' # Step 4: Copy the current context and print copied values copied_ctx = ctx.copy() print(copied_ctx.run(var1.get)) # Output: \'new_value1\' print(copied_ctx.run(var2.get)) # Output: \'new_value2\' # Step 5: Exit context and print default values # Since we\'re using contextvars which works more with async, # Exiting is shown by running in default context after modifications print(var1.get()) # Output: \'default_value1\' print(var2.get()) # Output: \'default_value2\' # Call the function to demonstrate functionality manage_context_variables()"},{"question":"Objective Implement a Python class that utilizes the `zipimport` module to load and interact with Python modules stored within a provided ZIP archive. Problem Statement Your task is to implement a class `ModuleLoader` that takes in a path to a ZIP file containing Python modules and provides methods to: 1. `list_modules`: List all Python modules available in the ZIP archive. 2. `load_and_execute`: Load and execute a specific module from the ZIP archive, returning the result of a specified function call within that module. 3. `get_module_source`: Retrieve the source code of a specified module. 4. `is_package`: Determine if a specified module is a package. Class and Method Specifications: ```python class ModuleLoader: def __init__(self, archive_path: str): Initializes the ModuleLoader with the given ZIP archive path. :param archive_path: str - A path to the ZIP file containing Python modules. pass def list_modules(self) -> list: Lists all available Python modules in the ZIP archive. :return: list - A list of strings representing the module names. pass def load_and_execute(self, module_name: str, func_name: str, *args, **kwargs): Loads a specific module from the ZIP archive and executes a specified function within it. :param module_name: str - The name of the module to load. :param func_name: str - The name of the function to execute. :param args: list - Positional arguments to pass to the function. :param kwargs: dict - Keyword arguments to pass to the function. :return: The result of the function call. pass def get_module_source(self, module_name: str) -> str: Retrieves the source code of a specified module. :param module_name: str - The name of the module. :return: str - The source code of the module. pass def is_package(self, module_name: str) -> bool: Determines if a specified module is a package. :param module_name: str - The name of the module. :return: bool - True if the module is a package, False otherwise. pass ``` Constraints: - You may assume the ZIP archive exists and is correctly formatted. - You may assume module names and function names provided are valid and exist within the ZIP archive. - Focus on correctly using `zipimporter` methods to achieve the desired functionality. Example Usage: ```python loader = ModuleLoader(\'path/to/archive.zip\') # List available modules modules = loader.list_modules() print(modules) # E.g., [\'module1\', \'module2\', \'package1.module3\'] # Load and execute a function result = loader.load_and_execute(\'module1\', \'some_function\', arg1, arg2, kwarg1=value1) print(result) # Output from some_function # Get module source code source = loader.get_module_source(\'module1\') print(source) # Source code of module1 # Check if a module is a package is_pkg = loader.is_package(\'package1\') print(is_pkg) # True or False ``` This question tests the students\' understanding of module importing, usage of the `zipimport` class, handling and interacting with Python modules within ZIP archives, and application of various `zipimport` methods.","solution":"import zipimport import pkgutil class ModuleLoader: def __init__(self, archive_path: str): Initializes the ModuleLoader with the given ZIP archive path. :param archive_path: str - A path to the ZIP file containing Python modules. self.importer = zipimport.zipimporter(archive_path) def list_modules(self) -> list: Lists all available Python modules in the ZIP archive. :return: list - A list of strings representing the module names. return self.importer._files.keys() def load_and_execute(self, module_name: str, func_name: str, *args, **kwargs): Loads a specific module from the ZIP archive and executes a specified function within it. :param module_name: str - The name of the module to load. :param func_name: str - The name of the function to execute. :param args: list - Positional arguments to pass to the function. :param kwargs: dict - Keyword arguments to pass to the function. :return: The result of the function call. module = self.importer.load_module(module_name) func = getattr(module, func_name) return func(*args, **kwargs) def get_module_source(self, module_name: str) -> str: Retrieves the source code of a specified module. :param module_name: str - The name of the module. :return: str - The source code of the module. source = self.importer.get_source(module_name) return source def is_package(self, module_name: str) -> bool: Determines if a specified module is a package. :param module_name: str - The name of the module. :return: bool - True if the module is a package, False otherwise. return self.importer.is_package(module_name)"},{"question":"**Objective**: Assess the understanding of asyncio Future objects and their manipulation in an asynchronous context. **Problem Statement**: You are required to implement an asynchronous task scheduler using asyncio.Future objects. The scheduler will handle multiple asynchronous tasks and ensure that all tasks are completed. Your implementation should include creating Future objects, setting their results, handling exceptions, and using callbacks. Implement the following functions: 1. `create_task(fut, delay, value)`: A coroutine that takes a Future object `fut`, sleeps for `delay` seconds, and then sets `value` as the result of `fut`. **Input**: - `fut`: an instance of `asyncio.Future` - `delay`: integer, the number of seconds the coroutine should sleep before setting the result - `value`: the result to be set after the delay **Output**: None 2. `task_scheduler(loop, tasks)`: A coroutine that takes an event loop `loop` and a list of tasks. Each task is a tuple containing `delay` and `value` (e.g., `(3, \'Task 1 completed\')`). This function should: - Create a Future object for each task. - Schedule each task using `loop.create_task()` to run `create_task()` with the corresponding Future, delay, and value. - Cancel any task that takes longer than 5 seconds to complete. - Collect and return the results of all completed tasks. **Input**: - `loop`: an instance of `asyncio.AbstractEventLoop` - `tasks`: a list of tuples, where each tuple contains: - `delay`: integer, seconds to delay - `value`: value to set as result **Output**: a list of results from completed tasks (in the order of task completion) 3. `main(tasks)`: An entry-point coroutine that: - Gets the current event loop. - Calls `task_scheduler()` with the current loop and tasks. - Prints the results. **Input**: - `tasks`: same as the `tasks` input in `task_scheduler()` **Output**: None (But it should print the results of the tasks). # Constraints - Use asyncio.Future for handling the asynchronous results. - Ensure that any Future taking more than 5 seconds is cancelled. - Handle exceptions gracefully and ensure all results are returned when tasks are completed. # Example ```python import asyncio async def create_task(fut, delay, value): await asyncio.sleep(delay) fut.set_result(value) async def task_scheduler(loop, tasks): futures = [] for delay, value in tasks: fut = loop.create_future() futures.append(fut) loop.create_task(create_task(fut, delay, value)) results = [] for fut in futures: try: result = await asyncio.wait_for(fut, timeout=5.0) results.append(result) except asyncio.TimeoutError: fut.cancel() results.append(\'Cancelled\') return results async def main(tasks): loop = asyncio.get_running_loop() results = await task_scheduler(loop, tasks) print(\'Task results:\', results) # Sample usage tasks = [ (2, \'Task 1 completed\'), (6, \'Task 2 should be cancelled\'), (4, \'Task 3 completed\') ] asyncio.run(main(tasks)) ``` In this example, the output should be: ``` Task results: [\'Task 1 completed\', \'Cancelled\', \'Task 3 completed\'] ``` **Note**: The implementation details such as exceptions, timeout handling, and edge cases are crucial for demonstrating the understanding of Future objects in asyncio.","solution":"import asyncio async def create_task(fut, delay, value): Coroutine that takes a Future object `fut`, sleeps for `delay` seconds, and then sets `value` as the result of `fut`. Parameters: - fut (asyncio.Future): Future object to set the result for. - delay (int): Number of seconds to sleep before setting the result. - value (any): The result to set after the delay. await asyncio.sleep(delay) fut.set_result(value) async def task_scheduler(loop, tasks): Coroutine that schedules tasks and ensures all tasks are completed. Parameters: - loop (asyncio.AbstractEventLoop): The event loop to use for scheduling tasks. - tasks (list of tuples): List of tasks where each tuple contains (delay, value). Returns: - list: List of results from completed tasks. futures = [] for delay, value in tasks: fut = loop.create_future() futures.append(fut) loop.create_task(create_task(fut, delay, value)) results = [] for fut in futures: try: result = await asyncio.wait_for(fut, timeout=5.0) results.append(result) except asyncio.TimeoutError: fut.cancel() results.append(\'Cancelled\') return results async def main(tasks): Entry-point coroutine that calls task_scheduler and prints the results. Parameters: - tasks (list of tuples): List of tasks to be scheduled. loop = asyncio.get_running_loop() results = await task_scheduler(loop, tasks) print(\'Task results:\', results) # Sample usage tasks = [ (2, \'Task 1 completed\'), (6, \'Task 2 should be cancelled\'), (4, \'Task 3 completed\') ] asyncio.run(main(tasks))"},{"question":"Objective: Assess the student\'s understanding of various compound statements in Python, including control structures (`if`, `while`, `for`), exception handling (`try`), context management (`with`), pattern matching (`match`), and asynchronous programming. Problem Statement: Write a Python function `process_data` that performs the following operations based on the control structures and concepts described: 1. Reads a list of tuples from a provided iterable, where each tuple contains a string and an integer. 2. Iterates over the tuples and processes them based on the following criteria: - If the string is `\\"skip\\"`, continue to the next tuple. - If the integer is negative, raise a `ValueError` with the message `\\"Negative value found\\"`. - If the string is `\\"exit\\"`, break out of the loop. - For other strings, if the integer is positive, add it to a cumulative sum, otherwise, append the string to a list of `strings_collected`. 3. After processing all tuples or encountering the `\\"exit\\"` condition: - Return a tuple containing the cumulative sum and the list of `strings_collected`. - Use proper exception handling to manage `ValueError` and ensure any resources (e.g., file handles) are cleaned up if used within a context manager. 4. **Extra Credit:** - Implement this function as an `async` function that can read from an asynchronous data source. Function Signature: ```python def process_data(data_source: Iterable[Tuple[str, int]]) -> Tuple[int, List[str]]: pass # Extra Credit async def process_data_async(data_source: AsyncIterable[Tuple[str, int]]) -> Tuple[int, List[str]]: pass ``` Constraints: - The input `data_source` is guaranteed to be an iterable of tuples where each tuple contains a string and an integer. - For `process_data_async`, the `data_source` is an `AsyncIterable` of tuples. - You must use the shown control structures properly and handle all exceptions as specified. Example Usage: ```python data_source = [(\\"hello\\", 5), (\\"skip\\", 0), (\\"error\\", -1), (\\"world\\", 3), (\\"exit\\", 0)] try: result = process_data(data_source) print(result) # Output: (8, []) except ValueError as e: print(e) # Extra Credit (Forget requires async execution context) import asyncio async def main(): data_source = [(\\"hello\\", 5), (\\"skip\\", 0), (\\"error\\", -1), (\\"world\\", 3), (\\"exit\\", 0)] try: result = await process_data_async(data_source) print(result) except ValueError as e: print(e) asyncio.run(main()) ``` Hints: 1. Use `if`, `elif`, and `else` for conditional logic. 2. Use `try` and `except` for handling exceptions. 3. Use `for` and `while` loops for iteration. 4. Use `with` statements for context management, if applicable. 5. Implement asynchronous behavior for handling async data sources for extra credit.","solution":"from typing import Iterable, Tuple, List def process_data(data_source: Iterable[Tuple[str, int]]) -> Tuple[int, List[str]]: cumulative_sum = 0 strings_collected = [] for string, number in data_source: if string == \\"skip\\": continue elif number < 0: raise ValueError(\\"Negative value found\\") elif string == \\"exit\\": break else: if number > 0: cumulative_sum += number else: strings_collected.append(string) return (cumulative_sum, strings_collected) # Extra Credit from typing import AsyncIterable import asyncio async def process_data_async(data_source: AsyncIterable[Tuple[str, int]]) -> Tuple[int, List[str]]: cumulative_sum = 0 strings_collected = [] async for string, number in data_source: if string == \\"skip\\": continue elif number < 0: raise ValueError(\\"Negative value found\\") elif string == \\"exit\\": break else: if number > 0: cumulative_sum += number else: strings_collected.append(string) return (cumulative_sum, strings_collected)"},{"question":"Objective: Design a function in Python that utilizes Python\'s C API (specifically the provided functions) to trace the execution of Python code and log details about its execution frames, including locally and globally scoped variables. Problem Statement: You are required to write a function, `trace_execution_code(code)`, which takes a string of Python code, executes it, and logs the details of its execution frames. The details should include: 1. Line numbers of execution. 2. Local and global variables at each line of execution. 3. Name and description of the function being executed (if applicable). Requirements: 1. You must use the functions outlined in the provided documentation. 2. Your function should log the following details for each frame: - Current line number using `PyFrame_GetLineNumber()`. - Local variables using `PyEval_GetLocals()`. - Global variables using `PyEval_GetGlobals()`. - Name and description of the current function being executed using `PyEval_GetFuncName()` and `PyEval_GetFuncDesc()` (if applicable). Input: - `code` (string): A string of valid Python code to be executed and traced. Output: - A log of the trace, which would ideally be printed to the console or saved to a file. Each log entry should provide details of the execution frame as mentioned above. Example: ```python def sample_function(x): y = x + 1 return y trace_execution_code(\'sample_function(5)\') ``` **Sample Log Output:** ``` Line Number: 2 Function Name: sample_function Function Description: () Local Variables: {\'x\': 5} Global Variables: {\'sample_function\': <function sample_function at 0x7fdc6c1f1c10>} Line Number: 3 Function Name: sample_function Function Description: () Local Variables: {\'x\': 5, \'y\': 6} Global Variables: {\'sample_function\': <function sample_function at 0x7fdc6c1f1c10>} ``` Constraints: - The provided code will always be syntactically correct. - The function must handle nested function calls accurately. Note: You may need to use Python\'s C API integration with your Python code to access these functions and gather necessary details. This might involve creating a custom C extension or using available Python packages that provide such capabilities.","solution":"import sys def trace_execution_code(code): def trace(frame, event, arg): if event == \'line\': co = frame.f_code function_name = co.co_name line_no = frame.f_lineno locals_ = frame.f_locals globals_ = frame.f_globals print(f\\"Line Number: {line_no}\\") print(f\\"Function Name: {function_name}\\") print(f\\"Local Variables: {locals_}\\") print(f\\"Global Variables: {globals_}\\") print(\\"-\\" * 40) return trace sys.settrace(trace) try: exec(code) finally: sys.settrace(None)"},{"question":"# Custom Codec Error Handler and Iterative Encoding/Decoding **Objective**: Implement a custom error handler and iteratively encode and decode a text using a specific encoding. **Problem Statement**: You are required to create a custom error handler and use it in conjunction with the incremental encoder and decoder. The custom error handler must replace any unencodable character with a predefined sequence of characters, e.g., \\"[UNK]\\". Then, using the incremental encoder and decoder interfaces, iteratively encode and decode a given text string. **Task**: 1. Implement a custom error handler named `unk_replace_errors` which replaces any unencodable character with the string \\"[UNK]\\". 2. Register the custom error handler. 3. Create a function `iterative_encode_decode` which takes a string and an encoding as input, and returns the encoded and decoded version of the string using the incremental encoder/decoder with the custom error handler. **Function Signatures**: - `def unk_replace_errors(exception):` - **Input**: `exception` (A `UnicodeEncodeError` or `UnicodeDecodeError` instance) - **Output**: A tuple with the replacement string \\"[UNK]\\" and the position to continue encoding/decoding. - `def iterative_encode_decode(text: str, encoding: str) -> str:` - **Input**: - `text` (A string to be encoded and decoded) - `encoding` (A string specifying the encoding to use) - **Output**: The decoded string after iterative encoding and decoding. **Constraints**: - Use only the provided encoding and decoding functions from the `codecs` module. - The solution should handle text strings with various Unicode characters, including those not supported by the specified encoding. - Ensure that the iterative process using incremental encoders and decoders is demonstrated. **Example**: ```python # Register the custom error handler codecs.register_error(\'unk_replace\', unk_replace_errors) # Test the iterative encode/decode function text = \'Hello, World! ÐŸÑ€Ð¸Ð²ÐµÑ‚, Ð¼Ð¸Ñ€!\' encoding = \'ascii\' result = iterative_encode_decode(text, encoding) print(result) # Expected Output: \'Hello, World! [UNK][UNK][UNK][UNK][UNK][UNK], [UNK][UNK][UNK][UNK]!\' ``` **Explanation**: - The given text contains both ASCII and non-ASCII characters. - The custom error handler must replace any non-ASCII characters with \\"[UNK]\\" during the encoding process. - The text should be encoded and then decoded iteratively to demonstrate the function\'s correctness.","solution":"import codecs def unk_replace_errors(exception): Custom error handler that replaces unencodable characters with \'[UNK]\'. if isinstance(exception, UnicodeEncodeError): # exception.object[exception.start:exception.end] gives the problematic characters return (\'[UNK]\', exception.end) elif isinstance(exception, UnicodeDecodeError): return (\'[UNK]\', exception.end) else: raise TypeError(\\"Don\'t know how to handle this type of exception\\") # Register the custom error handler codecs.register_error(\'unk_replace\', unk_replace_errors) def iterative_encode_decode(text, encoding): Encode and decode the text iteratively using the given encoding and custom error handler. # Initialize incremental encoder and decoder encoder = codecs.getincrementalencoder(encoding)(\'unk_replace\') decoder = codecs.getincrementaldecoder(encoding)(\'unk_replace\') # Encode the text incrementally encoded_bytes = bytearray() for char in text: encoded_chunk = encoder.encode(char) encoded_bytes.extend(encoded_chunk) # Flush the encoder to get remaining bytes, if any encoded_bytes.extend(encoder.encode(\'\', True)) # Decode the text incrementally decoded_text = [] for byte in encoded_bytes: decoded_chunk = decoder.decode(bytes([byte])) decoded_text.append(decoded_chunk) # Flush the decoder to get remaining text, if any decoded_text.append(decoder.decode(b\'\', True)) return \'\'.join(decoded_text)"},{"question":"**Problem Statement** In this assignment, you are required to perform clustering on a synthetic dataset using scikit-learn. The task involves generating a synthetic dataset using a Gaussian Mixture Model (GMM) and then applying different clustering algorithms to see how well they can recover the original cluster structure. Finally, you will evaluate and compare the performance of these clustering algorithms. **Instructions** 1. **Generate Synthetic Data:** - Use `sklearn.mixture.GaussianMixture` to generate a synthetic dataset with 3 clusters. - The dataset should have 500 samples, each with 2 features. - Ensure that the clusters have different variances. 2. **Clustering Algorithms:** - Apply the following clustering algorithms on the generated data: - KMeans - Agglomerative Clustering - DBSCAN 3. **Evaluation:** - Use the Adjusted Rand Index (ARI) from `sklearn.metrics` to evaluate the performance of each clustering algorithm. ARI measures the similarity between two data clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters. 4. **Implementation:** - Write a function `compare_clustering_algorithms` to perform the above tasks. - The function should return a dictionary with the clustering algorithm names as keys and their respective ARI scores as values. **Function Signature:** ```python def compare_clustering_algorithms(): pass ``` **Expected Output Format:** The output dictionary should look similar to this (with appropriate ARI score values): ```python { \'KMeans\': 0.85, \'AgglomerativeClustering\': 0.80, \'DBSCAN\': 0.75 } ``` **Constraints:** - You can assume that the Gaussian Mixture Model used to generate the data will have distinct clusters to ensure valid comparison. **Performance Requirements:** - The implementation should be efficient in terms of time complexity and should be able to handle the dataset within a reasonable amount of time. **Hints:** - Use `sklearn.datasets.make_blobs` to facilitate the generation of synthetic clusters if needed. - Refer to the scikit-learn documentation for usage examples of clustering algorithms and evaluation metrics. **Notes:** The focus of this problem is to assess your understanding of implementing and evaluating clustering algorithms using scikit-learn\'s unsupervised learning toolkit. Ensure your code is well-commented and follows best practices.","solution":"from sklearn.mixture import GaussianMixture from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.datasets import make_blobs from sklearn.metrics import adjusted_rand_score def compare_clustering_algorithms(): # Generate synthetic data X, y_true = make_blobs(n_samples=500, centers=3, n_features=2, cluster_std=[1.0, 2.5, 0.5], random_state=42) # Initialize the models kmeans = KMeans(n_clusters=3, random_state=42) agg_clustering = AgglomerativeClustering(n_clusters=3) dbscan = DBSCAN(eps=0.5) # Fit the models kmeans_labels = kmeans.fit_predict(X) agg_labels = agg_clustering.fit_predict(X) dbscan_labels = dbscan.fit_predict(X) # Compute Adjusted Rand Index ari_kmeans = adjusted_rand_score(y_true, kmeans_labels) ari_agg = adjusted_rand_score(y_true, agg_labels) ari_dbscan = adjusted_rand_score(y_true, dbscan_labels) return { \'KMeans\': ari_kmeans, \'AgglomerativeClustering\': ari_agg, \'DBSCAN\': ari_dbscan }"},{"question":"You are tasked with developing a function to perform a series of color space transformations using the \\"colorsys\\" module. Your function should demonstrate a comprehensive understanding of the various conversions between RGB, YIQ, HLS, and HSV color spaces. # Task Write a function `complex_color_transformation` that accepts an RGB color tuple (r, g, b), and performs the following sequence of transformations: 1. Convert the RGB color to YIQ. 2. Convert the resulting YIQ color to HLS. 3. Convert the resulting HLS color to HSV. 4. Finally, convert the resulting HSV color back to RGB. # Function Signature ```python def complex_color_transformation(r: float, g: float, b: float) -> tuple: pass ``` # Input - A tuple `(r, g, b)` where `r`, `g`, and `b` are floats representing the RGB color components. Each component is within the range [0, 1]. # Output - A tuple `(r, g, b)` where `r`, `g`, and `b` are floats representing the final RGB color components after the series of conversions. Each component is within the range [0, 1]. # Constraints 1. Ensure that all intermediate results during transformation stay within their respective valid ranges. 2. Utilize the \\"colorsys\\" moduleâ€™s functions for all conversions. # Example ```python >>> complex_color_transformation(0.2, 0.4, 0.4) (<final_r>, <final_g>, <final_b>) # Example output after the series of transformations ``` # Notes 1. The transformation must adhere strictly to the order specified. 2. The final RGB value after all transformations might differ slightly from the initial input due to floating-point precision limits.","solution":"import colorsys def complex_color_transformation(r: float, g: float, b: float) -> tuple: Perform a series of color space transformations: RGB -> YIQ -> HLS -> HSV -> RGB. # Step 1: Convert RGB to YIQ yiq = colorsys.rgb_to_yiq(r, g, b) # Step 2: Convert YIQ to HLS # There is no direct YIQ to HLS conversion function, so we convert YIQ back to RGB first rgb_from_yiq = colorsys.yiq_to_rgb(*yiq) hls = colorsys.rgb_to_hls(*rgb_from_yiq) # Step 3: Convert HLS to HSV # Again, no direct HLS to HSV, so convert HLS back to RGB first rgb_from_hls = colorsys.hls_to_rgb(*hls) hsv = colorsys.rgb_to_hsv(*rgb_from_hls) # Step 4: Convert HSV back to RGB final_rgb = colorsys.hsv_to_rgb(*hsv) return final_rgb"},{"question":"**Question: Visualizing Trends in the \'tips\' Dataset Using Seaborn Objects** You are required to create a plot that visualizes the trends in the \'tips\' dataset using the Seaborn `objects` interface. Follow the guidelines below to complete the task: 1. **Load the \'tips\' dataset**: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` 2. **Create a layered plot using `seaborn.objects` to visualize the relationship between \'day\' and \'total_bill\'**: - Use `Dots()` to plot the individual data points. - Apply `Jitter()` to the dots to avoid overlap. - Add a `Range()` with a 50% confidence interval (25th to 75th percentile) for each day. - Shift the range slightly on the x-axis for better visualization. **Implementation Details**: - Input: No input from the user; load the dataset directly within the notebook/script. - Output: The final plot visualized using Seaborn `objects`. **Expected Code Structure**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot ( so.Plot(tips, \\"day\\", \\"total_bill\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.1)) ) ``` **Constraints**: - Ensure the plot is clear and easy to interpret with no overlapping data points. - Utilize the Seaborn `objects` smoothly in conjunction with the given methods for plotting. **Performance Requirements**: - The plot generation should be efficient and render within a reasonable time frame for interactive data exploration. **Hints**: - Refer to the provided examples to understand how to layer multiple marks. - Explore the Seaborn documentation for additional customization options.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, \\"day\\", \\"total_bill\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.1)) ) # Show the plot plot.show()"},{"question":"**Python File I/O and Formatted Output** # Problem Description You are tasked with writing a Python function that reads data from a file, processes the data, and outputs it in a formatted manner. # Objective - Write a function `process_and_format_file(input_file: str, output_file: str) -> None` that: 1. Reads the structured data from `input_file`. Assume the file contains data in JSON format representing a list of dictionaries. 2. Each dictionary contains information about individuals, structured as follows: ```json [ {\\"name\\": \\"John Doe\\", \\"age\\": 29, \\"city\\": \\"New York\\"}, {\\"name\\": \\"Jane Smith\\", \\"age\\": 34, \\"city\\": \\"Los Angeles\\"} ] ``` 3. Processes the data to generate a formatted string for each individual. Use formatted string literals (f-strings) to format output as shown below: - Example: `Name: John Doe, Age: 29, City: New York`. 4. Writes the formatted string line-by-line to `output_file`. # Input - `input_file`: A string representing the path to the input file. - `output_file`: A string representing the path to the output file. # Output - The function does not return anything. It writes the formatted data to the specified `output_file`. # Constraints - Assume that the `input_file` always exists and contains valid JSON data in the specified format. - Handle any necessary file operations such as opening and closing the files to ensure there are no resource leaks. # Example Usage ```python # Assume input.json contains: # [ # {\\"name\\": \\"John Doe\\", \\"age\\": 29, \\"city\\": \\"New York\\"}, # {\\"name\\": \\"Jane Smith\\", \\"age\\": 34, \\"city\\": \\"Los Angeles\\"} # ] process_and_format_file(\'input.json\', \'output.txt\') # Contents of output.txt: # Name: John Doe, Age: 29, City: New York # Name: Jane Smith, Age: 34, City: Los Angeles ``` # Notes - You may use the `json` module to handle JSON serialization and deserialization. - Make sure to handle file operations (opening, reading, writing) properly to avoid any potential issues with resource management. **Tip:** Consider using Python\'s `with` statement to handle file operations efficiently.","solution":"import json def process_and_format_file(input_file: str, output_file: str) -> None: Reads the structured data from input_file in JSON format, processes the data to generate formatted strings, and writes them to output_file. Args: input_file (str): Path to the input file containing JSON data. output_file (str): Path to the output file where formatted strings will be written. with open(input_file, \'r\') as infile: data = json.load(infile) formatted_lines = [f\\"Name: {person[\'name\']}, Age: {person[\'age\']}, City: {person[\'city\']}\\" for person in data] with open(output_file, \'w\') as outfile: outfile.write(\'n\'.join(formatted_lines))"},{"question":"# Python Slice Implementation Your task is to implement a set of Python functions that mimic the C API slice handling functions described in the provided documentation. Specifically, you need to create the following functions: 1. **is_slice(obj)**: - Checks if `obj` is a slice object. **Input:** ```python obj: any ``` **Output:** ```python bool ``` 2. **new_slice(start, stop, step)**: - Creates a new slice object using the provided start, stop, and step values. **Input:** ```python start: int, or None stop: int, or None step: int, or None ``` **Output:** ```python slice ``` 3. **get_indices(slice_obj, length)**: - Returns the start, stop, and step values from a slice object for a sequence of length `length`. Raises an error if the indices are out of bounds. **Input:** ```python slice_obj: slice length: int ``` **Output:** ```python (int, int, int) ``` 4. **get_indices_ex(slice_obj, length)**: - Similar to `get_indices` but also returns the length of the slice. **Input:** ```python slice_obj: slice length: int ``` **Output:** ```python (int, int, int, int) ``` 5. **unpack_slice(slice_obj)**: - Unpacks `start`, `stop`, and `step` from a slice object. **Input:** ```python slice_obj: slice ``` **Output:** ```python (int, int, int) ``` 6. **adjust_indices(length, start, stop, step)**: - Adjusts the `start`, `stop`, and `step` indices for a sequence of length `length`. **Input:** ```python length: int start: int stop: int step: int ``` **Output:** ```python int ``` # Constraints - Your functions should mimic the behavior of their C API counterparts as described. - You should handle None values appropriately in the `start`, `stop`, and `step` parameters. - Assume input values and types are valid during the assessment. You do not need to implement extensive input validation. - Implement the functions to handle negative indices and slices properly, as per Python\'s slice behavior. # Examples Example 1 ```python is_slice(slice(1, 5, 2)) ``` **Output:** ```python True ``` Example 2 ```python new_slice(1, 10, 2) ``` **Output:** ```python slice(1, 10, 2) ``` Example 3 ```python get_indices(slice(1, 10, 2), 12) ``` **Output:** ```python (1, 10, 2) ``` Example 4 ```python get_indices_ex(slice(1, 10, 2), 12) ``` **Output:** ```python (1, 10, 2, 5) ``` Example 5 ```python unpack_slice(slice(1, 10, 2)) ``` **Output:** ```python (1, 10, 2) ``` Example 6 ```python adjust_indices(10, 0, 10, 2) ``` **Output:** ```python 5 ``` Your task is to implement these functions correctly. Good luck!","solution":"def is_slice(obj): Checks if obj is a slice object. return isinstance(obj, slice) def new_slice(start, stop, step): Creates a new slice object using the provided start, stop, and step values. return slice(start, stop, step) def get_indices(slice_obj, length): Returns the start, stop, and step values from a slice object for a sequence of length `length`. Raises an error if the indices are out of bounds. start, stop, step = slice_obj.indices(length) return (start, stop, step) def get_indices_ex(slice_obj, length): Similar to `get_indices` but also returns the length of the slice. start, stop, step = slice_obj.indices(length) slice_length = len(range(start, stop, step)) return (start, stop, step, slice_length) def unpack_slice(slice_obj): Unpacks `start`, `stop`, and `step` from a slice object. return (slice_obj.start, slice_obj.stop, slice_obj.step) def adjust_indices(length, start, stop, step): Adjusts the `start`, `stop`, and `step` indices for a sequence of length `length`. indices = slice(start, stop, step).indices(length) return len(range(*indices))"},{"question":"Objective: Your task is to demonstrate your proficiency in using the Seaborn library for data visualization, focusing specifically on the creation and application of customized color palettes. Problem Statement: 1. **Data Preparation**: - Create a DataFrame `df` containing three columns: `category`, `value1`, and `value2`. - The `category` column should have five unique categories: \'A\', \'B\', \'C\', \'D\', and \'E\'. - The `value1` and `value2` columns should each contain five random integer values between 10 and 100. 2. **Color Palette Creation**: - Create a light palette from a color of your choice using its hex code. Ensure this color palette contains 5 discrete colors. 3. **Visualization**: - Generate a bar plot using Seaborn to visualize the `value1` data for each category on the x-axis. - Use the customized color palette for the bars. - Overlay a line plot on the same figure to visualize the `value2` data for each category, customizing the line color differently from the palette. 4. **Customization**: - Add appropriate labels to the x-axis, y-axis, and a title for the plot. - Include a legend to differentiate between the bar plot and line plot. Constraints: - **Libraries Allowed**: You are only allowed to use `seaborn`, `pandas`, and `matplotlib` libraries. - **Performance**: The solution should execute within a reasonable time frame (less than a second). Example: Your plots should look professional, follow good practices in visualization, and leverage the capabilities of Seaborn effectively. Requirements: - Define a function `create_visualization()` that does not take any parameters and returns None. - The function should perform all tasks defined in the problem statement. Expectations: - Your solution should be functional and demonstrate a clear understanding of Seaborn\'s capabilities and customization options. - Ensure the code is clean, modular, and well-commented. ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np def create_visualization(): # Data Preparation np.random.seed(0) categories = [\'A\', \'B\', \'C\', \'D\', \'E\'] value1 = np.random.randint(10, 100, size=5) value2 = np.random.randint(10, 100, size=5) df = pd.DataFrame({\'category\': categories, \'value1\': value1, \'value2\': value2}) # Color Palette Creation palette = sns.light_palette(\\"#79C\\", n_colors=5) # Visualization sns.set_theme() fig, ax1 = plt.subplots() sns.barplot(x=\'category\', y=\'value1\', data=df, palette=palette, ax=ax1) ax2 = ax1.twinx() sns.lineplot(x=\'category\', y=\'value2\', data=df, marker=\\"o\\", sort=False, ax=ax2, color=\'red\') # Customization ax1.set_xlabel(\'Category\') ax1.set_ylabel(\'Value 1\') ax2.set_ylabel(\'Value 2\') plt.title(\'Bar and Line Plot using Custom Seaborn Palette\') fig.legend(labels=[\'value1\', \'value2\'], loc=\'upper right\') plt.show() # Call the function to display the plot create_visualization() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np def create_visualization(): # Data Preparation np.random.seed(0) categories = [\'A\', \'B\', \'C\', \'D\', \'E\'] value1 = np.random.randint(10, 100, size=5) value2 = np.random.randint(10, 100, size=5) df = pd.DataFrame({\'category\': categories, \'value1\': value1, \'value2\': value2}) # Color Palette Creation palette = sns.light_palette(\\"#79C\\", n_colors=5) # Visualization sns.set_theme() fig, ax1 = plt.subplots() sns.barplot(x=\'category\', y=\'value1\', data=df, palette=palette, ax=ax1) ax2 = ax1.twinx() sns.lineplot(x=\'category\', y=\'value2\', data=df, marker=\\"o\\", sort=False, ax=ax2, color=\'red\') # Customization ax1.set_xlabel(\'Category\') ax1.set_ylabel(\'Value 1\') ax2.set_ylabel(\'Value 2\') plt.title(\'Bar and Line Plot using Custom Seaborn Palette\') fig.legend(labels=[\'value1\', \'value2\'], loc=\'upper right\') plt.show() # Call the function to display the plot create_visualization()"},{"question":"**Objective:** To demonstrate your understanding of the `zipimport` module and import mechanisms in Python, you will create a script that utilizes the `zipimport` capabilities. **Question:** You are given a list of Python files, each containing a simple function definition. Your task is to: 1. Create a ZIP archive that includes these Python files. 2. Use the `zipimport` module to import and execute a function from each of these files. **Requirements:** 1. Implement a function `create_zip_archive(file_paths: List[str], zip_path: str) -> None`: - `file_paths`: A list of strings, each representing the path to a Python file. - `zip_path`: A string representing the path where the ZIP archive should be created. - The function should create a ZIP archive at `zip_path` containing all the files specified in `file_paths`. 2. Implement a function `import_and_execute(zip_path: str, module_name: str, function_name: str) -> Any`: - `zip_path`: A string representing the path to the ZIP archive. - `module_name`: The name of the module to import from the ZIP archive. - `function_name`: The name of the function to execute from the imported module. - The function should use the `zipimport` module to import `module_name` from the ZIP archive located at `zip_path` and execute the function `function_name` from this module. - The function should return the result of the executed function. **Example:** Suppose you have the following Python files: - `file1.py` containing: ```python def func1(): return \\"Function 1 Executed\\" ``` - `file2.py` containing: ```python def func2(): return \\"Function 2 Executed\\" ``` You should be able to: 1. Create a ZIP archive containing these files using: ```python create_zip_archive([\\"file1.py\\", \\"file2.py\\"], \\"example.zip\\") ``` 2. Import and execute `func1` from `file1.py` and `func2` from `file2.py` using: ```python result1 = import_and_execute(\\"example.zip\\", \\"file1\\", \\"func1\\") result2 = import_and_execute(\\"example.zip\\", \\"file2\\", \\"func2\\") ``` The expected results would be: ```python assert result1 == \\"Function 1 Executed\\" assert result2 == \\"Function 2 Executed\\" ``` **Constraints:** - Do not use any third-party libraries for creating the ZIP archive; use the built-in `zipfile` module. - Ensure the `zipimport` module is used for importing and executing the functions. - Assume that the Python files provided in `file_paths` each define at least one function and that the function names provided to `import_and_execute` exist in the respective files.","solution":"import zipfile from zipfile import ZipFile import zipimport from typing import List, Any def create_zip_archive(file_paths: List[str], zip_path: str) -> None: Creates a ZIP archive containing the specified Python files. :param file_paths: List of paths to Python files. :param zip_path: Path where the ZIP archive should be created. with ZipFile(zip_path, \'w\') as zipf: for file in file_paths: zipf.write(file, arcname=file.split(\'/\')[-1]) def import_and_execute(zip_path: str, module_name: str, function_name: str) -> Any: Imports a module from a ZIP archive and executes a specified function. :param zip_path: Path to the ZIP archive. :param module_name: Name of the module to import. :param function_name: Name of the function to execute. :return: Result of the executed function. importer = zipimport.zipimporter(zip_path) module = importer.load_module(module_name) func = getattr(module, function_name) return func()"},{"question":"**Objective:** You are required to write a Python function that uses the \\"ensurepip\\" module to programmatically install or upgrade pip in a specified environment. The function should handle various scenarios like choosing the root directory, installing in user site packages, handling version conflicts, and providing appropriate output messages. **Function Specifications:** 1. **Function Name:** `bootstrap_pip` 2. **Parameters:** - `root` (str or None): Path to the root directory where pip should be installed. Default is `None`. - `upgrade` (bool): Whether to upgrade an existing installation of pip. Default is `False`. - `user` (bool): Whether to install pip in the user site packages directory. Default is `False`. - `altinstall` (bool): Whether to avoid installing the \\"pipX\\" script. Default is `False`. - `default_pip` (bool): Whether to install the \\"pip\\" script. Default is `False`. 3. **Returns:** A string message indicating the outcome of the bootstrap process. **Constraints:** - The function should handle conflicting options (`altinstall` and `default_pip`) by raising a `ValueError` with an appropriate message. - The function should provide clear output messages for different scenarios such as successful installation, upgrade, and any errors encountered. **Example Use Cases:** ```python # Example 1: Basic installation print(bootstrap_pip()) # Output: \\"pip successfully installed.\\" # Example 2: Installing pip with upgrade in user site packages print(bootstrap_pip(upgrade=True, user=True)) # Output: \\"pip successfully upgraded in user site packages.\\" # Example 3: Handling conflicting options try: print(bootstrap_pip(altinstall=True, default_pip=True)) except ValueError as e: print(e) # Output: \\"Conflicting options: altinstall and default_pip cannot be both True.\\" # Example 4: Verbosity in installation process print(bootstrap_pip(verbosity=2)) ``` **Additional Information:** - Ensure proper usage of the `ensurepip.bootstrap()` function. - Handle different scenarios and edge cases, providing informative messages for each. - Use the `ensurepip.version()` function to include the version of pip being installed or upgraded in the output message.","solution":"import ensurepip def bootstrap_pip(root=None, upgrade=False, user=False, altinstall=False, default_pip=True): Install or upgrade pip in the specified environment. :param root: Path to the root directory where pip should be installed. :param upgrade: Whether to upgrade an existing installation of pip. :param user: Whether to install pip in the user site packages directory. :param altinstall: Whether to avoid installing the \\"pipX\\" script. :param default_pip: Whether to install the \\"pip\\" script. :returns: A string message indicating the outcome of the bootstrap process. if altinstall and default_pip: raise ValueError(\\"Conflicting options: altinstall and default_pip cannot be both True.\\") try: ensurepip.bootstrap(root=root, upgrade=upgrade, user=user, altinstall=altinstall, default_pip=default_pip) if upgrade: return \\"pip successfully upgraded.\\" else: return \\"pip successfully installed.\\" except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"Clustering Weather Data with Scikit-Learn # Background You are given a dataset of weather observations. Each observation records different measurements such as temperature, humidity, wind speed, and precipitation. Your task is to analyze this data using clustering algorithms from Scikit-Learn to identify patterns in the weather data. # Task Write a Python function using Scikit-Learn that performs the following steps: 1. **Load and preprocess the data:** - Assume the dataset is provided as a CSV file with columns `[\'temperature\', \'humidity\', \'wind_speed\', \'precipitation\']`. - Standardize the data (mean=0, variance=1) as clustering algorithms often perform better with standardized data. 2. **Apply the K-Means algorithm:** - Use the `KMeans` class from Scikit-Learn with `n_clusters=3` to cluster the data. - Fit the K-Means algorithm to the standardized weather data. - Predict the clusters for each data point and add this as a new column `cluster` to the original DataFrame. 3. **Evaluate the clustering quality:** - Compute the silhouette score for the K-Means clustering and store it in a variable `silhouette_avg`. # Expected Function Signature ```python import pandas as pd from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score def cluster_weather_data(file_path: str) -> pd.DataFrame: Parameters: file_path (str): The file path to the CSV containing weather data. Returns: pd.DataFrame: The original DataFrame with an additional \'cluster\' column indicating the cluster of each row. float: The silhouette score indicating the quality of the clustering. # Load the weather data from CSV weather_data = pd.read_csv(file_path) # Standardize the data (mean=0, var=1) scaler = StandardScaler() standardized_data = scaler.fit_transform(weather_data) # Apply K-Means clustering kmeans = KMeans(n_clusters=3) clusters = kmeans.fit_predict(standardized_data) # Add the cluster labels to the original DataFrame weather_data[\'cluster\'] = clusters # Compute the silhouette score silhouette_avg = silhouette_score(standardized_data, clusters) return weather_data, silhouette_avg ``` # Input - The function takes a single parameter `file_path` which is a string representing the file path to the CSV file containing the weather data. # Output - The function returns a pandas DataFrame with the original data and an additional `cluster` column that contains the cluster label for each data point. - The function also returns the average silhouette score as a float, which indicates the quality of clustering. # Constraints - You should use the KMeans algorithm from the Scikit-Learn library to fit and predict cluster labels. - The dataset will have valid numerical values and no missing entries. # Evaluation - Your solution will be evaluated based on the correctness of the implementation and the quality of clustering results indicated by the silhouette score.","solution":"import pandas as pd from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score def cluster_weather_data(file_path: str) -> pd.DataFrame: Parameters: file_path (str): The file path to the CSV containing weather data. Returns: pd.DataFrame: The original DataFrame with an additional \'cluster\' column indicating the cluster of each row. float: The silhouette score indicating the quality of the clustering. # Load the weather data from CSV weather_data = pd.read_csv(file_path) # Standardize the data (mean=0, variance=1) scaler = StandardScaler() standardized_data = scaler.fit_transform(weather_data) # Apply K-Means clustering kmeans = KMeans(n_clusters=3) clusters = kmeans.fit_predict(standardized_data) # Add the cluster labels to the original DataFrame weather_data[\'cluster\'] = clusters # Compute the silhouette score silhouette_avg = silhouette_score(standardized_data, clusters) return weather_data, silhouette_avg"},{"question":"Objective: To test your understanding of the essential `asyncio` concepts, event loops, task scheduling, and network communication using Python\'s `asyncio` library. Problem: Write an asynchronous function `async def echo_server(host: str, port: int) -> None` that implements a simple TCP echo server. The server should listen on a specified `host` and `port`, accept connections from clients, and echo back any data received from clients preceded by a custom string: `\\"Echo: \\"`. Requirements: 1. **Setting Up the Event Loop:** - You must use `asyncio.get_running_loop()` to obtain the current running event loop or create a new one if none exists. 2. **Handling Networking:** - Create a TCP server using `await loop.create_server()` which listens for incoming client connections. - Once a connection is made, read data from the client using appropriate transport and protocol methods (`data_received()` and `connection_lost()` callbacks should be implemented). - Echo back the received data prefixed with `\\"Echo: \\"`. 3. **Graceful Shutdown:** - Implement a signal handler to handle graceful shutdown of the server using `loop.add_signal_handler()`. - Ensure that the server closes properly when a termination signal is received. 4. **Concurrency:** - The server should be able to handle multiple connections concurrently. Input: - `host` (str): The hostname or IP address where the server will listen for incoming connections. - `port` (int): The port number on which the server will listen for incoming connections. Output: - The function does not return anything but will print output to the console/log for debugging purposes. Example: ```python import asyncio async def main(): await echo_server(\'127.0.0.1\', 8888) if __name__ == \\"__main__\\": asyncio.run(main()) ``` When the server is running, you can connect to it using a tool like `telnet` or a simple client written in Python. Sending the string `\\"hello\\"` should result in the server responding with `\\"Echo: hello\\"`. Constraints: - You must use the `asyncio` methods and functions as described. - The server should handle and respond to client messages asynchronously and concurrently. Notes: - Thoroughly test your implementation to ensure it handles multiple client connections and data transmission correctly. - Comment your code to explain the steps and `asyncio` methods used.","solution":"import asyncio import signal class EchoServerProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport def data_received(self, data): message = data.decode() echo_message = f\\"Echo: {message}\\" self.transport.write(echo_message.encode()) def connection_lost(self, exc): pass async def echo_server(host: str, port: int) -> None: loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), host, port ) print(f\'Serving on {host}:{port}\') # Handle graceful shutdown for sig in (signal.SIGINT, signal.SIGTERM): loop.add_signal_handler( sig, lambda: asyncio.create_task(shutdown(loop, server)) ) async with server: await server.serve_forever() async def shutdown(loop, server): print(\'Shutting down...\') server.close() await server.wait_closed() loop.stop()"},{"question":"# Email Message Parsing and Analysis **Objective:** Implement a function to parse an email message from different sources (bytes, string, file). The function should extract specific details like the subject, sender, recipients, and determine if the message is multipart. **Function Signature:** ```python def parse_email(source: Union[bytes, str, BinaryIO, TextIO], source_type: str) -> dict: Parse an email message from a given source and return details about the email. Args: - source (Union[bytes, str, BinaryIO, TextIO]): The source of the email message. - If source_type is \'bytes\', source will be of type bytes. - If source_type is \'string\', source will be of type str. - If source_type is \'binary_file\', source will be a file-like object opened in binary mode. - If source_type is \'text_file\', source will be a file-like object opened in text mode. - source_type (str): Specifies the type of the source. One of [\'bytes\', \'string\', \'binary_file\', \'text_file\']. Returns: - dict: A dictionary containing the following key-value pairs: - \'subject\': The subject of the email. - \'from\': The email address of the sender. - \'to\': A list of recipient email addresses. - \'is_multipart\': A boolean indicating if the email is multipart. pass ``` **Input:** - `source`: A bytes-like object, string, or file-like object containing the email message. - `source_type`: A string indicating the type of the source. **Output:** - A dictionary with the following keys: - `\'subject\'`: The subject of the email. - `\'from\'`: The sender\'s email address. - `\'to\'`: A list of recipients\' email addresses. - `\'is_multipart\'`: A boolean indicating whether the email is multipart. **Constraints:** - The email message provided in the source will follow standard email formatting rules. - You can assume the email headers \'Subject\', \'From\', and \'To\' are present in the email. **Example Usage:** ```python email_bytes = b\\"Subject: Test emailnFrom: sender@example.comnTo: recipient@example.comnnThis is a test email.\\" result = parse_email(email_bytes, \'bytes\') # Expected output: { # \'subject\': \'Test email\', # \'from\': \'sender@example.com\', # \'to\': [\'recipient@example.com\'], # \'is_multipart\': False # } ``` **Notes:** - Utilize the appropriate email parsing classes (`BytesParser`, `Parser`, `BytesFeedParser`, `FeedParser`) based on the source type. - Correctly handle the different input types and ensure the function is robust against potential email format variations.","solution":"from typing import Union, BinaryIO, TextIO from email.parser import BytesParser, Parser from email.message import Message def parse_email(source: Union[bytes, str, BinaryIO, TextIO], source_type: str) -> dict: Parse an email message from a given source and return details about the email. Args: - source (Union[bytes, str, BinaryIO, TextIO]): The source of the email message. - If source_type is \'bytes\', source will be of type bytes. - If source_type is \'string\', source will be of type str. - If source_type is \'binary_file\', source will be a file-like object opened in binary mode. - If source_type is \'text_file\', source will be a file-like object opened in text mode. - source_type (str): Specifies the type of the source. One of [\'bytes\', \'string\', \'binary_file\', \'text_file\']. Returns: - dict: A dictionary containing the following key-value pairs: - \'subject\': The subject of the email. - \'from\': The email address of the sender. - \'to\': A list of recipient email addresses. - \'is_multipart\': A boolean indicating if the email is multipart. email_message = None if source_type == \'bytes\': email_message = BytesParser().parsebytes(source) elif source_type == \'string\': email_message = Parser().parsestr(source) elif source_type == \'binary_file\': email_message = BytesParser().parse(source) elif source_type == \'text_file\': email_message = Parser().parse(source) else: raise ValueError(\\"source_type must be one of [\'bytes\', \'string\', \'binary_file\', \'text_file\']\\") subject = email_message[\'subject\'] email_from = email_message[\'from\'] email_to = email_message.get_all(\'to\', []) # Convert \'To\' to a list recipients = [] if email_to: for addr in email_to: recipients.extend(addr.split(\',\')) is_multipart = email_message.is_multipart() return { \'subject\': subject, \'from\': email_from, \'to\': [addr.strip() for addr in recipients], \'is_multipart\': is_multipart }"},{"question":"# Task You are given a dataset containing unlabeled data points. Your task is to implement a clustering algorithm using scikit-learn to identify clusters in the data. You will then evaluate the performance of your clustering using specific evaluation metrics. # Requirements 1. Implement the **K-Means** clustering algorithm using scikit-learn. 2. Apply the K-Means algorithm to the provided dataset. 3. Calculate the **Silhouette Score** and the **Calinski-Harabasz Index** to evaluate the quality of the clusters. 4. Interpret the results based on the evaluation metrics. # Input - A 2-dimensional numpy array `data` of shape `(n_samples, n_features)` representing the dataset. - An integer `n_clusters` representing the number of clusters. # Output - A list `cluster_labels` containing the cluster label for each data point. - A float `silhouette_score` representing the Silhouette Score. - A float `calinski_harabasz_index` representing the Calinski-Harabasz Index. # Constraints - `2 <= n_clusters <= 10` - The dataset will have at least 100 samples and no more than 10,000 samples. - The number of features will be between 2 and 10. # Performance Requirements - Your implementation should handle datasets with up to 10,000 samples efficiently. # Function Signature ```python def cluster_and_evaluate(data: np.ndarray, n_clusters: int): # Your implementation here pass ``` # Example ```python import numpy as np data = np.array([[1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0]]) n_clusters = 2 cluster_labels, silhouette, calinski_harabasz = cluster_and_evaluate(data, n_clusters) print(\\"Cluster Labels:\\", cluster_labels) print(\\"Silhouette Score:\\", silhouette) print(\\"Calinski-Harabasz Index:\\", calinski_harabasz) ``` # Evaluation Criteria - Correct implementation of the K-Means algorithm using scikit-learn. - Accurate computation of the evaluation metrics. - Clear interpretation of the results. # Notes - Make sure to handle edge cases, such as when the data cannot be clustered effectively. - Document your code and explain your implementation steps.","solution":"import numpy as np from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, calinski_harabasz_score def cluster_and_evaluate(data: np.ndarray, n_clusters: int): Perform K-Means clustering on the given data and evaluate the clustering performance. Parameters: - data: np.ndarray, the dataset to cluster. - n_clusters: int, the number of clusters to form. Returns: - cluster_labels: List[int], the cluster labels for each data point. - silhouette_score_value: float, the Silhouette Score of the clustering. - calinski_harabasz_index: float, the Calinski-Harabasz Index of the clustering. # Perform K-Means clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) cluster_labels = kmeans.fit_predict(data) # Calculate evaluation metrics silhouette_score_value = silhouette_score(data, cluster_labels) calinski_harabasz_index = calinski_harabasz_score(data, cluster_labels) return list(cluster_labels), silhouette_score_value, calinski_harabasz_index"},{"question":"**Objective:** To assess your understanding of the `netrc` module and to demonstrate your ability to handle file parsing, exception handling, and data manipulation in Python. **Problem Statement:** You are provided with a `.netrc` file format used for storing FTP login credentials. Your task is to create a Python function that reads a specified `.netrc` file, extracts all available host information, and returns a dictionary where each key is a hostname and each value is a tuple containing (login, account, password). Additionally, if any parsing error occurs, your function should log the error details (error message, file name, and line number) without terminating the script. Write a function `parse_netrc_file(file_path: str) -> dict` that accomplishes this. **Function Signature:** ```python def parse_netrc_file(file_path: str) -> dict: pass ``` **Input:** - `file_path` (str): A string representing the path to the `.netrc` file. **Output:** - A dictionary where keys are hostnames and values are tuples `(login, account, password)`. If the file contains a \'default\' entry, it should be included with the key \'default\'. **Constraints:** - The `.netrc` file follows the proper format and security constraints as described in the `netrc` documentation. - Safeguards must be put in place to catch and log `NetrcParseError`. **Example:** If you are given a `.netrc` file with the following content: ``` machine host1 login user1 password pass1 machine host2 login user2 password pass2 default login default_user password default_pass ``` Your function should return: ```python { \'host1\': (\'user1\', None, \'pass1\'), \'host2\': (\'user2\', None, \'pass2\'), \'default\': (\'default_user\', None, \'default_pass\') } ``` **Example Usage:** ```python file_path = \'/path/to/.netrc\' credentials = parse_netrc_file(file_path) print(credentials) ``` **Points to consider:** - Use proper error handling to catch and log `netrc.NetrcParseError`. - Make sure to handle default entries appropriately. - Ensure your implementation adheres to POSIX security behaviors if applicable.","solution":"import netrc import logging def parse_netrc_file(file_path: str) -> dict: Reads a specified `.netrc` file, extracts all available host information, and returns a dictionary where each key is a hostname and each value is a tuple containing (login, account, password). Parameters: file_path (str): A string representing the path to the `.netrc` file. Returns: dict: A dictionary with hostnames as keys and (login, account, password) as values. try: netrc_obj = netrc.netrc(file_path) credentials = {} for host, info in netrc_obj.hosts.items(): credentials[host] = (info[0], info[1], info[2]) if netrc_obj.macros: for macro, info in netrc_obj.macros.items(): credentials[macro] = (\'macro_string\', None, \'macro_commands\') return credentials except netrc.NetrcParseError as e: logging.error(f\\"NetrcParseError: {e} in file \'{file_path}\' on line {e.lineno}\\") return {}"},{"question":"Implementing Dynamic Shapes in PyTorch Objective The goal of this exercise is to demonstrate your understanding of dynamic shapes in PyTorch. You will need to write code that dynamically handles varying tensor shapes, utilizing symbolic tracking and guards to manage these dynamic sizes. Problem Statement You are provided with a function that processes batches of tensors. Each batch can have varying sizes and sequence lengths. Your task is to implement a function `process_dynamic_batch` using PyTorch dynamic shapes where the function handles tensors with dynamic batch sizes and sequence lengths efficiently. Function Signature ```python import torch def process_dynamic_batch(batch_tensors: torch.Tensor) -> torch.Tensor: pass ``` Input - `batch_tensors`: A 3-dimensional tensor of shape `(batch_size, sequence_length, feature_dimension)`, where `batch_size` and `sequence_length` are dynamic and may vary with each function call. `feature_dimension` is fixed and known. Output - Returns a processed tensor, maintaining equivalent dimensions as the input batch. The processing includes normalizing the input and applying an element-wise operation that branches based on the sequence length of each batch. Constraints - You must use PyTorch dynamic shapes utilities to handle the varying `batch_size` and `sequence_length`. - Implement symbolic tracking to ensure efficiency. - The function should avoid unnecessary recompilations. Example ```python # Assume the feature_dimension is 10 input_tensor = torch.randn(5, 8, 10) # A batch of 5 sequences, each of length 8 and feature dimension 10 # Expected to return processed tensor of same shape output_tensor = process_dynamic_batch(input_tensor) print(output_tensor.shape) # Should print torch.Size([5, 8, 10]) ``` Implementation Note Here is a basic structure to guide your implementation: 1. Mark the dynamic dimensions of the input tensor using `torch._dynamo.mark_dynamic`. 2. Trace through the function to handle branching conditions based on dynamic shapes and add necessary guards. 3. Apply normalization and an element-wise operation based on dynamic sequence lengths. Performance Requirements - Ensure that the implementation aggregates symbolic sizes properly to avoid runtime errors. - Verify the implementation by testing with varying batch sizes and sequence lengths without recompilation errors.","solution":"import torch def process_dynamic_batch(batch_tensors: torch.Tensor) -> torch.Tensor: Processes batches of tensors with dynamic shapes. Args: - batch_tensors (torch.Tensor): 3D tensor of shape (batch_size, sequence_length, feature_dimension) Returns: - torch.Tensor: Processed tensor of the same shape. batch_size, sequence_length, feature_dimension = batch_tensors.shape # Normalize the input tensor along the feature dimension normalized_tensor = batch_tensors / torch.norm(batch_tensors, dim=-1, keepdim=True) # Apply an element-wise operation based on the sequence length if sequence_length % 2 == 0: processed_tensor = torch.sin(normalized_tensor) else: processed_tensor = torch.cos(normalized_tensor) return processed_tensor"},{"question":"Question: UUIDs in Action You are tasked with creating a utility function for handling UUIDs as per RFC 4122. This function should be able to generate UUIDs using different specified methods and analyze the properties of these UUIDs. **Function Signature:** ```python def uuid_utility(action: str, version: int = 4, namespace: str = None, name: str = None) -> dict: ``` **Parameters:** - `action` (str): Indicates the action to perform. It can be one of the following: - `\'generate\'`: Generate a UUID. - `\'analyze\'`: Analyze a given UUID. - `version` (int): Version of the UUID to generate. Valid only if `action` is `\'generate\'`. It can be 1, 3, 4, or 5. Default is 4. - `namespace` (str): The namespace UUID in string format for versions 3 and 5. Must be provided only if `version` is 3 or 5. - `name` (str): The name string to generate a UUID from, for versions 3 and 5. Must be provided only if `version` is 3 or 5. **Returns:** - A dictionary containing the following keys and their corresponding values: - For `\'generate\'` action: - `\'uuid\'`: The generated UUID as a string. - `\'version\'`: The version of the UUID. - For `\'analyze\'` action: - `\'bytes\'`: The UUID as a 16-byte string. - `\'variant\'`: The UUID variant. - `\'version\'`: The UUID version. - `\'is_safe\'`: An enumeration indicating if the UUID was generated in a multiprocessing-safe way. - `\'fields\'`: A tuple of the six integer fields of the UUID. **Constraints:** - If `action` is `\'generate\'`, the `version` must be one of 1, 3, 4, or 5. - If `version` is 3 or 5, both `namespace` and `name` must be provided and valid. - The `namespace` string must be a valid UUID. **Example Usage:** ```python # Generate a random UUID (version 4) print(uuid_utility(\'generate\')) # Generate a UUID based on MD5 hash (version 3) print(uuid_utility(\'generate\', version=3, namespace=\'12345678-1234-5678-1234-567812345678\', name=\'example\')) # Analyze a given UUID print(uuid_utility(\'analyze\', namespace=\'12345678-1234-5678-1234-567812345678\')) ``` Implement the function `uuid_utility` that performs the described operations.","solution":"import uuid def uuid_utility(action: str, version: int = 4, namespace: str = None, name: str = None) -> dict: Handles UUID operations either to generate or analyze UUIDs. Parameters: - action (str): Action to perform (\'generate\' or \'analyze\') - version (int): Version of the UUID to generate (1, 3, 4 or 5) - namespace (str): Namespace UUID (required for versions 3 and 5) - name (str): Name string to generate UUID (required for versions 3 and 5) Returns: - dict: Result of the operation containing corresponding details. if action == \'generate\': if version == 1: generated_uuid = uuid.uuid1() elif version == 3: if namespace is None or name is None: raise ValueError(\\"Namespace and name must be provided for version 3 UUID\\") namespace_uuid = uuid.UUID(namespace) generated_uuid = uuid.uuid3(namespace_uuid, name) elif version == 4: generated_uuid = uuid.uuid4() elif version == 5: if namespace is None or name is None: raise ValueError(\\"Namespace and name must be provided for version 5 UUID\\") namespace_uuid = uuid.UUID(namespace) generated_uuid = uuid.uuid5(namespace_uuid, name) else: raise ValueError(\\"Invalid version value. Must be 1, 3, 4, or 5.\\") return { \'uuid\': str(generated_uuid), \'version\': generated_uuid.version } elif action == \'analyze\': if namespace is None: raise ValueError(\\"Namespace must be provided for analyzing UUID\\") uuid_to_analyze = uuid.UUID(namespace) return { \'bytes\': uuid_to_analyze.bytes, \'variant\': uuid_to_analyze.variant, \'version\': uuid_to_analyze.version, \'is_safe\': uuid_to_analyze.is_safe, \'fields\': uuid_to_analyze.fields } else: raise ValueError(\\"Invalid action. Must be \'generate\' or \'analyze\'\\")"},{"question":"# Advanced Configuration File Parsing with `ConfigParser` Problem Statement You are provided with a configuration file (`config.ini`) with the following sections and key-values: ``` [General] version = 1.2.3 default_path = /usr/local/bin [Database] host = localhost port = 5432 user = admin password = secret [Logging] level = INFO output = /var/logs/app.log [Features] enable_feature_x = yes enable_feature_y = no ``` You are required to write a Python function `parse_config(file_path: str) -> dict` that: 1. Reads and parses the provided configuration file using the `ConfigParser` class. 2. Extracts all the values from the configuration file and returns them in a nested dictionary format, preserving sections as top-level keys. 3. Ensures the boolean values in the `[Features]` section are converted to proper Python boolean type (`True` or `False`). 4. Provides a default value for any missing configuration in the `[Logging]` section: the `level` should default to `\'WARNING\'` and `output` should default to `\'/var/logs/default.log\'`. Function Signature ```python def parse_config(file_path: str) -> dict: pass ``` Constraints and Assumptions - Assume the configuration file follows a proper structure and will always be provided at a valid path. - The function should handle case insensitivity for configuration keys correctly. - The function should handle missing configurations gracefully with provided defaults. Example Usage ```python config_data = parse_config(\'config.ini\') # Expected Output { \'General\': { \'version\': \'1.2.3\', \'default_path\': \'/usr/local/bin\' }, \'Database\': { \'host\': \'localhost\', \'port\': 5432, \'user\': \'admin\', \'password\': \'secret\' }, \'Logging\': { \'level\': \'INFO\', # uses provided value \'output\': \'/var/logs/app.log\' }, \'Features\': { \'enable_feature_x\': True, \'enable_feature_y\': False } } ``` Hints - Utilize the `configparser.ConfigParser` class to load and parse the configuration file. - The `getboolean` method of `ConfigParser` can be useful for boolean conversions. - The `get` method with the `fallback` argument can handle default values. - Ensure appropriate exception handling when reading the configuration file.","solution":"import configparser def parse_config(file_path: str) -> dict: config = configparser.ConfigParser() config.read(file_path) result = {} # Section: General if \'General\' in config: result[\'General\'] = { \'version\': config.get(\'General\', \'version\'), \'default_path\': config.get(\'General\', \'default_path\') } # Section: Database if \'Database\' in config: result[\'Database\'] = { \'host\': config.get(\'Database\', \'host\'), \'port\': config.getint(\'Database\', \'port\'), \'user\': config.get(\'Database\', \'user\'), \'password\': config.get(\'Database\', \'password\') } # Section: Logging if \'Logging\' in config: result[\'Logging\'] = { \'level\': config.get(\'Logging\', \'level\', fallback=\'WARNING\'), \'output\': config.get(\'Logging\', \'output\', fallback=\'/var/logs/default.log\') } else: result[\'Logging\'] = { \'level\': \'WARNING\', \'output\': \'/var/logs/default.log\' } # Section: Features if \'Features\' in config: result[\'Features\'] = { \'enable_feature_x\': config.getboolean(\'Features\', \'enable_feature_x\'), \'enable_feature_y\': config.getboolean(\'Features\', \'enable_feature_y\') } return result"},{"question":"# Concurrent Matrix Multiplication with Multiprocessing Problem Statement You are required to implement a function to perform matrix multiplication using Python\'s `multiprocessing` module to parallelize the computation. Function Signature ```python def parallel_matrix_multiplication(matrix_a: List[List[int]], matrix_b: List[List[int]]) -> List[List[int]]: pass ``` Description 1. **Input:** - `matrix_a`: A 2D list of integers representing the first matrix (size MxN). - `matrix_b`: A 2D list of integers representing the second matrix (size NxP). 2. **Output:** - A 2D list of integers representing the resulting matrix (size MxP) from the multiplication of `matrix_a` and `matrix_b`. 3. **Constraints:** - The number of columns in `matrix_a` (N) must be equal to the number of rows in `matrix_b` (N). - Elements in all matrices are integers in the range [-1000, 1000]. - Both matrices will have dimensions that do not exceed 500x500. 4. **Requirements:** - You must use Python\'s `multiprocessing` module to implement this task. Specifically, use the `Process` class to parallelize the computation of matrix elements. Example ```python matrix_a = [ [1, 2, 3], [4, 5, 6] ] matrix_b = [ [7, 8], [9, 10], [11, 12] ] result = parallel_matrix_multiplication(matrix_a, matrix_b) print(result) # Output: [[58, 64], [139, 154]] ``` Notes - Implement the function such that each element in the resulting matrix is computed in a separate process. - Gather results from all processes and combine them to form the final result matrix. - Ensure proper synchronization and sharing of results between processes using appropriate `multiprocessing` mechanisms such as `Queue` or `Manager`. Hints - Consider dividing the task such that each process computes a single element in the result matrix. - Use a `Queue` or a `Manager` list to collect results from different processes. Performance Considerations - Your implementation should efficiently handle the maximum allowed matrix dimensions (500x500) without running into performance bottlenecks. - Avoid creating an excessive number of processes that could overwhelm system resources.","solution":"from multiprocessing import Process, Manager from typing import List def compute_element(result, row, col, matrix_a, matrix_b): Compute a single element (dot product) for the resulting matrix. result[row * len(matrix_b[0]) + col] = sum(matrix_a[row][k] * matrix_b[k][col] for k in range(len(matrix_a[0]))) def parallel_matrix_multiplication(matrix_a: List[List[int]], matrix_b: List[List[int]]) -> List[List[int]]: if len(matrix_a[0]) != len(matrix_b): raise ValueError(\\"Number of columns in matrix_a must be equal to number of rows in matrix_b.\\") rows_a, cols_a = len(matrix_a), len(matrix_a[0]) rows_b, cols_b = len(matrix_b), len(matrix_b[0]) manager = Manager() result = manager.list([0] * (rows_a * cols_b)) # Shared list to store result processes = [] for i in range(rows_a): for j in range(cols_b): p = Process(target=compute_element, args=(result, i, j, matrix_a, matrix_b)) processes.append(p) p.start() for p in processes: p.join() # Convert flat result list to 2D list result_matrix = [] for i in range(rows_a): row = result[i * cols_b:(i + 1) * cols_b] result_matrix.append(list(row)) return result_matrix"},{"question":"**Challenging Coding Assessment Question:** Given your knowledge of the `curses.panel` module, create a Python program that simulates a basic console-based user interface using panels. Your task is to implement a function `create_dashboard_layout` which sets up and manages a set of panels to represent a simple dashboard layout. The dashboard should contain a header, a footer, a navigation menu on the left, and a main content area. # Function Signature ```python def create_dashboard_layout(stdscr): pass ``` # Requirements 1. **Function Definition**: * The function takes one argument `stdscr`, which is the main window object that `curses.wrapper` will pass to it. 2. **Dashboard Layout**: * Header panel at the top. * Footer panel at the bottom. * Navigation menu panel on the left side. * Main content panel occupying the remaining space. 3. **Functional Operations**: * All panels should be stacked correctly with their content visible. * Implement functionality to switch focus between the panels using keyboard arrow keys. * Update the panels after each switch to show which panel is currently active. 4. **Panel Content**: * Each panel should display a border and a label (e.g., \\"Header\\", \\"Footer\\", \\"Navigation\\", and \\"Main Content\\"). * The active panel should be highlighted in some way. # Constraints * Ensure the function works within a standard terminal size of 80x24 characters. * The program should gracefully handle window resizing if possible. # Example Usage: ```python import curses def main(stdscr): # Clear screen stdscr.clear() create_dashboard_layout(stdscr) stdscr.refresh() curses.napms(5000) # Let it display for 5 seconds curses.endwin() curses.wrapper(main) ``` # Explanation Your implementation should leverage the functions and methods as described in the `curses.panel` documentation to manage the panels\' visibility and stacking. The interaction (focus switching) should demonstrate your understanding of how to manipulate the panel stack dynamically. The function `create_dashboard_layout` doesn\'t return anything but directly modifies the panels associated with `stdscr`. Ensure your solution is robust and follows best practices for panel management.","solution":"import curses import curses.panel def create_dashboard_layout(stdscr): curses.curs_set(0) sh, sw = stdscr.getmaxyx() header_h, header_w = 3, sw footer_h, footer_w = 3, sw nav_h, nav_w = sh - header_h - footer_h, 20 main_h, main_w = nav_h, sw - nav_w header_win = curses.newwin(header_h, header_w, 0, 0) footer_win = curses.newwin(footer_h, footer_w, sh - footer_h, 0) nav_win = curses.newwin(nav_h, nav_w, header_h, 0) main_win = curses.newwin(main_h, main_w, header_h, nav_w) header_panel = curses.panel.new_panel(header_win) footer_panel = curses.panel.new_panel(footer_win) nav_panel = curses.panel.new_panel(nav_win) main_panel = curses.panel.new_panel(main_win) header_win.box() footer_win.box() nav_win.box() main_win.box() header_win.addstr(1, 2, \\"Header\\") footer_win.addstr(1, 2, \\"Footer\\") nav_win.addstr(1, 2, \\"Navigation\\") main_win.addstr(1, 2, \\"Main Content\\") panels = [header_panel, footer_panel, nav_panel, main_panel] headers = [\'Header\', \'Footer\', \'Navigation\', \'Main Content\'] active_idx = 0 while True: for idx, panel in enumerate(panels): win = panel.window() if idx == active_idx: win.attron(curses.A_BOLD) win.border(\'|\', \'|\', \'-\', \'-\', \'+\', \'+\', \'+\', \'+\') win.attroff(curses.A_BOLD) else: win.box() win.addstr(1, 2, headers[idx]) stdscr.refresh() curses.panel.update_panels() stdscr.getch() ch = stdscr.getch() if ch == curses.KEY_UP: active_idx = (active_idx - 1) % len(panels) elif ch == curses.KEY_DOWN: active_idx = (active_idx + 1) % len(panels) elif ch == 27: break # Exit on ESC key curses.endwin()"}]'),D={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},R={class:"card-container"},F={key:0,class:"empty-state"},L=["disabled"],O={key:0},N={key:1};function q(s,e,l,h,i,o){const m=g("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"ðŸ¤”prompts chatðŸ§ ")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"ðŸ”",-1)),_(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," âœ• ")):d("",!0)]),t("div",R,[(a(!0),n(b,null,v(o.displayedPoems,(r,f)=>(a(),w(m,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),n("div",F,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),n("span",N,"Loading...")):(a(),n("span",O,"See more"))],8,L)):d("",!0)])}const M=p(D,[["render",q],["__scopeId","data-v-9ccc676f"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"chatai/55.md","filePath":"chatai/55.md"}'),j={name:"chatai/55.md"},V=Object.assign(j,{setup(s){return(e,l)=>(a(),n("div",null,[x(M)]))}});export{Y as __pageData,V as default};
