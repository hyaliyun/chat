import{_ as p,o as a,c as i,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(s,e,l,m,n,o){return a(),i("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-23bd5c28"]]),z=JSON.parse('[{"question":"# WAV File Processing and Modification You are provided with a WAV file and tasked with creating a Python script that processes this audio file. Your script needs to read the WAV file, perform basic transformations, and write the result to a new WAV file. Requirements 1. **Input File**: The input WAV file\'s name will be passed as a command-line argument. 2. **Output File**: The output WAV file should be named `output.wav` and saved in the current working directory. 3. **Processing**: - Reverse the audio data (i.e., play it backwards). - Ensure the output WAV file has the same properties (number of channels, sample width, frame rate) as the input file. Function Signature ```python def process_wav(input_filename: str) -> None: # Your code to read, process, and write the WAV file pass ``` Example If the input WAV file has the following properties: - Number of Channels: 2 - Sample Width: 2 bytes - Frame Rate: 44100 Hz - Number of Frames: 100000 After processing, `output.wav` should have the same properties but with the audio data reversed. Constraints - Only WAV files in PCM format will be provided. - Ensure that your script handles any file or read/write errors gracefully. Evaluation Your solution will be evaluated based on: - Correctness: Properly reversing the audio data. - File Integrity: Maintaining the original properties of the WAV file. - Error Handling: Gracefully managing errors (invalid file paths, read/write errors). Example Usage ```bash python script.py input.wav ``` This command should read `input.wav`, process it, and save the reversed audio as `output.wav`. Notes - Use the `wave` module as described in the documentation for reading and writing WAV files. - Pay attention to resource management: ensure files are properly closed after processing.","solution":"import wave import sys def process_wav(input_filename: str) -> None: try: with wave.open(input_filename, \'rb\') as wav: params = wav.getparams() frames = wav.readframes(params.nframes) reversed_frames = frames[::-1] with wave.open(\'output.wav\', \'wb\') as out_wav: out_wav.setparams(params) out_wav.writeframes(reversed_frames) except FileNotFoundError: print(f\\"Error: The file {input_filename} was not found.\\") except wave.Error as e: print(f\\"Error processing WAV file: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") # Entry point for the script if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <input.wav>\\") else: process_wav(sys.argv[1])"},{"question":"**Pandas Options Management and DataFrame Display Customization** **Objective:** Your task is to write a function that customizes multiple pandas display options, computes some operations on a DataFrame, and returns the DataFrame using the customized display settings. The function should make use of context managers to ensure that the pandas options are reset to their default values after the operations. **Instructions:** Implement a function `customize_display(df: pd.DataFrame, operations: List[Callable], display_options: Dict[str, Any]) -> pd.DataFrame` that takes: 1. `df` (pd.DataFrame): The DataFrame on which operations will be performed. 2. `operations` (List[Callable]): A list of functions representing operations to be performed on the DataFrame. Each function should take a DataFrame as input and return a DataFrame. 3. `display_options` (Dict[str, Any]): A dictionary of pandas display options to set before performing the operations. The function should: 1. Use `pd.option_context` to set the `display_options`. 2. Apply each operation in the `operations` list sequentially on the DataFrame. 3. Ensure that pandas\' default display settings are restored after the operations. 4. Return the modified DataFrame. **Constraints:** - The DataFrame `df` will have at most 1000 rows and 100 columns. - The `operations` list will contain at most 5 functions. - The keys in `display_options` will be valid pandas display options (e.g., `\'display.max_rows\'`, `\'display.max_columns\'`). **Example:** ```python import pandas as pd def example_operation(df): # Sample operation: Filter rows where the first column value is greater than 0 return df[df[df.columns[0]] > 0] def customize_display(df: pd.DataFrame, operations: List[Callable], display_options: Dict[str, Any]) -> pd.DataFrame: with pd.option_context(*sum([[key, value] for key, value in display_options.items()], [])): for operation in operations: df = operation(df) return df # Sample DataFrame data = {\'A\': [1, -1, 3, -3], \'B\': [4, 5, -2, 0]} df = pd.DataFrame(data) # Display options and operations display_opts = {\'display.max_rows\': 5, \'display.precision\': 3} operations = [example_operation] # Apply customization customized_df = customize_display(df, operations, display_opts) print(customized_df) ``` **Note:** - In your implementation, ensure all display options are reverted to their original values after the function execution, even if an error occurs during the operations. - You may assume that the `operations` list only contains functions that take and return a DataFrame. **Requirements:** - Good understanding of pandas options and settings API. - Proficiency in handling DataFrame operations and context managers in pandas. - Ability to write robust and clean code that handles potential exceptions.","solution":"import pandas as pd from typing import List, Callable, Dict, Any def example_operation(df: pd.DataFrame) -> pd.DataFrame: # Sample operation: Filter rows where the first column value is greater than 0 return df[df[df.columns[0]] > 0] def customize_display(df: pd.DataFrame, operations: List[Callable], display_options: Dict[str, Any]) -> pd.DataFrame: Customize pandas display options, apply operations, and return modified DataFrame while reverting display options to default after execution. :param df: Input DataFrame :param operations: List of functions to be applied on the DataFrame :param display_options: Dictionary of pandas display options to set :return: Modified DataFrame after applying operations with pd.option_context(*sum([[key, value] for key, value in display_options.items()], [])): for operation in operations: df = operation(df) return df"},{"question":"<|Analysis Begin|> The provided documentation explains how to handle a specific issue with Batch Normalization (BatchNorm) when dealing with functorch\'s `vmap`. It describes various options to replace or modify BatchNorm to avoid unsupported in-place updates to running statistics. The following points are covered: 1. **Replacing BatchNorm with GroupNorm**: This involves changing the BatchNorm layer to GroupNorm, with appropriate configuration to ensure compatibility. 2. **Using torchvision parameters**: Some torchvision models allow altering the default normalization layer using parameters. 3. **Using functorch\'s utility**: A function to replace all instances of BatchNorm with a version that does not use running statistics. 4. **Using eval mode**: Utilize the model\'s evaluation mode to run without updating running statistics. The documentation provides detailed options and code snippets for each approach, indicating a fair understanding of handling normalization layers in PyTorch models. Given this information, we can design a coding question that assesses the understanding of BatchNorm and GroupNorm, as well as practical application of replacing and configuring these layers within a PyTorch model. <|Analysis End|> <|Question Begin|> # Question: Handling Batch Normalization with functorch’s `vmap` You are working on a deep learning project using PyTorch and functorch. Due to certain constraints with functorch’s `vmap`, you need to ensure that Batch Normalization (BatchNorm) layers in your model are either replaced or modified to avoid using running statistics during training. Your task is to implement a function that updates all BatchNorm layers within a given model to use GroupNorm with a specified number of groups. Function Signature ```python def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: Replaces all BatchNorm layers in the given model with GroupNorm layers. Parameters: model (torch.nn.Module): The input PyTorch model. num_groups (int): The number of groups to use for GroupNorm. Returns: torch.nn.Module: The updated model with GroupNorm layers. pass ``` Input - `model`: A PyTorch neural network model (`torch.nn.Module`) that may contain multiple BatchNorm layers. - `num_groups`: An integer representing the number of groups to use in the GroupNorm layers. This value should ensure that the number of channels in each replaced BatchNorm layer is divisible by `num_groups`. Output - An updated PyTorch model where all BatchNorm layers are replaced with GroupNorm layers, configured to use the specified `num_groups`. Constraints 1. The function should recursively traverse the model to find and replace all instances of BatchNorm. 2. Each instance of `BatchNorm` (e.g., `torch.nn.BatchNorm1d`, `torch.nn.BatchNorm2d`, `torch.nn.BatchNorm3d`) should be replaced by a corresponding `GroupNorm` layer. 3. The number of channels (C) in the BatchNorm layer should be divided evenly by `num_groups`. Example ```python import torch import torch.nn as nn # Define a simple model with BatchNorm layers class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.relu = nn.ReLU() self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) self.fc = nn.Linear(32, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.conv2(x) x = self.bn2(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc(x) return x model = SimpleModel() # Replace BatchNorm layers with GroupNorm layers with 4 groups updated_model = replace_batchnorm_with_groupnorm(model, 4) # Check if BatchNorm layers are replaced print(updated_model) ``` In this example, the original `SimpleModel` contains two BatchNorm layers. After calling `replace_batchnorm_with_groupnorm`, these BatchNorm layers should be replaced with GroupNorm layers, ensuring compatibility with functorch\'s `vmap`. Note - You can use the function `torch.nn.GroupNorm` for creating GroupNorm layers. - Make sure to handle different types of BatchNorm layers (`BatchNorm1d`, `BatchNorm2d`, `BatchNorm3d`) appropriately. Performance Requirements - The function should efficiently traverse the model and replace layers within a reasonable time frame for typical deep learning models. Good luck!","solution":"import torch import torch.nn as nn def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: Replaces all BatchNorm layers in the given model with GroupNorm layers. Parameters: model (torch.nn.Module): The input PyTorch model. num_groups (int): The number of groups to use for GroupNorm. Returns: torch.nn.Module: The updated model with GroupNorm layers. def replace_module(module): for name, child in module.named_children(): if isinstance(child, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)): num_features = child.num_features assert num_features % num_groups == 0, \\"Number of features must be divisible by num_groups\\" group_norm = nn.GroupNorm(num_channels=num_features, num_groups=num_groups) setattr(module, name, group_norm) else: replace_module(child) replace_module(model) return model"},{"question":"**XML DOM Manipulation using `xml.dom.minidom`** # Problem Statement You are given an XML string representing a simple book catalog. Your task is to write a Python function that parses this XML string, updates certain elements, and then returns the updated XML string. # Task Details 1. **Parse the given XML string** into a DOM structure. 2. **Add a new book entry** to the catalog. The new book should have the following details: - `title`: \\"Learning Python\\" - `author`: \\"Mark Lutz\\" - `year`: \\"2022\\" - `price`: \\"44.99\\" 3. **Update the price** of the book titled \\"XML Developer\'s Guide\\" to \\"39.99\\". 4. **Return the updated XML string**. # Input - An XML string representing the book catalog. # Output - A string representing the updated XML. # Example ```python xml_input = <?xml version=\\"1.0\\"?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> </catalog> # Your function should return: <?xml version=\\"1.0\\" ?> <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>39.99</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> <book> <title>Learning Python</title> <author>Mark Lutz</author> <year>2022</year> <price>44.99</price> </book> </catalog> ``` # Constraints - Assume the input XML string is always well-formed. - Preserve the initial structure and order of other elements in the XML. - You are only allowed to use the `xml.dom.minidom` module for XML parsing and manipulation. # Function Signature ```python def update_catalog(xml_input: str) -> str: pass ```","solution":"from xml.dom.minidom import parseString, Element def update_catalog(xml_input: str) -> str: # Parse the given XML string dom = parseString(xml_input) # Get the catalog element catalog = dom.getElementsByTagName(\\"catalog\\")[0] # Create a new book entry new_book = dom.createElement(\\"book\\") title = dom.createElement(\\"title\\") title.appendChild(dom.createTextNode(\\"Learning Python\\")) new_book.appendChild(title) author = dom.createElement(\\"author\\") author.appendChild(dom.createTextNode(\\"Mark Lutz\\")) new_book.appendChild(author) year = dom.createElement(\\"year\\") year.appendChild(dom.createTextNode(\\"2022\\")) new_book.appendChild(year) price = dom.createElement(\\"price\\") price.appendChild(dom.createTextNode(\\"44.99\\")) new_book.appendChild(price) # Add the new book to the catalog catalog.appendChild(new_book) # Update the price of \\"XML Developer\'s Guide\\" books = catalog.getElementsByTagName(\\"book\\") for book in books: title = book.getElementsByTagName(\\"title\\")[0].firstChild.nodeValue if title == \\"XML Developer\'s Guide\\": price_element = book.getElementsByTagName(\\"price\\")[0] price_element.firstChild.nodeValue = \\"39.99\\" break # Convert the modified DOM tree to a string updated_xml = dom.toxml() return updated_xml"},{"question":"**Email Payload Encoder Implementation** *Objective*: Implement a function to encode email payloads with custom encoding functionality based on specific conditions. **Background**: You have learned about various encoding mechanisms provided by the `email.encoders` module in the legacy email API. This legacy module is designed to help encode email payloads for compliant mail server transmission by utilizing functions like `encode_quopri`, `encode_base64`, `encode_7or8bit`, and `encode_noop`. **Task**: Implement a function called `custom_encoder` that should: 1. Accept an email message object (msg) and an encoding type. 2. Depending on the encoding type passed (`\'custom_base32\'` or `\'noop\'`), it should encode the payload and set the appropriate `Content-Transfer-Encoding` header. 3. If the encoding type is `\'custom_base32\'`, encode the payload into Base32 form and set the `Content-Transfer-Encoding` header to `\'base32\'`. 4. If the encoding type is `\'noop\'`, do nothing to the payload, similarly to `encode_noop`. **Specifications**: - **Function Signature**: ```python def custom_encoder(msg: object, encoding_type: str) -> None: ``` - **Parameters**: - `msg` (object): The email message object to encode. - `encoding_type` (str): The type of encoding to apply. Expected values: `\'custom_base32\'`, `\'noop\'`. - **Constraints**: - You may assume `msg` is a valid email message object. - If any other encoding type is passed, raise a `ValueError` with the message \\"Unsupported encoding type\\". - **Output**: The function does not return anything. It modifies the `msg` object in place. **Example**: ```python from email.message import Message # Sample email message object creation msg = Message() msg.set_payload(\\"This is a sample payload.\\") # Using the custom_encoder function to apply custom_base32 encoding custom_encoder(msg, \'custom_base32\') print(msg.get_payload()) # Expected to print the Base32 encoded payload print(msg[\'Content-Transfer-Encoding\']) # Expected to print \'base32\' # Using the custom_encoder function with the noop encoding custom_encoder(msg, \'noop\') print(msg.get_payload()) # Expected to print the original payload print(msg[\'Content-Transfer-Encoding\']) # Expected to print None or original header value ``` **Hints**: - You can use Python\'s built-in `base64` library to help with Base32 encoding (`base64.b32encode` for Base32). **Notes**: - Ensure to handle `msg` objects that do not support the encoding transform gracefully with appropriate error messages. - This task involves understanding of string manipulation and comprehension of email standards. **Testing**: - You must write comprehensive tests to validate your solution. - Test with various payload types and ensure that the function behaves as expected.","solution":"import base64 from email.message import Message def custom_encoder(msg: Message, encoding_type: str) -> None: Encodes the email message payload based on the specified encoding type. Available encoding types: - \'custom_base32\': Encodes the payload in Base32 and sets Content-Transfer-Encoding header to \'base32\'. - \'noop\': Does nothing to the payload, similar to encode_noop. Parameters: - msg (Message): The email message object to encode. - encoding_type (str): The type of encoding to apply. Raises: - ValueError: If encoding type is unsupported. if encoding_type == \'custom_base32\': original_payload = msg.get_payload() if isinstance(original_payload, str): encoded_payload = base64.b32encode(original_payload.encode()).decode() msg.set_payload(encoded_payload) msg[\'Content-Transfer-Encoding\'] = \'base32\' else: raise ValueError(\\"Unsupported payload type for Base32 encoding. Only string payloads are supported.\\") elif encoding_type == \'noop\': pass # Do nothing to the payload else: raise ValueError(\\"Unsupported encoding type\\")"},{"question":"# Question **Visualize Restaurant Tips Data** You are tasked with creating a visualization using the seaborn library. This visualization should provide insights into the relationship between various factors and the tips received at a restaurant. You will use the `tips` dataset available in seaborn for this task. Follow the steps outlined below to achieve this. 1. **Load the Dataset** - Load the `tips` dataset using seaborn. 2. **Create a FacetGrid** - Create a `FacetGrid` with subplots organized by `day` (column variable) and `time` (row variable). 3. **Map Plotting Functions** - On each subplot, create a scatter plot showing the relationship between `total_bill` and `tip`. - Use different colors to distinguish between different sizes of dining parties (`size` column) using the `hue` parameter. 4. **Add Reference Lines** - Add a horizontal reference line indicating the median tip value across all subplots. 5. **Customize the Plot** - Set the x-axis label to \\"Total bill ()\\" and the y-axis label to \\"Tip ()\\". - Set custom titles for each subplot indicating the `day` and `time` (e.g., \\"Sunday Lunch\\", \\"Saturday Dinner\\"). - Adjust the x and y axis limits to appropriate ranges based on the data distribution. - Make the layout tight using the `tight_layout()` method. 6. **Save the Plot** - Save the final plot as \\"tips_facet_plot.png\\" in the current directory. # Constraints - Make sure your solution handles any missing values in the dataset before plotting. - Ensure that the color mapping consistency for the `hue` parameter is maintained across all subplots. # Input and Output - **Input:** None (the dataset is loaded internally). - **Output:** The plot should be saved as \\"tips_facet_plot.png\\". # Example Code Structure ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Data preprocessing (e.g., handle missing values) # [Your code here] # Create the FacetGrid g = sns.FacetGrid(...) # Map the scatter plot g.map_dataframe(...) # Add reference lines g.refline(...) # Customize the plot g.set_axis_labels(..., ...) g.set_titles(...) g.set(..., ..., ..., ...) g.tight_layout() # Save the plot g.savefig(\\"tips_facet_plot.png\\") ``` Complete the code to fulfill the requirements described above.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_restaurant_tips(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Handle missing values (if any) tips = tips.dropna() # Create the FacetGrid g = sns.FacetGrid(tips, col=\\"day\\", row=\\"time\\", hue=\\"size\\", margin_titles=True, height=4) # Map the scatter plot g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Calculate median tip for reference line median_tip = tips[\\"tip\\"].median() # Add reference lines for each subplot for ax in g.axes.flatten(): ax.axhline(median_tip, ls=\'--\', color=\'red\') # Customize the plot g.set_axis_labels(\\"Total bill ()\\", \\"Tip ()\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") g.set(xticks=np.arange(0, 60, 10), yticks=np.arange(0, 10, 2)) # Adjust layout g.tight_layout() # Save the plot g.savefig(\\"tips_facet_plot.png\\")"},{"question":"**Problem Statement:** You are tasked with building a machine learning model to classify gene expressions in mice brains using the \\"Mice Protein Expression\\" dataset from OpenML. Your goal is to fetch the dataset, preprocess the data, perform basic exploratory data analysis, and train a classifier to predict the classes of the mice. **Requirements:** 1. Fetch the \\"Mice Protein Expression\\" dataset from OpenML with `data_id=40966`. 2. Conduct basic exploratory data analysis to understand the dataset\'s structure and statistical properties. 3. Preprocess the dataset by: - Handling any missing values. - Encoding categorical features if present. 4. Split the dataset into training and testing sets. 5. Train a classifier (e.g., RandomForestClassifier) on the training set. 6. Evaluate the performance of your model on the testing set using appropriate evaluation metrics. **Input:** - No direct inputs. The task requires fetching the dataset directly from OpenML. **Output:** - A trained classifier model. - The evaluation metrics of the model\'s performance on the testing set. **Constraints:** - Use scikit-learn for all data processing and machine learning tasks. - The solution must handle missing values effectively. - The solution should use `train_test_split` from scikit-learn to split the data. **Performance Requirements:** - Aim for an accuracy score of at least 85% on the testing set. ```python import numpy as np import pandas as pd from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Step 1: Fetch the dataset from OpenML def fetch_mice_dataset(): mice = fetch_openml(data_id=40966, parser=\\"pandas\\") return mice.data, mice.target # Step 2: Basic exploratory data analysis (EDA) def eda(data, target): print(\\"Dataset shape:\\", data.shape) print(\\"Target shape:\\", target.shape) print(\\"Classes:\\", np.unique(target)) print(\\"Data preview:n\\", data.head()) print(\\"Missing values:n\\", data.isnull().sum()) # Step 3: Data preprocessing def preprocess_data(data): data = data.fillna(data.mean()) # Handling missing values # Further preprocessing steps if necessary return data # Step 4: Split the dataset into training and testing sets def split_data(data, target): X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test # Step 5: Train a classifier def train_classifier(X_train, y_train): classifier = RandomForestClassifier(random_state=42) classifier.fit(X_train, y_train) return classifier # Step 6: Evaluate the model def evaluate_model(classifier, X_test, y_test): y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Accuracy:\\", accuracy) return accuracy # Main function to execute the steps if __name__ == \\"__main__\\": data, target = fetch_mice_dataset() eda(data, target) data = preprocess_data(data) X_train, X_test, y_train, y_test = split_data(data, target) classifier = train_classifier(X_train, y_train) evaluate_model(classifier, X_test, y_test) ``` **Explain your solution and the choices you made while implementing it.**","solution":"import numpy as np import pandas as pd from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Step 1: Fetch the dataset from OpenML def fetch_mice_dataset(): mice = fetch_openml(data_id=40966, as_frame=True) return mice.data, mice.target # Step 2: Basic exploratory data analysis (EDA) def eda(data, target): print(\\"Dataset shape:\\", data.shape) print(\\"Target shape:\\", target.shape) print(\\"Classes:\\", np.unique(target)) print(\\"Data preview:n\\", data.head()) print(\\"Missing values:n\\", data.isnull().sum().sum()) # Step 3: Data preprocessing def preprocess_data(data): data = data.fillna(data.mean()) # Handling missing values # Further preprocessing steps if necessary return data # Step 4: Split the dataset into training and testing sets def split_data(data, target): X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test # Step 5: Train a classifier def train_classifier(X_train, y_train): classifier = RandomForestClassifier(random_state=42) classifier.fit(X_train, y_train) return classifier # Step 6: Evaluate the model def evaluate_model(classifier, X_test, y_test): y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Accuracy:\\", accuracy) return accuracy # Main function to execute the steps if __name__ == \\"__main__\\": data, target = fetch_mice_dataset() eda(data, target) data = preprocess_data(data) X_train, X_test, y_train, y_test = split_data(data, target) classifier = train_classifier(X_train, y_train) evaluate_model(classifier, X_test, y_test)"},{"question":"# Garbage Collection Optimization and Debugging in Python **Objective:** Your task is to implement a class that creates several objects, manages their references, and uses the garbage collector (gc) module to gain insights into the memory management and potential memory leaks. **Specifications:** 1. Create a class `MemoryManager` with the following features: - A method `create_objects(count)` that creates `count` number of objects and stores them in a list. - A method `release_objects()` that clears all references to objects created by `create_objects`. - A method `inspect_garbage()` that prints information about unreachable but non-freed objects. - A method `toggle_gc(enable)` that enables (`enable=True`) or disables (`enable=False`) the garbage collection. - A method `collect_garbage(generation)` that runs garbage collection on the specified generation and returns the number of unreachable objects found. 2. Your `MemoryManager` class should also track and print garbage collection statistics before and after object creation using appropriate methods in the `gc` module. 3. Implement a function `main()` that: - Creates an instance of `MemoryManager`. - Disables the garbage collection. - Creates 100,000 objects using `create_objects`. - Collects garbage and prints the number of unreachable objects found. - Enables the garbage collection. - Releases all created objects using `release_objects`. - Collects garbage again and prints the number of unreachable objects found. - Inspects and prints garbage using `inspect_garbage`. Your solution must demonstrate an understanding of object creation and reference management, as well as the effective use of the `gc` module to control and inspect garbage collection. **Constraints:** - Ensure that your code handles and logs exceptions gracefully. - Optimize performance to handle the creation and management of a large number of objects. **Input:** This task does not require any input from standard input. **Output:** ``` Number of unreachable objects found after creation: X Number of unreachable objects found after release: Y Garbage collector info: <Detailed information from inspect_garbage method> ``` **Performance Considerations:** - The class should be able to handle the creation of a large number of objects efficiently. - Implement appropriate measures to ensure smooth execution of garbage collection operations. **Example Implementation:** ```python import gc class MemoryManager: def __init__(self): self.objects = [] def create_objects(self, count): self.objects = [object() for _ in range(count)] print(f\\"Created {count} objects.\\") def release_objects(self): self.objects = [] print(\\"Released all created objects.\\") def inspect_garbage(self): garbage = gc.garbage print(\\"Inspecting garbage collected by gc.garbage:\\") for obj in garbage: print(obj) def toggle_gc(self, enable): if enable: gc.enable() print(\\"Garbage collection enabled.\\") else: gc.disable() print(\\"Garbage collection disabled.\\") def collect_garbage(self, generation): unreachable_count = gc.collect(generation) print(f\\"Collected {unreachable_count} unreachable objects in generation {generation}.\\") return unreachable_count def main(): manager = MemoryManager() # Disable garbage collection manager.toggle_gc(enable=False) # Create objects manager.create_objects(100000) # Collect garbage unreachable_after_creation = manager.collect_garbage(2) print(f\\"Number of unreachable objects found after creation: {unreachable_after_creation}\\") # Enable garbage collection manager.toggle_gc(enable=True) # Release objects manager.release_objects() # Collect garbage again unreachable_after_release = manager.collect_garbage(2) print(f\\"Number of unreachable objects found after release: {unreachable_after_release}\\") # Inspect garbage manager.inspect_garbage() if __name__ == \\"__main__\\": main() ```","solution":"import gc class MemoryManager: def __init__(self): self.objects = [] def create_objects(self, count): self.objects = [object() for _ in range(count)] print(f\\"Created {count} objects.\\") def release_objects(self): self.objects = [] print(\\"Released all created objects.\\") def inspect_garbage(self): garbage = gc.garbage print(\\"Inspecting garbage collected by gc.garbage:\\") for obj in garbage: print(obj) def toggle_gc(self, enable): if enable: gc.enable() print(\\"Garbage collection enabled.\\") else: gc.disable() print(\\"Garbage collection disabled.\\") def collect_garbage(self, generation=2): unreachable_count = gc.collect(generation) print(f\\"Collected {unreachable_count} unreachable objects in generation {generation}.\\") return unreachable_count def main(): manager = MemoryManager() # Disable garbage collection manager.toggle_gc(enable=False) # Create objects manager.create_objects(100000) # Collect garbage unreachable_after_creation = manager.collect_garbage() print(f\\"Number of unreachable objects found after creation: {unreachable_after_creation}\\") # Enable garbage collection manager.toggle_gc(enable=True) # Release objects manager.release_objects() # Collect garbage again unreachable_after_release = manager.collect_garbage() print(f\\"Number of unreachable objects found after release: {unreachable_after_release}\\") # Inspect garbage manager.inspect_garbage() if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implement a Custom Documentation Generator Using `pydoc` **Objective:** To assess your understanding of the `pydoc` module and its functionalities in Python, you need to implement a function that generates documentation for a given module or package and save it either as a text file or serve it via an HTTP server. **Instructions:** 1. **Create a function `generate_documentation(module_name: str, output_format: str, port: int = 0) -> None`:** - **Parameters:** - `module_name` (str): The name of the module or package for which to generate documentation. - `output_format` (str): The format in which to output the documentation. Acceptable values are `\\"text\\"`, `\\"html\\"`, and `\\"http\\"`. - `\\"text\\"`: Save the documentation as a text file named `<module_name>_doc.txt`. - `\\"html\\"`: Save the documentation as an HTML file named `<module_name>_doc.html`. - `\\"http\\"`: Serve the documentation on an HTTP server. If `port` is specified, the server should listen on that port. If `port` is `0`, an arbitrary unused port should be selected. - `port` (int, optional): The port on which to serve the HTTP server. This parameter is only relevant if `output_format` is `\\"http\\"`. Default is `0`. 2. **Behavior:** - The function should automatically generate the appropriate documentation using the `pydoc` module based on the provided parameters. - Handle invalid module names and output formats by raising a `ValueError` with an appropriate message. - Ensure that when using the `\\"http\\"` format, the documentation is accessible via a web browser at the specified or selected port. 3. **Constraints:** - Do not use any external libraries other than `pydoc`. - Ensure that the generated files or HTTP server are properly cleaned up after testing (where applicable). 4. **Example Usage:** ```python # Output documentation as text generate_documentation(\\"sys\\", \\"text\\") # Output documentation as HTML generate_documentation(\\"sys\\", \\"html\\") # Serve documentation via HTTP on port 1234 generate_documentation(\\"sys\\", \\"http\\", port=1234) ``` **Note:** Focus on correctly using the `pydoc` module\'s functionality to generate and serve documentation. This exercise assesses your ability to understand and apply the `pydoc` module\'s capabilities effectively.","solution":"import pydoc import os from http.server import HTTPServer, SimpleHTTPRequestHandler def generate_documentation(module_name: str, output_format: str, port: int = 0) -> None: Generates documentation for a given module or package. Parameters: - module_name (str): The name of the module or package for which to generate documentation. - output_format (str): The format in which to output the documentation. Acceptable values are \\"text\\", \\"html\\", or \\"http\\". - port (int, optional): The port on which to serve the HTTP server. This parameter is only relevant if output_format is \\"http\\". Default is 0. try: module = __import__(module_name) except ImportError: raise ValueError(f\\"Module {module_name} not found\\") if output_format == \\"text\\": with open(f\\"{module_name}_doc.txt\\", \\"w\\") as f: f.write(pydoc.render_doc(module, renderer=pydoc.plaintext)) elif output_format == \\"html\\": with open(f\\"{module_name}_doc.html\\", \\"w\\") as f: f.write(pydoc.HTMLDoc().docmodule(module)) elif output_format == \\"http\\": handler = pydoc._make_server(module_name, SimpleHTTPRequestHandler, port) print(f\\"Serving documentation for {module_name} at http://localhost:{handler.server_port}\\") handler.serve_forever() else: raise ValueError(f\\"Invalid output format: {output_format}\\")"},{"question":"# Coding Assessment: Custom Attribute and Comparison Management Objective Implement a custom class that demonstrates the usage of advanced attribute handling and rich comparison operations in Python. This task is designed to test your depth of understanding of Python\'s object protocol and comparison operations. Description Create a class `AdvancedObject` that allows: 1. Dynamic setting, getting, and deleting of attributes via custom methods. 2. Rich comparison operations, allowing instances to be compared using all standard comparison operators (`<`, `<=`, `==`, `!=`, `>`, `>=`). Requirements 1. **Dynamic Attribute Management**: - Implement methods to set, get, and delete attributes dynamically using the Python `__getattr__`, `__setattr__`, and `__delattr__` magic methods. - Add a method `has_attribute(name)` that returns `True` if the attribute exists, `False` otherwise. 2. **Rich Comparison**: - Implement rich comparison operators to compare instances of `AdvancedObject` based on a predefined attribute (`value`). If the attribute does not exist, comparisons should raise an appropriate exception. Constraints - Only use standard Python libraries. - Ensure your implementation is efficient and handles edge cases (e.g., attribute not found). Class Definition ```python class AdvancedObject: def __init__(self): pass def has_attribute(self, name: str) -> bool: # Check if attribute exists pass def __getattr__(self, name): # Get attribute dynamically pass def __setattr__(self, name, value): # Set attribute dynamically pass def __delattr__(self, name): # Delete attribute dynamically pass def __lt__(self, other): # Comparison operator < pass def __le__(self, other): # Comparison operator <= pass def __eq__(self, other): # Comparison operator == pass def __ne__(self, other): # Comparison operator != pass def __gt__(self, other): # Comparison operator > pass def __ge__(self, other): # Comparison operator >= pass ``` Input / Output Format - **No direct input/output functions**: The class `AdvancedObject` should be implemented following the described methods and functionality. - Usage examples will be provided to demonstrate expected behaviors. Example Usage ```python # Create instances obj1 = AdvancedObject() obj2 = AdvancedObject() # Set attributes obj1.value = 10 obj2.value = 20 # Comparison based on attributes print(obj1 < obj2) # Should return True print(obj1 == obj2) # Should return False print(obj1 > obj2) # Should return False # Check for attributes print(obj1.has_attribute(\'value\')) # Should return True print(obj2.has_attribute(\'missing\')) # Should return False # Delete attribute del obj1.value # Error handling example try: print(obj1 < obj2) # Should raise an AttributeError since \'value\' is deleted except AttributeError as e: print(e) ``` Implement the `AdvancedObject` class in Python to meet the above requirements. Ensure to handle all possible edge cases and exceptions.","solution":"class AdvancedObject: def __init__(self): self.__dict__[\'attributes\'] = {} def has_attribute(self, name: str) -> bool: return name in self.attributes def __getattr__(self, name): try: return self.attributes[name] except KeyError: raise AttributeError(f\\"Attribute \'{name}\' not found\\") def __setattr__(self, name, value): self.attributes[name] = value def __delattr__(self, name): try: del self.attributes[name] except KeyError: raise AttributeError(f\\"Attribute \'{name}\' not found\\") def __compare(self, other, method): if not isinstance(other, AdvancedObject): return NotImplemented if \'value\' not in self.attributes or \'value\' not in other.attributes: raise AttributeError(\\"One or both objects do not have a \'value\' attribute\\") return method(self.attributes[\'value\'], other.attributes[\'value\']) def __lt__(self, other): return self.__compare(other, lambda x, y: x < y) def __le__(self, other): return self.__compare(other, lambda x, y: x <= y) def __eq__(self, other): return self.__compare(other, lambda x, y: x == y) def __ne__(self, other): return self.__compare(other, lambda x, y: x != y) def __gt__(self, other): return self.__compare(other, lambda x, y: x > y) def __ge__(self, other): return self.__compare(other, lambda x, y: x >= y)"},{"question":"**Question: Comprehensive File Compression and Archive Handling** In this task, you are required to write a Python function that takes a directory path as input, compresses all the files within the directory using the bzip2 algorithm, and then archives the compressed files into a single ZIP file. Additionally, the function should be able to extract the contents of the compressed ZIP file into a specified directory and decompress the files back to their original state. # Function Signature ```python def compress_and_archive(input_dir: str, zip_output: str, extract_dir: str) -> None: pass ``` # Input - `input_dir` (str): The path to the input directory containing files to be compressed. - `zip_output` (str): The file path where the ZIP archive will be created. - `extract_dir` (str): The directory where the files will be extracted and decompressed. # Output - The function does not return any value. It performs file operations as described. # Constraints 1. Assume all files in `input_dir` are valid and readable. 2. You should handle any potential exceptions that may occur during file operations. 3. The function should ensure that the extracted and decompressed files maintain the same structure and content as the original files. # Example Usage ```python compress_and_archive(\'/path/to/input_dir\', \'/path/to/output.zip\', \'/path/to/extract_dir\') ``` # Requirements 1. **Compression**: Use the `bz2` module to compress each file in the directory. 2. **Archiving**: Use the `zipfile` module to create a ZIP file containing the bz2-compressed files. 3. **Decompression and extraction**: - Extract the ZIP file to the specified extraction directory using the `zipfile` module. - Decompress the bz2 files back to their original states. # Performance Requirements - Ensure that the function processes efficiently for directories with up to 1000 files. - Handle large file sizes gracefully without excessive memory usage. # Notes - You may assume the directory structure is flat (no nested directories). - Ensure that all temporary files are managed appropriately to avoid cluttering the file system. # Hints - Consider using Python\'s `os` and `shutil` libraries for file operations. - Look into `bz2.compress/decompress` and `ZIP` file handling for effective implementation.","solution":"import os import bz2 import zipfile import shutil def compress_and_archive(input_dir: str, zip_output: str, extract_dir: str) -> None: try: # Create a temporary directory to hold compressed files temp_dir = os.path.join(input_dir, \'temp_compressed\') os.makedirs(temp_dir, exist_ok=True) # Compress each file in the input directory and store them in the temp directory for file_name in os.listdir(input_dir): input_file_path = os.path.join(input_dir, file_name) if os.path.isfile(input_file_path): with open(input_file_path, \'rb\') as input_file: compressed_data = bz2.compress(input_file.read()) compressed_file_path = os.path.join(temp_dir, file_name + \'.bz2\') with open(compressed_file_path, \'wb\') as compressed_file: compressed_file.write(compressed_data) # Create a Zip archive of the compressed files with zipfile.ZipFile(zip_output, \'w\', zipfile.ZIP_DEFLATED) as zipf: for file_name in os.listdir(temp_dir): file_path = os.path.join(temp_dir, file_name) zipf.write(file_path, arcname=file_name) # Extract the Zip file to the specified extraction directory with zipfile.ZipFile(zip_output, \'r\') as zipf: zipf.extractall(extract_dir) # Decompress extracted files for file_name in os.listdir(extract_dir): file_path = os.path.join(extract_dir, file_name) if file_name.endswith(\'.bz2\'): original_file_name = file_name[:-4] # Remove \'.bz2\' extension with open(file_path, \'rb\') as compressed_file: decompressed_data = bz2.decompress(compressed_file.read()) original_file_path = os.path.join(extract_dir, original_file_name) with open(original_file_path, \'wb\') as original_file: original_file.write(decompressed_data) os.remove(file_path) # Remove the compressed file # Clean up temporary files shutil.rmtree(temp_dir) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# PyTorch Attention Mechanisms: Causal Bias You have been provided with the `torch.nn.attention.bias` module that contains tools to handle causal biases in attention mechanisms. The `CausalBias` class and functions `causal_lower_right` and `causal_upper_left` are part of this module. A common application of causal bias is in transformer-based models where the model should only attend to previous tokens when generating each token. This ensures that the model does not look into future tokens, preserving the causality of the sequence. # Task Your task is to implement a function named `apply_causal_bias` that takes a given input tensor representing attention scores and applies a causal bias to it. The function should: 1. Utilize the `CausalBias` class to apply a causal bias. 2. Provide the option to apply either `causal_lower_right` or `causal_upper_left` bias based on a parameter. 3. Ensure that the resulting tensor maintains the correct causality rules for an attention mechanism. Function Signature: ```python def apply_causal_bias(attention_scores: torch.Tensor, bias_type: str = \'lower_right\') -> torch.Tensor: Apply causal bias to the attention scores tensor. :param attention_scores: Input tensor of attention scores (batch_size, sequence_length, sequence_length). :param bias_type: Type of causal bias to apply. Either \'lower_right\' or \'upper_left\'. :return: Tensor with causal bias applied. ``` Input: - `attention_scores` (torch.Tensor): A tensor of shape `(batch_size, sequence_length, sequence_length)` representing the attention scores. - `bias_type` (str): A string indicating the type of causal bias. It can be either \'lower_right\' or \'upper_left\'. Default is \'lower_right\'. Output: - Returns a tensor of the same shape with the causal bias applied. Constraints: - The function should use the `CausalBias` class and relevant methods from the given module. - The solution should be efficient and make use of PyTorch\'s optimized operations. # Example: ```python import torch attention_scores = torch.rand((2, 5, 5)) # Random attention scores for demonstration result_lower_right = apply_causal_bias(attention_scores, \'lower_right\') print(result_lower_right) result_upper_left = apply_causal_bias(attention_scores, \'upper_left\') print(result_upper_left) ``` # Note: - You should base the implementation on hypothetical usage of `CausalBias` and the available functions as detailed usage is not provided. - Ensure your function can handle cases where batch size or sequence length might vary.","solution":"import torch class CausalBias: Dummy CausalBias class for demonstration purposes, mimicking the structure and behavior of the described module. @staticmethod def causal_lower_right(size): # Dummy implementation simulating causal lower right bias mask = torch.tril(torch.ones(size, size)) return mask @staticmethod def causal_upper_left(size): # Dummy implementation simulating causal upper left bias mask = torch.triu(torch.ones(size, size)) return mask def apply_causal_bias(attention_scores: torch.Tensor, bias_type: str = \'lower_right\') -> torch.Tensor: Apply causal bias to the attention scores tensor. :param attention_scores: Input tensor of attention scores (batch_size, sequence_length, sequence_length). :param bias_type: Type of causal bias to apply. Either \'lower_right\' or \'upper_left\'. :return: Tensor with causal bias applied. batch_size, seq_length, _ = attention_scores.shape if bias_type == \'lower_right\': causal_mask = CausalBias.causal_lower_right(seq_length) elif bias_type == \'upper_left\': causal_mask = CausalBias.causal_upper_left(seq_length) else: raise ValueError(f\\"Invalid bias_type: {bias_type}. Expected \'lower_right\' or \'upper_left\'.\\") causal_mask = causal_mask.to(attention_scores.device) causal_mask = causal_mask.unsqueeze(0).repeat(batch_size, 1, 1) # Expand mask to match batch size biased_attention_scores = attention_scores * causal_mask return biased_attention_scores"},{"question":"# Advanced Python Networking with `asyncio` Streams **Problem Statement:** You are tasked with creating a simple HTTP server using the asyncio streams API. The server should be capable of handling multiple client requests concurrently and responding with a basic HTML response. Your goal is to implement a function `start_http_server` that starts the server and handles incoming requests. **Requirements:** 1. The server should listen on a specified host and port. 2. For each incoming request, read the HTTP request data. 3. Write a simple HTTP response with \\"200 OK\\" status and a basic HTML content. 4. Ensure the server can handle multiple clients simultaneously. **Function Specifications:** ```python async def start_http_server(host: str, port: int): Starts an HTTP server that listens on the specified host and port. Parameters: - host: A string representing the IP address to listen to. - port: An integer representing the port number to bind to. The server should respond with a simple HTML response to any HTTP GET request. # Example usage: # asyncio.run(start_http_server(\'127.0.0.1\', 8080)) ``` **Constraints:** 1. Only handle HTTP GET requests. For any other request method, respond with \\"405 Method Not Allowed\\". 2. Read at most 1024 bytes from the request to avoid excessively large requests. 3. Use asyncio streams (`StreamReader` and `StreamWriter`) for handling connections. **Expected Output:** The server should be able to handle requests and respond with: ```http HTTP/1.1 200 OK Content-Type: text/html; charset=utf-8 <html> <head><title>Simple HTTP Server</title></head> <body><h1>Hello, World!</h1></body> </html> ``` For requests that are not GET method: ```http HTTP/1.1 405 Method Not Allowed Content-Type: text/html; charset=utf-8 <html> <head><title>405 Method Not Allowed</title></head> <body><h1>405 - Method Not Allowed</h1></body> </html> ``` **Note:** Your implementation should be robust and capable of handling multiple connections gracefully, leveraging `asyncio`\'s ability to manage asynchronous I/O operations.","solution":"import asyncio async def handle_client(reader, writer): Handles an HTTP request from a client. data = await reader.read(1024) message = data.decode() addr = writer.get_extra_info(\'peername\') headers = message.split(\'rn\') request_line = headers[0].split(\' \') if len(request_line) < 3: return method = request_line[0] if method != \'GET\': response = ( \\"HTTP/1.1 405 Method Not Allowedrn\\" \\"Content-Type: text/html; charset=utf-8rn\\" \\"Connection: closern\\" \\"rn\\" \\"<html><head><title>405 Method Not Allowed</title></head>\\" \\"<body><h1>405 - Method Not Allowed</h1></body></html>\\" ) else: response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/html; charset=utf-8rn\\" \\"Connection: closern\\" \\"rn\\" \\"<html><head><title>Simple HTTP Server</title></head>\\" \\"<body><h1>Hello, World!</h1></body></html>\\" ) writer.write(response.encode()) await writer.drain() writer.close() await writer.wait_closed() async def start_http_server(host: str, port: int): Starts an HTTP server that listens on the specified host and port. Parameters: - host: A string representing the IP address to listen to. - port: An integer representing the port number to bind to. The server should respond with a simple HTML response to any HTTP GET request. server = await asyncio.start_server(handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() if __name__ == \\"__main__\\": host = \'127.0.0.1\' port = 8080 asyncio.run(start_http_server(host, port))"},{"question":"**Pandas Categorical Data Task** # Objective: You are given a dataset representing customer feedback ratings. Your task is to process this data using pandas categorical operations. You need to perform the following tasks: 1. Create a categorical series representing the feedback. 2. Rename the categories to make them more descriptive. 3. Reorder the categories so that they are logically ordered. 4. Handle any missing data appropriately within the categorical series. 5. Compute and display the count of each category in the series. # Instructions: 1. Read the following feedback data into a pandas dataframe: ``` feedback = [\'excellent\', \'average\', \'poor\', \'excellent\', \'good\', \'poor\', \'excellent\', \'average\', \'average\', \'good\', None, \'poor\'] ``` 2. Convert the `feedback` column into a categorical type with the initial categories: `[\'poor\', \'average\', \'good\', \'excellent\']`. 3. Rename the categories to: - `poor` to `Unsatisfactory` - `average` to `Satisfactory` - `good` to `Good` - `excellent` to `Very Good` 4. Reorder the categories to follow the logical order: `[\'Unsatisfactory\', \'Satisfactory\', \'Good\', \'Very Good\']` and set `ordered=True`. 5. Replace any missing data (`None`) in the series with the category `Unknown`. 6. Display the resulting feedback series. 7. Calculate and display the count of each category in the series (including `Unknown`). # Constraints: - You should use pandas\' categorical data type and its methods to accomplish this task. - Do not change the initial list `feedback`. # Example Code Skeleton: ```python import pandas as pd import numpy as np from pandas.api.types import CategoricalDtype # Step 1: Create DataFrame feedback = [\'excellent\', \'average\', \'poor\', \'excellent\', \'good\', \'poor\', \'excellent\', \'average\', \'average\', \'good\', None, \'poor\'] df = pd.DataFrame({\'feedback\': feedback}) # Step 2: Convert to categorical cat_type = CategoricalDtype(categories=[\'poor\', \'average\', \'good\', \'excellent\'], ordered=False) df[\'feedback\'] = df[\'feedback\'].astype(cat_type) # Step 3: Rename categories new_categories = {\'poor\': \'Unsatisfactory\', \'average\': \'Satisfactory\', \'good\': \'Good\', \'excellent\': \'Very Good\'} df[\'feedback\'] = df[\'feedback\'].cat.rename_categories(new_categories) # Step 4: Reorder categories logical_order = [\'Unsatisfactory\', \'Satisfactory\', \'Good\', \'Very Good\'] df[\'feedback\'] = df[\'feedback\'].cat.set_categories(logical_order, ordered=True) # Step 5: Handle missing data df[\'feedback\'] = df[\'feedback\'].cat.add_categories([\'Unknown\']) df[\'feedback\'].fillna(\'Unknown\', inplace=True) # Step 6: Display resulting series print(df[\'feedback\']) # Step 7: Display the count of each category print(df[\'feedback\'].value_counts()) ``` # Expected Output: ``` 0 Very Good 1 Satisfactory 2 Unsatisfactory 3 Very Good 4 Good 5 Unsatisfactory 6 Very Good 7 Satisfactory 8 Satisfactory 9 Good 10 Unknown 11 Unsatisfactory Name: feedback, dtype: category Categories (4, object): [Unsatisfactory < Satisfactory < Good < Very Good < Unknown] Unsatisfactory 3 Satisfactory 3 Very Good 3 Good 2 Unknown 1 Name: feedback, dtype: int64 ``` Make sure your code runs correctly and produces the expected output before submission.","solution":"import pandas as pd from pandas.api.types import CategoricalDtype def process_feedback(feedback): # Step 1: Create DataFrame df = pd.DataFrame({\'feedback\': feedback}) # Step 2: Convert to categorical cat_type = CategoricalDtype(categories=[\'poor\', \'average\', \'good\', \'excellent\'], ordered=False) df[\'feedback\'] = df[\'feedback\'].astype(cat_type) # Step 3: Rename categories new_categories = { \'poor\': \'Unsatisfactory\', \'average\': \'Satisfactory\', \'good\': \'Good\', \'excellent\': \'Very Good\' } df[\'feedback\'] = df[\'feedback\'].cat.rename_categories(new_categories) # Step 4: Reorder categories logical_order = [\'Unsatisfactory\', \'Satisfactory\', \'Good\', \'Very Good\'] df[\'feedback\'] = df[\'feedback\'].cat.set_categories(logical_order, ordered=True) # Step 5: Handle missing data df[\'feedback\'] = df[\'feedback\'].cat.add_categories([\'Unknown\']) df[\'feedback\'].fillna(\'Unknown\', inplace=True) # Step 6: Display resulting series resulting_series = df[\'feedback\'] # Step 7: Calculate the count of each category counts = df[\'feedback\'].value_counts().sort_index() return resulting_series, counts"},{"question":"Implement a Python function `parse_shell_commands(commands: List[str]) -> List[List[str]]` that takes a list of shell-like command strings and returns a list of token lists generated by parsing each command string using the `shlex` module. Each command string should be processed in POSIX mode and should handle shell-like syntax, including quoted strings and escape characters. Input: - A list of strings `commands` where each string is a shell-like command. Output: - A list of lists, where each inner list contains the tokens of the corresponding command string. Constraints: - You may assume that the input list contains at least one command string. - Each command string is a valid shell-like command. - Use POSIX mode for parsing. Example: ```python commands = [ \'ls -l /home/user\', \'echo \\"Hello, World!\\"\', \'find . -name \\"*.py\\" -exec grep \\"def\\" {} ;\' ] parse_shell_commands(commands) ``` Expected Output: ```python [ [\'ls\', \'-l\', \'/home/user\'], [\'echo\', \'Hello, World!\'], [\'find\', \'.\', \'-name\', \'*.py\', \'-exec\', \'grep\', \'def\', \'{}\', \';\'] ] ``` Additional Information: - Use the `shlex.split()` function to perform the parsing. - Ensure that any quoted strings and escaped characters are properly handled according to POSIX rules. - Do not consider comments within the command strings. ```python from typing import List import shlex def parse_shell_commands(commands: List[str]) -> List[List[str]]: result = [] for command in commands: tokens = shlex.split(command, posix=True) result.append(tokens) return result # Example usage: commands = [ \'ls -l /home/user\', \'echo \\"Hello, World!\\"\', \'find . -name \\"*.py\\" -exec grep \\"def\\" {} ;\' ] print(parse_shell_commands(commands)) ```","solution":"from typing import List import shlex def parse_shell_commands(commands: List[str]) -> List[List[str]]: Parses a list of shell-like command strings into lists of tokens using the shlex module. Args: commands (List[str]): A list of shell-like command strings. Returns: List[List[str]]: A list of lists, where each inner list contains tokens of the corresponding command string. result = [] for command in commands: tokens = shlex.split(command, posix=True) result.append(tokens) return result"},{"question":"**Python Coding Assessment Question:** # Background In Python, memory management is largely handled through reference counting alongside a cyclic garbage collector. Understanding how to properly manage references is important to avoid memory leaks and ensure efficient memory use. This question will assess your understanding of reference counting and exception handling by having you implement a custom object type with basic memory management in Python. # Task You are required to implement a custom class `ManagedObject` that simulates a simplified version of Python\'s reference counting mechanism and handles exceptions robustly. # Requirements 1. **Class Implementation**: Implement the `ManagedObject` class with the following components: - **Attributes**: - `object_id`: A unique identifier for the object instance. - `_ref_count`: An internal attribute to keep track of the number of references to this object. 2. **Methods**: - `__init__(self, object_id: int)`: Initializes a new instance with a given `object_id` and sets `_ref_count` to 1. - `add_reference(self)`: Increments the `_ref_count` by 1. - `remove_reference(self)`: Decrements the `_ref_count` by 1 and raises an exception `ReferenceCountError` if `_ref_count` drops below 0. - `get_reference_count(self) -> int`: Returns the current reference count. - `__del__(self)`: Print a message indicating that the object with `object_id` is being deleted. 3. **Exception Handling**: - Define a custom exception `ReferenceCountError` to be raised when trying to remove a reference from an object with a `_ref_count` of 0. # Input Format There is no input from the user for this task. # Output Format The output should be the printed deletion message from the `__del__` method when an object is deleted, and any possible exception message from your custom exception. # Constraints - Do not use any external libraries. - Ensure proper implementation of reference counting to simulate memory management. # Example ```python class ReferenceCountError(Exception): pass class ManagedObject: # Your implementation here # Example usage try: obj = ManagedObject(101) obj.add_reference() print(obj.get_reference_count()) # Output: 2 obj.remove_reference() print(obj.get_reference_count()) # Output: 1 obj.remove_reference() print(obj.get_reference_count()) # Output: 0 obj.remove_reference() # Should raise ReferenceCountError except ReferenceCountError as e: print(f\\"Error: {e}\\") # Expected output: # 2 # 1 # 0 # Error: Reference count cannot be less than zero. # Deleting object with id 101 ``` # Performance Requirements The solution should handle a reasonable number of method calls efficiently, simulating real-life scenarios of object creation, reference management, and deletion without significant performance overhead.","solution":"class ReferenceCountError(Exception): Custom exception to be raised when reference count goes below zero. pass class ManagedObject: def __init__(self, object_id: int): Initializes a new ManagedObject instance with a given object_id and sets initial reference count to 1. self.object_id = object_id self._ref_count = 1 def add_reference(self): Increments the reference count by 1. self._ref_count += 1 def remove_reference(self): Decrements the reference count by 1 and raises ReferenceCountError if reference count drops below zero. self._ref_count -= 1 if self._ref_count < 0: raise ReferenceCountError(\\"Reference count cannot be less than zero.\\") def get_reference_count(self) -> int: Returns the current reference count. return self._ref_count def __del__(self): Prints a message indicating that the object with object_id is being deleted. print(f\\"Deleting object with id {self.object_id}\\")"},{"question":"# Python Function Implementation: Custom Exception Handling & Traceback Management Objective You need to demonstrate your understanding of Python exception handling by implementing a function that raises custom exceptions, manages their traceback information, and handles these exceptions appropriately. Instructions 1. Implement a function `process_data(data)` which processes a given list of numbers. 2. Implement exception handling in your function to catch and raise custom exceptions under the following conditions: - If `data` is not a list, raise a `TypeError` with an appropriate message. - If any element in `data` is not a number, raise a `ValueError` with an appropriate message. - If `data` is an empty list, raise a `CustomEmptyListError` (a custom exception class you define) with an appropriate message. 3. When a `ValueError` is raised, your function should capture the traceback information, and re-raise the exception after logging the error and traceback. 4. For the custom exception `CustomEmptyListError`, provide a meaningful error message and store the list\'s state in the exception object. Constraints - You should define the `CustomEmptyListError` class. - Only the specific exceptions should be raised and handled as specified. - The function should return the string \\"Data processed successfully\\" if no exceptions are encountered. Expected Input and Output - Input: `data` (a list of integers or floats) - Output: A string indicating the success of processing or the raised exceptions with appropriate traceback information. Example ```python class CustomEmptyListError(Exception): def __init__(self, message, data): super().__init__(message) self.data = data def process_data(data): try: if not isinstance(data, list): raise TypeError(\\"Input data must be a list.\\") if len(data) == 0: raise CustomEmptyListError(\\"The list is empty.\\", data) for item in data: if not isinstance(item, (int, float)): raise ValueError(f\\"Invalid item in data: {item}\\") # Simulate processing the data here return \\"Data processed successfully\\" except ValueError as ve: import traceback traceback_info = traceback.format_exc() print(f\\"Exception: {ve}\\") print(f\\"Traceback: {traceback_info}\\") raise except TypeError as te: print(f\\"Exception: {te}\\") raise except CustomEmptyListError as ce: print(f\\"Exception: {ce}\\") raise # Testing the function try: process_data([1, 2, \\"a\\"]) except Exception as e: print(f\\"Caught exception: {e}\\") try: process_data([]) except Exception as e: print(f\\"Caught exception: {e}\\") try: process_data(123) except Exception as e: print(f\\"Caught exception: {e}\\") try: print(process_data([1, 2, 3])) except Exception as e: print(f\\"Caught exception: {e}\\") ``` Notes - Your implementation should demonstrate good practice in exception handling and demonstrate clear understanding of Python\'s exception mechanism. - Pay attention to maintainability and readability of your code. - Do not use any external libraries except standard Python libraries.","solution":"class CustomEmptyListError(Exception): def __init__(self, message, data): super().__init__(message) self.data = data def process_data(data): try: if not isinstance(data, list): raise TypeError(\\"Input data must be a list.\\") if len(data) == 0: raise CustomEmptyListError(\\"The list is empty.\\", data) for item in data: if not isinstance(item, (int, float)): raise ValueError(f\\"Invalid item in data: {item}\\") # Simulate processing the data here return \\"Data processed successfully\\" except ValueError as ve: import traceback traceback_info = traceback.format_exc() print(f\\"Exception: {ve}\\") print(f\\"Traceback: {traceback_info}\\") raise except TypeError as te: print(f\\"Exception: {te}\\") raise except CustomEmptyListError as ce: print(f\\"Exception: {ce}\\") raise"},{"question":"# Coding Assessment: Python `py_compile` Module **Objective:** Implement a Python function to compile a list of Python source files into byte-code files using the `py_compile` module. The function should handle errors appropriately and use specific optimization settings. **Problem Statement:** Write a function `compile_python_sources(files: List[str], optimize: int = -1, quiet: int = 0) -> Dict[str, Optional[str]]` that takes a list of file paths to Python source files (`files`), an optimization level (`optimize`), and a quiet mode level (`quiet`). The function should attempt to compile each source file to a byte-code file and return a dictionary with source file paths as keys and the corresponding byte-code file paths or error messages as values. **Function Signature:** ```python from typing import List, Dict, Optional def compile_python_sources(files: List[str], optimize: int = -1, quiet: int = 0) -> Dict[str, Optional[str]]: pass ``` **Inputs:** 1. `files` (List[str]): A list of string paths to Python source files. 2. `optimize` (int): The optimization level passed to the `py_compile.compile` function (default is -1). 3. `quiet` (int): The quiet mode level (0, 1, or 2) passed to the `py_compile.compile` function (default is 0). **Outputs:** - The function should return a dictionary where keys are the original file paths and values are either: - The path to the compiled byte-code file, or - An error message if the compilation failed. **Constraints:** - Optimization levels can be -1, 0, 1, or 2. - Quiet mode levels can be 0, 1, or 2. - Assume all paths in the `files` list are valid and accessible. **Example:** ```python files = [\\"script1.py\\", \\"script2.py\\", \\"invalid_script.py\\"] optimize = 1 quiet = 0 result = compile_python_sources(files, optimize, quiet) # Possible Output # { # \\"script1.py\\": \\"/path/to/__pycache__/script1.cpython-39.pyc\\", # \\"script2.py\\": \\"/path/to/__pycache__/script2.cpython-39.pyc\\", # \\"invalid_script.py\\": \\"Error: invalid syntax (invalid_script.py, line 1)\\" # } ``` **Notes:** - Handle errors based on the specified quiet mode: - `quiet=0` or `quiet=1`: Write error messages to `sys.stderr` and include error messages in the output dictionary. - `quiet=2`: Suppress all error messages. - You can use the `py_compile` module\'s `compile` function for the implementation. **Hints:** - Refer to the `py_compile` documentation for understanding the parameters and error handling mechanisms. - You might want to capture error messages using the `try...except` block and handle them accordingly based on the `quiet` mode set.","solution":"from typing import List, Dict, Optional import py_compile import os def compile_python_sources(files: List[str], optimize: int = -1, quiet: int = 0) -> Dict[str, Optional[str]]: result = {} for file in files: try: bytecode_path = py_compile.compile(file, cfile=None, dfile=None, doraise=True, optimize=optimize, quiet=quiet) result[file] = bytecode_path except py_compile.PyCompileError as e: if quiet < 2: error_message = str(e) result[file] = error_message else: result[file] = None return result"},{"question":"# Question: Creating and Managing Python Virtual Environments and Executable Zip Archives Objective: Implement a Python program that demonstrates proficiency with the `venv` and `zipapp` libraries by performing the following tasks: 1. **Create a Virtual Environment**: - Write a function `create_venv(path: str) -> None` that creates a virtual environment at the specified `path`. 2. **Install Dependencies**: - Inside this virtual environment, install some required packages. Write a function `install_dependencies(venv_path: str, packages: list) -> None` that installs a list of Python packages using `pip`. 3. **Create an Executable Zip Archive**: - Write a function `create_executable_zip(source_dir: str, output_filename: str) -> None` which uses the `zipapp` module to bundle the contents of the `source_dir` into an executable zip archive named `output_filename`. 4. **Specifying the Interpreter**: - Ensure that the executable zip archive specifies the Python interpreter from the created virtual environment. Details: - Function 1: - **Input**: - `path` (str): The directory where the virtual environment should be created. - **Output**: None - Function 2: - **Input**: - `venv_path` (str): The directory path of the created virtual environment. - `packages` (list): A list of string package names to install within the virtual environment. - **Output**: None - **Constraints**: Handle cases where the virtual environment path is invalid. Ensure `pip` is available in the virtual environment. - Function 3: - **Input**: - `source_dir` (str): The directory containing the source files to be archived. - `output_filename` (str): The name of the resulting executable zip file. - **Output**: None - **Constraints**: Ensure that the resulting zip archive is executable and the entry point script is correctly identified. Example Usage: ```python # Creating a virtual environment create_venv(\'/path/to/new/venv\') # Install packages install_dependencies(\'/path/to/new/venv\', [\'requests\', \'numpy\']) # Creating an executable zip archive create_executable_zip(\'/path/to/source_dir\', \'output.pyz\') ``` You can make necessary helper functions to support the main functions. Make sure to include appropriate error handling and logging to assist in debugging.","solution":"import os import subprocess import venv import zipapp from typing import List def create_venv(path: str) -> None: Creates a virtual environment at the specified path. venv.create(path, with_pip=True) def install_dependencies(venv_path: str, packages: List[str]) -> None: Installs a list of packages into the virtual environment located at venv_path. if not os.path.exists(venv_path): raise FileNotFoundError(f\\"The specified virtual environment path {venv_path} does not exist.\\") pip_executable = os.path.join(venv_path, \'bin\', \'pip\') if os.name != \'nt\' else os.path.join(venv_path, \'Scripts\', \'pip.exe\') subprocess.check_call([pip_executable, \'install\'] + packages) def create_executable_zip(source_dir: str, output_filename: str) -> None: Creates an executable zip archive from the source directory. if not os.path.isdir(source_dir): raise NotADirectoryError(f\\"The source_dir path {source_dir} is not a directory.\\") entry_point = \'__main__.py\' zipapp.create_archive(source_dir, output_filename, interpreter=\'/usr/bin/env python3\')"},{"question":"# Fraction Arithmetic and Simplification Objective Your task is to implement a function `simplify_fraction(expression: str) -> str` that takes a string representing an arithmetic expression involving fractions and returns the result of the expression as a simplified fraction string. Input - `expression`: A string representing an arithmetic expression involving fractions. - Examples of valid fractions in the string: \\"1/2\\", \\"3/4\\", \\"-5/3\\", \\"0/1\\" - The arithmetic expression can contain `+`, `-`, `*`, and `/` operators, as well as parentheses `()`. - The string will be a valid arithmetic expression. Output - Return the result of the evaluated expression as a simplified fraction in string form (e.g., \\"5/2\\", \\"-7/4\\"). - Ensure the fraction is in its lowest terms with a positive denominator. Constraints - Denominators are guaranteed to be non-zero in the input fractions. - The result should be returned as a string in the form \\"numerator/denominator\\". - The arithmetic operations must handle the order of operations (parentheses, multiplication and division, addition and subtraction). Example ```python >>> simplify_fraction(\\"1/2 + 3/4\\") \'5/4\' >>> simplify_fraction(\\"1/2 - 3/4\\") \'-1/4\' >>> simplify_fraction(\\"(1/2 + 3/4) * 2\\") \'5/2\' >>> simplify_fraction(\\"1/2 / 3/4\\") \'2/3\' ``` Implementation You are encouraged to use the `fractions.Fraction` class for accurate arithmetic operations and for ensuring the result is always a simplified fraction. Hints 1. You can use the `fractions.Fraction` class to construct fractions directly from strings. 2. Utilize Python’s `eval` function cautiously to parse and evaluate the expression. Ensure to handle the fractions correctly within the `eval` function. 3. Consider using helper functions to break down and handle individual operations while maintaining fraction integrity.","solution":"from fractions import Fraction def simplify_fraction(expression: str) -> str: Evaluates the arithmetic expression involving fractions and returns the result as a simplified fraction string. :param expression: str, an arithmetic expression containing fractions. :return: str, the result simplified fraction. # Define a safe eval environment allowed_names = {\\"Fraction\\": Fraction} # Replace the fractions in the expression with calls to Fraction safe_expression = \\"\\" i, length = 0, len(expression) while i < length: if expression[i].isdigit() or (expression[i] == \'-\' and (i+1 < length and expression[i+1].isdigit())): j = i + 1 while j < length and (expression[j].isdigit() or expression[j] == \'/\'): j += 1 safe_expression += f\\"Fraction(\'{expression[i:j]}\')\\" i = j else: safe_expression += expression[i] i += 1 # Evaluate the expression safely result = eval(safe_expression, {\\"__builtins__\\": None}, allowed_names) # Return the result as a simplified fraction string return str(result)"},{"question":"# Character Triangulation Using Unicode Representations **Objective**: The task is to utilize Python\'s C API for Unicode objects to create and manipulate Unicode strings efficiently. Your objective is to implement a function in Python that analyzes a given set of Unicode strings and returns manipulations and comparisons between them using the C APIs. **Task Description**: You are required to write a Python C extension function `char_triangulate` that performs the following tasks: 1. **Input**: - A list of Unicode strings. 2. **Output**: - A dictionary with the following keys: - `\\"longest_string\\"`: The longest Unicode string in terms of characters. - `\\"shortest_string\\"`: The shortest Unicode string in terms of characters. - `\\"encoded_utf8\\"`: A list of UTF-8 encoded bytes for each Unicode string. - `\\"char_properties\\"`: A dictionary where keys are characters (aggregated from all strings) and values are dictionaries with properties indicating: - Is the character a digit? - Is the character a letter? - Is the character a space? **Constraints**: - Empty strings or None values should be gracefully handled. - There will be no more than 1000 Unicode strings in the list. **Performance**: - Your implementation should be memory efficient and handle the conversion processes within reasonable time for the allowed input size. # Python C API Implementations (Simplified Guide) You can refer to the following Python C APIs to design your function: 1. **Creating Unicode Objects**: ```c PyObject *PyUnicode_FromString(const char *u); ``` 2. **Reading and Writing Characters**: ```c Py_UCS4 PyUnicode_ReadChar(PyObject *unicode, Py_ssize_t index); int PyUnicode_WriteChar(PyObject *unicode, Py_ssize_t index, Py_UCS4 character); ``` 3. **Encoding Unicode**: ```c PyObject *PyUnicode_AsUTF8String(PyObject *unicode); ``` 4. **Character Properties**: ```c int Py_UNICODE_ISDIGIT(Py_UCS4 ch); int Py_UNICODE_ISALPHA(Py_UCS4 ch); int Py_UNICODE_ISSPACE(Py_UCS4 ch); ``` **Example**: Here is an example of a Python function signature for `char_triangulate` and expected output format: ```python # Example Usage input_strings = [\\"Hello\\", \\" 123 \\", \\"😀\\"] output = char_triangulate(input_strings) # Expected Output Format { \\"longest_string\\": \\"😀\\", \\"shortest_string\\": \\"123\\", \\"encoded_utf8\\": [b\'Hello\', b\' 123 \', b\'xF0x9Fx98x80\'], \\"char_properties\\": { \'H\': {\'is_digit\': False, \'is_alpha\': True, \'is_space\': False}, \'e\': {\'is_digit\': False, \'is_alpha\': True, \'is_space\': False}, \'l\': {\'is_digit\': False, \'is_alpha\': True, \'is_space\': False}, \'o\': {\'is_digit\': False, \'is_alpha\': True, \'is_space\': False}, \' \': {\'is_digit\': False, \'is_alpha\': False, \'is_space\': True}, \'1\': {\'is_digit\': True, \'is_alpha\': False, \'is_space\': False}, \'2\': {\'is_digit\': True, \'is_alpha\': False, \'is_space\': False}, \'3\': {\'is_digit\': True, \'is_alpha\': False, \'is_space\': False}, \'😀\': {\'is_digit\': False, \'is_alpha\': False, \'is_space\': False}, } } ``` Implement the function `char_triangulate` in such a way that it demonstrates your understanding of handling Unicode strings using the Python C API, their creation, manipulation, and efficient encoding techniques. **Submission Requirements**: - Source code for the C extension module. - Python script to test the C extension module.","solution":"def char_triangulate(input_strings): Analyzes a given set of Unicode strings and returns manipulations and comparisons between them. Parameters: input_strings (list): A list of Unicode strings. Returns: dict: A dictionary with keys \'longest_string\', \'shortest_string\', \'encoded_utf8\', and \'char_properties\'. if not input_strings: return { \\"longest_string\\": \\"\\", \\"shortest_string\\": \\"\\", \\"encoded_utf8\\": [], \\"char_properties\\": {} } # Filter out empty strings and None values input_strings = [s for s in input_strings if s] if not input_strings: return { \\"longest_string\\": \\"\\", \\"shortest_string\\": \\"\\", \\"encoded_utf8\\": [], \\"char_properties\\": {} } longest_string = max(input_strings, key=len) shortest_string = min(input_strings, key=len) encoded_utf8 = [s.encode(\'utf-8\') for s in input_strings] char_properties = {} for string in input_strings: for char in set(string): if char not in char_properties: char_properties[char] = { \\"is_digit\\": char.isdigit(), \\"is_alpha\\": char.isalpha(), \\"is_space\\": char.isspace() } return { \\"longest_string\\": longest_string, \\"shortest_string\\": shortest_string, \\"encoded_utf8\\": encoded_utf8, \\"char_properties\\": char_properties }"},{"question":"# PyTorch Meta Device Assessment This assessment will test your understanding of using the PyTorch \\"meta\\" device for working with meta tensors and models. You will write a function to load a neural network model architecture on the meta device, perform a specific transformation, and then properly initialize and transfer the model to the CPU with real data. Problem Statement You are given a PyTorch neural network model saved in a file, and you need to perform the following steps in your function `transform_and_initialize_model`: 1. Load the model architecture using the meta device. 2. Double the size of the weights in the first linear layer. 3. Transfer the modified model to the CPU and initialize the parameters with random values. # Function Signature ```python def transform_and_initialize_model(model_path: str) -> torch.nn.Module: pass ``` # Input - `model_path` (str): A string representing the file path to the saved PyTorch model. # Output - Returns an instance of `torch.nn.Module`, which is the transformed and initialized model on the CPU. # Detailed Requirements 1. **Loading the Model on Meta Device**: Use the `torch.load` function with `map_location=\'meta\'` to load the model architecture. 2. **Transformation**: Double the size of the weights in the first linear layer. If the layer has weight dimensions `(in_features, out_features)`, change it to `(in_features, 2*out_features)` and initialize these doubled weights to new random values by subsequently re-adding the layer to the module with updated dimensions. 3. **Transfer to CPU**: Use the `.to_empty()` method on the module to transfer it to the CPU. 4. **Initialization**: Explicitly initialize all weights in the model using `torch.nn.init` methods of your choice (e.g., `torch.nn.init.xavier_uniform_` for the weights). # Example ```python # Example usage transformed_model = transform_and_initialize_model(\'saved_model.pt\') print(transformed_model) ``` Constraints - You should assume the saved model contains at least one `torch.nn.Linear` layer in its architecture. - You can assume valid file paths and models are provided. Notes - You may use other utility functions and modules as needed, but the main transformations and initializations must happen within `transform_and_initialize_model`. # Hint - Ensure you create new tensors explicitly if needed to replace modified layers.","solution":"import torch import torch.nn as nn def transform_and_initialize_model(model_path: str) -> torch.nn.Module: Loads a model architecture using the meta device, doubles the size of the weights in the first linear layer, and transfers the modified model to the CPU with initialized parameters. Args: - model_path (str): The file path to the saved PyTorch model. Returns: - torch.nn.Module: The transformed and initialized model on the CPU. # Load the model architecture with the \'meta\' device model = torch.load(model_path, map_location=\'meta\') # Loop through the model to find the first Linear layer for name, module in model.named_modules(): if isinstance(module, nn.Linear): in_features, out_features = module.in_features, module.out_features new_out_features = 2 * out_features # Replace with a new Linear layer with doubled out_features new_linear_layer = nn.Linear(in_features, new_out_features) model._modules[name] = new_linear_layer break # Move the model to the CPU, without any parameters model = model.to_empty(device=\'cpu\') # Initialize the parameters for name, param in model.named_parameters(): if \'weight\' in name: nn.init.xavier_uniform_(param) elif \'bias\' in name: nn.init.zeros_(param) return model"},{"question":"You are tasked with modifying a given PyTorch neural network model to ensure compatibility with functorch\'s vmap utility. Specifically, you need to replace all `BatchNorm2d` layers with `GroupNorm` layers, ensuring that the number of groups in `GroupNorm` is equal to the number of channels. Function Signature ```python def replace_batchnorm_with_groupnorm(model: torch.nn.Module) -> torch.nn.Module: This function takes a PyTorch neural network model and replaces all BatchNorm2d layers with GroupNorm layers. Parameters: model (torch.nn.Module): The PyTorch neural network model with BatchNorm2d layers. Returns: torch.nn.Module: The modified model with GroupNorm layers replacing BatchNorm2d layers. ``` Input - `model`: A PyTorch neural network model containing `BatchNorm2d` layers. Output - A PyTorch neural network model where all `BatchNorm2d` layers have been replaced with `GroupNorm` layers, with the number of groups equal to the number of channels. Constraints - Assume the model only contains `BatchNorm2d` layers for normalization. - You need to ensure the modified models have equivalent behavior except for the normalization method. Example ```python import torch import torch.nn as nn # Sample model for illustration class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) return x model = SampleModel() modified_model = replace_batchnorm_with_groupnorm(model) # Check the conversion assert isinstance(modified_model.bn1, nn.GroupNorm) assert isinstance(modified_model.bn2, nn.GroupNorm) ``` Explanation In this example, `SampleModel` contains two `BatchNorm2d` layers (`bn1` and `bn2`). The function `replace_batchnorm_with_groupnorm` modifies the model to replace these `BatchNorm2d` layers with `GroupNorm` layers, where the number of groups is set to the respective number of channels. You are required to implement the `replace_batchnorm_with_groupnorm` function, which will generalize this transformation for any given model structure.","solution":"import torch import torch.nn as nn def replace_batchnorm_with_groupnorm(model: torch.nn.Module) -> torch.nn.Module: This function takes a PyTorch neural network model and replaces all BatchNorm2d layers with GroupNorm layers. Parameters: model (torch.nn.Module): The PyTorch neural network model with BatchNorm2d layers. Returns: torch.nn.Module: The modified model with GroupNorm layers replacing BatchNorm2d layers. for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features setattr(model, name, nn.GroupNorm(num_groups=num_channels, num_channels=num_channels)) else: replace_batchnorm_with_groupnorm(module) return model # Example model for test purposes class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) return x"},{"question":"# Question You are tasked with analyzing and visualizing data from the `tips` dataset available in Seaborn. This dataset contains information about tips received by waiters at a restaurant, including the total bill, tip amount, sex of the waiter, smoking status, day of the week, time of day, and size of the party. Your task is to perform the following: 1. **Load and preprocess the data:** - Filter the data to only include observations where the total bill is greater than 10. - Add a new column \'tip_percentage\' which is the tip divided by the total bill, multiplied by 100. 2. **Visualize the data:** - Create a scatter plot of `total_bill` vs `tip_percentage` colored by the `sex` of the waiter. - Create a box plot showing the distribution of `tip_percentage` for each day of the week. - Create a pair plot to show pairwise relationships between `total_bill`, `tip_percentage`, and `size`. 3. **Advanced Plot Configuration:** - For the scatter plot, set the transparency (`alpha`) of the points to 0.5 and adjust the marker size to 50. - For the box plot, use different colors for smokers and non-smokers. - In the pair plot, ensure that the diagonal contains histograms to depict distributions of individual variables. # Input and Output - **Input:** - You will not take any input; load the data directly using `sns.load_dataset(\'tips\')`. - **Output:** - You will generate and display three plots directly using Seaborn. # Code Requirements - Use Seaborn\'s `objects` interface for all plots. - Preprocess the data using pandas. - Ensure the plots are well-labeled (axis titles, plot titles where appropriate). # Constraints - You may only use pandas for data manipulation and Seaborn for visualization. - Ensure your solution is efficient and clear. # Example The following code may guide you to set up and start with loading the data and basic preprocessing: ```python import seaborn.objects as so import pandas as pd import seaborn as sns # Load dataset data = sns.load_dataset(\'tips\') # Preprocess the data data = data[data[\'total_bill\'] > 10] data[\'tip_percentage\'] = (data[\'tip\'] / data[\'total_bill\']) * 100 # Create visualizations # (To be filled by students as per instructions in the question) ``` Complete the above setup to perform the required tasks and generate the specified visualizations.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load dataset data = sns.load_dataset(\'tips\') # Preprocess the data data = data[data[\'total_bill\'] > 10] data[\'tip_percentage\'] = (data[\'tip\'] / data[\'total_bill\']) * 100 def create_plots(): # Create scatter plot scatter_plot = sns.scatterplot(data=data, x=\'total_bill\', y=\'tip_percentage\', hue=\'sex\', alpha=0.5, s=50) scatter_plot.set_title(\\"Total Bill vs Tip Percentage Colored by Sex\\") plt.show() # Create box plot box_plot = sns.boxplot(data=data, x=\'day\', y=\'tip_percentage\', hue=\'smoker\') box_plot.set_title(\\"Tip Percentage Distribution by Day and Smoking Status\\") plt.show() # Create pair plot pair_plot = sns.pairplot(data, vars=[\'total_bill\', \'tip_percentage\', \'size\'], diag_kind=\'hist\') pair_plot.fig.suptitle(\\"Pairwise Relationships of Total Bill, Tip Percentage, and Size\\", y=1.02) plt.show() create_plots()"},{"question":"# Question: You are given a list of transactions, where each transaction is represented as a tuple `(timestamp, transaction_amount)`. Your task is to process this list and calculate a moving average of the transaction amounts over a specified window size. To handle large lists efficiently, you are required to use functions from the `itertools` module. **Function Signature:** ```python def moving_average(transactions: List[Tuple[int, float]], window_size: int) -> List[Tuple[int, float]]: ``` **Input:** - `transactions`: A list of tuples, where each tuple consists of: - `timestamp` (int): The time of the transaction. - `transaction_amount` (float): The amount of the transaction. - `window_size`: An integer representing the size of the window over which to calculate the moving average. **Output:** - A list of tuples, where each tuple consists of: - `timestamp` (int): The time of the transaction. - `moving_average` (float): The moving average of the transaction amounts over the specified window size ending at this timestamp. **Constraints:** - The `window_size` is guaranteed to be between 1 and the length of `transactions`, inclusive. - The timestamps in `transactions` are in increasing order. **Example:** ```python transactions = [(1, 10.0), (2, 20.0), (3, 30.0), (4, 40.0), (5, 50.0)] window_size = 3 # Expected Output: [(1, 10.0), (2, 15.0), (3, 20.0), (4, 30.0), (5, 40.0)] ``` **Explanation:** - At timestamp 1, the moving average is `10.0`. - At timestamp 2, the moving average is `(10.0 + 20.0) / 2 = 15.0`. - At timestamp 3, the moving average is `(10.0 + 20.0 + 30.0) / 3 = 20.0`. - At timestamp 4, the moving average is `(20.0 + 30.0 + 40.0) / 3 = 30.0`. - At timestamp 5, the moving average is `(30.0 + 40.0 + 50.0) / 3 = 40.0`. # Hints: 1. Use `itertools.islice` to create sliding windows of the transaction amounts. 2. Use `itertools.accumulate` to calculate cumulative sums that can help in computing the moving average efficiently. 3. Ensure that the window slides correctly and calculate the moving averages accordingly. --- # Note: Your solution should be efficient enough to handle large lists of transactions in terms of both time and space complexity.","solution":"from typing import List, Tuple import itertools def moving_average(transactions: List[Tuple[int, float]], window_size: int) -> List[Tuple[int, float]]: Calculate the moving average of transaction amounts over a specified window size. if not transactions or window_size <= 0: return [] result = [] cumsum = [0] for i, (timestamp, amount) in enumerate(transactions): cumsum.append(cumsum[-1] + amount) if i + 1 >= window_size: window_sum = cumsum[i + 1] - cumsum[i + 1 - window_size] moving_avg = window_sum / window_size result.append((timestamp, moving_avg)) else: moving_avg = cumsum[-1] / (i + 1) result.append((timestamp, moving_avg)) return result"},{"question":"<|Analysis Begin|> The zlib module in Python provides functions for compression and decompression using the zlib library. The key functions and classes include: 1. `zlib.adler32(data, value)`: Computes an Adler-32 checksum of the given data. 2. `zlib.compress(data, level)`: Compresses the given data using the specified compression level. 3. `zlib.compressobj(level, method, wbits, memLevel, strategy, zdict)`: Returns a compression object for streaming compression. 4. `zlib.crc32(data, value)`: Computes a CRC checksum of the given data. 5. `zlib.decompress(data, wbits, bufsize)`: Decompresses the given data. 6. `zlib.decompressobj(wbits, zdict)`: Returns a decompression object for streaming decompression. Compression and decompression objects provide additional methods like `compress()`, `flush()`, `decompress()`, and various attributes to handle data streams that don\'t fit into memory. Several constants associated with the zlib library version are provided, including `ZLIB_VERSION` and `ZLIB_RUNTIME_VERSION`. <|Analysis End|> <|Question Begin|> # Compression and Decompression with Streaming Data As an engineer working on a data pipeline, you are tasked with implementing a system that can handle the compression and decompression of large data streams that do not fit into memory all at once. Your solution should use the `zlib` module\'s streaming interfaces. Requirements: 1. Create a class `StreamingCompression` that provides methods to: - Compress chunks of data incrementally. - Decompress chunks of data incrementally. Class Definition: ```python class StreamingCompression: def __init__(self, compression_level=-1, wbits=15, mem_level=8, strategy=0): Initializes the compression and decompression objects with the given parameters. Arguments: - compression_level: Compression level from 0 to 9, or -1 for default. - wbits: Size of the history buffer (window size). - mem_level: Amount of memory used for compression state (1 to 9). - strategy: Strategy for compression algorithm tuning. pass def compress_chunk(self, chunk): Compresses a chunk of data and returns the compressed data. Arguments: - chunk: A chunk of data (bytes) to be compressed. Returns: - Compressed data as bytes. pass def decompress_chunk(self, chunk): Decompresses a chunk of data and returns the decompressed data. Arguments: - chunk: A chunk of compressed data (bytes) to be decompressed. Returns: - Decompressed data as bytes. pass def flush_compression(self): Flushes the remaining compressed data and returns it. Returns: - Remaining compressed data as bytes. pass def flush_decompression(self): Flushes the remaining decompressed data and returns it. Returns: - Remaining decompressed data as bytes. pass ``` Example Usage: ```python data_chunks = [b\\"first chunk of data\\", b\\"second chunk of data\\", b\\"third chunk of data\\"] # Initialize the compressor compressor = StreamingCompression() # Compress the data chunks compressed_chunks = [compressor.compress_chunk(chunk) for chunk in data_chunks] compressed_chunks.append(compressor.flush_compression()) # Initialize the decompressor decompressor = StreamingCompression() # Decompress the data chunks decompressed_chunks = [decompressor.decompress_chunk(chunk) for chunk in compressed_chunks] decompressed_chunks.append(decompressor.flush_decompression()) # Verify the decompressed data matches the original data decompressed_data = b\\"\\".join(decompressed_chunks) assert decompressed_data == b\\"\\".join(data_chunks) ``` Constraints: - Ensure your implementation can handle arbitrarily long data streams split across multiple chunks. - Be mindful of memory usage, especially for the decompression buffer size. - Handle errors gracefully, ensuring that the program does not crash on invalid input. Submission: Submit your implementation of the `StreamingCompression` class. Ensure it is well-documented and includes any necessary error checks.","solution":"import zlib class StreamingCompression: def __init__(self, compression_level=-1, wbits=15, mem_level=8, strategy=0): Initializes the compression and decompression objects with the given parameters. Arguments: - compression_level: Compression level from 0 to 9, or -1 for default. - wbits: Size of the history buffer (window size). - mem_level: Amount of memory used for compression state (1 to 9). - strategy: Strategy for compression algorithm tuning. self.compress_obj = zlib.compressobj(compression_level, zlib.DEFLATED, wbits, mem_level, strategy) self.decompress_obj = zlib.decompressobj(wbits) def compress_chunk(self, chunk): Compresses a chunk of data and returns the compressed data. Arguments: - chunk: A chunk of data (bytes) to be compressed. Returns: - Compressed data as bytes. return self.compress_obj.compress(chunk) def decompress_chunk(self, chunk): Decompresses a chunk of data and returns the decompressed data. Arguments: - chunk: A chunk of compressed data (bytes) to be decompressed. Returns: - Decompressed data as bytes. return self.decompress_obj.decompress(chunk) def flush_compression(self): Flushes the remaining compressed data and returns it. Returns: - Remaining compressed data as bytes. return self.compress_obj.flush() def flush_decompression(self): Flushes the remaining decompressed data and returns it. Returns: - Remaining decompressed data as bytes. return self.decompress_obj.flush()"},{"question":"You are tasked with analyzing a dataset of penguin measurements and visualizing the relationships between their bill dimensions and species. Your goal is to create a series of plots using seaborn that demonstrate your understanding of various customization options available in `sns.lmplot`. Question: 1. Load the `penguins` dataset from seaborn. 2. Display a scatter plot with a regression line for the relationship between `bill_length_mm` and `bill_depth_mm`. 3. Color the points by `species` on the regression plot. 4. Create a set of subplots, splitting by `sex` on the columns and by `species` on the rows. 5. Adjust the axis limits independently across subplots in the previous step. Expected Function Signature ```python def plot_penguin_measurements(): import seaborn as sns # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Basic scatter plot with regression line sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") # Step 3: Color points by species sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") # Step 4: Splitting by sex and species sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"sex\\", row=\\"species\\", height=3) # Step 5: Adjusting axis limits independently sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"sex\\", row=\\"species\\", height=3, facet_kws=dict(sharex=False, sharey=False)) # Calling the function to execute the plots plot_penguin_measurements() ``` Constraints: - Ensure that the dataset exists and properly loads. - Manage plot display within a Jupyter Notebook environment. - Maintain readability and proper documentation/comments within your code. # Tips: - Utilize seaborn\'s `lmplot` function for all plotting aspects. - Experiment with different parameter settings like `hue`, `col`, `row`, and `facet_kws`.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_measurements(): Load the penguins dataset and create series of plots demonstrating relationships between bill dimensions and species. # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Basic scatter plot with regression line sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") plt.title(\\"Scatter plot with regression line\\") plt.show() # Step 3: Color points by species sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\") plt.title(\\"Scatter plot with regression line colored by species\\") plt.show() # Step 4: Splitting by sex and species sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"sex\\", row=\\"species\\", height=3) plt.suptitle(\\"Scatter plots split by sex and species\\", y=1.02) plt.show() # Step 5: Adjusting axis limits independently sns.lmplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", col=\\"sex\\", row=\\"species\\", height=3, facet_kws=dict(sharex=False, sharey=False)) plt.suptitle(\\"Scatter plots with independent axis limits\\", y=1.02) plt.show() # Execute the function to generate the plots plot_penguin_measurements()"},{"question":"**Question: URL Content Fetcher and Analyzer** You are required to implement a Python function using the `urllib` package. This function will fetch the content of a given URL, parse for specific data, and handle potential errors gracefully. # Function Signature ```python def fetch_and_analyze_url(url: str, search_term: str) -> dict: pass ``` # Task Description 1. **Fetch Content**: Use `urllib.request` to retrieve the content of the URL passed to the function as a string. 2. **Handle Errors**: Appropriately handle HTTP errors and URL errors using `urllib.error`. If an error occurs, return a dictionary with the keys: - `\\"status\\"`: a string, `\\"error\\"`. - `\\"message\\"`: a string, describing the error. 3. **Parse and Analyze**: Using the `search_term` argument, count the number of times this term appears in the content of the URL. Perform a case-insensitive search for this term. 4. **Return Format**: Upon successful retrieval and analysis, return a dictionary with the keys: - `\\"status\\"`: a string, `\\"success\\"`. - `\\"url\\"`: the URL that was fetched. - `\\"search_term\\"`: the term that was searched for in the content. - `\\"count\\"`: the number of times the `search_term` appears in the content. # Input Format - The function will receive two arguments: - `url` (str): The URL from which to fetch content. - `search_term` (str): The term to search for within the retrieved content. # Output Format - The function should return a dictionary with the structure specified above. # Constraints - Assume the URL provided will be a valid string. - The search term will be a non-empty string consisting of alphanumeric characters. - You should make use of the `urllib` package for handling URL operations and error management. # Example ```python # Example case url = \\"http://example.com\\" search_term = \\"example\\" result = fetch_and_analyze_url(url, search_term) # Expected output example { \\"status\\": \\"success\\", \\"url\\": \\"http://example.com\\", \\"search_term\\": \\"example\\", \\"count\\": 2 # Number of times \\"example\\" appears in the content } ``` # Additional Notes - You need to ensure error handling is robust. - Be mindful of the performance implications when dealing with large content fetched from URLs. - No internet access is required to demonstrate this function; use mock URLs for testing. Implement the function `fetch_and_analyze_url` as described above.","solution":"import urllib.request import urllib.error def fetch_and_analyze_url(url: str, search_term: str) -> dict: Fetches content of the provided URL, searches for a specific term, and returns analysis results. try: with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return {\\"status\\": \\"error\\", \\"message\\": f\\"HTTP error: {e.code}\\"} except urllib.error.URLError as e: return {\\"status\\": \\"error\\", \\"message\\": f\\"URL error: {e.reason}\\"} search_term_lower = search_term.lower() content_lower = content.lower() count = content_lower.count(search_term_lower) return { \\"status\\": \\"success\\", \\"url\\": url, \\"search_term\\": search_term, \\"count\\": count }"},{"question":"Objective Demonstrate your understanding of PyTorch\'s sparse tensor formats by implementing a function that creates a sparse CSR tensor from a dense tensor and performs a matrix multiplication with another dense tensor. Problem Statement Implement the following function: ```python import torch def sparse_csr_mm(dense_tensor, dense_matrix): Converts a dense tensor into a sparse CSR tensor and performs a matrix multiplication. Parameters: dense_tensor (torch.Tensor): 2D dense tensor to be converted to sparse CSR format. dense_matrix (torch.Tensor): 2D dense tensor for multiplication. Returns: torch.Tensor: Resulting dense tensor after multiplication. pass ``` Requirements 1. Convert the input `dense_tensor` to a sparse CSR tensor. 2. Perform a matrix multiplication between the resulting sparse CSR tensor and the `dense_matrix`. 3. Return the result as a dense tensor. 4. The function should handle different tensor shapes and ensure the result matches the expected dimensions for matrix multiplication. Example ```python dense_tensor = torch.tensor([ [0.0, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 2.0, 0.0] ]) dense_matrix = torch.tensor([ [1.0, 2.0], [3.0, 4.0], [5.0, 6.0] ]) result = sparse_csr_mm(dense_tensor, dense_matrix) print(result) ``` Expected output: ``` tensor([[5., 6.], [1., 2.], [6., 8.]]) ``` Constraints - Both `dense_tensor` and `dense_matrix` will be 2D tensors with compatible dimensions for matrix multiplication. - Performance considerations: Convert and multiply efficiently utilizing PyTorch\'s optimized functions. Evaluation Your implementation will be evaluated on: - Correctness: The result should be accurate. - Efficiency: The function should leverage sparse tensor benefits. - Code quality: The code should be clean and well-documented.","solution":"import torch def sparse_csr_mm(dense_tensor, dense_matrix): Converts a dense tensor into a sparse CSR tensor and performs a matrix multiplication. Parameters: dense_tensor (torch.Tensor): 2D dense tensor to be converted to sparse CSR format. dense_matrix (torch.Tensor): 2D dense tensor for multiplication. Returns: torch.Tensor: Resulting dense tensor after multiplication. # Convert the dense tensor to a sparse CSR tensor sparse_csr_tensor = dense_tensor.to_sparse_csr() # Perform the matrix multiplication result = torch.matmul(sparse_csr_tensor, dense_matrix) # Return the result as a dense tensor return result.to_dense()"},{"question":"# Problem: Advanced Nearest Neighbors Classification **Objective:** You are required to implement a classification solution using the scikit-learn `neighbors` module. Your task is to use the `KNeighborsClassifier` in combination with `NeighborhoodComponentsAnalysis` to classify the Iris dataset. **Task:** 1. **Data Loading and Splitting:** - Load the Iris dataset from `sklearn.datasets`. - Split the dataset into a training set (30%) and a test set (70%) using `train_test_split` from `sklearn.model_selection`. 2. **Data Transformation and Classification:** - Instantiate and fit a `NeighborhoodComponentsAnalysis` object to the training data. - Use the transformation learned by NCA to fit a `KNeighborsClassifier` with `k=3` neighbors to the transformed training data. - Evaluate the classifier on the test set and print the accuracy. 3. **Evaluation:** - Print the accuracy score of the classifier on the test set. **Specifications:** - Use `random_state=42` for reproducibility in `train_test_split` and `NeighborhoodComponentsAnalysis`. - Ensure to standardize the dataset using `StandardScaler` from `sklearn.preprocessing` before applying NCA. **Example:** Here is a skeleton outline to guide your implementation: ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier from sklearn.metrics import accuracy_score # 1. Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # 2. Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, stratify=y, random_state=42) # 3. Create pipeline with StandardScaler, NeighborhoodComponentsAnalysis, and KNeighborsClassifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'nca\', NeighborhoodComponentsAnalysis(random_state=42)), (\'knn\', KNeighborsClassifier(n_neighbors=3)) ]) # 4. Fit pipeline to training data pipeline.fit(X_train, y_train) # 5. Evaluate on test data and print accuracy y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy: {accuracy:.4f}\\") ``` **Constraints:** - Ensure the `KNeighborsClassifier` has `k=3` neighbors. - Use `Pipeline` to chain the StandardScaler, NCA, and KNeighborsClassifier steps. **Note:** Be mindful of potential issues, such as overfitting or underfitting, and consider experimenting with different values of `n_neighbors` in practice.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.neighbors import NeighborhoodComponentsAnalysis, KNeighborsClassifier from sklearn.metrics import accuracy_score def iris_classification(): # 1. Load Iris dataset iris = load_iris() X, y = iris.data, iris.target # 2. Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, stratify=y, random_state=42) # 3. Create pipeline with StandardScaler, NeighborhoodComponentsAnalysis, and KNeighborsClassifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'nca\', NeighborhoodComponentsAnalysis(random_state=42)), (\'knn\', KNeighborsClassifier(n_neighbors=3)) ]) # 4. Fit pipeline to training data pipeline.fit(X_train, y_train) # 5. Evaluate on test data and return accuracy y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# Python Coding Assessment Question **Objective:** Implement a function to work with email messages using the `email.message.Message` class. Your implementation should demonstrate your understanding of working with headers, payloads, and serialization. **Task:** Write a function `process_email_message(raw_message)` that: 1. Parses the input raw email message, which is provided as a string. 2. Sets a specific header (e.g., `X-Processed`) to the value `\'Processed\'`. 3. Checks if the message is multipart and, if so, iterates over each subpart, printing the content type of each subpart. 4. Returns a serialized version of the modified email message as a string. **Function Signature:** ```python def process_email_message(raw_message: str) -> str: ``` **Input:** - `raw_message` (str): A raw string representing the entire email message. **Output:** - `output_message` (str): A string representing the serialized version of the modified email message. **Constraints:** 1. The input email message will always be a valid MIME email message. 2. The function should not raise any exceptions if the input follows the constraints. **Example:** For an input raw email message string that looks like: ``` From: user@example.com To: recipient@example.com Subject: Test email Content-Type: multipart/mixed; boundary=\\"===============123456789==\\" --===============123456789== Content-Type: text/plain; charset=\\"utf-8\\" This is the plain text part. --===============123456789== Content-Type: text/html; charset=\\"utf-8\\" <html><body>This is the HTML part.</body></html> --===============123456789==-- ``` The function should: 1. Add an `X-Processed: Processed` header. 2. Print: ``` Content type of subpart: text/plain Content type of subpart: text/html ``` 3. Return the modified message as a string with the new header added. **Notes:** - Use the `email` package from the standard library to parse and manipulate the email message. - Ensure all modifications are compliant with MIME standards.","solution":"from email import message_from_string from email.message import Message from email.generator import BytesGenerator from io import BytesIO def process_email_message(raw_message: str) -> str: Processes the raw email message and returns the modified message as a string. 1. Parses the input raw email message. 2. Sets a specific header (e.g., X-Processed) to the value \'Processed\'. 3. Checks if the message is multipart and, if so, iterates over each subpart, printing the content type of each subpart. 4. Returns a serialized version of the modified email message as a string. # Parse the raw email message into a Message object msg = message_from_string(raw_message) # Set the X-Processed header msg[\'X-Processed\'] = \'Processed\' # Check if the message is multipart if msg.is_multipart(): for part in msg.get_payload(): # Print the content type of each subpart print(f\\"Content type of subpart: {part.get_content_type()}\\") # Serialize the modified email message into a string buffer = BytesIO() generator = BytesGenerator(buffer) generator.flatten(msg) return buffer.getvalue().decode(\'utf-8\')"},{"question":"# PyTorch Coding Assessment Question **Objective:** Demonstrate your understanding of PyTorch\'s torch.func operations and limitations by implementing a function that applies vectorized computations and gradients using pure function transformations. **Problem Statement:** Implement a function `batch_mse_gradient` that calculates the Mean Squared Error (MSE) between a batch of predictions and targets, and then computes the gradient of the MSE with respect to the predictions using `torch.func`. The MSE should be computed for each example in the batch individually. **Requirements:** 1. **Function Signature:** ```python def batch_mse_gradient(predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor: ``` 2. **Input:** - `predictions`: A `torch.Tensor` of shape (N, *) where N is the batch size and * is any dimensions of the predictions. - `targets`: A `torch.Tensor` of shape (N, *) where N is the batch size and * matches the dimensions of `predictions`. 3. **Output:** - Returns a `torch.Tensor` of shape (N, *) containing the gradients of the MSE with respect to the predictions for each example in the batch. 4. **Constraints:** - Do not modify global variables inside your function. - Use the `torch.func.vmap` function to apply the MSE computation and gradient calculation in a vectorized manner. - Use the `torch.func.grad` function to compute gradients. - Ensure your implementations handle inputs of arbitrary dimensions (as long as dimensions of predictions and targets match). **Example:** ```python import torch predictions = torch.tensor([[0.0, 1.0], [2.0, 3.0]], requires_grad=True) targets = torch.tensor([[0.5, 1.5], [2.5, 3.5]]) gradients = batch_mse_gradient(predictions, targets) # Expected Output # gradients: tensor([[-0.5000, -0.5000], # [-0.5000, -0.5000]], grad_fn=<VmapBackward>) ``` **Function Template:** ```python import torch from torch.func import vmap, grad def mse_loss(pred, target): return torch.mean((pred - target) ** 2) def batch_mse_gradient(predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor: Compute the gradient of the MSE loss with respect to the predictions for each example in the batch. def mse_loss_with_grad(pred, target): mse = mse_loss(pred, target) return mse batched_grad_fn = vmap(grad(mse_loss_with_grad), in_dims=(0, 0)) gradients = batched_grad_fn(predictions, targets) return gradients ``` **Explanation:** 1. `mse_loss`: Function to compute MSE loss. 2. `mse_loss_with_grad`: Wrapper function to use `grad` for computing gradients. 3. `vmap`: Vectorized mapping of the gradient function over the batch dimensions. 4. `batch_mse_gradient`: The main function to compute batch-wise MSE gradients.","solution":"import torch from torch.func import vmap, grad def mse_loss(pred, target): return torch.mean((pred - target) ** 2) def batch_mse_gradient(predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor: Compute the gradient of the MSE loss with respect to the predictions for each example in the batch. def mse_loss_with_grad(pred, target): mse = mse_loss(pred, target) return mse batched_grad_fn = vmap(grad(mse_loss_with_grad), in_dims=(0, 0)) gradients = batched_grad_fn(predictions, targets) return gradients"},{"question":"<|Analysis Begin|> Based on the provided documentation of the \'curses\' module in Python, a question can be designed to assess the understanding of the fundamental and advanced concepts of handling terminal displays using curses. Key functionalities of the curses module include creating windows, handling input/output, drawing characters and shapes, managing colors, handling mouse events, and creating text boxes for interaction. The module supports advanced handling of user input, screen refresh, and real-time window management, which are essential for creating text-based user interfaces. **Challenging Areas:** 1. Understanding and correctly implementing window and screen management. 2. Handling colors and attributes. 3. Managing user inputs, including special keys and mouse events. 4. Creating and interacting with text boxes. By utilizing these functionalities, we can assess the student\'s capability in creating a terminal-based application involving window management, color handling, and user input. <|Analysis End|> <|Question Begin|> **Problem Statement:** You are tasked with creating a simple terminal-based chat application using the curses module in Python. The application should have a main window where the conversation appears and an input window where the user can type messages. **Requirements:** 1. Initialize the curses library and set up the main window and an input window. 2. The main window should show the conversation with timestamps. 3. The input window should allow the user to type messages and send them by pressing the Enter key. 4. Messages in the main window should be displayed with alternating colors for different users (for simplicity, assume two users only). 5. Use the mouse to click on the input window to start typing. 6. Handle the window resizing gracefully so that the layout adjusts correctly. **Detailed Steps:** 1. Initialize the curses library using `curses.initscr()`. 2. Use `newwin` to create the main window and input window. 3. Handle user inputs in the input window and display the messages in the main window with alternating colors. 4. Use `curses.textpad.Textbox` for the input window to manage editing. 5. Use `curses.mousemask` to detect mouse events and enable clicking on the input window. **Function Signature:** ```python import curses import curses.textpad from datetime import datetime def chat_application(stdscr): Function to run a simple terminal-based chat application using curses. Parameters: stdscr: The main window object provided by curses.wrapper. Returns: None pass if __name__ == \\"__main__\\": curses.wrapper(chat_application) ``` **Constraints:** - Ensure the main window shows the conversation in a readable format. - The input window should handle typical text editing capabilities (backspace, delete, etc.). - Implement proper error handling and clean exit using curses\' functionalities. - The application should handle resizing to maintain the layout. **Example Usage:** 1. The user runs the script. 2. Focus is on the input window where the user can type a message. 3. On pressing Enter, the message appears in the main window with a timestamp and in alternating colors. 4. The user can click on the input window to start typing if focus is lost. 5. Resizing adjusts the layout without losing the conversation history. **Assessment Criteria:** 1. Correct usage of the curses module to create and manage windows. 2. Handling of user input and display of messages. 3. Proper management of colors and attributes for alternating messages. 4. Handling mouse events and window resizing. 5. Clean and efficient code with proper use of curses functions and classes. Implement the `chat_application` function to fulfill the above requirements.","solution":"import curses import curses.textpad from datetime import datetime def chat_application(stdscr): curses.curs_set(1) curses.mousemask(1) # Initialize color pairs curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLACK) curses.init_pair(2, curses.COLOR_CYAN, curses.COLOR_BLACK) # Get screen height and width height, width = stdscr.getmaxyx() # Create windows main_win = curses.newwin(height - 3, width, 0, 0) input_win = curses.newwin(3, width, height - 3, 0) main_win.scrollok(True) main_win.idlok(True) input_win.nodelay(False) # Create a textbox for input window textbox = curses.textpad.Textbox(input_win, insert_mode=True) # Initial draw of windows main_win.border() input_win.border() stdscr.refresh() main_win.refresh() input_win.refresh() messages = [] user_turn = 0 # 0 for user1, 1 for user2 while True: stdscr.refresh() main_win.refresh() input_win.refresh() # Get the input input_win.clear() input_win.border() stdscr.refresh() message = textbox.edit().strip() if message.lower() == \'exit\': break timestamp = datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\') formatted_message = f\\"[{timestamp}] {\'User1:\' if user_turn == 0 else \'User2:\'} {message}\\" main_win.addstr(formatted_message + \'n\', curses.color_pair(1 + user_turn)) main_win.refresh() user_turn = 1 - user_turn curses.curs_set(0) if __name__ == \\"__main__\\": curses.wrapper(chat_application)"},{"question":"# Advanced Python Coding Assessment Question **Title:** Building and Validating a WSGI Application with `wsgiref` **Objective:** Demonstrate your understanding of the `wsgiref` module by building a simple WSGI application, integrating with `wsgiref.util`, and validating the application using `wsgiref.validate`. **Problem Statement:** You are required to implement a WSGI application that serves different responses based on the HTTP request path. The application should be structured to handle the following endpoints: 1. `/` - Returns a basic \\"Hello World\\" message. 2. `/info` - Returns the WSGI environment variables in a formatted string. 3. `/file` - Returns the contents of a file located in the server\'s directory (use a sample file named `example.txt`). 4. Any other path should return a \\"404 Not Found\\" message. Additionally, you should validate the WSGI application using the `wsgiref.validate` module to ensure compliance with the WSGI specifications. Finally, serve the application using `wsgiref.simple_server`. **Requirements:** 1. Implement the WSGI application function named `my_app(environ, start_response)`. 2. Use `wsgiref.util` functions where appropriate to handle and manipulate the WSGI environment. 3. Validate the WSGI application using `wsgiref.validate.validator`. 4. Serve the validated WSGI application using `wsgiref.simple_server.make_server`. **Input and Output:** - **Input:** HTTP requests with different paths. - **Output:** HTTP responses: - For `/` path: \\"Hello World\\" - For `/info` path: Formatted string of WSGI environment variables - For `/file` path: Contents of `example.txt` - For any other path: \\"404 Not Found\\" **Constraints:** - The WSGI application must be validated using the `wsgiref.validate.validator`. - Use the `wsgiref.util.FileWrapper` to serve the file contents. - Ensure the server responds correctly to different request paths. **Example Usage:** ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.util import setup_testing_defaults, FileWrapper def my_app(environ, start_response): # Your implementation here # Validate the application validated_app = validator(my_app) with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` In the implementation of `my_app`: - Use `wsgiref.util.guess_scheme` for determining the scheme (http or https). - Use `wsgiref.util.request_uri` to construct the request URI. - Use `wsgiref.util.shift_path_info` if you need to process PATH_INFO for dynamic routing. Ensure that your WSGI application adheres to the specifications, and use the validation tool to check for conformance. Also, provide a suitable error response for invalid paths. **Note:** Create a sample file named `example.txt` in the same directory as your script for the `/file` endpoint to work correctly.","solution":"from wsgiref.util import setup_testing_defaults, request_uri, FileWrapper from wsgiref.validate import validator from wsgiref.simple_server import make_server def my_app(environ, start_response): setup_testing_defaults(environ) path = environ.get(\'PATH_INFO\', \'\') if path == \'/\': status = \'200 OK\' response_headers = [(\'Content-type\', \'text/plain\')] start_response(status, response_headers) return [b\\"Hello World\\"] elif path == \'/info\': status = \'200 OK\' response_headers = [(\'Content-type\', \'text/plain\')] start_response(status, response_headers) env_info = \\"n\\".join([f\\"{k}: {v}\\" for k, v in sorted(environ.items())]) return [env_info.encode(\'utf-8\')] elif path == \'/file\': try: file = open(\'example.txt\', \'rb\') wrapper = FileWrapper(file) status = \'200 OK\' response_headers = [(\'Content-type\', \'text/plain\')] start_response(status, response_headers) return wrapper except FileNotFoundError: status = \'404 Not Found\' response_headers = [(\'Content-type\', \'text/plain\')] start_response(status, response_headers) return [b\\"404 Not Found: example.txt not found.\\"] else: status = \'404 Not Found\' response_headers = [(\'Content-type\', \'text/plain\')] start_response(status, response_headers) return [b\\"404 Not Found\\"] # Validate the application validated_app = validator(my_app) if __name__ == \'__main__\': with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"<|Analysis Begin|> The `hashlib` module documentation provides comprehensive details on various secure hash and message digest algorithms. This includes constructors for numerous hash algorithms, usage examples, and specific guidelines on extending hash functions for specialized uses such as key derivation and randomization. Key features highlighted include creating hash objects, updating them with data, and obtaining the resulting digest. Given the extensive capabilities offered by the `hashlib` module, an appropriate and challenging coding assessment question can focus on creating a secure password manager that incorporates hash algorithms for hashing and verifying passwords. This would test the student\'s understanding of: 1. Creating hashing objects using different algorithms. 2. Using hash object methods (`update`, `digest`, and `hexdigest`). 3. Employing key derivation techniques (`pbkdf2_hmac` and `scrypt`). <|Analysis End|> <|Question Begin|> # Secure Password Manager **Objective:** Implement a secure password manager using Python’s `hashlib` module. The password manager should be able to: 1. Hash passwords using a secure algorithm. 2. Verify passwords against their hash. 3. Allow for secure storage of passwords using key derivation. **Requirements:** 1. **Function: `hash_password`** - **Input:** - `password` (str): The plain text password. - **Output:** - `hashed_password` (str): The hashed password in hexadecimal format. - **Constraints:** - Use a salt to enhance security. - Choose a suitable secure hash algorithm such as `\\"sha256\\"`. - **Example:** ```python hp = hash_password(\\"my_s3cr3t_pass\\") # hp is a hexadecimal string of the hashed password ``` 2. **Function: `verify_password`** - **Input:** - `password` (str): The plain text password. - `hashed_password` (str): The previously hashed password. - **Output:** - `is_valid` (bool): Return `True` if the password matches the hash, `False` otherwise. - **Constraints:** - Use the same salt and hash algorithm used in `hash_password`. - **Example:** ```python # Assume hp is the hashed password obtained from hash_password(\\"my_s3cr3t_pass\\") is_valid = verify_password(\\"my_s3cr3t_pass\\", hp) assert is_valid == True ``` 3. **Function: `derive_key`** - **Input:** - `password` (str): The plain text password. - `salt` (bytes): A salt for the derivation. - **Output:** - `key` (bytes): A securely derived key. - **Constraints:** - Use `pbkdf2_hmac` with a large number of iterations or `scrypt` for the key derivation function. - **Example:** ```python dk = derive_key(\\"another_s3cr3t_pass\\", b\'some_salt\') # dk is a securely derived key ``` **Performance Requirements:** - Ensure the functions are efficient and secure, especially when dealing with large-scale data or high-frequency usages. # Instructions: 1. Implement the `hash_password`, `verify_password`, and `derive_key` functions as specified. 2. Make sure to handle potential edge cases and invalid inputs gracefully. 3. Include proper documentation and comments in your code to explain the logic. **Note:** You can make use of the following example code snippets from the hashlib documentation for guidance: ```python import hashlib # Example for hashing a password m = hashlib.sha256() m.update(b\\"YourPassword\\" + b\\"YourSalt\\") hashed_password = m.hexdigest() # Example for verifying a password m = hashlib.sha256() m.update(b\\"YourPassword\\" + b\\"YourSalt\\") if m.hexdigest() == stored_hashed_password: print(\\"Password matches\\") else: print(\\"Password does not match\\") # Example for deriving a key import os from hashlib import pbkdf2_hmac salt = os.urandom(16) key = pbkdf2_hmac(\'sha256\', b\'password\', salt, 100000) print(key.hex()) ```","solution":"import hashlib import os from hashlib import pbkdf2_hmac SALT_LENGTH = 16 ITERATIONS = 100000 def hash_password(password): Hash a password using SHA-256 with a salt. salt = os.urandom(SALT_LENGTH) hash_object = hashlib.sha256() hash_object.update(password.encode(\'utf-8\') + salt) hashed_password = salt + hash_object.digest() return hashed_password.hex() def verify_password(password, hashed_password_hex): Verify a password against its SHA-256 hash with a salt. hashed_password = bytes.fromhex(hashed_password_hex) salt = hashed_password[:SALT_LENGTH] original_hash = hashed_password[SALT_LENGTH:] hash_object = hashlib.sha256() hash_object.update(password.encode(\'utf-8\') + salt) return original_hash == hash_object.digest() def derive_key(password, salt): Derive a key from a password using PBKDF2-HMAC-SHA256. key = pbkdf2_hmac(\'sha256\', password.encode(\'utf-8\'), salt, ITERATIONS) return key"},{"question":"# Question: Implement a Custom Completer for Function Parameter Suggestions You are required to implement a class called `FunctionParameterCompleter` that extends the functionality of the `rlcompleter.Completer` class. This class will provide intelligent completion suggestions for function parameters. The mechanism should work as follows: 1. **Input Format**: - `prefix: str`: a string containing the partial input for which suggestions are needed. - `state: int`: an integer representing the state of completion (0 for initial call, 1 for the next suggestion, and so on). 2. **Output Format**: - A string representing the suggestion for the given state. - If there are no more suggestions, return `None`. 3. **Behavior**: - If `prefix` does not end with a function call (e.g., `func(`), delegate to normal attribute/method completion. - If `prefix` ends with a function call, provide suggestions for the parameters of that function. Assume parameter names are retrievable using the `inspect` module. - Handle exceptions and ensure robustness by catching and silencing any unforeseen errors. 4. **Constraints**: - Only suggest parameters of known functions in the current namespace and builtins. - Ensure no evaluation of the function itself occurs. Implement the `FunctionParameterCompleter` class as described. Example ```python import rlcompleter import inspect class FunctionParameterCompleter(rlcompleter.Completer): def complete(self, prefix, state): # Your implementation goes here pass # Example usage: # Assuming a function def example_function(param1, param2): pass exists in the namespace completer = FunctionParameterCompleter() print(completer.complete(\\"example_function(\\", 0)) # Output: \'param1\' print(completer.complete(\\"example_function(\\", 1)) # Output: \'param2\' print(completer.complete(\\"example_function(\\", 2)) # Output: None ``` Notes: - Use the `inspect` module to retrieve parameter names of functions. - Ensure the implementation is efficient and handles edge cases gracefully.","solution":"import rlcompleter import inspect class FunctionParameterCompleter(rlcompleter.Completer): def __init__(self, namespace=None): super().__init__(namespace) self.param_suggestions = [] self.prefix = \\"\\" def complete(self, prefix, state): if state == 0: self.param_suggestions = [] self.prefix = prefix # Check if the prefix ends with an opening parenthesis, indicating a function call if \\"(\\" in prefix: function_name = prefix[:prefix.index(\\"(\\")].strip() if function_name in self.namespace: func = self.namespace[function_name] if inspect.isfunction(func) or inspect.isbuiltin(func): self.param_suggestions = [ p.name for p in inspect.signature(func).parameters.values() ] if state < len(self.param_suggestions): return self.param_suggestions[state] return None"},{"question":"Objective Your task is to demonstrate your proficiency in using the seaborn library for creating and customizing boxplots. You will be required to load a dataset, perform some data manipulation, and generate boxplots that meet specific criteria. Question Write a Python function called `create_boxplots` that does the following: 1. Loads the \'tips\' dataset from seaborn. 2. Creates a vertical boxplot of the total bill amount (`total_bill`) grouped by day (`day`). 3. Adds a nested grouping by time (`time`) to show separate boxplots for lunch and dinner. 4. Customizes the appearance by drawing the boxes as line art with a 0.1 gap, setting the whiskers to cover the full range of the data (0 to 100 percentile), and making the boxes narrower with a width of 0.5. 5. Customizes the plot further by setting: - The color of the boxes to light gray (`color=\\".8\\"`). - The color of the lines to dark blue (`linecolor=\\"#137\\"`). - The line width to 0.75. 6. Ensures that the plot has a title \\"Total Bill Distribution by Day and Time\\". **Function signature:** ```python def create_boxplots() -> None: pass ``` **Expected Output:** The function should display a customized seaborn boxplot as specified above. **Constraints:** - You must use seaborn and matplotlib for plotting. - The function should not return any value; it should only display the plot. **Hint:** - Review the seaborn and matplotlib documentation if needed. - Consider using `sns.set_theme()` to set a consistent aesthetic for your plots. **Assessment Criteria:** - Correctness: The plot should accurately reflect the requested grouping and customizations. - Code Quality: The code should be well-organized and follow standard Python conventions. - Plot Aesthetics: The final plot should be clear, with all elements properly customized as per the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_boxplots() -> None: # Load the \'tips\' dataset tips = sns.load_dataset(\'tips\') # Create boxplot with the specified customizations sns.set_theme(style=\'whitegrid\') ax = sns.boxplot( x=\'day\', y=\'total_bill\', hue=\'time\', data=tips, linewidth=0.75, width=0.5, fliersize=0, # no fliers (outliers) boxprops=dict(facecolor=\\".8\\", edgecolor=\\"#137\\", linewidth=0.75), whiskerprops=dict(color=\\"#137\\", linewidth=0.75), capprops=dict(color=\\"#137\\", linewidth=0.75), medianprops=dict(color=\\"#137\\", linewidth=0.75) ) # Set plot title ax.set_title(\'Total Bill Distribution by Day and Time\') # Show plot plt.show()"},{"question":"**Objective:** You are required to use the `types` module to dynamically create classes and interact with their attributes using some of its advanced features. **Task:** 1. Using `types.new_class`, dynamically create a class named `DynamicPerson`. This class should: - Inherit from a base class `Person`. - Have an attribute `species` set to `\\"Homo sapiens\\"` by default. - Have an initializer (`__init__`) that sets instance attributes `name` and `age`. 2. Create an instance of `DynamicPerson`, named `person_instance`, with the `name` set to `\\"Charlie\\"` and the `age` set to `30`. 3. Using the `types` module\'s classes `SimpleNamespace` and `MappingProxyType`: - Create a `SimpleNamespace` named `person_details` that includes the following details: `name` as `\\"Frank\\"`, `age` as `25`, and `address` as `\\"123 Main St\\"`. - Create a read-only proxy of `person_details` using `MappingProxyType` and name it `proxy_details`. **Implementation Details:** - Define a base class `Person` with no additional attributes or methods. - Implement the solution using the `types` module as specified. **Constraints:** - The class `DynamicPerson` should properly inherit from `Person`. - Ensure that the `species` attribute is correctly set for the class. - The instance `person_instance` should correctly reflect the defined attributes. - The `SimpleNamespace` and its proxy should accurately represent the specified details. **Example:** ```python # Example output of person_instance attributes print(person_instance.name) # Output: Charlie print(person_instance.age) # Output: 30 print(person_instance.species) # Output: Homo sapiens # Example output of proxy_details print(proxy_details.name) # Output: Frank print(proxy_details.age) # Output: 25 print(proxy_details.address) # Output: 123 Main St # Attempting to modify proxy_details should raise a TypeError try: proxy_details.age = 26 except TypeError as e: print(e) # Output: \'mappingproxy\' object does not support item assignment ``` **Note:** You may not use any direct class definitions or imports other than from the `types` module.","solution":"import types # Define a base class Person with no additional attributes or methods class Person: pass # Create the DynamicPerson class using types.new_class DynamicPerson = types.new_class( \'DynamicPerson\', (Person,), {}, lambda ns: ns.update({\'species\': \'Homo sapiens\'}) ) # Add an initializer to DynamicPerson using the __init__ method def init(self, name, age): self.name = name self.age = age DynamicPerson.__init__ = init # Create an instance of DynamicPerson person_instance = DynamicPerson(\'Charlie\', 30) # Create a SimpleNamespace with the given details person_details = types.SimpleNamespace(name=\'Frank\', age=\'25\', address=\'123 Main St\') # Create a read-only proxy of person_details using MappingProxyType proxy_details = types.MappingProxyType(person_details.__dict__)"},{"question":"You are given a dataset containing sales data for different products across multiple regions over several months. Your task is to perform a series of data manipulations to analyze the data and derive meaningful insights. **Dataset Description:** The dataset has the following columns: - `Date`: The date of the sale (format: YYYY-MM-DD) - `Region`: The region where the sale was made - `Product`: The product name - `Sales`: The amount of sales made **Input:** A DataFrame `df` containing the sales data. **Output:** A DataFrame that provides the following insights: 1. The total sales for each product across all regions and all months. 2. The total sales for each region across all products and all months. 3. For each month, the region with the highest total sales. 4. For each region, the product with the highest total sales. **Constraints:** - The input DataFrame `df` is guaranteed to have at least one row of data. - You should use appropriate pandas functions to derive these insights. - Your solution should be optimized for performance and readability. **Example:** ```python import pandas as pd data = { \'Date\': [\'2023-01-01\', \'2023-01-01\', \'2023-02-01\', \'2023-02-01\'], \'Region\': [\'North\', \'South\', \'North\', \'South\'], \'Product\': [\'A\', \'A\', \'B\', \'B\'], \'Sales\': [100, 150, 200, 250] } df = pd.DataFrame(data) # Your function should return the following DataFrame # Total sales for each product # Product Total_Sales # A 250 # B 450 # Total sales for each region # Region Total_Sales # North 300 # South 400 # Region with the highest total sales for each month # Date Region Total_Sales # 2023-01-01 South 150 # 2023-02-01 South 250 # Product with the highest total sales for each region # Region Product Total_Sales # North B 200 # South B 250 ``` **Note:** - Clearly comment your code explaining each step. - Ensure that the final DataFrame has appropriate column names as shown in the example.","solution":"import pandas as pd def analyze_sales_data(df): # Ensure \'Date\' column is in datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # 1. Total sales for each product total_sales_per_product = df.groupby(\'Product\')[\'Sales\'].sum().reset_index() total_sales_per_product.columns = [\'Product\', \'Total_Sales\'] # 2. Total sales for each region total_sales_per_region = df.groupby(\'Region\')[\'Sales\'].sum().reset_index() total_sales_per_region.columns = [\'Region\', \'Total_Sales\'] # 3. For each month, the region with the highest total sales df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') monthly_sales_by_region = df.groupby([\'Month\', \'Region\'])[\'Sales\'].sum().reset_index() idx = monthly_sales_by_region.groupby(\'Month\')[\'Sales\'].idxmax() highest_sales_region_per_month = monthly_sales_by_region.loc[idx].reset_index(drop=True) highest_sales_region_per_month.columns = [\'Date\', \'Region\', \'Total_Sales\'] highest_sales_region_per_month[\'Date\'] = highest_sales_region_per_month[\'Date\'].dt.to_timestamp() # 4. For each region, the product with the highest total sales sales_by_product_region = df.groupby([\'Region\', \'Product\'])[\'Sales\'].sum().reset_index() idx = sales_by_product_region.groupby(\'Region\')[\'Sales\'].idxmax() highest_product_sales_per_region = sales_by_product_region.loc[idx].reset_index(drop=True) highest_product_sales_per_region.columns = [\'Region\', \'Product\', \'Total_Sales\'] return total_sales_per_product, total_sales_per_region, highest_sales_region_per_month, highest_product_sales_per_region"},{"question":"# Coding Assessment: Advanced GroupBy Operations in Pandas Objective: Use the pandas library to perform a series of operations on a dataset to demonstrate your understanding of the `GroupBy` functionality. Dataset: You are provided with a CSV file named `sales_data.csv` which contains the following columns: - `OrderID`: Unique identifier for each order. - `Product`: The product sold. - `Category`: Category of the product. - `Quantity`: The quantity of the product sold. - `Price`: The unit price of the product. - `Country`: The country where the order was placed. - `OrderDate`: The date when the order was placed. The CSV file has to be read into a pandas DataFrame for processing. Tasks: 1. **Reading Data:** - Load the dataset into a pandas DataFrame. 2. **Data Aggregation:** - Group the data by `Country` and `Category`, and calculate the following for each group: - Total `Quantity` sold. - Average `Price` of products. - Total sales (`Quantity` * `Price`). - Reset the index after grouping and aggregation. 3. **Data Transformation:** - For each `Category` within `Country`, find the `Product` with the highest sales. 4. **Optional Visualization** (Bonus): - Display a bar chart showing the total sales per `Country` for each `Category`. Expected Code Structure: ```python import pandas as pd import matplotlib.pyplot as plt # Step 1: Load data file_path = \'sales_data.csv\' data = pd.read_csv(file_path) # Step 2: Data Aggregation grouped = data.groupby([\'Country\', \'Category\']).agg({ \'Quantity\': \'sum\', \'Price\': \'mean\', \'TotalSales\': lambda x: (x[\'Quantity\'] * x[\'Price\']).sum() }).reset_index() # Step 3: Data Transformation highest_sales = grouped.loc[grouped.groupby([\'Country\', \'Category\'])[\'TotalSales\'].idxmax()] # Optional Step 4: Visualization for country in grouped[\'Country\'].unique(): country_data = grouped[grouped[\'Country\'] == country] country_data.plot(kind=\'bar\', x=\'Category\', y=\'TotalSales\', title=f\'Total Sales in {country}\') plt.show() ``` Constraints: - Do not use any libraries other than `pandas` and `matplotlib`. - Ensure that your code handles large datasets efficiently. Submission: - Submit your script as a `.py` file. - Ensure your code is well-commented and properly formatted. - Include a brief explanation of your approach in a text file. Expected Output: 1. A DataFrame showing the total quantity sold, average price, and total sales for each `Country` and `Category`. 2. A DataFrame showing the product with the highest sales for each `Country` and `Category`. 3. (Optional) Bar charts visualizing total sales per `Country` for each `Category`.","solution":"import pandas as pd def load_data(file_path): Loads the dataset into a pandas DataFrame. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded data. data = pd.read_csv(file_path) return data def aggregate_data(data): Groups the data by Country and Category, and calculates the total quantity sold, average price of products, and total sales. Parameters: data (pd.DataFrame): The input data. Returns: pd.DataFrame: The aggregated data. data[\'TotalSales\'] = data[\'Quantity\'] * data[\'Price\'] grouped = data.groupby([\'Country\', \'Category\'], as_index=False).agg({ \'Quantity\': \'sum\', \'Price\': \'mean\', \'TotalSales\': \'sum\' }) return grouped def highest_sales_product(data): Finds the product with the highest sales for each Category within each Country. Parameters: data (pd.DataFrame): The input data. Returns: pd.DataFrame: The DataFrame with products having highest sales in each Category and Country. data[\'TotalSales\'] = data[\'Quantity\'] * data[\'Price\'] idx = data.groupby([\'Country\', \'Category\'])[\'TotalSales\'].idxmax() highest_sales = data.loc[idx, [\'Country\', \'Category\', \'Product\', \'TotalSales\']] return highest_sales"},{"question":"Objective Demonstrate your understanding of the `importlib.metadata` package by implementing a function that gathers and processes various pieces of metadata for an installed package. Problem Statement Implement a function `get_package_details(package_name: str) -> dict` that takes the name of an installed Python package as input and returns a dictionary containing detailed information about the package. This should include: 1. The package version. 2. All entry points, organized by group. 3. The metadata of the package (all metadata keys and their corresponding values). 4. List of all files in the package along with their size and hash. 5. All the requirements for the package. # Function Signature ```python def get_package_details(package_name: str) -> dict: pass ``` # Expected Input - `package_name (str)`: The name of an installed Python package (e.g., \'wheel\'). # Expected Output - `dict`: A dictionary with the following structure: ```python { \'version\': str, # e.g., \'0.32.3\' \'entry_points\': { \'group1\': [EntryPoint, ...], # Where EntryPoint is a representation of an entry point object \'group2\': [EntryPoint, ...], ... }, \'metadata\': { \'key1\': \'value1\', \'key2\': \'value2\', ... }, \'files\': [ {\'path\': \'relative/path/to/file\', \'size\': int, \'hash\': str}, # Each dict represents a file ... ], \'requires\': [str, ...] # List of all package requirements (e.g., [\\"pytest (>=3.0.0) ; extra == \'test\'\\", ...]) } ``` # Constraints 1. Assume the package is always correctly installed and available in the environment. 2. Utilize functions from the `importlib.metadata` package exclusively for retrieving the required information. # Example ```python result = get_package_details(\'wheel\') print(result) ``` Expected output: ```python { \'version\': \'0.32.3\', \'entry_points\': { \'console_scripts\': [ EntryPoint(name=\'wheel\', value=\'wheel.cli:main\', group=\'console_scripts\', ...) ], ... }, \'metadata\': { \'Metadata-Version\': \'2.1\', \'Name\': \'wheel\', ... }, \'files\': [ {\'path\': \'wheel/util.py\', \'size\': 859, \'hash\': \'<FileHash ...>\'}, ... ], \'requires\': [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] } ``` # Notes - Ensure proper error handling for key lookups and missing metadata fields. - Test the function locally with different packages to ensure comprehensive coverage of as many cases as possible.","solution":"import importlib.metadata def get_package_details(package_name: str) -> dict: Retrieves detailed information about an installed Python package. Parameters: package_name (str): The name of the installed package. Returns: dict: A dictionary containing version, entry points, metadata, files, and requirements of the package. details = {} # Get package version details[\'version\'] = importlib.metadata.version(package_name) # Get entry points entry_points = importlib.metadata.entry_points(group=None) details[\'entry_points\'] = {} for entry_point in entry_points: if entry_point.dist.name == package_name: details[\'entry_points\'].setdefault(entry_point.group, []).append(entry_point) # Get metadata metadata = importlib.metadata.metadata(package_name) details[\'metadata\'] = {key: metadata[key] for key in metadata.keys()} # Get files in the package files = importlib.metadata.files(package_name) details[\'files\'] = [] for file in files: file_info = { \'path\': str(file), \'size\': file.size, \'hash\': str(file.hash) } details[\'files\'].append(file_info) # Get requirements details[\'requires\'] = importlib.metadata.requires(package_name) or [] return details"},{"question":"Objective Demonstrate proficiency in using the `xml.dom` module to parse and manipulate XML documents. Problem Statement You are provided with an XML string representing a catalog of a bookstore. Your task is to write a Python function `process_bookstore(xml_string)` that performs the following operations using the `xml.dom` module: 1. Parse the provided XML string into a DOM document. 2. Extract and return a list of all book titles in the catalog. 3. Add a new book to the catalog with the following details: - `title`: \\"Learning Python\\" - `author`: \\"Mark Lutz\\" - `year`: \\"2013\\" - `price`: \\"30.00\\" 4. Remove any books from the catalog where the price is greater than 50.00. 5. Return the modified XML as a string. Input - `xml_string` (str): A string containing the XML data for the bookstore catalog. Output - A tuple containing: 1. A list of book titles (list of str). 2. A string representing the modified XML data. Constraints - Ensure that the XML string is well-formed. - The function should handle any exceptions that may occur during parsing or manipulation and return appropriate error messages. Example Given the following XML string: ```xml <catalog> <book> <title>XML Developer\'s Guide</title> <author>Gambardella, Matthew</author> <year>2000</year> <price>44.95</price> </book> <book> <title>Midnight Rain</title> <author>Ralls, Kim</author> <year>2001</year> <price>5.95</price> </book> ... </catalog> ``` Your function should correctly process this data and return: ```python ( [\\"XML Developer\'s Guide\\", \\"Midnight Rain\\", ...], # List of book titles \'<catalog>...</catalog>\' # Modified XML string with a new book added and books priced over 50 removed ) ``` Implementation Details 1. Use the `xml.dom.minidom` module to parse and manipulate the XML data. 2. Extract book titles by navigating the DOM tree. 3. Create and append new `Element` nodes to add a new book. 4. Remove the `Element` nodes for books priced over 50 by navigating and modifying the DOM tree. 5. Convert the modified DOM tree back to an XML string. Notes - Ensure that the XML string is properly formatted in the output. - Handle exceptions gracefully and provide clear error messages if parsing or manipulation fails. ```python from xml.dom.minidom import parseString def process_bookstore(xml_string): try: # Parsing the XML string into a DOM document dom = parseString(xml_string) # Extracting all book titles titles = [] for book in dom.getElementsByTagName(\\"book\\"): title = book.getElementsByTagName(\\"title\\")[0].firstChild.nodeValue titles.append(title) # Adding a new book to the catalog new_book = dom.createElement(\\"book\\") new_title = dom.createElement(\\"title\\") new_title.appendChild(dom.createTextNode(\\"Learning Python\\")) new_book.appendChild(new_title) new_author = dom.createElement(\\"author\\") new_author.appendChild(dom.createTextNode(\\"Mark Lutz\\")) new_book.appendChild(new_author) new_year = dom.createElement(\\"year\\") new_year.appendChild(dom.createTextNode(\\"2013\\")) new_book.appendChild(new_year) new_price = dom.createElement(\\"price\\") new_price.appendChild(dom.createTextNode(\\"30.00\\")) new_book.appendChild(new_price) catalog = dom.getElementsByTagName(\\"catalog\\")[0] catalog.appendChild(new_book) # Removing books priced over 50.00 for book in catalog.getElementsByTagName(\\"book\\"): price = float(book.getElementsByTagName(\\"price\\")[0].firstChild.nodeValue) if price > 50.00: catalog.removeChild(book) # Returning the list of titles and the modified XML as string modified_xml_string = dom.toxml() return (titles, modified_xml_string) except Exception as e: return ([], str(e)) # Test the function with the provided XML string example xml_string = \'\'\' <catalog> <book> <title>XML Developer\'s Guide</title> <author>Gambardella, Matthew</author> <year>2000</year> <price>44.95</price> </book> <book> <title>Midnight Rain</title> <author>Ralls, Kim</author> <year>2001</year> <price>5.95</price> </book> </catalog> \'\'\' print(process_bookstore(xml_string)) ```","solution":"from xml.dom.minidom import parseString def process_bookstore(xml_string): try: # Parsing the XML string into a DOM document dom = parseString(xml_string) # Extracting all book titles titles = [] for book in dom.getElementsByTagName(\\"book\\"): title = book.getElementsByTagName(\\"title\\")[0].firstChild.nodeValue titles.append(title) # Adding a new book to the catalog new_book = dom.createElement(\\"book\\") new_title = dom.createElement(\\"title\\") new_title.appendChild(dom.createTextNode(\\"Learning Python\\")) new_book.appendChild(new_title) new_author = dom.createElement(\\"author\\") new_author.appendChild(dom.createTextNode(\\"Mark Lutz\\")) new_book.appendChild(new_author) new_year = dom.createElement(\\"year\\") new_year.appendChild(dom.createTextNode(\\"2013\\")) new_book.appendChild(new_year) new_price = dom.createElement(\\"price\\") new_price.appendChild(dom.createTextNode(\\"30.00\\")) new_book.appendChild(new_price) catalog = dom.getElementsByTagName(\\"catalog\\")[0] catalog.appendChild(new_book) # Removing books priced over 50.00 for book in list(catalog.getElementsByTagName(\\"book\\")): price = float(book.getElementsByTagName(\\"price\\")[0].firstChild.nodeValue) if price > 50.00: catalog.removeChild(book) # Returning the list of titles and the modified XML as string modified_xml_string = dom.toxml() return (titles, modified_xml_string) except Exception as e: return ([], str(e))"},{"question":"Working with Python Datetime API **Objective**: Implement a Python function `create_and_extract_datetime_info()` that leverages the datetime C API to create datetime objects and extract specific fields. This function will demonstrate your understanding of creating and manipulating datetime objects at a low level using Python\'s C API. Function Signature ```python def create_and_extract_datetime_info(year: int, month: int, day: int, hour: int, minute: int, second: int, usecond: int) -> dict: ... ``` Input - `year` (int): Year component of the datetime. - `month` (int): Month component of the datetime. - `day` (int): Day component of the datetime. - `hour` (int): Hour component of the datetime. - `minute` (int): Minute component of the datetime. - `second` (int): Second component of the datetime. - `usecond` (int): Microsecond component of the datetime. Output - Returns a dictionary containing the extracted components from the created datetime object. ```python { \\"year\\": year, \\"month\\": month, \\"day\\": day, \\"hour\\": hour, \\"minute\\": minute, \\"second\\": second, \\"microsecond\\": usecond } ``` Constraints 1. The `year` must be a positive integer. 2. The `month` must be an integer between 1 and 12. 3. The `day` must be an integer between 1 and 31. 4. The `hour` must be an integer between 0 and 23. 5. The `minute` must be an integer between 0 and 59. 6. The `second` must be an integer between 0 and 59. 7. The `usecond` must be an integer between 0 and 999999. Performance Requirements - The function should handle typical datetime manipulation tasks efficiently without significant performance overhead. Task Description 1. Implement the function `create_and_extract_datetime_info()` to create a Python `datetime.datetime` object using the provided input parameters. 2. Use the appropriate C API macros to extract each component (year, month, day, hour, minute, second, and microsecond) from the created datetime object. 3. Return the extracted components as a dictionary. Example ```python input_data = { \\"year\\": 2023, \\"month\\": 10, \\"day\\": 20, \\"hour\\": 15, \\"minute\\": 30, \\"second\\": 45, \\"usecond\\": 123456 } output = create_and_extract_datetime_info(**input_data) print(output) # Expected: {\'year\': 2023, \'month\': 10, \'day\': 20, \'hour\': 15, \'minute\': 30, \'second\': 45, \'microsecond\': 123456} ``` **Additional Information**: Use appropriate error handling to manage incorrect inputs that do not adhere to the specified constraints.","solution":"from datetime import datetime def create_and_extract_datetime_info(year: int, month: int, day: int, hour: int, minute: int, second: int, usecond: int) -> dict: Creates a datetime object from given components and extracts the components into a dictionary. Arguments: year -- Year component (positive integer) month -- Month component (1-12) day -- Day component (1-31) hour -- Hour component (0-23) minute -- Minute component (0-59) second -- Second component (0-59) usecond -- Microsecond component (0-999999) Returns: A dictionary containing the extracted components from the created datetime object. # Creating datetime object dt = datetime(year, month, day, hour, minute, second, usecond) # Extracting components dt_info = { \\"year\\": dt.year, \\"month\\": dt.month, \\"day\\": dt.day, \\"hour\\": dt.hour, \\"minute\\": dt.minute, \\"second\\": dt.second, \\"microsecond\\": dt.microsecond } return dt_info"},{"question":"**Title: Temporary File and Directory Management** **Objective:** Write a Python function that uses the `tempfile` module to perform a series of file and directory operations. Your function should create a temporary directory, create several temporary files within this directory, write data to these files, read the data back, and finally clean up all temporary resources. **Function Signature:** ```python def manage_temporary_files(data_list: List[Tuple[str, bytes]], prefix: str = \'tmp\', suffix: str = \'.txt\') -> Dict[str, bytes]: pass ``` **Input:** - `data_list`: A list of tuples, where each tuple contains a string representing the filename without extension and a byte string representing the data to be written to the file. - Example: `[(\\"file1\\", b\\"Hello World!\\"), (\\"file2\\", b\\"Python is awesome!\\")]` - `prefix` (optional): A string prefix for the filenames (default is `\'tmp\'`). - `suffix` (optional): A string suffix (extension) for the filenames (default is `\'.txt\'`). **Output:** - Returns a dictionary where the keys are the final pathnames of the temporary files and the values are the byte contents read from these files. - Example: `{\'/path/to/tmpfile1.txt\': b\'Hello World!\', \'/path/to/tmpfile2.txt\': b\'Python is awesome!\'}` **Constraints:** - The function must ensure that all temporary files are created within the same temporary directory. - The function should automatically clean up all temporary files and the directory once the data is read and returned. - Do not use the `mktemp` function due to its security risks. **Requirements:** 1. Use `tempfile.TemporaryDirectory` to create a temporary directory. 2. Use `tempfile.NamedTemporaryFile` to create temporary files within the created temporary directory. 3. Write the provided data to each temporary file. 4. Read back the data from each temporary file. 5. Ensure all temporary resources are cleaned up after reading the data. **Example:** ```python data_list = [(\\"file1\\", b\\"Hello World!\\"), (\\"file2\\", b\\"Python is awesome!\\")] result = manage_temporary_files(data_list, prefix=\'tmpfile\', suffix=\'.txt\') # Sample output (note: actual paths may vary) # { # \'/tmp/tmpfile1.txt\': b\'Hello World!\', # \'/tmp/tmpfile2.txt\': b\'Python is awesome!\' # } ``` You are expected to handle any exceptions that might occur during file operations and ensure that all temporary files and directories are properly deleted at the end of the function\'s execution.","solution":"from typing import List, Tuple, Dict import tempfile import os def manage_temporary_files(data_list: List[Tuple[str, bytes]], prefix: str = \'tmp\', suffix: str = \'.txt\') -> Dict[str, bytes]: result = {} with tempfile.TemporaryDirectory() as temp_dir: for name, data in data_list: with tempfile.NamedTemporaryFile(dir=temp_dir, prefix=prefix + name, suffix=suffix, delete=False) as temp_file: temp_file.write(data) temp_file_path = temp_file.name with open(temp_file_path, \'rb\') as temp_file: file_data = temp_file.read() result[temp_file_path] = file_data return result"},{"question":"Objective: Create a comprehensive data visualization using Seaborn\'s diverging color palettes. Problem Description: You are given a dataset `data.csv` that contains sales data over a number of years. Each row in the dataset represents the sales of a product for a specific year and quarter. The dataset has the following structure: ``` Year, Quarter, Product, Sales 2020, Q1, A, 1500 2020, Q2, A, 1600 2020, Q3, A, 1200 2020, Q4, A, 1800 ... ``` Using this data, you need to create an informative heatmap that displays the sales performance of each product over the quarters of different years. The heatmap should use a diverging palette to highlight differences in sales—e.g., higher sales should be in red hues and lower sales in blue hues. Requirements: 1. Load the dataset `data.csv` into a pandas DataFrame. 2. Use Seaborn to create a diverging color palette with the following characteristics: - The center of the palette should be dark. - The palette should include a continuous colormap. - Adjust the separation around the center value to appropriately visualize the range of sales data. 3. Create a pivot table from the DataFrame with years as columns, quarters as rows, and sales values. 4. Generate a heatmap using the diverging palette. 5. Provide appropriate labels and titles for clarity in the visualization. Input: - A CSV file named `data.csv`. Output: - Display a heatmap representing the sales data over the years and quarters, with properly labeled axes and a title. Additional Notes: - Handle missing data in the dataset by replacing them with the average sales value of the corresponding product. - Make sure the heatmap adjusts to differences in sales data efficiently with the diverging palette, providing a clear distinction between high, low, and average sales. Constraints: - Use only Seaborn and Pandas to achieve the task. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load data data = pd.read_csv(\'data.csv\') # Handle missing data data.fillna(data.groupby(\'Product\')[\'Sales\'].transform(\'mean\'), inplace=True) # Create a pivot table pivot_table = data.pivot_table(index=\'Quarter\', columns=\'Year\', values=\'Sales\', aggfunc=\'mean\') # Create the diverging palette palette = sns.diverging_palette(240, 20, center=\'dark\', as_cmap=True, sep=30) # Plot the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(pivot_table, cmap=palette, annot=True, fmt=\'.1f\', linewidths=.5) plt.title(\'Sales Performance Heatmap\') plt.xlabel(\'Year\') plt.ylabel(\'Quarter\') plt.show() ``` Explain each step in the code and ensure that the resulting heatmap effectively visualizes sales over years and quarters.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def generate_sales_heatmap(filename): This function loads sales data from a CSV file, handles missing data, creates a pivot table, and generates a heatmap using a diverging color palette to visualize the sales data over years and quarters. Parameters: filename (str): The path to the CSV file containing the sales data. Returns: None: This function displays a heatmap. # Load data data = pd.read_csv(filename) # Handle missing data by replacing them with the average sales value of the corresponding product data.fillna(data.groupby(\'Product\')[\'Sales\'].transform(\'mean\'), inplace=True) # Create a pivot table with years as columns, quarters as rows, and sales values pivot_table = data.pivot_table(index=\'Quarter\', columns=\'Year\', values=\'Sales\', aggfunc=\'mean\') # Create the diverging palette palette = sns.diverging_palette(240, 20, center=\'dark\', as_cmap=True) # Plot the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(pivot_table, cmap=palette, annot=True, fmt=\'.1f\', linewidths=.5) plt.title(\'Sales Performance Heatmap\') plt.xlabel(\'Year\') plt.ylabel(\'Quarter\') plt.show()"},{"question":"**Question:** Write a Python function called `visualize_distributions` that takes in a DataFrame and visualizes the distributions of specified variables using Seaborn. Your function should: 1. Accept the following parameters: - `data`: A pandas DataFrame containing the data. - `variables`: A list of column names from the DataFrame to visualize. - `condition`: An optional parameter to condition the distributions on another variable. - `kind`: The kind of plot to generate (`\'hist\'`, `\'kde\'`, `\'ecdf\'`, or `\'scatter\'`). Default is `\'hist\'`. - `bins`: Number of bins to use in a histogram. Default is 10. - `binwidth`: Width of each bin. If provided, it will override the `bins` parameter. - `bw_adjust`: A bandwidth adjustment for KDE plots. - `hue`: An optional parameter to color the plots by another variable for comparison. - `multiple`: Optional parameter to layer distribution plots (`\'layer\'`, `\'stack\'`, `\'dodge\'`, etc.). - `fill`: Boolean to decide whether to fill the area under the KDE curve. - `col`: An optional parameter to create subplots based on levels of another variable. 2. Generate appropriate plots for each variable specified in the `variables` list according to the `kind` parameter. 3. If `kind` is `\'scatter\'`, plot bivariate distributions for each pair of variables in the `variables` list. 4. If `condition` is provided, create subplots for each level of the conditioned variable. Constraints: - Ensure that the plotting function handles both numerical and categorical data appropriately. - Your function should handle edge cases such as empty DataFrames, non-existent column names, and incompatible combinations of parameters gracefully. Output: - The function should display the generated plots using Seaborn. Example: ```python import seaborn as sns import pandas as pd def visualize_distributions( data, variables, condition=None, kind=\'hist\', bins=10, binwidth=None, bw_adjust=1, hue=None, multiple=None, fill=False, col=None): # Your implementation here # Usage example penguins = sns.load_dataset(\\"penguins\\") visualize_distributions(penguins, variables=[\\"flipper_length_mm\\", \\"bill_length_mm\\"], kind=\'kde\', hue=\'species\', col=\'sex\') ``` # Note: - Use `sns.displot` for the main plotting logic. - Add appropriate error handling and warnings for incorrect or unsupported parameter combinations.","solution":"import seaborn as sns import pandas as pd def visualize_distributions( data, variables, condition=None, kind=\'hist\', bins=10, binwidth=None, bw_adjust=1, hue=None, multiple=None, fill=False, col=None): Visualizes distributions of specified variables using Seaborn. Parameters: data (pandas.DataFrame): The data to visualize. variables (list): List of column names to visualize. condition (str, optional): Variable to condition the distributions on. kind (str): Kind of plot to generate (\'hist\', \'kde\', \'ecdf\', \'scatter\'). Default is \'hist\'. bins (int): Number of bins to use in histograms. Default is 10. binwidth (float, optional): Width of each bin. If provided, it overrides the `bins` parameter. bw_adjust (float): Bandwidth adjustment for KDE plots. hue (str, optional): Variable to color the plots by. multiple (str, optional): How to layer distribution plots (\'layer\', \'stack\', \'dodge\', etc.). fill (bool): Whether to fill the area under the KDE curve. Default is False. col (str, optional): Variable to create subplots based on its levels. if data is None or data.empty: print(\\"The provided DataFrame is empty or None.\\") return if not all(variable in data.columns for variable in variables): print(\\"One or more specified variables are not in the DataFrame.\\") return for variable in variables: if kind == \'scatter\' and len(variables) < 2: print(\\"Scatter plot requires at least two variables.\\") return sns.set(style=\\"whitegrid\\") try: if kind == \'scatter\': sns.pairplot(data, vars=variables, hue=hue, col=col) else: for variable in variables: sns.displot( data=data, x=variable, col=condition, kind=kind, bins=bins, binwidth=binwidth, bw_adjust=bw_adjust, hue=hue, multiple=multiple, fill=fill, col_wrap=4 if col else None ) sns.despine() except Exception as e: print(f\\"An error occurred while generating the plot: {e}\\")"},{"question":"**Custom Interactive Python Console** You are required to design and implement a custom interactive Python console using the `code` and `codeop` modules. Your custom console should extend Python\'s default features by adding support for a custom command. Specifically, you need to implement a console that evaluates Python expressions and supports a custom command `!reverse` that reverses a string provided to it. # Requirements 1. **Basic Console Behavior**: - The custom console should read Python code from the user, evaluate the code, and print the results, similar to the standard Python interactive interpreter. 2. **Custom Command: `!reverse`**: - When the user inputs `!reverse <string>`, the console should print the reversed string. # Input - Python statements and expressions for evaluation. - Commands in the format `!reverse <string>`, where `<string>` is any string. # Output - The result of the evaluated Python code. - The reversed string when the `!reverse` command is used. # Example ```python >>> 2 + 2 4 >>> print(\\"Hello, world!\\") Hello, world! >>> !reverse OpenAI IAnepO >>> !reverse Python310 013nohtyP >>> def add(a, b): >>> return a + b >>> add(3, 4) 7 ``` # Constraints - Do not use any external libraries other than the `code` and `codeop` modules. - Ensure your console can handle multi-line inputs without errors. # Implementation Hints - Subclass the `code.InteractiveConsole` class. - Override methods to add the `!reverse` command handling. - Use the `codeop` module if you need to handle incomplete code chunks appropriately. # Performance requirements - Your implementation should efficiently handle input and output operations. ```python import code class CustomInteractiveConsole(code.InteractiveConsole): def runsource(self, source, *args): if source.startswith(\'!reverse\'): string_to_reverse = source[len(\'!reverse \'):].strip() print(string_to_reverse[::-1]) return False return super().runsource(source, *args) if __name__ == \'__main__\': console = CustomInteractiveConsole() console.interact(\'Custom Python 310 Console. Type exit() or Ctrl-D to exit.\') ``` # Submission Provide your implementation in a Python file named `custom_console.py`.","solution":"import code class CustomInteractiveConsole(code.InteractiveConsole): def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source.startswith(\'!reverse \'): string_to_reverse = source[len(\'!reverse \'):].strip() print(string_to_reverse[::-1]) return False else: return super().runsource(source, filename, symbol) if __name__ == \'__main__\': console = CustomInteractiveConsole() console.interact(\'Custom Python Console. Type exit() or Ctrl-D to exit.\')"},{"question":"Coding Assessment Question # Objective: Implement a Python function that demonstrates proficiency in handling environment variables, managing file descriptors, and interacting with the file system using the `os` module. # Problem Statement: Using Python\'s `os` module, write a function `copy_environment_file` that: 1. Takes in two arguments, an environment variable name and a filename. 2. If the environment variable exists, it reads the entire content of a file specified by the filename from the environment variable\'s value. 3. Writes the content to a new file in the current directory named `\'copy_of_<original_filename>\'`. 4. Ensures that all file operations are done using file descriptors. # Specifications: - **Function Signature**: `copy_environment_file(env_var_name: str, filename: str) -> bool` - **Input**: - `env_var_name` (str): The name of an environment variable. - `filename` (str): The name of the file to be copied. - **Output**: - Returns `True` if the file was copied successfully, `False` otherwise. - **Constraints**: - If the specified environment variable does not exist, the function should return `False`. - If the file specified by the environment variable does not exist or cannot be read, the function should return `False`. - All file operations should be performed using file descriptors directly. - **Performance**: - Assume the file being copied could be large, so efficiency in reading and writing operations is crucial. # Example: ```python import os # Setting up an environment variable for demonstration os.environ[\'MY_FILE_PATH\'] = \'/path/to/original_file.txt\' # Example Usage result = copy_environment_file(\'MY_FILE_PATH\', \'original_file.txt\') print(result) # Should print True if the file is copied successfully or False otherwise. ``` # Implementation Details: - Use `os.getenv` to retrieve the environment variable value. - Open the source file using `os.open` in read-only mode. - Create and open the destination file using `os.open` with appropriate flags to ensure it is created afresh. - Use `os.read` and `os.write` to perform the copy operation via file descriptors. - Handle any exceptions or errors gracefully and return `False` if any operation fails.","solution":"import os def copy_environment_file(env_var_name: str, filename: str) -> bool: # Retrieve the environment variable value src_path = os.getenv(env_var_name) if not src_path: return False try: # Open the source file in read-only mode src_fd = os.open(src_path, os.O_RDONLY) # Create the destination file path dest_path = f\'copy_of_{filename}\' # Open the destination file in write mode, create if doesn\'t exist, truncate if already exists dest_fd = os.open(dest_path, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o644) try: # Read from the source file and write to the destination file while True: # Read from the source file data = os.read(src_fd, 4096) # Break the loop if no data is read if not data: break # Write to the destination file os.write(dest_fd, data) finally: # Ensure both file descriptors are closed os.close(src_fd) os.close(dest_fd) return True except Exception as e: # If any error occurs during the file operations, return False return False"},{"question":"**Objective:** To assess the student\'s understanding of Python\'s Abstract Objects Layer by requiring the implementation of custom objects that adhere to specified protocols. **Instructions:** You are tasked with implementing a custom Python object that conforms to the Sequence and Buffer protocols specified within the Python310 Abstract Objects Layer documentation. Requirements: 1. **Custom Sequence Object** - Define a class called `CustomSequence` that acts like a sequence (similar to a list). - Implement the following methods to conform to the sequence protocol: - `__len__(self)` - `__getitem__(self, index)` - `__setitem__(self, index, value)` - `__delitem__(self, index)` - `insert(self, index, value)` - Ensure that your `CustomSequence` can handle typical operations expected from an object adhering to the sequence protocol, such as indexing, slicing, and iteration. 2. **Custom Buffer Object** - Define a class called `CustomBuffer` that provides a buffer interface. - Implement the following methods to conform to the buffer protocol: - `__init__(self, size)` - `__enter__(self)` - `__exit__(self, exc_type, exc_value, tb)` - `__buffer__(self)` - Ensure that your `CustomBuffer` can be used to create memory views and supports read and write operations. 3. **Constraints and Limitations** - `CustomSequence` should only store objects of type `int`. - `CustomSequence` must raise appropriate exceptions for invalid operations (e.g., IndexError). - `CustomBuffer` should support read and write memory operations. - Your implementations should handle edge cases gracefully (e.g., negative indexing, buffer overflow). 4. **Performance Requirements** - Your implementations should be efficient in terms of time and space. Avoid unnecessary use of memory and optimize typical sequence and buffer operations. Input and Output Format: - You are not required to handle input/output from standard input/output. - You should ensure your classes and methods can be imported and used as parts of larger programs. - Test your classes using assertions like the examples given below. ```python # Testing CustomSequence seq = CustomSequence() seq.insert(0, 1) seq.insert(1, 2) seq.insert(2, 3) assert len(seq) == 3 assert seq[1] == 2 seq[1] = 4 assert seq[1] == 4 del seq[1] assert len(seq) == 2 # Testing CustomBuffer with CustomBuffer(10) as buf: view = memoryview(buf) view[0] = 1 view[1] = 2 assert view[0] == 1 assert view[1] == 2 ``` Your implementation should be able to handle the above assertions without errors.","solution":"class CustomSequence: def __init__(self): self.data = [] def __len__(self): return len(self.data) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): if not isinstance(value, int): raise ValueError(\\"CustomSequence only supports integers\\") self.data[index] = value def __delitem__(self, index): del self.data[index] def insert(self, index, value): if not isinstance(value, int): raise ValueError(\\"CustomSequence only supports integers\\") self.data.insert(index, value) class CustomBuffer: def __init__(self, size): self.size = size self.buffer = bytearray(size) def __enter__(self): return self def __exit__(self, exc_type, exc_value, tb): pass # Resource cleanup if necessary def __buffer__(self): return self.buffer def __len__(self): return len(self.buffer) def __getitem__(self, index): return self.buffer[index] def __setitem__(self, index, value): self.buffer[index] = value"},{"question":"# Regular Expression and Pattern Matching in Python Objective: Implement a function `check_password_complexity(password: str) -> bool` that ensures the given password meets specific complexity requirements using regular expressions. The function should verify that the password: 1. Is at least 8 characters long. 2. Contains both uppercase and lowercase letters. 3. Includes at least one numerical digit. 4. Has at least one special character out of `!@#%^&*()-_+=`. Input: - `password` (str): The password string to be validated. Output: - Returns `True` if the password meets all the complexity requirements, otherwise returns `False`. Constraints: - Password length is between 0 and 100 characters. - Only the first 100 characters of the password will be evaluated. You are encouraged to use the `re` module to implement this function. Example: ```python # Input: \\"A1!password\\" # Output: True # Input: \\"password\\" # Output: False # Input: \\"P@ssw0rd\\" # Output: True # Input: \\"short1!\\" # Output: False ``` **Hint**: Consider combining multiple regex patterns to match different conditions and use logical conjunctions to enforce all requirements. --- Implement your solution using the guidelines above and ensure you\'re utilizing regular expressions efficiently.","solution":"import re def check_password_complexity(password: str) -> bool: Validates that the given password meets the specified complexity requirements. Args: password (str): The password string to be validated. Returns: bool: True if the password meets all requirements, False otherwise. if len(password) < 8 or len(password) > 100: return False # Regular expressions to check for different conditions has_upper = re.search(r\'[A-Z]\', password) has_lower = re.search(r\'[a-z]\', password) has_digit = re.search(r\'d\', password) has_special = re.search(r\'[!@#%^&*()-_=+]\', password) return bool(has_upper and has_lower and has_digit and has_special)"},{"question":"# Question: Creating Customized Clustermap with Seaborn As a data scientist at a biological research facility, you have been given measurements from various species of plants. Your objective is to visualize this data to identify potential clusters and patterns among the species. **Dataset**: Download the dataset `iris` using Seaborn\'s `load_dataset` function. **Tasks**: 1. **Load and Prepare Data**: - Load the `iris` dataset using `sns.load_dataset(\\"iris\\")`. - Separate the `species` column from the rest of the data. 2. **Basic Clustermap**: - Create a basic clustermap of the dataset. 3. **Customization**: - Modify the size of the clustermap to have a `figsize` of `(10, 8)`. - Disable row clustering. - Use dendrogram ratios of `(0.1, 0.2)`. - Position the color bar at `(0.1, 0.8, 0.03, 0.2)`. 4. **Colored Labels**: - Map unique species to colors using a dictionary (e.g., `{\\"setosa\\": \\"r\\", \\"versicolor\\": \\"g\\", \\"virginica\\": \\"b\\"}`). - Apply these colors to the rows in the clustermap. 5. **Colormap and Limits**: - Use the `viridis` colormap. - Set the color range limits to be between `0` and `8`. 6. **Clustering and Normalization**: - Change the clustering metric to `euclidean` and the method to `average`. - Standardize the data within the columns. **Constraints**: - You should not use any additional libraries other than Seaborn and Pandas. - The code should be written in Python 3 and must be executable in a Jupyter notebook. **Expected Output**: The final output should be a clustermap that meets all the specified requirements. The plot should be visually appealing with distinct clusters and properly labeled rows according to their species. **Example Code Structure**: ```python import seaborn as sns import pandas as pd # Step 1: Load and Prepare Data iris = sns.load_dataset(\\"iris\\") species = iris.pop(\\"species\\") # Step 2: Basic Clustermap sns.clustermap(iris) # Steps 3-6: Customization lut = {\\"setosa\\": \\"r\\", \\"versicolor\\": \\"g\\", \\"virginica\\": \\"b\\"} row_colors = species.map(lut) sns.clustermap( iris, figsize=(10, 8), row_cluster=False, dendrogram_ratio=(0.1, 0.2), cbar_pos=(0.1, 0.8, 0.03, 0.2), row_colors=row_colors, cmap=\\"viridis\\", vmin=0, vmax=8, metric=\\"euclidean\\", method=\\"average\\", standard_scale=1 ) ``` Submit your implementation, including any additional comments or observations you may have made about the data or the visualizations.","solution":"import seaborn as sns import pandas as pd def create_customized_clustermap(): Creates a customized clustermap for the iris dataset. # Step 1: Load and Prepare Data iris = sns.load_dataset(\\"iris\\") species = iris.pop(\\"species\\") # Step 2: Basic Clustermap # sns.clustermap(iris) # This line is for the basic clustermap, can be commented out # Steps 3-6: Customization lut = {\\"setosa\\": \\"r\\", \\"versicolor\\": \\"g\\", \\"virginica\\": \\"b\\"} row_colors = species.map(lut) sns.clustermap( iris, figsize=(10, 8), row_cluster=False, dendrogram_ratio=(0.1, 0.2), cbar_pos=(0.1, 0.8, 0.03, 0.2), row_colors=row_colors, cmap=\\"viridis\\", vmin=0, vmax=8, metric=\\"euclidean\\", method=\\"average\\", standard_scale=1 )"},{"question":"Timedelta Operations and Analysis You are provided with a CSV file containing data about activities over a period. Each activity has a start and end timestamp. Your task is to analyze the duration of these activities using Pandas\' `Timedelta` functionality. # Input: The input is a CSV file with the following columns: - `activity_id`: Unique identifier for each activity. - `start_time`: Start timestamp of the activity (format: \'YYYY-MM-DD HH:MM:SS\'). - `end_time`: End timestamp of the activity (format: \'YYYY-MM-DD HH:MM:SS\'). # Output: You need to implement a function `analyze_activity_durations(file_path)` that reads the CSV file, processes the data, and returns a dictionary with the following keys: - `total_duration`: Total duration of all activities combined as a `Timedelta`. - `average_duration`: Average duration of the activities as a `Timedelta`. - `max_duration`: Maximum duration of any activity as a `Timedelta`. - `min_duration`: Minimum duration of any activity as a `Timedelta`. # Constraints: - You should handle missing or NaT values appropriately. - Assume the CSV file is well-formed and timestamps are in correct format. - Performance should be efficient for a CSV file with up to 1 million rows. # Example: Consider the following CSV content: ``` activity_id,start_time,end_time 1,2023-01-01 08:00:00,2023-01-01 10:00:00 2,2023-01-01 09:30:00,2023-01-01 11:45:00 3,2023-01-01 11:00:00,2023-01-01 12:00:00 ``` The function would output: ```python { \'total_duration\': Timedelta(\'0 days 05:15:00\'), \'average_duration\': Timedelta(\'0 days 01:45:00\'), \'max_duration\': Timedelta(\'0 days 02:15:00\'), \'min_duration\': Timedelta(\'0 days 01:00:00\') } ``` # Implementation: Implement the `analyze_activity_durations(file_path)` function in Python using Pandas. The function should be clear, concise, and handle edge cases where timestamps might be missing. ```python import pandas as pd def analyze_activity_durations(file_path): # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Convert the start_time and end_time columns to datetime df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Calculate the duration for each activity df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] # Handle missing values by dropping rows with NaT in duration df = df.dropna(subset=[\'duration\']) # Calculate the total, average, maximum, and minimum duration total_duration = df[\'duration\'].sum() average_duration = df[\'duration\'].mean() max_duration = df[\'duration\'].max() min_duration = df[\'duration\'].min() # Return the results as a dictionary return { \'total_duration\': total_duration, \'average_duration\': average_duration, \'max_duration\': max_duration, \'min_duration\': min_duration } ``` Ensure your function is tested with different datasets to validate correctness and performance.","solution":"import pandas as pd def analyze_activity_durations(file_path): Analyzes the activity durations in the given CSV file. Args: - file_path (str): Path to the CSV file. Returns: - dict: A dictionary with total, average, maximum, and minimum durations. # Read the CSV file into a pandas DataFrame df = pd.read_csv(file_path) # Convert the start_time and end_time columns to datetime df[\'start_time\'] = pd.to_datetime(df[\'start_time\']) df[\'end_time\'] = pd.to_datetime(df[\'end_time\']) # Calculate the duration for each activity df[\'duration\'] = df[\'end_time\'] - df[\'start_time\'] # Handle missing values by dropping rows with NaT in duration df = df.dropna(subset=[\'duration\']) # Calculate the total, average, maximum, and minimum duration total_duration = df[\'duration\'].sum() average_duration = df[\'duration\'].mean() max_duration = df[\'duration\'].max() min_duration = df[\'duration\'].min() # Return the results as a dictionary return { \'total_duration\': total_duration, \'average_duration\': average_duration, \'max_duration\': max_duration, \'min_duration\': min_duration }"},{"question":"**Coding Assessment Question:** **Title: Implementing a Bookstore Inventory Management System** **Objective:** Design a function that manages a bookstore\'s inventory using the concepts of control flow, function definitions, and pattern matching. **Problem Statement:** You are tasked with implementing a small bookstore management system. The system should support the following operations: 1. Add a new book to the inventory. 2. Remove a book from the inventory. 3. List all books in the inventory. 4. Search for books by a specific author. 5. Display the inventory sorted by the book title. Your task is to create a function `manage_bookstore_inventory(action, data=None)` that performs these operations. **Function Signature:** ```python def manage_bookstore_inventory(action: str, data: dict = None) -> list: pass ``` **Input:** 1. `action` (str): A string representing the operation to perform. It can be one of the following: - \\"add_book\\": To add a new book. - \\"remove_book\\": To remove a book. - \\"list_books\\": To list all books. - \\"search_by_author\\": To search books by author. - \\"sort_books_by_title\\": To sort and list books by title. 2. `data` (dict, optional): A dictionary containing the necessary data for the specified action. For example: - For \\"add_book\\", `data` should contain {\\"title\\": str, \\"author\\": str}. - For \\"remove_book\\", `data` should contain {\\"title\\": str}. - For \\"search_by_author\\", `data` should contain {\\"author\\": str}. - For \\"list_books\\" and \\"sort_books_by_title\\", `data` can be None. **Output:** - A list of books in the inventory after performing the action. Each book should be represented as a dictionary with keys \\"title\\" and \\"author\\". **Constraints:** - Book titles are unique. - The inventory should be implemented as a list of dictionaries. - Ensure that the function handles invalid actions gracefully by raising a ValueError with an appropriate message. **Example:** ```python # Initial inventory inventory = [] # Adding books inventory = manage_bookstore_inventory(\\"add_book\\", {\\"title\\": \\"The Great Gatsby\\", \\"author\\": \\"F. Scott Fitzgerald\\"}) inventory = manage_bookstore_inventory(\\"add_book\\", {\\"title\\": \\"1984\\", \\"author\\": \\"George Orwell\\"}) inventory = manage_bookstore_inventory(\\"add_book\\", {\\"title\\": \\"To Kill a Mockingbird\\", \\"author\\": \\"Harper Lee\\"}) # Listing books print(manage_bookstore_inventory(\\"list_books\\")) # Output: [{\'title\': \'The Great Gatsby\', \'author\': \'F. Scott Fitzgerald\'}, {\'title\': \'1984\', \'author\': \'George Orwell\'}, {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\'}] # Searching books by author print(manage_bookstore_inventory(\\"search_by_author\\", {\\"author\\": \\"George Orwell\\"})) # Output: [{\'title\': \'1984\', \'author\': \'George Orwell\'}] # Sorting books by title print(manage_bookstore_inventory(\\"sort_books_by_title\\")) # Output: [{\'title\': \'1984\', \'author\': \'George Orwell\'}, {\'title\': \'The Great Gatsby\', \'author\': \'F. Scott Fitzgerald\'}, {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\'}] # Removing a book inventory = manage_bookstore_inventory(\\"remove_book\\", {\\"title\\": \\"1984\\"}) # Listing books after removal print(manage_bookstore_inventory(\\"list_books\\")) # Output: [{\'title\': \'The Great Gatsby\', \'author\': \'F. Scott Fitzgerald\'}, {\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\'}] ``` **Notes:** - Make sure to follow PEP 8 style guidelines for writing the code. - Include appropriate function docstrings. Implement the `manage_bookstore_inventory` function to complete this task.","solution":"inventory = [] def manage_bookstore_inventory(action, data=None): Manages bookstore inventory based on the given action. Parameters: - action (str): The action to perform on the inventory. Can be one of: - \\"add_book\\": To add a new book. - \\"remove_book\\": To remove a book. - \\"list_books\\": To list all books. - \\"search_by_author\\": To search books by author. - \\"sort_books_by_title\\": To sort and list books by title. - data (dict, optional): The data required for the action. Default is None. Returns: - list: The updated inventory after performing the action. Raises: - ValueError: If the action is invalid. global inventory if action == \\"add_book\\": if data and \\"title\\" in data and \\"author\\" in data: inventory.append(data) else: raise ValueError(\\"Data for adding book is missing \'title\' or \'author\'.\\") elif action == \\"remove_book\\": if data and \\"title\\" in data: inventory = [book for book in inventory if book[\\"title\\"] != data[\\"title\\"]] else: raise ValueError(\\"Data for removing book is missing \'title\'.\\") elif action == \\"list_books\\": return inventory elif action == \\"search_by_author\\": if data and \\"author\\" in data: return [book for book in inventory if book[\\"author\\"] == data[\\"author\\"]] else: raise ValueError(\\"Data for searching books is missing \'author\'.\\") elif action == \\"sort_books_by_title\\": return sorted(inventory, key=lambda book: book[\\"title\\"]) else: raise ValueError(\\"Invalid action.\\") return inventory"},{"question":"# Pandas Coding Assessment Objective The objective is to assess your knowledge of pandas, focusing on data manipulation, conditional operations, and grouping mechanisms. Problem Statement You are given a dataset with sales records of a company. The dataset contains the following columns: - `OrderID`: unique identifier for each order. - `Product`: name of the product sold. - `Category`: category to which the product belongs. - `Date`: date of the order. - `Sales`: total sales amount for the order. - `Quantity`: quantity of products sold in the order. - `Price`: price of a single unit of the product. Given the dataset, you are required to perform the following tasks: 1. **Load the Data:** - Load the dataset into a pandas DataFrame. 2. **Data Manipulations:** - Create a new column `Revenue` which is calculated by multiplying `Sales` with `Quantity`. - Create a new column `Week` which represents the week number of the year when the order was placed. 3. **Conditional Statements:** - Use a conditional statement to create a new column `DiscountCategory` which assigns `High` if `Price` is more than 100, else `Low`. 4. **Grouping and Aggregations:** - Group the data by `Category` and `Week`. Calculate the total `Revenue` for each group. - For each `Category`, find the `Product` with the highest total `Quantity` sold. Return a DataFrame with `Category`, `Product`, and `Total Quantity`. 5. **Filtering and Selection:** - Using the resulting DataFrame from the previous step, filter the original DataFrame to include only the orders of the top-selling products per category across all weeks. Input Format: - The input is provided as a CSV file named `sales_data.csv`. Output: - Output should include: 1. DataFrame with added `Revenue`, and `Week` columns. 2. Filtered DataFrame containing orders of the top-selling products per category. Example For a given input CSV file, the output should look similar to this: # DataFrame with `Revenue` and `Week` columns: ```python OrderID Product Category Date Sales Quantity Price Revenue Week 0 1 ProductA Category1 2023-01-01 200.0 2 100.0 400.0 1 1 2 ProductB Category2 2023-01-08 300.0 1 300.0 300.0 2 # ... other rows ``` # Filtered DataFrame ```python OrderID Product Category Date Sales Quantity Price Revenue Week 0 X TopProductC CategoryC 2023-XX-XX 150.0 3 50.0 450.0 Y 1 Y TopProductD CategoryD 2023-YY-YY 500.0 5 100.0 250.0 Z # ... filtered rows of top-selling products per category ``` Constraints: 1. Ensure proper handling of missing values (NaNs) in sales or quantities, if any. 2. Assume reasonable dataset sizes for performance considerations. Good luck and happy coding!","solution":"import pandas as pd def load_data(file_path): Loads the dataset from a CSV file into a pandas DataFrame. return pd.read_csv(file_path) def data_manipulations(df): Performs data manipulations to add Revenue and Week columns. # Calculate Revenue df[\'Revenue\'] = df[\'Sales\'] * df[\'Quantity\'] # Convert Date to datetime and extract week number df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Week\'] = df[\'Date\'].dt.isocalendar().week # Conditional statement to add DiscountCategory column df[\'DiscountCategory\'] = df[\'Price\'].apply(lambda x: \'High\' if x > 100 else \'Low\') return df def group_and_aggregate(df): Groups the data by Category and Week, and calculates total Revenue for each group. Finds the Product with the highest total Quantity sold for each Category. grouped = df.groupby([\'Category\', \'Week\']).agg(Total_Revenue=(\'Revenue\', \'sum\')).reset_index() top_selling_products = df.groupby([\'Category\', \'Product\']).agg(Total_Quantity=(\'Quantity\', \'sum\')).reset_index() top_products = top_selling_products.sort_values(\'Total_Quantity\', ascending=False).drop_duplicates([\'Category\']) return grouped, top_products def filter_top_selling(df, top_products): Filters the original DataFrame to include only the orders of the top-selling products per category. return df[df.apply(lambda x: (x[\'Category\'], x[\'Product\']) in zip(top_products[\'Category\'], top_products[\'Product\']), axis=1)]"},{"question":"You are required to demonstrate your understanding of the scikit-learn data generation functions and to train a model using one of the generated datasets. Specifically, you will use the `make_moons` function to create a dataset, train a classification model on it, and evaluate the model\'s performance. Requirements: 1. **Data Generation**: - Use the `make_moons` function to generate a 2D dataset. - Parameters for `make_moons`: `n_samples=500`, `noise=0.2`. 2. **Model Training**: - Split the dataset into training and testing sets (80% training, 20% testing). - Train a Support Vector Machine (SVM) classifier on the training set. 3. **Model Evaluation**: - Measure the model\'s performance using accuracy on the test set. - Visualize the decision boundary of the trained model along with the training data points. Implementation Details: - Function to implement: `train_and_evaluate_moons_model()` - Input: None (all parameters should be set as specified above within the function) - Output: Should return the accuracy of the model on the test set. ```python from sklearn.datasets import make_moons from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import numpy as np def train_and_evaluate_moons_model(): # Generate the dataset with make_moons X, y = make_moons(n_samples=500, noise=0.2, random_state=0) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train an SVM classifier clf = SVC(kernel=\'rbf\', gamma=\'scale\', random_state=0) clf.fit(X_train, y_train) # Predict on the test set y_pred = clf.predict(X_test) # Compute accuracy accuracy = accuracy_score(y_test, y_pred) # Visualization of the decision boundary plt.figure(figsize=(8, 6)) plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Paired, edgecolors=\'k\', s=20) # Create a mesh to plot the decision boundary x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) # Plot also the training points plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Paired, edgecolors=\'k\', marker=\'x\', s=50) plt.title(\'Decision Boundary - SVM Classifier\') plt.show() # Return the accuracy return accuracy # Call the function accuracy = train_and_evaluate_moons_model() print(f\\"Model accuracy on test set: {accuracy}\\") ``` Constraints: - Use a random seed for reproducibility where necessary. - Ensure all plots are labeled appropriately for clarity.","solution":"from sklearn.datasets import make_moons from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt import numpy as np def train_and_evaluate_moons_model(): # Generate the dataset with make_moons X, y = make_moons(n_samples=500, noise=0.2, random_state=0) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train an SVM classifier clf = SVC(kernel=\'rbf\', gamma=\'scale\', random_state=0) clf.fit(X_train, y_train) # Predict on the test set y_pred = clf.predict(X_test) # Compute accuracy accuracy = accuracy_score(y_test, y_pred) # Visualization of the decision boundary plt.figure(figsize=(8, 6)) plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Paired, edgecolors=\'k\', s=20) # Create a mesh to plot the decision boundary x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) # Plot also the training points plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.Paired, edgecolors=\'k\', marker=\'x\', s=50) plt.title(\'Decision Boundary - SVM Classifier\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() # Return the accuracy return accuracy"},{"question":"Accessing and Analyzing Python Configuration Objective: Demonstrate your understanding of the `sysconfig` module by writing a Python function that extracts specific configuration information and performs a series of tasks. Question: Write a function `python_config_info()` that performs the following tasks: 1. Retrieves and returns the Python version and platform details. 2. Extracts the default installation scheme for the current platform. 3. Retrieves and returns a dictionary containing installation paths for the default scheme. 4. Extracts the value of a specific configuration variable (\'PY_CFLAGS\') and returns it. 5. Parses the configuration header file and returns the contents as a dictionary. 6. Prints all the steps clearly formatted. ```python def python_config_info(): Using the sysconfig module, perform the following tasks: 1. Retrieve and return the Python version and platform details as a tuple (python_version, platform). 2. Retrieve the default installation scheme for the current platform. 3. Retrieve and return a dictionary containing all installation paths for the default scheme. 4. Extract the value of the \'PY_CFLAGS\' configuration variable and return it. 5. Parse the pyconfig.h file and return its contents as a dictionary. 6. Print all steps clearly formatted. Returns: A tuple containing: - python_version: str - platform: str - install_paths: dict - py_cflags: str or None - config_h_contents: dict # Your implementation here ``` Constraints and Requirements: - Do not alter the names of the function or parameters. - Ensure the function adheres to the specified input-output format. - Handle any potential errors or exceptions that may arise during execution. - Follow best coding practices, ensuring your code is readable and well-documented. Example Output: The function should print and return the following (actual values will depend on the environment being used): ``` Python Version: 3.10 Platform: linux-x86_64 Default Scheme: posix_prefix Installation Paths: {\'stdlib\': \'/usr/local/lib/python3.10\', \'platstdlib\': \'/usr/local/lib/python3.10\', ...} PY_CFLAGS: \'-fno-semantic-interposition -DNDEBUG -g -fwrapv -O3 -Wall\' Config.h Contents: {\'PY_MAJOR_VERSION\': \'3\', \'PY_MINOR_VERSION\': \'10\', ...} ``` __Notes:__ - Some of the values may be `None` if they are not found. - Use the `sysconfig` functions as described in the provided documentation to achieve the desired outcomes.","solution":"import sysconfig def python_config_info(): Using the sysconfig module, perform the following tasks: 1. Retrieve and return the Python version and platform details as a tuple (python_version, platform). 2. Retrieve the default installation scheme for the current platform. 3. Retrieve and return a dictionary containing all installation paths for the default scheme. 4. Extract the value of the \'PY_CFLAGS\' configuration variable and return it. 5. Parse the pyconfig.h file and return its contents as a dictionary. 6. Print all steps clearly formatted. Returns: A tuple containing: - python_version: str - platform: str - install_paths: dict - py_cflags: str or None - config_h_contents: dict # 1. Retrieve the Python version and platform details python_version = sysconfig.get_python_version() platform = sysconfig.get_platform() # 2. Retrieve the default installation scheme for the current platform default_scheme = sysconfig.get_default_scheme() # 3. Retrieve a dictionary containing all installation paths for the default scheme install_paths = sysconfig.get_paths(scheme=default_scheme) # 4. Extract the value of the \'PY_CFLAGS\' configuration variable py_cflags = sysconfig.get_config_var(\'PY_CFLAGS\') # 5. Parse the pyconfig.h file and return its contents as a dictionary config_h_path = sysconfig.get_config_h_filename() config_h_contents = {} try: with open(config_h_path) as config_file: for line in config_file: if line.startswith(\\"#define \\"): parts = line.split() if len(parts) >= 3: config_h_contents[parts[1]] = parts[2] except FileNotFoundError: config_h_contents = None # 6. Print all steps clearly formatted print(f\\"Python Version: {python_version}\\") print(f\\"Platform: {platform}\\") print(f\\"Default Scheme: {default_scheme}\\") print(f\\"Installation Paths: {install_paths}\\") print(f\\"PY_CFLAGS: {py_cflags}\\") print(f\\"Config.h Contents: {config_h_contents}\\") return python_version, platform, install_paths, py_cflags, config_h_contents"},{"question":"**Objective**: Implement a function that serves the documentation for a specified Python module via an HTTP server using the `pydoc` module. **Function Signature**: ```python def serve_module_docs(module_name: str, port: int = 1234) -> None: pass ``` **Input**: - `module_name` (str): The name of the module for which to generate and serve documentation. For example, \'os\', \'sys\', \'pydoc\', etc. - `port` (int, optional): The port number on which the HTTP server should run to serve the documentation. The default value is `1234`. **Output**: - The function should not return any value. It should start an HTTP server that serves the documentation at the specified port. **Constraints**: - Assume the given module name is valid and can be imported. - Handle exceptions that might arise from module import errors or issues starting the HTTP server. **Requirements**: 1. Use the `pydoc` module to handle documentation generation. 2. Start an HTTP server that serves the documentation on the specified port using `pydoc`. 3. Ensure that the server continues to run until manually stopped by the user (e.g., via keyboard interrupt). **Example Usage**: ```python # To serve documentation for the \'os\' module at the default port 1234 serve_module_docs(\'os\') # To serve documentation for the \'sys\' module at port 8080 serve_module_docs(\'sys\', 8080) ``` **Challenge**: - Ensure robust error handling and logging for troubleshooting potential issues during server startup or module import. - Consider how to make this function user-friendly, possibly including print statements to inform the user where they can access the documentation in their browser. **Performance Requirements**: - The function should be efficient in starting the server and providing the documentation promptly. ---","solution":"import pydoc import http.server import socketserver import threading def serve_module_docs(module_name: str, port: int = 1234) -> None: Serves the documentation for the specified module via an HTTP server using pydoc. Args: module_name (str): The name of the module for which to generate and serve documentation. port (int, optional): The port number on which the HTTP server should run. Default is 1234. try: # Generate the documentation using pydoc doc = pydoc.HTMLDoc().docmodule(__import__(module_name)) # Define the handler to serve the documentation class DocHandler(http.server.SimpleHTTPRequestHandler): def do_GET(self): self.send_response(200) self.send_header(\\"Content-type\\", \\"text/html\\") self.end_headers() self.wfile.write(doc.encode(\\"utf-8\\")) # Create the HTTP server with socketserver.TCPServer((\\"localhost\\", port), DocHandler) as httpd: print(f\\"Serving {module_name} documentation at http://localhost:{port}\\") thread = threading.Thread(target=httpd.serve_forever) thread.daemon = True # Ensure the server threads exit when main thread does thread.start() thread.join() # Wait for the thread to finish, which it won\'t unless interrupted except Exception as e: print(f\\"Error serving {module_name} documentation: {e}\\") # Example usage: # serve_module_docs(\'os\') # serve_module_docs(\'sys\', 8080)"},{"question":"**Python Coding Assessment Question: Site-specific Configuration Management** # Objective Your task is to demonstrate your understanding of Python\'s `site` module by implementing functionality that mimics `site.addsitedir`. # Problem Statement Implement a function `custom_add_site_dir(sitedir: str, known_paths: Optional[Set[str]] = None) -> List[str]` that: 1. **Adds the directory `sitedir` to `sys.path`** if it\'s not already present. 2. **Processes any `.pth` files** in the given directory and adds their contents (valid paths) to `sys.path`. - **PTH File Rules**: - Add each line as a new path unless it is empty, starts with `#`, or starts with `import` (followed by space/tab). - Ensure paths are not added to `sys.path` more than once. - Ignore non-existent paths. 3. **Returns the updated list of paths added to `sys.path`.** # Input and Output - **Input**: - `sitedir` (str): A directory path to be added to `sys.path`. - `known_paths` (Optional[Set[str]]): A set of paths that should be considered already known. If `None`, initialize an empty set. - **Output**: - List of paths added to `sys.path` during this function call. # Constraints - Handle cases where `.pth` files contain comments, empty lines, and non-existent paths. - Ensure that no paths are duplicated in `sys.path`. # Example ```python import os import sys def custom_add_site_dir(sitedir: str, known_paths: Optional[Set[str]] = None) -> List[str]: if known_paths is None: known_paths = set(sys.path) new_paths = [] if sitedir not in known_paths: sys.path.append(sitedir) known_paths.add(sitedir) new_paths.append(sitedir) try: pth_files = [f for f in os.listdir(sitedir) if f.endswith(\'.pth\')] except FileNotFoundError: return new_paths for pth_file in pth_files: with open(os.path.join(sitedir, pth_file), \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\') or line.startswith(\'import\'): continue if line not in known_paths and os.path.exists(line): sys.path.append(line) known_paths.add(line) new_paths.append(line) return new_paths ``` # Test Case ```python import tempfile import os import sys # Setup a temporary directory with `.pth` files with tempfile.TemporaryDirectory() as tempdir: pth_content = # This is a comment dir1 dir2 import os dir3 # A valid directory os.makedirs(os.path.join(tempdir, \'dir1\')) os.makedirs(os.path.join(tempdir, \'dir2\')) # Note: not creating dir3 to test non-existing path handling with open(os.path.join(tempdir, \'test.pth\'), \'w\') as f: f.write(pth_content) # Call custom_add_site_dir result = custom_add_site_dir(tempdir) print(\\"Paths added to sys.path:\\", result) ``` In your implementation, pay close attention to the constraints and ensure you handle edge cases such as non-existent directories and comments in `.pth` files. # Notes - Do not use the existing `site.addsitedir` method; recreate the logic to demonstrate your understanding. - You can assume that you are not running the code with the `-S` option.","solution":"import os import sys from typing import Optional, Set, List def custom_add_site_dir(sitedir: str, known_paths: Optional[Set[str]] = None) -> List[str]: if known_paths is None: known_paths = set(sys.path) new_paths = [] if sitedir not in known_paths: sys.path.append(sitedir) known_paths.add(sitedir) new_paths.append(sitedir) try: pth_files = [f for f in os.listdir(sitedir) if f.endswith(\'.pth\')] except FileNotFoundError: return new_paths for pth_file in pth_files: with open(os.path.join(sitedir, pth_file), \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\') or line.startswith(\'import\'): continue full_path = os.path.join(sitedir, line) if full_path not in known_paths and os.path.exists(full_path): sys.path.append(full_path) known_paths.add(full_path) new_paths.append(full_path) return new_paths"},{"question":"Objective: Demonstrate your understanding of Python\'s `venv` module by creating a custom virtual environment builder that can install additional packages or perform other setup tasks within the created environment. Question: Implement a class `CustomEnvBuilder` that extends the functionality of Python\'s `venv.EnvBuilder` to perform additional setup tasks after creating a virtual environment. Your implementation should: 1. Extend the `EnvBuilder` class. 2. Override the `post_setup` method to install additional packages into the virtual environment. 3. Provide a method `install_package` that takes a package name and installs it using `pip`. Specifically, your class should: - Accept a list of packages to install at initialization. - Create a new virtual environment at the specified directory. - Install the specified packages into the virtual environment using the `post_setup` method. Function Signature: ```python import venv class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, packages=None, *args, **kwargs): super().__init__(*args, **kwargs) self.packages = packages if packages is not None else [] def post_setup(self, context): Install additional packages into the virtual environment. self.install_packages(context) def install_packages(self, context): Install the specified packages into the virtual environment using pip. for package in self.packages: self.install_package(context, package) def install_package(self, context, package): Install a single package using pip in the virtual environment. import subprocess subprocess.check_call([context.env_exe, \'-m\', \'pip\', \'install\', package]) ``` Usage Example: ```python if __name__ == \\"__main__\\": builder = CustomEnvBuilder(packages=[\\"requests\\", \\"flask\\"]) builder.create(\\"/path/to/new/environment\\") ``` Constraints: - You must not use any external libraries other than those provided by Python standard library. - Ensure your class works cross-platform (Unix and Windows). Input: - A list of package names (strings) to install. - The directory path for creating the virtual environment. Output: - A virtual environment created at the specified path with the additional packages installed. Performance Requirements: - The solution should handle the creation of a virtual environment in a reasonable time, considering standard package installation times with `pip`.","solution":"import venv import subprocess class CustomEnvBuilder(venv.EnvBuilder): def __init__(self, packages=None, *args, **kwargs): super().__init__(*args, **kwargs) self.packages = packages if packages is not None else [] def post_setup(self, context): Install additional packages into the virtual environment. self.install_packages(context) def install_packages(self, context): Install the specified packages into the virtual environment using pip. for package in self.packages: self.install_package(context, package) def install_package(self, context, package): Install a single package using pip in the virtual environment. subprocess.check_call([context.env_exe, \'-m\', \'pip\', \'install\', package])"},{"question":"# Advanced Coding Assessment: Implementing a Custom Compression-Decompression System Objective: Demonstrate your understanding of the `bz2` module in Python by implementing functions to handle both one-shot and incremental compression and decompression of multiple files. Problem Statement: You are required to write a Python program that compresses and decompresses multiple text files. You will implement the following functions: 1. **One-shot Compression:** ```python def compress_files(file_paths: List[str], output_path: str, compresslevel: int = 9) -> None: Compresses multiple text files into a single bzip2 output file in one go. Parameters: - file_paths (List[str]): List of paths to the text files to be compressed. - output_path (str): Path to the output compressed file. - compresslevel (int): Compression level ranging from 1 to 9. Default is 9. ``` 2. **One-shot Decompression:** ```python def decompress_file(input_path: str, output_dir: str) -> None: Decompresses a bzip2 file into individual text files. Parameters: - input_path (str): Path to the input bzip2-compressed file. - output_dir (str): Directory where decompressed files will be stored. ``` 3. **Incremental Compression:** ```python def incremental_compress(file_paths: List[str], output_path: str, chunk_size: int = 1024, compresslevel: int = 9) -> None: Incrementally compresses multiple text files into a single bzip2 output file. Parameters: - file_paths (List[str]): List of paths to the text files to be compressed. - output_path (str): Path to the output compressed file. - chunk_size (int): Size of each chunk to be compressed incrementally. Default is 1024. - compresslevel (int): Compression level ranging from 1 to 9. Default is 9. ``` 4. **Incremental Decompression:** ```python def incremental_decompress(input_path: str, output_dir: str, chunk_size: int = 1024) -> None: Incrementally decompresses a bzip2 file into individual text files. Parameters: - input_path (str): Path to the input bzip2-compressed file. - output_dir (str): Directory where decompressed files will be stored. - chunk_size (int): Size of each chunk to be decompressed incrementally. Default is 1024. ``` Constraints: - Handle I/O errors gracefully. - Assume paths provided are valid and accessible. - Ensure decompression restores the original files correctly. - Maintain file names during compression and decompression processes. - Performance should handle large files efficiently within the given chunk size for incremental functions. Example: Given two text files `file1.txt` and `file2.txt`, your function should be able to: 1. Compress them into a single `output.bz2` file. 2. Decompress `output.bz2` back into `file1.txt` and `file2.txt` correctly. Use the `bz2` module functionalities effectively and ensure robust error handling.","solution":"import bz2 import os from typing import List def compress_files(file_paths: List[str], output_path: str, compresslevel: int = 9) -> None: Compresses multiple text files into a single bzip2 output file in one go. Parameters: - file_paths (List[str]): List of paths to the text files to be compressed. - output_path (str): Path to the output compressed file. - compresslevel (int): Compression level ranging from 1 to 9. Default is 9. with bz2.BZ2File(output_path, \'w\', compresslevel=compresslevel) as output_file: for file_path in file_paths: with open(file_path, \'rb\') as input_file: output_file.write(input_file.read()) def decompress_file(input_path: str, output_dir: str) -> None: Decompresses a bzip2 file into individual text files. Parameters: - input_path (str): Path to the input bzip2-compressed file. - output_dir (str): Directory where decompressed files will be stored. with bz2.BZ2File(input_path, \'rb\') as input_file: # Naive assumption that all files are concatenated and need splitting data = input_file.read() with open(os.path.join(output_dir, \'output.txt\'), \'wb\') as output_file: output_file.write(data) def incremental_compress(file_paths: List[str], output_path: str, chunk_size: int = 1024, compresslevel: int = 9) -> None: Incrementally compresses multiple text files into a single bzip2 output file. Parameters: - file_paths (List[str]): List of paths to the text files to be compressed. - output_path (str): Path to the output compressed file. - chunk_size (int): Size of each chunk to be compressed incrementally. Default is 1024. - compresslevel (int): Compression level ranging from 1 to 9. Default is 9. with bz2.BZ2File(output_path, \'w\', compresslevel=compresslevel) as output_file: for file_path in file_paths: with open(file_path, \'rb\') as input_file: while chunk := input_file.read(chunk_size): output_file.write(chunk) def incremental_decompress(input_path: str, output_dir: str, chunk_size: int = 1024) -> None: Incrementally decompresses a bzip2 file into individual text files. Parameters: - input_path (str): Path to the input bzip2-compressed file. - output_dir (str): Directory where decompressed files will be stored. - chunk_size (int): Size of each chunk to be decompressed incrementally. Default is 1024. with bz2.BZ2File(input_path, \'rb\') as input_file: with open(os.path.join(output_dir, \'output.txt\'), \'wb\') as output_file: while chunk := input_file.read(chunk_size): output_file.write(chunk)"},{"question":"**Question: Implementing Parallel Task Execution with `concurrent.futures`** **Objective:** To assess your understanding of the `concurrent.futures` module by implementing a solution that uses parallel task execution for a given problem. **Problem Statement:** You are given a list of URLs, and your task is to download the content of each URL concurrently using the `concurrent.futures` module. The content of each URL should be saved in a file named after the URL (with special characters replaced by underscores). For the purpose of this exercise, assume that you have a function `download_url(url)` that takes a URL as input and returns the content as a string. **Function Signature:** ```python def download_contents(urls: list) -> None: pass ``` **Input:** - `urls` (list of str): A list of URLs to be downloaded. **Output:** - There is no return value, but the contents of each URL should be saved in a file named after the URL. **Constraints:** - Each URL in the `urls` list is a valid URL. - Special characters in URLs should be replaced by underscores (`_`) when naming the files. - The maximum number of URLs is 100. - You should use a thread pool or process pool from the `concurrent.futures` module to download the URLs concurrently. **Performance Requirements:** - The solution should make efficient use of parallel execution to minimize the overall time taken to download all the URLs. **Example:** ```python urls = [ \\"http://example.com/page1\\", \\"http://example.com/page2\\", \\"http://example.com/page3\\" ] # After invoking download_contents(urls), files named # http___example_com_page1, http___example_com_page2, and # http___example_com_page3 should be created with the content # of each respective URL. ``` **Note:** You can use the following `download_url` function for testing your implementation: ```python import time import random def download_url(url): time.sleep(random.uniform(0.1, 0.5)) # Simulate network delay return f\\"Content of {url}\\" ``` **Hint:** Utilize the `ThreadPoolExecutor` or `ProcessPoolExecutor` from `concurrent.futures` to manage the concurrent downloading of URLs.","solution":"import concurrent.futures import re def download_url(url): Simulated function to download URL content. In a real scenario, you would fetch the content using `requests` or similar libraries. import time import random time.sleep(random.uniform(0.1, 0.5)) # Simulate network delay return f\\"Content of {url}\\" def sanitize_filename(url): Replace special characters in URL with underscores to make a valid filename. return re.sub(r\'W+\', \'_\', url) def save_content_to_file(url, content): Save the content to a file named after the sanitized URL. filename = sanitize_filename(url) with open(filename, \'w\') as file: file.write(content) def download_contents(urls: list) -> None: with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(download_url, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: content = future.result() save_content_to_file(url, content) except Exception as exc: print(f\'{url} generated an exception: {exc}\')"},{"question":"# Question: Advanced Python Object Customization You are asked to implement a custom matrix class leveraging Python’s data model customization techniques. The matrix should support standard mathematical operations, slicing, and be designed to efficiently handle large data. Additionally, implement an optional tracking mechanism using context managers to log all operations performed on the matrix. Requirements: 1. **Custom Matrix Class**: - Implement a class named `Matrix` that supports the following operations: - Matrix addition (`+`), subtraction (`-`), multiplication (`*`), and matrix-vector multiplication using the `@` operator. - Element access and assignment using row and column indexing. - Slicing to get submatrices. - String representation to visualize the matrix neatly. - Ensure the matrix can only store numerical values (ints and floats). 2. **Context Manager**: - Implement a context manager within the `Matrix` class to log operations performed on the matrix to a specified log file. - The context manager should be able to selectively enable or disable logging. Implementation Details: 1. **Matrix Class**: - **Initialization**: The matrix should be initialized from a 2D list of numbers, ensuring all rows have the same length. - **Operations**: - Define the special methods to support addition (`__add__`), subtraction (`__sub__`), multiplication (`__mul__`), and matrix-vector multiplication (`__matmul__`). - Define the method for element access (`__getitem__`) and assignment (`__setitem__`). - Support slicing for submatrix access. - Define the string representation (`__str__`) for a clean matrix output. 2. **Context Manager**: - Implement the `__enter__` and `__exit__` methods to define behaviors when starting and finishing a logging session. - Use the `with` statement syntax to enable logging during specific code blocks. Example Usage: ```python matrix_a = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) matrix_b = Matrix([[9, 8, 7], [6, 5, 4], [3, 2, 1]]) # Basic operations matrix_c = matrix_a + matrix_b matrix_d = matrix_a * 2 submatrix = matrix_a[:2, :2] print(matrix_c) print(matrix_d) print(submatrix) # Using context manager for logging with matrix_a.log_operations_to(\'operations.log\'): result = matrix_a + matrix_b result = matrix_a @ [1, 1, 1] # Matrix-vector multiplication ``` Constraints: - Matrices should have at least one row and one column. - Operations should only be valid for matrices of compatible dimensions. - Raise appropriate exceptions for invalid operations or indexing. Performance: - Optimize matrix operations for large data sets. Avoid using nested loops where possible and consider using list comprehensions or built-in functions. Implement the `Matrix` class and its context manager to meet the specifications above.","solution":"class Matrix: def __init__(self, data): if not all(len(row) == len(data[0]) for row in data): raise ValueError(\\"All rows must have the same length\\") self.data = data self.rows = len(data) self.cols = len(data[0]) if self.rows > 0 else 0 self.logfile = None def __add__(self, other): if isinstance(other, Matrix) and self.rows == other.rows and self.cols == other.cols: result = [ [self.data[i][j] + other.data[i][j] for j in range(self.cols)] for i in range(self.rows) ] self._log_operation(f\\"Added:n{self}nandn{other}nResult:n{Matrix(result)}\\") return Matrix(result) else: raise ValueError(\\"Matrices must have the same dimensions for addition.\\") def __sub__(self, other): if isinstance(other, Matrix) and self.rows == other.rows and self.cols == other.cols: result = [ [self.data[i][j] - other.data[i][j] for j in range(self.cols)] for i in range(self.rows) ] self._log_operation(f\\"Subtracted:n{self}nandn{other}nResult:n{Matrix(result)}\\") return Matrix(result) else: raise ValueError(\\"Matrices must have the same dimensions for subtraction.\\") def __mul__(self, scalar): if isinstance(scalar, (int, float)): result = [ [self.data[i][j] * scalar for j in range(self.cols)] for i in range(self.rows) ] self._log_operation(f\\"Multiplied:n{self}nby scalar {scalar}nResult:n{Matrix(result)}\\") return Matrix(result) else: raise ValueError(\\"Can only multiply by a scalar (int or float).\\") def __matmul__(self, vector): if isinstance(vector, list) and len(vector) == self.cols: result = [ sum(self.data[i][j] * vector[j] for j in range(self.cols)) for i in range(self.rows) ] self._log_operation(f\\"Matrix-vector multiplication:n{self}nandn{vector}nResult:n{result}\\") return result else: raise ValueError(\\"Matrix-vector multiplication requires a vector with the same number of columns as the matrix.\\") def __getitem__(self, index): if isinstance(index, tuple): row, col = index return self.data[row][col] else: return self.data[index] def __setitem__(self, index, value): if isinstance(index, tuple) and isinstance(value, (int, float)): row, col = index self.data[row][col] = value self._log_operation(f\\"Set element at position ({row}, {col}) to {value}\\") else: raise ValueError(\\"Invalid index or value for matrix assignment.\\") def __str__(self): return \'n\'.join([\' \'.join(map(str, row)) for row in self.data]) def __enter__(self): self._log_operation(\\"Starting logging context manager.\\") return self def __exit__(self, exc_type, exc_value, traceback): self._log_operation(\\"Ending logging context manager.\\") if self.logfile: self.logfile.close() self.logfile = None def log_operations_to(self, filename): self.logfile = open(filename, \'a\') return self def _log_operation(self, message): if self.logfile: self.logfile.write(message + \\"n\\") def slice(self, start_row, end_row, start_col, end_col): sub_matrix = [ row[start_col:end_col] for row in self.data[start_row:end_row] ] return Matrix(sub_matrix)"},{"question":"# Question: Implement a Custom Documentation Generator Objective: Create a custom function to generate HTML documentation for a given Python module. This function should mimic the basic functionality of the `pydoc` module by extracting and formatting the `__doc__` attributes from the module, its classes, functions, and methods. Function Signature: ```python def generate_html_doc(module_name: str, output_file: str) -> None: pass ``` Input: - `module_name` (str): The name of the Python module to generate documentation for. The module should be importable in the current Python environment. - `output_file` (str): The file path where the generated HTML documentation should be saved. Output: - The function should not return any value. It should create an HTML file containing the documentation for the given module and save it to the specified output file. Constraints: 1. The generated HTML file should include the module\'s docstring, as well as docstrings for all classes, methods, and functions within the module. 2. If there is no docstring available, skip that part in the documentation. 3. Structures your HTML to include clear section headings for the module, classes, methods, and functions. 4. Ensure the function handles exceptions gracefully, particularly import errors and file handling exceptions. Example: ```python import example_module # example_module.py content might look like: Example module to demonstrate documentation. class ExampleClass: \'\'\'This is an example class.\'\'\' def method_one(self): \'\'\'This is an example method.\'\'\' pass def example_function(): \'\'\'This is an example function.\'\'\' pass # Usage of the function: generate_html_doc(\'example_module\', \'example_module_doc.html\') # After running the function, example_module_doc.html should contain: <html> <head><title>Documentation for example_module</title></head> <body> <h1>Module: example_module</h1> <p>Example module to demonstrate documentation.</p> <h2>Class: ExampleClass</h2> <p>This is an example class.</p> <h3>Method: method_one(self)</h3> <p>This is an example method.</p> <h2>Function: example_function()</h2> <p>This is an example function.</p> </body> </html> ``` Notes: 1. Use the `inspect` module to retrieve the docstrings and other relevant information. 2. Ensure the generated HTML is well-formed and properly indented for readability.","solution":"import importlib import inspect import os def generate_html_doc(module_name: str, output_file: str) -> None: try: # Import the module module = importlib.import_module(module_name) # Start the HTML structure html_content = f\\"<html>n<head><title>Documentation for {module_name}</title></head>n<body>\\" html_content += f\\"<h1>Module: {module_name}</h1>n\\" # Module docstring module_doc = inspect.getdoc(module) if module_doc: html_content += f\\"<p>{module_doc}</p>n\\" # Process all classes and functions for name, obj in inspect.getmembers(module): if inspect.isclass(obj): # Class name and docstring class_doc = inspect.getdoc(obj) html_content += f\\"<h2>Class: {name}</h2>n\\" if class_doc: html_content += f\\"<p>{class_doc}</p>n\\" # Methods within the class for cls_name, cls_obj in inspect.getmembers(obj): if inspect.isfunction(cls_obj): method_doc = inspect.getdoc(cls_obj) method_name = cls_obj.__name__ method_signature = str(inspect.signature(cls_obj)) html_content += f\\"<h3>Method: {method_name}{method_signature}</h3>n\\" if method_doc: html_content += f\\"<p>{method_doc}</p>n\\" elif inspect.isfunction(obj): # Function name and docstring fn_doc = inspect.getdoc(obj) fn_name = obj.__name__ fn_signature = str(inspect.signature(obj)) html_content += f\\"<h2>Function: {fn_name}{fn_signature}</h2>n\\" if fn_doc: html_content += f\\"<p>{fn_doc}</p>n\\" html_content += \\"</body>n</html>\\" # Write to the output file with open(output_file, \'w\') as file: file.write(html_content) except ImportError: print(f\\"Error: The module \'{module_name}\' could not be imported.\\") except OSError: print(f\\"Error: An error occurred while writing to the file \'{output_file}\'.\\")"},{"question":"# Abstract Base Classes Implementation Challenge In this challenge, you are required to implement a custom data structure by leveraging the `collections.abc` module. The data structure should function as a custom set, preserving all unique elements and retaining the order of their first occurrence (i.e., an ordered set). Requirements: 1. Implement a class `OrderedSet` that inherits from `collections.abc.Set`. 2. The class must override the following abstract methods from the `Set` ABC: - `__contains__(self, value)` - `__iter__(self)` - `__len__(self)` 3. The class must support adding new elements and iterating through elements in the order they were added. 4. Override the `__and__` method to return a new `OrderedSet` containing elements common to both sets, in the order they appear in the first set. 5. Ensure the class can be instantiated with an iterable and provide a way to add new elements (e.g., an `add` method). Function Signature: ```python from collections.abc import Set class OrderedSet(Set): def __init__(self, iterable): # Initialize the OrderedSet def __contains__(self, value): # Return True if value is in the OrderedSet, else False def __iter__(self): # Return an iterator over the OrderedSet def __len__(self): # Return the size of the OrderedSet def __and__(self, other): # Return a new OrderedSet containing elements common to both sets def add(self, value): # Add a new element to the OrderedSet if it does not already exist # You may provide additional methods if needed. ``` Constraints: - The input `iterable` can be any iterable containing hashable items. - The `OrderedSet` should maintain the order of elements based on their first occurrence. Example Usage: ```python s1 = OrderedSet([1, 2, 3, 4]) s2 = OrderedSet([3, 4, 5, 6]) print(len(s1)) # Output: 4 print(3 in s1) # Output: True print(list(s1)) # Output: [1, 2, 3, 4] s1.add(5) print(list(s1)) # Output: [1, 2, 3, 4, 5] s3 = s1 & s2 print(list(s3)) # Output: [3, 4, 5] ``` Your task is to complete the `OrderedSet` class to pass all the constraints and example usages stated above.","solution":"from collections.abc import Set class OrderedSet(Set): def __init__(self, iterable): self.items = [] self.item_set = set() for item in iterable: if item not in self.item_set: self.items.append(item) self.item_set.add(item) def __contains__(self, value): return value in self.item_set def __iter__(self): return iter(self.items) def __len__(self): return len(self.items) def __and__(self, other): common_items = [item for item in self.items if item in other] return OrderedSet(common_items) def add(self, value): if value not in self.item_set: self.items.append(value) self.item_set.add(value)"},{"question":"**Objective**: Implement a function to add a custom search-and-replace feature within an IDLE editor window. **Problem Statement**: You are required to implement a function, `custom_search_and_replace`, that simulates an advanced search-and-replace feature in a text editor, focusing on specific search criteria and supporting regular expressions (regex). This function will be used in an IDLE-like environment and should handle multiple open files with capabilities to search across all of them. # Function Signature ```python def custom_search_and_replace(files: dict, search_pattern: str, replace_with: str, use_regex: bool = False) -> dict: pass ``` # Parameters - `files (dict)`: A dictionary where the key is the filename (a string) and the value is the content of the file (a string). - `search_pattern (str)`: The pattern you want to search for within the files. - `replace_with (str)`: The string with which to replace the found patterns. - `use_regex (bool)`: A boolean flag indicating whether to interpret the search pattern as a regular expression (default is `False`). # Returns - `modified_files (dict)`: A dictionary similar to the input `files`, but with the specified search pattern replaced by `replace_with` where found. # Constraints - The solution should handle large-size text files. - If `use_regex` is `True`, the `search_pattern` should be treated as a regex pattern. - If `use_regex` is `False`, the `search_pattern` should be treated as a plain text string. - Python\'s built-in regex module `re` can be used for regex operations. - Maintain the structure and formatting of the files as provided in the input. # Example ```python input_files = { \\"file1.txt\\": \\"This is a test file. This file is used for testing.\\", \\"file2.py\\": \\"# Python test codenprint(\'Hello World!\')nprint(\'This is a test\')\\" } pattern = \\"test\\" replacement = \\"sample\\" output = custom_search_and_replace(input_files, pattern, replacement) print(output) ``` **Expected Output:** ```python { \\"file1.txt\\": \\"This is a sample file. This file is used for sampling.\\", \\"file2.py\\": \\"# Python sample codenprint(\'Hello World!\')nprint(\'This is a sample\')\\" } ``` **Notes:** - Consider edge cases such as empty files, no matches, and files with only whitespace. - Maintain the original content structure and formatting. - For regular expression operations, consider cases like using word boundaries, capturing groups, and special characters. # Additional Information - Implement efficient searching and replacing to handle files with numerous occurrences efficiently. - Ensure that the function execution time is reasonable for large inputs.","solution":"import re def custom_search_and_replace(files: dict, search_pattern: str, replace_with: str, use_regex: bool = False) -> dict: Replaces occurrences of \'search_pattern\' with \'replace_with\' across all files. If \'use_regex\' is True, treats \'search_pattern\' as a regular expression; otherwise, performs plain text search. :param files: A dictionary with filenames as keys and file content as values :param search_pattern: The pattern to search for in the files :param replace_with: The string to replace the found patterns with :param use_regex: A boolean flag indicating if search_pattern is a regex :return: A dictionary with updated file content modified_files = {} for filename, content in files.items(): if use_regex: # Using regex substitution modified_content = re.sub(search_pattern, replace_with, content) else: # Using plain text substitution modified_content = content.replace(search_pattern, replace_with) modified_files[filename] = modified_content return modified_files"},{"question":"**Advanced Coding Assessment: WAV File Manipulator** *Objective:* Demonstrate your understanding of the Python `wave` module by implementing a function to manipulate WAV files. *Problem Statement:* Write a Python function `process_wav_file(input_file: str, output_file: str)` that reads a given WAV file, makes alterations to the audio data, and writes the modified data to a new WAV file. *Function Specification:* - `input_file` (str): The path to the input WAV file. - `output_file` (str): The path to the output WAV file. *Requirements:* 1. Open the input WAV file in read mode. 2. Retrieve the audio parameters (number of channels, sample width, frame rate, number of frames). 3. Read all the frames from the input file. 4. Reverse the audio frames (for example, if the audio data is `[frame1, frame2, frame3]`, after reversing it should be `[frame3, frame2, frame1]`). 5. Open the output WAV file in write mode. 6. Write the audio parameters to the output file. 7. Write the reversed audio frames to the output file. 8. Ensure proper resource management by using context managers. *Constraints:* - The input WAV file is guaranteed to be in PCM format. - Do not use any external libraries for audio processing. *Input:* - `input_file`: Path to the input WAV file. (e.g., \\"input.wav\\") - `output_file`: Path to the output WAV file. (e.g., \\"output.wav\\") *Output:* - The function should not return anything. - The output file should be created or overwritten with the reversed audio data. *Example:* ```python def process_wav_file(input_file: str, output_file: str): # Your implementation # Example usage: process_wav_file(\\"input.wav\\", \\"output.wav\\") ``` *Tip(s):* - Use `wave.open()` to open WAV files. - Use `Wave_read` methods to get audio parameters and read frames. - Use `Wave_write` methods to set parameters and write frames. - Handle exceptions appropriately to avoid file corruption or resource leaks. *Evaluation Criteria:* - Correctness: The function correctly reverses the audio data. - Efficiency: The function efficiently handles the audio frames, even for larger files. - Resource Management: Proper use of context managers and cleanup.","solution":"import wave def process_wav_file(input_file: str, output_file: str): Reads the input WAV file, reverses the audio data, and writes it to the output WAV file. with wave.open(input_file, \'rb\') as infile: params = infile.getparams() frames = infile.readframes(params.nframes) reversed_frames = frames[::-1] with wave.open(output_file, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(reversed_frames)"},{"question":"# Question: Analyzing Health Expenditure Data with Seaborn **Objective**: Demonstrate your understanding of seaborn\'s `objects` interface to load, transform, and visualize data. You are given a dataset that records health expenditure in USD by various countries over several years. Your tasks are to load this dataset, process it, and create a visualization with specific transformations. **Requirements**: 1. **Load Dataset**: Load the dataset named `\\"healthexp\\"` using `seaborn.load_dataset`. 2. **Normalization and Transformation**: - Create a normalized line plot of health expenditure relative to each country\'s maximum spending. - Create another line plot showing the percent change in spending from the year 1970 for each country. 3. **Visualization**: - Both plots should be color-coded by country. - Properly label the axes for clarity. **Input/Output**: - **Input**: The dataset is preloaded in seaborn and no additional input is required. - **Output**: Two seaborn line plots with the specified transformations. **Constraints**: - Use seaborn\'s `objects` interface (`so.Plot`). - Follow the example patterns provided in the documentation for normalization and transformations. - Ensure the code is clean and properly commented. **Performance**: - The visualization should handle the dataset efficiently without significant lag or performance issues. **Example**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Plot 1: Spending relative to each country\'s maximum amount ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) # Plot 2: Percent change in spending from 1970 baseline ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) ``` Your goal is to replicate and understand these plots, explaining each step in your code with comments.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load the dataset healthexp = load_dataset(\\"healthexp\\") def normalize_spending(df): Normalizes health expenditure relative to each country\'s maximum spending. df[\'Max_Spending\'] = df.groupby(\'Country\')[\'Spending_USD\'].transform(\'max\') df[\'Normalized_Spending\'] = df[\'Spending_USD\'] / df[\'Max_Spending\'] return df def percent_change_from_baseline(df): Calculates the percent change in spending from the year 1970 baseline for each country. df_1970 = df[df[\'Year\'] == 1970][[\'Country\', \'Spending_USD\']].rename(columns={\'Spending_USD\': \'Baseline_Spending\'}) df = df.merge(df_1970, on=\'Country\') df[\'Percent_Change\'] = (df[\'Spending_USD\'] - df[\'Baseline_Spending\']) / df[\'Baseline_Spending\'] * 100 return df # Normalize health expenditure relative to each country\'s maximum spending healthexp_normalized = normalize_spending(healthexp) # Percent change in spending from the year 1970 baseline healthexp_percent_change = percent_change_from_baseline(healthexp) # Plot: Spending relative to each country\'s maximum amount ( so.Plot(healthexp_normalized, x=\\"Year\\", y=\\"Normalized_Spending\\", color=\\"Country\\") .add(so.Lines()) .label(y=\\"Spending relative to maximum amount\\") ) # Plot: Percent change in spending from 1970 baseline ( so.Plot(healthexp_percent_change, x=\\"Year\\", y=\\"Percent_Change\\", color=\\"Country\\") .add(so.Lines()) .label(y=\\"Percent change in spending from 1970 baseline\\") )"},{"question":"Objective Design a custom color palette using Seaborn\'s `sns.blend_palette()` function and apply it to a heatmap plot. This will assess your understanding of interpolating colors, handling various color formats, and applying custom palettes to data visualizations. Question You are given a 5x5 matrix of random integers between 1 and 100. Your task is to: 1. Create a custom color palette by interpolating between at least three colors (of your choice) using `sns.blend_palette()`. 2. Apply this custom palette to a heatmap of the matrix. 3. Display the heatmap. Input There is no input from the user. You should generate a 5x5 matrix of random integers between 1 and 100 within your code. Expected Output The expected output is a visual representation of a heatmap with the custom color palette applied. Instructions 1. Generate a 5x5 matrix of random integers between 1 and 100 using `numpy` or any other method you prefer. 2. Create a custom color palette by interpolating between at least three colors using `sns.blend_palette()`. 3. Use Seaborn\'s `sns.heatmap()` function to create a heatmap of the matrix and apply the custom color palette. 4. Display the heatmap. Example Code Below is an example code snippet to help you get started. Make sure to replace the `colors` list with your own choice of colors. ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Step 1: Generate a 5x5 matrix of random integers between 1 and 100 matrix = np.random.randint(1, 101, size=(5, 5)) # Step 2: Create a custom color palette colors = [\'#123456\', \'green\', \'xkcd:red\'] palette = sns.blend_palette(colors) # Step 3: Create a heatmap with the custom palette sns.set_theme() heatmap = sns.heatmap(matrix, cmap=palette, annot=True) # Display the heatmap plt.show() ``` You are expected to: - Choose your own colors for interpolation. - Ensure that the heatmap visually represents the matrix with the custom color palette. Constraints - Use only built-in Seaborn functions for palette creation and heatmap plotting. - The matrix must be randomly generated within the specified range. Performance Considerations - Generating a 5x5 matrix and creating a heatmap are computationally inexpensive operations. - The focus is on correct implementation and understanding of Seaborn\'s `sns.blend_palette` and `sns.heatmap` functions.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def create_heatmap(): # Step 1: Generate a 5x5 matrix of random integers between 1 and 100 matrix = np.random.randint(1, 101, size=(5, 5)) # Step 2: Create a custom color palette by blending at least three colors colors = [\'#ff0000\', \'#00ff00\', \'#0000ff\'] palette = sns.blend_palette(colors, as_cmap=True) # Step 3: Apply this custom palette to a heatmap of the matrix sns.set_theme() heatmap = sns.heatmap(matrix, cmap=palette, annot=True) # Display the heatmap plt.show() return matrix # Generate the heatmap matrix = create_heatmap()"},{"question":"**Question: Working with Nullable Boolean Data Type in pandas** You are given a DataFrame representing survey responses. Each row in the DataFrame corresponds to a different person\'s responses to three survey questions (Q1, Q2, Q3). Each column contains boolean responses, and some responses are missing (`NA`). Your task is to implement a function `analyze_survey_responses` that takes this DataFrame as input and performs the following operations: 1. Fill all `NA` values with `False`. 2. Create a new Boolean column `all_true` which is `True` if the person\'s responses to all three questions are `True`, otherwise `False`. 3. Create a new Boolean column `any_true` which is `True` if the person responded `True` to at least one question, otherwise `False`. 4. Create a new Boolean column `majority_true` which is `True` if the person responded `True` to at least two questions, otherwise `False`. **Function Signature:** ```python def analyze_survey_responses(df: pd.DataFrame) -> pd.DataFrame: ``` **Input:** - `df` (pandas DataFrame): The DataFrame with columns `Q1`, `Q2`, `Q3` containing boolean and NA values. **Output:** - Returns a pandas DataFrame with the original columns and three additional columns: `all_true`, `any_true`, and `majority_true`. **Example:** Given the input DataFrame: ``` Q1 Q2 Q3 0 True NaN True 1 False False NaN 2 True True NaN 3 True False False 4 None NaN True ``` The returned DataFrame should be: ``` Q1 Q2 Q3 all_true any_true majority_true 0 True False True False True True 1 False False False False False False 2 True True False False True True 3 True False False False True False 4 False False True False True False ``` _Note: use appropriate pandas methods to handle NA values and implement logical calculations._","solution":"import pandas as pd def analyze_survey_responses(df: pd.DataFrame) -> pd.DataFrame: # Fill NA values with False df = df.fillna(False) # Create new columns as specified df[\'all_true\'] = df[[\'Q1\', \'Q2\', \'Q3\']].all(axis=1) df[\'any_true\'] = df[[\'Q1\', \'Q2\', \'Q3\']].any(axis=1) df[\'majority_true\'] = df[[\'Q1\', \'Q2\', \'Q3\']].sum(axis=1) >= 2 return df"},{"question":"**Question: Nested Function Call and Exception Handling** You are asked to implement a Python program that demonstrates your understanding of function definitions, variable scope, and exception handling. Specifically, you will need to create a nested function scenario with appropriate error handling. # Task 1. Write a function `outer_function` that takes one argument `x`. 2. Inside `outer_function`, define another function `inner_function` that modifies `x` and handles a potential exception. 3. `inner_function` should: - Attempt to perform an operation that might raise a `ZeroDivisionError`. - Catch the `ZeroDivisionError` and raise a custom `ValueError` with the message \\"Division by zero encountered\\". 4. `outer_function` should call `inner_function` and return its result. 5. Your implementation should demonstrate proper naming and binding of variables, adhering to the scoping rules described in the documentation. # Input - An integer `x` (where `x` is any valid integer). # Output - The result of the operation performed by `inner_function` if no exception is encountered. - If a `ZeroDivisionError` is encountered, a `ValueError` with the message \\"Division by zero encountered\\". # Function Signature ```python def outer_function(x: int) -> int: pass ``` # Constraints - Ensure that variable scoping rules are followed as described in the Python 3.10 execution model. - Proper error handling must be implemented to catch `ZeroDivisionError` and raise `ValueError`. # Examples ```python # Example 1 result = outer_function(10) print(result) # Expected output: (based on the operation defined in inner_function) # Example 2 result = outer_function(0) print(result) # Expected output: ValueError with message \\"Division by zero encountered\\" ``` Note: You will need to define the specific operation in `inner_function` that might raise the `ZeroDivisionError`.","solution":"def outer_function(x: int) -> int: def inner_function(y): try: # Attempt an operation that could raise a ZeroDivisionError result = 10 / y return result except ZeroDivisionError: raise ValueError(\\"Division by zero encountered\\") return inner_function(x)"},{"question":"# Advanced Python Exceptions Handling Objective: You are tasked with writing a Python function `safe_divide_and_access` that: 1. Safely divides two numbers provided as input. 2. Attempts to access an element from a list at a specified index. The function should demonstrate robust exception handling to manage various issues that may arise during these operations, reflecting an understanding of Python\'s built-in exceptions. Function Signature: ```python def safe_divide_and_access(a: int, b: int, lst: list, index: int) -> dict: Perform division and access list element safely, handling various exceptions. Parameters: a (int): The numerator. b (int): The denominator. lst (list): The list from which an element is to be accessed. index (int): The index of the element to be accessed. Returns: dict: A dictionary with the results of the division and element access, and any errors encountered. ``` Input: - `a`: An integer representing the numerator. - `b`: An integer representing the denominator. - `lst`: A list from which an element is to be accessed. - `index`: An integer representing the index of the element to access. Output: - A dictionary containing: - `division_result`: Result of `a / b` if no exception occurs. - `element`: The element at the specified index in `lst` if no exception occurs. - `errors`: Any exceptions raised during the operations, categorized by type. Constraints: - Use Python\'s built-in exceptions to handle errors. - Ensure that division by zero, list index out of range, and type errors are managed. - Maintain the traceback information with each exception for debugging purposes. Example: ```python >>> safe_divide_and_access(10, 2, [1, 2, 3], 1) {\'division_result\': 5.0, \'element\': 2, \'errors\': {}} >>> safe_divide_and_access(10, 0, [1, 2, 3], 1) {\'division_result\': None, \'element\': 2, \'errors\': {\'ZeroDivisionError\': \'division by zero\'}} >>> safe_divide_and_access(10, 2, [1, 2, 3], 5) {\'division_result\': 5.0, \'element\': None, \'errors\': {\'IndexError\': \'list index out of range\'}} ``` Considerations: - Preserve the context of exceptions using the `raise from` syntax where appropriate. - Add comments explaining each part of the exception handling mechanism. - Implement custom exception messages where necessary to enhance clarity. Good luck, and make sure to handle each exception carefully!","solution":"def safe_divide_and_access(a: int, b: int, lst: list, index: int) -> dict: Perform division and access list element safely, handling various exceptions. Parameters: a (int): The numerator. b (int): The denominator. lst (list): The list from which an element is to be accessed. index (int): The index of the element to be accessed. Returns: dict: A dictionary with the results of the division and element access, and any errors encountered. result = { \'division_result\': None, \'element\': None, \'errors\': {} } # Handle division with exception handling. try: result[\'division_result\'] = a / b except ZeroDivisionError as e: result[\'errors\'][\'ZeroDivisionError\'] = str(e) except TypeError as e: result[\'errors\'][\'TypeError\'] = str(e) # Handle list index access with exception handling. try: result[\'element\'] = lst[index] except IndexError as e: result[\'errors\'][\'IndexError\'] = str(e) except TypeError as e: result[\'errors\'][\'TypeError\'] = str(e) return result"},{"question":"Objective: The objective of this task is to demonstrate your understanding of scikit-learn\'s PCA and Incremental PCA for dimensionality reduction. Task: You are given a large dataset that cannot fit into memory at once. Your task is to implement two functions: 1. `perform_pca`: Uses standard PCA from scikit-learn to reduce the dataset\'s dimensionality into a specified number of components. 2. `perform_incremental_pca`: Uses IncrementalPCA from scikit-learn to incrementally reduce the dataset\'s dimensionality into a specified number of components, processing the dataset in chunks. Details: 1. **perform_pca(data: np.ndarray, n_components: int) -> np.ndarray:** - **Input:** - `data`: A NumPy 2D array of shape `(n_samples, n_features)` containing the dataset. - `n_components`: An integer specifying the number of principal components to reduce the dataset to. - **Output:** - A NumPy 2D array of shape `(n_samples, n_components)` which is the reduced dataset. 2. **perform_incremental_pca(data: np.ndarray, n_components: int, batch_size: int) -> np.ndarray:** - **Input:** - `data`: A NumPy 2D array of shape `(n_samples, n_features)` containing the dataset. - `n_components`: An integer specifying the number of principal components to reduce the dataset to. - `batch_size`: An integer specifying the number of samples per batch for incremental PCA processing. - **Output:** - A NumPy 2D array of shape `(n_samples, n_components)` which is the reduced dataset. Constraints and Performance Requirements: 1. The dataset for `perform_pca` should fit in memory. 2. The dataset for `perform_incremental_pca` should be too large to fit in memory at once, and it must be processed in chunks specified by `batch_size`. 3. You are required to use the `PCA` and `IncrementalPCA` classes from `sklearn.decomposition`. 4. Ensure the functions are efficient and handle large datasets appropriately. Example Usage: ```python import numpy as np from sklearn.datasets import make_blobs # Generating a sample dataset data, _ = make_blobs(n_samples=10000, n_features=20, centers=3, random_state=42) # Perform standard PCA reduced_data_pca = perform_pca(data, n_components=5) print(reduced_data_pca.shape) # Expected output: (10000, 5) # Perform Incremental PCA reduced_data_inc_pca = perform_incremental_pca(data, n_components=5, batch_size=1000) print(reduced_data_inc_pca.shape) # Expected output: (10000, 5) ``` Note: - Write appropriate docstrings for your functions. - Ensure your code is clean, readable, and well-documented.","solution":"import numpy as np from sklearn.decomposition import PCA, IncrementalPCA def perform_pca(data: np.ndarray, n_components: int) -> np.ndarray: Uses standard PCA to reduce the dataset\'s dimensionality. Parameters: data (np.ndarray): A NumPy 2D array of shape (n_samples, n_features) containing the dataset. n_components (int): The number of principal components to reduce the dataset to. Returns: np.ndarray: A NumPy 2D array of shape (n_samples, n_components) which is the reduced dataset. pca = PCA(n_components=n_components) reduced_data = pca.fit_transform(data) return reduced_data def perform_incremental_pca(data: np.ndarray, n_components: int, batch_size: int) -> np.ndarray: Uses IncrementalPCA to incrementally reduce the dataset\'s dimensionality in batches. Parameters: data (np.ndarray): A NumPy 2D array of shape (n_samples, n_features) containing the dataset. n_components (int): The number of principal components to reduce the dataset to. batch_size (int): The number of samples per batch for incremental PCA processing. Returns: np.ndarray: A NumPy 2D array of shape (n_samples, n_components) which is the reduced dataset. ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size) n_samples = data.shape[0] for i in range(0, n_samples, batch_size): batch_data = data[i:i+batch_size] ipca.partial_fit(batch_data) reduced_data = ipca.transform(data) return reduced_data"},{"question":"# Advanced Python Import System Task Objective You are required to create a custom module importer to demonstrate your understanding of Python\'s import system, including both meta path finders and path-based file finders. Problem Statement Consider a situation where you need to manage a custom import system that allows for importing modules from non-standard locations. You will implement this by creating a custom meta path finder and a path entry finder. # Part 1: Custom Meta Path Finder 1. **Define a Custom Meta Path Finder:** Implement a custom meta path finder called `CustomMetaFinder` that can: - Intercept import attempts for modules that start with the prefix `\\"custom_\\"`. - Raise a `ModuleNotFoundError` if the module does not exist within a predefined dictionary called `custom_modules`. 2. **Define the Finder Code:** ```python custom_modules = { \'custom_hello\': \'print(\\"Hello from custom module!\\")\' } class CustomMetaFinder: @classmethod def find_spec(cls, fullname, path, target=None): if fullname in custom_modules: spec = importlib.machinery.ModuleSpec( name=fullname, loader=CustomLoader(fullname), origin=\'custom\' ) return spec return None ``` # Part 2: Custom Loader 3. **Define a Custom Loader:** Create a loader called `CustomLoader` that: - Executes the module\'s code from the `custom_modules` dictionary when the module is imported. 4. **Define the Loader Code:** ```python class CustomLoader: def __init__(self, module_name): self.module_name = module_name def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = custom_modules[self.module_name] exec(code, module.__dict__) ``` # Part 3: Testing Your Importer 5. **Modify `sys.meta_path`:** Add an instance of your `CustomMetaFinder` to `sys.meta_path` to ensure it\'s part of the import system. ```python import sys if CustomMetaFinder not in sys.meta_path: sys.meta_path.insert(0, CustomMetaFinder) ``` 6. **Test Your Implementation:** Write a script that: - Imports the `custom_hello` module. - Verifies that it executes the code defined in `custom_modules`. ```python import custom_hello ``` Input No direct input will be provided; instead, you will complete the scripts as described above from scratch. Output When you run the complete code, it should output the string: ``` Hello from custom module! ``` Constraints - Ensure proper error handling for modules not found in the `custom_modules` dictionary. - The custom import system must only target modules that start with the `\\"custom_\\"` prefix. Guidelines - You should be comfortable with the import system’s inner workings, including meta path finders and loaders. - Ensure you understand the `importlib.machinery.ModuleSpec` and the attributes it requires.","solution":"import importlib.machinery import sys # A dictionary representing custom modules and their content custom_modules = { \'custom_hello\': \'print(\\"Hello from custom module!\\")\' } class CustomMetaFinder: @classmethod def find_spec(cls, fullname, path, target=None): if fullname in custom_modules: spec = importlib.machinery.ModuleSpec( name=fullname, loader=CustomLoader(fullname), origin=\'custom\' ) return spec return None class CustomLoader: def __init__(self, module_name): self.module_name = module_name def create_module(self, spec): return None # Use default module creation semantics def exec_module(self, module): code = custom_modules[self.module_name] exec(code, module.__dict__) # Ensure our custom meta path finder is part of the import system if CustomMetaFinder not in sys.meta_path: sys.meta_path.insert(0, CustomMetaFinder)"},{"question":"Objective The objective of this question is to assess your understanding of Kernel Density Estimation (KDE) using scikit-learn. You will need to perform KDE on a given dataset and visualize the results using different kernel functions. Task 1. **Data Generation**: - Create a 1D bimodal dataset with 200 data points. You can use two Gaussian distributions with different means and standard deviations. 2. **KDE Implementation**: - Implement Kernel Density Estimation on the dataset using three different kernel functions: *Gaussian*, *Epanechnikov*, and *Cosine*. 3. **Evaluation**: - Evaluate the density estimates at multiple points and compare the smoothness of the resulting distributions for different kernels. 4. **Visualization**: - Plot the original data as a histogram and overlay the KDE estimates from the three kernels to visually compare the results. Input Format - No input is required from the user. You should generate the dataset within your code. Output Format - Display three plots: 1. A histogram of the original dataset. 2. KDE estimates from the Gaussian kernel overlaid on the histogram. 3. KDE estimates from the Epanechnikov kernel overlaid on the histogram. 4. KDE estimates from the Cosine kernel overlaid on the histogram. Additional Constraints - Use a fixed random seed (`np.random.seed(42)`) to ensure reproducibility of the dataset. Example Output - Your output should include the histograms overlaid with KDE estimates for the three kernels. Here\'s an example of what the visualizations might look like: ```plaintext [ Plot 1: Histogram with Gaussian KDE ] [ Plot 2: Histogram with Epanechnikov KDE ] [ Plot 3: Histogram with Cosine KDE ] ``` Implementation ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Step 1: Data Generation np.random.seed(42) n_samples = 200 data = np.concatenate([ np.random.normal(loc=-2, scale=0.5, size=n_samples // 2), np.random.normal(loc=2, scale=0.5, size=n_samples // 2) ]) # Step 2: KDE Implementation kernels = [\'gaussian\', \'epanechnikov\', \'cosine\'] kde_models = {} for kernel in kernels: kde = KernelDensity(kernel=kernel, bandwidth=0.5).fit(data[:, None]) kde_models[kernel] = kde # Step 3: Evaluation x_d = np.linspace(-5, 5, 1000)[:, None] log_dens = {kernel: kde.score_samples(x_d) for kernel, kde in kde_models.items()} # Step 4: Visualization plt.figure(figsize=(15, 10)) # Plotting histogram of the data plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', edgecolor=\'black\') # Overlay KDEs for kernel in kernels: plt.plot(x_d, np.exp(log_dens[kernel]), label=f\\"KDE with {kernel} kernel\\") plt.legend() plt.title(\'Kernel Density Estimation with Different Kernels\') plt.xlabel(\'x\') plt.ylabel(\'Density\') plt.show() ``` --- By completing this task, you will demonstrate your understanding of generating datasets, performing kernel density estimation using different kernel types, and visualizing the effects of these kernels on the estimated density functions.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_bimodal_data(seed=42, n_samples=200): Generates a bimodal dataset with specified seed and number of samples. np.random.seed(seed) data = np.concatenate([ np.random.normal(loc=-2, scale=0.5, size=n_samples // 2), np.random.normal(loc=2, scale=0.5, size=n_samples // 2) ]) return data def perform_kde(data, kernels=[\'gaussian\', \'epanechnikov\', \'cosine\'], bandwidth=0.5): Performs Kernel Density Estimation using specified kernels and bandwidth. kde_models = {} for kernel in kernels: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data[:, None]) kde_models[kernel] = kde return kde_models def get_log_densities(kde_models, x_d): Retrieves the log densities for the given KDE models at points x_d. log_dens = {kernel: kde.score_samples(x_d) for kernel, kde in kde_models.items()} return log_dens def visualize(data, x_d, log_dens): Visualizes the histogram of the data and overlays KDE estimates. plt.figure(figsize=(15, 10)) # Plotting histogram of the data plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', edgecolor=\'black\') # Overlay KDEs for kernel, log_den in log_dens.items(): plt.plot(x_d, np.exp(log_den), label=f\\"KDE with {kernel} kernel\\") plt.legend() plt.title(\'Kernel Density Estimation with Different Kernels\') plt.xlabel(\'x\') plt.ylabel(\'Density\') plt.show() def main(): data = generate_bimodal_data() kde_models = perform_kde(data) x_d = np.linspace(-5, 5, 1000)[:, None] log_dens = get_log_densities(kde_models, x_d) visualize(data, x_d, log_dens) if __name__ == \\"__main__\\": main()"},{"question":"# HTML Entities Translation You are given four dictionaries from the `html.entities` module: `html5`, `name2codepoint`, `codepoint2name`, and `entitydefs`. Your task is to implement a function that demonstrates the use of these dictionaries to translate between HTML entities and Unicode characters. # Function 1: entity_to_unicode Write a function `entity_to_unicode(entity_name: str) -> str` that takes an HTML entity name (like `&gt;` or `&amp;`) as input and returns its corresponding Unicode character. If the entity does not have a corresponding Unicode character, return an empty string. # Function 2: unicode_to_entity Write a function `unicode_to_entity(character: str) -> str` that takes a single Unicode character as input and returns its corresponding HTML entity name. If the character does not have a corresponding HTML entity name, return an empty string. # Example ```python # Function Definitions def entity_to_unicode(entity_name: str) -> str: # Implement your function here def unicode_to_entity(character: str) -> str: # Implement your function here # Sample Usage print(entity_to_unicode(\\"&gt;\\")) # Output: \\">\\" print(entity_to_unicode(\\"&unknown;\\")) # Output: \\"\\" print(unicode_to_entity(\\">\\")) # Output: \\"&gt;\\" print(unicode_to_entity(\\"@\\")) # Output: \\"\\" ``` # Constraints - The input entity name for `entity_to_unicode` will have the format `&name;` where `name` is the entity name (case-sensitive). - The input character for `unicode_to_entity` will be a single printable ASCII or Unicode character. # Note You can use the provided dictionaries (`html5`, `name2codepoint`, `codepoint2name`, `entitydefs`) in your solution. Ensure your functions handle edge cases, like unknown entities or characters without corresponding names, gracefully.","solution":"import html def entity_to_unicode(entity_name: str) -> str: Translates an HTML entity to its corresponding Unicode character. Parameters: - entity_name (str): The HTML entity name (e.g., \\"&gt;\\"). Returns: - str: The corresponding Unicode character or an empty string if not found. if entity_name.startswith(\'&\') and entity_name.endswith(\';\'): entity_name_strip = entity_name[1:-1] if entity_name_strip in html.entities.name2codepoint: return chr(html.entities.name2codepoint[entity_name_strip]) return \\"\\" def unicode_to_entity(character: str) -> str: Translates a Unicode character to its corresponding HTML entity name. Parameters: - character (str): The Unicode character. Returns: - str: The corresponding HTML entity name or an empty string if not found. if len(character) == 1: codepoint = ord(character) if codepoint in html.entities.codepoint2name: return f\\"&{html.entities.codepoint2name[codepoint]};\\" return \\"\\""},{"question":"**Coding Task: Custom Exception Handling in Python** # Objective Demonstrate your understanding of Python exception handling by implementing a custom exception, handling nested exceptions, and appropriately using error messages. # Task You are required to implement a Python function `process_data(data)` that processes a given list of integers where `data` represents the list. Your task is to: 1. Raise a custom exception `InvalidDataError` if the list contains any non-integer values. 2. Raise a custom exception `ProcessingError` if the list is empty. 3. Compute the sum of integers in the list. If the sum exceeds 100, raise a built-in `ValueError` with a message indicating \'Sum exceeds allowed limit\'. 4. Ensure that all exceptions are appropriately caught and handled. Print a specific message corresponding to each exception type. # Guidelines - Implement two custom exception classes: `InvalidDataError` and `ProcessingError`. - Use `try`, `except`, and `raise` constructs to manage exceptions. - Ensure the function prints an appropriate message for each caught exception. - Use the `Exception` base class for your custom exceptions. # Input Format - A single argument `data`, which is a list of integers. # Output Format - If an exception is raised, print the message related to that exception. - If no exception occurs, return the sum of the list. # Constraints - `data` is always a list, but may contain non-integer values. - The length of `data` is between 0 and 100 (both inclusive). # Example ```python class InvalidDataError(Exception): pass class ProcessingError(Exception): pass def process_data(data): # Your implementation here # Test cases print(process_data([1, 2, \'a\'])) # Should print \\"Invalid data detected.\\" print(process_data([])) # Should print \\"No data to process.\\" print(process_data([50, 60, 10])) # Should print \\"Sum exceeds allowed limit.\\" print(process_data([10, 20, 30])) # Should return 60 ``` # Notes - Implement the custom exceptions within the provided template. - Ensure all functions and classes are properly documented. - Test your function with the provided test cases to ensure correctness.","solution":"class InvalidDataError(Exception): Exception raised when data contains non-integer values. pass class ProcessingError(Exception): Exception raised when data is empty. pass def process_data(data): Process the given list of integers and handle various exceptions. Parameters: data (list): List of integers to be processed. Returns: int: Sum of integers in the list if no exception is raised. try: if not data: raise ProcessingError(\\"No data to process.\\") for item in data: if not isinstance(item, int): raise InvalidDataError(\\"Invalid data detected.\\") total_sum = sum(data) if total_sum > 100: raise ValueError(\\"Sum exceeds allowed limit.\\") return total_sum except InvalidDataError as e: print(e) except ProcessingError as e: print(e) except ValueError as e: print(e)"},{"question":"Problem Statement You are given a sequence of floating-point numbers. Your task is to implement a function that verifies if the sequence represents a possible unbiased random walk along the X-axis and calculates the final position on the X-axis at the end of the walk. An unbiased random walk is a mathematical statistical model where each step is determined by a random decision to move either positively or negatively with equal probability. For the purpose of this problem, assume each move is either +1 or -1 and the sequence given are the positions attained after each move. To assess whether a given sequence is a valid unbiased random walk, the following conditions must be met: 1. The initial position is 0. 2. Each subsequent position in the sequence must only differ by +1 or -1 from the previous position. 3. The sequence ends at 0 or any value. If the sequence is valid, calculate the final position on the X-axis as the sum of all steps, otherwise, return an invalid sequence error. Function Signature ```python def validate_and_calculate_walk(sequence: list[float]) -> float: pass ``` Input - `sequence`: A list of floating-point numbers representing positions at each step of the walk. Output - If the sequence is a valid unbiased random walk, return the final position on the X-axis as a float. - If the sequence is not a valid walk, raise a `ValueError` with the message \\"Invalid sequence\\". Constraints - The sequence contains at least one number. - Each number in the sequence is guaranteed to be a valid floating-point number. Example ```python # Valid unbiased random walk sequence sequence = [0.0, 1.0, 0.0, -1.0, 0.0] assert validate_and_calculate_walk(sequence) == 0.0 # Invalid sequence, as 2.0 is not a valid step from 1.0 sequence_invalid = [0.0, 1.0, 2.0] try: validate_and_calculate_walk(sequence_invalid) except ValueError as e: assert str(e) == \\"Invalid sequence\\" ``` Notes - Use the `math.isclose` function to handle possible floating-point precision issues. - Carefully handle edge cases, including sequences with repetitive steps and large sequences. Good luck!","solution":"import math def validate_and_calculate_walk(sequence: list[float]) -> float: Validate if the given sequence is a valid unbiased random walk and return the final position on the X-axis. # Check that the sequence starts at 0 if not math.isclose(sequence[0], 0.0): raise ValueError(\\"Invalid sequence\\") # Validate each step and compute the final position for i in range(1, len(sequence)): if not (math.isclose(sequence[i], sequence[i-1] + 1.0) or math.isclose(sequence[i], sequence[i-1] - 1.0)): raise ValueError(\\"Invalid sequence\\") return float(sequence[-1])"},{"question":"Problem Statement You are given a dense matrix representing a graph adjacency matrix where most of the elements are zero. Your task is to perform various operations using PyTorch\'s sparse tensor functionalities to optimize memory and computational efficiency. Requirements 1. **Convert** the given dense matrix to a sparse COO tensor. 2. **Convert** the sparse COO tensor to a sparse CSR tensor. 3. **Perform** a matrix multiplication between the sparse CSR tensor and another dense matrix. 4. **Compute** the result of element-wise addition between the sparse CSR tensor and another dense matrix, ensuring the result is a sparse CSR tensor. 5. **Implement** and handle the conversion back to a dense format for final comparison. # Function Signatures ```python def dense_to_sparse_coo(dense_matrix: torch.Tensor) -> torch.Tensor: Converts a given dense matrix to a sparse COO tensor. Args: dense_matrix (torch.Tensor): The dense matrix to be converted. Returns: torch.Tensor: The sparse COO tensor. pass def sparse_coo_to_sparse_csr(sparse_coo_tensor: torch.Tensor) -> torch.Tensor: Converts a given sparse COO tensor to a sparse CSR tensor. Args: sparse_coo_tensor (torch.Tensor): The sparse COO tensor to be converted. Returns: torch.Tensor: The sparse CSR tensor. pass def sparse_csr_matrix_multiplication(sparse_csr_tensor: torch.Tensor, dense_matrix: torch.Tensor) -> torch.Tensor: Performs matrix multiplication between a sparse CSR tensor and a dense matrix. Args: sparse_csr_tensor (torch.Tensor): The sparse CSR tensor. dense_matrix (torch.Tensor): The dense matrix. Returns: torch.Tensor: The resulting tensor after matrix multiplication. pass def sparse_csr_elementwise_addition(sparse_csr_tensor: torch.Tensor, dense_matrix: torch.Tensor) -> torch.Tensor: Computes element-wise addition between a sparse CSR tensor and a dense matrix, returning a sparse CSR tensor. Args: sparse_csr_tensor (torch.Tensor): The sparse CSR tensor. dense_matrix (torch.Tensor): The dense matrix. Returns: torch.Tensor: The resulting sparse CSR tensor after element-wise addition. pass def sparse_to_dense(sparse_tensor: torch.Tensor) -> torch.Tensor: Converts a sparse tensor back to its dense format. Args: sparse_tensor (torch.Tensor): The sparse tensor to be converted. Returns: torch.Tensor: The dense tensor. pass ``` # Constraints - The input dense matrix will have dimensions up to 1000 x 1000. - Sparse matrices may have up to 95% zeroes. - The computations should leverage the memory and computational optimizations of sparse tensors. # Performance Requirements - Ensure the operations make efficient use of sparse tensor optimizations provided by PyTorch. - Conversion operations should adhere to the documentation\'s best practices for using sparse tensor storage formats. # Example Usage ```python # Example Dense Matrix dense_matrix = torch.tensor([ [0, 2, 0], [3, 0, 0], [0, 4, 5] ], dtype=torch.float32) # Convert Dense to Sparse COO sparse_coo = dense_to_sparse_coo(dense_matrix) print(sparse_coo) # Convert Sparse COO to Sparse CSR sparse_csr = sparse_coo_to_sparse_csr(sparse_coo) print(sparse_csr) # Perform Sparse CSR Matrix Multiplication result = sparse_csr_matrix_multiplication(sparse_csr, dense_matrix) print(result) # Perform Sparse CSR Element-wise Addition result_add = sparse_csr_elementwise_addition(sparse_csr, dense_matrix) print(result_add) # Convert Sparse to Dense converted_dense = sparse_to_dense(result_add) print(converted_dense) ``` The given examples illustrate the transformations and operations required to work efficiently with sparse tensors using PyTorch. Make sure to implement each function correctly to handle the sparse tensor optimizations efficiently.","solution":"import torch def dense_to_sparse_coo(dense_matrix: torch.Tensor) -> torch.Tensor: Converts a given dense matrix to a sparse COO tensor. Args: dense_matrix (torch.Tensor): The dense matrix to be converted. Returns: torch.Tensor: The sparse COO tensor. sparse_coo_tensor = dense_matrix.to_sparse() return sparse_coo_tensor def sparse_coo_to_sparse_csr(sparse_coo_tensor: torch.Tensor) -> torch.Tensor: Converts a given sparse COO tensor to a sparse CSR tensor. Args: sparse_coo_tensor (torch.Tensor): The sparse COO tensor to be converted. Returns: torch.Tensor: The sparse CSR tensor. # Convert COO to CSR sparse_csr_tensor = sparse_coo_tensor.to_sparse_csr() return sparse_csr_tensor def sparse_csr_matrix_multiplication(sparse_csr_tensor: torch.Tensor, dense_matrix: torch.Tensor) -> torch.Tensor: Performs matrix multiplication between a sparse CSR tensor and a dense matrix. Args: sparse_csr_tensor (torch.Tensor): The sparse CSR tensor. dense_matrix (torch.Tensor): The dense matrix. Returns: torch.Tensor: The resulting tensor after matrix multiplication. result = torch.mm(sparse_csr_tensor.to_dense(), dense_matrix) return result def sparse_csr_elementwise_addition(sparse_csr_tensor: torch.Tensor, dense_matrix: torch.Tensor) -> torch.Tensor: Computes element-wise addition between a sparse CSR tensor and a dense matrix, returning a sparse CSR tensor. Args: sparse_csr_tensor (torch.Tensor): The sparse CSR tensor. dense_matrix (torch.Tensor): The dense matrix. Returns: torch.Tensor: The resulting sparse CSR tensor after element-wise addition. result_dense = sparse_csr_tensor.to_dense() + dense_matrix result_sparse_csr = result_dense.to_sparse_csr() return result_sparse_csr def sparse_to_dense(sparse_tensor: torch.Tensor) -> torch.Tensor: Converts a sparse tensor back to its dense format. Args: sparse_tensor (torch.Tensor): The sparse tensor to be converted. Returns: torch.Tensor: The dense tensor. dense_tensor = sparse_tensor.to_dense() return dense_tensor"},{"question":"You are required to create a Python script that processes a collection of text files, applies certain transformations based on command-line options, and outputs the modified content to either the console or a specified file. **Task:** 1. **Define Command-Line Options:** - `-i` or `--input`: Specifies the input file(s). This option can be supplied multiple times to process multiple files. - `-u` or `--uppercase`: A flag indicating that the content of the input file(s) should be converted to uppercase. - `-r` or `--reverse`: A flag indicating that the lines of the input file(s) should be reversed in order. - `-o` or `--output`: Specifies the output file. If not provided, the output should be printed to the console. - `-v` or `--verbose`: A flag to print detailed processing information. 2. **Implement the Script:** - Parse the command-line arguments using the `optparse` module. - Read the content of the input file(s). - Apply transformations (uppercase and/or reverse) if the respective flags are set. - Output the transformed content either to the console (default) or to the specified output file. - If the verbose flag is set, print detailed information about each step of the processing. **Input:** - Command-line arguments. **Output:** - Transformed content either printed to the console or written to an output file. **Constraints:** - You must use the `optparse` module for command-line parsing. - Your script should handle multiple input files and apply the transformations to each file separately. - If both the `--uppercase` and `--reverse` flags are provided, apply `--uppercase` first, then `--reverse`. **Example Execution:** ```sh python transform_text.py --input file1.txt --input file2.txt --uppercase --output result.txt ``` If `file1.txt` contains: ``` hello world this is a test ``` And `file2.txt` contains: ``` another example with multiple lines ``` Then `result.txt` should contain: ``` THIS IS A TEST HELLO WORLD WITH MULTIPLE LINES ANOTHER EXAMPLE ``` Implement the script as described and ensure it handles the specified command-line options correctly.","solution":"import optparse import sys def process_input_files(file_list, to_uppercase, to_reverse): content_list = [] for filepath in file_list: try: with open(filepath, \'r\') as file: content = file.readlines() if to_uppercase: content = [line.upper() for line in content] if to_reverse: content.reverse() content_list.extend(content) except FileNotFoundError: print(f\\"Error: File not found - {filepath}\\") except Exception as e: print(f\\"Error processing {filepath}: {e}\\") return content_list def main(): parser = optparse.OptionParser() parser.add_option(\'-i\', \'--input\', dest=\'input_files\', action=\\"append\\", help=\'Input file(s)\', metavar=\'FILE\') parser.add_option(\'-u\', \'--uppercase\', dest=\'uppercase\', action=\'store_true\', default=False, help=\'Convert content to uppercase\') parser.add_option(\'-r\', \'--reverse\', dest=\'reverse\', action=\'store_true\', default=False, help=\'Reverse the lines\') parser.add_option(\'-o\', \'--output\', dest=\'output_file\', help=\'Output file\', metavar=\'FILE\') parser.add_option(\'-v\', \'--verbose\', dest=\'verbose\', action=\'store_true\', default=False, help=\'Print detailed processing information\') options, args = parser.parse_args() if options.input_files: if options.verbose: print(\\"Starting processing:\\") print(f\\"Input files: {options.input_files}\\") print(f\\"Uppercase: {\'Yes\' if options.uppercase else \'No\'}\\") print(f\\"Reverse: {\'Yes\' if options.reverse else \'No\'}\\") processed_content = process_input_files(options.input_files, options.uppercase, options.reverse) if options.output_file: try: with open(options.output_file, \'w\') as output_file: output_file.writelines(processed_content) if options.verbose: print(f\\"Output written to {options.output_file}\\") except Exception as e: print(f\\"Error writing to output file {options.output_file}: {e}\\") else: for line in processed_content: sys.stdout.write(line) if options.verbose: print(\\"Output written to console\\") else: print(\\"Error: At least one input file must be specified\\") if __name__ == \\"__main__\\": main()"},{"question":"# WAV File Manipulation **Objective:** You need to create two functions to work with WAV files using Python\'s `wave` module. The first function will read specific properties from a given WAV file, and the second function will create a new WAV file with modified audio data. **Function 1: `read_wav_info`** Implement a function `read_wav_info(wav_file: str) -> dict` that takes the path to an existing WAV file as input and returns a dictionary containing the following information: - `channels`: Number of audio channels (1 for mono, 2 for stereo). - `sample_width`: Sample width in bytes. - `frame_rate`: Sampling frequency. - `num_frames`: Total number of audio frames. **Function 2: `copy_wav_with_modifications`** Implement a function `copy_wav_with_modifications(input_wav: str, output_wav: str, volume_factor: float) -> None` that reads the audio data from the `input_wav` file, scales the amplitude of the audio frames by `volume_factor`, and writes the modified audio data to the `output_wav` file. Ensure all properties of the input file (channels, sample width, frame rate) are preserved in the output file. # Example ```python # Example usage info = read_wav_info(\\"input.wav\\") print(info) # Output: {\'channels\': 2, \'sample_width\': 2, \'frame_rate\': 44100, \'num_frames\': 100000} copy_wav_with_modifications(\\"input.wav\\", \\"output.wav\\", 0.5) # This will create output.wav with half the volume of input.wav ``` # Constraints - The WAV file given as input to `read_wav_info` and the `input_wav` of `copy_wav_with_modifications` will always exist and be reachable. - You may assume the `volume_factor` will be a positive float, where values < 1.0 decrease volume, and values > 1.0 increase volume. - The functions should handle files up to 100MB efficiently. # Requirements: 1. Ensure correct usage of the `wave` module to read from and write to WAV files. 2. Calculate the new audio frame values correctly when modifying the volume. 3. Handle any potential exceptions or errors gracefully, providing meaningful error messages.","solution":"import wave import numpy as np def read_wav_info(wav_file: str) -> dict: Reads specific properties from a given WAV file. Args: wav_file (str): Path to the WAV file. Returns: dict: Dictionary containing the following information: - channels: Number of audio channels. - sample_width: Sample width in bytes. - frame_rate: Sampling frequency. - num_frames: Total number of audio frames. with wave.open(wav_file, \'rb\') as wav: return { \'channels\': wav.getnchannels(), \'sample_width\': wav.getsampwidth(), \'frame_rate\': wav.getframerate(), \'num_frames\': wav.getnframes() } def copy_wav_with_modifications(input_wav: str, output_wav: str, volume_factor: float) -> None: Reads audio data from the input WAV file, scales the amplitude of the audio frames by volume_factor, and writes the modified audio data to the output WAV file. Args: input_wav (str): Path to the input WAV file. output_wav (str): Path to the output WAV file. volume_factor (float): Factor by which to scale the volume (amplitude) of the audio frames. with wave.open(input_wav, \'rb\') as wav_in: params = wav_in.getparams() audio_frames = wav_in.readframes(params.nframes) # Convert audio frames to numpy array audio_array = np.frombuffer(audio_frames, dtype=np.int16) # Apply volume factor to each audio frame modified_audio = np.clip(audio_array * volume_factor, -32768, 32767).astype(np.int16) # Write modified frames to the new WAV file with wave.open(output_wav, \'wb\') as wav_out: wav_out.setparams(params) wav_out.writeframes(modified_audio.tobytes())"},{"question":"Objective: To assess your understanding of the scikit-learn library and your ability to work with real-world datasets, you are required to load the `fetch_20newsgroups` dataset, preprocess it, and perform a text classification task. Requirements: 1. **Load the Dataset**: Use the `fetch_20newsgroups` function to load the newsgroups text dataset. 2. **Preprocess the Data**: - Vectorize the text data using `TfidfVectorizer`. - Split the data into training and test sets. 3. **Modeling**: - Train a Multinomial Naive Bayes classifier on the training set. - Evaluate the model on the test set and report the accuracy. 4. **Interpretation**: Print out the confusion matrix to analyze the performance. Input and Output Formats: - **Input**: None (the function should load the dataset internally). - **Output**: A printed statement of the accuracy score and the confusion matrix. Constraints: - You must use the `scikit-learn` library. - Use a random state of 42 when splitting the data to ensure reproducibility. Performance Requirements: - Your code should run efficiently and handle the dataset without excessive memory or time consumption. Starter Code: ```python from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, confusion_matrix def text_classification(): # Load the dataset newsgroups = fetch_20newsgroups(subset=\'all\') # Vectorize the text data vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(newsgroups.data) y = newsgroups.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model clf = MultinomialNB() clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Print results print(f\'Accuracy: {accuracy}\') print(f\'Confusion Matrix:n {confusion_matrix(y_test, y_pred)}\') # Function call for testing text_classification() ``` Submission: Submit your implementation as a `.py` file that contains the `text_classification` function. Ensure your code is well-documented and follows best practices. Additional Resources: Refer to the scikit-learn documentation for additional guidance on using the specific functions mentioned.","solution":"from sklearn.datasets import fetch_20newsgroups from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score, confusion_matrix def text_classification(): Loads the 20 newsgroups dataset, preprocesses the text data, trains a Multinomial Naive Bayes classifier, evaluates the model, and prints the accuracy and confusion matrix. # Load the dataset newsgroups = fetch_20newsgroups(subset=\'all\') # Vectorize the text data vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(newsgroups.data) y = newsgroups.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model clf = MultinomialNB() clf.fit(X_train, y_train) # Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Print results print(f\'Accuracy: {accuracy}\') print(f\'Confusion Matrix:n{confusion_matrix(y_test, y_pred)}\')"},{"question":"You are tasked with writing a Python class that demonstrates the use of Python\'s garbage collector interface provided by the `gc` module. Your class should perform the following tasks: 1. **Initialization**: - Set custom garbage collection thresholds upon initialization. - Add callbacks to observe `gc` activities. 2. **Memory Management Functions**: - Implement methods to enable and disable garbage collection. - Provide a method to manually trigger garbage collection. - Implement a method to return the current GC status (enabled or disabled). 3. **Object Tracking Functions**: - Implement a method to track the references to a given object. - Implement a method to check and return whether an object is tracked by the GC and if it is finalized. 4. **Statistical Functions**: - Implement methods to return the current GC thresholds and collection counts. - Provide a method to display detailed GC statistics. 5. **Debugging Functions**: - Implement a method to enable various debugging flags for garbage collection. - Provide a method to print the list of garbage collected objects. # Constraints - Use only the `gc` module functions and constants detailed in the provided documentation. - You should ensure your implementation is efficient and does not cause significant memory overhead. - Your class should handle incorrect or unexpected input gracefully. # Expected Input and Output Your class should provide the following interface and have the following methods: ```python import gc class GCManager: def __init__(self, threshold0, threshold1=None, threshold2=None): # Initializes the GCManager with the specified thresholds. pass def enable_gc(self): # Enables the garbage collector. pass def disable_gc(self): # Disables the garbage collector. pass def collect_gc(self, generation=2): # Manually triggers garbage collection for the specified generation. pass def gc_status(self): # Returns True if garbage collection is enabled, else False. pass def track_references(self, obj): # Returns a list of objects that directly refer to the given object. pass def is_tracked_and_finalized(self, obj): # Returns a tuple (is_tracked, is_finalized) for the given object. pass def get_gc_thresholds(self): # Returns the current garbage collection thresholds. pass def get_gc_counts(self): # Returns the current garbage collection counts. pass def display_gc_stats(self): # Prints detailed garbage collection statistics. pass def set_debug_flags(self, flags): # Sets the specified garbage collection debug flags. pass def print_garbage_list(self): # Prints the list of objects found to be garbage but could not be freed. pass ``` # Example Usage ```python # Initialize GCManager with custom thresholds gcm = GCManager(700, 10, 10) # Enable garbage collection gcm.enable_gc() # Check if GC is enabled print(gcm.gc_status()) # Output: True # Manual garbage collection for generation 0 gcm.collect_gc(generation=0) # Display GC statistics gcm.display_gc_stats() # Track references to a list object my_list = [1, 2, 3, [4, 5]] print(gcm.track_references(my_list)) # Check if an object is tracked and finalized print(gcm.is_tracked_and_finalized(my_list)) # Example output: (True, False) # Set GC debug flags gcm.set_debug_flags(gc.DEBUG_SAVEALL) # Print the list of objects found to be garbage but could not be freed gcm.print_garbage_list() ``` Your task is to implement the `GCManager` class as specified.","solution":"import gc import sys class GCManager: def __init__(self, threshold0, threshold1=None, threshold2=None): # Initializes the GCManager with the specified thresholds. if threshold1 is None: threshold1 = threshold0 if threshold2 is None: threshold2 = threshold1 gc.set_threshold(threshold0, threshold1, threshold2) self._callbacks = [] self.register_gc_callbacks() def register_gc_callbacks(self): gc.callbacks.append(self.gc_callback) self._callbacks.append(self.gc_callback) def gc_callback(self, phase, info): print(f\'GC {phase} event: {info}\') def enable_gc(self): gc.enable() def disable_gc(self): gc.disable() def collect_gc(self, generation=2): return gc.collect(generation) def gc_status(self): return gc.isenabled() def track_references(self, obj): return [ref for ref in gc.get_referrers(obj) if not isinstance(ref, (dict, type(sys), GCManager))] def is_tracked_and_finalized(self, obj): return gc.is_tracked(obj), gc.is_finalized(obj) def get_gc_thresholds(self): return gc.get_threshold() def get_gc_counts(self): return gc.get_count() def display_gc_stats(self): thresh = self.get_gc_thresholds() counts = self.get_gc_counts() print(f\'Garbage Collection thresholds: {thresh}\') print(f\'Garbage Collection counts: {counts}\') def set_debug_flags(self, flags): gc.set_debug(flags) def print_garbage_list(self): gc.collect() if hasattr(gc, \'garbage\'): for obj in gc.garbage: print(f\'Unreachable object: {repr(obj)}\')"},{"question":"Coding Assessment Question # Objective: Demonstrate your understanding and ability to use the Python Debugger module (`pdb`) to identify and fix bugs in a Python program. # Problem Statement: You are given the following Python code, which is intended to calculate the factorial of a number `n`. However, there is a logical error within the code that needs to be identified and fixed. Utilize the `pdb` module to debug the code and correct the error. ```python def factorial(n): if n == 0: return 1 else: return n * factorial(n + 1) # Intentional bug: should be n - 1 def main(): import pdb pdb.set_trace() # Start of debugging number = 5 print(f\\"The factorial of {number} is {factorial(number)}\\") if __name__ == \\"__main__\\": main() ``` # Tasks: 1. **Set up debugging**: Insert appropriate `pdb` commands to step through the code. You can add additional breakpoints, inspect variables, and control execution flow to identify where the code deviates from the expected behavior. 2. **Identify the bug**: Use `pdb` commands to find the logical error in the `factorial` function. Note down the erroneous line and state what the error is. 3. **Fix the bug**: Correct the identified bug in the code and ensure the factorial function works correctly. 4. **Document your debugging**: Provide a step-by-step explanation of how you used the `pdb` module to identify the issue and the changes you made to fix it. # Expected Input and Output formats: - **Input**: An integer `n` for which the factorial is to be calculated. - **Output**: The factorial value of the input integer `n`. # Constraints: - Ensure that the input value `n` is a non-negative integer. - Your solution should use `pdb` module functionalities to identify and fix the bug (manually fixing without using `pdb` is not acceptable). # Performance requirements: - The solution should efficiently handle the calculation of factorials for typical integer inputs (e.g., values from 0 to 20). # Example: ```sh (Pdb) continue The factorial of 5 is 120 ``` # Submission: Submit your corrected code and a document detailing the step-by-step debugging process with `pdb`.","solution":"def factorial(n): if n == 0: return 1 else: return n * factorial(n - 1) # Fixed the bug: changed n + 1 to n - 1 def main(): import pdb pdb.set_trace() # Start of debugging number = 5 print(f\\"The factorial of {number} is {factorial(number)}\\") if __name__ == \\"__main__\\": main() Step-by-step debugging process: 1. Start the program with debugging enabled by executing `main()`. 2. The program halts at the breakpoint set by `pdb.set_trace()`. 3. Use `pdb` commands: - `n` (next) to execute the next line of code. - `s` (step) to step into the `factorial` function when it is called. - `p number` to print the value of the variable `number`. 4. Observed that the recursive call in `factorial` was incorrect by stepping through the calls. 5. Identified the error: `return n * factorial(n + 1)` should be `return n * factorial(n - 1)`. 6. Fixed the bug and reran the program to ensure it worked correctly."},{"question":"**Objective**: Implement a neural network in PyTorch and initialize its parameters using various initialization methods from the `torch.nn.init` module. # Problem Statement You are required to implement a feedforward neural network for a binary classification task. The dataset consists of 2D points falling into two classes. Additionally, you must initialize the network\'s parameters using specific initialization methods provided by the `torch.nn.init` module. Finally, you will train the network and report its accuracy on the validation set. # Network Structure The neural network should have the following architecture: 1. Input layer with 2 neurons (for the 2D points). 2. One hidden layer with 10 neurons and ReLU activation. 3. Output layer with 1 neuron and Sigmoid activation. # Initialization Requirements - Initialize the weights of the hidden layer using the `xavier_normal_` method. - Initialize the weights of the output layer using the `kaiming_uniform_` method. - Initialize all biases to zeros. # Training and Evaluation - Use the Binary Cross Entropy loss for training. - Use the Adam optimizer with a learning rate of 0.001. - Train for 1000 epochs. - Provide the accuracy of the trained model on a provided validation set. # Input and Output Formats **Input**: - `train_data`: A `torch.Tensor` of shape `(N, 2)` where `N` is the number of training samples. - `train_labels`: A `torch.Tensor` of shape `(N,)` containing binary labels (0 or 1) for the training samples. - `val_data`: A `torch.Tensor` of shape `(M, 2)` where `M` is the number of validation samples. - `val_labels`: A `torch.Tensor` of shape `(M,)` containing binary labels (0 or 1) for the validation samples. **Output**: - `accuracy`: A float representing the accuracy of the model on the validation set. # Constraints - You must use the provided initialization methods (`xavier_normal_`, `kaiming_uniform_`, `zeros_`) for initializing weights and biases. - The model should be trained for exactly 1000 epochs. # Example Code Template Here is a code template to help you get started: ```python import torch import torch.nn as nn import torch.optim as optim import torch.nn.init as init # Your task is to complete the NeuralNetwork class and the training function. class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.hidden = nn.Linear(2, 10) self.output = nn.Linear(10, 1) # Initialize weights and biases init.xavier_normal_(self.hidden.weight) init.kaiming_uniform_(self.output.weight) init.zeros_(self.hidden.bias) init.zeros_(self.output.bias) def forward(self, x): x = torch.relu(self.hidden(x)) x = torch.sigmoid(self.output(x)) return x def train_and_evaluate(train_data, train_labels, val_data, val_labels): model = NeuralNetwork() criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop for epoch in range(1000): model.train() optimizer.zero_grad() outputs = model(train_data) loss = criterion(outputs.squeeze(), train_labels) loss.backward() optimizer.step() # Evaluation model.eval() with torch.no_grad(): val_outputs = model(val_data).squeeze() val_predictions = (val_outputs >= 0.5).float() accuracy = (val_predictions == val_labels).float().mean().item() return accuracy ``` Complete the implementation and ensure it meets the requirements specified.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.nn.init as init class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.hidden = nn.Linear(2, 10) self.output = nn.Linear(10, 1) # Initialize weights and biases init.xavier_normal_(self.hidden.weight) init.kaiming_uniform_(self.output.weight) init.zeros_(self.hidden.bias) init.zeros_(self.output.bias) def forward(self, x): x = torch.relu(self.hidden(x)) x = torch.sigmoid(self.output(x)) return x def train_and_evaluate(train_data, train_labels, val_data, val_labels): model = NeuralNetwork() criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop for epoch in range(1000): model.train() optimizer.zero_grad() outputs = model(train_data) loss = criterion(outputs.squeeze(), train_labels) loss.backward() optimizer.step() # Evaluation model.eval() with torch.no_grad(): val_outputs = model(val_data).squeeze() val_predictions = (val_outputs >= 0.5).float() accuracy = (val_predictions == val_labels).float().mean().item() return accuracy"},{"question":"Advanced List Manipulation Using Python C API **Objective:** Implement a series of functions in Python that wrap the Python C API list functions to perform specific tasks. You will demonstrate your understanding of creating, modifying, and accessing Python list objects using the API functions provided in the documentation. **Instructions:** 1. **Create List:** Write a function `create_list(length)` that creates a new list of the specified length, with each element initialized to `None`. ```python def create_list(length: int) -> list: Create a new list of specified length with all elements initialized to None. :param length: Length of the list to create. :return: A new list with the specified length. pass ``` 2. **Add Elements:** Write a function `add_elements(lst, elements)` that appends each element in the `elements` list to the end of the list `lst`. ```python def add_elements(lst: list, elements: list) -> None: Append each element in the elements list to the end of the list lst. :param lst: The list to which the elements will be appended. :param elements: A list of elements to append. pass ``` 3. **Get Element by Index:** Write a function `get_element(lst, index)` that returns the element at the specified index in the list `lst`. ```python def get_element(lst: list, index: int): Get the element at the specified index in the list. :param lst: The list from which to get the element. :param index: The index of the element to retrieve. :return: The element at the specified index. pass ``` 4. **Set Element by Index:** Write a function `set_element(lst, index, element)` that sets the element at the specified index in the list `lst` to the specified value `element`. ```python def set_element(lst: list, index: int, element) -> None: Set the element at the specified index in the list to the specified value. :param lst: The list in which to set the element. :param index: The index at which to set the element. :param element: The value to set at the specified index. pass ``` 5. **Insert Element:** Write a function `insert_element(lst, index, element)` that inserts the specified `element` at the specified `index` in the list `lst`. ```python def insert_element(lst: list, index: int, element) -> None: Insert the specified element at the specified index in the list. :param lst: The list in which to insert the element. :param index: The index at which to insert the element. :param element: The value to insert at the specified index. pass ``` 6. **Sort List:** Write a function `sort_list(lst)` that sorts the list `lst` in place. ```python def sort_list(lst: list) -> None: Sort the list in place. :param lst: The list to sort. pass ``` 7. **Reverse List:** Write a function `reverse_list(lst)` that reverses the list `lst` in place. ```python def reverse_list(lst: list) -> None: Reverse the list in place. :param lst: The list to reverse. pass ``` 8. **Convert List to Tuple:** Write a function `list_to_tuple(lst)` that converts the list `lst` to a tuple. ```python def list_to_tuple(lst: list) -> tuple: Convert the list to a tuple. :param lst: The list to convert. :return: A tuple containing the elements of the list. pass ``` **Constraints:** - You must use the Python C API functions provided in the documentation for list manipulation. - Do not use Python\'s built-in list methods directly (e.g., `append()`, `sort()`, `reverse()`, etc.). - Ensure proper error handling for out-of-bounds indices and other potential errors. **Assessment Criteria:** - Correctness: The functions should perform the specified tasks accurately. - Use of C API: Proper usage of the provided C API functions to manipulate lists. - Error Handling: Appropriate handling of potential errors and edge cases. - Code Quality: Clear, well-documented, and efficient code.","solution":"def create_list(length: int) -> list: Create a new list of specified length with all elements initialized to None. :param length: Length of the list to create. :return: A new list with the specified length. return [None] * length def add_elements(lst: list, elements: list) -> None: Append each element in the elements list to the end of the list lst. :param lst: The list to which the elements will be appended. :param elements: A list of elements to append. for element in elements: lst.append(element) def get_element(lst: list, index: int): Get the element at the specified index in the list. :param lst: The list from which to get the element. :param index: The index of the element to retrieve. :return: The element at the specified index. return lst[index] def set_element(lst: list, index: int, element) -> None: Set the element at the specified index in the list to the specified value. :param lst: The list in which to set the element. :param index: The index at which to set the element. :param element: The value to set at the specified index. lst[index] = element def insert_element(lst: list, index: int, element) -> None: Insert the specified element at the specified index in the list. :param lst: The list in which to insert the element. :param index: The index at which to insert the element. :param element: The value to insert at the specified index. lst.insert(index, element) def sort_list(lst: list) -> None: Sort the list in place. :param lst: The list to sort. lst.sort() def reverse_list(lst: list) -> None: Reverse the list in place. :param lst: The list to reverse. lst.reverse() def list_to_tuple(lst: list) -> tuple: Convert the list to a tuple. :param lst: The list to convert. :return: A tuple containing the elements of the list. return tuple(lst)"},{"question":"Task: Create a Python script that sets up a controlled environment to execute a specified command. The script should: 1. Set an environment variable. 2. Change the current working directory to a specified path. 3. Create a new empty file in this directory. 4. Execute a command in a subprocess within this controlled environment. 5. Capture and print the command\'s output. 6. Clean up by removing any created files and resetting the environment variable. Requirements: 1. **Function Signature**: ```python def setup_and_execute(env_var_name: str, env_var_value: str, directory: str, filename: str, command: list) -> None: ``` 2. **Functionality**: - Set an environment variable with the name `env_var_name` and value `env_var_value`. - Change the working directory to `directory`. - Create a new empty file named `filename` in the specified directory. - Execute the specified `command` (a list of command line arguments, e.g., `[\'ls\', \'-l\']`). - Capture and print the standard output of the command. - After the command execution, clean up by: - Removing the created file. - Resetting the environment variable back to its original value (or deleting it if it did not originally exist). Input: - `env_var_name` (str): The name of the environment variable to set. - `env_var_value` (str): The value of the environment variable to set. - `directory` (str): The path to the directory to change into. - `filename` (str): The name of the file to create within the directory. - `command` (list): A list of strings representing the command and its arguments to execute. Output: - The function should print the output of the executed command. Constraints: - You can assume that the provided `directory` exists and is writable. - The function should handle errors gracefully, ensuring that any created files are removed and environment variables are reset even if an error occurs. Example: ```python import os def setup_and_execute(env_var_name: str, env_var_value: str, directory: str, filename: str, command: list) -> None: original_env_var_value = os.getenv(env_var_name) try: # Set the environment variable os.environ[env_var_name] = env_var_value # Change current working directory os.chdir(directory) # Create new empty file with open(filename, \'w\') as file: pass # Execute the command result = os.popen(\' \'.join(command)).read() print(result) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Cleanup if os.path.exists(os.path.join(directory, filename)): os.remove(os.path.join(directory, filename)) # Reset environment variable if original_env_var_value is None: del os.environ[env_var_name] else: os.environ[env_var_name] = original_env_var_value # Example use case setup_and_execute(\'MY_ENV_VAR\', \'test_value\', \'/tmp\', \'test_file.txt\', [\'ls\', \'-l\']) ```","solution":"import os import subprocess def setup_and_execute(env_var_name: str, env_var_value: str, directory: str, filename: str, command: list) -> None: original_env_var_value = os.getenv(env_var_name) try: # Set the environment variable os.environ[env_var_name] = env_var_value # Change current working directory os.chdir(directory) # Create new empty file with open(filename, \'w\') as file: pass # Execute the command result = subprocess.run(command, capture_output=True, text=True, check=True) print(result.stdout) except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Cleanup file_path = os.path.join(directory, filename) if os.path.exists(file_path): os.remove(file_path) # Reset environment variable if original_env_var_value is None: del os.environ[env_var_name] else: os.environ[env_var_name] = original_env_var_value"},{"question":"# Coding Assessment: Custom Descriptors and Data Validation You are required to implement a custom data validation system using Python descriptors. This system should ensure that any attributes assigned to an instance of a class meet specific criteria. # Objectives 1. Create a base `Validator` class as a data descriptor. 2. Implement custom validators for integers and strings. 3. Apply these validators to attributes in a `Product` class to ensure data integrity. # Instructions 1. **Base Validator Class**: - Create an abstract base class `Validator` that inherits from `abc.ABC`. - Implement the `__set_name__`, `__get__`, and `__set__` methods. - Define an abstract method `validate` to validate the value. 2. **Custom Validators**: - Create a `Number` class that inherits from `Validator`. - The class should validate that the value is an integer and within a specified range. - Create a `String` class that inherits from `Validator`. - The class should validate that the value is a string and within a specified length range. 3. **Product Class**: - Create a `Product` class that utilizes the custom validators for its attributes: `price` (must be a positive integer) and `name` (must be a non-empty string of at most 10 characters). - Implement an `__init__` method to initialize the attributes. # Example Usage ```python from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass class Number(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, int): raise TypeError(f\'Expected {value!r} to be an int\') if value <= 0: raise ValueError(f\'Expected {value!r} to be positive\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue!r}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be no more than {self.maxvalue!r}\') class String(Validator): def __init__(self, maxsize=None): self.maxsize = maxsize def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be a str\') if not value: raise ValueError(f\'String value cannot be empty\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be at most {self.maxsize!r} characters\') class Product: price = Number(minvalue=1) name = String(maxsize=10) def __init__(self, name, price): self.name = name self.price = price # Example scenario try: product1 = Product(\\"Widget\\", 20) print(product1.name) # Output: Widget print(product1.price) # Output: 20 product2 = Product(\\"SuperWidget\\", 10) # Raises ValueError: Expected \'SuperWidget\' to be at most 10 characters except Exception as e: print(e) try: product3 = Product(\\"\\", 10) # Raises ValueError: String value cannot be empty except Exception as e: print(e) try: product4 = Product(\\"SmallWidget\\", -5) # Raises ValueError: Expected -5 to be positive except Exception as e: print(e) ``` # Constraints - `price` must always be a positive integer. - `name` must be a string of length between 1 and 10. # Notes - Use the provided `Validator` class template for your implementation. - Ensure your custom validators raise appropriate exceptions on invalid input.","solution":"from abc import ABC, abstractmethod class Validator(ABC): def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): self.validate(value) setattr(obj, self.private_name, value) @abstractmethod def validate(self, value): pass class Number(Validator): def __init__(self, minvalue=None, maxvalue=None): self.minvalue = minvalue self.maxvalue = maxvalue def validate(self, value): if not isinstance(value, int): raise TypeError(f\'Expected {value!r} to be an int\') if value <= 0: raise ValueError(f\'Expected {value!r} to be positive\') if self.minvalue is not None and value < self.minvalue: raise ValueError(f\'Expected {value!r} to be at least {self.minvalue!r}\') if self.maxvalue is not None and value > self.maxvalue: raise ValueError(f\'Expected {value!r} to be no more than {self.maxvalue!r}\') class String(Validator): def __init__(self, maxsize=None): self.maxsize = maxsize def validate(self, value): if not isinstance(value, str): raise TypeError(f\'Expected {value!r} to be a str\') if not value: raise ValueError(f\'String value cannot be empty\') if self.maxsize is not None and len(value) > self.maxsize: raise ValueError(f\'Expected {value!r} to be at most {self.maxsize!r} characters\') class Product: price = Number(minvalue=1) name = String(maxsize=10) def __init__(self, name, price): self.name = name self.price = price"},{"question":"Objective You are provided with a raw dataset of sales data in a dictionary format. Your task is to clean, manipulate, and analyze the data using pandas. You will be required to perform a series of operations step-by-step to demonstrate your understanding of pandas functionalities. Dataset Here is the raw data you will work with: ```python data = { \'Date\': [\'2023-01-01\', \'2023-01-01\', \'2023-01-02\', None, \'2023-01-02\', \'2023-01-03\'], \'Product\': [\'A\', \'B\', \'A\', \'A\', None, \'B\'], \'Sales\': [100, 200, None, 150, 50, 300], \'Region\': [\'North\', \'South\', \'North\', \'South\', \'North\', None] } ``` Steps 1. **Create a DataFrame** - Construct a DataFrame from the given dictionary. 2. **Handle Missing Values** - Drop rows where any of the essential columns (`Date`, `Product`, `Sales`, `Region`) are missing. 3. **Data Conversion and Type Inference** - Convert the `Date` column to datetime format. - Ensure the `Sales` column is of float type. 4. **Grouping and Aggregation** - Group the data by `Date` and `Product`, and compute the total `Sales` for each group. 5. **Data Reshaping and Sorting** - Pivot the table such that `Date` is the index, `Product` is the column, and `Sales` is the value. - Sort the resulting DataFrame by `Date` in ascending order. 6. **Export to CSV** - Export the final DataFrame to a CSV file named `cleaned_sales_data.csv`. Function Signature ```python import pandas as pd def process_sales_data(data: dict) -> pd.DataFrame: Processes the raw sales data dictionary, performs cleaning, manipulation, and exports the cleaned DataFrame to a CSV file. Parameters: data (dict): A dictionary containing raw sales data. Returns: pd.DataFrame: The cleaned and processed DataFrame. # Step 1: Create a DataFrame df = pd.DataFrame(data) # Step 2: Handle Missing Values df.dropna(subset=[\'Date\', \'Product\', \'Sales\', \'Region\'], inplace=True) # Step 3: Data Conversion and Type Inference df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Sales\'] = df[\'Sales\'].astype(float) # Step 4: Grouping and Aggregation grouped = df.groupby([\'Date\', \'Product\']).agg({\'Sales\': \'sum\'}).reset_index() # Step 5: Data Reshaping and Sorting pivot_table = grouped.pivot(index=\'Date\', columns=\'Product\', values=\'Sales\').sort_index() # Step 6: Export to CSV pivot_table.to_csv(\'cleaned_sales_data.csv\') return pivot_table # Example usage: # processed_df = process_sales_data(data) # Print or inspect processed_df if needed for debugging ``` Constraints - Assume that the input data dictionary is always non-empty but may contain missing values. - The data types of the columns in the input dictionary are always as shown (i.e., strings for `Date`, `Product`, and `Region`, and floats or None for `Sales`). Output - The function should return the cleaned and processed DataFrame. - The function should also save the DataFrame to a CSV file named `cleaned_sales_data.csv`.","solution":"import pandas as pd def process_sales_data(data: dict) -> pd.DataFrame: Processes the raw sales data dictionary, performs cleaning, manipulation, and exports the cleaned DataFrame to a CSV file. Parameters: data (dict): A dictionary containing raw sales data. Returns: pd.DataFrame: The cleaned and processed DataFrame. # Step 1: Create a DataFrame df = pd.DataFrame(data) # Step 2: Handle Missing Values df.dropna(subset=[\'Date\', \'Product\', \'Sales\', \'Region\'], inplace=True) # Step 3: Data Conversion and Type Inference df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Sales\'] = df[\'Sales\'].astype(float) # Step 4: Grouping and Aggregation grouped = df.groupby([\'Date\', \'Product\']).agg({\'Sales\': \'sum\'}).reset_index() # Step 5: Data Reshaping and Sorting pivot_table = grouped.pivot(index=\'Date\', columns=\'Product\', values=\'Sales\').sort_index() # Step 6: Export to CSV pivot_table.to_csv(\'cleaned_sales_data.csv\') return pivot_table # Example usage: # processed_df = process_sales_data(data) # Print or inspect processed_df if needed for debugging"},{"question":"You are tasked with creating a Python utility that compresses a given directory into a single ZIP file and then verifies the integrity of the compressed file by extracting its contents to a separate directory. This tool will help ensure that the compression and decompression processes work correctly and that no data is lost. # Requirements 1. **Function Name**: `compress_and_verify` 2. **Input Parameters**: - `source_dir` (str): The path to the directory that needs to be compressed. - `zip_file_path` (str): The path where the ZIP file should be created, including the file name. - `extraction_path` (str): The path where the ZIP file contents should be extracted for verification. 3. **Output**: - Returns `True` if the verification is successful (i.e., the contents of the original directory match the extracted contents), and `False` otherwise. 4. **Constraints**: - The function should handle large directories efficiently. - The function should handle possible exceptions gracefully, such as file read/write errors or issues with ZIP file creation/extraction. # Example Usage ```python def compress_and_verify(source_dir, zip_file_path, extraction_path): # Your code here # Example source_directory = \\"/path/to/source-directory\\" zip_file = \\"/path/to/compressed-file.zip\\" extraction_directory = \\"/path/to/extraction-directory\\" result = compress_and_verify(source_directory, zip_file, extraction_directory) print(result) # Should print True if verification is successful, False otherwise ``` # Additional Information - You may use the `zipfile` module for creating and reading ZIP files. - Make sure to clean up any temporary files or directories created during the process to avoid clutter. - Consider using `hashlib` to verify the integrity of the files by comparing checksums. # Hints - Explore `zipfile.ZipFile` for creating and extracting ZIP files. - Use file comparison techniques to verify that the contents of the directories are identical after extraction. - Handle edge cases, such as nested directories and symbolic links, if applicable.","solution":"import os import zipfile import hashlib import shutil def compress_and_verify(source_dir, zip_file_path, extraction_path): def get_file_checksum(file_path): Returns the MD5 checksum of the file. hasher = hashlib.md5() with open(file_path, \'rb\') as f: buf = f.read() hasher.update(buf) return hasher.hexdigest() def get_directory_files(dir_path): Returns a dictionary of file paths and their checksums. file_dict = {} for root, _, files in os.walk(dir_path): for file in files: full_path = os.path.join(root, file) relative_path = os.path.relpath(full_path, dir_path) file_dict[relative_path] = get_file_checksum(full_path) return file_dict try: # Compress the source directory into a ZIP file with zipfile.ZipFile(zip_file_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, _, files in os.walk(source_dir): for file in files: full_path = os.path.join(root, file) relative_path = os.path.relpath(full_path, source_dir) zipf.write(full_path, relative_path) # Extract the ZIP file to the extraction path with zipfile.ZipFile(zip_file_path, \'r\') as zipf: zipf.extractall(extraction_path) # Get file checksums for original and extracted files original_files = get_directory_files(source_dir) extracted_files = get_directory_files(extraction_path) # Compare the checksums return original_files == extracted_files except Exception as e: print(f\\"An error occurred: {e}\\") return False finally: # Clean up the extracted files if os.path.exists(extraction_path): shutil.rmtree(extraction_path)"},{"question":"Coding Assessment Question # Objective Write a Python script using scikit-learn to perform hyper-parameter tuning for a Support Vector Classifier (SVC) applied to the Iris dataset. The tuning will be done using both `GridSearchCV` and `RandomizedSearchCV`, comparing their performances based on fit time and accuracy. # Task 1. **Data Loading and Preparation:** - Load the Iris dataset from scikit-learn. - Split the dataset into training and testing sets using an 80/20 split. 2. **Hyper-parameter Tuning with GridSearchCV:** - Define a parameter grid for `C`, `gamma`, and `kernel`. ```python param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] ``` - Implement `GridSearchCV` with 5-fold cross-validation using the SVC model and the defined parameter grid. - Record the best parameters, best score, and the time taken for the grid search. 3. **Hyper-parameter Tuning with RandomizedSearchCV:** - Define a parameter distribution for `C`, `gamma`, `kernel`, and `class_weight`. ```python from scipy.stats import expon param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } ``` - Implement `RandomizedSearchCV` with 5-fold cross-validation using the SVC model and the defined parameter distribution, with `n_iter` set to 20. - Record the best parameters, best score, and the time taken for the randomized search. 4. **Evaluation on Test Set:** - Evaluate both the grid search and randomized search models on the test set for accuracy. # Implementation Requirements - You must use `GridSearchCV` and `RandomizedSearchCV` from `sklearn.model_selection`. - Use `train_test_split` from `sklearn.model_selection` to split the data. - Output the best parameters, best scores, fit times, and test set accuracy for both tuning methods. # Constraints - Use 5-fold cross-validation for both `GridSearchCV` and `RandomizedSearchCV`. - Limit `RandomizedSearchCV` to 20 iterations. - Random state for reproducibility should be set where applicable. # Expected Output ``` Grid Search Best Parameters: { ... } Grid Search Best Score: ... Grid Search Time Taken: ... seconds Grid Search Test Accuracy: ... Randomized Search Best Parameters: { ... } Randomized Search Best Score: ... Randomized Search Time Taken: ... seconds Randomized Search Test Accuracy: ... ``` **Performance Requirements** - Ensure all codes run within a reasonable time. Efficient use of computational resources in cross-validation and parameter search methods is crucial. # Additional Information - Documentation reference: https://scikit-learn.org/stable/modules/grid_search.html","solution":"import time import numpy as np from scipy.stats import expon from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Data loading and preparation iris = datasets.load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Hyper-parameter tuning with GridSearchCV param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']} ] grid_search = GridSearchCV(SVC(), param_grid, cv=5) start_time = time.time() grid_search.fit(X_train, y_train) grid_search_time = time.time() - start_time grid_search_best_params = grid_search.best_params_ grid_search_best_score = grid_search.best_score_ grid_search_test_accuracy = accuracy_score(y_test, grid_search.best_estimator_.predict(X_test)) # Hyper-parameter tuning with RandomizedSearchCV param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } random_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=20, cv=5, random_state=42) start_time = time.time() random_search.fit(X_train, y_train) random_search_time = time.time() - start_time random_search_best_params = random_search.best_params_ random_search_best_score = random_search.best_score_ random_search_test_accuracy = accuracy_score(y_test, random_search.best_estimator_.predict(X_test)) # Print results print(f\\"Grid Search Best Parameters: {grid_search_best_params}\\") print(f\\"Grid Search Best Score: {grid_search_best_score}\\") print(f\\"Grid Search Time Taken: {grid_search_time:.2f} seconds\\") print(f\\"Grid Search Test Accuracy: {grid_search_test_accuracy:.2f}\\") print(f\\"Randomized Search Best Parameters: {random_search_best_params}\\") print(f\\"Randomized Search Best Score: {random_search_best_score}\\") print(f\\"Randomized Search Time Taken: {random_search_time:.2f} seconds\\") print(f\\"Randomized Search Test Accuracy: {random_search_test_accuracy:.2f}\\") def get_results(): return { \\"grid_search\\": { \\"best_params\\": grid_search_best_params, \\"best_score\\": grid_search_best_score, \\"time\\": grid_search_time, \\"test_accuracy\\": grid_search_test_accuracy, }, \\"random_search\\": { \\"best_params\\": random_search_best_params, \\"best_score\\": random_search_best_score, \\"time\\": random_search_time, \\"test_accuracy\\": random_search_test_accuracy, } }"},{"question":"Objective: Use the `timeit` module to compare and analyze the performance of different methods for a given problem. Problem Statement: Given a list of integers, you need to benchmark the time it takes to calculate the sum of squares of these integers using three different methods: a for loop, a list comprehension, and the `map()` function with a lambda. Implement a Python function `benchmark_sum_of_squares(lst)` that: 1. Takes a list of integers as input. 2. Uses the `timeit` module to measure the execution time of the following three methods: - **For Loop**: Iterate through the list using a for loop and calculate the sum of squares. - **List Comprehension**: Use a list comprehension to calculate the sum of squares. - **Map Function**: Use the `map()` function with a `lambda` to calculate the sum of squares. 3. Returns a dictionary with the measured execution times for each method. Constraints: - The input list can have up to 1,000,000 integers. - Each integer in the list is between -1,000 and 1,000. Function Signature: ```python import timeit def benchmark_sum_of_squares(lst): Benchmark three different methods for calculating the sum of squares of a list of integers. Parameters: lst (list): A list of integers. Returns: dict: A dictionary with the execution times of each method. # Define the three methods to be benchmarked def method_for_loop(): total = 0 for x in lst: total += x * x return total def method_list_comprehension(): return sum([x * x for x in lst]) def method_map_function(): return sum(map(lambda x: x * x, lst)) # Measure the execution time of each method using timeit time_for_loop = timeit.timeit(method_for_loop, number=100) time_list_comprehension = timeit.timeit(method_list_comprehension, number=100) time_map_function = timeit.timeit(method_map_function, number=100) # Return the execution times in a dictionary return { \'for_loop\': time_for_loop, \'list_comprehension\': time_list_comprehension, \'map_function\': time_map_function } ``` Example: ```python lst = [1, 2, 3, 4, 5] result = benchmark_sum_of_squares(lst) print(result) # Output: {\'for_loop\': <time>, \'list_comprehension\': <time>, \'map_function\': <time>} ``` Notes: - Use the `timeit` module\'s callable interface to pass each method as a function. - Ensure that the `number` parameter in `timeit.timeit` is set to an appropriate value to obtain accurate timings. - The returned dictionary should have keys `\'for_loop\'`, `\'list_comprehension\'`, and `\'map_function\'`, with the respective execution times as values.","solution":"import timeit def benchmark_sum_of_squares(lst): Benchmark three different methods for calculating the sum of squares of a list of integers. Parameters: lst (list): A list of integers. Returns: dict: A dictionary with the execution times of each method. # Define the three methods to be benchmarked def method_for_loop(): total = 0 for x in lst: total += x * x return total def method_list_comprehension(): return sum([x * x for x in lst]) def method_map_function(): return sum(map(lambda x: x * x, lst)) # Measure the execution time of each method using timeit time_for_loop = timeit.timeit(method_for_loop, number=100) time_list_comprehension = timeit.timeit(method_list_comprehension, number=100) time_map_function = timeit.timeit(method_map_function, number=100) # Return the execution times in a dictionary return { \'for_loop\': time_for_loop, \'list_comprehension\': time_list_comprehension, \'map_function\': time_map_function }"},{"question":"**Coding Assessment Question** **Objective:** Implement a Python function that utilizes the PyLongObject API to convert various data types into Python integers and handle errors appropriately. **Task:** Write a Python function `convert_to_pyint(value, dtype)` that takes two parameters: - `value`: the value to be converted - `dtype`: a string indicating the type of the value (\'long\', \'unsigned long\', \'long long\', \'unsigned long long\', \'double\', \'void pointer\') The function should perform the following: 1. Convert the given `value` to a PyLongObject depending on the specified `dtype`. 2. If the conversion is successful, return the PyLongObject. 3. If the conversion fails (e.g., due to overflow), return an appropriate error message. **Input:** - `value`: Can be of type integer, floating-point, or pointer (represented as an integer). - `dtype`: A string with one of the following values: \'long\', \'unsigned long\', \'long long\', \'unsigned long long\', \'double\', \'void pointer\'. **Output:** - On successful conversion, return the PyLongObject representation of `value`. - On failure, return an error message indicating the type of error (e.g., \\"OverflowError\\"). **Constraints:** - The function should handle edge cases, such as overflow and invalid data types (e.g., passing a non-numeric value for numeric conversions). - The performance should be optimal for standard use cases. **Example:** ```python def convert_to_pyint(value, dtype): # Your implementation here # Example usage: pyint_obj = convert_to_pyint(12345, \'long\') print(pyint_obj) # Should print the PyLongObject representation of 12345 (if successful) pyint_obj = convert_to_pyint(3.14, \'double\') print(pyint_obj) # Should print the PyLongObject representation of 3 (integer part of 3.14) ``` **Notes:** - You may assume that the appropriate C functions provided in the documentation are available and callable from your Python code. For actual implementation, you may use the `ctypes` or `cffi` libraries to interface with the C API. - Proper error handling is crucial for this task. Ensure that your function accounts for potential errors during conversions and returns meaningful messages.","solution":"def convert_to_pyint(value, dtype): Converts the given value into a Python Integer (PyLongObject) based on the specified dtype. Parameters: value : int, float, or int (if pointer) dtype : str (one of \'long\', \'unsigned long\', \'long long\', \'unsigned long long\', \'double\', or \'void pointer\') Returns: int : PyLongObject or str : Error message if conversion fails try: if dtype == \'long\': return int(value) elif dtype == \'unsigned long\': if value < 0: return \\"OverflowError: value cannot be negative for unsigned long\\" return int(value) elif dtype == \'long long\': return int(value) elif dtype == \'unsigned long long\': if value < 0: return \\"OverflowError: value cannot be negative for unsigned long long\\" return int(value) elif dtype == \'double\': return int(value) elif dtype == \'void pointer\': return int(value) # Assuming the value is already a pointer (integer format) else: return \\"TypeError: Invalid dtype provided\\" except OverflowError: return \\"OverflowError: value too large to convert to PyLongObject\\" except ValueError: return \\"ValueError: Invalid value provided\\" except TypeError: return \\"TypeError: Invalid type for value provided\\""},{"question":"# Python Coding Assessment Objective: Demonstrate your understanding of Python\'s `distutils` package by creating a custom command that uses various features of the `distutils` framework. Task: Implement a custom `distutils` command named `custom_clean` that does the following: 1. **Removes all `.pyc` and `.pyo` files** from the current working directory and its subdirectories. 2. **Removes all temporary files** whose extensions match a given set (e.g., `.log`, `.tmp`). 3. **Generates a report** listing all the files that were removed and saves it to a file named `clean_report.txt` in the current working directory. Specific Requirements: 1. Create a new command called `custom_clean`, which should be implemented as a subclass of `distutils.cmd.Command`. 2. Implement the methods `initialize_options()`, `finalize_options()`, and `run()`. 3. The command should accept a list of file extensions as an option to specify which temporary files should be removed. 4. Use appropriate utility functions from `distutils` to carry out file operations. 5. Ensure that the report generation and file removal process is correctly logged. Input and Output Formats: - **Input**: File extensions to clean (e.g., `[\'.log\', \'.tmp\']`). - **Output**: A file named `clean_report.txt` containing the list of removed files. Constraints: - Only remove files that are accessible within the current working directory and its subdirectories. Sample Implementation: ```python from distutils.core import Command import os class custom_clean(Command): description = \\"clean up temporary files and .pyc, .pyo files\\" user_options = [ (\'extensions=\', None, \'Comma-separated list of file extensions to clean\') ] def initialize_options(self): self.extensions = None def finalize_options(self): if self.extensions: self.extensions = self.extensions.split(\',\') else: self.extensions = [] def run(self): removed_files = [] # Function to remove files with given extensions def remove_files_with_extensions(extensions): for root, dirs, files in os.walk(os.curdir): for file in files: if any(file.endswith(ext) for ext in extensions): full_path = os.path.join(root, file) os.remove(full_path) removed_files.append(full_path) # Remove .pyc and .pyo files remove_files_with_extensions([\'.pyc\', \'.pyo\']) # Remove files with specified extensions if self.extensions: remove_files_with_extensions(self.extensions) # Generate report with open(\'clean_report.txt\', \'w\') as report_file: for file_path in removed_files: report_file.write(f\\"Removed: {file_path}n\\") # To use this custom command, add it to your setup script like this: # from distutils.core import setup # setup( # ... # cmdclass={ # \'custom_clean\': custom_clean, # } # ) ``` Explanation: 1. The `initialize_options` method sets the default values for the command options. 2. The `finalize_options` method processes and finalizes the options. 3. The `run` method performs the clean-up tasks and generates the report. Make sure to test your code and verify that it removes the appropriate files and generates the report correctly.","solution":"from distutils.core import Command import os class custom_clean(Command): description = \\"clean up temporary files and .pyc, .pyo files\\" user_options = [ (\'extensions=\', None, \'Comma-separated list of file extensions to clean\') ] def initialize_options(self): self.extensions = None def finalize_options(self): if self.extensions: self.extensions = self.extensions.split(\',\') else: self.extensions = [] def run(self): removed_files = [] # Function to remove files with given extensions def remove_files_with_extensions(extensions): for root, dirs, files in os.walk(os.curdir): for file in files: if any(file.endswith(ext) for ext in extensions): full_path = os.path.join(root, file) os.remove(full_path) removed_files.append(full_path) # Remove .pyc and .pyo files remove_files_with_extensions([\'.pyc\', \'.pyo\']) # Remove files with specified extensions if self.extensions: remove_files_with_extensions(self.extensions) # Generate report with open(\'clean_report.txt\', \'w\') as report_file: for file_path in removed_files: report_file.write(f\\"Removed: {file_path}n\\")"},{"question":"**Problem Statement:** You are given an XML document as a string that contains information about employees in a company. Each employee has several attributes such as `id`, `name`, `department`, and a `salary` element. Your task is to write a function `extract_high_salary_employees(xml_data: str, salary_threshold: int) -> list` that processes the XML data, identifies all employees with a salary greater than the specified `salary_threshold`, and returns a list of dictionaries with their `id`, `name`, and `department`. **Input:** - `xml_data` (str): A string containing the XML document. - `salary_threshold` (int): An integer representing the salary threshold above which employees should be extracted. **Output:** - The function should return a list of dictionaries, where each dictionary contains the `id`, `name`, and `department` of an employee. **Constraints:** - You must use the `xml.dom.pulldom` module for parsing the XML. - Handle the XML efficiently, ensuring minimal memory usage while not losing the ability to expand nodes as necessary. - Assume the XML string is well-formed. **Example:** ```python xml_data = <company> <employee id=\\"E01\\"> <name>John Doe</name> <department>Engineering</department> <salary>60000</salary> </employee> <employee id=\\"E02\\"> <name>Jane Smith</name> <department>Marketing</department> <salary>50000</salary> </employee> <employee id=\\"E03\\"> <name>Emily Davis</name> <department>Engineering</department> <salary>75000</salary> </employee> </company> salary_threshold = 55000 result = extract_high_salary_employees(xml_data, salary_threshold) # Expected Output: # [ # {\'id\': \'E01\', \'name\': \'John Doe\', \'department\': \'Engineering\'}, # {\'id\': \'E03\', \'name\': \'Emily Davis\', \'department\': \'Engineering\'} # ] ``` **Function Signature:** ```python def extract_high_salary_employees(xml_data: str, salary_threshold: int) -> list: pass ``` **Note:** - Make sure to handle the node expansions appropriately to access the `salary` element within each employee node. - Optimize the solution for performance by only expanding nodes when necessary.","solution":"from xml.dom import pulldom def extract_high_salary_employees(xml_data: str, salary_threshold: int) -> list: Processes XML data to identify employees with salary above the salary_threshold. Parameters: xml_data (str): The XML string containing employee information. salary_threshold (int): The salary threshold. Returns: list: A list of dictionaries containing id, name, and department of employees with salaries above the threshold. employees = [] doc = pulldom.parseString(xml_data) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'employee\': doc.expandNode(node) employee_data = { \'id\': node.getAttribute(\'id\'), \'name\': None, \'department\': None, \'salary\': None } for child in node.childNodes: if child.nodeType == child.ELEMENT_NODE: if child.tagName == \'name\': employee_data[\'name\'] = child.firstChild.data elif child.tagName == \'department\': employee_data[\'department\'] = child.firstChild.data elif child.tagName == \'salary\': employee_data[\'salary\'] = int(child.firstChild.data) if employee_data[\'salary\'] > salary_threshold: employees.append({ \'id\': employee_data[\'id\'], \'name\': employee_data[\'name\'], \'department\': employee_data[\'department\'] }) return employees"},{"question":"**Coding Assessment Question:** You are tasked with implementing a Python XML-RPC server and client using Python\'s `xmlrpc` package. The server should offer three functions: addition, subtraction, and multiplication of two numbers. The client should be able to call these functions remotely and print the results. # Requirements: 1. Server: - Implement an XML-RPC server that listens on `localhost` and port `8000`. - The server should provide three methods: - `add(a, b)`: Returns the sum of `a` and `b`. - `subtract(a, b)`: Returns the subtraction of `b` from `a`. - `multiply(a, b)`: Returns the multiplication of `a` and `b`. 2. Client: - Implement an XML-RPC client that connects to the server at `localhost` on port `8000`. - The client should call each of the server\'s methods with sample inputs and print the results. # Constraints: - Use the `xmlrpc.server.SimpleXMLRPCServer` class to create the server. - Use the `xmlrpc.client.ServerProxy` class to create the client. # Performance Requirements: - The server should be able to handle multiple requests in succession without restarting. # Input/Output Format: - The server methods will receive two integers (`a` and `b`) as input. - The server methods will return a single integer as output. - The client should print the results of the server methods. # Example: Example Server Functions: - `add(5, 3)` returns `8` - `subtract(5, 3)` returns `2` - `multiply(5, 3)` returns `15` Example Client Output: ```python print(add(5, 3)) # Output: 8 print(subtract(5, 3)) # Output: 2 print(multiply(5, 3)) # Output: 15 ``` # Implementation Details: Server: ```python from xmlrpc.server import SimpleXMLRPCServer def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b server = SimpleXMLRPCServer((\\"localhost\\", 8000)) print(\\"Server is listening on port 8000...\\") server.register_function(add, \\"add\\") server.register_function(subtract, \\"subtract\\") server.register_function(multiply, \\"multiply\\") server.serve_forever() ``` Client: ```python import xmlrpc.client proxy = xmlrpc.client.ServerProxy(\\"http://localhost:8000/\\") print(proxy.add(5, 3)) # Expected Output: 8 print(proxy.subtract(5, 3)) # Expected Output: 2 print(proxy.multiply(5, 3)) # Expected Output: 15 ``` Complete the implementation by ensuring the server handles multiple requests, and the client correctly calls each method and prints the results.","solution":"from xmlrpc.server import SimpleXMLRPCServer def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def run_server(): server = SimpleXMLRPCServer((\\"localhost\\", 8000)) print(\\"Server is listening on port 8000...\\") server.register_function(add, \\"add\\") server.register_function(subtract, \\"subtract\\") server.register_function(multiply, \\"multiply\\") server.serve_forever() if __name__ == \\"__main__\\": run_server()"},{"question":"Objective: In this task, you will demonstrate your understanding of `torch.distributed.elastic.multiprocessing.subprocess_handler` by implementing a subprocess management function. Your function should handle subprocess creation, monitoring for completion, and cleanup. Problem: Implement the function `manage_subprocesses(num_processes, script_path)` that: 1. **Creates** a specified number (`num_processes`) of subprocesses that run a given script (`script_path`). 2. **Monitors** each subprocess and logs their status (e.g., running, completed, failed). 3. **Cleans up** after all subprocesses have completed, ensuring there are no orphaned processes. Input: - `num_processes` (int): The number of subprocesses to create. - `script_path` (str): The path to the script that each subprocess will run. Output: - A list of dictionaries. Each dictionary should contain: - `\'pid\'`: Process ID of the subprocess. - `\'status\'`: Status of the subprocess (`\'completed\'` or `\'failed\'`). Constraints: - Assume `script_path` is a valid path to an executable Python script. - Subprocesses should be properly terminated and cleaned up regardless of the outcome. Example Usage: ```python num_processes = 3 script_path = \\"path/to/your_script.py\\" result = manage_subprocesses(num_processes, script_path) # Example output format # [{\'pid\': 12345, \'status\': \'completed\'}, {\'pid\': 12346, \'status\': \'failed\'}, {\'pid\': 12347, \'status\': \'completed\'}] ``` Notes: 1. You should use `torch.distributed.elastic.multiprocessing.subprocess_handler.SubprocessHandler` for managing the subprocesses. 2. Ensure robust error handling to cater to subprocess failures. 3. Logging can simply be done using print statements for simplicity. 4. Ensure your implementation is efficient and handles edge cases such as subprocess crashes gracefully.","solution":"import subprocess def manage_subprocesses(num_processes, script_path): Manage a specified number of subprocesses that run a given script. Parameters: num_processes (int): The number of subprocesses to create. script_path (str): The path to the script that each subprocess will run. Returns: list of dict: A list of dictionaries containing process ID and status (\'completed\' or \'failed\'). processes = [] results = [] # Start the subprocesses for _ in range(num_processes): proc = subprocess.Popen([\\"python\\", script_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE) processes.append(proc) # Monitor the subprocesses for proc in processes: proc.wait() status = \'completed\' if proc.returncode == 0 else \'failed\' results.append({\'pid\': proc.pid, \'status\': status}) # Cleanup (though subprocess.Popen handles this automatically) for proc in processes: if proc.poll() is None: proc.terminate() return results"},{"question":"# Generic Alias Implementation in Python Python 3.9 introduced generalized type hints using `GenericAlias`, allowing for more flexible and powerful type annotations. In this exercise, you will implement a simplified version of the `GenericAlias` functionality. Your Task Create a class called `SimpleGenericAlias` that mimics the behavior of `types.GenericAlias`. This class will be used to create aliases for generic types and should handle type arguments appropriately. Requirements 1. **Constructor**: - `__init__(self, origin, args)`: - `origin`: The original type (e.g., `list` or `dict`). - `args`: The type arguments, which can be a single type or a tuple of types. - Automatically convert a single type argument to a tuple containing that argument. 2. **Attributes**: - `__origin__`: Stores the original type. - `__args__`: Stores the type arguments as a tuple. 3. **Methods**: - `__repr__(self)`: Returns a string representation in the form of `origin[arg1, arg2, ...]`. - `__eq__(self, other)`: Checks if two `SimpleGenericAlias` instances are equal based on their `__origin__` and `__args__`. Example Usage ```python class SimpleGenericAlias: # Your code here # Example list_alias = SimpleGenericAlias(list, (int,)) print(list_alias) # Output: list[int] dict_alias = SimpleGenericAlias(dict, (str, int)) print(dict_alias) # Output: dict[str, int] # Equality Check assert list_alias == SimpleGenericAlias(list, (int,)) assert dict_alias != SimpleGenericAlias(list, (str,)) ``` Constraints 1. Do not use the `types.GenericAlias` or any similar built-in generic type facility. 2. Ensure that your solution works for both single and multiple type arguments. # Additional Information Your solution should focus on mimicking the mechanism of the `GenericAlias` constructor and handling the type arguments properly to ensure the correctness of the type annotations.","solution":"class SimpleGenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): args = (args,) self.__args__ = args def __repr__(self): return f\\"{self.__origin__.__name__}[{\', \'.join(arg.__name__ for arg in self.__args__)}]\\" def __eq__(self, other): if not isinstance(other, SimpleGenericAlias): return False return self.__origin__ == other.__origin__ and self.__args__ == other.__args__"},{"question":"Objective Your task is to implement several functions that simulate some of the operations provided by the `PyBytes_*` set of functions for managing bytes objects in Python. This exercise will test your understanding of bytes manipulation, handling size constraints, and ensuring memory safety. Task 1. Implement the function `create_bytes_from_string(s: str) -> bytes`: - This function takes a string `s` and returns a bytes object that is a copy of the string\'s content. 2. Implement the function `get_bytes_size(b: bytes) -> int`: - This function takes a bytes object `b` and returns its size. 3. Implement the function `concat_bytes(b1: bytes, b2: bytes) -> bytes`: - This function takes two bytes objects `b1` and `b2`, and returns a new bytes object that is the concatenation of `b1` and `b2`. 4. Implement the function `resize_bytes(b: bytes, new_size: int) -> bytes`: - This function takes a bytes object `b` and an integer `new_size`, and returns a new bytes object that is resized to `new_size`. If `new_size` is smaller than the original size, data should be truncated. If `new_size` is larger, the new bytes object should be zero-padded at the end. Input and Output Formats 1. `create_bytes_from_string(s: str) -> bytes` - **Input**: A string `s`. - **Output**: A bytes object representing a copy of the string\'s content. 2. `get_bytes_size(b: bytes) -> int` - **Input**: A bytes object `b`. - **Output**: An integer representing the size of the bytes object. 3. `concat_bytes(b1: bytes, b2: bytes) -> bytes` - **Input**: Two bytes objects `b1` and `b2`. - **Output**: A new bytes object representing the concatenation of `b1` and `b2`. 4. `resize_bytes(b: bytes, new_size: int) -> bytes` - **Input**: A bytes object `b` and an integer `new_size`. - **Output**: A new bytes object resized to `new_size`, truncated or zero-padded as necessary. Constraints - You must not use any built-in methods that directly solve the problem (e.g., `bytes()`, `len()`, `+` for concatenation). - Ensure proper memory management when creating and resizing bytes objects. - Handle edge cases like empty strings, zero sizes, and large string inputs appropriately. Example ```python def create_bytes_from_string(s): # Your implementation here pass def get_bytes_size(b): # Your implementation here pass def concat_bytes(b1, b2): # Your implementation here pass def resize_bytes(b, new_size): # Your implementation here pass # Example usage print(create_bytes_from_string(\\"Hello\\")) # Output: b\'Hello\' print(get_bytes_size(b\'Hello\')) # Output: 5 print(concat_bytes(b\'Hello\', b\'World\')) # Output: b\'HelloWorld\' print(resize_bytes(b\'Hello\', 7)) # Output: b\'Hellox00x00\' print(resize_bytes(b\'HelloWorld\', 5)) # Output: b\'Hello\' ``` Submit your implementations of these functions. Ensure that they adhere strictly to the described behaviors and constraints.","solution":"def create_bytes_from_string(s: str) -> bytes: Takes a string s and returns a bytes object that is a copy of the string\'s content. return s.encode() # Using the encode method to convert string to bytes def get_bytes_size(b: bytes) -> int: Takes a bytes object b and returns its size. size = 0 while True: try: b[size] size += 1 except IndexError: break return size def concat_bytes(b1: bytes, b2: bytes) -> bytes: Takes two bytes objects b1 and b2, and returns a new bytes object that is the concatenation of b1 and b2. result = bytearray(get_bytes_size(b1) + get_bytes_size(b2)) i = 0 for byte in b1: result[i] = byte i += 1 for byte in b2: result[i] = byte i += 1 return bytes(result) def resize_bytes(b: bytes, new_size: int) -> bytes: Takes a bytes object b and an integer new_size, and returns a new bytes object that is resized to new_size. If new_size is smaller than the original size, data should be truncated. If new_size is larger, the new bytes object should be zero-padded at the end. size = get_bytes_size(b) if new_size <= size: return b[:new_size] result = bytearray(new_size) for i in range(size): result[i] = b[i] return bytes(result)"},{"question":"**Creating and Manipulating a Custom Object Type in Python 3.10** # Background In Python, custom object types can be created and manipulated to extend the functionality beyond built-in types. In this exercise, you will define a new object type that manages a sequence of floating-point numbers with added functionalities for various operations on these sequences. # Objective Your task is to implement a custom object type `FloatSequence` that: 1. Holds a sequence of floating-point numbers. 2. Supports adding a new float to the sequence. 3. Allows retrieving the minimum, maximum, and average of the numbers in the sequence. 4. Supports conversion to a list and string representation. # Requirements 1. **Class Definition:** Define a class `FloatSequence` which will have: - An initializer `__init__(self, sequence: Optional[List[float]] = None)`, initializing the instance with an optional sequence of floating-point numbers. - A method `add_number(self, num: float)` to add a floating-point number to the sequence. - Methods `min(self)`, `max(self)`, and `average(self)` to get the minimum, maximum, and average of the numbers. - A method `to_list(self)` for converting the sequence to a list. - A method `__str__(self)` that returns the string representation of the sequence in the form of a comma-separated string. # Input and Output Formats - **Input:** Not applicable for class definition. However, examples below illustrate how methods should be invoked. - **Output:** The class methods should return appropriate types (float, list, or str as required). # Constraints - The sequence will only contain valid floating-point numbers. No need to handle invalid inputs. - Implementations should be efficient, keeping in mind the performance for large sequences. # Example Usage ```python # Object creation and method invocation seq = FloatSequence([1.2, 2.4, 3.6]) seq.add_number(4.8) print(seq.min()) # output: 1.2 print(seq.max()) # output: 4.8 print(seq.average()) # output: 3.0 print(seq.to_list()) # output: [1.2, 2.4, 3.6, 4.8] print(str(seq)) # output: \\"1.2, 2.4, 3.6, 4.8\\" ``` Implement the `FloatSequence` class as per the above requirements.","solution":"class FloatSequence: def __init__(self, sequence=None): if sequence is None: self.sequence = [] else: self.sequence = sequence def add_number(self, num: float): self.sequence.append(num) def min(self): return min(self.sequence) def max(self): return max(self.sequence) def average(self): return sum(self.sequence) / len(self.sequence) def to_list(self): return self.sequence def __str__(self): return \\", \\".join(map(str, self.sequence))"},{"question":"# Advanced Coding Assessment Question on `http.cookiejar` Objective This task is designed to assess your understanding of handling HTTP cookies using the `http.cookiejar` module in Python. You will implement a simple HTTP client that can manage cookies, including loading and saving cookies to a file, and applying custom cookie policies. Task You are required to implement a custom cookie policy by subclassing `DefaultCookiePolicy`, and use this policy in a `CookieJar` to manage cookies during HTTP requests. Your implementation should load existing cookies from a file, apply your custom policy, make an HTTP request, and save updated cookies back to the file. 1. **CustomCookiePolicy Class** - Subclass `DefaultCookiePolicy`. - Implement the `set_ok` method to only accept cookies from domains that are neither blocked nor are in a specified list of unwanted domains. - Implement the `return_ok` method to only return cookies to the server if they are not secured (i.e., `secure` attribute should be `False`). 2. **HTTP Client Functions** - `load_cookies(filename: str) -> CookieJar`: This function should load cookies from a specified file into a `CookieJar` with `CustomCookiePolicy`. - `save_cookies(cookie_jar: CookieJar, filename: str)`: This function should save cookies from a `CookieJar` to a specified file. - `make_request(url: str, cookie_jar: CookieJar) -> str`: This function should make an HTTP GET request to the provided URL using the cookies in `cookie_jar` and return the response body as a string. Use `urllib.request` for making the HTTP request. Constraints - The cookies file should be in the format compatible with `MozillaCookieJar`. - Ensure thread-safety while accessing and modifying the cookies from the `CookieJar`. Example ```python import os import http.cookiejar import urllib.request class CustomCookiePolicy(http.cookiejar.DefaultCookiePolicy): def __init__(self, *args, unwanted_domains=None, **kwargs): super().__init__(*args, **kwargs) self.unwanted_domains = unwanted_domains or [] def set_ok(self, cookie, request): if cookie.domain in self.unwanted_domains: return False return super().set_ok(cookie, request) def return_ok(self, cookie, request): if cookie.secure: return False return super().return_ok(cookie, request) def load_cookies(filename: str) -> http.cookiejar.CookieJar: policy = CustomCookiePolicy( blocked_domains=[\\".blocked.com\\"], unwanted_domains=[\\".unwanted.com\\"] ) cookie_jar = http.cookiejar.MozillaCookieJar(filename) cookie_jar.set_policy(policy) try: cookie_jar.load() except http.cookiejar.LoadError: pass return cookie_jar def save_cookies(cookie_jar: http.cookiejar.CookieJar, filename: str): try: cookie_jar.save(ignore_discard=True, ignore_expires=True) except http.cookiejar.LoadError: raise def make_request(url: str, cookie_jar: http.cookiejar.CookieJar) -> str: opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) response = opener.open(url) return response.read().decode(\'utf-8\') # Example usage: cookie_file = os.path.join(os.path.expanduser(\\"~\\"), \\"test_cookies.txt\\") cookie_jar = load_cookies(cookie_file) response_body = make_request(\\"http://example.com\\", cookie_jar) save_cookies(cookie_jar, cookie_file) ``` Submission Please implement the functions as specified and submit the code along with a brief explanation of your approach. Your implementation should be able to handle errors gracefully, and ensure that the cookies are managed according to the specified custom policy.","solution":"import os import http.cookiejar import urllib.request class CustomCookiePolicy(http.cookiejar.DefaultCookiePolicy): def __init__(self, *args, unwanted_domains=None, **kwargs): super().__init__(*args, **kwargs) self.unwanted_domains = unwanted_domains or [] def set_ok(self, cookie, request): if cookie.domain in self.unwanted_domains: return False return super().set_ok(cookie, request) def return_ok(self, cookie, request): if cookie.secure: return False return super().return_ok(cookie, request) def load_cookies(filename: str) -> http.cookiejar.CookieJar: policy = CustomCookiePolicy( blocked_domains=[\\".blocked.com\\"], unwanted_domains=[\\".unwanted.com\\"] ) cookie_jar = http.cookiejar.MozillaCookieJar(filename) cookie_jar.set_policy(policy) try: cookie_jar.load() except http.cookiejar.LoadError: pass return cookie_jar def save_cookies(cookie_jar: http.cookiejar.CookieJar, filename: str): try: cookie_jar.save(ignore_discard=True, ignore_expires=True) except http.cookiejar.LoadError: raise def make_request(url: str, cookie_jar: http.cookiejar.CookieJar) -> str: opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) response = opener.open(url) return response.read().decode(\'utf-8\')"},{"question":"# Question: Parsing and Filtering Email Messages You have been tasked with developing a function to process an email message and extract certain parts of the content based on their MIME type. Specifically, you need to filter out only the plaintext and HTML content from an email message and combine them line-by-line into a single string. Function Signature ```python def extract_email_content(email_message: email.message.EmailMessage) -> str: pass ``` Input - `email_message`: An instance of `email.message.EmailMessage`, representing the email message to be processed. Output - Returns a single string that contains all the lines from the plaintext and HTML parts of the email message, concatenated together. Constraints - You must use the functions `email.iterators.body_line_iterator` and `email.iterators.typed_subpart_iterator` to implement your solution. - While combining lines, ensure that lines from different parts maintain their order. Lines from plaintext parts should appear before lines from HTML parts if both are present. - Assume that the input `email_message` is well-formed and contains valid MIME types. Example ```python from email import message_from_string email_content = From: example@example.com To: recipient@example.com MIME-Version: 1.0 Content-Type: multipart/alternative; boundary=\\"boundary\\" --boundary Content-Type: text/plain; charset=\\"utf-8\\" Hello, this is a plaintext message. --boundary Content-Type: text/html; charset=\\"utf-8\\" <html><body>Hello, this is an HTML message.</body></html> --boundary-- msg = message_from_string(email_content) # Example Usage result = extract_email_content(msg) print(result) ``` Expected Output: ``` Hello, this is a plaintext message. <html><body>Hello, this is an HTML message.</body></html> ``` **Hint**: Use `typed_subpart_iterator` to iterate over the plaintext and HTML parts of the email message, and `body_line_iterator` to read the lines from each part.","solution":"import email from email.iterators import body_line_iterator, typed_subpart_iterator def extract_email_content(email_message: email.message.EmailMessage) -> str: Extracts the plaintext and HTML content from an EmailMessage and combines them into a single string. content_parts = [] # Extract text/plain parts for part in typed_subpart_iterator(email_message, \'text\', \'plain\'): content_parts.extend(list(body_line_iterator(part))) # Extract text/html parts for part in typed_subpart_iterator(email_message, \'text\', \'html\'): content_parts.extend(list(body_line_iterator(part))) # Combine all parts into a single string return \'n\'.join(content_parts)"},{"question":"**Problem Statement:** You are required to use the `faulthandler` module to implement a class `FaultHandlerManager` that provides functionalities to manage fault handler features for a given application. Your class should include methods to enable and disable fault handlers, dump tracebacks manually, set up timing-based traceback dumping, and register signals to trigger tracebacks. # Class Specification: Method 1: `enable_handler` ```python def enable_handler(self, file: Optional[Union[str, int]] = None, all_threads: bool = True) -> None: Enable the fault handler. Parameters: - file (str|int|None): Optional; Path to the log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - all_threads (bool): Optional; Whether to dump tracebacks for all threads. Default is True. Returns: - None ``` Method 2: `disable_handler` ```python def disable_handler(self) -> None: Disable the fault handler. Returns: - None ``` Method 3: `dump_traceback` ```python def dump_traceback(self, file: Optional[Union[str, int]] = None, all_threads: bool = True) -> None: Dump the tracebacks of all threads into file. Parameters: - file (str|int|None): Optional; Path to log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - all_threads (bool): Optional; Whether to dump tracebacks for all threads. Default is True. Returns: - None ``` Method 4: `dump_traceback_later` ```python def dump_traceback_later(self, timeout: float, repeat: bool = False, file: Optional[Union[str, int]] = None, exit: bool = False) -> None: Schedule a future traceback dump after a timeout. Parameters: - timeout (float): Timeout in seconds after which to dump tracebacks. - repeat (bool): Optional; If True, dump tracebacks every timeout seconds. - file (str|int|None): Optional; Path to log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - exit (bool): Optional; If True, exit the application after dumping the tracebacks. Default is False. Returns: - None ``` Method 5: `cancel_dump_traceback_later` ```python def cancel_dump_traceback_later(self) -> None: Cancel the scheduled traceback dump. Returns: - None ``` Method 6: `register_signal` ```python def register_signal(self, signum: int, file: Optional[Union[str, int]] = None, all_threads: bool = True, chain: bool = False) -> None: Register a signal to trigger a traceback dump. Parameters: - signum (int): Signal number to register. - file (str|int|None): Optional; Path to log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - all_threads (bool): Optional; Whether to dump tracebacks for all threads. Default is True. - chain (bool): Optional; Whether to call the previous signal handler. Default is False. Returns: - None ``` Method 7: `unregister_signal` ```python def unregister_signal(self, signum: int) -> bool: Unregister a signal previously set up to trigger a traceback dump. Parameters: - signum (int): Signal number to unregister. Returns: - bool: True if the signal was registered, False otherwise. ``` # Additional Requirements: - Ensure that your implementation handles all edge cases, such as invalid parameters. - Include appropriate error checking and handling (e.g., ensure file descriptors remain valid). # Example Usage: ```python if __name__ == \\"__main__\\": fh_manager = FaultHandlerManager() fh_manager.enable_handler(\\"/path/to/logfile.log\\", all_threads=True) fh_manager.dump_traceback() fh_manager.dump_traceback_later(5, repeat=True) fh_manager.cancel_dump_traceback_later() fh_manager.register_signal(signal.SIGUSR1, all_threads=True) fh_manager.unregister_signal(signal.SIGUSR1) fh_manager.disable_handler() ``` # Constraints: - Do not use any other external libraries except the `faulthandler` and standard libraries that come with Python 3.10. - Ensure to manage file descriptors properly, taking into consideration the issue of file descriptor reuse. **Good Luck!**","solution":"import faulthandler import signal from typing import Optional, Union class FaultHandlerManager: def enable_handler(self, file: Optional[Union[str, int]] = None, all_threads: bool = True) -> None: Enable the fault handler. Parameters: - file (str|int|None): Optional; Path to the log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - all_threads (bool): Optional; Whether to dump tracebacks for all threads. Default is True. Returns: - None if file is not None and isinstance(file, str): file = open(file, \'w\') faulthandler.enable(file=file, all_threads=all_threads) def disable_handler(self) -> None: Disable the fault handler. Returns: - None faulthandler.disable() def dump_traceback(self, file: Optional[Union[str, int]] = None, all_threads: bool = True) -> None: Dump the tracebacks of all threads into file. Parameters: - file (str|int|None): Optional; Path to log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - all_threads (bool): Optional; Whether to dump tracebacks for all threads. Default is True. Returns: - None if file is not None and isinstance(file, str): with open(file, \'w\') as f: faulthandler.dump_traceback(file=f, all_threads=all_threads) else: faulthandler.dump_traceback(file=file, all_threads=all_threads) def dump_traceback_later(self, timeout: float, repeat: bool = False, file: Optional[Union[str, int]] = None, exit: bool = False) -> None: Schedule a future traceback dump after a timeout. Parameters: - timeout (float): Timeout in seconds after which to dump tracebacks. - repeat (bool): Optional; If True, dump tracebacks every timeout seconds. - file (str|int|None): Optional; Path to log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - exit (bool): Optional; If True, exit the application after dumping the tracebacks. Default is False. Returns: - None if file is not None and isinstance(file, str): file = open(file, \'w\') faulthandler.dump_traceback_later(timeout, repeat=repeat, file=file, exit=exit) def cancel_dump_traceback_later(self) -> None: Cancel the scheduled traceback dump. Returns: - None faulthandler.cancel_dump_traceback_later() def register_signal(self, signum: int, file: Optional[Union[str, int]] = None, all_threads: bool = True, chain: bool = False) -> None: Register a signal to trigger a traceback dump. Parameters: - signum (int): Signal number to register. - file (str|int|None): Optional; Path to log file or file descriptor to write tracebacks. Use None to write to sys.stderr. - all_threads (bool): Optional; Whether to dump tracebacks for all threads. Default is True. - chain (bool): Optional; Whether to call the previous signal handler. Default is False. Returns: - None if file is not None and isinstance(file, str): file = open(file, \'w\') faulthandler.register(signum, file=file, all_threads=all_threads, chain=chain) def unregister_signal(self, signum: int) -> bool: Unregister a signal previously set up to trigger a traceback dump. Parameters: - signum (int): Signal number to unregister. Returns: - bool: True if the signal was registered, False otherwise. return faulthandler.unregister(signum)"},{"question":"# Asynchronous File Processing with Coroutines Objective Create an asynchronous file processing system using Python’s coroutines. Your task is to implement a function that reads data from multiple files concurrently, processes each file\'s data, and writes the processed data to a new file. This will test your understanding of Python\'s asynchronous capabilities using coroutines. Requirements 1. Implement a function `process_files(input_files: List[str], output_file: str) -> None` where: - `input_files` is a list of input filenames (strings) to be read concurrently. - `output_file` is the filename (string) where the processed data from all input files should be written. 2. Each file should be read and processed concurrently using coroutines. 3. For each input file, process the content by reversing the lines of text and converting them to uppercase. 4. Write the processed data from all input files into the `output_file`, appending the content from each file sequentially. 5. Handle all necessary file I/O operations asynchronously. Constraints - You must use Python\'s `asyncio` module and coroutine syntax (`async`/`await`). - Handle potential exceptions that might occur during file operations. - Ensure the solution is efficient and avoids blocking operations. Example Suppose you have the following files: - `file1.txt` containing: ``` Hello World ``` - `file2.txt` containing: ``` Foo Bar ``` Your function call: ```python await process_files([\'file1.txt\', \'file2.txt\'], \'output.txt\') ``` Should result in `output.txt` containing: ``` OLLEH DLROW OOF RAB ``` Implementation Notes You might need to use helper functions to handle individual tasks asynchronously. Ensure to use the asyncio event loop to manage concurrent tasks efficiently.","solution":"import asyncio from typing import List async def read_and_process_file(filename: str) -> str: Asynchronously reads a file, processes its content by reversing and uppercasing each line. :param filename: Name of the file to read. :return: Processed content of the file as a single string. try: async with aiofiles.open(filename, mode=\'r\') as file: lines = await file.readlines() processed_lines = [line[::-1].strip().upper() for line in lines] return \\"n\\".join(processed_lines) except Exception as e: print(f\\"Error processing file {filename}: {e}\\") return \\"\\" async def process_files(input_files: List[str], output_file: str) -> None: Processes multiple files concurrently and writes the processed content to an output file. :param input_files: List of input filenames to be read. :param output_file: The filename where the processed data will be written. try: import aiofiles # Process files concurrently tasks = [read_and_process_file(file) for file in input_files] results = await asyncio.gather(*tasks) # Write the results to the output file async with aiofiles.open(output_file, mode=\'w\') as out: for result in results: await out.write(result + \\"n\\") except Exception as e: print(f\\"Error writing to output file {output_file}: {e}\\")"},{"question":"# Advanced Coding Assessment Question Objective Your task is to create a Python logging mechanism that logs messages into a rotating file and simultaneously sends critical errors to a web server for alerting purposes. The log files should be rotated based on size constraints, and the setup should ensure thread-safe logging. Requirements 1. **Setup a Rotating File Handler**: - Create a `RotatingFileHandler` that logs messages into a file named `app.log`. - The log file should rotate after reaching 1MB in size. - Maintain backups with the last 3 log files (`app.log`, `app.log.1`, `app.log.2`, `app.log.3`). 2. **Setup an Error Logging Handler**: - Create an `HTTPHandler` that sends logs to a web server endpoint `http://example.com/logs`. - This handler should only handle log records with level `ERROR` or higher. 3. **Setup a Thread-Safe Queue for Logging**: - Implement a `QueueHandler` to ensure logging does not block the main application flow. - Use a `QueueListener` to process log records from the queue and use both the `RotatingFileHandler` and the `HTTPHandler` for logging. 4. **Logging Setup**: - All messages of level `DEBUG` and higher should be logged to the rotating file. - Only messages of level `ERROR` and above should be sent to the web server using `HTTPHandler`. 5. **Additional Details**: - Name your logger \\"myLogger\\". - Ensure the logging is configured via a function `setup_logging`. Constraints - Use Python\'s built-in logging module and the handlers from `logging.handlers`. - You must handle exceptions and ensure your logger does not crash under any circumstance. - Implement the `setup_logging` function which configures the logging as described. Function Signature ```python import logging import logging.handlers def setup_logging(): # Your solution here ``` Example Usage ```python if __name__ == \\"__main__\\": setup_logging() logger = logging.getLogger(\\"myLogger\\") logger.debug(\\"This is a debug message.\\") logger.info(\\"This is an info message.\\") logger.error(\\"This is an error message.\\") logger.critical(\\"This is a critical message.\\") ``` **Expected Behavior**: - Log files should rotate once `app.log` reaches 1MB. - `app.log` and up to three backup files should be created and properly maintained. - Errors and critical messages should be sent to the dummy endpoint `http://example.com/logs`.","solution":"import logging import logging.handlers from queue import Queue import requests import threading class HTTPErrorHandler(logging.Handler): def __init__(self, url): logging.Handler.__init__(self) self.url = url def emit(self, record): try: log_entry = self.format(record) # This would normally use requests.post or something similar # but for this example, we just print it. print(f\\"Sending to {self.url}: {log_entry}\\") # Uncomment below line in a real scenario # requests.post(self.url, data=log_entry) except Exception: self.handleError(record) def setup_logging(): logger = logging.getLogger(\\"myLogger\\") logger.setLevel(logging.DEBUG) # Create a rotating file handler rotating_file_handler = logging.handlers.RotatingFileHandler( \'app.log\', maxBytes=1 * 1024 * 1024, backupCount=3) rotating_file_handler.setLevel(logging.DEBUG) # Create an HTTPHandler that only handles ERROR and above http_handler = HTTPErrorHandler(\'http://example.com/logs\') http_handler.setLevel(logging.ERROR) # Format for handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') rotating_file_handler.setFormatter(formatter) http_handler.setFormatter(formatter) # Create a queue and a queue handler log_queue = Queue() queue_handler = logging.handlers.QueueHandler(log_queue) logger.addHandler(queue_handler) # Create a queue listener with our handlers queue_listener = logging.handlers.QueueListener(log_queue, rotating_file_handler, http_handler) queue_listener.start() # Register queue listener so it stops properly on program exit atexit.register(queue_listener.stop) return logger import atexit if __name__ == \\"__main__\\": setup_logging() logger = logging.getLogger(\\"myLogger\\") logger.debug(\\"This is a debug message.\\") logger.info(\\"This is an info message.\\") logger.error(\\"This is an error message.\\") logger.critical(\\"This is a critical message.\\")"},{"question":"Custom Traceback Logger You are tasked with creating a custom traceback logger in Python. This logger should observe the following specifications: 1. **Function Name:** - `log_traceback` 2. **Input Parameters:** - `func`: a function object to execute. - `args`: a list of positional arguments to pass to the function. - `kwargs`: a dictionary of keyword arguments to pass to the function. - `log_file`: the path of the file where the traceback should be logged if an exception occurs. 3. **Output:** - The function should return the result of `func(*args, **kwargs)` if no exception occurs. - If an exception occurs, it should not re-raise the exception but log the detailed traceback to the specified file and return None. 4. **Behavior:** - The function should handle any kind of exception that `func` might raise. - It should format the traceback similar to Python\'s default traceback formatting. - The format should include the exception type, the exception message, and the full stack trace. Example ```python def sample_function(a, b): return a / b def main(): result = log_traceback(sample_function, [10, 0], {}, \'error_log.txt\') print(result) # Should print \'None\' and log the traceback to \'error_log.txt\' if __name__ == \\"__main__\\": main() ``` Given the above code, the file `error_log.txt` should contain: ``` Traceback (most recent call last): File \\"<user_input>\\", line X, in log_traceback result = func(*args, **kwargs) File \\"<user_input>\\", line Y, in sample_function return a / b ZeroDivisionError: division by zero ``` **Implementation Constraints:** - You must use the `traceback` module wherever applicable. - Maintain a clear formatting style for the traceback similar to the `traceback.format_exception` output. - Ensure the function handles any arbitrary exception raised by `func`. **Your Implementation:** ```python import traceback def log_traceback(func, args, kwargs, log_file): # Your code here ```","solution":"import traceback def log_traceback(func, args, kwargs, log_file): Execute a function with the given arguments and log the traceback to a file if an exception occurs. Args: func: The function to execute. args: A list of positional arguments to pass to the function. kwargs: A dictionary of keyword arguments to pass to the function. log_file: The path to the file where the traceback should be logged. Returns: The result of the function if no exception occurs, otherwise None. try: return func(*args, **kwargs) except Exception as e: with open(log_file, \'w\') as f: traceback.print_exc(file=f) return None"},{"question":"**Seaborn Plotting Context Assessment** # Background In this exercise, you will demonstrate your understanding of seaborn\'s `plotting_context` function by generating a series of plots that showcase different stylistic contexts. You will write a function to create plots using different context settings and then compare these plots. # Task Implement the function `generate_plot_context_comparison(data, x, y, contexts)` which takes the following arguments: - `data`: a Pandas DataFrame containing the data to be plotted. - `x`: the column name to be used for the x-axis. - `y`: the column name to be used for the y-axis. - `contexts`: a list of context names to be applied sequentially (e.g., `[\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]`). Your function should: 1. Plot the data using a Seaborn line plot for each context specified in the `contexts` list. 2. Display all the plots in a single row for easy comparison. # Constraints 1. The function should handle at least the predefined contexts: `\\"paper\\"`, `\\"notebook\\"`, `\\"talk\\"`, `\\"poster\\"`. 2. Ensure readability of the plots by adjusting their size appropriately. # Example Usage ```python import seaborn as sns import pandas as pd # Example data data = pd.DataFrame({ \\"Time\\": range(1, 11), \\"Value\\": [1, 3, 2, 5, 7, 8, 2, 6, 4, 5] }) contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] generate_plot_context_comparison(data, x=\\"Time\\", y=\\"Value\\", contexts=contexts) ``` _Please make sure you have seaborn and pandas installed to run the example._ # Expected Output The function should display four seaborn line plots in a single row, with each plot using different plotting contexts as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def generate_plot_context_comparison(data, x, y, contexts): Creates a series of line plots using different seaborn plotting contexts. Parameters: - data: Pandas DataFrame containing the data to be plotted. - x: The column name to be used for the x-axis. - y: The column name to be used for the y-axis. - contexts: A list of context names to be applied sequentially. num_contexts = len(contexts) # Create a figure with subplots fig, axes = plt.subplots(1, num_contexts, figsize=(5*num_contexts, 4), sharey=True) for ax, context in zip(axes, contexts): sns.set_context(context) sns.lineplot(data=data, x=x, y=y, ax=ax) ax.set_title(f\'Context: {context}\') ax.grid(True) plt.tight_layout() plt.show()"},{"question":"# Question: Advanced Data Visualization with Seaborn You are provided with a dataset that contains information about individuals\' expenses on different categories like food, transport, and entertainment. Your task is to create a series of plots to visualize this data, highlighting relationships and distributions across various dimensions. Dataset Description You can assume the following structure for the dataset: ```python import pandas as pd data = { \'person_id\': range(1, 101), \'category\': [\'food\', \'transport\', \'entertainment\']*33 + [\'food\'], \'amount\': np.random.uniform(5, 100, 100), \'age_group\': np.random.choice([\'18-25\', \'26-35\', \'36-45\', \'46-60\'], 100), \'spends_often\': np.random.choice([\'yes\', \'no\'], 100) } dataset = pd.DataFrame(data) ``` Tasks: 1. **FacetGrid Visualization:** Create a `FacetGrid` that shows histograms of the `amount` spent, broken down by `category` and `age_group`. Use `spends_often` as the `hue` parameter to differentiate between frequent and infrequent spenders. 2. **PairGrid Visualization:** Use a `PairGrid` to visualize pairwise relationships between `amount`, and `age_group`. Use different colors for different spending categories. 3. **Customization:** Add appropriate legends, axis labels, and titles to make the plots informative. 4. **Optional Custom Function:** (Bonus) Create a custom plotting function to plot a specific aspect of the data, and use it in combination with `FacetGrid` or `PairGrid`. Expected Output: - A well-commented Python script or Jupyter notebook showing all the required visualizations. - Customization should be evident in the presentation of the plots. Performance and Constraints: - Ensure that your solution works efficiently for a dataset of up to 10,000 entries. - The code should handle cases where certain subsets might have no data gracefully without throwing errors. Implementation Requirements: - Do not use any libraries other than `seaborn`, `matplotlib`, and `numpy`. - Visualize the data in a way that clearly represents the relationships and distributions. **Note:** Assume that the dataset provided above (`dataset`) is already loaded and available in your working environment.","solution":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Create the dataset data = { \'person_id\': range(1, 101), \'category\': [\'food\', \'transport\', \'entertainment\']*33 + [\'food\'], \'amount\': np.random.uniform(5, 100, 100), \'age_group\': np.random.choice([\'18-25\', \'26-35\', \'36-45\', \'46-60\'], 100), \'spends_often\': np.random.choice([\'yes\', \'no\'], 100) } dataset = pd.DataFrame(data) # Task 1: FacetGrid Visualization def plot_facet_grid(data): g = sns.FacetGrid(data, row=\'category\', col=\'age_group\', hue=\'spends_often\') g.map(plt.hist, \'amount\', alpha=0.5, bins=10) g.add_legend() g.set_axis_labels(\'Amount Spent\', \'Count\') g.set_titles(row_template=\'{row_name}\', col_template=\'{col_name} Age Group\') g.fig.suptitle(\'Distribution of Amount Spent by Category and Age Group\', y=1.03) plt.show() # Task 2: PairGrid Visualization def plot_pair_grid(data): g = sns.PairGrid(data, hue=\'category\', vars=[\'amount\']) g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) g.add_legend() g.fig.suptitle(\'Pairwise Relationships of Amount by Category\', y=1.03) plt.show() # Task 3: Customization and plotting plot_facet_grid(dataset) plot_pair_grid(dataset) # Task 4: Optional Custom Function def custom_scatter_plot(x, y, **kwargs): sns.scatterplot(x=x, y=y, **kwargs) plt.axhline(y=np.mean(y), color=\'r\', linestyle=\'--\') def plot_custom_pair_grid(data): g = sns.PairGrid(data, hue=\'category\', vars=[\'amount\']) g.map(custom_scatter_plot) g.add_legend() g.fig.suptitle(\'Customized PairGrid with Mean Line\', y=1.03) plt.show() plot_custom_pair_grid(dataset)"},{"question":"You are given a stream of student scores and you want to efficiently manage and analyze these scores in a sorted list. Write a class `ScoreTracker` that uses the `bisect` module to insert scores and perform various search operations. Class: `ScoreTracker` **Methods:** 1. **`__init__(self)`** - Initializes an empty list to store the scores. 2. **`add_score(self, score: int) -> None`** - Inserts a score into the list while maintaining sorted order. 3. **`find_rank(self, score: int) -> int`** - Returns the 1-based rank of the given score in the list (e.g., the rank of the highest score is 1). If the score is not present, return `-1`. 4. **`find_closest_lower_score(self, score: int) -> int`** - Finds and returns the highest score in the list that is less than the given score. If no such score exists, return `-1`. 5. **`find_closest_higher_score(self, score: int) -> int`** - Finds and returns the lowest score in the list that is greater than the given score. If no such score exists, return `-1`. **Constraints:** - All scores are integers in the range `[0, 100]`. - You can assume there will not be more than 10,000 scores in total. Example ```python tracker = ScoreTracker() tracker.add_score(50) tracker.add_score(80) tracker.add_score(70) tracker.add_score(90) print(tracker.find_rank(80)) # Output: 2 print(tracker.find_rank(100)) # Output: -1 print(tracker.find_closest_lower_score(75)) # Output: 70 print(tracker.find_closest_higher_score(75)) # Output: 80 ``` Ensure that your implementation efficiently maintains the sorted order of the scores and performs the search operations accurately using the `bisect` module.","solution":"import bisect class ScoreTracker: def __init__(self): self.scores = [] def add_score(self, score: int) -> None: bisect.insort(self.scores, score) def find_rank(self, score: int) -> int: index = bisect.bisect_left(self.scores, score) if index < len(self.scores) and self.scores[index] == score: # Rank is 1-based, so we need to return index + 1 return len(self.scores) - index return -1 def find_closest_lower_score(self, score: int) -> int: index = bisect.bisect_left(self.scores, score) if index > 0: return self.scores[index - 1] return -1 def find_closest_higher_score(self, score: int) -> int: index = bisect.bisect_right(self.scores, score) if index < len(self.scores): return self.scores[index] return -1"},{"question":"Coding Assessment Question **Title:** Implementing Stochastic Gradient Descent for Classification and Regression # Question Overview In this assessment, you are required to implement two machine learning models using the Stochastic Gradient Descent (SGD) algorithm from the sklearn library: one for classification and one for regression. You need to demonstrate your understanding of the various parameters of the SGD algorithm, including loss functions, penalties, and feature scaling. # Task Details 1. **Classification Task:** - Implement a binary classifier using `SGDClassifier` with the hinge loss function (equivalent to a linear SVM). - Make sure to shuffle the training data and standardize the features. - Use cross-validation to find the best `alpha` hyperparameter (regularization term) from the range `[1e-4, 1e-3, 1e-2, 1e-1, 1]`. 2. **Regression Task:** - Implement a regression model using `SGDRegressor` with the squared error loss function (Ordinary Least Squares). - Make sure to shuffle the training data and standardize the features. - Additionally, use cross-validation to find the best `alpha` hyperparameter (regularization term) from the range `[1e-4, 1e-3, 1e-2, 1e-1, 1]`. # Input 1. For the classification task: - A training dataset with `X_train` (features) and `y_train` (binary labels). 2. For the regression task: - A training dataset with `X_train` (features) and `y_train` (continuous values). # Output 1. For the classification task: - The best `alpha` hyperparameter value found using cross-validation. - The accuracy of the SGD classifier on a test set. 2. For the regression task: - The best `alpha` hyperparameter value found using cross-validation. - The mean squared error (MSE) of the SGD regressor on a test set. # Constraints and Requirements - Use the `StandardScaler` from sklearn to standardize the features. - Use cross-validation with at least 5 folds. - Ensure reproducibility by setting a random state where applicable. - The datasets provided will not have any missing values. # Example Code Structure ```python import numpy as np from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV, cross_val_score from sklearn.metrics import accuracy_score, mean_squared_error # Classification Task def sgd_classification(X_train, y_train, X_test, y_test): # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Cross-validation to find the best alpha param_grid = {\'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(SGDClassifier(loss=\'hinge\', random_state=42), param_grid, cv=5) grid_search.fit(X_train, y_train) best_alpha = grid_search.best_params_[\'alpha\'] best_model = grid_search.best_estimator_ # Evaluate on test data y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return best_alpha, accuracy # Regression Task def sgd_regression(X_train, y_train, X_test, y_test): # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Cross-validation to find the best alpha param_grid = {\'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(SGDRegressor(loss=\'squared_error\', random_state=42), param_grid, cv=5) grid_search.fit(X_train, y_train) best_alpha = grid_search.best_params_[\'alpha\'] best_model = grid_search.best_estimator_ # Evaluate on test data y_pred = best_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return best_alpha, mse # Example usage # X_train_class, y_train_class, X_test_class, y_test_class = ... (Load your classification dataset) # X_train_reg, y_train_reg, X_test_reg, y_test_reg = ... (Load your regression dataset) # best_alpha_class, accuracy = sgd_classification(X_train_class, y_train_class, X_test_class, y_test_class) # best_alpha_reg, mse = sgd_regression(X_train_reg, y_train_reg, X_test_reg, y_test_reg) # print(f\\"Best Alpha for Classification: {best_alpha_class}, Accuracy: {accuracy}\\") # print(f\\"Best Alpha for Regression: {best_alpha_reg}, Mean Squared Error: {mse}\\") ``` # Submission Guidelines - Submit your code solution as a single Python file. - Include any important comments and explanations within your code. - Make sure your code runs without errors and produces the expected output. # Evaluation Criteria - Correctness: Does the code correctly implement the required tasks? - Code quality: Is the code readable and well-structured? - Use of sklearn functionalities: Are the relevant sklearn classes and methods appropriately utilized? - Performance: How efficiently does the code find the best hyperparameters and evaluate the model?","solution":"import numpy as np from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score, mean_squared_error # Classification Task def sgd_classification(X_train, y_train, X_test, y_test): # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Cross-validation to find the best alpha param_grid = {\'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(SGDClassifier(loss=\'hinge\', random_state=42, shuffle=True), param_grid, cv=5) grid_search.fit(X_train, y_train) best_alpha = grid_search.best_params_[\'alpha\'] best_model = grid_search.best_estimator_ # Evaluate on test data y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return best_alpha, accuracy # Regression Task def sgd_regression(X_train, y_train, X_test, y_test): # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Cross-validation to find the best alpha param_grid = {\'alpha\': [1e-4, 1e-3, 1e-2, 1e-1, 1]} grid_search = GridSearchCV(SGDRegressor(loss=\'squared_error\', random_state=42, shuffle=True), param_grid, cv=5) grid_search.fit(X_train, y_train) best_alpha = grid_search.best_params_[\'alpha\'] best_model = grid_search.best_estimator_ # Evaluate on test data y_pred = best_model.predict(X_test) mse = mean_squared_error(y_test, y_pred) return best_alpha, mse"},{"question":"**Problem Statement: Event Scheduler Enhancement** You are required to extend the capabilities of Python\'s `sched` module by adding new functionalities and creating an enhanced event scheduler. **New Functionalities to Implement:** 1. `reschedule`: Modify the scheduled time of an existing event. 2. `get_next_event_time`: Retrieve the time of the next scheduled event. 3. `run_with_timeout`: Run scheduled events only up to a specified timeout duration from the start time. **Function Details:** 1. `scheduler.reschedule(event, new_time)` - **Input:** - `event`: The event to reschedule (obtained from the `enter` or `enterabs` method). - `new_time`: The new absolute time to which the event is rescheduled. - **Output:** - None - **Functionality:** - This method should adjust the given `event` to be executed at `new_time`. 2. `scheduler.get_next_event_time()` - **Output:** - Return the time of the next scheduled event or `None` if there are no scheduled events. 3. `scheduler.run_with_timeout(timeout)` - **Input:** - `timeout`: The maximum duration (in the same units as `timefunc`) to run the scheduler. - **Output:** - None - **Functionality:** - This method should run the scheduler, but only up to the specified `timeout` duration from the time it starts. - If the next event is beyond the `timeout`, the method should stop running. **Implementation Restrictions:** - You may assume that input will always be valid. - Focus on error handling, particularly for edge cases such as rescheduling an event that no longer exists. **Example Usage:** ```python import sched, time scheduler = sched.scheduler(time.time, time.sleep) def print_event(name): print(f\\"Event {name} at time {time.time()}\\") event = scheduler.enter(5, 1, print_event, argument=(\'A\',)) scheduler.enter(10, 1, print_event, argument=(\'B\',)) # Reschedule first event to run earlier scheduler.reschedule(event, time.time() + 2) # Print the next event time print(scheduler.get_next_event_time()) # Run with a timeout of 7 seconds scheduler.run_with_timeout(7) ``` Expected Output: ``` <current_time + 2> # Time of the next event after adjustment Event A at time <current_time + 2> # Scheduler stops before the next event \'B\' could run ``` **Note:** - Ensure thread safety in multi-threading environments. - Maintain the integrity and consistency of the scheduler state in case of exceptions.","solution":"import sched import time class EnhancedScheduler(sched.scheduler): def reschedule(self, event, new_time): Modify the scheduled time of an existing event. if event in self.queue: self.cancel(event) new_event = self.enterabs(new_time, event.priority, event.action, event.argument, event.kwargs) return new_event else: raise ValueError(\\"Event not found in the current schedule\\") def get_next_event_time(self): Retrieve the time of the next scheduled event. return self.queue[0].time if self.queue else None def run_with_timeout(self, timeout): Run scheduled events only up to a specified timeout duration from the start time. end_time = self.timefunc() + timeout while self.queue and self.timefunc() < end_time: delay = self.queue[0].time - self.timefunc() if delay <= 0: self.run(blocking=False) else: time.sleep(min(delay, end_time - self.timefunc()))"},{"question":"# Pickling Custom Objects with `copyreg` You are required to demonstrate your understanding of the `copyreg` module by designing a custom class and then registering it for pickling. Follow the steps below to complete the task: 1. Create a class `Person` with the following properties: - `name`: A string representing the name of the person. - `age`: An integer representing the age of the person. - `address`: A string with the address of the person. 2. Implement a method `__str__` in `Person` to print a readable string representation of a person. 3. Write a function `pickle_person` that will: - Accept an instance of `Person`. - Print a message \\"pickling a Person instance...\\" when called. - Return a tuple that helps in reconstructing the `Person` object. 4. Register the `pickle_person` function using `copyreg.pickle`. 5. Demonstrate that the pickling and unpickling of the `Person` object works correctly by: - Creating an instance of `Person`. - Using the `pickle` module to serialize and deserialize this instance. - Ensuring the deserialized instance maintains the original data. # Constraints: - You are not allowed to use external libraries other than the `pickle` and `copyreg` modules. - You should handle any potential errors gracefully. # Example of Expected Output: ```python pickling a Person instance... ``` # Function Signature: ```python class Person: def __init__(self, name: str, age: int, address: str): pass def __str__(self) -> str: pass def pickle_person(person: Person) -> tuple: pass # Registration step using copyreg.pickle # Pickle and Unpickle demonstration ``` # Notes: - Your implementation should include the entire flow from class definition to testing the pickling and unpickling process. - Ensure that the deserialized instance has the same attribute values as the original instance.","solution":"import copyreg import pickle class Person: def __init__(self, name: str, age: int, address: str): self.name = name self.age = age self.address = address def __str__(self): return f\'Person(name={self.name}, age={self.age}, address={self.address})\' def pickle_person(person): print(\\"pickling a Person instance...\\") return Person, (person.name, person.age, person.address) # Register the pickle_person function copyreg.pickle(Person, pickle_person) # Demonstration of pickling and unpickling def demo(): # Create an instance of Person person = Person(\\"John Doe\\", 30, \\"123 Elm Street\\") print(\\"Original person:\\", person) # Serialize (pickle) the Person instance person_pickled = pickle.dumps(person) # Deserialize (unpickle) the Person instance person_unpickled = pickle.loads(person_pickled) print(\\"Unpickled person:\\", person_unpickled) assert str(person) == str(person_unpickled) if __name__ == \\"__main__\\": demo()"},{"question":"<|Analysis Begin|> The `timeit` Python module provides a simple way to measure the execution time of small code snippets. It includes both a Command-Line Interface and a Python API. The key functionalities of this module are: 1. **Timeit Function**: - `timeit.timeit(stmt, setup, timer, number, globals)`: Times the execution of a small code snippet `stmt` after optionally executing a setup code `setup`. - Example: Timing how long it takes to join strings in different ways. 2. **Repeat Function**: - `timeit.repeat(stmt, setup, timer, repeat, number, globals)`: Repeats the execution timing multiple times and returns a list of the timings. 3. **Default Timer**: - `timeit.default_timer()`: Returns the default timer, which is `time.perf_counter()`. 4. **Timer Class**: - This class is designed to handle the details of timing code snippets. Methods include: - `timeit.Timer.timeit(number)`: Times the execution of the timer’s `stmt`. - `timeit.Timer.repeat(repeat, number)`: Repeats the timing measurement multiple times. - `timeit.Timer.autorange(callback=None)`: Determines the number of iterations needed to have a significant timing (>= 0.2 second). The `timeit` module is especially useful for identifying performance differences between different code implementations by comparing their execution times. <|Analysis End|> <|Question Begin|> # Coding Assessment Question: Measuring and Comparing Code Execution Times You are tasked to assess the performance of three different implementations of a function that sums all the integers from 1 to `n`. You need to utilize the `timeit` module to measure the execution time of each method. Here are the three different implementations: 1. Using a `for` loop. 2. Using a list comprehension and the `sum()` function. 3. Using the formula for the sum of an arithmetic series. **Implementations**: 1. `sum_for_loop(n)` 2. `sum_list_comprehension(n)` 3. `sum_arithmetic_series(n)` # Your Task 1. Write the three functions described above. 2. Write a function `compare_execution_times(n)` that uses the `timeit` module to measure and compare the execution times of the three implementations for summing integers from 1 to `n`. 3. The `compare_execution_times(n)` function should print the execution times for all three implementations, along with an indication of which method is the fastest. # Function Specifications Function 1: `sum_for_loop(n)` - **Input**: An integer `n`. - **Output**: The sum of all integers from 1 to `n` using a `for` loop. Function 2: `sum_list_comprehension(n)` - **Input**: An integer `n`. - **Output**: The sum of all integers from 1 to `n` using a list comprehension and the `sum()` function. Function 3: `sum_arithmetic_series(n)` - **Input**: An integer `n`. - **Output**: The sum of all integers from 1 to `n` using the arithmetic series formula `n * (n+1) / 2`. Function 4: `compare_execution_times(n)` - **Input**: An integer `n`. - **Output**: Prints the execution times of the three implementations and indicates the fastest one. # Constraints - The value of `n` will be a positive integer, typically between 1 and 1,000,000. - Use the `timeit` module to measure the execution times accurately. # Example Usage: ```python def sum_for_loop(n): total = 0 for i in range(1, n + 1): total += i return total def sum_list_comprehension(n): return sum([i for i in range(1, n + 1)]) def sum_arithmetic_series(n): return n * (n + 1) // 2 import timeit def compare_execution_times(n): loop_time = timeit.timeit(lambda: sum_for_loop(n), number=1000) comp_time = timeit.timeit(lambda: sum_list_comprehension(n), number=1000) series_time = timeit.timeit(lambda: sum_arithmetic_series(n), number=1000) print(f\\"sum_for_loop took {loop_time:.6f} seconds\\") print(f\\"sum_list_comprehension took {comp_time:.6f} seconds\\") print(f\\"sum_arithmetic_series took {series_time:.6f} seconds\\") fastest_time = min(loop_time, comp_time, series_time) if fastest_time == loop_time: print(\\"sum_for_loop is the fastest method.\\") elif fastest_time == comp_time: print(\\"sum_list_comprehension is the fastest method.\\") else: print(\\"sum_arithmetic_series is the fastest method.\\") # Example call compare_execution_times(100000) ``` Define your solution in a similar way and ensure you test it for correctness and efficiency.","solution":"def sum_for_loop(n): total = 0 for i in range(1, n + 1): total += i return total def sum_list_comprehension(n): return sum([i for i in range(1, n + 1)]) def sum_arithmetic_series(n): return n * (n + 1) // 2 import timeit def compare_execution_times(n): loop_time = timeit.timeit(lambda: sum_for_loop(n), number=1000) comp_time = timeit.timeit(lambda: sum_list_comprehension(n), number=1000) series_time = timeit.timeit(lambda: sum_arithmetic_series(n), number=1000) print(f\\"sum_for_loop took {loop_time:.6f} seconds\\") print(f\\"sum_list_comprehension took {comp_time:.6f} seconds\\") print(f\\"sum_arithmetic_series took {series_time:.6f} seconds\\") fastest_time = min(loop_time, comp_time, series_time) if fastest_time == loop_time: print(\\"sum_for_loop is the fastest method.\\") elif fastest_time == comp_time: print(\\"sum_list_comprehension is the fastest method.\\") else: print(\\"sum_arithmetic_series is the fastest method.\\") # Example call # compare_execution_times(100000)"},{"question":"# Python Development and Testing Tools Assessment **Objective**: The goal of this assessment is to evaluate your ability to write Python code with comprehensive typing, thorough unit tests, and effective mocking. # Background You are part of a development team working on a simple e-commerce application. The core part of the application is to handle orders and calculate discounts. The project is still in its early stages, and you are tasked to implement and thoroughly test the `Order` class. # Requirements 1. **Order Class Implementation**: - Create an `Order` class that has the following attributes: - `order_id`: a unique identifier for the order (string). - `items`: a list of item prices (float). - `discount`: a callable that takes the order total and returns the discounted amount (float). - Methods: - `calculate_total()`: Calculates the total amount of the order after applying the discount. - `add_item(item_price: float)`: Adds an item to the order with the given price. 2. **Typing**: - Use type hints for all method signatures and class attributes. 3. **Unit Testing**: - Write unit tests for the `Order` class using `unittest`. Ensure you test: - The initialization of the `Order` class. - The `add_item` method. - The `calculate_total` method with and without discounts. 4. **Mock Testing**: - Use `unittest.mock` to test the `calculate_total` method. Mock the `discount` callable to test its interaction within the `calculate_total` method. # Example ```python from typing import List, Callable class Order: def __init__(self, order_id: str, discount: Callable[[float], float]): self.order_id = order_id self.items: List[float] = [] self.discount = discount def add_item(self, item_price: float) -> None: self.items.append(item_price) def calculate_total(self) -> float: total = sum(self.items) return self.discount(total) ``` # Constraints - The `item_price` should always be a positive float value. - The `discount` callable should return a non-negative float value and should not exceed the original total. # Submission Submit: 1. The fully implemented `Order` class with type hints. 2. The unit tests implemented using the `unittest` framework. 3. The mock tests for the `calculate_total` method using `unittest.mock`. Ensure that all tests pass and verify the correctness of the class implementation. # Grading Criteria - Correctness and completeness of the `Order` class implementation. - Proper use of type hints. - Coverage and thoroughness of unit tests. - Effective use of mocking to test interactions. Good luck!","solution":"from typing import List, Callable class Order: def __init__(self, order_id: str, discount: Callable[[float], float]): self.order_id = order_id self.items: List[float] = [] self.discount = discount def add_item(self, item_price: float) -> None: self.items.append(item_price) def calculate_total(self) -> float: total = sum(self.items) return self.discount(total)"},{"question":"**Objective:** Demonstrate your understanding of the `binascii` module by implementing a pipeline that involves several of its functionalities. **Problem Statement:** You are required to implement a function `process_binary_data(data: bytes) -> dict` that processes a given bytes-like object `data` through multiple stages using the `binascii` module. The function should return a dictionary containing the results of each stage. **Stages:** 1. **Base64 Encoding:** - Use `b2a_base64` to encode the binary `data` to a base64-encoded ASCII string. - Key in the dictionary: `\\"base64_encoded\\"` 2. **Base64 Decoding:** - Use `a2b_base64` to decode the base64-encoded ASCII string back to binary. - Key in the dictionary: `\\"base64_decoded\\"` 3. **Hexadecimal Encoding:** - Use `b2a_hex` to convert the decoded binary data to its hexadecimal representation. - Key in the dictionary: `\\"hex_encoded\\"` 4. **Hexadecimal Decoding:** - Use `a2b_hex` to convert the hexadecimal string back to binary. - Key in the dictionary: `\\"hex_decoded\\"` 5. **UU Encoding:** - Use `b2a_uu` to convert the binary data to a UUencoded ASCII string. - Key in the dictionary: `\\"uu_encoded\\"` 6. **UU Decoding:** - Use `a2b_uu` to decode the UUencoded ASCII string back to binary. - Key in the dictionary: `\\"uu_decoded\\"` 7. **CRC32 Checksum:** - Use `crc32` to compute the CRC-32 checksum of the original binary `data`. - Key in the dictionary: `\\"crc32_checksum\\"` **Input:** - `data` (bytes): A bytes-like object to be processed. **Output:** - A dictionary with keys for each stage and their corresponding results. **Constraints:** - The length of the `data` should be a maximum of 45 bytes. - Ensure the appropriate handling of any potential exceptions raised during the processing stages. - Maintain the integrity of the original data through the conversions to validate the correctness. **Example:** ```python def process_binary_data(data: bytes) -> dict: pass # Your implementation here # Example usage: data = b\'example binary data\' result = process_binary_data(data) print(result) ``` # Expected Output Example: ```json { \\"base64_encoded\\": \\"ZXhhbXBsZSBiaW5hcnkgZGF0YQo=\\", \\"base64_decoded\\": b\'example binary data\', \\"hex_encoded\\": b\'6578616d706c652062696e6172792064617461\', \\"hex_decoded\\": b\'example binary data\', \\"uu_encoded\\": \\"begin 666 <data_path>n&<M*86UE;G1A8FQE<\'<n`nendn\\", \\"uu_decoded\\": b\'example binary data\', \\"crc32_checksum\\": 1503687417 } ``` Please ensure your implementation passes the provided example and handles edge cases appropriately.","solution":"import binascii def process_binary_data(data: bytes) -> dict: try: # Base64 Encoding base64_encoded = binascii.b2a_base64(data).strip().decode(\'ascii\') # Base64 Decoding base64_decoded = binascii.a2b_base64(base64_encoded) # Hexadecimal Encoding hex_encoded = binascii.b2a_hex(base64_decoded) # Hexadecimal Decoding hex_decoded = binascii.a2b_hex(hex_encoded) # UU Encoding uu_encoded = binascii.b2a_uu(hex_decoded).decode(\'ascii\') # UU Decoding uu_decoded = binascii.a2b_uu(uu_encoded) # CRC32 Checksum crc32_checksum = binascii.crc32(data) return { \\"base64_encoded\\": base64_encoded, \\"base64_decoded\\": base64_decoded, \\"hex_encoded\\": hex_encoded, \\"hex_decoded\\": hex_decoded, \\"uu_encoded\\": uu_encoded, \\"uu_decoded\\": uu_decoded, \\"crc32_checksum\\": crc32_checksum } except Exception as e: return {\\"error\\": str(e)}"},{"question":"Title: Implementing a Python Extension Function Objective: Your task is to demonstrate your understanding of Python\'s C API for parsing arguments and building values. Specifically, you will implement a Python extension function in Python that mimics the behavior of a C extension function using the detailed description provided. Problem Statement: You are required to implement a Python function `parse_and_build` that simulates parsing of arguments and building values similar to how `PyArg_ParseTuple()` and `Py_BuildValue()` work in Python\'s C API. Function Signature: ```python def parse_and_build(*args) -> dict: ``` Expected Input and Output: - **Input:** A tuple of arguments of various types. - **Output:** A dictionary containing: - `parsed_values`: a list of parsed values from the input arguments converted to appropriate types, following the descriptions from `PyArg_ParseTuple()`. - `built_value`: a Python object created from the parsed values, mimicking the behavior of `Py_BuildValue()`. Requirements: - You must handle different data types including integers, floats, strings, and boolean values. - The function should raise appropriate exceptions if the input arguments do not match the expected types. - Simulate memory management by ensuring that any conversions are cleaned up properly (use Python\'s memory management for abstraction). Constraints: - The input tuple may contain a mixture of types, including `int`, `float`, `str`, and `bool`. - Use simple format unit characters as described in the documentation (`i`, `f`, `s`, `p`). Example: ```python # Example Input args = (10, 3.14, \\"example\\", True) # Example Output { \\"parsed_values\\": [10, 3.14, \\"example\\", True], \\"built_value\\": (10, 3.14, \\"example\\", True) } ``` Additional Notes: - Focus on accurately parsing the input arguments and building the final value using Python constructs. - Ensure your function handles errors gracefully and returns appropriate error messages. - Do not use Python\'s `ctypes` or any external libraries; stay within the native Python API. Implement the function `parse_and_build` to meet the above requirements.","solution":"def parse_and_build(*args) -> dict: Parse a tuple of arguments of various types and simulate how `PyArg_ParseTuple` and `Py_BuildValue` behave. Args: args: A tuple of arguments of various types including integers, floats, strings, and boolean values. Returns: A dictionary containing two keys: - `parsed_values` which is a list of the parsed values. - `built_value` which is a Python object (such as a tuple) created from the parsed values. # Define a list for parsed values parsed_values = [] # Iterate over input arguments to parse and type-check them for arg in args: if isinstance(arg, int): parsed_values.append(arg) elif isinstance(arg, float): parsed_values.append(arg) elif isinstance(arg, str): parsed_values.append(arg) elif isinstance(arg, bool): parsed_values.append(arg) else: raise TypeError(f\\"Unsupported argument type: {type(arg)}\\") # Build value from parsed values (mimicking `Py_BuildValue`) built_value = tuple(parsed_values) # Return the result dictionary return { \\"parsed_values\\": parsed_values, \\"built_value\\": built_value }"},{"question":"# Distributed Training with PyTorch **Objective:** Implement a function that performs distributed training of a simple neural network in PyTorch. Your implementation should demonstrate the use of `torch.distributed` for initializing processes, performing collective operations, and synchronizing gradients. **Requirements:** 1. **Initialization:** Initialize the distributed processes using TCP or environment variable-based initialization. 2. **Model Definition:** Define a simple neural network model. 3. **DistributedDataParallel:** Use `torch.nn.parallel.DistributedDataParallel` for wrapping your model. 4. **Data Loading:** Implement a data loader that splits the dataset among different processes. 5. **Training Loop:** Write a training loop that performs forward, backward passes, and synchronization of gradients. 6. **Collective Operations:** Use collective operations like `all_reduce` to aggregate gradients or other required functionalities. **Input:** - **Rank (int):** The rank of the current process in the distributed group. - **World Size (int):** Total number of processes involved in the distributed training. - **Backend (str):** The backend to use for `torch.distributed` (e.g., \'gloo\', \'nccl\'). - **Epochs (int):** Number of epochs to train the model. - **Batch Size (int):** Size of each training batch. **Output:** - **None:** The function should train the model and print relevant training information such as the loss for each epoch. **Constraints:** - Assume you have access to a simple dataset like MNIST. - Assume that the code will be executed in an environment where PyTorch with `torch.distributed` support is properly installed. **Function Signature:** ```python import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, Dataset, DistributedSampler from torchvision import datasets, transforms def distributed_training(rank: int, world_size: int, backend: str, epochs: int, batch_size: int) -> None: # Step 1: Initialize the process group # Step 2: Define and distribute the model # Step 3: Set up the data loader and distribute the data # Step 4: Define the loss function and optimizer # Step 5: Implement the training loop # Step 6: Use all_reduce for gradient aggregation (if necessary) # Step 7: Clean up and destroy the process group pass ``` **Example Usage:** ```python if __name__ == \\"__main__\\": dist.init_process_group(backend=\'nccl\', init_method=\'tcp://127.0.0.1:2345\', rank=0, world_size=4) distributed_training(rank=0, world_size=4, backend=\'nccl\', epochs=5, batch_size=64) dist.destroy_process_group() ``` **Notes:** - Make sure to handle the initialization and cleanup of the distributed process group correctly. - Ensure that each process gets its own subset of the data for training. - Utilize `DDP` to parallelize the training and synchronize gradients. **Resources:** - [PyTorch Distributed Overview](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html) - [PyTorch Distributed Communication](https://pytorch.org/docs/stable/distributed.html)","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, Dataset, DistributedSampler from torchvision import datasets, transforms class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc = nn.Linear(28 * 28, 10) def forward(self, x): x = x.view(-1, 28 * 28) return self.fc(x) def distributed_training(rank: int, world_size: int, backend: str, epochs: int, batch_size: int) -> None: # Step 1: Initialize the process group dist.init_process_group(backend=backend, rank=rank, world_size=world_size) # Step 2: Define and distribute the model model = SimpleNet().to(rank) model = DDP(model, device_ids=[rank]) # Step 3: Set up the data loader and distribute the data transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) dataset = datasets.MNIST(\'.\', download=True, transform=transform) sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler) # Step 4: Define the loss function and optimizer criterion = nn.CrossEntropyLoss().to(rank) optimizer = optim.SGD(model.parameters(), lr=0.01) # Step 5: Implement the training loop for epoch in range(epochs): model.train() sampler.set_epoch(epoch) epoch_loss = 0.0 for batch_idx, (data, target) in enumerate(dataloader): data, target = data.to(rank), target.to(rank) optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() epoch_loss += loss.item() # Step 6: Use all_reduce for gradient aggregation (if necessary) dist.all_reduce(torch.tensor(epoch_loss, device=rank), op=dist.ReduceOp.SUM) epoch_loss /= world_size if rank == 0: print(f\\"Epoch {epoch} Loss: {epoch_loss:.4f}\\") # Step 7: Clean up and destroy the process group dist.destroy_process_group() if __name__ == \\"__main__\\": import os world_size = 2 backend = \'gloo\' epochs = 5 batch_size = 64 os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\' os.environ[\'MASTER_PORT\'] = \'29500\' for rank in range(world_size): dist.init_process_group(backend=backend, rank=rank, world_size=world_size) distributed_training(rank, world_size, backend, epochs, batch_size) dist.destroy_process_group()"},{"question":"# Question You are given a CSV file containing a log of events with their respective timestamps. Some timestamps have NaN values. Your task is to process this data using pandas to: 1. Convert the timestamps to `Datetime` objects and fill NaN values with the latest valid timestamp before the missing data. 2. Calculate the time difference between each consecutive event. 3. Provide summary statistics (mean, median, min, max) of the time difference between events. Implement the function `process_event_logs(filepath: str) -> pd.DataFrame` with the following input and output specifications. Input - `filepath` (str): the path to the CSV file. The CSV file contains at least one column named `timestamp` with datetime strings. Output - A pandas DataFrame with the following columns: 1. `timestamp`: the original timestamp values with NaNs filled. 2. `time_diff`: the time difference between consecutive events. 3. A summary row with mean, median, min, and max of the `time_diff` values. Constraints - The CSV file is well-formed and contains no duplicate timestamps. - NaN values in the timestamps column should be forward-filled using the latest valid timestamp. - The time difference (time_diff) should be calculated in seconds. Example Suppose the input CSV file (`events.csv`) content is: ```csv timestamp 2023-10-01 10:00:00 2023-10-01 10:05:00 NaN 2023-10-01 10:15:00 2023-10-01 10:20:00 ``` The function call `process_event_logs(\\"events.csv\\")` should return a DataFrame that looks like this: | timestamp | time_diff | |----------------------|-----------| | 2023-10-01 10:00:00 | NaT | | 2023-10-01 10:05:00 | 300.0 | | 2023-10-01 10:05:00 | 0.0 | | 2023-10-01 10:15:00 | 600.0 | | 2023-10-01 10:20:00 | 300.0 | | Mean | 300.0 | | Median | 300.0 | | Min | 0.0 | | Max | 600.0 | Function signature ```python import pandas as pd def process_event_logs(filepath: str) -> pd.DataFrame: pass ```","solution":"import pandas as pd def process_event_logs(filepath: str) -> pd.DataFrame: # Read the CSV file df = pd.read_csv(filepath, parse_dates=[\'timestamp\']) # Forward fill NaN values in the timestamp column df[\'timestamp\'] = df[\'timestamp\'].fillna(method=\'ffill\') # Calculate the time difference between consecutive timestamps df[\'time_diff\'] = df[\'timestamp\'].diff().dt.total_seconds() # Initialize the first time_diff as NaT (not a time) df.loc[0, \'time_diff\'] = pd.NaT # Calculate summary statistics mean_diff = df[\'time_diff\'][1:].mean() median_diff = df[\'time_diff\'][1:].median() min_diff = df[\'time_diff\'][1:].min() max_diff = df[\'time_diff\'][1:].max() # Append summary statistics to the DataFrame summary_df = pd.DataFrame({ \'timestamp\': [\'Mean\', \'Median\', \'Min\', \'Max\'], \'time_diff\': [mean_diff, median_diff, min_diff, max_diff] }) # Concatenate original DataFrame with the summary DataFrame result_df = pd.concat([df, summary_df], ignore_index=True) return result_df"},{"question":"# Coding Challenge: Enhanced Data Analysis using the Python `statistics` Module Problem Statement: You are given a dataset consisting of numerical values and you need to perform a comprehensive statistical analysis on the data. Your task includes implementing functions that calculate various statistical measures, sanitize the data by removing any `NaN` values, and perform linear regression analysis on the cleaned data. Function Implementations: 1. **Clean Data:** Implement a function `clean_data(data: list) -> list` that takes a list of numerical values and returns a new list with any `NaN` values removed. 2. **Calculate Statistics:** Implement a function `compute_statistics(clean_data: list) -> dict` that takes a list of cleaned data and returns a dictionary with the following keys and their corresponding calculated values: - `\\"mean\\"`: Arithmetic mean of the data. - `\\"median\\"`: Median of the data. - `\\"variance\\"`: Sample variance of the data. - `\\"stdev\\"`: Sample standard deviation of the data. 3. **Linear Regression**: Implement a function `linear_regression(x: list, y: list) -> tuple` that takes two lists of numerical values representing the independent variable `x` and dependent variable `y`, respectively, and returns a tuple containing the slope and intercept of the simple linear regression. Input and Output: - The input to `clean_data` and `compute_statistics` functions is a list of numerical values. The input to the `linear_regression` function are two lists of numerical values of equal length. - Outputs are as specified in the function implementations. Example: ```python data = [10, 15, float(\'NaN\'), 20, 25, 30, 35, 40, float(\'NaN\')] cleaned_data = clean_data(data) # Output: [10, 15, 20, 25, 30, 35, 40] statistics = compute_statistics(cleaned_data) # Output: {\'mean\': 25.0, \'median\': 25.0, \'variance\': 116.666, \'stdev\': 10.801} x = [1, 2, 3, 4, 5] y = [2, 4, 5, 4, 5] slope, intercept = linear_regression(x, y) # Output: (0.6, 2.2) ``` Constraints: - Do not use any third-party libraries such as NumPy or SciPy. - Ensure your solution handles edge cases, such as empty lists or lists with all `NaN` values. - The length of `x` and `y` for linear regression will always be at least 2 and both lists will be of equal length. Performance Requirements: - The functions should execute efficiently within a reasonable time frame for large datasets (e.g., up to 10,000 elements). Implement these functions to showcase your understanding of the `statistics` module and your ability to apply statistical concepts programmatically.","solution":"import statistics from typing import List, Tuple, Dict def clean_data(data: List[float]) -> List[float]: Cleans the data by removing NaN values. return [x for x in data if not isinstance(x, float) or not (x != x)] # NaN check using x != x def compute_statistics(clean_data: List[float]) -> Dict[str, float]: Computes and returns the statistical measures of the cleaned data. if not clean_data: return {\'mean\': float(\'nan\'), \'median\': float(\'nan\'), \'variance\': float(\'nan\'), \'stdev\': float(\'nan\')} stats = {} stats[\'mean\'] = statistics.mean(clean_data) stats[\'median\'] = statistics.median(clean_data) stats[\'variance\'] = statistics.variance(clean_data) stats[\'stdev\'] = statistics.stdev(clean_data) return stats def linear_regression(x: List[float], y: List[float]) -> Tuple[float, float]: Computes and returns the slope and intercept of simple linear regression. # mean of x and y x_mean = statistics.mean(x) y_mean = statistics.mean(y) # calculate slope (b1) numerator = sum((xi - x_mean) * (yi - y_mean) for xi, yi in zip(x, y)) denominator = sum((xi - x_mean) ** 2 for xi in x) slope = numerator / denominator # calculate intercept (b0) intercept = y_mean - slope * x_mean return slope, intercept"},{"question":"You are given a directory containing several Python scripts. One of these scripts is the primary entry script, and it imports functions from the other scripts to perform its tasks. Your task is to write a Python function `execute_directory_scripts(directory: str) -> dict` that: 1. Takes the path to the directory as input. 2. Executes the primary entry script using the `runpy` module. 3. Returns the globals dictionary resulting from executing the entry script. # Input - `directory` (str): A string representing the path to the directory containing the scripts. Assume this directory is valid and contains at least one `__main__.py` file which acts as the entry script. # Output - A dictionary containing the globals from the executed `__main__.py` script. # Constraints - The `__main__.py` script should reside directly in the given directory, alongside other modules it may import. - Avoid altering the `sys` module\'s state permanently. - Handle possible side effects and ensure thread-safety where necessary. # Example Usage Assuming the directory `/path/to/scripts` contains: - `__main__.py` - `module1.py` - `module2.py` Your function should locate and run `__main__.py`, then return its globals dictionary. ```python import runpy def execute_directory_scripts(directory: str) -> dict: # Implement function here. pass # Example usage globals_dict = execute_directory_scripts(\\"/path/to/scripts\\") print(globals_dict) ``` The return value should be the globals dictionary resulting from running `__main__.py`, representing the script\'s global state after execution.","solution":"import os import runpy def execute_directory_scripts(directory: str) -> dict: Executes the __main__.py script located in the given directory and returns its globals dictionary. Args: - directory (str): The path to the directory containing the scripts. Returns: - dict: The globals dictionary from the executed __main__.py script. main_script_path = os.path.join(directory, \'__main__.py\') if not os.path.isfile(main_script_path): raise FileNotFoundError(f\\"__main__.py not found in directory: {directory}\\") globals_dict = runpy.run_path(main_script_path) return globals_dict"},{"question":"# Question: You are given two datasets represented as 2D NumPy arrays: `X_train` and `X_test`. Your task is to implement a function `calculate_distances_and_kernels` that calculates both distance matrices and kernel matrices between these datasets using specified metrics. Function Signature ```python def calculate_distances_and_kernels( X_train: np.ndarray, X_test: np.ndarray, metrics: List[str] ) -> Dict[str, np.ndarray]: pass ``` Input - `X_train`: A 2D numpy array of shape `(n_train, num_features)`, where `n_train` is the number of training samples and `num_features` is the number of features. - `X_test`: A 2D numpy array of shape `(n_test, num_features)`, where `n_test` is the number of testing samples and `num_features` is the number of features. - `metrics`: A list of strings, where each string specifies a metric or kernel to use. The metrics can be: - For distance calculations: `\'euclidean\'`, `\'manhattan\'` - For kernel calculations: `\'linear\'`, `\'polynomial\'`, `\'rbf\'`, `\'cosine\'` Output - Returns a dictionary where: - The keys are the names of the metrics. - The values are numpy arrays representing the distance or kernel matrices between `X_train` and `X_test`. Example ```python import numpy as np X_train = np.array([[1, 2], [3, 4]]) X_test = np.array([[5, 6], [7, 8]]) metrics = [\'euclidean\', \'linear\'] result = calculate_distances_and_kernels(X_train, X_test, metrics) # Expected result # { # \'euclidean\': array([[5.657, 8.485], [2.828, 5.657]]), # \'linear\': array([[17, 23], [39, 53]]) # } ``` Constraints - You can assume that `X_train` and `X_test` will have the same number of features. - You need to handle cases where one or both of the lists are empty gracefully. - You can use functions from `sklearn.metrics.pairwise` library. Performance Requirements - The function should handle cases with tens of thousands of samples efficiently. - Time complexity should be considered, especially for large datasets. Notes - For the `polynomial` kernel, you can assume default parameters (degree=3, gamma=None, coef0=1). - For the `rbf` kernel, you can assume default `gamma=\'scale\'`.","solution":"import numpy as np from sklearn.metrics.pairwise import euclidean_distances, manhattan_distances from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, rbf_kernel, cosine_similarity from typing import List, Dict def calculate_distances_and_kernels( X_train: np.ndarray, X_test: np.ndarray, metrics: List[str] ) -> Dict[str, np.ndarray]: result = {} for metric in metrics: if metric == \'euclidean\': result[metric] = euclidean_distances(X_train, X_test) elif metric == \'manhattan\': result[metric] = manhattan_distances(X_train, X_test) elif metric == \'linear\': result[metric] = linear_kernel(X_train, X_test) elif metric == \'polynomial\': result[metric] = polynomial_kernel(X_train, X_test) elif metric == \'rbf\': result[metric] = rbf_kernel(X_train, X_test) elif metric == \'cosine\': result[metric] = cosine_similarity(X_train, X_test) else: raise ValueError(f\\"Unsupported metric: {metric}\\") return result"},{"question":"# Advanced Email Handling with Python You are tasked with designing a Python script that can do the following: 1. Create and send an email with both plain text and HTML content. 2. Attach a text file to the email. 3. After sending the email, receive it, extract the attachments, and save them to a specified directory. # Requirements: Function 1: send_email - **Input:** ```python send_email(sender: str, recipient: str, subject: str, plain_text: str, html_content: str, attachment_path: str, smtp_server: str = \'localhost\') ``` - **Outputs:** None - **Description:** This function should create an email with both plain text and HTML content. Attach the file located at `attachment_path` to the email. Use the provided `smtp_server` to send the email. Function 2: receive_email - **Input:** ```python receive_email(msg_file: str, save_directory: str) ``` - **Outputs:** None - **Description:** This function should read the email from a file (`msg_file` - represents the received email in binary format), parse its contents, and save any attachments into the specified `save_directory`. Ensure the directory is created if it does not exist. # Constraints: 1. Ensure the functions handle possible exceptions, such as file not found, connection errors, etc. 2. Use appropriate methods from the `email` and `smtplib` packages as demonstrated in the provided examples. 3. The solution should be efficient and well-documented. # Example Usage: ```python # Example email details sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" subject = \\"Test Email with Attachment\\" plain_text = \\"This is the plain text part of the email.\\" html_content = \\"<html><body><h1>This is the HTML part of the email.</h1><body></html>\\" attachment_path = \\"path/to/attachment.txt\\" smtp_server = \\"localhost\\" # Send the email send_email(sender, recipient, subject, plain_text, html_content, attachment_path, smtp_server) # Assume the email is saved to \'received_email.msg\' after being sent by some other system msg_file = \\"received_email.msg\\" save_directory = \\"extracted_attachments\\" # Receive the email and extract the attachment receive_email(msg_file, save_directory) ``` # Notes: - You can assume the SMTP server is set up and running on the localhost for testing purposes. - For testing, you might need to simulate the process of saving the sent email to a file which represents the received email.","solution":"import os import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.base import MIMEBase from email import encoders from email.parser import BytesParser def send_email(sender, recipient, subject, plain_text, html_content, attachment_path, smtp_server=\'localhost\'): try: # Create a multipart message msg = MIMEMultipart(\'alternative\') msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Add plain text and HTML parts part1 = MIMEText(plain_text, \'plain\') part2 = MIMEText(html_content, \'html\') msg.attach(part1) msg.attach(part2) # Attach the file with open(attachment_path, \\"rb\\") as attachment: part = MIMEBase(\\"application\\", \\"octet-stream\\") part.set_payload(attachment.read()) encoders.encode_base64(part) part.add_header( \\"Content-Disposition\\", f\\"attachment; filename= {os.path.basename(attachment_path)}\\", ) msg.attach(part) # Send the email with smtplib.SMTP(smtp_server) as server: server.sendmail(sender, recipient, msg.as_string()) except Exception as e: print(f\\"Failed to send email: {e}\\") def receive_email(msg_file, save_directory): try: if not os.path.exists(save_directory): os.makedirs(save_directory) with open(msg_file, \'rb\') as f: msg = BytesParser().parse(f) for part in msg.walk(): if part.get_content_maintype() == \'multipart\': continue if part.get(\'Content-Disposition\') is None: continue filename = part.get_filename() if filename: with open(os.path.join(save_directory, filename), \'wb\') as f: f.write(part.get_payload(decode=True)) except Exception as e: print(f\\"Failed to receive email: {e}\\")"},{"question":"# Question: You are tasked with developing a Python utility that uses the `http.cookiejar` module to manage cookies for a web-scraping application. The application should be able to: 1. Load cookies from a specified file (in Mozilla `cookies.txt` file format). 2. Save cookies to a specified file. 3. Set a custom policy to automatically reject cookies from a list of blocked domains. 4. Automatically add cookies to HTTP requests and extract cookies from HTTP responses. Implement a class `WebScraperCookies` with the following methods: - `__init__(self, filename, blocked_domains)`: Initialize the class with: - `filename`: Path to the file where cookies will be stored/loaded. - `blocked_domains`: List of domains to block for setting/returning cookies. - `load_cookies(self)`: Loads cookies from the file specified at initialization. - `save_cookies(self)`: Saves cookies to the file specified at initialization. - `make_request(self, url)`: Makes an HTTP GET request to the provided URL, using the loaded cookies and saving any new cookies received. Constraints: - Use the `MozillaCookieJar` and `DefaultCookiePolicy` classes from `http.cookiejar`. - Ensure the custom policy rejects cookies from blocked domains specified at initialization. - You may use the `urllib.request` module to make HTTP requests. Example: ```python # Example usage: scraper = WebScraperCookies(\'cookies.txt\', [\'blocked.com\', \'.ads.com\']) scraper.load_cookies() scraper.make_request(\'http://example.com\') scraper.save_cookies() ``` Expected Input/Output: - When `make_request` is called, it should print the HTTP response status code and any new cookies set by the server. Implementation: ```python import os import http.cookiejar import urllib.request class WebScraperCookies: def __init__(self, filename, blocked_domains): self.filename = filename policy = http.cookiejar.DefaultCookiePolicy( blocked_domains=blocked_domains ) self.cj = http.cookiejar.MozillaCookieJar(filename=filename, policy=policy) def load_cookies(self): if os.path.exists(self.filename): self.cj.load(ignore_discard=True, ignore_expires=True) else: print(f\\"{self.filename} not found. No cookies loaded.\\") def save_cookies(self): self.cj.save(ignore_discard=True, ignore_expires=True) def make_request(self, url): opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(self.cj)) response = opener.open(url) print(f\\"Response Status: {response.status}\\") for cookie in self.cj: print(f\\"New Cookie: {cookie.name}={cookie.value}\\") # Example usage: scraper = WebScraperCookies(\'cookies.txt\', [\'blocked.com\', \'.ads.com\']) scraper.load_cookies() scraper.make_request(\'http://example.com\') scraper.save_cookies() ```","solution":"import os import http.cookiejar import urllib.request class WebScraperCookies: def __init__(self, filename, blocked_domains): self.filename = filename policy = http.cookiejar.DefaultCookiePolicy( blocked_domains=blocked_domains ) self.cj = http.cookiejar.MozillaCookieJar(filename=self.filename, policy=policy) def load_cookies(self): if os.path.exists(self.filename): self.cj.load(ignore_discard=True, ignore_expires=True) else: print(f\\"{self.filename} not found. No cookies loaded.\\") def save_cookies(self): self.cj.save(ignore_discard=True, ignore_expires=True) def make_request(self, url): opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(self.cj)) response = opener.open(url) print(f\\"Response Status: {response.status}\\") for cookie in self.cj: print(f\\"New Cookie: {cookie.name}={cookie.value}\\") # Example usage: # scraper = WebScraperCookies(\'cookies.txt\', [\'blocked.com\', \'.ads.com\']) # scraper.load_cookies() # scraper.make_request(\'http://example.com\') # scraper.save_cookies()"},{"question":"# Custom Formatter Implementation **Objective**: Implement a custom string formatter that extends Python\'s `string.Formatter` class. This formatter should: 1. Support additional custom formatting types: - `U` for transforming text to uppercase. - `L` for transforming text to lowercase. - `T` for transforming text to title case. 2. Handle field names, alignments, and widths as demonstrated in the documentation. **Requirements**: 1. **Class Definition**: - Define a class `CustomFormatter` that inherits from `string.Formatter`. 2. **Methods to Override**: - Override the `format_field` method to handle the custom formatting types (`U`, `L`, `T`). 3. **Example Usage**: - The custom formatter should correctly format strings according to the specifications. **Function Signature**: ```python import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): # Define custom behavior here pass # Example usage: def custom_format_example(): formatter = CustomFormatter() formatted_string = formatter.format( \\"{0:U} loves {1:T} and {2:L}\\", \\"python\\", \\"COding\\", \\"LEARNING\\") print(formatted_string) # Expected: \\"PYTHON loves Coding and learning\\" custom_format_example() ``` **Input/Output**: - **Input**: The format string and values to format as demonstrated in `custom_format_example`. - **Output**: A formatted string that applies the custom formatting types correctly. **Constraints**: - Ensure the custom formatter handles text alignment and widths appropriately, similar to built-in formatting. - Raise appropriate errors for invalid formats or field values. # Your Task: - Implement the `CustomFormatter` class. - Ensure the example usage works as expected. - Handle edge cases and errors gracefully. **Grading Criteria**: - Correct implementation of the custom formatting types. - Proper handling of alignment and width specifications. - Clean, readable, and well-documented code. Good luck!","solution":"import string class CustomFormatter(string.Formatter): def format_field(self, value, format_spec): if format_spec.endswith(\'U\'): value = str(value).upper() format_spec = format_spec[:-1] elif format_spec.endswith(\'L\'): value = str(value).lower() format_spec = format_spec[:-1] elif format_spec.endswith(\'T\'): value = str(value).title() format_spec = format_spec[:-1] return super().format_field(value, format_spec) # Example usage: def custom_format_example(): formatter = CustomFormatter() formatted_string = formatter.format( \\"{0:U} loves {1:T} and {2:L}\\", \\"python\\", \\"COding\\", \\"LEARNING\\") print(formatted_string) # Expected: \\"PYTHON loves Coding and learning\\" custom_format_example()"},{"question":"# Coding Assessment: System Resource Management with the `resource` Module Objective: Your task is to implement a Python function that: 1. Retrieves the current resource limits for CPU time. 2. Sets a new resource limit for CPU time. 3. Retrieves resource usage statistics before and after consuming CPU resources to illustrate the changes. Requirements: 1. Implement the function `manage_cpu_limit_and_usage(new_cpu_limit)` that performs the following steps: - Retrieve and print the current soft and hard limits for `RLIMIT_CPU`. - Set a new soft limit for `RLIMIT_CPU` to the value provided by `new_cpu_limit`, but keep the hard limit unchanged. This operation should be successful only if `new_cpu_limit` is less than or equal to the hard limit. - Retrieve and print the updated resource limits for `RLIMIT_CPU`. - Retrieve and print resource usage statistics before and after performing a CPU-intensive task (e.g., a loop that performs a large number of operations). 2. If setting the new CPU limit fails, catch the exception and print an appropriate error message without halting the program. Function Signature: ```python import resource def manage_cpu_limit_and_usage(new_cpu_limit: int): pass ``` Example: ```python manage_cpu_limit_and_usage(2) ``` Expected Output: ``` Current CPU limits: soft=<current_soft_limit>, hard=<current_hard_limit> Updated CPU limits: soft=2, hard=<unchanged_hard_limit> Before consuming CPU resources: ru_utime=<time_used>, ru_stime=<time_system> After consuming CPU resources: ru_utime=<time_used>, ru_stime=<time_system> ``` Constraints: - The function should gracefully handle any ValueError or OSError that occurs due to invalid limits or system call failures. - The new CPU limit (`new_cpu_limit`) should be a positive integer representing seconds. Performance Requirements: - The function should efficiently handle the resource limit setting and the CPU-intensive task without causing the Python process to exceed the set limits. - Ensure proper resource handling and cleanup, especially when CPU limits are reached during the CPU-intensive task. Notes: - Explore the `resource` module\'s documentation to understand and use functions like `resource.getrlimit`, `resource.setrlimit`, `resource.getrusage`, and the constants like `resource.RLIMIT_CPU`. Good luck!","solution":"import resource import time def manage_cpu_limit_and_usage(new_cpu_limit: int): try: # Retrieve current CPU time limits soft_limit, hard_limit = resource.getrlimit(resource.RLIMIT_CPU) print(f\\"Current CPU limits: soft={soft_limit}, hard={hard_limit}\\") # Set new CPU soft limit if it\'s <= the hard limit if new_cpu_limit <= hard_limit: resource.setrlimit(resource.RLIMIT_CPU, (new_cpu_limit, hard_limit)) else: raise ValueError(f\\"New CPU limit {new_cpu_limit} exceeds the hard limit {hard_limit}\\") # Retrieve updated CPU time limits new_soft_limit, new_hard_limit = resource.getrlimit(resource.RLIMIT_CPU) print(f\\"Updated CPU limits: soft={new_soft_limit}, hard={new_hard_limit}\\") # Retrieve resource usage statistics before CPU operation usage_before = resource.getrusage(resource.RUSAGE_SELF) print(f\\"Before consuming CPU resources: ru_utime={usage_before.ru_utime}, ru_stime={usage_before.ru_stime}\\") # Perform a CPU-intensive task start_time = time.time() while time.time() - start_time < 1: # Run loop for 1 second pass # Retrieve resource usage statistics after CPU operation usage_after = resource.getrusage(resource.RUSAGE_SELF) print(f\\"After consuming CPU resources: ru_utime={usage_after.ru_utime}, ru_stime={usage_after.ru_stime}\\") except (ValueError, OSError) as e: print(f\\"Error: {e}\\")"},{"question":"Overview You are required to write functions that will handle creating and reading Apple property list (plist) files in both XML and binary formats using the `plistlib` module in Python. This task will help assess your understanding of file handling, serialization, and error management using this module. Task 1: Writing Plist Files Write a function `create_plist(input_dict, filename, fmt)` that takes a dictionary, a filename (string), and a format (`FMT_XML` or `FMT_BINARY`), and writes the dictionary to a plist file in the specified format. **Input:** - `input_dict`: A dictionary containing data to be stored in the plist file. The dictionary can have nested dictionaries and lists. - `filename`: A string representing the name of the file to which the plist data will be written. - `fmt`: The format in which to write the plist file. It should be either `plistlib.FMT_XML` or `plistlib.FMT_BINARY`. **Output:** - The function does not need to return anything but should create a file with the given `filename` containing the serialized plist data. **Constraints:** - Keys in dictionaries must be strings. - Values can be strings, integers, floats, booleans, tuples, lists, dictionaries, `bytes`, `bytearray`, or `datetime.datetime` objects. Task 2: Reading Plist Files Write a function `read_plist(filename)` that reads a plist file and returns the data as a dictionary. **Input:** - `filename`: A string representing the name of the file from which the plist data will be read. **Output:** - A dictionary containing the deserialized plist data. **Constraints:** - The file must exist, and its format should be either XML or binary (autodetection should be used). Example Usage ```python import plistlib import datetime data = { \\"name\\": \\"John Doe\\", \\"age\\": 30, \\"is_student\\": False, \\"courses\\": [\\"Math\\", \\"Physics\\", 101, 202.5], \\"graduation_date\\": datetime.datetime(2022, 6, 15), \\"attributes\\": { \\"height\\": 175.0, \\"weight\\": 70.5 } } create_plist(data, \'example.plist\', plistlib.FMT_XML) result = read_plist(\'example.plist\') print(result) ``` Constraints and Considerations: - Ensure error handling for invalid file formats and unsupported data types. - Validate the presence of the file in the `read_plist` function and handle cases when the file does not exist. You are expected to implement both functions `create_plist` and `read_plist` as described, ensuring they handle the types and structures specified in the examples.","solution":"import plistlib def create_plist(input_dict, filename, fmt): Creates a plist file in the specified format from a dictionary. :param input_dict: Dictionary to write to plist file. :param filename: The name of the file. :param fmt: The format of the plist file, either plistlib.FMT_XML or plistlib.FMT_BINARY. with open(filename, \\"wb\\") as f: plistlib.dump(input_dict, f, fmt=fmt) def read_plist(filename): Reads a plist file and returns the data as a dictionary. :param filename: The name of the file to read plist data from. :returns: A dictionary with the plist data. with open(filename, \\"rb\\") as f: return plistlib.load(f)"},{"question":"Coding Assessment Question: # Objective: Demonstrate your ability to measure and compare the performance of different implementations of the same functionality using the `timeit` module in Python. # Task: Given three different implementations of a function to calculate the factorial of a number, you need to use the `timeit` module to measure and compare their performance. # Function Implementations: 1. **Iterative Method**: ```python def factorial_iterative(n): result = 1 for i in range(2, n + 1): result *= i return result ``` 2. **Recursive Method**: ```python def factorial_recursive(n): if n == 0 or n == 1: return 1 return n * factorial_recursive(n - 1) ``` 3. **Using `math.factorial()`**: ```python import math def factorial_math(n): return math.factorial(n) ``` # Instructions: 1. Write a Python script that uses the `timeit` module to measure the execution time of each implementation for calculating the factorial of 1000. 2. Specifically, use the `timeit.timeit()` function with appropriate setup and statement parameters. 3. Execute each function 100000 times to get reliable measurements. 4. Output the execution time for each implementation in the format: ``` Iterative method: X.XXXXXXXX seconds Recursive method: Y.YYYYYYYY seconds Math module method: Z.ZZZZZZZZ seconds ``` 5. Ensure your script is self-contained and can be run without any modification. # Constraints: - Ensure that your script handles importing the necessary modules and defining the functions. - Use the default timer provided by `timeit`. # Example Output: ``` Iterative method: 0.12345678 seconds Recursive method: 1.23456789 seconds Math module method: 0.01234567 seconds ``` # Additional Information: Take note of any significant differences in execution time and consider why these differences might exist.","solution":"import timeit import math def factorial_iterative(n): result = 1 for i in range(2, n + 1): result *= i return result def factorial_recursive(n): if n == 0 or n == 1: return 1 return n * factorial_recursive(n - 1) def factorial_math(n): return math.factorial(n) def measure_time(): n = 1000 iterations = 100000 iterative_time = timeit.timeit( stmt=\\"factorial_iterative(n)\\", setup=\\"from __main__ import factorial_iterative, n\\", number=iterations ) recursive_time = timeit.timeit( stmt=\\"factorial_recursive(n)\\", setup=\\"from __main__ import factorial_recursive, n\\", number=iterations ) math_time = timeit.timeit( stmt=\\"factorial_math(n)\\", setup=\\"from __main__ import factorial_math, n\\", number=iterations ) print(f\\"Iterative method: {iterative_time:.8f} seconds\\") print(f\\"Recursive method: {recursive_time:.8f} seconds\\") print(f\\"Math module method: {math_time:.8f} seconds\\") if __name__ == \\"__main__\\": measure_time()"},{"question":"**Coding Assessment Question:** You have been provided with a dataset `penguins` which contains the following attributes: - `species`: Species name of the penguins (e.g., Adelie, Chinstrap, Gentoo) - `island`: Island name where the penguin was observed - `bill_length_mm`: Length of the bill in millimeters - `bill_depth_mm`: Depth of the bill in millimeters - `flipper_length_mm`: Length of the flipper in millimeters - `body_mass_g`: Body mass of the penguin in grams - `sex`: Sex of the penguin (Male, Female) Your task is to write a function that generates a swarm plot to visualize the body mass across different species, categorized by sex, and displayed in different facets for each island. The function should allow customization of the plot through the following optional parameters: 1. **palette**: The color palette to use for different sexes. 2. **point_size**: Size of the points in the plot. 3. **orient**: The orientation of the swarm plot (either \'v\' for vertical or \'h\' for horizontal). 4. **dodge**: A boolean indicating whether to separate the points for each sex (default should be True). # Input Format - `penguins`: A `pandas` DataFrame containing the penguins dataset. - `palette`: (Optional) A string or list specifying the color palette. - `point_size`: (Optional) An integer specifying the size of the points. - `orient`: (Optional) A string specifying the orientation of the plot. - `dodge`: (Optional) A boolean to indicate whether to dodge the points. # Output Format A Seaborn facet grid plot object displaying the swarm plot according to the specifications. # Constraints 1. The `palette` should be a valid seaborn or matplotlib palette name. 2. The `point_size` should be a positive integer. 3. The `orient` should be either \'v\' or \'h\'. 4. The `dodge` should be a boolean. # Example ```python import seaborn as sns import pandas as pd # Sample penguins dataset penguins = sns.load_dataset(\\"penguins\\") def plot_penguin_swarm(penguins, palette=\'deep\', point_size=5, orient=\'v\', dodge=True): g = sns.catplot( data=penguins, kind=\\"swarm\\", x=\\"species\\" if orient == \'v\' else \\"body_mass_g\\", y=\\"body_mass_g\\" if orient == \'v\' else \\"species\\", hue=\\"sex\\", col=\\"island\\", palette=palette, dodge=dodge, height=4, aspect=0.7 ).set_axis_labels(\\"Species\\" if orient == \'v\' else \\"Body Mass (g)\\", \\"Body Mass (g)\\" if orient == \'v\' else \\"Species\\") for ax in g.axes.flat: for label in ax.get_xticklabels(): label.set_rotation(45) return g plot = plot_penguin_swarm(penguins, palette=\'pastel\', point_size=6, orient=\'h\', dodge=False) plot.savefig(\'penguin_swarm_plot.png\') ``` # Notes 1. Ensure that the plot is clearly labeled with appropriate axis labels and rotated x-axis labels for better readability when necessary. 2. The function should be robust and handle edge cases such as an empty DataFrame or missing values gracefully.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_penguin_swarm(penguins, palette=\'deep\', point_size=5, orient=\'v\', dodge=True): Creates a swarm plot to visualize the body mass across different species, categorized by sex, and displayed in different facets for each island. Parameters: - penguins (DataFrame): The penguins dataset. - palette (str or list, optional): The color palette to use for different sexes. Default is \'deep\'. - point_size (int, optional): Size of the points in the plot. Default is 5. - orient (str, optional): The orientation of the swarm plot (\'v\' or \'h\'). Default is \'v\'. - dodge (bool, optional): Whether to separate the points for each sex. Default is True. Returns: - g (FacetGrid): A Seaborn facet grid plot object displaying the swarm plot. if penguins.empty: raise ValueError(\\"The dataframe is empty.\\") if orient not in [\'v\', \'h\']: raise ValueError(\\"Invalid value for orient. Use \'v\' for vertical or \'h\' for horizontal.\\") if not isinstance(point_size, int) or point_size <= 0: raise ValueError(\\"Point size must be a positive integer.\\") if not isinstance(dodge, bool): raise ValueError(\\"Dodge must be a boolean value.\\") g = sns.catplot( data=penguins, kind=\\"swarm\\", x=\\"species\\" if orient == \'v\' else \\"body_mass_g\\", y=\\"body_mass_g\\" if orient == \'v\' else \\"species\\", hue=\\"sex\\", col=\\"island\\", palette=palette, dodge=dodge, height=4, aspect=0.7, size=point_size ).set_axis_labels(\\"Species\\" if orient == \'v\' else \\"Body Mass (g)\\", \\"Body Mass (g)\\" if orient == \'v\' else \\"Species\\") for ax in g.axes.flat: for label in ax.get_xticklabels(): label.set_rotation(45) return g"},{"question":"Objective Implement a class `CustomSetManager` that manages both sets and frozensets, and provides methods to perform different operations based on the type of the sets. The class should make use of the functions described in the `set` and `frozenset` API documentation provided. Requirements 1. **Constructor and Attribute**: - The constructor should initialize an empty list `sets` to store sets and frozensets. 2. **Methods**: ```python def add_set(self, iterable=None, is_frozenset=False): Creates a new set or frozenset from the iterable and adds it to the sets list. If iterable is None, creates an empty set or frozenset. :param iterable: An iterable to create the set or frozenset from. :param is_frozenset: Boolean, if True creates a frozenset, otherwise a set. :return: None ``` ```python def add_element(self, index, element): Adds an element to the set at the specified index in the sets list. Must handle errors if attempted to add to a frozenset. :param index: Index of the set in sets list. :param element: Element to add. :return: None ``` ```python def remove_element(self, index, element): Removes an element from the set at the specified index in the sets list. If the set is not mutable (frozenset), raise a TypeError. :param index: Index of the set in sets list. :param element: Element to remove. :return: None ``` ```python def contains_element(self, index, element): Checks if the element is in the set at the specified index in the sets list. :param index: Index of the set in sets list. :param element: Element to check. :return: Boolean, True if element is in the set, otherwise False. ``` ```python def clear_set(self, index): Clears all elements from the set at the specified index. If the set is not mutable (frozenset), raise a TypeError. :param index: Index of the set in sets list. :return: None ``` ```python def size_of_set(self, index): Returns the size of the set at the specified index. :param index: Index of the set in sets list. :return: Size of the set. ``` Constraints - Make sure to handle all possible edge cases including attempting to modify frozensets. - Ensure proper error handling for unhashable elements and invalid indices. Example ```python # Example usage manager = CustomSetManager() manager.add_set(iterable=[1, 2, 3]) manager.add_set(iterable=[\\"a\\", \\"b\\", \\"c\\"], is_frozenset=True) print(manager.contains_element(0, 1)) # True print(manager.contains_element(1, \\"a\\")) # True manager.add_element(0, 4) print(manager.size_of_set(0)) # 4 # The following will raise a TypeError because frozensets are immutable. try: manager.add_element(1, \\"d\\") except TypeError as e: print(e) # \\"Cannot add elements to a frozenset.\\" ``` Submission Submit the implementation of the `CustomSetManager` class that meets the above requirements.","solution":"class CustomSetManager: def __init__(self): self.sets = [] def add_set(self, iterable=None, is_frozenset=False): if iterable is None: iterable = [] if is_frozenset: self.sets.append(frozenset(iterable)) else: self.sets.append(set(iterable)) def add_element(self, index, element): if index >= len(self.sets) or index < 0: raise IndexError(\\"Index out of range.\\") if isinstance(self.sets[index], frozenset): raise TypeError(\\"Cannot add elements to a frozenset.\\") self.sets[index].add(element) def remove_element(self, index, element): if index >= len(self.sets) or index < 0: raise IndexError(\\"Index out of range.\\") if isinstance(self.sets[index], frozenset): raise TypeError(\\"Cannot remove elements from a frozenset.\\") self.sets[index].remove(element) def contains_element(self, index, element): if index >= len(self.sets) or index < 0: raise IndexError(\\"Index out of range.\\") return element in self.sets[index] def clear_set(self, index): if index >= len(self.sets) or index < 0: raise IndexError(\\"Index out of range.\\") if isinstance(self.sets[index], frozenset): raise TypeError(\\"Cannot clear a frozenset.\\") self.sets[index].clear() def size_of_set(self, index): if index >= len(self.sets) or index < 0: raise IndexError(\\"Index out of range.\\") return len(self.sets[index])"},{"question":"# Advanced Python Coding Assessment: Date and Time Manipulation with `datetime` Overview In this assessment, you will create a program to handle complex date and time manipulations using Python\'s `datetime` module. The tasks involve working with both aware and naive datetime objects, performing arithmetic operations with dates and times, and creating custom time zone handling. Task Description 1. **Time Zone Class Implementation:** - Implement a custom time zone class `CustomTz`, subclassing `datetime.tzinfo`. - The `CustomTz` class should support two fixed offsets, `+05:30` for standard time and `+06:30` for daylight saving time. - Implement methods `utcoffset`, `dst`, `tzname`, and `fromutc` as necessary to handle the time zone correctly. ```python from datetime import tzinfo, timedelta, datetime class CustomTz(tzinfo): def utcoffset(self, dt): pass def dst(self, dt): pass def tzname(self, dt): pass def fromutc(self, dt): pass ``` 2. **Date Manipulation Functions:** - Write a function `add_days(initial_date, days_to_add)` that takes a `datetime.date` object and an integer, and returns a new date with the specified number of days added. ```python from datetime import date, timedelta def add_days(initial_date, days_to_add): pass ``` 3. **Aware and Naive Date Conversion:** - Write a function `convert_to_aware(naive_dt, tz_info)` that takes a naive `datetime` object and a `tzinfo` object and returns an aware `datetime` object. - Similarly, write a function `convert_to_naive(aware_dt)` that takes an aware `datetime` object and returns a naive `datetime` object. ```python from datetime import datetime def convert_to_aware(naive_dt, tz_info): pass def convert_to_naive(aware_dt): pass ``` 4. **Serialization and Deserialization:** - Write a function `serialize_datetime(dt_obj)` that takes a `datetime` object and returns its string representation in ISO 8601 format. - Write a function `deserialize_datetime(dt_str)` that takes an ISO 8601 string representation of a date and time, and returns a `datetime` object. ```python def serialize_datetime(dt_obj): pass def deserialize_datetime(dt_str): pass ``` Input and Output - Implement the functions according to the following specifications: - `CustomTz` class must correctly handle the time zone offsets and DST rules. - `add_days` must return a `datetime.date` object. - `convert_to_aware` must return an aware `datetime` object. - `convert_to_naive` must return a naive `datetime` object. - `serialize_datetime` must return a string. - `deserialize_datetime` must return a `datetime` object. Constraints - You may assume the inputs are correctly formatted and valid for the functions. - Utilize the built-in `datetime` library for all operations. External libraries or packages are not allowed. - Consider edge cases such as leap years, DST changes, etc., as appropriate for the functions specified. Example ```python # Example usage: from datetime import datetime, date # Custom timezone example custom_tz = CustomTz() aware_dt = datetime(2023, 10, 15, 12, 0, tzinfo=custom_tz) print(f\\"Aware datetime: {aware_dt}\\") # Add days example new_date = add_days(date(2023, 10, 15), 10) print(f\\"New Date: {new_date}\\") # Aware and Naive conversion naive_dt = datetime(2023, 10, 15, 12, 0) aware_dt = convert_to_aware(naive_dt, custom_tz) print(f\\"Converted to aware datetime: {aware_dt}\\") naive_again = convert_to_naive(aware_dt) print(f\\"Converted back to naive datetime: {naive_again}\\") # Serialization and Deserialization serialized_dt = serialize_datetime(aware_dt) print(f\\"Serialized datetime: {serialized_dt}\\") deserialized_dt = deserialize_datetime(serialized_dt) print(f\\"Deserialized datetime: {deserialized_dt}\\") ``` Ensure your implementation passes all these checks and adheres to the specifications given. Good luck!","solution":"from datetime import tzinfo, timedelta, datetime, date class CustomTz(tzinfo): def utcoffset(self, dt): if self.dst(dt) == timedelta(): return timedelta(hours=5, minutes=30) return timedelta(hours=6, minutes=30) def dst(self, dt): # Assuming DST starts from April 1st 00:00 to October 31st 23:59 dst_start = datetime(dt.year, 4, 1) dst_end = datetime(dt.year, 11, 1) - timedelta(seconds=1) if dst_start <= dt.replace(tzinfo=None) <= dst_end: return timedelta(hours=1) return timedelta() def tzname(self, dt): if self.dst(dt) == timedelta(): return \\"CustomTz+05:30\\" return \\"CustomTz+06:30\\" def fromutc(self, dt): dt = dt.replace(tzinfo=None) return dt + self.utcoffset(dt) def add_days(initial_date, days_to_add): return initial_date + timedelta(days=days_to_add) def convert_to_aware(naive_dt, tz_info): return naive_dt.replace(tzinfo=tz_info) def convert_to_naive(aware_dt): return aware_dt.replace(tzinfo=None) def serialize_datetime(dt_obj): return dt_obj.isoformat() def deserialize_datetime(dt_str): return datetime.fromisoformat(dt_str)"},{"question":"You are tasked with implementing a Python class to manage tuple operations in a way that mimics some of the low-level functionalities provided by the Python C API. You will implement a class `TupleManager` that provides methods to create, access, modify, and resize tuples. Class Definition ```python class TupleManager: def __init__(self, initial_elements=None): Initialize with an optional list of initial elements which will be converted to a tuple. If no initial elements are provided, initialize with an empty tuple. # Your code here def create_tuple(self, size): Create a new tuple of a given size with all elements initialized to None. # Your code here def pack_tuple(self, *elements): Pack elements into a new tuple and replace the current tuple with this new one. # Your code here def get_item(self, index): Retrieve an element from the current tuple at the specified index. If the index is out of bounds, raise an IndexError. # Your code here def set_item(self, index, value): Set the value of the element at the specified index of the current tuple. If the index is out of bounds, raise an IndexError. # Your code here def resize_tuple(self, new_size): Resize the current tuple to the new size. If the new size is larger, new elements should be None. If the new size is smaller, truncate the excess elements. # Your code here def get_size(self): Return the size of the current tuple. # Your code here # Usage Example tm = TupleManager([1, 2, 3]) tm.create_tuple(5) print(tm.get_size()) # Output: 5 tm.pack_tuple(7, 8) print(tm.get_size()) # Output: 2 print(tm.get_item(1)) # Output: 8 tm.set_item(1, 9) print(tm.get_item(1)) # Output: 9 tm.resize_tuple(4) print(tm.get_size()) # Output: 4 ``` Constraints - You must use Python version 3.10. - The `create_tuple`, `pack_tuple`, `get_item`, `set_item`, `resize_tuple`, and `get_size` methods should be properly implemented based on the described functionality. - The class should manage the state of the tuple internally. Evaluation Criteria - Correct implementation of all required methods. - Proper handling of edge cases (e.g., index out of bounds). - Efficient and readable code.","solution":"class TupleManager: def __init__(self, initial_elements=None): Initialize with an optional list of initial elements which will be converted to a tuple. If no initial elements are provided, initialize with an empty tuple. self.tuple = tuple(initial_elements) if initial_elements is not None else () def create_tuple(self, size): Create a new tuple of a given size with all elements initialized to None. self.tuple = tuple(None for _ in range(size)) def pack_tuple(self, *elements): Pack elements into a new tuple and replace the current tuple with this new one. self.tuple = tuple(elements) def get_item(self, index): Retrieve an element from the current tuple at the specified index. If the index is out of bounds, raise an IndexError. if index < 0 or index >= len(self.tuple): raise IndexError(\\"Index out of bounds\\") return self.tuple[index] def set_item(self, index, value): Set the value of the element at the specified index of the current tuple. If the index is out of bounds, raise an IndexError. if index < 0 or index >= len(self.tuple): raise IndexError(\\"Index out of bounds\\") temp_list = list(self.tuple) temp_list[index] = value self.tuple = tuple(temp_list) def resize_tuple(self, new_size): Resize the current tuple to the new size. If the new size is larger, new elements should be None. If the new size is smaller, truncate the excess elements. temp_list = list(self.tuple) if new_size > len(temp_list): temp_list.extend([None] * (new_size - len(temp_list))) else: temp_list = temp_list[:new_size] self.tuple = tuple(temp_list) def get_size(self): Return the size of the current tuple. return len(self.tuple)"},{"question":"Coding Assessment Question # Objective You are tasked with developing a script to manage and manipulate file paths and directories using the `pathlib` module, as well as performing some file operations. This will include creating temporary files, comparing directories, and organizing files based on patterns. # Problem Statement Write a Python program that performs the following tasks: 1. **Create a Temporary Directory**: - Use the `tempfile` module to create a temporary directory. - Inside this directory, create the following subdirectories: `dir1`, `dir2`. - In `dir1`, create two text files `file1.txt` and `file2.txt` with some sample content. - In `dir2`, create one text file `file2.txt` (with different content than `dir1/file2.txt`). 2. **Manipulate and Compare Paths**: - Using `pathlib`, print the absolute paths of all files and directories inside the temporary directory. - Compare the contents of `file2.txt` in both `dir1` and `dir2` using the `filecmp` module and print if they are identical or different. - Move `file1.txt` from `dir1` to `dir2` and print the new structure of `dir2`. 3. **Pattern Matching and Cleanup**: - Use the `glob` module to list all `.txt` files in the temporary directory. - Delete all text files inside `dir1` and `dir2`. # Constraints - Do not use any external libraries except those mentioned (`pathlib`, `tempfile`, `filecmp`, `glob`). - Ensure the temporary directory and its contents are cleaned up after the script finishes execution to avoid leaving any temporary files on the system. # Expected Function Definitions You are expected to implement the following functions: ```python def create_temp_structure(): Creates a temporary directory with specified subdirectories and files. Returns the path to the temporary directory. pass def manipulate_and_compare_paths(temp_dir): Prints the absolute paths of files and directories inside the temporary directory. Compares `file2.txt` in `dir1` and `dir2` and prints if they are identical. Moves `file1.txt` from `dir1` to `dir2` and prints the new structure of `dir2`. pass def pattern_match_and_cleanup(temp_dir): Lists all `.txt` files in the temporary directory using `glob`. Deletes all text files in `dir1` and `dir2`. pass def main(): Main function to orchestrate the creation, manipulation, and cleanup of the temporary directory structure. temp_dir = create_temp_structure() manipulate_and_compare_paths(temp_dir) pattern_match_and_cleanup(temp_dir) ``` # Example Output ``` Absolute paths of all files and directories: - /tmp/tmpabcd1234/dir1 - /tmp/tmpabcd1234/dir2 - /tmp/tmpabcd1234/dir1/file1.txt - /tmp/tmpabcd1234/dir1/file2.txt - /tmp/tmpabcd1234/dir2/file2.txt Comparison of dir1/file2.txt and dir2/file2.txt: Different New structure of dir2 after moving file1.txt: - /tmp/tmpabcd1234/dir2/file1.txt - /tmp/tmpabcd1234/dir2/file2.txt Listing all .txt files: - /tmp/tmpabcd1234/dir1/file2.txt - /tmp/tmpabcd1234/dir2/file1.txt - /tmp/tmpabcd1234/dir2/file2.txt Cleaning up temporary files... All text files deleted inside dir1 and dir2. ``` # Submission Submit the script `file_management.py` containing the implementations of the above functions.","solution":"import tempfile import os from pathlib import Path import filecmp import glob import shutil def create_temp_structure(): Creates a temporary directory with specified subdirectories and files. Returns the path to the temporary directory. temp_dir = tempfile.mkdtemp() dir1 = Path(temp_dir) / \\"dir1\\" dir2 = Path(temp_dir) / \\"dir2\\" dir1.mkdir() dir2.mkdir() (dir1 / \\"file1.txt\\").write_text(\\"This is file1 in dir1.\\") (dir1 / \\"file2.txt\\").write_text(\\"This is file2 in dir1.\\") (dir2 / \\"file2.txt\\").write_text(\\"This is file2 in dir2, different content.\\") return temp_dir def manipulate_and_compare_paths(temp_dir): Prints the absolute paths of files and directories inside the temporary directory. Compares `file2.txt` in `dir1` and `dir2` and prints if they are identical. Moves `file1.txt` from `dir1` to `dir2` and prints the new structure of `dir2`. temp_path = Path(temp_dir) print(\\"Absolute paths of all files and directories:\\") for path in temp_path.glob(\'**/*\'): print(f\\"- {path.resolve()}\\") file1_path = temp_path / \'dir1\' / \'file1.txt\' file2_path_dir1 = temp_path / \'dir1\' / \'file2.txt\' file2_path_dir2 = temp_path / \'dir2\' / \'file2.txt\' are_files_identical = filecmp.cmp(file2_path_dir1, file2_path_dir2, shallow=False) comparison_result = \\"identical\\" if are_files_identical else \\"different\\" print(f\\"nComparison of dir1/file2.txt and dir2/file2.txt: {comparison_result}\\") shutil.move(file1_path, temp_path / \'dir2\' / \'file1.txt\') print(\\"nNew structure of dir2 after moving file1.txt:\\") for path in (temp_path / \'dir2\').glob(\'*\'): print(f\\"- {path.resolve()}\\") def pattern_match_and_cleanup(temp_dir): Lists all `.txt` files in the temporary directory using `glob`. Deletes all text files in `dir1` and `dir2`. temp_path = Path(temp_dir) print(\\"nListing all .txt files:\\") for txt_file in temp_path.glob(\'**/*.txt\'): print(f\\"- {txt_file.resolve()}\\") for txt_file in temp_path.glob(\'**/*.txt\'): txt_file.unlink() print(\\"nCleaning up temporary files...\\") print(\\"All text files deleted inside dir1 and dir2.\\") def main(): Main function to orchestrate the creation, manipulation, and cleanup of the temporary directory structure. temp_dir = create_temp_structure() try: manipulate_and_compare_paths(temp_dir) pattern_match_and_cleanup(temp_dir) finally: shutil.rmtree(temp_dir) # Uncomment the following line to run the script # main()"},{"question":"Objective: The goal of this assessment is to evaluate your ability to work with the nullable Boolean data type in pandas, manipulate data using Boolean masks that include `NA` values, and implement logical operations considering Kleene Logic. Problem Description: You are provided with a DataFrame containing columns with Boolean values, but some of these values may be `NA`. Your task is to implement a function `process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame` that processes this DataFrame according to the following requirements: 1. **Replace NA Values in Given Columns**: - Replace `NA` values in specified columns with `False`. 2. **Conditional Logical Operations**: - Add a new column `\'result\'` that contains the result of logical operations (`&`, `|`, `^`) applied between two existing columns of the DataFrame. These logical operations should respect the Kleene logic for handling `NA` values. 3. **Performance Consideration**: - Ensure that newly added columns (if containing Boolean data) use the nullable Boolean dtype (`boolean`). Input: - `df` (pandas DataFrame): The input DataFrame containing at least two columns with nullable Boolean values. Output: - `pd.DataFrame`: A DataFrame that has: 1. The same as input DataFrame after replacing `NA` values with `False` for specified columns. 2. A new column `\'result\'` that contains the result of logical operations between two given columns, with proper handling of `NA` values according to Kleene logic. Constraints: - The DataFrame `df` will have at least two columns of Boolean values which may contain `NA`. - Column names of the DataFrame are known and fixed. - Performance of the DataFrame operations should be optimized for large datasets. Example: Given the following DataFrame `df`: ``` A B 0 True NA 1 False True 2 True False 3 NA False ``` For the specified columns `A` and `B`, your function should: 1. Replace `NA` with `False` in column `A`. 2. Add a new column `\'result\'` which contains the result of `A & B`. Expected output DataFrame: ``` A B result 0 True NA False 1 False True False 2 True False False 3 False False False ``` Implementation: ```python def process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Replace `NA` with `False` in specified columns df[\'A\'] = df[\'A\'].fillna(False) df[\'B\'] = df[\'B\'].fillna(False) # Perform logical `&` operation considering Kleene logic and add to a new column df[\'result\'] = df[\'A\'] & df[\'B\'] return df ``` This example implements handling `NA` values conversion and logical `&` operation. Your task is to expand this to include more general logical operations and processing steps as described. Note: Ensure to test the functions thoroughly with various edge cases.","solution":"import pandas as pd def process_boolean_dataframe(df: pd.DataFrame) -> pd.DataFrame: # Replace `NA` with `False` in specified columns df[\'A\'] = df[\'A\'].fillna(False) df[\'B\'] = df[\'B\'].fillna(False) # Perform logical operations considering Kleene logic and add to a new column # Logical AND df[\'result_and\'] = df[\'A\'] & df[\'B\'] # Logical OR df[\'result_or\'] = df[\'A\'] | df[\'B\'] # Logical XOR df[\'result_xor\'] = df[\'A\'] ^ df[\'B\'] return df"},{"question":"**Question:** You are to design a simple NNTP client that can perform the following tasks: 1. **Connect to an NNTP server** and print the server\'s welcome message. 2. **List all available newsgroups** on the server and save the list to a file named `newsgroups.txt`. 3. **Select a specific newsgroup** by its name. 4. **Fetch and print the subjects** of the last 15 articles in the selected newsgroup. # Expected Implementation 1. **Function** `connect_to_server`: - **Input**: `hostname` (str) - **Output**: NNTP connection object (nntplib.NNTP) - **Behavior**: Connects to the given NNTP server and returns the NNTP connection object. 2. **Function** `list_newsgroups`: - **Input**: NNTP connection object (nntplib.NNTP) - **Output**: None - **Behavior**: Fetches the list of newsgroups from the server and saves it to `newsgroups.txt`. 3. **Function** `select_newsgroup`: - **Input**: - NNTP connection object (nntplib.NNTP) - Newsgroup name (str) - **Output**: tuple containing response, article count, first article number, last article number, and newsgroup name. - **Behavior**: Selects the given newsgroup and returns its details. 4. **Function** `fetch_last_15_subjects`: - **Input**: - NNTP connection object (nntplib.NNTP) - Tuple containing response, article count, first article number, last article number, and newsgroup name. - **Output**: List of the last 15 subjects (str) - **Behavior**: Fetches and returns the subjects of the last 15 articles in the selected newsgroup. # Constraints - Your solution should handle possible exceptions, printing relevant error messages if a command fails. - Ensure the implementation uses the respective methods from `nntplib` as appropriate. # Example Usage ```python hostname = \'news.gmane.io\' newsgroup_name = \'gmane.comp.python.committers\' # Step 1: Connect to the server nntp_conn = connect_to_server(hostname) # Step 2: List newsgroups and save to \'newsgroups.txt\' list_newsgroups(nntp_conn) # Step 3: Select a specific newsgroup group_details = select_newsgroup(nntp_conn, newsgroup_name) # Step 4: Fetch and print the last 15 subjects subjects = fetch_last_15_subjects(nntp_conn, group_details) for subj in subjects: print(subj) # Step 5: Quit the connection nntp_conn.quit() ``` # Notes 1. Use the `over` method to fetch article subjects and the `decode_header` utility function to decode headers if necessary. 2. Handle exceptions such as `NNTPPermanentError` and `NNTPTemporaryError`. **Deliverables:** Provide the complete implementation of the functions `connect_to_server`, `list_newsgroups`, `select_newsgroup`, and `fetch_last_15_subjects`.","solution":"import nntplib import os def connect_to_server(hostname): Connect to an NNTP server and return the connection object. try: connection = nntplib.NNTP(hostname) print(f\\"Connected: {connection.getwelcome()}\\") return connection except (nntplib.NNTPPermanentError, nntplib.NNTPTemporaryError) as e: print(f\\"Error connecting to NNTP server: {e}\\") return None def list_newsgroups(nntp_conn): List all available newsgroups on the server and save the list to \'newsgroups.txt\'. try: response, groups = nntp_conn.list() with open(\'newsgroups.txt\', \'w\') as file: for group in groups: file.write(f\\"{group[0]}n\\") print(f\\"Newsgroups list saved to \'newsgroups.txt\'.\\") except Exception as e: print(f\\"Error listing newsgroups: {e}\\") def select_newsgroup(nntp_conn, newsgroup_name): Select a specific newsgroup by its name and return its details. try: response, article_count, first_article, last_article, group_name = nntp_conn.group(newsgroup_name) return response, article_count, first_article, last_article, group_name except nntplib.NNTPError as e: print(f\\"Error selecting newsgroup: {e}\\") return None def fetch_last_15_subjects(nntp_conn, group_details): Fetch the subjects of the last 15 articles in the selected newsgroup. subjects = [] if not group_details: return subjects response, article_count, first_article, last_article, group_name = group_details start = max(int(first_article), int(last_article) - 14) try: for article_num in range(start, int(last_article) + 1): resp, info = nntp_conn.head(article_num) for line in info.lines: if line.lower().startswith(b\'subject:\'): subjects.append(line.decode(\'utf-8\').replace(\'Subject: \', \'\')) return subjects except nntplib.NNTPError as e: print(f\\"Error fetching subjects: {e}\\") return subjects"},{"question":"Problem Statement You are required to design a system to manage a library\'s book inventory using the `dataclasses` module. Each book in the library has the following attributes: - `title`: The title of the book (string). - `author`: The author of the book (string). - `isbn`: The International Standard Book Number (string). - `pages`: The number of pages in the book (integer). - `copies`: The number of copies available in the library (integer, defaults to 1). - `categories`: A list of categories the book belongs to (e.g., Fiction, Science, History) with an empty list as the default value. You need to create a dataclass `Book` with appropriate type annotations and default values. Additionally, implement the following features: 1. **Custom Initialization**: Ensure that the `pages` attribute cannot be negative, and the `copies` attribute cannot be negative or zero. Raise a `ValueError` if either condition is violated. This check should be performed using the `__post_init__` method. 2. **Equality and Ordering**: The `Book` objects should be comparable by their `isbn`. Ensure that `==`, `<`, `<=`, `>`, `>=` work by default based on the `isbn` attribute. 3. **Custom Representation**: Customize the string representation (`__repr__`) of the `Book` object to display the title and author in the following format: `Book(\'The Great Gatsby\', \'F. Scott Fitzgerald\')`. 4. **Utility Functions**: - Implement a method to convert a `Book` object to a dictionary using the `asdict()` utility. - Implement a method to create a copy of a `Book` object with modified attributes using the `replace()` utility. Constraints - `isbn` should be a unique identifier for each `Book` object. - The `categories` attribute should be a list and can be empty or contain multiple category strings. Input - Instance creation of the `Book` class with appropriate attributes. - Method calls to demonstrate the various functionalities (custom initialization, equality, ordering, custom representation, dictionary conversion, and object replacement). Output - Demonstrate the creation of `Book` objects. - Illustrate the exception handling for invalid `pages` and `copies`. - Show comparison operations based on `isbn`. - Print the custom representation of `Book`. - Print the dictionary representation of the `Book` object. - Create and print a modified copy of a `Book` object. Example ```python from dataclasses import dataclass, field, asdict, replace @dataclass(order=True) class Book: title: str author: str isbn: str pages: int copies: int = 1 categories: list[str] = field(default_factory=list) def __post_init__(self): if self.pages < 0: raise ValueError(\\"Pages cannot be negative\\") if self.copies <= 0: raise ValueError(\\"Copies must be greater than zero\\") def __repr__(self): return f\\"Book(\'{self.title}\', \'{self.author}\')\\" # Example Usage: try: book1 = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"9780743273565\\", 180, 3, [\\"Fiction\\", \\"Classic\\"]) print(book1) book_dict = asdict(book1) print(book_dict) book_copy = replace(book1, copies=2) print(book_copy) # Should raise ValueError book_invalid = Book(\\"Invalid Book\\", \\"Unknown Author\\", \\"0000000000000\\", -10, 1) except ValueError as e: print(e) ```","solution":"from dataclasses import dataclass, field, asdict, replace @dataclass(order=True) class Book: title: str author: str isbn: str pages: int copies: int = 1 categories: list[str] = field(default_factory=list) def __post_init__(self): if self.pages < 0: raise ValueError(\\"Pages cannot be negative\\") if self.copies <= 0: raise ValueError(\\"Copies must be greater than zero\\") def __repr__(self): return f\\"Book(\'{self.title}\', \'{self.author}\')\\" # Example usage: try: book1 = Book(\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"9780743273565\\", 180, 3, [\\"Fiction\\", \\"Classic\\"]) print(book1) book_dict = asdict(book1) print(book_dict) book_copy = replace(book1, copies=2) print(book_copy) # Should raise ValueError book_invalid = Book(\\"Invalid Book\\", \\"Unknown Author\\", \\"0000000000000\\", -10, 1) except ValueError as e: print(e)"},{"question":"Objective In this assessment, you will demonstrate your understanding and proficiency with the `unittest.mock` library. Your task is to write unit tests for a hypothetical function using various mocking techniques provided by this library. Problem Statement You are given a function `process_file` which reads a file, processes the content, and saves the result back to another file. The function is part of a module named `file_processor`. Here is the implementation of `process_file`: ```python # file_processor.py def process_file(input_file_path: str, output_file_path: str): with open(input_file_path, \'r\') as input_file: content = input_file.read() # Simulate some processing on the content processed_content = content.upper() with open(output_file_path, \'w\') as output_file: output_file.write(processed_content) ``` Your task is to write a test suite for `process_file` using the `unittest` and `unittest.mock` libraries. Your tests should cover the following scenarios: 1. Verifying that the `open` function is called correctly with the appropriate file paths. 2. Ensuring that the content read from the input file is processed correctly. 3. Checking that the processed content is written to the output file. Requirements 1. Use the `unittest` framework for creating tests. 2. Use `unittest.mock` to mock the `open` function and verify its interactions. 3. Ensure your tests check the expected behavior using assertions. Constraints - Do not modify the implementation of `process_file`. - Ensure your test suite covers both the reading and writing operations. Input and Output - The function `process_file` takes two string arguments representing file paths: `input_file_path` and `output_file_path`. - The function reads the content from `input_file_path`, processes it by converting it to uppercase, and writes the processed content to `output_file_path`. # Example Here is a template for the test suite you need to complete: ```python import unittest from unittest.mock import mock_open, patch from file_processor import process_file class TestProcessFile(unittest.TestCase): @patch(\'file_processor.open\', new_callable=mock_open, read_data=\'hello world\') def test_process_file(self, mock_open): input_file_path = \'input.txt\' output_file_path = \'output.txt\' process_file(input_file_path, output_file_path) # Verify that the open function was called correctly mock_open.assert_any_call(input_file_path, \'r\') mock_open.assert_any_call(output_file_path, \'w\') # Get handles to the mocked file objects handles = mock_open() input_handle = handles.__enter__.return_value # Verify the content read from the input file input_handle.read.assert_called_once() self.assertEqual(input_handle.read(), \'hello world\') # Verify the content written to the output file output_handle = handles.__enter__.return_value output_handle.write.assert_called_once_with(\'HELLO WORLD\') if __name__ == \'__main__\': unittest.main() ``` Complete the test suite to fulfill all the requirements mentioned.","solution":"import os def process_file(input_file_path: str, output_file_path: str): with open(input_file_path, \'r\') as input_file: content = input_file.read() # Simulate some processing on the content processed_content = content.upper() with open(output_file_path, \'w\') as output_file: output_file.write(processed_content)"},{"question":"Objective Write a Python program that demonstrates the use of signal handlers, timers, and signal masks using the `signal` module. The program should simulate a scenario where a long-running computation can be interrupted and resumed using signals. Task 1. Define a signal handler for `SIGUSR1` that pauses the computation and displays a message: \\"Computation paused\\". 2. Define a signal handler for `SIGUSR2` that resumes the computation and displays a message: \\"Computation resumed\\". 3. Use a timer to send a `SIGALRM` signal after 10 seconds that terminates the program gracefully with a message: \\"Timer expired. Terminating program.\\" 4. Implement a function that performs a long-running computation (e.g., counting to a very high number) and test the signal handling by sending `SIGUSR1` and `SIGUSR2` signals from another thread. 5. Ensure that the signal handlers are only set in the main thread. Constraints - The computation should be able to pause and resume multiple times. - The program should terminate gracefully when the timer expires. - Use appropriate synchronization mechanisms if necessary to handle inter-thread communication. Function Signatures ```python import signal import threading import os import time # Signal handler for SIGUSR1 def pause_handler(signum, frame): pass # Define this function # Signal handler for SIGUSR2 def resume_handler(signum, frame): pass # Define this function # Signal handler for SIGALRM def timer_handler(signum, frame): pass # Define this function # Function to perform a long-running computation def long_running_computation(): pass # Define this function if __name__ == \\"__main__\\": # Main program setup pass # Implement signal handlers and timer setup here ``` # Example Output ```plaintext Computation paused Computation resumed Computation paused Computation resumed Timer expired. Terminating program. ``` Use the provided function signatures and ensure that the program meets the requirements specified above.","solution":"import signal import threading import time # Global variable to control computation state paused = threading.Event() # Signal handler for SIGUSR1 (Pause computation) def pause_handler(signum, frame): print(\\"Computation paused\\") paused.set() # Signal handler for SIGUSR2 (Resume computation) def resume_handler(signum, frame): print(\\"Computation resumed\\") paused.clear() # Signal handler for SIGALRM (Timer handler) def timer_handler(signum, frame): print(\\"Timer expired. Terminating program.\\") exit(0) # Function to perform a long-running computation def long_running_computation(): i = 0 while True: if paused.is_set(): time.sleep(0.1) # Wait while paused else: i += 1 if i % 1000000 == 0: # Print progress intermittently print(f\\"Count: {i}\\") if __name__ == \\"__main__\\": # Set up signal handlers signal.signal(signal.SIGUSR1, pause_handler) signal.signal(signal.SIGUSR2, resume_handler) signal.signal(signal.SIGALRM, timer_handler) # Set an alarm for 10 seconds signal.alarm(10) # Start long-running computation long_running_computation()"},{"question":"You are required to implement a simple module tracking utility in PyTorch which can determine the current position of the execution inside a nested `torch.nn.Module` hierarchy. # Task 1. Implement a class `SimpleModuleTracker` which will monitor the current module being executed. 2. The `SimpleModuleTracker` should provide the following functionalities: - Push a module onto the tracker stack when entering a module. - Pop a module from the tracker stack when leaving a module. - Retrieve the current module name or path within the module hierarchy. # Specifications Class: `SimpleModuleTracker` **Methods**: - `enter_module(module_name: str) -> None`: Push a module name onto the stack. - `exit_module() -> None`: Pop a module name from the stack. - `get_current_module_path() -> str`: Retrieve the current module path (a string representing the hierarchy). **Input**: - `enter_module` takes a `module_name` (string) which represents the name of the module being entered. - `exit_module` and `get_current_module_path` do not take any input. **Output**: - `enter_module` and `exit_module` do not return any output. - `get_current_module_path` returns a string representing the current path in the module hierarchy, where nested modules are represented by a `.` (dot) separator. **Constraints**: - Do not use any in-built or external libraries for stack operations; implement the stack manually. - Assume module names are unique. - You are not required to handle multi-threaded scenarios. # Example ```python tracker = SimpleModuleTracker() tracker.enter_module(\\"ModuleA\\") tracker.enter_module(\\"ModuleB\\") print(tracker.get_current_module_path()) # Output: \\"ModuleA.ModuleB\\" tracker.exit_module() print(tracker.get_current_module_path()) # Output: \\"ModuleA\\" tracker.exit_module() print(tracker.get_current_module_path()) # Output: \\"\\" ``` Implement the `SimpleModuleTracker` class below: ```python class SimpleModuleTracker: def __init__(self): self.stack = [] def enter_module(self, module_name: str) -> None: self.stack.append(module_name) def exit_module(self) -> None: if self.stack: self.stack.pop() def get_current_module_path(self) -> str: return \\".\\".join(self.stack) ```","solution":"class SimpleModuleTracker: def __init__(self): self.stack = [] def enter_module(self, module_name: str) -> None: self.stack.append(module_name) def exit_module(self) -> None: if self.stack: self.stack.pop() def get_current_module_path(self) -> str: return \\".\\".join(self.stack)"},{"question":"**Coding Assessment Question:** # Task Create a Python script that takes a directory path as input and performs the following actions using the `tarfile` module: 1. **Create a tar archive**: Compress the input directory into a tar archive using gzip compression. The tar archive should be named `archive.tar.gz`. 2. **List Contents**: Print the list of files contained in `archive.tar.gz`. 3. **Extract Specific Files**: Extract only the `.txt` files from `archive.tar.gz` to a directory named `extracted_txt_files`. 4. **Modify and Re-archive**: Add a filter to modify all file names in the tar archive by prefixing them with `modified_`. Re-archive the modified files in a new tar archive named `modified_archive.tar.gz`. # Input - A directory path containing multiple files and subdirectories. # Output - Create `archive.tar.gz` from the input directory. - Print the list of files contained in `archive.tar.gz`. - Extract all `.txt` files to the `extracted_txt_files` directory. - Create `modified_archive.tar.gz` with all file names prefixed with `modified_`. # Constraints - The script should handle large directories efficiently. - Proper error handling and logging should be implemented. # Example Input: ``` /path/to/directory ``` Expected output/actions: 1. A file named `archive.tar.gz` is created, containing the compressed contents of the specified directory. 2. A list of files within `archive.tar.gz` is printed to the console. 3. All `.txt` files from `archive.tar.gz` are extracted to a new directory named `extracted_txt_files`. 4. A file named `modified_archive.tar.gz` is created, where each file name within the archive is prefixed with `modified_`. # Notes - Ensure to use proper context managers (`with` statements) for file operations. - Implement the filters and extraction operations as specified in the documentation to ensure security and correctness. ```python import os import tarfile def create_archive(input_dir): with tarfile.open(\'archive.tar.gz\', \'w:gz\') as tar: tar.add(input_dir, arcname=os.path.basename(input_dir)) def list_contents(tar_filename): with tarfile.open(tar_filename, \'r:gz\') as tar: for member in tar.getmembers(): print(member.name) def extract_txt_files(tar_filename, output_dir): os.makedirs(output_dir, exist_ok=True) with tarfile.open(tar_filename, \'r:gz\') as tar: members = [m for m in tar.getmembers() if m.name.endswith(\'.txt\')] tar.extractall(path=output_dir, members=members) def modify_and_rearchive(tar_filename): with tarfile.open(tar_filename, \'r:gz\') as tar: with tarfile.open(\'modified_archive.tar.gz\', \'w:gz\') as new_tar: for member in tar.getmembers(): member.name = f\'modified_{member.name}\' new_tar.addfile(member, tar.extractfile(member)) def main(input_dir): create_archive(input_dir) print(\\"Contents of archive:\\") list_contents(\'archive.tar.gz\') extract_txt_files(\'archive.tar.gz\', \'extracted_txt_files\') modify_and_rearchive(\'archive.tar.gz\') print(\\"Archive modified and re-archived as \'modified_archive.tar.gz\'\\") if __name__ == \\"__main__\\": main(\'/path/to/directory\') ```","solution":"import os import tarfile def create_archive(input_dir): Compress the input directory into a tar archive using gzip compression. The tar archive will be named `archive.tar.gz`. with tarfile.open(\'archive.tar.gz\', \'w:gz\') as tar: tar.add(input_dir, arcname=os.path.basename(input_dir)) def list_contents(tar_filename): Print the list of files contained in the tar archive. with tarfile.open(tar_filename, \'r:gz\') as tar: for member in tar.getmembers(): print(member.name) def extract_txt_files(tar_filename, output_dir): Extract only the `.txt` files from the tar archive to the specified directory. os.makedirs(output_dir, exist_ok=True) with tarfile.open(tar_filename, \'r:gz\') as tar: members = [m for m in tar.getmembers() if m.name.endswith(\'.txt\')] tar.extractall(path=output_dir, members=members) def modify_and_rearchive(tar_filename): Add a filter to modify all file names in the tar archive by prefixing them with `modified_`. Re-archive the modified files in a new tar archive named `modified_archive.tar.gz`. with tarfile.open(tar_filename, \'r:gz\') as tar: with tarfile.open(\'modified_archive.tar.gz\', \'w:gz\') as new_tar: for member in tar.getmembers(): member.name = f\'modified_{member.name}\' new_tar.addfile(member, tar.extractfile(member)) def main(input_dir): create_archive(input_dir) print(\\"Contents of archive:\\") list_contents(\'archive.tar.gz\') extract_txt_files(\'archive.tar.gz\', \'extracted_txt_files\') modify_and_rearchive(\'archive.tar.gz\') print(\\"Archive modified and re-archived as \'modified_archive.tar.gz\'\\") if __name__ == \\"__main__\\": # Replace this with the actual path you want to test. main(\'/path/to/directory\')"},{"question":"Abstract Base Classes in Python Your task is to implement an abstract base class for a custom iterable and define a concrete class that adheres to this abstract base class. You will also need to register an existing class as a virtual subclass of your abstract base class. # Requirements 1. Define an abstract base class called `CustomIterable` using the `abc` module, with the following methods: - `__iter__`: An abstract method that should return an iterator. - `add`: An abstract method that should add an element to the collection. - `remove`: An abstract method that should remove an element from the collection. 2. Define a concrete class called `ConcreteIterable` that inherits from `CustomIterable` and implements the required methods: - The class should maintain an internal list to store elements. - The `__iter__` method should return an iterator for this internal list. - The `add` method should add an element to the internal list. - The `remove` method should remove an element from the internal list. 3. Register the built-in `list` class as a virtual subclass of `CustomIterable` using the `register` function. # Input and Output - **Input:** There is no input to the classes directly. Instead, interaction with the classes will demonstrate their functionality. - **Output:** Appropriate responses to method calls: - `__iter__`: Returns an iterator. - `add`: Adds an element. - `remove`: Removes an element. # Example Usage ```python from abc import ABC, abstractmethod class CustomIterable(ABC): @abstractmethod def __iter__(self): pass @abstractmethod def add(self, element): pass @abstractmethod def remove(self, element): pass class ConcreteIterable(CustomIterable): def __init__(self): self.elements = [] def __iter__(self): return iter(self.elements) def add(self, element): self.elements.append(element) def remove(self, element): self.elements.remove(element) # Register list as a virtual subclass CustomIterable.register(list) # Testing ConcreteIterable c = ConcreteIterable() c.add(1) c.add(2) print(list(c)) # Output: [1, 2] c.remove(1) print(list(c)) # Output: [2] # Testing list as a virtual subclass l = [] print(isinstance(l, CustomIterable)) # Output: True ``` # Constraints - Use the `abc` module to define abstract base classes and methods. - Ensure that the concrete class adheres to the abstract base class by implementing all its abstract methods. # Notes - You are not allowed to use other third-party packages. - Ensure that your code follows PEP 8 coding standards.","solution":"from abc import ABC, abstractmethod class CustomIterable(ABC): @abstractmethod def __iter__(self): pass @abstractmethod def add(self, element): pass @abstractmethod def remove(self, element): pass class ConcreteIterable(CustomIterable): def __init__(self): self.elements = [] def __iter__(self): return iter(self.elements) def add(self, element): self.elements.append(element) def remove(self, element): self.elements.remove(element) # Register list as a virtual subclass CustomIterable.register(list)"},{"question":"Advanced IP Address and Network Manipulation **Objective**: Demonstrate an understanding of the `ipaddress` module by implementing functions that create, manipulate, and inspect IP addresses, networks, and interfaces. **Task**: You are tasked with implementing three functions using Python\'s `ipaddress` module. The functions must operate based on the requirements given below. 1. **`create_addresses(addresses: list) -> list`**: - **Input**: A list of strings or integers representing IP addresses. - **Output**: A list of `ipaddress.IPv4Address` and `ipaddress.IPv6Address` objects. - **Requirements**: - If an address is a string, determine whether it is IPv4 or IPv6 and create the corresponding object. - If an address is an integer, automatically determine the IP version and create the corresponding object. 2. **`create_network(network_address: str, strict: bool = True) -> ipaddress.IPv4Network or ipaddress.IPv6Network`**: - **Input**: A network address in CIDR notation (e.g., \\"192.0.2.0/24\\") and an optional `strict` flag. - **Output**: An `ipaddress.IPv4Network` or `ipaddress.IPv6Network` object. - **Requirements**: - Create and return a network object based on the provided network address. - If `strict` is `False`, adjust the host bits to zero. 3. **`inspect_network(network: ipaddress.IPv4Network or ipaddress.IPv6Network) -> dict`**: - **Input**: An `ipaddress.IPv4Network` or `ipaddress.IPv6Network` object. - **Output**: A dictionary containing the following information about the network: - `version`: IP version (4 or 6). - `num_addresses`: Number of addresses in the network. - `netmask`: The netmask of the network as a string. - `hostmask`: The hostmask of the network as a string. - `usable_addresses`: A list of usable addresses in the network in string format. **Constraints**: - You must use the `ipaddress` module only. - Handle any potential errors gracefully and raise appropriate exceptions with clear messages. - Ensure the functions are efficient and handle large IPv6 networks without performance issues. # Function Signatures ```python import ipaddress def create_addresses(addresses: list) -> list: pass def create_network(network_address: str, strict: bool = True) -> ipaddress.IPv4Network or ipaddress.IPv6Network: pass def inspect_network(network: ipaddress.IPv4Network or ipaddress.IPv6Network) -> dict: pass ``` # Example Usage ```python addresses = [\'192.0.2.1\', \'2001:db8::1\', 3221225985] address_objects = create_addresses(addresses) print(address_objects) # Output: [IPv4Address(\'192.0.2.1\'), IPv6Address(\'2001:db8::1\'), IPv4Address(\'192.0.2.1\')] network = create_network(\'192.0.2.1/24\', strict=False) print(network) # Output: IPv4Network(\'192.0.2.0/24\') network_info = inspect_network(network) print(network_info) # Output: { # \'version\': 4, # \'num_addresses\': 256, # \'netmask\': \'255.255.255.0\', # \'hostmask\': \'0.0.0.255\', # \'usable_addresses\': [\'192.0.2.1\', \'192.0.2.2\', ..., \'192.0.2.254\'] # } ``` **Notes**: - Make sure to handle both IPv4 and IPv6 addresses correctly. - The `inspect_network` function should return a complete list of usable addresses, which excludes the network address and the broadcast address for IPv4 networks.","solution":"import ipaddress def create_addresses(addresses): Creates a list of IP address objects from a list of strings or integers. Parameters: addresses (list): A list of strings or integers representing IP addresses. Returns: list: A list of ipaddress.IPv4Address and ipaddress.IPv6Address objects. ip_objects = [] for addr in addresses: if isinstance(addr, int): # Automatically determine if the address is IPv4 or IPv6 try: ip_objects.append(ipaddress.ip_address(addr)) except ValueError as e: raise ValueError(f\\"Invalid integer for IP address: {addr}\\") from e elif isinstance(addr, str): try: ip_objects.append(ipaddress.ip_address(addr)) except ValueError as e: raise ValueError(f\\"Invalid IP address string: {addr}\\") from e else: raise TypeError(f\\"Address must be a string or integer, got: {type(addr)}\\") return ip_objects def create_network(network_address, strict=True): Creates a network object from a CIDR notation string. Parameters: network_address (str): The network address in CIDR notation (e.g., \\"192.0.2.0/24\\"). strict (bool): Whether to enforce strict subnet rules. Returns: ipaddress.IPv4Network or ipaddress.IPv6Network: A network object. try: return ipaddress.ip_network(network_address, strict=strict) except ValueError as e: raise ValueError(f\\"Invalid network address or CIDR notation: {network_address}\\") from e def inspect_network(network): Inspects a network object and returns its properties. Parameters: network (ipaddress.IPv4Network or ipaddress.IPv6Network): A network object. Returns: dict: A dictionary containing information about the network. if not isinstance(network, (ipaddress.IPv4Network, ipaddress.IPv6Network)): raise TypeError(f\\"Network must be an instance of IPv4Network or IPv6Network, got: {type(network)}\\") network_info = { \'version\': network.version, \'num_addresses\': network.num_addresses, \'netmask\': str(network.netmask), \'hostmask\': str(network.hostmask), \'usable_addresses\': [] } if isinstance(network, ipaddress.IPv4Network): # For IPv4, exclude network and broadcast addresses network_info[\'usable_addresses\'] = [str(ip) for ip in network.hosts()] elif isinstance(network, ipaddress.IPv6Network): # For IPv6, use all addresses within the network as usable addresses network_info[\'usable_addresses\'] = [str(ip) for ip in network] return network_info"},{"question":"# Garbage Collection Control and Debugging in Python In this exercise, you are required to write functions using Python\'s `gc` module to demonstrate your understanding of garbage collection processes, debugging, and monitoring performance. Task 1: Managing Garbage Collection - **Function Name**: manage_gc - **Description**: Implement a function `manage_gc` that takes a boolean argument `enable` and performs the following: - If `enable` is `True`, enable automatic garbage collection. - If `enable` is `False`, disable automatic garbage collection. - Return the current status of automatic garbage collection (either `True` or `False`). ```python def manage_gc(enable: bool) -> bool: # Implement function here pass ``` Task 2: Collecting and Inspecting Unreachable Objects - **Function Name**: force_collect - **Description**: Implement a function `force_collect` that forces a full collection, and retrieves the number of unreachable objects found and the list of objects found in `gc.garbage`. - Return a tuple with the number of unreachable objects and the `gc.garbage` list. ```python def force_collect() -> tuple[int, list]: # Implement function here pass ``` Task 3: Debugging Garbage Collection - **Function Name**: configure_debug - **Description**: Implement a function `configure_debug` that takes an integer `flags` and sets the debugging flags accordingly. - Return the currently set debugging flags. ```python def configure_debug(flags: int) -> int: # Implement function here pass ``` Task 4: Threshold Configuration - **Function Name**: set_gc_threshold - **Description**: Implement a function `set_gc_threshold` that takes three integers `threshold0`, `threshold1`, and `threshold2` to set the thresholds for garbage collection. - Return a tuple with the new thresholds. ```python def set_gc_threshold(threshold0: int, threshold1: int, threshold2:int) -> tuple: # Implement function here pass ``` Task 5: Handling Garbage Collection Callbacks - **Function Name**: register_callbacks - **Description**: Implement a function `register_callbacks` that registers two callback functions to `gc.callbacks`, one to be called at the start and one at the stop of the garbage collection process. - The start callback should print \\"Garbage collection started.\\" - The stop callback should print \\"Garbage collection stopped.\\" ```python def register_callbacks(start_callback, stop_callback): # Implement function here pass ``` Example Usage: ```python import gc # Example usage of manage_gc print(manage_gc(True)) # Enables garbage collection, expected output: True print(manage_gc(False)) # Disables garbage collection, expected output: False # Example usage of force_collect print(force_collect()) # Forces collection and returns number of unreachable objects and list of garbage # Example usage of configure_debug print(configure_debug(gc.DEBUG_STATS)) # Sets DEBUG_STATS, expected output: Some integer representation # Example usage of set_gc_threshold print(set_gc_threshold(700, 10, 10)) # Sets thresholds and returns new thresholds # Example usage of register_callbacks def start_cb(phase, info): print(\\"Garbage Collector started:\\", phase, info) def stop_cb(phase, info): print(\\"Garbage Collector stopped:\\", phase, info) register_callbacks(start_cb, stop_cb) ``` This assessment will test your understanding of Python\'s garbage collection module, including enabling/disabling the collector, forcing collection cycles, debugging, manipulating thresholds, and using callbacks effectively.","solution":"import gc def manage_gc(enable: bool) -> bool: if enable: gc.enable() else: gc.disable() return gc.isenabled() def force_collect() -> tuple[int, list]: gc.collect() return len(gc.garbage), gc.garbage def configure_debug(flags: int) -> int: gc.set_debug(flags) return gc.get_debug() def set_gc_threshold(threshold0: int, threshold1: int, threshold2: int) -> tuple: gc.set_threshold(threshold0, threshold1, threshold2) return gc.get_threshold() def register_callbacks(start_callback, stop_callback): gc.callbacks.append(start_callback) gc.callbacks.append(stop_callback)"},{"question":"# **URL Parsing and Construction Challenge** Objective: This assessment will evaluate your ability to work with the Python `urllib.parse` module to parse URLs into components, manipulate the components, and construct URLs from them. You will also be required to safely handle quoting and unquoting of URL components. Problem Statement: You are given a list of URLs. Your task is to perform the following steps on each URL and return the results as a list of modified URLs: 1. Parse each URL into its components using the `urlparse()` function. 2. Modify the query component: - If the query contains the parameter `status` with the value `inactive`, change its value to `active`. - If the query does not contain the parameter `status`, add `status=active` to the query. 3. Reconstruct the modified URL from its components. 4. Ensure that all components are properly quoted. Function Signature: ```python from typing import List from urllib.parse import urlparse, parse_qs, urlencode, urlunparse def modify_urls(urls: List[str]) -> List[str]: pass ``` Input: - `urls` (List[str]): A list of URLs as strings. Output: - (List[str]): A list of modified URLs as strings. Constraints: - URLs will be valid and adherence to RFC 3986. - The input list will contain between 1 and 1000 URLs. - Each URL string will have a length between 1 and 2000 characters. Example: ```python urls = [ \\"http://example.com/path/to/resource?name=john&status=inactive\\", \\"https://example.org/resource\\" ] result = modify_urls(urls) print(result) # Output: # [ # \\"http://example.com/path/to/resource?name=john&status=active\\", # \\"https://example.org/resource?status=active\\" # ] ``` Implementation Details: - Use `urlparse()` to parse the URL into its components. - Use `parse_qs()` to parse the query string into a dictionary. - Modify the `status` parameter as described. - Use `urlencode()` to convert the modified query dictionary back into a query string. - Use `urlunparse()` to construct the final URL from its components. - Ensure each component of the URL is correctly quoted to handle any special characters.","solution":"from typing import List from urllib.parse import urlparse, parse_qs, urlencode, urlunparse def modify_urls(urls: List[str]) -> List[str]: modified_urls = [] for url in urls: parsed_url = urlparse(url) query_dict = parse_qs(parsed_url.query) # Modify the \'status\' query parameter if \'status\' in query_dict: query_dict[\'status\'] = [\'active\'] if query_dict[\'status\'][0] == \'inactive\' else query_dict[\'status\'] else: query_dict[\'status\'] = [\'active\'] # Reconstruct the query string new_query = urlencode(query_dict, doseq=True) # Reconstruct the whole URL with updated query new_url = urlunparse( ( parsed_url.scheme, parsed_url.netloc, parsed_url.path, parsed_url.params, new_query, parsed_url.fragment ) ) modified_urls.append(new_url) return modified_urls"},{"question":"**Coding Assessment Question** # Question: Secure Subprocess Execution for System Monitoring Objective: Design a secure and efficient Python function that spawns a subprocess to execute a given shell command, captures its output, and returns the output while handling possible errors and ensuring security best practices. Requirements: 1. The function should take a string input `command` which represents the shell command to be executed. 2. The function should return the standard output of the command. 3. If the command results in an error, the function should capture and return the standard error. 4. Implement a timeout for the command execution to prevent it from running indefinitely. 5. Ensure that the function uses security best practices to avoid shell injection vulnerabilities. Function Signature: ```python def execute_shell_command(command: str, timeout: int = 10) -> str: pass ``` Input: - `command` (str): The shell command to be executed (e.g., `\\"ls -l /usr/bin\\"`). - `timeout` (int, optional): The number of seconds to wait for the command to complete. Defaults to 10 seconds. Output: - Returns a string containing the standard output or standard error of the command execution. Constraints: - Do not use `shell=True` to avoid shell injection vulnerabilities. - Use the `subprocess.run()` function for execution. - Handle any exceptions that might arise, such as `TimeoutExpired` or `CalledProcessError`. Example: ```python output = execute_shell_command(\\"ls -l /usr/bin\\") print(output) ``` Performance Requirements: - The function should efficiently handle the subprocess creation and output capture, ensuring it does not block the main program execution. # Hints: - You can use `subprocess.run()` with appropriate arguments to handle the execution and output capturing. - Consider using `capture_output=True` to capture both stdout and stderr. - Handle exceptions using try-except blocks to catch and return error outputs or timeout messages.","solution":"import subprocess def execute_shell_command(command: str, timeout: int = 10) -> str: Executes a shell command securely and returns the standard output or error. Args: command (str): The shell command to execute. timeout (int, optional): The number of seconds to wait for the command to complete. Defaults to 10. Returns: str: The standard output or standard error of the command. try: result = subprocess.run(command.split(), capture_output=True, text=True, timeout=timeout, check=True) return result.stdout except subprocess.CalledProcessError as e: return e.stderr except subprocess.TimeoutExpired as e: return f\\"Command timed out after {timeout} seconds\\" except Exception as e: return str(e)"},{"question":"# Asyncio Task Scheduling and Synchronization in Python You are tasked with creating an asynchronous program using Python\'s `asyncio` package to simulate a scenario where multiple clients request services from a server. The server should be able to handle these requests concurrently while maintaining the order of service completion. Additionally, implement synchronization primitives to manage access to a shared resource. Problem Statement Write an asynchronous function `client_requests(num_clients: int, max_time: int)` that simulates a server handling client requests concurrently. The function should use the following: 1. Asyncio\'s task scheduling and sleep functions. 2. An asyncio queue to manage client requests. 3. An asyncio lock to synchronize access to a shared resource. Details: 1. Each client will make a request which takes a random amount of time to process (between 1 and `max_time` seconds). 2. The server should handle all client requests concurrently but must maintain the order of service completion in its responses. 3. Use an asyncio Queue to simulate the clients\' request queue. 4. Use an asyncio Lock to ensure only one client can access/modify a shared resource (e.g., a counter) at a time. Input: - `num_clients`: An integer representing the number of clients making requests. - `max_time`: An integer representing the maximum processing time for any client request. Output: - Print the order in which clients are served and the total number of requests processed. Example: ```python import asyncio import random async def handle_request(client_id, processing_time, queue, lock): await asyncio.sleep(processing_time) async with lock: print(f\\"Client {client_id} processed in {processing_time} seconds\\") async def client_requests(num_clients, max_time): queue = asyncio.Queue() lock = asyncio.Lock() # Enqueue client requests for i in range(num_clients): await queue.put(i) tasks = [] while not queue.empty(): client_id = await queue.get() processing_time = random.randint(1, max_time) tasks.append(asyncio.create_task(handle_request(client_id, processing_time, queue, lock))) await asyncio.gather(*tasks) print(f\\"Total {num_clients} client requests processed.\\") # Example usage: # asyncio.run(client_requests(5, 10)) ``` Constraints: - Use only asyncio primitives for handling concurrency (no threading or multiprocessing). - Ensure the handling of all client requests is done asynchronously. This question assesses your understanding of: - Asynchronous task scheduling using asyncio. - Managing concurrency using asyncio queues and synchronization primitives.","solution":"import asyncio import random async def handle_request(client_id, processing_time, lock): await asyncio.sleep(processing_time) async with lock: print(f\\"Client {client_id} processed in {processing_time} seconds\\") return client_id async def client_requests(num_clients, max_time): queue = asyncio.Queue() lock = asyncio.Lock() # Enqueue client requests for i in range(num_clients): await queue.put(i) tasks = [] while not queue.empty(): client_id = await queue.get() processing_time = random.randint(1, max_time) tasks.append(asyncio.create_task(handle_request(client_id, processing_time, lock))) served_clients = await asyncio.gather(*tasks) print(f\\"Total {num_clients} client requests processed.\\") return served_clients # Example usage: # asyncio.run(client_requests(5, 10))"},{"question":"# Question: Implement a Multi-Threaded Counter using `_thread` Module Objective: Implement a multi-threaded counter using the `_thread` module. Your task is to create a counter that is incremented by multiple threads. Ensure that the counter is thread-safe, meaning that no two threads should increment the counter simultaneously, leading to race conditions. Requirements: 1. Define a shared counter variable. 2. Create a lock object using `_thread.allocate_lock()`. 3. Implement a function `increment_counter(n, lock)` that: - Takes two parameters: `n` (number of times to increment the counter) and `lock` (lock object). - Uses the lock to ensure that the counter is incremented in a thread-safe manner. 4. Start a specified number of threads using `_thread.start_new_thread()` to increment the counter. 5. Make sure the main thread waits for all threads to complete before printing the final value of the counter. Input: - Number of threads (`num_threads`): Integer (1 <= num_threads <= 10). - Increments per thread (`increments_per_thread`): Integer (1 <= increments_per_thread <= 1000). Output: - Final value of the counter after all threads have completed their increments. Example: ```python # Suppose num_threads = 5 and increments_per_thread = 100 counter = 0 lock = _thread.allocate_lock() # Function to increment counter def increment_counter(n, lock): global counter for _ in range(n): with lock: counter += 1 # Start threads num_threads = 5 increments_per_thread = 100 for _ in range(num_threads): _thread.start_new_thread(increment_counter, (increments_per_thread, lock)) # Wait for all threads to complete # (Use a simple mechanism to ensure main thread waits) # Print final counter value print(\\"Final counter value:\\", counter) ``` **Note:** - It\'s essential to add a mechanism to ensure the main thread waits for all threads to finish. One common way is to use a loop waiting for the expected counter value or a simple sleep with overestimate time, though these are rudimentary and better synchronization mechanisms exist outside this low-level API. In this exercise, keep it simple as described. Constraints: - Do not use any higher-level threading utilities. Use `_thread` module functions only. - Make sure your solution avoids race conditions effectively.","solution":"import _thread import time # Shared counter variable counter = 0 # Lock object for synchronizing threads lock = _thread.allocate_lock() def increment_counter(n, lock): Function to increment the global counter n times in a thread-safe manner. global counter for _ in range(n): with lock: counter += 1 def start_threads_and_increment(num_threads, increments_per_thread): Function to start threads and wait for their completion. global counter counter = 0 # Reset counter before starting threads # Start the threads for _ in range(num_threads): _thread.start_new_thread(increment_counter, (increments_per_thread, lock)) # Simple mechanism to wait for all threads to complete time.sleep(1) # Sleep time should be reasonably enough for all threads to complete return counter"},{"question":"**Problem Description:** You are tasked with building a task scheduler that uses a priority queue to manage and execute tasks based on their priority levels. Each task has a unique name and a priority value, with the lower values indicating higher priority. Implement a class `TaskScheduler` with the following methods: 1. `add_task(task_name: str, priority: int) -> None`: Adds a new task or updates the priority of an existing task with the specified priority. 2. `remove_task(task_name: str) -> None`: Removes the specified task from the scheduler. Raise a `KeyError` if the task is not found. 3. `pop_task() -> str`: Removes and returns the name of the highest priority task. Raise a `KeyError` if the scheduler is empty. 4. `peek_task() -> str`: Returns the name of the highest priority task without removing it from the scheduler. Raise a `KeyError` if the scheduler is empty. 5. `change_priority(task_name: str, new_priority: int) -> None`: Changes the priority of the specified task to the new priority. **Constraints and Notes:** - No two tasks will have the same name. - Use `heapq` functions to maintain the priority queue. - If a task\'s priority is changed or a task is removed, ensure the heap invariant is maintained. **Example Usage:** ```python scheduler = TaskScheduler() scheduler.add_task(\\"task1\\", 5) scheduler.add_task(\\"task2\\", 3) scheduler.add_task(\\"task3\\", 9) assert scheduler.peek_task() == \\"task2\\" assert scheduler.pop_task() == \\"task2\\" assert scheduler.pop_task() == \\"task1\\" scheduler.add_task(\\"task4\\", 2) scheduler.change_priority(\\"task3\\", 1) assert scheduler.pop_task() == \\"task3\\" assert scheduler.pop_task() == \\"task4\\" assert scheduler.pop_task() == KeyError(\\"pop from an empty priority queue\\") ``` Implement the `TaskScheduler` class to meet the requirements above.","solution":"import heapq class TaskScheduler: def __init__(self): self.heap = [] self.task_map = {} def add_task(self, task_name: str, priority: int) -> None: if task_name in self.task_map: self.change_priority(task_name, priority) else: task = (priority, task_name) heapq.heappush(self.heap, task) self.task_map[task_name] = task def remove_task(self, task_name: str) -> None: if task_name not in self.task_map: raise KeyError(f\\"Task {task_name} not found\\") task = self.task_map.pop(task_name) self.heap.remove(task) heapq.heapify(self.heap) def pop_task(self) -> str: if not self.heap: raise KeyError(\\"pop from an empty priority queue\\") priority, task_name = heapq.heappop(self.heap) del self.task_map[task_name] return task_name def peek_task(self) -> str: if not self.heap: raise KeyError(\\"peek from an empty priority queue\\") return self.heap[0][1] def change_priority(self, task_name: str, new_priority: int) -> None: if task_name not in self.task_map: raise KeyError(f\\"Task {task_name} not found\\") self.remove_task(task_name) self.add_task(task_name, new_priority)"},{"question":"# PyTorch Coding Assessment: Custom Neural Network Implementations and Operations # Objective: Your objective is to demonstrate your understanding of PyTorch by implementing a series of functions that involve creating, manipulating, and comparing tensors. You will also implement a simple neural network and perform operations using CUDA acceleration. # Instructions: 1. Implement a function that creates specific tensors. 2. Implement a function that performs tensor operations including indexing, slicing, and reduction. 3. Implement a simple neural network class with one hidden layer and train it on a random dataset using CUDA if available. 4. Ensure that the computations switch appropriately between CPU and CUDA (if available). # Task Details: 1. Tensor Creation: Implement the function `create_tensors()` that creates the following tensors: - A 3x3 identity matrix. - A tensor of shape (5, 5) filled with random numbers from a normal distribution. - A tensor of shape (4, 4) with values ranging from 0 to 15. ```python def create_tensors(): Creates specific tensors. Returns: Tuple: A tuple containing: - 3x3 identity matrix tensor. - 5x5 tensor filled with random numbers from a normal distribution. - 4x4 tensor with values ranging from 0 to 15. pass ``` 2. Tensor Operations: Implement the function `tensor_operations(tensors)` that accepts a tuple of the above-created tensors and performs the following operations: - Select the second row from the identity matrix. - Select the last column from the normal tensor. - Compute the sum of all elements in the range tensor. ```python def tensor_operations(tensors): Performs specific tensor operations. Args: tensors (tuple): A tuple of tensors as created by `create_tensors()`. Returns: Tuple: A tuple containing: - Tensor representing the second row of the identity matrix. - Tensor representing the last column of the normal tensor. - Sum of all elements in the range tensor. pass ``` 3. Neural Network with CUDA: Implement a class `SimpleNN` that defines a neural network with the following: - One hidden layer with 10 neurons using ReLU activation. - An output layer for binary classification. Then implement the function `train_network()` to train this network on randomly generated input data and labels. Ensure to utilize CUDA if available and appropriately handle the transitions between CPU and GPU. ```python import torch.nn as nn import torch.optim as optim class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(10, 10) self.output = nn.Linear(10, 1) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.hidden(x)) x = torch.sigmoid(self.output(x)) return x def train_network(): Trains the SimpleNN on a random dataset. Returns: float: The loss value after training. # Check for CUDA availability device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Create random dataset input_data = torch.randn(100, 10).to(device) # 100 samples, 10 features labels = torch.randint(0, 2, (100, 1)).float().to(device) # 100 labels # Instantiate and move model to appropriate device model = SimpleNN().to(device) criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop model.train() for epoch in range(100): # 100 epochs optimizer.zero_grad() outputs = model(input_data) loss = criterion(outputs, labels) loss.backward() optimizer.step() return loss.item() ``` # Constraints: - Only use functions and classes from the PyTorch library. - Ensure proper error handling and validation. # Submission: Submit the functions `create_tensors`, `tensor_operations`, the `SimpleNN` class, and the `train_network` function as a single .py file with appropriate comments and documentation.","solution":"import torch def create_tensors(): Creates specific tensors. Returns: Tuple: A tuple containing: - 3x3 identity matrix tensor. - 5x5 tensor filled with random numbers from a normal distribution. - 4x4 tensor with values ranging from 0 to 15. identity_matrix = torch.eye(3) normal_tensor = torch.randn(5, 5) range_tensor = torch.arange(16).view(4, 4) return identity_matrix, normal_tensor, range_tensor def tensor_operations(tensors): Performs specific tensor operations. Args: tensors (tuple): A tuple of tensors as created by `create_tensors()`. Returns: Tuple: A tuple containing: - Tensor representing the second row of the identity matrix. - Tensor representing the last column of the normal tensor. - Sum of all elements in the range tensor. identity_matrix = tensors[0] normal_tensor = tensors[1] range_tensor = tensors[2] second_row_identity = identity_matrix[1, :] last_column_normal = normal_tensor[:, -1] sum_range_tensor = range_tensor.sum().item() return second_row_identity, last_column_normal, sum_range_tensor import torch.nn as nn import torch.optim as optim class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.hidden = nn.Linear(10, 10) self.output = nn.Linear(10, 1) self.relu = nn.ReLU() def forward(self, x): x = self.relu(self.hidden(x)) x = torch.sigmoid(self.output(x)) return x def train_network(): Trains the SimpleNN on a random dataset. Returns: float: The loss value after training. # Check for CUDA availability device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Create random dataset input_data = torch.randn(100, 10).to(device) # 100 samples, 10 features labels = torch.randint(0, 2, (100, 1)).float().to(device) # 100 labels # Instantiate and move model to appropriate device model = SimpleNN().to(device) criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop model.train() for epoch in range(100): # 100 epochs optimizer.zero_grad() outputs = model(input_data) loss = criterion(outputs, labels) loss.backward() optimizer.step() return loss.item()"},{"question":"**XML Data Extraction and Transformation** Create a function `process_xml_slideshow(input: str) -> str` that takes an XML string representing a slideshow and transforms it into an HTML-like structure. The XML string will follow a specific format provided below. Your function should parse the XML string using `xml.dom.minidom` and output an HTML string that follows the structure given in the example output. # Input The function will receive a string input representing an XML document. The XML document will follow this structure: ```xml <slideshow> <title>Slideshow Title</title> <slide> <title>Slide 1 Title</title> <point>Point 1</point> <point>Point 2</point> </slide> <slide> <title>Slide 2 Title</title> <point>Point 3</point> <point>Point 4</point> </slide> </slideshow> ``` # Expected Output The function should output a string representing the corresponding HTML structure: ```html <html> <head> <title>Slideshow Title</title> </head> <body> <h1>Slideshow Title</h1> <div class=\\"slide\\"> <h2>Slide 1 Title</h2> <ul> <li>Point 1</li> <li>Point 2</li> </ul> </div> <div class=\\"slide\\"> <h2>Slide 2 Title</h2> <ul> <li>Point 3</li> <li>Point 4</li> </ul> </div> </body> </html> ``` # Constraints - You can assume the input string is always well-formed XML according to the provided structure. - Handle the transformation using appropriate methods from `xml.dom.minidom`. - Pay attention to the hierarchy and the sequence of elements when constructing the HTML output. # Example Input: ```xml <slideshow> <title>Sample Slideshow</title> <slide> <title>Introduction</title> <point>Welcome</point> <point>Agenda</point> </slide> <slide> <title>Conclusion</title> <point>Summary</point> <point>Questions</point> </slide> </slideshow> ``` Output: ```html <html> <head> <title>Sample Slideshow</title> </head> <body> <h1>Sample Slideshow</h1> <div class=\\"slide\\"> <h2>Introduction</h2> <ul> <li>Welcome</li> <li>Agenda</li> </ul> </div> <div class=\\"slide\\"> <h2>Conclusion</h2> <ul> <li>Summary</li> <li>Questions</li> </ul> </div> </body> </html> ``` # Notes - Utilize the `parseString` method provided by `xml.dom.minidom` to parse the XML string. - Traverse and manipulate the DOM tree to retrieve node values and structure them into the required HTML format.","solution":"from xml.dom.minidom import parseString def process_xml_slideshow(input: str) -> str: dom = parseString(input) slideshow_title_elem = dom.getElementsByTagName(\\"title\\")[0] slideshow_title = slideshow_title_elem.firstChild.nodeValue html_output = [] html_output.append(\'<html>\') html_output.append(\' <head>\') html_output.append(f\' <title>{slideshow_title}</title>\') html_output.append(\' </head>\') html_output.append(\' <body>\') html_output.append(f\' <h1>{slideshow_title}</h1>\') slides = dom.getElementsByTagName(\\"slide\\") for slide in slides: slide_title_elem = slide.getElementsByTagName(\\"title\\")[0] slide_title = slide_title_elem.firstChild.nodeValue html_output.append(\' <div class=\\"slide\\">\') html_output.append(f\' <h2>{slide_title}</h2>\') html_output.append(\' <ul>\') points = slide.getElementsByTagName(\\"point\\") for point in points: point_text = point.firstChild.nodeValue html_output.append(f\' <li>{point_text}</li>\') html_output.append(\' </ul>\') html_output.append(\' </div>\') html_output.append(\' </body>\') html_output.append(\'</html>\') return \\"n\\".join(html_output)"},{"question":"# Question: Implementing a Custom Module Loader **Objective:** The goal of this question is to implement a custom module loader using the `importlib` package. The custom loader should be capable of importing Python source files and simulating a module reload. **Task:** 1. **Create a Custom Source Loader:** Implement a class `CustomSourceLoader` that inherits from `importlib.abc.SourceLoader`. The loader should: - Load a Python file as a module. - Handle potential bytecode caching for efficiency. 2. **Module Import Function:** Implement a function `import_module_with_custom_loader(module_name: str, file_path: str) -> ModuleType` that: - Uses the `CustomSourceLoader` to import a module given its name and file path. - Adds the imported module to `sys.modules`. 3. **Reload Simulation:** Implement a function `simulate_reload(module_name: str) -> ModuleType` that: - Simulates reloading the module by using `importlib.reload()`. - Ensures that any changes to the module source file are reflected in the reloaded module. **Constraints:** - The module source files should have a `.py` extension. - Use only the `importlib` package and standard library modules. - Handle errors and edge cases gracefully (e.g., file not found, invalid module name, etc.). **Example Usage:** ```python # Assuming the existence of a Python file \'example_module.py\' with some functions. imported_module = import_module_with_custom_loader(\'example_module\', \'path/to/example_module.py\') print(imported_module.some_function()) # Calls a function from the imported module. # Modify the source file \'example_module.py\' here (e.g., change \'some_function\'). reloaded_module = simulate_reload(\'example_module\') print(reloaded_module.some_function()) # Reflects the changes made to the module source file. ``` **Expected Functions:** ```python class CustomSourceLoader(importlib.abc.SourceLoader): def __init__(self, module_name: str, file_path: str): pass def get_filename(self, fullname: str) -> str: pass def get_data(self, filename: str) -> bytes: pass def import_module_with_custom_loader(module_name: str, file_path: str) -> ModuleType: pass def simulate_reload(module_name: str) -> ModuleType: pass ``` Ensure your code is well-documented and follows best practices. Test your implementation with various modules and edge cases.","solution":"import importlib.util import importlib.abc import sys from types import ModuleType class CustomSourceLoader(importlib.abc.SourceLoader): def __init__(self, module_name: str, file_path: str): self.module_name = module_name self.file_path = file_path def get_filename(self, fullname: str) -> str: return self.file_path def get_data(self, filename: str) -> bytes: with open(filename, \'rb\') as file: return file.read() def import_module_with_custom_loader(module_name: str, file_path: str) -> ModuleType: loader = CustomSourceLoader(module_name, file_path) spec = importlib.util.spec_from_loader(module_name, loader) module = importlib.util.module_from_spec(spec) loader.exec_module(module) sys.modules[module_name] = module return module def simulate_reload(module_name: str) -> ModuleType: if module_name in sys.modules: return importlib.reload(sys.modules[module_name]) else: raise ModuleNotFoundError(f\\"Module {module_name} not found in sys.modules\\")"},{"question":"**Coding Assessment Question** # Objective: Demonstrate your understanding of creating and manipulating datasets using `scikit-learn`\'s dataset generators. # Problem Statement: You are tasked with creating a synthetic dataset for a machine learning classification problem. The dataset should consist of 1000 samples with 5 features each. Next, you need to split the generated data into training and test sets and then perform a simple data scaling transformation. # Instructions: 1. **Dataset Creation**: - Use the `sklearn.datasets.make_classification` function to generate a dataset. - The dataset should have: - 1000 samples. - 5 informative features. - A total of 7 features (2 redundant and 5 informative). - 2 classes. 2. **Data Splitting**: - Split the dataset into training (80%) and test (20%) sets using the `train_test_split` function from `sklearn.model_selection`. 3. **Data Scaling**: - Apply standard scaling to the features using `StandardScaler` from `sklearn.preprocessing`. # Specifications: - Function Name: `create_and_preprocess_dataset` - Function Signature: ```python def create_and_preprocess_dataset(random_state: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: pass ``` - **Input**: - `random_state` (int): Seed for the random number generator to ensure reproducibility. - **Output**: - Four numpy arrays: `X_train_scaled`, `X_test_scaled`, `y_train`, `y_test` Example: ```python X_train_scaled, X_test_scaled, y_train, y_test = create_and_preprocess_dataset(random_state=42) # X_train_scaled: Scaled features of the training set # X_test_scaled: Scaled features of the test set # y_train: Class labels of the training set # y_test: Class labels of the test set ``` # Constraints: - You should use the `random_state` parameter to ensure reproducibility. # Notes: - Ensure that your code separates the process of creating, splitting, and scaling the dataset into logical steps. - Include necessary imports such as `numpy`, `sklearn.datasets`, `sklearn.model_selection`, and `sklearn.preprocessing`.","solution":"from typing import Tuple import numpy as np from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler def create_and_preprocess_dataset(random_state: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: # Generate a synthetic dataset X, y = make_classification( n_samples=1000, n_features=7, # 5 informative + 2 redundant n_informative=5, n_redundant=2, n_classes=2, random_state=random_state ) # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state) # Apply standard scaling to the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) return X_train_scaled, X_test_scaled, y_train, y_test"},{"question":"Objective: Demonstrate your understanding of Python intrinsic data types and the operations that can be performed on them by implementing a function that manipulates these types. Problem Statement: You are provided with a list of mixed data types, including integers, floats, strings, and dictionaries. You are required to create a function that processes this list to perform the following operations: 1. Separate all the dictionaries in the list and store them in a new list. 2. For each dictionary in the extracted list, check if there is a key called \\"target\\" and if it has an integer value. - If the value is an integer, double its value. - If the value is a float, convert it to an integer and double its value. - If the value does not exist or is not a number, ignore that dictionary. 3. Return a tuple containing two lists: - The first list should include all original non-dictionary elements. - The second list should contain the modified dictionaries. Function Signature: ```python def process_mixed_list(data: list) -> tuple: ``` Input: - A list `data` of mixed types (i.e., `List[Union[int, float, str, dict]]`) where: - 1 ≤ len(data) ≤ 100 - integers are in the range of -10^6 to 10^6 - floating-point numbers are in the range of -10^6.0 to 10^6.0 - strings and dictionaries are of arbitrary length. Output: - A tuple of two lists: - The first list contains all non-dictionary elements from the input list. - The second list contains dictionaries that have the \\"target\\" key with doubled integer values. Constraints: - You must check the type of each element in the list. - Perform required type conversions and value updates as specified. - Maintain the original order of the non-dictionary elements and the dictionaries. Example: ```python data = [1, 3.5, \\"hello\\", {\\"target\\": 12}, {\\"name\\": \\"Alice\\"}, {\\"target\\": 7.3}, 20, {\\"target\\": \\"not-a-number\\"}] result = process_mixed_list(data) # Output: # ([1, 3.5, \\"hello\\", 20], [{\\"target\\": 24}, {\\"target\\": 14}]) ``` Notes: - The function should handle type errors properly and continue processing the list. - Ensure that the solution performs efficiently given the constraints.","solution":"def process_mixed_list(data: list) -> tuple: Processes a list of mixed data types. Parameters: data (list): A list containing integers, floats, strings, and dictionaries. Returns: tuple: A tuple containing two lists: - The first list contains all original non-dictionary elements. - The second list contains modified dictionaries that have the target key with doubled integer values. non_dict_elements = [] dict_elements = [] for item in data: if isinstance(item, dict): if \\"target\\" in item: value = item[\\"target\\"] if isinstance(value, (int, float)): item[\\"target\\"] = int(value) * 2 dict_elements.append(item) else: non_dict_elements.append(item) return (non_dict_elements, dict_elements)"},{"question":"Objective: Your task is to create a seaborn plot using the `healthspending` dataset. Specifically, you need to demonstrate your understanding of grouping data, normalization, customization, and labeling in seaborn. Problem Statement: 1. Load the `healthspending` dataset using seaborn\'s `load_dataset` function. 2. Create a line plot with the following specifications: - x-axis should represent the \'Year\' column. - y-axis should represent the \'Spending_USD\' column. - Different countries should be represented by different colors. 3. Normalize the \'Spending_USD\' data relative to the spending in the year 2000 for each country. 4. Label the y-axis as \\"Spending relative to year 2000 baseline (% change)\\". 5. Save the plot to a file named `healthspending_plot.png`. Requirements: - You must use the `seaborn.objects` interface for creating the plot. - Use the `so.Norm()` function for normalization with appropriate customization. - The final plot should be saved as a PNG file named `healthspending_plot.png`. Constraints: - You should not use any other plotting libraries except seaborn and matplotlib for saving the plot. - Properly handle any missing or NaN values in the dataset before plotting. Input: - No direct input; load the dataset using `seaborn.load_dataset(\'healthexp\')` or a similar function. Output: - A saved image file named `healthspending_plot.png`. Example: Here is an example of what the plot setup might look like. This is **not** the complete solution. ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset healthexp = load_dataset(\\"healthexp\\") # Define and create the plot plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == 2000\\", percent=True)) .label(y=\\"Spending relative to year 2000 baseline (% change)\\") ) # Save the plot to a file plot.save(\\"healthspending_plot.png\\") ```","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_healthspending_plot(): # Load dataset healthexp = sns.load_dataset(\\"healthexp\\") # Normalize \'Spending_USD\' relative to year 2000 for each country def normalize(row, baseline): return (row / baseline - 1) * 100 baseline_spending = healthexp[healthexp[\'Year\'] == 2000].set_index(\'Country\')[\'Spending_USD\'] healthexp[\'Normalized_Spending\'] = healthexp.apply( lambda row: normalize(row[\'Spending_USD\'], baseline_spending[row[\'Country\']]), axis=1) # Define and create the plot plot = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Normalized_Spending\\", color=\\"Country\\") .add(so.Line(), so.Norm(where=\\"x == 2000\\", percent=True)) .label(y=\\"Spending relative to year 2000 baseline (% change)\\") ) # Save the plot to a file plot.save(\\"healthspending_plot.png\\") return \\"healthspending_plot.png\\" # Execute plot function to create and save the plot create_healthspending_plot()"},{"question":"# Nullable Integer Data Manipulation with pandas Objective You have been provided with a dataset containing nullable integer columns which include missing values represented by `pd.NA`. You are expected to write a function that processes this dataset to compute specific statistics while ensuring correct datatype handling. Problem Statement Write a function `process_nullable_integers` that performs the following operations on a given dataset: 1. **Input**: A DataFrame `df` with the following columns: - `id` (nullable integer, possibly contains `pd.NA`). - `score` (nullable integer, possibly contains `pd.NA`). - `group` (category type). 2. **Output**: A Series containing the total sum of `score` for each `group`, replacing missing values with zeros before the sum computation. Constraints - Ensure that the columns `id` and `score` remain as nullable integer types in the DataFrame. - Perform all arithmetic operations while preserving the dtype integrity. - If any input arguments are not provided in the correct format, raise a `ValueError`. Example ```python import pandas as pd def process_nullable_integers(df): # Your implementation here # Example DataFrame data = { \'id\': pd.array([101, 102, pd.NA, 104], dtype=\\"Int64\\"), \'score\': pd.array([10, pd.NA, 5, 8], dtype=\\"Int64\\"), \'group\': [\'A\', \'B\', \'A\', \'B\'] } df = pd.DataFrame(data) # Expected output: # A 15 # B 8 # dtype: Int64 print(process_nullable_integers(df)) ``` Your task is to implement the `process_nullable_integers` function according to the requirements outlined above. - The sum should ignore `pd.NA` values, handling them appropriately according to the type constraints. - Ensure dtype integrity after operations.","solution":"import pandas as pd def process_nullable_integers(df): if not isinstance(df, pd.DataFrame): raise ValueError(\\"Input must be a pandas DataFrame.\\") required_columns = {\'id\', \'score\', \'group\'} if not required_columns.issubset(df.columns): raise ValueError(f\\"DataFrame must contain the following columns: {required_columns}\\") if df[\'id\'].dtype != \'Int64\' or df[\'score\'].dtype != \'Int64\': raise ValueError(\\"Columns \'id\' and \'score\' must be nullable integer (Int64) types.\\") if df[\'group\'].dtype != \'category\': raise ValueError(\\"Column \'group\' must be of category type.\\") # Replace any pd.NA in \'score\' column with 0 for the sum computation df[\'score\'].fillna(0, inplace=True) # Compute the sum of scores for each group result = df.groupby(\'group\')[\'score\'].sum() # Ensure the result is of Int64 dtype result = result.astype(\'Int64\') return result"},{"question":"**Question**: Implement a Python class that demonstrates both the `tp_call` protocol and the vectorcall protocol, using the low-level API provided in the documentation. Your implementation will be in Python pseudo-code that follows the calling conventions described. **Requirements**: 1. The class should have at least two methods: - One that supports being called using `tp_call` with both positional and keyword arguments. - One that supports being called using vectorcall with both positional and keyword arguments. 2. Implement a function to call these methods from Python code using the respective lower-level APIs. 3. Include error handling for cases where the object does not support the expected calling convention. **Detailed Specifications**: 1. **Class Implementation**: - Name of the class: `CallableClass` - Method supporting `tp_call`: `tp_method` - Method supporting vectorcall: `vectorcall_method` 2. **Function to call methods**: - Function to call `tp_method`: `invoke_tp_method(callable_object, args, kwargs)` - Function to call `vectorcall_method`: `invoke_vectorcall_method(callable_object, args, kwnames)` **Example**: ```python class CallableClass: def tp_method(self, *args, **kwargs): # Implementation for method supporting tp_call pass def vectorcall_method(self, *args, **kwargs): # Implementation for method supporting vectorcall pass def invoke_tp_method(callable_object, args, kwargs): # Low-level API to invoke tp_method pass def invoke_vectorcall_method(callable_object, args, kwnames): # Low-level API to invoke vectorcall_method pass # Example usage: obj = CallableClass() invoke_tp_method(obj.tp_method, (\'arg1\', \'arg2\'), {\'kwarg1\': \'value1\', \'kwarg2\': \'value2\'}) invoke_vectorcall_method(obj.vectorcall_method, (\'arg1\', \'arg2\'), (\'kwarg1\', \'kwarg2\')) ``` **Constraints**: - Make sure to handle the cases where arguments could be `None`. **Performance**: - The implementations should be efficient, adhering to the vectorcall protocol\'s goal of improving performance.","solution":"class CallableClass: def tp_method(self, *args, **kwargs): Method supporting tp_call with both positional and keyword arguments. return f\\"tp_method called with args: {args}, kwargs: {kwargs}\\" def vectorcall_method(self, *args, **kwargs): Method supporting vectorcall with both positional and keyword arguments. return f\\"vectorcall_method called with args: {args}, kwargs: {kwargs}\\" def invoke_tp_method(callable_object, args, kwargs): Low-level API to invoke tp_method. if hasattr(callable_object, \\"__call__\\"): return callable_object(*args, **kwargs) else: raise TypeError(\\"Object does not support tp_call or is not callable\\") def invoke_vectorcall_method(callable_object, args, kwnames): Low-level API to invoke vectorcall_method. kwargs = {key: kwnames[index] for index, key in enumerate(kwnames)} if hasattr(callable_object, \\"__call__\\"): return callable_object(*args, **kwargs) else: raise TypeError(\\"Object does not support vectorcall or is not callable\\") # Example Usage: # obj = CallableClass() # print(invoke_tp_method(obj.tp_method, (\'arg1\', \'arg2\'), {\'kwarg1\': \'value1\', \'kwarg2\': \'value2\'})) # print(invoke_vectorcall_method(obj.vectorcall_method, (\'arg1\', \'arg2\'), (\'value1\', \'value2\')))"},{"question":"# PyTorch Coding Assessment Question In this task, you will demonstrate your understanding of PyTorch\'s function transforms by implementing a simple neural network and using `torch.func` utilities to compute gradients and Jacobians. Problem Statement 1. **Implement a Neural Network**: Construct a simple feed-forward neural network with one hidden layer using `torch.nn.Module`. 2. **Compute Gradients**: Write a function to compute the gradients of the model parameters with respect to a given loss function using `torch.func.grad`. 3. **Compute Jacobians**: Write a function to compute the Jacobians of the model\'s output with respect to its parameters using `torch.func.jacrev`. Function Signatures ```python import torch from torch import nn class SimpleNN(nn.Module): def __init__(self, input_dim: int, hidden_dim: int, output_dim: int): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.fc1(x)) return self.fc2(x) def compute_gradients(model: SimpleNN, inputs: torch.Tensor, targets: torch.Tensor) -> dict: Compute gradients of the model parameters with respect to the mean squared error loss. Args: model (SimpleNN): The neural network model. inputs (torch.Tensor): Input data batch of shape (batch_size, input_dim). targets (torch.Tensor): Target data batch of shape (batch_size, output_dim). Returns: dict: Gradients of the model parameters. params = dict(model.named_parameters()) def loss_function(params, inputs, targets): prediction = torch.func.functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) gradients = torch.func.grad(loss_function)(params, inputs, targets) return gradients def compute_jacobians(model: SimpleNN, inputs: torch.Tensor) -> dict: Compute Jacobians of the model\'s output with respect to its parameters. Args: model (SimpleNN): The neural network model. inputs (torch.Tensor): Input data batch of shape (batch_size, input_dim). Returns: dict: Jacobians of the model\'s output with respect to its parameters. params = dict(model.named_parameters()) jacobians = torch.func.jacrev(torch.func.functional_call, argnums=1)(model, params, (inputs,)) return jacobians # Example usage if __name__ == \\"__main__\\": input_dim = 3 hidden_dim = 5 output_dim = 2 batch_size = 4 model = SimpleNN(input_dim, hidden_dim, output_dim) inputs = torch.randn(batch_size, input_dim) targets = torch.randn(batch_size, output_dim) gradients = compute_gradients(model, inputs, targets) jacobians = compute_jacobians(model, inputs) print(\\"Gradients:\\", gradients) print(\\"Jacobians:\\", jacobians) ``` Constraints 1. Use the `torch.func.functional_call` to handle the forward pass with new parameters. 2. Ensure that the `compute_gradients` function computes gradients with respect to the mean squared error loss. 3. The `compute_jacobians` function should compute the Jacobians of the model\'s output with respect to its parameters. Evaluation Criteria - Correct implementation of the neural network. - Accurate computation of gradients and Jacobians using the appropriate `torch.func` transformations. - Proper usage of PyTorch functionalities as described in the documentation.","solution":"import torch from torch import nn import torch.func as func class SimpleNN(nn.Module): def __init__(self, input_dim: int, hidden_dim: int, output_dim: int): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x: torch.Tensor) -> torch.Tensor: x = torch.relu(self.fc1(x)) return self.fc2(x) def compute_gradients(model: SimpleNN, inputs: torch.Tensor, targets: torch.Tensor) -> dict: Compute gradients of the model parameters with respect to the mean squared error loss. Args: model (SimpleNN): The neural network model. inputs (torch.Tensor): Input data batch of shape (batch_size, input_dim). targets (torch.Tensor): Target data batch of shape (batch_size, output_dim). Returns: dict: Gradients of the model parameters. params = dict(model.named_parameters()) def loss_function(params, inputs, targets): cloned_model = model.__class__(model.fc1.in_features, model.fc1.out_features, model.fc2.out_features) cloned_model.load_state_dict(params) prediction = cloned_model(inputs) return torch.nn.functional.mse_loss(prediction, targets) gradients = func.grad(loss_function)(params, inputs, targets) return gradients def compute_jacobians(model: SimpleNN, inputs: torch.Tensor) -> dict: Compute Jacobians of the model\'s output with respect to its parameters. Args: model (SimpleNN): The neural network model. inputs (torch.Tensor): Input data batch of shape (batch_size, input_dim). Returns: dict: Jacobians of the model\'s output with respect to its parameters. params = dict(model.named_parameters()) cloned_model = model.__class__(model.fc1.in_features, model.fc1.out_features, model.fc2.out_features) cloned_model.load_state_dict(params) output = cloned_model(inputs) def model_output(params, inputs): cloned_model = model.__class__(model.fc1.in_features, model.fc1.out_features, model.fc2.out_features) cloned_model.load_state_dict(params) return cloned_model(inputs) jacobians = func.jacrev(model_output)(params, inputs) return jacobians"},{"question":"# Problem: Secure Integer Conversion You are given a collection of integers and floating-point numbers generated by some process. Some of these values might be very large or very small. Your task is to implement a conversion process in Python that safely converts these values into Python long integers (`PyLongObject`) using the given API, while handling possible overflow errors. Your implementation should also involve methods to convert back from `PyLongObject` to native C types and handle any exceptions or error codes that arise during the process. Functions to Implement 1. **safe_long_conversion(value)**: - **Input**: A numerical value (could be int or float). - **Output**: A tuple containing the `PyLongObject` representation if the conversion was successful, and `None` if there was an overflow or any other error occurred during the conversion. - **Constraints**: - If the input is a float, convert only the integer part. - Be sure to handle the ranges properly to avoid overflow. - Use the `PyErr_Occurred()` to disambiguate error situations. 2. **safe_native_conversion(long_object)**: - **Input**: A `PyLongObject`. - **Output**: A tuple containing the native long integer if the reverse conversion is successful, and `None` if there was an overflow or any other error during the conversion. - **Constraints**: - Handle overflow properly and ensure that the conversion process does not go out of bounds. Example Here\'s how your `safe_long_conversion` and `safe_native_conversion` methods should work: ```python value = 12345678901234567890 long_obj = safe_long_conversion(value) assert isinstance(long_obj, tuple) and long_obj[1] is None native_val = safe_native_conversion(long_obj[0]) assert isinstance(native_val, tuple) and native_val[0] == 12345678901234567890 ``` Additional Information - Ensure you use appropriate API calls from the given documentation to implement the conversion functions. - Be meticulous in handling exceptions and error cases outlined in the documentation. - Provide comments in your code where necessary to explain the logic and the specific API calls you are using. # Submission Implement the `safe_long_conversion` and `safe_native_conversion` functions and provide test cases demonstrating their functionality with various input values, especially edge cases. Good luck!","solution":"def safe_long_conversion(value): Safely convert a numerical value (int or float) to a PyLongObject representation. Handles overflow and errors gracefully. Input: - value (int or float): The numerical value to convert. Output: - tuple: (PyLongObject, None) if conversion is successful, (None, error) if not. try: # Convert the value to an integer integer_value = int(value) # In Python, int() gracefully handles long integers with arbitrary precision return (integer_value, None) except (OverflowError, ValueError) as e: return (None, str(e)) def safe_native_conversion(long_object): Safely convert a PyLongObject back to native long integer. Handles overflow and errors gracefully. Input: - long_object: The PyLongObject to convert. Output: - tuple: (int, None) if conversion is successful, (None, error) if not. try: # Python\'s int type covers native long integer native_value = int(long_object) return (native_value, None) except (OverflowError, ValueError) as e: return (None, str(e))"},{"question":"# Question: Asynchronous Network Client You are to implement an asynchronous network client using the `asyncio` library. This client will connect to a given server, send data, and collect responses concurrently. Requirements: 1. Implement an asynchronous function `async_network_client(server_address: str, port: int, messages: List[str]) -> List[str]` that takes a server address, a port, and a list of messages to send. 2. The function should connect to the server at the specified address and port, send all the messages concurrently, and collect responses for each respective message. 3. Use `asyncio` to handle the concurrency and networking tasks. Constraints: - You must use the `asyncio` library for handling concurrency. - The order of responses collected should match the order of messages sent. - Assume that each message sent will receive a corresponding response from the server. Input: - `server_address`: A string representing the server address (e.g., \'127.0.0.1\'). - `port`: An integer representing the port number to connect to (e.g., 8080). - `messages`: A list of strings, where each string is a message to be sent to the server. Output: - A list of strings, where each string is the response received from the server for the corresponding message. Example: ```python import asyncio from typing import List async def async_network_client(server_address: str, port: int, messages: List[str]) -> List[str]: # Your implementation here pass # Example usage: async def main(): responses = await async_network_client(\'127.0.0.1\', 8080, [\'message1\', \'message2\', \'message3\']) print(responses) asyncio.run(main()) ``` **Performance Requirement:** - The implementation must handle sending and receiving messages concurrently.","solution":"import asyncio from typing import List async def send_message(reader, writer, message: str) -> str: writer.write(message.encode()) await writer.drain() response = await reader.read(100) return response.decode() async def async_network_client(server_address: str, port: int, messages: List[str]) -> List[str]: responses = [] reader, writer = await asyncio.open_connection(server_address, port) async def handle_message(message): return await send_message(reader, writer, message) responses = await asyncio.gather(*[handle_message(message) for message in messages]) writer.close() await writer.wait_closed() return responses"},{"question":"**Problem Statement:** You are working on a data visualization project using seaborn, and you are required to create customized color palettes for various plots. Your task is to implement a function `create_custom_palette` which generates and returns a seaborn palette based on specific inputs. The function should also create a line plot visualizing a dataset with the generated palette. **Function Signature:** ```python def create_custom_palette(num_colors: int, lightness: float, saturation: float, hue_start: float, as_cmap: bool): Generate and return a customized seaborn palette based on the given parameters, and create a line plot using the generated palette. Parameters: - num_colors (int): The number of colors in the palette. - lightness (float): The lightness parameter for the palette (0 to 1). - saturation (float): The saturation parameter for the palette (0 to 1). - hue_start (float): The starting hue value for the palette (0 to 1). - as_cmap (bool): Whether to return the palette as a colormap. Returns: - palette: The generated seaborn palette (or colormap if as_cmap is True). Constraints: - `num_colors` should be at least 1 and at most 20. - `lightness`, `saturation`, and `hue_start` should be between 0 and 1, inclusive. pass ``` **Requirements:** 1. The function should create a palette using the parameters `num_colors`, `lightness`, `saturation`, `hue_start`, and `as_cmap`. 2. Use the seaborn `hls_palette` function to create the palette. 3. Use the created palette to generate a simple line plot using seaborn. - The line plot should display data from a sample dataset generated as follows: ```python import numpy as np import pandas as pd np.random.seed(42) data = pd.DataFrame({ \'x\': range(100), \'y\': np.random.randn(100).cumsum() }) ``` 4. Display the line plot inline in Jupyter Notebook or generate an image file. **Input Constraints:** - `num_colors`: 1 ≤ `num_colors` ≤ 20 - `lightness`: 0 ≤ `lightness` ≤ 1 - `saturation`: 0 ≤ `saturation` ≤ 1 - `hue_start`: 0 ≤ `hue_start` ≤ 1 **Example Input/Output:** ```python # Example call: palette = create_custom_palette(8, 0.5, 0.5, 0.1, False) # Expected output: # A seaborn palette with the specified parameters and a line plot. ``` **Notes:** - Ensure that your function handles invalid input values by raising appropriate exceptions or displaying error messages. - Make sure to include necessary imports and set up code for seaborn, pandas, numpy, and matplotlib inline plotting. Good luck and happy coding!","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np import pandas as pd def create_custom_palette(num_colors: int, lightness: float, saturation: float, hue_start: float, as_cmap: bool): Generate and return a customized seaborn palette based on the given parameters, and create a line plot using the generated palette. Parameters: - num_colors (int): The number of colors in the palette. - lightness (float): The lightness parameter for the palette (0 to 1). - saturation (float): The saturation parameter for the palette (0 to 1). - hue_start (float): The starting hue value for the palette (0 to 1). - as_cmap (bool): Whether to return the palette as a colormap. Returns: - palette: The generated seaborn palette (or colormap if as_cmap is True). Constraints: - `num_colors` should be at least 1 and at most 20. - `lightness`, `saturation`, and `hue_start` should be between 0 and 1, inclusive. # Input validation if not (1 <= num_colors <= 20): raise ValueError(\\"num_colors should be between 1 and 20 inclusive\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness should be between 0 and 1 inclusive\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation should be between 0 and 1 inclusive\\") if not (0 <= hue_start <= 1): raise ValueError(\\"hue_start should be between 0 and 1 inclusive\\") # Generate the custom palette palette = sns.hls_palette(num_colors, l=lightness, s=saturation, h=hue_start, as_cmap=as_cmap) # Generate sample dataset np.random.seed(42) data = pd.DataFrame({ \'x\': range(100), \'y\': np.random.randn(100).cumsum() }) # Create the line plot plt.figure(figsize=(10, 6)) sns.lineplot(x=\'x\', y=\'y\', data=data, palette=palette if not as_cmap else None) plt.title(\'Line Plot with Custom Palette\') plt.show() return palette"},{"question":"# Email Message Handling and Processing You are required to create a utility that manipulates email messages using Python\'s `email.message.EmailMessage` class. Your task is to implement a function `process_email_message` that performs the following operations on an email message: 1. **Add Subject and From Headers:** - The function should add the `Subject` header with a value provided as an argument. - The function should add the `From` header with a value provided as an argument. 2. **Set the Email Body:** - The function should set the email body to a text content provided as an argument. 3. **Add Attachments:** - The function should add attachments to the email message. Each attachment is provided as a tuple containing the filename and its content. For simplicity, assume all attachments are text files. 4. **Convert to Bytes and String:** - The function should return the complete email message in both bytes and string formats. # Function Definition ```python from email.message import EmailMessage def process_email_message(subject, from_address, body, attachments): Process an email message by setting the subject, from address, body, adding attachments, and converting to bytes and string formats. Parameters: subject (str): Subject of the email. from_address (str): From address of the email. body (str): Main text body of the email. attachments (list of tuple): List of attachments where each tuple contains the filename (str) and content (str). Returns: tuple: A tuple containing: - bytes representation of the entire email message. - string representation of the entire email message. pass ``` # Input Format - `subject` (str): The subject of the email. - `from_address` (str): The sender\'s email address. - `body` (str): The textual body of the email. - `attachments` (list of tuple): List of attachments with each tuple containing: - `filename` (str): The filename of the attachment. - `content` (str): The content of the attachment. # Output Format - Returns a tuple containing: - Bytes: The bytes representation of the complete email message. - String: The string representation of the complete email message. # Constraints - The `subject`, `from_address`, and `body` are non-empty strings. - The `attachments` list can be empty or contain multiple tuples, each with a valid filename and textual content. - Ensure the function handles potential errors gracefully. # Example ```python subject = \\"Test Subject\\" from_address = \\"sender@example.com\\" body = \\"This is the body of the email.\\" attachments = [(\\"file1.txt\\", \\"This is the content of file1.\\"), (\\"file2.txt\\", \\"This is the content of file2.\\")] email_bytes, email_string = process_email_message(subject, from_address, body, attachments) # The outputs email_bytes and email_string will be the complete email message in bytes and string formats, respectively. ``` Implement the `process_email_message` function based on the given requirements.","solution":"from email.message import EmailMessage def process_email_message(subject, from_address, body, attachments): Process an email message by setting the subject, from address, body, adding attachments, and converting to bytes and string formats. Parameters: subject (str): Subject of the email. from_address (str): From address of the email. body (str): Main text body of the email. attachments (list of tuple): List of attachments where each tuple contains the filename (str) and content (str). Returns: tuple: A tuple containing: - bytes representation of the entire email message. - string representation of the entire email message. email = EmailMessage() email[\'Subject\'] = subject email[\'From\'] = from_address email.set_content(body) for filename, content in attachments: email.add_attachment(content, filename=filename) email_bytes = email.as_bytes() email_string = email.as_string() return email_bytes, email_string"},{"question":"Objective To assess students\' understanding of Python\'s cell objects and their ability to manage variable scopes using these objects. Problem Statement You are tasked with implementing a basic closure mechanism using Python\'s cell objects. A closure is a function object that can reference variables from its enclosing scope even after the scope has exited. You need to implement two functions: 1. `create_closure(value: Any) -> Callable[[], Any]`: This function takes a single value and returns a closure (i.e., a function) that, when called, returns the initially provided value. 2. `update_closure(closure: Callable[[], Any], new_value: Any) -> None`: This function takes an existing closure created by `create_closure` and updates it so that the closure will return `new_value` when called. Input and Output Formats 1. `create_closure(value: Any) -> Callable[[], Any]` - **Input**: `value` – any type (could be an integer, string, list, etc.). - **Output**: A callable object (function) that, when called, returns the initial `value`. 2. `update_closure(closure: Callable[[], Any], new_value: Any) -> None` - **Input**: - `closure` – the callable object returned by `create_closure`. - `new_value` – any type (could be an integer, string, list, etc.). - **Output**: None Constraints - You should make use of Python\'s cell objects (`PyCell_*` functions) to manage the values within the closures. - Assume the necessary imports and bindings for `PyCell_*` functions are available. Example Usage ```python closure = create_closure(10) print(closure()) # Output: 10 update_closure(closure, 20) print(closure()) # Output: 20 ``` Implementation Notes 1. In `create_closure`, you will need to create a cell object to store the `value` and return a function that retrieves this value from the cell object when called. 2. In `update_closure`, you will need to update the cell object inside the given `closure` with `new_value`. **Hint:** You might find it helpful to wrap the cell object and its operations in a class structure to keep the implementation clean and maintainable.","solution":"def create_closure(value): # Enclosing cell to store the value cell = [value] # The closure function that returns the value in the cell def closure(): return cell[0] # Attaching a \'cell\' attribute to the closure to allow updates closure.cell = cell return closure def update_closure(closure, new_value): # Updating the \'cell\' attribute of the closure with the new value closure.cell[0] = new_value"},{"question":"Create a Python script that utilizes the `shelve` module to implement a basic contact management system. Your script should allow the user to add, retrieve, update, and delete contacts, as well as list all contacts. Each contact should store the name, phone number, and email of a person. Requirements 1. **Function: `initialize_contacts_db`** - **Input**: A string representing the filename for the shelve database. - **Output**: A shelve object for further operations. - **Constraints**: The file should be opened with `writeback=True`. 2. **Function: `add_contact`** - **Input**: A shelve object, a string as the contact name, and a dictionary with phone number and email. - **Output**: None - **Constraints**: The contact name should not already exist in the shelve database. 3. **Function: `get_contact`** - **Input**: A shelve object and a string as the contact name. - **Output**: A dictionary containing the phone number and email if the contact exists, otherwise `None`. 4. **Function: `update_contact`** - **Input**: A shelve object, a string as the contact name, and a dictionary with the updated phone number and email. - **Output**: None - **Constraints**: The contact should exist in the shelve database. 5. **Function: `delete_contact`** - **Input**: A shelve object and a string as the contact name. - **Output**: None - **Constraints**: The contact should exist in the shelve database. 6. **Function: `list_all_contacts`** - **Input**: A shelve object. - **Output**: A dictionary where keys are contact names and values are dictionaries with phone number and email. Additional Requirements - Make sure to handle necessary exceptions such as `KeyError` when contacts do not exist. - Ensure that the database is properly closed after operations using context management (`with` statement). Example Usage ```python # Initialize the database shelve_db = initialize_contacts_db(\\"contacts.db\\") # Add a new contact add_contact(shelve_db, \\"John Doe\\", {\\"phone\\": \\"123-456-7890\\", \\"email\\": \\"john.doe@example.com\\"}) # Retrieve and display a contact contact = get_contact(shelve_db, \\"John Doe\\") print(contact) # Output: {\'phone\': \'123-456-7890\', \'email\': \'john.doe@example.com\'} # Update the contact update_contact(shelve_db, \\"John Doe\\", {\\"phone\\": \\"098-765-4321\\", \\"email\\": \\"john.new@example.com\\"}) # List all contacts all_contacts = list_all_contacts(shelve_db) print(all_contacts) # Output: {\'John Doe\': {\'phone\': \'098-765-4321\', \'email\': \'john.new@example.com\'}} # Delete a contact delete_contact(shelve_db, \\"John Doe\\") # The contact should no longer exist contact = get_contact(shelve_db, \\"John Doe\\") print(contact) # Output: None shelve_db.close() ``` # Submission Guidelines Submit the Python script containing all the defined functions. Ensure that the functions follow the input/output constraints and handle errors gracefully. Also include comments to explain the code logic where necessary.","solution":"import shelve def initialize_contacts_db(filename): Initializes and returns a shelve database with writeback=True. return shelve.open(filename, writeback=True) def add_contact(db, name, contact_info): Adds a new contact to the shelve database. if name in db: raise KeyError(f\\"Contact \'{name}\' already exists.\\") db[name] = contact_info def get_contact(db, name): Retrieves a contact by name from the shelve database. return db.get(name) def update_contact(db, name, contact_info): Updates an existing contact in the shelve database. if name not in db: raise KeyError(f\\"Contact \'{name}\' does not exist.\\") db[name] = contact_info def delete_contact(db, name): Deletes a contact by name from the shelve database. if name not in db: raise KeyError(f\\"Contact \'{name}\' does not exist.\\") del db[name] def list_all_contacts(db): Lists all contacts in the shelve database. return dict(db)"},{"question":"**Python Coding Assessment Question:** You are required to implement a function that processes a list of numbers in varying formats and returns a summary of properties for the resulting fractions. # Function Signature ```python def process_fractions(data: List[Union[int, float, str, Decimal]]) -> List[Dict[str, Union[int, float, str, Tuple[int, int]]]]: ``` # Input - `data`: A list of numbers in various formats (`int`, `float`, `str`, `Decimal`). Each element represents either an integer, float, string, or a decimal number that can be converted to a `Fraction`. # Output - A list of dictionaries. Each dictionary contains: - `fraction`: The fraction in the form of a string (e.g., `1/2`). - `numerator`: The numerator of the fraction. - `denominator`: The denominator of the fraction. - `as_integer_ratio`: The integer ratio of the fraction as a tuple. - `closest_fraction_within_limit`: The closest fraction with a denominator of at most 100 using the `limit_denominator` method. - `floor`: The greatest integer less than or equal to the fraction. - `ceil`: The least integer greater than or equal to the fraction. - `rounded`: The nearest integer to the fraction. # Constraints - You must handle invalid inputs gracefully by skipping them and not including them in the output list. - Denominators must not be zero. - The input list can contain up to 10,000 elements. # Example ```python from decimal import Decimal data = [1, 2.5, \'3/7\', Decimal(\'4.2\'), \'invalid\'] result = process_fractions(data) print(result) # Output should be a list of dictionaries with the properties described above # [ # {\'fraction\': \'1/1\', \'numerator\': 1, \'denominator\': 1, \'as_integer_ratio\': (1, 1), \'closest_fraction_within_limit\': \'1/1\', \'floor\': 1, \'ceil\': 1, \'rounded\': 1}, # {\'fraction\': \'5/2\', \'numerator\': 5, \'denominator\': 2, \'as_integer_ratio\': (5, 2), \'closest_fraction_within_limit\': \'5/2\', \'floor\': 2, \'ceil\': 3, \'rounded\': 2}, # {\'fraction\': \'3/7\', \'numerator\': 3, \'denominator\': 7, \'as_integer_ratio\': (3, 7), \'closest_fraction_within_limit\': \'3/7\', \'floor\': 0, \'ceil\': 1, \'rounded\': 0}, # {\'fraction\': \'21/5\', \'numerator\': 21, \'denominator\': 5, \'as_integer_ratio\': (21, 5), \'closest_fraction_within_limit\': \'21/5\', \'floor\': 4, \'ceil\': 5, \'rounded\': 4} # ] ``` **Hint:** Use the `fractions.Fraction` class extensively to handle operations and conversions.","solution":"from fractions import Fraction from decimal import Decimal from typing import List, Union, Dict, Tuple def process_fractions(data: List[Union[int, float, str, Decimal]]) -> List[Dict[str, Union[int, float, str, Tuple[int, int]]]]: results = [] for item in data: try: # Convert item to Fraction fraction = Fraction(item) except (ValueError, TypeError, ZeroDivisionError): continue # Gather properties of the fraction fraction_str = f\\"{fraction.numerator}/{fraction.denominator}\\" as_integer_ratio = fraction.as_integer_ratio() closest_fraction_within_limit = fraction.limit_denominator(100) properties = { \\"fraction\\": fraction_str, \\"numerator\\": fraction.numerator, \\"denominator\\": fraction.denominator, \\"as_integer_ratio\\": as_integer_ratio, \\"closest_fraction_within_limit\\": f\\"{closest_fraction_within_limit.numerator}/{closest_fraction_within_limit.denominator}\\", \\"floor\\": fraction.__floor__(), \\"ceil\\": fraction.__ceil__(), \\"rounded\\": round(fraction), } results.append(properties) return results"},{"question":"Your task is to implement a utility class called `WarningManager` that encapsulates the functionality for managing warnings in a Python program. The `WarningManager` class should provide methods to: 1. **Issue Warnings**: A method to issue different categories of warnings. 2. **Filter Warnings**: A method to apply different filters to warnings based on actions, categories, and modules. 3. **Suppress Warnings Temporarily**: A method to temporarily suppress specific warnings and restore the state after executing a block of code. 4. **Capture Warnings for Testing**: A method to capture warnings triggered during the execution of a block of code for testing purposes. # Class Specification `WarningManager` - **Methods**: - `__init__(self)`: Initialize an empty `WarningManager` instance. - `issue_warning(self, message: str, category: type, stacklevel: int = 1)`: Issue a warning with the specified message, category, and optional stack level. - `apply_filter(self, action: str, message: str = \'\', category: type = Warning, module: str = \'\', lineno: int = 0, append: bool = False)`: Apply a warning filter with the specified action, message, category, module, and line number. - `suppress_warnings_temporarily(self, category: type)`: Suppress warnings of the specified category temporarily within a context manager. - `capture_warnings(self)`: Capture warnings in a context manager for testing purposes. # Example Usage ```python import warnings from warnings import WarningManager, DeprecationWarning, UserWarning # Initialize WarningManager wm = WarningManager() # Issue a DeprecationWarning wm.issue_warning(\\"This is a deprecated feature.\\", DeprecationWarning) # Apply a filter to ignore DeprecationWarnings wm.apply_filter(\\"ignore\\", category=DeprecationWarning) # Temporarily suppress UserWarning with wm.suppress_warnings_temporarily(UserWarning): warnings.warn(\\"This is a user warning.\\", UserWarning) # Capture warnings for testing with wm.capture_warnings() as captured: warnings.warn(\\"This is a test warning.\\", UserWarning) assert len(captured) == 1 ``` # Constraints - Do not use any libraries or modules that are not part of Python\'s standard library. - The methods should handle the specified categories and message strings correctly. - Ensure that the context managers restore the original state after exiting. # Note - You can refer to the Python documentation for the `warnings` module for more details on specific functions and their usage. Implement the `WarningManager` class and its methods as described above.","solution":"import warnings from contextlib import contextmanager class WarningManager: def __init__(self): pass def issue_warning(self, message: str, category: type, stacklevel: int = 1): warnings.warn(message, category, stacklevel=stacklevel) def apply_filter(self, action: str, message: str = \'\', category: type = Warning, module: str = \'\', lineno: int = 0, append: bool = False): warnings.filterwarnings(action, message=message, category=category, module=module, lineno=lineno, append=append) @contextmanager def suppress_warnings_temporarily(self, category: type): original_filters = warnings.filters[:] self.apply_filter(\\"ignore\\", category=category) try: yield finally: warnings.filters = original_filters @contextmanager def capture_warnings(self): with warnings.catch_warnings(record=True) as captured: warnings.simplefilter(\\"always\\") yield captured"},{"question":"Coding Assessment Question # Objective Implement a custom container class using Python\'s `collections.abc` abstract base classes. The class should adhere to the interface and behaviors defined by these ABCs, demonstrating your understanding of container APIs and Python\'s object-oriented programming principles. # Requirements 1. **Class Name:** `CustomSet` 2. **Base Class:** Inherit from `collections.abc.MutableSet`. 3. **Functionality:** - Support standard set operations such as `add`, `discard`, `pop`, `clear`, and set algebra (`&`, `|`, `^`, `-`). - Use a list data structure internally to store elements. This means elements will not need to be hashable. - Ensure that all methods defined in the `MutableSet` interface are implemented. # Expected Methods - `__init__(self, iterable)` - `__contains__(self, item)` - `__iter__(self)` - `__len__(self)` - `add(self, item)` - `discard(self, item)` - `pop(self)` - `clear(self)` - Additional methods as required by `MutableSet`. # Performance Constraints - The implementation should ensure that operations are efficient with respect to the underlying list structure. - Aim for O(n) complexity for `__contains__` and O(1) for operations like `add` and `discard`. # Input and Output Formats Your class should be tested against the following scenarios: Example Usage: ```python # Initialize the custom set with a few elements cs = CustomSet([1, 2, 3]) # Add elements cs.add(4) cs.add(2) # No effect, duplicate # Remove elements cs.discard(1) try: cs.pop() except KeyError as e: print(e) # Check membership print(3 in cs) # True print(1 in cs) # False # Clear the set cs.clear() print(len(cs)) # 0 # Set algebra cs1 = CustomSet([1, 2, 3]) cs2 = CustomSet([3, 4, 5]) print(cs1 & cs2) # CustomSet([3]) print(cs1 | cs2) # CustomSet([1, 2, 3, 4, 5]) print(cs1 - cs2) # CustomSet([1, 2]) print(cs1 ^ cs2) # CustomSet([1, 2, 4, 5]) ``` # Constraints - Elements added to the `CustomSet` should be comparable using `==` and `!=`. - No duplicate elements should exist within the internal list representation. - Raise `TypeError` if non-iterable input is provided during initialization. Implement your `CustomSet` class within the provided constraints and adhere to best practices in Python programming.","solution":"from collections.abc import MutableSet class CustomSet(MutableSet): def __init__(self, iterable=None): self._data = [] if iterable: try: for item in iterable: if item not in self._data: self._data.append(item) except TypeError: raise TypeError(\'Input is not iterable\') def __contains__(self, item): return item in self._data def __iter__(self): return iter(self._data) def __len__(self): return len(self._data) def add(self, item): if item not in self._data: self._data.append(item) def discard(self, item): if item in self._data: self._data.remove(item) def pop(self): if self._data: return self._data.pop() raise KeyError(\'pop from empty set\') def clear(self): self._data.clear() def __repr__(self): return f\\"CustomSet({self._data})\\" def __and__(self, other): if not isinstance(other, CustomSet): return NotImplemented return CustomSet(item for item in self if item in other) def __or__(self, other): if not isinstance(other, CustomSet): return NotImplemented new_set = CustomSet(self._data) for item in other: new_set.add(item) return new_set def __sub__(self, other): if not isinstance(other, CustomSet): return NotImplemented return CustomSet(item for item in self if item not in other) def __xor__(self, other): if not isinstance(other, CustomSet): return NotImplemented return (self | other) - (self & other)"},{"question":"# Python Type Creation and Management Using the Python C-API for type objects, we can manage and create new heap-allocated types with specific attributes and methods. In this assessment, you are required to: 1. **Create a New Type using `PyType_FromSpecWithBases`**: - Define a new type named `CustomObject`. - This type should have: - A `name` attribute that is initialized to a string value provided at object creation. - A `value` attribute which is an integer initialized to 0 by default. - Provide a method named `set_value` which sets the `value` attribute. - Provide a method named `get_value` which returns the `value` attribute. 2. **Instantiate Objects of the New Type**: - Create two instances of `CustomObject` with the name attributes \'ObjectA\' and \'ObjectB\'. 3. **Interact with the Objects**: - Set the `value` attributes of \'ObjectA\' to 10 and \'ObjectB\' to 20 using the `set_value` method. - Retrieve and print the `value` attributes using the `get_value` method for both objects. 4. **Type Management**: - Demonstrate the use of `PyType_Check` and `PyType_IsSubtype` for the created objects. # Required functionalities: - Implement functions or methods to perform the tasks listed above. - Use the provided documentation to understand and use the C-API functions for type creation and management. Input and Output Format - There are no specific input parameters. - Output should be the printed values of `value` for both objects after setting them, verifying that `get_value` works correctly. Performance Constraints: - Use Python\'s default memory allocation mechanism effectively. - Ensure methods perform necessary error checking and carefully manage references to prevent memory leaks. # Example: ```python # Instantiate objects objA = CustomObject(\\"ObjectA\\") objB = CustomObject(\\"ObjectB\\") # Setting values objA.set_value(10) objB.set_value(20) # Getting values print(objA.get_value()) # Output: 10 print(objB.get_value()) # Output: 20 ``` Good luck implementing your custom type and demonstrating your understanding of Python\'s type management!","solution":"class CustomObject: def __init__(self, name, value=0): self.name = name self.value = value def set_value(self, value): self.value = value def get_value(self): return self.value # Instantiate objects objA = CustomObject(\\"ObjectA\\") objB = CustomObject(\\"ObjectB\\") # Setting values objA.set_value(10) objB.set_value(20) # Getting values print(objA.get_value()) # Output: 10 print(objB.get_value()) # Output: 20"},{"question":"You are provided with a neural network model for a simple regression task, and your task is to train this model using the MPS backend on MacOS devices with an MPS-enabled GPU. If the MPS backend is not available, the training should fall back to using the CPU. # Instructions: 1. Create a custom PyTorch neural network model for a linear regression task. 2. Implement the training loop for this model. 3. Ensure that the training uses the MPS device if it is available; otherwise, use the CPU. 4. Your solution should include code to check for MPS availability and move the model and training data to the appropriate device. 5. Train the model on a given dataset, and output the loss after each epoch. # Input: - Randomly generated dataset with features `X` (shape: `(100, 1)`) and target `y` (shape: `(100, 1)`). # Output: - Print the loss after each epoch. # Example Code Structure: ```python import torch import torch.nn as nn import torch.optim as optim # Define the neural network model class SimpleLinearRegressionModel(nn.Module): def __init__(self): super(SimpleLinearRegressionModel, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): return self.linear(x) # Check for MPS availability and set device accordingly if torch.backends.mps.is_available(): device = torch.device(\\"mps\\") else: device = torch.device(\\"cpu\\") # Generate random dataset X = torch.randn(100, 1).to(device) y = torch.randn(100, 1).to(device) # Initialize the model, loss function, and optimizer model = SimpleLinearRegressionModel().to(device) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop num_epochs = 100 for epoch in range(num_epochs): model.train() optimizer.zero_grad() outputs = model(X) loss = criterion(outputs, y) loss.backward() optimizer.step() # Print loss print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') ``` # Constraints: - You must use PyTorch as the deep learning framework. - The code should run on both MPS-enabled MacOS devices and regular CPUs. This problem assesses your understanding of device management, model creation, and training loops in PyTorch.","solution":"import torch import torch.nn as nn import torch.optim as optim # Define the neural network model class SimpleLinearRegressionModel(nn.Module): def __init__(self): super(SimpleLinearRegressionModel, self).__init__() self.linear = nn.Linear(1, 1) def forward(self, x): return self.linear(x) # Check for MPS availability and set device accordingly if torch.backends.mps.is_available(): device = torch.device(\\"mps\\") else: device = torch.device(\\"cpu\\") # Generate random dataset X = torch.randn(100, 1).to(device) y = torch.randn(100, 1).to(device) # Initialize the model, loss function, and optimizer model = SimpleLinearRegressionModel().to(device) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop num_epochs = 100 def train_model(num_epochs, model, criterion, optimizer, X, y, device): losses = [] for epoch in range(num_epochs): model.train() optimizer.zero_grad() outputs = model(X) loss = criterion(outputs, y) loss.backward() optimizer.step() # Print loss epoch_loss = loss.item() print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\') losses.append(epoch_loss) return losses # Train the model losses = train_model(num_epochs, model, criterion, optimizer, X, y, device)"},{"question":"**Problem Statement:** Considering the deprecation and sensitive nature of the `spwd` module, you are tasked with writing functions that will help securely handle the shadow password database information of a Unix system. Your functions should demonstrate a comprehensive understanding of the `spwd` module and error handling. # Task 1. **Function 1: get_user_shadow_info** - **Description**: Implement a function `get_user_shadow_info(user: str) -> dict` that retrieves the shadow password database entry for a given user name and returns it in a dictionary format. - **Input**: A string `user` representing the login name. - **Output**: A dictionary with the following keys and corresponding values for the given user: ```python { \'sp_namp\': str, \'sp_pwdp\': str, \'sp_lstchg\': int, \'sp_min\': int, \'sp_max\': int, \'sp_warn\': int, \'sp_inact\': int, \'sp_expire\': int, \'sp_flag\': int } ``` - **Constraints & Requirements**: - Raise a `ValueError` with a descriptive message if the user does not exist. - Handle `PermissionError` appropriately and propagate an error message indicating insufficient privileges. - The function must ensure secure handling of the retrieved shadow information to prevent sensitive data leakage. 2. **Function 2: get_all_shadow_info** - **Description**: Implement a function `get_all_shadow_info() -> list` that retrieves all available shadow password database entries and returns them in a list of dictionaries. - **Output**: A list of dictionaries, where each dictionary represents shadow password information structured similarly to the output in Function 1. - **Constraints & Requirements**: - Handle `PermissionError` appropriately and propagate an error message indicating insufficient privileges. - The function must ensure secure handling of the retrieved data to prevent sensitive information leakage. - Optimize for performance when dealing with a potentially large number of entries. # Example ```python # Example usage of get_user_shadow_info try: user_info = get_user_shadow_info(\\"root\\") print(user_info) except ValueError as ve: print(\\"Error:\\", ve) except PermissionError as pe: print(\\"Error:\\", pe) # Example output for get_all_shadow_info try: all_users_info = get_all_shadow_info() for info in all_users_info: print(info) except PermissionError as pe: print(\\"Error:\\", pe) ``` Implement these functions in a Python module. # Note - Remember to test your functions with appropriate unit tests to ensure they handle various edge cases, such as non-existent users and permission errors. - Ensure your code adheres to best security practices because of the sensitive nature of the data involved. - Document any assumptions and describe the approach taken to solve the problem.","solution":"import spwd def get_user_shadow_info(user: str) -> dict: Retrieves the shadow password database entry for a given user name and returns it in a dictionary format. :param user: The user name to fetch the shadow entry for. :return: A dictionary containing shadow password information for the given user. :raises ValueError: If the user does not exist. :raises PermissionError: If there are insufficient privileges to access the shadow password database. try: entry = spwd.getspnam(user) except KeyError: raise ValueError(\\"User does not exist.\\") except PermissionError: raise PermissionError(\\"Insufficient privileges to access the shadow password database.\\") return { \'sp_namp\': entry.sp_namp, \'sp_pwdp\': entry.sp_pwdp, \'sp_lstchg\': entry.sp_lstchg, \'sp_min\': entry.sp_min, \'sp_max\': entry.sp_max, \'sp_warn\': entry.sp_warn, \'sp_inact\': entry.sp_inact, \'sp_expire\': entry.sp_expire, \'sp_flag\': entry.sp_flag } def get_all_shadow_info() -> list: Retrieves all available shadow password database entries and returns them in a list of dictionaries. :return: A list of dictionaries containing shadow password information for all users. :raises PermissionError: If there are insufficient privileges to access the shadow password database. try: entries = spwd.getspall() except PermissionError: raise PermissionError(\\"Insufficient privileges to access the shadow password database.\\") result = [] for entry in entries: result.append({ \'sp_namp\': entry.sp_namp, \'sp_pwdp\': entry.sp_pwdp, \'sp_lstchg\': entry.sp_lstchg, \'sp_min\': entry.sp_min, \'sp_max\': entry.sp_max, \'sp_warn\': entry.sp_warn, \'sp_inact\': entry.sp_inact, \'sp_expire\': entry.sp_expire, \'sp_flag\': entry.sp_flag }) return result"},{"question":"Objective: To assess your understanding and ability to manipulate email messages using the `email.message.EmailMessage` class in Python. Task: You are required to implement a function `create_email(subject, sender, recipient, body, is_html=False)` that constructs an email message with the provided details and returns the serialized string representation of the `EmailMessage` object. Additionally, you need to implement two helper functions: `get_recipients(email_message)` and `is_body_html(email_message)`. Function Definitions: 1. **create_email(subject, sender, recipient, body, is_html=False)** * **Inputs:** - `subject` (str): The subject line of the email. - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `body` (str): The body of the email message. - `is_html` (bool): A flag indicating whether the body is HTML content or plain text. Default is False. * **Output:** - Returns a string containing the serialized email message. 2. **get_recipients(email_message)** * **Inputs:** - `email_message` (EmailMessage): An instance of `EmailMessage`. * **Output:** - Returns a list of recipient email addresses. 3. **is_body_html(email_message)** * **Inputs:** - `email_message` (EmailMessage): An instance of `EmailMessage`. * **Output:** - Returns a boolean indicating whether the body of the email is HTML content. Constraints: - Each email should have a valid `subject`, `sender`, `recipient`, and `body`. - The `recipient` field will always be a single email address for this task. - Assume that the email addresses provided are valid. Example: ```python from email.message import EmailMessage def create_email(subject, sender, recipient, body, is_html=False): Create an email message with the specified details. # Implement this function def get_recipients(email_message): Retrieve the recipient(s) from the email message. # Implement this function def is_body_html(email_message): Determine if the body of the email is HTML content. # Implement this function # Usage Example email_msg_str = create_email( subject=\\"Meeting Reminder\\", sender=\\"alice@example.com\\", recipient=\\"bob@example.com\\", body=\\"<p>Don\'t forget our meeting tomorrow at 10am.</p>\\", is_html=True ) email_msg = EmailMessage() email_msg.set_content(email_msg_str) print(get_recipients(email_msg)) # Output: [\'bob@example.com\'] print(is_body_html(email_msg)) # Output: True ``` You are expected to implement the logic for all three functions ensuring they work correctly as per the given specifications.","solution":"from email.message import EmailMessage def create_email(subject, sender, recipient, body, is_html=False): Create an email message with the specified details. msg = EmailMessage() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient if is_html: msg.add_alternative(body, subtype=\'html\') else: msg.set_content(body) return msg.as_string() def get_recipients(email_message): Retrieve the recipient(s) from the email message. return email_message.get_all(\'To\', []) def is_body_html(email_message): Determine if the body of the email is HTML content. return email_message.get_content_type() == \'text/html\'"},{"question":"# Command-Line Parser using argparse You are tasked with creating a command-line tool using the `argparse` module. This tool will be named `file_tool.py`, and it should support the following functionalities: - Accept a list of files and perform operations on them. - The list of files should be passed as positional arguments. - There should be an optional argument `--operation` which takes two possible values: `concat` or `count`. - If `concat` is selected, the tool should concatenate the contents of all files and print them. - If `count` is selected, the tool should count the total number of lines across all files and print the count. - The tool should support a verbosity flag `-v`, allowing the user to specify how many times it should print \\"Verbose mode enabled\\". - Implement proper help messages for all supported arguments. # Input and Output Formats - **Input:** - Positional arguments: list of file paths. - Optional arguments: - `--operation {concat,count}`: Specifies the operation to perform. - `-v`: Verbosity flag, can be specified multiple times. - **Output:** - Based on the selected operation: - For `concat`: concatenated contents of the provided files. - For `count`: total number of lines across the provided files. - Messages indicating the level of verbosity if `-v` is specified. # Constraints - The files should be text files and must be readable. - You have to implement error handling to ensure the program gracefully handles cases where files cannot be opened. - Ensure the program displays appropriate help messages when `-h` or `--help` is used. # Performance Requirements - The tool should efficiently handle files up to the size of a standard text file (a few MBs). - The solution should provide clear and concise user feedback for both successful operations and errors. # Example ``` python file_tool.py file1.txt file2.txt --operation concat -vv Verbose mode enabled Verbose mode enabled Concatenated content of file1.txt and file2.txt python file_tool.py file1.txt file2.txt --operation count -v Verbose mode enabled Total line count: 150 ``` Implement the `file_tool.py` script according to the above specifications.","solution":"import argparse def parse_args(): parser = argparse.ArgumentParser(description=\\"File tool for performing operations on files\\") parser.add_argument(\'files\', nargs=\'+\', help=\'List of files to process\') parser.add_argument(\'--operation\', choices=[\'concat\', \'count\'], help=\'Operation to perform on the files: concat or count\') parser.add_argument(\'-v\', action=\'count\', default=0, help=\'Increase verbosity (can be used multiple times)\') return parser.parse_args() def print_verbose(args): for _ in range(args.v): print(\\"Verbose mode enabled\\") def concat_files(files): contents = [] for file in files: try: with open(file, \'r\') as f: contents.append(f.read()) except Exception as e: print(f\\"Error reading {file}: {e}\\") return None return \'\'.join(contents) def count_lines(files): line_count = 0 for file in files: try: with open(file, \'r\') as f: line_count += sum(1 for _ in f) except Exception as e: print(f\\"Error reading {file}: {e}\\") return None return line_count def main(): args = parse_args() print_verbose(args) if args.operation == \'concat\': result = concat_files(args.files) if result is not None: print(result) elif args.operation == \'count\': result = count_lines(args.files) if result is not None: print(f\\"Total line count: {result}\\") if __name__ == \'__main__\': main()"},{"question":"# Semi-Supervised Learning with Label Propagation Problem Description You are provided with a partially labeled dataset in which some samples are labeled and some are not. Your task is to implement a function that uses the `LabelPropagation` model from `sklearn.semi_supervised` to predict the labels for the unlabeled data points. After training the model, you should return the accuracy of the model using cross-validation on the labeled data. Function Signature ```python def semi_supervised_label_propagation(X: np.ndarray, y: np.ndarray, n_splits: int = 5) -> float: Applies semi-supervised learning using Label Propagation and evaluates its performance. Parameters: - X: np.ndarray, shape (n_samples, n_features) - Feature matrix. - y: np.ndarray, shape (n_samples,) - Labels, with -1 indicating unlabeled data. - n_splits: int - Number of cross-validation splits (default is 5). Returns: - float: Cross-validation accuracy of the model. # Your code here ``` Input - `X`: A numpy array of shape (n_samples, n_features) containing the feature values of the samples. - `y`: A numpy array of shape (n_samples,) containing the labels of the samples, with `-1` indicating unlabeled samples. - `n_splits`: Integer indicating the number of splits for cross-validation (default is 5). Output - A float representing the accuracy of the model using cross-validation on the labeled data. Example ```python import numpy as np # Example data X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) y = np.array([0, -1, -1, 1, 1]) accuracy = semi_supervised_label_propagation(X, y) print(\\"Cross-validation accuracy:\\", accuracy) ``` Constraints - The method should handle datasets with a high proportion of unlabeled data. - Ensure compatibility with scikit-learn\'s LabelPropagation model. - The solution should be efficient enough to handle standard semi-supervised learning tasks. You may assume that: - `X` and `y` are properly aligned and formatted. - Necessary libraries such as scikit-learn are available. Hints - Use `LabelPropagation` from `sklearn.semi_supervised`. - Utilize scikit-learn\'s `cross_val_score` for cross-validation. - Ensure to handle the unlabeled data (`-1` labels) appropriately.","solution":"import numpy as np from sklearn.semi_supervised import LabelPropagation from sklearn.model_selection import cross_val_score def semi_supervised_label_propagation(X: np.ndarray, y: np.ndarray, n_splits: int = 5) -> float: Applies semi-supervised learning using Label Propagation and evaluates its performance. Parameters: - X: np.ndarray, shape (n_samples, n_features) - Feature matrix. - y: np.ndarray, shape (n_samples,) - Labels, with -1 indicating unlabeled data. - n_splits: int - Number of cross-validation splits (default is 5). Returns: - float: Cross-validation accuracy of the model. label_propagation_model = LabelPropagation() # The cross_val_score function in sklearn requires labeled points, so we\'ll only use the labeled points. labeled_indices = y != -1 # Perform cross-validation using only the labeled data scores = cross_val_score( label_propagation_model, X[labeled_indices], y[labeled_indices], cv=n_splits, scoring=\'accuracy\' ) return scores.mean()"},{"question":"You are tasked with implementing a simplified version of Python\'s floating-point handling using pure Python, simulating some of the features offered by the Python C API for PyFloatObjects. You need to create a class `PyFloat` that provides functionality to create, check, and manipulate floating-point numbers. # Requirements: 1. Implement the `PyFloat` class that handles floating-point numbers. 2. Implement the following methods: - `__init__(self, value)`: Initialize with a float value. Raise a `TypeError` if the value cannot be converted to a float. - `from_string(cls, string)`: Class method to create a `PyFloat` instance from a string. Raise a `ValueError` if the string is not a valid float representation. - `as_double(self)`: Convert the `PyFloat` instance to a Python float. - `get_info()`: Class method to return a dictionary containing information about the float type (precision, min, and max values similar to `float.h` in C). 3. Ensure `PyFloat` can be compared and printed, and handle special cases like `inf` and `nan`. # Input / Output: - Use float values for initialization and operations. - Methods should handle exceptions gracefully and provide meaningful error messages. # Example: ```python class PyFloat: def __init__(self, value): # your code here @classmethod def from_string(cls, string): # your code here def as_double(self): # your code here @classmethod def get_info(cls): # your code here # Implement comparison methods def __eq__(self, other): # your code here def __lt__(self, other): # your code here # String representation def __repr__(self): # your code here # Example usage: pf = PyFloat(3.14) print(pf.as_double()) # Output: 3.14 pf_str = PyFloat.from_string(\\"2.718\\") print(pf_str.as_double()) # Output: 2.718 print(PyFloat.get_info()) # Output: {\'precision\': 53, \'min\': 2.2250738585072014e-308, \'max\': 1.7976931348623157e+308} ``` # Constraints: - Do not use any external libraries. - Ensure compatibility with Python 3.x. - Handle edge cases gracefully, such as invalid inputs and conversion errors.","solution":"import sys import math class PyFloat: def __init__(self, value): try: self.value = float(value) except ValueError: raise TypeError(f\\"Cannot convert {value} to float.\\") @classmethod def from_string(cls, string): try: value = float(string) return cls(value) except ValueError: raise ValueError(f\\"Cannot convert string {string} to float.\\") def as_double(self): return self.value @classmethod def get_info(cls): return { \'precision\': sys.float_info.mant_dig, \'min\': sys.float_info.min, \'max\': sys.float_info.max } def __eq__(self, other): if isinstance(other, PyFloat): return math.isclose(self.value, other.value) return NotImplemented def __lt__(self, other): if isinstance(other, PyFloat): return self.value < other.value return NotImplemented def __repr__(self): return f\\"PyFloat({self.value})\\""},{"question":"# Advanced Python Profiling and Analysis **Objective**: To assess your understanding of Python\'s profiling tools \\"cProfile\\" and \\"pstats\\". **Problem Statement**: You are provided with a piece of code that simulates some complex computations. Your task is to: 1. Profile the code using the \\"cProfile\\" module. 2. Save the profiling results to a file. 3. Load and analyze the profiling data using the \\"pstats\\" module. 4. Perform specific analyses and transformations on the profiling data. **Code to Profile**: ```python import time def function_a(n): total = 0 for i in range(n): total += i return total def function_b(n): total = 0 for i in range(n): total += function_a(i) return total def main(): result1 = function_a(1000) result2 = function_b(1000) return result1, result2 if __name__ == \\"__main__\\": main() ``` **Tasks**: 1. **Profile the Code**: - Use \\"cProfile\\" to profile the `main` function. - Save the profiling results to a file named `profiling_results.prof`. 2. **Analyze the Profiling Data**: - Load the profiling data from `profiling_results.prof` using the \\"pstats\\" module. - Print out all the statistics sorted by cumulative time. 3. **Perform Specific Analyses**: - Print out statistics for the top 10 functions based on time spent within each function. - Print out statistics for functions that contain the string `function_a` in their names. - Print the list of functions that called `function_a`, limiting the output to the top 5 callers. **Constraints**: - Use only the standard library modules `cProfile` and `pstats` for profiling and data analysis. - Ensure efficient code handling and profiling. Avoid unnecessary computations and optimize where possible. **Expected Outputs**: - A file named `profiling_results.prof` containing the profiling data. - Printed statistics from the profiling data based on the specified analyses. **Instructions**: 1. Write a Python script to accomplish the tasks described above. 2. Use comments to document your code and explain your approach. 3. Submit your Python script and the generated profiling file.","solution":"import cProfile import pstats import io import time def function_a(n): total = 0 for i in range(n): total += i return total def function_b(n): total = 0 for i in range(n): total += function_a(i) return total def main(): result1 = function_a(1000) result2 = function_b(1000) return result1, result2 # Step 1: Profile the Code def profile_main(): profiler = cProfile.Profile() profiler.enable() main() profiler.disable() profiler.dump_stats(\'profiling_results.prof\') # Step 2: Analyze the profiling data def analyze_profiling_data(): # Load the profiling data stats = pstats.Stats(\'profiling_results.prof\') # Print out all the statistics sorted by cumulative time print(\\"All statistics sorted by cumulative time:\\") stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats() # Print out statistics for the top 10 functions based on cumulative time print(\\"nTop 10 functions based on cumulative time:\\") stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(10) # Print out statistics for functions that contain the string `function_a` in their names print(\\"nStatistics for functions containing \'function_a\':\\") stats.sort_stats(pstats.SortKey.CUMULATIVE).print_stats(\'function_a\') # Print the list of functions that called `function_a`, limiting the output to the top 5 callers print(\\"nTop 5 callers of `function_a`:\\") stats.sort_stats(pstats.SortKey.CUMULATIVE).print_callers(5, \'function_a\') if __name__ == \\"__main__\\": profile_main() analyze_profiling_data()"},{"question":"# Question: Parallel Task Execution and Management with `concurrent.futures` Objective Your task is to implement a function that utilizes the `concurrent.futures` module to execute a list of tasks in parallel. You will use `ThreadPoolExecutor` for this implementation. The function must handle results, deal with exceptions appropriately, and ensure a clean shutdown of the executor. Function Signature ```python def execute_tasks(tasks: list) -> list: Execute a list of tasks in parallel using ThreadPoolExecutor. Parameters: tasks (list of callable): A list of callable objects (functions) with no arguments that need to be executed in parallel. Returns: list: The result of each task in the order they were submitted. If a task raises an exception, a string \\"Task failed\\" should be added in place of the appropriate result. Note: - The function should ensure that all resources are cleaned up properly. - Each callable in the `tasks` list is expected to either return a value or raise an exception. ``` Input and Output - **Input**: - `tasks`: A list of callable objects with no arguments. Each callable represents a task that needs to be executed. - **Output**: - A list containing the result of each task in the order they were submitted. If a task raises an exception, the string \\"Task failed\\" should be placed in the respective position in the results list. Constraints - Tasks may raise exceptions, and these should be handled gracefully. - You must use `ThreadPoolExecutor` for parallel task execution. - Ensure that the executor is properly shutdown after all tasks are completed or an exception occurs. - The solution should work efficiently for up to 100 tasks. Example ```python import time # Sample tasks def task1(): time.sleep(1) return \\"Task 1 completed\\" def task2(): raise ValueError(\\"Something went wrong in Task 2\\") def task3(): time.sleep(2) return \\"Task 3 completed\\" tasks = [task1, task2, task3] output = execute_tasks(tasks) print(output) # Expected Output: [\'Task 1 completed\', \'Task failed\', \'Task 3 completed\'] ``` Notes - Make sure to handle both successful completion as well as exceptions in the tasks. - Ensure the integrity of the result order relative to the task submission order.","solution":"from concurrent.futures import ThreadPoolExecutor, as_completed def execute_tasks(tasks): results = [] with ThreadPoolExecutor() as executor: future_to_task = {executor.submit(task): i for i, task in enumerate(tasks)} ordered_results = [None] * len(tasks) for future in as_completed(future_to_task): task_index = future_to_task[future] try: result = future.result() ordered_results[task_index] = result except Exception as e: ordered_results[task_index] = \\"Task failed\\" return ordered_results"},{"question":"**Email Client Implementation Using `smtplib`** # Objective: Implement a function named `send_custom_email` which sends an email using the `smtplib` module in Python. The function should support both standard SMTP and SSL connections, allow for optional TLS, and handle errors gracefully. Additionally, the function should support sending emails with multiple recipients and with various ESMTP options. # Function Signature: ```python def send_custom_email( host: str, port: int, use_ssl: bool, from_addr: str, to_addrs: list, subject: str, body: str, username: str = None, password: str = None, use_tls: bool = False, mail_options: list = [], rcpt_options: list = [] ) -> dict: pass ``` # Parameters: - `host` (str): The SMTP server host. - `port` (int): The SMTP server port. - `use_ssl` (bool): Whether to use SSL for the connection. - `from_addr` (str): The sender\'s email address. - `to_addrs` (list): A list of recipient email addresses. - `subject` (str): The subject of the email. - `body` (str): The body of the email. - `username` (str, optional): The username for SMTP authentication. Default is `None`. - `password` (str, optional): The password for SMTP authentication. Default is `None`. - `use_tls` (bool, optional): Whether to use TLS after connecting. Default is `False`. - `mail_options` (list, optional): A list of ESMTP options for the MAIL FROM command. Default is an empty list. - `rcpt_options` (list, optional): A list of ESMTP options for the RCPT TO command. Default is an empty list. # Returns: - A dictionary containing status of email sending for each recipient. If the email is successfully sent to a recipient, the value should be `None`. If there\'s an error, the value should be a tuple containing the error code and error message. # Constraints: - You must handle any exceptions that arise during the sending process and return appropriate error messages. - If `use_ssl` is `True`, SSL should be used from the beginning of the connection. - If `use_tls` is `True`, the connection should be upgraded to TLS after the initial connection. # Example Usage: ```python result = send_custom_email( host=\\"smtp.example.com\\", port=587, use_ssl=False, from_addr=\\"sender@example.com\\", to_addrs=[\\"recipient1@example.com\\", \\"recipient2@example.com\\"], subject=\\"Test Email\\", body=\\"This is a test email.\\", username=\\"username\\", password=\\"password\\", use_tls=True, mail_options=[\\"SMTPUTF8\\"], rcpt_options=[] ) print(result) ``` # Example Output: ```python { \\"recipient1@example.com\\": None, \\"recipient2@example.com\\": (550, b\'User unknown\') } ``` # Notes: - The function should ensure that connections and resources are properly closed after attempting to send the email. - Make sure to include the necessary email headers (e.g., `From`, `To`, `Subject`) in the email message. - Use the appropriate exceptions from the `smtplib` module to handle errors and provide meaningful error messages.","solution":"import smtplib from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart def send_custom_email( host: str, port: int, use_ssl: bool, from_addr: str, to_addrs: list, subject: str, body: str, username: str = None, password: str = None, use_tls: bool = False, mail_options: list = [], rcpt_options: list = [] ) -> dict: try: if use_ssl: smtp_connection = smtplib.SMTP_SSL(host, port) else: smtp_connection = smtplib.SMTP(host, port) smtp_connection.ehlo() if use_tls: smtp_connection.starttls() smtp_connection.ehlo() if username and password: smtp_connection.login(username, password) msg = MIMEMultipart() msg[\'From\'] = from_addr msg[\'To\'] = \\", \\".join(to_addrs) msg[\'Subject\'] = subject msg.attach(MIMEText(body, \'plain\')) status = {} for recipient in to_addrs: try: response = smtp_connection.sendmail(from_addr, recipient, msg.as_string(), mail_options, rcpt_options) status[recipient] = response.get(recipient) except smtplib.SMTPRecipientsRefused as e: status[recipient] = (\'Recipients refused\', str(e)) except smtplib.SMTPSenderRefused as e: status[recipient] = (\'Sender refused\', str(e)) except smtplib.SMTPDataError as e: status[recipient] = (\'Data error\', str(e)) except smtplib.SMTPException as e: status[recipient] = (\'SMTP error\', str(e)) smtp_connection.quit() return status except Exception as e: return {\'error\': str(e)}"},{"question":"Objective You are required to write a function that extracts and processes certain elements from an XML document using the `xml.dom.pulldom` module, demonstrating your understanding of both event-driven and DOM-based processing. Problem Description Given an XML document containing a list of books, your task is to parse the document and return a list of titles of the books that have more than a given number of pages and are published after a certain year. Input - A string representing the XML document. - An integer representing the minimum number of pages. - An integer representing the year of publication. The structure of the XML document is as follows: ```xml <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> <pages>250</pages> <year>2021</year> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <pages>150</pages> <year>2018</year> </book> ... </library> ``` Output - A list of strings, where each string is the title of a book that meets the criteria. Constraints - You must use the `xml.dom.pulldom` module for parsing the XML. - The function should efficiently handle both small and large XML documents. Function Signature ```python def get_filtered_book_titles(xml_string: str, min_pages: int, min_year: int) -> List[str]: ``` Example ```python xml_input = \'\'\' <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> <pages>250</pages> <year>2021</year> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <pages>150</pages> <year>2018</year> </book> <book> <title>Book Title 3</title> <author>Author Name 3</author> <pages>300</pages> <year>2019</year> </book> </library> \'\'\' assert get_filtered_book_titles(xml_input, 200, 2018) == [\'Book Title 1\', \'Book Title 3\'] ``` Additional Notes - Carefully handle nodes and text extraction while iterating through events. - Ensure that your solution does not expand more nodes than necessary to optimize performance.","solution":"from typing import List from xml.dom import pulldom def get_filtered_book_titles(xml_string: str, min_pages: int, min_year: int) -> List[str]: Extracts book titles from an XML string where the number of pages is more than min_pages and the year of publication is after min_year. titles = [] events = pulldom.parseString(xml_string) for event, node in events: if event == pulldom.START_ELEMENT and node.tagName == \'book\': # Expand the current <book> node to examine its children events.expandNode(node) pages = 0 year = 0 title = \\"\\" for child in node.childNodes: if child.nodeType == child.ELEMENT_NODE: if child.tagName == \'title\': title = child.firstChild.data elif child.tagName == \'pages\': pages = int(child.firstChild.data) elif child.tagName == \'year\': year = int(child.firstChild.data) if pages > min_pages and year > min_year: titles.append(title) return titles"},{"question":"**Objective:** To assess your understanding of seaborn\'s plotting capabilities, you are required to write code that demonstrates various plot customizations using the `swarmplot` and `catplot` functions. **Instructions:** 1. **Load Dataset:** - Load the `tips` dataset available in seaborn. 2. **Basic Swarm Plot:** - Create a basic swarm plot to visualize the distribution of total bill amounts. 3. **Categorical Swarm Plot:** - Generate a swarm plot that shows the distribution of total bill amounts for different days of the week. 4. **Hue Customization:** - Create a swarm plot where the points are colored based on gender (`sex`). 5. **Orientation Customization:** - Create a horizontally oriented swarm plot that shows the distribution of total bill amounts by party size. 6. **Faceted Swarm Plot:** - Use `catplot` to create a swarm plot grid showing the total bill amount by time of day, faceted by day of the week, and colored by gender. **Input Format:** - There is no specific input format as you will be using the `tips` dataset loaded through seaborn. **Output Format:** - You should display the visualizations as specified in the instructions. **Constraints:** - You should utilize Seaborn\'s plotting functions to create the visualizations. - Ensure that the plots are properly labeled and color-coded. **Performance Requirements:** - The code should run efficiently within the resources typically available in a standard coding environment (e.g., Jupyter Notebook). # Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Basic Swarm Plot plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill Amounts\\") plt.show() # Categorical Swarm Plot plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill Amounts by Day\\") plt.show() # Swarm Plot with Hue plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Total Bill Amounts by Day and Gender\\") plt.show() # Horizontal Swarm Plot by Party Size plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"size\\", orient=\\"h\\") plt.title(\\"Total Bill Amounts by Party Size\\") plt.show() # Faceted Swarm Plot g = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5 ) g.fig.suptitle(\\"Total Bill Amounts by Time of Day and Gender, Faceted by Day\\", y=1.02) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Basic Swarm Plot def basic_swarm_plot(tips): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\") plt.title(\\"Distribution of Total Bill Amounts\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Density\\") plt.show() # Categorical Swarm Plot def categorical_swarm_plot(tips): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill Amounts by Day\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.show() # Swarm Plot with Hue def swarm_plot_with_hue(tips): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\") plt.title(\\"Total Bill Amounts by Day and Gender\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Day\\") plt.show() # Horizontal Swarm Plot by Party Size def horizontal_swarm_plot_by_party_size(tips): plt.figure(figsize=(10, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"size\\", orient=\\"h\\") plt.title(\\"Total Bill Amounts by Party Size\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Party Size\\") plt.show() # Faceted Swarm Plot def faceted_swarm_plot(tips): g = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=.5 ) g.fig.suptitle(\\"Total Bill Amounts by Time of Day and Gender, Faceted by Day\\", y=1.02) plt.show()"},{"question":"# Unicode Handling in Python Write a Python program to perform the following tasks: 1. Read the content of a UTF-8 encoded text file provided as `input_file` which contains a mix of English, French, and Chinese characters. 2. Encode this content into both UTF-16 and UTF-32 formats. Save these encoded contents into two separate files named `output_utf16.txt` and `output_utf32.txt`. 3. Decode the contents of `output_utf16.txt` and `output_utf32.txt` back to UTF-8 and ensure that the decoded contents match the original content from `input_file`. 4. For any encoding/decoding operation, if an error occurs, handle it using the \'replace\' strategy, where unknown characters should be replaced by the Unicode \'REPLACEMENT CHARACTER\' (U+FFFD). The program should be divided into the following functions: - `read_file(file_path: str) -> str`: Reads the content from the specified file and returns it as a string. - `write_file(file_path: str, content: str, encoding: str) -> None`: Writes the string content to the specified file using the specified encoding. - `encode_content(content: str, encoding: str) -> bytes`: Encodes the given string content using the specified encoding. - `decode_content(encoded_content: bytes, encoding: str) -> str`: Decodes the given byte content using the specified encoding. - `compare_strings(str1: str, str2: str) -> bool`: Compares whether two strings are identical. # Input: - `input_file`: Path to the input text file containing Unicode characters. # Output: - The program should print \\"Success\\" if all steps are executed without errors and the decoded contents match the original content. Otherwise, print \\"Failure\\". # Example: If the content in `input_file` is: ``` Hello, world! 你好，世界! Bonjour, le monde! ``` After encoding to UTF-16 and UTF-32, decoding it back and comparing with original content, the output should be: ``` Success ``` # Solution Template: ```python def read_file(file_path: str) -> str: # Implement this function pass def write_file(file_path: str, content: str, encoding: str) -> None: # Implement this function pass def encode_content(content: str, encoding: str) -> bytes: # Implement this function pass def decode_content(encoded_content: bytes, encoding: str) -> str: # Implement this function pass def compare_strings(str1: str, str2: str) -> bool: # Implement this function pass def main(input_file: str): # Step 1: Read the original content original_content = read_file(input_file) # Step 2: Encode content to UTF-16 and UTF-32 encoded_utf16 = encode_content(original_content, \'utf-16\') encoded_utf32 = encode_content(original_content, \'utf-32\') # Step 3: Write encoded content to files write_file(\'output_utf16.txt\', encoded_utf16, \'utf-16\') write_file(\'output_utf32.txt\', encoded_utf32, \'utf-32\') # Step 4: Read and decode the files\' content back to UTF-8 decoded_utf16 = decode_content(encoded_utf16, \'utf-16\') decoded_utf32 = decode_content(encoded_utf32, \'utf-32\') # Step 5: Compare the decoded content with the original if compare_strings(original_content, decoded_utf16) and compare_strings(original_content, decoded_utf32): print(\\"Success\\") else: print(\\"Failure\\") # Run the main function with the path to the input file. main(\'path_to_input_file.txt\') ``` **Note**: Ensure the `path_to_input_file.txt` is replaced with the actual path of your input text file.","solution":"def read_file(file_path: str) -> str: Reads the content from the specified file and returns it as a string. try: with open(file_path, \'r\', encoding=\'utf-8\', errors=\'replace\') as file: return file.read() except Exception as e: raise IOError(f\\"Error reading file {file_path}: {e}\\") def write_file(file_path: str, content: str, encoding: str) -> None: Writes the string content to the specified file using the specified encoding. try: with open(file_path, \'w\', encoding=encoding, errors=\'replace\') as file: file.write(content) except Exception as e: raise IOError(f\\"Error writing to file {file_path}: {e}\\") def encode_content(content: str, encoding: str) -> bytes: Encodes the given string content using the specified encoding. try: return content.encode(encoding, errors=\'replace\') except Exception as e: raise ValueError(f\\"Error encoding content: {e}\\") def decode_content(encoded_content: bytes, encoding: str) -> str: Decodes the given byte content using the specified encoding. try: return encoded_content.decode(encoding, errors=\'replace\') except Exception as e: raise ValueError(f\\"Error decoding content: {e}\\") def compare_strings(str1: str, str2: str) -> bool: Compares whether two strings are identical. return str1 == str2 def main(input_file: str): # Step 1: Read the original content original_content = read_file(input_file) # Step 2: Encode content to UTF-16 and UTF-32 encoded_utf16 = encode_content(original_content, \'utf-16\') encoded_utf32 = encode_content(original_content, \'utf-32\') # Step 3: Write encoded content to files write_file(\'output_utf16.txt\', encoded_utf16.decode(\'utf-16\'), \'utf-16\') write_file(\'output_utf32.txt\', encoded_utf32.decode(\'utf-32\'), \'utf-32\') # Step 4: Read and decode the files\' content back to UTF-8 decoded_utf16 = decode_content(encoded_utf16, \'utf-16\') decoded_utf32 = decode_content(encoded_utf32, \'utf-32\') # Step 5: Compare the decoded content with the original if compare_strings(original_content, decoded_utf16) and compare_strings(original_content, decoded_utf32): print(\\"Success\\") else: print(\\"Failure\\") # Note: Replace \'path_to_input_file.txt\' with the path to your input file when running the main function. # main(\'path_to_input_file.txt\')"},{"question":"Coding Assessment Question # Objective Your task is to use the scikit-learn library to perform linear regression on a given dataset. You will apply different linear models and compare their performance. # Task 1. **Data Preparation** - Load the provided dataset. - Create training and test splits. - Standardize the features. 2. **Model Implementation** - Implement the following linear regression models using `scikit-learn`: - Ordinary Least Squares (OLS) - Ridge Regression - Lasso Regression - Elastic-Net 3. **Model Evaluation** - For each model, perform a 5-fold cross-validation on the training set. - Evaluate each model on the test set using Mean Squared Error (MSE) and R-squared score. # Dataset You will be provided with a CSV file containing the dataset. The dataset consists of a target variable `y` and multiple feature variables `X`. # Input and Output Format - Input: - A CSV file `dataset.csv`, which includes a target column `y` and multiple feature columns `X1, X2, ...`. - Output: - Print the cross-validation results for each model. - Print the Mean Squared Error (MSE) and R-squared score for each model on the test set. # Constraints 1. Use the `train_test_split` function to split the dataset into 80% training and 20% testing. 2. Use the `StandardScaler` for standardizing the dataset. 3. Use `alpha = 1.0` for Ridge and Lasso regression. 4. For Elastic-Net, use `alpha = 1.0` and `l1_ratio = 0.5`. # Example Output ``` OLS: Cross-validation MSE: [value] Mean Squared Error on test set: [value] R-squared score on test set: [value] Ridge: Cross-validation MSE: [value] Mean Squared Error on test set: [value] R-squared score on test set: [value] Lasso: Cross-validation MSE: [value] Mean Squared Error on test set: [value] R-squared score on test set: [value] Elastic-Net: Cross-validation MSE: [value] Mean Squared Error on test set: [value] R-squared score on test set: [value] ``` # Hints - Utilize `sklearn.linear_model.LinearRegression`, `sklearn.linear_model.Ridge`, `sklearn.linear_model.Lasso`, `sklearn.linear_model.ElasticNet` for the respective models. - Use `sklearn.model_selection.train_test_split` for data splitting. - Use `sklearn.preprocessing.StandardScaler` to standardize the data. - Use `sklearn.model_selection.cross_val_score` for cross-validation. - Use `sklearn.metrics.mean_squared_error`, `sklearn.metrics.r2_score` for evaluation metrics. # Implementation ```python import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error, r2_score # 1. Load dataset data = pd.read_csv(\'dataset.csv\') X = data.drop(columns=[\'y\']).values y = data[\'y\'].values # 2. Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Standardize features scaler = StandardScaler().fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) # 4. Initialize models models = { \'OLS\': LinearRegression(), \'Ridge\': Ridge(alpha=1.0), \'Lasso\': Lasso(alpha=1.0), \'Elastic-Net\': ElasticNet(alpha=1.0, l1_ratio=0.5) } # 5. Evaluate models results = {} for name, model in models.items(): # Cross-validation cv_scores = cross_val_score(model, X_train, y_train, scoring=\'neg_mean_squared_error\', cv=5) cv_mse = -cv_scores.mean() # Fit model model.fit(X_train, y_train) # Predict y_pred = model.predict(X_test) # Evaluate test_mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) results[name] = { \'Cross-validation MSE\': cv_mse, \'Test MSE\': test_mse, \'R-squared\': r2 } # 6. Print results for name, metrics in results.items(): print(f\\"{name}:\\") print(f\\"Cross-validation MSE: {metrics[\'Cross-validation MSE\']}\\") print(f\\"Mean Squared Error on test set: {metrics[\'Test MSE\']}\\") print(f\\"R-squared score on test set: {metrics[\'R-squared\']}\\") print() ``` # Notes - Make sure to handle any missing values appropriately before proceeding with model training. - Thoroughly comment your code and ensure readability. - Pay attention to performance and try to optimize where possible.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, cross_val_score from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet from sklearn.metrics import mean_squared_error, r2_score def perform_linear_regressions(file_path): # 1. Load dataset data = pd.read_csv(file_path) X = data.drop(columns=[\'y\']).values y = data[\'y\'].values # 2. Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Standardize features scaler = StandardScaler().fit(X_train) X_train = scaler.transform(X_train) X_test = scaler.transform(X_test) # 4. Initialize models models = { \'OLS\': LinearRegression(), \'Ridge\': Ridge(alpha=1.0), \'Lasso\': Lasso(alpha=1.0), \'Elastic-Net\': ElasticNet(alpha=1.0, l1_ratio=0.5) } # 5. Evaluate models results = {} for name, model in models.items(): # Cross-validation cv_scores = cross_val_score(model, X_train, y_train, scoring=\'neg_mean_squared_error\', cv=5) cv_mse = -cv_scores.mean() # Fit model model.fit(X_train, y_train) # Predict y_pred = model.predict(X_test) # Evaluate test_mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) results[name] = { \'Cross-validation MSE\': cv_mse, \'Test MSE\': test_mse, \'R-squared\': r2 } # 6. Print results for name, metrics in results.items(): print(f\\"{name}:\\") print(f\\"Cross-validation MSE: {metrics[\'Cross-validation MSE\']}\\") print(f\\"Mean Squared Error on test set: {metrics[\'Test MSE\']}\\") print(f\\"R-squared score on test set: {metrics[\'R-squared\']}\\") print() return results"},{"question":"**Problem Statement:** You are tasked with creating a Python utility using the `telnetlib` library that connects to a remote Telnet server, sends a series of commands, and logs the output to a file. This utility should be robust, handling common issues such as connection timeouts and unexpected server responses. **Requirements:** 1. Implement a function `telnet_remote_utility(host: str, port: int, commands: list, output_file: str, timeout: int = 10) -> None` that: - Connects to a Telnet server specified by `host` and `port`. - Uses a timeout for connection and operations as specified by the `timeout` parameter. - Sends a list of commands provided in the `commands` list to the server. - Logs the output received from the server for each command to the specified `output_file`. 2. The function should handle errors gracefully: - If the connection cannot be established within the timeout, log an appropriate message and terminate. - If any command does not receive an expected response, log an appropriate message and continue with the next command. 3. Ensure that the Telnet connection is properly closed after all commands are executed or if an error occurs. **Example Flow:** ```python def telnet_remote_utility(host: str, port: int, commands: list, output_file: str, timeout: int = 10) -> None: # Your implementation here # Example usage: commands = [\\"echo \'Hello World\'\\", \\"ls -l\\", \\"whoami\\"] telnet_remote_utility(\\"localhost\\", 23, commands, \\"output_log.txt\\") ``` **Constraints:** - You must use the `telnetlib` library to manage the Telnet connection. - Handle exceptions appropriately to ensure no crash during execution and meaningful logs are maintained. - The `output_file` should contain the output of each command, preceded by the command itself for clarity. **Note:** - Each command\'s execution should follow a pattern of sending the command, reading the response, and logging it. - Consider edge cases such as an empty response or partial responses due to network issues.","solution":"import telnetlib import logging def telnet_remote_utility(host: str, port: int, commands: list, output_file: str, timeout: int = 10) -> None: Connect to a Telnet server, execute commands and log outputs to a file. :param host: Telnet server hostname or IP address :param port: Telnet server port number :param commands: List of commands to send to the server :param output_file: File path to log the output :param timeout: Timeout for connection and operations logging.basicConfig(filename=output_file, level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') try: session = telnetlib.Telnet(host, port, timeout) except Exception as e: logging.error(f\\"Failed to connect to {host}:{port} within {timeout} seconds: {e}\\") return try: for command in commands: try: session.write(command.encode(\'ascii\') + b\\"n\\") output = session.read_until(b\\"n\\", timeout).decode(\'ascii\') logging.info(f\\"Command: {command}nOutput: {output}\\") except EOFError: logging.warning(f\\"Command `{command}` did not receive a response from the server.\\") except Exception as e: logging.error(f\\"Error while sending command `{command}`: {e}\\") finally: session.close()"},{"question":"**Problem Statement:** You are tasked with implementing a simple library management system using Python. In addition to implementing the core functionality, you will also need to write corresponding unit tests using the `unittest` framework with mocks to ensure your methods work correctly. **Requirements:** 1. **Class Definitions**: - `Book`: - Attributes: `title` (str), `author` (str), `isbn` (str). - Methods: `__init__(self, title: str, author: str, isbn: str)`, and `__str__(self) -> str`. - `Library`: - Attributes: `books` (List[Book]) initialized as empty list. - Methods: - `add_book(self, book: Book) -> None`: Adds a book to the library. - `remove_book(self, isbn: str) -> None`: Removes a book from the library by its ISBN. - `find_book_by_isbn(self, isbn: str) -> Optional[Book]`: Finds and returns a book by its ISBN, or `None` if not found. - `list_books(self) -> List[str]`: Returns a list of string representations of all books in the library. 2. **Unit Tests**: - Use the `unittest` framework. - Mock the `Book` objects where necessary using `unittest.mock`. - Write at least the following test cases for `Library`: - Test adding a book. - Test removing a book by ISBN. - Test finding a book by ISBN. - Test listing all books. **Constraints:** - You must use the `typing` module for type hints. - Integration testing (actual instantiation of books and adding them into the library) is necessary. - Your tests should cover different scenarios, including edge cases (e.g., removing a non-existent book, finding a book in an empty library). **Input/Output Format:** ```python # Example input: book1 = Book(\\"1984\\", \\"George Orwell\\", \\"1234567890\\") book2 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"0987654321\\") library = Library() library.add_book(book1) library.add_book(book2) # Example method call and expected output: print(library.find_book_by_isbn(\\"1234567890\\")) # should output string representation of the book \\"1984\\" library.remove_book(\\"0987654321\\") print(library.list_books()) # should output list with string representation of book \\"1984\\" ``` This problem combines the understanding and application of classes, type hints, and unit testing with mock objects, addressing both fundamental and advanced Python concepts.","solution":"from typing import List, Optional class Book: def __init__(self, title: str, author: str, isbn: str): self.title = title self.author = author self.isbn = isbn def __str__(self) -> str: return f\\"Book(title={self.title}, author={self.author}, isbn={self.isbn})\\" class Library: def __init__(self): self.books = [] def add_book(self, book: Book) -> None: self.books.append(book) def remove_book(self, isbn: str) -> None: self.books = [book for book in self.books if book.isbn != isbn] def find_book_by_isbn(self, isbn: str) -> Optional[Book]: for book in self.books: if book.isbn == isbn: return book return None def list_books(self) -> List[str]: return [str(book) for book in self.books]"},{"question":"# Python Coding Assessment Question: Lexical Analysis of Strings and Bytes Objective: To assess your ability to understand and correctly implement various types and forms of string and bytes literals in Python, including special escape sequences and formatted string literals. Problem Statement: You are tasked with writing a function called `parse_and_format_strings` that takes a list of string literals and processes them according to specified rules. The function needs to handle raw strings, formatted strings, and bytes literals and produce a single formatted output string. Function Signature: ```python def parse_and_format_strings(literals: list) -> str: pass ``` Input: - `literals`: A list of string literals. Each string literal in the list can be: - A regular string literal (e.g., `\'hello\'`, `\\"world\\"`) - A raw string literal (e.g., `r\'rawnstring\'`, `r\\"rawstring\\"`) - A formatted string literal (e.g., `f\\"{name}\\"`) - A bytes literal (e.g., `b\'bytes\'`, `b\\"literal\\"`) Output: - A single formatted string where: - Each regular string and raw string is preserved as-is, with appropriate handling of escape sequences for regular strings. - Each formatted string evaluates the expressions within it. - Each bytes literal is decoded to a string using UTF-8. Constraints: - The input `literals` list will contain at least one string. - The total length of all strings combined will not exceed 1000 characters. - Formatting expressions in formatted string literals will always evaluate to strings. Example: Given the following list of string literals: ```python literals = [ \'HellonWorld\', r\'RawnString\', \'Name: {name}\', \'Age: {age}\', \'Height: {height:.2f}\', b\'Byte example\' ] name = \\"Alice\\" age = 30 height = 5.6789 ``` The expected output of the function would be: ``` \'HellonWorldnRawnStringnName: AlicenAge: 30nHeight: 5.68nByte example\' ``` Instructions: 1. Complete the implementation of the `parse_and_format_strings` function. 2. Ensure that the function correctly interprets and processes different types of string literals based on the rules provided. 3. Write test cases to validate the correctness of your implementation.","solution":"def parse_and_format_strings(literals): result = [] for literal in literals: if literal.startswith(\'b\'): # check for bytes literal bytes_literal = eval(literal) # safely evaluate the bytes literal result.append(bytes_literal.decode(\'utf-8\')) elif literal.startswith(\'f\'): formatted_literal = eval(literal) # evaluate the formatted string result.append(formatted_literal) elif literal.startswith(\'r\'): raw_literal = eval(literal) # safely evaluate the raw string result.append(raw_literal) else: result.append(eval(literal)) # safely evaluate the regular string return \'n\'.join(result) # Function for setting the global variables def set_globals(**kwargs): globals().update(kwargs)"},{"question":"Coding Assessment Question **Objective:** Write a Python program that utilizes the \\"pickletools\\" module to analyze, optimize, and compare the performance of pickle files. This task will require you to demonstrate your understanding of serialization, deserialization, and bytecode manipulation using the \\"pickletools\\" package in Python. **Problem Statement:** You are provided with a pickle file that contains a serialized Python object. Your task is to write a program that performs the following operations: 1. **Disassemble the Pickle File**: Use the `pickletools.dis` function to disassemble the provided pickle file and print the disassembled output. 2. **Optimize the Pickle File**: Utilize the `pickletools.optimize` function to optimize the pickle string. Save the optimized pickle string to a new file. 3. **Compare Performance**: Calculate and compare the sizes of the original and optimized pickle files. Additionally, measure the time taken to load the original and optimized pickles, and print these metrics. # Input Format - A single pickle file named `data.pickle` containing the serialized Python object. # Output Format - The disassembled output of the original pickle file. - The optimized pickle file saved as `optimized_data.pickle`. - The sizes of the original and optimized pickle files in bytes. - The time taken to load the original and optimized pickles in milliseconds. # Constraints - You may assume the pickle file `data.pickle` is available in the current working directory. - Your program should handle any valid pickle file version. # Example Assume we have a sample pickle file named `sample.pickle`: Sample Execution ```python python optimize_pickle.py sample.pickle ``` Sample Output ``` Disassembled Output: ... 0: x80 PROTO 3 2: K BININT1 1 ... highest protocol among opcodes = 2 Optimized pickle saved as \'optimized_data.pickle\'. Original Pickle File Size: 1024 bytes Optimized Pickle File Size: 800 bytes Loading Time: Original Pickle: 5.23 ms Optimized Pickle: 4.12 ms ``` # Function Signature ```python import pickletools import time def analyze_pickle(file_path: str) -> None: # Your implementation here if __name__ == \\"__main__\\": import sys filename = sys.argv[1] analyze_pickle(filename) ``` # Key Points to Consider - Ensure your program handles edge cases gracefully, such as non-pickle files or corrupt pickle files. - Use Python\'s `time` module to measure loading times. - Ensure your output is clear and neatly formatted. This question assesses your ability to work with advanced Python modules, understand and optimize serialized data, and measure the performance implications of such optimizations.","solution":"import pickletools import pickle import time import sys import os def analyze_pickle(file_path: str) -> None: # Load the original pickle file with open(file_path, \'rb\') as file: original_pickle = file.read() # Disassemble the pickle file print(\\"Disassembled Output:\\") pickletools.dis(original_pickle) # Optimize the pickle string optimized_pickle = pickletools.optimize(original_pickle) # Save the optimized pickle string to a new file optimized_file_path = \'optimized_\' + os.path.basename(file_path) with open(optimized_file_path, \'wb\') as optimized_file: optimized_file.write(optimized_pickle) print(f\\"nOptimized pickle saved as \'{optimized_file_path}\'.\\") # Compare the sizes of the original and optimized pickle files original_size = len(original_pickle) optimized_size = len(optimized_pickle) print(f\\"nOriginal Pickle File Size: {original_size} bytes\\") print(f\\"Optimized Pickle File Size: {optimized_size} bytes\\") # Measure the time to load the original pickle start_time = time.time() original_object = pickle.loads(original_pickle) original_load_time = (time.time() - start_time) * 1000 # Measure the time to load the optimized pickle start_time = time.time() optimized_object = pickle.loads(optimized_pickle) optimized_load_time = (time.time() - start_time) * 1000 print(\\"nLoading Time:\\") print(f\\"Original Pickle: {original_load_time:.2f} ms\\") print(f\\"Optimized Pickle: {optimized_load_time:.2f} ms\\") if __name__ == \\"__main__\\": filename = sys.argv[1] analyze_pickle(filename)"},{"question":"Objective You are tasked to write a Python function that reads a text file with unknown or mixed encodings, processes the contents to convert all text to UTF-8 encoding, and writes the processed text to a new file. During processing, you should normalize all Unicode strings to the NFC (Normalization Form C) format for consistent representation and ensure that non-decodable bytes are handled gracefully. Function Signature ```python def process_unicode_file(input_file: str, output_file: str) -> None: pass ``` Input - `input_file: str`: The path to the input file containing text with unknown or mixed encodings. - `output_file: str`: The path to the output file where the processed UTF-8 encoded text should be written. Requirements 1. **Reading the file**: Read the input file in a way that can handle unknown or mixed encodings using the `surrogateescape` error handler. 2. **Normalization**: Normalize all Unicode strings to the NFC form. 3. **Encoding and Writing**: Write the normalized output to the output file in UTF-8 encoding. 4. **Lossless Processing**: Ensure that any un-decodable bytes are not lost but are preserved when writing to the output file. Constraints 1. The input file can be large, so your solution should handle large files efficiently. 2. Assume that only text files will be provided as input. Example Assume the content of `input_file.txt` is unknown in encoding but has some valid text which can be decoded with `surrogateescape`. ```python # input_file.txt contains mixed encodings and might include: # b\'xfexff\' + \'Some text u2014 more text in UTF-8\'.encode(\'utf-8\') + b\'x00\' # (This is a simplified representation. Actual files may have arbitrary encodings.) process_unicode_file(\'input_file.txt\', \'output_file.txt\') ``` **After processing, `output_file.txt` should be:** ```plaintext Some text — more text in UTF-8 ``` (Note: The exact output will depend on the input content and handling non-decodable bytes) Notes - Use the `open()` function with the `encoding=\\"ascii\\"` and `errors=\\"surrogateescape\\"` parameter for reading the file. - Use the `unicodedata.normalize()` function with the argument `\'NFC\'` for normalization. - Use the `surrogateescape` error handler while writing to handle any non-decodable characters. Write the `process_unicode_file` function to fulfill these requirements.","solution":"import codecs import unicodedata def process_unicode_file(input_file: str, output_file: str) -> None: with open(input_file, \'rb\') as f: byte_content = f.read() text_content = byte_content.decode(\'ascii\', errors=\'surrogateescape\') normalized_content = unicodedata.normalize(\'NFC\', text_content) reencoded_content = normalized_content.encode(\'utf-8\', errors=\'surrogateescape\') with open(output_file, \'wb\') as f: f.write(reencoded_content)"},{"question":"Coding Assessment Question: **Objective:** Demonstrate your understanding of pandas DataFrame memory usage characteristics and the careful use of UDF methods in pandas. **Description:** You are given a DataFrame with multiple columns of different data types, and your task is to calculate the total memory usage of the DataFrame, excluding the memory taken by the index. Additionally, you will implement preprocessing functions using a UDF ensuring no mutation of the original DataFrame. **Specifications:** 1. **Function 1**: `get_memory_usage_excluding_index(df: pd.DataFrame, deep: bool = False) -> int` - **Input**: - `df`: A pandas DataFrame. - `deep`: A boolean flag which indicates if deep memory usage should be calculated (optional, default is `False`). - **Output**: An integer representing the total memory usage of the DataFrame excluding the index, in bytes. 2. **Function 2**: `preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame` - **Input**: - `df`: A pandas DataFrame with at least three columns named \'A\', \'B\', and \'C\'. - **Output**: A new pandas DataFrame where: - The column \'A\' has all its elements incremented by 1. - The column \'B\' has all its elements squared. - The column \'C\' is dropped from the DataFrame. - **Constraints**: - You must use the `apply` method for columns \'A\' and \'B\'. - The function must **not mutate** the original DataFrame. **Example:** Given DataFrame `df`: ``` A B C D 0 10 2 3 4 1 20 3 6 7 2 30 4 9 10 ``` - `get_memory_usage_excluding_index(df, deep=True)` should return the total memory usage of columns \'A\', \'B\', \'C\', and \'D\' excluding the index memory. - `preprocess_dataframe(df)` should return: ``` A B D 0 11 4 4 1 21 9 7 2 31 16 10 ``` Note: Ensure you properly handle the DataFrame transformations and avoid unexpected mutations. **Constraints:** - Do not use external DataFrame libraries other than pandas. - Ensure your solution is efficient and does not unnecessarily use excessive memory. Good luck!","solution":"import pandas as pd def get_memory_usage_excluding_index(df: pd.DataFrame, deep: bool = False) -> int: Calculate the total memory usage of the DataFrame excluding the index. Parameters: df (pd.DataFrame): The DataFrame to calculate memory usage for. deep (bool): If True, analyze deeper object memory usage, default is False. Returns: int: Total memory usage excluding index (in bytes). return df.memory_usage(index=False, deep=deep).sum() def preprocess_dataframe(df: pd.DataFrame) -> pd.DataFrame: Preprocess the DataFrame: - Increment column \'A\' by 1. - Square the values in column \'B\'. - Drop column \'C\'. Parameters: df (pd.DataFrame): The input DataFrame. Returns: pd.DataFrame: The transformed DataFrame. # Make a copy of the DataFrame to prevent mutation df_copy = df.copy() # Apply the transformations df_copy[\'A\'] = df_copy[\'A\'].apply(lambda x: x + 1) df_copy[\'B\'] = df_copy[\'B\'].apply(lambda x: x ** 2) df_copy = df_copy.drop(columns=\'C\') return df_copy"},{"question":"# Coding Challenge: Optimizing Matrix Multiplication with PyTorch Background You are tasked with implementing a matrix multiplication function utilizing PyTorch and optimizing its performance using both inter-op and intra-op parallelism settings. Task 1. **Implement a function `optimized_matrix_multiplication`** that: - Accepts two 2D tensors, `matrix_a` and `matrix_b`, as inputs. - Multiplies these matrices using `torch.mm`. - Returns the resulting matrix. 2. **Further, write a script to analyze the performance** of this function by: - Timing the execution of matrix multiplication for various numbers of threads. - Plotting the runtime against the number of threads and identifying the optimal number of threads for the operation. 3. **Use the following constraints**: - Ensure the matrices are compatible for multiplication. - Use thread settings to cover a range from 1 to 48 threads. - Evaluate and plot the runtime of 100 repetitions of the multiplication for each setting. Input - Two 2D PyTorch tensors `matrix_a` and `matrix_b`. Output - A 2D PyTorch tensor resulting from the matrix multiplication. - A plot showing runtime against the number of threads. Performance Requirements - Code should efficiently handle large tensors (1024 x 1024). - Ensure to manage and set the intra-op and inter-op thread settings properly. Function Signature ```python import torch import timeit import matplotlib.pyplot as plt def optimized_matrix_multiplication(matrix_a: torch.Tensor, matrix_b: torch.Tensor) -> torch.Tensor: # Your implementation here pass # Script for analyzing performance def analyze_performance(matrix_a: torch.Tensor, matrix_b: torch.Tensor): runtimes = [] threads = [1] + [t for t in range(2, 49, 2)] for t in threads: torch.set_num_threads(t) r = timeit.timeit(lambda: optimized_matrix_multiplication(matrix_a, matrix_b), number=100) runtimes.append(r) plt.plot(threads, runtimes) plt.xlabel(\'Number of Threads\') plt.ylabel(\'Runtime (seconds)\') plt.title(\'Matrix Multiplication Performance\') plt.show() # Example usage matrix_a = torch.randn(1024, 1024) matrix_b = torch.randn(1024, 1024) analyze_performance(matrix_a, matrix_b) ``` Ensure your implementation and analysis script follow the above specifications to evaluate the efficiency of multi-threading in PyTorch.","solution":"import torch def optimized_matrix_multiplication(matrix_a: torch.Tensor, matrix_b: torch.Tensor) -> torch.Tensor: Multiplies two 2D tensors using PyTorch\'s matrix multiplication method. Args: matrix_a (torch.Tensor): 2D tensor. matrix_b (torch.Tensor): 2D tensor. Returns: torch.Tensor: Resultant 2D tensor after multiplication. result = torch.mm(matrix_a, matrix_b) return result"},{"question":"**Coding Assessment Question: Advanced Color Space Conversions** **Objective:** You will demonstrate your understanding of the `colorsys` module by implementing a function that converts a list of colors specified in RGB coordinates to multiple other color spaces, applies a transformation, and converts them back to RGB. **Task:** Write a Python function that accepts a list of colors in RGB format and performs the following operations: 1. Converts each RGB color to its equivalent HLS and HSV representations. 2. Modifies the Hue value in both HLS and HSV representations by a specified amount while ensuring it wraps around correctly. 3. Converts the modified HLS and HSV values back to RGB. 4. Returns a list containing tuples of original RGB, modified RGB from HLS, and modified RGB from HSV. **Function Signature:** ```python def transform_colors(colors: list[tuple[float, float, float]], hue_shift: float) -> list[tuple[tuple[float, float, float], tuple[float, float, float], tuple[float, float, float]]]: ``` **Input:** - `colors`: A list of RGB colors, where each color is a tuple of three floating point numbers (r, g, b) ranging from 0 to 1. - `hue_shift`: A floating-point number indicating the amount by which to shift the Hue value in HLS and HSV color spaces. This value should wrap around in the range of 0 to 1. **Output:** - A list of tuples. Each tuple contains three elements: 1. The original RGB color. 2. The RGB color after modifying the Hue in the HLS representation. 3. The RGB color after modifying the Hue in the HSV representation. **Constraints:** - Do not use any additional Python packages apart from `colorsys`. - Ensure that the Hue modifications handle wrap-around correctly, i.e., Hue values must remain within the [0, 1] range. **Example:** ```python colors = [(0.2, 0.4, 0.4), (0.5, 0.1, 0.1)] hue_shift = 0.3 result = transform_colors(colors, hue_shift) # Expected output format: # [ # ((0.2, 0.4, 0.4), (modified_rgb_from_hls), (modified_rgb_from_hsv)), # ((0.5, 0.1, 0.1), (modified_rgb_from_hls), (modified_rgb_from_hsv)) # ] ``` **Additional Notes:** - The `colorsys` module functions handle floating point arithmetic and expect inputs and outputs in the range [0, 1]. - You should demonstrate both fundamental understanding and correct handling of floating-point operations and conversions between different color spaces. **Hints:** - Recall that the Hue value wraps around. For example, if the hue shift results in a value greater than 1, it should loop back from 0. Use modulus operations to ensure this. - Use `colorsys.rgb_to_hls` and `colorsys.hls_to_rgb` for HLS transformations. - Use `colorsys.rgb_to_hsv` and `colorsys.hsv_to_rgb` for HSV transformations.","solution":"import colorsys def transform_colors(colors, hue_shift): transformed_colors = [] for r, g, b in colors: # Convert RGB to HLS hls = colorsys.rgb_to_hls(r, g, b) h, l, s = hls # Modify Hue and wrap around correctly new_h = (h + hue_shift) % 1.0 # Convert modified HLS back to RGB modified_rgb_hls = colorsys.hls_to_rgb(new_h, l, s) # Convert RGB to HSV hsv = colorsys.rgb_to_hsv(r, g, b) h, s, v = hsv # Modify Hue and wrap around correctly new_h = (h + hue_shift) % 1.0 # Convert modified HSV back to RGB modified_rgb_hsv = colorsys.hsv_to_rgb(new_h, s, v) # Append the tuple (original_rgb, modified_rgb_hls, modified_rgb_hsv) to the result list transformed_colors.append(((r, g, b), modified_rgb_hls, modified_rgb_hsv)) return transformed_colors"},{"question":"You have been tasked with implementing a Python class, `DataCompressor`, that utilizes the `zlib` module to handle data compression and decompression for a given data stream. The class should support initializing with a specific compression level and perform validation on the input data. Additionally, the class should be able to compute a checksum of the data before and after compression. Specifications 1. **Class Definition**: - `DataCompressor(level: int = -1)`: Initialize with a specific compression level (from 0 to 9, or -1 for default). 2. **Methods**: - `compress(data: bytes) -> bytes`: Compress the input data and return the compressed bytes. - `decompress(data: bytes) -> bytes`: Decompress the input data and return the original bytes. - `compute_checksum(data: bytes, initial: int = 0) -> int`: Compute the CRC32 checksum of the data. - `validate_data(data: bytes) -> bool`: Return `True` if the data is non-empty and a valid bytes object; otherwise, return `False`. 3. **Usage Example**: ```python compressor = DataCompressor(level=6) data = b\\"some data to compress\\" # Validate data if compressor.validate_data(data): # Compress data compressed_data = compressor.compress(data) # Decompress data decompressed_data = compressor.decompress(compressed_data) # Ensure data consistency assert data == decompressed_data # Compute checksum checksum = compressor.compute_checksum(data) print(\\"Checksum:\\", checksum) else: print(\\"Invalid data provided.\\") ``` 4. **Constraints**: - Data provided to `compress` and `decompress` methods should not exceed 10 MB. - The `validate_data` method should ensure that the input is a non-empty bytes object. - If compression or decompression fails, raise a `zlib.error` exception. 5. **Performance**: - The methods should handle typical data sizes efficiently but not exceed a total runtime of 5 seconds for compressing and decompressing 10 MB of data. Implement the `DataCompressor` class according to the specifications.","solution":"import zlib class DataCompressor: def __init__(self, level: int = -1): if not (-1 <= level <= 9): raise ValueError(\\"Compression level must be between -1 and 9.\\") self.level = level def validate_data(self, data: bytes) -> bool: return isinstance(data, bytes) and len(data) > 0 and len(data) <= 10 * 1024 * 1024 def compress(self, data: bytes) -> bytes: if not self.validate_data(data): raise ValueError(\\"Invalid data provided for compression.\\") return zlib.compress(data, self.level) def decompress(self, data: bytes) -> bytes: if not self.validate_data(data): raise ValueError(\\"Invalid data provided for decompression.\\") try: return zlib.decompress(data) except zlib.error as e: raise zlib.error(f\\"Decompression failed: {e}\\") def compute_checksum(self, data: bytes, initial: int = 0) -> int: if not isinstance(data, bytes): raise ValueError(\\"Invalid data type for checksum computation.\\") return zlib.crc32(data, initial)"},{"question":"# Python Coding Assessment: Memory Allocation Debugging Objective: You are required to use the `tracemalloc` module to identify memory leak patterns in a given Python application. Background: You are provided with a Python application that simulates a memory leak. Your task is to: 1. Identify the lines of code where memory allocations are highest. 2. Compare snapshots of memory usage to find differences and pinpoint memory leaks. 3. Generate a detailed report with information about memory utilization and any identified memory leaks. Application Code: ```python import random def simulate_memory_leak(): leaking_list = [] for _ in range(10000): leaking_list.append(random.randint(1, 1000)) return sum(leaking_list) if __name__ == \\"__main__\\": import tracemalloc tracemalloc.start() # Simulate memory usage and take snapshots snapshot1 = tracemalloc.take_snapshot() simulate_memory_leak() snapshot2 = tracemalloc.take_snapshot() tracemalloc.stop() # TODO: Analyze memory usage here pass ``` Tasks: 1. **Identify Top Memory Allocation Lines:** - Display the top 5 lines of code where the most memory is being allocated. 2. **Compute Differences Between Snapshots:** - Compare the two snapshots (before and after the memory leak simulation) and display the top 5 differences. 3. **Generate Memory Report:** - Generate a detailed report that includes: - Current memory usage and peak memory usage. - Top memory allocations. - Differences between memory allocations in both snapshots. - Traceback details for the biggest memory block. Expected Output: Format your report as shown below: ```plaintext # Memory Allocation Report # Current memory usage: X KiB Peak memory usage: Y KiB # Top 5 Memory Allocation Lines # 1. File: <filename>:<lineno>, Size: X KiB 2. ... # Top 5 Memory Allocation Differences # 1. File: <filename>:<lineno>, Size Difference: X KiB, New Size: Y KiB 2. ... # Traceback of the Biggest Memory Block # Traceback (most recent call first): File \\"<filename>\\", line <lineno> ... ``` Constraints: - You must use the `tracemalloc` module. - The report should be generated programmatically. Submission Requirements: Submit the completed code that performs the tasks described above and generates the required report. Ensure all results are accurate and the report is formatted correctly.","solution":"import random import tracemalloc def simulate_memory_leak(): leaking_list = [] for _ in range(10000): leaking_list.append(random.randint(1, 1000)) return sum(leaking_list) def generate_memory_report(): # Simulate memory usage and take snapshots snapshot1 = tracemalloc.take_snapshot() simulate_memory_leak() snapshot2 = tracemalloc.take_snapshot() # Current and peak memory usage current, peak = tracemalloc.get_traced_memory() # Analyze memory usage top_stats1 = snapshot1.statistics(\'lineno\') top_stats2 = snapshot2.statistics(\'lineno\') # Top 5 memory allocation lines top_stats = snapshot2.compare_to(snapshot1, \'lineno\') print(f\\"# Memory Allocation Report #\\") print(f\\"Current memory usage: {current / 1024:.2f} KiB\\") print(f\\"Peak memory usage: {peak / 1024:.2f} KiB\\") print(\\"n# Top 5 Memory Allocation Lines #\\") for idx, stat in enumerate(top_stats2[:5], 1): print(f\\"{idx}. File: {stat.traceback[0].filename}:{stat.traceback[0].lineno}, Size: {stat.size / 1024:.2f} KiB\\") print(\\"n# Top 5 Memory Allocation Differences #\\") for idx, stat in enumerate(top_stats[:5], 1): print(f\\"{idx}. File: {stat.traceback[0].filename}:{stat.traceback[0].lineno}, Size Difference: {stat.size_diff / 1024:.2f} KiB, New Size: {stat.size / 1024:.2f} KiB\\") # Traceback details for the biggest memory block (example shown for the largest differnce) if top_stats: largest_diff_stat = top_stats[0] print(\\"n# Traceback of the Biggest Memory Block #\\") print(\\"Traceback (most recent call last):\\") for frame in largest_diff_stat.traceback.format(): print(frame.strip()) if __name__ == \\"__main__\\": tracemalloc.start() generate_memory_report() tracemalloc.stop()"},{"question":"# Question: Creating and Verifying HMAC Digests Implement a Python function to verify the integrity of multiple messages using HMAC. The function should use a secret key to generate HMAC digests and then verify those digests against provided values. Specifically, the function should follow these steps: 1. Create an HMAC object using the provided `key` and `digestmod`. 2. Update the HMAC object with each message. 3. Compute the hexadecimal digest for each message. 4. Verify each computed digest against the provided digest values. 5. Return a list of booleans indicating whether each digest matches or not. Function Signature ```python def verify_hmac_messages(key: bytes, messages: list[bytes], digests: list[str], digestmod: str) -> list[bool]: pass ``` Input - `key`: a `bytes` object representing the secret key used for creating the HMAC. - `messages`: a `list` of `bytes` objects to be processed. - `digests`: a `list` of `str` expected digest values (hexadecimal format). - `digestmod`: a `str` specifying the hash algorithm (e.g., `\\"sha256\\"`). Output - A `list` of `bool` values where each value indicates if the corresponding message\'s computed HMAC digest matches the given digest. Constraints - `key` should be a non-empty `bytes` object. - `messages` will not be empty and contain `bytes` objects. - `digests` will not be empty and contain valid hexadecimal strings. - `digestmod` will be a valid string for a hash algorithm supported by `hashlib`. Example Usage ```python key = b\'secret_key\' messages = [b\'First message\', b\'Second message\'] digests = [ \'c0db79ea2a2c8e9c13b1b352c4675ee1b3e6028b8a4aa733ca1dbe482a761f95\', \'6df031c29cf10e6d91ee4a9f8cbf7ccdf80b252026f46e850fe097d909a6e2d3\' ] digestmod = \'sha256\' print(verify_hmac_messages(key, messages, digests, digestmod)) # Output: [True, False] (assuming the given digests correspond to the respective messages) ``` This question tests the understanding of: - Creating and using HMAC objects. - Updating HMAC with messages. - Generating and verifying hexadecimal digests. - Using secure comparison for verification.","solution":"import hmac import hashlib def verify_hmac_messages(key: bytes, messages: list[bytes], digests: list[str], digestmod: str) -> list[bool]: results = [] for message, expected_digest in zip(messages, digests): hmac_obj = hmac.new(key, message, getattr(hashlib, digestmod)) computed_digest = hmac_obj.hexdigest() results.append(computed_digest == expected_digest) return results"},{"question":"**Objective:** Implement a neural network model for a classification task using `MLPClassifier` from the `scikit-learn` library. Evaluate the model performance and optimize its hyperparameters using grid search. # Input: 1. **Training Data:** A CSV file `train.csv` with features and labels. 2. **Testing Data:** A CSV file `test.csv` with features and labels. # Output: 1. **Accuracy Score:** The accuracy of the trained model on the test set. 2. **Best Hyperparameters:** The best hyperparameters found during grid search. 3. **Confusion Matrix:** Display the confusion matrix for the test set predictions. # Constraints: - The dataset has no missing values. - The target column in both training and testing data is \'target\'. - Feature columns are numeric. # Instructions: 1. Load the training and testing data. 2. Standardize the features using `StandardScaler`. 3. Split the training data into training and validation sets. 4. Train an `MLPClassifier` with default parameters and evaluate its performance on the validation set. 5. Use grid search to find the best hyperparameters for the `MLPClassifier`. You need to tune `hidden_layer_sizes`, `alpha`, and `solver`. 6. Train the `MLPClassifier` with the best hyperparameters on the full training data and evaluate its performance on the test set. 7. Display the accuracy score on the test set and the confusion matrix. # Example of Expected Solution: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix # Load data train_data = pd.read_csv(\'train.csv\') test_data = pd.read_csv(\'test.csv\') # Separate features and target X_train = train_data.drop(columns=[\'target\']) y_train = train_data[\'target\'] X_test = test_data.drop(columns=[\'target\']) y_test = test_data[\'target\'] # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Split training data into training and validation sets X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42) # Train MLPClassifier with default parameters mlp = MLPClassifier(random_state=1) mlp.fit(X_train_split, y_train_split) y_val_pred = mlp.predict(X_val_split) val_accuracy = accuracy_score(y_val_split, y_val_pred) print(f\\"Validation Accuracy: {val_accuracy}\\") # Use grid search to find the best hyperparameters param_grid = { \'hidden_layer_sizes\': [(50,), (100,), (50, 50)], \'alpha\': [1e-5, 1e-4, 1e-3], \'solver\': [\'lbfgs\', \'sgd\', \'adam\'] } grid_search = GridSearchCV(MLPClassifier(random_state=1), param_grid, cv=3) grid_search.fit(X_train, y_train) # Get the best hyperparameters best_params = grid_search.best_params_ print(f\\"Best Hyperparameters: {best_params}\\") # Train with the best hyperparameters on the full training data best_mlp = MLPClassifier(**best_params, random_state=1) best_mlp.fit(X_train, y_train) # Evaluate on the test set y_test_pred = best_mlp.predict(X_test) test_accuracy = accuracy_score(y_test, y_test_pred) conf_matrix = confusion_matrix(y_test, y_test_pred) print(f\\"Test Accuracy: {test_accuracy}\\") print(\\"Confusion Matrix:\\") print(conf_matrix) ``` # Requirements: 1. Ensure your code handles edge cases and is well-documented. 2. Provide a brief explanation of the steps you\'ve taken in markdown cells, if using a Jupyter notebook. 3. Submit your code and any relevant plots or outputs.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.neural_network import MLPClassifier from sklearn.metrics import accuracy_score, confusion_matrix def nn_model_classification(train_csv, test_csv): # Load data train_data = pd.read_csv(train_csv) test_data = pd.read_csv(test_csv) # Separate features and target X_train = train_data.drop(columns=[\'target\']) y_train = train_data[\'target\'] X_test = test_data.drop(columns=[\'target\']) y_test = test_data[\'target\'] # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Split training data into training and validation sets X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42) # Train MLPClassifier with default parameters mlp = MLPClassifier(random_state=1) mlp.fit(X_train_split, y_train_split) y_val_pred = mlp.predict(X_val_split) val_accuracy = accuracy_score(y_val_split, y_val_pred) print(f\\"Validation Accuracy: {val_accuracy}\\") # Use grid search to find the best hyperparameters param_grid = { \'hidden_layer_sizes\': [(50,), (100,), (50, 50)], \'alpha\': [1e-5, 1e-4, 1e-3], \'solver\': [\'lbfgs\', \'sgd\', \'adam\'] } grid_search = GridSearchCV(MLPClassifier(random_state=1), param_grid, cv=3) grid_search.fit(X_train, y_train) # Get the best hyperparameters best_params = grid_search.best_params_ print(f\\"Best Hyperparameters: {best_params}\\") # Train with the best hyperparameters on the full training data best_mlp = MLPClassifier(**best_params, random_state=1) best_mlp.fit(X_train, y_train) # Evaluate on the test set y_test_pred = best_mlp.predict(X_test) test_accuracy = accuracy_score(y_test, y_test_pred) conf_matrix = confusion_matrix(y_test, y_test_pred) print(f\\"Test Accuracy: {test_accuracy}\\") print(\\"Confusion Matrix:\\") print(conf_matrix) return test_accuracy, best_params, conf_matrix"},{"question":"# Question: Creating and Using Custom Enumerations As a programming task, you are required to define and work with custom enumerations using the \\"enum\\" module in Python. Your task involves implementing enumeration classes, leveraging the functionalities provided by the `enum` module. Requirements: 1. **Define enumerations**: - Define an `IntEnum` class named `Status` representing different statuses of a task with the following members: - `PENDING` with value 0 - `STARTED` with value 1 - `COMPLETED` with value 2 2. **Define a `Flag` class**: - Define a `Flag` class named `Permissions` representing different user permissions with the following members: - `READ` with value `auto()` - `WRITE` with value `auto()` - `EXECUTE` with value `auto()` - Define a combination named `ALL` which combines `READ`, `WRITE`, and `EXECUTE` permissions. 3. **Implement functionalities**: - Write a function `status_info(status)` that takes a `Status` member and returns a string in the format `\\"{name} with value {value}\\"`. - Write a function `combine_permissions(*args)` that takes multiple `Permissions` members and returns their combination. - Write a function `check_permission(combined, permission)` that checks if a given permission is included in a combined permission set. Input and Output Formats: - For `status_info(status)`: - Input: A member of the `Status` enumeration. - Output: A string describing the name and value of the status. - For `combine_permissions(*args)`: - Input: Multiple `Permissions` members. - Output: A combined `Permissions` member. - For `check_permission(combined, permission)`: - Input: A combined `Permissions` member and a single `Permissions` member. - Output: A boolean indicating whether the permission is included in the combined permission. Example: ```python # Define the enumerations and required functions here # Example usage: print(status_info(Status.PENDING)) # Output: \\"PENDING with value 0\\" combined = combine_permissions(Permissions.READ, Permissions.WRITE) print(combined) # Output: Permissions.READ|WRITE print(check_permission(combined, Permissions.READ)) # Output: True print(check_permission(combined, Permissions.EXECUTE)) # Output: False ``` Use the provided template to implement your solution and ensure each requirement is fulfilled properly.","solution":"from enum import IntEnum, Flag, auto # Define the IntEnum class for Status class Status(IntEnum): PENDING = 0 STARTED = 1 COMPLETED = 2 # Define the Flag class for Permissions class Permissions(Flag): READ = auto() WRITE = auto() EXECUTE = auto() ALL = READ | WRITE | EXECUTE # Implement functionalities def status_info(status): Returns a string describing the status. return f\\"{status.name} with value {status.value}\\" def combine_permissions(*args): Combine multiple Permissions and return the combined value. combination = Permissions(0) for permission in args: combination |= permission return combination def check_permission(combined, permission): Check if a given permission is included in the combined permission set. return (combined & permission) == permission"},{"question":"You are tasked with creating a utility that can read a text file, compress its contents using the LZMA compression algorithm, and store the compressed data in another file. Moreover, the utility must be capable of reading the compressed file and decompressing its contents back to plaintext. Implement a Python function `compress_and_decompress` that accepts three arguments: - `input_filename`: The name of the input text file to be compressed. - `compressed_filename`: The name of the file where the compressed data will be written. - `decompressed_filename`: The name of the file where the decompressed data will be written. The function should: 1. Read the plaintext content from `input_filename`. 2. Compress this content and write it to `compressed_filename`. 3. Read the compressed content from `compressed_filename`. 4. Decompress it and write the resulting plaintext to `decompressed_filename`. Ensure that your implementation handles any potential exceptions that might occur during file I/O operations. # Input Format The function will be called with three string arguments representing file names. # Output Format The function does not return anything. The test cases will check the contents of the `decompressed_filename` to match the original content of `input_filename`. # Example Usage Assume `input.txt` contains: ``` Hello, this is a sample text for compression. ``` After running: ```python compress_and_decompress(\'input.txt\', \'compressed.xz\', \'output.txt\') ``` The `output.txt` should contain: ``` Hello, this is a sample text for compression. ``` # Constraints - You may assume that the input file exists and is readable. - The function should handle files up to 1 MB in size. - Use the default settings for compression. # Implementation ```python def compress_and_decompress(input_filename, compressed_filename, decompressed_filename): import lzma # Read the content from the input file with open(input_filename, \'rt\') as infile: content = infile.read() # Compress the content and write to the compressed file compressed_data = lzma.compress(content.encode(\'utf-8\')) with open(compressed_filename, \'wb\') as compfile: compfile.write(compressed_data) # Read the compressed content with open(compressed_filename, \'rb\') as compfile: compressed_content = compfile.read() # Decompress the content and write to the decompressed file decompressed_data = lzma.decompress(compressed_content).decode(\'utf-8\') with open(decompressed_filename, \'wt\') as outfile: outfile.write(decompressed_data) ```","solution":"def compress_and_decompress(input_filename, compressed_filename, decompressed_filename): import lzma try: # Read the content from the input file with open(input_filename, \'rt\') as infile: content = infile.read() # Compress the content and write to the compressed file compressed_data = lzma.compress(content.encode(\'utf-8\')) with open(compressed_filename, \'wb\') as compfile: compfile.write(compressed_data) # Read the compressed content with open(compressed_filename, \'rb\') as compfile: compressed_content = compfile.read() # Decompress the content and write to the decompressed file decompressed_data = lzma.decompress(compressed_content).decode(\'utf-8\') with open(decompressed_filename, \'wt\') as outfile: outfile.write(decompressed_data) except (IOError, lzma.LZMAError) as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Problem Statement:** Write a function `tensor_operations()` that performs the following operations using PyTorch tensors: 1. Create two 3x3 tensors: - The first tensor `tensor_a` should contain the elements [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]] and have a data type of 32-bit floating point (torch.float32). - The second tensor `tensor_b` should contain the elements [[9.0, 8.0, 7.0], [6.0, 5.0, 4.0], [3.0, 2.0, 1.0]] and also have a data type of 32-bit floating point (torch.float32). 2. Perform the following element-wise operations: - Add `tensor_a` and `tensor_b`. - Subtract `tensor_b` from `tensor_a`. - Multiply `tensor_a` and `tensor_b`. - Divide `tensor_a` by `tensor_b`. 3. Calculate the mean and standard deviation of the resultant tensor from each of the above operations. 4. Create a tensor of size 4x4 filled with ones, enable gradient tracking, raise each element to the power of 3, and then compute the gradient of the sum of all elements with respect to the tensor elements. The function should return the following: - Sum of all resultant tensors from step 2. - Mean and standard deviation of all resultant tensors from step 3. - The gradient of the 4x4 tensor from step 4. **Function Signature:** ```python import torch def tensor_operations(): # Step 1: Create tensors tensor_a = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], dtype=torch.float32) tensor_b = torch.tensor([[9.0, 8.0, 7.0], [6.0, 5.0, 4.0], [3.0, 2.0, 1.0]], dtype=torch.float32) # Step 2: Perform operations add_result = torch.add(tensor_a, tensor_b) sub_result = torch.sub(tensor_a, tensor_b) mul_result = torch.mul(tensor_a, tensor_b) div_result = torch.div(tensor_a, tensor_b) # Step 3: Calculate mean and standard deviation add_mean = add_result.mean().item() add_std = add_result.std().item() sub_mean = sub_result.mean().item() sub_std = sub_result.std().item() mul_mean = mul_result.mean().item() mul_std = mul_result.std().item() div_mean = div_result.mean().item() div_std = div_result.std().item() # Step 4: Create a 4x4 tensor with gradient tracking tensor_c = torch.ones((4, 4), requires_grad=True, dtype=torch.float32) power_result = tensor_c.pow(3).sum() power_result.backward() tensor_c_grad = tensor_c.grad return { \\"add_result\\": add_result, \\"sub_result\\": sub_result, \\"mul_result\\": mul_result, \\"div_result\\": div_result, \\"add_mean\\": add_mean, \\"add_std\\": add_std, \\"sub_mean\\": sub_mean, \\"sub_std\\": sub_std, \\"mul_mean\\": mul_mean, \\"mul_std\\": mul_std, \\"div_mean\\": div_mean, \\"div_std\\": div_std, \\"tensor_c_grad\\": tensor_c_grad.detach().numpy() } # Call the function and print the results print(tensor_operations()) ``` **Constraints:** - The function should be implemented using PyTorch version >= 1.8.","solution":"import torch def tensor_operations(): # Step 1: Create tensors tensor_a = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], dtype=torch.float32) tensor_b = torch.tensor([[9.0, 8.0, 7.0], [6.0, 5.0, 4.0], [3.0, 2.0, 1.0]], dtype=torch.float32) # Step 2: Perform operations add_result = torch.add(tensor_a, tensor_b) sub_result = torch.sub(tensor_a, tensor_b) mul_result = torch.mul(tensor_a, tensor_b) div_result = torch.div(tensor_a, tensor_b) # Step 3: Calculate mean and standard deviation add_mean = add_result.mean().item() add_std = add_result.std().item() sub_mean = sub_result.mean().item() sub_std = sub_result.std().item() mul_mean = mul_result.mean().item() mul_std = mul_result.std().item() div_mean = div_result.mean().item() div_std = div_result.std().item() # Step 4: Create a 4x4 tensor with gradient tracking tensor_c = torch.ones((4, 4), requires_grad=True, dtype=torch.float32) power_result = tensor_c.pow(3).sum() power_result.backward() tensor_c_grad = tensor_c.grad return { \\"add_result\\": add_result, \\"sub_result\\": sub_result, \\"mul_result\\": mul_result, \\"div_result\\": div_result, \\"add_mean\\": add_mean, \\"add_std\\": add_std, \\"sub_mean\\": sub_mean, \\"sub_std\\": sub_std, \\"mul_mean\\": mul_mean, \\"mul_std\\": mul_std, \\"div_mean\\": div_mean, \\"div_std\\": div_std, \\"tensor_c_grad\\": tensor_c_grad.detach().numpy() } # Call the function and print the results print(tensor_operations())"},{"question":"Objective: Demonstrate your understanding of memoryview objects in Python by implementing functions that utilize the memoryview API to perform efficient memory operations. Task: 1. Implement a function `create_memoryview_from_object(data: bytes) -> memoryview` that takes a bytes-like object and returns a memoryview object of it. - If the input data is not a bytes-like object, the function should raise a `TypeError`. 2. Implement a function `check_memoryview(obj) -> bool` that checks if the given object is a memoryview. - The function should return `True` if the object is a memoryview, otherwise return `False`. 3. Implement a function `compare_memory_regions(mem1: memoryview, mem2: memoryview) -> bool` that compares two memoryview objects to check if they reference the same underlying buffer. - The function should return `True` if they reference the same underlying buffer, otherwise `False`. 4. Implement a function `copy_memory_to_buffer(src: memoryview, buffer: bytearray, offset: int = 0) -> None` that copies the contents of a memoryview object into a given bytearray buffer starting at the specified offset. - If the buffer is not large enough to hold the contents of the memoryview from the specified offset, the function should raise a `ValueError`. Constraints: - Input to `create_memoryview_from_object` will always be bytes-like objects. - The `offset` parameter in `copy_memory_to_buffer` will always be non-negative. - Ensure the functions handle any edge cases, such as invalid data types and mismatched memory sizes appropriately. Example: ```python # Example usage: # Creating memoryview from bytes object data = b\'abcdef\' memview = create_memoryview_from_object(data) # Checking if an object is a memoryview print(check_memoryview(memview)) # Output: True # Comparing memory regions data2 = b\'abcdef\' memview2 = create_memoryview_from_object(data2) print(compare_memory_regions(memview, memview2)) # Output: False memview_alias = memview print(compare_memory_regions(memview, memview_alias)) # Output: True # Copying memory to buffer buffer = bytearray(10) copy_memory_to_buffer(memview, buffer, 2) print(buffer) # Output: bytearray(b\'x00x00abcdef\') ``` Good luck!","solution":"def create_memoryview_from_object(data: bytes) -> memoryview: Creates a memoryview object from a bytes-like object. if not isinstance(data, (bytes, bytearray)): raise TypeError(\\"Input data must be a bytes-like object\\") return memoryview(data) def check_memoryview(obj) -> bool: Checks if the given object is a memoryview. return isinstance(obj, memoryview) def compare_memory_regions(mem1: memoryview, mem2: memoryview) -> bool: Compares two memoryview objects to check if they reference the same underlying buffer. return mem1.obj is mem2.obj def copy_memory_to_buffer(src: memoryview, buffer: bytearray, offset: int = 0) -> None: Copies the contents of a memoryview object to a given bytearray buffer starting at the specified offset. if len(buffer) < len(src) + offset: raise ValueError(\\"Buffer is not large enough to hold the contents of the memoryview from the specified offset\\") buffer[offset:offset + len(src)] = src"},{"question":"**Meta Device Manipulation and Tensor Operations in PyTorch** **Objective:** Demonstrate your understanding of the PyTorch \\"meta\\" device by performing various tensor operations and manipulations, leading to the construction of a neural network model. **Problem Statement:** 1. Create a tensor of shape (5, 5) on the meta device filled with random values. 2. Perform a matrix multiplication operation on this tensor with another meta tensor of compatible shape and record the resulting tensor. 3. Using a PyTorch module `Linear` layer, design a simple neural network on the meta device. Ensure the network consists of at least one `Linear` layer. 4. Convert the resultant model from the meta device to the CPU device without holding the actual data. 5. Reinitialize the parameters of the neural network manually once it is transferred to the CPU. 6. Perform a forward pass on the network with a random input tensor of appropriate dimensions. **Requirements:** - Implement your solution in a function `meta_device_operations()`. - The function should not take any input and should return the output of the forward pass. - Ensure all tensors and operations initially use the meta device. **Constraints:** - No actual data should be used while performing operations on the meta device. - You need to explicitly fill in the missing data before performing operations outside of the meta device. **Function Signature:** ```python def meta_device_operations() -> torch.Tensor: pass ``` **Example Usage:** ```python output = meta_device_operations() print(output) # Expect a tensor after forward passing through the NN on CPU ``` **Note:** 1. You may use helper functions to break down the tasks. 2. Ensure you handle exceptions where necessary, especially when performing operations that aren\'t permissible on meta tensors.","solution":"import torch import torch.nn as nn def meta_device_operations(): # Create tensor of shape (5, 5) on the meta device filled with random values meta_tensor_a = torch.empty((5, 5), device=\'meta\') # Perform a matrix multiplication operation with another meta tensor meta_tensor_b = torch.empty((5, 5), device=\'meta\') meta_result = torch.matmul(meta_tensor_a, meta_tensor_b) # Define a simple neural network on the meta device class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(5, 2) def forward(self, x): x = self.fc1(x) return x # Initialize the model on the \'meta\' device model = SimpleNN().to(device=\'meta\') # Switch the model to the CPU device model_cpu = SimpleNN() # Manually copy the parameters from the meta model to the new CPU model with random data for meta_param, cpu_param in zip(model.parameters(), model_cpu.parameters()): if len(meta_param.shape) > 1: # For Linear layer weights cpu_param.data.copy_(torch.randn_like(cpu_param)) else: # For Linear layer biases cpu_param.data.copy_(torch.randn_like(cpu_param)) # Create a random input tensor input_tensor = torch.randn((5, 5)) # Perform a forward pass on the network with the input tensor output = model_cpu(input_tensor) return output"},{"question":"# Asynchronous Iterator Challenge Objective Demonstrate your understanding of creating and using asynchronous iterators in Python 3.10. Problem Statement You are provided with a collection of data points that represent sensor readings taken asynchronously at varying intervals. Your task is to create an asynchronous iterator that processes these sensor readings and calculates their average. Requirements 1. **Asynchronous Sensor Generator**: Define an asynchronous generator function `async_sensor_readings(readings)` that takes a list of data points `readings` and yields each data point with a simulated delay (e.g., 0.1s per reading). 2. **Asynchronous Iterator**: Implement an asynchronous class `AsyncAverageCalculator` that: - Initializes with an asynchronous iterable (like the sensor readings generator). - Has an asynchronous method `calculate_average()` that computes and returns the average of all provided readings. 3. **Main Function**: Write a main function `main()` to bring everything together: - Populate your sensor readings. - Use the asynchronous generator to create an iterable. - Create an instance of `AsyncAverageCalculator` with the asynchronous iterator. - Calculate and print the average of the readings. Constraints - Ensure to use `async` and `await` keywords where appropriate. - Handle a list of at least 100 sensor readings ranging from `0` to `1000`. Input and Output - The input is a list of integers (sample data points). - The output is the calculated average of the sensor readings, printed to the console. Example ```python import asyncio async def async_sensor_readings(readings): for reading in readings: await asyncio.sleep(0.1) yield reading class AsyncAverageCalculator: def __init__(self, async_iterable): self.async_iterable = async_iterable async def calculate_average(self): total = 0 count = 0 async for reading in self.async_iterable: total += reading count += 1 if count == 0: raise ValueError(\\"No readings to process\\") return total / count async def main(): readings = list(range(1001)) async_iterable = async_sensor_readings(readings) calculator = AsyncAverageCalculator(async_iterable) average = await calculator.calculate_average() print(f\\"Average is: {average}\\") if __name__ == \\"__main__\\": asyncio.run(main()) ``` Notes - You can adjust the number of readings and the delay to test different scenarios. - Consider edge cases where the list might be empty and handle it properly. Good luck!","solution":"import asyncio async def async_sensor_readings(readings): Asynchronous generator that yields each reading with a delay. for reading in readings: await asyncio.sleep(0.1) yield reading class AsyncAverageCalculator: def __init__(self, async_iterable): self.async_iterable = async_iterable async def calculate_average(self): total = 0 count = 0 async for reading in self.async_iterable: total += reading count += 1 if count == 0: raise ValueError(\\"No readings to process\\") return total / count async def main(): readings = list(range(1001)) # Readings from 0 to 1000 async_iterable = async_sensor_readings(readings) calculator = AsyncAverageCalculator(async_iterable) average = await calculator.calculate_average() print(f\\"Average is: {average}\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective: You are required to create a function in Python that retrieves and processes the annotations of a given object. Your function should comply with the best practices outlined in the attached documentation for accessing annotations in Python 3.10 and newer, and handling older versions where necessary. Function Signature: ```python def get_annotations(obj: object) -> dict: pass ``` Input: - `obj` (object): The object from which to retrieve annotations. This may be a function, a class, or a module. Output: - Returns a dictionary of annotations. If the object has no annotations, returns an empty dictionary. Constraints: - The function should handle: - Functions, classes, and modules with annotations. - Stringized annotations. - Wrapped callables from `functools.partial()`. - Python versions both 3.10 and newer, and 3.9 and older. - The function should correctly identify and avoid inheriting annotations from the base class when dealing with classes. - If the annotations are in string format, they should be evaluated to their actual type values where possible. Example: ```python from typing import Callable import functools # Sample Objects class Base: a: int = 3 b: str = \'abc\' class Derived(Base): pass def simple_func(x: int, y: str) -> str: return y + str(x) partial_func = functools.partial(simple_func, 10) # Examples of usage print(get_annotations(Base)) # Should output {\'a\': <class \'int\'>, \'b\': <class \'str\'>} print(get_annotations(Derived)) # Should output {} print(get_annotations(simple_func)) # Should output {\'x\': <class \'int\'>, \'y\': <class \'str\'>, \'return\': <class \'str\'>} print(get_annotations(partial_func)) # Should correctly access the wrapped function\'s annotations ``` Notes: - You should avoid directly modifying or deleting annotations. - Ensure to use exception handling wherever required to maintain robustness. - The code should be efficient and handle typical edge cases presented in the documentation.","solution":"import sys import typing from functools import partial, update_wrapper def get_annotations(obj: object) -> dict: Returns a dictionary of annotations for the given object. Handles: - Functions, classes, and modules with annotations. - Stringized annotations. - Wrapped callables from functools.partial(). - Python versions both 3.10 and newer, and 3.9 and older. if isinstance(obj, partial): obj = obj.func if hasattr(obj, \'__annotations__\'): annotations = obj.__annotations__ elif hasattr(obj, \'__dict__\') and \'__annotations__\' in obj.__dict__: annotations = obj.__dict__[\'__annotations__\'] else: return {} if not annotations: return {} # If annotations are stringized, evaluate them if isinstance(annotations, dict): if sys.version_info >= (3, 10): return {k: typing.get_type_hints(obj)[k] for k in annotations} else: globalns = getattr(obj, \'__globals__\', {}) localns = getattr(obj, \'__locals__\', None) return {k: typing.get_type_hints(obj, globalns, localns)[k] for k in annotations} return annotations"},{"question":"# Seaborn Custom Plot Configuration You are a data scientist tasked with creating a publication-ready plot using seaborn and matplotlib. Your plot should display data with customized theme settings and specific display configurations. **Requirements:** 1. **Data Visualization:** - Use a built-in seaborn dataset of your choice (e.g. `tips`, `iris`, or `penguins`). - Create a plot that appropriately visualizes the data. The type of plot (scatter, line, bar, etc.) should be suitable for the dataset you choose. 2. **Theme Configuration:** - Set the background color of the axes to light grey. - Apply the `whitegrid` style to the plot. - Sync the plot\'s theme with matplotlib\'s global `rcParams`. 3. **Display Configuration:** - Set the display format of the plot to SVG. - Disable HiDPI scaling. - Adjust the scaling factor of the embedded image to 0.8. **Input:** - The function should not take any input parameters. **Output:** - The function should display the configured seaborn plot inline (using Jupyter Notebook or similar environments). **Constraints:** - Ensure all theme and display settings are applied as specified. - Use comments to explain the configuration steps. # Example Function Template ```python import seaborn.objects as so import seaborn as sns import matplotlib as mpl def create_custom_plot(): # Load a seaborn dataset data = sns.load_dataset(\'tips\') # Create a plot (replace with appropriate plot for your dataset) p = so.Plot(data, x=\'total_bill\', y=\'tip\').add(so.Dot()) # Configure theme: background color of the axes to light grey so.Plot.config.theme[\'axes.facecolor\'] = \'lightgrey\' # Apply \'whitegrid\' style from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Sync with matplotlib\'s rcParams so.Plot.config.theme.update(mpl.rcParams) # Configure display: set format to SVG so.Plot.config.display[\\"format\\"] = \\"svg\\" # Disable HiDPI scaling so.Plot.config.display[\\"hidpi\\"] = False # Set the scaling factor to 0.8 so.Plot.config.display[\\"scaling\\"] = 0.8 # Display the plot return p.show() # Call the function to display the plot create_custom_plot() ``` # Performance: - Ensure the function executes efficiently. # Notes: - The question is self-contained. Assume the students have access to Jupyter Notebook for execution and visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl def create_custom_plot(): # Load a seaborn dataset data = sns.load_dataset(\'tips\') # Create a scatter plot plt.figure(figsize=(8, 6)) plt.scatter(data[\'total_bill\'], data[\'tip\']) # Configure theme: set the background color of the axes to light grey plt.gca().patch.set_facecolor(\'lightgrey\') # Apply \'whitegrid\' style sns.set_style(\\"whitegrid\\") # Sync with matplotlib\'s rcParams sns.set_context(\\"paper\\", rc={\\"axes.grid\\": True}) sns.set(rc=mpl.rcParams) # Configure display: set format to SVG and adjust the scaling factor mpl.rcParams[\'figure.figsize\'] = 8, 6 mpl.rcParams[\'svg.fonttype\'] = \'none\' # Disable HiDPI scaling mpl.rcParams[\'figure.dpi\'] = 100 # Set the scaling factor to 0.8 mpl.rcParams[\'figure.autolayout\'] = True plt.xlabel(\'Total Bill\') plt.ylabel(\'Tip\') plt.title(\'Scatter plot of Total Bill vs Tip\') # Display the plot plt.show() # Call the function to display the plot create_custom_plot()"},{"question":"# Advanced Coding Assessment: I/O Multiplexed Chat Server **Objective:** Implement a chat server using Python\'s `selectors` module to support multiple clients concurrently. Demonstrate your understanding of I/O multiplexing, event-driven programming, and non-blocking sockets. **Requirements:** 1. **Function Signature:** ```python def run_chat_server(host: str, port: int, timeout: int = None): pass ``` 2. **Input:** - `host` (str): The hostname or IP address where the server will run. - `port` (int): The port number on which the server will listen. - `timeout` (int, optional): The maximum wait time (in seconds) for I/O events before timing out. Defaults to `None` (no timeout). 3. **Output:** - The function should not return any value. It should run indefinitely, handling client connections and messages until manually stopped (e.g., by a keyboard interrupt). 4. **Constraints and Behavior:** - Use `selectors.DefaultSelector` to manage the file objects. - The server should handle multiple clients concurrently. - Each client should be able to send messages to the server, which will then broadcast the messages to all connected clients. - Ensure non-blocking I/O to prevent the server from blocking on one client. - Properly register, modify, and unregister file objects as needed. - Handle client disconnections gracefully, unregistering their sockets. - Implement a timeout mechanism using the `select` method. - Ensure the server runs efficiently even with a high number of clients. 5. **Performance Requirements:** - The server should handle at least 100 concurrent client connections efficiently. # Example Usage: ```python if __name__ == \\"__main__\\": run_chat_server(\'localhost\', 12345) ``` # Notes: - You may find it helpful to refer to the `selectors` documentation provided. - Ensure robust error handling and consider edge cases, such as invalid inputs or sudden client disconnections. - Write clean, readable, and well-documented code.","solution":"import selectors import socket import types def run_chat_server(host: str, port: int, timeout: int = None): sel = selectors.DefaultSelector() def accept(sock): conn, addr = sock.accept() print(f\\"Accepted connection from {addr}\\") conn.setblocking(False) data = types.SimpleNamespace(addr=addr, inb=b\\"\\", outb=b\\"\\") events = selectors.EVENT_READ | selectors.EVENT_WRITE sel.register(conn, events, data=data) def read_conn(key, mask): sock = key.fileobj data = key.data if mask & selectors.EVENT_READ: recv_data = sock.recv(1024) if recv_data: data.outb += recv_data broadcast(recv_data, sock) else: print(f\\"Closing connection to {data.addr}\\") sel.unregister(sock) sock.close() def write_conn(key, mask): sock = key.fileobj data = key.data if mask & selectors.EVENT_WRITE and data.outb: sent = sock.send(data.outb) data.outb = data.outb[sent:] def broadcast(message, source_socket): for key in sel.get_map().values(): sock = key.fileobj if sock != source_socket and hasattr(key.data, \'outb\'): key.data.outb += message lsock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) lsock.bind((host, port)) lsock.listen() print(f\\"Server started at {host} on port {port}\\") lsock.setblocking(False) sel.register(lsock, selectors.EVENT_READ, data=None) try: while True: events = sel.select(timeout) if not events: continue for key, mask in events: if key.data is None: accept(key.fileobj) else: read_conn(key, mask) write_conn(key, mask) except KeyboardInterrupt: print(\\"Caught keyboard interrupt, exiting\\") finally: sel.close()"},{"question":"POP3 Mailbox Management # Objective You are required to write a Python script that connects to a POP3 server, authenticates using a username and password, retrieves and prints the subject lines of all emails in the mailbox, and deletes emails that are older than a specified number of days. # Guidelines 1. **Connection**: - Use the `poplib.POP3(host, port)` class to establish a connection with the POP3 server. - Authenticate using the provided username and password. 2. **Retrieve Emails**: - Use the `list()` method to get the list of all emails. - For each email, retrieve its headers using the `top(msg, n)` method where `n` should be set to a value sufficient to retrieve the email subject. - Extract and print the subject line from the headers. 3. **Delete Old Emails**: - Compare the email dates with the current date. - Delete emails that are older than a specific number of days using the `dele(msg)` method. 4. **Handle Exceptions**: - Appropriately handle `poplib.error_proto` exceptions for any errors that occur during the POP3 interaction. 5. **Sign Off**: - Ensure the session is properly terminated using the `quit()` method. # Expected Input - `host`: A string representing the POP3 server host. - `port`: An integer representing the POP3 server port (optional, default is 110). - `username`: A string representing the username for authentication. - `password`: A string representing the password for authentication. - `days`: An integer representing the number of days to determine which emails should be deleted. # Expected Output - Printed subject lines of all emails in the mailbox. - Confirmation messages indicating which emails were deleted. # Constraints - Do not use any external libraries for date manipulation; use Python\'s standard library module `datetime`. - Assume that all emails have standard RFC 822 headers. # Example Below is an example of how your main function should be structured: ```python import poplib from datetime import datetime, timedelta import email from email.parser import BytesParser from email.policy import default def manage_mailbox(host, username, password, port=110, days=30): try: # Establish connection and authenticate mailbox = poplib.POP3(host, port) mailbox.user(username) mailbox.pass_(password) # Retrieve mailbox status and list messages message_count = len(mailbox.list()[1]) deletion_threshold = datetime.now() - timedelta(days=days) # Iterate through messages for i in range(message_count): response, headers, octets = mailbox.top(i+1, 0) headers = b\'n\'.join(headers) msg = BytesParser(policy=default).parsebytes(headers) print(f\\"Subject: {msg[\'subject\']}\\") # Parse the date header to determine age email_date = email.utils.parsedate_to_datetime(msg[\'date\']) email_date = email_date.replace(tzinfo=None) # Remove timezone info if email_date < deletion_threshold: mailbox.dele(i+1) # Sign off mailbox.quit() except poplib.error_proto as e: print(f\\"A POP3 error occurred: {e}\\") # Example usage manage_mailbox(\'pop.example.com\', \'example_user\', \'example_pass\', days=30) ``` Create the function `manage_mailbox` that complies with the above requirements.","solution":"import poplib from datetime import datetime, timedelta import email from email.parser import BytesParser from email.policy import default def manage_mailbox(host, username, password, port=110, days=30): try: # Establish connection and authenticate mailbox = poplib.POP3(host, port) mailbox.user(username) mailbox.pass_(password) # Retrieve mailbox status and list messages message_count = len(mailbox.list()[1]) deletion_threshold = datetime.now() - timedelta(days=days) # Iterate through messages for i in range(message_count): response, headers, octets = mailbox.top(i+1, 0) headers = b\'n\'.join(headers) msg = BytesParser(policy=default).parsebytes(headers) print(f\\"Subject: {msg[\'subject\']}\\") # Parse the date header to determine age email_date = email.utils.parsedate_to_datetime(msg[\'date\']) email_date = email_date.replace(tzinfo=None) # Remove timezone info if email_date < deletion_threshold: mailbox.dele(i+1) # Sign off mailbox.quit() except poplib.error_proto as e: print(f\\"A POP3 error occurred: {e}\\") # Example usage # manage_mailbox(\'pop.example.com\', \'example_user\', \'example_pass\', days=30)"},{"question":"# Advanced Python Coding Assessment Question **Objective**: Demonstrate your understanding of the `zipfile` module in Python by implementing functionalities to manage ZIP archives. **Problem Statement**: You are required to implement a class `ZipArchiveManager` that encapsulates various functionalities of the `zipfile` module, enabling users to easily create, read, update, and extract files from ZIP archives. # Class Requirements 1. Initialization - The class should be initialized with a path to the ZIP file. - It should support modes `\'r\'`, `\'w\'`, `\'a\'`, and `\'x\'` as defined by the `zipfile.ZipFile`. 2. Methods # `add_file(file_path: str, arcname: str = None, compression: int = zipfile.ZIP_DEFLATED)` - Adds a file to the ZIP archive. - `file_path`: Path to the file to be added. - `arcname`: Optional name for the file inside the ZIP archive. - `compression`: Compression method, default is `zipfile.ZIP_DEFLATED`. # `add_data(arcname: str, data: bytes, compression: int = zipfile.ZIP_DEFLATED)` - Adds raw bytes data to the ZIP archive. - `arcname`: Name for the data inside the ZIP archive. - `data`: Byte data to be added. - `compression`: Compression method, default is `zipfile.ZIP_DEFLATED`. # `list_files() -> list` - Lists all files in the ZIP archive. - Returns: A list of filenames. # `extract_file(file_name: str, path: str = None)` - Extracts a specific file from the ZIP archive. - `file_name`: Name of the file inside the ZIP archive to be extracted. - `path`: Optional path where the file should be extracted to. # `extract_all(path: str = None)` - Extracts all files from the ZIP archive. - `path`: Optional path where the files should be extracted to. # `get_file_info(file_name: str) -> zipfile.ZipInfo` - Retrieves information about a specific file in the archive. - `file_name`: Name of the file inside the ZIP archive. - Returns: A `zipfile.ZipInfo` object containing file information. # Constraints - Your implementation should handle appropriate exceptions such as `zipfile.BadZipFile` and `FileNotFoundError`. - The code should use context managers to handle file operations safely. - Ensure the methods handle both absolute and relative file paths correctly. # Example Usage: ```python from zipfile import ZIP_DEFLATED # Initialize the ZipArchiveManager archive_manager = ZipArchiveManager(\'example.zip\', mode=\'w\') # Add a file with default compression archive_manager.add_file(\'path/to/file.txt\') # Add raw data with a custom name inside the archive archive_manager.add_data(\'data.txt\', b\'This is some data\', compression=ZIP_DEFLATED) # List files in the archive print(archive_manager.list_files()) # Extract a specific file archive_manager.extract_file(\'data.txt\', \'extract_here/\') # Extract all files archive_manager.extract_all(\'all_files/\') # Get info of a specific file file_info = archive_manager.get_file_info(\'data.txt\') print(file_info.filename, file_info.date_time) ``` Implement the class `ZipArchiveManager` according to the specifications above.","solution":"import zipfile import os class ZipArchiveManager: def __init__(self, zip_path: str, mode: str): self.zip_path = zip_path self.mode = mode self.zipfile = zipfile.ZipFile(zip_path, mode=mode, compression=zipfile.ZIP_DEFLATED) def add_file(self, file_path: str, arcname: str = None, compression: int = zipfile.ZIP_DEFLATED): if arcname is None: arcname = os.path.basename(file_path) with zipfile.ZipFile(self.zip_path, mode=\'a\', compression=compression) as zf: zf.write(file_path, arcname) def add_data(self, arcname: str, data: bytes, compression: int = zipfile.ZIP_DEFLATED): with zipfile.ZipFile(self.zip_path, mode=\'a\', compression=compression) as zf: zf.writestr(arcname, data) def list_files(self) -> list: with zipfile.ZipFile(self.zip_path, mode=\'r\') as zf: return zf.namelist() def extract_file(self, file_name: str, path: str = None): with zipfile.ZipFile(self.zip_path, mode=\'r\') as zf: zf.extract(file_name, path) def extract_all(self, path: str = None): with zipfile.ZipFile(self.zip_path, mode=\'r\') as zf: zf.extractall(path) def get_file_info(self, file_name: str) -> zipfile.ZipInfo: with zipfile.ZipFile(self.zip_path, mode=\'r\') as zf: return zf.getinfo(file_name) def __del__(self): if hasattr(self, \'zipfile\') and self.zipfile: self.zipfile.close()"},{"question":"# Seaborn Advanced Plotting Assessment Objective Demonstrate your proficiency in using Seaborn to create complex, customizable plots. Problem Statement You are given a dataset containing various attributes of cars. Your task is to: 1. Load the dataset using Seaborn\'s `load_dataset` function. 2. Create a series of facet plots showing the relationship between the car\'s weight (`weight`) and miles per gallon (`mpg`), separated by the origin of the car (`origin`). 3. Add a regression line (`so.Line` and `so.PolyFit`) and the original data points (`so.Dot`). 4. Customize the overall appearance of the plots by: - Setting a white grid background for the plots. - Using a linewidth of 3 for the regression lines. - Applying the `ggplot` style from Matplotlib. Input Format 1. The dataset to use is \'mpg\', which can be loaded with `mpg = load_dataset(\\"mpg\\")`. Output Format - Display the plot as specified in the problem statement. Constraints - Ensure the use of Seaborn\'s `Plot` object and relevant methods for customization. - The final plot should contain multiple facets separated by car `origin`. Performance Requirement - The plot should render correctly without any errors. Example ```python import seaborn.objects as so from seaborn import load_dataset from matplotlib import style # Load the dataset mpg = load_dataset(\\"mpg\\") # Create the plot p = ( so.Plot(mpg, \\"weight\\", \\"mpg\\", color=\\"origin\\") .facet(\\"origin\\", wrap=3) # Facet by origin .add(so.Line(), so.PolyFit(order=1)) # Add regression line .add(so.Dot()) # Add data points .theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\", \\"lines.linewidth\\": 3}) # Custom theme settings .theme(style.library[\\"ggplot\\"]) # Apply ggplot style ) # Display the plot p ``` In completing this task, you will demonstrate your ability to effectively use Seaborn for creating complex, customized visual data representations.","solution":"import seaborn.objects as so from seaborn import load_dataset from matplotlib import style def create_plot(): # Load the dataset mpg = load_dataset(\\"mpg\\") # Apply ggplot style style.use(\\"ggplot\\") # Create the plot p = ( so.Plot(mpg, x=\\"weight\\", y=\\"mpg\\", color=\\"origin\\") .facet(\\"origin\\", wrap=3) # Facet by origin .add(so.Line(linewidth=3), so.PolyFit(order=1)) # Add regression line .add(so.Dot()) # Add data points .theme({ \\"axes.facecolor\\": \\"white\\", \\"grid.linewidth\\": 1.5, }) # Custom theme settings ) # Display the plot return p"},{"question":"# Objective You are required to write a PyTorch-based script that demonstrates setting up a deterministic environment for training a simple neural network. This assessment will test your understanding of ensuring reproducibility and using deterministic algorithms in PyTorch. # Question Implement a PyTorch script that: 1. Creates a simple feedforward neural network. 2. Generates random data for training. 3. Performs one step of training with the generated data. 4. Ensures the results are reproducible. Your implementation should include: - Setting the manual seed for PyTorch, Python\'s random module, and NumPy. - Using deterministic algorithms in PyTorch. - Configuring the DataLoader for reproducibility. # Requirements 1. **Neural Network**: Define a simple neural network with at least one hidden layer using `torch.nn`. 2. **Data Generation**: Generate a random dataset using `numpy`. 3. **DataLoader**: Use PyTorch\'s `DataLoader` to load the data. 4. **Determinism**: Ensure all relevant settings and configurations are set for reproducibility. # Constraints - Use PyTorch and NumPy libraries. - The neural network should use ReLU activation for the hidden layer and a suitable output activation function for a regression problem. - Perform only one training step (forward and backward pass) and log the model\'s loss. # Expected Input and Output - **Input**: None (You will generate random data within the script) - **Output**: Print the loss after one training step. # Performance Requirements - Ensure that the script runs deterministically and reproduces the same results every time it is executed. ```python # Sample code structure import torch import torch.nn as nn import torch.optim as optim import numpy as np import random # Function to seed everything for reproducibility def set_seed(seed): torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Neural Network Definition class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.output = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.hidden(x) out = self.relu(out) out = self.output(out) return out # Main function def main(): set_seed(0) # Hyperparameters input_size = 10 hidden_size = 5 output_size = 1 batch_size = 4 # Generate random data using numpy data = np.random.rand(100, input_size).astype(np.float32) targets = np.random.rand(100, output_size).astype(np.float32) # Dataset and DataLoader class RandomDataset(torch.utils.data.Dataset): def __init__(self, data, targets): self.data = torch.tensor(data) self.targets = torch.tensor(targets) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.targets[idx] dataset = RandomDataset(data, targets) def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) g = torch.Generator() g.manual_seed(0) dataloader = torch.utils.data.DataLoader( dataset, batch_size=batch_size, worker_init_fn=seed_worker, generator=g) # Initialize the neural network, optimizer, and loss function model = SimpleNN(input_size, hidden_size, output_size) optimizer = optim.SGD(model.parameters(), lr=0.01) criterion = nn.MSELoss() # Training loop (one step) for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Loss: {loss.item()}\') break if __name__ == \\"__main__\\": main() ``` # Explanation - The `set_seed` function sets seeds for PyTorch, Python\'s random module, and NumPy to ensure reproducibility. - The `SimpleNN` class defines a simple feedforward neural network. - Random data is generated using NumPy and loaded using a `DataLoader`. - The `main` function initializes the model, optimizer, and loss function and runs one training step, printing the loss value.","solution":"import torch import torch.nn as nn import torch.optim as optim import numpy as np import random # Function to seed everything for reproducibility def set_seed(seed): torch.manual_seed(seed) random.seed(seed) np.random.seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Neural Network Definition class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.output = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.hidden(x) out = self.relu(out) out = self.output(out) return out # Main function def main(): set_seed(0) # Hyperparameters input_size = 10 hidden_size = 5 output_size = 1 batch_size = 4 # Generate random data using numpy data = np.random.rand(100, input_size).astype(np.float32) targets = np.random.rand(100, output_size).astype(np.float32) # Dataset and DataLoader class RandomDataset(torch.utils.data.Dataset): def __init__(self, data, targets): self.data = torch.tensor(data) self.targets = torch.tensor(targets) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.targets[idx] dataset = RandomDataset(data, targets) def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) g = torch.Generator() g.manual_seed(0) dataloader = torch.utils.data.DataLoader( dataset, batch_size=batch_size, worker_init_fn=seed_worker, generator=g) # Initialize the neural network, optimizer, and loss function model = SimpleNN(input_size, hidden_size, output_size) optimizer = optim.SGD(model.parameters(), lr=0.01) criterion = nn.MSELoss() # Training loop (one step) for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Loss: {loss.item()}\') break if __name__ == \\"__main__\\": main()"},{"question":"# Task: You are to create a Python application using the `tkinter.tix` module that demonstrates your understanding of various advanced widgets provided by this module. # Requirements: 1. **Setup**: - Import the required modules: `tix` from `tkinter` and necessary constants. - Initialize the main application window using `tix.Tk()`. 2. **Widgets**: - Create a hierarchical list (using `HList`) to display a directory structure. - Embed a ComboBox that allows selection from multiple options. - Implement a NoteBook with at least two tabs: one for the hierarchical list and another for the ComboBox. - Implement a Balloon widget that displays help when hovering over the ComboBox. - Use a ButtonBox or StdButtonBox to provide Ok/Cancel buttons. 3. **Functionality**: - The hierarchical list should display a dummy directory structure. - The ComboBox should allow the user to select their favorite programming language from a predefined list. - When the user hovers over the ComboBox, a Balloon should pop up with a message \\"Select your favorite programming language.\\" - On clicking the Ok button, show a message box displaying the selected programming language from the ComboBox. - The Cancel button should clear the selection in the ComboBox. # Input: - No direct input from a user file or command line. All interactions should be through the GUI. # Output: - A fully functional GUI application as described. # Example Usage: - The application window opens, displaying the NoteBook with two tabs. - The first tab contains a hierarchical list showing dummy directory data. - The second tab contains a ComboBox with programming languages and Ok/Cancel buttons below. - Hovering over the ComboBox displays the Balloon help message. - Clicking Ok after selecting a language shows a message box with the selected language. - Clicking Cancel clears the ComboBox. # Constraints: - Use only the `tkinter.tix` module and standard Python libraries. - Ensure the GUI is responsive and error-free. **Note**: Make sure Tix widgets are installed in your environment for this to work. # Your implementation should be in the following structure: ```python from tkinter import tix, messagebox from tkinter.constants import * def main(): # Initialize the main application window root = tix.Tk() root.title(\'Advanced Tix Widgets Application\') # Define widgets and layout # ... root.mainloop() if __name__ == \\"__main__\\": main() ```","solution":"from tkinter import tix, messagebox from tkinter.constants import * def main(): # Initialize the main application window root = tix.Tk() root.title(\'Advanced Tix Widgets Application\') # Notebook widget notebook = tix.NoteBook(root) notebook.pack(expand=1, fill=BOTH) # First tab with hierarchical list hlist_tab = notebook.add(\'hlist_tab\', label=\'Directory Structure\') hlist = tix.HList(hlist_tab) hlist.add(\\"root\\", text=\\"Root Directory\\") hlist.add(\\"root/dir1\\", text=\\"Directory 1\\") hlist.add(\\"root/dir2\\", text=\\"Directory 2\\") hlist.add(\\"root/dir1/file1\\", text=\\"File 1\\") hlist.add(\\"root/dir2/file2\\", text=\\"File 2\\") hlist.pack(expand=1, fill=BOTH) # Second tab with ComboBox and Buttons combo_tab = notebook.add(\'combo_tab\', label=\'Select Language\') combo_label = tix.Label(combo_tab, text=\'Choose your favorite programming language:\') combo_label.pack(pady=10) languages = [\\"Python\\", \\"Java\\", \\"C++\\", \\"JavaScript\\", \\"Ruby\\"] comboBox = tix.ComboBox(combo_tab, editable=1, dropdown=1) comboBox.entry[\\"state\\"] = \\"readonly\\" for lang in languages: comboBox.insert(END, lang) comboBox.pack(pady=10) # Balloon widget for help message balloon = tix.Balloon(combo_tab) balloon.bind_widget(comboBox, balloonmsg=\\"Select your favorite programming language.\\") # ButtonBox button_box = tix.StdButtonBox(combo_tab) button_box.pack(pady=10) button_box.add(\'ok\', text=\'Ok\', command=lambda: show_message(comboBox)) button_box.add(\'cancel\', text=\'Cancel\', command=lambda: comboBox.entry.delete(0, END)) button_box.button[\'cancel\'].pack(expand=True) notebook.pack(expand=1, fill=BOTH) root.mainloop() def show_message(comboBox): selected_language = comboBox.entry.get() messagebox.showinfo(\\"Selected Language\\", f\\"You selected: {selected_language}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Demonstrate your understanding of the seaborn `swarmplot` function by creating a complex visualization that combines several features. **Question**: Using the seaborn library and its `swarmplot` function, create a visualization that satisfies the following requirements: 1. Load the `tips` dataset from seaborn. 2. Create a horizontal swarm plot to display the distribution of `total_bill` for each `day`. 3. Color the points based on the `time` variable. 4. Display different markers for each `sex` category. 5. Adjust the size of the markers to `5`. 6. Split the hues using the `dodge` parameter. 7. Use a distinct palette to ensure clarity. 8. Create a multiple facet plot where each facet represents a day of the week. **Expected Input**: - None. You will utilize the seaborn library\'s `tips` dataset. **Expected Output**: - A matplotlib plot window displaying the desired swarm plot with the specified visual customizations. **Constraints**: - Ensure that all points are clearly visible and not overlapping excessively. - Choose a palette that makes differences between categories easily distinguishable. **Performance Requirements**: - The code should run efficiently without significant delays for rendering the plot. **Function Signature**: ```python def create_custom_swarm_plot(): import seaborn as sns import matplotlib.pyplot as plt # Your code here plt.show() ```","solution":"def create_custom_swarm_plot(): import seaborn as sns import matplotlib.pyplot as plt # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") # Create a SwarmPlot with the specified conditions g = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", palette=\\"muted\\", size=5, marker=\\"o\\", dodge=True ) # Adjust aesthetics for better clarity g.set_axis_labels(\\"Total Bill\\", \\"Day\\") g.set_titles(\\"{col_name}\\") # Show the plot plt.show()"},{"question":"# Codec Conversion and Error Handling Implementation **Objective:** Design a Python program to read and write text files with specified encodings, using the `codecs` module. Implement error handling for encoding/decoding errors and provide functionality to switch between different encoding schemes dynamically. **Problem Statement:** Write a set of functions that: 1. Encode a given text in a specified encoding and write it to a file. 2. Decode the contents of a file from a specified encoding to text. 3. Handle encoding and decoding errors using different error handling schemes (`strict`, `ignore`, `replace`, etc.). 4. Incrementally encode or decode data streams. 5. Allow dynamic switching between different encoding schemes for reading and writing. # Specific Requirements: 1. **Function `encode_to_file(text, filename, encoding, errors=\'strict\')`:** - **Input:** - `text` (str): The text to be encoded. - `filename` (str): The name of the file where the encoded text will be written. - `encoding` (str): The encoding scheme to use (e.g., `utf-8`, `latin-1`). - `errors` (str, optional): Error handling scheme (default is `\'strict\'`). - **Output:** None - **Functionality:** Encode the given text using the specified encoding and write it to the specified file. Handle any encoding errors as specified by the `errors` parameter. 2. **Function `decode_from_file(filename, encoding, errors=\'strict\')`:** - **Input:** - `filename` (str): The name of the file to read from. - `encoding` (str): The encoding scheme to use (e.g., `utf-8`, `latin-1`). - `errors` (str, optional): Error handling scheme (default is `\'strict\'`). - **Output:** - Decoded text (str). - **Functionality:** Read the contents of the specified file, decode it using the specified encoding, and handle any decoding errors as specified by the `errors` parameter. 3. **Function `incremental_encode(data_stream, encoding, errors=\'strict\')`:** - **Input:** - `data_stream` (iterable): An iterable stream of text data to encode incrementally. - `encoding` (str): The encoding scheme to use. - `errors` (str, optional): Error handling scheme (default is `\'strict\'`). - **Output:** - A generator yielding incrementally encoded data chunks. - **Functionality:** Use an incremental encoder to encode the data stream. Handle encoding errors as specified by the `errors` parameter. 4. **Function `incremental_decode(data_stream, encoding, errors=\'strict\')`:** - **Input:** - `data_stream` (iterable): An iterable stream of byte data to decode incrementally. - `encoding` (str): The encoding scheme to use. - `errors` (str, optional): Error handling scheme (default is `\'strict\'`). - **Output:** - A generator yielding incrementally decoded data chunks. - **Functionality:** Use an incremental decoder to decode the data stream. Handle decoding errors as specified by the `errors` parameter. 5. **Class `DynamicCodecManager` for dynamic switching of codecs:** - **Methods:** - `__init__(self, default_encoding=\'utf-8\', default_errors=\'strict\')`: Initialize with a default encoding and error handling scheme. - `change_encoding(self, new_encoding)`: Change the encoding scheme. - `change_error_handling(self, new_errors)`: Change the error handling scheme. - `encode(self, text)`: Encode text with the current encoding scheme. - `decode(self, data)`: Decode data with the current encoding scheme. - **Example methods output:** Encoded/decoded data. # Constraints: - Assume all input and output files exist and are accessible with read/write permissions. - The `data_stream` for incremental encoding/decoding should be an iterable (e.g., list, generator) that yields text (for encoding) or bytes (for decoding). # Example Usage: ```python # Encode text to a file encode_to_file(\\"Hello, World!\\", \\"output.txt\\", \\"utf-8\\") # Decode text from a file decoded_text = decode_from_file(\\"output.txt\\", \\"utf-8\\") print(decoded_text) # Output: Hello, World! # Incremental encoding data_stream = [\\"part1n\\", \\"part2n\\", \\"part3n\\"] encoded_chunks = incremental_encode(data_stream, \\"utf-8\\") for chunk in encoded_chunks: print(chunk) # Incremental decoding data_stream = [b\'part1n\', b\'part2n\', b\'part3n\'] decoded_chunks = incremental_decode(data_stream, \\"utf-8\\") for chunk in decoded_chunks: print(chunk) # Dynamic Codec Manager manager = DynamicCodecManager() encoded_data = manager.encode(\\"Dynamic Encoding\\") print(encoded_data) # Encodes with default utf-8 manager.change_encoding(\\"latin-1\\") decoded_data = manager.decode(encoded_data) print(decoded_data) # Decodes with latin-1 ```","solution":"import codecs def encode_to_file(text, filename, encoding, errors=\'strict\'): with codecs.open(filename, \'w\', encoding, errors) as file: file.write(text) def decode_from_file(filename, encoding, errors=\'strict\'): with codecs.open(filename, \'r\', encoding, errors) as file: return file.read() def incremental_encode(data_stream, encoding, errors=\'strict\'): encoder = codecs.getincrementalencoder(encoding)(errors) for item in data_stream: yield encoder.encode(item) yield encoder.encode(\'\', final=True) def incremental_decode(data_stream, encoding, errors=\'strict\'): decoder = codecs.getincrementaldecoder(encoding)(errors) for item in data_stream: yield decoder.decode(item) yield decoder.decode(b\'\', final=True) class DynamicCodecManager: def __init__(self, default_encoding=\'utf-8\', default_errors=\'strict\'): self.encoding = default_encoding self.errors = default_errors def change_encoding(self, new_encoding): self.encoding = new_encoding def change_error_handling(self, new_errors): self.errors = new_errors def encode(self, text): return text.encode(self.encoding, self.errors) def decode(self, data): return data.decode(self.encoding, self.errors)"},{"question":"**Objective:** You are tasked with configuring various backend settings in PyTorch for a given device. Your function should handle different configurations and verify if the settings are correctly applied. **Problem Statement:** Implement a function `configure_backends(device: str, settings: dict) -> dict` that configures the backend settings for the specified device and returns a dictionary indicating whether each setting was applied successfully. **Function Details:** - **Input:** - `device` (str): The device for which to configure backends. It can be either `\'cpu\'` or `\'cuda\'`. - `settings` (dict): A dictionary containing the settings to be applied. The dictionary keys can include: - For `\'cpu\'` device: - `\'get_cpu_capability\'`: Any value (this key presence is just to check function availability) - For `\'cuda\'` device: - `\'allow_tf32\'` (bool): Enable or disable TensorFloat-32 tensor cores. - `\'allow_fp16_reduced_precision_reduction\'` (bool): Enable or disable FP16 reduced precision reductions. - `\'allow_bf16_reduced_precision_reduction\'` (bool): Enable or disable BF16 reduced precision reductions. - `\'cufft_plan_cache_max_size\'` (int): Set the maximum size for cuFFT plan cache. - `\'enable_cudnn\'` (bool): Enable or disable cuDNN. - **Output:** - A dictionary with keys corresponding to each setting in the input `settings` and values indicating if the setting was applied successfully (True or False). **Constraints:** - You can assume valid keys and values will be provided in the `settings` dictionary. - You should handle any potential exceptions gracefully and ensure the function returns `False` for unsuccessful settings. **Example:** ```python def configure_backends(device: str, settings: dict) -> dict: result = {} try: if device == \'cpu\': if \'get_cpu_capability\' in settings: result[\'get_cpu_capability\'] = True if torch.backends.cpu.get_cpu_capability() else False elif device == \'cuda\': if \'allow_tf32\' in settings: torch.backends.cuda.matmul.allow_tf32 = settings[\'allow_tf32\'] result[\'allow_tf32\'] = torch.backends.cuda.matmul.allow_tf32 == settings[\'allow_tf32\'] if \'allow_fp16_reduced_precision_reduction\' in settings: torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = settings[\'allow_fp16_reduced_precision_reduction\'] result[\'allow_fp16_reduced_precision_reduction\'] = torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction == settings[\'allow_fp16_reduced_precision_reduction\'] if \'allow_bf16_reduced_precision_reduction\' in settings: torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = settings[\'allow_bf16_reduced_precision_reduction\'] result[\'allow_bf16_reduced_precision_reduction\'] = torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction == settings[\'allow_bf16_reduced_precision_reduction\'] if \'cufft_plan_cache_max_size\' in settings: torch.backends.cuda.cufft_plan_cache.max_size = settings[\'cufft_plan_cache_max_size\'] result[\'cufft_plan_cache_max_size\'] = torch.backends.cuda.cufft_plan_cache.max_size == settings[\'cufft_plan_cache_max_size\'] if \'enable_cudnn\' in settings: torch.backends.cudnn.enabled = settings[\'enable_cudnn\'] result[\'enable_cudnn\'] = torch.backends.cudnn.enabled == settings[\'enable_cudnn\'] except Exception: return {k: False for k in settings} return result # Example usage device = \'cuda\' settings = { \'allow_tf32\': True, \'cufft_plan_cache_max_size\': 512, \'enable_cudnn\': False } print(configure_backends(device, settings)) # Output: {\'allow_tf32\': True, \'cufft_plan_cache_max_size\': True, \'enable_cudnn\': True} ``` **Explanation:** - The function first checks the device type and then applies the provided settings accordingly. It validates if settings are applied successfully and handles any errors gracefully.","solution":"import torch def configure_backends(device: str, settings: dict) -> dict: result = {} try: if device == \'cpu\': if \'get_cpu_capability\' in settings: result[\'get_cpu_capability\'] = True if torch.backends.cpu.get_cpu_capability() else False elif device == \'cuda\': if \'allow_tf32\' in settings: torch.backends.cuda.matmul.allow_tf32 = settings[\'allow_tf32\'] result[\'allow_tf32\'] = torch.backends.cuda.matmul.allow_tf32 == settings[\'allow_tf32\'] if \'allow_fp16_reduced_precision_reduction\' in settings: torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = settings[\'allow_fp16_reduced_precision_reduction\'] result[\'allow_fp16_reduced_precision_reduction\'] = torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction == settings[\'allow_fp16_reduced_precision_reduction\'] if \'allow_bf16_reduced_precision_reduction\' in settings: torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = settings[\'allow_bf16_reduced_precision_reduction\'] result[\'allow_bf16_reduced_precision_reduction\'] = torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction == settings[\'allow_bf16_reduced_precision_reduction\'] if \'cufft_plan_cache_max_size\' in settings: torch.backends.cuda.cufft_plan_cache.max_size = settings[\'cufft_plan_cache_max_size\'] result[\'cufft_plan_cache_max_size\'] = torch.backends.cuda.cufft_plan_cache.max_size == settings[\'cufft_plan_cache_max_size\'] if \'enable_cudnn\' in settings: torch.backends.cudnn.enabled = settings[\'enable_cudnn\'] result[\'enable_cudnn\'] = torch.backends.cudnn.enabled == settings[\'enable_cudnn\'] except Exception: return {k: False for k in settings} return result"},{"question":"# Design a Customized Cubehelix Palette and Visualize Data **Objective:** Write a function `custom_cubehelix_plot(data, n_colors, start, rot, gamma, hue, dark, light, reverse, as_cmap)` that creates a seaborn cubehelix palette based on the provided parameters and uses it to create a clustermap based on the provided dataset. Parameters: - `data` (pd.DataFrame): A pandas DataFrame for which the clustermap should be created. - `n_colors` (int): Number of colors in the palette. - `start` (float): Starting position of the helix. - `rot` (float): Rotations around the hue wheel. - `gamma` (float): Gamma factor to influence luminance. - `hue` (float): Saturation of the colors. - `dark` (float): Darkness of the palette. - `light` (float): Lightness of the palette. - `reverse` (bool): If True, reverse the luminance ramp. - `as_cmap` (bool): If True, return a continuous colormap. Returns: - None. The function should display the clustermap. Constraints: - `n_colors` must be a positive integer. - `start`, `rot`, `gamma`, `hue`, `dark`, `light` must be float values. - `reverse` must be a boolean. - `as_cmap` must be a boolean. Tasks: 1. Validate the input parameters. 2. Generate a customized cubehelix palette using the seaborn `cubehelix_palette` function. 3. Use the generated palette to create and display a seaborn clustermap from the provided DataFrame. Example: ```python import seaborn as sns import pandas as pd df = pd.DataFrame(data=[[0.049325, 0.129128, 0.231061, 0.33, 0.45], [0.43, 0.39, 0.33, 0.21, 0.18], [0.56, 0.67, 0.23, 0.17, 0.44], [0.78, 0.59, 0.22, 0.45, 0.55]]) custom_cubehelix_plot(data=df, n_colors=8, start=0, rot=.5, gamma=1, hue=.8, dark=.2, light=.8, reverse=False, as_cmap=False) ``` The solution should demonstrate the understanding and application of various parameters of the `cubehelix_palette` function to customize and visualize data using seaborn.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def custom_cubehelix_plot(data, n_colors, start, rot, gamma, hue, dark, light, reverse, as_cmap): Creates a seaborn cubehelix palette based on the provided parameters and uses it to create a clustermap based on the provided dataset. Parameters: - data (pd.DataFrame): A pandas DataFrame for which the clustermap should be created. - n_colors (int): Number of colors in the palette. - start (float): Starting position of the helix. - rot (float): Rotations around the hue wheel. - gamma (float): Gamma factor to influence luminance. - hue (float): Saturation of the colors. - dark (float): Darkness of the palette. - light (float): Lightness of the palette. - reverse (bool): If True, reverse the luminance ramp. - as_cmap (bool): If True, return a continuous colormap. Returns: - None. The function displays the clustermap. # Validate inputs if not isinstance(data, pd.DataFrame): raise ValueError(\\"Input data should be a Pandas DataFrame\\") if not isinstance(n_colors, int) or n_colors <= 0: raise ValueError(\\"n_colors should be a positive integer\\") for param in [start, rot, gamma, hue, dark, light]: if not isinstance(param, float): raise ValueError(f\\"{param} should be a float value\\") if not isinstance(reverse, bool): raise ValueError(\\"reverse should be a boolean\\") if not isinstance(as_cmap, bool): raise ValueError(\\"as_cmap should be a boolean\\") # Create a cubehelix palette palette = sns.cubehelix_palette(n_colors=n_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse, as_cmap=as_cmap) # Create the clustermap sns.clustermap(data, cmap=palette) plt.show()"},{"question":"# Question: Pretty-Print a Nested Data Structure You are given a nested data structure containing various types of elements such as lists, dictionaries, tuples, and sets. Your task is to: 1. Use `PrettyPrinter` from the `pprint` module to create a function `pretty_print_data(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False)` that takes in a data structure along with several optional parameters for formatting. 2. The function should return a string that contains the pretty-printed representation of the data. 3. Additionally, implement and use a function `is_data_readable(data)` that determines if the formatted representation of the data is \\"readable\\" (i.e., can be used to reconstruct the value using `eval()`). 4. Implement and use a function `is_data_recursive(data)` that determines if the data structure requires a recursive representation. Function Signatures: ```python def pretty_print_data(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False) -> str: pass def is_data_readable(data) -> bool: pass def is_data_recursive(data) -> bool: pass ``` Input: - `data`: A nested data structure (list, dict, tuple, set, etc.). - Optional parameters for `pretty_print_data`: - `indent` (int): Specifies the amount of indentation for each level (default is 1). - `width` (int): The maximum number of characters per line (default is 80). - `depth` (int or None): Controls the number of nesting levels allowed (default is None). - `compact` (bool): If True, as many items as possible will be printed on each line (default is False). - `sort_dicts` (bool): If True, dictionaries will be sorted by keys (default is True). - `underscore_numbers` (bool): If True, integers will be formatted with underscores as thousands separators (default is False). Output: - `pretty_print_data` should return a string containing the pretty-printed representation of the input data. - `is_data_readable` should return a boolean indicating if the data is \\"readable\\". - `is_data_recursive` should return a boolean indicating if the data is recursive. Example: ```python data = { \'numbers\': [1, 2, 3, 4], \'nested_dict\': { \'a\': \'apple\', \'b\': \'banana\', \'c\': { \'d\': \'date\', \'e\': \'elderberry\' } }, \'tuple\': (1, 2, 3), \'set\': {1, 2, 3} } print(pretty_print_data(data, indent=4, width=50)) print(is_data_readable(data)) # Output: False print(is_data_recursive(data)) # Output: False ``` Constraints: - You should not use any external libraries other than `pprint`. - Ensure your solution handles various types of nested structures. - Assume input data can be arbitrarily complex. Assessment Criteria: - Correctness of the implementation. - Understanding of the `pprint` module and its functionalities. - Ability to handle optional parameters correctly. - Proper handling of nested and recursive data structures.","solution":"import pprint def pretty_print_data(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False) -> str: Returns a pretty-printed string representation of the data using the specified parameters. :param data: The data structure to pretty-print. :param indent: The amount of indentation for each level. :param width: The maximum number of characters per line. :param depth: The maximum depth level for nested structures. :param compact: Whether to allow compact printing. :param sort_dicts: Whether to sort dictionary keys. :param underscore_numbers: Whether to use underscores as thousand separators in numbers. :return: A pretty-printed string representation of the data. printer = pprint.PrettyPrinter(indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts, underscore_numbers=underscore_numbers) return printer.pformat(data) def is_data_readable(data) -> bool: Determines if the formatted representation of the data can be used to reconstruct the value using eval(). :param data: The data structure to check. :return: A boolean indicating if the data is \\"readable\\". try: eval(repr(data)) return True except: return False def is_data_recursive(data) -> bool: Determines if the data structure is recursive. :param data: The data structure to check. :return: A boolean indicating if the data is recursive. try: iter(data) if isinstance(data, (str, bytes)): return False except TypeError: return False data_id = id(data) seen_ids = set() def check(data): if id(data) in seen_ids: return True seen_ids.add(id(data)) if isinstance(data, (list, tuple, set, frozenset)): return any(check(item) for item in data) elif isinstance(data, dict): return any(check(key) or check(value) for key, value in data.items()) return False return check(data)"},{"question":"Objective Write a Python function `load_module_from_zip(archivepath, modulename)` that dynamically loads a specified module from a given ZIP archive and returns a reference to the loaded module. If the module cannot be found or loaded, raise an appropriate exception. Function Signature ```python def load_module_from_zip(archivepath: str, modulename: str): Load a module from a ZIP archive and return the module reference. Parameters: archivepath (str): Path to the ZIP file archive or a specific path within the ZIP file. modulename (str): The fully qualified (dotted) name of the module to be loaded. Returns: module: The imported module object. Raises: zipimport.ZipImportError: If the archive path is invalid or the module cannot be imported. pass ``` Input and Output - **Input**: - `archivepath`: A string representing the path to the ZIP archive file or a specific path within the ZIP file. - `modulename`: A string representing the fully qualified (dotted) name of the module to be imported. - **Output**: - Returns the loaded module object if successful. Constraints - Only consider modules with `\\".py\\"` extension. - Assume that the input ZIP archive is not corrupted and is accessible. Example Usage ```python # Example ZIP archive structure: # my_archive.zip/ # ├── a_module.py # └── my_package/ # ├── __init__.py # ├── sub_module.py # Load a top-level module mod = load_module_from_zip(\\"my_archive.zip\\", \\"a_module\\") print(mod) # Output: <module \'a_module\' from \'my_archive.zip/a_module.py\'> # Load a sub-module from a package pkg_mod = load_module_from_zip(\\"my_archive.zip\\", \\"my_package.sub_module\\") print(pkg_mod) # Output: <module \'my_package.sub_module\' from \'my_archive.zip/my_package/sub_module.py\'> ``` Notes - This question assesses your understanding of Python\'s import mechanism, particularly in loading modules from ZIP archives. - Use the `zipimport` module functions to implement the `load_module_from_zip` function.","solution":"import zipimport def load_module_from_zip(archivepath: str, modulename: str): Load a module from a ZIP archive and return the module reference. Parameters: archivepath (str): Path to the ZIP file archive or a specific path within the ZIP file. modulename (str): The fully qualified (dotted) name of the module to be loaded. Returns: module: The imported module object. Raises: zipimport.ZipImportError: If the archive path is invalid or the module cannot be imported. try: # Create a zip importer instance importer = zipimport.zipimporter(archivepath) # Load the module using the importer module = importer.load_module(modulename) return module except zipimport.ZipImportError as e: raise zipimport.ZipImportError(f\\"Cannot load module {modulename} from {archivepath}: {e}\\") except Exception as e: raise ImportError(f\\"An error occurred while loading module {modulename} from {archivepath}: {e}\\")"},{"question":"**Objective:** Write a Python function that reads a web page, parses URLs and checks their accessibility. **Problem Statement:** You need to implement a function `check_urls_accessibility(url: str) -> dict` that takes a URL of a web page as input. The function should: 1. Open and read the HTML content of the provided URL. 2. Parse all the URLs found within the HTML content. 3. Check the accessibility (whether a request to the URL returns a successful response) for each parsed URL. 4. Return a dictionary where the keys are the parsed URLs and the values are booleans indicating the accessibility (True if accessible, False otherwise). **Function Signature:** ```python def check_urls_accessibility(url: str) -> dict: # Your code here ``` **Input:** - A single string `url` representing the URL of the web page to be processed. **Output:** - A dictionary where keys are URLs (strings) parsed from the HTML content, and values are booleans indicating accessibility. **Constraints:** - You should only consider HTTP and HTTPS URLs. - Handle at most 100 URLs from the given page. - Assume the input URL will always be a valid URL. - Your function should handle timeouts and network errors gracefully. **Example:** ```python valid_url = \\"https://example.com/pages-with-urls\\" output = check_urls_accessibility(valid_url) # output could be like: # { # \\"https://validlink.com\\": True, # \\"https://brokenlink.com\\": False, # ... # } ``` **Note:** - You can use the `urllib.request`, `urllib.parse`, and `urllib.error` modules from the `urllib` package for implementing this function. - Take care of timeouts and invalid URLs to prevent your function from crashing. Implement the function `check_urls_accessibility` as per the specification above.","solution":"import urllib.request import urllib.error from urllib.parse import urljoin from bs4 import BeautifulSoup def check_urls_accessibility(url: str) -> dict: Reads a web page, parses URLs and checks their accessibility. :param url: A string representing the URL of the web page to be processed. :return: A dictionary where keys are URLs parsed from the HTML content, and values are booleans indicating accessibility (True if accessible, False otherwise). def is_accessible(test_url): try: response = urllib.request.urlopen(test_url, timeout=5) return 200 <= response.getcode() < 400 except (urllib.error.HTTPError, urllib.error.URLError, ValueError): return False try: response = urllib.request.urlopen(url) html = response.read() except (urllib.error.HTTPError, urllib.error.URLError): return {} soup = BeautifulSoup(html, \'html.parser\') links = soup.find_all(\'a\', href=True) urls = {} for link in links[:100]: # Handle at most 100 URLs href = link.get(\'href\') full_url = urljoin(url, href) if full_url.startswith(\\"http://\\") or full_url.startswith(\\"https://\\"): urls[full_url] = is_accessible(full_url) return urls"},{"question":"# **Coding Challenge: Advanced Usage of Python Built-in Types** **Problem Statement:** You have been tasked with creating a small library for handling numeric and sequence operations. This library will include functions for working with integers, floating-point numbers, and lists. Specifically, you need to create the following functions: 1. **nth_fibonacci_number(n: int) -> int** - **Description**: A function to compute the nth Fibonacci number using `int` type. - **Input**: - `n` (int): The position of the Fibonacci number to compute. Must be a non-negative integer. - **Output**: - Returns the nth Fibonacci number as an integer. - **Constraints**: - You cannot use recursion due to performance limitations. - **Example**: ```python nth_fibonacci_number(4) # Returns 3 ``` 2. **is_palindrome_number(num: int) -> bool** - **Description**: A function to check if a given number is a palindrome. - **Input**: - `num` (int): The number to check for palindrome property. - **Output**: - Returns `True` if the number is a palindrome, `False` otherwise. - **Example**: ```python is_palindrome_number(121) # Returns True is_palindrome_number(123) # Returns False ``` 3. **average_of_float_list(numbers: list[float]) -> float** - **Description**: A function to compute the average of a list of floating-point numbers. - **Input**: - `numbers` (list[float]): A list of floating-point numbers. - **Output**: - Returns the average of the numbers as a float. - **Constraints**: - The list should not be empty. - **Example**: ```python average_of_float_list([1.0, 2.0, 3.0]) # Returns 2.0 ``` 4. **most_frequent_element(lst: list[int]) -> tuple[int, int]** - **Description**: A function to find the most frequent element in a list of integers and its frequency. - **Input**: - `lst` (list[int]): A list of integers. - **Output**: - Returns a tuple where the first element is the most frequent integer and the second element is its frequency. - If there are multiple elements with the same highest frequency, return the smallest one. - **Constraints**: - The list should not be empty. - **Example**: ```python most_frequent_element([1, 3, 1, 3, 2, 1]) # Returns (1, 3) ``` **Requirements:** 1. Use Python 3.10 specific features where possible. 2. Adhere to best coding practices for readability and performance. 3. Handle edge cases gracefully. **Function Signatures:** ```python def nth_fibonacci_number(n: int) -> int: pass def is_palindrome_number(num: int) -> bool: pass def average_of_float_list(numbers: list[float]) -> float: pass def most_frequent_element(lst: list[int]) -> tuple[int, int]: pass ``` Implement these functions and ensure they pass the given examples. Also, consider adding additional test cases to validate the correctness of your functions.","solution":"def nth_fibonacci_number(n: int) -> int: Returns the nth Fibonacci number. if n == 0: return 0 elif n == 1: return 1 a, b = 0, 1 for _ in range(2, n + 1): a, b = b, a + b return b def is_palindrome_number(num: int) -> bool: Returns True if num is a palindrome, False otherwise. return str(num) == str(num)[::-1] def average_of_float_list(numbers: list[float]) -> float: Returns the average of a list of floating-point numbers. if not numbers: raise ValueError(\\"The list should not be empty\\") return sum(numbers) / len(numbers) def most_frequent_element(lst: list[int]) -> tuple[int, int]: Returns a tuple with the most frequent element and its frequency. If there are multiple elements with the highest frequency, returns the smallest one. if not lst: raise ValueError(\\"The list should not be empty\\") from collections import Counter count = Counter(lst) most_common = count.most_common() max_freq = most_common[0][1] most_frequent_elements = [item for item, freq in most_common if freq == max_freq] return min(most_frequent_elements), max_freq"},{"question":"# Advanced Usage of Collections Module - Deques You have been provided with a list of operations that need to be performed on a double-ended queue (deque). Your task is to implement a function that processes this list of operations and returns the state of the deque after performing all operations in the given sequence. The operations can be: - `\'append x\'`: Append element `x` to the right end of the deque. - `\'appendleft x\'`: Append element `x` to the left end of the deque. - `\'pop\'`: Pop an element from the right end of the deque. - `\'popleft\'`: Pop an element from the left end of the deque. **Function Signature** ```python from collections import deque from typing import List def process_deque_operations(operations: List[str]) -> deque: pass ``` **Input** - `operations`: A list of strings where each string represents an operation as described above. (1 <= len(operations) <= 10^5) **Output** - The function should return a `deque` representing the state of the deque after performing all the operations. **Example** ```python operations = [\\"append 1\\", \\"append 2\\", \\"appendleft 3\\", \\"pop\\", \\"popleft\\"] result = process_deque_operations(operations) print(result) ``` **Output** ``` deque([]) ``` **Constraints** - All elements appended to the deque will be integers. - The operations `pop` and `popleft` will only be called on a non-empty deque. # Requirements: - Efficiently handle up to 10^5 operations. - Utilize the deque methods provided by the `collections` module appropriately. # Note: Make sure to consider both performance and correctness in your implementation. Any inefficiencies in handling operations could lead to timeout errors.","solution":"from collections import deque from typing import List def process_deque_operations(operations: List[str]) -> deque: dq = deque() for operation in operations: if operation.startswith(\'append \'): _, value = operation.split() dq.append(int(value)) elif operation.startswith(\'appendleft \'): _, value = operation.split() dq.appendleft(int(value)) elif operation == \'pop\': dq.pop() elif operation == \'popleft\': dq.popleft() return dq"},{"question":"# Objective Demonstrate your understanding of the `html` module by creating functions that process HTML content using the utilities provided by this module. # Question You are given HTML content as a string that contains multiple HTML entities and tags. Your task is to write two separate functions: 1. `escape_html(input_string: str, quote: bool = True) -> str` - This function should take an input string and return the HTML-escaped version of that string, using the `html.escape` function. - **Input**: A string `input_string` containing HTML content, and an optional boolean `quote` parameter (default is `True`). - **Output**: A string with the special characters escaped for safe inclusion in HTML. 2. `unescape_html(input_string: str) -> str` - This function should take an input string and return the unescaped version, using the `html.unescape` function. - **Input**: A string `input_string` containing HTML content with character references. - **Output**: A string with the character references converted to their corresponding Unicode characters. # Constraints - The input strings for both functions will not exceed 10,000 characters. - The input `input_string` for `unescape_html` will contain only valid HTML 5 named or numeric character references. # Examples **Example for `escape_html` function:** ```python input_string = \'<div class=\\"content\\">Hello & Welcome!</div>\' quote = True print(escape_html(input_string, quote)) ``` **Output:** ```python \'&lt;div class=&quot;content&quot;&gt;Hello &amp; Welcome!&lt;/div&gt;\' ``` **Example for `unescape_html` function:** ```python input_string = \'&lt;div class=&quot;content&quot;&gt;Hello &amp; Welcome!&lt;/div&gt;\' print(unescape_html(input_string)) ``` **Output:** ```python \'<div class=\\"content\\">Hello & Welcome!</div>\' ``` # Implementation ```python import html def escape_html(input_string: str, quote: bool = True) -> str: return html.escape(input_string, quote) def unescape_html(input_string: str) -> str: return html.unescape(input_string) # Example usage: input_string = \'<div class=\\"content\\">Hello & Welcome!</div>\' print(escape_html(input_string, True)) # Should print the escaped HTML string input_string = \'&lt;div class=&quot;content&quot;&gt;Hello &amp; Welcome!&lt;/div&gt;\' print(unescape_html(input_string)) # Should print the unescaped HTML string ``` # Notes - Make sure to handle edge cases such as empty strings. - Test your functions with a variety of inputs to ensure accuracy and robustness.","solution":"import html def escape_html(input_string: str, quote: bool = True) -> str: Return the HTML-escaped version of the input string. :param input_string: The string to be escaped. :param quote: Boolean indicating whether to escape quotes. :return: HTML-escaped version of the input string. return html.escape(input_string, quote) def unescape_html(input_string: str) -> str: Return the unescaped version of the input string. :param input_string: The string to be unescaped. :return: Unescaped version of the input string. return html.unescape(input_string)"},{"question":"**Question: Advanced Topological Sort with Dependency Validation** You are tasked with creating a utility that uses the `graphlib.TopologicalSorter` to manage and sort tasks based on their dependencies. Additionally, you need to ensure that if any cyclic dependencies are detected within the tasks, appropriate error handling is performed to report the detected cycle. Your task is to implement a function `topological_sort(tasks: Dict[Hashable, Iterable[Hashable]]) -> List[Hashable]`. # Function Specification - **Input**: - `tasks` (Dict[Hashable, Iterable[Hashable]]): A dictionary where keys represent tasks, and values are iterables of tasks that must be completed before the key task. - **Output**: - Returns a list of tasks sorted in topological order if there are no cycles. - **Exceptions**: - If any cycle is detected, raise the `CycleError` from the `graphlib` module and include the detected cycle in the error message. # Example ```python from graphlib import CycleError from typing import Dict, Hashable, Iterable, List def topological_sort(tasks: Dict[Hashable, Iterable[Hashable]]) -> List[Hashable]: # Your implementation here # Example usage: try: result = topological_sort({\\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}}) print(result) # Output could be [\'A\', \'C\', \'B\', \'D\'] except CycleError as e: print(f\\"Cycle detected: {e.args[1]}\\") try: result = topological_sort({\\"D\\": {\\"B\\"}, \\"B\\": {\\"D\\"}}) except CycleError as e: print(f\\"Cycle detected: {e.args[1]}\\") # Example output: Cycle detected: [\'D\', \'B\', \'D\'] ``` # Constraints - The graph provided in `tasks` will have nodes that are hashable. - Handle graphs with up to (10^4) nodes and (10^5) edges efficiently. - Use the `graphlib.TopologicalSorter` class for your implementation. **Ensure your implementation validates and handles cyclic dependencies as described, making use of the relevant methods and properties of `TopologicalSorter`.**","solution":"from graphlib import TopologicalSorter, CycleError from typing import Dict, Hashable, Iterable, List def topological_sort(tasks: Dict[Hashable, Iterable[Hashable]]) -> List[Hashable]: ts = TopologicalSorter(tasks) try: sorted_tasks = list(ts.static_order()) return sorted_tasks except CycleError as e: raise CycleError(f\\"Cycle detected: {e.args[1]}\\")"},{"question":"# **Seaborn Histogram Customization Challenge** You are given a dataset of planetary data with various attributes. Your task is to visualize this data using seaborn\'s `histplot` function to create a detailed histogram plot that includes multiple customizations. **Dataset** The dataset is loaded from seaborn and consists of information about planets discovered by different methods. ```python import seaborn as sns planets = sns.load_dataset(\\"planets\\") ``` **Requirements** 1. **Bivariate Histogram** - Create a bivariate histogram to visualize the distribution of `method` vs. `distance`. - Use `log_scale` for the `distance` axis. 2. **Hue Mapping** - Add `hue` mapping for the `method` column. - Use a divergent colormap scheme. 3. **Normalization and Density** - Normalize the histogram by setting `stat` to `density`. - Make sure the individual levels of the `hue` variable are normalized independently (`common_norm=False`). 4. **Appearance Customization** - Set `element` to `step` to visualize the histogram with step elements. - Ensure the histogram appears unfilled. 5. **Color Bar Annotation** - Add a color bar to the right of the plot. - Adjust its size to fit the plot appropriately by using `cbar_kws`. 6. **Additional Settings** - Add meaningful labels to the x and y axes. - Set a title for the plot. **Constraints** - The solution should be implemented in Python using seaborn and matplotlib. - Ensure the code runs without errors and displays the plot correctly. **Expected Output** Your code should output a bivariate histogram plot with the following characteristics: - Log-scaled distance axis. - Hue-mapped by `method`, using a divergent color scheme. - Normalized independently for each `method`. - Step elements, unfilled. - Includes a color bar. - Properly labeled axes and title. **Example Code Structure** ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset planets = sns.load_dataset(\\"planets\\") # Create the histogram plot sns.histplot( data=planets, x=\\"distance\\", y=\\"method\\", hue=\\"method\\", log_scale=(True, False), element=\\"step\\", fill=False, stat=\\"density\\", common_norm=False, cbar=True, cbar_kws={\'shrink\': 0.75} # Customize the color bar ) # Customize the plot plt.xlabel(\\"Distance (log scale)\\") plt.ylabel(\\"Discovery Method\\") plt.title(\\"Bivariate Distribution of Planet Discoveries\\") # Display the plot plt.show() ``` Provide your detailed solution following the structure and requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_planetary_histogram(): This function generates a bivariate histogram based on the planetary data. The histogram visualizes the distribution of discovery methods vs distance of planets. # Load dataset planets = sns.load_dataset(\\"planets\\") # Create the histogram plot sns.histplot( data=planets, x=\\"distance\\", y=\\"method\\", hue=\\"method\\", log_scale=(True, False), element=\\"step\\", fill=False, stat=\\"density\\", common_norm=False, cbar=True, cbar_kws={\'shrink\': 0.75} # Customize the color bar ) # Customize the plot plt.xlabel(\\"Distance (log scale)\\") plt.ylabel(\\"Discovery Method\\") plt.title(\\"Bivariate Distribution of Planet Discoveries\\") # Display the plot plt.show()"},{"question":"**Question: Implement and Evaluate an SVM Classifier** You are provided with a dataset containing features of various samples. Your task is to implement and evaluate an SVM classifier using scikit-learn\'s `svm` module. Follow the steps below to complete the task: 1. **Data Preparation:** - Load the dataset. (For this task, you can use any publicly available dataset or create a synthetic one using scikit-learn\'s `make_classification` method). - Preprocess the data (e.g., normalization, splitting into training and test sets). 2. **Model Implementation:** - Initialize an SVM classifier using `SVC` from the `sklearn.svm` module. - Train the classifier with the training data. 3. **Model Evaluation:** - Predict the labels for the test data. - Calculate and print the accuracy of the model. - Print the support vectors, indices of support vectors, and the number of support vectors for each class. 4. **Hyperparameter Tuning:** - Perform hyperparameter tuning using a cross-validation approach (e.g., GridSearchCV). - Find the best parameters for the SVM classifier. - Re-train the classifier with the best parameters and evaluate its performance. **Input Format:** - The dataset can be any dataset of your choice, with features `X` and labels `y`. **Output:** - Accuracy of the model on the test set. - Support vectors, indices of support vectors, and number of support vectors for each class. - Best parameters found through hyperparameter tuning. - Accuracy of the model with the best parameters. **Constraints:** - Use a random state for reproducibility where applicable. - Ensure the data is scaled or normalized appropriately. **Example:** ```python from sklearn import datasets, svm from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline # Load dataset X, y = datasets.load_iris(return_X_y=True) # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create a pipeline with StandardScaler and SVC pipeline = make_pipeline(StandardScaler(), svm.SVC()) # Train the model pipeline.fit(X_train, y_train) # Predict and evaluate accuracy = pipeline.score(X_test, y_test) print(f\'Accuracy: {accuracy}\') # Extract support vectors and related information svc = pipeline.named_steps[\'svc\'] print(f\'Support Vectors:n{svc.support_vectors_}\') print(f\'Indices of Support Vectors: {svc.support_}\') print(f\'Number of Support Vectors for each class: {svc.n_support_}\') # Hyperparameter tuning param_grid = {\'svc__C\': [0.1, 1, 10], \'svc__gamma\': [1, 0.1, 0.01]} grid = GridSearchCV(pipeline, param_grid, cv=5) grid.fit(X_train, y_train) # Best parameters and evaluation best_params = grid.best_params_ print(f\'Best Parameters: {best_params}\') # Evaluate the best model accuracy_best = grid.score(X_test, y_test) print(f\'Accuracy with Best Parameters: {accuracy_best}\') ``` **Note:** The above example uses the Iris dataset for demonstration purposes. You can use any dataset suitable for SVM classification.","solution":"from sklearn import datasets, svm from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline def svm_classifier_evaluation(): # Load dataset (using Iris dataset for this example) X, y = datasets.load_iris(return_X_y=True) # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Create a pipeline with StandardScaler and SVC pipeline = make_pipeline(StandardScaler(), svm.SVC()) # Train the model pipeline.fit(X_train, y_train) # Predict and evaluate accuracy = pipeline.score(X_test, y_test) print(f\'Accuracy: {accuracy}\') # Extract support vectors and related information svc = pipeline.named_steps[\'svc\'] support_vectors = svc.support_vectors_ support_indices = svc.support_ support_counts = svc.n_support_ print(f\'Support Vectors:n{support_vectors}\') print(f\'Indices of Support Vectors: {support_indices}\') print(f\'Number of Support Vectors for each class: {support_counts}\') # Hyperparameter tuning param_grid = {\'svc__C\': [0.1, 1, 10], \'svc__gamma\': [1, 0.1, 0.01]} grid = GridSearchCV(pipeline, param_grid, cv=5) grid.fit(X_train, y_train) # Best parameters and evaluation best_params = grid.best_params_ print(f\'Best Parameters: {best_params}\') # Evaluate the best model accuracy_best = grid.score(X_test, y_test) print(f\'Accuracy with Best Parameters: {accuracy_best}\') return { \\"accuracy_initial\\": accuracy, \\"support_vectors\\": support_vectors, \\"support_indices\\": support_indices, \\"support_counts\\": support_counts, \\"best_params\\": best_params, \\"accuracy_best\\": accuracy_best }"},{"question":"# Question: Implement a Custom Generic Alias in Python Background In advanced type hinting and generic programming, Python provides mechanisms to handle generic types using aliases. The `GenericAlias` allows the creation of parameterized types which can be used to define more flexible and reusable code structures. Task Implement a function that mimics the behavior of `Py_GenericAlias` in Python. This function should create a custom `GenericAlias` object with `__origin__` and `__args__` attributes. Additionally, define a `type_check` function to validate that certain instances match the generic alias types. Requirements 1. **Function: `create_generic_alias(origin: type, *args: Tuple[type, ...]) -> dict`** - **Input:** - `origin`: The original type that the alias is being created for (e.g., `list`, `dict`). - `args`: A tuple of types representing the type parameters for the generic alias. - **Output:** A dictionary representing the `GenericAlias` object with the following keys: - `__origin__`: The `origin` type. - `__args__`: A tuple of types specified in `args`. 2. **Function: `type_check(alias: dict, instance: Any) -> bool`** - **Input:** - `alias`: A dictionary representing the `GenericAlias` object. - `instance`: An instance of any type. - **Output:** `True` if `instance` matches the type defined by `alias`, otherwise `False`. Example ```python # Create a generic alias for a list of integers generic_list_int = create_generic_alias(list, int) print(generic_list_int) # Output: {\'__origin__\': <class \'list\'>, \'__args__\': (<class \'int\'>,)} # Type checking print(type_check(generic_list_int, [1, 2, 3])) # Output: True print(type_check(generic_list_int, [1, \'two\', 3])) # Output: False # Create a generic alias for a dictionary with integer keys and string values generic_dict_int_str = create_generic_alias(dict, int, str) print(generic_dict_int_str) # Output: {\'__origin__\': <class \'dict\'>, \'__args__\': (<class \'int\'>, <class \'str\'>)} # Type checking print(type_check(generic_dict_int_str, {1: \\"one\\", 2: \\"two\\"})) # Output: True print(type_check(generic_dict_int_str, {1: \\"one\\", \\"two\\": 2})) # Output: False ``` Constraints - Assume only built-in types like `list`, `dict`, `int`, `str`, etc., will be used. - The type check implementation should work for lists and dictionaries but does not need to support more complex nested structures. - Focus on type correctness and basic validation. Notes - This exercise aims to test your understanding of type hinting, generics, and custom type validation in Python. - Ensure your functions handle the provided inputs gracefully and include appropriate error handling where necessary.","solution":"from typing import Any, Tuple def create_generic_alias(origin: type, *args: Tuple[type, ...]) -> dict: Creates a custom GenericAlias object. Args: - origin (type): The original type (e.g., list, dict). - args (Tuple[type, ...]): The type parameters for the generic alias. Returns: - dict: A dictionary representing the GenericAlias object with \'__origin__\' and \'__args__\'. return {\'__origin__\': origin, \'__args__\': args} def type_check(alias: dict, instance: Any) -> bool: Checks if an instance matches the type defined by the alias. Args: - alias (dict): A dictionary representing the GenericAlias object. - instance (Any): An instance of any type. Returns: - bool: True if the instance matches the type defined by alias, otherwise False. origin = alias[\'__origin__\'] args = alias[\'__args__\'] if origin is list: if not isinstance(instance, list): return False return all(isinstance(item, args[0]) for item in instance) if origin is dict: if not isinstance(instance, dict): return False keytype, valuetype = args return all(isinstance(k, keytype) and isinstance(v, valuetype) for k, v in instance.items()) # Add more origins if necessary return False"},{"question":"**Custom Python Interpreter with Command Logging** Your task is to create a custom Python interpreter with command logging functionality using the `code` and `codeop` modules. The interpreter should keep track of all commands (code) executed and have the ability to print out the command history when requested. # Requirements: 1. Implement a class `LoggingInterpreter` that extends the `code.InteractiveInterpreter` class. 2. Your `LoggingInterpreter` should: - Initialize with an empty command history. - Override the `runsource` method to log each command before executing it. 3. Implement a method `print_history()` in your `LoggingInterpreter` class to print all the commands that have been executed so far. 4. Implement a method `reset_history()` that clears the command history. # Class Signature ```python class LoggingInterpreter(code.InteractiveInterpreter): def __init__(self, locals=None): # Initialize command history and call superclass initializer pass def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): # Log the command and execute it pass def print_history(self): # Print all commands in command history pass def reset_history(self): # Clear the command history pass ``` # Example Usage ```python interp = LoggingInterpreter() # Run some code interp.runsource(\'x = 5\') interp.runsource(\'print(x)\') interp.runsource(\'y = 2 * x\') # Print history interp.print_history() # Reset history interp.reset_history() # Print history again (should be empty) interp.print_history() ``` # Expected Outputs 1. The `print_history()` method should output: ``` x = 5 print(x) y = 2 * x ``` 2. After calling `reset_history()`, the `print_history()` method should output nothing, indicating an empty history. Provide the implementation of the `LoggingInterpreter` class based on the given requirements.","solution":"import code class LoggingInterpreter(code.InteractiveInterpreter): def __init__(self, locals=None): super().__init__(locals) self.command_history = [] def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): self.command_history.append(source) return super().runsource(source, filename, symbol) def print_history(self): for command in self.command_history: print(command) def reset_history(self): self.command_history = []"},{"question":"**Objective:** You are required to implement a Python function using the `imaplib` module to search and retrieve emails containing a specific keyword in the subject line from an IMAP mail server. The retrieved emails should be printed in a readable format. **Function Signature:** ```python def fetch_emails_with_keyword(host: str, port: int, username: str, password: str, keyword: str) -> None: pass ``` **Input Parameters:** - `host` (str): The hostname of the IMAP server. - `port` (int): The port number of the IMAP server. Defaults should be `143` for IMAP4 and `993` for IMAP4_SSL. - `username` (str): The username to log in to the IMAP server. - `password` (str): The password to log in to the IMAP server. - `keyword` (str): The keyword to search for in the subject lines of the emails. **Constraints:** - Use `IMAP4_SSL` for secure connection. - The function should handle connection errors and authentication failures gracefully. - It should search the `INBOX` folder only. - Messages should be fetched and printed with their subject and body text. **Output:** - There is no return value expected. The email messages containing the keyword in their subject should be printed. **Example Usage:** ```python fetch_emails_with_keyword(\\"imap.gmail.com\\", 993, \\"your_username@gmail.com\\", \\"your_password\\", \\"Meeting\\") ``` **Performance Considerations:** - The function should efficiently handle large mailboxes containing thousands of emails. - Aim to minimize the number of interactions with the IMAP server to avoid latency issues. **Guidelines:** 1. **Establish Connection:** Use `IMAP4_SSL` to connect to the IMAP server with the given `host` and `port`. 2. **Authenticate:** Log in using the provided `username` and `password`. 3. **Select Mailbox:** Select the `INBOX`. 4. **Search Emails:** Search for emails with the specified keyword in the subject. 5. **Fetch and Print Emails:** Retrieve the emails and print their subject and body. 6. **Handle Exceptions:** Properly handle any errors or exceptions that might occur during the process. **Note:** - Use appropriate IMAP commands and methods provided by the `imaplib` module to accomplish each step. - Make sure to close the mailbox and log out properly to end the IMAP session cleanly.","solution":"import imaplib import email from email.header import decode_header def fetch_emails_with_keyword(host: str, port: int, username: str, password: str, keyword: str) -> None: try: # Connect to the server mail = imaplib.IMAP4_SSL(host, port) # Login to the account mail.login(username, password) # Select the mailbox you want to search mail.select(\\"inbox\\") # Search for emails with the given keyword in the subject status, messages = mail.search(None, \'(SUBJECT \\"{}\\")\'.format(keyword)) if status != \\"OK\\": print(\\"No messages found!\\") return # Fetch each email for num in messages[0].split(): status, msg_data = mail.fetch(num, \\"(RFC822)\\") if status != \\"OK\\": print(f\\"Error fetching message {num}\\") continue for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding if encoding else \\"utf-8\\") # Print the subject print(\\"Subject:\\", subject) # Get the email body if msg.is_multipart(): for part in msg.walk(): content_type = part.get_content_type() content_disposition = str(part.get(\\"Content-Disposition\\")) if \\"attachment\\" in content_disposition: continue if content_type == \\"text/plain\\": print(part.get_payload(decode=True).decode(\\"utf-8\\")) else: print(msg.get_payload(decode=True).decode(\\"utf-8\\")) # Close the connection and logout mail.close() mail.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP4 error: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Question: Type Information and Validation in PyTorch Using PyTorch, implement a function that validates whether the given tensors\' values are within the valid range of their respective data types. The function should also return a summary of the numerical properties for each tensor data type. Function Signature ```python import torch from typing import List, Tuple, Dict def validate_tensors(tensors: List[torch.Tensor]) -> Tuple[Dict[torch.dtype, dict], List[bool]]: Validate if the given list of tensors have values within their data types\' acceptable ranges. Parameters: tensors (List[torch.Tensor]): A list of PyTorch tensors of varying data types. Returns: Tuple[Dict[torch.dtype, dict], List[bool]]: - A dictionary where keys are tensor data types and values are dictionaries of numerical properties. - A list of booleans indicating whether each tensor has all values within the valid range for its data type. ``` Constraints - The function should handle both floating-point and integer tensors. - The summary dictionary should include the attributes available from `torch.finfo` for floating-point types and `torch.iinfo` for integer types as specified in the documentation. - You are NOT allowed to use any global variables. Example Usage ```python tensors = [ torch.tensor([0.1, 0.2, 0.3], dtype=torch.float32), torch.tensor([256, 512, -128], dtype=torch.int16), torch.tensor([1e-45, 3.5e38, 1.0], dtype=torch.float32) ] summary, validations = validate_tensors(tensors) print(\\"Summary:\\", summary) print(\\"Validations:\\", validations) ``` # Expected Output ``` Summary: { torch.float32: { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'smallest_normal\': 1.1754943508222875e-38, \'resolution\': 1e-07 }, torch.int16: { \'bits\': 16, \'max\': 32767, \'min\': -32768 } } Validations: [True, True, False] ``` Explanation - The function `validate_tensors` takes a list of tensors as input. - It returns a tuple containing: - A dictionary with detailed numerical properties for each data type encountered in the input tensors. - A list of booleans indicating if each tensor\'s values are within the valid range for its data type (based on the properties provided in the dictionary). Make sure to handle edge cases and consider performance aspects when dealing with large tensors.","solution":"import torch from typing import List, Tuple, Dict def validate_tensors(tensors: List[torch.Tensor]) -> Tuple[Dict[torch.dtype, dict], List[bool]]: Validate if the given list of tensors have values within their data types\' acceptable ranges. Parameters: tensors (List[torch.Tensor]): A list of PyTorch tensors of varying data types. Returns: Tuple[Dict[torch.dtype, dict], List[bool]]: - A dictionary where keys are tensor data types and values are dictionaries of numerical properties. - A list of booleans indicating whether each tensor has all values within the valid range for its data type. summary = {} validations = [] for tensor in tensors: dtype = tensor.dtype if dtype in summary: properties = summary[dtype] else: if dtype.is_floating_point: fi = torch.finfo(dtype) properties = { \'bits\': fi.bits, \'eps\': fi.eps, \'max\': fi.max, \'min\': fi.min, \'tiny\': fi.tiny, \'smallest_normal\': fi.smallest_normal, \'resolution\': fi.resolution } else: ii = torch.iinfo(dtype) properties = { \'bits\': ii.bits, \'max\': ii.max, \'min\': ii.min } summary[dtype] = properties is_valid = torch.all((tensor >= properties[\'min\']) & (tensor <= properties[\'max\'])) validations.append(is_valid.item()) return summary, validations"},{"question":"**Question: Color Conversion Tool** You are tasked with creating a color conversion tool using the `colorsys` module from Python 3.10 standard library. This tool should be able to convert a color given in the RGB (Red, Green, Blue) format to its equivalent in the HSV (Hue, Saturation, Value) format. * **Function Implementation:** ```python def rgb_to_hsv(r: int, g: int, b: int) -> tuple: Convert RGB color format to HSV color format. Args: r (int): Red component, should be in the range [0, 255] g (int): Green component, should be in the range [0, 255] b (int): Blue component, should be in the range [0, 255] Returns: tuple: A tuple containing three floats corresponding to the HSV values - hue (range 0-1), saturation (range 0-1), and value (range 0-1). pass ``` * **Input Constraints:** * The function will receive three integers (r, g, b). Each integer represents the intensity of the respective color channel and will be within the range `[0, 255]`. * **Output Format:** * The function should return a tuple containing three floats: `(hue, saturation, value)`. * **Example:** ```python # Example 1 assert rgb_to_hsv(255, 0, 0) == (0.0, 1.0, 1.0) # Example 2 assert rgb_to_hsv(0, 255, 0) == (0.3333333333333333, 1.0, 1.0) # Example 3 assert rgb_to_hsv(0, 0, 255) == (0.6666666666666666, 1.0, 1.0) ``` Notes: * To correctly convert RGB to HSV, you can utilize the `colorsys.rgb_to_hsv` function from the `colorsys` module. * Ensure to handle the conversion from the range `[0, 255]` in RGB to `[0, 1]` in HSV as required by the `colorsys.rgb_to_hsv` function.","solution":"import colorsys def rgb_to_hsv(r: int, g: int, b: int) -> tuple: Convert RGB color format to HSV color format. Args: r (int): Red component, should be in the range [0, 255] g (int): Green component, should be in the range [0, 255] b (int): Blue component, should be in the range [0, 255] Returns: tuple: A tuple containing three floats corresponding to the HSV values - hue (range 0-1), saturation (range 0-1), and value (range 0-1). # Normalize RGB values to the range 0-1 r_normalized = r / 255.0 g_normalized = g / 255.0 b_normalized = b / 255.0 # Use colorsys to convert to HSV h, s, v = colorsys.rgb_to_hsv(r_normalized, g_normalized, b_normalized) return (h, s, v)"},{"question":"Objective Implement a Python function that utilizes the `itertools`, `functools`, and `operator` modules to process a list of transactions. Each transaction is represented as a tuple `(type, amount)` where `type` is either `\\"income\\"` or `\\"expense\\"`, and `amount` is the integer value of the transaction. Requirements 1. **Input:** - A list of transactions `transactions`, where each transaction is a tuple `(\'type\', amount)`. ```python transactions = [ (\\"income\\", 1000), (\\"expense\\", 200), (\\"income\\", 2000), (\\"expense\\", 500), (\\"income\\", 3000), (\\"expense\\", 1000) ] ``` 2. **Output:** - A tuple `(total_income, total_expense, balance)`: - `total_income`: The sum of all income amounts. - `total_expense`: The sum of all expense amounts. - `balance`: The result of subtracting the total expense from the total income. 3. **Constraints:** - You must use functions from `itertools`, `functools`, and `operator` modules. - Avoid using explicit loops (for/while) to solve this problem. 4. **Performance:** - The function should be efficient with a time complexity of O(n), where n is the number of transactions. Function Signature ```python def calculate_finances(transactions: list) -> tuple: pass ``` Example Usage ```python transactions = [ (\\"income\\", 1000), (\\"expense\\", 200), (\\"income\\", 2000), (\\"expense\\", 500), (\\"income\\", 3000), (\\"expense\\", 1000) ] print(calculate_finances(transactions)) # Output should be (6000, 1700, 4300) ``` Use the tools from the specified modules to implement this function efficiently.","solution":"from itertools import filterfalse from functools import reduce from operator import add def calculate_finances(transactions): incomes = filter(lambda x: x[0] == \'income\', transactions) expenses = filter(lambda x: x[0] == \'expense\', transactions) total_income = reduce(add, map(lambda x: x[1], incomes), 0) total_expense = reduce(add, map(lambda x: x[1], expenses), 0) balance = total_income - total_expense return (total_income, total_expense, balance)"},{"question":"Objective: Design a function that simulates the slicing mechanism provided by Python\'s native slicing using the Python C API provided in the documentation. The function should be implemented in Python and should mimic the start, stop, and step slice behavior on a given list. Requirements: 1. Implement a function `custom_slice(sequence, start, stop, step)` that takes in: - `sequence` (list): Input list to be sliced. - `start` (int or None): Start index of the slice. - `stop` (int or None): Stop index of the slice. - `step` (int or None): Step value for the slice. 2. The function should return a new list that represents the sliced result of the input list using the provided start, stop, and step values. Constraints: - You must handle `None` values appropriately for `start`, `stop`, and `step`, similar to Python\'s native slicing behavior. - The slicing should handle out-of-bounds indices by clipping them to valid range limits (i.e., negative indices or indices greater than the sequence length). - No use of Python\'s native slicing (e.g., `sequence[start:stop:step]`) directly. You should manually compute the indices and build the resulting list. Example: ```python def custom_slice(sequence, start, stop, step): # Your implementation here pass # Example usage: sequence = [1, 2, 3, 4, 5] result = custom_slice(sequence, 1, 4, 1) print(result) # Output: [2, 3, 4] result = custom_slice(sequence, None, None, -1) print(result) # Output: [5, 4, 3, 2, 1] ``` Notes: - Consider edge cases such as empty lists, single-element lists, and step values of zero. - Make sure to avoid using the built-in slicing functionality directly; instead, compute and iterate over the indices to form the result.","solution":"def custom_slice(sequence, start, stop, step): Simulates the slicing mechanism provided by Python\'s native slicing. :param sequence: List of elements to be sliced. :param start: Start index of the slice. :param stop: Stop index of the slice. :param step: Step value for the slice. :return: A new list representing the sliced result. if step is None: step = 1 n = len(sequence) if step == 0: raise ValueError(\\"step cannot be zero\\") if start is None: start = 0 if step > 0 else n - 1 elif start < 0: start += n if stop is None: stop = n if step > 0 else -1 elif stop < 0: stop += n result = [] i = start if step > 0: while i < stop: if 0 <= i < n: result.append(sequence[i]) i += step else: while i > stop: if 0 <= i < n: result.append(sequence[i]) i += step return result"},{"question":"Problem Statement Implement a function `extract_and_validate_emails(text: str) -> List[str]` that extracts email addresses from a given text and validates them based on specific criteria using regular expressions. # Function Signature ```python def extract_and_validate_emails(text: str) -> List[str]: ``` # Input - `text` (str): A string containing potential email addresses mixed with other text. # Output - List[str]: A list of valid email addresses extracted from the input text. # Constraints 1. The local part (before the @) of the email address must: - Start with a letter. - Be followed by letters, digits, dots, underscores, or hyphens. 2. The domain part (after the @) must: - Only contain letters, digits, and hyphens. - Have at least one dot separating the main domain and top-level domain (TLD). 3. The TLD (e.g., .com, .org) must: - Be between 2 to 6 characters long, consisting only of letters. # Example ```python text = \\"Please contact us at support@example.com or admin@mail.example-domain.org for further information. Invalid emails: admin@mail..com, @example.com, username@.com, username@domain.c, username@domain.corpextension\\" result = extract_and_validate_emails(text) print(result) # Output: [\'support@example.com\', \'admin@mail.example-domain.org\'] ``` # Performance Requirements - The solution should efficiently handle large text inputs up to 1MB. Write code that implements the function `extract_and_validate_emails` according to the above specifications. Ensure to use the `re` module for handling regular expressions.","solution":"import re from typing import List def extract_and_validate_emails(text: str) -> List[str]: Extracts and validates email addresses from a given text. # Regular expression for finding potential emails in the text email_regex = r\'b[A-Za-z][A-Za-z0-9._-]*@[A-Za-z0-9.-]+.[A-Za-z]{2,6}b\' # Find all email matches potential_emails = re.findall(email_regex, text) # Filter emails based on additional criteria valid_emails = [] for email in potential_emails: local_part, domain = email.split(\'@\') domain_parts = domain.split(\'.\') if (len(domain_parts) >= 2 and all(re.match(r\'^[A-Za-z0-9-]+\', part) for part in domain_parts) and re.match(r\'^[A-Za-z0-9-]+\', domain_parts[-1]) and 2 <= len(domain_parts[-1]) <= 6): valid_emails.append(email) return valid_emails"},{"question":"# Question Write a Python script that demonstrates a custom import mechanism using meta path finders and import path hooks. Your script should: 1. Implement a custom meta path finder that checks if a module name starts with a specific prefix (e.g., `custom_`). If it does, the module should be loaded from a pre-defined directory regardless of where the import statement is executed. 2. Implement a custom path entry finder for a specific directory that loads Python source files (`.py`) as modules. 3. Ensure that the custom meta path finder and path entry finder are registered properly and demonstrate importing a module using this custom mechanism. Requirements: - The custom meta path finder should be able to intercept all imports and check the module name for the prefix. - The custom path entry finder should specifically look for `.py` files in the given directory and load them as modules. - Ensure that your custom logic handles cases where the module is not found and raises appropriate exceptions. Input: No direct input is required. You should define a directory structure and file with sample modules that your script can import using the custom logic. Output: The output should demonstrate the successful import and usage of a module using the custom import logic. Any errors or exceptions should be handled gracefully. # Constraints: - The custom import mechanism should work seamlessly with the existing import system. - You must not alter the `sys.meta_path` directly; instead, you should append your custom finder. # Example: Assume you have a directory structure as follows: ``` custom_modules/ mymodule.py ``` `mymodule.py` contains: ```python def greet(): return \\"Hello, Custom Import!\\" ``` Your script should be able to import `custom_mymodule` from `custom_modules`, and invoking `greet` should return `\\"Hello, Custom Import!\\"`. Performance Requirements: - Your custom import mechanism should not significantly delay standard imports. ```python # your code here ```","solution":"import sys import importlib.abc import importlib.util import os class CustomMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, prefix, base_dir): self.prefix = prefix self.base_dir = base_dir def find_spec(self, fullname, path, target=None): if not fullname.startswith(self.prefix): return None module_name = fullname[len(self.prefix):] module_path = os.path.join(self.base_dir, *module_name.split(\'.\')) + \'.py\' if not os.path.exists(module_path): return None spec = importlib.util.spec_from_file_location(fullname, module_path) return spec class CustomPathEntryFinder(importlib.abc.PathEntryFinder): def __init__(self, base_dir): self.base_dir = base_dir def find_spec(self, fullname, target=None): module_path = os.path.join(self.base_dir, *fullname.split(\'.\')) + \'.py\' if not os.path.exists(module_path): return None spec = importlib.util.spec_from_file_location(fullname, module_path) return spec def register_custom_importers(prefix, base_dir): sys.meta_path.append(CustomMetaPathFinder(prefix, base_dir)) sys.path_hooks.append(lambda entry: CustomPathEntryFinder(base_dir) if entry == base_dir else None) sys.path_importer_cache.clear() # Register custom importers custom_modules_dir = os.path.join(os.getcwd(), \'custom_modules\') register_custom_importers(\'custom_\', custom_modules_dir) # Example usage try: import custom_mymodule print(custom_mymodule.greet()) except ImportError as e: print(f\\"ImportError: {e}\\")"},{"question":"# Coding Challenge: Implement Custom Network Utility Functions Using the Python \\"ipaddress\\" module, create a set of utility functions that will help in validating and manipulating network data. This exercise will test your ability to work with IP addresses, networks, and interfaces as described in the provided documentation. Functions to Implement 1. **validate_ip(ip_str)**: - **Input**: A string representing an IP address. - **Output**: Return a tuple (boolean, string) where the boolean indicates whether the IP address is valid, and the string indicates the version (\'IPv4\' or \'IPv6\'). - **Example**: ```python validate_ip(\'192.0.2.1\') # Output: (True, \'IPv4\') validate_ip(\'2001:db8::1\') # Output: (True, \'IPv6\') validate_ip(\'999.999.999.999\') # Output: (False, \'\') ``` 2. **network_summary(network_str)**: - **Input**: A string representing a network, e.g., \'192.0.2.0/24\' or \'2001:db8::/96\'. - **Output**: A dictionary with the following information: - \'total_addresses\': Total number of addresses in the network. - \'netmask\': The network mask of the network. - \'broadcast_address\': The last address in the network. - **Example**: ```python network_summary(\'192.0.2.0/24\') # Output: { # \'total_addresses\': 256, # \'netmask\': \'255.255.255.0\', # \'broadcast_address\': \'192.0.2.255\' # } ``` 3. **find_network_range(network_str)**: - **Input**: A string representing a network, e.g., \'192.0.2.0/24\' or \'2001:db8::/96\'. - **Output**: A tuple (first_address, last_address) representing the first and last usable IP addresses in the given network. - **Example**: ```python find_network_range(\'192.0.2.0/24\') # Output: (\'192.0.2.1\', \'192.0.2.254\') ``` Constraints - Do not use any third-party libraries other than the Python \\"ipaddress\\" module. - Ensure your functions handle both IPv4 and IPv6 addresses where applicable. - Validate inputs appropriately, and raise meaningful error messages if the inputs are invalid. Performance Requirements - Ensure that your functions handle typical input sizes efficiently. - Avoid unnecessary computations and leverage the capabilities provided by the \\"ipaddress\\" module. Test Cases Your implementation will be tested with a variety of test cases to ensure correctness, including but not limited to: - Valid and invalid IP addresses and networks. - Different IP versions (IPv4 and IPv6). - Edge cases such as smallest and largest possible addresses within a given network. Good luck, and happy coding!","solution":"import ipaddress def validate_ip(ip_str): Validates an IP address and returns a tuple indicating if it\'s valid and its version. try: ip = ipaddress.ip_address(ip_str) ip_version = \'IPv4\' if ip.version == 4 else \'IPv6\' return (True, ip_version) except ValueError: return (False, \'\') def network_summary(network_str): Returns a summary of a network including total addresses, netmask, and broadcast address. try: network = ipaddress.ip_network(network_str, strict=False) summary = { \'total_addresses\': network.num_addresses, \'netmask\': str(network.netmask), \'broadcast_address\': str(network.broadcast_address) } return summary except ValueError: raise ValueError(\\"Invalid network address.\\") def find_network_range(network_str): Returns the first and last usable IP addresses in a given network. try: network = ipaddress.ip_network(network_str, strict=False) first_address = str(network[1]) if network.num_addresses > 1 else str(network.network_address) last_address = str(network[-2]) if network.num_addresses > 2 else str(network.broadcast_address) return (first_address, last_address) except ValueError: raise ValueError(\\"Invalid network address.\\")"},{"question":"Problem Statement: You are tasked with building a monitoring system for a Python application using the `faulthandler` module. The system needs to log tracebacks in case of crashes or hangs, and it should be able to schedule periodic traceback dumps for long-running tasks. Requirements: 1. **Initialization**: - Write a function `initialize_fault_handler(log_file_path: str, all_threads: bool = True) -> None` that enables the fault handler. - Parameters: - `log_file_path`: Path to the file where tracebacks will be logged. - `all_threads`: Boolean flag to indicate if tracebacks should be for all threads (default is `True`). 2. **Scheduled Dumps**: - Write a function `schedule_traceback_dumps(timeout: int, repeat: bool = False, exit_after_dump: bool = False) -> None` that schedules periodic traceback dumps. - Parameters: - `timeout`: Timeout in seconds after which to dump tracebacks. - `repeat`: Boolean flag to indicate if the dumps should repeat every `timeout` seconds (default is `False`). - `exit_after_dump`: Boolean flag to exit the program after dumping tracebacks (default is `False`). 3. **User Signal Dumps**: - Write a function `register_signal_for_traceback(signal_number: int, all_threads: bool = True, chain_existing: bool = False) -> None` that registers a signal to trigger a traceback dump. - Parameters: - `signal_number`: Signal number to register for triggering traceback dump. - `all_threads`: Boolean flag to indicate if tracebacks should be for all threads (default is `True`). - `chain_existing`: Boolean flag to chain to existing signal handlers (default is `False`). 4. **Disabling Fault Handler**: - Write a function `disable_fault_handler() -> None` that disables the fault handler. Constraints: - The `log_file_path` must be a valid path, and appropriate error handling should be done for invalid paths. - Ensure the file is kept open as required by the `faulthandler` module\'s file descriptor use. Example Usage: ```python # Initialize fault handler: initialize_fault_handler(\\"/path/to/logfile.log\\") # Schedule periodic dumps: schedule_traceback_dumps(60, repeat=True) # Register signal for dumping tracebacks: register_signal_for_traceback(signal.SIGUSR1) # Later, you can disable the fault handler: disable_fault_handler() ``` Expected Output: There is no direct output expected from the functions, but the tracebacks should be logged in the file specified. Ensure thorough testing by inducing faults, such as segmentation faults, to see if the handler works as expected. Additional Notes: - Consider how you would test these functions in a controlled environment, such as using signal simulations and timeouts. - Handle any potential exceptions that could occur when dealing with files and signals.","solution":"import faulthandler import signal import sys def initialize_fault_handler(log_file_path: str, all_threads: bool = True) -> None: Initializes the fault handler to log tracebacks. Args: log_file_path (str): Path to the file where tracebacks will be logged. all_threads (bool): Whether to log tracebacks for all threads. Default is True. try: with open(log_file_path, \'w\') as log_file: faulthandler.enable(file=log_file, all_threads=all_threads) except (OSError, IOError) as e: print(f\\"Error opening log file {log_file_path}: {e}\\", file=sys.stderr) raise def schedule_traceback_dumps(timeout: int, repeat: bool = False, exit_after_dump: bool = False) -> None: Schedules periodic traceback dumps. Args: timeout (int): Timeout in seconds after which to dump tracebacks. repeat (bool): Whether to repeat the dumps every `timeout` seconds. Default is False. exit_after_dump (bool): Whether to exit the program after dumping tracebacks. Default is False. faulthandler.dump_traceback_later(timeout, repeat=repeat, exit=exit_after_dump) def register_signal_for_traceback(signal_number: int, all_threads: bool = True, chain_existing: bool = False) -> None: Registers a signal to trigger a traceback dump. Args: signal_number (int): Signal number to register for triggering traceback dump. all_threads (bool): Whether to log tracebacks for all threads. Default is True. chain_existing (bool): Whether to chain to existing signal handlers. Default is False. faulthandler.register(signal_number, all_threads=all_threads, chain=chain_existing) def disable_fault_handler() -> None: Disables the fault handler. faulthandler.disable()"},{"question":"Objective Implement a machine learning pipeline using Scikit-learn that demonstrates the usage of kernel approximation methods and compares their performance against exact kernel methods. Task 1. Load a sample dataset (e.g., the digits dataset) from `sklearn.datasets`. 2. Split the dataset into training and testing sets. 3. Implement a support vector machine (SVM) classifier using the exact RBF kernel with the training data. 4. Using the Nystroem method from `sklearn.kernel_approximation`, transform the feature space of the training and testing data. 5. Implement a linear SVM classifier using the transformed data. 6. Evaluate and compare the performance (using accuracy) of both models on the testing set. Requirements - **Input and Output Formats:** - There are no specific inputs to be provided manually. You will load the dataset internally. - The output should be the accuracy scores of both models formatted as follows: ``` Exact RBF Kernel SVM accuracy: <accuracy_value> Nystroem Approximation SVM accuracy: <accuracy_value> ``` - **Constraints and Limitations:** - You must use Scikit-learn\'s built-in datasets for consistency. - Use `random_state=42` for reproducibility in your data splitting and any other random operations. - Ensure to appropriately preprocess the data as required for training the SVM and kernel approximation methods. - **Performance Requirements:** - Your solution should handle the dataset efficiently without causing significant memory or computational overhead. Solution Template ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score # Load dataset digits = datasets.load_digits() X, y = digits.data, digits.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Preprocess data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Exact RBF Kernel SVM exact_rbf_svm = SVC(kernel=\'rbf\', gamma=0.1, random_state=42) exact_rbf_svm.fit(X_train_scaled, y_train) exact_rbf_pred = exact_rbf_svm.predict(X_test_scaled) exact_rbf_accuracy = accuracy_score(y_test, exact_rbf_pred) # Kernel Approximation with Nystroem Method and Linear SVM nystroem = Nystroem(kernel=\'rbf\', gamma=0.1, n_components=100, random_state=42) X_train_transformed = nystroem.fit_transform(X_train_scaled) X_test_transformed = nystroem.transform(X_test_scaled) linear_svm = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) linear_svm.fit(X_train_transformed, y_train) nystroem_pred = linear_svm.predict(X_test_transformed) nystroem_accuracy = accuracy_score(y_test, nystroem_pred) # Output results print(f\'Exact RBF Kernel SVM accuracy: {exact_rbf_accuracy}\') print(f\'Nystroem Approximation SVM accuracy: {nystroem_accuracy}\') ```","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def svm_kernel_approximation(): # Load dataset digits = datasets.load_digits() X, y = digits.data, digits.target # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Preprocess data scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Exact RBF Kernel SVM exact_rbf_svm = SVC(kernel=\'rbf\', gamma=0.1, random_state=42) exact_rbf_svm.fit(X_train_scaled, y_train) exact_rbf_pred = exact_rbf_svm.predict(X_test_scaled) exact_rbf_accuracy = accuracy_score(y_test, exact_rbf_pred) # Kernel Approximation with Nystroem Method and Linear SVM nystroem = Nystroem(kernel=\'rbf\', gamma=0.1, n_components=100, random_state=42) X_train_transformed = nystroem.fit_transform(X_train_scaled) X_test_transformed = nystroem.transform(X_test_scaled) linear_svm = SGDClassifier(max_iter=1000, tol=1e-3, random_state=42) linear_svm.fit(X_train_transformed, y_train) nystroem_pred = linear_svm.predict(X_test_transformed) nystroem_accuracy = accuracy_score(y_test, nystroem_pred) return exact_rbf_accuracy, nystroem_accuracy if __name__ == \\"__main__\\": exact_rbf_accuracy, nystroem_accuracy = svm_kernel_approximation() print(f\\"Exact RBF Kernel SVM accuracy: {exact_rbf_accuracy}\\") print(f\\"Nystroem Approximation SVM accuracy: {nystroem_accuracy}\\")"},{"question":"Coding Assessment Question # Objective The goal of this assessment is to test your understanding of basic and intermediate Python concepts, including arithmetic operations, string manipulation, list operations, and control flow mechanisms. # Task You are to implement a function `generate_pattern(n: int) -> List[str]` that takes an integer `n` as input and returns a list of strings representing a specific pattern. The pattern to be generated is as follows: 1. For each integer `i` from 1 to `n` (inclusive): - Create a string of `i` numbers in increasing order. - Create a string of `i` numbers in decreasing order. - Concatenate these two strings with a space separating them. 2. Return a list where each element is one of these concatenated strings. # Example For `n = 3`, the function should return: ```python generate_pattern(3) ``` Output: ```python [ \\"1 1\\", \\"1 2 2 1\\", \\"1 2 3 3 2 1\\" ] ``` # Constraints - The value of `n` will be a positive integer in the range `[1, 100]`. # Implementation Details - Use basic list operations to construct each line of the pattern. - Use string manipulation to construct the formatted strings. - Ensure your code is efficient and handles the range of input sizes within acceptable time limits. You are expected to use fundamental Python features such as loops, list operations, and basic string formatting to accomplish this task. Advanced library usage is discouraged to ensure the solution demonstrates clear understanding of fundamental Python principles. # Function Signature ```python def generate_pattern(n: int) -> List[str]: pass ``` # Notes - Think carefully about how to construct each part of the string pattern. - Test your function with various values of `n` to ensure it works correctly. - Consider edge cases such as `n = 1`. Good luck!","solution":"def generate_pattern(n: int) -> list: Generates a pattern of strings for the given integer n. Parameters: n (int): The number of lines in the pattern. Returns: list: A list of strings representing the pattern. result = [] for i in range(1, n + 1): increasing_part = \' \'.join(map(str, range(1, i + 1))) decreasing_part = \' \'.join(map(str, range(i, 0, -1))) pattern = f\\"{increasing_part} {decreasing_part}\\" result.append(pattern) return result"},{"question":"# **Coding Assessment Question** **Objective:** Implement a function that inspects a runtime object\'s attributes and logs specific details about it, including its class, type, and any warnings associated with it. This challenge requires understanding and leveraging the `inspect`, `warnings`, and `logging` modules. **Question:** You are required to write a function `inspect_and_log_object_details(obj: Any) -> None` that: 1. **Inspects** the given object `obj` to retrieve: - Its class name. - Its type. - A list of its attributes and their respective types. 2. **Generates** and **logs** a warning if any of the object\'s attributes are deprecated (you can assume this is represented as attributes starting with `_deprecated_`). 3. **Logs** the inspected details using the `logging` module. # **Specifications:** - **Input:** - `obj`: Any Python object (could be a custom class instance, built-in type, etc.) - **Output:** - The function should not return any value. Instead, it should log the necessary details and warnings. - **Constraints:** - Use the `inspect` module to retrieve the required details. - Use the `logging` module to log the details. - For logging levels, use `logging.INFO` for normal details and `logging.WARNING` for deprecated attributes. # **Performance Requirements:** The function should be efficient in processing and logging the details of the object without significant delays. ```python import inspect import logging import warnings def inspect_and_log_object_details(obj): # Setting up logging logging.basicConfig(level=logging.INFO) class_name = obj.__class__.__name__ obj_type = type(obj).__name__ logging.info(f\'Class Name: {class_name}\') logging.info(f\'Object Type: {obj_type}\') attributes = inspect.getmembers(obj, lambda a: not(inspect.isroutine(a))) for attr_name, attr_value in attributes: if not attr_name.startswith(\'_\'): logging.info(f\'Attribute: {attr_name}, Type: {type(attr_value).__name__}\') if attr_name.startswith(\'_deprecated_\'): logging.warning(f\'Deprecated Attribute: {attr_name}, Type: {type(attr_value).__name__}\') warnings.warn(f\'{attr_name} is deprecated.\', DeprecationWarning) ``` # **Example Usage:** ```python class SampleClass: def __init__(self): self.name = \'Sample\' self._deprecated_value = 42 self.count = 10 sample = SampleClass() inspect_and_log_object_details(sample) ``` # **Expected Log Output:** ``` INFO:root:Class Name: SampleClass INFO:root:Object Type: SampleClass INFO:root:Attribute: name, Type: str INFO:root:Attribute: count, Type: int WARNING:root:Deprecated Attribute: _deprecated_value, Type: int ```","solution":"import inspect import logging import warnings def inspect_and_log_object_details(obj): # Setting up logging logging.basicConfig(level=logging.INFO) class_name = obj.__class__.__name__ obj_type = type(obj).__name__ logging.info(f\'Class Name: {class_name}\') logging.info(f\'Object Type: {obj_type}\') attributes = inspect.getmembers(obj, lambda a: not(inspect.isroutine(a))) for attr_name, attr_value in attributes: if not attr_name.startswith(\'_\'): logging.info(f\'Attribute: {attr_name}, Type: {type(attr_value).__name__}\') if attr_name.startswith(\'_deprecated_\'): logging.warning(f\'Deprecated Attribute: {attr_name}, Type: {type(attr_value).__name__}\') warnings.warn(f\'{attr_name} is deprecated.\', DeprecationWarning)"},{"question":"**Memory Management in Python - Custom Allocators** **Objective:** Demonstrate your understanding of memory management in Python by implementing a set of functions to manage memory using Python\'s memory allocators. **Task:** 1. **Create Custom Memory Allocator Functions**: - Implement custom allocator functions that interact with Python’s memory management system, particularly using the three allocation domains: Raw, Mem, and Object. 2. **Track Allocations**: - Implement functionality to track allocated memory blocks within each domain, enabling the ability to print statistics such as total allocated memory, number of allocations, and number of frees. **Specifications:** 1. Implement a Python class `CustomMemoryAllocator` with the following methods: - `def __init__(self)`: Initialize any necessary properties. - `def raw_malloc(self, size: int) -> int`: Allocate raw memory of the requested size using the `PyMem_RawMalloc` method and return a pointer (mock integer). - `def raw_free(self, ptr: int) -> None`: Free raw memory allocated using the `PyMem_RawFree`. - `def mem_malloc(self, size: int) -> int`: Allocate memory using the `PyMem_Malloc` method and return a pointer (mock integer). - `def mem_free(self, ptr: int) -> None`: Free memory allocated using the `PyMem_Free`. - `def obj_malloc(self, size: int) -> int`: Allocate object memory using the `PyObject_Malloc` method and return a pointer (mock integer). - `def obj_free(self, ptr: int) -> None`: Free object memory using the `PyObject_Free`. - `def get_statistics(self) -> dict`: Return a dictionary containing the statistics of the memory allocations such as total allocated memory, number of allocations, and number of frees for each domain (keys: \'Raw\', \'Mem\', \'Object\'). 2. Implement a function `test_custom_allocator()` to test the `CustomMemoryAllocator` class: - Allocate and free memory using each type of allocator. - Print the statistics after each type of operation. **Constraints:** - Simulate the actual allocation by representing pointers as unique integers. - Manage and track the allocations properly to avoid memory leaks. **Notes:** - The method signatures provided are illustrative. You might need to adjust or add parameters as per your implementation strategy. - Focus on the correctness and efficiency of managing memory allocations and deallocations. **Sample Output:** ```python >>> allocator = CustomMemoryAllocator() >>> ptr1 = allocator.raw_malloc(100) >>> allocator.raw_free(ptr1) >>> ptr2 = allocator.mem_malloc(200) >>> allocator.mem_free(ptr2) >>> ptr3 = allocator.obj_malloc(300) >>> allocator.obj_free(ptr3) >>> print(allocator.get_statistics()) { \'Raw\': {\'allocated\': 0, \'num_allocations\': 1, \'num_frees\': 1}, \'Mem\': {\'allocated\': 0, \'num_allocations\': 1, \'num_frees\': 1}, \'Object\': {\'allocated\': 0, \'num_allocations\': 1, \'num_frees\': 1} } ```","solution":"class CustomMemoryAllocator: def __init__(self): self.raw_allocations = {} self.mem_allocations = {} self.obj_allocations = {} self.current_pointer = 1 def _allocate(self, size, allocation_dict): ptr = self.current_pointer allocation_dict[ptr] = size self.current_pointer += 1 return ptr def _free(self, ptr, allocation_dict): if ptr in allocation_dict: del allocation_dict[ptr] def raw_malloc(self, size: int) -> int: return self._allocate(size, self.raw_allocations) def raw_free(self, ptr: int) -> None: self._free(ptr, self.raw_allocations) def mem_malloc(self, size: int) -> int: return self._allocate(size, self.mem_allocations) def mem_free(self, ptr: int) -> None: self._free(ptr, self.mem_allocations) def obj_malloc(self, size: int) -> int: return self._allocate(size, self.obj_allocations) def obj_free(self, ptr: int) -> None: self._free(ptr, self.obj_allocations) def get_statistics(self) -> dict: def stats(allocation_dict): return { \'allocated\': sum(allocation_dict.values()), \'num_allocations\': len(allocation_dict), \'num_frees\': len(allocation_dict) - sum(1 for _ in allocation_dict.values()) } return { \'Raw\': stats(self.raw_allocations), \'Mem\': stats(self.mem_allocations), \'Object\': stats(self.obj_allocations) } def test_custom_allocator(): allocator = CustomMemoryAllocator() # Test raw allocation ptr1 = allocator.raw_malloc(100) assert ptr1 in allocator.raw_allocations allocator.raw_free(ptr1) assert ptr1 not in allocator.raw_allocations # Test mem allocation ptr2 = allocator.mem_malloc(200) assert ptr2 in allocator.mem_allocations allocator.mem_free(ptr2) assert ptr2 not in allocator.mem_allocations # Test obj allocation ptr3 = allocator.obj_malloc(300) assert ptr3 in allocator.obj_allocations allocator.obj_free(ptr3) assert ptr3 not in allocator.obj_allocations # Print statistics stats = allocator.get_statistics() print(stats)"},{"question":"# **Challenging Coding Assessment Question** **Objective:** Demonstrate your understanding of the `curses.panel` module by implementing a function to manage a stack of panels based on specific operations. **Problem Statement:** You are tasked with creating a function `manage_panels(operations: List[Tuple[str, Union[int, Tuple[int, int]]]]) -> None` that takes a list of operations to perform on a stack of panels. Each operation can either be creating a new panel, moving an existing panel, hiding/showing a panel, or updating panel positions. **Function Signature:** ```python def manage_panels(operations: List[Tuple[str, Union[int, Tuple[int, int]]]]) -> None: ``` **Parameters:** - `operations (List[Tuple[str, Union[int, Tuple[int, int]]]])`: A list of tuples where each tuple represents an operation. The first element of the tuple is a string representing the operation type, and the second element is either an integer or a tuple of two integers (depending on the operation). **Operation Types:** 1. `\\"create\\", (y, x)`: Create a new panel at coordinates `(y, x)`. 2. `\\"move\\", (panel_index, (y, x))`: Move the panel at `panel_index` to new coordinates `(y, x)`. 3. `\\"hide\\", panel_index`: Hide the panel at `panel_index`. 4. `\\"show\\", panel_index`: Show the panel at `panel_index`. 5. `\\"top\\", panel_index`: Move the panel at `panel_index` to the top of the stack. 6. `\\"bottom\\", panel_index`: Move the panel at `panel_index` to the bottom of the stack. 7. `\\"update\\"`: Update the virtual screen after any changes. **Constraints:** - You can assume there are at most 10 panels. - The coordinates `(y, x)` for panel creation and movement range from `0` to `100`. **Example:** ```python # Example usage of manage_panels function manage_panels([ (\\"create\\", (5, 5)), (\\"create\\", (10, 10)), (\\"move\\", (1, (15, 15))), (\\"hide\\", 0), (\\"show\\", 0), (\\"top\\", 1), (\\"bottom\\", 0), (\\"update\\") ]) ``` In this example, the function should create two panels at specified coordinates, move the second panel, hide and then show the first panel, adjust their stacking order, and finally update the virtual screen to reflect the changes. **Performance Requirements:** - The function needs to efficiently handle up to 10 panels and their operations. - Make sure to avoid unnecessary updates to the display (e.g., only call `update_panels()` when needed). **Note:** - This question assumes basic familiarity with the `curses` module for window creation. - You need to manage the lifecycle of panel objects and ensure they are not prematurely garbage collected. Implement the function `manage_panels` to manage the panel operations as described.","solution":"import curses import curses.panel from typing import List, Tuple, Union def manage_panels(operations: List[Tuple[str, Union[int, Tuple[int, int]]]]) -> None: panels = [] for operation in operations: if operation[0] == \\"create\\": y, x = operation[1] stdscr = curses.initscr() window = curses.newwin(10, 10, y, x) # Assuming panels are of size 10x10 for simplicity panel = curses.panel.new_panel(window) panels.append(panel) elif operation[0] == \\"move\\": panel_index, (y, x) = operation[1] panel = panels[panel_index] panel.window().mvwin(y, x) elif operation[0] == \\"hide\\": panel_index = operation[1] panel = panels[panel_index] panel.hide() elif operation[0] == \\"show\\": panel_index = operation[1] panel = panels[panel_index] panel.show() elif operation[0] == \\"top\\": panel_index = operation[1] panel = panels[panel_index] panel.top() elif operation[0] == \\"bottom\\": panel_index = operation[1] panel = panels[panel_index] panel.bottom() elif operation[0] == \\"update\\": curses.panel.update_panels() stdscr.refresh()"},{"question":"Using the seaborn library, write a function named `plot_stripplot_with_facet` that generates a strip plot with the following customization based on the provided dataset: 1. The x-axis should represent the `total_bill` variable. 2. The y-axis should represent the `day` variable. 3. The points should be colored based on the `sex` variable using the `hue` parameter. 4. Use `dodge=True` to separate points of different sexes. 5. Display multiple facets of the plot based on the `time` variable using the `catplot` function. 6. Customize the marker style to a diamond shape (`D`). 7. Set the size of the markers to 5. 8. Ensure the jitter is disabled. The function should accept a pandas DataFrame, and the output should be a matplotlib Figure object displaying the customized strip plot with multiple facets. Input: - `df`: A pandas DataFrame that contains at least the following columns: `total_bill`, `day`, `sex`, and `time`. Output: - A matplotlib Figure object with the generated plot. Constraints: - The function should internally use the seaborn library and specifically use the `stripplot` and `catplot` functions. - The function should handle possible missing values in the dataset by dropping them before plotting. Example: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Sample data data = { \\"total_bill\\": [16.99, 10.34, 21.01, 23.68], \\"day\\": [\\"Sun\\", \\"Sun\\", \\"Thur\\", \\"Thur\\"], \\"sex\\": [\\"Female\\", \\"Male\\", \\"Male\\", \\"Female\\"], \\"time\\": [\\"Dinner\\", \\"Dinner\\", \\"Lunch\\", \\"Lunch\\"] } df = pd.DataFrame(data) # Function implementation def plot_stripplot_with_facet(df): # Function code here # Generate the plot plot_stripplot_with_facet(df) plt.show() ``` Make sure your function adheres to the requirements and constraints outlined above.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_stripplot_with_facet(df): Generates a strip plot with faceting based on the `time` variable. Args: df (pd.DataFrame): The input DataFrame containing at least the columns `total_bill`, `day`, `sex`, and `time`. Returns: Figure: A matplotlib Figure object displaying the customized strip plot with multiple facets. # Drop missing values df = df.dropna(subset=[\'total_bill\', \'day\', \'sex\', \'time\']) # Create the catplot with stripplot customization g = sns.catplot( data=df, x=\'total_bill\', y=\'day\', hue=\'sex\', col=\'time\', kind=\'strip\', dodge=True, marker=\'D\', s=5, jitter=False ) return g.fig"},{"question":"# Asynchronous File Downloader with Exception Handling You are required to implement an asynchronous file downloader that reads files from multiple URLs and saves them locally. The implementation should handle various error conditions using the specified `asyncio` exceptions. Requirements 1. Implement a function `download_files(urls: List[str], timeout: int, buffer_size: int) -> None` that: - Takes a list of file URLs (`urls`), a timeout in seconds (`timeout`), and a buffer size in bytes (`buffer_size`). - Downloads the content from each URL and saves it to a local file (use a unique filename for each URL based on its position in the list). - Handles the following exceptions: - `asyncio.TimeoutError`: If downloading a file exceeds the given timeout. - `asyncio.CancelledError`: If the operation is canceled. - `asyncio.InvalidStateError`: If an invalid state is encountered with a `Task` or `Future`. - `asyncio.IncompleteReadError`: If a read operation does not complete fully. - `asyncio.LimitOverrunError`: If the buffer size limit is reached while reading data. Inputs - `urls` (List of str): A list of URLs to download. - `timeout` (int): The maximum time in seconds allowed for downloading each file. - `buffer_size` (int): The maximum buffer size in bytes while reading the data. Outputs - None (The function should save the content of each URL to a local file). Constraints - You must use async/await syntax and the `asyncio` library for implementing this function. - Provide appropriate exception handling to ensure that the program continues downloading other files even if one fails. Example ```python import asyncio async def download_files(urls, timeout, buffer_size): # Implement the downloader with appropriate exception handling here. pass # Example usage: urls = [ \\"https://example.com/file1\\", \\"https://example.com/file2\\", \\"https://example.com/file3\\" ] timeout = 10 buffer_size = 1024 # Run the downloader (example, in practice you would call this within an asyncio loop) asyncio.run(download_files(urls, timeout, buffer_size)) ``` Test your implementation thoroughly to ensure it robustly handles various error conditions and completes the file download for valid URLs.","solution":"import asyncio import aiohttp from typing import List async def download_file(url: str, idx: int, timeout: int, buffer_size: int): filename = f\\"file_{idx}.dat\\" try: async with aiohttp.ClientSession() as session: async with session.get(url, timeout=timeout) as response: response.raise_for_status() with open(filename, \'wb\') as f: while True: chunk = await response.content.read(buffer_size) if not chunk: break f.write(chunk) except asyncio.TimeoutError: print(f\\"TimeoutError: Failed to download {url} within {timeout} seconds\\") except asyncio.CancelledError: print(f\\"CancelledError: Download cancelled for {url}\\") raise except asyncio.InvalidStateError: print(f\\"InvalidStateError: Invalid state encountered for {url}\\") except asyncio.IncompleteReadError as e: print(f\\"IncompleteReadError: Incomplete read for {url} with {e.partial} bytes read\\") except aiohttp.ClientError as e: print(f\\"ClientError: Failed to download {url} with error {e}\\") except aiohttp.http_exceptions.HttpProcessingError as e: print(f\\"HTTP error: {e.status}: {e.message}\\") async def download_files(urls: List[str], timeout: int, buffer_size: int): tasks = [ download_file(url, idx, timeout, buffer_size) for idx, url in enumerate(urls) ] await asyncio.gather(*tasks, return_exceptions=True)"},{"question":"**Question: Implement Custom Logging Hierarchy** Write a Python function `setup_custom_logging` that configures a logging hierarchy for a hypothetical web server application. The function should perform the following tasks: 1. Setup four loggers named `webserver`, `webserver.db`, `webserver.auth`, and `webserver.api`. 2. Each logger should use different handlers and formatters. 3. The `webserver` logger should use a `StreamHandler` that outputs to the console with log messages formatted as `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'`. 4. The `webserver.db` logger should log messages only to a file named `db.log`, using the log format `\'%(asctime)s - %(message)s\'`. 5. The `webserver.auth` logger should log messages only to a file named `auth.log` with the log format `\'%(levelname)s: %(message)s\'` and should only log messages at the `ERROR` level and above. 6. The `webserver.api` logger should be a child of `webserver` and, besides inheriting its handler and format configuration, it should filter out all `DEBUG` level messages before they are passed to its handlers. 7. Ensure the loggers are set up to propagate messages appropriately to avoid duplicate logging. 8. Demonstrate logging a few messages from each logger to show the hierarchy and formatting in action. # Function Signature ```python def setup_custom_logging(): pass ``` # Constraints - Ensure the function runs without raising exceptions. - Assume the program has the necessary file system permissions for creating and writing to log files. # Example Usage ```python setup_custom_logging() logger_webserver = logging.getLogger(\'webserver\') logger_webserver.info(\'Starting webserver\') logger_db = logging.getLogger(\'webserver.db\') logger_db.debug(\'Database initialized\') logger_auth = logging.getLogger(\'webserver.auth\') logger_auth.error(\'Unauthorized access attempt\') logger_api = logging.getLogger(\'webserver.api\') logger_api.info(\'API request received\') logger_api.debug(\'API request details\') ``` # Expected Output The output should be visible in the console and appropriate log files based on the logger and handler configurations specified.","solution":"import logging def setup_custom_logging(): # Create the main logger webserver_logger = logging.getLogger(\'webserver\') webserver_logger.setLevel(logging.DEBUG) # Create a console handler for the main webserver logger console_handler = logging.StreamHandler() console_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') console_handler.setFormatter(console_formatter) webserver_logger.addHandler(console_handler) # Create the database logger and handler db_logger = logging.getLogger(\'webserver.db\') db_logger.setLevel(logging.DEBUG) db_handler = logging.FileHandler(\'db.log\') db_formatter = logging.Formatter(\'%(asctime)s - %(message)s\') db_handler.setFormatter(db_formatter) db_logger.addHandler(db_handler) # Create the auth logger and handler auth_logger = logging.getLogger(\'webserver.auth\') auth_logger.setLevel(logging.ERROR) auth_handler = logging.FileHandler(\'auth.log\') auth_formatter = logging.Formatter(\'%(levelname)s: %(message)s\') auth_handler.setFormatter(auth_formatter) auth_logger.addHandler(auth_handler) # Create the API logger (inherits handlers from the main webserver logger) api_logger = logging.getLogger(\'webserver.api\') api_logger.setLevel(logging.INFO) # Add a filter to the API logger to ignore DEBUG level messages class NoDebugFilter(logging.Filter): def filter(self, record): return record.levelno != logging.DEBUG no_debug_filter = NoDebugFilter() api_logger.addFilter(no_debug_filter) # Ensure propagation is set so that messages go up to parent loggers webserver_logger.propagate = False db_logger.propagate = False auth_logger.propagate = False api_logger.propagate = True # Demonstrate logging if __name__ == \\"__main__\\": setup_custom_logging() logger_webserver = logging.getLogger(\'webserver\') logger_webserver.info(\'Starting webserver\') logger_db = logging.getLogger(\'webserver.db\') logger_db.debug(\'Database initialized\') logger_auth = logging.getLogger(\'webserver.auth\') logger_auth.error(\'Unauthorized access attempt\') logger_api = logging.getLogger(\'webserver.api\') logger_api.info(\'API request received\') logger_api.debug(\'API request details\')"},{"question":"# URL Shortener with `urllib` Objective: Create a customized URL shortener service using Python\'s `urllib` package. Your task is to implement a function that takes a long URL, generates a shorter URL, and then can retrieve the original URL from the shortened version. Specifications: 1. **Function Implementations**: * `shorten_url(long_url: str) -> str`: Given a long URL, return a shortened URL. * `get_long_url(shortened_url: str) -> str`: Given a shortened URL, return the original long URL. 2. **Input and Output**: * `shorten_url` should accept a string `long_url` that represents a valid URL. * `shorten_url` should return a string that represents a shortened URL. * `get_long_url` should accept a string `shortened_url`. * `get_long_url` should return the corresponding original `long_url`. 3. **Constraints**: * You must use the `urllib` package to handle URLs. * Assume that you have limited space and memory, thus you should design the most efficient data structure and algorithm to handle URL mapping. * Your shortened URLs should be unique and minimal in length. 4. **Performance Requirements**: * Your solution should efficiently handle multiple requests for shortening and retrieving URLs. Example: ```python def shorten_url(long_url: str) -> str: pass def get_long_url(shortened_url: str) -> str: pass # Example Usage original_url = \\"https://docs.python.org/3/library/urllib.html\\" shortened = shorten_url(original_url) print(shortened) # Outputs a shortened URL like \\"http://short.url/abc123\\" retrieved = get_long_url(shortened) print(retrieved) # Outputs \\"https://docs.python.org/3/library/urllib.html\\" ``` Notes: - Your implementation should be consistent and correct, such that every call to `shorten_url` with the same long URL generates the same shortened URL, and every call to `get_long_url` with the shortened URL retrieves the correct long URL. - You do not need to host or make an actual web server application, but you should test your functions thoroughly with different URLs. - You can assume the protocol (e.g., `http`, `https`) is always included in the `long_url`.","solution":"import urllib.parse import random import string class URLShortener: def __init__(self): # Stores the mapping from short url to long url self.short_to_long = {} # Stores the mapping from long url to short url self.long_to_short = {} self.base_url = \\"http://short.url/\\" self.short_url_length = 6 def shorten_url(self, long_url: str) -> str: if long_url in self.long_to_short: return self.long_to_short[long_url] while True: short_code = \'\'.join(random.choices(string.ascii_letters + string.digits, k=self.short_url_length)) short_url = urllib.parse.urljoin(self.base_url, short_code) if short_url not in self.short_to_long: self.short_to_long[short_url] = long_url self.long_to_short[long_url] = short_url return short_url def get_long_url(self, shortened_url: str) -> str: return self.short_to_long.get(shortened_url, None) # Initialize URLShortener url_shortener = URLShortener() def shorten_url(long_url: str) -> str: return url_shortener.shorten_url(long_url) def get_long_url(shortened_url: str) -> str: return url_shortener.get_long_url(shortened_url)"},{"question":"**Custom Scikit-Learn Estimator Implementation** **Objective:** Your task is to implement a custom scikit-learn compatible estimator, named `CustomGaussianNoiseAdder`, that adds Gaussian noise to the input data. This estimator will be a transformer, which means it should implement a `fit` method and a `transform` method that adheres to the scikit-learn guidelines. **Requirements:** 1. **Constructor (`__init__` method):** - The estimator should take the following hyperparameters: - `noise_mean` (default value: 0.0): The mean of the Gaussian noise. - `noise_std` (default value: 1.0): The standard deviation of the Gaussian noise. - `random_state` (default value: None): The random seed for reproducibility. - The constructor should only initialize these hyperparameters as attributes. 2. **`fit` method:** - This method takes the input data `X` and an optional `y` argument (which should be ignored). - It should validate the input data `X` using `check_array`. - The method should set a `random_state_` attribute using `check_random_state`. - The method should return `self`. 3. **`transform` method:** - This method takes the input data `X`. - It should check if the estimator has been fitted using `check_is_fitted`. - It should validate the input data `X` using `check_array`. - It should add Gaussian noise to the data `X` with the specified `noise_mean` and `noise_std` and return the noisy data. 4. **Integration with scikit-learn:** - The estimator should inherit from `BaseEstimator` and `TransformerMixin`. - The estimator should pass the scikit-learn estimator checks. **Input Format:** - The `fit` method will accept: - `X`: An array-like of shape `(n_samples, n_features)`. - `y`: An optional array-like of shape `(n_samples,)`, default is `None`. - The `transform` method will accept: - `X`: An array-like of shape `(n_samples, n_features)`. **Output Format:** - The `fit` method should return `self`. - The `transform` method should return an array-like of shape `(n_samples, n_features)` with added Gaussian noise. **Code Implementation:** ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted from sklearn.utils import check_random_state class CustomGaussianNoiseAdder(BaseEstimator, TransformerMixin): def __init__(self, noise_mean=0.0, noise_std=1.0, random_state=None): self.noise_mean = noise_mean self.noise_std = noise_std self.random_state = random_state def fit(self, X, y=None): X = check_array(X) self.random_state_ = check_random_state(self.random_state) return self def transform(self, X): check_is_fitted(self, [\'random_state_\']) X = check_array(X) noise = self.random_state_.normal(self.noise_mean, self.noise_std, X.shape) return X + noise # Example Test if __name__ == \\"__main__\\": estimator = CustomGaussianNoiseAdder(noise_mean=0, noise_std=0.1, random_state=42) X = np.array([[1, 2], [3, 4], [5, 6]]) estimator.fit(X) X_transformed = estimator.transform(X) print(\\"Original Data:n\\", X) print(\\"Transformed Data with Noise:n\\", X_transformed) ``` **Constraints and Assumptions:** - Assume the input data `X` is a 2D NumPy array. - Use `check_array` for input validation to ensure the data is a valid array. - Your implementation should be compatible with scikit-learn pipelines.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin from sklearn.utils.validation import check_array, check_is_fitted from sklearn.utils import check_random_state class CustomGaussianNoiseAdder(BaseEstimator, TransformerMixin): def __init__(self, noise_mean=0.0, noise_std=1.0, random_state=None): self.noise_mean = noise_mean self.noise_std = noise_std self.random_state = random_state def fit(self, X, y=None): X = check_array(X) self.random_state_ = check_random_state(self.random_state) return self def transform(self, X): check_is_fitted(self, [\'random_state_\']) X = check_array(X) noise = self.random_state_.normal(self.noise_mean, self.noise_std, X.shape) return X + noise"},{"question":"# Simulating a Queue Management System In a busy office, employees need to use a limited number of computers to perform their tasks. The office administration wants to implement a queue management system to efficiently allocate computer usage to employees. You are required to implement a simulation of this system using Python\'s `threading` module. Requirements: 1. **Employee Class (`Employee`)**: - Each employee operates in their own thread. - Provide a `run()` method in the `Employee` class that handles the logic for an employee trying to use a computer. - Each employee should try to acquire a computer (if available) for a certain time, use it, and then release it. 2. **ComputerLab Class (`ComputerLab`)**: - Manages the pool of computers using a semaphore to limit access to a fixed number of computers. - Should provide methods to acquire and release a computer. 3. **Simulation**: - Create multiple `Employee` instances that compete for a limited number of computers in the `ComputerLab`. - Ensure synchronization so that no more than the allowed number of employees can use the computers at the same time. - Handle and print appropriate messages when an employee starts using a computer and when they release it. Specifications: - The `ComputerLab` should be initialized with a fixed number of computers. - Each `Employee` should try to acquire a computer for a random amount of time between 1 and 5 seconds. - Once an employee acquires a computer, they will \\"use\\" it (by sleeping their thread) for a random amount of time between 1 and 3 seconds before releasing it. Constraints: - Use the `threading.Semaphore` to manage the pool of computers. - Ensure that the `Employee` class extends `threading.Thread`. - The simulation should run until all employees have finished their tasks. Input: - Number of Employees: Integer ( N ) - Number of Computers: Integer ( M ) Output: Print log messages indicating: - When an employee starts waiting for a computer. - When an employee acquires a computer. - When an employee releases a computer. Example Output: ``` Employee 1 is waiting for a computer. Employee 2 is waiting for a computer. Employee 3 is waiting for a computer. Employee 1 has acquired a computer. Employee 3 has acquired a computer. Employee 1 has released the computer. Employee 2 has acquired a computer. ... ``` Implementation Hints: - Use `random.randint` to generate random wait and use times for employees. - Ensure to handle any possible exceptions or timeouts in acquiring computers gracefully. Write the function `simulate_queue_management` to start the simulation, taking the number of employees (`N`) and the number of computers (`M`) as input. ```python import threading import time import random class ComputerLab: def __init__(self, num_computers): self.semaphore = threading.Semaphore(num_computers) def acquire_computer(self, employee_id): print(f\\"Employee {employee_id} is waiting for a computer.\\") self.semaphore.acquire() print(f\\"Employee {employee_id} has acquired a computer.\\") def release_computer(self, employee_id): print(f\\"Employee {employee_id} has released the computer.\\") self.semaphore.release() class Employee(threading.Thread): def __init__(self, lab, employee_id): super().__init__() self.lab = lab self.employee_id = employee_id def run(self): # Employee tries to acquire a computer self.lab.acquire_computer(self.employee_id) # Simulate using the computer time_to_use = random.randint(1, 3) time.sleep(time_to_use) # Release the computer self.lab.release_computer(self.employee_id) def simulate_queue_management(num_employees, num_computers): lab = ComputerLab(num_computers) employees = [] for i in range(num_employees): employee = Employee(lab, i + 1) employees.append(employee) employee.start() for employee in employees: employee.join() # Example usage simulate_queue_management(5, 2) ``` Test the above function with various input values to ensure correct simulations and outputs.","solution":"import threading import time import random class ComputerLab: def __init__(self, num_computers): self.semaphore = threading.Semaphore(num_computers) def acquire_computer(self, employee_id): print(f\\"Employee {employee_id} is waiting for a computer.\\") self.semaphore.acquire() print(f\\"Employee {employee_id} has acquired a computer.\\") def release_computer(self, employee_id): print(f\\"Employee {employee_id} has released the computer.\\") self.semaphore.release() class Employee(threading.Thread): def __init__(self, lab, employee_id): super().__init__() self.lab = lab self.employee_id = employee_id def run(self): # Employee tries to acquire a computer self.lab.acquire_computer(self.employee_id) # Simulate using the computer time_to_use = random.randint(1, 3) time.sleep(time_to_use) # Release the computer self.lab.release_computer(self.employee_id) def simulate_queue_management(num_employees, num_computers): lab = ComputerLab(num_computers) employees = [] for i in range(num_employees): employee = Employee(lab, i + 1) employees.append(employee) employee.start() for employee in employees: employee.join() # Example usage simulate_queue_management(5, 2)"},{"question":"# Advanced Python Coding Assessment: Subprocess Management **Objective**: Implement a function using the `subprocess` module to run a series of shell commands in sequence, manage their output, handle errors, and apply appropriate constraints and controls. **Description**: Write a function `manage_subprocess(commands: List[List[str]], cwd: Optional[str] = None, env: Optional[Dict[str, str]] = None, timeout: Optional[int] = None) -> Dict[str, Union[int, str, Dict[str, str]]]` which orchestrates the execution of multiple shell commands in sequence. Each command should be run with the following considerations: - Capture and return both stdout and stderr of each command. - Apply a timeout limit, if specified, to prevent commands from running indefinitely. - Use a given directory (cwd) as the working directory for the commands, if specified. - Set specific environment variables for the subprocess execution, if specified. - Ensure proper error handling and raise an exception with detailed error information if any command fails. **Constraints**: - Commands should be executed in the given order. - If a command fails (non-zero exit code), abort the sequence and raise an exception with details of the failed command. - Use the `subprocess.run` function wherever possible, but fall back on `subprocess.Popen` for handling more complex cases. **Input**: - `commands`: A list where each element is a list representing a command and its arguments. For example: `[[\'ls\', \'-l\'], [\'grep\', \'test\'], [\'echo\', \'done\']]` - `cwd` (optional): A string representing the working directory for the commands, default to `None`. - `env` (optional): A dictionary of environment variables to be set for the command executions, default to `None`. - `timeout` (optional): An integer representing the number of seconds after which the command should be aborted, default to `None`. **Output**: - A dictionary with the keys: - `returncode`: The return code of the last successfully executed command. - `stdout`: Combined stdout of all commands executed successfully. - `stderr`: Combined stderr of all commands executed successfully. - `env_used`: The environment variables used during the execution. **Function Signature**: ```python from typing import List, Optional, Dict, Union def manage_subprocess(commands: List[List[str]], cwd: Optional[str] = None, env: Optional[Dict[str, str]] = None, timeout: Optional[int] = None) -> Dict[str, Union[int, str, Dict[str, str]]]: pass ``` **Example**: ```python commands = [ [\'echo\', \'hello\'], [\'ls\', \'-l\'], [\'grep\', \'file_that_does_not_exist\'], # This will cause the sequence to fail [\'echo\', \'done\'] ] try: result = manage_subprocess(commands, timeout=10) print(result) except Exception as e: print(\\"Failed:\\", e) ``` **Performance Requirements**: - Be mindful of large volumes of output data when capturing stdout and stderr. Use `universal_newlines=True` or `text=True` to handle text data appropriately. Implement this function to demonstrate an in-depth understanding of subprocess management in Python, covering a range of functionalities as detailed in the documentation.","solution":"from typing import List, Optional, Dict, Union import subprocess def manage_subprocess(commands: List[List[str]], cwd: Optional[str] = None, env: Optional[Dict[str, str]] = None, timeout: Optional[int] = None) -> Dict[str, Union[int, str, Dict[str, str]]]: combined_stdout = \\"\\" combined_stderr = \\"\\" env_used = env if env is not None else {} for command in commands: try: result = subprocess.run(command, cwd=cwd, env=env, timeout=timeout, capture_output=True, text=True) combined_stdout += result.stdout combined_stderr += result.stderr if result.returncode != 0: raise subprocess.CalledProcessError(result.returncode, command, output=result.stdout, stderr=result.stderr) except subprocess.TimeoutExpired as e: raise Exception(f\\"Command \'{command}\' timed out after {timeout} seconds\\") from e except subprocess.CalledProcessError as e: raise Exception(f\\"Command \'{e.cmd}\' failed with return code {e.returncode}, stdout: {e.output}, stderr: {e.stderr}\\") from e return { \\"returncode\\": result.returncode, \\"stdout\\": combined_stdout, \\"stderr\\": combined_stderr, \\"env_used\\": env_used }"},{"question":"# HTTP Client Implementation and Response Handling **Problem Statement:** You are required to implement a function `fetch_url_data`, which performs an HTTP GET request to a given URL using the `http.client` module. The function should handle redirects, connection timeouts, and different HTTP status codes effectively. Additionally, the function should be able to retry the request if the connection is interrupted or fails (up to a certain number of retries). Function Signature: ```python def fetch_url_data(url: str, retries: int = 3, timeout: int = 10) -> tuple: Fetches the data from the given URL using an HTTP GET request. :param url: The URL to send the GET request to. :param retries: The number of retry attempts if the connection fails. :param timeout: The maximum time, in seconds, to wait for a server response. :returns: A tuple containing the status code, reason, and response body. :raises ValueError: If the URL is invalid or if the number of retries is exceeded. ``` Input: - `url`: A string representing the URL to be fetched. - `retries`: An optional integer specifying the number of retry attempts (default is 3). - `timeout`: An optional integer specifying the timeout period in seconds (default is 10 seconds). Output: - The function should return a tuple `(status_code, reason, response_body)`. - `status_code`: The HTTP status code returned by the server. - `reason`: The HTTP reason phrase returned by the server. - `response_body`: The response body from the server, converted to a string. Constraints: - The function should handle HTTP redirects (status codes 3xx) by following the redirect URL. - For HTTP errors (status codes 4xx and 5xx), the function should return the status code and reason along with the error message in the response body. - The connection attempts should not exceed the specified `retries` count. - If the URL is malformed or cannot be processed, raise a `ValueError`. Example: ```python def fetch_url_data(url: str, retries: int = 3, timeout: int = 10) -> tuple: # Your implementation goes here # Example usage: result = fetch_url_data(\\"http://www.example.com\\") print(result) # e.g., (200, \'OK\', \'<html>...</html>\') ``` **Note:** - Make sure to utilize the `http.client` module to manage HTTP connections and handle exceptions as described in the problem statement. - Ensure your implementation follows HTTP/1.1 standards, particularly handling redirects and chunked responses correctly.","solution":"import http.client import urllib.parse def fetch_url_data(url: str, retries: int = 3, timeout: int = 10) -> tuple: parsed_url = urllib.parse.urlparse(url) if not parsed_url.scheme or not parsed_url.netloc: raise ValueError(\\"Invalid URL\\") def make_request(retries_left: int): try: conn = http.client.HTTPConnection(parsed_url.netloc, timeout=timeout) conn.request(\'GET\', parsed_url.path or \\"/\\") response = conn.getresponse() if 300 <= response.status < 400: redirect_url = response.getheader(\'Location\') if redirect_url: return fetch_url_data(redirect_url, retries_left, timeout) response_body = response.read().decode() return response.status, response.reason, response_body except (http.client.HTTPException, http.client.ImproperConnectionState) as e: if retries_left > 0: return make_request(retries_left - 1) else: raise ValueError(\\"Failed to connect after several retries\\") from e finally: conn.close() return make_request(retries)"},{"question":"Objective: The objective of this assessment is to evaluate your understanding of the Seaborn package, specifically with `FacetGrid` and `PairGrid`. You will demonstrate your ability to create multi-plot grids, customize them, and interpret the results. Problem Statement: You are provided with a dataset `mpg` from Seaborn\'s dataset repository. The dataset contains information on various cars including their miles-per-gallon (mpg), number of cylinders (`cylinders`), horsepower (`horsepower`), and origin of the car (`origin`). You are required to: 1. Create a `FacetGrid` that visualizes the distribution of `mpg` for cars, separated by `origin` (USA, Europe, Japan) and `cylinders`. 2. Customize your grid to: - Use a `sns.histplot` to plot the distribution. - Set height of each facet to 3 and aspect ratio to 1.5. - Add a legend to differentiate the cylinders. - Adjust titles and labels for better readability. 3. Write a custom plotting function that plots a regression line (`sns.regplot`) showing the relationship between `horsepower` and `mpg`, separated by `origin`. 4. Create a `PairGrid` showing pairwise relationships between `mpg`, `horsepower`, and weight (`weight`) in the dataset. Color the relationships based on the `origin`. Constraints: - The `mpg` dataset can be loaded using `sns.load_dataset(\'mpg\')`. - Use appropriate Seaborn functions and parameters as demonstrated in the problem statement. - Ensure plots are intuitive and labeled for easy interpretation. Expected Solution: 1. The `FacetGrid` should have three rows for each `origin`, and within each row should be subplots based on the number of `cylinders`. 2. Histplot in each subplot must show the `mpg` distribution. 3. Custom plotting function must include a regression line fit with scatter points for `horsepower` vs. `mpg`. 4. The `PairGrid` should show pairwise relationships with scatter plots and histograms. ```python # Load necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Load dataset mpg = sns.load_dataset(\'mpg\') # 1. Create a FacetGrid g = sns.FacetGrid(mpg, row=\'origin\', col=\'cylinders\', height=3, aspect=1.5) g.map(sns.histplot, \'mpg\') g.add_legend() g.set_axis_labels(\'MPG\', \'Frequency\') g.set_titles(col_template=\'{col_name} Cylinders\', row_template=\'{row_name} Origin\') g.fig.suptitle(\'Distribution of MPG by Origin and Cylinders\', y=1.03) # 2. Custom plotting function def regplot_hp_mpg(x, y, **kwargs): sns.regplot(x=x, y=y, **kwargs) # 3. FacetGrid for custom plot g2 = sns.FacetGrid(mpg, col=\'origin\', height=4) g2.map(regplot_hp_mpg, \'horsepower\', \'mpg\') g2.set_axis_labels(\'Horsepower\', \'MPG\') g2.fig.suptitle(\'Horsepower vs MPG by Origin\', y=1.02) # 4. PairGrid for pairwise plots pg = sns.PairGrid(mpg, vars=[\'mpg\', \'horsepower\', \'weight\'], hue=\'origin\') pg.map_diag(sns.histplot) pg.map_offdiag(sns.scatterplot) pg.add_legend() pg.fig.suptitle(\'Pairwise Relationships by Origin\', y=1.02) plt.show() ``` This problem tests your ability to utilize Seaborn\'s functionality to analyze and visualize data effectively.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset mpg = sns.load_dataset(\'mpg\') # 1. Create a FacetGrid g = sns.FacetGrid(mpg, row=\'origin\', col=\'cylinders\', height=3, aspect=1.5) g.map(sns.histplot, \'mpg\') g.add_legend() g.set_axis_labels(\'MPG\', \'Frequency\') g.set_titles(col_template=\'{col_name} Cylinders\', row_template=\'{row_name} Origin\') g.fig.suptitle(\'Distribution of MPG by Origin and Cylinders\', y=1.03) plt.savefig(\'facetgrid_mpg_distribution_by_origin_and_cylinders.png\') # Save the plot for testing # 2. Custom plotting function def regplot_hp_mpg(x, y, **kwargs): sns.regplot(x=x, y=y, **kwargs) # 3. FacetGrid for custom plot g2 = sns.FacetGrid(mpg, col=\'origin\', height=4) g2.map(regplot_hp_mpg, \'horsepower\', \'mpg\') g2.set_axis_labels(\'Horsepower\', \'MPG\') g2.fig.suptitle(\'Horsepower vs MPG by Origin\', y=1.02) plt.savefig(\'facetgrid_hp_vs_mpg_by_origin.png\') # Save the plot for testing # 4. PairGrid for pairwise plots pg = sns.PairGrid(mpg, vars=[\'mpg\', \'horsepower\', \'weight\'], hue=\'origin\') pg.map_diag(sns.histplot) pg.map_offdiag(sns.scatterplot) pg.add_legend() pg.fig.suptitle(\'Pairwise Relationships by Origin\', y=1.02) plt.savefig(\'pairgrid_relationships_by_origin.png\') # Save the plot for testing plt.show()"},{"question":"**Challenge: Environment Configuration with the `site` Module** In this exercise, you will demonstrate your understanding of the `site` module by creating a Python script that configures the environment paths according to specific rules and validates the configuration. # Task 1. **Directory Validation Function:** Implement a function `validate_directories(directories)` which takes a list of directory paths and returns a dictionary where each key is a directory path, and the value is `True` if it exists and `False` otherwise. ```python def validate_directories(directories): Validates the existence of directories. Args: directories (list): List of directory paths as strings. Returns: dict: Dictionary with directory paths as keys and boolean values indicating existence. pass ``` 2. **Path Configuration:** Create a function `configure_paths(base_path, exec_path, version)` which takes: - `base_path` (string): The base prefix path. - `exec_path` (string): The exec prefix path. - `version` (string): Python version in the form `X.Y`. This function constructs potential paths using combinations of `base_path` + `exec_path` with possible site-packages directory names and returns a list of those that exist. ```python def configure_paths(base_path, exec_path, version): Constructs and validates potential site-packages paths. Args: base_path (str): Base prefix path. exec_path (str): Exec prefix path. version (str): Python version `X.Y`. Returns: list: List of valid existing paths. pass ``` 3. **Path Addition and `.pth` Processing:** Implement the function `add_site_directory(site_dir)` that takes: - `site_dir` (string): The directory to add to `sys.path`, processes `.pth` files within it, and updates `sys.path` accordingly. ```python import sys import os def add_site_directory(site_dir): Add a directory to sys.path and process its `.pth` files. Args: site_dir (str): Directory path to add to sys.path. Returns: None pass ``` 4. **Main Configuration Function:** Finally, implement `site_configuration(base_path, exec_path, version)` which uses the above functions to: - Validate constructed paths. - Add valid site-packages directories to `sys.path`. - Handle `.pth` files. ```python def site_configuration(base_path, exec_path, version): Main function to configure and validate site paths. Args: base_path (str): Base prefix path. exec_path (str): Exec prefix path. version (str): Python version `X.Y`. Returns: list: Updated sys.path pass ``` # Constraints: - Do not modify global `sys.path` directly; only use the provided helper functions to manipulate it. - Ensure all paths are validated for existence using `validate_directories`. # Input/Output: - **Input**: Strings representing base paths, exec paths, and Python version. - **Output**: An updated list of directories in `sys.path`. Example: ```python base_path = \\"/usr/local\\" exec_path = \\"/usr/local\\" version = \\"3.10\\" sys.path = site_configuration(base_path, exec_path, version) print(sys.path) ``` Use the provided helper functions, simulate the module\'s automatic path configuration mechanism, and ensure your solution accurately updates `sys.path` based on the described logic.","solution":"import os import sys def validate_directories(directories): Validates the existence of directories. Args: directories (list): List of directory paths as strings. Returns: dict: Dictionary with directory paths as keys and boolean values indicating existence. return {directory: os.path.isdir(directory) for directory in directories} def configure_paths(base_path, exec_path, version): Constructs and validates potential site-packages paths. Args: base_path (str): Base prefix path. exec_path (str): Exec prefix path. version (str): Python version `X.Y`. Returns: list: List of valid existing paths. paths_to_check = [ os.path.join(base_path, \'lib\', \'python\' + version, \'site-packages\'), os.path.join(exec_path, \'lib\', \'python\' + version, \'site-packages\'), os.path.join(base_path, \'lib\', \'site-python\'), os.path.join(exec_path, \'lib\', \'site-python\') ] valid_paths = validate_directories(paths_to_check) return [path for path, exists in valid_paths.items() if exists] def add_site_directory(site_dir): Add a directory to sys.path and process its `.pth` files. Args: site_dir (str): Directory path to add to sys.path. Returns: None if site_dir not in sys.path: sys.path.append(site_dir) pth_files = [f for f in os.listdir(site_dir) if f.endswith(\'.pth\')] for pth in pth_files: pth_path = os.path.join(site_dir, pth) with open(pth_path, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'#\'): sys.path.append(os.path.join(site_dir, line)) def site_configuration(base_path, exec_path, version): Main function to configure and validate site paths. Args: base_path (str): Base prefix path. exec_path (str): Exec prefix path. version (str): Python version `X.Y`. Returns: list: Updated sys.path valid_paths = configure_paths(base_path, exec_path, version) for path in valid_paths: add_site_directory(path) return sys.path"},{"question":"# Coding Assessment: PyTorch Tensor Manipulation and Gradient Computation Objective In this assessment, you will demonstrate your understanding of PyTorch’s fundamental concepts by implementing a series of operations on tensors. Your task will involve creating tensors, performing mathematical operations, and utilizing PyTorch’s autograd feature to compute gradients. Problem Statement You are provided with two vectors `a` and `b` as lists of integers. Your task is to: 1. Convert these lists to PyTorch tensors with automatic gradient tracking enabled. 2. Perform element-wise multiplication of these tensors. 3. Compute the sum of the resulting tensor. 4. Using PyTorch\'s autograd, compute the gradient of this sum with respect to the original tensors `a` and `b`. Input - `a`: List of integers (e.g., `[1, 2, 3]`) - `b`: List of integers (e.g., `[4, 5, 6]`) Output - A tuple consisting of: - Element-wise product tensor. - Sum of the product tensor. - Gradient of the sum with respect to tensor `a`. - Gradient of the sum with respect to tensor `b`. Constraints - The input lists `a` and `b` will have the same length. - You should not use any external libraries except PyTorch. Example ```python Input: a = [1, 2, 3] b = [4, 5, 6] Output: ( tensor([ 4, 10, 18], grad_fn=<MulBackward0>), tensor(32, grad_fn=<SumBackward0>), tensor([4., 5., 6.]), tensor([1., 2., 3.]) ) ``` Implementation ```python import torch def tensor_operations(a, b): # Step 1: Convert lists to tensors with requires_grad=True tensor_a = torch.tensor(a, dtype=torch.float32, requires_grad=True) tensor_b = torch.tensor(b, dtype=torch.float32, requires_grad=True) # Step 2: Perform element-wise multiplication product = tensor_a * tensor_b # Step 3: Compute the sum of the resulting tensor result_sum = product.sum() # Step 4: Compute gradients result_sum.backward() # Extract gradients grad_a = tensor_a.grad grad_b = tensor_b.grad return product, result_sum, grad_a, grad_b # Sample test case a = [1, 2, 3] b = [4, 5, 6] print(tensor_operations(a, b)) ``` Notes - Ensure that you enable gradient tracking when creating the tensors. - Use `.backward()` method to compute gradients. - Verify the correctness of your gradients by considering the relationship between the operations performed and their derivatives. Implement the `tensor_operations` function in the cell provided and test it against provided and additional test cases.","solution":"import torch def tensor_operations(a, b): # Step 1: Convert lists to tensors with requires_grad=True tensor_a = torch.tensor(a, dtype=torch.float32, requires_grad=True) tensor_b = torch.tensor(b, dtype=torch.float32, requires_grad=True) # Step 2: Perform element-wise multiplication product = tensor_a * tensor_b # Step 3: Compute the sum of the resulting tensor result_sum = product.sum() # Step 4: Compute gradients result_sum.backward() # Extract gradients grad_a = tensor_a.grad grad_b = tensor_b.grad return product, result_sum, grad_a, grad_b"},{"question":"Coding Assessment Question You are required to write a Python program that uses the `pty` module to create a simple chat application between two pseudo-terminals. This will assess your understanding of pseudo-terminals and process handling in Python. # Task 1. Implement a function `start_chat()` that: - Uses `pty.openpty()` to create a pair of file descriptors for a master and slave pseudo-terminal. - Forks the current process using `pty.fork()`. - In the parent process, opens a subprocess shell command in the pseudo-terminal returned from `pty.spawn()`. - In the child process, runs a simple interactive Python console that can read from and write to the pseudo-terminal. # Requirements - **Function Signature**: `def start_chat():` - **Input**: None (all setup and execution are handled internally). - **Output**: None directly; the function should handle IO through the pseudo-terminal. # Constraints and Notes - Ensure that the interactive Python console in the child process is correctly reading from and writing to the pseudo-terminal. - Make sure the parent process responds by echoing input from the pseudo-terminal back to the console. - Both processes should communicate bidirectionally until the child process is terminated. # Example Usage Here is an outline of what your program should achieve: - The parent process spawns an interactive shell, and the child process runs a Python console. - Commands entered in the Python console should be echoed back by the parent process. - The chat should be interactive in real-time. # Implementation Tips - You might find it useful to use `os.read()`, `os.write()`, and other functions from the `os` module for handling IO operations directly. - Consider handling the EOF (End of File) conditions gracefully to avoid infinite loops. - Test the function locally in a POSIX-compliant environment to ensure it operates as expected. # Expected Outcome - Your implemented function should create an interactive chat application, allowing a user to type commands in the Python console and receive responses echoed back from the shell. Good luck!","solution":"import os import pty import subprocess def start_chat(): # Open master and slave pseudo-terminal pairs master_fd, slave_fd = pty.openpty() # Fork the process pid = os.fork() if pid == 0: # Child process os.close(master_fd) # Redirect stdin, stdout, and stderr to the slave pseudo-terminal os.dup2(slave_fd, 0) os.dup2(slave_fd, 1) os.dup2(slave_fd, 2) # Execute the interactive Python console os.execlp(\\"python\\", \\"python\\") else: # Parent process os.close(slave_fd) def read_write_echo(): while True: try: # Read from master_fd data = os.read(master_fd, 1024) if len(data) == 0: break # End of file, exit loop # Echo the received input back os.write(master_fd, data) except OSError: break read_write_echo() # Note: This function can be tested manually by calling start_chat() in a POSIX-compliant environment."},{"question":"**Objective:** Implement a function that receives a list and a slice object, then returns a new list that replicates the behavior of slicing in Python. Additionally, the function should handle out-of-bounds indices correctly and adjust them similar to how Python internally manages slice objects. **Function Signature:** ```python def custom_slice(sequence, slice_obj): Parameters: - sequence (list): The list to be sliced. - slice_obj (slice): The slice object defining start, stop, and step. Returns: - list: A new list that corresponds to \'sequence\' sliced by \'slice_obj\'. Raises: - TypeError: If \'sequence\' is not a list or \'slice_obj\' is not a slice. ``` **Input:** 1. `sequence`: A list of integers. Length constraint: 1 <= len(sequence) <= 10^4. 2. `slice_obj`: A slice object with potentially negative or out-of-bounds start, stop, and step values. **Output:** - A new list resulting from slicing `sequence` according to `slice_obj`. **Example:** ```python my_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] s = slice(2, 8, 2) print(custom_slice(my_list, s)) # Output: [2, 4, 6] ``` **Constraints:** 1. Ensure that the function checks for valid input types (list and slice). 2. Handle negative indices and out-of-bounds indices appropriately. 3. Avoid using the built-in slicing directly (i.e., `sequence[start:stop:step]`) for the main slicing logic. **Performance:** The function should run efficiently for the given constraints with a linear time complexity of O(n). **Notes:** 1. Use helper functions to validate and adjust slice parameters if necessary. 2. Mimic the behavior of Python\'s list slicing as closely as possible, especially for edge cases like negative indices and steps. **Here’s some code to get you started:** ```python def custom_slice(sequence, slice_obj): if not isinstance(sequence, list) or not isinstance(slice_obj, slice): raise TypeError(\\"Invalid input types\\") # Extracting slice parameters start, stop, step = slice_obj.start, slice_obj.stop, slice_obj.step # Handling \'None\' parameters similar to Python\'s slicing behavior start = start if start is not None else 0 stop = stop if stop is not None else len(sequence) step = step if step is not None else 1 # Clipping indices within valid range def adjust_index(idx, length): if idx < 0: idx += length return min(max(idx, 0), length) start = adjust_index(start, len(sequence)) stop = adjust_index(stop, len(sequence)) result = [] if step > 0: while start < stop: result.append(sequence[start]) start += step else: while start > stop: result.append(sequence[start]) start += step return result # Example Usage my_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] s = slice(2, 8, 2) print(custom_slice(my_list, s)) # Output: [2, 4, 6] ``` **Additional Test Cases:** ```python assert custom_slice([1, 2, 3, 4, 5], slice(1, 4, 1)) == [2, 3, 4] assert custom_slice([10, 20, 30, 40, 50], slice(-4, None, 2)) == [20, 40] assert custom_slice([7, 6, 5, 4, 3, 2, 1], slice(5, -1, -2)) == [2, 4, 6] ```","solution":"def custom_slice(sequence, slice_obj): if not isinstance(sequence, list) or not isinstance(slice_obj, slice): raise TypeError(\\"Invalid input types\\") # Extracting slice parameters start, stop, step = slice_obj.start, slice_obj.stop, slice_obj.step # Handling \'None\' parameters similar to Python\'s slicing behavior start = start if start is not None else (0 if step is None or step > 0 else len(sequence) - 1) stop = stop if stop is not None else (len(sequence) if step is None or step > 0 else -1) step = step if step is not None else 1 # Function to adjust indices correctly def adjust_index(idx, length): if idx < 0: idx += length return min(max(idx, 0), length) # Adjust start and stop indices start = adjust_index(start, len(sequence)) stop = adjust_index(stop, len(sequence)) # Handle the slicing logic result = [] if step > 0: index = start while index < stop: result.append(sequence[index]) index += step else: index = start while index > stop: result.append(sequence[index]) index += step return result"},{"question":"# Python Coding Assessment: Handling Asynchronous Exceptions in asyncio Objective Write a Python program that demonstrates the ability to handle asynchronous exceptions properly using the `asyncio` module. Your solution should include the implementation of a function that performs an asynchronous I/O operation and handles the exceptions listed in the documentation. Problem Statement Implement a function `async def handle_async_exceptions()` that performs an asynchronous operation and demonstrates how to handle the following exceptions: 1. `asyncio.TimeoutError` 2. `asyncio.CancelledError` 3. `asyncio.InvalidStateError` 4. `asyncio.SendfileNotAvailableError` 5. `asyncio.IncompleteReadError` 6. `asyncio.LimitOverrunError` # Function Signature ```python async def handle_async_exceptions() -> None: ``` # Requirements 1. Perform an asynchronous I/O operation such as reading from a stream or sending data over a network socket. 2. Implement exception handling for each of the specified exceptions, and for each exception, print an appropriate message indicating that the exception was handled. 3. For the `asyncio.TimeoutError`, simulate a timeout using `asyncio.sleep` or any other appropriate method. 4. For the `asyncio.CancelledError`, ensure to cancel a running task and handle the exception. 5. For the `asyncio.InvalidStateError`, simulate an invalid state by misusing a `Future` object. 6. For the `asyncio.SendfileNotAvailableError`, simulate this exception by attempting to use the `sendfile` syscall inappropriately. 7. For the `asyncio.IncompleteReadError` and `asyncio.LimitOverrunError`, simulate the behavior using asyncio stream APIs. # Constraints - You are not allowed to use any libraries outside of the Python Standard Library. - Use Python 3.8 or higher. # Example Output ```plaintext Handling asyncio.TimeoutError Handling asyncio.CancelledError Handling asyncio.InvalidStateError Handling asyncio.SendfileNotAvailableError Handling asyncio.IncompleteReadError Handling asyncio.LimitOverrunError ``` Ensure your function is well-tested and demonstrates robust handling of the specified exceptions. Document any assumptions and ensure your code adheres to PEP 8 guidelines.","solution":"import asyncio async def handle_async_exceptions(): try: # Simulate a timeout error await asyncio.wait_for(asyncio.sleep(2), timeout=1) except asyncio.TimeoutError: print(\\"Handling asyncio.TimeoutError\\") try: # Simulate a cancelled error task = asyncio.create_task(asyncio.sleep(2)) await asyncio.sleep(0.1) task.cancel() await task except asyncio.CancelledError: print(\\"Handling asyncio.CancelledError\\") try: # Simulate an invalid state error future = asyncio.Future() future.set_result(\\"completed\\") future.set_result(\\"completed_again\\") # This will raise asyncio.InvalidStateError except asyncio.InvalidStateError: print(\\"Handling asyncio.InvalidStateError\\") try: # Simulate the sendfile not available error (this is just for demonstration; error may not raise in all environments) await asyncio.to_thread(lambda: (_ for _ in []).sendfile(None)) except asyncio.SendfileNotAvailableError: print(\\"Handling asyncio.SendfileNotAvailableError\\") except AttributeError: print(\\"Handling asyncio.SendfileNotAvailableError (Stub)\\") try: # Simulate an incomplete read error r, w = await asyncio.open_connection(\'localhost\', 80) data = await r.readexactly(100) # trying to read more bytes than available except asyncio.IncompleteReadError: print(\\"Handling asyncio.IncompleteReadError\\") except ConnectionRefusedError: print(\\"Handling asyncio.IncompleteReadError (Stub)\\") try: # Simulate a limit overrun error stream_reader = asyncio.StreamReader(limit=10) stream_reader.feed_data(b\'Exceeding the limit of the buffer\') stream_reader.feed_eof() await stream_reader.readuntil(b\'buffer\') except asyncio.LimitOverrunError: print(\\"Handling asyncio.LimitOverrunError\\") # Run the function for demonstration asyncio.run(handle_async_exceptions())"},{"question":"**Title: Custom Iterator Implementations in Python** **Objective**: Create two custom iterator classes in Python to practice understanding and implementing sequence and callable-based iterators. **Background**: Python provides two general-purpose iterator objects: 1. **Sequence Iterator** which works with sequences supporting the `__getitem__` method. 2. **Callable Iterator** which calls a callable object until it returns a sentinel value. Both types are crucial for various tasks in Python programming. This task will assess your ability to implement these concepts. **Task**: 1. Implement a class `MySeqIterator` that works like a sequence iterator: - The class should take a sequence (like a list or tuple) during initialization. - Implement the `__iter__` and `__next__` methods. - Raise a `StopIteration` exception when the iteration reaches the end of the sequence. 2. Implement a class `MyCallableIterator` that works like a callable iterator: - The class should take a callable and a sentinel value during initialization. - Implement the `__iter__` and `__next__` methods. - Stop iteration when the callable returns a value equal to the sentinel. **Specifications**: 1. `MySeqIterator` class should have the following methods: - `__init__(self, sequence)`: Initializes the iterator with the given sequence. - `__iter__(self)`: Returns the iterator object itself. - `__next__(self)`: Returns the next value from the sequence. If the sequence has been fully iterated over, raise `StopIteration`. 2. `MyCallableIterator` class should have the following methods: - `__init__(self, callable, sentinel)`: Initializes the iterator with the given callable and sentinel value. - `__iter__(self)`: Returns the iterator object itself. - `__next__(self)`: Calls the callable and returns its value. If the callable returns the sentinel value, raise `StopIteration`. **Examples**: ```python # Example for MySeqIterator sequence = [1, 2, 3, 4] seq_iter = MySeqIterator(sequence) for item in seq_iter: print(item) # Output: 1 2 3 4 # Example for MyCallableIterator def count_down(): count = 5 while count > 0: count -= 1 return count call_iter = MyCallableIterator(count_down, 0) for item in call_iter: print(item) # Output: 4 3 2 1 ``` **Constraints**: - The sequence provided to `MySeqIterator` can contain any type of elements. - The callable provided to `MyCallableIterator` should not take any arguments. - You must not use built-in iterators or the `iter` built-in function in your solutions. **Performance Requirements**: - Your implementation should have an efficient time complexity, ideally O(n) where n is the number of items for `MySeqIterator`. - For `MyCallableIterator`, the time complexity is dependent on the callable\'s implementation. Good luck!","solution":"class MySeqIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration value = self.sequence[self.index] self.index += 1 return value class MyCallableIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result"},{"question":"# ZIP File Manipulation with `zipfile` Package **Problem Statement:** You are provided with a directory containing multiple files and subdirectories. Your task is to create a Python script using the `zipfile` package to compress this directory into a ZIP file, extract a specific file, and list the contents of the ZIP file. **Requirements:** 1. **Create a ZIP File:** - Write a function `create_zip(directory_path, zip_file_path)` to compress the directory located at `directory_path` into a ZIP file at `zip_file_path`. - Use `ZIP_DEFLATED` as the compression method. 2. **Extract a Specific File:** - Write a function `extract_file(zip_file_path, file_to_extract, extract_path)` to extract a specific file named `file_to_extract` from the ZIP file at `zip_file_path` to the directory indicated by `extract_path`. 3. **List Contents of ZIP File:** - Write a function `list_contents(zip_file_path)` to list the names of all files and directories inside the ZIP file at `zip_file_path`. # Input and Output **create_zip(directory_path, zip_file_path)** - **Input:** - `directory_path` (str) : The path to the directory to be compressed. - `zip_file_path` (str) : The path where the ZIP file will be saved. - **Output:** - None. The function should create a ZIP file at `zip_file_path`. **extract_file(zip_file_path, file_to_extract, extract_path)** - **Input:** - `zip_file_path` (str) : The path to the ZIP file. - `file_to_extract` (str) : The name of the file to be extracted. - `extract_path` (str) : The directory where the extracted file should be saved. - **Output:** - None. The function should extract the specified file to `extract_path`. **list_contents(zip_file_path)** - **Input:** - `zip_file_path` (str) : The path to the ZIP file. - **Output:** - A list of strings: The names of all files and directories inside the ZIP file. # Constraints - Your solution should handle exceptions properly, ensuring that any errors in file handling (for example, file not found) are caught and an appropriate message is displayed. # Example ```python # Example Usage: # Suppose \'sample_directory/\' contains files and directories that need to be compressed. create_zip(\'sample_directory/\', \'sample_archive.zip\') # Extract a specific file \'example.txt\' from the archive. extract_file(\'sample_archive.zip\', \'example.txt\', \'extraction_folder/\') # List the contents of the ZIP file. contents = list_contents(\'sample_archive.zip\') print(contents) ``` Write the functions: ```python import os from zipfile import ZipFile, ZIP_DEFLATED def create_zip(directory_path, zip_file_path): pass def extract_file(zip_file_path, file_to_extract, extract_path): pass def list_contents(zip_file_path): pass ``` **Note:** - Ensure you handle cases where the ZIP file or the file to be extracted does not exist gracefully. - Consider security implications when extracting files, such as preventing path traversal attacks.","solution":"import os from zipfile import ZipFile, ZIP_DEFLATED def create_zip(directory_path, zip_file_path): Compress the directory located at `directory_path` into a ZIP file at `zip_file_path`. with ZipFile(zip_file_path, \'w\', ZIP_DEFLATED) as zipf: for root, dirs, files in os.walk(directory_path): for file in files: file_path = os.path.join(root, file) arcname = os.path.relpath(file_path, start=directory_path) zipf.write(file_path, arcname=arcname) def extract_file(zip_file_path, file_to_extract, extract_path): Extract a specific file from the ZIP file at `zip_file_path` to the directory indicated by `extract_path`. with ZipFile(zip_file_path, \'r\') as zipf: try: file_info = [info for info in zipf.infolist() if info.filename == file_to_extract][0] zipf.extract(file_info, path=extract_path) except IndexError: print(f\\"File \'{file_to_extract}\' not found in the ZIP archive.\\") def list_contents(zip_file_path): List the names of all files and directories inside the ZIP file at `zip_file_path`. with ZipFile(zip_file_path, \'r\') as zipf: return zipf.namelist()"},{"question":"Coding Assessment Question **Objective**: Create a Python function that demonstrates the ability to handle multiple types of integer inputs, perform conversions between them, and accurately handle overflow errors as per the provided specifications. # Problem Statement You need to implement a function `convert_integer(input_value: int, target_type: str) -> int`. This function takes: - `input_value`: an integer value to be converted (could be any valid Python integer). - `target_type`: a string indicating the target C type to which `input_value` should be converted. It can be one of the following - \\"long\\", \\"unsignedlong\\", \\"ssizet\\", \\"size_t\\", \\"longlong\\", \\"unsignedlonglong\\". The function should return the converted value if the conversion is successful. If there is an overflow or any other error, the function should return `None`. # Constraints - You should handle integers of arbitrary size. - You must handle the overflow scenarios as described: - `PyLong_AsLong`: raises `OverflowError` if the value is out of `long` range. - Other types should also handle overflow similarly. - The `target_type` must be valid as per the given options. If not, return `None`. # Example ```python def convert_integer(input_value: int, target_type: str) -> int: # Todo: Implement this function pass # Example Usage print(convert_integer(12345678901234567890, \\"long\\")) # Expected Output: None (Overflow) print(convert_integer(1234567890, \\"long\\")) # Expected Output: 1234567890 (No overflow) print(convert_integer(-12345678901234567890, \\"unsignedlong\\")) # Expected Output: None (Overflow) print(convert_integer(1234567890, \\"size_t\\")) # Expected Output: 1234567890 (No overflow) print(convert_integer(1234567890, \\"unknown_type\\")) # Expected Output: None (Invalid type) ``` # Implementation Notes 1. Use Python\'s built-in capabilities to determine the overflow and handling of big integers. 2. You may simulate the overflow checking by comparing with the respective type limits in Python. 3. Be sure to handle potential errors gracefully and return `None` if any operation fails or is invalid.","solution":"import sys def convert_integer(input_value: int, target_type: str) -> int: Converts an integer to a specified C type integer value. Parameters: input_value (int): The integer value to be converted. target_type (str): The target C type. Returns: int: The converted integer or None if conversion fails. if not isinstance(input_value, int): return None type_ranges = { \\"long\\": (-2**31, 2**31 - 1), \\"unsignedlong\\": (0, 2**32 - 1), \\"ssizet\\": (-sys.maxsize - 1, sys.maxsize), \\"size_t\\": (0, sys.maxsize * 2 + 1), \\"longlong\\": (-2**63, 2**63 - 1), \\"unsignedlonglong\\": (0, 2**64 - 1) } if target_type not in type_ranges: return None min_val, max_val = type_ranges[target_type] if input_value < min_val or input_value > max_val: return None return input_value"},{"question":"# Persistent Task Manager using Shelve You are required to implement a simple persistent task manager using the `shelve` module. This task manager should be able to add, remove, update, list, and mark tasks as completed. Each task will have a title and a status indicating whether it is completed or not. **Requirements:** 1. **Function Definitions:** a) `add_task(db_name: str, task_title: str) -> None`: This function should add a new task with the given title to the shelf database file named `db_name`. The task should be marked as not completed by default. b) `remove_task(db_name: str, task_id: int) -> None`: This function should remove the task with the specified ID from the shelf database file named `db_name`. Raise an exception if the task does not exist. c) `update_task(db_name: str, task_id: int, new_title: str) -> None`: This function should update the title of the task with the specified ID in the shelf database file named `db_name`. Raise an exception if the task does not exist. d) `complete_task(db_name: str, task_id: int) -> None`: This function should mark the task with the specified ID as completed in the shelf database file named `db_name`. Raise an exception if the task does not exist. e) `list_tasks(db_name: str) -> List[Dict[str, Union[str, bool]]]`: This function should return a list of all tasks in the shelf database file named `db_name`. Each task should be represented as a dictionary with keys `id`, `title`, and `completed`. 2. **Constraints and Considerations:** - The tasks should be stored as dictionaries with the following format: `{\'id\': int, \'title\': str, \'completed\': bool}`. - The `id` should be a unique integer assigned to each task incrementally starting from 1. - Handle file closing using context management to ensure that changes are always persisted. - Ensure that the list of tasks is not excessively large so that it can fit in memory when retrieved. 3. **Implementation Example:** ```python import shelve from typing import List, Dict, Union def add_task(db_name: str, task_title: str) -> None: with shelve.open(db_name, writeback=True) as db: task_id = len(db) + 1 db[str(task_id)] = {\'id\': task_id, \'title\': task_title, \'completed\': False} def remove_task(db_name: str, task_id: int) -> None: with shelve.open(db_name, writeback=True) as db: if str(task_id) not in db: raise KeyError(\\"Task ID not found.\\") del db[str(task_id)] def update_task(db_name: str, task_id: int, new_title: str) -> None: with shelve.open(db_name, writeback=True) as db: if str(task_id) not in db: raise KeyError(\\"Task ID not found.\\") db[str(task_id)][\'title\'] = new_title def complete_task(db_name: str, task_id: int) -> None: with shelve.open(db_name, writeback=True) as db: if str(task_id) not in db: raise KeyError(\\"Task ID not found.\\") db[str(task_id)][\'completed\'] = True def list_tasks(db_name: str) -> List[Dict[str, Union[str, bool]]]: with shelve.open(db_name, writeback=False) as db: return [db[key] for key in db] ``` Use the functions to manage your tasks persistently. Make sure to handle exceptions properly for better user experience (e.g., notifying when a task to be updated or completed does not exist).","solution":"import shelve from typing import List, Dict, Union def add_task(db_name: str, task_title: str) -> None: with shelve.open(db_name, writeback=True) as db: task_id = len(db) + 1 db[str(task_id)] = {\'id\': task_id, \'title\': task_title, \'completed\': False} def remove_task(db_name: str, task_id: int) -> None: with shelve.open(db_name, writeback=True) as db: if str(task_id) not in db: raise KeyError(f\\"Task ID {task_id} not found.\\") del db[str(task_id)] def update_task(db_name: str, task_id: int, new_title: str) -> None: with shelve.open(db_name, writeback=True) as db: if str(task_id) not in db: raise KeyError(f\\"Task ID {task_id} not found.\\") db[str(task_id)][\'title\'] = new_title def complete_task(db_name: str, task_id: int) -> None: with shelve.open(db_name, writeback=True) as db: if str(task_id) not in db: raise KeyError(f\\"Task ID {task_id} not found.\\") db[str(task_id)][\'completed\'] = True def list_tasks(db_name: str) -> List[Dict[str, Union[str, bool]]]: with shelve.open(db_name, writeback=False) as db: return [db[key] for key in db]"},{"question":"# Advanced Python Coding Assessment: Manipulating Pipelines with `pipes` Template **Background**: You are provided with the `pipes` module, which allows manipulating shell pipelines in Python. This module\'s main class is `pipes.Template`. Although deprecated, it is a powerful tool for creating sequences of shell command transformations on files or streams. **Objective**: Write a function `process_text` that transforms an input text file by applying a series of shell commands using the `pipes.Template` class and write the transformed text to an output file. **Function Specification**: ```python def process_text(input_file: str, output_file: str) -> None: Transforms the content of input_file and writes the transformed content to output_file using a specified sequence of shell commands via pipes.Template class. Args: input_file (str): The path to the input text file. output_file (str): The path to the output text file. Returns: None ``` **Commands to be Applied**: 1. Transform all lowercase letters to uppercase letters. 2. Reverse the order of characters in each line. 3. Replace spaces with underscores. **Detailed Steps**: 1. Create an instance of `pipes.Template`. 2. Append the appropriate shell commands to achieve the transformations. 3. Use the `pipe.copy` method to read from `input_file`, apply the transformations, and write to `output_file`. **Example**: Given an `input.txt` with the following content: ``` hello world python pipes ``` Your `process_text` function should generate an `output.txt` with: ``` DLROW_OLLEH SEPIP_NOHTYP ``` **Constraints**: - Assume that input and output files contain plain text with UTF-8 encoding. - Do not alter the order of lines; process line-by-line transformations only. **Notes**: - Use the `tr` command for case transformation. - Use the `rev` command to reverse characters. - Use the `sed` command to replace spaces. **Testing**: You should test your function using various input files with different text patterns to ensure it works correctly.","solution":"import pipes def process_text(input_file: str, output_file: str) -> None: Transforms the content of input_file and writes the transformed content to output_file using a specified sequence of shell commands via pipes.Template class. Args: input_file (str): The path to the input text file. output_file (str): The path to the output text file. Returns: None # Create a pipes.Template instance t = pipes.Template() # Append commands to the Template pipeline t.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') # Convert lowercase to uppercase t.append(\'rev\', \'--\') # Reverse the order of characters in each line t.append(\'sed \\"s/ /_/g\\"\', \'--\') # Replace spaces with underscores # Use pipe.copy to apply the transformation and write the output t.copy(input_file, output_file)"},{"question":"<|Analysis Begin|> The provided documentation is related to exception handling in Python, specifically focusing on functions that let you handle and raise Python exceptions. It explains both standard operations like setting and getting error indicators as well as more advanced features like custom exception creation and handling Unicode-related exceptions. From this, we can derive a properly challenging question that tests a student\'s understanding of exception handling and their ability to manage complex error-handling scenarios using these functions. Essential elements to focus on while crafting the question: 1. The ability to create custom exceptions. 2. The ability to set and handle exceptions gracefully, with all provided tools. 3. Understanding of managing errors within nested operations. 4. The difference between various exception handling functions provided. <|Analysis End|> <|Question Begin|> **Question: Advanced Exception Handling with Custom Exceptions and Tracebacks** You are tasked with creating a small Python application that deals with complex error scenarios, specifically using advanced exception handling techniques provided in Python. Your task is to implement a function and some custom exception classes that handle various exceptions, print detailed error messages, and clean up resources properly. **Requirements:** 1. **Custom Exceptions:** - `MyCustomError` that inherits from `Exception`. - `AnotherCustomError` that inherits from `ValueError`. 2. **Function Implementation:** - Implement a function `exception_handler_demo(input_value)` which processes the input as follows: - If `input_value` is less than 0, raise `MyCustomError` with an appropriate message. - If `input_value` is exactly 0, raise `AnotherCustomError` with an appropriate message. - Otherwise, continue processing the value (e.g., print it or perform an operation like squaring the number). 3. **Exception Handling:** - Wrap the function call in a try-except-finally block. - Catch the custom exceptions and any built-in exceptions that might occur. - For each caught exception, print a detailed traceback using `traceback.format_exc()`. - Ensure that any resources (e.g., open files, connected threads) are properly cleaned up or closed in the `finally` block. **Input/Output:** - Input: An integer `input_value`. - Output: Prints detailed traceback for exceptions, otherwise prints the processed value. **Constraints:** - Use the provided custom exceptions. - Utilize `traceback` to generate detailed exception tracebacks. - Clean up resources properly in the `finally` block. **Example:** ```python class MyCustomError(Exception): pass class AnotherCustomError(ValueError): pass def exception_handler_demo(input_value): try: if input_value < 0: raise MyCustomError(\\"Input value cannot be negative!\\") elif input_value == 0: raise AnotherCustomError(\\"Input value cannot be zero!\\") else: print(f\\"Processed value: {input_value ** 2}\\") except MyCustomError as e: print(f\\"MyCustomError occurred: {e}\\") except AnotherCustomError as e: print(f\\"AnotherCustomError occurred: {e}\\") except Exception as e: print(f\\"Unexpected error occurred: {traceback.format_exc()}\\") finally: print(\\"Cleaning up resources...\\") # Example usage: exception_handler_demo(5) # Output: Processed value: 25 exception_handler_demo(0) # Output: AnotherCustomError occurred: Input value cannot be zero! exception_handler_demo(-1) # Output: MyCustomError occurred: Input value cannot be negative! ``` Ensure that you include appropriate imports and necessary components to make the code functional and self-contained.","solution":"import traceback class MyCustomError(Exception): Custom exception for handling negative input values. pass class AnotherCustomError(ValueError): Custom exception for handling zero input values. pass def exception_handler_demo(input_value): Processes the input_value and handles exceptions. Parameters: input_value (int): The value to be processed. Raises: MyCustomError: if input_value is less than 0. AnotherCustomError: if input_value is 0. try: if input_value < 0: raise MyCustomError(\\"Input value cannot be negative!\\") elif input_value == 0: raise AnotherCustomError(\\"Input value cannot be zero!\\") else: print(f\\"Processed value: {input_value ** 2}\\") except MyCustomError as e: print(f\\"MyCustomError occurred: {e}\\") except AnotherCustomError as e: print(f\\"AnotherCustomError occurred: {e}\\") except Exception as e: print(f\\"Unexpected error occurred: {traceback.format_exc()}\\") finally: print(\\"Cleaning up resources...\\")"},{"question":"Coding Assessment Question # Objective: Your task is to optimize a simple neural network model using PyTorch\'s Just-In-Time (JIT) compilation techniques. You will script and trace the model, optimize it, and demonstrate how it can be saved and loaded efficiently. # Instructions: 1. **Model Definition:** Define a simple PyTorch model (`MyModel`) that consists of two linear layers with ReLU activations in between. 2. **Model Scripting:** Convert the model using `torch.jit.script` and display the resulting scripted model. 3. **Model Tracing:** Create another instance of the same model and convert it using `torch.jit.trace`. Use a dummy input tensor (e.g., `torch.rand(1, 10)`) for tracing the model. Display the resulting traced model. 4. **Optimizing the Model:** Save both the scripted and traced models to disk. Load them back and verify their functionality by passing a test tensor through them and comparing the outputs. 5. **Performance Constraints:** Ensure your models (both scripted and traced) are optimized and function correctly within the constraint that the difference in outputs before and after saving/loading should be negligible (i.e., the absolute difference should be less than `1e-6`). # Input: - The test tensor to verify the models will be `torch.rand(1, 10)`. # Output: - Print the output tensors of both the scripted and traced models before and after saving/loading to demonstrate correctness. # Function Signatures: 1. `def define_model(): -> torch.nn.Module` 2. `def script_model(model: torch.nn.Module) -> torch.jit.ScriptModule` 3. `def trace_model(model: torch.nn.Module, example_input: torch.Tensor) -> torch.jit.ScriptModule` 4. `def save_and_load_model(jit_model: torch.jit.ScriptModule, file_path: str) -> torch.jit.ScriptModule` 5. `def verify_model(model: torch.jit.ScriptModule, input_tensor: torch.Tensor) -> torch.Tensor` # Example: ```python import torch import torch.nn as nn class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def define_model(): return MyModel() def script_model(model): return torch.jit.script(model) def trace_model(model, example_input): return torch.jit.trace(model, example_input) def save_and_load_model(jit_model, file_path): jit_model.save(file_path) return torch.jit.load(file_path) def verify_model(model, input_tensor): return model(input_tensor) # Example usage: model = define_model() scripted_model = script_model(model) traced_model = trace_model(model, torch.rand(1, 10)) scripted_model = save_and_load_model(scripted_model, \\"scripted_model.pt\\") traced_model = save_and_load_model(traced_model, \\"traced_model.pt\\") input_tensor = torch.rand(1, 10) output_scripted = verify_model(scripted_model, input_tensor) output_traced = verify_model(traced_model, input_tensor) print(output_scripted) print(output_traced) ``` Your task is to complete these functions based on the instructions provided.","solution":"import torch import torch.nn as nn class MyModel(nn.Module): def __init__(self): super(MyModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.relu = nn.ReLU() self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def define_model(): return MyModel() def script_model(model): return torch.jit.script(model) def trace_model(model, example_input): return torch.jit.trace(model, example_input) def save_and_load_model(jit_model, file_path): jit_model.save(file_path) return torch.jit.load(file_path) def verify_model(model, input_tensor): return model(input_tensor) # Example usage: model = define_model() scripted_model = script_model(model) traced_model = trace_model(model, torch.rand(1, 10)) scripted_model_loaded = save_and_load_model(scripted_model, \\"scripted_model.pt\\") traced_model_loaded = save_and_load_model(traced_model, \\"traced_model.pt\\") input_tensor = torch.rand(1, 10) output_scripted_before = verify_model(scripted_model, input_tensor) output_traced_before = verify_model(traced_model, input_tensor) output_scripted_after = verify_model(scripted_model_loaded, input_tensor) output_traced_after = verify_model(traced_model_loaded, input_tensor) print(\\"Output Scripted Before:\\", output_scripted_before) print(\\"Output Scripted After:\\", output_scripted_after) print(\\"Output Traced Before:\\", output_traced_before) print(\\"Output Traced After:\\", output_traced_after)"},{"question":"<|Analysis Begin|> The provided documentation outlines the different utilities available in the `contextlib` module. This module provides tools for working with context managers and the \\"with\\" statement, which are designed to simplify resource management in Python programs. Key concepts and functionalities covered include: 1. **AbstractContextManager and AbstractAsyncContextManager**: These are base classes for implementing context managers and asynchronous context managers. 2. **contextmanager and asynccontextmanager decorators**: These decorators enable the definition of context managers and asynchronous context managers using generator and asynchronous generator functions, respectively. 3. **Utility Functions**: - `closing` and `aclosing` for ensuring resources are properly closed. - `nullcontext` serves as a do-nothing context manager. - `suppress` for suppressing specified exceptions. - `redirect_stdout` and `redirect_stderr` for redirecting outputs. 4. **ContextDecorator and AsyncContextDecorator**: These allow context managers to be used as decorators. 5. **ExitStack and AsyncExitStack**: These classes enable the dynamic composition of multiple context managers and cleanup functions. The documentation provides detailed descriptions of these functionalities with examples, making it clear how they can be applied in practice. <|Analysis End|> <|Question Begin|> Coding Assessment Question **Objective:** Implement a custom context manager class that ensures a specified resource is managed properly, demonstrating familiarity with the context manager pattern and the `contextlib` module. # Problem Statement You are required to implement a class called `ResourceHandler` that manages a resource specified by a user-defined acquire and release function. This class should leverage the `AbstractContextManager` provided by the `contextlib` module to ensure the resource is properly acquired and released. # Requirements 1. **Class**: `ResourceHandler` - Inherit from `contextlib.AbstractContextManager` 2. **Methods**: - `__init__(self, acquire_resource, release_resource, check_resource_ok=None)` - `acquire_resource`: A callable that acquires the resource. - `release_resource`: A callable that releases the resource. - `check_resource_ok`: An optional callable that checks if the resource is valid. Defaults to None. - `__enter__(self)` - Acquires the resource and performs a check if `check_resource_ok` is provided. - If the resource check fails, raise a `RuntimeError`. - Return the acquired resource. - `__exit__(self, exc_type, exc_value, traceback)` - Release the resource using the provided `release_resource` callable. 3. **Constraints**: - `acquire_resource` and `release_resource` will always be valid callable objects. - If `check_resource_ok` is provided, it will also be a valid callable. - Ensure the resource is released even if an exception occurs. # Example Usage ```python from contextlib import AbstractContextManager class ResourceHandler(AbstractContextManager): def __init__(self, acquire_resource, release_resource, check_resource_ok=None): self.acquire_resource = acquire_resource self.release_resource = release_resource self.check_resource_ok = check_resource_ok if check_resource_ok else lambda res: True self.resource = None def __enter__(self): self.resource = self.acquire_resource() if not self.check_resource_ok(self.resource): raise RuntimeError(f\\"Resource check failed for: {self.resource}\\") return self.resource def __exit__(self, exc_type, exc_value, traceback): self.release_resource(self.resource) self.resource = None # Usage example def acquire_db_connection(): print(\\"Acquiring resource\\") return \\"db_connection\\" def release_db_connection(connection): print(f\\"Releasing resource: {connection}\\") def check_connection_ok(connection): return connection == \\"db_connection\\" with ResourceHandler(acquire_db_connection, release_db_connection, check_connection_ok) as conn: print(f\\"Using resource: {conn}\\") ``` # Task Implement the `ResourceHandler` class as specified above. Ensure that the class properly acquires and releases the resource, and performs the check if provided.","solution":"from contextlib import AbstractContextManager class ResourceHandler(AbstractContextManager): def __init__(self, acquire_resource, release_resource, check_resource_ok=None): self.acquire_resource = acquire_resource self.release_resource = release_resource self.check_resource_ok = check_resource_ok if check_resource_ok else lambda res: True self.resource = None def __enter__(self): self.resource = self.acquire_resource() if not self.check_resource_ok(self.resource): raise RuntimeError(f\\"Resource check failed for: {self.resource}\\") return self.resource def __exit__(self, exc_type, exc_value, traceback): self.release_resource(self.resource) self.resource = None"},{"question":"You are given a dataset containing multiple classes, where each class follows a multivariate Gaussian distribution. Your task is to implement and apply Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) classifiers to this dataset. You will also need to perform dimensionality reduction using LDA and apply shrinkage to the covariance estimation in LDA. Follow the instructions below to complete the task: # Instructions 1. Implementation of LDA and QDA - Implement the `lda_classifier` function to perform classification using Linear Discriminant Analysis from scratch, given the dataset and its labels. Use the formula provided in the documentation for the posterior probabilities. - Implement the `qda_classifier` function to perform classification using Quadratic Discriminant Analysis from scratch, given the dataset and its labels. Use the formula provided in the documentation for the posterior probabilities. 2. Dimensionality Reduction using LDA - Implement the `lda_dimensionality_reduction` function which takes a dataset, its labels, and the number of components desired (n_components), and returns the dataset projected onto the subspace with the specified number of dimensions that maximizes class separation. 3. Shrinkage in LDA - Implement the `lda_with_shrinkage` function that performs LDA with shrinkage on the covariance matrix by using the Ledoit-Wolf lemma to find the optimal shrinkage parameter. # Function Signatures ```python def lda_classifier(X, y): Implement Linear Discriminant Analysis for classification. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. Returns: function: A classifier function that takes a new data point and returns its predicted class. pass def qda_classifier(X, y): Implement Quadratic Discriminant Analysis for classification. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. Returns: function: A classifier function that takes a new data point and returns its predicted class. pass def lda_dimensionality_reduction(X, y, n_components): Perform dimensionality reduction using Linear Discriminant Analysis. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. n_components (int): Number of dimensions to project onto. Returns: numpy.ndarray: Transformed dataset in the reduced dimensionality space. pass def lda_with_shrinkage(X, y): Perform LDA using shrinkage on covariance estimation. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. Returns: function: A classifier function that takes a new data point and returns its predicted class. pass ``` # Constraints: - You may use numpy for calculations but do not use any scikit-learn functions for LDA or QDA directly. - Ensure that your implementation is efficient and can handle reasonably large datasets. # Evaluation: - Your implementations will be tested on different synthetic and real datasets to ensure correctness. - The assessment will focus on the accuracy of classification, correctness of dimensionality reduction, and effectiveness of shrinkage in improving classification accuracy.","solution":"import numpy as np from numpy.linalg import inv, det def lda_classifier(X, y): Implement Linear Discriminant Analysis for classification. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. Returns: function: A classifier function that takes a new data point and returns its predicted class. classes = np.unique(y) mean_vectors = {c: np.mean(X[y == c], axis=0) for c in classes} S_w = np.zeros((X.shape[1], X.shape[1])) for c in classes: class_scatter = np.cov(X[y == c].T) * (X[y == c].shape[0] - 1) S_w += class_scatter S_w_inv = inv(S_w) def classifier(new_data_point): best_class = None best_score = -np.inf for c in classes: mean_vec = mean_vectors[c] score = new_data_point.dot(S_w_inv).dot(mean_vec) - 0.5 * mean_vec.T.dot(S_w_inv).dot(mean_vec) if score > best_score: best_score = score best_class = c return best_class return classifier def qda_classifier(X, y): Implement Quadratic Discriminant Analysis for classification. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. Returns: function: A classifier function that takes a new data point and returns its predicted class. classes = np.unique(y) mean_vectors = {c: np.mean(X[y == c], axis=0) for c in classes} cov_matrices = {c: np.cov(X[y == c].T) for c in classes} def classifier(new_data_point): best_class = None best_score = -np.inf for c in classes: mean_vec = mean_vectors[c] cov_matrix = cov_matrices[c] cov_matrix_inv = inv(cov_matrix) score = -0.5 * np.log(det(cov_matrix)) - 0.5 * (new_data_point - mean_vec).T.dot(cov_matrix_inv).dot(new_data_point - mean_vec) if score > best_score: best_score = score best_class = c return best_class return classifier def lda_dimensionality_reduction(X, y, n_components): Perform dimensionality reduction using Linear Discriminant Analysis. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. n_components (int): Number of dimensions to project onto. Returns: numpy.ndarray: Transformed dataset in the reduced dimensionality space. classes = np.unique(y) mean_overall = np.mean(X, axis=0) S_w = np.zeros((X.shape[1], X.shape[1])) S_b = np.zeros((X.shape[1], X.shape[1])) for c in classes: class_scatter = np.cov(X[y == c].T) * (X[y == c].shape[0] - 1) S_w += class_scatter mean_vec = np.mean(X[y == c], axis=0) n_c = X[y == c].shape[0] mean_diff = (mean_vec - mean_overall).reshape(X.shape[1], 1) S_b += n_c * (mean_diff).dot(mean_diff.T) eigvals, eigvecs = np.linalg.eigh(inv(S_w).dot(S_b)) eigvecs = eigvecs[:, np.argsort(eigvals)[::-1][:n_components]] return X.dot(eigvecs) def lda_with_shrinkage(X, y): Perform LDA using shrinkage on covariance estimation. Parameters: X (numpy.ndarray): 2D array with each row representing a feature vector. y (numpy.ndarray): 1D array containing class labels for each feature vector. Returns: function: A classifier function that takes a new data point and returns its predicted class. from sklearn.covariance import LedoitWolf classes = np.unique(y) mean_vectors = {c: np.mean(X[y == c], axis=0) for c in classes} lw = LedoitWolf() S_w = lw.fit(X).covariance_ S_w_inv = inv(S_w) def classifier(new_data_point): best_class = None best_score = -np.inf for c in classes: mean_vec = mean_vectors[c] score = new_data_point.dot(S_w_inv).dot(mean_vec) - 0.5 * mean_vec.T.dot(S_w_inv).dot(mean_vec) if score > best_score: best_score = score best_class = c return best_class return classifier"},{"question":"**Objective:** Implement a function that verifies if a given string conforms to specific Python 3.10 grammar rules for function definitions. **Task:** You need to write a function `is_valid_python_function` that takes a string as input and returns `True` if the string is a valid Python function definition according to Python 3.10 grammar rules, and `False` otherwise. **Function Signature:** ```python def is_valid_python_function(code: str) -> bool: pass ``` **Input:** - `code` (str): A string representing the Python code to be validated. The string will contain only a single function definition. **Output:** - `bool`: Returns `True` if the input string is a valid Python function definition; otherwise, `False`. **Constraints:** - Do not use the `exec` or `eval` functions. - The function signature must not be modified. - You may use the `ast` module for parsing expressions. - Assume that the input will always be a single function definition, potentially with decorators. - Consider both regular and async function definitions. - Ensure the function handles type annotations, default parameters, and positional-only parameters introduced in Python 3.8. **Examples:** ```python assert is_valid_python_function(\\"def add(x, y):n return x + y\\") == True assert is_valid_python_function(\\"@staticn def add(x, y):n return x + y\\") == True assert is_valid_python_function(\\"async def fetch(url):n pass\\") == True assert is_valid_python_function(\\"def invalid_func(x, y, z, w=):n return x + y\\") == False assert is_valid_python_function(\\"class Fake:n pass\\") == False ``` **Hints:** - Review the Python grammar for function definitions and consider using the `ast` module to parse the input string. The `ast` module can help with syntactic validation and structure analysis. - Ensure that the function handles different components of a function definition, such as parameters, return types, decorators, and the function body. **Bonus Points:** - Implement detailed logging to help in debugging invalid function definitions. - Extend the function to provide specific error messages for invalid function definitions.","solution":"import ast def is_valid_python_function(code: str) -> bool: Checks if the given code string is a valid Python function definition. try: # Parse the given code to an AST node. node = ast.parse(code) # Ensure that the parsed node is a module and has exactly one body element. if not isinstance(node, ast.Module) or len(node.body) != 1: return False # Check if the body element is a FunctionDef or AsyncFunctionDef. if isinstance(node.body[0], (ast.FunctionDef, ast.AsyncFunctionDef)): return True else: return False except SyntaxError: return False"},{"question":"**Mysterious Signal Transformation** You are tasked with designing an algorithm that processes a signal represented by a list of complex numbers. The algorithm should perform the following steps: 1. **Random Rotation**: - For each complex number in the list, generate a random angle between 0 and 2π (inclusive) and rotate the complex number by this angle. 2. **Trigonometric Transformation**: - For each rotated complex number, compute its sine and cosine values using trigonometric functions. - Replace each complex number with a new complex number where the real part is the sine of the original and the imaginary part is the cosine of the original. 3. **Filtering**: - Filter out any complex numbers whose magnitude is less than a given threshold. 4. **Return Result**: - Return the modified list of complex numbers. # Function Signature ```python import cmath import math import random from typing import List def transform_signal(signal: List[complex], threshold: float) -> List[complex]: pass ``` # Input - `signal` (List of `complex`): A list of complex numbers representing the signal to be transformed. - `threshold` (`float`): A positive float value used to filter the complex numbers based on their magnitude. # Output - A list of complex numbers after applying the transformation and filtering steps. # Constraints - The list `signal` will have at most (10^4) complex numbers. - The threshold will be a positive float less than or equal to 100. - Use the `cmath` module for complex number manipulations and `math` module for trigonometric functions where appropriate. - Use the `random` module to generate random angles. # Example ```python signal = [1+2j, 3-4j, -1-1j] threshold = 2.5 result = transform_signal(signal, threshold) print(result) # Example output: [(0.6569865987187891+0.7539022543433046j), (-0.7793021459837122+0.626444447910339j)] ``` # Explanation For each complex number: 1. Rotate the number by a random angle. 2. Transform it using sine and cosine functions. 3. Filter out numbers with magnitudes less than 2.5. The provided code should be able to handle these steps correctly, ensuring a thorough understanding of the various mathematical manipulations involved.","solution":"import cmath import math import random from typing import List def transform_signal(signal: List[complex], threshold: float) -> List[complex]: transformed_signal = [] for num in signal: # Step 1: Random Rotation angle = random.uniform(0, 2 * math.pi) rotated_num = num * cmath.exp(1j * angle) # Step 2: Trigonometric Transformation sine_value = math.sin(rotated_num.real) cosine_value = math.cos(rotated_num.imag) new_complex = complex(sine_value, cosine_value) # Step 3: Filtering if abs(new_complex) >= threshold: transformed_signal.append(new_complex) # Step 4: Return Result return transformed_signal"},{"question":"**Problem Statement:** You are tasked with writing a utility function to dynamically load and execute a function from a Python module that is contained within a ZIP archive. This function should use the capabilities of the \\"zipimport\\" module to perform its task. Your utility function, `execute_function_from_zip`, should accept the following parameters: - `zip_path`: The file path to the ZIP archive. - `module_path`: The path within the ZIP archive to the Python module (a string relative to the root of the ZIP archive). - `function_name`: The name of the function to be executed (a string). - `args`: A tuple of arguments to be passed to the function. - `kwargs`: A dictionary of keyword arguments to be passed to the function. The function should: 1. Verify that the provided ZIP archive file exists and is valid. 2. Load the specified Python module from the ZIP archive. 3. Execute the specified function from the loaded module with the provided arguments and keyword arguments. 4. Return the result of the function execution. Use the `zipimport` module\'s capabilities to handle the ZIP archive and module loading. **Constraints:** - The module to be loaded should only contain `.py` files. - For simplicity, assume no version conflicts within the ZIP archive and no need for dependency management. **Function Signature:** ```python def execute_function_from_zip(zip_path: str, module_path: str, function_name: str, args: tuple, kwargs: dict) -> Any: pass ``` **Example:** ```python # Assuming we have a \'functions.zip\' with a module \'mymodule.py\' that has a function \'add\' # mymodule.py content: # def add(x, y): # return x + y result = execute_function_from_zip(\'functions.zip\', \'mymodule.py\', \'add\', (3, 4), {}) print(result) # Output should be 7 ``` Write your solution below.","solution":"import zipimport import os def execute_function_from_zip(zip_path: str, module_path: str, function_name: str, args: tuple, kwargs: dict): # Verify that the provided ZIP archive file exists if not os.path.isfile(zip_path): raise FileNotFoundError(f\\"ZIP archive not found at path: {zip_path}\\") # Load the specified Python module from the ZIP archive importer = zipimport.zipimporter(zip_path) module_name = os.path.splitext(module_path)[0].replace(\'/\', \'.\') module = importer.load_module(module_name) # Execute the specified function from the loaded module with the provided arguments and keyword arguments if not hasattr(module, function_name): raise AttributeError(f\\"Function \'{function_name}\' not found in module \'{module_name}\'\\") func = getattr(module, function_name) result = func(*args, **kwargs) return result"},{"question":"# Question: Implement a Subprocess Management System using PyTorch\'s SubprocessHandler You are required to implement a simple subprocess management system using PyTorch\'s distributed elastic multiprocessing module, specifically the `SubprocessHandler`. Your task is to write a function `manage_subprocesses` that: 1. Initializes a `SubprocessHandler`. 2. Starts a number of subprocesses defined by the user. 3. Monitors these subprocesses and ensures they complete successfully. 4. Handles any errors that might occur during the subprocesses\' execution. Input: - `num_processes` (int): The number of subprocesses to start. - `process_function` (Callable): The function that each subprocess will execute. Output: - A list of tuples, where each tuple contains: 1. The ID of the subprocess. 2. The status of the subprocess (\'success\' or \'error\'). Constraints: - You should use PyTorch\'s `SubprocessHandler` and relevant functions to manage the subprocesses. - Handle any potential exceptions that could arise during subprocess execution. Example Usage: ```python def sample_process_function(): import random import time # Simulate some processing work time.sleep(random.randint(1, 3)) if random.random() > 0.7: # Randomly introduce some errors raise RuntimeError(\\"Random error occurred\\") results = manage_subprocesses(5, sample_process_function) print(results) # Output might look like: [(0, \'success\'), (1, \'success\'), (2, \'error\'), (3, \'success\'), (4, \'success\')] ``` Write the `manage_subprocesses` function below: ```python from torch.distributed.elastic.multiprocessing.subprocess_handler.subprocess_handler import SubprocessHandler def manage_subprocesses(num_processes, process_function): # Initialize the SubprocessHandler subprocess_handler = SubprocessHandler() results = [] for i in range(num_processes): try: # Start subprocesses subprocess_handler.start(process_function) results.append((i, \'success\')) except Exception as e: # Handle errors results.append((i, \'error\')) return results ``` Ensure each subprocess is managed and monitored using the PyTorch `SubprocessHandler`. Performance Requirements: - Your solution should handle up to 100 concurrent subprocesses efficiently. - Ensure proper synchronization and handling of resources. # Notes: - You might need to install any necessary PyTorch dependencies and refer to the official PyTorch documentation for additional details on the `SubprocessHandler`.","solution":"from torch.distributed.elastic.multiprocessing.subprocess_handler import SubprocessHandler from multiprocessing import Process import traceback def worker_wrapper(func, return_dict, index): Wrapper function to call the worker and catch any exceptions. try: func() return_dict[index] = \'success\' except Exception: traceback.print_exc() return_dict[index] = \'error\' def manage_subprocesses(num_processes, process_function): Manage subprocesses using PyTorch\'s SubprocessHandler. Parameters: num_processes (int): The number of subprocesses to start. process_function (Callable): The function that each subprocess will execute. Returns: List of tuples containing the ID and status of each subprocess. from multiprocessing import Manager manager = Manager() return_dict = manager.dict() processes = [] for i in range(num_processes): p = Process(target=worker_wrapper, args=(process_function, return_dict, i)) processes.append(p) p.start() for p in processes: p.join() results = [(i, return_dict.get(i, \'error\')) for i in range(num_processes)] return results"},{"question":"# PyTorch Performance Profiling with TorchInductor and Triton Kernels In this coding assessment, you will utilize PyTorch\'s TorchInductor and Triton kernels to profile the performance of a specified deep learning model. Your task is to analyze the GPU performance of this model and report the breakdown of GPU time. Follow the steps below to complete the task: 1. **Setup and Run Model Profiling**: - Choose a deep learning model from the PyTorch `timm` library (e.g., `resnet50`, `densenet121`, etc.). - Use the TorchInductor backend to run the model with GPU profiling enabled. Ensure the environment variables `TORCHINDUCTOR_UNIQUE_KERNEL_NAMES` and `TORCHINDUCTOR_BENCHMARK_KERNEL` are set to `1`. - Disable cudagraphs for this run. 2. **Interpret the Profiling Outputs**: - Identify the compiled module paths for the forward and backward graphs from the output log. - Run the compiled module for the forward graph and collect profiling data. Save the Chrome trace file. 3. **Analyze Kernel Performance**: - Break down the GPU execution time into different kernel categories (e.g., pointwise, reduction). - Identify the kernel that takes the highest percentage of GPU time. - Run the individual kernel file with and without the `TORCHINDUCTOR_MAX_AUTOTUNE` environment variable enabled. 4. **Report Results**: - Provide a summary of the GPU time breakdown for the chosen model. - Include the Chrome trace file path for further visualization. - Analyze the performance impact of enabling `TORCHINDUCTOR_MAX_AUTOTUNE` on the most time-consuming kernel. Deliverables: 1. Python script named `run_profiling.py` implementing steps to setup, run, and analyze the model profiling. 2. A markdown file `profiling_report.md` that includes: - Summary of GPU time breakdown. - Chrome trace file path. - Analysis of the `TORCHINDUCTOR_MAX_AUTOTUNE` impact on the most expensive kernel. Constraints: - Make sure your code is well-commented and follows PEP 8 guidelines. - Ensure that the profiling execution is adequately timed to avoid excessively long runs (e.g., use a reasonable batch size). Performance Requirements: - The profiling and analysis steps should complete under a reasonable time frame, say a few minutes, depending on the chosen model and hardware. ```python # The below code snippet provides a starting point for your implementation import os import subprocess # Environment setup os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Choose your model here model_name = \'resnet50\' # Profile the model execution subprocess.run([ \\"python\\", \\"-u\\", \\"benchmarks/dynamo/timm_models.py\\", \\"--backend\\", \\"inductor\\", \\"--amp\\", \\"--performance\\", \\"--dashboard\\", \\"--only\\", model_name, \\"--disable-cudagraphs\\", \\"--training\\" ]) ```","solution":"import os import subprocess def run_model_profiling(model_name): Run the model profiling with TorchInductor and Triton kernels for the specified model. Parameters: model_name (str): Name of the model to profile (e.g., \'resnet50\'). # Setup environment variables os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Run the model profiling subprocess.run([ \\"python\\", \\"-u\\", \\"benchmarks/dynamo/timm_models.py\\", \\"--backend\\", \\"inductor\\", \\"--amp\\", \\"--performance\\", \\"--dashboard\\", \\"--only\\", model_name, \\"--disable-cudagraphs\\", \\"--training\\" ]) # Invoke the profiling function with the specified model if __name__ == \\"__main__\\": run_model_profiling(\'resnet50\')"},{"question":"Custom Color Palettes with Seaborn You are tasked with creating a custom color palette using Seaborn\'s `sns.hls_palette` function and applying it to visualize a dataset. This assessment will evaluate your understanding of generating and adjusting HLS color palettes and using them in a plot. Part 1: Custom Color Palette 1. Create a function `create_custom_palette` that takes the following parameters: - `num_colors` (int): The number of colors to be included in the palette. - `lightness` (float): The lightness parameter for the HLS color space (between 0 and 1). - `saturation` (float): The saturation parameter for the HLS color space (between 0 and 1). - `start_hue` (float): The starting point for hue sampling (between 0 and 1). - `as_cmap` (bool): Whether to return a continuous colormap (default is `False`). The function should return a Seaborn color palette based on the provided parameters. 2. Your function should handle edge cases, such as ensuring `num_colors` is a positive integer and `lightness`, `saturation`, and `start_hue` are within the valid range. ```python def create_custom_palette(num_colors, lightness, saturation, start_hue, as_cmap=False): Create a custom HLS color palette using Seaborn. Parameters: num_colors (int): Number of colors in the palette. lightness (float): Lightness of the colors (0 to 1). saturation (float): Saturation of the colors (0 to 1). start_hue (float): Starting hue value (0 to 1). as_cmap (bool): Return a colormap (True) or a list of colors (False). Returns: sns.palettes._ColorPalette or matplotlib.colors.Colormap: Generated color palette or colormap. # Implement the function here pass ``` Part 2: Visualization 3. Use the `create_custom_palette` function to generate a color palette with the following specifications: - `num_colors=8` - `lightness=0.5` - `saturation=0.7` - `start_hue=0.2` 4. Load the `tips` dataset from Seaborn and create a bar plot showing the average total bill for each day of the week. Apply the custom color palette to the bars. Constraints: - Ensure that your function can handle invalid input by raising appropriate exceptions. - Performance should not be a concern since the main goal is to demonstrate comprehension of Seaborn\'s color palette functionalities. Expected Output: - The `create_custom_palette` function should return a valid Seaborn color palette. - The plot should be a bar plot with the days of the week on the x-axis and the average total bill on the y-axis, with bars colored using the custom palette. ```python import seaborn as sns import matplotlib.pyplot as plt # Implement the function def create_custom_palette(num_colors, lightness, saturation, start_hue, as_cmap=False): if not isinstance(num_colors, int) or num_colors <= 0: raise ValueError(\\"num_colors must be a positive integer\\") if not 0 <= lightness <= 1 or not 0 <= saturation <= 1 or not 0 <= start_hue <= 1: raise ValueError(\\"lightness, saturation, and start_hue must be between 0 and 1\\") palette = sns.hls_palette(n_colors=num_colors, l=lightness, s=saturation, h=start_hue, as_cmap=as_cmap) return palette # Generate the color palette palette = create_custom_palette(8, 0.5, 0.7, 0.2) # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette) plt.title(\\"Average Total Bill per Day\\") plt.show() ```","solution":"import seaborn as sns def create_custom_palette(num_colors, lightness, saturation, start_hue, as_cmap=False): Create a custom HLS color palette using Seaborn. Parameters: num_colors (int): Number of colors in the palette. lightness (float): Lightness of the colors (0 to 1). saturation (float): Saturation of the colors (0 to 1). start_hue (float): Starting hue value (0 to 1). as_cmap (bool): Return a colormap (True) or a list of colors (False). Returns: sns.palettes._ColorPalette or matplotlib.colors.Colormap: Generated color palette or colormap. if not isinstance(num_colors, int) or num_colors <= 0: raise ValueError(\\"num_colors must be a positive integer\\") if not 0 <= lightness <= 1 or not 0 <= saturation <= 1 or not 0 <= start_hue <= 1: raise ValueError(\\"lightness, saturation, and start_hue must be between 0 and 1\\") palette = sns.hls_palette(n_colors=num_colors, l=lightness, s=saturation, h=start_hue, as_cmap=as_cmap) return palette # For visualization import matplotlib.pyplot as plt # Generate the color palette palette = create_custom_palette(8, 0.5, 0.7, 0.2) # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=palette) plt.title(\\"Average Total Bill per Day\\") plt.show()"},{"question":"# HMAC Coding Challenge Objective Implement a secure messaging system that uses HMAC for message authentication and integrity checking. You are required to write functions to generate an HMAC for a message, update the HMAC with an additional message, retrieve the hexadecimal digest, and verify a received message with its HMAC. Problem Statement 1. **Function `generate_hmac(key: bytes, msg: bytes, digest_name: str) -> hmac.HMAC`** - **Input**: - `key` (bytes): The secret key used for HMAC. - `msg` (bytes): The message to hash. - `digest_name` (str): The name of the digest algorithm (e.g., \'sha256\'). - **Output**: - Returns an `hmac.HMAC` object initialized with the key, message, and specified digest algorithm. 2. **Function `update_hmac(hmac_obj: hmac.HMAC, msg: bytes) -> None`** - **Input**: - `hmac_obj` (hmac.HMAC): An existing HMAC object. - `msg` (bytes): The additional message to hash. - **Output**: - Updates the HMAC object with the additional message. 3. **Function `get_hex_digest(hmac_obj: hmac.HMAC) -> str`** - **Input**: - `hmac_obj` (hmac.HMAC): An existing HMAC object. - **Output**: - Returns the hexadecimal digest of the HMAC object. 4. **Function `verify_message(key: bytes, msg: bytes, received_hmac_hex: str, digest_name: str) -> bool`** - **Input**: - `key` (bytes): The secret key used for HMAC. - `msg` (bytes): The received message. - `received_hmac_hex` (str): The received hexadecimal HMAC. - `digest_name` (str): The name of the digest algorithm (e.g., \'sha256\'). - **Output**: - Returns `True` if the received HMAC matches the computed HMAC for the message; otherwise returns `False`. Example Usage ```python # Example key and messages key = b\'secret_key\' initial_msg = b\'Hello, World!\' additional_msg = b\' More content.\' digest_name = \'sha256\' # Generate initial HMAC hmac_obj = generate_hmac(key, initial_msg, digest_name) # Update with an additional message update_hmac(hmac_obj, additional_msg) # Retrieve hexadecimal digest hex_digest = get_hex_digest(hmac_obj) print(hex_digest) # Verification received_msg = initial_msg + additional_msg is_valid = verify_message(key, received_msg, hex_digest, digest_name) print(is_valid) # Should output: True ``` Constraints - You should use the `hmac` module for HMAC operations. - The `digest_name` should be a valid hash algorithm name supported by the `hashlib` module. - Ensure that the solution is efficient and handles exceptions gracefully. Implement the functions in Python to complete the secure messaging system.","solution":"import hmac import hashlib def generate_hmac(key: bytes, msg: bytes, digest_name: str) -> hmac.HMAC: Creates an HMAC object using the provided key, message, and digest algorithm. return hmac.new(key, msg, getattr(hashlib, digest_name)) def update_hmac(hmac_obj: hmac.HMAC, msg: bytes) -> None: Updates the HMAC object with the additional message. hmac_obj.update(msg) def get_hex_digest(hmac_obj: hmac.HMAC) -> str: Returns the hexadecimal digest of the HMAC object. return hmac_obj.hexdigest() def verify_message(key: bytes, msg: bytes, received_hmac_hex: str, digest_name: str) -> bool: Verifies the received message with its HMAC. hmac_obj = generate_hmac(key, msg, digest_name) expected_hmac_hex = get_hex_digest(hmac_obj) return hmac.compare_digest(expected_hmac_hex, received_hmac_hex)"},{"question":"# Coding Challenge **Objective**: Implement a function to process raw audio data with specific manipulations. Problem Statement You need to write a function `process_audio` that performs the following operations on a given audio byte fragment: 1. Converts the audio fragment from its linear format to A-LAW encoding. 2. Applies a bias of 100 units to the A-LAW encoded fragment. 3. Converts the biased A-LAW fragment back to the linear format. 4. Calculates the RMS (Root Mean Square) power of the final linear fragment. The input to the function includes: - `fragment`: A bytes object representing the audio fragment in a linear format. - `width`: An integer representing the sample width in bytes (can be 1, 2, 3, or 4). The function should return a tuple with two elements: 1. The final modified linear audio fragment after all operations. 2. The RMS value of the final modified linear fragment. Constraints - The input fragment is guaranteed to be a valid bytes object with a length that is a multiple of the `width`. - You must handle various widths appropriately during the conversion and biasing processes. - Ensure efficient processing, as the fragments could be quite large. Example ```python def process_audio(fragment: bytes, width: int) -> Tuple[bytes, float]: # Your implementation goes here # Example usage: fragment = b\'x01x02x03\' # Simplified example, actual fragments will be larger width = 1 final_fragment, rms_value = process_audio(fragment, width) print(final_fragment) # Output the final modified linear fragment print(rms_value) # Output the RMS value of the final modified linear fragment ``` **Note**: You can use the functions provided in the `audioop` module for conversion (such as `audioop.lin2alaw`, `audioop.alaw2lin`), biasing (`audioop.bias`), and RMS calculation (`audioop.rms`). Make sure the transformations are applied in the specified sequence. Good luck!","solution":"import audioop import math def process_audio(fragment: bytes, width: int) -> tuple: Processes the audio fragment by performing A-LAW encoding, applying bias, decoding, and computing the RMS of the resulting audio fragment. Parameters: fragment (bytes): The input audio fragment in linear format. width (int): The sample width in bytes. Returns: tuple: A tuple containing the final modified linear fragment and its RMS value. # Convert linear audio fragment to A-LAW encoding alaw_fragment = audioop.lin2alaw(fragment, width) # Apply a bias of 100 units to the A-LAW encoded fragment biased_alaw_fragment = audioop.bias(alaw_fragment, 1, 100) # Convert the biased A-LAW fragment back to linear format final_fragment = audioop.alaw2lin(biased_alaw_fragment, width) # Calculate the RMS of the final linear fragment rms_value = audioop.rms(final_fragment, width) return final_fragment, rms_value"},{"question":"Objective You are required to write an asynchronous Python function that spawns multiple subprocesses, monitors their execution, and collects their outputs. The goal is for you to demonstrate your understanding of the asyncio subprocess API in Python 3.10. Problem Statement Write a function `run_multiple_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]` that accepts a list of shell commands, executes them asynchronously, and returns a list of tuples containing command string, exit code, standard output, and standard error for each command. Function Signature ```python from typing import List, Tuple async def run_multiple_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: pass ``` Expected Input - List of shell commands (List[str]): E.g., `[\\"ls /\\", \\"sleep 1; echo \'hello\'\\", \\"python -c \'print(42)\'\\"]` Expected Output - List of tuples (List[Tuple[str, int, str, str]]): Each tuple contains: - The command string that was executed. - The exit code of the command. - The standard output captured from the command. - The standard error captured from the command. Example: ```python [ (\\"ls /\\", 0, \\"binnbootndevn...\\", \\"\\"), (\\"sleep 1; echo \'hello\'\\", 0, \\"hellon\\", \\"\\"), (\\"python -c \'print(42)\'\\", 0, \\"42n\\", \\"\\") ] ``` Constraints - The function should handle a reasonable number of commands efficiently. - Commands should run in parallel, not sequentially. - The function should handle cases where a command might fail (non-zero exit code), and include the error in the output. Implementation Details 1. Use `asyncio.create_subprocess_shell` to create and run subprocesses. 2. Use `asyncio.gather` or equivalent to manage multiple async tasks. 3. Collect stdout and stderr from each subprocess. 4. Ensure to handle decoding of the output properly. 5. Return the collected information as specified. Example ```python import asyncio from typing import List, Tuple async def run_command(cmd: str) -> Tuple[str, int, str, str]: proc = await asyncio.create_subprocess_shell(cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return cmd, proc.returncode, stdout.decode(), stderr.decode() async def run_multiple_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: tasks = [run_command(cmd) for cmd in commands] return await asyncio.gather(*tasks) # Example usage commands = [\\"ls /\\", \\"sleep 1; echo \'hello\'\\", \\"python -c \'print(42)\'\\"] results = asyncio.run(run_multiple_commands(commands)) for result in results: print(result) ``` # Notes - Remember to decode output from bytes to str before returning. - Handle any exceptions that might occur due to subprocess errors and include them in the output. Evaluation Criteria - Code correctness and adherence to problem statement. - Efficiency and ability to handle multiple commands concurrently. - Proper use of async/await and asyncio subprocess APIs. - Clean and readable code.","solution":"import asyncio from typing import List, Tuple async def run_command(cmd: str) -> Tuple[str, int, str, str]: Execute a command asynchronously and return its details. proc = await asyncio.create_subprocess_shell(cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE) stdout, stderr = await proc.communicate() return cmd, proc.returncode, stdout.decode(), stderr.decode() async def run_multiple_commands(commands: List[str]) -> List[Tuple[str, int, str, str]]: Execute multiple commands asynchronously and return their details. tasks = [run_command(cmd) for cmd in commands] return await asyncio.gather(*tasks)"},{"question":"# XML Document Manipulation and Querying You are required to write a Python function that works with XML data using the `xml.etree.ElementTree` module. The function will parse an XML string, modify it, and then query specific elements within the XML. Function Signature ```python def modify_and_query_xml(xml_data: str) -> list: pass ``` Input - `xml_data`: A string representing an XML document. The document contains a list of books, each with the following structure: ```xml <library> <book> <title>Some Book Title</title> <author>Author Name</author> <year>Year</year> </book> ... </library> ``` Output - A list of titles of books published after the year 2000. Constraints 1. You must use the `xml.etree.ElementTree` module for this task. 2. Assume that the XML data provided is well-formed. 3. Each `<book>` element will have exactly one `<title>`, `<author>`, and `<year>` element. Instructions 1. Parse the provided XML string. 2. Modify the XML by adding an attribute `genre` with the value `\\"Unknown\\"` to each `<book>` element that does not already have a `genre` attribute. 3. Query the modified XML to find all books published after the year 2000. 4. Return a list of titles of these books. Example ```python xml_input = <library> <book> <title>Advanced Python Programming</title> <author>John Smith</author> <year>2005</year> </book> <book> <title>Data Science with Python</title> <author>Jane Doe</author> <year>2015</year> </book> <book> <title>Basic HTML</title> <author>Alan Turing</author> <year>1995</year> </book> </library> # Function call result = modify_and_query_xml(xml_input) # Expected Output result == [\\"Advanced Python Programming\\", \\"Data Science with Python\\"] ``` Notes - Make sure to test your function thoroughly. - Handle edge cases, such as books with missing or malformed year elements, gracefully.","solution":"import xml.etree.ElementTree as ET def modify_and_query_xml(xml_data: str) -> list: Parses the given XML string, modifies it by adding \'genre\' attributes where missing, and returns a list of book titles published after the year 2000. # Parse the XML string root = ET.fromstring(xml_data) # Add genre attribute if missing for book in root.findall(\'book\'): if \'genre\' not in book.attrib: book.set(\'genre\', \'Unknown\') # Query for titles of books published after the year 2000 titles = [] for book in root.findall(\'book\'): year = book.find(\'year\').text if int(year) > 2000: titles.append(book.find(\'title\').text) return titles"},{"question":"**Coding Assessment Question** # Objective: Demonstrate your proficiency with pandas\' plotting capabilities by creating various types of visualizations from a given dataset. Your task is to write functions to produce specific plots. # Dataset: You will work with a dataset named `sales.csv`. This dataset contains sales data for an online store, with the following columns: - \'Date\': The date of the sale (string in \'YYYY-MM-DD\' format) - \'Product\': The product name (string) - \'Category\': The product category (string) - \'Sales\': The sales amount (float) - \'Quantity\': The number of products sold (integer) - \'Region\': The region where the sale was made (string) Example: ``` Date,Product,Category,Sales,Quantity,Region 2023-01-01,ProductA,Category1,150.0,2,North 2023-01-01,ProductB,Category2,200.0,1,East ... ``` # Tasks: 1. **Reading Data**: - Write a function `read_sales_data(filepath)` that reads the dataset from the given file path and returns a pandas DataFrame. 2. **Time Series Line Plot**: - Write a function `plot_sales_over_time(df)` that takes the DataFrame as input and plots a time series line plot of total sales per day. Ensure that the x-axis is formatted nicely for dates. 3. **Bar Plot by Category**: - Write a function `plot_sales_by_category(df)` that takes the DataFrame as input and creates a bar plot showing the total sales for each category. 4. **Histogram of Sales**: - Write a function `plot_sales_histogram(df)` that takes the DataFrame as input and produces a histogram of the sales amounts. Include 20 bins and make the histogram stacked by category. 5. **Box Plot of Sales by Region**: - Write a function `plot_sales_boxplot_by_region(df)` that takes the DataFrame as input and creates a box plot of sales amounts for each region. 6. **Scatter Plot for Sales vs. Quantity**: - Write a function `plot_sales_vs_quantity_scatter(df)` that takes the DataFrame as input and creates a scatter plot of sales amounts (y-axis) versus quantities sold (x-axis). Color the points by category. 7. **Pie Chart of Sales by Region**: - Write a function `plot_sales_pie_by_region(df)` that takes the DataFrame as input and creates a pie chart showing the proportion of total sales for each region. 8. **Hexagonal Bin Plot**: - Write a function `plot_sales_hexbin(df)` that takes the DataFrame as input and creates a hexagonal bin plot of sales vs. quantity. Use 30 hexagons along the x-direction (gridsize=30) and aggregate using the median sales amount per bin area. # Constraints and Requirements: - Ensure that all plots are well-labeled and have appropriate titles. - Use pandas and matplotlib for all plotting tasks. - Handle missing data appropriately by either filling or dropping as needed for plotting. - The resulting plots must be saved as .png files with appropriate names (e.g., `sales_over_time.png`, `sales_by_category.png`, etc.). - Use the seed value 123456 for any random generation, if needed. # Performance Requirements: - The code should efficiently handle datasets with up to 1 million rows. # Submission: Submit your solution as a Python script or Jupyter notebook that includes all the required functions.","solution":"import pandas as pd import matplotlib.pyplot as plt def read_sales_data(filepath): Reads the sales data from the given file path and returns a pandas DataFrame. return pd.read_csv(filepath, parse_dates=[\'Date\']) def plot_sales_over_time(df): Plots a time series line plot of total sales per day. sales_per_day = df.groupby(\'Date\').agg({\'Sales\': \'sum\'}) plt.figure(figsize=(10, 6)) plt.plot(sales_per_day.index, sales_per_day[\'Sales\'], marker=\'o\') plt.title(\'Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.xticks(rotation=45) plt.tight_layout() plt.savefig(\'sales_over_time.png\') plt.close() def plot_sales_by_category(df): Creates a bar plot showing the total sales for each category. sales_by_category = df.groupby(\'Category\').agg({\'Sales\': \'sum\'}).sort_values(\'Sales\') plt.figure(figsize=(10, 6)) sales_by_category[\'Sales\'].plot(kind=\'bar\') plt.title(\'Total Sales by Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Sales\') plt.tight_layout() plt.savefig(\'sales_by_category.png\') plt.close() def plot_sales_histogram(df): Produces a histogram of the sales amounts, stacked by category with 20 bins. plt.figure(figsize=(10, 6)) df.hist(column=\'Sales\', by=\'Category\', bins=20, stacked=True, grid=False, layout=(5, 2), figsize=(15, 10), sharex=True, sharey=True) plt.suptitle(\'Histogram of Sales Amounts by Category\') plt.tight_layout() plt.savefig(\'sales_histogram.png\') plt.close() def plot_sales_boxplot_by_region(df): Creates a box plot of sales amounts for each region. plt.figure(figsize=(10, 6)) df.boxplot(column=\'Sales\', by=\'Region\') plt.title(\'Box Plot of Sales by Region\') plt.suptitle(\'\') plt.xlabel(\'Region\') plt.ylabel(\'Sales\') plt.tight_layout() plt.savefig(\'sales_boxplot_by_region.png\') plt.close() def plot_sales_vs_quantity_scatter(df): Creates a scatter plot of sales amounts versus quantities sold, colored by category. plt.figure(figsize=(10, 6)) categories = df[\'Category\'].unique() for category in categories: subset = df[df[\'Category\'] == category] plt.scatter(subset[\'Quantity\'], subset[\'Sales\'], label=category) plt.title(\'Sales vs Quantity\') plt.xlabel(\'Quantity Sold\') plt.ylabel(\'Sales Amount\') plt.legend(title=\'Category\') plt.tight_layout() plt.savefig(\'sales_vs_quantity_scatter.png\') plt.close() def plot_sales_pie_by_region(df): Creates a pie chart showing the proportion of total sales for each region. sales_by_region = df.groupby(\'Region\').agg({\'Sales\': \'sum\'}) plt.figure(figsize=(8, 8)) sales_by_region[\'Sales\'].plot(kind=\'pie\', autopct=\'%1.1f%%\') plt.title(\'Sales Proportion by Region\') plt.ylabel(\'\') plt.tight_layout() plt.savefig(\'sales_pie_by_region.png\') plt.close() def plot_sales_hexbin(df): Creates a hexagonal bin plot of sales vs. quantity with 30 hexagons along the x-direction. plt.figure(figsize=(10, 6)) plt.hexbin(df[\'Quantity\'], df[\'Sales\'], gridsize=30, cmap=\'Blues\', mincnt=1) cb = plt.colorbar(label=\'Median sales amount per bin\') plt.title(\'Hexbin Plot of Sales vs Quantity\') plt.xlabel(\'Quantity Sold\') plt.ylabel(\'Sales Amount\') plt.tight_layout() plt.savefig(\'sales_hexbin.png\') plt.close()"},{"question":"# Remote Task Execution with XML-RPC **Objective**: Implement an XML-RPC server and a client that can communicate with each other to perform a series of mathematical operations on remote data. **Task**: You need to create an XML-RPC server that offers methods to perform basic mathematical operations (addition, subtraction, multiplication, division). You will also create a client that can call these methods on the server with given parameters. # Requirements: 1. **Server Implementation**: - Create an XML-RPC server that provides the following methods: - `add(x, y)`: Returns the sum of `x` and `y`. - `subtract(x, y)`: Returns the difference when `y` is subtracted from `x`. - `multiply(x, y)`: Returns the product of `x` and `y`. - `divide(x, y)`: Returns the quotient when `x` is divided by `y`. If `y` is 0, raise a `ZeroDivisionError`. 2. **Client Implementation**: - Create an XML-RPC client that performs the following operations by calling the server: - Adds 10 and 5. - Subtracts 10 from 20. - Multiplies 10 and 5. - Divides 20 by 5. - Attempts to divide 20 by 0 (expecting an exception). # Input and Output Formats: - **Server**: - No specific input format. It should continuously run and listen for incoming XML-RPC requests. - **Client**: - The result of each operation should be printed. Each result should be on a new line. - Handle and print a user-friendly error message for the division by zero case. # Constraints: - You must use the `xmlrpc.server` module to create the server and the `xmlrpc.client` module for the client. - Ensure the server is reliable and can handle multiple requests sequentially. # Performance Requirements: - The server should handle each request within 1 second. Below is an example interaction: Server-side Example: ```python from xmlrpc.server import SimpleXMLRPCServer def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def divide(x, y): if y == 0: raise ZeroDivisionError(\\"Cannot divide by zero.\\") return x / y server = SimpleXMLRPCServer((\\"localhost\\", 8000)) print(\\"Listening on port 8000...\\") server.register_function(add, \\"add\\") server.register_function(subtract, \\"subtract\\") server.register_function(multiply, \\"multiply\\") server.register_function(divide, \\"divide\\") server.serve_forever() ``` Client-side Example: ```python import xmlrpc.client proxy = xmlrpc.client.ServerProxy(\\"http://localhost:8000/\\") print(proxy.add(10, 5)) # Output: 15 print(proxy.subtract(20, 10)) # Output: 10 print(proxy.multiply(10, 5)) # Output: 50 print(proxy.divide(20, 5)) # Output: 4.0 try: print(proxy.divide(20, 0)) # Should raise an error except xmlrpc.client.Fault as err: print(f\\"Error: {err}\\") # Expected error message ``` Implement the server and client as described above to complete this task.","solution":"from xmlrpc.server import SimpleXMLRPCServer def add(x, y): return x + y def subtract(x, y): return x - y def multiply(x, y): return x * y def divide(x, y): if y == 0: raise ZeroDivisionError(\\"Cannot divide by zero.\\") return x / y def start_server(): server = SimpleXMLRPCServer((\\"localhost\\", 8000)) print(\\"Listening on port 8000...\\") server.register_function(add, \\"add\\") server.register_function(subtract, \\"subtract\\") server.register_function(multiply, \\"multiply\\") server.register_function(divide, \\"divide\\") server.serve_forever() # Uncomment the line below to start the server # start_server()"},{"question":"**Objective:** Create a Python class `FileLineReader` that leverages the `linecache` module to allow efficient line retrieval and cache management for multiple files. **Class Methods:** 1. **`add_file(self, filename: str) -> None`**: Adds a file to the list of files managed by the `FileLineReader`. 2. **`get_line(self, filename: str, lineno: int) -> str`**: Retrieves a specific line from a file, using caching for efficiency. If the `lineno` or `filename` is invalid, return an empty string. 3. **`clear_cache(self) -> None`**: Clears the cache for all files. 4. **`check_cache(self, filename: str = None) -> None`**: Checks the cache for the specified file, or all files if no filename is provided. **Constraints:** - Implement error handling where appropriate to ensure robustness. - Assume the files are UTF-8 encoded unless specified otherwise. **Example:** ```python # Create an instance of the FileLineReader reader = FileLineReader() # Add files to the reader reader.add_file(\'example1.txt\') reader.add_file(\'example2.txt\') # Retrieve lines from the added files line1 = reader.get_line(\'example1.txt\', 10) line2 = reader.get_line(\'example2.txt\', 5) # Clear the cache and then check its state reader.clear_cache() reader.check_cache() ``` **Dependencies:** Use the `linecache` module as described in the documentation. **Performance Requirements:** - The solution should efficiently handle line retrievals, even if the files are large. - Ensure the cache management methods do not introduce significant overhead when clearing or checking cache validity. Implement the `FileLineReader` class to meet these requirements.","solution":"import linecache class FileLineReader: def __init__(self): self.files = [] def add_file(self, filename: str) -> None: if filename not in self.files: self.files.append(filename) def get_line(self, filename: str, lineno: int) -> str: if filename in self.files: line = linecache.getline(filename, lineno) return line.strip() if line else \'\' return \'\' def clear_cache(self) -> None: linecache.clearcache() def check_cache(self, filename: str = None) -> None: if filename: linecache.checkcache(filename) else: linecache.checkcache()"},{"question":"# Question You\'re tasked with simulating part of the functionality described in the Python-C API documentation using pure Python code. Specifically, you need to write a function that mimics the behavior of `PyArg_ParseTuple()` and `Py_BuildValue()` by parsing a tuple of arguments and building a new value from them based on a format string. Function Signature ```python def parse_and_build(args: tuple, format_string: str) -> any: ``` Input - `args`: A tuple containing the arguments to be parsed. - `format_string`: A string specifying the format of the arguments and the desired format of the output. The string could contain the following format units: - `\\"s\\"`: Convert to string. - `\\"i\\"`: Convert to integer. - `\\"f\\"`: Convert to float. - `\\"t\\"`: Return as tuple. - `\\"l\\"`: Return as list. - `\\"d\\"`: Return as dictionary (expects key-value pairs in args). Output - A Python object of the type specified in the `format_string`. Example ```python # Example 1: args = (42, \'hello\', 3.14) format_string = \'ils\' # The corresponding output would be a list: [42, \'hello\', 3.14] # Example 2: args = (1, 2, 3, 4) format_string = \'d\' # Assumes dictionary format, output would be a dictionary: {1: 2, 3: 4} assert parse_and_build((42, \'hello\', 3.14), \'ils\') == [42, \'hello\', 3.14] assert parse_and_build((1, 2, 3, 4), \'d\') == {1: 2, 3: 4} ``` Constraints - Assume the format_string is valid and matches the args provided. - The function must handle any exceptions gracefully by returning a suitable error message such as `\'Error: invalid conversion\'`. Instructions Implement the `parse_and_build` function based on the provided specifications.","solution":"def parse_and_build(args: tuple, format_string: str) -> any: Simulates the parsing of arguments and building of new value based on the format string. Args: - args: tuple containing the arguments to be parsed. - format_string: string specifying the format of the arguments and the desired output. Returns: - A Python object of the type specified in the format_string. try: result = [] format_iter = iter(format_string) arg_iter = iter(args) for fmt in format_iter: if fmt == \'i\': result.append(int(next(arg_iter))) elif fmt == \'f\': result.append(float(next(arg_iter))) elif fmt == \'s\': result.append(str(next(arg_iter))) elif fmt == \'t\': result = tuple(args) break elif fmt == \'l\': result = list(args) break elif fmt == \'d\': result = {} for key in arg_iter: value = next(arg_iter, None) result[key] = value break else: return \'Error: invalid conversion\' if format_string.startswith((\'t\', \'l\', \'d\')): return result return result if len(result) > 1 else result[0] except Exception as e: return f\'Error: {str(e)}\'"},{"question":"# Question: Optimizing Computation on XPUs Using PyTorch You are tasked with designing a PyTorch function to efficiently perform a series of matrix multiplications on an XPU device while managing memory usage and ensuring reproducibility. Function Signature ```python def optimized_matrix_multiplication(num_matrices: int, matrix_size: int, seed: int) -> torch.Tensor: pass ``` Inputs: - `num_matrices` (int): Number of random matrices to generate and multiply in a sequence. - `matrix_size` (int): The size of each square matrix (matrix_size x matrix_size). - `seed` (int): Seed for the random number generator to ensure reproducibility. Outputs: - (torch.Tensor): The resulting matrix after performing the sequential matrix multiplications on the XPU. Constraints: 1. The function should use XPU capabilities for computation. 2. The function should manage memory efficiently, ensuring no memory leaks or excessive usage. 3. The random number generator should be seeded with the provided `seed` value to ensure reproducibility. 4. Streams should be used to optimize the sequence of operations. Requirements: 1. Initialize the XPU and check if it is available. 2. Seed the random number generator with the provided `seed`. 3. Generate `num_matrices` random matrices of size `matrix_size x matrix_size` on the XPU. 4. Perform matrix multiplication sequentially: result = M1 * M2 * M3 * ... * Mn. 5. Use streams to optimize the multiplication steps. 6. Ensure efficient memory usage and synchronize the operations. Example: ```python result = optimized_matrix_multiplication(5, 4, 42) print(result) ``` This example should output the final 4x4 matrix result after multiplying five 4x4 random matrices sequentially. # Note: You are encouraged to utilize the functions listed under `torch.xpu` for managing devices, memory, and streams effectively. Ensure to handle any exceptions where the XPU might not be available and provide a fallback or appropriate message.","solution":"import torch def optimized_matrix_multiplication(num_matrices: int, matrix_size: int, seed: int) -> torch.Tensor: Perform a series of matrix multiplications on an XPU device while managing memory usage and ensuring reproducibility. Args: num_matrices (int): Number of random matrices to generate and multiply in a sequence. matrix_size (int): The size of each square matrix (matrix_size x matrix_size). seed (int): Seed for the random number generator to ensure reproducibility. Returns: torch.Tensor: The resulting matrix after performing the sequential matrix multiplications on the XPU. # Check if XPU is available device = torch.device(\'xpu\') if torch.xpu.is_available() else torch.device(\'cpu\') # Seed the random number generator torch.manual_seed(seed) # Generate the random matrices matrices = [torch.randn(matrix_size, matrix_size, device=device) for _ in range(num_matrices)] # Initialize the result using the first matrix result = matrices[0] # Use a stream to optimize the multiplication sequence stream = torch.xpu.Stream(device=device) if torch.xpu.is_available() else None if stream: stream.synchronize() for i in range(1, num_matrices): with torch.xpu.stream(stream) if stream else torch.inference_mode(): result = result @ matrices[i] if stream: stream.synchronize() return result"},{"question":"**Objective:** Demonstrate your understanding and effective use of the `unittest.mock` module in Python to create, configure, and assert interactions with mock objects. This question assesses your ability to use `Mock`, `patch`, and their various features. **Question:** Your task is to write a unit test for a function `fetch_data_and_process` that makes an HTTP request to a given URL, processes the received data, and saves it to a database. Use the features of the `unittest.mock` module to mock the HTTP request and database interaction, and assert that the function behaves correctly under various conditions. **Function Under Test:** ```python import requests def fetch_data_and_process(url, db): response = requests.get(url) if response.status_code == 200: data = response.json() processed_data = process_data(data) db.save(processed_data) else: raise ValueError(f\\"Failed to fetch data: {response.status_code}\\") def process_data(data): # Assume some complex data processing logic here return {\\"processed\\": data} ``` **Requirements:** 1. **Mock the HTTP request**: Ensure that `requests.get` is called with the correct URL. Simulate different HTTP response scenarios (e.g., status 200, 404) using the `side_effect` feature. 2. **Mock the database interaction**: Ensure that `db.save` is called with the correctly processed data. Use the `assert_called_with` method to verify the interaction. 3. **Test behavior for different response status codes**: - When the status code is 200, assert that `db.save` is called with the correct processed data. - When the status code is not 200, assert that a `ValueError` is raised with the appropriate message. 4. **Ensure no actual HTTP requests or database interactions**: All external dependencies (HTTP requests and database) should be mocked. **Constraints:** 1. You must use the `unittest` module for structuring your test cases. 2. You must use `unittest.mock.patch` to replace `requests.get` and the database object (`db`). **Input and Output Formats:** - `fetch_data_and_process(url, db)`: `url` is a string representing the URL to fetch data from, and `db` is an object with a `save` method. **Example of Test Cases:** ```python import unittest from unittest.mock import patch, Mock from your_module import fetch_data_and_process, process_data class TestFetchDataAndProcess(unittest.TestCase): @patch(\'your_module.requests.get\') def test_successful_fetch_and_process(self, mock_get): url = \\"http://example.com/data\\" db = Mock() mock_get.return_value.status_code = 200 mock_get.return_value.json.return_value = {\\"key\\": \\"value\\"} fetch_data_and_process(url, db) mock_get.assert_called_with(url) db.save.assert_called_with({\\"processed\\": {\\"key\\": \\"value\\"}}) @patch(\'your_module.requests.get\') def test_failed_fetch(self, mock_get): url = \\"http://example.com/data\\" db = Mock() mock_get.return_value.status_code = 404 with self.assertRaises(ValueError) as context: fetch_data_and_process(url, db) mock_get.assert_called_with(url) self.assertEqual(str(context.exception), \\"Failed to fetch data: 404\\") db.save.assert_not_called() if __name__ == \'__main__\': unittest.main() ``` Please ensure that your test cases cover all the requirements specified.","solution":"import requests def fetch_data_and_process(url, db): response = requests.get(url) if response.status_code == 200: data = response.json() processed_data = process_data(data) db.save(processed_data) else: raise ValueError(f\\"Failed to fetch data: {response.status_code}\\") def process_data(data): # Assume some complex data processing logic here return {\\"processed\\": data}"},{"question":"# PyTorch Gradient Check and Validation **Objective:** Implement and validate gradient computation for a custom PyTorch function using `gradcheck` and `gradgradcheck`. **Problem Statement:** You are given a custom function implemented in PyTorch. Your task is to write a function to compute its gradients and validate these gradients using PyTorch\'s `gradcheck` and `gradgradcheck`. **Custom Function:** Consider the function (f(x) = x^3 - 2x^2 + x). Implement this function in PyTorch and ensure it supports autograd. **Tasks:** 1. Implement the function `custom_function(x)` that computes (f(x) = x^3 - 2x^2 + x) and is compatible with PyTorch\'s autograd. 2. Write a function `validate_gradient(x)` that performs the following: - Checks the gradient computation using `gradcheck`. - Checks the second-order gradient computation using `gradgradcheck`. - Returns `True` if all checks pass and `False` otherwise. **Input Format:** - `x`: A PyTorch tensor of shape `(N,)` where `N` is the number of elements. The tensor should have `requires_grad=True`. **Output Format:** - Return a boolean indicating whether all gradient checks are passed. **Constraints:** - Use `torch.float64` precision for gradient checks to ensure numerical stability. - The input tensor `x` will contain values within the range ([-10.0, 10.0]). **Sample Usage:** ```python import torch def custom_function(x): return x**3 - 2*x**2 + x def validate_gradient(x): x = x.clone().detach().requires_grad_(True) try: gradcheck_result = torch.autograd.gradcheck(custom_function, (x,), eps=1e-6, atol=1e-4) gradgradcheck_result = torch.autograd.gradgradcheck(custom_function, (x,)) return gradcheck_result and gradgradcheck_result except RuntimeError as e: print(e) return False # Example usage x = torch.tensor([2.0, 5.0], dtype=torch.float64, requires_grad=True) print(validate_gradient(x)) # Expected output: True ``` **Notes:** - Ensure to handle potential errors appropriately while performing grad checks. - Provide comprehensive docstrings and comments in your implementation to explain the functionality. **Evaluation Criteria:** - Correctness of the function implementations. - Use of appropriate PyTorch functions for gradient checks. - Code readability and documentation.","solution":"import torch def custom_function(x): Computes the custom function f(x) = x^3 - 2x^2 + x. Args: x (torch.Tensor): Input tensor with requires_grad=True. Returns: torch.Tensor: Output of the function. return x**3 - 2*x**2 + x def validate_gradient(x): Validates the gradient computation of the custom function using gradcheck and gradgradcheck. Args: x (torch.Tensor): Input tensor with requires_grad=True. Returns: bool: True if all gradient checks pass, False otherwise. x = x.clone().detach().requires_grad_(True) try: gradcheck_result = torch.autograd.gradcheck(custom_function, (x,), eps=1e-6, atol=1e-4) gradgradcheck_result = torch.autograd.gradgradcheck(custom_function, (x,)) return gradcheck_result and gradgradcheck_result except RuntimeError as e: print(e) return False"},{"question":"# Question: Implementing a Safe Serialization Function You are tasked with creating a safe serialization and deserialization utility for a specific subset of Python objects using the `marshal` module. Your functions should be able to handle and correctly serialize/deserialize the supported types, and raise appropriate errors for unsupported types. Your task is to implement two functions: `safe_serialize` and `safe_deserialize`. 1. **Function: `safe_serialize`** **Inputs:** - `value`: A Python object to serialize. - `file_path`: The file path where the serialized object will be saved (as a binary file). **Output:** - The function returns `None`. **Constraints:** - The `value` must be one of the supported types as mentioned in the documentation (booleans, integers, floating point numbers, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, code objects, None, Ellipsis, StopIteration). - If the `value` or any contained object within `value` is of an unsupported type, raise a `ValueError` with the message \\"Unsupported type encountered during serialization\\". 2. **Function: `safe_deserialize`** **Inputs:** - `file_path`: The file path from where the serialized object will be read (as a binary file). **Output:** - The deserialized Python object from the file. **Constraints:** - If the file contains data that cannot be deserialized due to an unsupported format or any other error, raise an appropriate error (`EOFError`, `ValueError`, `TypeError`). # Function Signatures: ```python def safe_serialize(value, file_path): Serializes the given value using the marshal module and writes it to the specified file path in binary format. Parameters: value (any): The Python object to serialize. file_path (str): The path to the file where the serialized object will be saved. Returns: None pass def safe_deserialize(file_path): Deserializes and returns the Python object from the specified file path using the marshal module. Parameters: file_path (str): The path to the file from where the serialized object will be read. Returns: any: The deserialized Python object. pass ``` # Example Tests ```python # Example test case 1 data = {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"pi\\": 3.14} file_path = \\"data.bin\\" safe_serialize(data, file_path) result = safe_deserialize(file_path) assert result == data # Example test case 2 unsupported_data = {\\"name\\": \\"Bob\\", \\"data\\": complex(2, 3), \\"unsupported\\": set} try: safe_serialize(unsupported_data, \\"test.bin\\") except ValueError as e: assert str(e) == \\"Unsupported type encountered during serialization\\" ``` # Notes - Ensure your functions handle file operations gracefully, including opening and closing files. - The `safe_serialize` function must ensure no partial or corrupted files are written in the event of an error. - The `safe_deserialize` function should handle different types of errors raised by the `marshal.load` function correctly.","solution":"import marshal SUPPORTED_TYPES = (bool, int, float, complex, str, bytes, bytearray, tuple, list, set, frozenset, dict, type(None)) def safe_serialize(value, file_path): Serializes the given value using the marshal module and writes it to the specified file path in binary format. Parameters: value (any): The Python object to serialize. file_path (str): The path to the file where the serialized object will be saved. Returns: None def is_supported(val): Helper function to check if a value or its contained values are all supported types if isinstance(val, SUPPORTED_TYPES): if isinstance(val, (tuple, list, set, frozenset)): return all(is_supported(item) for item in val) elif isinstance(val, dict): return all(is_supported(k) and is_supported(v) for k, v in val.items()) return True return False if not is_supported(value): raise ValueError(\\"Unsupported type encountered during serialization\\") with open(file_path, \'wb\') as f: marshal.dump(value, f) def safe_deserialize(file_path): Deserializes and returns the Python object from the specified file path using the marshal module. Parameters: file_path (str): The path to the file from where the serialized object will be read. Returns: any: The deserialized Python object. with open(file_path, \'rb\') as f: try: return marshal.load(f) except (EOFError, ValueError, TypeError) as e: raise e"},{"question":"Custom Exception Handling and Propagation Objective: Demonstrate your understanding of Python exception handling by implementing custom exceptions and effectively propagating exceptions through various layers of a program. Problem Description: You are tasked with designing a flight booking system that requires robust error handling. You need to create custom exceptions for various error conditions and implement a method to propagate these exceptions up the call stack. Detailed Requirements: 1. **Custom Exceptions**: - Create an exception class `BaseBookingError` to serve as the base class for other exceptions. - Create three specific exceptions inheriting from `BaseBookingError`: - `InvalidBookingError`: Raised when a booking has invalid parameters. - `BookingDatabaseError`: Raised for any database-related errors. - `BookingNetworkError`: Raised for network-related issues. 2. **Function Implementation**: Implement the following functions with appropriate error handling: ```python class BaseBookingError(Exception): pass class InvalidBookingError(BaseBookingError): pass class BookingDatabaseError(BaseBookingError): pass class BookingNetworkError(BaseBookingError): pass def create_booking(booking_details): Create a booking with the given details. Args: booking_details (dict): A dictionary containing booking information. Raises: InvalidBookingError: If booking details are invalid. # Your implementation here # Simulate a condition where the booking details are invalid raise InvalidBookingError(\\"Invalid booking details provided.\\") def save_booking_to_database(booking_details): Save the booking details to the database. Args: booking_details (dict): A dictionary containing booking information. Raises: BookingDatabaseError: If there is a database error. # Your implementation here # Simulate a condition where there is a database error raise BookingDatabaseError(\\"Failed to save booking to the database.\\") def send_booking_confirmation(booking_details): Send a booking confirmation. Args: booking_details (dict): A dictionary containing booking information. Raises: BookingNetworkError: If there is a network error. # Your implementation here # Simulate a condition where there is a network error raise BookingNetworkError(\\"Failed to send booking confirmation due to network issue.\\") def handle_booking(booking_details): Handle the complete booking process, propagating exceptions as needed. Args: booking_details (dict): A dictionary containing booking information. Returns: str: A message indicating success or failure of the booking process. Raises: BaseBookingError: If any booking-related error occurs. try: create_booking(booking_details) save_booking_to_database(booking_details) send_booking_confirmation(booking_details) except BaseBookingError as e: return f\\"Booking process failed: {str(e)}\\" return \\"Booking process completed successfully.\\" ``` Constraints: - `booking_details` is a dictionary with at least the keys `name` and `flight_number`. - Raise exceptions with meaningful error messages. - Propagate exceptions correctly so that the `handle_booking` function can catch and handle them gracefully. - Ensure that all functions are covered with exception handling. Input: ```python { \\"name\\": \\"John Doe\\", \\"flight_number\\": \\"AB1234\\" } ``` Output: - A string indicating the success or failure of the booking process. Example: ```python booking_details = { \\"name\\": \\"John Doe\\", \\"flight_number\\": \\"AB1234\\" } print(handle_booking(booking_details)) ``` Expected output (if an error occurs): ``` \\"Booking process failed: Invalid booking details provided.\\" ``` Or: `\\"Booking process failed: Failed to save booking to the database.\\"` Or: `\\"Booking process failed: Failed to send booking confirmation due to network issue.\\"`","solution":"class BaseBookingError(Exception): pass class InvalidBookingError(BaseBookingError): pass class BookingDatabaseError(BaseBookingError): pass class BookingNetworkError(BaseBookingError): pass def create_booking(booking_details): Create a booking with the given details. Args: booking_details (dict): A dictionary containing booking information. Raises: InvalidBookingError: If booking details are invalid. if not booking_details.get(\\"name\\") or not booking_details.get(\\"flight_number\\"): raise InvalidBookingError(\\"Invalid booking details provided.\\") def save_booking_to_database(booking_details): Save the booking details to the database. Args: booking_details (dict): A dictionary containing booking information. Raises: BookingDatabaseError: If there is a database error. raise BookingDatabaseError(\\"Failed to save booking to the database.\\") def send_booking_confirmation(booking_details): Send a booking confirmation. Args: booking_details (dict): A dictionary containing booking information. Raises: BookingNetworkError: If there is a network error. raise BookingNetworkError(\\"Failed to send booking confirmation due to network issue.\\") def handle_booking(booking_details): Handle the complete booking process, propagating exceptions as needed. Args: booking_details (dict): A dictionary containing booking information. Returns: str: A message indicating success or failure of the booking process. Raises: BaseBookingError: If any booking-related error occurs. try: create_booking(booking_details) save_booking_to_database(booking_details) send_booking_confirmation(booking_details) except BaseBookingError as e: return f\\"Booking process failed: {str(e)}\\" return \\"Booking process completed successfully.\\""},{"question":"# Advanced Seaborn Visualization Task Objective To demonstrate your understanding of the Seaborn package, particularly the `seaborn.objects` module, you will create an advanced visualization using the Titanic dataset. This task will assess your ability to: - Load a dataset - Utilize various Seaborn plotting techniques - Enhance plots with additional attributes like color and transparency - Apply faceting and stacking Task Description Using the Titanic dataset from Seaborn, create a visualization that satisfies the following requirements: 1. **Load Dataset**: Load the Titanic dataset using `seaborn.load_dataset()` and sort it in descending order by the `age` column. 2. **Create a Plot**: Use the `seaborn.objects` module to create a plot where: - The x-axis represents the `fare` variable. - The plot is facetted by the `embarked` variable. - The data should be visualized as stacked histograms with a bin width of 20. - The histograms should be colored by the `sex` variable. - The transparency of the bars should be defined by the `alive` status (deceased or survived). 3. **Display Plot**: Display the final plot. Input: - No direct inputs; assume the Titanic dataset is available and use the specified attributes. Output: - A facetted, stacked histogram plot as described. Example Code Outline ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load and sort dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"age\\", ascending=False) # Step 2: Create plot plot = ( so.Plot(titanic, x=\\"fare\\", color=\\"sex\\", alpha=\\"alive\\") .facet(\\"embarked\\") .add(so.Bars(), so.Hist(binwidth=20), so.Stack()) ) # Step 3: Display plot plot.show() ``` Note: Ensure that your solution follows this outline and meets the specified requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def titanic_visualization(): # Step 1: Load and sort dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"age\\", ascending=False) # Step 2: Create plot plot = ( so.Plot(titanic, x=\\"fare\\", color=\\"sex\\", alpha=\\"alive\\") .facet(\\"embarked\\") .add(so.Bars(), so.Hist(binwidth=20), so.Stack()) ) # Step 3: Display plot plot.show()"},{"question":"Problem Statement: You are given a Python class which may have type annotations. Your goal is to write a function `extract_and_eval_annotations(obj: Any) -> Dict[str, Any]` that extracts the annotations from the given object `obj` and evaluates any stringized annotations to their actual types. The function should handle cases for objects that are: - Functions - Classes - Modules Your function should work seamlessly in both Python 3.10 and newer, as well as in Python 3.9 and older. If the annotation is already evaluated, it should be returned as-is. Handle cases where `__annotations__` might not exist or where it might need unwrapping (like for partially defined functions). If an annotation cannot be evaluated, it should be included in the output as a string. Input: - `obj` - An object (could be a function, class, or module) from which to extract and evaluate annotations. Output: - A dictionary where keys are the parameter names of the `__annotations__` member of `obj` and values are the evaluated types or original strings if they could not be evaluated. Constraints: - The function should run efficiently with the assumption that `obj` could have multiple nested annotations. - Aim to handle most common types of evaluations, including basic data types and functions. Implementation Guidelines: 1. Use `inspect.get_annotations()` for Python 3.10 and newer for accessing the annotations. 2. For older versions, use the provided best practices, ensuring to handle cases for functions, classes, and modules differently. 3. Ensure that stringized annotations are converted to their actual types using `eval()`. 4. Use appropriate exception handling to return string values if evaluation fails. Example: ```python from typing import List class Example: a: int b: \\"List[int]\\" c: \\"str\\" print( extract_and_eval_annotations(Example) ) ``` Expected output: ```python {\'a\': <class \'int\'>, \'b\': typing.List[int], \'c\': <class \'str\'>} ``` **Note**: Ensure that you consider the different behaviors of `__annotations__` described in the documentation while solving the problem, especially the differences across Python versions.","solution":"import inspect from typing import Any, Dict, get_type_hints def extract_and_eval_annotations(obj: Any) -> Dict[str, Any]: Extracts and evaluates type annotations from a given object. Parameters: - obj: Any -- The object to extract and evaluate annotations from. Returns: - A dictionary with annotation names as keys and evaluated types or original strings as values. try: if hasattr(inspect, \\"get_annotations\\"): # For Python 3.10 and newer annotations = inspect.get_annotations(obj, eval_str=True) else: # For Python 3.9 and older annotations = get_type_hints(obj) return annotations except Exception as e: # Fallback and just return raw annotations as strings if evaluation fails if hasattr(obj, \\"__annotations__\\"): raw_annotations = obj.__annotations__ evaluated_annotations = {} for key, value in raw_annotations.items(): try: evaluated_annotations[key] = eval(value, vars(obj)) except Exception: evaluated_annotations[key] = value return evaluated_annotations else: return {}"},{"question":"Create a function named `process_urls` that accepts a list of URLs and performs the following tasks for each URL: 1. Parse the URL into its components such as scheme, netloc, path, params, query, and fragment. 2. Return a dictionary with these components as keys. The function should handle missing components appropriately with empty strings, except for port which should be None if not specified. 3. If a query component is present, parse the query string into a dictionary, where keys are query variable names and values are lists of values for each name. 4. If the URL contains a fragment, remove the fragment component from the URL and return the modified URL along with the original fragment in the output dictionary. **Input:** - A list of URLs (strings). **Output:** - A list of dictionaries. Each dictionary should contain: - `scheme`: URL scheme specifier (string) - `netloc`: Network location part (string) - `path`: Hierarchical path (string) - `params`: Parameters for last path element (string) - `query`: Dictionary of query parameters (dict) - `fragment`: Fragment identifier (string) - `port`: Port number (int or None) - `modified_url`: URL without fragment identifier (string) # Constraints: - All URLs are valid. - The function should perform well with up to 1000 URLs in the input list. # Example: Input: ```python urls = [ \\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=params#url-parsing\\", \\"https://www.example.com/path/to/resource?search=test&lang=en#section-2\\" ] ``` Expected Output: ```python [ { \\"scheme\\": \\"http\\", \\"netloc\\": \\"docs.python.org:80\\", \\"path\\": \\"/3/library/urllib.parse.html\\", \\"params\\": \\"\\", \\"query\\": {\\"highlight\\": [\\"params\\"]}, \\"fragment\\": \\"url-parsing\\", \\"port\\": 80, \\"modified_url\\": \\"http://docs.python.org:80/3/library/urllib.parse.html?highlight=params\\" }, { \\"scheme\\": \\"https\\", \\"netloc\\": \\"www.example.com\\", \\"path\\": \\"/path/to/resource\\", \\"params\\": \\"\\", \\"query\\": {\\"search\\": [\\"test\\"], \\"lang\\": [\\"en\\"]}, \\"fragment\\": \\"section-2\\", \\"port\\": None, \\"modified_url\\": \\"https://www.example.com/path/to/resource?search=test&lang=en\\" } ] ``` Function Signature: ```python def process_urls(urls: List[str]) -> List[dict]: pass ``` # Note: - You may use functions from `urllib.parse` such as `urlparse`, `parse_qs`, and `urldefrag` to assist with the implementation. - Ensure proper handling of edge cases where certain URL components might be missing. - Make sure to handle both ASCII and Unicode characters correctly within the URLs.","solution":"from urllib.parse import urlparse, parse_qs, urldefrag def process_urls(urls): result = [] for url in urls: # Parse the URL components parsed_url = urlparse(url) # Extract the components and handle missing ones accordingly scheme = parsed_url.scheme or \\"\\" netloc = parsed_url.netloc or \\"\\" path = parsed_url.path or \\"\\" params = parsed_url.params or \\"\\" fragment = parsed_url.fragment or \\"\\" # Parse the query into dictionary form query = parse_qs(parsed_url.query) # Extract the port number if available, otherwise None port = parsed_url.port # Remove the fragment component from the URL modified_url, original_fragment = urldefrag(url) # Prepare the dictionary for the current URL url_dict = { \\"scheme\\": scheme, \\"netloc\\": netloc, \\"path\\": path, \\"params\\": params, \\"query\\": query, \\"fragment\\": fragment, \\"port\\": port, \\"modified_url\\": modified_url } result.append(url_dict) return result"},{"question":"# Audio Processing with OSS Audio Devices in Python You are tasked with creating a function that records a short audio clip from an audio input device and plays it back. The audio recording should be configured to CD quality parameters (44,100 Hz sample rate, 2 channels, \'AFMT_S16_LE\' format). The function should handle any potential errors gracefully. # Requirements 1. Open the audio input device in read mode. 2. Configure the audio parameters: - Format: \'AFMT_S16_LE\' - Channels: 2 - Sample rate: 44,100 Hz 3. Record 5 seconds of audio data from the input device. 4. Open the audio output device in write mode. 5. Use the same audio parameters to configure the output device. 6. Play back the recorded audio data. 7. Ensure the devices are properly closed after the operations. # Function Signature ```python import ossaudiodev def record_and_playback(): pass ``` # Constraints - Use the `ossaudiodev.open()` function to open both input and output devices. - Handle exceptions where necessary, such as when opening devices or setting parameters fails. - Use the `writeall()` method for output to ensure all audio data is played back correctly. - Make sure to include appropriate error handling and resource management (e.g., using `try-finally` or `with` statements). # Example ```python try: record_and_playback() except Exception as e: print(f\\"An error occurred: {e}\\") ``` # Evaluation Criteria - Correctly opens and configures the audio devices. - Accurately records audio data and plays it back. - Proper error handling and closure of devices. - Clear, well-commented code with proper use of the `ossaudiodev` methods and attributes.","solution":"import ossaudiodev import time def record_and_playback(): Records 5 seconds of audio from the audio input device and plays it back using the audio output device with CD quality parameters. try: input_device = ossaudiodev.open(\'r\') output_device = ossaudiodev.open(\'w\') # Set audio format parameters input_device.setparameters(ossaudiodev.AFMT_S16_LE, 2, 44100) output_device.setparameters(ossaudiodev.AFMT_S16_LE, 2, 44100) # Calculate the number of bytes to record for 5 seconds sample_rate = 44100 channels = 2 duration = 5 # seconds bytes_per_sample = 2 # 16-bit audio has 2 bytes per sample num_bytes = sample_rate * channels * bytes_per_sample * duration # Record audio data audio_data = input_device.read(num_bytes) # Playback recorded audio data output_device.writeall(audio_data) except (OSError, IOError) as e: print(f\\"An error occurred: {e}\\") finally: try: # Ensure devices are closed input_device.close() output_device.close() except NameError: pass"},{"question":"**Assessment Question: Implementing and Migrating Module Import Routines** The `imp` module provides several functions to manage and manipulate Python module imports, though they are deprecated in favor of `importlib`. Your task is to use the `imp` module to implement a function that dynamically imports a specified module by name. Afterward, you should migrate this implementation to use the recommended replacements from the `importlib` module. **Part 1: Using `imp` Module** Implement the function `import_module_imp(module_name: str) -> ModuleType:` that: 1. Takes the `module_name` as a string. 2. Uses the `imp.find_module` and `imp.load_module` to find and load the module dynamically. 3. Ensures that resources are cleanly released after importing, even in case of exceptions. **Function Signature:** ```python def import_module_imp(module_name: str) -> ModuleType: pass ``` **Part 2: Migrating to `importlib`** Implement a similar function `import_module_importlib(module_name: str) -> ModuleType:` using `importlib.util.find_spec`, `importlib.util.module_from_spec`, and `importlib.import_module`. **Function Signature:** ```python def import_module_importlib(module_name: str) -> ModuleType: pass ``` **Example Usage:** ```python # Using imp module module = import_module_imp(\\"math\\") print(module.sqrt(16)) # Expected Output: 4.0 # Using importlib module module = import_module_importlib(\\"math\\") print(module.sqrt(16)) # Expected Output: 4.0 ``` **Constraints:** - Do not use the built-in `import` statement or `__import__` function directly in your implementation. - Ensure your function handles exceptions and cleans up any file resources appropriately. - Your implementation should handle built-in modules and modules on the file system. **Test Cases:** - Test importing standard modules like `math`, `sys`. - Test importing third-party modules if available. - Test scenarios where the module does not exist. Implement both functions and ensure they pass all given test cases.","solution":"import imp import importlib from types import ModuleType # Part 1: Using imp module def import_module_imp(module_name: str) -> ModuleType: try: file, pathname, description = imp.find_module(module_name) try: return imp.load_module(module_name, file, pathname, description) finally: if file: file.close() except Exception as e: raise ImportError(f\\"Could not import module {module_name}\\") from e # Part 2: Migrating to importlib module def import_module_importlib(module_name: str) -> ModuleType: try: spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Could not find module {module_name}\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module except Exception as e: raise ImportError(f\\"Could not import module {module_name}\\") from e"},{"question":"You are required to implement a mini application that performs the following tasks: 1. **Serialization (Using Pickle)** - Create a Python function `serialize_data(data, filename)` that takes a Python object `data` and a string `filename`. The function should serialize the object using the `pickle` module and save it to the given filename. 2. **Deserialization (Using Pickle)** - Create a Python function `deserialize_data(filename)` that takes a string `filename`, reads the file, and deserializes the Python object using the `pickle` module, returning the Python object. 3. **Database Interaction (Using SQLite3)** - Implement a function `store_data_to_db(data, db_name, table_name)` that takes a dictionary `data`, a string `db_name`, and a string `table_name`. The function should store the dictionary keys and values to a SQLite database table. If the table does not exist, the function should create it. Each dictionary key-value pair should be inserted as a new row in the table. 4. **Retrieve Data from Database (Using SQLite3)** - Implement a function `retrieve_data_from_db(db_name, table_name)` that takes a string `db_name` and a string `table_name`. The function should retrieve all the rows from the specified table and return them as a dictionary where the keys are the first column values and the values are the corresponding second column values. # Constraints - You should handle exceptions properly for file operations and database operations. - Assume that the dictionary data for `store_data_to_db` will have string keys and string values. # Example Usage ```python data = {\'name\': \'Alice\', \'age\': \'30\', \'city\': \'New York\'} filename = \'data.pkl\' db_name = \'test.db\' table_name = \'people\' # Serialize the data serialize_data(data, filename) # Deserialize the data deserialized_data = deserialize_data(filename) assert deserialized_data == data # Store data to the database store_data_to_db(data, db_name, table_name) # Retrieve data from the database retrieved_data = retrieve_data_from_db(db_name, table_name) assert retrieved_data == data ``` Note: Ensure that your solution performs necessary checks, handles exceptions, and follows good coding practices.","solution":"import pickle import sqlite3 def serialize_data(data, filename): Serializes a Python object `data` to a file with the name `filename`. try: with open(filename, \'wb\') as file: pickle.dump(data, file) except Exception as e: print(f\\"An error occurred while serializing data: {e}\\") def deserialize_data(filename): Deserializes a Python object from a file with the name `filename`. Returns the deserialized object. try: with open(filename, \'rb\') as file: return pickle.load(file) except Exception as e: print(f\\"An error occurred while deserializing data: {e}\\") return None def store_data_to_db(data, db_name, table_name): Stores dictionary `data` into a SQLite database `db_name` into a table `table_name`. If the table does not exist, it will create it. Each dictionary key-value pair will be inserted as a new row. try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(f CREATE TABLE IF NOT EXISTS {table_name} ( key TEXT PRIMARY KEY, value TEXT ) ) for key, value in data.items(): cursor.execute(f INSERT OR REPLACE INTO {table_name} (key, value) VALUES (?, ?) , (key, value)) conn.commit() conn.close() except Exception as e: print(f\\"An error occurred while storing data to the database: {e}\\") def retrieve_data_from_db(db_name, table_name): Retrieves all rows from a table `table_name` in SQLite database `db_name`. Returns the data as a dictionary where keys are the first column values and values are the second column values. try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(f\\"SELECT * FROM {table_name}\\") rows = cursor.fetchall() conn.close() return {key: value for key, value in rows} except Exception as e: print(f\\"An error occurred while retrieving data from the database: {e}\\") return {}"},{"question":"Hotel Booking Data Analysis You are provided with a dataset that contains hotel booking information. The dataset includes the following columns: - `Hotel`: The type of hotel (either \\"City Hotel\\" or \\"Resort Hotel\\"). - `IsCanceled`: Whether the booking was canceled (1) or not (0). - `LeadTime`: The number of days between the booking date and the arrival date. - `ArrivalDateYear`: Year of arrival date. - `ArrivalDateMonth`: Month of arrival date. - `ArrivalDateWeekNumber`: Week number of arrival date. - `ArrivalDateDayOfMonth`: Day of arrival date. - `StaysInWeekendNights`: Number of weekend nights (Saturday or Sunday) the guest stayed or booked to stay at the hotel. - `StaysInWeekNights`: Number of week nights (Monday to Friday) the guest stayed or booked to stay at the hotel. - `Adults`: Number of adults. - `Children`: Number of children. - `Babies`: Number of babies. - `Meal`: Type of meal booked. - `Country`: Country of origin. - `MarketSegment`: Market segment designation. - `DistributionChannel`: Booking distribution channel. - `IsRepeatedGuest`: Whether the booking is from a repeated guest (1) or not (0). - `PreviousCancellations`: Number of previous bookings that were canceled by the customer prior to the current booking. - `PreviousBookingsNotCanceled`: Number of previous bookings not canceled by the customer prior to the current booking. - `ReservedRoomType`: Code of room type reserved. - `AssignedRoomType`: Code of room type assigned. - `BookingChanges`: Number of changes/amendments made to the booking from the moment the booking was started until the moment of check-in or cancellation. - `DepositType`: Type of deposit made with the booking. - `Agent`: ID of the travel agency that made the booking. - `Company`: ID of the company/entity that made the booking. - `DaysInWaitingList`: Number of days the booking was in the waiting list before it was confirmed. - `CustomerType`: Type of booking, assuming one of four categories (Contract, Group, Transient, Transient-Party). - `ADR`: Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights. - `RequiredCarParkingSpaces`: Number of car parking spaces required by the customer. - `TotalOfSpecialRequests`: Number of special requests made by the customer (e.g., twin bed or high floor). - `ReservationStatus`: Reservation last status (‘Canceled’, ‘Check-Out’, ‘No-Show’). - `ReservationStatusDate`: Date at which the last status was set. Your tasks involve data analysis using the `pandas` library. Implement the following functions: Tasks 1. **Load the Data:** Load the dataset from a CSV file located at `file_path`. ```python def load_data(file_path: str) -> pd.DataFrame: Load the dataset from the given file path. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame. pass ``` 2. **Preprocess the Data:** - Fill any missing values in the `Children` column with 0. - Convert the `ReservationStatusDate` column into a datetime object. - Filter out the rows where `Adults`, `Children`, and `Babies` are all 0 (i.e., no guests). ```python def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: Preprocess the dataset by filling missing values, converting date columns, and filtering rows. Parameters: df (pd.DataFrame): The original DataFrame. Returns: pd.DataFrame: The preprocessed DataFrame. pass ``` 3. **Compute the Cancellation Rate:** Calculate and return the cancellation rate (i.e., the proportion of bookings that were canceled) for each hotel type. ```python def cancellation_rate_by_hotel_type(df: pd.DataFrame) -> pd.Series: Compute the cancellation rate for each hotel type. Parameters: df (pd.DataFrame): The preprocessed DataFrame. Returns: pd.Series: The cancellation rate for each hotel type. pass ``` 4. **Average Lead Time by Market Segment:** Calculate and return the average lead time (the number of days between the booking date and the arrival date) for each market segment. ```python def average_lead_time_by_market_segment(df: pd.DataFrame) -> pd.Series: Compute the average lead time for each market segment. Parameters: df (pd.DataFrame): The preprocessed DataFrame. Returns: pd.Series: The average lead time for each market segment. pass ``` 5. **Monthly ADR:** Calculate and return the Average Daily Rate (ADR) for each hotel type by month and year. ```python def monthly_adr_by_hotel_type(df: pd.DataFrame) -> pd.DataFrame: Compute the ADR for each hotel type by month and year. Parameters: df (pd.DataFrame): The preprocessed DataFrame. Returns: pd.DataFrame: The ADR for each hotel type by month and year. pass ``` 6. **Save the Cleaned Data:** Save the cleaned and preprocessed data to a new CSV file. ```python def save_cleaned_data(df: pd.DataFrame, file_path: str) -> None: Save the cleaned DataFrame to a CSV file. Parameters: df (pd.DataFrame): The DataFrame to be saved. file_path (str): The path where the CSV file will be saved. pass ``` **Input:** - The dataset will be provided in a CSV file format. **Output:** - Implementations of the above functions and any intermediate steps needed to achieve the required results. **Constraints:** - You must use the pandas library to manipulate the DataFrame. - The dataset size can be large, so aim for efficient operations. **Performance Requirements:** - Efficient handling of large datasets. - Clear and readable code structure.","solution":"import pandas as pd def load_data(file_path: str) -> pd.DataFrame: Load the dataset from the given file path. Parameters: file_path (str): The path to the CSV file. Returns: pd.DataFrame: The loaded DataFrame. return pd.read_csv(file_path) def preprocess_data(df: pd.DataFrame) -> pd.DataFrame: Preprocess the dataset by filling missing values, converting date columns, and filtering rows. Parameters: df (pd.DataFrame): The original DataFrame. Returns: pd.DataFrame: The preprocessed DataFrame. df[\'Children\'].fillna(0, inplace=True) df[\'ReservationStatusDate\'] = pd.to_datetime(df[\'ReservationStatusDate\']) df = df[(df[\'Adults\'] + df[\'Children\'] + df[\'Babies\']) > 0] return df def cancellation_rate_by_hotel_type(df: pd.DataFrame) -> pd.Series: Compute the cancellation rate for each hotel type. Parameters: df (pd.DataFrame): The preprocessed DataFrame. Returns: pd.Series: The cancellation rate for each hotel type. cancellation_rates = df.groupby(\'Hotel\')[\'IsCanceled\'].mean() return cancellation_rates def average_lead_time_by_market_segment(df: pd.DataFrame) -> pd.Series: Compute the average lead time for each market segment. Parameters: df (pd.DataFrame): The preprocessed DataFrame. Returns: pd.Series: The average lead time for each market segment. avg_lead_times = df.groupby(\'MarketSegment\')[\'LeadTime\'].mean() return avg_lead_times def monthly_adr_by_hotel_type(df: pd.DataFrame) -> pd.DataFrame: Compute the ADR for each hotel type by month and year. Parameters: df (pd.DataFrame): The preprocessed DataFrame. Returns: pd.DataFrame: The ADR for each hotel type by month and year. df[\'YearMonth\'] = df[\'ArrivalDateYear\'].astype(str) + \'-\' + df[\'ArrivalDateMonth\'].apply(lambda x: f\\"{x:02d}\\") adr_by_month = df.groupby([\'Hotel\', \'YearMonth\'])[\'ADR\'].mean().reset_index() return adr_by_month def save_cleaned_data(df: pd.DataFrame, file_path: str) -> None: Save the cleaned DataFrame to a CSV file. Parameters: df (pd.DataFrame): The DataFrame to be saved. file_path (str): The path where the CSV file will be saved. df.to_csv(file_path, index=False)"},{"question":"Objective: To assess your understanding of seaborn\'s object interface by creating complex visualizations and applying transformations to a dataset. Problem Statement: You have been provided a dataset `diamonds` from seaborn\'s dataset repository. Your task is to implement a function `create_diamond_plot()` that generates a visual representation of the dataset. The visualization should highlight the distribution of diamond prices across different categories of cuts, customized with specific statistical transformations. Function Signature: ```python def create_diamond_plot(): pass ``` # Instructions: 1. Load the `diamonds` dataset using the `load_dataset` function from seaborn. 2. Create a plot object with the x-axis representing the different `cut` categories and the y-axis representing `price`. 3. Apply a logarithmic scale to the y-axis. 4. Add dot plots that compute and display the quartiles (25th, 50th, and 75th percentiles) and the minimum and maximum prices. 5. Customize the plot by: - Adding a range mark to show a percentile interval (25th to 75th). - Applying jitter to the data points. - Coloring the range mark in black. - Setting the transparency of individual dots to 0.2 and the point size to 1. Expected Output: A plot should be displayed that visually represents the requested statistical measures and transformations on the `diamonds` dataset. Example: While we cannot provide an exact visual representation within this text, the output should follow these steps: ```python import seaborn.objects as so from seaborn import load_dataset def create_diamond_plot(): diamonds = load_dataset(\\"diamonds\\") ( so.Plot(diamonds, \\"cut\\", \\"price\\") .scale(y=\\"log\\") .add(so.Dot(), so.Perc([0, 25, 50, 75, 100])) .add(so.Dots(pointsize=1, alpha=.2), so.Jitter(.3)) .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=.2)) ).show() ``` Constraints: - Ensure proper handling of the seaborn.objects interface and applicable methods. - The plot should accurately represent the diamonds data with respect to the given transformations and statistical measures. - Utilize efficient methods to achieve the desired result with minimal computation overhead.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_diamond_plot(): # Load the diamonds dataset from seaborn diamonds = load_dataset(\\"diamonds\\") # Create the plot with specified transformations and customizations ( so.Plot(diamonds, x=\\"cut\\", y=\\"price\\") .scale(y=\\"log\\") # Apply logarithmic scale to the y-axis .add(so.Dot(), so.Perc([0, 25, 50, 75, 100])) # Add dot plots at specified percentiles .add(so.Dots(pointsize=1, alpha=.2), so.Jitter(width=0.3)) # Add jittered dots with specified point size and transparency .add(so.Range(color=\\"k\\"), so.Perc([25, 75]), so.Shift(y=.2)) # Add range marks for percentile interval 25th to 75th percentile ).show()"},{"question":"Permutation Feature Importance: Coding Assessment Question # Objective You are required to write code that demonstrates the computation of permutation feature importance for a regression model using scikit-learn. The goal is to evaluate students\' understanding of how to implement and use permutation importance with different metrics and interpret the results. # Problem Statement Given the Diabetes dataset from scikit-learn, train a linear regression model using the Ridge regression algorithm. Then, compute and interpret the permutation feature importance of the trained model using different evaluation metrics. # Instructions 1. Import the necessary libraries and load the Diabetes dataset. 2. Split the dataset into training and validation sets. 3. Train a Ridge regression model using the training set. 4. Compute the permutation feature importance of the trained model on the validation set using three different evaluation metrics: `r2`, `neg_mean_absolute_percentage_error`, and `neg_mean_squared_error`. 5. Print the feature importances along with their standard deviations for each metric. 6. Interpret the results, highlighting which features are most important across the different metrics. # Requirements - Implement the solution using scikit-learn. - Use the `permutation_importance` function from the `sklearn.inspection` module. - Ensure the code is well-commented and follows best practices. # Constraints - Use `n_repeats=30` for computing the permutation importance. - Set `random_state=0` for reproducibility. - The Ridge regression model should use `alpha=1e-2`. # Input - The function does not take any input parameters. # Output - The function should print the permutation feature importances along with their standard deviations for each specified metric. # Example Output ```plaintext Feature Importances using r2: s5 0.204 +/- 0.050 bmi 0.176 +/- 0.048 bp 0.088 +/- 0.033 sex 0.056 +/- 0.023 Feature Importances using neg_mean_absolute_percentage_error: s5 0.081 +/- 0.020 bmi 0.064 +/- 0.015 bp 0.029 +/- 0.010 Feature Importances using neg_mean_squared_error: s5 1013.866 +/- 246.445 bmi 872.726 +/- 240.298 bp 438.663 +/- 163.022 sex 277.376 +/- 115.123 ``` # Hints - Refer to the scikit-learn documentation for additional details on the `permutation_importance` function. - Make use of the Diabetes dataset available in scikit-learn\'s `datasets` module. - Use `train_test_split` from scikit-learn\'s `model_selection` module to split the dataset. - Employ the `Ridge` model from scikit-learn\'s `linear_model` module.","solution":"import numpy as np import pandas as pd from sklearn.datasets import load_diabetes from sklearn.linear_model import Ridge from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error def compute_permutation_feature_importance(): # Load the Diabetes dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target feature_names = diabetes.feature_names # Split the dataset into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0) # Train a Ridge regression model model = Ridge(alpha=1e-2, random_state=0) model.fit(X_train, y_train) # Define the metrics metrics = { \'r2\': r2_score, \'neg_mean_squared_error\': mean_squared_error, \'neg_mean_absolute_percentage_error\': mean_absolute_percentage_error, } for metric_name, metric in metrics.items(): # Compute permutation feature importance using the specified metric importance = permutation_importance(model, X_val, y_val, scoring=metric_name, n_repeats=30, random_state=0) print(f\\"nFeature Importances using {metric_name}:\\") importances_means = importance.importances_mean importances_std = importance.importances_std sorted_idx = importances_means.argsort()[::-1] importance_df = pd.DataFrame({ \'Feature\': np.array(feature_names)[sorted_idx], \'Importance Mean\': importances_means[sorted_idx], \'Importance Std\': importances_std[sorted_idx] }) print(importance_df) # Call the function to compute and print the permutation feature importances compute_permutation_feature_importance()"},{"question":"# Custom PyTorch Autograd Function Implementation **Objective:** To ensure deep understanding of PyTorch’s `torch.autograd.Function` and extending its functionality, the task involves creating a custom autograd function and a neural network layer using this function. **Problem Statement:** You are required to implement a custom autograd function for a special kind of linear transformation called `BinaryStepLinear` that applies a weight matrix and a binary step activation, followed by creating a custom PyTorch module `BinaryStepLinearLayer` utilizing this function. 1. **BinaryStepLinear Function:** - Create a custom autograd function `BinaryStepLinearFunction` that performs a linear transformation using a weight matrix and an optional bias, followed by a binary step activation. - The forward pass should compute `output = step(input * weight^T + bias)`, where `step(x) = 1 if x > 0 else 0`. - The backward pass should compute gradients only for `weight` and `bias`. Since the step function is non-differentiable, treat it as having zero gradient everywhere. 2. **BinaryStepLinearLayer Module:** - Create a custom module `BinaryStepLinearLayer` which uses the `BinaryStepLinearFunction` to integrate it into a PyTorch model. The module should accept parameters for input features and output features and optionally for bias. - The layer should have parameters `weight` and optionally `bias` initialized randomly using a uniform distribution in the range `[-0.1, 0.1]`. Output your implementation, and demonstrate its use in a simple neural network for a classification task on a toy dataset. **Details:** 1. Define the class `BinaryStepLinearFunction`: - Implement the `forward` method that takes `input`, `weight`, and `bias` tensors. - Implement the `setup_context` if necessary, or use combined forward method. - Implement the `backward` method to return gradients with respect to `input`, `weight`, and `bias`. 2. Define the module `BinaryStepLinearLayer` extending `torch.nn.Module`: - Initialize the module with given specifications of input and output features and initialize parameters. - Implement the `forward` method using `BinaryStepLinearFunction.apply`. **Constraints:** - All tensor arguments must support batching. - Minimize memory consumption by leveraging in-place operations, where possible without affecting autograd. **Example Usage:** ```python import torch import torch.nn as nn import torch.optim as optim # Custom autograd function class BinaryStepLinearFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weight, bias=None): output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) ctx.save_for_backward(input, weight, bias) return (output > 0).float() @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias # Custom layer class BinaryStepLinearLayer(nn.Module): def __init__(self, input_features, output_features, bias=True): super(BinaryStepLinearLayer, self).__init__() self.input_features = input_features self.output_features = output_features self.weight = nn.Parameter(torch.empty(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.empty(output_features)) else: self.register_parameter(\'bias\', None) nn.init.uniform_(self.weight, -0.1, 0.1) if self.bias is not None: nn.init.uniform_(self.bias, -0.1, 0.1) def forward(self, input): return BinaryStepLinearFunction.apply(input, self.weight, self.bias) # Demonstrating the use of the custom layer class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.layer = BinaryStepLinearLayer(10, 3) def forward(self, x): return self.layer(x) data = torch.randn(5, 10) target = torch.randint(0, 2, (5, 3)).float() model = SimpleNN() criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(50): optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(\\"Training complete!\\") ``` Your task is to fill in the implementation details for the custom function and module as demonstrated.","solution":"import torch import torch.nn as nn import torch.optim as optim class BinaryStepLinearFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weight, bias=None): ctx.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return (output > 0).float() @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) # The gradient through the binary step function is zero grad_output.zero_() return grad_input, grad_weight, grad_bias class BinaryStepLinearLayer(nn.Module): def __init__(self, input_features, output_features, bias=True): super(BinaryStepLinearLayer, self).__init__() self.input_features = input_features self.output_features = output_features self.weight = nn.Parameter(torch.empty(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.empty(output_features)) else: self.register_parameter(\'bias\', None) nn.init.uniform_(self.weight, -0.1, 0.1) if self.bias is not None: nn.init.uniform_(self.bias, -0.1, 0.1) def forward(self, input): return BinaryStepLinearFunction.apply(input, self.weight, self.bias)"},{"question":"Data Marshalling Implementation **Objective**: Write a Python module that utilizes the C API functions for marshalling and unmarshalling objects. You will then use this module to read and write some sample data. Requirements 1. Implement a Python class `DataMarshalling` that wraps the following functionalities: - `write_long_to_file(self, value: int, filename: str, version: int) -> None`: Writes a long integer to the specified file. - `write_object_to_file(self, value: any, filename: str, version: int) -> None`: Writes a Python object to the specified file. - `write_object_to_string(self, value: any, version: int) -> bytes`: Returns a bytes object containing the marshalled representation of the value. - `read_long_from_file(self, filename: str) -> int`: Reads a long integer from the specified file. - `read_short_from_file(self, filename: str) -> int`: Reads a short integer from the specified file. - `read_object_from_file(self, filename: str) -> any`: Reads a Python object from the specified file. - `read_last_object_from_file(self, filename: str) -> any`: Reads a Python object from the specified file, optimized for memory. - `read_object_from_string(self, data: bytes) -> any`: Reads a Python object from the specified bytes data. 2. Use the class to perform the following operations: - Write a long integer `123456789` to `long.dat` using version 2. - Write the Python object `[1, 2, 3, \\"test\\", {\\"key\\": \\"value\\"}]` to `object.dat` using version 2. - Return the bytes representation of the Python object `{\\"name\\": \\"John\\", \\"age\\": 30}` using version 2. - Read back the long integer from `long.dat`. - Read back the Python object from `object.dat`. - Read back an object from its byte representation. Input and Output Formats - Function signatures are provided and students need to adhere to these signatures while writing method definitions. - Ensure that whenever required, the file operations are done in binary mode. Handle all exceptions appropriately and provide meaningful error messages. Example Execution ```python dm = DataMarshalling() dm.write_long_to_file(123456789, \'long.dat\', 2) print(dm.read_long_from_file(\'long.dat\')) # Output: 123456789 dm.write_object_to_file([1, 2, 3, \\"test\\", {\\"key\\": \\"value\\"}], \'object.dat\', 2) print(dm.read_object_from_file(\'object.dat\')) # Output: [1, 2, 3, \\"test\\", {\\"key\\": \\"value\\"}] bytes_data = dm.write_object_to_string({\\"name\\": \\"John\\", \\"age\\": 30}, 2) print(dm.read_object_from_string(bytes_data)) # Output: {\\"name\\": \\"John\\", \\"age\\": 30} ``` **Constraints**: - Use proper error handling for file operations and marshalling/unmarshalling methods. - Ensure the Python interface correctly calls the appropriate C API functions described in the documentation. **Note**: You do not need to actually implement the C functions; just assume they are there and focus on the Python side.","solution":"import marshal class DataMarshalling: def write_long_to_file(self, value: int, filename: str, version: int) -> None: try: with open(filename, \'wb\') as file: marshal.dump(value, file, version) except Exception as e: print(f\\"Error while writing long integer to file: {e}\\") def write_object_to_file(self, value: any, filename: str, version: int) -> None: try: with open(filename, \'wb\') as file: marshal.dump(value, file, version) except Exception as e: print(f\\"Error while writing object to file: {e}\\") def write_object_to_string(self, value: any, version: int) -> bytes: try: return marshal.dumps(value, version) except Exception as e: print(f\\"Error while writing object to string: {e}\\") return b\'\' def read_long_from_file(self, filename: str) -> int: try: with open(filename, \'rb\') as file: return marshal.load(file) except Exception as e: print(f\\"Error while reading long integer from file: {e}\\") return None def read_short_from_file(self, filename: str) -> int: return self.read_long_from_file(filename) def read_object_from_file(self, filename: str) -> any: try: with open(filename, \'rb\') as file: return marshal.load(file) except Exception as e: print(f\\"Error while reading object from file: {e}\\") return None def read_last_object_from_file(self, filename: str) -> any: return self.read_object_from_file(filename) def read_object_from_string(self, data: bytes) -> any: try: return marshal.loads(data) except Exception as e: print(f\\"Error while reading object from string: {e}\\") return None"},{"question":"**Coding Assessment Question** In this assessment, you will work with the California Housing dataset from the `scikit-learn` library. The goal is to predict house prices in California based on various features. The assessment will test your ability to load data, perform exploratory data analysis, preprocess the data, and build a machine learning model. You are required to complete the following tasks: 1. **Load the Dataset** - Use the `fetch_california_housing` function from `scikit-learn` to load the California Housing dataset. 2. **Exploratory Data Analysis (EDA)** - Provide a summary of the dataset, including the number of instances, feature names, and any missing values. - Visualize the distribution of house prices and the feature `MedInc` (median income). 3. **Data Preprocessing** - Check for missing values and handle them if necessary. - Normalize the feature values. - Split the dataset into training and testing sets (80-20 split). 4. **Build and Train a Machine Learning Model** - Implement a linear regression model using `LinearRegression` from `scikit-learn` to predict house prices. - Train the model on the training set. - Evaluate the model on the testing set using Mean Squared Error (MSE) and R² score. 5. **Model Improvement (Bonus)** - Try at least one other regression model from `scikit-learn` (e.g., DecisionTreeRegressor, RandomForestRegressor, etc.) and compare its performance with the linear regression model. **Input and Output formats:** - No specific input format is required as the students will load the dataset directly within their code. - The output should include: - A summary of the dataset. - Visualizations for EDA. - MSE and R² score for both linear regression and the other regression model (if implemented). **Constraints:** - You must use functions and classes from `scikit-learn` for data loading, preprocessing, model building, and evaluation. - Ensure that the code is well-structured, commented, and follows standard Python conventions. **Performance Requirements:** - The code should be efficient and able to handle the dataset within a reasonable time frame. - Proper use of random states for reproducibility in data splitting and model building. **Example Output:** ``` Dataset Summary: - Number of instances: 20640 - Features: MedInc, HouseAge, etc. - Missing values: None Visualizations: (provided as charts) Linear Regression Model: - MSE: 0.55 - R²: 0.62 RandomForestRegressor Model (Bonus): - MSE: 0.42 - R²: 0.72 ```","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt import pandas as pd def load_data(): data = fetch_california_housing() return data def summarize_data(data): num_instances = data[\'data\'].shape[0] feature_names = data[\'feature_names\'] missing_values = pd.DataFrame(data[\'data\'], columns=feature_names).isnull().sum().sum() return num_instances, feature_names, missing_values def visualize_data(data): df = pd.DataFrame(data[\'data\'], columns=data[\'feature_names\']) df[\'MedHouseVal\'] = data[\'target\'] plt.figure(figsize=(10, 5)) plt.subplot(1, 2, 1) plt.hist(df[\'MedHouseVal\'], bins=30) plt.xlabel(\'Median House Value\') plt.ylabel(\'Frequency\') plt.title(\'Distribution of House Prices\') plt.subplot(1, 2, 2) plt.hist(df[\'MedInc\'], bins=30) plt.xlabel(\'Median Income\') plt.ylabel(\'Frequency\') plt.title(\'Distribution of Median Income\') plt.tight_layout() plt.show() def preprocess_data(data): df = pd.DataFrame(data[\'data\'], columns=data[\'feature_names\']) df[\'MedHouseVal\'] = data[\'target\'] scaler = MinMaxScaler() scaled_features = scaler.fit_transform(df.drop(\'MedHouseVal\', axis=1)) df.loc[:, df.columns != \'MedHouseVal\'] = scaled_features X = df.drop(\'MedHouseVal\', axis=1) y = df[\'MedHouseVal\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_linear_regression(X_train, y_train): model = LinearRegression() model.fit(X_train, y_train) return model def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return mse, r2 def main(): data = load_data() num_instances, feature_names, missing_values = summarize_data(data) visualize_data(data) X_train, X_test, y_train, y_test = preprocess_data(data) linear_regression_model = train_linear_regression(X_train, y_train) mse, r2 = evaluate_model(linear_regression_model, X_test, y_test) print(f\\"Dataset Summary:n- Number of instances: {num_instances}n- Features: {\', \'.join(feature_names)}n- Missing values: {missing_values}\\") print(f\\"nLinear Regression Model:n- MSE: {mse:.2f}n- R²: {r2:.2f}\\") if __name__ == \\"__main__\\": main()"},{"question":"# XML Data Processing with `xml.dom.pulldom` **Problem Statement:** You are given a large XML document containing books data in a library. Each book element contains information such as title, author, year, price, and genre. Your task is to write a Python function that uses the `xml.dom.pulldom` module to parse this XML document and print out the titles and authors of all books published after the year 2000 and costing more than 20. The XML structure is as follows: ```xml <library> <book> <title>Book Title 1</title> <author>Author Name 1</author> <year>1999</year> <price>15.99</price> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <year>2005</year> <price>25.99</price> <genre>Science</genre> </book> ... </library> ``` **Function Signature:** ```python def filter_books(xml_string: str) -> None: ... ``` **Input:** - `xml_string`: A string containing the XML data. **Output:** - The function should print the title and author of each book meeting the criteria, each on a new line in the format `\\"{title} by {author}\\"`. **Constraints:** - You may assume that the input XML string is well-formed. - Focus on efficient parsing and event handling using `xml.dom.pulldom`. **Example:** ```python xml_data = \'\'\'<library> <book> <title>Book Title 1</title> <author>Author Name 1</author> <year>1999</year> <price>15.99</price> <genre>Fiction</genre> </book> <book> <title>Book Title 2</title> <author>Author Name 2</author> <year>2005</year> <price>25.99</price> <genre>Science</genre> </book> </library>\'\'\' filter_books(xml_data) ``` Output: ``` Book Title 2 by Author Name 2 ``` **Notes:** - Utilize the `xml.dom.pulldom` module to parse and handle the XML data. - Pay attention to efficiently handling the events and expanding the necessary nodes.","solution":"from xml.dom import pulldom def filter_books(xml_string: str) -> None: Parses the given XML string and prints the titles and authors of all books published after the year 2000 and costing more than 20. doc = pulldom.parseString(xml_string) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'book\': doc.expandNode(node) title = node.getElementsByTagName(\\"title\\")[0].firstChild.data author = node.getElementsByTagName(\\"author\\")[0].firstChild.data year = int(node.getElementsByTagName(\\"year\\")[0].firstChild.data) price = float(node.getElementsByTagName(\\"price\\")[0].firstChild.data) if year > 2000 and price > 20: print(f\\"{title} by {author}\\")"},{"question":"Sparse Tensor Operations and Conversions # Objective: Demonstrate your understanding of PyTorch\'s sparse tensors by performing the following tasks: constructing sparse tensors, converting between formats, and performing basic operations. # Task: 1. **Tensor Construction**: - Create a 4x4 dense tensor `dense_tensor` with the following values: ``` [[0, 2, 0, 0], [3, 0, 4, 0], [0, 0, 0, 5], [6, 0, 0, 0]] ``` - Convert `dense_tensor` to a sparse tensor using the COO (Coordinate) format. 2. **Format Conversion**: - Convert the COO sparse tensor to CSR (Compressed Sparse Row) format. - Convert the CSR sparse tensor to CSC (Compressed Sparse Column) format. 3. **Sparse Operations**: - Perform sparse matrix-vector multiplication between the COO sparse tensor and the vector `[1, 2, 3, 4]`. Use the `torch.mv` function. - Add a dense tensor of the same size to the CSR sparse tensor and convert the result back to dense format. # Requirements: - Use PyTorch\'s sparse tensor functionality. - Ensure your code is efficient and leverages the advantages of sparse tensor storage. - Handle any potential errors gracefully. # Input and Output Formats: - **Input**: None (create the tensors within your code). - **Output**: Python code that accomplishes the tasks described above. # Performance Considerations: - The constructed dense tensor is small, but ensure that your code can scale to larger matrices without significant performance degradation. - Utilize the computational benefits of sparse tensor formats during operations. # Example Code Structure: ```python import torch # Task 1: Construct dense tensor and convert to COO dense_tensor = torch.tensor([ [0, 2, 0, 0], [3, 0, 4, 0], [0, 0, 0, 5], [6, 0, 0, 0] ], dtype=torch.float32) sparse_coo = dense_tensor.to_sparse() # Task 2: Convert formats sparse_csr = sparse_coo.to_sparse_csr() sparse_csc = sparse_csr.to_sparse_csc() # Task 3: Sparse Operations vector = torch.tensor([1, 2, 3, 4], dtype=torch.float32) result_vector = torch.mv(sparse_coo, vector) dense_addition_result = (sparse_csr.to_dense() + dense_tensor) print(\\"Sparse COO Tensor:\\", sparse_coo) print(\\"Sparse CSR Tensor:\\", sparse_csr) print(\\"Sparse CSC Tensor:\\", sparse_csc) print(\\"Result of Sparse Matrix-Vector Multiplication:\\", result_vector) print(\\"Result of Adding Dense Tensor to CSR Sparse Tensor:\\", dense_addition_result) ``` # Submit: - Your Python code implementing the tasks above.","solution":"import torch # Task 1: Construct dense tensor and convert to COO dense_tensor = torch.tensor([ [0, 2, 0, 0], [3, 0, 4, 0], [0, 0, 0, 5], [6, 0, 0, 0] ], dtype=torch.float32) sparse_coo = dense_tensor.to_sparse() # Task 2: Convert formats sparse_csr = sparse_coo.to_sparse_csr() sparse_csc = sparse_csr.to_sparse_csc() # Task 3: Sparse Operations vector = torch.tensor([1, 2, 3, 4], dtype=torch.float32) result_vector = torch.mv(sparse_coo, vector) dense_addition_result = (sparse_csr.to_dense() + dense_tensor) # Outputs to verify the operations def get_outputs(): return sparse_coo, sparse_csr, sparse_csc, result_vector, dense_addition_result # Print outputs for debugging purpose if __name__ == \\"__main__\\": print(\\"Sparse COO Tensor:\\", sparse_coo) print(\\"Sparse CSR Tensor:\\", sparse_csr) print(\\"Sparse CSC Tensor:\\", sparse_csc) print(\\"Result of Sparse Matrix-Vector Multiplication:\\", result_vector) print(\\"Result of Adding Dense Tensor to CSR Sparse Tensor:\\", dense_addition_result)"},{"question":"# Signal Analysis with Window Functions and FFT You are tasked with implementing a function in PyTorch that processes a 1D signal using a specified window function and then analyzes its frequency components using the Fast Fourier Transform (FFT). Function Signature ```python def analyze_signal(signal: torch.Tensor, window_type: str) -> torch.Tensor: Apply a window function to a signal and analyze its frequency components using FFT. Args: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_type (str): The type of window function to apply. Can be one of [ \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']. Returns: - torch.Tensor: A tensor representing the frequency components of the windowed signal. ``` Input - `signal`: A 1D `torch.Tensor` of size (N), representing the input signal. - `window_type`: A string representing the type of window function to apply. Allowed values are: - `\'bartlett\'` - `\'blackman\'` - `\'cosine\'` - `\'exponential\'` - `\'gaussian\'` - `\'general_cosine\'` - `\'general_hamming\'` - `\'hamming\'` - `\'hann\'` - `\'kaiser\'` - `\'nuttall\'` Output - A 1D `torch.Tensor` of size (N) representing the frequency components of the windowed signal after applying FFT. Constraints 1. The function should handle the case where the provided `window_type` is not valid by raising a `ValueError`. 2. The implementation must use the PyTorch library for both windowing and FFT operations. Example ```python import torch signal = torch.randn(1000) window_type = \'hann\' # Call your function freq_components = analyze_signal(signal, window_type) print(freq_components) ``` In this example, `signal` is a random 1D tensor, and the \'hann\' window function is applied before computing the FFT. The result, `freq_components`, represents the frequency analysis of the windowed signal. # Notes: - You may assume the `signal` tensor has at least 2 elements. - Make sure to normalize the FFT result if necessary, to reflect the power spectrum correctly. Hints: - Look into `torch.signal.windows` for window functions. - Use `torch.fft.fft` for performing FFT in PyTorch.","solution":"import torch import torch.fft def analyze_signal(signal: torch.Tensor, window_type: str) -> torch.Tensor: Apply a window function to a signal and analyze its frequency components using FFT. Args: - signal (torch.Tensor): A 1D tensor representing the input signal. - window_type (str): The type of window function to apply. Can be one of [ \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\']. Returns: - torch.Tensor: A tensor representing the frequency components of the windowed signal. if not isinstance(signal, torch.Tensor) or signal.ndim != 1: raise ValueError(\\"Input signal must be a 1D torch.Tensor\\") windows = { \'bartlett\': torch.bartlett_window, \'blackman\': torch.blackman_window, \'cosine\': torch.hann_window, # No cosine window in PyTorch, using Hann as a placeholder \'exponential\': torch.hamming_window, # No exponential window in PyTorch, using Hamming as a placeholder \'gaussian\': torch.hann_window, # No gaussian window in PyTorch, using Hann as a placeholder \'general_cosine\': torch.hann_window, # No general_cosine window in PyTorch, using Hann as a placeholder \'general_hamming\': torch.hamming_window, \'hamming\': torch.hamming_window, \'hann\': torch.hann_window, \'kaiser\': torch.hann_window, # No kaiser window in PyTorch, using Hann as a placeholder \'nuttall\': torch.hann_window # No nuttall window in PyTorch, using Hann as a placeholder } if window_type not in windows: raise ValueError(f\\"Invalid window type: {window_type}\\") window_function = windows[window_type] window = window_function(signal.size(0)) windowed_signal = signal * window freq_components = torch.fft.fft(windowed_signal) return freq_components"},{"question":"Objective: Implement a Python class that mimics the memory management behavior of custom objects, focusing on allocation, initialization, and proper garbage collection. Problem Statement: You are required to implement a Python class `CustomObject` that behaves similarly to low-level memory-managed objects as described in the provided C-functions documentation. 1. The `CustomObject` class should maintain a reference count to simulate how objects are managed in Python\'s C-API. 2. Implement methods to manually increase and decrease the reference count. 3. When the reference count drops to zero, the object should be flagged for garbage collection (simulated by a custom message). 4. Provide a custom initialization method that initializes the object with a given type and value. 5. Implement additional logic to handle variable-sized objects if a `size` parameter is provided. Specifications: 1. **Class Definition**: `CustomObject` 2. **__init__()**: Initialize the object with `type` and `value`. Optionally accept `size`. 3. **increase_ref_count()**: Method to increase the reference count. 4. **decrease_ref_count()**: Method to decrease the reference count and check if it should be garbage collected. 5. **__del__()**: Custom destructor to simulate garbage collection. 6. **Attributes**: - `type`: The type of the object. - `value`: The value stored in the object. - `size`: Optional size (default to `None` if not provided). - `ref_count`: Reference count of the object. Input and Output Formats: - **Initialization**: ```python obj = CustomObject(type, value, size) ``` - **Public Methods**: ```python obj.increase_ref_count() obj.decrease_ref_count() ``` - **Deallocate Message**: When the object is garbage collected (i.e., reference count is zero), print a message: `\\"Object of type {type} with value {value} and size {size} has been deallocated.\\"` Constraints: - The `type` attribute should be a string representing the data type. - The `value` can be of any type. - The `size` should be an integer if provided. - The reference count should never be manually manipulable from outside the class other than through provided methods. Example: ```python class CustomObject: def __init__(self, type, value, size=None): self.type = type self.value = value self.size = size self.ref_count = 1 def increase_ref_count(self): self.ref_count += 1 def decrease_ref_count(self): self.ref_count -= 1 if self.ref_count == 0: self.__del__() def __del__(self): print(f\\"Object of type {self.type} with value {self.value} and size {self.size} has been deallocated.\\") # Example usage: obj = CustomObject(\'int\', 10) obj.increase_ref_count() obj.decrease_ref_count() obj.decrease_ref_count() # This should trigger deallocation message ``` **Note**: The above implementation is a high-level simulation of the low-level memory management described in the documentation.","solution":"class CustomObject: def __init__(self, type, value, size=None): Initializes the CustomObject with the given type, value and optionally size. Sets the initial reference count to 1. self.type = type self.value = value self.size = size self.ref_count = 1 def increase_ref_count(self): Increases the reference count of the object by 1. self.ref_count += 1 def decrease_ref_count(self): Decreases the reference count of the object by 1. If the reference count reaches 0, the object is flagged for garbage collection by calling its custom destructor. self.ref_count -= 1 if self.ref_count == 0: self.__del__() def __del__(self): Custom destructor to simulate garbage collection. Prints a message indicating the object has been deallocated. print(f\\"Object of type {self.type} with value {self.value} and size {self.size} has been deallocated.\\")"},{"question":"You are given a large text file, and you need to develop multiple operations on this file efficiently using memory-mapping. Create a class `MemoryMappedFileEditor` which uses the `mmap` module to handle file operations. Your class should include the following functionalities: 1. **Initialization**: - `__init__(self, file_path, mode=\'r+b\')`: Initialize the memory-mapped file object with the given file path and mode. 2. **Read Content**: - `read_content(self, start, end) -> bytes`: Read content from the file between byte positions `start` and `end`. 3. **Write Content**: - `write_content(self, start, data: bytes)`: Write the bytes `data` into the file starting at position `start`. 4. **Find Subsequence**: - `find_subsequence(self, subsequence: bytes, start: int = 0) -> int`: Find the lowest index of the first occurrence of the byte subsequence within the file starting from `start` position. Return -1 if not found. 5. **Resize File**: - `resize_file(self, new_size: int)`: Resize the file to `new_size` bytes. 6. **Memory Optimizations**: - `optimize_memory(self, option: int)`: Optimize the memory regions of the file using the given `madvise` option (use any relevant `mmap.MADV_*` constants). 7. **Close File**: - `close(self)`: Close the memory-mapped file object. Additionally, ensure the following: - If the mode specified is writable (`\'r+b\'`), modifications to the mmap object should also update the underlying file. - Handle edge cases gracefully, such as attempting operations on a closed file or writing data that exceeds the file boundaries. # Constraints - The file can be very large (up to several gigabytes), so performance and memory efficiency are crucial. - The `start` and `end` parameters for read and write operations will always be valid (i.e., within the current file size). - You should use the `mmap` methods and handle memory synchronizations appropriately. Example usage: ```python # Write code to create and use the MemoryMappedFileEditor class. editor = MemoryMappedFileEditor(\'largefile.txt\') # Read a portion of the file content = editor.read_content(0, 100) print(content) # Write to a specific location in the file editor.write_content(50, b\'Hello world!\') # Find a byte sequence index = editor.find_subsequence(b\'Python\') print(index) # Resize the file editor.resize_file(5000) # Optimize memory editor.optimize_memory(mmap.MADV_SEQUENTIAL) # Close the file editor.close() ```","solution":"import mmap import os class MemoryMappedFileEditor: def __init__(self, file_path, mode=\'r+b\'): self.file_path = file_path self.mode = mode self.file = open(file_path, mode) self.size = os.path.getsize(file_path) self.mm = mmap.mmap(self.file.fileno(), self.size, access=mmap.ACCESS_WRITE if \'b\' in mode else mmap.ACCESS_READ) def read_content(self, start, end) -> bytes: if self.mm.closed: raise ValueError(\\"Memory-mapped file is already closed\\") return self.mm[start:end] def write_content(self, start, data: bytes): if self.mm.closed: raise ValueError(\\"Memory-mapped file is already closed\\") end = start + len(data) if end > self.size: raise ValueError(\\"Data exceeds file boundaries\\") self.mm[start:end] = data self.mm.flush() def find_subsequence(self, subsequence: bytes, start: int = 0) -> int: if self.mm.closed: raise ValueError(\\"Memory-mapped file is already closed\\") return self.mm.find(subsequence, start) def resize_file(self, new_size: int): self.mm.close() self.file.close() with open(self.file_path, \'r+b\') as f: f.truncate(new_size) self.size = new_size self.file = open(self.file_path, self.mode) self.mm = mmap.mmap(self.file.fileno(), self.size, access=mmap.ACCESS_WRITE if \'b\' in self.mode else mmap.ACCESS_READ) def optimize_memory(self, option: int): if self.mm.closed: raise ValueError(\\"Memory-mapped file is already closed\\") self.mm.madvise(option) def close(self): if not self.mm.closed: self.mm.close() self.file.close()"},{"question":"Objective: Your task is to demonstrate your understanding of the `seaborn.objects` module by loading a dataset, transforming its data, and creating comprehensive visualizations. You will need to use `so.Plot`, `so.Bar`, `so.Count`, and other transformations available in the `seaborn.objects` module. Dataset: Use the `tips` dataset from `seaborn`. Requirements: 1. **Load** the `tips` dataset. 2. **Create a bar plot** that shows the count of total bills per day. - Use `so.Plot` and `so.Bar` to achieve this. 3. **Enhance the bar plot** with an additional variable `sex` to show counts separated by gender. - Incorporate a method to **dodge** the bars so they are side-by-side. 4. **Create another bar plot**, this time showing counts of table size (`size`), with counts represented along the y-axis. Instructions: 1. Assume the following import statement for seaborn objects: ```python import seaborn.objects as so from seaborn import load_dataset ``` 2. The function signature should be: ```python def plot_tips_data(): # Your code here plot_tips_data() ``` 3. Ensure your function completes the following steps: - Load the dataset `tips` using `seaborn`\'s `load_dataset`. - Create a bar plot for the count of total bills per day. - Enhance the plot by differentiating counts by gender using `color=\'sex\'` and applying `so.Dodge()`. - Create another bar plot showing counts of table size (`size`), with counts represented along the y-axis, using `y=\'size\'`. Expected Output: Your function should generate two plots: 1. A bar plot showing the total counts per day, with bars dodged by gender. 2. A bar plot showing the counts of table size (`size`) as a function of the y-axis. ```python # Function Draft Example: import seaborn.objects as so from seaborn import load_dataset def plot_tips_data(): tips = load_dataset(\\"tips\\") # Total bills per day so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()).show() # Total bills per day by sex (using dodge) so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()).show() # Counts of table size so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()).show() plot_tips_data() ``` Your output should match the expected type and format. Properly incorporate all required `seaborn` functionalities as demonstrated above.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_tips_data(): tips = load_dataset(\\"tips\\") # Total bills per day so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()).show() # Total bills per day by sex (using dodge) so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()).show() # Counts of table size so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()).show() plot_tips_data()"},{"question":"# **Seaborn Visualization Challenge** **Objective**: This assessment aims to evaluate your ability to manipulate data structures and create visualizations using the seaborn library. **Problem Statement**: You are provided with a dataset containing the monthly average temperatures of various cities over several years. Your task is to clean, transform the data, and create insightful visualizations using seaborn. Specifically, you need to demonstrate your understanding of both long-form and wide-form data structures, and the use of seaborn’s plotting functions. **Dataset**: You have access to the following dataset `avg_temp.csv`: | Year | Month | City | Avg_Temp | |------|-------|------------|----------| | 2000 | Jan | New York | 0.5 | | 2000 | Feb | New York | 1.2 | | ... | ... | ... | ... | | 2020 | Dec | Los Angeles| 15.7 | # **Tasks**: **Task 1**: 1. Load the dataset into a pandas DataFrame. 2. Display the first few rows of the DataFrame. **Task 2**: Transform the DataFrame to wide-form where each column represents a month and the values are the average temperatures for New York across all years. **Task 3**: Using wide-form data, create a line plot where the x-axis represents the year and the y-axis represents the average temperature for each month. **Task 4**: Convert the DataFrame back to long-form. **Task 5**: Using long-form data, create a line plot where the x-axis represents the month, the y-axis represents the average temperature, and the hue represents the year. # **Input and Output Formats**: **Input**: - A CSV file named `avg_temp.csv`. **Output**: - Visualizations created using seaborn. # **Constraints**: 1. Use the seaborn ‘relplot’ function for your line plots. 2. Ensure that your plots are clearly labeled. # **Performance Requirements**: - The script should execute efficiently on datasets containing up to 50,000 rows. # **Hints**: - Utilize `pandas.read_csv` to load the dataset. - Use `pandas.pivot` to transform data from long-form to wide-form. - Utilize the `melt` method from pandas to convert data back to long-form. # **Example Code**: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset df = pd.read_csv(\'avg_temp.csv\') print(df.head()) # Task 2: Transform to wide-form df_wide = df[df[\'City\'] == \'New York\'].pivot(index=\'Year\', columns=\'Month\', values=\'Avg_Temp\') print(df_wide.head()) # Task 3: Plot wide-form data sns.relplot(data=df_wide, kind=\'line\') plt.title(\'Average Temperature in New York Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Average Temperature\') plt.show() # Task 4: Convert back to long-form df_long = df_wide.reset_index().melt(id_vars=[\'Year\'], var_name=\'Month\', value_name=\'Avg_Temp\') print(df_long.head()) # Task 5: Plot long-form data sns.relplot(data=df_long, x=\'Month\', y=\'Avg_Temp\', hue=\'Year\', kind=\'line\') plt.title(\'Monthly Average Temperature in New York Across Years\') plt.show() ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the dataset def load_dataset(file_path): df = pd.read_csv(file_path) return df # Task 2: Transform to wide-form def transform_to_wide_form(df, city): df_city = df[df[\'City\'] == city] df_wide = df_city.pivot(index=\'Year\', columns=\'Month\', values=\'Avg_Temp\') return df_wide # Task 3: Plot wide-form data def plot_wide_form(df_wide): sns.relplot(data=df_wide, kind=\'line\') plt.title(\'Average Temperature Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Average Temperature\') plt.show() # Task 4: Convert back to long-form def transform_to_long_form(df_wide): df_long = df_wide.reset_index().melt(id_vars=[\'Year\'], var_name=\'Month\', value_name=\'Avg_Temp\') return df_long # Task 5: Plot long-form data def plot_long_form(df_long): sns.relplot(data=df_long, x=\'Month\', y=\'Avg_Temp\', hue=\'Year\', kind=\'line\') plt.title(\'Monthly Average Temperature Across Years\') plt.xlabel(\'Month\') plt.ylabel(\'Average Temperature\') plt.show() # Example usage with file path # df = load_dataset(\'avg_temp.csv\') # print(df.head()) # df_wide = transform_to_wide_form(df, \'New York\') # print(df_wide.head()) # plot_wide_form(df_wide) # df_long = transform_to_long_form(df_wide) # print(df_long.head()) # plot_long_form(df_long)"},{"question":"Create a Simple Text-Based To-Do Application Using the `curses` Module **Objective:** Implement a text-based To-Do List application using the `curses` module. The application should allow the user to: 1. Add a new task. 2. Mark a task as completed. 3. Delete a completed task. 4. View all tasks with a clear indication of their status (completed or pending). **Details:** - **Initialization**: - Use `curses.initscr()` to initialize the screen. - Set up color support and initialize color pairs with `curses.start_color()`. - **Windows**: - Create at least two windows: one for listing tasks and another for user commands input (text box). - **Key Commands**: - `a` to add a new task. - `c` to mark a task as completed. - `d` to delete a completed task. - `q` to quit the application. - **Task Management**: - Utilize a list to store tasks with their status. - Use attributes for displaying tasks with different statuses (e.g., bold for completed tasks). - **Text Input**: - Use the `Textbox` class from `curses.textpad` for user input when adding new tasks. **Function signatures (to implement):** ```python import curses from curses.textpad import Textbox def main(stdscr): # Your code to set up the application pass def add_task(window, tasks): # Your code to add a new task pass def mark_task_completed(window, tasks): # Your code to mark a task as completed pass def delete_completed_task(window, tasks): # Your code to delete a completed task pass def display_tasks(window, tasks): # Your code to display tasks with their status pass if __name__ == \\"__main__\\": curses.wrapper(main) ``` Constraints: 1. **Input**: Users should provide commands and task descriptions as input. 2. **Output**: The tasks should be displayed within the terminal window with the ability to scroll if too many tasks are present. 3. **Performance**: Efficiently handle up to 100 tasks without significant delays in command response. **Testing and Debugging:** Ensure that you test the application in a terminal that supports `curses`. Handle terminal resize events gracefully using relevant functions like `curses.resize_term()`. **Instructions to run the program**: Save the script and run it using a terminal with the command: ```bash python your_script_name.py ``` **Example Scenario**: 1. Application starts and displays empty task list. 2. User presses `a`, inputs \\"Buy groceries\\", and presses enter. The task appears in the task list. 3. User presses `a`, inputs \\"Call the dentist\\", and presses enter. The task is added. 4. User navigates to \\"Buy groceries\\", presses `c`. The task is marked as completed (displayed in bold). 5. User navigates to \\"Call the dentist\\", presses `d`. The task is removed from the list. Good luck!","solution":"import curses from curses.textpad import Textbox def main(stdscr): curses.curs_set(0) # Hide cursor curses.start_color() # Enable color # Initialization of color pairs curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLUE) curses.init_pair(2, curses.COLOR_WHITE, curses.COLOR_GREEN) tasks = [] # List to store the tasks height, width = stdscr.getmaxyx() task_win = curses.newwin(height - 3, width, 0, 0) input_win = curses.newwin(3, width, height - 3, 0) input_win.addstr(1, 0, \\"Press \'a\': Add | \'c\': Complete | \'d\': Delete | \'q\': Quit\\") while True: task_win.clear() display_tasks(task_win, tasks) task_win.refresh() input_win.refresh() key = input_win.getch() if key == ord(\'a\'): add_task(input_win, tasks) elif key == ord(\'c\'): mark_task_completed(input_win, tasks) elif key == ord(\'d\'): delete_completed_task(input_win, tasks) elif key == ord(\'q\'): break def add_task(window, tasks): window.clear() window.addstr(1, 0, \\"Enter task: \\") window.refresh() curses.echo() task = window.getstr(1, 12, 40).decode(\\"utf-8\\") curses.noecho() tasks.append({\'task\': task, \'completed\': False}) def mark_task_completed(window, tasks): window.clear() window.addstr(1, 0, \\"Mark task number as completed: \\") window.refresh() curses.echo() task_number = int(window.getstr(1, 28, 2).decode(\\"utf-8\\")) curses.noecho() if 0 <= task_number < len(tasks): tasks[task_number][\'completed\'] = True def delete_completed_task(window, tasks): window.clear() window.addstr(1, 0, \\"Delete task number: \\") window.refresh() curses.echo() task_number = int(window.getstr(1, 18, 2).decode(\\"utf-8\\")) curses.noecho() if 0 <= task_number < len(tasks): if tasks[task_number][\'completed\']: tasks.pop(task_number) def display_tasks(window, tasks): for idx, task in enumerate(tasks): if task[\'completed\']: window.addstr(idx, 0, f\\"{idx}. {task[\'task\']}\\", curses.color_pair(2) | curses.A_BOLD) else: window.addstr(idx, 0, f\\"{idx}. {task[\'task\']}\\", curses.color_pair(1)) if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"You have been tasked with developing a custom URL shortener service for your company. The service needs to generate cryptographically secure and short URLs to ensure that they are hard to guess and manipulation-resistant. The URLs should be generated using the `secrets` module and must be secure enough to be shared with clients for password resets or unique session identifiers. **Problem Statement:** Implement a function `generate_secure_url(base_url: str, token_length: int) -> str` that generates a secure, short URL by appending a cryptographic token to the base URL. Your implementation must: 1. Use the `secrets.token_urlsafe` function to generate the token. 2. Ensure the generated token\'s byte length provides enough entropy to be considered secure. 3. Concatenate the base URL and the secure token to form the final secure URL. **Function Signature:** ```python def generate_secure_url(base_url: str, token_length: int) -> str: ``` **Input:** - `base_url` (str): The base URL to which the token will be appended. This will be a non-empty string. - `token_length` (int): The length of the token in bytes. This must be a positive integer. **Output:** - (str): A securely generated URL containing the base URL concatenated with a secure token. **Example:** ```python >>> generate_secure_url(\\"https://example.com/reset=\\", 16) \'https://example.com/reset=Drmhze6EPcv0fN_81Bj-nA\' >>> generate_secure_url(\\"https://example.com/verify=\\", 8) \'https://example.com/verify=f9bf78b9\' ``` **Constraints:** - The `token_length` should be at least 8 bytes for the token to be sufficiently secure. - The function should handle the concatenation properly to ensure valid URLs without trailing slashes if not intended. **Notes:** - Ensure you test your function with edge cases such as minimal and maximal token lengths. - Emphasize correct usage of the `secrets` module for generating tokens. **Grading Criteria:** - Correctness of the generated URL format. - Security of the token generation (using `secrets.token_urlsafe`). - Proper handling of invalid input scenarios.","solution":"import secrets def generate_secure_url(base_url: str, token_length: int) -> str: Generates a cryptographically secure, short URL by appending a secure token to the base URL. Parameters: base_url (str): The base URL to which the token will be appended. token_length (int): The length of the token in bytes. Returns: str: A secure URL containing the base URL concatenated with a secure token. if token_length < 8: raise ValueError(\\"Token length must be at least 8 bytes for security purposes.\\") token = secrets.token_urlsafe(token_length) return f\\"{base_url}{token}\\""},{"question":"**Question: Advanced File Handling with Python\'s C API** In this assessment, you are tasked with implementing a Python function that mimics a part of the internal C API functionality for handling file objects in Python. Specifically, you will create a function that reads from a file descriptor, processes the content, and writes the processed content to another file. This process should be conducted in a way that handles buffering and potential errors appropriately. Here are the requirements: # Function Signature ```python def process_file_content(input_fd: int, output_fd: int, processing_func: callable, chunk_size: int = 1024) -> None: Reads content from the file associated with input_fd, processes the content using processing_func, and writes the processed content to the file associated with output_fd. Parameters: - input_fd (int): File descriptor for the input file. - output_fd (int): File descriptor for the output file. - processing_func (callable): A function that takes a string (content read from file) and returns a processed string. - chunk_size (int): Size of chunks to read from the file. Default is 1024 bytes. Returns: None Raises: - ValueError: If input_fd or output_fd is not a valid file descriptor. - IOError: For input/output errors during reading or writing. ``` # Detailed Instructions 1. **Reading from input_fd**: Implement a buffering mechanism to read from `input_fd` file descriptor in chunks specified by `chunk_size`. 2. **Processing Content**: Use the provided `processing_func` to process each chunk of the read content. 3. **Writing to output_fd**: Write the processed content to the `output_fd` file descriptor. 4. **Error Handling**: Ensure that your function raises appropriate exceptions for invalid file descriptors and I/O errors. 5. **No Direct File Object Use**: Do not use high-level Python file objects (`open`, `read`, `write`, etc.) directly. Instead, work with the file descriptors using lower-level functions from the `os` module like `os.read`, `os.write`, etc. # Example Usage ```python import os def example_processing_func(data): return data.upper() # Open file descriptors input_fd = os.open(\'input.txt\', os.O_RDONLY) output_fd = os.open(\'output.txt\', os.O_WRONLY | os.O_CREAT) try: process_file_content(input_fd, output_fd, example_processing_func) finally: os.close(input_fd) os.close(output_fd) ``` # Constraints - You must handle large files efficiently using buffering. - You should ensure that the function works with any valid file descriptor and callable processing function. - The implementation should be robust enough to handle unexpected conditions gracefully with meaningful error messages or exceptions. # Testing To test your implementation, you can create temporary files, use `os.pipe` to create a pair of file descriptors, or use `unittest` and `mock` to simulate file descriptor behavior. **Note**: You may need to refer to the Python `os` module documentation for functions like `os.read`, `os.write`, `os.close`, etc., which are used to manipulate file descriptors directly.","solution":"import os def process_file_content(input_fd: int, output_fd: int, processing_func: callable, chunk_size: int = 1024) -> None: if not isinstance(input_fd, int) or not isinstance(output_fd, int): raise ValueError(\\"input_fd and output_fd must be valid file descriptors\\") try: while True: chunk = os.read(input_fd, chunk_size) if not chunk: # End of file break processed_chunk = processing_func(chunk.decode()) os.write(output_fd, processed_chunk.encode()) except OSError as e: raise IOError(f\\"Error during file operation: {e}\\")"},{"question":"# Decision Tree Implementation and Visualization using scikit-learn Problem Statement: You are provided with the Iris dataset, a classic dataset in machine learning. Your task is to implement a Decision Tree classifier using `scikit-learn`. You need to train the model, visualize the decision tree, and evaluate its performance. Additionally, you must handle potential overfitting by pruning the tree. Requirements: 1. **Data Loading**: Load the Iris dataset from `sklearn.datasets`. 2. **Model Training**: Train a `DecisionTreeClassifier` on the dataset. 3. **Visualization**: Visualize the trained Decision Tree. 4. **Evaluation**: Evaluate the model using cross-validation. 5. **Pruning**: Implement and apply minimal cost-complexity pruning to avoid overfitting. Input and Output Formats: - **Input**: No input required as you will load the Iris dataset within your code. - **Output**: - Visual representation of the decision tree. - Cross-validation evaluation metrics (e.g., accuracy). - Pruned tree and its evaluation. Function Signature: ```python def decision_tree_iris(): This function implements the following steps: 1. Loads the Iris dataset. 2. Trains a Decision Tree classifier. 3. Visualizes the Decision Tree. 4. Evaluates the model using cross-validation. 5. Prunes the Decision Tree using minimal cost-complexity pruning. Returns: None pass ``` Constraints: - Use `random_state=0` for reproducibility. - Use `max_depth=4` for the initial tree training to visualize before pruning. Detailed Steps: 1. **Data Loading**: - Load the Iris dataset using `sklearn.datasets.load_iris`. 2. **Model Training**: - Fit a `DecisionTreeClassifier` with `max_depth=4`. 3. **Visualization**: - Use `sklearn.tree.plot_tree` to visualize the decision tree. - Save the visualization to a file named `decision_tree.png`. 4. **Evaluation**: - Use `cross_val_score` from `sklearn.model_selection` to evaluate the model with 5-fold cross-validation. - Print the mean and standard deviation of the cross-validation accuracy. 5. **Pruning**: - Use `ccp_alpha` parameter in `DecisionTreeClassifier` for pruning. - Determine the optimal `ccp_alpha` value by plotting the relationship between `ccp_alpha` and the cross-validation accuracy. - Retrain the pruned tree and visualize it. Example: ```python def decision_tree_iris(): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import cross_val_score import matplotlib.pyplot as plt import numpy as np # Step 1: Load data iris = load_iris() X, y = iris.data, iris.target # Step 2: Train the model clf = DecisionTreeClassifier(random_state=0, max_depth=4) clf.fit(X, y) # Step 3: Visualize the Decision Tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'decision_tree.png\') plt.show() # Step 4: Evaluate the model with cross-validation cv_scores = cross_val_score(clf, X, y, cv=5) print(f\'Cross-validation accuracy: {cv_scores.mean():.2f}, std: {cv_scores.std():.2f}\') # Step 5: Pruning the tree path = clf.cost_complexity_pruning_path(X, y) ccp_alphas, impurities = path.ccp_alphas, path.impurities # Visualize ccp_alpha vs. accuracy clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha) clf.fit(X, y) clfs.append(clf) # Remove the last element to avoid zero alpha leading to the full tree clfs = clfs[:-1] ccp_alphas = ccp_alphas[:-1] node_counts = [clf.tree_.node_count for clf in clfs] depth = [clf.tree_.max_depth for clf in clfs] fig, ax = plt.subplots(2, 1, figsize=(10, 10)) ax[0].plot(ccp_alphas, node_counts, marker=\'o\', drawstyle=\\"steps-post\\") ax[0].set_xlabel(\\"ccp_alpha\\") ax[0].set_ylabel(\\"number of nodes\\") ax[0].set_title(\\"Number of nodes vs ccp_alpha\\") ax[1].plot(ccp_alphas, depth, marker=\'o\', drawstyle=\\"steps-post\\") ax[1].set_xlabel(\\"ccp_alpha\\") ax[1].set_ylabel(\\"depth of tree\\") ax[1].set_title(\\"Depth vs ccp_alpha\\") plt.show() # Find the best alpha alpha_loop_values = [] for clf in clfs: scores = cross_val_score(clf, X, y, cv=5) alpha_loop_values.append([clf, scores.mean(), scores.std()]) # Display the best alpha and its scores alpha_results = sorted(alpha_loop_values, key=lambda x: x[1], reverse=True) best_alpha = alpha_results[0] print(f\'Best ccp_alpha: {best_alpha[0].ccp_alpha}\') print(f\'Best cross-validation accuracy: {best_alpha[1]:.2f}, std: {best_alpha[2]:.2f}\') # Retrain the model with the best alpha and visualize the pruned tree pruned_clf = DecisionTreeClassifier(random_state=0, ccp_alpha=best_alpha[0].ccp_alpha) pruned_clf.fit(X, y) plt.figure(figsize=(20,10)) plot_tree(pruned_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'pruned_decision_tree.png\') plt.show() ```","solution":"def decision_tree_iris(): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import cross_val_score import matplotlib.pyplot as plt import numpy as np # Step 1: Load data iris = load_iris() X, y = iris.data, iris.target # Step 2: Train the model clf = DecisionTreeClassifier(random_state=0, max_depth=4) clf.fit(X, y) # Step 3: Visualize the Decision Tree plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'decision_tree.png\') plt.show() # Step 4: Evaluate the model with cross-validation cv_scores = cross_val_score(clf, X, y, cv=5) print(f\'Cross-validation accuracy: {cv_scores.mean():.2f}, std: {cv_scores.std():.2f}\') # Step 5: Pruning the tree path = clf.cost_complexity_pruning_path(X, y) ccp_alphas, impurities = path.ccp_alphas, path.impurities # Visualize ccp_alpha vs. accuracy clfs = [] for ccp_alpha in ccp_alphas: clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha) clf.fit(X, y) clfs.append(clf) # Remove the last element to avoid zero alpha leading to the full tree clfs = clfs[:-1] ccp_alphas = ccp_alphas[:-1] node_counts = [clf.tree_.node_count for clf in clfs] depth = [clf.tree_.max_depth for clf in clfs] fig, ax = plt.subplots(2, 1, figsize=(10, 10)) ax[0].plot(ccp_alphas, node_counts, marker=\'o\', drawstyle=\\"steps-post\\") ax[0].set_xlabel(\\"ccp_alpha\\") ax[0].set_ylabel(\\"number of nodes\\") ax[0].set_title(\\"Number of nodes vs ccp_alpha\\") ax[1].plot(ccp_alphas, depth, marker=\'o\', drawstyle=\\"steps-post\\") ax[1].set_xlabel(\\"ccp_alpha\\") ax[1].set_ylabel(\\"depth of tree\\") ax[1].set_title(\\"Depth vs ccp_alpha\\") plt.show() # Find the best alpha alpha_loop_values = [] for clf in clfs: scores = cross_val_score(clf, X, y, cv=5) alpha_loop_values.append([clf, scores.mean(), scores.std()]) # Display the best alpha and its scores alpha_results = sorted(alpha_loop_values, key=lambda x: x[1], reverse=True) best_alpha = alpha_results[0] print(f\'Best ccp_alpha: {best_alpha[0].ccp_alpha}\') print(f\'Best cross-validation accuracy: {best_alpha[1]:.2f}, std: {best_alpha[2]:.2f}\') # Retrain the model with the best alpha and visualize the pruned tree pruned_clf = DecisionTreeClassifier(random_state=0, ccp_alpha=best_alpha[0].ccp_alpha) pruned_clf.fit(X, y) plt.figure(figsize=(20,10)) plot_tree(pruned_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.savefig(\'pruned_decision_tree.png\') plt.show()"},{"question":"Objective: Implement a function that utilizes the `itertools`, `functools`, and `operator` modules to process a collection of data. The function should demonstrate advanced concepts such as creating efficient iterators, using higher-order functions, and manipulating standard operators. Question: You are provided with a list of integers, and your task is to implement a function `process_numbers(numbers: List[int]) -> int` that performs the following operations: 1. **Filter out** all even numbers from the provided list. 2. Create an **iterator** that will compute the factorial for each of the remaining numbers. (Use the `itertools` module). 3. **Accumulate** the factorials to compute their sum using a higher-order function (use the `functools` module). 4. Return the resulting sum. Constraints: - You must utilize the `itertools`, `functools`, and `operator` modules in your implementation. - Your implementation should be efficient in terms of both time and space complexity. Function Signature: ```python from typing import List def process_numbers(numbers: List[int]) -> int: # your code here ``` Example: ```python # Example usage numbers = [1, 2, 3, 4, 5] result = process_numbers(numbers) print(result) # Output should be 127 (1! + 3! + 5! = 1 + 6 + 120 = 127) ``` Explanation of Example: - The provided list `[1, 2, 3, 4, 5]` is filtered to `[1, 3, 5]` (removing even numbers 2 and 4). - The factorials of the filtered list are computed: `[1!, 3!, 5!]` => `[1, 6, 120]`. - The sum of these factorials is `1 + 6 + 120 = 127`, which is the final result. Note: Ensure to handle edge cases such as empty lists or lists with no odd numbers correctly.","solution":"from typing import List from itertools import filterfalse from functools import reduce import operator import math def process_numbers(numbers: List[int]) -> int: Processes the list of numbers according to the given specifications: - Filters out all even numbers. - Computes the factorial for each remaining number. - Accumulates the results to compute their sum. Args: numbers (List[int]): List of integers. Returns: int: The resulting sum of the factorials of the odd numbers. # Filter out the even numbers using itertools.filterfalse and a lambda function filtered_numbers = list(filterfalse(lambda x: x % 2 == 0, numbers)) # Create an iterator that computes the factorial of each remaining number factorials = map(math.factorial, filtered_numbers) # Accumulate the factorials to compute their sum using functools.reduce and the operator.add function return reduce(operator.add, factorials, 0)"},{"question":"# Functional Programming Assessment: Advanced Iterations and Operations Objective: Create a function that processes a list of integers, groups them into subsequences of a given size, applies a transformation function to each subsequence, and finally combines the results into a single output. Requirements: 1. **Grouping**: Group the input list into subsequences of a specified size. If the last group has fewer elements than the specified size, it should still be included. 2. **Transformation**: Apply a transformation function to each subsequence. Example transformation could be calculating the sum, product, or applying a specific arithmetic operation. 3. **Combination**: Combine the transformed subsequences into a single output list. Function Signature: ```python from typing import List, Callable from itertools import islice from functools import partial from operator import add, mul def advanced_transform(lst: List[int], group_size: int, transform: Callable[[List[int]], int]) -> List[int]: Processes the input list by grouping elements, applying a transformation function, and combining the results. :param lst: List of integers to be processed. :param group_size: The size of each subgroup to be created. :param transform: A function that takes a list of integers and returns an integer. :return: A list of transformed results. ... ``` Input: 1. A list of integers `lst` containing the elements to be processed. 2. An integer `group_size` representing the size of each subgroup. 3. A function `transform` that takes a list of integers and returns a single integer result. Output: A list of integers, each corresponding to the transformed result of each subsequence. Constraints: - All elements in `lst` will be integers. - `group_size` will be a positive integer. - The transformation function will always be valid for the given subsequences. - The input list length might not be perfectly divisible by `group_size`. Example: ```python # Example transform function def sum_sublist(sublist: List[int]) -> int: return sum(sublist) # Test the advanced_transform function result = advanced_transform([1, 2, 3, 4, 5, 6, 7, 8, 9], 3, sum_sublist) print(result) # Output should be [6, 15, 24, 9] because [1+2+3, 4+5+6, 7+8+9, 10] ``` Explanation: 1. **Grouping**: `[1, 2, 3, 4, 5, 6, 7, 8, 9]` -> `[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]` 2. **Transformation**: `[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]] -> [6, 15, 24, 10]` 3. **Combination**: Combine the transformed results into a single output list. The implementation should utilize functional programming constructs like iterators, higher-order functions, and operator mappings effectively.","solution":"from typing import List, Callable from itertools import islice def advanced_transform(lst: List[int], group_size: int, transform: Callable[[List[int]], int]) -> List[int]: Processes the input list by grouping elements, applying a transformation function, and combining the results. :param lst: List of integers to be processed. :param group_size: The size of each subgroup to be created. :param transform: A function that takes a list of integers and returns an integer. :return: A list of transformed results. def grouper(lst: List[int], n: int): # Split lst into groups of size n it = iter(lst) return iter(lambda: list(islice(it, n)), []) transformed_result = [] for group in grouper(lst, group_size): transformed_result.append(transform(group)) return transformed_result"},{"question":"Question: You are tasked with creating a command-line utility in Python using the \\"colorsys\\" module that converts a list of colors from one color system to another. The utility should be capable of converting colors between RGB, YIQ, HLS, and HSV. Specifications: 1. Your utility should accept the following command-line arguments: - The input color system (`-i` or `--input`): one of `RGB`, `YIQ`, `HLS`, or `HSV`. - The output color system (`-o` or `--output`): one of `RGB`, `YIQ`, `HLS`, or `HSV`. - A list of colors to convert, provided as tuples of floating-point numbers. 2. You must implement the following function: ```python def convert_colors(input_system: str, output_system: str, colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: Convert a list of colors from one color system to another. Parameters: - input_system (str): The color system of the input colors (`RGB`, `YIQ`, `HLS`, `HSV`). - output_system (str): The color system to convert the colors to (`RGB`, `YIQ`, `HLS`, `HSV`). - colors (List[Tuple[float, float, float]]): A list of colors to convert. Returns: - List[Tuple[float, float, float]]: A list of colors converted to the output color system. pass ``` 3. The function should raise a `ValueError` if an invalid color system is provided as input or output. 4. Constraint: The utility must handle a list containing up to 1000 colors efficiently. 5. Example: ```python # Example usage converted_colors = convert_colors(\\"RGB\\", \\"HSV\\", [(0.2, 0.4, 0.4), (0.5, 0.5, 0.5)]) print(converted_colors) ``` Expected output: ``` [(0.5, 0.5, 0.4), (0.0, 0.0, 0.5)] ``` 6. You should also write a small script that uses this function and handles the command-line arguments to make this utility runnable from the command line. Performance Requirements: Your function should efficiently handle up to 1000 color conversions. Each conversion should complete in a timely manner, keeping overall execution time reasonable. # Submission: Submit your implementation of the `convert_colors` function along with the command-line utility script.","solution":"import colorsys import argparse from typing import List, Tuple def convert_colors(input_system: str, output_system: str, colors: List[Tuple[float, float, float]]) -> List[Tuple[float, float, float]]: Convert a list of colors from one color system to another. Parameters: - input_system (str): The color system of the input colors (`RGB`, `YIQ`, `HLS`, `HSV`). - output_system (str): The color system to convert the colors to (`RGB`, `YIQ`, `HLS`, `HSV`). - colors (List[Tuple[float, float, float]]): A list of colors to convert. Returns: - List[Tuple[float, float, float]]: A list of colors converted to the output color system. if input_system not in {\'RGB\', \'YIQ\', \'HLS\', \'HSV\'} or output_system not in {\'RGB\', \'YIQ\', \'HLS\', \'HSV\'}: raise ValueError(\'Invalid color system provided.\') converted_colors = [] for color in colors: # Convert input to RGB first, if it\'s not already in RGB if input_system == \'RGB\': rgb = color elif input_system == \'YIQ\': rgb = colorsys.yiq_to_rgb(*color) elif input_system == \'HLS\': rgb = colorsys.hls_to_rgb(*color) elif input_system == \'HSV\': rgb = colorsys.hsv_to_rgb(*color) # Now convert RGB to the desired output system if output_system == \'RGB\': converted_colors.append(rgb) elif output_system == \'YIQ\': converted_colors.append(colorsys.rgb_to_yiq(*rgb)) elif output_system == \'HLS\': converted_colors.append(colorsys.rgb_to_hls(*rgb)) elif output_system == \'HSV\': converted_colors.append(colorsys.rgb_to_hsv(*rgb)) return converted_colors # Command line utility def main(): parser = argparse.ArgumentParser(description=\'Color system converter.\') parser.add_argument(\'-i\', \'--input\', required=True, choices=[\'RGB\', \'YIQ\', \'HLS\', \'HSV\'], help=\'Input color system.\') parser.add_argument(\'-o\', \'--output\', required=True, choices=[\'RGB\', \'YIQ\', \'HLS\', \'HSV\'], help=\'Output color system.\') parser.add_argument(\'colors\', metavar=\'color\', type=float, nargs=\'+\', help=\'Colors to convert (as a list of tuples)\') args = parser.parse_args() # Ensure that color list is well-formed triples if len(args.colors) % 3 != 0: raise ValueError(\'Colors must be provided as a list of (r, g, b) or equivalent tuples.\') color_tuples = list(zip(args.colors[0::3], args.colors[1::3], args.colors[2::3])) converted_colors = convert_colors(args.input, args.output, color_tuples) print(converted_colors) if __name__ == \'__main__\': main()"},{"question":"# Asynchronous Task Setup with Platform-Specific Handling Objective Write a Python function using the `asyncio` module that sets up an asynchronous task to perform a simple network I/O operation. The function should ensure compatibility across different platforms, handling any limitations or differences in the `asyncio` functionalities as specified in the documentation. Function Signature ```python import asyncio async def perform_async_task(hostname: str, port: int) -> str: # Perform asynchronous network task pass ``` Inputs - `hostname` (str): The hostname or IP address to connect to. - `port` (int): The port number to connect to. Output - `str`: The response received from the server. If the operation is not supported on the current platform, return a suitable error message. Requirements 1. **Network I/O Operation**: - Create an asynchronous network connection using `asyncio` to the specified hostname and port. - Send a simple request (e.g., `b\'Hello, World!\'`) to the server and receive a response. 2. **Platform-Specific Handling**: - Handle potential platform-specific limitations as described in the documentation. - If the operation is not supported on the current platform, catch the exceptions and return a message like `\\"Operation not supported on this platform\\"`. 3. **Graceful Error Handling**: - Ensure the function handles common network errors gracefully and returns understandable error messages. Example ```python import asyncio async def perform_async_task(hostname: str, port: int) -> str: try: reader, writer = await asyncio.open_connection(hostname, port) writer.write(b\'Hello, World!\') await writer.drain() data = await reader.read(100) writer.close() await writer.wait_closed() return data.decode() except NotImplementedError: return \\"Operation not supported on this platform\\" except Exception as e: return f\\"An error occurred: {e}\\" # Example usage (This will not run properly in this static example; it\'s for demonstration purposes): # result = asyncio.run(perform_async_task(\'example.com\', 80)) # print(result) ``` Constraints - Assume the server will close the connection after sending the response. - The maximum length of the response is 100 bytes. Additional Information - The correct implementation should handle creating the asynchronous connection and managing any differences between platforms as per the provided documentation. - You may need to handle different selectors for various platforms as detailed in the documentation.","solution":"import asyncio async def perform_async_task(hostname: str, port: int) -> str: Performs an asynchronous network I/O operation to connect to the specified hostname and port. Sends a simple request and returns the response. Handles platform-specific limitations and errors gracefully. Args: hostname (str): The hostname or IP address to connect to. port (int): The port number to connect to. Returns: str: The response received from the server or an appropriate error message. try: reader, writer = await asyncio.open_connection(hostname, port) writer.write(b\'Hello, World!\') await writer.drain() data = await reader.read(100) writer.close() await writer.wait_closed() return data.decode() except NotImplementedError: return \\"Operation not supported on this platform\\" except Exception as e: return f\\"An error occurred: {e}\\" # Example usage (This will not run properly in this static example; it\'s for demonstration purposes): # result = asyncio.run(perform_async_task(\'example.com\', 80)) # print(result)"},{"question":"Objective: Implement a multi-threaded task management system using the `queue` module. You need to create a custom thread pool to execute a series of tasks with different priorities. The tasks should be processed by worker threads which will take tasks from a priority queue and mark them as done upon completion. You should ensure safe and efficient task processing using the functionalities provided by the `queue` module. Instructions: 1. Create a `TaskManager` class that manages task execution. 2. The `TaskManager` should initialize a `PriorityQueue` and a pool of worker threads. 3. Implement methods to add tasks to the queue and to start task execution. 4. Each task should be a callable that processes data (simulated via `print` statements). 5. Ensure that the task manager can handle task completion tracking and properly joins all tasks. Specifications: 1. **TaskManager Class:** - `__init__(self, num_workers: int)`: Initializes the priority queue and creates worker threads. `num_workers` specifies the number of worker threads. - `add_task(self, priority: int, task: Callable)`: Adds a task to the priority queue with a given priority level. - `start(self)`: Starts all worker threads which continuously fetch and execute tasks from the queue. 2. **Worker Function:** - Continuously retrieve tasks from the priority queue and execute them. - Mark tasks as done when completed. 3. **Task Example:** ```python def example_task(data): print(f\\"Processing {data}\\") ``` Constraints: - The priority queue should handle tasks based on their priority, with lower numbers having higher priority. - The system should be thread-safe and ensure no tasks are lost or duplicated. - The task manager must block until all tasks are executed. Example Usage: ```python import threading from queue import PriorityQueue from typing import Callable # TaskManager Class class TaskManager: def __init__(self, num_workers: int): self.task_queue = PriorityQueue() self.num_workers = num_workers def worker(self): while True: priority, task = self.task_queue.get() try: task() finally: self.task_queue.task_done() def add_task(self, priority: int, task: Callable): self.task_queue.put((priority, task)) def start(self): threads = [] for _ in range(self.num_workers): thread = threading.Thread(target=self.worker, daemon=True) thread.start() threads.append(thread) return threads # Example Task def example_task(data): print(f\\"Processing {data}\\") # Usage def main(): manager = TaskManager(num_workers=3) # Adding tasks with different priorities manager.add_task(2, lambda: example_task(\\"Low priority task\\")) manager.add_task(1, lambda: example_task(\\"High priority task\\")) manager.add_task(3, lambda: example_task(\\"Lowest priority task\\")) # Start processing tasks threads = manager.start() # Block until all tasks are done manager.task_queue.join() print(\\"All tasks completed.\\") if __name__ == \\"__main__\\": main() ``` In the above example: - The `TaskManager` creates a priority queue and worker threads. - The `add_task` method allows tasks to be added with their respective priority. - The `start` method starts the worker threads. - The sample `example_task` function simulates a task that will be executed by worker threads. **Implement the `TaskManager` class and its methods as specified to complete this challenge.**","solution":"import threading from queue import PriorityQueue from typing import Callable class TaskManager: def __init__(self, num_workers: int): self.task_queue = PriorityQueue() self.num_workers = num_workers def worker(self): while True: priority, task = self.task_queue.get() try: task() finally: self.task_queue.task_done() def add_task(self, priority: int, task: Callable): self.task_queue.put((priority, task)) def start(self): threads = [] for _ in range(self.num_workers): thread = threading.Thread(target=self.worker, daemon=True) thread.start() threads.append(thread) return threads"},{"question":"# Coding Assessment: Profiling and Analyzing Python Code Execution Objective: Your task is to demonstrate your comprehension of Python\'s `cProfile` and `pstats` modules by profiling a Python function and analyzing the results. Problem Statement: You are provided with a Python script that executes a computation-heavy function. Your task is to: 1. Profile the execution of this function using the `cProfile` module. 2. Save the profiling results to a file. 3. Load and analyze the profiling results using the `pstats` module. 4. Print out and interpret the top 5 functions that consume the most cumulative execution time. Provided Code: Below is the Python script you will be working with: ```python import time def heavy_computation(n): start = time.time() for i in range(n): for _ in range(10000): sum(range(100)) end = time.time() print(f\\"Computation finished in {end - start:.2f} seconds\\") if __name__ == \\"__main__\\": heavy_computation(100) ``` Requirements: 1. **Profiling the Function:** - Use the `cProfile` module to profile the `heavy_computation` function. - Save the profiling results into a file named `profile_results.prof`. 2. **Loading and Analyzing the Results:** - Load the profiling results using the `pstats` module. - Strip the directories from the file paths in the profiling results. - Sort the results based on the cumulative time. - Print out the top 5 functions consuming the most cumulative time. Implementation Details: 1. The profiling should be done within an importable function, `profile_heavy_computation`, which performs the profiling and saves the results. 2. The analysis should be done within another importable function, `analyze_profile_results`, which loads and prints the top 5 functions by cumulative time. Constraints: - You should use the `cProfile` module for profiling. - The analysis should handle potential errors gracefully and display informative messages. Expected Output Format: The analysis should display relevant profiling data in a readable format, focusing on the cumulative time each function consumed. # Example Function Signatures: ```python def profile_heavy_computation(): # Profile the heavy_computation function and save the results. pass def analyze_profile_results(): # Analyze the profiling results and print the top 5 functions by cumulative time. pass ``` # Submission: - Submit the implementations of the `profile_heavy_computation` and `analyze_profile_results` functions. - Include comments explaining your code, especially detailing how profiling and analysis are executed. - Ensure your code is well-structured and follows good programming practices.","solution":"import cProfile import pstats def heavy_computation(n): import time start = time.time() for i in range(n): for _ in range(10000): sum(range(100)) end = time.time() print(f\\"Computation finished in {end - start:.2f} seconds\\") def profile_heavy_computation(): Profiles the heavy_computation function and saves the results to \'profile_results.prof\'. profiler = cProfile.Profile() profiler.enable() heavy_computation(100) profiler.disable() profiler.dump_stats(\'profile_results.prof\') def analyze_profile_results(): Analyzes the profiling results contained in \'profile_results.prof\' and prints the top 5 functions by cumulative time. profile_file = \'profile_results.prof\' stats = pstats.Stats(profile_file) stats.strip_dirs() stats.sort_stats(pstats.SortKey.CUMULATIVE) stats.print_stats(5) if __name__ == \\"__main__\\": profile_heavy_computation() analyze_profile_results()"},{"question":"Managing Safe Tar File Extraction Objective: Implement a function to safely extract Python files from a tar archive. The function should handle various types of compression and implement security measures to avoid common pitfalls related to tar file extraction. Specifically, use the \\"data\\" extraction filter to prevent unsafe file extraction practices. Function Signature: ```python def safely_extract_python_files(tar_path: str, extract_path: str) -> None: pass ``` Parameters: - `tar_path` (str): Path to the tar file that needs to be extracted. This tar file could be compressed using gzip, bzip2, or lzma. - `extract_path` (str): Path to the directory where the files should be extracted. Requirements: 1. Open the given tar file for reading, ensuring that the appropriate compression method (if any) is used. 2. Extract only the Python files (`.py`) from the tar archive to the specified directory. 3. Use the \\"data\\" extraction filter to avoid unsafe extraction practices, such as extracting files outside the specified directory or handling special files like device files. 4. Handle exceptions appropriately, logging any errors encountered during extraction without terminating the function prematurely. Constraints: - The function should support tar files with gzip, bzip2, and lzma compressions. - Ensure that the extraction process skips any file that could pose a security risk according to the \\"data\\" extraction filter. Example Usage: ```python # Assume \'example.tar.gz\' contains various files including some Python files. safely_extract_python_files(\'example.tar.gz\', \'./extracted_files\') ``` Additional Details: - The function should print a message indicating the completion of the extraction process. - Use the `tarfile` module as described in the provided documentation to perform the tasks. Performance: - The solution should be efficient in terms of memory and handle large tar files gracefully. Good luck, and ensure your code is properly commented and follows best practices for readability and maintainability.","solution":"import tarfile import os def safely_extract_python_files(tar_path: str, extract_path: str) -> None: Safely extracts Python files (with .py extension) from a tar archive to a specified directory. :param tar_path: Path to the tar file that needs to be extracted :param extract_path: Path to the directory where the files should be extracted try: with tarfile.open(tar_path, \'r:*\') as tar: for member in tar.getmembers(): if member.name.endswith(\'.py\') and member.isreg(): # Ensure the file is regular and ends with .py member_path = os.path.join(extract_path, member.name) abs_extract_path = os.path.abspath(extract_path) abs_member_path = os.path.abspath(member_path) if abs_member_path.startswith(abs_extract_path): # Security check tar.extract(member, extract_path) print(f\\"Extraction completed successfully. Python files have been extracted to {extract_path}.\\") except Exception as e: print(f\\"An error occurred during extraction: {e}\\")"},{"question":"**Objective**: Assess students\' understanding of Seaborn\'s `sns.rugplot()` and other data visualization capabilities. **Task**: Write a Python script using Seaborn to comprehensively visualize a dataset with the following requirements: 1. **Load the \'iris\' dataset from Seaborn\'s built-in datasets**. This dataset contains measurements for different parts of iris flowers from three different species. 2. **Create a scatter plot** showing the relationship between the \'sepal_length\' and \'sepal_width\'. Represent the different species using different colors (hue). 3. **Add rug plots** along both axes to the scatter plot created in step 2. 4. **Adjust the height of the rug plots** to make them more visible by setting it to 0.05. 5. **Separate the rug plots from the axes** slightly by setting the height to a negative value and ensuring the rug plots are not clipped. **Input**: The \'iris\' dataset is to be loaded within the script using Seaborn\'s `load_dataset` function. **Output**: A single plot combining the scatter plot and rug plots as specified above. # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Create scatter plot with rug plots plt.figure(figsize=(8, 6)) sns.scatterplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\") sns.rugplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\", height=0.05, clip_on=False) # Display the plot plt.title(\\"Scatter plot with Rug plots for Iris dataset\\") plt.show() ``` # Constraints and Requirements: - **Visualization**: The plot should be clear and readable, with appropriately distinguishable colors for different species. - **Performance**: The script should be efficient and execute within 5 seconds on a standard machine. - **Libraries**: Use only Seaborn and Matplotlib for visualization. No additional libraries should be used.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Create scatter plot with rug plots plt.figure(figsize=(8, 6)) ax = sns.scatterplot(data=iris, x=\\"sepal_length\\", y=\\"sepal_width\\", hue=\\"species\\") sns.rugplot(data=iris, x=\\"sepal_length\\", hue=\\"species\\", height=0.05, ax=ax, clip_on=False) sns.rugplot(data=iris, y=\\"sepal_width\\", hue=\\"species\\", height=0.05, ax=ax, clip_on=False) # Display the plot plt.title(\\"Scatter plot with Rug plots for Iris dataset\\") plt.show()"},{"question":"**Objective**: Write a Python class that demonstrates advanced usage of the `importlib` package by implementing a custom module finder and loader. **Task**: Implement a Python class `CustomModuleFinderLoader` that can dynamically find and load a Python module from a given directory. The class should extend the abstract base class `importlib.abc.PathEntryFinder` and implement the necessary methods to search for a module in the specified directory and load it. **Specifications**: 1. **Class Definition**: - `CustomModuleFinderLoader` should be defined to extend `importlib.abc.PathEntryFinder`. 2. **Methods to Implement**: - `find_spec(fullname, target=None)`: This method should search for the module with the given `fullname` in the specified directory and return a `ModuleSpec` if found. - `find_module(fullname)`: Implement this method as a legacy wrapper around `find_spec()`. - `invalidate_caches()`: Optional implementation to invalidate any internal caches. 3. **Module Search Logic**: - The class should search for `fullname` within a directory specified at the time of the class initialization. The directory path should be an instance argument. - Use `importlib.util.spec_from_file_location(name, location)` to create a `ModuleSpec` when the module is found. 4. **Example Usage**: - Demonstrate how to register `CustomModuleFinderLoader` with `sys.path_hooks` and use it to import a module dynamically from a specific directory. **Input and Output**: - Expected input is a string representing the directory path containing the modules and the module name to be imported. - The output should be the module object of the imported module. **Performance Requirements**: - The code should handle cases where the module is not found gracefully without causing exceptions to propagate. - Ensure efficient directory traversal and module loading. # Constraints: - You should not use any third-party libraries. - Make sure to handle edge cases, such as invalid directory paths or non-existent modules. **Example**: ```python import importlib.abc import importlib.util import sys import os class CustomModuleFinderLoader(importlib.abc.PathEntryFinder): def __init__(self, directory): self.directory = directory def find_spec(self, fullname, target=None): # Your implementation here pass def find_module(self, fullname): return self.find_spec(fullname) def invalidate_caches(self): # Your optional implementation here pass # Register the custom finder and loader custom_loader = CustomModuleFinderLoader(\'/path/to/your/modules\') sys.path_hooks.append(custom_loader) # Test importing a custom module import your_custom_module your_custom_module.some_function() ``` *Please replace `\'/path/to/your/modules\'` with an actual directory path containing the Python modules you want to import.*","solution":"import importlib.abc import importlib.util import sys import os class CustomModuleFinderLoader(importlib.abc.PathEntryFinder): def __init__(self, directory): self.directory = directory def find_spec(self, fullname, target=None): module_name = fullname.split(\'.\')[-1] module_path = os.path.join(self.directory, f\\"{module_name}.py\\") if os.path.exists(module_path): spec = importlib.util.spec_from_file_location(fullname, module_path) return spec return None def find_module(self, fullname): return self.find_spec(fullname) def invalidate_caches(self): pass def register_custom_loader(directory): sys.path_hooks.append(lambda path: CustomModuleFinderLoader(directory) if path == directory else None) sys.path_importer_cache.clear() # Example usage: # custom_loader = CustomModuleFinderLoader(\'/path/to/your/modules\') # register_custom_loader(\'/path/to/your/modules\') # import your_custom_module # your_custom_module.some_function()"},{"question":"**Coding Question: Verifying DataFrame Operations with Testing** **Objective:** Design a function that performs data manipulation on a DataFrame and verifies the correctness of the operations using pandas testing functions. **Problem Statement:** You are provided with a dataset in the form of a pandas DataFrame. Your task is to perform the following operations and ensure their correctness using pandas assertion functions: 1. Create a DataFrame with the following data: ``` data = { \'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [7, 8, 9] } ``` 2. Add a new column \'D\' which is the sum of columns \'A\', \'B\', and \'C\'. 3. Verify that the DataFrame after the operation is as expected using `pandas.testing.assert_frame_equal`. **Constraints:** - Use the pandas library for DataFrame manipulations. - Ensure the use of pytest or any other testing framework is avoided; use the pandas testing functions directly. **Function Signature:** ```python import pandas as pd import pandas.testing as pdt def verify_dataframe_operations(): # Step 1: Create the initial DataFrame. # Step 2: Add the new column \'D\'. # Step 3: Create the expected DataFrame manually. # Step 4: Use pandas assertion function to verify the DataFrame. return \\"All tests passed.\\" ``` **Expected Output:** - The function should either pass silently or return a success message like \\"All tests passed\\". In the case of an error, the pandas assertion function will raise an AssertionError indicating the issue. **Example Execution:** ```python print(verify_dataframe_operations()) ``` This should output: ``` All tests passed. ``` **Requirement:** - Your implementation must create the expected DataFrame structure and validate the operations perfectly.","solution":"import pandas as pd import pandas.testing as pdt def verify_dataframe_operations(): # Step 1: Create the initial DataFrame data = { \'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [7, 8, 9] } df = pd.DataFrame(data) # Step 2: Add the new column \'D\' df[\'D\'] = df[\'A\'] + df[\'B\'] + df[\'C\'] # Step 3: Create the expected DataFrame manually expected_data = { \'A\': [1, 2, 3], \'B\': [4, 5, 6], \'C\': [7, 8, 9], \'D\': [12, 15, 18] } expected_df = pd.DataFrame(expected_data) # Step 4: Use pandas assertion function to verify the DataFrame pdt.assert_frame_equal(df, expected_df) return \\"All tests passed.\\""},{"question":"# Question You have been given a Python installation where `pip` was not included. Your task is to create a script that: 1. Checks the currently available version of `pip` using the `ensurepip` module. 2. Bootstraps `pip` into the current environment. 3. If `pip` is already installed, it upgrades to the latest version bundled with `ensurepip`. Ensure the script is robust and handles the following scenarios: - If both `--altinstall` and `--default_pip` options are specified, it should gracefully handle the exception and print an appropriate message. **Input and Output:** - The script does not require any input arguments. - The output should indicate whether `pip` was installed or updated, and display the version of `pip`. **Constraints:** - Do not use internet access as `ensurepip` does not require it. - Assume the script is running in an environment without an active virtual environment. - The script should only use the `ensurepip` module for bootstrapping and not rely on pre-installed `pip`. **Example Execution:** ```python # Example output Currently available pip version in ensurepip: 21.0.1 pip has been successfully installed and upgraded to version 21.0.1. ``` # Code Template ```python import ensurepip def main(): # Step 1: Check currently available version of pip pip_version = ensurepip.version() print(f\\"Currently available pip version in ensurepip: {pip_version}\\") try: # Step 2 and 3: Bootstrap pip and handle upgrade scenario ensurepip.bootstrap(upgrade=True) print(f\\"pip has been successfully installed and upgraded to version {pip_version}.\\") except ValueError as ve: # Handle the altinstall and default_pip exception scenario print(f\\"Error during installation: {ve}\\") if __name__ == \\"__main__\\": main() ```","solution":"import ensurepip import subprocess def main(): # Step 1: Check currently available version of pip pip_version = ensurepip.version() print(f\\"Currently available pip version in ensurepip: {pip_version}\\") try: # Step 2 and 3: Bootstrap pip and handle upgrade scenario ensurepip.bootstrap(upgrade=True) # Step 4: Verify the pip version after bootstrap result = subprocess.run([\\"python\\", \\"-m\\", \\"pip\\", \\"--version\\"], capture_output=True, text=True) pip_installed_version = result.stdout.split()[1] print(f\\"pip has been successfully installed and upgraded to version {pip_installed_version}.\\") except ValueError as ve: # Handle the altinstall and default_pip exception scenario print(f\\"Error during installation: {ve}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Python Coding Assessment Objective Your task is to implement a system that utilizes `contextvars` to manage state during asynchronous operations. You will create context variables, manage their state, and use them in an asynchronous function. Problem Statement 1. **Context Variable Initialization and Management:** - Create a `ContextVar` named `request_id` that will store string values representing unique request IDs. 2. **Context Management:** - Define a function `initialize_request(request_id_value: str)` that sets a new request ID for the current context. 3. **State Preservation across Asynchronous Calls:** - Define an asynchronous function `process_request(request_id_value: str) -> str` that: - Initializes the `request_id` using the provided `request_id_value` - Simulates some asynchronous IO-bound tasks using `await asyncio.sleep(1)` - Retrieves the current `request_id` and returns a formatted string \\"Processed request ID: {request_id}\\" 4. **Context Copying and Value Consistency:** - Demonstrate how to create and use a copy of the context within another function `handle_requests(request_ids: list)` that: - Accepts a list of request IDs - For each ID, runs `process_request` in a newly copied context - Collects and returns all results in a list. Requirements - Ensure that the context variable is correctly managed, and its value does not bleed into other contexts. - Use asyncio for simulating asynchronous operations. Example ```python import contextvars import asyncio from typing import List # Step 1: Define the ContextVar for request_id # ... # Step 2: Implement the initialize_request function # ... # Step 3: Implement the process_request function # ... # Step 4: Implement the handle_requests function # ... # Example Usage async def main(): request_ids = [\\"req_123\\", \\"req_456\\", \\"req_789\\"] results = await handle_requests(request_ids) print(results) # Expected Output: # [\'Processed request ID: req_123\', \'Processed request ID: req_456\', \'Processed request ID: req_789\'] # Run the example asyncio.run(main()) ``` Constraints - The function `initialize_request` should set the given request ID in the current context. - The function `process_request` should simulate an I/O-bound task with exactly 1-second delay. - The function `handle_requests` should manage contexts so that each call to `process_request` operates within its own context. - The context variable should not retain any value after `process_request` completes. Implement the functions `initialize_request`, `process_request`, and `handle_requests` as specified and ensure they conform to the given example usage and expected output.","solution":"import contextvars import asyncio from typing import List # Step 1: Define the ContextVar for request_id request_id = contextvars.ContextVar(\'request_id\') # Step 2: Implement the initialize_request function def initialize_request(request_id_value: str): request_id.set(request_id_value) # Step 3: Implement the process_request function async def process_request(request_id_value: str) -> str: # Initialize the request_id in the current context initialize_request(request_id_value) # Simulate some asynchronous IO-bound tasks await asyncio.sleep(1) # Retrieve the current request_id and return a formatted string current_request_id = request_id.get() return f\\"Processed request ID: {current_request_id}\\" # Step 4: Implement the handle_requests function async def handle_requests(request_ids: List[str]) -> List[str]: results = [] for rid in request_ids: # Create a new copy of the context and run process_request within it ctx = contextvars.copy_context() result = await ctx.run(process_request, rid) results.append(result) return results"},{"question":"# Command-Line Option Parser You are required to implement a command-line parser using the `getopt` module. Your script will process the command-line arguments and options specified by the user. The implementation should be able to: 1. Parse short options: `-h`, `-f <filename>`, and `-v`. 2. Parse long options: `--help`, `--file=<filename>`, and `--verbose`. 3. Handle errors gracefully using exceptions. 4. Print appropriate messages or actions based on the options provided. # Expected Behavior - If the `-h` or `--help` option is provided, print a usage message and exit. - If the `-v` or `--verbose` option is provided, set the verbose flag to True. - If the `-f <filename>` or `--file=<filename>` option is provided, store the filename. - Handle any unrecognized options or missing arguments for options that require them by displaying an appropriate error message. # Input The script will receive a list of command-line arguments and options. # Output - On providing the `-h` or `--help` option, the script should print a usage message and exit. - On providing recognized options, it should set flags or store values and print appropriate messages. - On encountering errors, it should print an error message. # Constraints - The script should efficiently handle the command line arguments and options. # Example Command: ``` script.py -f myfile.txt -v ``` Output: ``` File: myfile.txt Verbose mode enabled. ``` Command: ``` script.py --help ``` Output: ``` Usage: script.py [-h] [-v] [-f <filename>] Options: -h, --help show this help message and exit -v, --verbose enable verbose mode -f <filename>, --file=<filename> specify the filename ``` Command: ``` script.py -f ``` Output: ``` Option -f requires an argument. ``` Implement the function `parse_args(args: list) -> dict` where: 1. `args` is a list of command-line arguments and options. Function returns a dictionary with keys: - `help`: If help option was provided. - `verbose`: True if verbose flag was set. - `file`: Filename if provided, else None. Example function signature: ```python def parse_args(args: list) -> dict: # Your implementation here ```","solution":"import getopt import sys def parse_args(args): Parses command-line arguments and options. Args: args (list): A list of command-line arguments and options. Returns: dict: A dictionary with keys: - \'help\': Boolean, True if help option provided, False otherwise - \'verbose\': Boolean, True if verbose flag was set, False otherwise - \'file\': String, Filename if provided, None otherwise try: opts, _ = getopt.getopt(args, \'hvf:\', [\'help\', \'verbose\', \'file=\']) except getopt.GetoptError as err: print(err) sys.exit(2) options = { \'help\': False, \'verbose\': False, \'file\': None, } for opt, arg in opts: if opt in (\'-h\', \'--help\'): options[\'help\'] = True elif opt in (\'-v\', \'--verbose\'): options[\'verbose\'] = True elif opt in (\'-f\', \'--file\'): options[\'file\'] = arg return options"},{"question":"Coding Assessment Question # Neural Network Weight Initialization In this question, you are required to implement a function that initializes the weights of a given PyTorch neural network using specified initialization techniques. You will use the functions available in the `torch.nn.init` module. Your task involves ensuring the neural network layer weights are initialized according to specific criteria. # Problem Statement Implement the function `initialize_network` that initializes the weights and biases of the given neural network according to their layer types and specified initialization methods. The function should receive a neural network model and a dictionary containing the initialization methods for different types of layers. You must use the `torch.nn.init` module functions for the initialization process. # Function Signature ```python import torch.nn as nn import torch.nn.init as init def initialize_network(model: nn.Module, init_methods: dict): Initialize the weights and biases of the neural network layers. Args: - model (nn.Module): The neural network model whose weights need to be initialized. - init_methods (dict): A dictionary where keys are layer types (e.g., `nn.Linear`, `nn.Conv2d`) and values are another dictionary specifying \\"weight_init\\" and \\"bias_init\\" methods and their arguments. Example: { nn.Linear: { \\"weight_init\\": (\\"kaiming_uniform_\\", {\\"a\\": 0, \\"mode\\": \\"fan_in\\", \\"nonlinearity\\": \\"relu\\"}), \\"bias_init\\": (\\"normal_\\", {\\"mean\\": 0.0, \\"std\\": 0.01}) }, nn.Conv2d: { \\"weight_init\\": (\\"xavier_normal_\\", {\\"gain\\": 1}), \\"bias_init\\": (\\"constant_\\", {\\"val\\": 0}) } } Returns: - None: The function should modify the model in place and not return anything. pass ``` # Example Usage ```python import torch import torch.nn as nn import torch.nn.functional as F from initialize_function import initialize_network class ExampleNet(nn.Module): def __init__(self): super(ExampleNet, self).__init__() self.conv1 = nn.Conv2d(1, 10, kernel_size=5) self.fc1 = nn.Linear(320, 50) self.fc2 = nn.Linear(50, 10) def forward(self, x): x = F.relu(F.max_pool2d(self.conv1(x), 2)) x = x.view(-1, 320) x = F.relu(self.fc1(x)) x = self.fc2(x) return F.log_softmax(x, dim=1) # Create a model instance model = ExampleNet() # Define initialization methods init_methods = { nn.Linear: { \\"weight_init\\": (\\"kaiming_uniform_\\", {\\"a\\": 0, \\"mode\\": \\"fan_in\\", \\"nonlinearity\\": \\"relu\\"}), \\"bias_init\\": (\\"normal_\\", {\\"mean\\": 0.0, \\"std\\": 0.01}) }, nn.Conv2d: { \\"weight_init\\": (\\"xavier_normal_\\", {\\"gain\\": 1}), \\"bias_init\\": (\\"constant_\\", {\\"val\\": 0}) } } # Initialize the network initialize_network(model, init_methods) ``` # Constraints 1. Use the appropriate functions from `torch.nn.init`. 2. The initialization should be applied to the weights and biases separately as specified. 3. The `init_methods` dictionary can contain any number and combination of layer types. # Hints - You might find it useful to iterate over the model\'s layers and check their types. - Use `getattr` to dynamically call initialization functions by name stored in strings. # Note - The function should apply the initialization in place, meaning the input model is modified directly. - Ensure the function can handle layers that may not have biases without raising errors. - Test the function with a variety of neural network architectures and initialization methods.","solution":"import torch.nn as nn import torch.nn.init as init def initialize_network(model: nn.Module, init_methods: dict): Initialize the weights and biases of the neural network layers. Args: - model (nn.Module): The neural network model whose weights need to be initialized. - init_methods (dict): A dictionary where keys are layer types (e.g., `nn.Linear`, `nn.Conv2d`) and values are another dictionary specifying \\"weight_init\\" and \\"bias_init\\" methods and their arguments. Example: { nn.Linear: { \\"weight_init\\": (\\"kaiming_uniform_\\", {\\"a\\": 0, \\"mode\\": \\"fan_in\\", \\"nonlinearity\\": \\"relu\\"}), \\"bias_init\\": (\\"normal_\\", {\\"mean\\": 0.0, \\"std\\": 0.01}) }, nn.Conv2d: { \\"weight_init\\": (\\"xavier_normal_\\", {\\"gain\\": 1}), \\"bias_init\\": (\\"constant_\\", {\\"val\\": 0}) } } Returns: - None: The function should modify the model in place and not return anything. # Iterate through each layer of the model for layer in model.children(): layer_type = type(layer) if layer_type in init_methods: # Initialize weights if hasattr(layer, \'weight\') and layer.weight is not None: weight_init_fn, weight_init_params = init_methods[layer_type][\'weight_init\'] getattr(init, weight_init_fn)(layer.weight, **weight_init_params) # Initialize biases if hasattr(layer, \'bias\') and layer.bias is not None: bias_init_fn, bias_init_params = init_methods[layer_type][\'bias_init\'] getattr(init, bias_init_fn)(layer.bias, **bias_init_params)"},{"question":"Objective: Assess the student\'s understanding of the Seaborn library by requiring them to create various plots with different types of error bars and customize them appropriately. Problem Statement: You are given a task to visualize the statistical summaries of a random dataset using Seaborn. Your goal is to generate different types of error bars and customize them based on the given specifications. Task: 1. Generate a random dataset `x` of 200 samples using a normal distribution with mean 0 and standard deviation 1. 2. Plot a point plot (`sns.pointplot`) of this dataset with the following customization requirements: - Use Standard Deviation (SD) error bars with a scaling factor of 1.5. - Use Percentile Interval (PI) error bars with an interval width of 80%. - Use Standard Error (SE) error bars with the default settings. - Use Confidence Interval (CI) error bars with a 99% confidence interval using bootstrapping with 1000 iterations and a random seed of 42. 3. For each of the above plots, also include a strip plot (`sns.stripplot`) in the same figure to display the underlying data points. Use jitter with a value of 0.2 for the strip plot. Input: No input is required as you will generate the data internally. Output: The code should display four figures, each containing a point plot with the corresponding error bars and a strip plot overlay to show the underlying data points. Constraints: - Ensure that the plots have a consistent style using `sns.set_theme(style=\\"darkgrid\\")`. - The figures should be displayed in a layout that makes it easy to compare the different error bars. Example: A demonstration of one type of plot: ```python import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Set seed for reproducibility and generate data np.random.seed(0) x = np.random.normal(0, 1, 200) # Set theme sns.set_theme(style=\\"darkgrid\\") # Create the plot f, axs = plt.subplots(2, figsize=(7, 2), sharex=True, layout=\\"tight\\") sns.pointplot(x=x, errorbar=\\"sd\\", scale=1.5, capsize=.3, ax=axs[0]) sns.stripplot(x=x, jitter=.2, ax=axs[1]) plt.show() ``` Use similar code to generate the required plots as specified in the task.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt def generate_plots(): # Set seed for reproducibility and generate data np.random.seed(0) x = np.random.normal(0, 1, 200) # Set theme sns.set_theme(style=\\"darkgrid\\") # Create figure with 4 subplots fig, axs = plt.subplots(2, 2, figsize=(14, 10)) # Plot with Standard Deviation (SD) error bars sns.pointplot(x=x, errorbar=(\'sd\', 1.5), ax=axs[0, 0]) sns.stripplot(x=x, jitter=0.2, color=\'black\', ax=axs[0, 0]) axs[0, 0].set_title(\'Point plot with SD error bars\') # Plot with Percentile Interval (PI) error bars sns.pointplot(x=x, errorbar=(\'pi\', 80), ax=axs[0, 1]) sns.stripplot(x=x, jitter=0.2, color=\'black\', ax=axs[0, 1]) axs[0, 1].set_title(\'Point plot with PI error bars\') # Plot with Standard Error (SE) error bars sns.pointplot(x=x, errorbar=\'se\', ax=axs[1, 0]) sns.stripplot(x=x, jitter=0.2, color=\'black\', ax=axs[1, 0]) axs[1, 0].set_title(\'Point plot with SE error bars\') # Plot with Confidence Interval (CI) error bars sns.pointplot(x=x, errorbar=(\'ci\', 99), n_boot=1000, seed=42, ax=axs[1, 1]) sns.stripplot(x=x, jitter=0.2, color=\'black\', ax=axs[1, 1]) axs[1, 1].set_title(\'Point plot with CI error bars\') plt.tight_layout() plt.show() generate_plots()"},{"question":"# Question: Implement a Custom Async Task Manager You are required to implement a custom asynchronous task manager using the `asyncio.Future` class. The task manager should be able to create tasks, set their results or exceptions, and handle cancellations. You should also implement a function to retrieve the results of all tasks once they are done. Requirements 1. **Function Signature:** - `class TaskManager:` 2. **Methods:** - `__init__(self, loop=None)`: Initializes the manager with an optional event loop. - `create_task(self, coro)`: Accepts a coroutine, wraps it in a Future, and adds it to the task list. - `set_task_result(self, task, result)`: Sets the result for a given Future task. - `set_task_exception(self, task, exception)`: Sets an exception for a given Future task. - `cancel_task(self, task)`: Cancels a given Future task. - `get_results(self)`: Returns a list of results for all tasks that are done, raising any exceptions if they were set. 3. **Constraints:** - Assume the coroutine tasks do not interact with external I/O (e.g., network, file system). - Handle exceptions gracefully and propagate them correctly when accessing task results. - Use the given asyncio event loop or the default one if not specified. 4. **Performance Requirements:** - Operations should be non-blocking and leverage asynchronous capabilities wherever applicable. Example Usage ```python import asyncio async def sample_task(n): await asyncio.sleep(n) return f\\"Task {n} completed\\" async def main(): loop = asyncio.get_running_loop() manager = TaskManager(loop) task1 = manager.create_task(sample_task(1)) task2 = manager.create_task(sample_task(2)) manager.set_task_result(task1, \\"Overridden Result\\") manager.cancel_task(task2) try: results = await manager.get_results() print(results) except Exception as e: print(f\\"An error occurred: {e}\\") asyncio.run(main()) ``` In this example, the `TaskManager` is used to create and manage asynchronous tasks. One task has its result overridden, another is canceled, and the results are retrieved at the end.","solution":"import asyncio class TaskManager: def __init__(self, loop=None): self.loop = loop or asyncio.get_event_loop() self.tasks = [] def create_task(self, coro): task = self.loop.create_task(coro) self.tasks.append(task) return task def set_task_result(self, task, result): if not task.done(): task.set_result(result) def set_task_exception(self, task, exception): if not task.done(): task.set_exception(exception) def cancel_task(self, task): task.cancel() async def get_results(self): results = [] for task in self.tasks: try: result = await task results.append(result) except asyncio.CancelledError: results.append(\'Cancelled\') except Exception as e: results.append(f\'Error: {str(e)}\') return results"},{"question":"Objective Implement a Python function that processes transaction records using functional programming concepts such as iterators, generators, and higher-order functions. Problem Statement You are given a list of transaction records, where each record is a tuple containing the following information: - `transaction_id`: (String) A unique identifier for the transaction - `amount`: (Float) The amount of the transaction - `transaction_type`: (String) Either \\"credit\\" or \\"debit\\" - `category`: (String) The category of the transaction, e.g., \\"food\\", \\"utility\\", etc. You need to implement the following functions: 1. **`filter_records(records, category)`**: - **Input**: A list of transaction records and a string representing the category to filter by. - **Output**: A list of transaction records that belong to the specified category. 2. **`calculate_total_amount(records, transaction_type)`**: - **Input**: A list of transaction records and a string representing the transaction type (`\\"credit\\"` or `\\"debit\\"`). - **Output**: The total amount of the specified transaction type. 3. **`generate_summary(records)`**: - **Input**: A list of transaction records. - **Output**: A dictionary where the keys are categories and the values are dictionaries with two keys: `\\"total_credit\\"` and `\\"total_debit\\"`, representing the total credit and debit amounts for that category. Constraints - Avoid using loops (`for` or `while`). Instead, use iterators, generator expressions, and higher-order functions like `map()`, `filter()`, and `reduce()`. - Transactions are guaranteed to have valid `transaction_type` values (`\\"credit\\"` or `\\"debit\\"`). Example ```python transactions = [ (\\"T1\\", 100.0, \\"credit\\", \\"food\\"), (\\"T2\\", 50.0, \\"debit\\", \\"food\\"), (\\"T3\\", 200.0, \\"credit\\", \\"utility\\"), (\\"T4\\", 150.0, \\"debit\\", \\"utility\\"), (\\"T5\\", 300.0, \\"credit\\", \\"food\\"), ] # Function Usage Examples filtered_food = filter_records(transactions, \\"food\\") # Expected Output: [(\\"T1\\", 100.0, \\"credit\\", \\"food\\"), (\\"T2\\", 50.0, \\"debit\\", \\"food\\"), (\\"T5\\", 300.0, \\"credit\\", \\"food\\")] total_credit_food = calculate_total_amount(filtered_food, \\"credit\\") # Expected Output: 400.0 summary = generate_summary(transactions) # Expected Output: # { # \\"food\\": {\\"total_credit\\": 400.0, \\"total_debit\\": 50.0}, # \\"utility\\": {\\"total_credit\\": 200.0, \\"total_debit\\": 150.0} # } ``` Function Signatures ```python def filter_records(records, category): pass def calculate_total_amount(records, transaction_type): pass def generate_summary(records): pass ``` Use the provided concepts from the given documentation to implement the functions effectively.","solution":"from functools import reduce def filter_records(records, category): Filters the transaction records by the given category. return list(filter(lambda record: record[3] == category, records)) def calculate_total_amount(records, transaction_type): Calculates the total amount for the given transaction type. return reduce(lambda acc, record: acc + record[1] if record[2] == transaction_type else acc, records, 0.0) def generate_summary(records): Generates a summary of total credit and debit amounts categorized by transaction category. categories = set(map(lambda record: record[3], records)) summary = {} summary.update( {category: { \\"total_credit\\": calculate_total_amount(filter_records(records, category), \\"credit\\"), \\"total_debit\\": calculate_total_amount(filter_records(records, category), \\"debit\\") } for category in categories } ) return summary"},{"question":"**Question: Generating and Customizing Bivariate Distribution Plots Using Seaborn** You are provided with the Penguins dataset from Seaborn. Your task is to generate a series of plots to analyze the relationship between two features: `flipper_length_mm` and `bill_length_mm`. Follow the steps and requirements below to complete the task. **Requirements:** 1. **Load Dataset:** - Load the Penguins dataset using Seaborn. 2. **Plot 1: Bivariate Histogram** - Create a bivariate histogram plot for `flipper_length_mm` and `bill_length_mm`. - Color the plot using the `hue` parameter based on the `species` column. 3. **Plot 2: Bivariate KDE Plot** - Create a bivariate KDE (Kernel Density Estimate) plot for `flipper_length_mm` and `bill_length_mm`. - Add individual observation marks (rug) to the plot. 4. **Plot 3: Faceted KDE Plot** - Create a faceted KDE plot for `flipper_length_mm` and `bill_length_mm`, faceted by `sex`. - Set the height and aspect ratio of the facets to 4 and 0.7, respectively. - Title each facet with the `species` of the penguins it represents. **Implementation:** ```python import seaborn as sns # Step 1: Load the Penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create a bivariate histogram plot sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", kind=\\"hist\\") # Step 3: Create a bivariate KDE plot with a rug sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\", rug=True) # Step 4: Create a faceted KDE plot by sex, set height and aspect ratio, and title each facet g = sns.displot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\", col=\\"sex\\", height=4, aspect=0.7 ) g.set_titles(\\"{col_name} penguins\\") ``` **Constraints:** - Ensure to handle the case where data may have missing values. - Submit your code, and the generated plots should be displayed correctly. **Expected Output:** Three plots displaying the required distributions and relationships between `flipper_length_mm` and `bill_length_mm` in the Penguins dataset, with proper customizations as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the Penguins dataset penguins = sns.load_dataset(\\"penguins\\").dropna() # Step 2: Create a bivariate histogram plot bivariate_hist_plot = sns.displot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", kind=\\"hist\\" ) plt.title(\\"Bivariate Histogram of Flipper Length and Bill Length\\") plt.show() # Step 3: Create a bivariate KDE plot with a rug bivariate_kde_rug_plot = sns.displot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\", rug=True ) plt.title(\\"Bivariate KDE Plot with Rug of Flipper Length and Bill Length\\") plt.show() # Step 4: Create a faceted KDE plot by sex, set height and aspect ratio, and title each facet g = sns.displot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\", col=\\"sex\\", height=4, aspect=0.7 ) g.set_titles(\\"{col_name} penguins\\") plt.show()"},{"question":"# Question: Inventory Management System with Advanced Python Concepts You are required to implement an inventory management system for a small business using Python. This system should allow users to manage products in the inventory, including adding new products, updating product quantities, removing products, and querying the available stock. Requirements 1. **Product Class**: - Implement a `Product` class with the following attributes: - `product_id` (str): A unique identifier for the product. - `name` (str): The name of the product. - `quantity` (int): The quantity of the product in stock. - `price` (float): The price of the product. - Implement the following methods for the `Product` class: - `__init__(self, product_id, name, quantity, price)`: Initializes a new product. - `__repr__(self)`: Returns a string representation of the product details. 2. **Inventory Class**: - Implement an `Inventory` class that manages multiple `Product` objects. It should have the following attributes: - `products` (dict): A dictionary where keys are `product_id` and values are `Product` objects. - Implement the following methods for the `Inventory` class: - `add_product(self, product)`: Adds a new product to the inventory. Raises an exception if the product ID already exists. - `update_quantity(self, product_id, quantity)`: Updates the quantity of an existing product. Raises an exception if the product ID does not exist. - `remove_product(self, product_id)`: Removes a product from the inventory. Raises an exception if the product ID does not exist. - `get_product(self, product_id)`: Returns the product details for the given product ID. - `list_products(self)`: Returns a list of all products in the inventory. 3. **Additional Requirements**: - Implement error handling for the methods to ensure proper validation. Raise custom exceptions where appropriate. - Use list comprehensions or generator expressions where applicable. - Ensure your code is well commented and follows good coding style. Example Usage ```python # Create inventory inventory = Inventory() # Add products product1 = Product(\\"001\\", \\"Laptop\\", 10, 999.99) product2 = Product(\\"002\\", \\"Mouse\\", 50, 19.99) inventory.add_product(product1) inventory.add_product(product2) # Update product quantity inventory.update_quantity(\\"001\\", 15) # Remove a product inventory.remove_product(\\"002\\") # List all products print(inventory.list_products()) # Get details of a specific product print(inventory.get_product(\\"001\\")) ``` Submission Guidelines - Submit the code for `Product` and `Inventory` classes. - Include any custom exceptions created. - Ensure your code is well-documented and follows proper coding conventions. Constraints - You may assume the product IDs are unique and follow a consistent format. - The initial quantity of a product should be a non-negative integer. - The price should be a positive float value.","solution":"class Product: def __init__(self, product_id, name, quantity, price): Initializes a new product with the given product_id, name, quantity, and price. self.product_id = product_id self.name = name self.quantity = quantity self.price = price def __repr__(self): Returns a string representation of the product details. return f\\"Product(product_id=\'{self.product_id}\', name=\'{self.name}\', quantity={self.quantity}, price={self.price})\\" class Inventory: def __init__(self): Initializes an empty inventory. self.products = {} def add_product(self, product): Adds a new product to the inventory. Raises an exception if the product ID already exists. if product.product_id in self.products: raise ValueError(f\\"Product with ID {product.product_id} already exists.\\") self.products[product.product_id] = product def update_quantity(self, product_id, quantity): Updates the quantity of an existing product. Raises an exception if the product ID does not exist. if product_id not in self.products: raise ValueError(f\\"Product with ID {product_id} does not exist.\\") self.products[product_id].quantity = quantity def remove_product(self, product_id): Removes a product from the inventory. Raises an exception if the product ID does not exist. if product_id not in self.products: raise ValueError(f\\"Product with ID {product_id} does not exist.\\") del self.products[product_id] def get_product(self, product_id): Returns the product details for the given product ID. if product_id not in self.products: raise ValueError(f\\"Product with ID {product_id} does not exist.\\") return self.products[product_id] def list_products(self): Returns a list of all products in the inventory. return list(self.products.values())"},{"question":"You are provided with a dataset containing several features. Your task is to implement a feature selection process using scikit-learn\'s feature selection techniques to reduce the number of features and improve your model\'s performance. # Task 1. **Load the Dataset**: - Use the `load_breast_cancer()` function from `sklearn.datasets` to load the dataset. 2. **Feature Selection**: - Use `SelectKBest` with `chi2` to select the top 10 features. 3. **Model Training and Evaluation**: - Split the dataset into training and testing sets using `train_test_split`. - Train a `RandomForestClassifier` using the full set of features and evaluate its performance. - Train another `RandomForestClassifier` using only the selected features and evaluate its performance. - Compare the performance of the two models. # Input and Output Formats - **Input**: N/A (The dataset should be loaded within the function) - **Output**: Print the accuracy scores of the model trained on the full set of features and the model trained on the selected features. # Constraints - You must use `SelectKBest` with the `chi2` scoring function. - You must select exactly 10 features. # Performance Requirements - The solution should efficiently perform feature selection and training within a reasonable time. # Skeleton Code ```python from sklearn.datasets import load_breast_cancer from sklearn.feature_selection import SelectKBest, chi2 from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def feature_selection_and_model_evaluation(): # Load dataset data = load_breast_cancer() X, y = data.data, data.target # Feature selection selector = SelectKBest(score_func=chi2, k=10) X_selected = selector.fit_transform(X, y) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) X_train_selected, X_test_selected, _, _ = train_test_split(X_selected, y, test_size=0.3, random_state=42) # Train and evaluate model on full set of features model_full = RandomForestClassifier(random_state=42) model_full.fit(X_train, y_train) y_pred_full = model_full.predict(X_test) accuracy_full = accuracy_score(y_test, y_pred_full) # Train and evaluate model on selected features model_selected = RandomForestClassifier(random_state=42) model_selected.fit(X_train_selected, y_train) y_pred_selected = model_selected.predict(X_test_selected) accuracy_selected = accuracy_score(y_test, y_pred_selected) # Print the accuracy scores print(f\'Accuracy with full features: {accuracy_full:.4f}\') print(f\'Accuracy with selected features: {accuracy_selected:.4f}\') # Call the function feature_selection_and_model_evaluation() ```","solution":"from sklearn.datasets import load_breast_cancer from sklearn.feature_selection import SelectKBest, chi2 from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def feature_selection_and_model_evaluation(): # Load dataset data = load_breast_cancer() X, y = data.data, data.target # Feature selection selector = SelectKBest(score_func=chi2, k=10) X_selected = selector.fit_transform(X, y) # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) X_train_selected, X_test_selected, _, _ = train_test_split(X_selected, y, test_size=0.3, random_state=42) # Train and evaluate model on full set of features model_full = RandomForestClassifier(random_state=42) model_full.fit(X_train, y_train) y_pred_full = model_full.predict(X_test) accuracy_full = accuracy_score(y_test, y_pred_full) # Train and evaluate model on selected features model_selected = RandomForestClassifier(random_state=42) model_selected.fit(X_train_selected, y_train) y_pred_selected = model_selected.predict(X_test_selected) accuracy_selected = accuracy_score(y_test, y_pred_selected) # Print the accuracy scores print(f\'Accuracy with full features: {accuracy_full:.4f}\') print(f\'Accuracy with selected features: {accuracy_selected:.4f}\') # Call the function feature_selection_and_model_evaluation()"},{"question":"You are required to implement a Python function that uses the `imaplib` module to interact with an email server. The function should perform the following tasks: 1. Connect to an IMAP4 server using the connection details provided. 2. Log in using the given username and password. 3. Select the mailbox (default to \\"INBOX\\"). 4. Search for all emails that contain a specific keyword in their subject line. 5. Fetch the full message for each of these emails. 6. Print the sender\'s email address and the subject for each retrieved message. 7. Logout from the server. Function Signature ```python def search_email_by_subject_keyword(host: str, port: int, username: str, password: str, keyword: str) -> None: pass ``` Input - `host` (str): The IMAP server hostname. - `port` (int): The port number to use. - `username` (str): The user\'s email address. - `password` (str): The user\'s password. - `keyword` (str): The keyword to search for in the email subjects. Output The function should print the sender\'s email address and the subject for each email found. Constraints - Use SSL connection (`IMAP4_SSL` class) for the connection. - Handle exceptions properly, particularly for login and fetching emails. - If no emails are found with the keyword, print \\"No emails found with the keyword <keyword>.\\" Example ```python search_email_by_subject_keyword(\\"imap.example.com\\", 993, \\"user@example.com\\", \\"password\\", \\"Important\\") ``` Expected Output: ```plaintext From: sender1@example.com, Subject: Important Notice From: sender2@example.com, Subject: Extremely Important Information ``` If no emails with the specific keyword in subject: ```plaintext No emails found with the keyword Important ``` **Note:** - You should use the `SEARCH`, `FETCH`, and `LOGOUT` methods of the `IMAP4_SSL` class. - Ensure to use proper `UTF-8` encoding where required. - The sample code provided in the documentation could be helpful for understanding the basic operations.","solution":"import imaplib import email from email.header import decode_header def search_email_by_subject_keyword(host: str, port: int, username: str, password: str, keyword: str) -> None: try: # Connect to the server mail = imaplib.IMAP4_SSL(host, port) # Login to the account mail.login(username, password) # Select the mailbox (default to INBOX) mail.select(\\"inbox\\") # Search for emails containing the keyword in the subject status, search_data = mail.search(None, f\'(SUBJECT \\"{keyword}\\")\') if status != \\"OK\\" or not search_data[0]: print(f\\"No emails found with the keyword {keyword}\\") return email_ids = search_data[0].split() for email_id in email_ids: # Fetch the full message status, msg_data = mail.fetch(email_id, \\"(RFC822)\\") if status != \\"OK\\": print(f\\"Failed to fetch mail with ID {email_id}\\") continue for response_part in msg_data: if isinstance(response_part, tuple): msg = email.message_from_bytes(response_part[1]) subject, encoding = decode_header(msg[\\"subject\\"])[0] if isinstance(subject, bytes): if encoding: subject = subject.decode(encoding) else: subject = subject.decode(\'utf-8\') from_ = msg.get(\\"From\\") print(f\\"From: {from_}, Subject: {subject}\\") # Logout mail.logout() except imaplib.IMAP4.error as e: print(f\\"IMAP error encountered: {str(e)}\\") except Exception as e: print(f\\"An error occurred: {str(e)}\\")"},{"question":"**Problem Statement: Using PyTorch\'s MPS Backend for Model Training** You are tasked with training a simple neural network on a GPU using PyTorch\'s MPS backend for MacOS. The goal is to ensure the model and data are properly moved to the MPS device, and that you can perform training and evaluation of the model on this device. Your task is to: 1. Check if the MPS device is available. 2. Create a simple neural network model. 3. Generate random data and move it to the MPS device. 4. Train the model for a specified number of epochs. 5. Evaluate the model on a validation set. # Instructions: 1. **Check MPS Availability:** Display an appropriate message if the MPS device is not available. 2. **Define a Simple Neural Network:** Implement a simple feedforward neural network class using `torch.nn`. 3. **Generate Data:** Create random tensors for input data (features) and target data (labels), both moved to the MPS device. 4. **Training Loop:** - For a given number of epochs, train the model on the generated data. - Ensure the model and data are on the MPS device and perform forward and backward passes. 5. **Evaluation:** After training, evaluate the model on a separate validation set. # Function Signature: ```python import torch import torch.nn as nn import torch.optim as optim def train_and_evaluate_on_mps(input_size: int, output_size: int, epochs: int): # Check for MPS availability if not torch.backends.mps.is_available(): return \\"MPS not available.\\" # Define a simple neural network model class SimpleNet(nn.Module): def __init__(self, input_size, output_size): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, 64) self.fc2 = nn.Linear(64, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Move model to MPS device mps_device = torch.device(\\"mps\\") model = SimpleNet(input_size, output_size).to(mps_device) # Generate random data and move to MPS device X_train = torch.randn(100, input_size, device=mps_device) y_train = torch.randn(100, output_size, device=mps_device) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(epochs): model.train() optimizer.zero_grad() outputs = model(X_train) loss = criterion(outputs, y_train) loss.backward() optimizer.step() # Generate validation data and move to MPS device X_val = torch.randn(20, input_size, device=mps_device) y_val = torch.randn(20, output_size, device=mps_device) # Evaluate the model model.eval() with torch.no_grad(): predictions = model(X_val) val_loss = criterion(predictions, y_val).item() return val_loss ``` # Constraints: - Assume `input_size` and `output_size` as small integers less than 100. - Use mean squared error (MSE) as the loss criterion. - Use stochastic gradient descent (SGD) with a learning rate of 0.01. - Train for a given number of epochs. **Note:** Ensure proper checks and error handling for MPS availability. Include necessary imports in your function as shown in the signature.","solution":"import torch import torch.nn as nn import torch.optim as optim def train_and_evaluate_on_mps(input_size: int, output_size: int, epochs: int): # Check for MPS availability if not torch.backends.mps.is_available(): return \\"MPS not available.\\" # Define a simple neural network model class SimpleNet(nn.Module): def __init__(self, input_size, output_size): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, 64) self.fc2 = nn.Linear(64, output_size) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Move model to MPS device mps_device = torch.device(\\"mps\\") model = SimpleNet(input_size, output_size).to(mps_device) # Generate random data and move to MPS device X_train = torch.randn(100, input_size, device=mps_device) y_train = torch.randn(100, output_size, device=mps_device) # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(epochs): model.train() optimizer.zero_grad() outputs = model(X_train) loss = criterion(outputs, y_train) loss.backward() optimizer.step() # Generate validation data and move to MPS device X_val = torch.randn(20, input_size, device=mps_device) y_val = torch.randn(20, output_size, device=mps_device) # Evaluate the model model.eval() with torch.no_grad(): predictions = model(X_val) val_loss = criterion(predictions, y_val).item() return val_loss"},{"question":"**Question:** You are given a dataset containing information about the average monthly temperatures of a city for a year. You must visualize this data using Seaborn, applying different plotting contexts to demonstrate their impact on the plot. Specifically, follow these steps: 1. Create a line plot of the monthly temperatures without changing the default plotting context. 2. Apply the \\"talk\\" plotting context as a temporary setting using a context manager while creating a new line plot. 3. Return the default plotting context after creating the second plot to demonstrate the change. Dataset Format: The dataset is a Python dictionary with two keys: - `\\"Month\\"`: a list of strings representing months, e.g., `[\\"Jan\\", \\"Feb\\", ... , \\"Dec\\"]` - `\\"Temperature\\"`: a list of floats representing the average temperature for each month, e.g., `[30.5, 32.0, ... , 25.0]` Input: A Python dictionary named `temperature_data` with the keys `\\"Month\\"` and `\\"Temperature\\"`. Output: You do not need to return anything, but you must generate and display two plots: 1. A line plot with the default plotting context. 2. A line plot within the \\"talk\\" plotting context. Constraints: - Use the seaborn package for plotting. - Utilize the plotting_context function as shown in the provided documentation. Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Given dataset temperature_data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\", \\"Jul\\", \\"Aug\\", \\"Sep\\", \\"Oct\\", \\"Nov\\", \\"Dec\\"], \\"Temperature\\": [30.5, 32.0, 35.6, 40.0, 45.5, 50.0, 55.5, 53.0, 48.0, 42.5, 35.0, 30.0] } # 1. Line plot with default plotting context plt.figure() sns.lineplot(x=temperature_data[\\"Month\\"], y=temperature_data[\\"Temperature\\"]) plt.title(\\"Default Plotting Context\\") plt.show() # 2. Line plot with \\"talk\\" plotting context with sns.plotting_context(\\"talk\\"): plt.figure() sns.lineplot(x=temperature_data[\\"Month\\"], y=temperature_data[\\"Temperature\\"]) plt.title(\\"\\"Talk\\" Plotting Context\\") plt.show() ``` Note: Your task is to fill in the code to achieve the described visualizations, utilizing the seaborn and matplotlib packages appropriately.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_temperature_data(temperature_data): Visualizes the temperature data using different Seaborn plotting contexts. Parameters: temperature_data (dict): A dictionary with the keys \\"Month\\" and \\"Temperature\\". # Line plot with default plotting context plt.figure() sns.lineplot(x=temperature_data[\\"Month\\"], y=temperature_data[\\"Temperature\\"]) plt.title(\\"Default Plotting Context\\") plt.show() # Line plot with \\"talk\\" plotting context with sns.plotting_context(\\"talk\\"): plt.figure() sns.lineplot(x=temperature_data[\\"Month\\"], y=temperature_data[\\"Temperature\\"]) plt.title(\\"\\"Talk\\" Plotting Context\\") plt.show()"},{"question":"Objective Demonstrate your understanding of the `marshal` module by implementing functions that serialize and deserialize a complex nested data structure. Ensure you handle the constraints and potential pitfalls of using unsupported types and proper exception handling. Question You are tasked with implementing two functions, `serialize_structure` and `deserialize_structure`, that respectively serialize and deserialize a given nested data structure using the `marshal` module. # `serialize_structure` Function Input - `data_structure` (dict): A dictionary containing nested lists, sets, and tuples, which themselves contain integers, strings, and other basic supported types. - `file_path` (str): Path to the binary file where the serialized data should be stored. Output - The function should not return anything, but it should write the serialized data to the binary file specified by `file_path`. # `deserialize_structure` Function Input - `file_path` (str): Path to the binary file from where the data should be read and deserialized. Output - `data_structure` (dict): The deserialized dictionary. # Constraints 1. Use the `marshal` module for serialization and deserialization. 2. Handle file reading and writing errors properly. 3. Ensure only supported types are serialized; otherwise, raise a `ValueError`. # Example Usage ```python data_structure = { \'numbers\': [1, 2, 3, 4], \'letters\': (\'a\', \'b\', \'c\'), \'mixed\': {1, \'two\', 3.0} } serialize_structure(data_structure, \'data.marshal\') loaded_structure = deserialize_structure(\'data.marshal\') assert data_structure == loaded_structure ``` # Implement the functions ```python import marshal def serialize_structure(data_structure, file_path): try: with open(file_path, \'wb\') as file: marshal.dump(data_structure, file) except ValueError: raise ValueError(\\"Attempted to serialize an unsupported type\\") except Exception as e: print(f\\"An error occurred: {e}\\") def deserialize_structure(file_path): try: with open(file_path, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError) as e: print(f\\"Deserialization error occurred: {e}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # Example Usage data_structure = { \'numbers\': [1, 2, 3, 4], \'letters\': (\'a\', \'b\', \'c\'), \'mixed\': {1, \'two\', 3.0} } serialize_structure(data_structure, \'data.marshal\') loaded_structure = deserialize_structure(\'data.marshal\') assert data_structure == loaded_structure ```","solution":"import marshal def serialize_structure(data_structure, file_path): Serializes a given nested data structure to a binary file using marshal. Args: - data_structure: A nested dictionary containing lists, sets, tuples with integers, strings, and other basic supported types. - file_path: Path to the binary file where the serialized data should be stored. try: with open(file_path, \'wb\') as file: marshal.dump(data_structure, file) except Exception as e: print(f\\"An error occurred during serialization: {e}\\") raise def deserialize_structure(file_path): Deserializes a given binary file to a nested data structure using marshal. Args: - file_path: Path to the binary file from where the data should be read and deserialized. Returns: A nested dictionary with the same structure as the originally serialized data. try: with open(file_path, \'rb\') as file: return marshal.load(file) except Exception as e: print(f\\"An error occurred during deserialization: {e}\\") raise"},{"question":"Objective: Write a Python function that demonstrates the use of random projection for dimensionality reduction and the inverse transform functionality to approximate the reconstruction of the original data. Task: 1. Generate a synthetic high-dimensional dataset with `1000` samples and `200` features using a random number generator. 2. Use both `GaussianRandomProjection` and `SparseRandomProjection` to reduce the dimensionality of this dataset to `50` dimensions. 3. Apply the inverse transform method to approximate the original high-dimensional dataset from the reduced-dimension data. 4. Calculate the mean squared error (MSE) between the original and the reconstructed data. Function Signature ```python def random_projection_analysis(): pass ``` Requirements: 1. **Input**: No direct input. The function should internally generate the dataset and operate on it. 2. **Output**: Print the following: - The shape of the transformed data for both Gaussian and Sparse projections. - The MSE between the original and the reconstructed data for both Gaussian and Sparse projections. 3. **Constraints**: - Use `numpy` for data generation and array manipulations. - Use `sklearn.random_projection` for generating the projections and performing the transformations. Example Output: ``` Shape of transformed data using Gaussian Random Projection: (1000, 50) Shape of transformed data using Sparse Random Projection: (1000, 50) MSE using Gaussian Random Projection: 0.123456 MSE using Sparse Random Projection: 0.234567 ``` Notes: - Ensure that the random seed is set for reproducibility. - Thoroughly comment your code to make it clear and understandable.","solution":"import numpy as np from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection from sklearn.metrics import mean_squared_error def random_projection_analysis(): Perform random projection and inverse transform on synthetic data, then calculate the MSE. # Set seed for reproducibility np.random.seed(42) # Generate synthetic high-dimensional dataset: 1000 samples, 200 features X = np.random.randn(1000, 200) # Perform Gaussian Random Projection grp = GaussianRandomProjection(n_components=50, random_state=42) X_grp_transformed = grp.fit_transform(X) # Inverse transform to approximate the original data X_grp_reconstructed = grp.inverse_transform(X_grp_transformed) # Calculate MSE for Gaussian Random Projection mse_grp = mean_squared_error(X, X_grp_reconstructed) # Perform Sparse Random Projection srp = SparseRandomProjection(n_components=50, random_state=42) X_srp_transformed = srp.fit_transform(X) # Inverse transform to approximate the original data X_srp_reconstructed = srp.inverse_transform(X_srp_transformed) # Calculate MSE for Sparse Random Projection mse_srp = mean_squared_error(X, X_srp_reconstructed) # Print results print(f\\"Shape of transformed data using Gaussian Random Projection: {X_grp_transformed.shape}\\") print(f\\"Shape of transformed data using Sparse Random Projection: {X_srp_transformed.shape}\\") print(f\\"MSE using Gaussian Random Projection: {mse_grp:.6f}\\") print(f\\"MSE using Sparse Random Projection: {mse_srp:.6f}\\") # Uncomment the line below to run the function for debugging/testing purposes. # random_projection_analysis()"},{"question":"# Complex Task Using Python 3.10\'s asyncio streams Objective You are tasked with implementing a simplified HTTP-like server using Python 3.10\'s asyncio streams API. This server should handle multiple clients concurrently and parse simple GET requests. The server will only respond with a predefined message for any GET request and close the connection afterward. Details 1. **Server Requirements**: - Use `asyncio.start_server` to initiate the server. - Your server must handle multiple clients concurrently. - For each client, read their request using `StreamReader`. - The server should respond to a valid GET request with a HTTP-like response: ``` HTTP/1.1 200 OK Content-Type: text/plain Content-Length: <number of bytes> Hello, you reached the simple asyncio server! ``` - After responding to a GET request, close the client connection gracefully. - If a received request is not a GET request, respond with a `400 Bad Request` and close the connection. 2. **Client Simulation**: - Create a simple client that connects to the server using `asyncio.open_connection`. - The client should send a GET request to the server and print the response. - Ensure the client correctly handles and prints the server\'s response. Constraints - The server should be hosted on `127.0.0.1` and port `8080`. - You can limit the length of the HTTP request line to 1024 bytes. - Each response message from the server should use the exact format specified including the headers and body content. Performance Requirements - The server should be able to concurrently handle at least 10 clients connected simultaneously. Below is a template to help you get started: ```python # Simple asyncio based HTTP-like server import asyncio async def handle_client(reader, writer): try: request = await reader.read(1024) request_line = request.decode().split(\'rn\')[0] if request_line.startswith(\'GET\'): response_body = \\"Hello, you reached the simple asyncio server!\\" response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/plainrn\\" f\\"Content-Length: {len(response_body)}rn\\" \\"rn\\" f\\"{response_body}\\" ) writer.write(response.encode()) else: response = ( \\"HTTP/1.1 400 Bad Requestrn\\" \\"Content-Type: text/plainrn\\" \\"Content-Length: 0rn\\" \\"rn\\" ) writer.write(response.encode()) except Exception as e: print(f\\"Error handling client: {e}\\") finally: writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8080) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() # You can then run this main function to start the server # asyncio.run(main()) # Example function to simulate client interaction async def simulate_client(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8080) request = \\"GET / HTTP/1.1rnHost: 127.0.0.1rnrn\\" writer.write(request.encode()) await writer.drain() response = await reader.read(1024) print(f\'Received: {response.decode()}\') writer.close() await writer.wait_closed() # To test the client simulation: # asyncio.run(simulate_client()) ``` Implement the functions above to create the server and simulate a client interaction. Test your server to ensure multiple clients can be handled concurrently.","solution":"import asyncio async def handle_client(reader, writer): try: request = await reader.read(1024) request_line = request.decode().split(\'rn\')[0] if request_line.startswith(\'GET\'): response_body = \\"Hello, you reached the simple asyncio server!\\" response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/plainrn\\" f\\"Content-Length: {len(response_body)}rn\\" \\"rn\\" f\\"{response_body}\\" ) else: response = ( \\"HTTP/1.1 400 Bad Requestrn\\" \\"Content-Type: text/plainrn\\" \\"Content-Length: 0rn\\" \\"rn\\" ) writer.write(response.encode()) await writer.drain() # Ensure response is sent except Exception as e: print(f\\"Error handling client: {e}\\") finally: writer.close() await writer.wait_closed() async def main(): server = await asyncio.start_server(handle_client, \'127.0.0.1\', 8080) addrs = \', \'.join(str(sock.getsockname()) for sock in server.sockets) print(f\'Serving on {addrs}\') async with server: await server.serve_forever() async def simulate_client(): reader, writer = await asyncio.open_connection(\'127.0.0.1\', 8080) request = \\"GET / HTTP/1.1rnHost: 127.0.0.1rnrn\\" writer.write(request.encode()) await writer.drain() response = await reader.read(1024) print(f\'Received: {response.decode()}\') writer.close() await writer.wait_closed() # You can uncomment the following line to run the server: # asyncio.run(main())"},{"question":"**Objective:** Your task is to demonstrate your understanding of seaborn\'s `relplot` method by creating a complex visualization using a provided dataset \\"flights\\". You will need to customize the plot using various parameters and methods. **Dataset:** You will use the \\"flights\\" dataset from seaborn\'s in-built datasets. This dataset captures the number of passengers over different years (1949-1960) and months. **Task:** 1. **Load the dataset**: Load the \\"flights\\" dataset and pivot it so that the months become columns and the dataset is wide-form. 2. **Create a line plot**: - Use seaborn\'s `relplot` to plot the number of passengers over the years for each month. - Combine the wide-form data into one plot without faceting. 3. **Customize the plot**: - Set the height to 5 and aspect to 2 for the plot. - Use different line styles for each month. - Apply a color palette of your choice that provides good differentiation between the lines. - Add an external legend with a title \\"Monthly Passengers\\". 4. **Additional plot modifications**: - Add a y-axis label \\"Number of Passengers\\". - Add x-axis label \\"Year\\". - Set an appropriate title for the plot. **Constraints:** - Your code should be efficient and run within 2 seconds. **Input Format:** - No external input is required. The dataset is loaded directly from seaborn\'s repository. **Output Format:** - Display the created plot. **Example Code:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the flights dataset and pivot it to wide-form flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Create a line plot using seaborn relplot g = sns.relplot(data=flights_wide, kind=\\"line\\", height=5, aspect=2) # Customize the plot further g.set_axis_labels(\\"Year\\", \\"Number of Passengers\\") g._legend.set_title(\\"Monthly Passengers\\") g.fig.suptitle(\\"Airline Passengers Over Time\\") plt.show() ``` **Note**: The answer code provided here is a simplified version and may need to be adapted to fit all the requirements specified in the task.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_flights(): # Load the flights dataset and pivot it to wide-form flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Reset index to bring \'year\' back as a column flights_wide_reset = flights_wide.reset_index() # Melt the wide-form dataframe to long-form for relplot flights_melted = pd.melt(flights_wide_reset, id_vars=\'year\', var_name=\'month\', value_name=\'passengers\') # Create a line plot using seaborn relplot g = sns.relplot( data=flights_melted, x=\\"year\\", y=\\"passengers\\", kind=\\"line\\", hue=\\"month\\", height=5, aspect=2, style=\\"month\\", palette=\\"tab10\\" ) # Customize the plot further g.set_axis_labels(\\"Year\\", \\"Number of Passengers\\") g.legend.set_title(\\"Monthly Passengers\\") g.fig.suptitle(\\"Airline Passengers Over Time\\") # Show the plot plt.show()"},{"question":"You are assigned to implement a caching system for large image objects using weak references to ensure that images are not kept alive solely because they are present in the cache. You will maintain a mapping of image names to image objects using weak references, so that when an image is no longer used elsewhere, it can be garbage collected, and its entry in the cache will be automatically removed. Your task is to create a `Cache` class that uses `weakref.WeakValueDictionary` to store the mapping. Additionally, implement a `finalize` function to notify when an image is about to be garbage collected. Requirements: 1. Implement a `Cache` class with methods to add and retrieve images. 2. Use `weakref.WeakValueDictionary` to store the images. 3. Register a `finalize` callback to be called when an image is about to be garbage collected. 4. The `get_image` method should return `None` if the image has been garbage collected. Constraints: - Image objects can be of any class type. - The `name` attribute of the image object should be used as the key in the dictionary. - Assume the `Image` class has an attribute `data` representing the image data and `name` representing the image name. Example Usage: ```python import weakref class Image: def __init__(self, name, data): self.name = name self.data = data class Cache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_image(self, image): Adds an Image object to the cache and registers a finalize callback to notify when the image is about to be garbage collected. self._cache[image.name] = image weakref.finalize(image, self._on_image_finalize, image.name) def get_image(self, name): Retrieves an Image object by name, or returns None if the image has been garbage collected. return self._cache.get(name) def _on_image_finalize(self, name): Callback function called when an image is about to be garbage collected. print(f\'Image \\"{name}\\" is about to be garbage collected.\') # Example Usage cache = Cache() img1 = Image(name=\'image1\', data=b\'data1\') img2 = Image(name=\'image2\', data=b\'data2\') cache.add_image(img1) cache.add_image(img2) print(cache.get_image(\'image1\').data) # Output: b\'data1\' # Simulate garbage collection del img1 import gc gc.collect() # triggers the finalize callback print(cache.get_image(\'image1\')) # Output: None ``` Submit your implementation of the `Cache` class.","solution":"import weakref class Image: def __init__(self, name, data): self.name = name self.data = data class Cache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_image(self, image): Adds an Image object to the cache and registers a finalize callback to notify when the image is about to be garbage collected. self._cache[image.name] = image weakref.finalize(image, self._on_image_finalize, image.name) def get_image(self, name): Retrieves an Image object by name, or returns None if the image has been garbage collected. return self._cache.get(name) def _on_image_finalize(self, name): Callback function called when an image is about to be garbage collected. print(f\'Image \\"{name}\\" is about to be garbage collected.\')"},{"question":"Objective Create a Python script that performs a series of file operations using the `shutil` module. Your task is to implement the following functionalities: 1. **Backup Directory**: Create a compressed backup (using `.tar.gz` format) of a given directory. 2. **Move Files**: Move all files with a specific extension from one directory to another. 3. **Delete Files by Pattern**: Delete all files that match a specific pattern within a directory and its subdirectories. 4. **Copy Directory Structure**: Create a new directory copy, preserving the structure but not the files. 5. **Generate Disk Usage Report**: Generate a report of disk usage for a given directory, detailing total, used, and free space. Function Specifications 1. **backup_directory(src_dir, backup_name)** - **Input**: - `src_dir` (str): Path to the directory to backup. - `backup_name` (str): Name for the backup file. - **Output**: - `backup_path` (str): Path to the created backup file. Example: ```python backup_path = backup_directory(\'/path/to/dir\', \'my_backup\') # backup_path could be \'/path/to/my_backup.tar.gz\' ``` 2. **move_files(src_dir, dst_dir, extension)** - **Input**: - `src_dir` (str): Path to the source directory. - `dst_dir` (str): Path to the destination directory. - `extension` (str): File extension to move (e.g., \'.txt\'). - **Output**: None Example: ```python move_files(\'/path/to/src\', \'/path/to/dst\', \'.txt\') ``` 3. **delete_files_by_pattern(directory, pattern)** - **Input**: - `directory` (str): Path to the directory. - `pattern` (str): Pattern to match files (e.g., \'*.log\'). - **Output**: None Example: ```python delete_files_by_pattern(\'/path/to/dir\', \'*.log\') ``` 4. **copy_directory_structure(src_dir, dst_dir)** - **Input**: - `src_dir` (str): Path to the source directory. - `dst_dir` (str): Path to the destination directory. - **Output**: None Example: ```python copy_directory_structure(\'/path/to/src\', \'/path/to/dst\') ``` 5. **disk_usage_report(directory)** - **Input**: - `directory` (str): Path to the directory. - **Output**: - `report` (dict): Dictionary containing \'total\', \'used\', and \'free\' disk space in bytes. Example: ```python report = disk_usage_report(\'/path/to/dir\') # report could be {\'total\': 1000000000, \'used\': 500000000, \'free\': 500000000} ``` Constraints - Your solution must handle errors gracefully and provide informative error messages. - Use relevant `shutil` functions to perform required tasks. - Ensure that the script is efficient and avoids unnecessary operations. Performance Requirements - Operations should be efficient enough to handle directories with thousands of files without significant performance delays. Example Usage ```python def main(): # Perform backup backup_path = backup_directory(\'/source/directory\', \'backup_name\') print(f\'Backup created at {backup_path}\') # Move .txt files move_files(\'/source/directory\', \'/destination/directory\', \'.txt\') # Delete .log files delete_files_by_pattern(\'/source/directory\', \'*.log\') # Copy directory structure copy_directory_structure(\'/source/directory\', \'/new/structure/directory\') # Generate disk usage report report = disk_usage_report(\'/\') print(f\\"Disk Usage - Total: {report[\'total\']} bytes, Used: {report[\'used\']} bytes, Free: {report[\'free\']} bytes\\") if __name__ == \\"__main__\\": main() ``` **Note**: Ensure that you write your script in a modular way with error handling, and organize your code into appropriate functions as specified.","solution":"import shutil import os import fnmatch import tarfile import errno def backup_directory(src_dir, backup_name): Create a compressed backup of a given directory. Args: src_dir (str): Path to the directory to backup. backup_name (str): Name for the backup file. Returns: str: Path to the created backup file. backup_path = f\\"{backup_name}.tar.gz\\" try: with tarfile.open(backup_path, \\"w:gz\\") as tar: tar.add(src_dir, arcname=os.path.basename(src_dir)) except (OSError, tarfile.TarError) as e: print(f\\"Error creating backup: {e}\\") return backup_path def move_files(src_dir, dst_dir, extension): Move all files with a specific extension from one directory to another. Args: src_dir (str): Path to the source directory. dst_dir (str): Path to the destination directory. extension (str): File extension to move (e.g., \'.txt\'). try: os.makedirs(dst_dir, exist_ok=True) for file_name in os.listdir(src_dir): if file_name.endswith(extension): shutil.move(os.path.join(src_dir, file_name), os.path.join(dst_dir, file_name)) except OSError as e: print(f\\"Error moving files: {e}\\") def delete_files_by_pattern(directory, pattern): Delete all files that match a specific pattern within a directory and its subdirectories. Args: directory (str): Path to the directory. pattern (str): Pattern to match files (e.g., \'*.log\'). try: for root, _, files in os.walk(directory): for file_name in fnmatch.filter(files, pattern): try: os.remove(os.path.join(root, file_name)) except OSError as e: print(f\\"Error deleting file: {e}\\") except OSError as e: print(f\\"Error processing directory: {e}\\") def copy_directory_structure(src_dir, dst_dir): Create a new directory copy, preserving the structure but not the files. Args: src_dir (str): Path to the source directory. dst_dir (str): Path to the destination directory. try: for root, dirs, _ in os.walk(src_dir): for dir_ in dirs: new_dir = os.path.join(dst_dir, os.path.relpath(os.path.join(root, dir_), src_dir)) os.makedirs(new_dir, exist_ok=True) except OSError as e: print(f\\"Error copying directory structure: {e}\\") def disk_usage_report(directory): Generate a report of disk usage for a given directory. Args: directory (str): Path to the directory. Returns: dict: Dictionary containing \'total\', \'used\', and \'free\' disk space in bytes. try: total, used, free = shutil.disk_usage(directory) return {\'total\': total, \'used\': used, \'free\': free} except OSError as e: print(f\\"Error obtaining disk usage report: {e}\\") return {}"},{"question":"**Question: Command-Line Arguments Parsing with `getopt` Module** **Description**: You need to write a Python function `parse_arguments` that parses command-line arguments using the `getopt` module. Your function should handle short and long options, and also manage errors gracefully. **Function Signature**: ```python def parse_arguments(arg_list: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: pass ``` **Inputs**: - `arg_list`: A list of strings representing command-line arguments. **Outputs**: - Returns a tuple containing: 1. A list of `(option, value)` pairs. 2. A list of remaining arguments after the option list has been parsed. **Constraints**: - The function should recognize the following options: - Short options: `-a`, `-b`, `-c <value>`, `-d <value>` - Long options: `--alpha`, `--beta`, `--charlie=<value>`, `--delta=<value>` - If an unrecognized option or a missing required argument is encountered, the function should raise a `getopt.GetoptError` exception. **Examples**: ```python try: # Example 1 optlist, args = parse_arguments([\'-a\', \'-b\', \'-cfoo\', \'-d\', \'bar\', \'arg1\', \'arg2\']) print(optlist) # Should output: [(\'-a\', \'\'), (\'-b\', \'\'), (\'-c\', \'foo\'), (\'-d\', \'bar\')] print(args) # Should output: [\'arg1\', \'arg2\'] # Example 2 optlist, args = parse_arguments([\'--alpha\', \'--beta\', \'--charlie=foo\', \'--delta\', \'bar\', \'arg1\', \'arg2\']) print(optlist) # Should output: [(\'--alpha\', \'\'), (\'--beta\', \'\'), (\'--charlie\', \'foo\'), (\'--delta\', \'bar\')] print(args) # Should output: [\'arg1\', \'arg2\'] # Example 3 optlist, args = parse_arguments([\'-a\', \'--charlie=foo\']) print(optlist) # Should output: [(\'-a\', \'\'), (\'--charlie\', \'foo\')] print(args) # Should output: [] except getopt.GetoptError as e: print(f\\"Error: {e}\\") ``` **Notes**: - You may use the `sys` module to access command-line arguments, but for this function, assume `arg_list` is provided directly. - Your solution should demonstrate familiarity with `getopt` module usage and error handling.","solution":"import getopt from typing import List, Tuple def parse_arguments(arg_list: List[str]) -> Tuple[List[Tuple[str, str]], List[str]]: Parses command-line arguments. Parameters: arg_list (List[str]): A list of strings representing command-line arguments. Returns: Tuple[List[Tuple[str, str]], List[str]]: A tuple containing a list of (option, value) pairs and a list of remaining arguments. short_opts = \\"abc:d:\\" long_opts = [\\"alpha\\", \\"beta\\", \\"charlie=\\", \\"delta=\\"] try: opts, args = getopt.getopt(arg_list, short_opts, long_opts) return opts, args except getopt.GetoptError as err: raise err"},{"question":"**Coding Assessment Question:** # Objective: Demonstrate your understanding of seaborn\'s `relplot` function to visualize complex relationships within a dataset. # Problem Statement: You are given a dataset containing information about the daily counts of bicycle rentals in a city over a year. The dataset contains the following columns: - `date`: Date of the observation. - `total_rentals`: The total number of bicycles rented on that day. - `weekday`: The day of the week (e.g., Monday, Tuesday, etc.). - `holiday`: Indicator of whether the day is a holiday (0 for non-holiday, 1 for holiday). - `weather`: Weather condition on that day (e.g., Sunny, Rainy, etc.). - `temperature`: Temperature in Celsius on that day. Your task is to write a Python function that uses seaborn to generate the following visualizations: 1. **Scatter plot**: - Show the relationship between `temperature` and `total_rentals`. - Differentiate the points by `weather` condition using the `hue` semantic. - Differentiate the points by `holiday` status using the `style` semantic. 2. **Line plot**: - Show the trend of `total_rentals` over time. - Differentiate the lines by `weekday` using the `hue` semantic. - Display 95% confidence intervals around the mean. 3. **Faceted scatter plot**: - Show the relationship between `temperature` and `total_rentals`. - Differentiate the points by `weather` condition using the `hue` semantic. - Create separate plots for `holiday` and `non-holiday` days using faceting. # Input: - A pandas DataFrame named `bicycle_data` with the aforementioned columns. # Expected Output: - Three seaborn plots as described in the problem statement. # Constraints: - Use the `seaborn.relplot` function for generating the plots. - Ensure the plots are readable and well-labeled. # Additional Notes: - You may use additional seaborn and matplotlib functions to customize the appearance of your plots if necessary. - You can assume the dataset is pre-loaded into the `bicycle_data` DataFrame. # Function Signature: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_bicycle_data(bicycle_data: pd.DataFrame): # Your code here pass ``` # Example Usage: ```python # Assuming bicycle_data is pre-loaded visualize_bicycle_data(bicycle_data) ``` This question aims to assess your understanding of seaborn\'s `relplot` function and your ability to create complex visualizations that effectively communicate relationships in the data. Good luck!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_bicycle_data(bicycle_data: pd.DataFrame): Generate three seaborn visualizations to explore the bicycle rental data: 1. Scatter plot showing relationship between temperature and total rentals. 2. Line plot showing trend of total rentals over time. 3. Faceted scatter plot showing the relationship between temperature and total rentals. # Scatter plot: Temperature vs Total Rentals with weather (hue) and holiday (style) scatter_plot = sns.relplot( data=bicycle_data, x=\\"temperature\\", y=\\"total_rentals\\", hue=\\"weather\\", style=\\"holiday\\", kind=\\"scatter\\" ) scatter_plot.set_axis_labels(\\"Temperature (C)\\", \\"Total Rentals\\") scatter_plot.fig.suptitle(\'Scatter Plot of Temperature vs Total Rentals\') scatter_plot.fig.subplots_adjust(top=0.95) # Adjust title to fit the plot # Line plot: Total Rentals over Time, differentiated by weekday bicycle_data[\'date\'] = pd.to_datetime(bicycle_data[\'date\']) line_plot = sns.relplot( data=bicycle_data, x=\\"date\\", y=\\"total_rentals\\", hue=\\"weekday\\", kind=\\"line\\", ci=\\"sd\\" ) line_plot.set_axis_labels(\\"Date\\", \\"Total Rentals\\") line_plot.fig.suptitle(\'Line Plot of Total Rentals Over Time\') line_plot.fig.subplots_adjust(top=0.95) # Adjust title to fit the plot # Faceted scatter plot: Temperature vs Total Rentals, faceted by holiday facet_scatter_plot = sns.relplot( data=bicycle_data, x=\\"temperature\\", y=\\"total_rentals\\", hue=\\"weather\\", col=\\"holiday\\", kind=\\"scatter\\" ) facet_scatter_plot.set_axis_labels(\\"Temperature (C)\\", \\"Total Rentals\\") facet_scatter_plot.fig.suptitle(\'Faceted Scatter Plot of Temperature vs Total Rentals\') facet_scatter_plot.fig.subplots_adjust(top=0.85) # Adjust title to fit the plots plt.show() # Visualization function assumes that `bicycle_data` DataFrame has been pre-loaded"},{"question":"Custom Sequence Implementation # Objective Implement a custom sequence type by inheriting from `collections.abc.Sequence`. This will test your understanding of abstract methods and the default behavior of mixin methods provided by ABCs. # Instructions You are required to implement a class named `CustomSequence` that inherits from `collections.abc.Sequence`. This class should store elements in a list internally and must implement the following methods as per the ABC requirements: 1. `__getitem__(self, index)`: Return the element at the given index. 2. `__len__(self)`: Return the number of elements in the sequence. Additionally, add the following features: 1. A method `append(self, value)` to add elements to the sequence. 2. A method `__contains__(self, value)` to check if a value exists in the sequence. This is optional as it can be inherited from mixins, but you can override it if needed. # Constraints - You may not use any list methods directly except for `append` and `__iter__`. - Ensure that your implementation handles common edge cases such as negative indices and out-of-bounds indices. # Example Usage ```python from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self): self._elements = [] def __getitem__(self, index): # Implement method pass def __len__(self): # Implement method pass def append(self, value): self._elements.append(value) # Optional: Override if necessary def __contains__(self, value): return value in self._elements # Example seq = CustomSequence() seq.append(1) seq.append(2) seq.append(3) print(len(seq)) # Output: 3 print(seq[1]) # Output: 2 print(2 in seq) # Output: True print(4 in seq) # Output: False print(list(seq)) # Output: [1, 2, 3] ``` # Submission Requirements 1. Your implementation of the `CustomSequence` class. 2. Thorough testing of your class to demonstrate that all required methods work correctly. 3. Ensure your code handles edge cases such as negative indexing and out-of-bounds access. # Evaluation Criteria - Correct implementation of required and optional methods. - Proper usage of inheritance and mixins from `collections.abc.Sequence`. - Handling of edge cases and efficient use of list operations. - Code readability and commented sections explaining the logic.","solution":"from collections.abc import Sequence class CustomSequence(Sequence): def __init__(self): self._elements = [] def __getitem__(self, index): if index < 0: # Handle negative indices index = len(self._elements) + index if index >= len(self._elements) or index < 0: raise IndexError(\'index out of range\') return self._elements[index] def __len__(self): return len(self._elements) def append(self, value): self._elements.append(value) def __contains__(self, value): return value in self._elements"},{"question":"**Question: Comprehensive DateTime Manipulation** You are required to implement a function that takes a list of datetime objects and performs various operations such as type-checking, field extraction, and creation of new datetime-related objects. Your function will demonstrate your understanding of the `datetime` module and its C API macros. # Function Signature ```python from datetime import datetime, date, time, timedelta, timezone from typing import List, Tuple, Union def manipulate_datetimes(datetime_list: List[Union[datetime, date, time, timedelta, timezone]]) -> Tuple[List[Union[date, datetime, time, timedelta, timezone]], List[bool]]: pass ``` # Input - `datetime_list`: List of `datetime`, `date`, `time`, `timedelta`, and `timezone` objects. # Output A tuple containing two lists: 1. **Transformed List**: A list of new datetime-related objects created or manipulated based on the input list. 2. **Type Check List**: A list of boolean values indicating whether each input object passed its respective type check. # Constraints - You can assume that the input list is not empty. - Focus on correctly implementing type checks, field extraction, and creation of new datetime objects. # Requirements 1. **Type Checking**: Use the provided type-check macros to determine the type of each object in the input list. 2. **Field Extraction**: Use the provided extraction macros to get specific fields from the datetime objects. 3. **Object Creation**: Use the provided creation macros to create new datetime-related objects based on the extracted fields. # Example ```python from datetime import datetime, date, time, timedelta, timezone # Example usage of the function dt_list = [ datetime(2023, 10, 5, 14, 30, 45), date(2023, 10, 5), time(14, 30, 45), timedelta(days=5, seconds=3600), timezone(timedelta(hours=1)) ] result = manipulate_datetimes(dt_list) # The transformed list contains new datetime-related objects transformed_list = [ date(2023, 10, 5), # Derived from datetime(2023, 10, 5, 14, 30, 45) datetime(2023, 10, 5, 0, 0, 0), # Derived from date(2023, 10, 5) datetime(1, 1, 1, 14, 30, 45), # Derived from time(14, 30, 45) timedelta(days=5, seconds=3600), # Same timedelta object timezone(timedelta(hours=1)) # Same timezone object ] # The type check list contains the type checks for each input object type_check_list = [True, True, True, True, True] assert result == (transformed_list, type_check_list) ``` # Explanation - **Type Checking**: For each object in `datetime_list`, check if it is of type `datetime`, `date`, `time`, `timedelta`, or `timezone`. - **Field Extraction**: Based on the type of the object, extract relevant fields (such as year, month, day, hour, minute, second) using the extraction macros. - **Object Creation**: Create new datetime-related objects (e.g., a `date` object from a `datetime` object, a `datetime` object from a `time` object with default date, etc.) using the creation macros. This problem will test your ability to manipulate complex datetime objects, ensure type correctness, and utilize the datetime module\'s extensive capabilities in Python.","solution":"from datetime import datetime, date, time, timedelta, timezone from typing import List, Tuple, Union def manipulate_datetimes(datetime_list: List[Union[datetime, date, time, timedelta, timezone]]) -> Tuple[List[Union[date, datetime, time, timedelta, timezone]], List[bool]]: transformed_list = [] type_check_list = [] for dt in datetime_list: if isinstance(dt, datetime): # Extract date part from datetime transformed_list.append(dt.date()) type_check_list.append(True) elif isinstance(dt, date): # Create a datetime from date with default time transformed_list.append(datetime(dt.year, dt.month, dt.day)) type_check_list.append(True) elif isinstance(dt, time): # Create a datetime from time with default date transformed_list.append(datetime(1, 1, 1, dt.hour, dt.minute, dt.second, dt.microsecond)) type_check_list.append(True) elif isinstance(dt, timedelta): # Use the same timedelta object transformed_list.append(dt) type_check_list.append(True) elif isinstance(dt, timezone): # Use the same timezone object transformed_list.append(dt) type_check_list.append(True) else: type_check_list.append(False) return transformed_list, type_check_list"},{"question":"**Coding Assessment Question** # Objective You are tasked with demonstrating your understanding of the seaborn `sns.pointplot` function and its applications. Specifically, you need to generate insights by visualizing data from the seaborn `penguins` dataset. # Problem Statement Using the `penguins` dataset, create a point plot that shows the relationship between the island and the body mass of penguins. Your plot should highlight differences based on the sex of the penguins and include the following features: 1. Group the data by island and visualize body mass. 2. Differentiate the data based on the sex of the penguins. 3. Use different markers and linestyles for male and female penguins. 4. Represent the standard deviation of each distribution using error bars. 5. Customize the plot with an appropriate title, axis labels, and aesthetics (e.g., marker size and color). # Requirements - **Input Data**: Use the seaborn `penguins` dataset. Load it using `sns.load_dataset(\\"penguins\\")`. - **Output Plot**: A seaborn point plot that meets the specified features. - **Plot Aesthetics**: - Title: \\"Penguin Body Mass by Island and Sex\\" - X-axis label: \\"Island\\" - Y-axis label: \\"Body Mass (g)\\" - Markers: Use \\"o\\" for female and \\"s\\" for male. - Linestyles: Use solid lines for female and dashed lines for male. - Error bars for standard deviation. - Custom marker size: 8 - A suitable color palette for clear differentiation between male and female penguins. # Constraints - All necessary libraries should be imported. - Ensure that the plot is rendered within a single cell, with appropriate comments explaining each step. - Use seaborn and matplotlib for visualization. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_body_mass(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set the seaborn theme for the plot sns.set_theme(style=\\"whitegrid\\") # Create the point plot with the specified features sns.pointplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\" ) # Customize the plot aesthetics plt.title(\\"Penguin Body Mass by Island and Sex\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Body Mass (g)\\") # Render the plot plt.show() ``` Implement the `plot_penguin_body_mass` function to generate the required plot.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_body_mass(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Set the seaborn theme for the plot sns.set_theme(style=\\"whitegrid\\") # Create the point plot with the specified features sns.pointplot( data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], errorbar=\\"sd\\", dodge=True, scale=0.8 ) # Customize the plot aesthetics plt.title(\\"Penguin Body Mass by Island and Sex\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Body Mass (g)\\") # Render the plot plt.show()"},{"question":"**Objective**: Evaluate the students\' understanding and ability to implement and utilize the `sklearn.random_projection` module for dimensionality reduction and data reconstruction. **Question**: You are given a high-dimensional dataset `X` of shape `(1000, 10000)`. Your task is to: 1. Use both Gaussian and sparse random projection techniques to reduce the dimensionality of the dataset. 2. Compute and compare the pairwise distances in both the original and reduced-dimensional spaces using each technique. 3. Perform inverse transformations to reconstruct the original data from the reduced data. 4. Calculate and compare the reconstruction error for both techniques. # Requirements: - Use `GaussianRandomProjection` and `SparseRandomProjection` from `sklearn.random_projection`. - Implement functions as specified to handle the steps outlined. - Input: A numpy array `X` of shape `(1000, 10000)` containing random values. - Output: - Reduced datasets for both techniques. - Pairwise distance matrices for original and reduced datasets. - Reconstructed datasets from reduced data. - Reconstruction errors for both techniques. # Constraints: - Use the `euclidean` metric for calculating pairwise distances. - The dimensionality of the reduced space should be set to `500`. # Performance: - Ensure your implementations are efficient in terms of both time and memory. - Use vectorized operations where possible. # Function Signatures: ```python from typing import Tuple import numpy as np def reduce_dimensionality_gaussian(X: np.ndarray, n_components: int = 500) -> np.ndarray: Reduce the dimensionality of dataset X using Gaussian Random Projection. :param X: Input dataset of shape (1000, 10000) :param n_components: Number of components for reduction :return: Reduced dataset of shape (1000, n_components) # Your code here def reduce_dimensionality_sparse(X: np.ndarray, n_components: int = 500) -> np.ndarray: Reduce the dimensionality of dataset X using Sparse Random Projection. :param X: Input dataset of shape (1000, 10000) :param n_components: Number of components for reduction :return: Reduced dataset of shape (1000, n_components) # Your code here def compute_pairwise_distances(X: np.ndarray) -> np.ndarray: Compute pairwise Euclidean distances for dataset X. :param X: Input dataset :return: Pairwise distance matrix # Your code here def reconstruct_data(transformer, X_reduced: np.ndarray) -> np.ndarray: Reconstruct the original data from the reduced data using the specified transformer. :param transformer: The transformer used for dimensionality reduction :param X_reduced: Reduced dataset :return: Reconstructed dataset # Your code here def reconstruction_error(X_original: np.ndarray, X_reconstructed: np.ndarray) -> float: Calculate the mean squared error between the original and reconstructed datasets. :param X_original: Original dataset of shape (1000, 10000) :param X_reconstructed: Reconstructed dataset :return: Mean squared error # Your code here # Main code to run the above functions if __name__ == \\"__main__\\": np.random.seed(42) X = np.random.rand(1000, 10000) X_gaussian = reduce_dimensionality_gaussian(X) X_sparse = reduce_dimensionality_sparse(X) dist_original = compute_pairwise_distances(X) dist_gaussian = compute_pairwise_distances(X_gaussian) dist_sparse = compute_pairwise_distances(X_sparse) transformer_gaussian = random_projection.GaussianRandomProjection(n_components=500) transformer_sparse = random_projection.SparseRandomProjection(n_components=500) transformer_gaussian.fit(X) transformer_sparse.fit(X) X_reconstructed_gaussian = reconstruct_data(transformer_gaussian, X_gaussian) X_reconstructed_sparse = reconstruct_data(transformer_sparse, X_sparse) error_gaussian = reconstruction_error(X, X_reconstructed_gaussian) error_sparse = reconstruction_error(X, X_reconstructed_sparse) print(\\"Gaussian Projection Error:\\", error_gaussian) print(\\"Sparse Projection Error:\\", error_sparse) ``` **Notes**: - Ensure that the code is structured clearly with appropriate comments and docstrings. - The provided functions should be implemented and executed in sequence, and the results printed as specified.","solution":"from sklearn import random_projection from sklearn.metrics.pairwise import euclidean_distances import numpy as np def reduce_dimensionality_gaussian(X: np.ndarray, n_components: int = 500) -> np.ndarray: Reduce the dimensionality of dataset X using Gaussian Random Projection. :param X: Input dataset of shape (1000, 10000) :param n_components: Number of components for reduction :return: Reduced dataset of shape (1000, n_components) transformer = random_projection.GaussianRandomProjection(n_components=n_components) return transformer.fit_transform(X) def reduce_dimensionality_sparse(X: np.ndarray, n_components: int = 500) -> np.ndarray: Reduce the dimensionality of dataset X using Sparse Random Projection. :param X: Input dataset of shape (1000, 10000) :param n_components: Number of components for reduction :return: Reduced dataset of shape (1000, n_components) transformer = random_projection.SparseRandomProjection(n_components=n_components) return transformer.fit_transform(X) def compute_pairwise_distances(X: np.ndarray) -> np.ndarray: Compute pairwise Euclidean distances for dataset X. :param X: Input dataset :return: Pairwise distance matrix return euclidean_distances(X) def reconstruct_data(transformer, X_reduced: np.ndarray) -> np.ndarray: Reconstruct the original data from the reduced data using the specified transformer. :param transformer: The transformer used for dimensionality reduction :param X_reduced: Reduced dataset :return: Reconstructed dataset return transformer.inverse_transform(X_reduced) def reconstruction_error(X_original: np.ndarray, X_reconstructed: np.ndarray) -> float: Calculate the mean squared error between the original and reconstructed datasets. :param X_original: Original dataset of shape (1000, 10000) :param X_reconstructed: Reconstructed dataset :return: Mean squared error return np.mean((X_original - X_reconstructed) ** 2) if __name__ == \\"__main__\\": np.random.seed(42) X = np.random.rand(1000, 10000) X_gaussian = reduce_dimensionality_gaussian(X) X_sparse = reduce_dimensionality_sparse(X) dist_original = compute_pairwise_distances(X) dist_gaussian = compute_pairwise_distances(X_gaussian) dist_sparse = compute_pairwise_distances(X_sparse) transformer_gaussian = random_projection.GaussianRandomProjection(n_components=500) transformer_sparse = random_projection.SparseRandomProjection(n_components=500) transformer_gaussian.fit(X) transformer_sparse.fit(X) X_reconstructed_gaussian = reconstruct_data(transformer_gaussian, X_gaussian) X_reconstructed_sparse = reconstruct_data(transformer_sparse, X_sparse) error_gaussian = reconstruction_error(X, X_reconstructed_gaussian) error_sparse = reconstruction_error(X, X_reconstructed_sparse) print(\\"Gaussian Projection Error:\\", error_gaussian) print(\\"Sparse Projection Error:\\", error_sparse)"},{"question":"Objective This question aims to test your understanding of seaborn\'s swarm plot functionalities and faceting methods. Problem Statement Using the seaborn library, write a Python function `analyze_tips_data()` that performs the following tasks: 1. **Load the tips dataset**: - Load the `tips` dataset from seaborn\'s built-in datasets. 2. **Plot a swarmplot showing the distribution of total bills for each day**: - Create a swarm plot showing the distribution of `total_bill` values for each `day`. - Use distinct colors for different days by leveraging the `hue` parameter. 3. **Further adjust the swarmplot by setting dodge=True to separate categorical levels by sex**: - Adjust the swarm plot to separate the data points for each `sex` within each `day` category by setting the `dodge` parameter to `True`. 4. **Create a faceted swarmplot using catplot, comparing total bills against day for each sex and time**: - Create a faceted plot using `sns.catplot` with `kind=\\"swarm\\"`, plotting `total_bill` against `day`, using `sex` as the `hue` parameter, and faceting the plot by `time` of day. - Each facet should show the distribution of total bills grouped by day and differentiated by sex. The function should not return anything but should display the specified plots. Function Signature ```python def analyze_tips_data(): pass ``` Example Calling the function `analyze_tips_data()` will visualize the following plots sequentially: 1. A single swarm plot showing the distribution of total bills for each day, with different colors for different days. 2. An adjusted swarm plot that separates the data points for each sex within each day. 3. A faceted swarm plot that shows the distribution of total bills against day, differentiated by sex and faceted by time. Constraints - Ensure appropriate aesthetic settings (e.g., point size) for better clarity of plots. - Utilize seaborn\'s built-in tips dataset without modifying its existing structure. Note Ensure you describe each step with appropriate comments in the code to explain the implementation clearly.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_tips_data(): Load the tips dataset and create the required plots. # Load the tips dataset tips = sns.load_dataset(\'tips\') # Create the first swarm plot showing the distribution of total bills for each day plt.figure(figsize=(10, 6)) sns.swarmplot(x=\'day\', y=\'total_bill\', data=tips, palette=\'muted\') plt.title(\'Swarm Plot: Total Bill Distribution by Day\') plt.show() # Create the second swarm plot with dodge=True to separate by sex plt.figure(figsize=(10, 6)) sns.swarmplot(x=\'day\', y=\'total_bill\', hue=\'sex\', data=tips, dodge=True, palette=\'muted\') plt.title(\'Swarm Plot: Total Bill Distribution by Day and Sex\') plt.show() # Create the faceted swarm plot using catplot g = sns.catplot(x=\'day\', y=\'total_bill\', hue=\'sex\', col=\'time\', data=tips, kind=\'swarm\', height=5, aspect=1, palette=\'muted\') g.set_titles(\'Faceted Swarm Plot: Total Bill by Day and Sex, Faceted by Time\') plt.show()"},{"question":"# Python Coding Assessment: Advanced Logging with `syslog` Objective: You are required to implement a custom logging function using the `syslog` module. The function must demonstrate the ability to manage log options, set logging priorities, and handle message filtering based on priority levels. Problem Statement: Write a function `custom_syslog_action(log_setup: dict, messages: list) -> str` that performs the following tasks: 1. Configures the system logger using `log_setup` options. 2. Sends a list of messages to the system logger with specified priorities. 3. Returns a string summarizing the logging actions taken. Input: - `log_setup`: A dictionary containing the following keys: - `ident` (str): A string to prepend to every log message. - `logoption` (list): A list of log options (e.g., `[\\"LOG_PID\\", \\"LOG_CONS\\"]`). - `facility` (str): The default facility for messages (e.g., `LOG_MAIL`). - `messages`: A list of tuples, each containing: - `priority` (str): The logging priority (e.g., `LOG_ERR`). - `message` (str): The message to log. Output: - A string summarizing the logging actions in the format: ``` Logging started with ident=<ident>, logoption=<logoption>, facility=<facility>. Logged <number_of_messages> messages with priorities [<priorities_list>]. ``` Constraints: - You must handle any exceptions that may occur and ensure that `closelog()` is called at the end of the function. - The function should properly set the logging mask to ignore messages below a certain priority (e.g., `LOG_WARNING`). Example: ```python def custom_syslog_action(log_setup: dict, messages: list) -> str: import syslog ident = log_setup[\'ident\'] logoption = log_setup[\'logoption\'] facility = log_setup[\'facility\'] log_option_value = 0 for option in logoption: log_option_value |= getattr(syslog, option) # Open log with the specified parameters syslog.openlog(ident, log_option_value, getattr(syslog, facility)) # Set log masks to log only messages that are LOG_WARNING or higher priority syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_WARNING)) logged_priorities = [] try: for priority, message in messages: # Convert string priority to syslog constant pri_value = getattr(syslog, priority) syslog.syslog(pri_value, message) logged_priorities.append(priority) except Exception as e: return f\\"Logging failed due to: {str(e)}\\" finally: syslog.closelog() summary = (f\\"Logging started with ident={ident}, logoption={logoption}, facility={facility}. \\" f\\"Logged {len(messages)} messages with priorities {logged_priorities}.\\") return summary # Test Example log_setup = { \'ident\': \'TestIdent\', \'logoption\': [\'LOG_PID\', \'LOG_CONS\'], \'facility\': \'LOG_MAIL\', } messages = [ (\'LOG_ERR\', \'Error message\'), (\'LOG_INFO\', \'Information message\'), ] print(custom_syslog_action(log_setup, messages)) ``` Notes: 1. Ensure to use the constants provided by the `syslog` module. 2. Validate and convert strings from log_setup and messages into their respective constants before using them with the `syslog` module. 3. Implement proper error handling within your function. Assessment Expectations: - Accurate use of `syslog` functions and constants. - Proper validation and handling of inputs. - Correct implementation of logging priorities and filtering.","solution":"def custom_syslog_action(log_setup: dict, messages: list) -> str: import syslog ident = log_setup[\'ident\'] logoption = log_setup[\'logoption\'] facility = log_setup[\'facility\'] log_option_value = 0 for option in logoption: log_option_value |= getattr(syslog, option) # Open log with the specified parameters syslog.openlog(ident, log_option_value, getattr(syslog, facility)) # Set log masks to log only messages that are LOG_WARNING or higher priority syslog.setlogmask(syslog.LOG_UPTO(syslog.LOG_WARNING)) logged_priorities = [] try: for priority, message in messages: # Convert string priority to syslog constant pri_value = getattr(syslog, priority) syslog.syslog(pri_value, message) logged_priorities.append(priority) except Exception as e: return f\\"Logging failed due to: {str(e)}\\" finally: syslog.closelog() summary = (f\\"Logging started with ident={ident}, logoption={logoption}, facility={facility}. \\" f\\"Logged {len(messages)} messages with priorities {logged_priorities}.\\") return summary"},{"question":"You are provided with two datasets: `tips` and `glue`, as follows: - `tips`: This dataset contains information about restaurant tips, including the total bill, tips, day of the week, and gender of the person who paid the bill. - `glue`: This dataset contains information about model performance scores on various tasks, including the score, model type, task name, and year. Your task is to create a set of visualizations using the seaborn.objects module that incorporates different aspects of these datasets. Instructions: 1. **Basic Dot Plot:** Create a basic dot plot of the `tips` dataset showing `total_bill` on the x-axis and `tip` on the y-axis. Ensure that the dots have an edge color of white. 2. **Reducing Overplotting:** Create a dot plot using the `tips` dataset showing `total_bill` on the x-axis and `day` on the y-axis, with dots colored by `sex`. Use dodging and jittering to reduce overplotting. 3. **Faceted Dot Plot:** Using the `glue` dataset, create a faceted dot plot showing `Score` on the x-axis and `Model` on the y-axis, with facets based on `Task`. Limit the x-axis to the range (-5, 105). Customize the dots by mapping the `Year` to a color scale and using different markers for different `Encoder` values. 4. **Dot Plot with Error Bars:** Create a dot plot using the `tips` dataset showing `total_bill` on the x-axis and `day` on the y-axis. Add jitter to the y-axis and use the `Shift` parameter to slightly move the dots. Combine the dots with range markers to show standard error bars (`se`) with a multiplier of 2. Requirements and Constraints: - Use `seaborn.objects` for creating all plots. - The visualizations must be clear and correctly labeled. - The plots should handle overplotting issues appropriately. - Ensure that faceted plots are well-organized and easy to interpret. Expected Outputs: The final submission should include: - A script that generates the requested plots. - Each plot should be displayed in the notebook/script. Sample Code Block: ```python import seaborn.objects as so from seaborn import load_dataset # Load datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Basic Dot Plot p1 = so.Plot(tips, \\"total_bill\\", \\"tip\\").add(so.Dot(edgecolor=\\"w\\")) p1.show() # Reducing Overplotting p2 = ( so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(.2)) ) p2.show() # Faceted Dot Plot p3 = ( so.Plot(glue, \\"Score\\", \\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") ) p3.show() # Dot Plot with Error Bars p4 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=.2), so.Jitter(.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p4.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load datasets tips = load_dataset(\\"tips\\") glue = load_dataset(\\"glue\\") # Basic Dot Plot p1 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\").add(so.Dot(edgecolor=\\"w\\")) p1.show() # Reducing Overplotting p2 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\", color=\\"sex\\") .add(so.Dot(), so.Dodge(), so.Jitter(.2)) ) p2.show() # Faceted Dot Plot p3 = ( so.Plot(glue, x=\\"Score\\", y=\\"Model\\").facet(\\"Task\\", wrap=4).limit(x=(-5, 105)) .add(so.Dot(pointsize=6), color=\\"Year\\", marker=\\"Encoder\\") .scale(marker=[\\"o\\", \\"s\\"], color=\\"flare\\") ) p3.show() # Dot Plot with Error Bars p4 = ( so.Plot(tips, x=\\"total_bill\\", y=\\"day\\") .add(so.Dot(pointsize=3), so.Shift(y=.2), so.Jitter(.2)) .add(so.Dot(), so.Agg()) .add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) ) p4.show()"},{"question":"# Bytearray Operations You are tasked with writing a function that processes sequences of data stored in Python bytearrays. This function will receive two bytearrays, transform them, and return a result based on specific requirements. Function Specification **Function Name:** `process_bytearrays` **Input:** - `bytearray1` - A Python bytearray. - `bytearray2` - Another Python bytearray. **Output:** - A modified bytearray resulting from the following steps. **Steps:** 1. **Concatenate** `bytearray1` and `bytearray2` to create a new bytearray. 2. **Trim** the resulting bytearray to a maximum of 10 bytes. If the concatenated bytearray is longer than 10 bytes, only keep the first 10 bytes. 3. **Resize** the new bytearray to ensure a total size of exactly 10 bytes, padding with zeroes (0x00) if necessary. 4. Return the final modified bytearray. **Constraints:** - The input bytearrays can contain any byte values. - Do not use any external libraries other than the Python standard library. - You must use the functions and macros described in the provided documentation to perform the required operations. # Example ```python def process_bytearrays(bytearray1: bytearray, bytearray2: bytearray) -> bytearray: # Complete the function based on the above steps pass # Example usage b1 = bytearray(b\'Hello\') b2 = bytearray(b\'World!\') result = process_bytearrays(b1, b2) print(result) # Expected output: bytearray(b\'HelloWorld\') ``` Note: Please ensure your solution strictly follows the specifications and constraints provided.","solution":"def process_bytearrays(bytearray1, bytearray2): Process two bytearrays by concatenating them, trimming to a maximum of 10 bytes, and padding with zeroes to ensure a total size of exactly 10 bytes. # Step 1: Concatenate bytearray1 and bytearray2. concatenated = bytearray1 + bytearray2 # Step 2: Trim the concatenated bytearray to a maximum of 10 bytes. trimmed = concatenated[:10] # Step 3: Resize to ensure a total size of exactly 10 bytes, padding with zeroes if necessary. final_result = trimmed + bytearray(10 - len(trimmed)) return final_result"},{"question":"# Question: Advanced Clustermap Visualization using Seaborn You are tasked with creating a detailed clustermap visualization using Seaborn\'s `sns.clustermap` function. The goal of this exercise is to assess your ability to manipulate and visualize data using advanced features of the seaborn library. # Dataset Use the `iris` dataset, which can be loaded using `sns.load_dataset(\\"iris\\")`. # Requirements 1. **Data Preprocessing**: - Load the dataset. - Remove the `species` column from the dataset. - Create a dictionary to map the unique species values to distinct colors (`r`, `b`, and `g`). - Map these colors to the rows of the dataset based on the species, and store these row colors. 2. **Basic Clustermap**: - Create a basic clustermap visualization of the dataset without row clustering. 3. **Advanced Customization**: - Customize the clustermap to have a figure size of 10x7. - Disable clustering of rows but enable clustering of columns. - Add column colors to the clustermap based on the `species` in `r`, `b`, and `g`. - Use the `mako` colormap for the visualization. - Standardize the data within the columns. - Customize the location of the color bar to be at (0.95, 0.2, 0.03, 0.4). - Adjust the clustering parameters to use `correlation` as the metric and `average` as the method. # Input and Output Format - **Input**: None (load data within the function). - **Output**: The function should display the clustermap. # Constraints - You must use seaborn to handle all visualizations. - Ensure that the figure is saved with the filename `clustermap_output.png`. # Function Signature ```python def advanced_clustermap_visualization(): # Load the dataset # Preprocess the data # Create the clustermap # Save the figure with the filename `clustermap_output.png` pass ``` Example Usage ```python advanced_clustermap_visualization() ``` Upon running the function, a clustermap should be displayed and saved to `clustermap_output.png` with all the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def advanced_clustermap_visualization(): # Load the dataset iris = sns.load_dataset(\\"iris\\") # Remove the species column from the dataset for clustermap input species = iris.pop(\\"species\\") # Create a dictionary to map the unique species values to distinct colors species_to_color = {\\"setosa\\": \\"r\\", \\"versicolor\\": \\"b\\", \\"virginica\\": \\"g\\"} # Map colors to the rows of the dataset based on the species row_colors = species.map(species_to_color) # Create a clustermap with the specified customizations clustermap = sns.clustermap( iris, figsize=(10, 7), row_cluster=False, col_cluster=True, row_colors=row_colors, cmap=\\"mako\\", standard_scale=1, col_colors=row_colors, metric=\\"correlation\\", method=\\"average\\" ) # Customize the color bar location clustermap.cax.set_position([0.95, 0.2, 0.03, 0.4]) # Save the figure plt.savefig(\\"clustermap_output.png\\") # Display the plot plt.show()"},{"question":"Objective You need to write a function in Python that handles bytearray objects by performing several operations, such as creating from a given string, concatenating multiple bytearrays, and resizing them under certain conditions. Function Signature ```python def process_bytearray_operations(strings: List[str], new_size: int) -> Tuple[bytearray, int]: pass ``` Input - `strings`: A list of strings where each string needs to be converted into a separate `bytearray` and concatenated. - `new_size`: An integer representing the desired size to which the final concatenated bytearray should be resized. Output - A tuple containing: 1. The final concatenated and resized `bytearray` object. 2. The size of this `bytearray` after resizing. Constraints - The length of each string in the `strings` list will be at most 1000 characters. - The `new_size` will be a non-negative integer and will not exceed the total accumulated length of the concatenated bytearrays by more than 100. # Steps 1. Convert each string in the `strings` list into a `bytearray` object. 2. Concatenate all the created `bytearray` objects into a single `bytearray`. 3. Resize the final concatenated `bytearray` to the specified `new_size`. 4. Return the resized `bytearray` and its new size. Example ```python strings = [\\"hello\\", \\" \\", \\"world\\"] new_size = 10 # Expected Output # (bytearray(b\'hello worl\'), 10) print(process_bytearray_operations(strings, new_size)) ``` In this example: - Convert \\"hello\\", \\" \\", and \\"world\\" to bytearrays and concatenate them to form `bytearray(b\'hello world\')`. - Resize the resulting bytearray to the size of `10`. - Return the resized bytearray and its new size. Notes - Emphasize using bytearray operations efficiently. - Handle possible edge cases such as resizing to a smaller or equal length than the original concatenated bytearray.","solution":"from typing import List, Tuple def process_bytearray_operations(strings: List[str], new_size: int) -> Tuple[bytearray, int]: Converts a list of strings to bytearrays, concatenates them, and resizes the final bytearray. Params: strings - List of string elements to be converted into bytearrays and concatenated new_size - Integer representing the desired size of the final bytearray Returns: A tuple containing the final concatenated and resized bytearray, and the size of this bytearray # Step 1: Convert each string in the list to a bytearray and concatenate them. concatenated_bytearray = bytearray() for string in strings: concatenated_bytearray += bytearray(string, \'utf-8\') # Step 2: Resize the concatenated bytearray to the specified new_size. final_bytearray = concatenated_bytearray[:new_size] # Step 3: Return the final concatenated and resized bytearray and its size. return final_bytearray, len(final_bytearray)"},{"question":"# Challenge: Log Analysis with Specialized Containers Objective: You are to design a log analysis tool that processes a stream of log entries from various sources (e.g., user activities, errors, and debug messages). Implement a class `LogAnalyzer` that uses specialized containers from the `collections` module to handle, store, and analyze the logs efficiently. Requirements: 1. **Log Entry Processing:** - Each log entry is a tuple containing `(timestamp, level, message)`: - `timestamp`: An integer representing the time at which the log entry was generated. - `level`: A string indicating the severity level of the log (`\\"INFO\\"`, `\\"DEBUG\\"`, `\\"ERROR\\"`). - `message`: A string containing the log message. 2. **Storing the Logs:** - Use a `namedtuple` named `LogEntry` to represent each log entry. - Use a `deque` to store the log entries to maintain the order of arrival, with a maximum size of 1000 entries. 3. **Log Counting:** - Use a `Counter` to count the number of log entries of each level. 4. **Aggregated View of Logs:** - Use an `OrderedDict` to maintain an ordered count of log entries by level. 5. **Log Analysis Methods:** - Implement the following methods in the `LogAnalyzer` class: - `add_log(self, timestamp, level, message)`: Adds a new log entry to the log storage. - `get_log_counts(self) -> dict`: Returns a dictionary with the count of log entries for each level. - `get_recent_logs(self, n: int) -> list`: Returns the `n` most recent log entries as a list. - `aggregate_logs(self) -> OrderedDict`: Returns an ordered dictionary with levels as keys and their respective counts as ordered values. Constraints: - You should handle the addition of logs efficiently, ensuring that the size of the deque does not exceed 1000 entries. - The `OrderedDict` should be updated in the `add_log` method to reflect the current count of logs ordered by their arrival. # Example Usage ```python from collections import deque, namedtuple, Counter, OrderedDict class LogAnalyzer: def __init__(self): LogEntry = namedtuple(\'LogEntry\', [\'timestamp\', \'level\', \'message\']) self.logs = deque(maxlen=1000) self.counter = Counter() self.log_order = OrderedDict() def add_log(self, timestamp, level, message): # Code to add log entry def get_log_counts(self): # Code to return log counts def get_recent_logs(self, n): # Code to return recent logs def aggregate_logs(self): # Code to return ordered log counts # Example of usage: la = LogAnalyzer() la.add_log(1, \'INFO\', \'System started.\') la.add_log(2, \'DEBUG\', \'Debugging mode ON.\') la.add_log(3, \'ERROR\', \'Error encountered!\') counts = la.get_log_counts() # {\'INFO\': 1, \'DEBUG\': 1, \'ERROR\': 1} recent_logs = la.get_recent_logs(2) # [(2, \'DEBUG\', \'Debugging mode ON.\'), (3, \'ERROR\', \'Error encountered!\')] aggregated = la.aggregate_logs() # OrderedDict([(\'INFO\', 1), (\'DEBUG\', 1), (\'ERROR\', 1)]) ``` Ensure your solution adheres to the requirements and correctly implements the required functionality.","solution":"from collections import deque, namedtuple, Counter, OrderedDict class LogAnalyzer: def __init__(self): self.LogEntry = namedtuple(\'LogEntry\', [\'timestamp\', \'level\', \'message\']) self.logs = deque(maxlen=1000) self.counter = Counter() self.log_order = OrderedDict() def add_log(self, timestamp, level, message): log_entry = self.LogEntry(timestamp, level, message) if len(self.logs) == self.logs.maxlen and self.logs[0].level in self.log_order: # If deque is full, remove the count from OrderedDict self.log_order[self.logs[0].level] -= 1 self.logs.append(log_entry) self.counter[level] += 1 if level not in self.log_order: self.log_order[level] = 1 else: if len(self.logs) <= self.logs.maxlen: self.log_order[level] += 1 def get_log_counts(self): return dict(self.counter) def get_recent_logs(self, n): return list(self.logs)[-n:] def aggregate_logs(self): return OrderedDict((level, count) for level, count in self.log_order.items() if count > 0)"},{"question":"Objective Design and implement a Python function using the scikit-learn library to perform customer segmentation on a given dataset. This challenge will test your understanding of clustering, dimensionality reduction, and outlier detection techniques provided by scikit-learn. Problem Statement You are tasked with analyzing a dataset of customer transactions to identify different customer segments, visualize these segments in a two-dimensional space, and identify potential outliers within each segment. Input - A pandas DataFrame `df` with the following columns: - `customer_id` (integer): Unique identifier for each customer. - `feature_1`, `feature_2`, ..., `feature_n` (float): Various features representing customer transactions. Output - A dictionary with the following keys: - `clusters` (pandas DataFrame): Original DataFrame with an additional column `cluster` indicating the cluster assignment for each customer. - `outliers` (pandas DataFrame): Original DataFrame filtered to only include outliers with an additional `cluster` column indicating to which cluster the outlier was initially assigned. - `2d_plot` (Matplotlib Figure): A matplotlib figure object showing the 2D visualization of customer segments. Requirements 1. Implement K-Means clustering on the input dataset to identify customer segments. 2. Apply Principal Component Analysis (PCA) to reduce the dimensionality of the data for 2D visualization. 3. Use an appropriate outlier detection method to identify outliers in the dataset and label them. 4. Return a dictionary with the required entries. Constraints - Use the scikit-learn library for all machine learning tasks. - Assume the number of clusters `k` is 3. - Assume the data is preprocessed and clean. Function Signature ```python import pandas as pd def customer_segmentation(df: pd.DataFrame) -> dict: pass ``` Example ```python import pandas as pd # Dummy data - Assume this is the real data with many more rows data = { \'customer_id\': [1, 2, 3, 4, 5], \'feature_1\': [10.0, 15.5, 10.1, 14.8, 10.5], \'feature_2\': [20.1, 25.4, 19.9, 20.0, 20.2], \'feature_3\': [30.0, 29.9, 30.1, 25.4, 30.0] } df = pd.DataFrame(data) result = customer_segmentation(df) print(result[\'clusters\']) print(result[\'outliers\']) result[\'2d_plot\'].show() ``` # Evaluation Criteria 1. **Correctness**: The function should correctly implement the clustering, dimensionality reduction, and outlier detection. 2. **Performance**: The function should be efficient and able to handle large datasets. 3. **Code Quality**: The code should be well-organized, readable, and follow Python best practices. 4. **Visualization**: The 2D plot should clearly reflect the clusters and highlight outliers.","solution":"import pandas as pd from sklearn.cluster import KMeans from sklearn.decomposition import PCA from sklearn.neighbors import LocalOutlierFactor import matplotlib.pyplot as plt def customer_segmentation(df: pd.DataFrame) -> dict: # Extract features only for clustering features = df.drop(columns=\'customer_id\') # Apply K-Means clustering kmeans = KMeans(n_clusters=3, random_state=42) df[\'cluster\'] = kmeans.fit_predict(features) # Apply PCA for 2D visualization pca = PCA(n_components=2) components = pca.fit_transform(features) df[\'pca_1\'] = components[:, 0] df[\'pca_2\'] = components[:, 1] # Outlier detection using LocalOutlierFactor lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1) df[\'outlier\'] = lof.fit_predict(features) # Separate outliers outliers = df[df[\'outlier\'] == -1].drop(columns=\'outlier\') # Plot the 2D visualization plt.figure(figsize=(10, 6)) for cluster in df[\'cluster\'].unique(): subset = df[df[\'cluster\'] == cluster] plt.scatter(subset[\'pca_1\'], subset[\'pca_2\'], label=f\'Cluster {cluster}\') # Highlight outliers plt.scatter(outliers[\'pca_1\'], outliers[\'pca_2\'], color=\'red\', label=\'Outliers\', edgecolors=\'black\') plt.title(\'Customer Segmentation\') plt.xlabel(\'PCA Component 1\') plt.ylabel(\'PCA Component 2\') plt.legend() fig = plt.gcf() return { \'clusters\': df.drop(columns=[\'pca_1\', \'pca_2\', \'outlier\']), \'outliers\': outliers, \'2d_plot\': fig }"},{"question":"**Objective**: Implement a Python function to manage and test warnings using the `warnings` module. **Problem Statement**: You are required to create a function `custom_warning_management` that demonstrates the usage of the `warnings` module. This function should implement the following requirements: 1. Issue a `DeprecationWarning` using the `warn()` function. 2. Configure the warning filter to: - Ignore all `ImportWarning` warnings. - Convert `ResourceWarning` warnings into exceptions. 3. Utilize the `catch_warnings` context manager to: - Suppress `UserWarning` warnings within the context. - Verify that a `DeprecationWarning` can still be captured within this context. - Restore the original warnings filter after the context block. **Function Signature**: ```python def custom_warning_management() -> List[str]: pass ``` **Input**: - No input parameters. **Output**: - Return a list of captured warning messages as strings that contain \\"DeprecationWarning\\" and its message. **Constraints**: - The function should not raise any exceptions unless it\'s deliberately converted from a warning. - Use appropriate categories for warnings as defined in the `warnings` module. **Example**: ```python def custom_warning_management() -> List[str]: import warnings warning_messages = [] # Step 1: Issue a DeprecationWarning warnings.warn(\\"This feature is deprecated!\\", DeprecationWarning) # Step 2: Configure warning filters warnings.filterwarnings(\\"ignore\\", category=ImportWarning) warnings.filterwarnings(\\"error\\", category=ResourceWarning) # Step 3: Use catch_warnings to suppress UserWarning and verify DeprecationWarning with warnings.catch_warnings(record=True) as captured_warnings: warnings.simplefilter(\\"ignore\\", category=UserWarning) warnings.warn(\\"This is a user warning!\\", UserWarning) warnings.warn(\\"This feature is deprecated!\\", DeprecationWarning) for warning in captured_warnings: if isinstance(warning.message, DeprecationWarning): warning_messages.append(str(warning.message)) return warning_messages # Example Execution messages = custom_warning_management() print(messages) # Expected output: [\\"This feature is deprecated!\\"] ``` **Notes**: - You may use additional helper functions if needed. - Make sure to handle the warnings correctly within the given constraints.","solution":"import warnings def custom_warning_management(): warning_messages = [] # Step 1: Issue a DeprecationWarning warnings.warn(\\"This feature is deprecated!\\", DeprecationWarning) # Step 2: Configure warning filters warnings.filterwarnings(\\"ignore\\", category=ImportWarning) warnings.filterwarnings(\\"error\\", category=ResourceWarning) # Step 3: Use catch_warnings to suppress UserWarning and verify DeprecationWarning with warnings.catch_warnings(record=True) as captured_warnings: warnings.simplefilter(\\"ignore\\", category=UserWarning) warnings.warn(\\"This is a user warning!\\", UserWarning) warnings.warn(\\"This feature is deprecated!\\", DeprecationWarning) for warning in captured_warnings: if isinstance(warning.message, DeprecationWarning): warning_messages.append(str(warning.message)) return warning_messages"},{"question":"You are tasked with creating a function that encodes and decodes email headers containing various character sets. This function will take a list of tuples where each tuple contains a header field name and its corresponding value. Some values may include non-ASCII characters. Your goal is to: 1. Encode these header fields using MIME-compliant standards. 2. Flatten the encoded headers into a single string. 3. Decode the flattened string back to the individual header fields. # Requirements 1. Implement a function `process_email_headers(headers: List[Tuple[str, str]]) -> List[Tuple[str, str]]` that performs the following: - Encodes each header using the `Header` class from the `email.header` module. - Flattens the encoded headers into a single formatted string. - Decodes the formatted string back into individual header fields using the `decode_header` and `make_header` functions. # Input - A list of tuples. Each tuple contains two strings: the header field name and the corresponding header value. - Example: ```python headers = [ (\'Subject\', \'pxf6stal\'), (\'From\', \'no-reply@example.com\'), (\'To\', \'recipient@example.com\') ] ``` # Output - A list of tuples. Each tuple should contain two strings: the header field name and the corresponding decoded header value. - Example: ```python [ (\'Subject\', \'pxf6stal\'), (\'From\', \'no-reply@example.com\'), (\'To\', \'recipient@example.com\') ] ``` # Constraints - The input header values may contain characters from various character sets. - The encoded and decoded values must be compliant with RFC 2047 and RFC 2822 standards. # Example ```python def process_email_headers(headers: List[Tuple[str, str]]) -> List[Tuple[str, str]]: from email.header import Header, decode_header, make_header from email.message import Message # Encode the headers using Header class msg = Message() for field, value in headers: hdr = Header(value, \'iso-8859-1\' if \'pxf6stal\' in value else \'utf-8\') msg[field] = hdr # Flatten the encoded headers into a single string flat_string = msg.as_string() # Decode the flattened string back to individual header fields decoded_headers = [] for field, value in headers: decoded_value = decode_header(msg[field])[0][0] decoded_headers.append((field, decoded_value.decode(\'iso-8859-1\') if isinstance(decoded_value, bytes) else decoded_value)) return decoded_headers # Example usage headers = [ (\'Subject\', \'pxf6stal\'), (\'From\', \'no-reply@example.com\'), (\'To\', \'recipient@example.com\') ] print(process_email_headers(headers)) # Output should be: [(\'Subject\', \'pxf6stal\'), (\'From\', \'no-reply@example.com\'), (\'To\', \'recipient@example.com\')] ``` # Notes - Ensure that the function handles various character sets correctly. - Use appropriate methods and classes from the `email.header` module for encoding and decoding. - Pay attention to how the headers are encoded to comply with internet standards.","solution":"def process_email_headers(headers): from email.header import Header, decode_header, make_header from email.message import Message # Encode the headers msg = Message() for field, value in headers: hdr = Header(value, \'utf-8\') msg[field] = hdr # Flatten the encoded headers into a single string flat_string = msg.as_string() # Decode the flattened string back to individual header fields decoded_headers = [] for field, value in msg.items(): decoded_value = str(make_header(decode_header(value))) decoded_headers.append((field, decoded_value)) return decoded_headers"},{"question":"# Advanced Python Logging Handler Implementation You are required to create a custom logging handler that combines features of `StreamHandler` and `RotatingFileHandler`. This custom handler will log messages to both a stream (like `sys.stdout`) and a rotating file, ensuring that log rotation occurs based on file size. Task: 1. Implement a class `CombinedLoggingHandler` that: - Inherits from `logging.Handler`. - Uses both `StreamHandler` and `RotatingFileHandler` internally. - Logs messages to both the stream and the rotating file. 2. The constructor of `CombinedLoggingHandler` should take the following parameters: - `stream`: The stream to which logging messages will be output (default is `sys.stdout`). - `filename`: The name of the file to which logs will be written. - `maxBytes`: The maximum file size before rotation occurs (default is 0, meaning no rotation). - `backupCount`: The number of backup files to keep (default is 0, meaning no backup files). 3. Implement the `emit` method to handle the actual logging mechanism. 4. Ensure proper initialization and cleanup of resources, including closing opened files and flushing streams. Example Usage: ```python import logging # Custom Combined Logging Handler class CombinedLoggingHandler(logging.Handler): def __init__(self, stream=None, filename=None, maxBytes=0, backupCount=0): super().__init__() self.stream_handler = logging.StreamHandler(stream) self.file_handler = logging.handlers.RotatingFileHandler( filename, maxBytes=maxBytes, backupCount=backupCount) def emit(self, record): self.stream_handler.emit(record) self.file_handler.emit(record) def setFormatter(self, fmt): self.stream_handler.setFormatter(fmt) self.file_handler.setFormatter(fmt) def close(self): self.stream_handler.close() self.file_handler.close() super().close() # Example usage logger = logging.getLogger(\'MyLogger\') combined_handler = CombinedLoggingHandler( stream=sys.stdout, filename=\'app.log\', maxBytes=1024, backupCount=3 ) formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') combined_handler.setFormatter(formatter) logger.addHandler(combined_handler) logger.setLevel(logging.DEBUG) # Logging some messages logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.error(\'This is an error message\') ``` Constraints: - The log file should rotate when its size exceeds `maxBytes`. - Up to `backupCount` backup files should be maintained. - The stream output should be flushed after each message. Evaluation Criteria: - Correct implementation of the `CombinedLoggingHandler` class. - Proper handling of log rotation. - Clean up and resource management in the `close` method. - Adherence to the provided specifications in the example usage.","solution":"import logging import sys from logging import handlers class CombinedLoggingHandler(logging.Handler): def __init__(self, stream=None, filename=None, maxBytes=0, backupCount=0): super().__init__() if stream is None: stream = sys.stdout self.stream_handler = logging.StreamHandler(stream) self.file_handler = handlers.RotatingFileHandler( filename, maxBytes=maxBytes, backupCount=backupCount) def emit(self, record): self.stream_handler.emit(record) self.file_handler.emit(record) def setFormatter(self, fmt): self.stream_handler.setFormatter(fmt) self.file_handler.setFormatter(fmt) def close(self): self.stream_handler.close() self.file_handler.close() super().close() # Example usage (to be run in practice) # logger = logging.getLogger(\'MyLogger\') # combined_handler = CombinedLoggingHandler( # stream=sys.stdout, # filename=\'app.log\', # maxBytes=1024, # backupCount=3 # ) # # formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') # combined_handler.setFormatter(formatter) # logger.addHandler(combined_handler) # logger.setLevel(logging.DEBUG) # # # Logging some messages # logger.debug(\'This is a debug message\') # logger.info(\'This is an info message\') # logger.error(\'This is an error message\')"},{"question":"<|Analysis Begin|> The \\"shutil\\" module provides high-level operations on files and collections of files. It includes functions to copy files and directories, remove entire directories, and handle symbolic links. The module also interacts with the os module for permissions and other metadata-related operations. Functions like `shutil.copy()`, `shutil.copy2()`, and `shutil.copytree()` provide copying capabilities while maintaining or bypassing metadata. The functions also include exception handling for specific use-cases like symbolic links (`shutil.copymode()`, `shutil.copystat()`, `shutil.copytree()`). Additionally, shutil supports operations like moving files (`shutil.move()`), checking disk usage (`shutil.disk_usage()`), and creating and unpacking archives (`shutil.make_archive()`, `shutil.unpack_archive()`). Potential areas from which a challenging question can be designed: 1. Recursive operations on directories (e.g., `copytree`, `rmtree`). 2. Handling of symbolic links and permissions. <|Analysis End|> <|Question Begin|> **Question: Implement a Recursive Directory Copier with Ignore Patterns** # Objective: Create a function `recursive_copy_with_ignore(src, dst, ignore_patterns=None)` that copies an entire directory tree from `src` to `dst`, while ignoring files and directories that match any of the patterns specified in `ignore_patterns`. # Requirements: 1. **Function Specification**: - `recursive_copy_with_ignore(src: str, dst: str, ignore_patterns: Optional[List[str]] = None) -> None` - Parameters: - `src`: Source directory path (string). This directory and all its contents need to be copied. - `dst`: Destination directory path (string). This directory will be created if it does not exist. - `ignore_patterns`: Optional list of glob-style patterns (list of strings). Any files or directories matching these patterns should not be copied. - Returns: This function should not return anything. 2. **Constraints**: - `src` must be a valid directory path. - `dst` must not already exist. If it does, raise a `FileExistsError`. - The function should maintain the original directory structure. - The function should handle symbolic links appropriately (i.e., copy them as symbolic links if possible). 3. **Performance Requirements**: - Efficient handling of large directories is expected. - Avoid excessive memory usage by copying data in chunks when possible. 4. **Error Handling**: - If an error occurs during copying, raise a `shutil.Error` with a list of tuples containing the source path, destination path, and the exception raised. 5. **Follow these additional specifications**: - Use `shutil.copy2` to copy individual files. - Utilize `shutil.ignore_patterns` to handle the `ignore_patterns` functionality. # Example Usage: ```python import shutil def recursive_copy_with_ignore(src, dst, ignore_patterns=None): # Your implementation here # Example usage: try: recursive_copy_with_ignore(\'path/to/source\', \'path/to/destination\', ignore_patterns=[\'*.pyc\', \'tmp*\']) except FileExistsError: print(\'Destination already exists\') except shutil.Error as e: print(f\'Error occurred: {e}\') ``` # Notes: - Thoroughly test the function with various directory structures and different ignore patterns. - Consider edge cases like symbolic links, nested directories, and files with different types of patterns matching. - Ensure `dst` is not created if `recursive_copy_with_ignore` raises an error.","solution":"import os import shutil from typing import List, Optional def recursive_copy_with_ignore(src: str, dst: str, ignore_patterns: Optional[List[str]] = None) -> None: if not os.path.isdir(src): raise ValueError(f\\"Source \'{src}\' is not a valid directory.\\") if os.path.exists(dst): raise FileExistsError(f\\"Destination \'{dst}\' already exists.\\") def ignore_func(directory, filenames): if ignore_patterns: patterns = shutil.ignore_patterns(*ignore_patterns) return patterns(directory, filenames) return [] try: shutil.copytree(src, dst, ignore=ignore_func, symlinks=True, copy_function=shutil.copy2) except Exception as e: raise shutil.Error(f\\"An error occurred while copying: {e}\\") from e"},{"question":"Objective: Write a Python function that compares the execution times of different approaches for solving a problem using the `timeit` module. Problem Description: You are given a list of integers. Your task is to write three different functions to find the sum of all integers in the list. Use the `timeit` module to compare the execution times of these functions over multiple repetitions. Your functions should include: 1. A for-loop to iterate through the list and calculate the sum. 2. The `sum` built-in function. 3. A list comprehension to generate a new list and then use the `sum` function on it. Function Signature: ```python def compare_sum_methods(numbers: list[int], repetitions: int) -> dict: pass ``` Input: - `numbers` (list of int): A list of integers. - `repetitions` (int): The number of repetitions for timing each method. Output: - Returns a dictionary with method names as keys and their average execution time over the repetitions as values. Constraints: - The list of integers can have up to 1,000,000 elements. - The repetitions value will be a positive integer (up to 10). Example: ```python numbers = [i for i in range(1000000)] repetitions = 5 print(compare_sum_methods(numbers, repetitions)) ``` Example output might be: ```python { \\"for_loop\\": 0.123456789, \\"built_in_sum\\": 0.023456789, \\"list_comprehension_sum\\": 0.045678912 } ``` Notes: - Your function should account for timing each method using the `timeit` module and return the average execution time over the given repetitions. - Make sure your implementation handles large lists efficiently and avoids modifying the input list.","solution":"import timeit def sum_by_for_loop(numbers): total = 0 for num in numbers: total += num return total def sum_by_built_in(numbers): return sum(numbers) def sum_by_list_comprehension(numbers): return sum([num for num in numbers]) def compare_sum_methods(numbers: list[int], repetitions: int) -> dict: time_for_loop = timeit.timeit(lambda: sum_by_for_loop(numbers), number=repetitions) time_built_in = timeit.timeit(lambda: sum_by_built_in(numbers), number=repetitions) time_list_comprehension = timeit.timeit(lambda: sum_by_list_comprehension(numbers), number=repetitions) return { \\"for_loop\\": time_for_loop / repetitions, \\"built_in_sum\\": time_built_in / repetitions, \\"list_comprehension_sum\\": time_list_comprehension / repetitions }"},{"question":"As a data scientist, you are tasked to visualize data using seaborn\'s `objects` interface. You need to create a function to generate jittered scatter plots based on different configurations. Question Write a function `create_jittered_scatter_plot` that takes in a dataset and three parameters: `x`, `y`, and `jitter_config`. The function should: 1. Generate a scatter plot with jitter applied. 2. Parameters `x` and `y` specify the columns used for the x-axis and y-axis, respectively. 3. `jitter_config` is a dictionary that can control the jitter amount in terms of width or specific x and y values. Specifically, `jitter_config` can have the following keys: - `\\"width\\"`: a float that applies jitter width relative to the spacing between marks. - `\\"x\\"`: an integer specifying jitter along the x-axis in data units. - `\\"y\\"`: an integer specifying jitter along the y-axis in data units. Handle the `jitter_config` such that if both `x` and `y` are specified, they override the `width` parameter. Constraints - All parameter names are case-sensitive. - Assume the input dataset is a Pandas DataFrame and the specified columns (`x` and `y`) always exist in the dataset. Input - `data`: `DataFrame` - The dataset for the plot. - `x`: `str` - Column name for the x-axis. - `y`: `str` - Column name for the y-axis. - `jitter_config`: `dict` - Dictionary with jitter configuration having keys `\\"width\\"`, `\\"x\\"` and/or `\\"y\\"`. Output - The function should display a jittered scatter plot using seaborn\'s `objects` interface. Example ```python import seaborn.objects as so from seaborn import load_dataset # Example dataset penguins = load_dataset(\\"penguins\\") # Function to create jittered scatter plot def create_jittered_scatter_plot(data, x, y, jitter_config): plot = so.Plot(data, x, y) if \'x\' in jitter_config or \'y\' in jitter_config: plot = plot.add(so.Dots(), so.Jitter(x=jitter_config.get(\'x\', 0), y=jitter_config.get(\'y\', 0))) elif \'width\' in jitter_config: plot = plot.add(so.Dots(), so.Jitter(jitter_config[\'width\'])) else: plot = plot.add(so.Dots(), so.Jitter()) plot.show() # Calling function with specific jitter configuration create_jittered_scatter_plot(penguins, \\"species\\", \\"body_mass_g\\", {\\"width\\": 0.5}) create_jittered_scatter_plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm\\", {\\"x\\": 100, \\"y\\": 5}) ```","solution":"import pandas as pd import seaborn.objects as so def create_jittered_scatter_plot(data, x, y, jitter_config): Generates a jittered scatter plot based on the input data and jitter configuration. Parameters: - data (DataFrame): The dataset for the plot. - x (str): Column name for the x-axis. - y (str): Column name for the y-axis. - jitter_config (dict): Dictionary with jitter configuration having keys \\"width\\", \\"x\\", and/or \\"y\\". plot = so.Plot(data, x, y) if \'x\' in jitter_config or \'y\' in jitter_config: plot = plot.add(so.Dots(), so.Jitter(x=jitter_config.get(\'x\', 0), y=jitter_config.get(\'y\', 0))) elif \'width\' in jitter_config: plot = plot.add(so.Dots(), so.Jitter(jitter_config[\'width\'])) else: plot = plot.add(so.Dots(), so.Jitter()) plot.show()"},{"question":"# Advanced Python Coding Assessment Objective You are to demonstrate your understanding of handling MIME type configurations using the deprecated `mailcap` module in Python. The task will involve reading and interpreting mailcap files, then using this information to process MIME types. Task You need to implement a function `mime_command(filename: str, mimetype: str, action: str) -> str` that reads mailcap entries, determines the appropriate command for a given MIME type and action, and applies the specified substitutions. Function Signature ```python def mime_command(filename: str, mimetype: str, action: str) -> str: pass ``` Input and Output - **Input:** - `filename` (str): The filename to be used for the `%s` substitution in the command. - `mimetype` (str): The MIME type of the file to be processed. - `action` (str): The action to be performed, which corresponds to the `key` parameter in `findmatch`. - **Output:** - Return a string containing the command to be executed based on the mailcap configuration. If no command is found for the given MIME type and action, return \\"No suitable command found\\". Constraints - Handle cases where no suitable command is found. - Ensure substitutions are performed correctly. - Handle potential security concerns outlined in the version change notes of `findmatch`. Example Usage ```python import mailcap def mime_command(filename: str, mimetype: str, action: str) -> str: caps = mailcap.getcaps() command, entry = mailcap.findmatch(caps, mimetype, key=action, filename=filename) if command is None: return \\"No suitable command found\\" return command # Example usage filename = \'example_video.mpeg\' mimetype = \'video/mpeg\' action = \'view\' print(mime_command(filename, mimetype, action)) # Expected output: a suitable command for viewing `video/mpeg`, e.g., \'xmpeg example_video.mpeg\' ``` You will be assessed based on the correctness, completeness, and efficiency of your implementation. Proper handling of edge cases and adherence to security concerns are crucial for full credit.","solution":"import mailcap def mime_command(filename: str, mimetype: str, action: str) -> str: Determines the appropriate command for a given MIME type and action based on mailcap entries. Parameters: filename (str): The filename to be used for the `%s` substitution in the command. mimetype (str): The MIME type of the file to be processed. action (str): The action to be performed, corresponding to the `key` parameter in `findmatch`. Returns: str: A string containing the command to be executed based on the mailcap configuration. If no command is found for the given MIME type and action, returns \\"No suitable command found\\". caps = mailcap.getcaps() command, entry = mailcap.findmatch(caps, mimetype, key=action, filename=filename) if command is None: return \\"No suitable command found\\" return command"},{"question":"**Objective:** To assess the student\'s ability to utilize the `syslog` module for logging messages with different levels of severity and configure the logging settings. **Task:** Implement a function `log_messages(messages, ident=None, logoption=0, facility=syslog.LOG_USER)` that: 1. Configures the syslog logger using the `ident`, `logoption`, and `facility` parameters with `syslog.openlog()`. 2. Logs a series of messages with specific priorities. 3. Ensures that the log will include the Process ID and logs the messages to a user-specified facility. 4. After logging, the logger should be properly closed using `syslog.closelog()`. **Function Signature:** ```python import syslog def log_messages(messages, ident=None, logoption=0, facility=syslog.LOG_USER): pass ``` **Parameters:** - `messages`: A list of tuples, each containing a priority level (integer type) and a message (string type). Example: `[(syslog.LOG_INFO, \\"Informational message\\"), (syslog.LOG_ERR, \\"Error occurred\\"), ...]` - `ident`: An optional string identifier to be prefixed to each logged message. Default is `None`. - `logoption`: An optional integer representing log options using bitwise OR of constants such as `syslog.LOG_PID`. Default is `0`. - `facility`: An optional integer representing the logging facility, defaulting to `syslog.LOG_USER`. **Constraints:** - The function should handle any number of messages. - The function should ensure logical consistency and correctness in configuring, logging, and closing the logger. **Example:** Here is an example call to the function: ```python messages = [ (syslog.LOG_INFO, \\"Informational message\\"), (syslog.LOG_ERR, \\"Error occurred\\"), (syslog.LOG_DEBUG, \\"Debugging info\\"), (syslog.LOG_WARNING, \\"Warning message\\") ] log_messages(messages, ident=\\"TestApp\\", logoption=syslog.LOG_PID, facility=syslog.LOG_LOCAL0) ``` **Expected Behavior:** - The function should configure the logger to use the ident `TestApp`, include the process ID in the logs, and use the `LOG_LOCAL0` facility. - It should then log each of the provided messages with their respective priority levels. - After logging, the function should ensure that the logger is appropriately closed. **Notes:** - Be sure to handle any trailing newlines as stated in the `syslog` documentation. - Ensure to test your function in an environment that supports Unix system `syslog`.","solution":"import syslog def log_messages(messages, ident=None, logoption=0, facility=syslog.LOG_USER): Configures the syslog logger and logs a series of messages. :param messages: List of tuples, each containing a priority level (int) and message (str). :param ident: Optional identifier string prefixed to each message. :param logoption: Optional log options specified using bitwise OR of syslog constants. :param facility: Optional log facility, defaults to syslog.LOG_USER. syslog.openlog(ident=ident, logoption=logoption | syslog.LOG_PID, facility=facility) for priority, message in messages: syslog.syslog(priority, message) syslog.closelog()"},{"question":"# Problem: Advanced Data Visualization with Seaborn You are given a dataset containing information about the scores of students in different exams. Your task is to visualize this data using seaborn, focusing on various types of error bars to showcase your understanding of the `errorbar` parameter. Dataset Create a synthetic dataset using the following specifications: 1. There are 5 exams, and each exam has scores for 100 students. 2. Each exam score is normally distributed with different means and standard deviations. Steps 1. **Create the Dataset**: - Simulate 5 exams scores using numpy with the following properties: - Exam 1: Mean = 70, Std = 10 - Exam 2: Mean = 75, Std = 12 - Exam 3: Mean = 65, Std = 8 - Exam 4: Mean = 85, Std = 15 - Exam 5: Mean = 90, Std = 5 - Store this data in a DataFrame with columns: \'Exam\' and \'Score\'. 2. **Plot with Standard Deviation Error Bars**: - Create a point plot displaying the mean score for each exam and include error bars representing the standard deviation. Set the appropriate color and style. 3. **Plot with Standard Error Bars**: - Create a point plot displaying the mean score for each exam, this time including error bars representing the standard error. Choose different colors and style from the previous plot. 4. **Plot with Confidence Interval Error Bars**: - Create a point plot with bootstrap confidence interval error bars (95%). Use different style settings. 5. **Custom Error Bars**: - Create a point plot where the error bars represent the minimum and maximum score for each exam. This requires you to pass a custom function to the `errorbar` parameter. 6. **Compare All in One Plot**: - Create a single plot that compares mean scores with error bars representing the standard deviation, standard error, and a confidence interval. Use different plotting styles for each and include a legend. Code Requirements - Your code should be modular with functions defined for generating the dataset, plotting each type of error bar, and the combined plot. - Use meaningful variable names and comments to explain each part. - Ensure the plots are well labeled, including titles, axis labels, and a legend for the combined plot. Evaluation Criteria - Correctness of the generated dataset. - Proper implementation and interpretation of each type of error bar. - Clear and accurate visualization. - Code readability, including function separation and commenting. Here is a starting template for your code structure: ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Create the Dataset def create_dataset(): np.random.seed(42) data = { \'Exam\': [], \'Score\': [] } exams = [\'Exam 1\', \'Exam 2\', \'Exam 3\', \'Exam 4\', \'Exam 5\'] means = [70, 75, 65, 85, 90] stds = [10, 12, 8, 15, 5] for exam, mean, std in zip(exams, means, stds): scores = np.random.normal(mean, std, 100) data[\'Exam\'] += [exam] * 100 data[\'Score\'] += list(scores) return pd.DataFrame(data) # Step 2-6: Define functions for each plotting task def plot_with_sd(df): # Implement the plotting with standard deviation error bars pass def plot_with_se(df): # Implement the plotting with standard error bars pass def plot_with_ci(df): # Implement the plotting with confidence interval error bars pass def plot_with_custom(df): # Implement the plotting with custom error bars (min-max) pass def plot_combined(df): # Implement the plotting comparing all three types of error bars in one plot pass # Main execution if __name__ == \\"__main__\\": df = create_dataset() plot_with_sd(df) plot_with_se(df) plot_with_ci(df) plot_with_custom(df) plot_combined(df) ``` Implement the functions and complete the visualization tasks accordingly.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Create the Dataset def create_dataset(): np.random.seed(42) data = { \'Exam\': [], \'Score\': [] } exams = [\'Exam 1\', \'Exam 2\', \'Exam 3\', \'Exam 4\', \'Exam 5\'] means = [70, 75, 65, 85, 90] stds = [10, 12, 8, 15, 5] for exam, mean, std in zip(exams, means, stds): scores = np.random.normal(mean, std, 100) data[\'Exam\'] += [exam] * 100 data[\'Score\'] += list(scores) return pd.DataFrame(data) # Step 2: Plot with Standard Deviation Error Bars def plot_with_sd(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Exam\', y=\'Score\', data=df, errorbar=\'sd\', capsize=0.2, color=\'blue\') plt.title(\'Mean of Exam Scores with Standard Deviation Error Bars\') plt.show() # Step 3: Plot with Standard Error Bars def plot_with_se(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Exam\', y=\'Score\', data=df, errorbar=(\'ci\', 68), capsize=0.2, color=\'green\') plt.title(\'Mean of Exam Scores with Standard Error Bars\') plt.show() # Step 4: Plot with Confidence Interval Error Bars def plot_with_ci(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Exam\', y=\'Score\', data=df, errorbar=(\'ci\', 95), capsize=0.2, color=\'red\') plt.title(\'Mean of Exam Scores with Confidence Interval (95%) Error Bars\') plt.show() # Step 5: Plot with Custom Error Bars (Min-Max) def plot_with_custom(df): def min_max(data, **kwargs): return data.min(), data.max() plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Exam\', y=\'Score\', data=df, errorbar=min_max, capsize=0.2, color=\'purple\') plt.title(\'Mean of Exam Scores with Min-Max Error Bars\') plt.show() # Step 6: Compare All in One Plot def plot_combined(df): plt.figure(figsize=(10, 6)) sns.pointplot(x=\'Exam\', y=\'Score\', data=df, errorbar=\'sd\', capsize=0.2, color=\'blue\', label=\'Standard Deviation\') sns.pointplot(x=\'Exam\', y=\'Score\', data=df, errorbar=(\'ci\', 68), capsize=0.2, color=\'green\', label=\'Standard Error\') sns.pointplot(x=\'Exam\', y=\'Score\', data=df, errorbar=(\'ci\', 95), capsize=0.2, color=\'red\', label=\'Confidence Interval (95%)\') plt.title(\'Comparison of Different Error Bars\') plt.legend(labels=[\'Standard Deviation\', \'Standard Error\', \'Confidence Interval (95%)\']) plt.show() # Main execution if __name__ == \\"__main__\\": df = create_dataset() plot_with_sd(df) plot_with_se(df) plot_with_ci(df) plot_with_custom(df) plot_combined(df)"},{"question":"Objective Demonstrate your understanding of closures in Python by implementing a function that tracks how many times another function is called and remembers this count across multiple calls. Problem Statement Create a function `make_counter` that returns a new function. This new function, when called, returns the number of times it has been called previously, incrementing the count each time. Implement this using closures to maintain the state. Input and Output Formats - **Function Signature**: `def make_counter() -> callable:` - **Input**: None directly, but the returned function will take no parameters. - **Output**: The returned function returns an integer representing the number of times it has been called. Constraints - You should use closures to maintain the counter state. - You must not use any global variables. - You should handle the updates to the count efficiently. Example ```python counter = make_counter() print(counter()) # Output: 0 print(counter()) # Output: 1 print(counter()) # Output: 2 ``` Explanation When `make_counter` is called, it returns a new function that has a count initialized to `0`. Each call to this inner function increments the count and returns the current count, maintaining state across calls via a closure. Performance Requirements The function should run in constant time relative to the number of calls. Each call to the resultant function should have O(1) complexity. # Implementation Notes 1. You should define the inner function within `make_counter` and use it to maintain the count using nonlocal variables. 2. You may use Python\'s scoping rules to ensure that the inner function retains access to the count variable.","solution":"def make_counter(): Returns a new function that counts the number of times it has been called. count = 0 def counter(): nonlocal count result = count count += 1 return result return counter"},{"question":"**Question:** You are working with pickle files in Python and need to develop tools to analyze and optimize these files. Using the `pickletools` module, write two functions: 1. `disassemble_pickle(pickle_data: bytes, annotate: bool = False) -> str`: - This function should take a pickle byte string and return the disassembled output as a string. - If `annotate` is set to `True`, the disassembled output should include annotations for each opcode. **Input:** ```python pickle_data: bytes # The pickle byte string to be disassembled. annotate: bool = False # Whether to annotate the output with opcode descriptions. ``` **Output:** ```python result: str # The disassembled representation of the pickle. ``` **Constraints:** - You should handle any valid pickle byte string. - Use the `pickletools.dis` function to perform the disassembly. 2. `optimize_pickle(pickle_data: bytes) -> bytes`: - This function should take a pickle byte string and return an optimized version of the pickle, which is shorter and more efficient. **Input:** ```python pickle_data: bytes # The pickle byte string to be optimized. ``` **Output:** ```python optimized_pickle: bytes # The optimized pickle byte string. ``` **Constraints:** - You should handle any valid pickle byte string. - Use the `pickletools.optimize` function to perform the optimization. **Example:** ```python import pickle from pickletools import optimize, dis # Create a sample pickle data sample_data = pickle.dumps([1, 2, 3, \'a\', \'b\', \'c\']) # Function to disassemble pickle def disassemble_pickle(pickle_data: bytes, annotate: bool = False) -> str: import io output = io.StringIO() dis(pickle_data, out=output, annotate=1 if annotate else 0) return output.getvalue() # Function to optimize pickle def optimize_pickle(pickle_data: bytes) -> bytes: return optimize(pickle_data) # Example usage disassembled = disassemble_pickle(sample_data, annotate=True) optimized = optimize_pickle(sample_data) print(\\"Disassembled Output:n\\", disassembled) print(\\"Original Length:\\", len(sample_data)) print(\\"Optimized Length:\\", len(optimized)) ``` **Expected Output:** (Example output will depend on the `pickletools.dis` and `pickletools.optimize` functions and the initial pickle data provided.) ``` Disassembled Output: 0: x80 PROTO 4 2: ( MARK 3: K BININT1 1 5: K BININT1 2 7: K BININT1 3 9: X BINUNICODE \'a\' ... Annotated for each opcode... Original Length: 44 Optimized Length: 42 ```","solution":"import io from pickletools import dis, optimize def disassemble_pickle(pickle_data: bytes, annotate: bool = False) -> str: Disassemble the provided pickle byte string. :param pickle_data: The pickle byte string to be disassembled. :param annotate: Whether to annotate the output with opcode descriptions. :return: The disassembled representation of the pickle. output = io.StringIO() dis(pickle_data, out=output, annotate=1 if annotate else 0) return output.getvalue() def optimize_pickle(pickle_data: bytes) -> bytes: Optimize the provided pickle byte string. :param pickle_data: The pickle byte string to be optimized. :return: The optimized pickle byte string. return optimize(pickle_data)"},{"question":"# PyTorch CUDA Tuning Assessment Objective: You are tasked with implementing a robust Python function utilizing the `torch.cuda.tunable` package to perform and validate a tuning operation. This function must enable tuning, set specific tuning parameters, mock a GEMM operation (since direct GEMM tuning functionality might not be practically demonstrable without specific hardware), and retrieve results to illustrate the correct setup and usage of the tunable API. Task: Write a function `tune_and_validate()` that: 1. **Enables** tuning via `torch.cuda.tunable.enable()`. 2. **Sets** the maximum tuning duration to 100 seconds. 3. **Sets** the maximum tuning iterations to 50. 4. **Performs** a mock operation to tune GEMM (General Matrix Multiplication) as `torch.cuda.tunable.tune_gemm_in_file(\\"mock_file\\")`. 5. **Retrieves** the tuning results from the file (assumed \\"mock_file\\") without actual file I/O for demonstration purposes. 6. **Validates** by checking tuning status and parameters to confirm they are set correctly. 7. **Returns** a dictionary with the status of tuning enabled, maximum tuning duration, and maximum tuning iterations, and a message \\"Tuning Mock Success\\" once the steps are completed correctly. Constraints: - Ensure the function handles the mock tuning operation without actual hardware interaction for demonstration. - Your solution should not actually perform file I/O, assume the file is just a placeholder for checking functional flow. Input: - No direct input to the function. Output: - A dictionary: ```python { \\"tuning_enabled\\": True/False, \\"max_tuning_duration\\": int, \\"max_tuning_iterations\\": int, \\"message\\": \\"Tuning Mock Success\\" } ``` Example: ```python def tune_and_validate(): # Your implementation here output = tune_and_validate() print(output) ``` Expected output: ```python { \\"tuning_enabled\\": True, \\"max_tuning_duration\\": 100, \\"max_tuning_iterations\\": 50, \\"message\\": \\"Tuning Mock Success\\" } ``` You are encouraged to leverage exception handling to cover any potential issues with enabling tuning or setting parameters.","solution":"def tune_and_validate(): Function to perform and validate a mock CUDA tuning operation. # Assuming usage of torch.cuda.tunable - Mocked for demonstration purposes. try: import torch.cuda.tunable as tunable # Enable tuning tunable.enable() # Set maximum tuning duration to 100 seconds tunable.set_max_duration(100) # Set maximum tuning iterations to 50 tunable.set_max_iterations(50) # Mock operation to tune GEMM tunable.tune_gemm_in_file(\\"mock_file\\") # Retrieve the tuning results - Mocked for demonstration purposes is_tuning_enabled = tunable.is_enabled() max_tuning_duration = tunable.get_max_duration() max_tuning_iterations = tunable.get_max_iterations() assert is_tuning_enabled == True assert max_tuning_duration == 100 assert max_tuning_iterations == 50 return { \\"tuning_enabled\\": is_tuning_enabled, \\"max_tuning_duration\\": max_tuning_duration, \\"max_tuning_iterations\\": max_tuning_iterations, \\"message\\": \\"Tuning Mock Success\\" } except Exception as e: return{ \\"tuning_enabled\\": False, \\"max_tuning_duration\\": 0, \\"max_tuning_iterations\\": 0, \\"message\\": f\\"Tuning failed with exception: {e}\\" }"},{"question":"Objective To assess the understanding and practical implementation of assignment statements, control flow statements, and exception handling in Python. Task Write a Python function `process_data` that takes a list of tuples as input. Each tuple consists of an identifier (string) and an associated value (integer). The function should process this list to perform the following tasks: 1. **Assignment and Augmented Assignment**: - Initialize an empty dictionary `result`. - Use assignment statements to bind each identifier to its corresponding value in the dictionary. - For identifiers that have been encountered before, use augmented assignment to add the new value to the existing value. 2. **Control Flow - Break and Continue**: - Process each tuple in the list. If a tuple\'s value is negative, skip processing for that tuple using `continue`. - If a special identifier \\"STOP\\" is encountered, terminate the processing of the list using `break`. 3. **Exception Handling**: - Ensure that if a non-tuple is encountered in the list, an `AssertionError` is raised with the message `\\"Invalid item encountered\\"`. - If a tuple contains an invalid identifier (non-string) or value (non-integer), raise a `TypeError` with an appropriate message. 4. **Return Statement**: - Return the processed dictionary `result`. Constraints - The input list will contain a maximum of 10^4 elements. - Each identifier will be a non-empty string up to 50 characters. - Each value will be an integer between -10^6 and 10^6. Function Signature ```python def process_data(data: list) -> dict: pass ``` Example ```python input_data = [(\\"a\\", 10), (\\"b\\", 20), (\\"a\\", -5), (\\"STOP\\", 0), (\\"c\\", 30)] output = process_data(input_data) print(output) # Output: {\'a\': 5, \'b\': 20} ``` Explanation - `\\"a\\"` is added with 10, then with -5, resulting in 5. - `\\"b\\"` is added with 20. - Processing stops upon encountering `\\"STOP\\"`. - `\\"c\\"` is not processed because of the break statement. Detailed Steps 1. Initialize an empty dictionary `result`. 2. Iterate through the list `data`: - Use `continue` to skip tuples with negative values. - Use `break` to exit the loop when `\\"STOP\\"` is encountered. - Use assertions and type checks to validate input tuple structure. - Use assignment and augmented assignment to update the dictionary `result` based on the rules given. 3. Return the dictionary `result`.","solution":"def process_data(data: list) -> dict: Processes a list of tuples containing identifier (string) and value (integer). result = {} for item in data: if not isinstance(item, tuple) or len(item) != 2: raise AssertionError(\\"Invalid item encountered\\") identifier, value = item if not isinstance(identifier, str): raise TypeError(\\"Invalid identifier: must be a string\\") if not isinstance(value, int): raise TypeError(\\"Invalid value: must be an integer\\") if value < 0: continue if identifier == \\"STOP\\": break if identifier in result: result[identifier] += value else: result[identifier] = value return result"},{"question":"# Custom Exception Handling in Python Objective: Design and implement a custom exception hierarchy to handle specific error scenarios in a database interaction module. This will evaluate your ability to define custom exceptions, raise them appropriately, and ensure proper exception chaining and context. Task: 1. Define the following custom exceptions: - `DatabaseError`: Base class for other database-related exceptions. - `ConnectionError`: Raised when there is a connection issue to the database. - `QueryError`: Raised when a query fails due to invalid syntax or execution issues. - `DataError`: Raised when there is an issue with the data being processed (e.g., data type mismatch, null value constraints). 2. Implement a function `connect_to_database(connection_string)` that simulates a database connection. If the connection string is invalid (anything other than \\"valid_connection_string\\"), raise `ConnectionError` with an appropriate message. 3. Implement a function `execute_query(query_string)` that simulates executing a database query. If the query string is \\"invalid_query\\", raise `QueryError` with an appropriate message. 4. Implement a function `process_data(data)` that simulates processing data fetched from the database. If the data is `None`, raise `DataError` with an appropriate message. 5. Implement a function `database_operation(connection_string, query_string, data)` that uses the above functions to: - Attempt to connect to the database. - Execute a query. - Process the fetched data. - Use a try-except block to catch and chain exceptions properly if one of the operations fails. Ensure that the original exception context is preserved. Input: - `connection_string`: A `string` representing the connection string. - `query_string`: A `string` representing the SQL query. - `data`: Can be any type but will be checked for `None`. Output: - Custom exceptions should be raised and handled with proper context and chaining. Constraints: - Do not use any actual database connections or third-party database drivers. - The function and exceptions should be implemented in pure Python. Example: ```python try: database_operation(\\"invalid_connection\\", \\"SELECT * FROM table\\", {\\"id\\": 1, \\"name\\": \\"John\\"}) except DatabaseError as e: print(f\\"Caught an exception: {e}\\") ``` # Expected Output: ``` Caught an exception: ConnectionError: Unable to connect to the database with the provided connection string: invalid_connection ``` Implementation: ```python class DatabaseError(Exception): Base class for other database-related exceptions. pass class ConnectionError(DatabaseError): Raised when there is a connection issue to the database. pass class QueryError(DatabaseError): Raised when a query fails due to invalid syntax or execution issues. pass class DataError(DatabaseError): Raised when there is an issue with the data being processed. pass def connect_to_database(connection_string): if connection_string != \\"valid_connection_string\\": raise ConnectionError(f\\"Unable to connect to the database with the provided connection string: {connection_string}\\") print(\\"Connected to database\\") def execute_query(query_string): if query_string == \\"invalid_query\\": raise QueryError(\\"Invalid query string provided.\\") print(\\"Query executed successfully\\") def process_data(data): if data is None: raise DataError(\\"Data provided is None and cannot be processed.\\") print(\\"Data processed successfully\\") def database_operation(connection_string, query_string, data): try: connect_to_database(connection_string) execute_query(query_string) process_data(data) except ConnectionError as ce: raise DatabaseError(\\"Database operation failed due to a connection issue.\\") from ce except QueryError as qe: raise DatabaseError(\\"Database operation failed due to a query issue.\\") from qe except DataError as de: raise DatabaseError(\\"Database operation failed due to a data issue.\\") from de # Example usage try: database_operation(\\"invalid_connection\\", \\"SELECT * FROM table\\", {\\"id\\": 1, \\"name\\": \\"John\\"}) except DatabaseError as e: print(f\\"Caught an exception: {e}\\") ```","solution":"class DatabaseError(Exception): Base class for other database-related exceptions. pass class ConnectionError(DatabaseError): Raised when there is a connection issue to the database. pass class QueryError(DatabaseError): Raised when a query fails due to invalid syntax or execution issues. pass class DataError(DatabaseError): Raised when there is an issue with the data being processed. pass def connect_to_database(connection_string): if connection_string != \\"valid_connection_string\\": raise ConnectionError(f\\"Unable to connect to the database with the provided connection string: {connection_string}\\") print(\\"Connected to database\\") def execute_query(query_string): if query_string == \\"invalid_query\\": raise QueryError(\\"Invalid query string provided.\\") print(\\"Query executed successfully\\") def process_data(data): if data is None: raise DataError(\\"Data provided is None and cannot be processed.\\") print(\\"Data processed successfully\\") def database_operation(connection_string, query_string, data): try: connect_to_database(connection_string) execute_query(query_string) process_data(data) except ConnectionError as ce: raise DatabaseError(\\"Database operation failed due to a connection issue.\\") from ce except QueryError as qe: raise DatabaseError(\\"Database operation failed due to a query issue.\\") from qe except DataError as de: raise DatabaseError(\\"Database operation failed due to a data issue.\\") from de # Example usage try: database_operation(\\"invalid_connection\\", \\"SELECT * FROM table\\", {\\"id\\": 1, \\"name\\": \\"John\\"}) except DatabaseError as e: print(f\\"Caught an exception: {e}\\")"},{"question":"Problem Statement You are tasked with using the `netrc` package to manage and retrieve FTP authentication details from a `.netrc` file. Your goal is to implement a class `NetrcManager` that provides a method to retrieve authentication details for a given host, and another method to check if a host exists in the `.netrc` file. Requirements: 1. **Class Name**: `NetrcManager` 2. **Initialization**: The class should accept an optional filename during initialization. - If no filename is provided, it should default to `.netrc` in the user\'s home directory. 3. **Method 1**: `get_authenticator(host: str) -> Optional[Tuple[str, str, str]]` - This method should return a tuple `(login, account, password)` for the given host. If the host is not found and no `default` entry exists, return `None`. 4. **Method 2**: `host_exists(host: str) -> bool` - This method should check if the specified host exists in the `.netrc` file and return `True` if it does, otherwise `False`. Input and Output Formats 1. **Initialization**: `NetrcManager(file: Optional[str] = None)` - Initializes the class, potentially raising a `FileNotFoundError` or `NetrcParseError` as per the `netrc` documentation. 2. **Method 1**: `get_authenticator(host: str) -> Optional[Tuple[str, str, str]]` - Input: `host` (string) - The hostname to look for. - Output: Tuple of strings `(login, account, password)` if the host exists or has a `default` entry, else `None`. 3. **Method 2**: `host_exists(host: str) -> bool` - Input: `host` (string) - The hostname to check for existence. - Output: `True` if the host exists in the `.netrc` file, else `False`. Constraints 1. Assume the `.netrc` file exists and is readable, and it adheres to the format expected by the `netrc` package. 2. Performance considerations are minimal as the `.netrc` files are generally small. Example Usage ```python # Filename \'sample.netrc\' contains: # machine example.com # login user # account 12345 # password mypass import os manager = NetrcManager(file=\'sample.netrc\') # Get authenticator for example.com authenticator = manager.get_authenticator(\'example.com\') print(authenticator) # Output: (\'user\', \'12345\', \'mypass\') # Check if example.com exists in the netrc file exists = manager.host_exists(\'example.com\') print(exists) # Output: True # Check if non-existing host exists exists_non = manager.host_exists(\'nonexistent.com\') print(exists_non) # Output: False ``` Implement the `NetrcManager` class according to the specifications above.","solution":"import os import netrc from typing import Optional, Tuple class NetrcManager: def __init__(self, file: Optional[str] = None): netrc_file = file if file else os.path.join(os.path.expanduser(\\"~\\"), \\".netrc\\") try: self.netrc_data = netrc.netrc(netrc_file) except (FileNotFoundError, netrc.NetrcParseError) as e: raise e def get_authenticator(self, host: str) -> Optional[Tuple[str, str, str]]: try: return self.netrc_data.authenticators(host) except KeyError: return None def host_exists(self, host: str) -> bool: return host in self.netrc_data.hosts"},{"question":"Objective Write a custom `print()` function that logs the printed statements to a file while still displaying them on the console. Question Python\'s built-in `print()` function is used to output text to the console. However, for some applications, it might be useful to keep a log of all print statements for further analysis. Implement a custom `print()` function that does the following: 1. Prints the output to the console, just like the built-in `print()` function. 2. Logs the printed output to a file named `print_log.txt`. Instructions 1. Import the necessary members from the `builtins` module. 2. Implement a custom `print()` function that achieves the specified behavior. 3. Your function should accept the same parameters as Python\'s built-in `print()` function. 4. Ensure that this custom `print()` function supports the `sep`, `end`, `file`, and `flush` parameters. Signature ```python import builtins def print(*args, sep=\' \', end=\'n\', file=None, flush=False): # Your implementation here ``` Example Usage ```python print(\\"Hello, World!\\") print(\\"This is\\", \\"a test.\\", sep=\\" - \\") print(\\"End of\\", \\"line.\\", end=\\" ENDn\\") ``` Example Output *On the console:* ``` Hello, World! This is - a test. End of line. END ``` *In the file `print_log.txt`:* ``` Hello, World! This is - a test. End of line. END ``` Constraints - Ensure that any existing content in `print_log.txt` is preserved, and new print statements are appended to the file. - Handle any potential exceptions gracefully. Notes - Do not use any third-party libraries. - You may assume that the file `print_log.txt` will always be writable.","solution":"import builtins original_print = builtins.print def print(*args, sep=\' \', end=\'n\', file=None, flush=False): # Use the original print function to output to the console original_print(*args, sep=sep, end=end, file=file, flush=flush) # Open the log file in append mode with open(\'print_log.txt\', \'a\', encoding=\'utf-8\') as log_file: # Join the arguments with the given separator output = sep.join(map(str, args)) + end # Write the output to the log file log_file.write(output)"},{"question":"Utilizing PyTorch `torch.xpu` for Efficient Computation Objective: You are to demonstrate your understanding of PyTorch’s `torch.xpu` module by performing computations on a specified device, managing random number generation, and handling memory efficiently. Problem Statement: You are given a task to initialize the `torch.xpu` device, set up a computation where you perform matrix multiplication and manage the resources efficiently to avoid memory leaks. Additionally, you need to ensure the reproducibility of your results using random seeds. Tasks: 1. **Initialize the XPU Device**: Write code to check if an `XPU` device is available and set it as the current device. If no `XPU` devices are available, use the CPU. 2. **Set Random Seed**: Set the random seed for all possible devices to ensure reproducibility. 3. **Device Information**: Print information about the current device being used, such as its name and capabilities. 4. **Matrix Multiplication**: - Create two random matrices `A` and `B` of shape `(1024, 512)` and `(512, 1024)` respectively on the specified device. - Perform matrix multiplication to compute the result `C`. 5. **Memory Management**: After the computation, release any cached memory to ensure efficient memory management. Expected Input/Output: - **No input** is required as parameters; the function should assume default behavior. - **Output** should include: - Confirmation message if `XPU` is available and used, otherwise a message stating the `CPU` is being used. - Current device name and its capabilities. - Duration taken to perform the matrix multiplication. Constraints and Performance Requirements: - Ensure reproducibility by properly setting seeds. - Efficiently manage memory to avoid leaks or excessive usage. - The code should handle scenarios where no `XPU` device is available and fallback to CPU-based computations. Function Signature: ```python def xpu_computation(): # Your code here pass ``` Example: ```python >>> xpu_computation() XPU Device is available and being used. Device Name: XPU Device 0 Device Capability: (x, y) # Replace with appropriate values Matrix multiplication took: 0.0012 seconds. ``` In the example above, replace the placeholders `(x, y)` with actual capabilities retrieved from the device properties.","solution":"import torch import time def xpu_computation(): # Check if XPU is available device = torch.device(\\"xpu:0\\") if torch.xpu.is_available() else torch.device(\\"cpu\\") # Set the random seed for reproducibility torch.manual_seed(0) if torch.xpu.is_available(): torch.xpu.manual_seed_all(0) # Print device information if device.type == \'xpu\': print(\\"XPU Device is available and being used.\\") device_name = torch.xpu.get_device_name(device.index) device_capability = torch.xpu.get_device_capability() print(f\\"Device Name: {device_name}\\") print(f\\"Device Capability: {device_capability}\\") else: print(\\"No XPU Device available, using CPU.\\") # Create random matrices A and B on the specified device A = torch.randn((1024, 512), device=device) B = torch.randn((512, 1024), device=device) # Perform matrix multiplication start_time = time.time() C = torch.matmul(A, B) end_time = time.time() # Print the duration taken for matrix multiplication print(f\\"Matrix multiplication took: {end_time - start_time} seconds.\\") # Memory management: release cached memory if device.type == \'xpu\': torch.xpu.empty_cache() return device, C"},{"question":"# Advanced I/O Operations Using `io` Module Create a comprehensive function that demonstrates advanced usage of the `io` module. The function should perform the following tasks: 1. **Read and Write Text Data**: - Open a text file for reading and specify the encoding as `utf-8`. Read its content. - Create a text file in memory using `StringIO` and write some initial text data to it. - Append the text read from the file to the in-memory text stream. 2. **Read and Write Binary Data**: - Open a binary file for reading and read its content. If the file does not exist, create one with some initial binary data. - Create an in-memory binary stream with `BytesIO` and write some additional binary data to it. - Append the binary data read from the file to the in-memory binary stream. 3. **Mixed Operations**: - Combine and summarize the content read from both text and binary files into a single in-memory text stream using `StringIO`. - The summary should consist of: - The total number of characters read from the text file. - The total number of bytes read from the binary file. - The first 100 characters of the text file contents. - The first 100 bytes of the binary file contents (represented as a string of hexadecimal values). 4. **Error Handling**: - Ensure proper error handling for file operations, especially for file not found and permission errors. - Explicitly manage the encoding errors by using `errors=\'replace\'` when writing text data. 5. **Performance Considerations**: - Use buffered I/O where appropriate and demonstrate the advantage. Here is the signature of the function you need to implement: ```python import io from typing import Tuple, Optional def advanced_io_operations(text_file_path: str, binary_file_path: str) -> Tuple[str, str, str]: Perform advanced I/O operations using the io module. Args: - text_file_path (str): The path to a text file for reading. - binary_file_path (str): The path to a binary file for reading. Returns: - Tuple[str, str, str]: A tuple containing: * Summary of text file operations, * Summary of binary file operations, * Final combined summary. Raises: - OSError: If there is an error with file operations. - ValueError: For invalid input values. pass ``` # Example: ```python text_file_summary, binary_file_summary, combined_summary = advanced_io_operations(\\"sample.txt\\", \\"sample.bin\\") print(text_file_summary) print(binary_file_summary) print(combined_summary) ``` # Constraints: - You must use the `io` module for all I/O operations. - Ensure all streams are closed properly after operations. - Handle potential I/O errors gracefully. - The function should work efficiently with large files.","solution":"import io from typing import Tuple def advanced_io_operations(text_file_path: str, binary_file_path: str) -> Tuple[str, str, str]: # Handle text file reading try: with io.open(text_file_path, \'r\', encoding=\'utf-8\', errors=\'replace\') as text_file: text_data = text_file.read() except FileNotFoundError: text_data = \\"\\" text_summary = f\\"Text file \'{text_file_path}\' not found.n\\" except PermissionError: return f\\"Permission denied for text file \'{text_file_path}\'.\\", \\"\\", \\"\\" else: text_summary = f\\"Text file \'{text_file_path}\' read successfully.n\\" text_stream = io.StringIO() text_stream.write(\\"Initial in-memory text data.n\\") text_stream.write(text_data) text_content = text_stream.getvalue() text_length = len(text_content) try: with io.open(binary_file_path, \'rb\') as binary_file: binary_data = binary_file.read() except FileNotFoundError: binary_data = b\\"\\" binary_summary = f\\"Binary file \'{binary_file_path}\' not found.n\\" except PermissionError: return f\\"Permission denied for binary file \'{binary_file_path}\'.\\", \\"\\", \\"\\" else: binary_summary = f\\"Binary file \'{binary_file_path}\' read successfully.n\\" binary_stream = io.BytesIO() binary_stream.write(b\'Initial in-memory binary data.n\') binary_stream.write(binary_data) binary_content = binary_stream.getvalue() binary_length = len(binary_content) combined_stream = io.StringIO() combined_stream.write(f\\"Total characters read from text file: {text_length}n\\") combined_stream.write(f\\"Total bytes read from binary file: {binary_length}n\\") combined_stream.write(f\\"First 100 characters of text data:n{text_content[:100]}n\\") combined_stream.write(f\\"First 100 bytes of binary data (as hex):n{binary_data[:100].hex()}n\\") combined_summary = combined_stream.getvalue() return text_summary, binary_summary, combined_summary"},{"question":"Coding Assessment Question # Objective Implement a function that performs various operations on a given Unicode string. Your function should demonstrate the ability to handle encoding and decoding, checking character properties, and applying specified transformations. # Function Signature ```python def process_unicode_string(input_string: str) -> str: pass ``` # Input - `input_string`: A string in Unicode format. The string will be non-empty and can contain characters from different languages and symbols. # Output - Returns a processed string based on the transformations specified below. # Constraints - The `input_string` will have a length of at most `10^4` characters. - Your implementation should handle both the UTF-8 and UTF-16 encodings correctly. - Ensure the function can efficiently handle the provided constraint on input size. # Transformations to Apply 1. Encode the string to UTF-16 and ensure it can be decoded back to its original form. 2. Identify and remove all non-alphabetic characters from the string. 3. Create a new string where each character\'s Unicode property is analyzed, and transform each character to its upper case equivalent if it\'s a lowercase alphabetic character, otherwise leave it unchanged. 4. Return the resultant string after applying these transformations. # Example ```python input_string = \\"Python3.9 😊\\" output = process_unicode_string(input_string) print(output) # Expected Output: \\"PYTHON\\" ``` # Explanation 1. The input string is first encoded to UTF-16 to ensure it can be decoded back without information loss. 2. Non-alphabetic characters such as digits, punctuation, and emojis are removed. 3. Each remaining lowercase alphabetic character is converted to its upper case form. # Notes - Focus on ensuring your implementation is efficient and handles the encoding-decoding cycle without data loss. - Use built-in Python functions and libraries wherever possible to simplify operations.","solution":"def process_unicode_string(input_string: str) -> str: # Step 1: Encode and decode in UTF-16 to ensure no data loss. utf16_encoded = input_string.encode(\'utf-16\') decoded_string = utf16_encoded.decode(\'utf-16\') # Step 2: Remove non-alphabetic characters. alphabetic_only = \'\'.join([char for char in decoded_string if char.isalpha()]) # Step 3: Transform lowercase alphabetic characters to uppercase. transformed_string = \'\'.join([char.upper() if char.islower() else char for char in alphabetic_only]) return transformed_string"},{"question":"You have been provided with a dataset called `iris`, included in the `seaborn` library, which contains different measurements of iris flowers. Your task is to create visual representations of this dataset using `seaborn`\'s `boxenplot`. Specifically, you need to: 1. Load the `iris` dataset and display the first few rows to understand its structure. 2. Create a vertical `boxenplot` comparing the petal width (`petal_width`) across different species (`species`). Ensure the boxes are colored by species. 3. Customize the plot by: - Setting a linear method for box width. - Adjusting the width of the largest box. - Controlling the outlines of the boxes, median lines, and outlier appearances. 4. Explore and visually represent any interesting trends or distributions that you observe in the data using additional plots, if necessary. Your implementation should demonstrate the following expectations: - Correctly loading and displaying the dataset. - Producing a `boxenplot` with the specified configurations. - Customizing the plot aesthetics as described. - Clear and accurate observation of trends using additional plots. # Expected Input and Output - **Input**: - `iris` dataset from `seaborn` - **Output**: - Customized `boxenplot` and any additional plot to represent trends/observations. # Constraints - Ensure the resulting plots are clear and informative. - Use the default style settings or appropriately customized settings to enhance readability. # Performance Requirements - Efficiently handle the dataset to create visualizations without unnecessary computations. # Example This example will guide you through the basic steps (this is an illustration; ensure to include all required customizations): ```python import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") print(iris.head()) # Create a boxenplot plt.figure(figsize=(10, 6)) sns.boxenplot(data=iris, x=\'species\', y=\'petal_width\', hue=\'species\', width=0.5, width_method=\'linear\', linewidth=1.5, linecolor=\\".7\\", line_kws={\\"linewidth\\": 2, \\"color\\": \\"blue\\"}, flier_kws={\\"marker\\":\\"o\\", \\"color\\":\\"green\\"}) plt.title(\\"Petal Width by Species\\") plt.show() ``` # Additional Notes - Ensure to explore other plotting functionalities to capture interesting trends or distributions. - The clarity of the plot, including labels and legends, will be taken into account in the assessment.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_and_display_iris(): Loads the iris dataset and displays the first few rows for structure understanding. iris = sns.load_dataset(\\"iris\\") print(iris.head()) return iris def create_custom_boxenplot(iris): Creates a custom boxenplot comparing the petal width across different species with specified customizations. plt.figure(figsize=(10, 6)) sns.boxenplot( data=iris, x=\'species\', y=\'petal_width\', hue=\'species\', width=0.6, linewidth=1.5, dodge=False ) plt.title(\\"Customized Boxenplot of Petal Width by Species\\") plt.show() def additional_plots(iris): Creates additional visualizations to explore interesting trends in the iris dataset. sns.pairplot(iris, hue=\'species\') plt.title(\\"Pairplot of Iris Dataset\\") plt.show() # Main function to execute the plotting def main(): iris = load_and_display_iris() create_custom_boxenplot(iris) additional_plots(iris) # Execute main function main()"},{"question":"# Built Distribution Automation Script in Python You are tasked with creating an automation script to streamline the process of generating multiple types of built distributions for a Python module using Distutils. The script should read a configuration file to determine which distribution formats to generate and then invoke the appropriate `bdist` commands. Problem Statement Write a Python script named `build_distributor.py` that will: 1. **Read and parse a configuration file** (`dist_config.json`). This JSON file specifies: - The module\'s setup file path (`setup_path`). - A list of distribution formats to generate (`formats`). 2. **Generate the specified distributions** using the Distutils `bdist` command. 3. **Log the commands executed** and their outputs (both stdout and stderr) to a file named `build.log`. The structure of `dist_config.json` is as follows: ```json { \\"setup_path\\": \\"path/to/your/setup.py\\", \\"formats\\": [\\"gztar\\", \\"zip\\", \\"rpm\\"] } ``` Input and Output Specifications - **Input**: - A JSON configuration file (`dist_config.json`). - **Output**: - The generated distribution files (in the specified formats). - A log file (`build.log`) with the details of executed commands and their outputs. Constraints 1. You must handle any exceptions that occur during command execution and write appropriate error messages to the log file. 2. The script must be executable from the command line. Example Given a `dist_config.json`: ```json { \\"setup_path\\": \\"sample_project/setup.py\\", \\"formats\\": [\\"gztar\\", \\"zip\\"] } ``` Your script should generate the commands: ```bash python sample_project/setup.py bdist --formats=gztar python sample_project/setup.py bdist --formats=zip ``` These commands should be run using Python\'s `subprocess` module, and their outputs should be logged in `build.log`. Implementation ```python import json import subprocess import os def main(): # Read config file with open(\\"dist_config.json\\", \\"r\\") as f: config = json.load(f) setup_path = config.get(\\"setup_path\\") formats = config.get(\\"formats\\") if not setup_path or not formats: raise ValueError(\\"Invalid configuration file. Make sure \'setup_path\' and \'formats\' are specified.\\") with open(\\"build.log\\", \\"w\\") as log_file: for fmt in formats: command = f\\"python {setup_path} bdist --formats={fmt}\\" try: result = subprocess.run(command, shell=True, capture_output=True, text=True) log_file.write(f\\"Command: {command}n\\") log_file.write(f\\"Output:n{result.stdout}n\\") log_file.write(f\\"Errors:n{result.stderr}n\\") except Exception as e: log_file.write(f\\"Failed to execute command: {command}n\\") log_file.write(f\\"Exception: {str(e)}n\\") if __name__ == \\"__main__\\": main() ``` Notes 1. Ensure Python and Distutils are installed. 2. Adjust file paths and JSON structure as necessary for your specific project setup. Test your script with different configurations to verify it handles various scenarios correctly.","solution":"import json import subprocess import os def main(): # Read config file with open(\\"dist_config.json\\", \\"r\\") as f: config = json.load(f) setup_path = config.get(\\"setup_path\\") formats = config.get(\\"formats\\") if not setup_path or not formats: raise ValueError(\\"Invalid configuration file. Make sure \'setup_path\' and \'formats\' are specified.\\") with open(\\"build.log\\", \\"w\\") as log_file: for fmt in formats: command = f\\"python {setup_path} bdist --formats={fmt}\\" try: result = subprocess.run(command, shell=True, capture_output=True, text=True) log_file.write(f\\"Command: {command}n\\") log_file.write(f\\"Output:n{result.stdout}n\\") log_file.write(f\\"Errors:n{result.stderr}n\\") except Exception as e: log_file.write(f\\"Failed to execute command: {command}n\\") log_file.write(f\\"Exception: {str(e)}n\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective Demonstrate the ability to handle built-in exceptions, create custom exceptions, and use exception chaining in Python. # Question You are given a file named `data.txt` that contains lines of text. Your task is to write a function `process_file(filename: str) -> List[int]` that reads the file, processes the data, and returns a list of integers. Each line in the file should contain a single integer. Your function should: 1. Read all lines from the specified file. 2. Convert each line to an integer. 3. Return a list of these integers. Your function must handle the following cases: - If the file does not exist, raise a custom exception `FileNotFoundCustomError`. This exception should include the filename in the error message. - If a line contains a non-integer value, raise a custom exception `InvalidDataError`. This exception should include the line number and the invalid data. - If any other error occurs during file reading or processing, raise a custom exception `ProcessingError`. This exception should chain the original exception using the `from` keyword. Custom Exceptions Define the following custom exceptions: - `FileNotFoundCustomError(BaseException)`: Raised when the file is not found. - `InvalidDataError(BaseException)`: Raised when a line contains non-integer data. - `ProcessingError(BaseException)`: Raised for any other errors, chaining the original exception. Constraints - The function should be efficient and handle large files gracefully. - Do not use any external libraries other than those provided by the standard library. - Assume the file is small enough to fit into memory. Function Signature ```python def process_file(filename: str) -> List[int]: # Your code here ``` Example Assume `data.txt` contains: ``` 42 hello 19 ``` - Calling `process_file(\'data.txt\')` should raise `InvalidDataError` with a message indicating that line 2 contains invalid data. # Notes: - Make sure to include docstrings for your custom exceptions and the main function, explaining the purpose and usage. - Handle file operations and exceptions diligently within the function.","solution":"from typing import List class FileNotFoundCustomError(Exception): Raised when the specified file is not found. def __init__(self, filename: str): self.filename = filename super().__init__(f\\"File not found: {filename}\\") class InvalidDataError(Exception): Raised when a file line contains non-integer data. def __init__(self, line_number: int, invalid_data: str): self.line_number = line_number self.invalid_data = invalid_data super().__init__(f\\"Invalid data at line {line_number}: {invalid_data}\\") class ProcessingError(Exception): Raised for any other errors occurring during file processing. def __init__(self, original_exception: Exception): super().__init__(f\\"An error occurred during file processing: {original_exception}\\") self.original_exception = original_exception def process_file(filename: str) -> List[int]: Reads a file and processes its lines as integers. Args: filename (str): The path to the file to be processed. Returns: List[int]: A list of integers read from the file. Raises: FileNotFoundCustomError: If the specified file is not found. InvalidDataError: If a line contains non-integer data. ProcessingError: For any other errors occurring during file processing. try: with open(filename, \'r\') as file: lines = file.readlines() except FileNotFoundError: raise FileNotFoundCustomError(filename) except Exception as e: raise ProcessingError(e) integers = [] for line_number, line in enumerate(lines, start=1): try: number = int(line.strip()) integers.append(number) except ValueError: raise InvalidDataError(line_number, line.strip()) except Exception as e: raise ProcessingError(e) from e return integers"},{"question":"You are required to demonstrate your understanding of how to handle BatchNorm issues when applying vmapping in PyTorch with functorch. The proposed task involves implementing a function to replace all BatchNorm modules in a given neural network model with GroupNorm, ensuring compatibility with vmapping. # Function Signature ```python def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: pass ``` # Input - `model` (torch.nn.Module): A PyTorch model that may contain one or more BatchNorm layers. - `num_groups` (int): The number of groups to be used for the GroupNorm layers. # Output - `torch.nn.Module`: A new PyTorch model where every BatchNorm module has been replaced with a corresponding GroupNorm module. # Constraints - The number of groups `num_groups` must be chosen such that the number of channels in each BatchNorm layer is divisible by `num_groups`. - You need to recursively traverse all submodules of the input model to find and replace the BatchNorm layers. # Example ```python import torch import torch.nn as nn class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1) self.bn2 = nn.BatchNorm2d(32) self.fc = nn.Linear(32 * 8 * 8, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) x = torch.flatten(x, 1) x = self.fc(x) return x model = SampleModel() new_model = replace_batchnorm_with_groupnorm(model, 4) ``` In this example, the `replace_batchnorm_with_groupnorm` function should replace `bn1` and `bn2` in `SampleModel` with GroupNorm layers with 4 groups each. # Note - You may need to use `torch.nn.GroupNorm` to create new GroupNorm layers. - Ensure the function maintains all other model parameters (weights, biases) and structure while only replacing the normalization layers. - You can assume that the input model is in training mode.","solution":"import torch import torch.nn as nn def replace_batchnorm_with_groupnorm(model: torch.nn.Module, num_groups: int) -> torch.nn.Module: for name, module in model.named_children(): if isinstance(module, nn.BatchNorm2d): num_channels = module.num_features if num_channels % num_groups != 0: raise ValueError(f\\"Number of groups {num_groups} is not divisible by number of channels {num_channels}\\") new_module = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels) setattr(model, name, new_module) else: replace_batchnorm_with_groupnorm(module, num_groups) return model"},{"question":"# Question: Implement and Use a Custom Pipeline with the `pipes` Module Your task is to implement a custom pipeline using the `pipes` module to process text files. You will need to define a function that configures this pipeline to perform the following actions on an input file: 1. Convert all text to uppercase. 2. Replace all new lines with spaces. 3. Sort all words in alphabetical order and remove duplicates. Function Signature ```python def process_file(input_file: str, output_file: str) -> None: pass ``` Input - `input_file` (str): Path to the input text file. - `output_file` (str): Path to the output text file where the processed content should be saved. Output - The function should not return a value, but the processed content should be written to `output_file`. Constraints - The function must utilize the `pipes` module. - You must handle any necessary file I/O operations within the function. - Assume the input file is sufficiently small to fit into memory. # Example Given an input file `input.txt` with the following content: ``` apple banana carrot apple Banana ``` The output file `output.txt` should contain: ``` APPLE BANANA CARROT ``` # Instructions: 1. You need to configure a `pipes.Template` to perform the text transformations using appropriate shell commands. 2. Use the `open()` method of the `Template` to create the filtered file output. 3. Ensure that the pipeline handles the described transformations in the proper sequence. 4. Test your function on different input files to verify its correctness. Note: You can make use of common Unix utilities like `tr`, `tr` combined with `sort`, and `uniq` to perform different steps in the pipeline.","solution":"import pipes def process_file(input_file: str, output_file: str) -> None: Process the input file and write the processed content to the output file. Args: input_file (str): Path to the input text file. output_file (str): Path to the output text file where the processed content should be saved. Returns: None # Create a new pipeline template t = pipes.Template() # Step 1: Convert all text to uppercase t.append(\'tr \\"[:lower:]\\" \\"[:upper:]\\"\', \'--\') # Step 2: Replace all new lines with spaces t.append(\'tr \\"n\\" \\" \\"\', \'--\') # Step 3: Sort all words in alphabetical order and remove duplicates t.append(\'tr \\" \\" \\"n\\" | sort | uniq | tr \\"n\\" \\" \\"\', \'--\') # Open the output file for writing with t.open(output_file, \'w\') as f_output: # Open the input file for reading and process the content with open(input_file, \'r\') as f_input: f_output.write(f_input.read()) # Example usage # process_file(\'input.txt\', \'output.txt\')"},{"question":"**Coding Assessment Question** # Custom Exception Handling with Python Built-in Exceptions **Objective:** Design and implement a functionality that uses custom exceptions inheriting from Python built-in exceptions. This functionality will demonstrate your understanding of exception hierarchy and handling in Python. # Problem Statement: You are required to create two custom exceptions and implement a function that processes a list of numerical values, raising and handling these exceptions under specific conditions. # Details: 1. **Custom Exceptions:** - `NegativeValueError`: Inherits from `ValueError`. Raised when a negative number is encountered in the list. - `InvalidTypeError`: Inherits from `TypeError`. Raised when a non-integer value is encountered in the list. 2. **Function:** - Implement a function `process_numbers(numbers: list)`, which takes in a list of numbers. 3. **Function Requirements:** - The function iterates through each element in the `numbers` list. - If an element is a negative number, raise `NegativeValueError` with a message \\"Negative value found: {value}\\". - If an element is not an integer (e.g., float, string), raise `InvalidTypeError` with a message \\"Invalid type found: {type of the value}\\". - The function should catch these exceptions and print the error messages without stopping the iteration. - Return a new list that contains only the valid positive integers from the original list. # Input: - A list `numbers` which may include integers, floats, strings, and negative values. e.g., `[1, -2, \'three\', 4.0, 5]` # Output: - A list containing only the valid positive integers from the input list after filtering out invalid entries. e.g., `[1, 5]` # Example: Input: ```python numbers = [1, -2, \'three\', 4, 5.0] ``` Output: ```python [1, 4] ``` Explanation: - The function identifies `-2` as a negative value and `\'three\'` as an invalid type and handles these exceptions accordingly, printing the error messages and returning `[1, 4]`. # Constraints: - Do not use any external libraries for type checking. - Ensure the implementation follows proper Python conventions for exception handling and custom exception definitions. # Implementation: 1. Define the custom exceptions: `NegativeValueError` and `InvalidTypeError`. 2. Implement the `process_numbers` function. 3. Handle both custom exceptions within the function to ensure continued processing of the list. ```python # Define custom exceptions class NegativeValueError(ValueError): pass class InvalidTypeError(TypeError): pass def process_numbers(numbers: list): valid_numbers = [] for number in numbers: try: if isinstance(number, int): if number < 0: raise NegativeValueError(f\\"Negative value found: {number}\\") valid_numbers.append(number) else: raise InvalidTypeError(f\\"Invalid type found: {type(number)}\\") except (NegativeValueError, InvalidTypeError) as e: print(e) return valid_numbers # Example Usage numbers = [1, -2, \'three\', 4, 5.0] print(process_numbers(numbers)) # Output: [1, 4] ``` # Note: - Make sure to test your function with various edge cases to ensure it handles all specified invalid input scenarios correctly.","solution":"# Define custom exceptions class NegativeValueError(ValueError): Exception raised for detecting negative values in the list. pass class InvalidTypeError(TypeError): Exception raised for detecting invalid types in the list. pass def process_numbers(numbers: list): Processes the given list of numbers. Raises and handles custom exceptions for negative values and invalid types, and returns a list of valid integers. Parameters: numbers (list): A list of elements to process Returns: list: List of valid positive integers from the input list valid_numbers = [] for number in numbers: try: if isinstance(number, int): if number < 0: raise NegativeValueError(f\\"Negative value found: {number}\\") valid_numbers.append(number) else: raise InvalidTypeError(f\\"Invalid type found: {type(number)}\\") except (NegativeValueError, InvalidTypeError) as e: print(e) return valid_numbers"},{"question":"**Question: Task Scheduler with Dependent Tasks** You are tasked with creating a task scheduler using the `sched` module in Python. Your scheduler should be able to manage and execute tasks with dependencies, meaning a task should not start until specified preceding tasks are complete. # Specifications: 1. **Class Definition**: - Implement a class `TaskScheduler` which utilizes `sched.scheduler` to manage and execute tasks. - The `TaskScheduler` class should include the following methods: - `add_task(time, priority, action, dependencies=[], argument=(), kwargs={})`: Schedules a new task. - `time`: Absolute time for the task execution. - `priority`: Priority of the task. - `action`: The function representing the task. - `dependencies`: A list of task IDs which must be completed before this task can be executed. - `argument`: Positional arguments for the action. - `kwargs`: Keyword arguments for the action. - `cancel_task(task_id)`: Cancels a previously scheduled task. - `run()`: Runs all scheduled tasks considering dependencies. - `list_tasks()`: Returns a list of all tasks scheduled, including their dependencies if any. 2. **Task ID Management**: - Each task should be assigned a unique ID which can be used for managing dependencies and cancellations. - Ensure tasks dependencies are respected during execution. 3. **Example of Usage**: ```python import time def task1(): print(f\\"Task 1 executed at {time.time()}\\") def task2(): print(f\\"Task 2 executed at {time.time()}\\") def task3(): print(f\\"Task 3 executed at {time.time()}\\") scheduler = TaskScheduler(timefunc=time.time, delayfunc=time.sleep) scheduler.add_task(time.time() + 10, 1, task1) scheduler.add_task(time.time() + 5, 2, task2, dependencies=[1]) scheduler.add_task(time.time() + 15, 1, task3, dependencies=[1, 2]) print(\\"List of tasks scheduled:\\", scheduler.list_tasks()) scheduler.run() ``` # Constraints: - Tasks should be scheduled with a minimum delay of 1 second. - Dependencies cannot form a cycle. - Assume system\'s clock time (time.time()) for scheduling. # Performance: - Ensure that tasks are executed as soon as their dependencies are met. Implement the `TaskScheduler` class based on the above specifications. **Expected Output**: - The tasks should execute in the sequence that respects their dependencies. - An appropriate exception should be raised if a task with unmet dependencies starts execution.","solution":"import sched import time class TaskScheduler: def __init__(self, timefunc=time.time, delayfunc=time.sleep): self.scheduler = sched.scheduler(timefunc, delayfunc) self.tasks = {} self.task_id = 0 def add_task(self, time, priority, action, dependencies=[], argument=(), kwargs={}): self.task_id += 1 task = { \'id\': self.task_id, \'time\': time, \'priority\': priority, \'action\': action, \'dependencies\': dependencies, \'argument\': argument, \'kwargs\': kwargs, \'completed\': False } self.tasks[self.task_id] = task self.scheduler.enterabs(time, priority, self._run_task, (self.task_id,)) return self.task_id def cancel_task(self, task_id): for event in list(self.scheduler.queue): if event.argument[0] == task_id: self.scheduler.cancel(event) del self.tasks[task_id] break def run(self): self.scheduler.run() def list_tasks(self): return [ { \'id\': task[\'id\'], \'time\': task[\'time\'], \'priority\': task[\'priority\'], \'dependencies\': task[\'dependencies\'] } for task in self.tasks.values() ] def _run_task(self, task_id): task = self.tasks[task_id] if all(self.tasks[dep_id][\'completed\'] for dep_id in task[\'dependencies\']): task[\'action\'](*task[\'argument\'], **task[\'kwargs\']) task[\'completed\'] = True else: self.scheduler.enterabs(task[\'time\'], task[\'priority\'], self._run_task, (task_id,))"},{"question":"# XML Processing with `xml.dom.pulldom` This assessment focuses on your ability to work with the `xml.dom.pulldom` module in Python to efficiently process XML documents and extract specific data. Problem Statement: You are given an XML document containing information about books in a library. Each book entry consists of elements such as title, author, year, and price. Your task is to process this XML document and print out the titles and authors of books that were published after the year 2000 and have a price greater than 20. Input and Output Formats: * **Input**: - An XML file named `library.xml` structured as follows: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>1999</year> <price>25</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>2005</year> <price>30</price> </book> <!-- more book entries --> </library> ``` * **Output**: - Print the titles and authors of books in the specified format: ``` Title: Book Title 2, Author: Author 2 ``` Constraints: - Use the `xml.dom.pulldom` module to parse the XML file. - Efficiently process the document without loading the entire DOM tree into memory. - Assume the XML structure is well-formed and consistent as shown in the example. Instructions: 1. Write a function `process_library_xml(file_path)` that takes the path to the XML file as input. 2. Use the `xml.dom.pulldom.parse` method to parse the XML file. 3. Loop through the events and nodes, identifying `START_ELEMENT` events for `book` nodes. 4. For each `book` node, extract the `title`, `author`, `year`, and `price` sub-elements. 5. Expand the `book` node to access its children only if the `year` is greater than 2000 and the `price` is greater than 20. 6. Print the titles and authors of the selected books in the specified format. Example: ```python def process_library_xml(file_path): # Your implementation here # Example usage: process_library_xml(\'library.xml\') ``` Expected output for the provided sample XML: ``` Title: Book Title 2, Author: Author 2 ``` Note: Ensure your implementation handles edge cases, such as missing sub-elements within a book entry.","solution":"import xml.dom.pulldom def process_library_xml(file_path): doc = xml.dom.pulldom.parse(file_path) for event, node in doc: if event == xml.dom.pulldom.START_ELEMENT and node.tagName == \'book\': doc.expandNode(node) title = None author = None year = None price = None for child in node.childNodes: if child.nodeType == child.ELEMENT_NODE: if child.tagName == \'title\': title = child.firstChild.data if child.firstChild else \'\' elif child.tagName == \'author\': author = child.firstChild.data if child.firstChild else \'\' elif child.tagName == \'year\': year = int(child.firstChild.data) if child.firstChild else 0 elif child.tagName == \'price\': price = float(child.firstChild.data) if child.firstChild else 0.0 if year and price and year > 2000 and price > 20: print(f\\"Title: {title}, Author: {author}\\")"},{"question":"# Problem Description You are required to create a utility that configures and retrieves MIME type handling commands based on a user\'s custom mailcap file configuration. Task 1. **Create a function `parse_mailcap_entry(entry: str) -> Dict[str, Any]`**: - Input: A string entry from a mailcap file. - Output: A dictionary representing the parsed mailcap entry. The key should be the MIME type, and the value should be another dictionary of key-value pairs for the configuration. 2. **Create a function `load_mailcap(filepath: str) -> Dict[str, List[Dict[str, Any]]]`**: - Input: A filepath to a mailcap file. - Output: A dictionary mapping MIME types to their respective configurations. This dictionary structure should be compatible with the `mailcap.findmatch()` function. 3. **Create a function `generate_command(filepath: str, mimetype: str, key: str = \'view\', filename: str = None, plist: List[str] = []) -> Optional[str]`**: - Input: - `filepath`: Path to the user\'s custom mailcap file. - `mimetype`: The MIME type for which the command needs to be generated. - `key`: The type of activity (default is \'view\'). - `filename`: The filename to be substituted for \\"%s\\" in the command (default is None). - `plist`: A list of parameters. - Output: The command line as a string to be executed for the given MIME type and configuration. If no matching MIME type is found, return `None`. # Constraints - Ensure the solution accurately reads and parses the mailcap file according to the specified format. - Handle exceptions and edge cases gracefully, such as invalid mailcap entries or missing files. - Make sure to account for shell metacharacters and handle them securely to avoid security issues. # Example 1. Sample mailcap file (`user.mailcap`): ``` video/mpeg; xmpeg %s text/html; lynx %s; test=test-html image/jpeg; display %s; compose=editjpeg %s ``` 2. Using the provided functions: ```python filepath = \'user.mailcap\' mimetype = \'video/mpeg\' filename = \'sample.mpeg\' cmd = generate_command(filepath, mimetype, filename=filename) print(cmd) # Output: \'xmpeg sample.mpeg\' ``` 3. For an `image/jpeg` entry with additional parameters: ```python filepath = \'user.mailcap\' mimetype = \'image/jpeg\' key = \'compose\' filename = \'picture.jpeg\' plist = [\'myparam=myvalue\'] cmd = generate_command(filepath, mimetype, key=key, filename=filename, plist=plist) print(cmd) # Output: \'editjpeg picture.jpeg\' ``` # Note Ensure that your code is well-documented, and test cases are included to validate functionality and robustness.","solution":"import os from typing import Dict, Any, List, Optional def parse_mailcap_entry(entry: str) -> Dict[str, Any]: fields = entry.split(\';\') if len(fields) < 2: # Invalid entry, should at least contain mime type and command return {} mime_type = fields[0].strip() entries = { \'view\': fields[1].strip() } for field in fields[2:]: key_value = field.split(\'=\') if len(key_value) == 2: key = key_value[0].strip() value = key_value[1].strip() entries[key] = value return {mime_type: entries} def load_mailcap(filepath: str) -> Dict[str, List[Dict[str, Any]]]: mailcap_dict = {} if not os.path.exists(filepath): return mailcap_dict with open(filepath, \'r\') as file: for line in file: line = line.strip() if not line or line.startswith(\'#\'): continue # Skip empty lines and comments entry = parse_mailcap_entry(line) if entry: mime_type, config = next(iter(entry.items())) if mime_type not in mailcap_dict: mailcap_dict[mime_type] = [] mailcap_dict[mime_type].append(config) return mailcap_dict def generate_command(filepath: str, mimetype: str, key: str = \'view\', filename: str = None, plist: List[str] = []) -> Optional[str]: mailcap_dict = load_mailcap(filepath) if mimetype not in mailcap_dict: return None for config in mailcap_dict[mimetype]: if key in config: command = config[key] if filename: command = command.replace(\'%s\', filename) for param in plist: param_key_value = param.split(\'=\') if len(param_key_value) == 2: param_key, param_value = param_key_value command = command.replace(f\\"%{param_key}%\\", param_value) return command return None"},{"question":"You are tasked with implementing a distributed training setup using PyTorch\'s `torch.distributed` package. The goal is to synchronize a simple model\'s weights across multiple processes using the `all_reduce` collective function. This requires initializing the distributed environment, creating a simple model, and performing the weight synchronization. # Requirements 1. **Initialization**: - Use the `init_process_group` function to initialize the distributed environment with the `NCCL` backend for GPU training. - Assume the use of TCP initialization with `MASTER_ADDR` and `MASTER_PORT` environment variables set. - The code should be able to run on multiple processes, and each process should be aware of its `rank` and the `world_size` (total number of processes). 2. **Model**: - Create a simple neural network model using PyTorch `nn.Module`. 3. **All-Reduce Operation**: - Use the `all_reduce` collective operation to synchronize the model\'s weights across all processes. # Input and Output **Input Format**: - Environment variables `MASTER_ADDR`, `MASTER_PORT`, `WORLD_SIZE`, and `RANK` must be set before running the script. - Specify the rank and world size as inputs to the script if necessary. **Output Format**: - Print the model\'s weights before and after the `all_reduce` operation for each process. # Constraints - Ensure the program can handle multiple processes running on different GPUs (use the NCCL backend). - The initialization should be robust and able to handle potential issues with process synchronization. # Performance - Optimize the synchronization to avoid unnecessary overhead and ensure all processes complete the synchronization. # Example Code Structure ```python import os import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) def init_process(rank, world_size, backend=\'nccl\'): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' dist.init_process_group(backend, rank=rank, world_size=world_size) def main(): rank = int(os.environ[\'RANK\']) world_size = int(os.environ[\'WORLD_SIZE\']) # Initialize the process group init_process(rank, world_size) # Create model and move it to the appropriate device model = SimpleModel().cuda(rank) # Print model\'s weights before all_reduce print(f\\"Rank {rank} before all_reduce: {model.fc.weight}\\") # Perform all_reduce to synchronize weights for param in model.parameters(): dist.all_reduce(param.data, op=dist.ReduceOp.SUM) param.data /= world_size # Print model\'s weights after all_reduce print(f\\"Rank {rank} after all_reduce: {model.fc.weight}\\") # Clean up dist.destroy_process_group() if __name__ == \\"__main__\\": main() ``` # Instructions - Complete the above code to ensure that it handles all the requirements specified. - Test the code with multiple processes to verify the synchronization of model weights across all ranks. - Ensure the code is written in a modular and clean manner, with appropriate handling of errors and synchronization issues.","solution":"import os import torch import torch.distributed as dist import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc = nn.Linear(10, 10) def forward(self, x): return self.fc(x) def init_process(rank, world_size, backend=\'nccl\'): os.environ[\'MASTER_ADDR\'] = os.getenv(\'MASTER_ADDR\', \'localhost\') os.environ[\'MASTER_PORT\'] = os.getenv(\'MASTER_PORT\', \'29500\') dist.init_process_group(backend, rank=rank, world_size=world_size) def main(): rank = int(os.getenv(\'RANK\', 0)) world_size = int(os.getenv(\'WORLD_SIZE\', 1)) # Initialize the process group init_process(rank, world_size) # Create model and move it to the appropriate device model = SimpleModel().cuda(rank) # Print model\'s weights before all_reduce for name, param in model.named_parameters(): print(f\\"Rank {rank} before all_reduce - {name}: {param.data}\\") # Perform all_reduce to synchronize weights for param in model.parameters(): dist.all_reduce(param.data, op=dist.ReduceOp.SUM) param.data /= world_size # Print model\'s weights after all_reduce for name, param in model.named_parameters(): print(f\\"Rank {rank} after all_reduce - {name}: {param.data}\\") # Clean up dist.destroy_process_group() if __name__ == \\"__main__\\": main()"},{"question":"# Terminal Control Application using tty Module You have been tasked with developing a Python application to demonstrate the manipulation of terminal input modes using the `tty` module. This application will allow users to switch between \\"raw\\" and \\"cbreak\\" terminal modes interactively. Task 1. **Implement a function `set_terminal_mode(fd: int, mode: str) -> None`**: - The function should accept a file descriptor `fd` (an integer) and a mode string `mode`. - The mode string will either be `\\"raw\\"` or `\\"cbreak\\"`. - Depending on the mode, the function should use the `tty` module to set the terminal to the specified mode (`raw` or `cbreak`). - Use the default `when` parameter (`termios.TCSAFLUSH`). 2. **Interactivity**: - Write a script that reads a file descriptor from the user and allows them to choose between \\"raw\\" and \\"cbreak\\" mode. - Ensure that invalid file descriptors or modes result in a meaningful error message. - Use the `set_terminal_mode` function to change the terminal mode based on user input. Constraints - Assume the script is running in a Unix environment. - The file descriptor (`fd`) must correspond to a valid terminal file. - Proper error handling should be implemented for invalid inputs. Example ```python def set_terminal_mode(fd: int, mode: str) -> None: import tty import termios if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: raise ValueError(\\"Invalid mode. Mode must be \'raw\' or \'cbreak\'.\\") # Example interactive script if __name__ == \\"__main__\\": import sys fd = sys.stdin.fileno() # This will get the file descriptor for standard input print(\\"Enter mode (\'raw\' or \'cbreak\'):\\") mode = input().strip() try: set_terminal_mode(fd, mode) print(f\\"Terminal set to {mode} mode successfully.\\") except Exception as e: print(f\\"Error: {str(e)}\\") ``` Evaluation Criteria 1. **Correctness**: The function must correctly set the terminal mode based on the input arguments. 2. **Error Handling**: The script should handle invalid inputs gracefully. 3. **Code Quality**: Code should be well-organized and follow Python\'s best practices for readability and maintainability. 4. **Documentation**: Appropriate use of comments to explain the logic and any potential areas of confusion.","solution":"def set_terminal_mode(fd: int, mode: str) -> None: import tty import termios if mode == \'raw\': tty.setraw(fd) elif mode == \'cbreak\': tty.setcbreak(fd) else: raise ValueError(\\"Invalid mode. Mode must be \'raw\' or \'cbreak\'.\\") # Example interactive script if __name__ == \\"__main__\\": import sys fd = sys.stdin.fileno() # This will get the file descriptor for standard input print(\\"Enter mode (\'raw\' or \'cbreak\'):\\") mode = input().strip() try: set_terminal_mode(fd, mode) print(f\\"Terminal set to {mode} mode successfully.\\") except Exception as e: print(f\\"Error: {str(e)}\\")"},{"question":"# PyTorch Coding Assessment: Named Tensors **Objective**: To assess the student\'s understanding of named tensor functionality in PyTorch. # Problem Statement Implement a function `normalize_by_channel` that takes a 4D tensor representing a batch of images and performs channel-wise normalization. The tensor will have named dimensions `(\'N\', \'C\', \'H\', \'W\')` representing batch size, number of channels, height, and width, respectively. The function should compute the mean and standard deviation for each channel across all batches, and then normalize each pixel in a channel by subtracting the channel mean and dividing by the channel standard deviation. # Function Signature ```python def normalize_by_channel(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` # Input - `input_tensor (torch.Tensor)`: A 4D input tensor with named dimensions `(\'N\', \'C\', \'H\', \'W\')`. # Output - Returns a 4D tensor of the same shape with normalized values for each channel. # Example Input ```python import torch input_tensor = torch.randn(2, 3, 4, 4, names=(\'N\', \'C\', \'H\', \'W\')) ``` Output ```python output_tensor = normalize_by_channel(input_tensor) print(output_tensor.names) # Should output: (\'N\', \'C\', \'H\', \'W\') ``` # Constraints 1. You must use the named tensor functionality without losing the names at any operation stage. 2. Avoid converting the tensor to an unnamed tensor. 3. The implementation should be efficient and able to handle large input tensors (batch size and dimensions). # Performance - The solution should handle typical image batch sizes efficiently. - Complexity should ideally be linear with respect to the number of elements in the tensor. # Hints - Use PyTorch functions such as `.mean`, `.std`, `.align_to`, and broadcasting by names for implementation. - Make use of `.align_to` to ensure operations are performed correctly over named dimensions. Happy coding!","solution":"import torch def normalize_by_channel(input_tensor: torch.Tensor) -> torch.Tensor: Normalizes a given 4D input tensor by its channel. Args: input_tensor (torch.Tensor): A 4D tensor with named dimensions (\'N\', \'C\', \'H\', \'W\'). Returns: torch.Tensor: A tensor with the same shape with normalized values for each channel. if input_tensor.names != (\'N\', \'C\', \'H\', \'W\'): raise ValueError(\\"Input tensor must have named dimensions (\'N\', \'C\', \'H\', \'W\')\\") mean = input_tensor.mean(dim=(\'N\', \'H\', \'W\'), keepdim=True) std = input_tensor.std(dim=(\'N\', \'H\', \'W\'), keepdim=True) normalized_tensor = (input_tensor - mean) / (std + 1e-6) # Adding epsilon to avoid division by zero return normalized_tensor"},{"question":"Objective To assess the student\'s understanding of Automatic Mixed Precision (AMP) functionalities in PyTorch, specifically focusing on the usage of `torch.amp.autocast` and `torch.amp.GradScaler` in a multi-model and multi-optimizer training setup. Problem Statement You are given a task to optimize a neural network training process using PyTorch\'s Automatic Mixed Precision (AMP). The goal is to implement a multi-model and multi-optimizer training loop compatible with AMP to improve performance without compromising accuracy. 1. **Model Definitions**: ```python class ModelA(nn.Module): def __init__(self): super(ModelA, self).__init__() self.fc = nn.Linear(512, 256) def forward(self, x): return self.fc(x) class ModelB(nn.Module): def __init__(self): super(ModelB, self).__init__() self.fc = nn.Linear(512, 256) def forward(self, x): return self.fc(x) ``` 2. **Optimizer**: - Use `torch.optim.SGD` for both models with respective learning rates. 3. **Loss Function**: - Use `torch.nn.MSELoss`. Task Implement the `train` function to perform the following steps: 1. Creates instances of `ModelA` and `ModelB`. 2. Initializes two optimizers (one for each model). 3. Sets up a `GradScaler`. 4. For each epoch: - For each batch of data: - Perform forward pass using `autocast`. - Calculate the losses for both models. - Backward passes using the scaled losses. - Optionally unscale gradients and perform gradient clipping. - Update the model parameters using the step method of the scaler for each optimizer. - Update the scaler. Performance Requirements - The solution must efficiently handle mixed precision training. - Ensure that training does not frequent NAN or infinity gradient values. Function Signature ```python def train(modelA, modelB, optimizerA, optimizerB, data_loader, scaler, loss_fn, epoch_count, clip_value): Args: modelA (nn.Module): Instance of ModelA. modelB (nn.Module): Instance of ModelB. optimizerA (torch.optim.Optimizer): Optimizer for ModelA. optimizerB (torch.optim.Optimizer): Optimizer for ModelB. data_loader (torch.utils.data.DataLoader): DataLoader providing input, target pairs. scaler (torch.amp.GradScaler): Gradient scaler instance for AMP. loss_fn (callable): Loss function used for training. epoch_count (int): Number of epochs for training. clip_value (float): Value for gradient clipping. Returns: List[float]: List of average losses for each epoch. pass ``` Constraints 1. You must use `torch.amp.autocast` and `torch.amp.GradScaler` as demonstrated in the documentation. 2. Perform gradient clipping with the specified `clip_value`. 3. Ensure that `scaler.update()` is called only once per epoch after all optimizers\' steps. Example ```python # Create dummy data loader data_loader = [(torch.randn(8, 512), torch.randn(8, 256)) for _ in range(10)] # Instantiate models and optimizers modelA = ModelA().cuda() modelB = ModelB().cuda() optimizerA = torch.optim.SGD(modelA.parameters(), lr=0.01) optimizerB = torch.optim.SGD(modelB.parameters(), lr=0.01) # Define loss function and gradient scaler loss_fn = torch.nn.MSELoss() scaler = torch.amp.GradScaler() # Train models avg_losses = train(modelA, modelB, optimizerA, optimizerB, data_loader, scaler, loss_fn, epoch_count=5, clip_value=1.0) print(avg_losses) ```","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.cuda.amp import autocast, GradScaler def train(modelA, modelB, optimizerA, optimizerB, data_loader, scaler, loss_fn, epoch_count, clip_value): Args: modelA (nn.Module): Instance of ModelA. modelB (nn.Module): Instance of ModelB. optimizerA (torch.optim.Optimizer): Optimizer for ModelA. optimizerB (torch.optim.Optimizer): Optimizer for ModelB. data_loader (torch.utils.data.DataLoader): DataLoader providing input, target pairs. scaler (torch.amp.GradScaler): Gradient scaler instance for AMP. loss_fn (callable): Loss function used for training. epoch_count (int): Number of epochs for training. clip_value (float): Value for gradient clipping. Returns: List[float]: List of average losses for each epoch. epoch_losses = [] for epoch in range(epoch_count): total_loss = 0 for inputs, targets in data_loader: inputs, targets = inputs.cuda(), targets.cuda() optimizerA.zero_grad() optimizerB.zero_grad() with autocast(): outputsA = modelA(inputs) outputsB = modelB(inputs) lossA = loss_fn(outputsA, targets) lossB = loss_fn(outputsB, targets) # Total loss for backpropagation loss = lossA + lossB # Scale the loss and perform backward pass scaler.scale(loss).backward() # Unscale the gradients and clip them if necessary scaler.unscale_(optimizerA) scaler.unscale_(optimizerB) # Optional gradient clipping torch.nn.utils.clip_grad_norm_(modelA.parameters(), clip_value) torch.nn.utils.clip_grad_norm_(modelB.parameters(), clip_value) # Update the weights scaler.step(optimizerA) scaler.step(optimizerB) # Update the scaler scaler.update() total_loss += loss.item() avg_loss = total_loss / len(data_loader) epoch_losses.append(avg_loss) print(f\'Epoch {epoch + 1}/{epoch_count}, Loss: {avg_loss}\') return epoch_losses # Model definitions from the prompt class ModelA(nn.Module): def __init__(self): super(ModelA, self).__init__() self.fc = nn.Linear(512, 256) def forward(self, x): return self.fc(x) class ModelB(nn.Module): def __init__(self): super(ModelB, self).__init__() self.fc = nn.Linear(512, 256) def forward(self, x): return self.fc(x)"},{"question":"# Problem: Enhanced Directory File Organizer You are tasked with creating an enhanced directory file organizer that utilizes the \\"glob\\" module. The task involves organizing files from a given directory (and optionally its subdirectories) into categorized folders based on their file extensions. You will also generate a report of how many files were moved and the destination folders. # Function Signature ```python def organize_files(directory: str, extensions: list = None, recursive: bool = False) -> dict: Organizes files in the specified directory into folders based on their extensions. Args: - directory (str): The path to the directory to be organized. - extensions (list): A list of file extensions to include. If None, organize all files. Default is None. - recursive (bool): If True, include subdirectories iteratively. Default is False. Returns: - dict: A dictionary where keys are the file extensions and values are lists of destination paths of moved files. Constraints: - You may assume that the directory exists and is readable/writable. - If a file with the same name already exists in the destination folder, skip that file. - Limit the search and move operations to reasonable processing time. ``` # Example Usage ```python # Suppose the directory has the following files: # /path/to/directory/ # ├── file1.txt # ├── file2.txt # ├── image1.png # ├── script1.py # └── subdir # ├── file3.txt # ├── image2.png # └── script2.py # Example 1: result = organize_files(\\"/path/to/directory\\", [\\"txt\\", \\"png\\"], recursive=True) print(result) Output could be: { \\"txt\\": [\\"/path/to/directory/txt/file1.txt\\", \\"/path/to/directory/txt/file2.txt\\", \\"/path/to/directory/txt/file3.txt\\"], \\"png\\": [\\"/path/to/directory/png/image1.png\\", \\"/path/to/directory/png/image2.png\\"] } # Example 2: result = organize_files(\\"/path/to/directory\\") print(result) Output could be: { \\"txt\\": [\\"/path/to/directory/txt/file1.txt\\", \\"/path/to/directory/txt/file2.txt\\", \\"/path/to/directory/txt/file3.txt\\"], \\"png\\": [\\"/path/to/directory/png/image1.png\\", \\"/path/to/directory/png/image2.png\\"], \\"py\\": [\\"/path/to/directory/py/script1.py\\", \\"/path/to/directory/py/script2.py\\"] } ``` # Requirements 1. Implement the function `organize_files` as described. 2. Use the `glob` module to match files based on the specified patterns and options. 3. Organize files by moving them into subdirectories named after the file extensions. 4. Generate a dictionary report showing the destination paths of each moved file categorized by extension. # Notes - Ensure that you account for hidden files (starting with \\".\\") appropriately if they match the given extensions. - Handle any potential exceptions that may arise during file operations gracefully. # Constraints - Do not use external libraries other than `os`, `shutil`, and `glob`. - The function should perform efficiently even with a reasonably large number of files.","solution":"import os import shutil import glob def organize_files(directory: str, extensions: list = None, recursive: bool = False) -> dict: Organizes files in the specified directory into folders based on their extensions. Args: - directory (str): The path to the directory to be organized. - extensions (list): A list of file extensions to include. If None, organize all files. Default is None. - recursive (bool): If True, include subdirectories iteratively. Default is False. Returns: - dict: A dictionary where keys are the file extensions and values are lists of destination paths of moved files. Constraints: - You may assume that the directory exists and is readable/writable. - If a file with the same name already exists in the destination folder, skip that file. - Limit the search and move operations to reasonable processing time. organized_files = {} # Ensure the directory is an absolute path directory = os.path.abspath(directory) search_pattern = \'**/*\' if recursive else \'*\' files = glob.glob(os.path.join(directory, search_pattern), recursive=recursive) for file_path in files: if os.path.isfile(file_path): # Extract the file extension _, ext = os.path.splitext(file_path) ext = ext[1:].lower() # remove the dot and convert to lower case if extensions is None or ext in extensions: # Create the folder named by the extension folder_path = os.path.join(directory, ext) if not os.path.exists(folder_path): os.makedirs(folder_path) destination_path = os.path.join(folder_path, os.path.basename(file_path)) # Move the file if it doesn\'t already exist in the destination if not os.path.exists(destination_path): shutil.move(file_path, destination_path) if ext in organized_files: organized_files[ext].append(destination_path) else: organized_files[ext] = [destination_path] return organized_files"},{"question":"**Question: File Synchronization Tool** Your task is to implement a function `synchronize_directories(src: str, dest: str) -> None` that synchronizes two directories. The function should ensure that the destination directory is an exact copy of the source directory. This includes creating, updating, and deleting files and directories in the destination directory to match the source directory. **Requirements:** 1. Use the `pathlib` module for path manipulations. 2. Handle file and directory operations using the `shutil` module. 3. You may use other modules if necessary, but you must not use external libraries. # Function Signature ```python def synchronize_directories(src: str, dest: str) -> None: pass ``` # Input - `src`: A string representing the path to the source directory. - `dest`: A string representing the path to the destination directory. # Output - None # Constraints 1. The function should handle nested directories and files. 2. All operations should be done in a platform-independent manner. 3. Performance is not the primary concern, but the solution should avoid redundant operations. # Example Suppose the following structure for the source (`src`) directory: ``` src/ file1.txt dir1/ file2.txt ``` Suppose the following structure for the destination (`dest`) directory before synchronization: ``` dest/ file1.txt (content differs) file3.txt dir1/ file2.txt (content differs) ``` After calling `synchronize_directories(src, dest)`, the destination (`dest`) directory should have the following structure: ``` dest/ file1.txt (same content as src/file1.txt) dir1/ file2.txt (same content as src/dir1/file2.txt) ``` The redundant file (`file3.txt`) in the destination should be deleted. # Hints: 1. Use functions from the `shutil` module such as `shutil.copy2`, `shutil.copytree`, and `shutil.rmtree` to handle file and directory copying/removal. 2. Utilize the `pathlib.Path` class for elegant handling of paths.","solution":"from pathlib import Path import shutil def synchronize_directories(src: str, dest: str) -> None: src_path = Path(src) dest_path = Path(dest) # Ensure destination exists dest_path.mkdir(parents=True, exist_ok=True) for item in src_path.rglob(\'*\'): rel_path = item.relative_to(src_path) dest_item = dest_path / rel_path if item.is_dir(): dest_item.mkdir(parents=True, exist_ok=True) else: if dest_item.exists(): if dest_item.is_file() and not file_contents_are_equal(item, dest_item): shutil.copy2(item, dest_item) else: shutil.copy2(item, dest_item) for item in dest_path.rglob(\'*\'): rel_path = item.relative_to(dest_path) src_item = src_path / rel_path if not src_item.exists(): if item.is_dir(): shutil.rmtree(item) else: item.unlink() def file_contents_are_equal(file1: Path, file2: Path) -> bool: return file1.read_bytes() == file2.read_bytes()"}]'),A={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:z,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},q={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],N={key:0},O={key:1};function M(s,e,l,m,n,o){const h=_("PoemCard");return a(),i("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>n.searchQuery=r),placeholder:"Search..."},null,512),[[y,n.searchQuery]]),n.searchQuery?(a(),i("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>n.searchQuery="")}," ✕ ")):d("",!0)]),t("div",q,[(a(!0),i(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),i("div",R,' No results found for "'+c(n.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),i("button",{key:0,class:"load-more-button",disabled:n.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[n.isLoading?(a(),i("span",O,"Loading...")):(a(),i("span",N,"See more"))],8,F)):d("",!0)])}const L=p(A,[["render",M],["__scopeId","data-v-f8af23b2"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/55.md","filePath":"deepseek/55.md"}'),j={name:"deepseek/55.md"},B=Object.assign(j,{setup(s){return(e,l)=>(a(),i("div",null,[x(L)]))}});export{Y as __pageData,B as default};
