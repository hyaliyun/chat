import{_ as p,o as a,c as n,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(s,e,l,m,i,o){return a(),n("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-f0d82238"]]),A=JSON.parse('[{"question":"# Question: Handling Special Exceptions in Asynchronous Programming You are tasked with writing an asynchronous function that performs read and write operations on multiple files. This function should handle various exceptions from the `asyncio` module that may be raised during these operations. Function Signature ```python import asyncio async def read_write_async(files: list, data: bytes, timeout: float) -> dict: Reads from and writes to a list of files asynchronously while handling specific asyncio exceptions. Parameters: files (list): A list of file paths (strings) to be read from and written to. data (bytes): Data to write to each file. timeout (float): Maximum time (in seconds) to wait for each read/write operation. Returns: dict: A dictionary with file paths as keys and a tuple containing two elements: (success: bool, error: str) - Whether the operation was successful, and any error message if not. ``` Requirements and Constraints 1. For each file in the `files` list: - Perform a read operation first. - If the read operation is successful and no exceptions are raised, perform a write operation with the provided `data`. 2. Handle the following exceptions specifically: - `asyncio.TimeoutError`: Handle the timeout explicitly and return an appropriate error message. - `asyncio.CancelledError`: Ensure any custom cleanup or log operation is performed and re-raise the exception. - `asyncio.InvalidStateError`: Return an appropriate error message if this error occurs. - `asyncio.SendfileNotAvailableError`: Return an appropriate error message if this error occurs. - `asyncio.IncompleteReadError`: Use the attributes `expected` and `partial` to format the error message. - `asyncio.LimitOverrunError`: Use the `consumed` attribute to format the error message. 3. If an unknown exception is raised, handle it gracefully and include its details in the error message. 4. Ensure that each file operation respects the provided timeout value. If an operation exceeds the timeout, it should be handled with the `asyncio.TimeoutError`. 5. The function should return a dictionary where the keys are file paths and the values are tuples: - `success`: A boolean indicating whether the operation was successful. - `error`: A string describing any error encountered during the operation. Example Usage ```python import asyncio async def main(): files = [\\"file1.txt\\", \\"file2.txt\\", \\"file3.txt\\"] data = b\\"Example data\\" timeout = 5.0 result = await read_write_async(files, data, timeout) print(result) # Run the example asyncio.run(main()) ``` Your implementation should ensure robustness in the face of various exceptions and demonstrate a good understanding of asynchronous programming with the `asyncio` module in Python.","solution":"import asyncio async def read_file(file_path, timeout): try: with open(file_path, \\"rb\\") as file: return await asyncio.wait_for(file.read(), timeout=timeout) except asyncio.TimeoutError: return None, \\"Read operation timed out\\" except asyncio.CancelledError as e: return None, \\"Read operation was cancelled\\" except asyncio.InvalidStateError: return None, \\"Invalid state error occurred\\" except asyncio.SendfileNotAvailableError: return None, \\"SendfileNotAvailableError occurred\\" except asyncio.IncompleteReadError as e: return None, f\\"Incomplete read error: expected {e.expected}, got {e.partial}\\" except asyncio.LimitOverrunError as e: return None, f\\"Limit overrun error, consumed: {e.consumed}\\" except Exception as e: return None, f\\"Unexpected error: {str(e)}\\" async def write_file(file_path, data, timeout): try: with open(file_path, \\"wb\\") as file: return await asyncio.wait_for(file.write(data), timeout=timeout) except asyncio.TimeoutError: return False, \\"Write operation timed out\\" except asyncio.CancelledError as e: return False, \\"Write operation was cancelled\\" except asyncio.InvalidStateError: return False, \\"Invalid state error occurred\\" except asyncio.SendfileNotAvailableError: return False, \\"SendfileNotAvailableError occurred\\" except asyncio.IncompleteReadError as e: return False, f\\"Incomplete read error: expected {e.expected}, got {e.partial}\\" except asyncio.LimitOverrunError as e: return False, f\\"Limit overrun error, consumed: {e.consumed}\\" except Exception as e: return False, f\\"Unexpected error: {str(e)}\\" async def read_write_async(files: list, data: bytes, timeout: float) -> dict: results = {} for file_path in files: read_result, read_error = await read_file(file_path, timeout) if read_result is not None: write_result, write_error = await write_file(file_path, data, timeout) results[file_path] = (write_result is not False, write_error if write_error else None) else: results[file_path] = (False, read_error) return results"},{"question":"Kernel Density Estimation with Scikit-learn Objective In this task, you are required to implement kernel density estimation using the scikit-learn library to analyze a given dataset. Your solution should demonstrate the ability to use different kernel functions, adjust bandwidth parameters, and visualize the resulting density estimates. Instructions 1. **Data Generation:** - Generate a dataset of 200 random points drawn from a bimodal distribution. Specifically, create two sets of 100 points each drawn from normal distributions `N(-2, 0.5)` and `N(2, 0.5)`. 2. **Kernel Density Estimation:** - Implement kernel density estimation using the `KernelDensity` class from `sklearn.neighbors`. - Create KDE models for three different kernels: Gaussian, Tophat, and Epanechnikov. - Use a bandwidth parameter of 0.5 for each model. 3. **Visualization:** - For each kernel, visualize the resulting density estimates over a range of values. Ensure to plot the density estimate for the entire data range from `-5` to `5` with 1000 equally spaced points. - Use subplots to display the density estimates for the different kernels in a single figure for easy comparison. 4. **Evaluation:** - Calculate and print the log-likelihood of the data under each of the three KDE models. Expected Output - Three visuals (subplots in one figure) comparing the kernel density estimates for Gaussian, Tophat, and Epanechnikov kernels. - Log-likelihood values for each KDE model printed to the console. Constraints and Considerations - Ensure your code is efficient and well-documented. - Use the specified bandwidth value. - Ensure the dataset generation is reproducible by setting a random seed. Example ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Data Generation np.random.seed(42) data1 = np.random.normal(-2, 0.5, 100) data2 = np.random.normal(2, 0.5, 100) X = np.concatenate([data1, data2])[:, np.newaxis] # Kernels to evaluate kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] # Bandwidth bandwidth = 0.5 # Visualization setup x_d = np.linspace(-5, 5, 1000)[:, np.newaxis] plt.figure(figsize=(15, 5)) # Kernel Density Estimation and Visualization for i, kernel in enumerate(kernels): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) log_dens = kde.score_samples(x_d) plt.subplot(1, 3, i + 1) plt.fill(x_d, np.exp(log_dens), alpha=0.5) plt.plot(X[:, 0], -0.01 - 0.02 * np.random.random(X.shape[0]), \'|k\') plt.title(f\'Kernel: {kernel}\') print(f\'Log-likelihood for kernel {kernel}: {kde.score(X)}\') plt.subplots_adjust(hspace=0.4) plt.show() ``` Ensure your solution is complete and well-commented with appropriate function and variable names.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kernel_density_estimation(): # Data Generation np.random.seed(42) data1 = np.random.normal(-2, 0.5, 100) data2 = np.random.normal(2, 0.5, 100) X = np.concatenate([data1, data2])[:, np.newaxis] # Kernels to evaluate kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] # Bandwidth bandwidth = 0.5 # Visualization setup x_d = np.linspace(-5, 5, 1000)[:, np.newaxis] plt.figure(figsize=(15, 5)) # Dictionary to store log-likelihoods log_likelihoods = {} # Kernel Density Estimation and Visualization for i, kernel in enumerate(kernels): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(X) log_dens = kde.score_samples(x_d) plt.subplot(1, 3, i + 1) plt.fill(x_d, np.exp(log_dens), alpha=0.5) plt.plot(X[:, 0], -0.01 - 0.02 * np.random.random(X.shape[0]), \'|k\') plt.title(f\'Kernel: {kernel}\') log_likelihoods[kernel] = kde.score(X) print(f\'Log-likelihood for kernel {kernel}: {kde.score(X)}\') plt.subplots_adjust(wspace=0.4) plt.show() return log_likelihoods"},{"question":"**Question: Implementing and Evaluating an SGD Classifier** **Objective:** Demonstrate your understanding of Scikit-learn\'s `SGDClassifier` by implementing a classification model for the popular Iris dataset. Your solution should showcase your ability to preprocess data, implement the classifier, tune hyperparameters, and evaluate the model\'s performance. **Task:** 1. **Data Loading and Preprocessing:** - Load the Iris dataset from `sklearn.datasets`. - Split the dataset into features `X` and target `y`. - Split the data into training and testing sets using a 80/20 split. - Perform feature scaling using `StandardScaler`. 2. **Model Implementation:** - Initialize an `SGDClassifier` with the following parameters: - `loss`: `\'hinge\'` - `penalty`: `\'l2\'` - `max_iter`: `1000` - `tol`: `1e-3` - `random_state`: `42` - Fit the classifier on the training data. 3. **Hyperparameter Tuning:** - Use `GridSearchCV` to find the best hyperparameters for the following: - `alpha`: `[0.0001, 0.001, 0.01, 0.1, 1]` - `learning_rate`: `[\'constant\', \'optimal\', \'invscaling\', \'adaptive\']` - Evaluate the tuned model using 5-fold cross-validation. 4. **Model Evaluation:** - Evaluate the performance of the model on the test set using the following metrics: - Accuracy - Confusion Matrix - Classification Report (Precision, Recall, F1-score for each class) **Constraints and Requirements:** - **Libraries:** You should use `scikit-learn` for all implementations. - **Performance:** The implementation and evaluation should complete within a reasonable amount of time on a typical laptop/desktop. - **Code Quality:** Ensure your code is well-organized, commented, and follows best practices. **Expected Input and Output:** **Input:** - None (The script should load the Iris dataset internally) **Output:** - Best hyperparameters from `GridSearchCV` - Accuracy of the model on the test set - Confusion Matrix and Classification Report **Example Code Structure:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, confusion_matrix, classification_report import numpy as np # 1. Data Loading and Preprocessing iris = load_iris() X = iris.data y = iris.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Feature scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # 2. Model Implementation sgd = SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) sgd.fit(X_train, y_train) # 3. Hyperparameter Tuning param_grid = { \'alpha\': [0.0001, 0.001, 0.01, 0.1, 1], \'learning_rate\': [\'constant\', \'optimal\', \'invscaling\', \'adaptive\'] } grid_search = GridSearchCV(estimator=sgd, param_grid=param_grid, cv=5) grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ print(\\"Best hyperparameters:\\", best_params) # Re-train model with best parameters best_sgd = grid_search.best_estimator_ y_pred = best_sgd.predict(X_test) # 4. Model Evaluation accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) class_report = classification_report(y_test, y_pred) print(\\"Accuracy:\\", accuracy) print(\\"Confusion Matrix:n\\", conf_matrix) print(\\"Classification Report:n\\", class_report) ``` **Evaluation Criteria:** - Correctness of the implementation. - Proper preprocessing and handling of data. - Efficient hyperparameter tuning and model evaluation. - Clarity and organization of the code. - Detailed and accurate model evaluation.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score, confusion_matrix, classification_report def load_and_preprocess_data(): iris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_sgd_classifier(X_train, y_train): sgd = SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) sgd.fit(X_train, y_train) return sgd def tune_hyperparameters(X_train, y_train): param_grid = { \'alpha\': [0.0001, 0.001, 0.01, 0.1, 1], \'learning_rate\': [\'constant\', \'optimal\', \'invscaling\', \'adaptive\'] } sgd = SGDClassifier(loss=\'hinge\', penalty=\'l2\', max_iter=1000, tol=1e-3, random_state=42) grid_search = GridSearchCV(estimator=sgd, param_grid=param_grid, cv=5) grid_search.fit(X_train, y_train) return grid_search.best_estimator_, grid_search.best_params_ def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) class_report = classification_report(y_test, y_pred) return accuracy, conf_matrix, class_report # Main execution X_train, X_test, y_train, y_test = load_and_preprocess_data() sgd = train_sgd_classifier(X_train, y_train) best_model, best_params = tune_hyperparameters(X_train, y_train) print(\\"Best hyperparameters:\\", best_params) # Evaluate the tuned model accuracy, conf_matrix, class_report = evaluate_model(best_model, X_test, y_test) print(\\"Accuracy:\\", accuracy) print(\\"Confusion Matrix:n\\", conf_matrix) print(\\"Classification Report:n\\", class_report)"},{"question":"**Question: Manipulating Sun AU Audio Files** You are tasked with processing Sun AU audio files using the `sunau` module. Your goal is to write a program that reads an input AU audio file, modifies it by doubling the sample rate, and writes the result to a new AU file. # Function Signature ```python def process_au_file(input_file: str, output_file: str) -> None: Reads an AU audio file from the provided input path, doubles the sample rate, and writes the modified audio to the provided output path. :param input_file: Path to the input AU file. :param output_file: Path to the output AU file. pass ``` # Requirements 1. **Reading the Input File**: Use `sunau.open` to open the input AU file in read mode. 2. **Extract Audio Parameters**: Extract necessary audio parameters such as the number of channels, sample width, original sample rate, and number of frames. 3. **Modify Parameters**: Double the original sample rate. 4. **Read Audio Data**: Read the audio frames from the input file. 5. **Write to Output File**: Use `sunau.open` to write the modified audio data to the output AU file with the new parameters. # Constraints - Assume the input file is always a valid AU file. - You must handle files that have uncompressed PCM data (i.e., comptype is \'NONE\'). - The input file can have mono or stereo audio channels. # Example Given an input AU file `input.au`, double its sample rate and write the result to `output.au`. Your solution should demonstrate a correct and efficient method to achieve the requirements using the `sunau` module. # Additional Information You can assume the necessary imports and that the `sunau` module is available in the environment where your code will run.","solution":"import sunau def process_au_file(input_file: str, output_file: str) -> None: Reads an AU audio file from the provided input path, doubles the sample rate, and writes the modified audio to the provided output path. :param input_file: Path to the input AU file. :param output_file: Path to the output AU file. with sunau.open(input_file, \'rb\') as infile: n_channels = infile.getnchannels() sampwidth = infile.getsampwidth() samp_rate = infile.getframerate() n_frames = infile.getnframes() comptype = infile.getcomptype() compname = infile.getcompname() # Read audio data audio_data = infile.readframes(n_frames) new_samp_rate = samp_rate * 2 with sunau.open(output_file, \'wb\') as outfile: outfile.setnchannels(n_channels) outfile.setsampwidth(sampwidth) outfile.setframerate(new_samp_rate) outfile.setnframes(n_frames) outfile.setcomptype(comptype, compname) # Write audio data with modified sample rate outfile.writeframes(audio_data)"},{"question":"**Question: Implement a File Filter Using fnmatch** You are tasked with implementing a function `filter_files(filenames, patterns)` that filters a list of filenames, keeping only those that match any of the given Unix shell-style wildcard patterns. # Requirements: - The function should handle multiple patterns. A filename matches if it meets at least one of the patterns. - Implement case-insensitive matching. - Use the `fnmatch` module for pattern matching. # Function Signature: ```python def filter_files(filenames: list[str], patterns: list[str]) -> list[str]: Filters filenames based on multiple patterns. Args: filenames (list of str): List of filenames to be filtered. patterns (list of str): List of Unix shell-style patterns to match. Returns: list of str: List of filenames that match any of the patterns. ``` # Input: - `filenames` is a list of strings representing filenames (1 ≤ len(filenames) ≤ 1000). - `patterns` is a list of strings representing patterns (1 ≤ len(patterns) ≤ 10). # Output: - The function should return a list of strings representing the filenames that match at least one of the provided patterns. # Example: ```python filenames = [\\"data1.csv\\", \\"data2.txt\\", \\"image.png\\", \\"report.pdf\\", \\"Data3.CSV\\"] patterns = [\\"*.csv\\", \\"*.CSV\\", \\"*.txt\\"] result = filter_files(filenames, patterns) print(result) # Output: [\'data1.csv\', \'data2.txt\', \'Data3.CSV\'] ``` # Constraints: - The solution should be efficient enough to handle the given constraints. - Ensure that you use the `fnmatch` module appropriately to match the patterns against the filenames. # Hints: 1. You may use the `fnmatch.fnmatch` function for pattern matching. 2. Iterate over filenames and check against each pattern to see if any pattern matches.","solution":"import fnmatch def filter_files(filenames, patterns): Filters filenames based on multiple patterns. Args: filenames (list of str): List of filenames to be filtered. patterns (list of str): List of Unix shell-style patterns to match. Returns: list of str: List of filenames that match any of the patterns. matched_files = [] lower_patterns = [pattern.lower() for pattern in patterns] for filename in filenames: lower_filename = filename.lower() if any(fnmatch.fnmatch(lower_filename, pattern) for pattern in lower_patterns): matched_files.append(filename) return matched_files"},{"question":"Title: Statistical Analysis and Random Data Generation Objective: Write a Python function that generates a list of pseudo-random decimal numbers and performs statistical analysis on them. Description: You need to create a function `generate_and_analyze_data` that accepts three parameters: the number of random decimals to generate (`n`), the precision of these decimal numbers (`precision`), and the seed for the random number generator (`seed`). The function should: 1. Generate `n` pseudo-random decimal numbers between 0 and 1, ensuring that the decimal numbers have exactly `precision` decimal places. 2. Calculate and return the mean, median, variance, and standard deviation of the generated numbers. Input: - `n` (int): The number of random decimal numbers to generate (1 ≤ n ≤ 10^6). - `precision` (int): The precision (number of decimal places) for the random decimal numbers (1 ≤ precision ≤ 10). - `seed` (int): The seed for the random number generator to ensure reproducibility. Output: A dictionary with the following keys and their corresponding values: - `mean` (float): The mean of the generated numbers. - `median` (float): The median of the generated numbers. - `variance` (float): The variance of the generated numbers. - `std_dev` (float): The standard deviation of the generated numbers. Constraints: - Use the `decimal` module to ensure correct precision. - Use the `random` module for random number generation. - Use the `statistics` module for computing the mean, median, variance, and standard deviation. Example: ```python from decimal import Decimal, getcontext def generate_and_analyze_data(n, precision, seed): import random import statistics from decimal import Decimal, getcontext # Set precision getcontext().prec = precision # Set seed for reproducibility random.seed(seed) # Generate random decimals random_decimals = [Decimal(random.random()).quantize(Decimal(10) ** -precision) for _ in range(n)] # Calculate statistics mean = float(statistics.mean(random_decimals)) median = float(statistics.median(random_decimals)) variance = float(statistics.variance(random_decimals)) std_dev = float(statistics.stdev(random_decimals)) return { \'mean\': mean, \'median\': median, \'variance\': variance, \'std_dev\': std_dev } # Example usage: result = generate_and_analyze_data(1000, 5, 42) print(result) ``` Note: - Ensure that the precision of the random decimal numbers is correctly handled. - The seed value should ensure that the function produces the same output for the same inputs. This question tests the students\' ability to integrate different concepts and modules to solve a complex problem, emphasizing precision handling and statistical computations.","solution":"from decimal import Decimal, getcontext import random import statistics def generate_and_analyze_data(n, precision, seed): Generates a list of pseudo-random decimal numbers with given precision and calculates mean, median, variance, and standard deviation. Args: - n (int): Number of random decimal numbers to generate. - precision (int): Number of decimal places for random numbers. - seed (int): Seed for the random number generator. Returns: - dict: A dictionary containing mean, median, variance, and std_dev. # Set precision for Decimal getcontext().prec = precision # Set seed for random number generation random.seed(seed) # Generate n random decimal numbers with the given precision random_decimals = [Decimal(random.random()).quantize(Decimal(10) ** -precision) for _ in range(n)] # Calculate statistical measures mean = float(statistics.mean(random_decimals)) median = float(statistics.median(random_decimals)) variance = float(statistics.variance(random_decimals)) std_dev = float(statistics.stdev(random_decimals)) # Return result as a dictionary return { \'mean\': mean, \'median\': median, \'variance\': variance, \'std_dev\': std_dev }"},{"question":"You are tasked with implementing a Python class that emulates some basic and advanced numeric operations for its instances. The operations are primarily reflective of standard arithmetic and bitwise procedures, as well as type conversions. Class Design Implement a class named `NumericOperations` with the following methods: 1. **`__init__(self, value)`**: Constructor that initializes the object with a numeric value (int or float). 2. **`add(self, other)`**: Adds the instance value to `other`. 3. **`subtract(self, other)`**: Subtracts `other` from the instance value. 4. **`multiply(self, other)`**: Multiplies the instance value by `other`. 5. **`floordiv(self, other)`**: Performs floor division of the instance value by `other`. 6. **`truediv(self, other)`**: Performs true division of the instance value by `other`. 7. **`remainder(self, other)`**: Computes the remainder of the instance value divided by `other`. 8. **`power(self, other)`**: Raises the instance value to the power of `other`. 9. **`negate(self)`**: Negates the instance value. 10. **`bitwise_and(self, other)`**: Computes the bitwise AND of the instance value with `other`. 11. **`bitwise_or(self, other)`**: Computes the bitwise OR of the instance value with `other`. 12. **`to_int(self)`**: Converts the instance value to an integer. 13. **`to_float(self)`**: Converts the instance value to a float. 14. **`convert_base(self, base)`**: Converts the integer instance value to a string representation in a specified base (2, 8, 10, 16). Constraints 1. The `other` parameter is always a numeric value (int or float). 2. Raise appropriate errors if operations are not valid (e.g., division by zero). 3. For bitwise operations, the instance value and `other` must be integers. Raise `ValueError` otherwise. 4. Ensure proper handling of integer and float conversions. 5. `convert_base` should return strings prefixed with \\"0b\\", \\"0o\\", or \\"0x\\" for bases 2, 8, and 16 respectively. Ensure the instance value is an integer before performing this conversion. Example Usage ```python num1 = NumericOperations(10) num2 = NumericOperations(20.5) print(num1.add(5)) # 15 print(num1.subtract(2)) # 8 print(num1.multiply(3)) # 30 print(num1.floordiv(3)) # 3 print(num1.truediv(4)) # 2.5 print(num1.remainder(3)) # 1 print(num1.power(2)) # 100 print(num1.negate()) # -10 print(num1.bitwise_and(6)) # 2 print(num1.bitwise_or(6)) # 14 print(num1.to_int()) # 10 print(num1.to_float()) # 10.0 print(num1.convert_base(2)) # \'0b1010\' print(num2.convert_base(8)) # ValueError: Instance value must be an integer for base conversion ``` You are required to implement the class definition and ensure each method works according to the specifications provided above.","solution":"class NumericOperations: def __init__(self, value): self.value = value def add(self, other): return self.value + other def subtract(self, other): return self.value - other def multiply(self, other): return self.value * other def floordiv(self, other): if other == 0: raise ZeroDivisionError(\\"Division by zero is not allowed\\") return self.value // other def truediv(self, other): if other == 0: raise ZeroDivisionError(\\"Division by zero is not allowed\\") return self.value / other def remainder(self, other): if other == 0: raise ZeroDivisionError(\\"Division by zero is not allowed\\") return self.value % other def power(self, other): return self.value ** other def negate(self): return -self.value def bitwise_and(self, other): if not isinstance(self.value, int) or not isinstance(other, int): raise ValueError(\\"Both the instance value and \'other\' must be integers for bitwise operations\\") return self.value & other def bitwise_or(self, other): if not isinstance(self.value, int) or not isinstance(other, int): raise ValueError(\\"Both the instance value and \'other\' must be integers for bitwise operations\\") return self.value | other def to_int(self): return int(self.value) def to_float(self): return float(self.value) def convert_base(self, base): if not isinstance(self.value, int): raise ValueError(\\"Instance value must be an integer for base conversion\\") if base == 2: return bin(self.value) elif base == 8: return oct(self.value) elif base == 10: return str(self.value) elif base == 16: return hex(self.value) else: raise ValueError(\\"Unsupported base. Supported bases are 2, 8, 10, and 16\\")"},{"question":"# Objective: Implement a function using PyTorch\'s `torch.distributed.elastic.multiprocessing` to perform a distributed computation where multiple worker processes independently compute parts of a task and then aggregate the results. # Description: You are required to implement a function `distributed_sum` that performs the summation of a range of integers using multiple subprocesses. 1. **Function Signature:** ```python def distributed_sum(start: int, end: int, num_workers: int) -> int: Calculate the sum of integers in the range [start, end) using `num_workers` parallel processes. Args: - start (int): The starting integer (inclusive). - end (int): The ending integer (exclusive). - num_workers (int): The number of worker processes to use. Returns: - int: The sum of integers from `start` to `end-1`. ``` 2. **Function Details:** - The function should divide the range `[start, end)` into approximately equal parts, each to be processed by one worker. - Each worker will compute the sum of its respective subrange and return it. - Use `torch.distributed.elastic.multiprocessing.start_processes` to start the worker processes. - Collect the sums from all workers and return the total sum. 3. **Constraints:** - You must use PyTorch\'s `torch.distributed.elastic.multiprocessing` to handle process management. - Ensure that the solution is efficient and can handle large ranges and a high number of processes. 4. **Example:** ```python if __name__ == \\"__main__\\": result = distributed_sum(1, 10, 3) print(result) # Expected output: 45 (1+2+3+4+5+6+7+8+9) ``` # Notes: - The function should be robust and handle edge cases, such as when `start` is equal to `end`. - Ensure proper handling of process cleanup and exceptions. - You can assume input values are well-formed integers with `start` <= `end` and `num_workers > 0`.","solution":"import torch import torch.multiprocessing as mp def worker_sub_sum(start, end, return_dict, worker_id): return_dict[worker_id] = sum(range(start, end)) def distributed_sum(start: int, end: int, num_workers: int) -> int: if start >= end: return 0 total_range = end - start chunk_size = (total_range + num_workers - 1) // num_workers processes = [] manager = mp.Manager() return_dict = manager.dict() for i in range(num_workers): sub_start = start + i * chunk_size sub_end = min(start + (i + 1) * chunk_size, end) if sub_start >= sub_end: break # No more work to distribute p = mp.Process(target=worker_sub_sum, args=(sub_start, sub_end, return_dict, i)) processes.append(p) p.start() for p in processes: p.join() total_sum = sum(return_dict.values()) return total_sum"},{"question":"# Pandas Advanced Operations and Memory Management **Objective** You are required to create a DataFrame with multiple data types, perform various operations including memory usage reporting, handle boolean values properly, manage missing values, and avoid mutation pitfalls when using User Defined Functions (UDFs). **Task Description** 1. **Create a DataFrame**: - Construct a DataFrame `df` with at least the following columns: - `int_col`: containing integers. - `float_col`: containing floats. - `datetime_col`: containing datetime objects. - `timedelta_col`: containing timedelta values. - `complex_col`: containing complex numbers. - `object_col`: containing strings. - `bool_col`: containing boolean values. - Add 1000 rows of random values to each column. Ensure the `object_col` contains some repetitive values that can be categorized later. 2. **Memory Usage**: - Display the memory usage of the entire DataFrame in a human-readable format using `df.info()`. - Calculate and display the deep memory usage of the DataFrame. 3. **Handling Booleans**: - Implement a function `check_any_true` that accepts the DataFrame and a column name, and returns `True` if any value in that column is `True` (or equivalent), otherwise `False`. - Do not convert the Series directly to a boolean. 4. **Avoiding Mutation in UDFs**: - Write a function `non_mutating_transform` that takes a DataFrame and returns a modified version of the DataFrame where: - Any value in `int_col` that is even is incremented by 1. - Any string in `object_col` is converted to uppercase. - Ensure that the original DataFrame is not mutated during this operation. 5. **Handling Missing Values**: - Introduce some missing values (NaNs) in the `float_col` and `int_col`. - Implement a function `fill_missing_values` that fills missing values with the mean of the column for `float_col` and with the median of the column for `int_col`. **Constraints** - Ensure that the DataFrame contains exactly 1000 rows. - Use pandas functionalities effectively to optimize operations. - Do not mutate the original DataFrame in any of the functions (except where explicitly required). **Function Signatures** ```python import pandas as pd import numpy as np # Function to construct the DataFrame def create_dataframe() -> pd.DataFrame: pass # Function to display memory usage def display_memory_usage(df: pd.DataFrame) -> None: pass # Function to calculate deep memory usage def calculate_deep_memory_usage(df: pd.DataFrame) -> int: pass # Function to check if any value is True in the specified column def check_any_true(df: pd.DataFrame, column: str) -> bool: pass # Function to apply non-mutating transform using UDF def non_mutating_transform(df: pd.DataFrame) -> pd.DataFrame: pass # Function to fill missing values def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Expected Output** - Properly formatted output for memory usage. - Correct filling of missing values. - Properly structure and function implementation to return the expected results.","solution":"import pandas as pd import numpy as np def create_dataframe() -> pd.DataFrame: np.random.seed(0) data = { \'int_col\': np.random.randint(0, 100, 1000), \'float_col\': np.random.randn(1000), \'datetime_col\': pd.date_range(start=\'2023-01-01\', periods=1000, freq=\'D\'), \'timedelta_col\': pd.timedelta_range(start=\'1 days\', periods=1000, freq=\'H\'), \'complex_col\': np.random.randn(1000) + 1j * np.random.randn(1000), \'object_col\': np.random.choice([\'a\', \'b\', \'c\', \'d\'], 1000), \'bool_col\': np.random.choice([True, False], 1000) } df = pd.DataFrame(data) return df def display_memory_usage(df: pd.DataFrame) -> None: print(df.info(memory_usage=\'deep\')) def calculate_deep_memory_usage(df: pd.DataFrame) -> int: return df.memory_usage(deep=True).sum() def check_any_true(df: pd.DataFrame, column: str) -> bool: return df[column].any() def non_mutating_transform(df: pd.DataFrame) -> pd.DataFrame: transformed_df = df.copy() transformed_df[\'int_col\'] = transformed_df[\'int_col\'].apply(lambda x: x + 1 if x % 2 == 0 else x) transformed_df[\'object_col\'] = transformed_df[\'object_col\'].str.upper() return transformed_df def fill_missing_values(df: pd.DataFrame) -> pd.DataFrame: filled_df = df.copy() filled_df[\'float_col\'].fillna(filled_df[\'float_col\'].mean(), inplace=True) filled_df[\'int_col\'].fillna(filled_df[\'int_col\'].median(), inplace=True) return filled_df"},{"question":"Objective: Write a Python script to define a class for representing a `Book` using Python\'s `dataclasses` module. The `Book` class will facilitate the management of a mini-library system. Requirements: 1. **Class Name:** `Book` 2. **Attributes:** - `title` (str): The title of the book. - `author` (str): The author of the book. - `year` (int): The year the book was published. - `checked_out` (bool): Whether the book is currently checked out (default: False). 3. Use `dataclasses` to define the `Book` class. 4. Implement post-init processing to ensure that the `year` attribute is not in the future (i.e., greater than the current year). 5. Define a method `checkout` to mark a book as checked out. If the book is already checked out, raise a `ValueError`. 6. Define a method `return_book` to mark a book as returned (i.e., not checked out). 7. Implement a `__str__` method to return a user-friendly representation of the book. Constraints: - You may assume standard library imports are allowed. - Year cannot be in the future relative to the current year. - Titles and authors are non-empty strings. Example: ```python from book_module import Book import datetime book = Book(title=\'1984\', author=\'George Orwell\', year=1949) print(book) # Should display book details book.checkout() print(book.checked_out) # Should be True book.return_book() print(book.checked_out) # Should be False try: book.checkout() book.checkout() # Should raise ValueError because the book is already checked out except ValueError as e: print(e) ``` Input: You do not need to handle input; rather, the class will be tested through function calls. Output: Proper functioning of methods and class definition will be tested.","solution":"from dataclasses import dataclass, field from datetime import datetime @dataclass class Book: title: str author: str year: int checked_out: bool = field(default=False) def __post_init__(self): current_year = datetime.now().year if self.year > current_year: raise ValueError(f\\"Year cannot be in the future. Year provided: {self.year}\\") if not self.title: raise ValueError(\\"Title cannot be an empty string\\") if not self.author: raise ValueError(\\"Author cannot be an empty string\\") def checkout(self): if self.checked_out: raise ValueError(\\"The book is already checked out\\") self.checked_out = True def return_book(self): self.checked_out = False def __str__(self): status = \\"Checked out\\" if self.checked_out else \\"Available\\" return f\\"Title: {self.title}, Author: {self.author}, Year: {self.year}, Status: {status}\\""},{"question":"Objective Implement a function that can dynamically parse a list of arguments based on provided types and then construct a corresponding dictionary where keys are argument names and values are parsed accordingly. The function needs to parse different types of arguments including strings, integers, and float numbers. Task Write a function `parse_and_build` that receives two inputs: 1. A list of argument specifications (`arg_specs`), where each specification is a tuple containing an argument name and its expected type (`\'str\'`, `\'int\'`, or `\'float\'`). 2. A list of values (`values`), where the index corresponds to the argument specifications provided in the first input. The function should return a dictionary with the argument names as keys and the parsed values as shown below. Function Signature ```python def parse_and_build(arg_specs: list[tuple[str, str]], values: list[str]) -> dict: pass ``` Input 1. `arg_specs`: A list of tuples, where each tuple contains: - `name` (str): The name of the argument. - `type` (str): The type of the argument, which can be `\'str\'`, `\'int\'`, or `\'float\'`. 2. `values`: A list of strings, representing the values to be parsed and assigned to the corresponding arguments. Output Return a dictionary with argument names as keys and their parsed values as values. Constraints - The length of `arg_specs` will be equal to the length of `values`. - Types in `arg_specs` will always be one of `\'str\'`, `\'int\'`, or `\'float\'`. Examples Example 1: ```python arg_specs = [(\'name\', \'str\'), (\'age\', \'int\'), (\'height\', \'float\')] values = [\'Alice\', \'30\', \'5.5\'] output = parse_and_build(arg_specs, values) # output should be {\'name\': \'Alice\', \'age\': 30, \'height\': 5.5} ``` Example 2: ```python arg_specs = [(\'city\', \'str\'), (\'population\', \'int\')] values = [\'New York\', \'8419000\'] output = parse_and_build(arg_specs, values) # output should be {\'city\': \'New York\', \'population\': 8419000} ``` # Note Implement error handling to manage cases where the values cannot be parsed to the specified type, and in such cases, assign `None` to that argument in the resulting dictionary. Performance Requirements - The function should handle up to 1000 arguments and values efficiently. - Aim for a time complexity of O(n), where n is the number of arguments/values.","solution":"def parse_and_build(arg_specs: list[tuple[str, str]], values: list[str]) -> dict: parsed_dict = {} for index, (name, type_str) in enumerate(arg_specs): value = values[index] try: if type_str == \'str\': parsed_value = str(value) elif type_str == \'int\': parsed_value = int(value) elif type_str == \'float\': parsed_value = float(value) parsed_dict[name] = parsed_value except ValueError: parsed_dict[name] = None return parsed_dict"},{"question":"Suppose you are developing a tool to analyze the Unix group database on a system. Your task is to implement a function that provides insights about the group membership in the system. Specifically, you need to write a function called `group_members_statistics` that returns a dictionary with the following information: 1. **Total number of groups**. 2. **Groups with no members**. 3. **Groups with the highest number of members**. 4. **Groups with exactly one member**. Implement the function `group_members_statistics` which accepts no arguments and returns a dictionary with the keys: - `\\"total_groups\\"`: an integer representing the total number of groups. - `\\"no_members\\"`: a list of group names that have no members. - `\\"most_members\\"`: a list of group names that have the highest number of members. - `\\"one_member\\"`: a list of group names that have exactly one member. # Function Signature ```python def group_members_statistics() -> dict: pass ``` # Constraints - Use the `grp` module to access the group database information. - You may assume the environment is a Unix-like system with an accessible group database. - Your solution should handle the case where there are no groups gracefully. # Example Usage ```python result = group_members_statistics() expected_result = { \\"total_groups\\": 5, \\"no_members\\": [\\"group2\\", \\"group4\\"], \\"most_members\\": [\\"group1\\"], \\"one_member\\": [\\"group3\\"] } assert result == expected_result ``` In this example, assume that there are five groups in total. The names \\"group2\\" and \\"group4\\" have no members. \\"Group1\\" has the highest number of members, while \\"group3\\" has exactly one member. You can use these functions from the `grp` module: - `grp.getgrall()`: to get all group entries. - Access attributes of each group entry to gather the required data. **Note**: Handle potential edge cases such as empty group entries or invalid data formats.","solution":"import grp def group_members_statistics() -> dict: all_groups = grp.getgrall() total_groups = len(all_groups) no_members = [] most_members = [] one_member = [] max_members = 0 members_dict = {} for group in all_groups: members_count = len(group.gr_mem) members_dict[group.gr_name] = members_count if members_count == 0: no_members.append(group.gr_name) elif members_count == 1: one_member.append(group.gr_name) if members_count > max_members: max_members = members_count most_members = [group.gr_name] elif members_count == max_members: most_members.append(group.gr_name) return { \\"total_groups\\": total_groups, \\"no_members\\": no_members, \\"most_members\\": most_members, \\"one_member\\": one_member }"},{"question":"# Advanced Cookie Management in Python You are tasked with creating a custom cookie management solution for a web application using Python\'s `http.cookies` module. The application must handle complex cookie scenarios, including encoding non-string data types, managing cookie attributes, and generating appropriate HTTP headers. Requirements: 1. **Class Definition**: - Define a class `CustomCookie` that extends `http.cookies.BaseCookie`. - Add methods to encode non-string values to the cookies and decode them back. Override `value_encode` and `value_decode` methods. 2. **Custom Encoding/Decoding**: - Implement a custom encoding scheme that converts Python data types (like lists and dictionaries) to JSON strings. - Implement a custom decoding scheme that converts JSON strings back to their original data types. 3. **Cookie Management**: - Methods to create, update, and retrieve cookies. - A method that generates appropriate HTTP headers for the cookies, adhering to HTTP standards. Input/Output: 1. **Input**: - A dictionary of cookie names and values. Values can be any serializable data type. - A dictionary of cookie attributes (optional), such as `\\"expires\\"`, `\\"path\\"`, `\\"domain\\"`, etc. 2. **Output**: - A string representing HTTP headers for setting the cookies. - The encoded values should be in JSON format if they are non-string types. Constraints: - Ensure all cookie names and values are valid according to the rules provided in the documentation. - Handle up to 100 cookies in one go with no particular constraints on the size of each cookie value. - Raise a `CookieError` if an invalid cookie name or value is encountered. Performance Requirements: - The solution should efficiently handle a reasonable number of cookies (up to 100), ensuring that parsing, encoding, and decoding operations are performant. # Example Usage: ```python from http.cookies import CookieError from custom_cookie_module import CustomCookie # Your implementation # Define some cookies with different data types cookie_data = { \\"user\\": {\\"name\\": \\"Alice\\", \\"age\\": 30}, \\"session_id\\": \\"XYZ12345\\", \\"preferences\\": [\\"dark_mode\\", \\"notifications\\"] } # Define some attributes for cookies cookie_attrs = { \\"path\\": \\"/\\", \\"domain\\": \\"example.com\\", \\"secure\\": True, \\"httponly\\": True } # Create a custom cookie handler cookie_jar = CustomCookie() # Set cookies with attributes try: for key, value in cookie_data.items(): cookie_jar[key] = value for attr, attr_val in cookie_attrs.items(): cookie_jar[key][attr] = attr_val # Output the HTTP headers for setting cookies http_headers = cookie_jar.output() print(http_headers) except CookieError as e: print(f\\"Cookie error encountered: {e}\\") ``` Your task is to implement the `CustomCookie` class described above.","solution":"import json from http.cookies import BaseCookie, CookieError class CustomCookie(BaseCookie): def value_encode(self, val): Encode the cookie\'s value using JSON if it\'s not a string. if not isinstance(val, str): val = json.dumps(val) return val, val def value_decode(self, val): Decode the cookie\'s value, attempting to parse it as JSON if possible. try: return json.loads(val) except json.JSONDecodeError: return val def create_cookie(self, key, value, attributes={}): Create or update an individual cookie. self[key] = value self.set_attributes(key, attributes) def set_attributes(self, key, attributes): Set attributes for an individual cookie. for attr, attr_val in attributes.items(): self[key][attr] = attr_val def generate_http_headers(self): Generate HTTP headers for the cookies. return self.output() def test_demo(): from http.cookies import SimpleCookie cookie_jar = CustomCookie() cookie_jar[\\"user\\"] = {\\"name\\": \\"Alice\\", \\"age\\": 30} cookie_jar[\\"session_id\\"] = \\"XYZ12345\\" cookie_jar[\\"preferences\\"] = [\\"dark_mode\\", \\"notifications\\"] for key in [\\"user\\", \\"session_id\\", \\"preferences\\"]: cookie_jar[key][\\"path\\"] = \\"/\\" cookie_jar[key][\\"domain\\"] = \\"example.com\\" cookie_jar[key][\\"secure\\"] = True cookie_jar[key][\\"httponly\\"] = True http_headers = cookie_jar.generate_http_headers() print(http_headers) if __name__ == \\"__main__\\": test_demo()"},{"question":"**Objective:** To assess your knowledge and understanding of the `re` module for working with regular expressions in Python, you are tasked with implementing a function that extracts and formats data from a structured text input. **Problem Description:** You are given a string that contains multiple records of data. Each record consists of various fields separated by specific delimiters and possibly some surrounding text noise. Your task is to write a function `extract_data` that will parse this string and extract relevant information according to the given requirements. **Function Signature:** ```python def extract_data(input_text: str) -> list: pass ``` **Input:** - `input_text` (str): A string containing multiple records of data. Each record might include several fields such as name, email, phone number, and address, structured in repetitive patterns but possibly with some variations. **Output:** - `List[Dict[str, str]]`: A list of dictionaries where each dictionary represents a record with the following keys: \\"Name\\", \\"Email\\", \\"Phone\\", and \\"Address\\". # Constraints: 1. A name consists of alphabetic characters and spaces (e.g., \\"John Doe\\"). 2. An email address must follow the standard email format (e.g., \\"example@domain.com\\"). 3. A phone number is given in a specific format (e.g., \\"(123) 456-7890\\"). 4. An address is a string that can contain alphanumeric characters, spaces, and punctuation marks. **Examples:** 1. **Example 1:** ```python input_text = Name: John Doe Email: john.doe@example.com Phone: (123) 456-7890 Address: 123 Elm St, Springfield, IL Name: Jane Smith Email: jane_smith@sample.org Phone: (987) 654-3210 Address: 456 Oak Ave, Metropolis, NY expected_output = [ { \\"Name\\": \\"John Doe\\", \\"Email\\": \\"john.doe@example.com\\", \\"Phone\\": \\"(123) 456-7890\\", \\"Address\\": \\"123 Elm St, Springfield, IL\\" }, { \\"Name\\": \\"Jane Smith\\", \\"Email\\": \\"jane_smith@sample.org\\", \\"Phone\\": \\"(987) 654-3210\\", \\"Address\\": \\"456 Oak Ave, Metropolis, NY\\" } ] assert extract_data(input_text) == expected_output ``` 2. **Example 2:** ```python input_text = Some intro text... Name: Alice Johnson Email: alice.j@example.com Phone: (555) 123-4567 Address: 789 Birch Road, Hometown, CA Some unusual text and more noise... Name: Bob Brown Email: bob.brown@domain.net Phone: (444) 333-2222 Address: 999 Pine St, Gotham, TX expected_output = [ { \\"Name\\": \\"Alice Johnson\\", \\"Email\\": \\"alice.j@example.com\\", \\"Phone\\": \\"(555) 123-4567\\", \\"Address\\": \\"789 Birch Road, Hometown, CA\\" }, { \\"Name\\": \\"Bob Brown\\", \\"Email\\": \\"bob.brown@domain.net\\", \\"Phone\\": \\"(444) 333-2222\\", \\"Address\\": \\"999 Pine St, Gotham, TX\\" } ] assert extract_data(input_text) == expected_output ``` **Notes:** - The function should be able to handle various text noise or differences in record separation but should reliably extract the fields using regular expressions. - Regular expressions should be crafted to handle the specific formats of name, email, phone, and address. - Consider the performance implications of your regex patterns, especially if input_text is large. # Implementation Note: Use the `re` module functions such as `re.findall`, `re.search`, and groups to extract the data from the input string and structure it into the required format.","solution":"import re def extract_data(input_text: str) -> list: pattern = re.compile( r\'Name:s*(?P<Name>[A-Za-zs]+)s*\' r\'Email:s*(?P<Email>[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,})s*\' r\'Phone:s*(?P<Phone>(d{3})s*d{3}-d{4})s*\' r\'Address:s*(?P<Address>.+)\' ) matches = pattern.findall(input_text) results = [] for match in matches: results.append({ \\"Name\\": match[0].strip(), \\"Email\\": match[1].strip(), \\"Phone\\": match[2].strip(), \\"Address\\": match[3].strip() }) return results"},{"question":"# Pandas Advanced Options and Numeric Formatting You are provided with a dataset and a few specific requirements related to the display configuration of pandas. You need to write a function that: 1. Sets the display precision for floating-point numbers to 4 decimal places globally. 2. Configures pandas to display a truncated version of large DataFrames and Series (i.e., it should not try to print the entire DataFrame). 3. Provides an option context where the display settings are temporarily modified to show the full DataFrame when needed. 4. Resets the options back to their default values after use. Write a function `configure_pandas_display` that meets the above requirements. The function should accept a DataFrame as input and return a string containing the representation of the DataFrame as displayed with the full context. **Function Signature:** ```python def configure_pandas_display(df: pd.DataFrame) -> str: pass ``` # Input: - `df` (pd.DataFrame): A pandas DataFrame containing numeric data. # Output: - `str`: A string containing the DataFrame representation when displayed with full context settings. # Example: ```python import pandas as pd df = pd.DataFrame({ \'A\': range(1, 21), \'B\': range(21, 41), \'C\': range(41, 61) }) output = configure_pandas_display(df) print(output) ``` Expected output for above example (slightly truncated for brevity): ``` A B C 0 1 21 41 1 2 22 42 2 3 23 43 ... 18 19 39 59 19 20 40 60 ``` # Constraints: - The function should use pandas\' options and settings functions effectively. - Avoid hardcoding the default and context settings; use pandas functions to handle these transitions. # Notes: - You can use pandas functions like `set_option`, `reset_option`, `option_context`, and others as needed. - Make sure not to modify the input DataFrame in place.","solution":"import pandas as pd def configure_pandas_display(df: pd.DataFrame) -> str: Configure pandas display settings for floating-point precision, truncation of large DataFrames, and create an option context to display the full DataFrame. # Set the global options for float precision and data display pd.set_option(\'display.float_format\', \'{:.4f}\'.format) pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 10) # Use option_context to temporarily display the full DataFrame with pd.option_context(\'display.max_rows\', None, \'display.max_columns\', None): full_display = df.to_string() # Reset options to defaults (since `option_context` automatically resets within context) pd.reset_option(\'display.float_format\') pd.reset_option(\'display.max_rows\') pd.reset_option(\'display.max_columns\') return full_display"},{"question":"**Title: Implementing a Custom Python 2 to Python 3 Transformation** **Objective:** You are required to manually implement a function that transforms a piece of Python 2.x code to Python 3.x code by applying specific fixers. This exercise will test your understanding of common compatibility issues between Python 2 and Python 3 and your ability to modify code programmatically. **Problem Statement:** Your task is to implement a function `transform_code(code: str, fixers: List[str]) -> str` that takes a string of Python 2.x code and a list of fixer names. The function should return a string where the specified transformations are applied to the input code. For this exercise, you need to implement three specific fixers: 1. **print fixer**: Convert `print \\"text\\"` statements to `print(\\"text\\")`. 2. **raw_input fixer**: Convert `raw_input()` to `input()`. 3. **except fixer**: Convert `except Exception, e` to `except Exception as e`. **Function Signature:** ```python def transform_code(code: str, fixers: List[str]) -> str: pass ``` **Input:** - `code`: A string containing Python 2.x code. - `fixers`: A list of fixer names. Valid fixer names are `\\"print\\"`, `\\"raw_input\\"`, and `\\"except\\"`. **Output:** - A string containing the transformed Python 3.x code. **Constraints:** - The input code will be valid Python 2.x code. - Only the three defined fixers need to be implemented. - The input code may contain multiple lines, and each fixer should be applied globally as required. **Example:** ```python code = \'\'\'def greet(name): print \\"Hello, {0}!\\".format(name) print \\"What\'s your name?\\" name = raw_input() try: raise ValueError(\'A custom error\') except ValueError, e: print e\'\'\' fixers = [\\"print\\", \\"raw_input\\", \\"except\\"] print(transform_code(code, fixers)) ``` **Expected Output:** ```python def greet(name): print(\\"Hello, {0}!\\".format(name)) print(\\"What\'s your name?\\") name = input() try: raise ValueError(\'A custom error\') except ValueError as e: print(e) ``` **Instructions:** 1. Implement each fixer as a separate function. 2. Use regular expressions or other string manipulation techniques to apply the fixers. 3. Ensure the order of fixers does not impact the final output. 4. Write your code in a Pythonic manner, ensuring readability and maintainability. **Notes:** - Focus on creating readable and maintainable code. - You do not need to handle comments or string literals containing these patterns.","solution":"import re from typing import List def print_fixer(code: str) -> str: Convert Python 2 print statements to Python 3 print function. return re.sub(r\'print (.*)\', r\'print(1)\', code) def raw_input_fixer(code: str) -> str: Convert Python 2 raw_input() to Python 3 input(). return code.replace(\\"raw_input()\\", \\"input()\\") def except_fixer(code: str) -> str: Convert Python 2 except clauses to Python 3 except clauses. return re.sub(r\'except ([w.]+), (w+)\', r\'except 1 as 2\', code) def transform_code(code: str, fixers: List[str]) -> str: Apply specified fixers to the input Python 2 code to transform it into Python 3 code. for fixer in fixers: if fixer == \\"print\\": code = print_fixer(code) elif fixer == \\"raw_input\\": code = raw_input_fixer(code) elif fixer == \\"except\\": code = except_fixer(code) return code"},{"question":"**Problem Statement:** You are tasked with processing a large text file in a memory-efficient manner while ensuring that lines containing numeric data are identified and stored separately. Given the constraints on memory usage, you should use buffered I/O to read the file in chunks and process these chunks to extract the desired information. **Requirements:** 1. Implement a function `process_text_file(input_filepath: str, output_filepath: str, encoding: str = \'utf-8\') -> int` that: - Reads the file at `input_filepath` using buffered I/O. - Extracts lines that contain numeric data (numbers with or without decimals). - Writes these lines to `output_filepath` using buffered I/O. - Ensures text encoding is managed properly, using the given `encoding` parameter which defaults to `\'utf-8\'`. - Returns the count of lines containing numeric data. 2. The function should handle the text lines correctly, ensuring that lines are fully read even if they span across chunks. 3. The function should be efficient in terms of memory usage, avoiding loading the entire file into memory at once. **Function Signature:** ```python def process_text_file(input_filepath: str, output_filepath: str, encoding: str = \'utf-8\') -> int: pass ``` **Example:** Assume the contents of \\"input.txt\\" are: ``` This is the first line. 123 is a number. This line contains 456.78 and more text. Another text line. ``` After running `process_text_file(\'input.txt\', \'output.txt\')`, \\"output.txt\\" should contain: ``` 123 is a number. This line contains 456.78 and more text. ``` And the function should return `2`. **Constraints:** - The text file may be very large (several gigabytes), thus should be processed in chunks. - Buffering should be employed to ensure efficient memory usage. - Proper exception handling should be included to manage potential I/O errors. Use appropriate classes from the `io` module to handle the buffered I/O operations and text encoding.","solution":"import re from io import open def process_text_file(input_filepath: str, output_filepath: str, encoding: str = \'utf-8\') -> int: Reads the file at input_filepath using buffered I/O, extracts lines containing numeric data, writes them to output_filepath, and returns the count of such lines. numeric_line_count = 0 numeric_line_pattern = re.compile(r\'d\') with open(input_filepath, \'r\', encoding=encoding, buffering=1024) as infile, open(output_filepath, \'w\', encoding=encoding, buffering=1024) as outfile: for line in infile: if numeric_line_pattern.search(line): outfile.write(line) numeric_line_count += 1 return numeric_line_count"},{"question":"# Question: Context Management with `contextvars` in Python Objective: Your task is to demonstrate your understanding of the `contextvars` module by implementing a function that manages context-local state and performs a specific computation asynchronously. Problem Statement: Implement a function `process_numbers(numbers: List[int]) -> List[int]` that processes a list of integers asynchronously. The function should double each number and store the context of each calculation. The context of the calculation includes the input number and its doubled value. Further implement another function `get_context_summary() -> Dict[int, int]` which returns a dictionary representing the summary of all contexts created by the `process_numbers` function, where keys are the input numbers and values are their doubled values. Requirements: 1. Use the `contextvars` module to manage context-local state. 2. Utilize asyncio to perform the number processing concurrently. 3. Ensure that context data can be retrieved correctly using the `get_context_summary` function. Constraints: - You should define a context variable at the module level. - Do not use global or shared state variables other than context variables. - The function should handle any number of integers within the list efficiently. Specifications: - Function 1: ```python async def process_numbers(numbers: List[int]) -> List[int]: pass ``` - **Input**: A list of integers `numbers`. - **Output**: A list of doubled integers. - Function 2: ```python def get_context_summary() -> Dict[int, int]: pass ``` - **Output**: A dictionary where keys are input numbers and values are their doubled values. Example: ```python import asyncio # Example usage numbers = [1, 2, 3] # Start the event loop to process numbers doubled_numbers = asyncio.run(process_numbers(numbers)) # doubled_numbers should be [2, 4, 6] # Retrieve the context summary summary = get_context_summary() # summary should be {1: 2, 2: 4, 3: 6} ``` Ensure your solution is robust and handles edge cases. Performance considerations should be made for large lists of integers.","solution":"import asyncio import contextvars from typing import List, Dict # Define context variable to store the current dictionary context = contextvars.ContextVar(\'context\', default={}) async def process_number(number: int) -> int: # Get the current context state ctx = context.get() # Double the number doubled = number * 2 # Update the context state with the new value ctx[number] = doubled context.set(ctx) return doubled async def process_numbers(numbers: List[int]) -> List[int]: # Reset the context variable for each run context.set({}) # Create tasks for each number processing tasks = [process_number(number) for number in numbers] # Gather results asynchronously results = await asyncio.gather(*tasks) return results def get_context_summary() -> Dict[int, int]: return context.get()"},{"question":"**Objective:** You have been given the task to process multiple files using the `binhex` module. Your task is to implement a process that takes a list of files, encodes each file using binhex encoding, and then attempts to decode them back to verify the integrity of the encoding process. **Function Specification:** ```python def process_binhex_files(file_list): Encodes and decodes a list of binary files to verify the integrity of the binhex encoding and decoding process. Parameters: file_list (list): A list of binary file paths. Returns: dict: A dictionary with the original file names as keys and a boolean value as values. The boolean value indicates whether the decoded files match the original files. Raises: binhex.Error: If there is an error during encoding or decoding. pass ``` **Input:** - `file_list`: A list of file paths pointing to binary files. The files are assumed to be readable. **Output:** - Return a dictionary where the keys are the original file names (not paths) and the values are boolean flags. Each flag indicates whether the file was successfully encoded and then decoded to match the original file content. **Constraints:** 1. Use the `binhex` module\'s functions `binhex` and `hexbin` for encoding and decoding, respectively. 2. Handle exceptions correctly and propagate `binhex.Error` as necessary. 3. Assume that provided file paths are valid and accessible for read/write operations. 4. The conversion process should handle text files using the old Macintosh newline convention (carriage-return as end of line). **Example:** Assume you have a file `example.bin`: - File content (binary): `b\'x00x01x02x03\'` ```python file_list = [\'example.bin\'] result = process_binhex_files(file_list) print(result) # Expected Output: {\'example.bin\': True} ``` **Notes:** - You will need to create temporary files for the binhex encoded and decoded output. - Ensure that the decoded content exactly matches the original file content.","solution":"import binhex import tempfile import os def process_binhex_files(file_list): Encodes and decodes a list of binary files to verify the integrity of the binhex encoding and decoding process. Parameters: file_list (list): A list of binary file paths. Returns: dict: A dictionary with the original file names as keys and a boolean value as values. The boolean value indicates whether the decoded files match the original files. Raises: binhex.Error: If there is an error during encoding or decoding. result = {} for file_path in file_list: file_name = os.path.basename(file_path) # Create temporary files for encoded and decoded outputs encoded_temp_file = tempfile.NamedTemporaryFile(delete=False) decoded_temp_file = tempfile.NamedTemporaryFile(delete=False) try: # Encode the file binhex.binhex(file_path, encoded_temp_file.name) # Decode the file binhex.hexbin(encoded_temp_file.name, decoded_temp_file.name) # Verify the integrity with open(file_path, \'rb\') as original_file, open(decoded_temp_file.name, \'rb\') as decoded_file: original_content = original_file.read() decoded_content = decoded_file.read() # Check if contents are the same result[file_name] = original_content == decoded_content except binhex.Error: raise finally: # Clean up temporary files encoded_temp_file.close() decoded_temp_file.close() os.unlink(encoded_temp_file.name) os.unlink(decoded_temp_file.name) return result"},{"question":"You are provided with the `penguins` dataset from seaborn, which contains measurements of penguins from different species. Your task is to create a multi-faceted visualization using the `seaborn.objects` module to compare the flipper length distributions of different penguin species across various sex groups. You need to demonstrate understanding and utilization of KDE, faceting, and conditional densities. Follow the specific requirements below: # Requirements 1. **Load the Dataset:** - Load the `penguins` dataset from seaborn. 2. **Set Up the Plot:** - Initialize a plot object using `seaborn.objects.Plot()`, setting \\"flipper_length_mm\\" as the x-axis variable. 3. **Add KDE and Faceting:** - Add a KDE area plot to your initialized plot. - Facet the plot by the \\"sex\\" variable, displaying the density for each sex group separately. 4. **Customize Density Visualization:** - Use appropriate options to ensure conditional densities are shown for each \\"species\\". - Adjust the plot to ensure a common normalization for comparison across species. - Ensure the density is evaluated over the full range of observed data. 5. **Additional Customization:** - Add another element to your plot (such as dots or lines) to enhance the visualization. - Make sure plots are well-labeled and include legends where appropriate. # Input and Output - **Input:** No direct input is needed from the user for this task. - **Output:** A multi-faceted plot visualizing the flipper length distributions for different penguin species across sex groups, meeting the requirements above. # Constraints - Use the `seaborn.objects` module. - Handle any potential missing values that might disrupt your analysis. - Ensure readability and clarity in your plots. # Example Please note that the following is a conceptual example to guide your implementation. ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Initialize the plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") # Add KDE, faceting, and customizations ( p.facet(\\"sex\\") .add(so.Area(), so.KDE(common_norm=[\\"col\\"]), color=\\"species\\") .add(so.Line(), so.KDE(gridsize=100)) ) ``` The final plot should display the KDE area plots for flipper lengths, faceted by sex and color-coded by species, while complying with the given conditions and additional customizations.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_penguins_flipper_length_distribution(): # Load the dataset penguins = load_dataset(\\"penguins\\") # Handle missing values by dropping rows with any null values penguins = penguins.dropna(subset=[\\"flipper_length_mm\\", \\"sex\\", \\"species\\"]) # Initialize the plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") # Add KDE, faceting, and customizations plot = ( p.facet(\\"sex\\") .add(so.Area(), so.KDE(), color=\\"species\\", alpha=0.5) .add(so.Line(), so.KDE(), color=\\"species\\") .scale(x_continuous={\'range\': [160, 240]}) .layout(size=(10, 6)) .label(x=\'Flipper Length (mm)\', y=\'Density\', color=\'Species\') ) # Display the plot plot.show() # Call the plotting function to generate the plot plot_penguins_flipper_length_distribution()"},{"question":"Objective Demonstrate your understanding of module importing mechanisms in Python using both the deprecated `imp` module and the recommended `importlib` module. Problem Statement You are given two functions that mimic the import mechanism using the deprecated `imp` module functionality. Your task is to refactor these functions to use the modern `importlib` API while preserving their original behavior. Here are the two functions: ```python import imp import sys def old_find_and_load_module(module_name): try: file, pathname, description = imp.find_module(module_name) return imp.load_module(module_name, file, pathname, description) except ImportError: return None finally: if file: file.close() def old_get_module_file_extensions(): return imp.get_suffixes() ``` # Constraints 1. Do not use the `imp` module in your refactored functions. 2. Use only `importlib` and standard Python libraries. 3. Ensure compatibility with Python 3.8+. Expected Input and Output **Input for `new_find_and_load_module()` function:** - `module_name` (str): The name of the module to find and load. **Output for `new_find_and_load_module()` function:** - The module object if found and loaded successfully, otherwise `None`. **Input for `new_get_module_file_extensions()` function:** - No input parameters. **Output for `new_get_module_file_extensions()` function:** - A list of tuples, each describing a particular type of module file extension. The tuples follow the format `(suffix, mode, type)`. Example ```python # Example usage mod = new_find_and_load_module(\'json\') print(mod) # <module \'json\' from \'.../json/__init__.py\'> ext = new_get_module_file_extensions() print(ext) # [(\'.py\', \'r\', 1), (\'.pyc\', \'rb\', 2), ...] ``` Your Task 1. Refactor `old_find_and_load_module` to `new_find_and_load_module` using `importlib`. 2. Refactor `old_get_module_file_extensions` to `new_get_module_file_extensions` using `importlib`. ```python def new_find_and_load_module(module_name): # Your implementation here pass def new_get_module_file_extensions(): # Your implementation here pass ``` Ensure your refactored functions are efficiently implemented and compatible with Python 3.8+.","solution":"import importlib.util import importlib.machinery import sys def new_find_and_load_module(module_name): Finds and loads a module by name using the modern importlib API. Args: - module_name (str): The name of the module to find and load. Returns: - The module object if found and loaded successfully, otherwise None. try: spec = importlib.util.find_spec(module_name) if spec is None: return None module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module except ImportError: return None def new_get_module_file_extensions(): Returns a list of available module file extensions using the modern importlib API. Returns: - A list of tuples, each describing a particular type of module file extension. extensions = importlib.machinery.all_suffixes() return [(suffix, \'rb\' if suffix.endswith(\'c\') else \'r\', 2 if suffix.endswith(\'c\') else 1) for suffix in extensions]"},{"question":"# Python Coding Assessment Question Objective To assess your proficiency in using Python\'s `multiprocessing` module to create and manage processes, establish communication between them, and ensure proper synchronization. Problem Statement You are tasked with creating a parallel processing system using Python\'s `multiprocessing` module. The system will perform a computational task (calculating the square of numbers) using multiple processes. You need to: 1. Create a list of integer numbers ranging from 1 to 20. 2. Use the `Pool` class to create a pool of worker processes to calculate the square of each number. 3. Use a `Queue` to collect the results from the worker processes. 4. Use synchronization primitives to ensure only one process writes to the console at a time. 5. Combine the results and display the sum of squares. Requirements 1. Define a function `calculate_square` that takes an integer `n` and returns its square. 2. Create a pool of 4 worker processes using the `Pool` class. 3. Use the `map` method of the pool to distribute the task of calculating squares among the worker processes. 4. Ensure that the results are collected and displayed using a `Queue`. 5. Use a `Lock` to synchronize the writing to the console so that each process prints its result in a orderly fashion. Constraints - Use the `multiprocessing.Pool` to manage the worker processes. - Use appropriate synchronization mechanisms to ensure thread/process safety. Expected Input and Output - **Input**: No input required (numbers are pre-defined). - **Output**: Orderly display of each number and its square, followed by the sum of all squares. Example ```plaintext Process-1: 1^2 = 1 Process-2: 2^2 = 4 Process-3: 3^2 = 9 ... Sum of squares: 287 ``` Code Skeleton ```python from multiprocessing import Pool, Queue, Lock, Process # Define the function to calculate the square of a number. def calculate_square(n): # Implement the logic to calculate square pass def worker_task(n, output_queue, lock): result = calculate_square(n) with lock: print(f\\"Process-{n}: {n}^2 = {result}\\") output_queue.put(result) if __name__ == \\"__main__\\": numbers = list(range(1, 21)) pool = Pool(processes=4) output_queue = Queue() lock = Lock() # Submit tasks to the pool results = pool.map(lambda n: worker_task(n, output_queue, lock), numbers) pool.close() pool.join() # Collect results from the queue and calculate sum of squares sum_of_squares = sum([output_queue.get() for _ in numbers]) print(f\\"Sum of squares: {sum_of_squares}\\") ``` Additional Information - Remember to guard the main entry point using `if __name__ == \'__main__\':` to ensure the code works correctly when run as a script. - Ensure all processes and pools are properly closed to avoid any deadlocks or hanging processes.","solution":"from multiprocessing import Pool, Queue, Lock, Process def calculate_square(n): return n ** 2 def worker_task(n, output_queue, lock): result = calculate_square(n) with lock: print(f\\"Process-{n}: {n}^2 = {result}\\") output_queue.put(result) def main(): numbers = list(range(1, 21)) pool = Pool(processes=4) output_queue = Queue() lock = Lock() # Submit tasks to the pool results = pool.starmap(worker_task, [(n, output_queue, lock) for n in numbers]) pool.close() pool.join() # Collect results from the queue and calculate sum of squares sum_of_squares = sum([output_queue.get() for _ in numbers]) print(f\\"Sum of squares: {sum_of_squares}\\") if __name__ == \'__main__\': main()"},{"question":"# Problem: Generative Model using Kernel Density Estimation You are provided with a dataset consisting of two-dimensional points sampled from a complex distribution. Your task is to implement a function that uses kernel density estimation to learn a non-parametric generative model of the dataset. You will then use this model to generate new data points that follow the same distribution as the input data. Function Signature: ```python def generate_samples(data: np.ndarray, kernel: str, bandwidth: float, num_samples: int) -> np.ndarray: Generates new samples from the given data using Kernel Density Estimation. Parameters: - data: np.ndarray (A 2D array with shape (n_samples, 2) representing the input dataset) - kernel: str (The kernel to use for the KernelDensity estimator. Must be one of {\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'}) - bandwidth: float (The bandwidth parameter for the KernelDensity estimator) - num_samples: int (The number of new samples to generate) Returns: - np.ndarray: A 2D array with shape (num_samples, 2) representing the generated samples. ``` Input: - `data` is a 2D numpy array of shape `(n_samples, 2)` where `n_samples` is the number of samples in the input dataset. - `kernel` is a string indicating the type of kernel to use. It must be one of the following: `\'gaussian\'`, `\'tophat\'`, `\'epanechnikov\'`, `\'exponential\'`, `\'linear\'`, `\'cosine\'`. - `bandwidth` is a float that controls the smoothing of the KDE. - `num_samples` is an integer representing the number of new samples to generate from the learned KDE model. Output: - The function should return a 2D numpy array of shape `(num_samples, 2)` containing the generated samples. Example: ```python import numpy as np # Sample dataset data = np.array([ [1.0, 2.0], [3.0, 3.0], [3.5, 1.5], [2.0, 2.0], [1.5, 2.5] ]) # Generate new samples generated_data = generate_samples(data, kernel=\'gaussian\', bandwidth=0.5, num_samples=100) print(generated_data.shape) # Expected output: (100, 2) ``` Constraints: - Ensure the function handles edge cases such as empty input data gracefully. - Perform input validation to check that `kernel` is one of the valid options and `bandwidth` is positive. - Optimize the function for performance, as high-dimensional KDE can be computationally expensive. Notes: - You may use the `KernelDensity` class from `sklearn.neighbors` to implement the KDE. - The output should be a set of new data points sampled from the generative model learned from the input data.","solution":"import numpy as np from sklearn.neighbors import KernelDensity def generate_samples(data: np.ndarray, kernel: str, bandwidth: float, num_samples: int) -> np.ndarray: Generates new samples from the given data using Kernel Density Estimation. Parameters: - data: np.ndarray (A 2D array with shape (n_samples, 2) representing the input dataset) - kernel: str (The kernel to use for the KernelDensity estimator. Must be one of {\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'}) - bandwidth: float (The bandwidth parameter for the KernelDensity estimator) - num_samples: int (The number of new samples to generate) Returns: - np.ndarray: A 2D array with shape (num_samples, 2) representing the generated samples. if not isinstance(data, np.ndarray) or data.ndim != 2 or data.shape[1] != 2: raise ValueError(\\"data must be a 2D numpy array with shape (n_samples, 2)\\") if kernel not in {\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'}: raise ValueError(\\"Invalid kernel specified. Must be one of {\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'}\\") if not isinstance(bandwidth, (float, int)) or bandwidth <= 0: raise ValueError(\\"bandwidth must be a positive number\\") if not isinstance(num_samples, int) or num_samples <= 0: raise ValueError(\\"num_samples must be a positive integer\\") kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data) new_samples = kde.sample(num_samples) return new_samples"},{"question":"**Title: Customizing Python Objects to Emulate User-Defined Type** **Problem Statement:** You are required to implement a custom Python class named `CustomList` that emulates the behavior of a typical list, but with additional features and constraints. This class must implement special methods to customize its behavior for specific operations. The implemented class should demonstrate the understanding of: - Basic customization of instances. - Emulating container types. - Implementing context managers. - Supporting coroutines and asynchronous iterators. **Requirements:** 1. **Initialization and Representation:** - The class should be initialized with an iterable. - The string representation of the object should include the class name and list contents, in the format: `CustomList([1, 2, 3])`. 2. **Container Behavior:** - Implement `__getitem__`, `__setitem__`, `__delitem__`, `__len__`, and `__contains__` methods to emulate list behavior. - Only allow integers as valid indices. Raise `TypeError` for non-integer indices. 3. **Context Management:** - Implement context manager protocol methods (`__enter__` and `__exit__`). - On entering the context, print a message \\"Entering context for CustomList\\". - On exiting the context, print a message \\"Exiting context for CustomList\\". 4. **Coroutines:** - Implement an asynchronous method `async_sum` that computes the sum of the list\'s elements. - This method should utilize `await` in some capacity, such as awaiting a simple coroutine that returns each element. 5. **Asynchronous Iterator:** - Implement asynchronous iterator methods (`__aiter__` and `__anext__`). - The asynchronous iterator should return each element of the list, one by one, when iterated using `async for`. **Function Signature:** ```python class CustomList: def __init__(self, iterable): # Initialize with an iterable, store it as a list def __repr__(self): # Return string representation of the class and its contents def __getitem__(self, index): # Return item at the given index def __setitem__(self, index, value): # Set item at the given index def __delitem__(self, index): # Delete item at the given index def __len__(self): # Return length of the list def __contains__(self, item): # Check if item is in the list def __enter__(self): # Context management enter method def __exit__(self, exc_type, exc_value, traceback): # Context management exit method async def async_sum(self): # Asynchronous method to compute sum of elements def __aiter__(self): # Asynchronous iterator aiter method async def __anext__(self): # Asynchronous iterator anext method ``` **Example Usage:** ```python # Initialization and Basic Operations cl = CustomList([1, 2, 3]) print(cl) # Output: CustomList([1, 2, 3]) print(len(cl)) # Output: 3 cl[1] = 5 print(cl[1]) # Output: 5 del cl[1] print(cl) # Output: CustomList([1, 3]) # Context Management with CustomList([1, 2, 3]) as cl: pass # Output: Entering context for CustomList # Output: Exiting context for CustomList # Coroutines import asyncio cl = CustomList([1, 2, 3]) result = asyncio.run(cl.async_sum()) print(result) # Output: 6 # Asynchronous Iterator async def main(): cl = CustomList([1, 2, 3]) async for value in cl: print(value) # Output: # 1 # 2 # 3 asyncio.run(main()) ``` **Constraints:** - Do not use built-in list methods directly (like `append`, `extend`, etc.) except for initialization and internal storage. - Handle appropriate exceptions for invalid operations. **Assessment Criteria:** - Correct implementation of special methods. - Effective use of context management principles. - Proper handling of coroutines and asynchronous iteration. - Clear and well-documented code.","solution":"class CustomList: def __init__(self, iterable): self._list = list(iterable) def __repr__(self): return f\\"CustomList({self._list})\\" def __getitem__(self, index): if not isinstance(index, int): raise TypeError(\\"Index must be an integer\\") return self._list[index] def __setitem__(self, index, value): if not isinstance(index, int): raise TypeError(\\"Index must be an integer\\") self._list[index] = value def __delitem__(self, index): if not isinstance(index, int): raise TypeError(\\"Index must be an integer\\") del self._list[index] def __len__(self): return len(self._list) def __contains__(self, item): return item in self._list def __enter__(self): print(\\"Entering context for CustomList\\") return self def __exit__(self, exc_type, exc_value, traceback): print(\\"Exiting context for CustomList\\") async def async_sum(self): total = 0 for item in self._list: total += await self._return_item(item) return total async def _return_item(self, item): return item def __aiter__(self): self._current = 0 return self async def __anext__(self): if self._current >= len(self._list): raise StopAsyncIteration item = self._list[self._current] self._current += 1 return item"},{"question":"You are developing a simple content viewer application that uses mailcap files to determine how to handle different MIME types. Your task is to implement a Python function that uses the `mailcap` module to determine the appropriate command for handling a given MIME type and to execute that command with a specified file. Implement the function `handle_mime_type` as described below: Function Signature: ```python def handle_mime_type(mime_type: str, file_path: str, action: str = \'view\', params: list = []) -> str: Determines and executes the appropriate command for handling the given MIME type and file. Parameters: mime_type (str): The MIME type to handle (e.g., \'text/html\'). file_path (str): The path to the file to be used with the command. action (str): The type of activity (default is \'view\'). params (list): A list of named parameters for the command (default is an empty list). Returns: str: The command that was executed, or an appropriate error message if no valid command was found. ``` Your Implementation Should: 1. Retrieve the mailcap entries using `mailcap.getcaps()`. 2. Use `mailcap.findmatch` to find the appropriate command based on the provided MIME type, file path, action, and parameters. 3. If a suitable command is found, execute it using the `os.system()` function and return the command as a string. 4. If no suitable command is found or if an error occurs, return an appropriate error message. Constraints: - The function must not inject unsafe characters into the command line (ensure compliance with security restrictions). - Assume the necessary mailcap files are present on the system. Example Usage: ```python import os def handle_mime_type(mime_type: str, file_path: str, action: str = \'view\', params: list = []) -> str: import mailcap # Retrieve the mailcap entries caps = mailcap.getcaps() # Find the appropriate command command, entry = mailcap.findmatch(caps, mime_type, key=action, filename=file_path, plist=params) # Check if any command is found if command: os.system(command) return command else: return \\"No suitable command found for MIME type: \\" + mime_type # Example print(handle_mime_type(\'video/mpeg\', \'example.mpeg\')) ``` In this example, `handle_mime_type` would look up the command to handle \'video/mpeg\' files and execute it with \'example.mpeg\' if a matching entry is found in the mailcap files. If no match is found, it would return an error message.","solution":"import os import mailcap def handle_mime_type(mime_type: str, file_path: str, action: str = \'view\', params: list = []) -> str: Determines and executes the appropriate command for handling the given MIME type and file. Parameters: mime_type (str): The MIME type to handle (e.g., \'text/html\'). file_path (str): The path to the file to be used with the command. action (str): The type of activity (default is \'view\'). params (list): A list of named parameters for the command (default is an empty list). Returns: str: The command that was executed, or an appropriate error message if no valid command was found. # Retrieve the mailcap entries caps = mailcap.getcaps() # Find the appropriate command command, entry = mailcap.findmatch(caps, mime_type, key=action, filename=file_path, plist=params) # Check if any command is found if command: os.system(command) return command else: return \\"No suitable command found for MIME type: \\" + mime_type"},{"question":"**Objective**: Implement a custom container and verify its interface implementation using the `collections.abc` module. **Task**: You are required to design a custom mutable sequence class named `CustomList`. This class should mimic the behavior of a Python list but will store elements in a set internally to ensure only unique elements are stored. Use the `collections.abc.MutableSequence` abstract base class to ensure compliance with the expected interface. **Requirements**: 1. Implement all required abstract methods from `collections.abc.MutableSequence`. 2. Override the mixin methods only if necessary. 3. Ensure the class maintains the order of insertion and does not allow duplicate elements. 4. Use the `issubclass()` and `isinstance()` methods to validate your implementation. **Input and Output**: - The class will be instantiated without initial elements. - Elements should be added using the `append()` method. - The `len()` function will be used to get the number of elements. - Elements should be accessible using indexing. - Duplicates should be ignored if attempted to be added. **Example**: ```python from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._data = [] def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): if value not in self._data: self._data[index] = value def __delitem__(self, index): del self._data[index] def insert(self, index, value): if value not in self._data: self._data.insert(index, value) def append(self, value): if value not in self._data: self._data.append(value) # Testing the implementation custom_list = CustomList() custom_list.append(1) custom_list.append(2) custom_list.append(2) # Duplicate, should be ignored custom_list.append(3) print(custom_list) # Expected: [1, 2, 3] print(len(custom_list)) # Expected: 3 print(custom_list[1]) # Expected: 2 print(isinstance(custom_list, MutableSequence)) # Expected: True ``` **Constraints**: - Your implementation should manage insertion order and uniqueness without using an underlying set directly for storage. - Ensure to follow the `MutableSequence` protocol as closely as possible. **Considerations**: - Pay attention to method performance, especially for methods that make repeated use of the underlying `_data` list. - Only override mixin methods if necessary to ensure compliance.","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._data = [] def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): if value not in self._data: self._data[index] = value def __delitem__(self, index): del self._data[index] def insert(self, index, value): if value not in self._data: self._data.insert(index, value) def append(self, value): if value not in self._data: self._data.append(value) def __repr__(self): return repr(self._data) def __contains__(self, item): return item in self._data"},{"question":"# Asynchronous File Transfer using asyncio Transports and Protocols **Objective:** Create a minimal asynchronous file transfer system using asyncio Transports and Protocols which includes a server and a client that can send and receive a file. **Requirements:** 1. **File Transfer Server:** - The server should use `loop.create_server()` to listen for incoming TCP connections. - Upon receiving a connection, it should use custom Protocol to handle file transfer. - The server should receive a file from the client and save it to the local file system. - Ensure proper handling of data transmission and end-of-file signals. 2. **File Transfer Client:** - The client should connect to the server using `loop.create_connection()`. - It should use a custom Protocol to send a specified file to the server. - Ensure the file is sent in chunks and handle the end-of-file transmission properly. **Specifications:** - Use the given file path as a command-line argument for the client. - Ensure any exceptions during the transfer are properly handled and reported. - Support large files by sending/receiving data in chunks. - Ensure the data is received in the correct order. **Input and Output:** - **Client:** - Input: path to the file to be sent (string). - Expected behavior: Connect to the server, send the file content, and handle reconnection attempts if needed. - **Server:** - Input: Port to listen on (integer). - Expected behavior: Accept client connections, receive files, and save them locally with a modified filename (e.g., add a \\"_received\\" suffix to the original filename). **Constraints:** - You must use asyncio Transports and Protocols to handle the connections and data transfer. - Ensure resources (e.g., file descriptors) are properly managed to avoid leaks. - Implement graceful handling of connection closures. **Performance Considerations:** - Efficiently handle file transfer using buffers. - Minimize the number of memory copies during the file transfer. **Complete the following code templates:** File Transfer Server Template ```python import asyncio class FileTransferServerProtocol(asyncio.Protocol): def __init__(self): self.file = None self.buffer = b\\"\\" def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(f\'Connection from {self.peername}\') def data_received(self, data): if not self.file: filename = data.decode().strip() self.file = open(f\\"{filename}_received\\", \\"wb\\") else: self.file.write(data) def connection_lost(self, exc): if self.file: self.file.close() print(f\'Connection lost from {self.peername}\') async def main(port): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: FileTransferServerProtocol(), \'0.0.0.0\', port) async with server: await server.serve_forever() if __name__ == \'__main__\': import sys port = int(sys.argv[1]) asyncio.run(main(port)) ``` File Transfer Client Template ```python import asyncio import os class FileTransferClientProtocol(asyncio.Protocol): def __init__(self, filename, on_con_lost): self.filename = filename self.on_con_lost = on_con_lost self.file = None def connection_made(self, transport): self.transport = transport print(f\'Send: {self.filename}\') self.file = open(self.filename, \'rb\') self.transport.write(self.filename.encode() + b\'n\') # Send filename first self._send_chunk() def _send_chunk(self): chunk = self.file.read(1024) if not chunk: self.transport.close() else: self.transport.write(chunk) def data_received(self, data): pass # Not expecting data from server def connection_lost(self, exc): if self.file: self.file.close() print(\'The server closed the connection\') self.on_con_lost.set_result(True) async def main(filename, host, port): loop = asyncio.get_running_loop() on_con_lost = loop.create_future() transport, protocol = await loop.create_connection( lambda: FileTransferClientProtocol(filename, on_con_lost), host, port) try: await on_con_lost finally: transport.close() if __name__ == \'__main__\': import sys filename = sys.argv[1] host = \'127.0.0.1\' port = 8888 asyncio.run(main(filename, host, port)) ``` **Note**: Complete the provided templates to meet the requirements.","solution":"import asyncio class FileTransferServerProtocol(asyncio.Protocol): def __init__(self): self.file = None self.buffer = b\\"\\" def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') print(f\'Connection from {self.peername}\') def data_received(self, data): if not self.file: filename = data.decode().strip() self.file = open(f\\"{filename}_received\\", \\"wb\\") else: self.file.write(data) def connection_lost(self, exc): if self.file: self.file.close() print(f\'Connection lost from {self.peername}\') async def main(port): loop = asyncio.get_running_loop() server = await loop.create_server(lambda: FileTransferServerProtocol(), \'0.0.0.0\', port) async with server: await server.serve_forever() if __name__ == \'__main__\': import sys port = int(sys.argv[1]) asyncio.run(main(port))"},{"question":"Your task is to implement a Python function `extract_features_from_dict` that uses the `DictVectorizer` class from `sklearn.feature_extraction` to perform feature extraction on a list of dictionaries. Each dictionary in the list represents a sample with categorical and numerical features. Function Signature ```python def extract_features_from_dict(data: list) -> tuple: pass ``` Input - `data`: A list of dictionaries, where each dictionary represents a sample. Each dictionary can contain both categorical and numerical features. Example: ```python [ {\'city\': \'Dubai\', \'temperature\': 33.}, {\'city\': \'London\', \'temperature\': 12.}, {\'city\': \'San Francisco\', \'temperature\': 18.}, ] ``` Output - The function should return a tuple containing two elements: 1. A 2D numpy array with the feature vectors. 2. A list of feature names corresponding to the columns in the array. Example: ```python ( array([[ 1., 0., 0., 33.], [ 0., 1., 0., 12.], [ 0., 0., 1., 18.]]), [\'city=Dubai\', \'city=London\', \'city=San Francisco\', \'temperature\'] ) ``` Constraints - The input list may contain a variable number of samples and features. - Only the features present in the dictionaries should be included in the output. Requirements - Use the `DictVectorizer` from `sklearn.feature_extraction`. - Handle both categorical and numerical data appropriately. Example ```python data = [ {\'city\': \'Dubai\', \'temperature\': 33.}, {\'city\': \'London\', \'temperature\': 12.}, {\'city\': \'San Francisco\', \'temperature\': 18.}, ] result = extract_features_from_dict(data) ``` Expected Output: ```python ( array([[ 1., 0., 0., 33.], [ 0., 1., 0., 12.], [ 0., 0., 1., 18.]]), [\'city=Dubai\', \'city=London\', \'city=San Francisco\', \'temperature\'] ) ``` Implement the function `extract_features_from_dict` ensuring it meets the specifications outlined.","solution":"from sklearn.feature_extraction import DictVectorizer import numpy as np def extract_features_from_dict(data: list) -> tuple: Extracts feature vectors from a list of dictionaries using DictVectorizer. Args: - data: list of dictionaries, where each dictionary represents a sample with categorical and numerical features. Returns: - A tuple containing: - A 2D numpy array with the feature vectors. - A list of feature names corresponding to the columns in the array. vec = DictVectorizer(sparse=False) feature_array = vec.fit_transform(data) feature_names = vec.get_feature_names_out() return feature_array, feature_names.tolist()"},{"question":"# **Coding Assessment Question** **Objective** To assess the understanding of advanced object-oriented programming concepts, including custom class creation, special methods, and context management in Python. **Problem Statement** Design and implement a custom class `CircularQueue` which emulates the behavior of a circular queue data structure. The class should incorporate specific special methods to handle queue operations, printing, and context management. **Requirements and Specifications** 1. **Class Initialization** - `__init__(self, k: int)`: Initializes the circular queue with a specific capacity `k`. 2. **Queue Operations** - `enqueue(self, value: int) -> bool`: Adds an element to the circular queue if there is space. Returns `True` if the operation is successful, otherwise returns `False`. - `dequeue(self) -> int`: Removes an element from the circular queue. Returns the removed element or raises an `IndexError` if the queue is empty. - `front(self) -> int`: Returns the front element of the queue without removing it, or raises an `IndexError` if the queue is empty. - `rear(self) -> int`: Returns the last element of the queue without removing it, or raises an `IndexError` if the queue is empty. 3. **Special Methods** - `__len__(self) -> int`: Returns the number of elements in the queue. - `__iter__(self)`: Returns an iterator for the queue. - `__contains__(self, item: int) -> bool`: Checks if an item is in the queue. - `__str__(self) -> str`: Returns a string representation of the queue in the form `[front,...,rear]`. 4. **Context Management** - Implement `CircularQueue` as a context manager using `__enter__` and `__exit__` to handle any setup or cleanup operations when used in a `with` statement. **Constraints** - The size of the queue `k` will be in the range [1, 1000]. - Integer values will be in the range [-1000, 1000]. - Implementations should be optimally efficient with respect to time and space complexity. **Performance Requirements** - Enqueue and dequeue operations should have O(1) time complexity. - Ensure proper memory management and avoid unnecessary use of additional data structures. **Input and Output Format** # Example: ```python # Example usage of CircularQueue queue = CircularQueue(3) # Enqueue elements assert queue.enqueue(1) == True assert queue.enqueue(2) == True assert queue.enqueue(3) == True assert queue.enqueue(4) == False # Queue is full # Check queue state assert str(queue) == \'[1, 2, 3]\' assert queue.front() == 1 assert queue.rear() == 3 assert 2 in queue == True # Dequeue elements assert queue.dequeue() == 1 assert queue.dequeue() == 2 assert str(queue) == \'[3]\' assert queue.enqueue(4) == True # Use as context manager with CircularQueue(2) as cq: assert cq.enqueue(10) == True assert cq.enqueue(20) == True assert cq.dequeue() == 10 print(\\"All tests passed!\\") ``` # **Submission Requirements** Implement and submit the `CircularQueue` class that adheres to the mentioned specifications and passes the provided example tests.","solution":"class CircularQueue: def __init__(self, k: int): Initializes the circular queue with a specific capacity k. self.capacity = k self.queue = [None] * k self.front_idx = 0 self.rear_idx = -1 self.size = 0 def enqueue(self, value: int) -> bool: Adds an element to the circular queue if there is space. Returns True if the operation is successful, otherwise returns False. if self.size == self.capacity: return False self.rear_idx = (self.rear_idx + 1) % self.capacity self.queue[self.rear_idx] = value self.size += 1 return True def dequeue(self) -> int: Removes an element from the circular queue. Returns the removed element or raises an IndexError if the queue is empty. if self.size == 0: raise IndexError(\\"dequeue from empty queue\\") value = self.queue[self.front_idx] self.front_idx = (self.front_idx + 1) % self.capacity self.size -= 1 return value def front(self) -> int: Returns the front element of the queue without removing it, or raises an IndexError if the queue is empty. if self.size == 0: raise IndexError(\\"front from empty queue\\") return self.queue[self.front_idx] def rear(self) -> int: Returns the last element of the queue without removing it, or raises an IndexError if the queue is empty. if self.size == 0: raise IndexError(\\"rear from empty queue\\") return self.queue[self.rear_idx] def __len__(self) -> int: Returns the number of elements in the queue. return self.size def __iter__(self): Returns an iterator for the queue. for i in range(self.size): yield self.queue[(self.front_idx + i) % self.capacity] def __contains__(self, item: int) -> bool: Checks if an item is in the queue. for element in self: if element == item: return True return False def __str__(self) -> str: Returns a string representation of the queue in the form [front,...,rear]. return \\"[\\" + \\", \\".join(map(str, list(self))) + \\"]\\" def __enter__(self): Enter the runtime context related to this object. return self def __exit__(self, exc_type, exc_value, traceback): Exit the runtime context related to this object. pass"},{"question":"**Objective**: Test students\' understanding of seaborn\'s pointplot() functionality, including grouping, customization, error bars, and handling different data structures. **Question**: You are given two datasets, `penguins` and `flights`, which you can load using seaborn\'s load_dataset function. Using these datasets, follow the tasks below to create visualizations with seaborn point plots. 1. **Task 1**: Create a point plot using the `penguins` dataset to visualize the body mass (`body_mass_g`) of penguins across different islands (`island`). Group the data by the sex (`sex`) of the penguins and use different colors for each group. Use standard deviation (`sd`) for the error bars. Customize the plot by using circular markers (`o`) for male penguins and square markers (`s`) for female penguins, and use solid lines for both groups. 2. **Task 2**: Use the `flights` dataset to create a wide-format DataFrame where `year` is the index and each `month` is a column, with the values representing the number of passengers. Create a point plot to visualize the number of passengers for each month from this wide-format DataFrame. Ensure the native scale of the grouping variable is preserved. 3. **Task 3**: Modify the point plot in Task 2 to: - Highlight the data point for the month of June in the year 1955 with a red star marker (`*`). - Customize the appearance of the plot to use diamond markers (`D`), and set the marker size to 8. **Constraints and Notes**: - Your solution should be efficient and make use of seaborn\'s functionalities as demonstrated in the provided documentation. - Ensure your plots are well-labeled and easy to interpret. - You are encouraged to use Python\'s plotting libraries (seaborn, matplotlib) and pandas for data manipulation. **Expected Input**: - No specific input format required; you will load the datasets via seaborn. **Expected Output**: - Three visualizations as described in the tasks above. **Example Code**: ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Point plot with penguins data penguins = sns.load_dataset(\\"penguins\\") sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", errorbar=\\"sd\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"-\\"]) plt.title(\'Penguin Body Mass by Island and Gender\') plt.show() # Task 2: Point plot with flights data flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") ax = sns.pointplot(data=flights_wide) ax.plot(1955, flights_wide.loc[1955, \'Jun\'], \'r*\', markersize=10) plt.title(\'Number of Passengers by Month Over Years\') plt.show() # Task 3: Customizing point plot with flights data ax = sns.pointplot(data=flights_wide, markers=\'D\', linestyles=\'-\', capsize=.4) ax.plot(1955, flights_wide.loc[1955, \'Jun\'], \'r*\', markersize=10) plt.title(\'Customized Plot: Number of Passengers by Month Over Years\') plt.show() ``` **Submission**: - Submit a Python script or Jupyter notebook that performs the tasks and generates the required visualizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_penguins_pointplot(): penguins = sns.load_dataset(\\"penguins\\") plt.figure(figsize=(10, 6)) sns.pointplot(data=penguins, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", errorbar=\\"sd\\", markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"-\\"]) plt.title(\'Penguin Body Mass by Island and Gender\') plt.xlabel(\'Island\') plt.ylabel(\'Body Mass (g)\') plt.legend(title=\'Sex\') plt.show() def create_flights_pointplot(): flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") plt.figure(figsize=(12, 8)) sns.pointplot(data=flights_wide, markersize=8) plt.title(\'Number of Passengers by Month Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.xticks(rotation=90) plt.show() def highlight_june_1955(): flights = sns.load_dataset(\\"flights\\") flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") plt.figure(figsize=(12, 8)) ax = sns.pointplot(data=flights_wide, markers=\'D\', linestyles=\'-\', capsize=.4, markersize=8) ax.plot(flights_wide.index.get_loc(1955), flights_wide.loc[1955, \'Jun\'], \'r*\', markersize=10) plt.title(\'Customized Plot: Number of Passengers by Month Over Years\') plt.xlabel(\'Year\') plt.ylabel(\'Number of Passengers\') plt.xticks(rotation=90) plt.show()"},{"question":"Objective Implement a function that processes a list of elements and performs specific operations based on the presence of `None` values and other conditions. Problem Statement You are given a list of elements, which may include the `None` object and various other data types (integers, strings, etc.). Your task is to implement a function that follows these rules: 1. If the list contains `None`, replace it with the string `\\"NoneValue\\"` (without quotes). 2. If the list contains integers, ensure they are converted to their string representation. 3. Keep all other elements unchanged. 4. Return the modified list. Input - A single list `lst` containing elements of varying data types. Output - A list with the same elements but modified according to the rules above. Constraints - The list can contain up to `10^3` elements. - Elements can be of any data type, but the focus should be on handling `None` and integers. Example ```python def process_list(lst): pass # Example usage: print(process_list([1, None, \\"hello\\", 3, None, \\"world\\"])) # Output: [\\"1\\", \\"NoneValue\\", \\"hello\\", \\"3\\", \\"NoneValue\\", \\"world\\"] ``` Function Signature ```python def process_list(lst: list) -> list: ``` Notes - Ensure that the function is efficient and handles edge cases, such as an empty list or a list without any `None` or integers.","solution":"def process_list(lst): Processes a list by replacing None with \\"NoneValue\\" and converting integers to strings. :param lst: List containing elements of varying datatypes. :return: Modified list based on specified conditions. processed_list = [] for element in lst: if element is None: processed_list.append(\\"NoneValue\\") elif isinstance(element, int): processed_list.append(str(element)) else: processed_list.append(element) return processed_list"},{"question":"# Path Manipulation with `pathlib` Objective This assessment tests your understanding and implementation skills with the `pathlib` module. You are required to write functions that perform various filesystem path manipulations and directory traversals. Task Implement a function `traverse_and_copy_py_files(source_dir: str, target_dir: str) -> int` that finds all `.py` files within the `source_dir` directory (including subdirectories), and copies them to the `target_dir` while retaining the directory structure. If necessary, create the corresponding directories in `target_dir`. Function Signature ```python def traverse_and_copy_py_files(source_dir: str, target_dir: str) -> int: pass ``` Input - `source_dir` (str): A string representing the source directory path where the `.py` files need to be searched. - `target_dir` (str): A string representing the target directory path where the `.py` files should be copied. Output - `int`: The number of `.py` files copied. Constraints - Assume `source_dir` and `target_dir` are valid and accessible directories. - The function should create any necessary directories in the `target_dir`. - The function should raise an appropriate exception if a file cannot be copied. Example ```python # Suppose the directory structure is as follows: # source_dir/ # ├── a/ # │ ├── example.py # │ └── test.py # └── b/ # └── sample.py # After calling traverse_and_copy_py_files(\'source_dir\', \'target_dir\') # The target_dir should have the following structure: # target_dir/ # ├── a/ # │ ├── example.py # │ └── test.py # └── b/ # └── sample.py # If there are 3 files copied, the function should return 3. ``` Notes - Use the `pathlib` module for all path manipulations, file searches, and I/O operations. - Maintain the directory structure as close to the source structure as possible in the target directory. # Implementation Guidelines 1. Use `Path` objects from `pathlib` to manage paths. 2. Employ methods like `glob()` or `rglob()` to recursively find `.py` files. 3. Use methods like `mkdir()`, `copy()`, and relevant properties to handle directory and file operations. 4. Ensure error handling for any file or directory access issues. # Hints - Pay particular attention to handling relative paths when copying files. - Consider testing with various directory structures to ensure robustness.","solution":"from pathlib import Path import shutil def traverse_and_copy_py_files(source_dir: str, target_dir: str) -> int: source_path = Path(source_dir) target_path = Path(target_dir) py_files = list(source_path.rglob(\'*.py\')) for py_file in py_files: relative_path = py_file.relative_to(source_path) target_file_path = target_path / relative_path target_file_path.parent.mkdir(parents=True, exist_ok=True) shutil.copy(py_file, target_file_path) return len(py_files)"},{"question":"You are required to implement a function `process_ip_addresses` that processes a list of given IP addresses and networks. The function should: 1. Validate each IP address and network string. 2. Differentiate between IPv4 and IPv6 addresses. 3. Calculate the number of usable host addresses within each provided network. 4. Determine and return how many of the provided IP addresses belong to each given network. The function signature should be: ```python from ipaddress import ip_address, ip_network, AddressValueError, NetmaskValueError def process_ip_addresses(ips, networks): Process a list of IP addresses and networks. Args: ips (list): A list of strings representing IP addresses. networks (list): A list of strings representing IP networks. Returns: dict: A dictionary where keys are network strings and values are dictionaries with: - \'num_hosts\' (int): Number of usable host addresses in the network. - \'contained_ips\' (list): List of IP addresses (from `ips`) that belong to this network. Raises: ValueError: If an IP address or network string is invalid. pass ``` # Example Given the following input: ```python ips = [\'192.0.2.1\', \'192.0.2.128\', \'2001:db8::1\', \'2001:db8::1001\', \'192.0.3.1\'] networks = [\'192.0.2.0/24\', \'2001:db8::/96\'] ``` The function should return: ```python { \'192.0.2.0/24\': { \'num_hosts\': 254, \'contained_ips\': [\'192.0.2.1\', \'192.0.2.128\'] }, \'2001:db8::/96\': { \'num_hosts\': 4294967294, \'contained_ips\': [\'2001:db8::1\', \'2001:db8::1001\'] } } ``` # Notes 1. An IP address is considered part of a network if it lies within the specified prefix. 2. The number of usable host addresses excludes the network and the broadcast addresses for IPv4. 3. You should handle possible exceptions and return or raise appropriate error messages. # Constraints - You may assume that the length of `ips` will not exceed 1000 and the length of `networks` will not exceed 100. - Valid input data types should be strings as specified. - Ensure the function is efficient and properly handles both IPv4 and IPv6 address formats.","solution":"from ipaddress import ip_address, ip_network, AddressValueError, NetmaskValueError def process_ip_addresses(ips, networks): Process a list of IP addresses and networks. Args: ips (list): A list of strings representing IP addresses. networks (list): A list of strings representing IP networks. Returns: dict: A dictionary where keys are network strings and values are dictionaries with: - \'num_hosts\' (int): Number of usable host addresses in the network. - \'contained_ips\' (list): List of IP addresses (from `ips`) that belong to this network. Raises: ValueError: If an IP address or network string is invalid. result = {} # Validate networks and calculate number of hosts for network in networks: try: net = ip_network(network, strict=False) except (AddressValueError, NetmaskValueError): raise ValueError(f\\"Invalid network: {network}\\") num_hosts = net.num_addresses - 2 if net.version == 4 else net.num_addresses - 2 result[network] = {\'num_hosts\': num_hosts, \'contained_ips\': []} # Validate IPs and check containment within networks for ip_str in ips: try: ip = ip_address(ip_str) except AddressValueError: raise ValueError(f\\"Invalid IP address: {ip_str}\\") for network in networks: net = ip_network(network, strict=False) if ip in net: result[network][\'contained_ips\'].append(ip_str) return result"},{"question":"Coding Assessment Question # Objective: You will be assessing the performance of a provided scikit-learn algorithm, identify bottlenecks in its implementation, and optimize it for better performance using profiling tools and techniques. # Problem Statement: You are provided with a scikit-learn implementation of K-Means clustering. Your tasks are as follows: 1. Use profiling to identify performance bottlenecks in the given implementation. 2. Optimize the identified bottlenecks by converting critical sections to Cython or applying relevant algorithmic improvements. 3. Demonstrate the performance improvements with appropriate benchmarks. # Provided Code: ```python from sklearn.datasets import make_blobs import numpy as np def kmeans(X, n_clusters, max_iter=300, tol=1e-4): n_samples, n_features = X.shape centroids = X[np.random.choice(n_samples, n_clusters, replace=False)] for i in range(max_iter): distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2) labels = np.argmin(distances, axis=1) new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(n_clusters)]) if np.all(np.linalg.norm(new_centroids - centroids) < tol): break centroids = new_centroids return centroids, labels # Generate sample data X, _ = make_blobs(n_samples=10000, centers=5, random_state=42) # Profile the original implementation %timeit kmeans(X, n_clusters=5) ``` # Tasks: 1. **Profiling Phase:** - Use profiling tools (`%timeit`, `%prun`, or any other relevant tools) to identify the performance bottlenecks in the above `kmeans` function. 2. **Optimization Phase:** - Based on the profiling results, optimize critical sections of the `kmeans` function using Cython or relevant algorithmic improvements. Provide an optimized version of the `kmeans` function. 3. **Benchmarking Phase:** - Demonstrate the performance improvement by benchmarking the original and optimized versions of the `kmeans` function using `%timeit`. # Constraints: - Do not use external libraries that are not part of the standard scikit-learn, Numpy, or Scipy packages. - Ensure that the optimized `kmeans` function returns the same results as the original implementation. # Expected Output Format: 1. Profiling results for the original implementation. 2. The optimized version of the `kmeans` function. 3. Benchmark results demonstrating the performance improvement. # Notes: - Pay attention to code readability and maintainability. - Include comments to explain the changes and their impact.","solution":"# Import necessary libraries from sklearn.datasets import make_blobs import numpy as np def kmeans(X, n_clusters, max_iter=300, tol=1e-4): n_samples, n_features = X.shape centroids = X[np.random.choice(n_samples, n_clusters, replace=False)] for i in range(max_iter): distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2) labels = np.argmin(distances, axis=1) new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(n_clusters)]) if np.all(np.linalg.norm(new_centroids - centroids) < tol): break centroids = new_centroids return centroids, labels # Generate sample data X, _ = make_blobs(n_samples=10000, centers=5, random_state=42) # Profiling the original implementation import timeit original_time = timeit.timeit(\'kmeans(X, n_clusters=5)\', globals=globals(), number=1) print(\\"Original kmeans execution time:\\", original_time) def optimized_kmeans(X, n_clusters, max_iter=300, tol=1e-4): n_samples, n_features = X.shape centroids = X[np.random.choice(n_samples, n_clusters, replace=False)] for i in range(max_iter): distances = np.sum((X[:, np.newaxis, :] - centroids[np.newaxis, :, :]) ** 2, axis=2) labels = np.argmin(distances, axis=1) new_centroids = np.array([X[labels == j].mean(axis=0) for j in range(n_clusters)]) if np.all(np.sum((new_centroids - centroids) ** 2, axis=1) < tol ** 2): break centroids = new_centroids return centroids, labels # Benchmark the optimized implementation optimized_time = timeit.timeit(\'optimized_kmeans(X, n_clusters=5)\', globals=globals(), number=1) print(\\"Optimized kmeans execution time:\\", optimized_time)"},{"question":"Task You are tasked with writing a Python function that byte-compiles all Python source files in a specified directory tree based on user-defined criteria. Function Signature ```python def custom_compile(directory: str, *, force: bool = False, quiet: bool = False, maxlevels: int = None, workers: int = 1, exclude_pattern: str = None) -> bool: pass ``` Parameters - `directory` (str): The root directory in which to start the compilation. - `force` (bool, optional): If `True`, force the recompilation of all files even if they are up-to-date. Default is `False`. - `quiet` (bool, optional): If `True`, suppress output except for errors. Default is `False`. - `maxlevels` (int, optional): The maximum depth of recursion into subdirectories. If `None`, it defaults to the system\'s recursion limit. - `workers` (int, optional): The number of worker processes to use for parallel compilation. Default is `1` (no parallel compilation). - `exclude_pattern` (str, optional): A regex pattern to exclude files from compilation. If `None`, no files are excluded. Return Value - Returns `True` if all files compiled successfully, `False` otherwise. Constraints - Your function should utilize the `compileall` module, specifically its `compile_dir` method. - You must use the parameters provided to configure the compilation behavior. - The function should handle invalid input gracefully, raising appropriate errors if necessary. Examples ```python # Example 1: Compile all Python files in the \'./src\' directory print(custom_compile(\'./src\')) # Expected output: True (if all files compiled successfully) # Example 2: Compile all Python files in the \'./src\' directory using 4 worker processes print(custom_compile(\'./src\', workers=4)) # Expected output: True (if all files compiled successfully) # Example 3: Force a recompile of all Python files in the \'./src\' directory, excluding files in \'tests\' subdirectory print(custom_compile(\'./src\', force=True, exclude_pattern=r\'.*/tests/.*\')) # Expected output: True (if all files compiled successfully) ``` Notes - Consider edge cases where the directory does not contain any Python files or does not exist. - Ensure that the function can handle symlinks and special characters in file paths where relevant. - Performance and resource utilization should be optimized for large directories with many subdirectories and files.","solution":"import compileall import os import re from typing import Optional def custom_compile(directory: str, *, force: bool = False, quiet: bool = False, maxlevels: Optional[int] = None, workers: int = 1, exclude_pattern: Optional[str] = None) -> bool: Byte-compiles all Python source files in the specified directory tree based on user-defined criteria. Parameters: - directory (str): The root directory in which to start the compilation. - force (bool, optional): If True, force the recompilation of all files even if they are up-to-date. - quiet (bool, optional): If True, suppress output except for errors. - maxlevels (int, optional): The maximum depth of recursion into subdirectories. - workers (int, optional): The number of worker processes to use for parallel compilation. - exclude_pattern (str, optional): A regex pattern to exclude files from compilation. Returns: - bool: True if all files compiled successfully, False otherwise. if not os.path.isdir(directory): raise ValueError(f\\"The provided path \'{directory}\' is not a valid directory.\\") exclude = re.compile(exclude_pattern) if exclude_pattern else None def exclude_dir(d, files): return [f for f in files if not (exclude and exclude.search(os.path.join(d, f)))] return compileall.compile_dir( dir=directory, force=force, quiet=quiet, legacy=True, # Set to True to ensure backward compatibility with Python 3.8 and earlier. maxlevels=maxlevels, workers=workers, rx=exclude )"},{"question":"**Logging Configuration Challenge:** Design a function `generate_logging_config` that generates a logging configuration dictionary for a given set of parameters. The dictionary should conform to the schema required by `logging.config.dictConfig`. # Function Signature: ```python def generate_logging_config(loggers: list, log_level: str, output_type: str, destination: str) -> dict: ... ``` # Parameters: - **loggers: list**: A list of logger names (strings) to configure. - **log_level: str**: The log level for all loggers. Acceptable values are `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`. - **output_type: str**: The type of output handler. Acceptable values are `console`, `file`. - **destination: str**: The destination for the log output. If `output_type` is `file`, this should be the filename. If `output_type` is `console`, this can be ignored (pass an empty string). # Returns: - **dict**: A dictionary structured for the `dictConfig` function that sets up loggers, handlers, and formatters according to the provided parameters. # Constraints and Requirements: 1. **All loggers** should use a single handler, which can either be a `StreamHandler` (console) or a `FileHandler` (file). 2. The **Formatter** should format the log messages as follows: `\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\'` 3. The **logging level** should be set to the provided log_level. 4. The **handlers** key should define at least one handler identified by `output_type`. 5. The **formatters** section should define at least one formatter. 6. The **root logger** should also be configured with the same handler and log level. 7. If the `output_type` is `file`, ensure the file is written in append mode (`\'a\'`). # Example: Calling the function with: ```python config = generate_logging_config([\'app\', \'app.module\'], \'DEBUG\', \'file\', \'app.log\') ``` Should result in a dictionary like: ```python { \'version\': 1, \'formatters\': { \'default\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, }, \'handlers\': { \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'DEBUG\', \'formatter\': \'default\', \'filename\': \'app.log\', \'mode\': \'a\', } }, \'loggers\': { \'app\': { \'handlers\': [\'file\'], \'level\': \'DEBUG\', \'propagate\': False }, \'app.module\': { \'handlers\': [\'file\'], \'level\': \'DEBUG\', \'propagate\': False }, }, \'root\': { \'handlers\': [\'file\'], \'level\': \'DEBUG\' } } ``` **Students are expected to write Python code that adheres to proper logging configuration schema and ensures the dictionary returned can be directly used with `logging.config.dictConfig`.**","solution":"def generate_logging_config(loggers, log_level, output_type, destination): Generates a logging configuration dictionary for given parameters. Parameters: loggers (list): A list of logger names to configure. log_level (str): The log level for all loggers. output_type (str): The type of output handler (\'console\' or \'file\'). destination (str): The destination for log output. If file, this is the filename. If console, pass an empty string. Returns: dict: A logging configuration dictionary. format_str = \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' if output_type == \'console\': handler_config = { \'class\': \'logging.StreamHandler\', \'level\': log_level, \'formatter\': \'default\', \'stream\': \'ext://sys.stdout\' } elif output_type == \'file\': handler_config = { \'class\': \'logging.FileHandler\', \'level\': log_level, \'formatter\': \'default\', \'filename\': destination, \'mode\': \'a\' } else: raise ValueError(\\"Invalid output type. Must be \'console\' or \'file\'.\\") logging_config = { \'version\': 1, \'formatters\': { \'default\': { \'format\': format_str }, }, \'handlers\': { output_type: handler_config }, \'loggers\': { logger: { \'handlers\': [output_type], \'level\': log_level, \'propagate\': False } for logger in loggers }, \'root\': { \'handlers\': [output_type], \'level\': log_level } } return logging_config"},{"question":"# HTML Entity Conversion Functions Objective You are required to implement two functions that demonstrate your understanding of HTML entity handling using the `html.entities` module. These functions will convert between HTML entity names and their corresponding characters. Function 1: `html_entities_to_characters` Implement a function that takes a string containing HTML entity names and converts them to their corresponding characters. **Input:** * A string `text` that may contain HTML entity names like `&gt;`, `&lt;`, and so forth. **Output:** * A string with all HTML entities converted to their corresponding characters. Example: ```python text = \\"Hello &gt; World &amp; Everyone\\" output = html_entities_to_characters(text) print(output) # \\"Hello > World & Everyone\\" ``` Function 2: `characters_to_html_entities` Implement a function that takes a string containing characters and converts certain characters to their corresponding HTML entity names. Specifically, convert `>`, `<`, and `&` to their HTML entity equivalents. **Input:** * A string `text` that may contain the characters `>`, `<`, and `&`. **Output:** * A string with `>`, `<`, and `&` converted to their corresponding HTML entities. Example: ```python text = \\"Hello > World & Everyone\\" output = characters_to_html_entities(text) print(output) # \\"Hello &gt; World &amp; Everyone\\" ``` Constraints - You can assume that the input strings will not contain nested HTML entities. - You should use the `html.entities` module to look up the entity definitions. Performance Requirements - Your implementation should handle strings of length up to 10,000 characters efficiently. Notes - Pay special attention to edge cases, such as strings that do not contain any entities or characters to be converted. - Make sure your implementation is robust enough to handle a variety of input scenarios.","solution":"import html def html_entities_to_characters(text): Converts HTML entity names in a string to their corresponding characters. Parameters: text (str): A string that contains HTML entity names (e.g., &gt;, &lt;, &amp;) Returns: str: A string with all HTML entities converted to their corresponding characters. return html.unescape(text) def characters_to_html_entities(text): Converts certain characters (`>`, `<`, `&`) in a string to their corresponding HTML entity names. Parameters: text (str): A string that may contain the characters `>`, `<`, and `&` Returns: str: A string with `>`, `<`, and `&` converted to their corresponding HTML entities. return text.replace(\\"&\\", \\"&amp;\\").replace(\\">\\", \\"&gt;\\").replace(\\"<\\", \\"&lt;\\")"},{"question":"**Question: Memory Efficient Data Analysis with Pandas** You are given a directory containing multiple parquet files with timestamped data, where each file represents one year of data. Each dataset has the following columns: `timestamp`, `name`, `id`, `x`, and `y`. Your task is to implement a function that performs memory-efficient data analysis using pandas. Specifically, you need to: 1. **Load only required columns** (`timestamp`, `name`, `id`) from each parquet file. 2. **Convert the `name` column to a memory-efficient datatype** using `Categorical`. 3. **Convert numerical columns (`id`) to the most memory-efficient numeric types**. 4. **Compute value counts for the `name` column across all files collectively.** # Function Signature ```python def memory_efficient_name_counts(directory: str) -> pd.Series: directory: str - Path to the directory containing the parquet files. Returns: pd.Series - A Series containing the value counts for the `name` column across all files, with memory usage minimized. ``` # Constraints - The function should process files in chunks that fit into memory. - Use efficient data types to minimize memory usage without altering the logical structure of the data. # Example ```python # Assuming the directory structure as follows, and parquet files contain data as described above. # directory/ # ├── ts-00.parquet # ├── ts-01.parquet # ├── ts-02.parquet # ├── ... # └── ts-11.parquet result = memory_efficient_name_counts(\'directory\') print(result) ``` # Expected Output Your function should return a pandas Series with the counts of each unique name across all files, utilizing memory-efficient techniques. # Notes - Ensure to handle the `name` column appropriately to minimize memory usage. - Perform the operations in a way that avoids loading entire files into memory whenever possible.","solution":"import pandas as pd import os import numpy as np def memory_efficient_name_counts(directory: str) -> pd.Series: directory: str - Path to the directory containing the parquet files. Returns: pd.Series - A Series containing the value counts for the `name` column across all files, with memory usage minimized. name_counts = pd.Series(dtype=\\"int64\\") for file in os.listdir(directory): if file.endswith(\\".parquet\\"): file_path = os.path.join(directory, file) # Load only the required columns df = pd.read_parquet(file_path, columns=[\\"timestamp\\", \\"name\\", \\"id\\"]) # Convert \'name\' column to categorical df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") # Convert \'id\' column to a memory-efficient numeric type df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"integer\\") # Compute value counts for the \'name\' column current_counts = df[\\"name\\"].value_counts() # Combine with the global count name_counts = name_counts.add(current_counts, fill_value=0) return name_counts"},{"question":"# Title: Implement a Multi-threaded Producer-Consumer System # Objective: Create a producer-consumer system using Python\'s `threading` module. This exercise will test your ability to efficiently manage threads and ensure proper synchronization between them. # Problem Statement: You need to implement a system with a shared buffer where producer threads add items to the buffer, and consumer threads remove items from it. Implement the solution by following these steps: 1. **Shared Buffer Creation:** - Create a `Queue` object with a maximum size of 10 to act as the shared buffer. 2. **Producer Thread:** - Implement a `Producer` thread that generates data (integers in sequence starting from 0) and puts them into the buffer. - The producer should stop after producing 20 items. 3. **Consumer Thread:** - Implement a `Consumer` thread that takes data from the buffer and processes it (prints the item). - The consumer should stop after consuming 20 items. 4. **Synchronization:** - Use appropriate synchronization primitives (e.g., `Condition`, `Lock`) to ensure no race conditions occur between producers and consumers. The buffer size constraint should always be respected. 5. **Program Execution:** - Start two producers and two consumers. - Ensure that the main thread waits for all producer and consumer threads to finish before terminating. # Constraints: - Use a queue size of 10 for the buffer. - Each Producer thread should only produce 20 items. - Each Consumer thread should only consume 20 items. - Implement proper synchronization to avoid race conditions. # Input: None # Output: The consumer should print the produced data items as they are consumed. Expected output will resemble: ``` Consumer {ThreadName} consumed item {item} ... ``` # Example Code Structure: ```python import threading import queue import time # Constants BUFFER_SIZE = 10 PRODUCE_COUNT = 20 CONSUME_COUNT = 20 # Shared Buffer buffer = queue.Queue(BUFFER_SIZE) class Producer(threading.Thread): def __init__(self, name): super().__init__() self.name = name def run(self): for item in range(PRODUCE_COUNT): # Add item to buffer with proper synchronization # (Implement your logic here) print(f\\"Producer {self.name} produced item {item}\\") time.sleep(0.1) # Simulate time taken to produce item class Consumer(threading.Thread): def __init__(self, name): super().__init__() self.name = name def run(self): for _ in range(CONSUME_COUNT): # Remove item from buffer with proper synchronization # (Implement your logic here) print(f\\"Consumer {self.name} consumed item {item}\\") time.sleep(0.1) # Simulate time taken to consume item # Create producer and consumer threads producers = [Producer(f\'P{i}\') for i in range(2)] consumers = [Consumer(f\'C{i}\') for i in range(2)] # Start all threads for p in producers: p.start() for c in consumers: c.start() # Wait for all threads to finish for p in producers: p.join() for c in consumers: c.join() print(\\"All producers and consumers have finished.\\") ``` # Notes: - Remember to handle the case where the buffer is full (for producers) or empty (for consumers) efficiently. - Use appropriate synchronization methods like `Condition.wait()`, `Condition.notify()`, or locks to handle buffer state changes. # Evaluation Criteria: - Correct implementation of threading and synchronization - Efficient and deadlock-free code - Clean and readable code structure and comments","solution":"import threading import queue import time # Constants BUFFER_SIZE = 10 PRODUCE_COUNT = 20 CONSUME_COUNT = 20 # Shared Buffer buffer = queue.Queue(BUFFER_SIZE) class Producer(threading.Thread): def __init__(self, name, buffer): super().__init__() self.name = name self.buffer = buffer def run(self): for item in range(PRODUCE_COUNT): self.buffer.put(item) print(f\\"Producer {self.name} produced item {item}\\") time.sleep(0.1) # Simulate time taken to produce item class Consumer(threading.Thread): def __init__(self, name, buffer): super().__init__() self.name = name self.buffer = buffer def run(self): for _ in range(CONSUME_COUNT): item = self.buffer.get() print(f\\"Consumer {self.name} consumed item {item}\\") time.sleep(0.1) # Simulate time taken to consume item # Create producer and consumer threads producers = [Producer(f\'P{i}\', buffer) for i in range(2)] consumers = [Consumer(f\'C{i}\', buffer) for i in range(2)] # Start all threads for p in producers: p.start() for c in consumers: c.start() # Wait for all threads to finish for p in producers: p.join() for c in consumers: c.join() print(\\"All producers and consumers have finished.\\")"},{"question":"Context You are required to implement a configuration manager for a software application using `dataclasses` to handle configuration details and `contextlib` to manage context for reading and writing configurations. The configuration manager should support updating configuration settings within a context and ensure that all resources are cleaned up properly (e.g., file handles are closed). Task 1. **Create a dataclass `Config`**: - Fields: `host` (str), `port` (int), `use_ssl` (bool), `timeout` (float). - Ensure appropriate type annotations and default values. 2. **Implement a context manager `ConfigManager`** using `contextlib`: - This context manager should: - Initialize with a file path. - Read configuration from a JSON file when context begins. - Write configuration to the JSON file when context ends. - Ensure file handles are properly closed. - Allow modifications to the configuration within the context. Input and Output Formats - The JSON file should have a structure like: ```json { \\"host\\": \\"localhost\\", \\"port\\": 8080, \\"use_ssl\\": true, \\"timeout\\": 30.0 } ``` - Expected method signatures: ```python from dataclasses import dataclass from contextlib import contextmanager import json @dataclass class Config: host: str port: int use_ssl: bool timeout: float class ConfigManager: def __init__(self, filepath: str): self.filepath = filepath def __enter__(self): with open(self.filepath, \'r\') as file: config_data = json.load(file) self.config = Config(**config_data) return self.config def __exit__(self, exc_type, exc_val, exc_tb): with open(self.filepath, \'w\') as file: json.dump(self.config.__dict__, file) return False ``` Example Usage Provide an example to demonstrate the functionality: ```python # Assume config.json contains the initial configuration # Usage with ConfigManager(\'config.json\') as config: config.host = \'127.0.0.1\' config.port = 9090 # After the context, config.json should be updated accordingly: # { # \\"host\\": \\"127.0.0.1\\", # \\"port\\": 9090, # \\"use_ssl\\": true, # \\"timeout\\": 30.0 # } ``` Constraints - The JSON file must exist before using the `ConfigManager`. - Handle any JSON decoding errors gracefully. - Ensure the file is properly closed even if an exception occurs within the context. Implement the `Config` dataclass and `ConfigManager` context manager as described above.","solution":"from dataclasses import dataclass, field from contextlib import contextmanager import json import os @dataclass class Config: host: str = \\"localhost\\" port: int = 8080 use_ssl: bool = True timeout: float = 30.0 class ConfigManager: def __init__(self, filepath: str): self.filepath = filepath self.config = None def __enter__(self): try: with open(self.filepath, \'r\') as file: config_data = json.load(file) self.config = Config(**config_data) except (json.JSONDecodeError, FileNotFoundError) as e: # if error occurs, initialize with default config self.config = Config() return self.config def __exit__(self, exc_type, exc_val, exc_tb): with open(self.filepath, \'w\') as file: json.dump(self.config.__dict__, file, indent=4) return False"},{"question":"**Question: Regression on California Housing Prices** In this assignment, you are asked to use scikit-learn to perform regression on the California Housing dataset. Your task is to write a function that fetches the data, splits it into training and testing sets, applies a regression model, and evaluates its performance. # Requirements 1. **Function Name**: `california_housing_regression` 2. **Inputs**: - `random_state` (int): The seed used by the random number generator for reproducibility. - `test_size` (float): The proportion of the dataset to include in the test split. 3. **Outputs**: - `mean_squared_error` (float): The Mean Squared Error of the regression model on the test data. 4. **Constraints**: - Use the `fetch_california_housing` function from `sklearn.datasets` to load the dataset. - Perform a train-test split using `train_test_split` from `sklearn.model_selection`. - Use any suitable regression model from `sklearn.linear_model`. # Function Signature ```python def california_housing_regression(random_state: int, test_size: float) -> float: ``` # Instructions 1. **Data Loading**: - Load the California Housing dataset using `fetch_california_housing`. 2. **Train-Test Split**: - Split the data into training and testing sets using `train_test_split` with the specified `random_state` and `test_size`. 3. **Model Training**: - Train a regression model using the training data. 4. **Evaluation**: - Predict housing prices for the test set. - Calculate the Mean Squared Error (MSE) between the predicted and actual test values and return it. # Example ```python mse = california_housing_regression(random_state=42, test_size=0.2) print(f\\"Mean Squared Error: {mse}\\") ``` # Notes - Ensure that the code runs efficiently and handles the data appropriately. - The Mean Squared Error should be calculated using `mean_squared_error` from `sklearn.metrics`. - You may use any suitable regression model from `sklearn.linear_model`, such as `LinearRegression`.","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def california_housing_regression(random_state: int, test_size: float) -> float: Perform regression on the California Housing dataset and return the Mean Squared Error. Parameters: random_state (int): The seed used by the random number generator for reproducibility. test_size (float): The proportion of the dataset to include in the test split. Returns: float: The Mean Squared Error of the regression model on the test dataset. # Load the California Housing dataset data = fetch_california_housing() X, y = data.data, data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state) # Train a regression model using the training data model = LinearRegression() model.fit(X_train, y_train) # Predict housing prices for the test set y_pred = model.predict(X_test) # Calculate the Mean Squared Error mse = mean_squared_error(y_test, y_pred) return mse"},{"question":"You are tasked with implementing a class `CustomBinHex` that mimics the functionality of the deprecated `binhex` module. Your implementation should support encoding and decoding files in a simple custom ASCII representation. This exercise tests your understanding of file operations and basic encoding principles. Specifications 1. **Class**: - `CustomBinHex` 2. **Methods**: - `encode(input_file: str, output_file: str) -> None` - `decode(input_file: str, output_file: str) -> None` 3. **Functionality**: - The `encode` method should read the binary content of `input_file`, convert it to a custom ASCII format, and write the result to `output_file`. - The `decode` method should read the custom ASCII content of `input_file`, convert it back to binary, and write the result to `output_file`. 4. **Custom ASCII Format**: - Each byte of the binary file should be split into its 8-bit binary representation. - Each 8-bit binary string should be split into two 4-bit nibbles. - Each 4-bit nibble should be converted to a hexadecimal character. - Therefore, every byte is represented by two hexadecimal characters. 5. **Exceptions**: - If an error occurs during encoding or decoding, your methods should raise a custom exception `CustomBinHexError` with an appropriate message. Constraints - The input files may be large, so memory efficiency is crucial. - Ensure that all file handles are safely closed after operations. # Example Usage ```python class CustomBinHexError(Exception): pass class CustomBinHex: @staticmethod def encode(input_file: str, output_file: str) -> None: # Implement encoding logic here pass @staticmethod def decode(input_file: str, output_file: str) -> None: # Implement decoding logic here pass # Example Usage try: CustomBinHex.encode(\'input_binary.dat\', \'encoded_output.txt\') CustomBinHex.decode(\'encoded_output.txt\', \'decoded_binary.dat\') except CustomBinHexError as e: print(f\\"An error occurred: {e}\\") ``` You are required to implement the `encode` and `decode` methods in the `CustomBinHex` class as described. Ensure your solution is efficient and handles all edge cases.","solution":"class CustomBinHexError(Exception): pass class CustomBinHex: @staticmethod def encode(input_file: str, output_file: str) -> None: try: with open(input_file, \'rb\') as infile, open(output_file, \'w\') as outfile: while True: byte = infile.read(1) if not byte: break hex_str = byte.hex() outfile.write(hex_str) except Exception as e: raise CustomBinHexError(f\\"Encoding failed: {str(e)}\\") @staticmethod def decode(input_file: str, output_file: str) -> None: try: with open(input_file, \'r\') as infile, open(output_file, \'wb\') as outfile: hex_data = infile.read() byte_data = bytes.fromhex(hex_data) outfile.write(byte_data) except Exception as e: raise CustomBinHexError(f\\"Decoding failed: {str(e)}\\")"},{"question":"Objective: Demonstrate your understanding of working with Python code objects at a low level, including creating new code objects, retrieving their properties, and determining line numbers based on byte offsets. Problem Statement: You are provided with a code snippet and your task is to create a corresponding `PyCodeObject` using low-level CPython functions. Then, write a Python function to retrieve specific properties of this code object and map bytecode offsets to line numbers. 1. **Write a function `create_code_object` with the following specifications:** - **Input:** ```python source_code (str): A string of valid Python source code. filename (str): The name of the file containing the source code. funcname (str): The name of the function defined by the source code. ``` - **Output:** Returns a `PyCodeObject` representing the compiled code of the given source. 2. **Write a function `get_code_object_properties` with the following specifications:** - **Input:** `code_object (PyCodeObject)` - The code object created in part 1. - **Output:** ```python { \'nlocals\': int, \'stacksize\': int, \'freevars\': list of str, \'cellvars\': list of str, \'firstlineno\': int } ``` 3. **Write a function `map_byte_offset_to_line` with the following specifications:** - **Input:** `code_object (PyCodeObject), byte_offset (int)` - **Output:** The line number in the source code that corresponds to the given byte offset. Constraints: - **source_code** is a string of valid Python syntax for a single function. - **filename** and **funcname** are non-empty strings. - The byte offset provided to `map_byte_offset_to_line` is a valid offset within the bytecode of `code_object`. Example: ```python source_code = def example_func(x, y): result = x + y return result filename = \\"example.py\\" funcname = \\"example_func\\" # Part 1 code_obj = create_code_object(source_code, filename, funcname) # Part 2 properties = get_code_object_properties(code_obj) # properties should be a dictionary with relevant details of the code object # Part 3 line_number = map_byte_offset_to_line(code_object, byte_offset=6) # Assuming byte offset 6 points to the line containing \'result = x + y\' ``` **Note:** You may use the `compile` function to initially create code objects for the provided source code, and then utilize the CPython API functions to analyze and extract information from these objects.","solution":"import dis def create_code_object(source_code, filename, funcname): Creates a code object from the given source code, filename, and function name. compiled_code = compile(source_code, filename, \'exec\') # Extract the specific function\'s code object for const in compiled_code.co_consts: if isinstance(const, type(compiled_code)) and const.co_name == funcname: return const raise ValueError(\\"Function name not found in source code\\") def get_code_object_properties(code_object): Retrieves properties of the given code object. return { \'nlocals\': code_object.co_nlocals, \'stacksize\': code_object.co_stacksize, \'freevars\': list(code_object.co_freevars), \'cellvars\': list(code_object.co_cellvars), \'firstlineno\': code_object.co_firstlineno } def map_byte_offset_to_line(code_object, byte_offset): Maps a given byte offset to the corresponding line number in the source code. line_number = None for i, (offset, lineno) in enumerate(dis.findlinestarts(code_object)): if offset > byte_offset: break line_number = lineno return line_number"},{"question":"Objective Write a Python function using the `importlib` module that dynamically imports a Python module from a given file path, executes a specified function within that module, and returns the result. This task will test your understanding of dynamic imports and programmatically executing code. Task Implement a function `dynamic_import_and_execute(module_path: str, function_name: str, *args, **kwargs) -> Any` that takes the following parameters: - `module_path` (str): The file path to the Python module that needs to be imported dynamically. - `function_name` (str): The name of the function within the module that needs to be executed. - `*args`: Positional arguments to be passed to the function. - `**kwargs`: Keyword arguments to be passed to the function. The function should: 1. Dynamically import the module from the specified file path. 2. Retrieve the function specified by `function_name` within the imported module. 3. Execute the retrieved function with the provided `*args` and `**kwargs`. 4. Return the result of the function execution. Constraints - You should handle cases where the module path or function name is not valid by raising an appropriate exception. - Assume that the functions will have well-defined parameters and expected output. - This function should be compatible with Python 3.10. Example Usage ```python # Assume there is a module at \\"example_module.py\\" with a function defined as: # def sample_function(a, b): # return a + b result = dynamic_import_and_execute(\'example_module.py\', \'sample_function\', 5, b=10) print(result) # Output should be 15 ``` Expected Output The output should be the result of the specified function execution from the dynamically imported module based on the given arguments.","solution":"import importlib.util import os def dynamic_import_and_execute(module_path: str, function_name: str, *args, **kwargs): Dynamically imports a module from the given file path, executes the specified function within that module and returns the result. if not os.path.exists(module_path): raise FileNotFoundError(f\\"Module file {module_path} does not exist.\\") module_name = os.path.splitext(os.path.basename(module_path))[0] spec = importlib.util.spec_from_file_location(module_name, module_path) module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) if not hasattr(module, function_name): raise AttributeError(f\\"Module \'{module_name}\' does not have function \'{function_name}\'.\\") function = getattr(module, function_name) return function(*args, **kwargs)"},{"question":"# Python Resource Management and Development Mode **Objective:** You are required to implement a function that reads all lines from a given text file and returns them as a list. Furthermore, you should ensure proper resource management to avoid any `ResourceWarning` when the Python Development Mode is enabled. **Function Signature:** ```python def read_file_lines(file_path: str) -> list[str]: pass ``` **Input:** - `file_path` (str): The path to the text file. **Output:** - `list[str]`: A list where each element is a line from the file. **Constraints:** - Assume the file is a valid text file and exists at the given path. - Ensure the function does not leave any resources open and can be executed under Python Development Mode without any warnings. **Example:** ```python lines = read_file_lines(\\"example.txt\\") print(lines) ``` If `example.txt` contains: ``` line 1 line 2 line 3 ``` The function should output: ``` [\'line 1n\', \'line 2n\', \'line 3n\'] ``` **Notes:** - You should use Python\'s context managers to handle the file resource properly. - Your implementation will be tested in an environment where Python Development Mode is enabled to ensure no `ResourceWarning` is emitted.","solution":"def read_file_lines(file_path: str) -> list[str]: Reads all lines from a given text file and returns them as a list. Parameters: file_path (str): The path to the text file. Returns: list[str]: A list where each element is a line from the file. with open(file_path, \'r\') as file: lines = file.readlines() return lines"},{"question":"# Advanced Python Import System Exercise Objective: Your task is to implement a custom module loader that supports a simulated \\"remote\\" package system. Given a base URL, your loader should be able to fetch and execute Python modules from this URL. To simplify, we\'ll have the base URL simulate a file system structure. Requirements: 1. **Custom Finder and Loader:** - Implement a custom finder and loader that can handle importing modules from a remote base URL. - The finder should be registered as a meta path finder. - If the base URL contains Python scripts, those should be fetched and executed by the loader. 2. **Simulated Remote File System:** - Create a simulated remote file system structure with URLs as follows (you can use server URLs if you are able to set it up or mock these URL fetches in the code): ``` base_url/ __init__.py module1.py module2.py ``` - `__init__.py` should initiate the package namespace. - Each `.py` file should contain some basic Python code for testing purposes. Constraints: - Only use standard library modules. - Handle and raise appropriate exceptions for any missing modules or fetch errors. Function Signatures: ```python import sys import importlib.abc import importlib.util from types import ModuleType class RemoteMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, base_url: str): self.base_url = base_url def find_spec(self, fullname, path, target=None): # Implement finder logic to return the module spec if the module is found. pass class RemoteLoader: def __init__(self, base_url: str, name: str): self.base_url = base_url self.name = name def create_module(self, spec): # Optionally create a new module object. pass def exec_module(self, module): # Execute the module by fetching the source file from the remote URL. pass def install_remote_finder(base_url: str): Registers the remote finder into the sys.meta_path sys.meta_path.insert(0, RemoteMetaPathFinder(base_url)) ``` Example Usage: ```python # Simulated remote URL structure (use mocks in the absence of an actual server) base_url = \\"http://example.com/python_packages\\" # Installing the remote finder install_remote_finder(base_url) # Now attempting to import remote modules as if they were local import module1 import module2 ``` Performance: - Module import should be efficient and only fetch modules when necessary. Caching strategies may be employed as needed. Provide a robust implementation for: 1. `RemoteMetaPathFinder` by writing the `find_spec` method. 2. `RemoteLoader` by writing the `exec_module` method. 3. `install_remote_finder` function to integrate your custom finder with Python\'s import machinery. 4. Demonstrate the implementation with a mock setup for the `base_url`.","solution":"import sys import importlib.abc import importlib.util from types import ModuleType import urllib.request class RemoteMetaPathFinder(importlib.abc.MetaPathFinder): def __init__(self, base_url: str): self.base_url = base_url def find_spec(self, fullname, path, target=None): filename = fullname.split(\'.\')[-1] + \'.py\' module_url = f\\"{self.base_url}/{filename}\\" try: with urllib.request.urlopen(module_url) as response: if response.status == 200: loader = RemoteLoader(module_url, fullname) return importlib.util.spec_from_loader(fullname, loader) except: return None class RemoteLoader(importlib.abc.Loader): def __init__(self, module_url: str, name: str): self.module_url = module_url self.name = name def create_module(self, spec): return ModuleType(spec.name) def exec_module(self, module): try: with urllib.request.urlopen(self.module_url) as response: source = response.read().decode(\'utf-8\') exec(source, module.__dict__) except Exception as e: raise ImportError(f\\"Could not load module {self.name}\\") from e def install_remote_finder(base_url: str): Registers the remote finder into the sys.meta_path sys.meta_path.insert(0, RemoteMetaPathFinder(base_url))"},{"question":"# Custom PyTorch Operation Implementation and Integration You are tasked with implementing a custom linear transformation operation and integrating it with PyTorch\'s `autograd` and `nn` modules. Task Overview 1. **Custom Function**: - Implement a custom `Function` named `MyLinearFunction` that performs a linear transformation `Y = XW^T + b`, where `X` is the input tensor, `W` is the weight tensor, and `b` is the bias tensor. - Implement both `forward` and `backward` methods for this custom `Function`. 2. **Custom Module**: - Implement a custom `Module` named `MyLinear` that utilizes `MyLinearFunction`. - The module should have parameters `weight` and `bias`. Specifications 1. **Function Implementation (`MyLinearFunction`)**: - **`forward` Method**: - Inputs: `input` (tensor), `weight` (tensor), `bias` (tensor, optional). - Compute and return the output of the linear transformation. - **`backward` Method**: - Inputs: `ctx` (context from `forward`), `grad_output` (tensor). - Compute and return gradients with respect to the inputs (`input`, `weight`, `bias`). 2. **Module Implementation (`MyLinear`)**: - **`__init__` Method**: - Inputs: `input_features` (int), `output_features` (int), `bias` (bool, default True). - Initialize parameters `weight` and `bias`. - **`forward` Method**: - Input: `input` (tensor). - Use `MyLinearFunction.apply` to perform the linear transformation. 3. **Testing**: - Instantiate the custom module and perform a forward pass. - Verify that gradients are correctly computed using `torch.autograd.gradcheck`. Constraints - Your implementation must be optimized and should adhere to PyTorch\'s best practices. - Ensure that the gradient computation in `backward` is correct. - Validate gradients using `torch.autograd.gradcheck`. # Example Usage ```python import torch from torch.autograd import Function import torch.nn as nn import torch.nn.functional as F class MyLinearFunction(Function): @staticmethod def forward(ctx, input, weight, bias=None): # Save context for backward pass ctx.save_for_backward(input, weight, bias) # Compute the forward pass output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None # Compute gradients if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias class MyLinear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(MyLinear, self).__init__() self.input_features = input_features self.output_features = output_features self.weight = nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.Tensor(output_features)) else: self.register_parameter(\'bias\', None) self.reset_parameters() def reset_parameters(self): nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) nn.init.uniform_(self.bias, -bound, bound) def forward(self, input): return MyLinearFunction.apply(input, self.weight, self.bias) # Testing the custom module input_features = 5 output_features = 3 input_tensor = torch.randn(2, input_features, dtype=torch.double, requires_grad=True) linear = MyLinear(input_features, output_features).double() output = linear(input_tensor) print(\'Output:\', output) # Gradcheck test = torch.autograd.gradcheck(linear, input_tensor, eps=1e-6, atol=1e-4) print(\'Gradcheck:\', test) ``` # Requirements Submit the complete implementation of `MyLinearFunction` and `MyLinear` class along with a script that tests their functionality and validates gradients using `gradcheck`.","solution":"import torch from torch.autograd import Function import torch.nn as nn import math class MyLinearFunction(Function): @staticmethod def forward(ctx, input, weight, bias=None): ctx.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias class MyLinear(nn.Module): def __init__(self, input_features, output_features, bias=True): super(MyLinear, self).__init__() self.input_features = input_features self.output_features = output_features self.weight = nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = nn.Parameter(torch.Tensor(output_features)) else: self.register_parameter(\'bias\', None) self.reset_parameters() def reset_parameters(self): nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) nn.init.uniform_(self.bias, -bound, bound) def forward(self, input): return MyLinearFunction.apply(input, self.weight, self.bias)"},{"question":"Objective Your task is to demonstrate your understanding of the core aspects of seaborn\'s objects API by creating and customizing a plot using a given dataset. Problem Statement You are provided with the \'penguins\' dataset. Your task is to: 1. Create a dot plot showing the relationship between the \'bill_length_mm\' and \'bill_depth_mm\' of different penguin species. 2. Customize the plot by modifying both the axis labels and the title. 3. Further enhance the plot by faceting it based on the \'sex\' of the penguins and customizing the facet titles. # Requirements 1. **Input Data:** Use the pre-loaded \'penguins\' dataset in seaborn. 2. **Plot Details:** - Use a dot plot to display \'bill_length_mm\' vs \'bill_depth_mm\' colored by species. - Set custom axis labels: \'Length (mm)\' for the x-axis and \'Depth (mm)\' for the y-axis. - Add a title: \'Penguin Bill Dimensions\'. 3. **Faceting:** - Facet the plot based on the \'sex\' of the penguins. - Customize the facet titles to be in uppercase. Constraints - Ensure that the axis labels and titles are clearly visible and informative. - The customization must be performed using seaborn\'s objects interface where applicable. Expected Output You should provide both the code and the plot as output. # Example Here\'s an example of how you might begin: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create and customize the plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") .label(x=\\"Length (mm)\\", y=\\"Depth (mm)\\") .label(title=\\"Penguin Bill Dimensions\\") .facet(\\"sex\\") .label(title=str.upper) ) # Display the plot p ``` Make sure to display the plot in a way that it clearly shows different facets for \'sex\' with the title in uppercase.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_penguin_bill_dimensions(): Creates and customizes a dot plot showing the relationship between \'bill_length_mm\' and \'bill_depth_mm\' of different penguin species, faceted by their sex. # Load the dataset penguins = load_dataset(\\"penguins\\") # Create and customize the plot p = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .add(so.Dot(), color=\\"species\\") .label(x=\\"Length (mm)\\", y=\\"Depth (mm)\\") .label(title=\\"Penguin Bill Dimensions\\") .facet(\\"sex\\") .label(facet=\\"sex\\", title=str.upper) ) # Display the plot p.show() # Uncomment the following line to execute the function # plot_penguin_bill_dimensions()"},{"question":"# Advanced Coding Assessment Question: Custom Serialization with `copyreg` **Objective:** The goal of this task is to test your comprehension of custom object serialization in Python using the `copyreg` module. You are required to implement custom pickling and unpickling logic for a set of classes. **Task:** 1. Implement a class `Person`, representing a person\'s attributes (name and age). 2. Implement a class `Employee`, which inherits from `Person` and adds an additional attribute (employee_id). 3. Register custom serialization (pickling) and deserialization (unpickling) functions for both classes using the `copyreg` module. **Input and Output:** - **Input:** Instances of `Person` and `Employee` classes. - **Output:** Serialized (pickled) representations of these instances, and their correctly deserialized (unpickled) instances. **Specifications:** 1. **Class Definitions:** ```python class Person: def __init__(self, name, age): self.name = name self.age = age class Employee(Person): def __init__(self, name, age, employee_id): super().__init__(name, age) self.employee_id = employee_id ``` 2. **Custom Serialization Functions:** - For `Person`, the pickle function should output a tuple containing the class reference and a tuple of name and age. - For `Employee`, the pickle function should output a tuple containing the class reference and a tuple of name, age, and employee_id. 3. **Register Serialization Functions using `copyreg`:** ```python import copyreg def pickle_person(person): return Person, (person.name, person.age) def pickle_employee(employee): return Employee, (employee.name, employee.age, employee.employee_id) copyreg.pickle(Person, pickle_person) copyreg.pickle(Employee, pickle_employee) ``` 4. Implement a test scenario: - Create instances of `Person` and `Employee`. - Serialize these instances using `pickle.dumps`. - Deserialize the results using `pickle.loads`. - Verify that the deserialized objects have the same attributes as the original objects. **Constraints:** - Ensure that all custom serialization functions are registered correctly. - Handle errors gracefully, such as invalid attributes or non-callable constructors. **Performance Requirements:** - The solution should efficiently handle serialization and deserialization of objects, and be robust to various input scenarios. ```python import pickle # Implement the classes, serialization functions, and registration here # Testing Serialization and Deserialization def test_serialization(): person = Person(\\"Alice\\", 30) employee = Employee(\\"Bob\\", 40, \\"E123\\") # Serialize person_serialized = pickle.dumps(person) employee_serialized = pickle.dumps(employee) # Deserialize person_deserialized = pickle.loads(person_serialized) employee_deserialized = pickle.loads(employee_serialized) assert person.name == person_deserialized.name and person.age == person_deserialized.age assert (employee.name == employee_deserialized.name and employee.age == employee_deserialized.age and employee.employee_id == employee_deserialized.employee_id) return \\"Serialization and Deserialization Tests Passed\\" # Run the test print(test_serialization()) ``` **Submission:** Submit the fully implemented classes, serialization functions, registration logic, and the test scenario.","solution":"import copyreg import pickle class Person: def __init__(self, name, age): self.name = name self.age = age class Employee(Person): def __init__(self, name, age, employee_id): super().__init__(name, age) self.employee_id = employee_id def pickle_person(person): return Person, (person.name, person.age) def pickle_employee(employee): return Employee, (employee.name, employee.age, employee.employee_id) copyreg.pickle(Person, pickle_person) copyreg.pickle(Employee, pickle_employee)"},{"question":"You are provided with an XML file that contains a list of students and their respective grades in different subjects. You need to parse this XML, perform some modifications based on specific rules, and then generate a modified XML document that reflects these changes. Here’s the sample structure of the XML data (you can assume the file is named `students.xml`): ```xml <students> <student> <name>John Doe</name> <grade> <subject>Math</subject> <mark>75</mark> </grade> <grade> <subject>English</subject> <mark>82</mark> </grade> </student> <student> <name>Jane Smith</name> <grade> <subject>Math</subject> <mark>92</mark> </grade> <grade> <subject>English</subject> <mark>88</mark> </grade> </student> </students> ``` # Task: 1. **Parse the XML file** to create a DOM document. 2. **Add a new student** to the list with at least two subject grades. 3. **Modify the student grades** based on the following rules: - If the grade in Math is below 80, increase it by 10 points but ensure it does not exceed 100. - If the grade in English is below 85, increase it by 5 points. 4. After making these modifications, **generate a new XML string** representation of the modified DOM and print it using pretty-print formatting. # Requirements: - You should use the `xml.dom.minidom` module to perform XML parsing, manipulation, and XML generation. - Your function should dynamically manage the XML structure, ensuring correct node additions and modifications. - The output XML should maintain the same structure and be pretty-printed. # Function Signature: ```python def process_students(filename: str) -> str: Parses an XML file, modifies the student grades based on specified rules, adds a new student and returns the modified XML string. :param filename: The filename of the XML file to process. :return: A pretty-printed XML string of the modified content. pass ``` # Example Usage: ```python xml_output = process_students(\'students.xml\') print(xml_output) ``` # Constraints: - Each student will have grades for both Math and English. - The new student you add should have grades in at least two subjects of your choice. # Note: This task assesses your ability to parse and manipulate XML data using DOM in Python, which is crucial for handling structured data in various applications.","solution":"from xml.dom.minidom import parse, parseString from xml.dom.minidom import Document def process_students(filename: str) -> str: Parses an XML file, modifies the student grades based on specified rules, adds a new student and returns the modified XML string. :param filename: The filename of the XML file to process. :return: A pretty-printed XML string of the modified content. # Parse the XML file dom = parse(filename) # Add a new student students = dom.documentElement new_student = dom.createElement(\'student\') name = dom.createElement(\'name\') name.appendChild(dom.createTextNode(\'Alice Johnson\')) new_student.appendChild(name) grade1 = dom.createElement(\'grade\') subject1 = dom.createElement(\'subject\') subject1.appendChild(dom.createTextNode(\'Math\')) mark1 = dom.createElement(\'mark\') mark1.appendChild(dom.createTextNode(\'85\')) grade1.appendChild(subject1) grade1.appendChild(mark1) new_student.appendChild(grade1) grade2 = dom.createElement(\'grade\') subject2 = dom.createElement(\'subject\') subject2.appendChild(dom.createTextNode(\'English\')) mark2 = dom.createElement(\'mark\') mark2.appendChild(dom.createTextNode(\'90\')) grade2.appendChild(subject2) grade2.appendChild(mark2) new_student.appendChild(grade2) students.appendChild(new_student) # Modify grades according to the rules student_list = dom.getElementsByTagName(\'student\') for student in student_list: grades = student.getElementsByTagName(\'grade\') for grade in grades: subject_name = grade.getElementsByTagName(\'subject\')[0].childNodes[0].nodeValue mark = grade.getElementsByTagName(\'mark\')[0] mark_value = int(mark.childNodes[0].nodeValue) if subject_name == \'Math\' and mark_value < 80: mark_value = min(mark_value + 10, 100) mark.childNodes[0].nodeValue = str(mark_value) elif subject_name == \'English\' and mark_value < 85: mark_value += 5 mark.childNodes[0].nodeValue = str(mark_value) # Return the modified XML as a pretty-printed string return dom.toprettyxml(indent=\\" \\")"},{"question":"**Implementing a University Management System** You are tasked with creating a small part of a university management system using Python classes. This system will have different types of members, such as Students and Professors, who share some commonalities but also have distinct attributes and behaviors. Implement the following requirements using Python classes: 1. **Class `Person`:** - Attributes: - `name` (str): The name of the person. - `age` (int): The age of the person. - Method: - `__init__(self, name: str, age: int)`: Constructor to initialize `name` and `age`. - `__str__(self)`: Returns a string in the format `\\"Name: [name], Age: [age]\\"`. 2. **Class `Student` (inherits from `Person`):** - Attributes: - Inherits `name` and `age` from `Person`. - `student_id` (str): The student\'s ID. - `courses` (list): A list of courses the student is enrolled in. - Methods: - `__init__(self, name: str, age: int, student_id: str)`: Constructor to initialize attributes including `student_id` and an empty `courses` list. - `enroll(self, course: str)`: Adds a course to the `courses` list. - `__str__(self)`: Returns a string in the format `\\"[name] (ID: [student_id]) is [age] years old and enrolled in: [course_1, course_2, ...]\\"`. 3. **Class `Professor` (inherits from `Person`):** - Attributes: - Inherits `name` and `age` from `Person`. - `professor_id` (str): The professor\'s ID. - `courses_taught` (list): A list of courses the professor is teaching. - Methods: - `__init__(self, name: str, age: int, professor_id: str)`: Constructor to initialize attributes including `professor_id` and an empty `courses_taught` list. - `assign_course(self, course: str)`: Adds a course to the `courses_taught` list. - `__str__(self)`: Returns a string in the format `\\"[name] (ID: [professor_id]) is [age] years old and teaches: [course_1, course_2, ...]\\"`. 4. **Class `University`:** - Attributes: - `members` (list of `Person`): A list of all university members (students and professors). - Methods: - `__init__(self)`: Initializes an empty `members` list. - `add_member(self, member: Person)`: Adds a `Person` (Student or Professor) to the `members` list. - `list_members(self)`: Returns a list of strings, each representing a member (using their `__str__` method). # Example Usage: ```python stud = Student(name=\\"Alice\\", age=20, student_id=\\"S123\\") stud.enroll(\\"Math 101\\") stud.enroll(\\"CS 101\\") prof = Professor(name=\\"Dr. Smith\\", age=45, professor_id=\\"P456\\") prof.assign_course(\\"Math 101\\") uni = University() uni.add_member(stud) uni.add_member(prof) for member in uni.list_members(): print(member) ``` # Expected Output: ``` Alice (ID: S123) is 20 years old and enrolled in: [Math 101, CS 101] Dr. Smith (ID: P456) is 45 years old and teaches: [Math 101] ``` # Constraints: - The `name` attribute for a `Person` must be a non-empty string. - The `age` attribute for a `Person` must be a positive integer. - The `student_id` and `professor_id` must be non-empty strings. # Performance Requirements: - The solution should be efficient in terms of time complexity, particularly in iterating through and listing members.","solution":"class Person: def __init__(self, name: str, age: int): if not name or not isinstance(name, str): raise ValueError(\\"Name must be a non-empty string\\") if not isinstance(age, int) or age <= 0: raise ValueError(\\"Age must be a positive integer\\") self.name = name self.age = age def __str__(self): return f\\"Name: {self.name}, Age: {self.age}\\" class Student(Person): def __init__(self, name: str, age: int, student_id: str): super().__init__(name, age) if not student_id or not isinstance(student_id, str): raise ValueError(\\"Student ID must be a non-empty string\\") self.student_id = student_id self.courses = [] def enroll(self, course: str): self.courses.append(course) def __str__(self): courses_str = \\", \\".join(self.courses) return f\\"{self.name} (ID: {self.student_id}) is {self.age} years old and enrolled in: [{courses_str}]\\" class Professor(Person): def __init__(self, name: str, age: int, professor_id: str): super().__init__(name, age) if not professor_id or not isinstance(professor_id, str): raise ValueError(\\"Professor ID must be a non-empty string\\") self.professor_id = professor_id self.courses_taught = [] def assign_course(self, course: str): self.courses_taught.append(course) def __str__(self): courses_str = \\", \\".join(self.courses_taught) return f\\"{self.name} (ID: {self.professor_id}) is {self.age} years old and teaches: [{courses_str}]\\" class University: def __init__(self): self.members = [] def add_member(self, member: Person): self.members.append(member) def list_members(self): return [str(member) for member in self.members]"},{"question":"<|Analysis Begin|> The provided documentation describes the `audioop` module, which facilitates various operations on raw audio data. These operations include tasks like adding and converting audio fragments, computing statistics such as average values or root mean square, and transforming stereo signals into mono ones or vice versa. To utilize this module, one needs to have a fundamental understanding of audio data manipulation, bytes-like objects, and sometimes more complex concepts like a-LAW and u-LAW encoding or ADPCM coding. Many operations require knowledge of sample width, which necessitates an understanding of different audio bit depths. A well-designed question can assess the student\'s ability to use multiple functions from this module, thereby evaluating their grasp of audio data manipulation and their Python programming skills. Given the module\'s capability, a good assessment question might ask the student to implement a function that applies a series of transformations and computations to an audio fragment. <|Analysis End|> <|Question Begin|> # Coding Assessment: Audio Manipulation with `audioop` Module **Objective:** Implement a function in Python using the `audioop` module that performs a series of operations on a given audio fragment to transform it and compute certain statistics. This will test your ability to manipulate raw audio data and utilize the advanced features of the `audioop` module. **Function Signature:** ```python def process_audio_data(audio_fragment: bytes, width: int) -> dict: Process the input audio fragment and return a dictionary with specific results. Parameters: audio_fragment (bytes): The input audio fragment consisting of signed integer samples. width (int): The sample width in bytes, which can be 1, 2, 3, or 4. Returns: dict: A dictionary containing the following keys and their respective results: - \'rms\': Root mean square of the input fragment. - \'max\': Maximum sample value. - \'minmax\': Tuple containing the minimum and maximum sample values. - \'reverse\': The audio fragment with samples in reverse order (as bytes). - \'stereo\': Converted stereo fragment where: - The left channel is the original samples multiplied by 1.5, - The right channel is the original samples multiplied by 0.5. ``` **Expected Input and Output Formats:** - `audio_fragment` is a bytes-like object containing the raw audio data with signed integer samples. - `width` is an integer representing the sample width in bytes, with allowed values being 1, 2, 3, or 4. - The function returns a dictionary with keys `\'rms\'`, `\'max\'`, `\'minmax\'`, `\'reverse\'`, and `\'stereo\'`, each containing their respective computed values as described. **Constraints:** - The length of the `audio_fragment` is guaranteed to be a multiple of `width`. **Performance Requirements:** - The function should handle audio fragments up to 1 MB in size efficiently. **Example Usage:** ```python audio_fragment = b\'x00x01x02x03x04x05x06x07\' width = 2 result = process_audio_data(audio_fragment, width) print(result) ``` **Example Output:** ```python { \'rms\': 2, \'max\': 1792, \'minmax\': (0, 1792), \'reverse\': b\'x07x06x05x04x03x02x01x00\', \'stereo\': b\'x00x01x00x00x03x04x05x02x06x07x04x03x09x07x06x03\' } ``` **Instructions:** 1. Implement the `process_audio_data` function. 2. Use the following `audioop` functions as necessary: `audioop.rms`, `audioop.max`, `audioop.minmax`, `audioop.reverse`, and `audioop.tostereo`. 3. Ensure your code is clean, well-commented, and adheres to Python best practices. This task evaluates your understanding of the `audioop` module and your ability to manipulate audio data programmatically.","solution":"import audioop def process_audio_data(audio_fragment: bytes, width: int) -> dict: Process the input audio fragment and return a dictionary with specific results. Parameters: audio_fragment (bytes): The input audio fragment consisting of signed integer samples. width (int): The sample width in bytes, which can be 1, 2, 3, or 4. Returns: dict: A dictionary containing the following keys and their respective results: - \'rms\': Root mean square of the input fragment. - \'max\': Maximum sample value. - \'minmax\': Tuple containing the minimum and maximum sample values. - \'reverse\': The audio fragment with samples in reverse order (as bytes). - \'stereo\': Converted stereo fragment where: - The left channel is the original samples multiplied by 1.5, - The right channel is the original samples multiplied by 0.5. rms = audioop.rms(audio_fragment, width) max_val = audioop.max(audio_fragment, width) minmax = audioop.minmax(audio_fragment, width) reversed_fragment = audioop.reverse(audio_fragment, width) stereo_fragment = audioop.tostereo(audio_fragment, width, 1.5, 0.5) return { \'rms\': rms, \'max\': max_val, \'minmax\': minmax, \'reverse\': reversed_fragment, \'stereo\': stereo_fragment }"},{"question":"# Coding Challenge Problem Statement You are given a list of dictionaries, where each dictionary represents a student\'s record containing their name, age, and list of scores in various subjects. Your task is to write a function that processes this list and returns the names of students who meet the following criteria: 1. The student\'s age is greater than or equal to `15`. 2. The student has an average score (rounded to two decimal places) of greater than or equal to `75` for all the subjects combined. Function Signature `def filter_students(records: List[Dict[str, Any]]) -> List[str]:` Input - `records` (List[Dict[str, Any]]): A list of dictionaries, where each dictionary contains: - `\'name\'`: A string representing the student\'s name. - `\'age\'`: An integer representing the student\'s age. - `\'scores\'`: A list of integers representing the student\'s scores in various subjects. Output - Returns a list of strings, where each string is the name of a student who meets the criteria listed above. Example ```python records = [ {\'name\': \'Alice\', \'age\': 16, \'scores\': [80, 85, 78]}, {\'name\': \'Bob\', \'age\': 14, \'scores\': [90, 78, 88]}, {\'name\': \'Charlie\', \'age\': 15, \'scores\': [75, 70, 80]}, {\'name\': \'David\', \'age\': 17, \'scores\': [60, 65, 70]} ] print(filter_students(records)) # Output: [\'Alice\', \'Charlie\'] ``` Constraints - All names, ages, and scores are guaranteed to be valid and non-null values. - The list of records is not empty. - The list of scores for each student contains at least one score. You are allowed to use list comprehensions, looping techniques, and other Python list and dictionary methods as necessary.","solution":"from typing import List, Dict, Any def filter_students(records: List[Dict[str, Any]]) -> List[str]: Filters students based on age and average score. Args: records (List[Dict[str, Any]]): List of student records, each containing \'name\', \'age\', and \'scores\'. Returns: List[str]: List of names of students who are eligible. eligible_students = [] for record in records: if record[\'age\'] >= 15: average_score = sum(record[\'scores\']) / len(record[\'scores\']) if average_score >= 75: eligible_students.append(record[\'name\']) return eligible_students"},{"question":"# Pandas Coding Assessment Question **Objective:** To evaluate your understanding of fundamental and advanced concepts in the pandas library through a series of tasks involving data manipulation, aggregation, and visualization. **Problem Statement:** You are given a CSV file named `sales_data.csv`, which contains sales records of a company. The CSV file has the following columns: - `Date`: The date of the sale (format: YYYY-MM-DD). - `Region`: The region where the sale was made (e.g., \'North\', \'South\', \'East\', \'West\'). - `Product`: The name of the product sold. - `Revenue`: The revenue generated from the sale (in USD). - `Quantity`: The quantity of the product sold. Your task is to write a Python function `analyze_sales_data(file_path: str) -> pd.DataFrame` that performs the following operations: 1. **Load the CSV file** into a pandas DataFrame. 2. **Add a new column** called `Month` which extracts the month from the `Date` column. 3. **Group the data** by `Region` and `Month`, then **calculate the total revenue** and the total quantity sold for each group. 4. **Create an additional column** called `Average Revenue per Sale` which is the total revenue divided by the total quantity sold for each group. 5. Return the resulting DataFrame sorted by `Region` and `Month` in ascending order. **Bonus Task:** Implement a function `plot_sales_data(df: pd.DataFrame) -> None` that takes the resulting DataFrame from `analyze_sales_data` and creates a line plot showing the trend of total revenue for each region over the months. Use different colors for each region. **Expectations:** - For `analyze_sales_data`, ensure that the DataFrame is correctly loaded, transformed, and grouped. - The final DataFrame should have the following columns: `Region`, `Month`, `Total Revenue`, `Total Quantity`, and `Average Revenue per Sale`. - For `plot_sales_data`, ensure that the visualization is clear and appropriately labeled. **Constraints:** - Assume the file path provided as `file_path` is always valid. - Use only public APIs of the pandas library. - Do not use private modules or functions. - Your solution should be efficient and avoid unnecessary computations. # Example Usage: ```python # Example CSV content: # Date,Region,Product,Revenue,Quantity # 2023-01-05,North,Widget,100,2 # 2023-01-07,South,Gadget,200,3 # 2023-02-10,North,Widget,150,3 # 2023-02-14,West,Gadget,300,5 file_path = \'sales_data.csv\' # Analyzing sales data result_df = analyze_sales_data(file_path) print(result_df) # Plotting sales data plot_sales_data(result_df) ``` # Evaluation Criteria: - Correctness: The implementation should meet all specified requirements and produce the correct result. - Code Quality: The solution should be well-structured, readable, and follow good coding practices. - Efficiency: The solution should efficiently handle the data without unnecessary computations. - Visualization (Bonus): The line plot should be clear, correct, and visually distinct for different regions.","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> pd.DataFrame: # Load data df = pd.read_csv(file_path) # Extract month from date df[\'Month\'] = pd.to_datetime(df[\'Date\']).dt.to_period(\'M\') # Group by Region and Month grouped_df = df.groupby([\'Region\', \'Month\']).agg( Total_Revenue=(\'Revenue\', \'sum\'), Total_Quantity=(\'Quantity\', \'sum\') ).reset_index() # Calculate Average Revenue per Sale grouped_df[\'Average_Revenue_per_Sale\'] = grouped_df[\'Total_Revenue\'] / grouped_df[\'Total_Quantity\'] # Sort the result by Region and Month sorted_df = grouped_df.sort_values(by=[\'Region\', \'Month\']) return sorted_df def plot_sales_data(df: pd.DataFrame) -> None: import matplotlib.pyplot as plt # Pivot the DataFrame for easier plotting pivot_df = df.pivot(index=\'Month\', columns=\'Region\', values=\'Total_Revenue\') # Plot the data pivot_df.plot(kind=\'line\', marker=\'o\') plt.title(\'Total Revenue by Region Over Time\') plt.xlabel(\'Month\') plt.ylabel(\'Total Revenue\') plt.legend(title=\'Region\') plt.grid(True) plt.show()"},{"question":"# Coding Challenge: File Compression and Decompression with Error Handling In this task, you will implement a function that compresses and decompresses files using the bzip2 compression algorithm. You are to handle both single-shot and incremental (de)compression as specified by user inputs. Additionally, you need to incorporate error handling to manage common exceptions. Function Signature ```python def handle_compression(input_file: str, output_file: str, mode: str, operation: str, compresslevel: int = 9) -> str: pass ``` Inputs - `input_file` (str): The path to the input file to be compressed or decompressed. - `output_file` (str): The path to the output file to be created after (de)compression. - `mode` (str): The mode of operation for file handling. It can be one of the following: - `\'one-shot\'`: Use single-shot (de)compression. - `\'incremental\'`: Use incremental (de)compression. - `operation` (str): Specifies whether to perform compression or decompression. It can be one of the following: - `\'compress\'`: Compress the input file. - `\'decompress\'`: Decompress the input file. - `compresslevel` (int): Compression level for the `BZ2Compressor`. It must be an integer between 1 and 9. Default is 9. Outputs - Returns a string message indicating the success or failure of the operation. Constraints - You must handle file reading and writing in both binary and text modes as appropriate. - Implement error handling to capture and manage exceptions that could occur during file operations (e.g., file not found, read/write errors). - Ensure efficient memory usage, particularly when using incremental (de)compression. Examples ```python # Example 1: One-shot compression handle_compression(\\"input.txt\\", \\"output.bz2\\", \\"one-shot\\", \\"compress\\", 5) # Expected output: \\"Compression completed successfully.\\" # Example 2: Incremental decompression handle_compression(\\"compressed.bz2\\", \\"decompressed.txt\\", \\"incremental\\", \\"decompress\\") # Expected output: \\"Decompression completed successfully.\\" ``` Notes - For single-shot operations, use `bz2.compress()` and `bz2.decompress()`. - For incremental operations, use `bz2.BZ2Compressor` and `bz2.BZ2Decompressor`. - Ensure that file handling is performed robustly, respecting resource management best practices like using `with` statements for file operations. - Provide meaningful error messages in cases of failure. Good luck, and happy coding!","solution":"import bz2 def handle_compression(input_file: str, output_file: str, mode: str, operation: str, compresslevel: int = 9) -> str: try: if mode not in [\'one-shot\', \'incremental\']: return \\"Error: Invalid mode. Must be \'one-shot\' or \'incremental\'.\\" if operation not in [\'compress\', \'decompress\']: return \\"Error: Invalid operation. Must be \'compress\' or \'decompress\'.\\" if not (1 <= compresslevel <= 9): return \\"Error: compresslevel must be an integer between 1 and 9.\\" if mode == \'one-shot\': if operation == \'compress\': with open(input_file, \'rb\') as f_in: data = f_in.read() compressed_data = bz2.compress(data, compresslevel) with open(output_file, \'wb\') as f_out: f_out.write(compressed_data) return \\"Compression completed successfully.\\" elif operation == \'decompress\': with open(input_file, \'rb\') as f_in: compressed_data = f_in.read() data = bz2.decompress(compressed_data) with open(output_file, \'wb\') as f_out: f_out.write(data) return \\"Decompression completed successfully.\\" elif mode == \'incremental\': if operation == \'compress\': compressor = bz2.BZ2Compressor(compresslevel) with open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: for chunk in iter(lambda: f_in.read(1024 * 1024), b\'\'): f_out.write(compressor.compress(chunk)) f_out.write(compressor.flush()) return \\"Compression completed successfully.\\" elif operation == \'decompress\': decompressor = bz2.BZ2Decompressor() with open(input_file, \'rb\') as f_in, open(output_file, \'wb\') as f_out: for chunk in iter(lambda: f_in.read(1024 * 1024), b\'\'): f_out.write(decompressor.decompress(chunk)) return \\"Decompression completed successfully.\\" except FileNotFoundError: return \\"Error: File not found.\\" except OSError as e: return f\\"Error: {e.strerror}\\" except Exception as e: return f\\"Error: Unexpected error occurred - {str(e)}\\""},{"question":"Objective Using the `seaborn.objects` package, create a bar plot visual that clearly presents the distribution of two different variables with overlapping data points in a dataset. Problem Statement You need to demonstrate your understanding of advanced Seaborn functionalities by visualizing data from a dataset and skillfully managing overlapping bars using `dodge`, `empty`, and other relevant parameters. Task 1. **Load the Dataset**: Use the `load_dataset` function to load the \\"tips\\" dataset. 2. **Create the Plot**: Generate a bar plot to visualize the distribution of `total_bill` across different `days` and `time` (lunch or dinner). 3. **Apply Transformations**: - Use `so.Dodge` to handle overlapping bars. - Ensure that there is no empty space for unused categories using the `empty` parameter. - Add gaps between the dodged bars. Expected Output - A well-formatted bar plot showing total bill amounts across different days and time categories, with clear handling of overlaps. - Ensure all relevant categories are shown without empty spaces. Input and Output Formats Use the `seaborn.objects` module for importing and manipulating the dataset, and for plotting the bar chart. Constraints - Use only the provided dataset (\\"tips\\"). - Apply transformations effectively to avoid overlapping elements and unnecessary empty spaces. - Ensure the plot is legible and informative. Solution Template ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot p = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\") # Add total aggregation of total_bill using bars, dodge them properly and ensure all necessary transformations are applied p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1, empty=\\"fill\\")) # Show the plot p.show() # or p.plot() based on your seaborn/plotting environment ``` Ensure to execute this code correctly and provide comments to explain each step. Your final output should be a visible bar plot as described.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot p = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") # Add total aggregation of total_bill using bars, dodge them properly and ensure all necessary transformations are applied p.add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1, empty=\\"fill\\")) # Show the plot p.show()"},{"question":"<|Analysis Begin|> The provided documentation thoroughly covers the `unittest.mock` library, including its main classes (`Mock`, `MagicMock`, `AsyncMock`, etc.), various functionalities like `patch`, `patch.dict`, `patch.object`, and `create_autospec`, as well as usage tips and specific features like `side_effect`, `return_value`, and mock assertions. It provides detailed explanations and examples of how to use these tools for creating mock behaviors, making assertions, and managing mock configurations. Main features highlighted: 1. The `Mock` and `MagicMock` classes with methods for setting and asserting mock behavior. 2. The `patch` decorators and their use in mocking classes, functions, and namespaces within tests. 3. Detailed descriptions of how to implement side effects, configure return values, and handle attributes. 4. The functionality to mock magic methods. 5. Methods for handling nested and multiple patches, using `patch.object`, `patch.dict`, and `patch.multiple`. 6. The `create_autospec` function to enforce method signatures similar to the real objects they mock. Due to the depth and breadth of the `unittest.mock` library, a good coding assessment question can focus on creating and managing mock objects, making assertions about their usage, and configuring them with specific behaviors. A problem that requires the student to demonstrate their understanding of creating mock objects and verifying interactions would be suitable. <|Analysis End|> <|Question Begin|> # Question: Mocking Database Interactions As a software developer, you are expected to write unit tests to check the integrity of your software components. In this task, you will write a Python function that interacts with a database and a corresponding unit test using the `unittest.mock` library to mock the database interactions. Function Implementation Implement a function `fetch_user_data(user_id: int) -> dict` that retrieves user data from the database. This function should: 1. Use a `Database` class instance to query user information. 2. Return a dictionary with the user data (`user_id`, `name`, `email`). 3. Raise a `ValueError` if the user is not found. Example: ```python class Database: def fetch_data(self, user_id: int) -> dict: # Simulates fetching data from the database pass def fetch_user_data(user_id: int) -> dict: db = Database() user_data = db.fetch_data(user_id) if user_data: return { \\"user_id\\": user_id, \\"name\\": user_data.get(\'name\'), \\"email\\": user_data.get(\'email\') } else: raise ValueError(f\\"User with id {user_id} not found\\") ``` Constraints 1. Assume the `Database` class and its `fetch_data` method are defined externally and you need to mock this interaction. 2. You should not make any actual database calls in your test; use mocking to simulate the `Database` class behavior. Unit Test Implementation Write a unit test for the `fetch_user_data` function to: 1. Verify that the function returns the correct data when the user exists. 2. Verify that the function raises a `ValueError` when the user does not exist. 3. Use the `unittest.mock` library to create mock objects and configure their behavior. Example: ```python import unittest from unittest.mock import patch, Mock from mymodule import fetch_user_data, Database # import the function and the class class TestFetchUserData(unittest.TestCase): @patch(\'mymodule.Database\') def test_fetch_user_data_success(self, MockDatabase): # Mock the Database instance mock_db = MockDatabase.return_value mock_db.fetch_data.return_value = {\'name\': \'John Doe\', \'email\': \'john@example.com\'} # Call the function result = fetch_user_data(1) # Assertions self.assertEqual(result, { \\"user_id\\": 1, \\"name\\": \'John Doe\', \\"email\\": \'john@example.com\' }) mock_db.fetch_data.assert_called_once_with(1) @patch(\'mymodule.Database\') def test_fetch_user_data_user_not_found(self, MockDatabase): # Mock the Database instance mock_db = MockDatabase.return_value mock_db.fetch_data.return_value = None # Call the function and assert it raises ValueError with self.assertRaises(ValueError): fetch_user_data(99) mock_db.fetch_data.assert_called_once_with(99) if __name__ == \'__main__\': unittest.main() ``` # Submission Submit your implementation of the `fetch_user_data` function and the corresponding unit tests. Ensure your tests cover both scenarios: when user data is found and when it is not found.","solution":"class Database: def fetch_data(self, user_id: int) -> dict: # Simulates fetching data from the database pass def fetch_user_data(user_id: int) -> dict: db = Database() user_data = db.fetch_data(user_id) if user_data: return { \\"user_id\\": user_id, \\"name\\": user_data.get(\'name\'), \\"email\\": user_data.get(\'email\') } else: raise ValueError(f\\"User with id {user_id} not found\\")"},{"question":"**Objective:** You are required to demonstrate your understanding of `seaborn.objects` by implementing a function that processes data, applies specified transformations on plot limits, and creates a custom plot. **Task:** Write a function `custom_plot(data: dict, x_limits: tuple, y_limits: tuple) -> None` that performs the following: 1. Takes in a dictionary `data` with keys `\'x\'` and `\'y\'`, representing lists of x and y values respectively, and tuples `x_limits` and `y_limits` representing the desired axis limits for x and y axes respectively. 2. Creates a line plot using the provided data. 3. Applies the specified `x_limits` and `y_limits` to the plot. 4. Saves the resulting plot as a PNG file named `\'custom_plot.png\'`. **Input:** - `data`: A dictionary with keys `\'x\'` and `\'y\'`. Each key maps to a list of integers or float values of the same length. - `x_limits`: A tuple of two elements (min, max) specifying the limits for the x-axis. One or both elements can be `None`, which will maintain the default value for that limit. - `y_limits`: A tuple of two elements (min, max) specifying the limits for the y-axis. One or both elements can be `None`, which will maintain the default value for that limit. **Output:** - The function does not return anything but saves the plot as a PNG file named `\'custom_plot.png\'`. **Constraints:** - You can assume that the lists in `data` have at least two elements. - You are required to use the `seaborn.objects` interface for this task. **Example:** ```python data = {\'x\': [1, 2, 3, 4], \'y\': [10, 20, 15, 25]} x_limits = (0, 5) y_limits = (5, 30) custom_plot(data, x_limits, y_limits) ``` Expected output: A file named `\'custom_plot.png\'` should be created with the specified x and y limits. **Note:** You can refer to the seaborn documentation provided for any helpers or example code snippets to implement this function. Ensure to test your code and validate the resulting file format and limits applied.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plot(data, x_limits, y_limits): Creates a line plot using seaborn, applies specified limits, and saves the plot as a PNG file. Parameters: data (dict): A dictionary with keys \'x\' and \'y\', containing lists of x and y values. x_limits (tuple): A tuple (min, max) specifying limits for the x-axis. None indicates no limit. y_limits (tuple): A tuple (min, max) specifying limits for the y-axis. None indicates no limit. # Create a Seaborn plot p = sns.lineplot(x=data[\'x\'], y=data[\'y\']) # Apply x and y limits if x_limits[0] is not None: plt.xlim(left=x_limits[0]) if x_limits[1] is not None: plt.xlim(right=x_limits[1]) if y_limits[0] is not None: plt.ylim(bottom=y_limits[0]) if y_limits[1] is not None: plt.ylim(top=y_limits[1]) # Save the plot to a file plt.savefig(\'custom_plot.png\') plt.close()"},{"question":"Problem Statement Implement a class `CustomFileHandler` that provides high-level file I/O functionalities over a text file. Your class should handle both reading and writing operations with specific encoding and error handling features. # Requirements 1. **Initialization:** - The class should be initialized with `file_path`, `mode`, `encoding`, and `errors`. Use default values where appropriate. - Use `io.open()` to open the file. 2. **Writing Method:** - Implement a method `write_data(data: str) -> None`. - This method should write the provided string `data` to the file. - It should handle exceptions raised due to encoding issues and print an appropriate message. 3. **Reading Method:** - Implement a method `read_data() -> str`. - This method should read the entire content of the file and return it as a string. - Include an optional parameter to specify the maximum number of characters to be read. 4. **Handling Context Manager:** - Implement the context management methods `__enter__` and `__exit__` to use the class with a `with` statement. 5. **File Closing:** - Ensure the file is properly closed after any operation or if any exception occurs. # Input - `file_path: str` - Path to the file to be handled. - `mode: str` - Mode in which the file should be opened (default is \'r+\'). - `encoding: str` - Encoding to be used for text operations (default is \'utf-8\'). - `errors: str` - Error handling scheme (default is \'strict\'). # Constraints - Ensure that the methods handle appropriate exceptions and do not crash the program. - Utilize the `io` module\'s facilities for handling various I/O operations. # Expected Output ```python # Example usage with CustomFileHandler(\'sample.txt\', \'w+\', encoding=\'utf-8\', errors=\'replace\') as handler: handler.write_data(\'Hello, World!\') with CustomFileHandler(\'sample.txt\', \'r\', encoding=\'utf-8\') as handler: content = handler.read_data() print(content) # Output: Hello, World! ``` **Starter Code:** ```python import io class CustomFileHandler: def __init__(self, file_path, mode=\'r+\', encoding=\'utf-8\', errors=\'strict\'): self.file_path = file_path self.mode = mode self.encoding = encoding self.errors = errors self.file = None def __enter__(self): # Implement context management entry pass def __exit__(self, exc_type, exc_val, exc_tb): # Implement context management exit pass def write_data(self, data): # Implement the write data method pass def read_data(self, size=-1): # Implement the read data method pass # Example usage with CustomFileHandler(\'sample.txt\', \'w+\', encoding=\'utf-8\', errors=\'replace\') as handler: handler.write_data(\'Hello, World!\') with CustomFileHandler(\'sample.txt\', \'r\', encoding=\'utf-8\') as handler: content = handler.read_data() print(content) # Output: Hello, World! ``` Complete the implementation of the `CustomFileHandler` class based on the problem statement above.","solution":"import io class CustomFileHandler: def __init__(self, file_path, mode=\'r+\', encoding=\'utf-8\', errors=\'strict\'): self.file_path = file_path self.mode = mode self.encoding = encoding self.errors = errors self.file = None def __enter__(self): self.file = io.open(self.file_path, self.mode, encoding=self.encoding, errors=self.errors) return self def __exit__(self, exc_type, exc_val, exc_tb): if self.file is not None: self.file.close() def write_data(self, data): try: self.file.write(data) self.file.flush() except UnicodeEncodeError as e: print(f\\"Encoding error: {e}\\") def read_data(self, size=-1): try: return self.file.read(size) except UnicodeDecodeError as e: print(f\\"Decoding error: {e}\\") return \\"\\" # Example usage with CustomFileHandler(\'sample.txt\', \'w+\', encoding=\'utf-8\', errors=\'replace\') as handler: handler.write_data(\'Hello, World!\') with CustomFileHandler(\'sample.txt\', \'r\', encoding=\'utf-8\') as handler: content = handler.read_data() print(content) # Output: Hello, World!"},{"question":"# Managing and Serializing Complex Email Messages You have been provided with the following email message content in string format: ``` From: user@example.com To: recipient@example.com Subject: Test Email Content-Type: multipart/mixed; boundary=\\"===============boundary==\\" --===============boundary== Content-Type: text/plain Hello, this is the plain text part of the email. --===============boundary== Content-Type: text/html <html> <body> <p>Hello, this is the <b>HTML</b> part of the email.</p> </body> </html> --===============boundary==-- ``` Your task is to write a Python function that: 1. **Parses the given email string to a `Message` object.** 2. **Adds a new header \\"X-Unique-Id\\" with a unique identifier value to the message.** 3. **Adds an attachment to the email with the following properties:** - Content: `b\'This is a simple attachment\'` - MIME type: `application/octet-stream` - Filename: `attachment.txt` 4. **Returns the serialized string representation of the updated email message with all changes applied.** # Function Signature ```python def manage_email_message(email_content: str) -> str: pass ``` # Constraints: - The input email content will be well-formed and follow standard email formatting rules. - The function should handle text and HTML content properly. - Use the `email` package\'s `Message` class and relevant methods for parsing and manipulating the email content. - Make sure the returned string representation includes the newly added header and attachment properly formatted. # Example ```python email_content = From: user@example.com To: recipient@example.com Subject: Test Email Content-Type: multipart/mixed; boundary=\\"===============boundary==\\" --===============boundary== Content-Type: text/plain Hello, this is the plain text part of the email. --===============boundary== Content-Type: text/html <html> <body> <p>Hello, this is the <b>HTML</b> part of the email.</p> </body> </html> --===============boundary==-- result = manage_email_message(email_content) print(result) ``` The output should include the new \\"X-Unique-Id\\" header and the new attachment part with the specified content.","solution":"import email from email import policy from email.parser import BytesParser from email.generator import BytesGenerator from email.mime.application import MIMEApplication from email.utils import make_msgid def manage_email_message(email_content: str) -> str: Manages an email message by adding a unique identifier header and an attachment. Args: email_content (str): The raw email content as a string. Returns: str: The updated email content as a string. # Parse the email content msg = BytesParser(policy=policy.default).parsebytes(email_content.encode(\'utf-8\')) # Add a new header \\"X-Unique-Id\\" with a unique identifier value unique_id = make_msgid() msg[\'X-Unique-Id\'] = unique_id # Create an attachment attachment = MIMEApplication(b\'This is a simple attachment\', _subtype=\'octet-stream\') attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\'attachment.txt\') # Append the attachment to the message msg.attach(attachment) # Serialize to string from io import BytesIO output = BytesIO() BytesGenerator(output, policy=policy.default).flatten(msg) return output.getvalue().decode(\'utf-8\')"},{"question":"Write a Python function `pack_and_unpack_data(data_list, format_string)` that performs the following tasks: 1. **Pack the data**: Convert a list of Python values (`data_list`) into a bytes object using the `struct.pack` method with a given `format_string`. 2. **Unpack the data**: Convert the packed bytes object back into a tuple of Python values using the `struct.unpack` method with the same `format_string`. 3. **Return the result**: Return a tuple containing the original `data_list`, the packed bytes object, and the unpacked data tuple. Function Signature ```python def pack_and_unpack_data(data_list: list, format_string: str) -> tuple: pass ``` Input - `data_list`: A list of Python values (integers, floats, bytes) to be packed. - `format_string`: A string that specifies the format for packing and unpacking the data. Output - A tuple containing three elements: 1. The original `data_list`. 2. The packed bytes object. 3. The unpacked data as a tuple. Constraints - The length and types of elements in `data_list` must exactly match those specified by the `format_string`. - Ensure that the `format_string` is correctly applied for both packing and unpacking. - Handle any potential errors by raising appropriate exceptions. Example ```python # Example input data = [1, 2.0, b\'A\'] format_str = \'i f c\' # Function execution result = pack_and_unpack_data(data, format_str) # Example output print(result) # ([1, 2.0, b\'A\'], b\'x01x00x00x00x00x00x00x00...x00A\', (1, 2.0, b\'A\')) ``` **Note**: The packed bytes object may look different based on the system architecture and the exact content of the bytes. The main focus should be on correctly packing and unpacking the data based on the supplied format string. Additional Notes - Consider edge cases such as incorrect format strings or data types that do not match the format layout. - The format string can include byte order markers (\'@\', \'=\', \'<\', \'>\', \'!\') and data type markers (e.g., \'i\' for int, \'f\' for float, \'c\' for char). Implement the function `pack_and_unpack_data(data_list, format_string)`.","solution":"import struct def pack_and_unpack_data(data_list, format_string): Packs a list of data into a bytes object and unpacks it back into a tuple. :param data_list: List of data to be packed. :param format_string: Struct format string. :return: Tuple containing original data_list, packed bytes object, and unpacked data tuple. packed_data = struct.pack(format_string, *data_list) unpacked_data = struct.unpack(format_string, packed_data) return data_list, packed_data, unpacked_data"},{"question":"# Question: Create a Text Adventure Game Using Python `curses` Module As a programming challenge, you will create a simple text adventure game using the `curses` module. In this game, the player explores a grid-based dungeon, moving their character \'@\' around and avoiding or interacting with various objects. Requirements: 1. **Dungeon Layout**: - The dungeon is a grid of 20x20 characters. - Walls (`#`) are placed around the perimeter of the dungeon. - Place 10 treasure chests (``) at random locations within the dungeon. - Place 5 monsters (`M`) at random locations within the dungeon. - The player starts at position `(1, 1)` within the dungeon. 2. **Player Movement**: - The player can move up, down, left, or right using the arrow keys. - The player cannot move into walls or out of the dungeon bounds. - Each move will refresh the dungeon display. 3. **Interactions**: - If the player moves to a position containing a treasure chest (``), the treasure chest disappears and the player collects it. - If the player moves to a position containing a monster (`M`), the game ends and displays \\"Game Over\\". 4. **Terminal Graphics**: - Use `curses` to handle window creation and user input. - Draw the dungeon, walls, treasures, monsters, and player within the `curses` window. - Display a message at the bottom of the window indicating the player\'s collected treasures and the current status. Constraints: - You must use the `curses` module for terminal handling. - Implement appropriate functions to initialize the dungeon, handle player movement, and update the display. - Use exception handling to manage `curses` window initialization and termination properly. Example Input/Output: ``` # #@ # # # # # # # # # # # # # # # # M # # # # # # # # # # # # # # # # # # # # Treasures Collected: 0 ``` When the player moves to position (4, 9): ``` # # # # # # # # @ # # # # # # # # # # M # # # # # # # # # # # # # # # # # # # # Treasures Collected: 1 ``` If the player moves to the monster\'s position: ``` # # # # # # # # # # # # # # # # # # @ # # # # # # # # # # # # # # # # # # # # Game Over ``` # Implementation Details: You can use the following template to start your implementation. Make sure to properly handle the `curses` initialization and cleanup to ensure the terminal state is restored correctly if your script exits prematurely. ```python import curses import random def start_curses_app(stdscr): curses.curs_set(0) # Hide cursor stdscr.keypad(True) # Enable special keys curses.start_color() # Initialize the dungeon create_dungeon(stdscr) # Game loop while True: key = stdscr.getch() # Handle player movement and interactions here if game_over: break # Cleanup stdscr.keypad(False) curses.endwin() def create_dungeon(stdscr): # Code to create walls, treasures, monsters, and player pass def main(): curses.wrapper(start_curses_app) if __name__ == \\"__main__\\": main() ``` Ensure your solution effectively demonstrates the use of the `curses` module by leveraging its functionalities to manage the terminal-based game state dynamically.","solution":"import curses import random def start_curses_app(stdscr): curses.curs_set(0) # Hide cursor stdscr.keypad(True) # Enable special keys curses.start_color() height, width = 20, 20 treasures_collected = 0 game_over = False # Create dungeon and place elements dungeon, player_pos = create_dungeon(height, width) while True: stdscr.clear() render_dungeon(stdscr, dungeon, height, width, player_pos, treasures_collected) key = stdscr.getch() if key in [curses.KEY_UP, curses.KEY_DOWN, curses.KEY_LEFT, curses.KEY_RIGHT]: player_pos, treasures_collected, game_over = handle_movement(key, player_pos, dungeon, treasures_collected) if game_over: stdscr.addstr(height, 0, \\"Game Over. Press any key to exit.\\") stdscr.refresh() stdscr.getch() break stdscr.keypad(False) curses.endwin() def create_dungeon(height, width): # Create the dungeon structure dungeon = [[\' \' for _ in range(width)] for _ in range(height)] for i in range(height): dungeon[i][0] = dungeon[i][width - 1] = \'#\' for j in range(width): dungeon[0][j] = dungeon[height - 1][j] = \'#\' # Place treasures for _ in range(10): place_randomly(dungeon, height, width, \'\') # Place monsters for _ in range(5): place_randomly(dungeon, height, width, \'M\') player_pos = (1, 1) dungeon[player_pos[0]][player_pos[1]] = \'@\' return dungeon, player_pos def place_randomly(dungeon, height, width, item): while True: x = random.randint(1, height - 2) y = random.randint(1, width - 2) if dungeon[x][y] == \' \': dungeon[x][y] = item break def render_dungeon(stdscr, dungeon, height, width, player_pos, treasures_collected): for i in range(height): for j in range(width): stdscr.addch(i, j, dungeon[i][j]) stdscr.addstr(height, 0, f\'Treasures Collected: {treasures_collected}\') def handle_movement(key, player_pos, dungeon, treasures_collected): x, y = player_pos if key == curses.KEY_UP: new_pos = (x - 1, y) elif key == curses.KEY_DOWN: new_pos = (x + 1, y) elif key == curses.KEY_LEFT: new_pos = (x, y - 1) elif key == curses.KEY_RIGHT: new_pos = (x, y + 1) new_x, new_y = new_pos if dungeon[new_x][new_y] == \'#\': return player_pos, treasures_collected, False elif dungeon[new_x][new_y] == \'M\': return new_pos, treasures_collected, True elif dungeon[new_x][new_y] == \'\': treasures_collected += 1 dungeon[x][y] = \' \' dungeon[new_x][new_y] = \'@\' return new_pos, treasures_collected, False def main(): curses.wrapper(start_curses_app) if __name__ == \\"__main__\\": main()"},{"question":"Problem Statement: You are tasked with implementing a simplified version of the `netrc` class that reads and parses a netrc file format. The class should be able to provide authentication information for a given host and handle errors appropriately. Class Definition: Design a class named `SimpleNetrc` with the following requirements: 1. **Initialization**: - Accept an optional file argument. - If no argument is given, raise a `FileNotFoundError`. - If the file is provided but does not exist, raise a `FileNotFoundError`. 2. **Methods**: - `authenticators(host)`: - Input: a string `host` representing the hostname. - Output: a tuple `(login, account, password)` if the host or a default entry exists. - If the host is not found and no default entry exists, return `None`. - `__repr__()`: - Output: a string representation of the class data in netrc format. 3. **Instance Variables**: - `hosts`: - A dictionary mapping hostnames to tuples `(login, account, password)`. - `macros`: - A dictionary mapping macro names to string lists. 4. **Error Handling**: - Raise a `ParsingError` (custom Exception) if there are errors in the format of the netrc file. File Format: The netrc file follows a specific format where login credentials for hosts are stored. Each entry follows this format: ``` machine <hostname> login <username> account <account_name> password <password> ``` Entries can be repeated for multiple hosts. A default entry is specified using: ``` default login <username> account <account_name> password <password> ``` Example of a netrc file: ``` machine host1 login user1 account account1 password pass1 default login default_user account default_account password default_pass ``` Constraints: - The `login`, `account`, and `password` fields are mandatory for each host entry. - Handle only ASCII punctuation for passwords. - Whitespace and non-printable characters are not allowed in passwords. Implementation: ```python class ParsingError(Exception): pass class SimpleNetrc: def __init__(self, file=None): self.hosts = {} self.macros = {} if file is None: raise FileNotFoundError(\\"No netrc file provided and default not applicable.\\") try: with open(file, \'r\') as f: self._parse(f) except FileNotFoundError: raise FileNotFoundError(f\\"No such file: \'{file}\'\\") def _parse(self, f): # Implement the parsing logic here pass def authenticators(self, host): # Implement the logic to return authenticators for a given host pass def __repr__(self): # Implement the logic to return a string representation of class data pass # Example Usage netrc = SimpleNetrc(\\"path_to_netrc_file\\") auth_info = netrc.authenticators(\\"host1\\") print(auth_info) ``` Notes: - You will need to implement the `_parse` method to read the file and populate the `hosts` and `macros` attributes. - Ensure that you handle file reading and parsing errors effectively.","solution":"class ParsingError(Exception): pass class SimpleNetrc: def __init__(self, file=None): self.hosts = {} self.macros = {} if file is None: raise FileNotFoundError(\\"No netrc file provided and default not applicable.\\") try: with open(file, \'r\') as f: self._parse(f) except FileNotFoundError: raise FileNotFoundError(f\\"No such file: \'{file}\'\\") def _parse(self, f): current_host = None for line in f: line = line.strip() if line.startswith(\\"machine \\"): parts = line.split(\\" \\", 1) if len(parts) < 2: raise ParsingError(\\"Invalid machine entry line\\") current_host = parts[1].strip() if current_host not in self.hosts: self.hosts[current_host] = {} elif line.startswith(\\"default\\"): current_host = \\"default\\" if current_host not in self.hosts: self.hosts[current_host] = {} elif current_host: parts = line.split(None, 1) if len(parts) != 2: raise ParsingError(\\"Invalid entry line\\") key, value = parts self.hosts[current_host][key] = value else: raise ParsingError(\\"Host entry expected before details\\") for host in self.hosts: if \'login\' not in self.hosts[host] or \'password\' not in self.hosts[host]: raise ParsingError(\\"Login and password are mandatory for each host\\") def authenticators(self, host): if host in self.hosts: info = self.hosts[host] return (info.get(\'login\'), info.get(\'account\'), info.get(\'password\')) elif \\"default\\" in self.hosts: info = self.hosts[\\"default\\"] return (info.get(\'login\'), info.get(\'account\'), info.get(\'password\')) else: return None def __repr__(self): lines = [] for host in self.hosts: if host != \\"default\\": lines.append(f\\"machine {host}\\") else: lines.append(\\"default\\") for key in self.hosts[host]: lines.append(f\\" {key} {self.hosts[host][key]}\\") return \\"n\\".join(lines)"},{"question":"Objective Create a WSGI-compliant web application using the `wsgiref` package. Implement a simple WSGI application that returns a plain text greeting message along with a list of environment variables received in the request. Use `wsgiref.validate` to ensure the application conforms to the WSGI specification, and serve the application using `wsgiref.simple_server`. Requirements 1. **Create a WSGI application**: - Implement a function `greeting_app` that takes two parameters (`environ` and `start_response`). - The application should set the HTTP status to `200 OK` and the `Content-Type` to `text/plain`. - The application should return a greeting message and a list of environment variables in the HTTP response body. 2. **Validate the WSGI application**: - Use `wsgiref.validate.validator` to wrap the `greeting_app` and validate its WSGI compliance. 3. **Serve the WSGI application**: - Use `wsgiref.simple_server.make_server` to create an HTTP server to serve the validated application. - The server should listen on `localhost` and port `8080`. Input - No input from the user is required during runtime. Output - The application should output a plain text response with a greeting message and the environment variables. Constraints - Follow WSGI specification as closely as possible. Example Response ``` Hello, this is your WSGI application! Environment Variables: REQUEST_METHOD: GET PATH_INFO: / ... ``` Code Template ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator def greeting_app(environ, start_response): # [1] Set the HTTP status and headers status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) # [2] Generate the response body response_body = [ b\\"Hello, this is your WSGI application!n\\", b\\"nEnvironment Variables:n\\" ] for key, value in environ.items(): response_body.append(f\\"{key}: {value}n\\".encode(\\"utf-8\\")) # [3] Return the response body as a list of byte strings return response_body # [4] Wrap the application with the validator validated_app = validator(greeting_app) # [5] Serve the application using wsgiref.simple_server.make_server if __name__ == \'__main__\': with make_server(\'localhost\', 8080, validated_app) as httpd: print(\\"Serving on http://localhost:8080/ ...\\") httpd.serve_forever() ``` Instructions: - Complete the function `greeting_app` to fulfill the requirements. - Ensure to wrap it with the validator. - Run the server, and it should serve the application on `http://localhost:8080/`. Validate your implementation by visiting `http://localhost:8080/` in a web browser and checking the response.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator def greeting_app(environ, start_response): WSGI application that returns a greeting message and a list of environment variables. # Set the HTTP status and headers status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) # Generate the response body response_body = [ b\\"Hello, this is your WSGI application!n\\", b\\"nEnvironment Variables:n\\" ] for key, value in environ.items(): response_body.append(f\\"{key}: {value}n\\".encode(\\"utf-8\\")) # Return the response body as a list of byte strings return response_body # Wrap the application with the validator validated_app = validator(greeting_app) # Serve the application using wsgiref.simple_server.make_server if __name__ == \'__main__\': with make_server(\'localhost\', 8080, validated_app) as httpd: print(\\"Serving on http://localhost:8080/ ...\\") httpd.serve_forever()"},{"question":"# Assessment Question **Objective**: This assessment aims to evaluate your understanding of Seaborn for creating structured multi-plot grids. You will use Seaborn\'s `FacetGrid` and `PairGrid` to visualize relationships in datasets. # Question Consider the following dataset of cars which contains information on `mpg` (miles per gallon), `cylinders` (number of cylinders), `horsepower`, and `origin` (categorical variable representing the car\'s origin: `USA`, `Europe`, `Japan`). ```python import seaborn as sns import pandas as pd data = { \\"mpg\\": [18, 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 13, 14, 15, 13, 14, 14, 12, 13, 13, 14, 15, 14, 15, 16, 16, 15, 16, 14, 13, 14], \\"cylinders\\": [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], \\"horsepower\\": [130, 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 160, 150, 225, 210, 193, 155, 175, 165, 170, 160, 140, 165, 150, 170, 175, 170, 160, 140, 165, 130, 150, 145], \\"origin\\": [\\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\"] } cars = pd.DataFrame(data) ``` 1. **FacetGrid Visualization**: - Use the `FacetGrid` class to visualize the distribution of `mpg` across different `origin` of cars. - Present the data using histograms. 2. **PairGrid Visualization**: - Initialize a `PairGrid` for the `cars` dataset that includes `mpg`, `horsepower`, `cylinders`, and visualizes pairwise relationships. - Use scatter plots for the pairwise relationships. - Color the points by the `origin`. 3. **Matplotlib Customization**: - Customize the `FacetGrid` plots by adding relevant axis labels and a title. - Add a regression line to the scatter plots in the `PairGrid`. # Expected Input and Output - **Input**: The provided dataset `cars` and your code to create customized visualizations. - **Output**: Two visualizations: one `FacetGrid` showing `mpg` distributions and one `PairGrid` visualizing pairwise relationships, including a regression line. # Constraints - Use Seaborn for primary visualization tasks and Matplotlib only for additional customizations. - Ensure the plots are well-labeled and clear. # Performance Requirements - The code should efficiently handle datasets of similar size and structure to the provided example.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np # Define the dataset data = { \\"mpg\\": [18, 15, 18, 16, 17, 15, 14, 14, 14, 15, 15, 14, 15, 13, 14, 15, 13, 14, 14, 12, 13, 13, 14, 15, 14, 15, 16, 16, 15, 16, 14, 13, 14], \\"cylinders\\": [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8], \\"horsepower\\": [130, 165, 150, 150, 140, 198, 220, 215, 225, 190, 170, 160, 150, 225, 210, 193, 155, 175, 165, 170, 160, 140, 165, 150, 170, 175, 170, 160, 140, 165, 130, 150, 145], \\"origin\\": [\\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\", \\"USA\\"] } cars = pd.DataFrame(data) # FacetGrid Visualization def plot_facetgrid(cars): g = sns.FacetGrid(cars, col=\\"origin\\", margin_titles=True) g.map(plt.hist, \\"mpg\\", bins=np.arange(10, 45, 5), color=\\"skyblue\\", edgecolor=\\"black\\") g.set_axis_labels(\\"Miles Per Gallon (mpg)\\", \\"Frequency\\") g.fig.suptitle(\\"Distribution of MPG Across Car Origins\\", y=1.02) plt.show() # PairGrid Visualization def plot_pairgrid(cars): g = sns.PairGrid(cars, vars=[\\"mpg\\", \\"horsepower\\", \\"cylinders\\"], hue=\\"origin\\") g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) g.map_lower(sns.regplot, scatter_kws={\'s\': 10}, line_kws={\'color\': \'red\'}) g.add_legend() g.fig.suptitle(\\"PairGrid of MPG, Horsepower, Cylinders by Origin\\", y=1.02) plt.show()"},{"question":"Email Content Extraction and Filtering # Background Given an email message object, it might contain various types of content parts, such as plain text, HTML text, images, attachments, etc. To efficiently handle, inspect, or process such emails, you might need to extract specific parts of the content or filter them based on MIME types. # Task Given an email message object, implement the following function: ```python def extract_and_filter_email_content(msg, maintype=\'text\', subtype=None, decode=False): Extract and filter email content based on MIME types. Parameters: msg (email.message.Message): The email message object to be processed. maintype (str): The main MIME type of the subparts to be extracted. Default is \'text\'. subtype (str, optional): The subtype of the MIME type to be extracted. Default is None, meaning all subtypes under the main type are included. decode (bool): Flag indicating whether to decode the payload of the email subparts. Returns: dict: A dictionary with keys as part description (e.g., \\"text/plain\\", \\"text/html\\") and values as lists of line-by-line content of each matching subpart. pass ``` # Input Format - `msg` is an email.message.Message object. - `maintype` (default: \'text\') is a string representing the main MIME type of the subparts that need to be extracted. - `subtype` (optional, default: None) is a string representing the specific subtype of the MIME type that needs to be matched. If not provided, all subtypes under the main type will be included. - `decode` (default: False) determines whether to decode the payloads of the email subparts. # Output Format - Return a dictionary where each key is the MIME type (\\"maintype/subtype\\") of a matching subpart and the corresponding value is a list of strings, representing the lines of content in that subpart. # Constraints - The email message can have a complex nested structure, but you only need to consider subparts at any level matching the provided MIME type criteria. - The function should handle both single-part and multi-part email messages. - Ensure the function efficiently uses the provided iterators from the `email.iterators` module for processing. # Example Usage: ```python from email import message_from_string import email.iterators # Example email message string (for testing purposes) raw_email = MIME-Version: 1.0 Content-Type: multipart/alternative; boundary=\\"000000000000\\" --000000000000 Content-Type: text/plain; charset=\\"UTF-8\\" Hello, This is a plain text part of the email message. --000000000000 Content-Type: text/html; charset=\\"UTF-8\\" <html> <body> <p>Hello,</p> <p>This is an HTML part of the email message.</p> </body> </html> --000000000000-- # Convert raw email string to email.message.Message object msg = message_from_string(raw_email) # Function usage result = extract_and_filter_email_content(msg, maintype=\'text\', subtype=\'plain\', decode=True) # Expected output: {\'text/plain\': [\'Hello,\', \'This is a plain text part of the email message.\']} print(result) ``` Implement the `extract_and_filter_email_content` function to correctly filter and extract email content based on the specified MIME types.","solution":"from email import message_from_string from email.iterators import typed_subpart_iterator def extract_and_filter_email_content(msg, maintype=\'text\', subtype=None, decode=False): Extract and filter email content based on MIME types. Parameters: msg (email.message.Message): The email message object to be processed. maintype (str): The main MIME type of the subparts to be extracted. Default is \'text\'. subtype (str, optional): The subtype of the MIME type to be extracted. Default is None, meaning all subtypes under the main type are included. decode (bool): Flag indicating whether to decode the payload of the email subparts. Returns: dict: A dictionary with keys as part description (e.g., \\"text/plain\\") and values as lists of line-by-line content of each matching subpart. content_dict = {} for part in typed_subpart_iterator(msg, maintype, subtype): part_type = part.get_content_type() payload = part.get_payload(decode=decode) # Ensure payload is a string if decode and isinstance(payload, bytes): charset = part.get_content_charset() or \'utf-8\' payload = payload.decode(charset) lines = payload.splitlines() if part_type not in content_dict: content_dict[part_type] = [] content_dict[part_type].extend(lines) return content_dict"},{"question":"Implement a function `visualize_hls_palettes` that takes in four parameters: `num_colors`, `lightness`, `saturation`, and `hue_start`. The function should generate and visualize four different HLS palettes using seaborn based on the provided parameters: 1. A palette with `num_colors` colors. 2. A palette with `num_colors` colors and adjusted lightness. 3. A palette with `num_colors` colors and adjusted saturation. 4. A palette with `num_colors` colors and adjusted hue start. Ensure the function meets the following requirements: - The function should generate each of the four palettes using seaborn\'s `hls_palette` function. - It should then visualize each palette using seaborn\'s `palplot` function. - The visualization should be displayed in a single plot with 4 subplots (2x2 grid). ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_hls_palettes(num_colors, lightness, saturation, hue_start): Visualize different HLS palettes based on the provided parameters. Args: num_colors (int): The number of colors to include in the palette. lightness (float): Lightness value in the palette (0 to 1). saturation (float): Saturation value in the palette (0 to 1). hue_start (float): Starting point for hue sampling (0 to 1). Returns: None plt.figure(figsize=(10, 8)) # Default palette with num_colors plt.subplot(2, 2, 1) sns.palplot(sns.hls_palette(num_colors)) plt.title(\\"Default Palette\\") # Palette with adjusted lightness plt.subplot(2, 2, 2) sns.palplot(sns.hls_palette(num_colors, l=lightness)) plt.title(\\"Adjusted Lightness\\") # Palette with adjusted saturation plt.subplot(2, 2, 3) sns.palplot(sns.hls_palette(num_colors, s=saturation)) plt.title(\\"Adjusted Saturation\\") # Palette with adjusted hue start plt.subplot(2, 2, 4) sns.palplot(sns.hls_palette(num_colors, h=hue_start)) plt.title(\\"Adjusted Hue Start\\") plt.tight_layout() plt.show() # Example usage visualize_hls_palettes(8, 0.3, 0.3, 0.5) ``` # Example Usage: ```python visualize_hls_palettes(8, 0.3, 0.3, 0.5) ``` This will generate a plot with four subplots, each showing different color palettes based on the parameters. Adjust the parameters to see the changes visually. # Constraints: - `num_colors`: An integer value greater than 0. - `lightness`, `saturation`, `hue_start`: Float values between 0 and 1.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_hls_palettes(num_colors, lightness, saturation, hue_start): Visualize different HLS palettes based on the provided parameters. Args: num_colors (int): The number of colors to include in the palette. lightness (float): Lightness value in the palette (0 to 1). saturation (float): Saturation value in the palette (0 to 1). hue_start (float): Starting point for hue sampling (0 to 1). Returns: None plt.figure(figsize=(10, 8)) # Default palette with num_colors plt.subplot(2, 2, 1) sns.palplot(sns.hls_palette(num_colors)) plt.title(\\"Default Palette\\") # Palette with adjusted lightness plt.subplot(2, 2, 2) sns.palplot(sns.hls_palette(num_colors, l=lightness)) plt.title(\\"Adjusted Lightness\\") # Palette with adjusted saturation plt.subplot(2, 2, 3) sns.palplot(sns.hls_palette(num_colors, s=saturation)) plt.title(\\"Adjusted Saturation\\") # Palette with adjusted hue start plt.subplot(2, 2, 4) sns.palplot(sns.hls_palette(num_colors, h=hue_start)) plt.title(\\"Adjusted Hue Start\\") plt.tight_layout() plt.show()"},{"question":"# Generator Object Management Objective In this exercise, you will demonstrate your understanding of Python generators by implementing a function that simulates a real-world scenario and utilizes generator functions. # Problem Statement Write a function `data_stream_generator` that simulates a data stream, yielding data items one at a time. Function Specification - `def data_stream_generator(data: List[int]) -> Generator[int, None, None]:` - **Input**: A list of integers `data` - **Output**: A generator that yields integers from the list one at a time Constraints 1. You should use `yield` to create the generator. 2. The input list can have a length of up to 10^6. 3. You may not use external libraries for this implementation. 4. **Performance**: Your solution should be optimized to handle large lists efficiently. Examples 1. Example 1 ```python data = [1, 2, 3, 4, 5] gen = data_stream_generator(data) result = [] for item in gen: result.append(item) assert result == [1, 2, 3, 4, 5] ``` 2. Example 2 ```python data = [10, 20, 30] gen = data_stream_generator(data) result = list(gen) assert result == [10, 20, 30] ``` # Notes - You should write additional test cases to verify the correctness of your implementation. - The generator should not load all data at once; it should yield items one by one to mimic streaming. - Make sure your generator can handle streams larger than the available memory efficiently.","solution":"from typing import List, Generator def data_stream_generator(data: List[int]) -> Generator[int, None, None]: A generator that yields integers from the input list one at a time. :param data: List of integers :return: A generator that yields each integer from the list for item in data: yield item"},{"question":"**Question: Implement a Data Preprocessing Pipeline** You are provided with a dataset containing both numerical and categorical features. Your task is to implement a preprocessing pipeline using `scikit-learn` that performs the following steps: 1. **Standardize** the numerical features (i.e., transform them to have zero mean and unit variance). 2. **Scale** the numerical features to lie between 0 and 1. 3. **Encode** the categorical features using one-hot encoding. 4. **Handle** missing values in the categorical features by treating them as an additional category. Implement the function `preprocessing_pipeline` that takes as input a Pandas DataFrame `df` and returns the transformed dataset as a NumPy array. ```python import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer def preprocessing_pipeline(df: pd.DataFrame) -> np.ndarray: Applies preprocessing to numerical and categorical features. Parameters: df (pd.DataFrame): Input DataFrame containing numerical and categorical features. Returns: np.ndarray: Transformed dataset. # Separate numerical and categorical features numerical_features = df.select_dtypes(include=[\'int64\', \'float64\']).columns.tolist() categorical_features = df.select_dtypes(include=[\'object\']).columns.tolist() # Define the numerical pipeline numerical_pipeline = Pipeline([ (\'scale\', StandardScaler()), (\'minmax\', MinMaxScaler()) ]) # Define the categorical pipeline categorical_pipeline = Pipeline([ (\'impute\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine numerical and categorical pipelines into a ColumnTransformer preprocessor = ColumnTransformer([ (\'num\', numerical_pipeline, numerical_features), (\'cat\', categorical_pipeline, categorical_features) ]) # Fit and transform the input dataframe transformed_data = preprocessor.fit_transform(df) return transformed_data # Example Usage df_example = pd.DataFrame({ \'age\': [25, np.nan, 35, 45], \'income\': [50000, 64000, 58000, 52000], \'gender\': [\'male\', \'female\', np.nan, \'female\'], \'city\': [\'New York\', \'Los Angeles\', \'Chicago\', np.nan] }) output = preprocessing_pipeline(df_example) print(output) ``` **Input:** - A Pandas DataFrame `df` containing numerical and categorical features. **Output:** - A transformed dataset as a NumPy array, with numerical features standardized and scaled, and categorical features one-hot encoded. Constraints: - Handle missing numerical values by default methods of the used transformers. - Ensure the transformer pipelines are properly structured to handle mixed types of data. **Expectation:** - Demonstrate the understanding of using `ColumnTransformer` and `Pipeline` for preprocessing. - Show the ability to combine multiple preprocessing steps. - Ensure data consistency by fitting the pipeline properly.","solution":"import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer def preprocessing_pipeline(df: pd.DataFrame) -> np.ndarray: Applies preprocessing to numerical and categorical features. Parameters: df (pd.DataFrame): Input DataFrame containing numerical and categorical features. Returns: np.ndarray: Transformed dataset. # Separate numerical and categorical features numerical_features = df.select_dtypes(include=[\'int64\', \'float64\']).columns.tolist() categorical_features = df.select_dtypes(include=[\'object\']).columns.tolist() # Define the numerical pipeline numerical_pipeline = Pipeline([ (\'impute\', SimpleImputer(strategy=\'mean\')), (\'scale\', StandardScaler()), (\'minmax\', MinMaxScaler()) ]) # Define the categorical pipeline categorical_pipeline = Pipeline([ (\'impute\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine numerical and categorical pipelines into a ColumnTransformer preprocessor = ColumnTransformer([ (\'num\', numerical_pipeline, numerical_features), (\'cat\', categorical_pipeline, categorical_features) ]) # Fit and transform the input dataframe transformed_data = preprocessor.fit_transform(df) return transformed_data"},{"question":"Objective Write a Python function named `generate_module_documentation` that uses the `pydoc` module to generate HTML documentation for a given Python module. The function should save the documentation to an HTML file. Optionally, the function can start an HTTP server to serve the generated documentation to web browsers. Function Signature ```python def generate_module_documentation(module_name: str, html_file: str, start_server: bool = False, port: int = 0) -> None: pass ``` Parameters - `module_name` (str): The name of the Python module for which to generate documentation. - `html_file` (str): The path to the HTML file where the documentation will be saved. - `start_server` (bool, optional): If `True`, an HTTP server should be started to serve the documentation. Default is `False`. - `port` (int, optional): The port number on which the HTTP server should listen. Ignored if `start_server` is `False`. Default is `0`, which means an arbitrary unused port will be selected. Constraints - The module specified by `module_name` should be importable. - If `start_server` is `True`, make sure the HTTP server is gracefully started and can be stopped with a keyboard interrupt (Ctrl+C). Example Usage ```python # This should generate documentation for the \\"math\\" module and save it to \\"math.html\\". generate_module_documentation(\\"math\\", \\"math.html\\") # This should generate documentation for the \\"math\\" module, save it to \\"math.html\\", # and start an HTTP server on port 12345 to serve the documentation. generate_module_documentation(\\"math\\", \\"math.html\\", start_server=True, port=12345) ``` Hints - Use `pydoc.HTMLDoc` to generate HTML documentation for a module. - Use `pydoc.writedoc` if available for easy HTML generation. - Use the `http.server` module to start an HTTP server if `start_server` is `True`. Requirements - Ensure you handle exceptions that may occur during the import of the module. - Make sure the server starts only if `start_server` is set to `True`.","solution":"import pydoc import http.server import socketserver import importlib def generate_module_documentation(module_name: str, html_file: str, start_server: bool = False, port: int = 0) -> None: Generates HTML documentation for the given module and optionally starts an HTTP server to serve it. Parameters: - module_name (str): The name of the Python module for which to generate documentation. - html_file (str): The path to the HTML file where the documentation will be saved. - start_server (bool, optional): If True, an HTTP server should be started to serve the documentation. - port (int, optional): The port number on which the HTTP server should listen. try: module = importlib.import_module(module_name) except ImportError as e: raise ValueError(f\\"Module \'{module_name}\' cannot be imported: {e}\\") try: html_content = pydoc.HTMLDoc().docmodule(module) with open(html_file, \'w\') as f: f.write(html_content) except Exception as e: raise RuntimeError(f\\"Failed to generate or write documentation: {e}\\") if start_server: handler = http.server.SimpleHTTPRequestHandler httpd = socketserver.TCPServer((\'\', port), handler) print(f\\"Serving {html_file} on port {httpd.server_address[1]}\\") try: httpd.serve_forever() except KeyboardInterrupt: print(\\"nServer stopped.\\") httpd.shutdown() httpd.server_close()"},{"question":"# Algorithm Optimization with Profiling and Cython **Objective:** Implement a simple algorithm, profile its performance, identify a bottleneck, and optimize this bottleneck using Cython. **Problem Statement:** 1. Implement a matrix multiplication function that multiplies two 2D Numpy arrays. 2. Profile the function to identify performance bottlenecks. 3. Optimize the identified bottleneck using Cython. 4. Compare the performance of the original and optimized functions. **Function Signature:** ```python def matrix_multiplication(A: np.ndarray, B: np.ndarray) -> np.ndarray: Multiply two 2D Numpy arrays using Python. Args: A (np.ndarray): First 2D Numpy array. B (np.ndarray): Second 2D Numpy array. Returns: np.ndarray: Product of A and B. pass def optimize_matrix_multiplication(A: np.ndarray, B: np.ndarray) -> np.ndarray: An optimized version of matrix multiplication using Cython. Args: A (np.ndarray): First 2D Numpy array. B (np.ndarray): Second 2D Numpy array. Returns: np.ndarray: Product of A and B. pass ``` **Instructions:** 1. **Matrix Multiplication (Python Implementation):** - Implement `matrix_multiplication(A, B)` that multiplies two 2D Numpy arrays. - The function should handle cases where `A` and `B` have compatible shapes for matrix multiplication. 2. **Profiling:** - Use `%timeit`, `%prun`, or any other profiling tools to measure the performance of your matrix multiplication function. - Identify the main bottleneck in your Python implementation. 3. **Optimize with Cython:** - Implement `optimize_matrix_multiplication(A, B)` using Cython to optimize the identified bottleneck. - Ensure that the function returns the same result as the original Python implementation. 4. **Comparison:** - Compare the performance of the original and optimized functions using profiling tools. - Provide a summary of the performance improvement. **Constraints:** - Both `A` and `B` will be 2D Numpy arrays with shapes `(n, m)` and `(m, p)` respectively, where `1 <= n, m, p <= 1000`. **Notes:** - Use Numpy for array manipulations and calculations. - Ensure the correct setup of Cython for optimization. - Comment on any challenges you faced and how you addressed them. **Example:** ```python import numpy as np A = np.random.rand(500, 300) B = np.random.rand(300, 200) C = matrix_multiplication(A, B) C_opt = optimize_matrix_multiplication(A, B) assert np.allclose(C, C_opt), \\"The matrices are not equal!\\" ``` **Submission:** Submit a Jupyter notebook or a Python script with: 1. Your implementation of the functions. 2. Profiling results before and after optimization. 3. Any challenges faced and how you overcame them.","solution":"import numpy as np def matrix_multiplication(A: np.ndarray, B: np.ndarray) -> np.ndarray: Multiply two 2D Numpy arrays using Python. Args: A (np.ndarray): First 2D Numpy array. B (np.ndarray): Second 2D Numpy array. Returns: np.ndarray: Product of A and B. return np.dot(A, B) # To write optimized matrix multiplication with Cython, we need to add a separate file and compile it. # Assuming we have a file named `matrix_multiplication.pyx` with the following code: # matrix_multiplication.pyx content: # def optimize_matrix_multiplication(np.ndarray[double, ndim=2] A, np.ndarray[double, ndim=2] B): # # Optimized version of the matrix multiplication using Cython. # # cdef int i, j, k # cdef int n = A.shape[0] # cdef int m = A.shape[1] # cdef int p = B.shape[1] # cdef np.ndarray[double, ndim=2] C = np.zeros((n, p)) # for i in range(n): # for j in range(p): # for k in range(m): # C[i, j] += A[i, k] * B[k, j] # return C # This needs to be compiled using Cython tools. # After compiling the `.pyx` file, we can use the function `optimize_matrix_multiplication` here."},{"question":"# Question: Frequency Analysis and Filtering using `torch.fft` You are given a 1D time-domain signal represented as a PyTorch tensor. Your task is to implement a function that performs the following operations: 1. Compute the Fast Fourier Transform (FFT) of the signal. 2. Generate the corresponding frequency bins. 3. Apply a low-pass filter that removes all frequencies above a given cutoff. 4. Perform an inverse FFT to transform the filtered signal back to the time domain. Function Signature ```python def low_pass_filter(signal: torch.Tensor, cutoff: float, sampling_rate: float) -> torch.Tensor: Applies a low-pass filter to the input signal. Parameters: signal (torch.Tensor): 1D tensor representing the time-domain signal. cutoff (float): The cutoff frequency for the low-pass filter in Hz. sampling_rate (float): The sampling rate of the signal in samples per second (Hz). Returns: torch.Tensor: The filtered signal in the time domain. ``` Input - `signal`: A 1D PyTorch tensor representing the input time-domain signal. - `cutoff`: A floating-point number representing the cutoff frequency (in Hz). - `sampling_rate`: A floating-point number representing the sampling rate (in Hz) of the signal. Output - A 1D PyTorch tensor representing the filtered signal in the time domain. Constraints - The input `signal` tensor can have a length between 1 and 1000000. - The `cutoff` frequency will be between 0 and half of the `sampling_rate`. - The `sampling_rate` will be a positive number. Example ```python import torch # Example Signal sampling_rate = 1000.0 # Hz t = torch.arange(0, 1, 1/sampling_rate) # 1 second of data signal = torch.sin(2 * torch.pi * 50 * t) + 0.5 * torch.sin(2 * torch.pi * 120 * t) # Apply low-pass filter with cutoff frequency of 60 Hz cutoff = 60.0 # Hz filtered_signal = low_pass_filter(signal, cutoff, sampling_rate) # filtered_signal now contains the original signal with frequency components above 60 Hz removed. ``` Hints - Use `torch.fft.fft` to compute the FFT of the signal. - Use `torch.fft.fftfreq` to generate the frequency bins. - Apply the filter in the frequency domain by zeroing out components above the cutoff frequency. - Use `torch.fft.ifft` to transform the filtered frequency-domain signal back to the time domain.","solution":"import torch def low_pass_filter(signal: torch.Tensor, cutoff: float, sampling_rate: float) -> torch.Tensor: Applies a low-pass filter to the input signal. Parameters: signal (torch.Tensor): 1D tensor representing the time-domain signal. cutoff (float): The cutoff frequency for the low-pass filter in Hz. sampling_rate (float): The sampling rate of the signal in samples per second (Hz). Returns: torch.Tensor: The filtered signal in the time domain. # Step 1: Compute the FFT of the signal signal_fft = torch.fft.fft(signal) # Step 2: Generate the corresponding frequency bins freq_bins = torch.fft.fftfreq(signal.size(0), 1.0 / sampling_rate) # Step 3: Apply the low-pass filter filter_mask = abs(freq_bins) <= cutoff filtered_fft = signal_fft * filter_mask # Step 4: Compute the inverse FFT to get back to the time domain filtered_signal = torch.fft.ifft(filtered_fft).real return filtered_signal"},{"question":"# Assessing Advanced Python Programming Skills with `termios` Module Objective You have been tasked with creating a function that configures terminal settings to act as a simple raw mode terminal and implementing a few basic terminal control features as specified. Problem Statement Write a Python function `configure_raw_mode(fd)` that takes an integer file descriptor `fd` and puts the terminal into raw mode. In raw mode, the terminal provides the minimal processing of input and output. Additionally, implement the function `reset_terminal(fd, original_attributes)` to restore the terminal settings to their original state. Provide another function `manipulate_terminal(fd, commands)` which will: 1. Send a break signal of a specified duration. 2. Flush the terminal input and/or output queue based on the given command. 3. Suspend or resume input/output on the terminal based on the command. # Specifications 1. `configure_raw_mode(fd)`: - **Input**: an integer `fd` representing a file descriptor. - **Output**: a list of the original terminal attributes. 2. `reset_terminal(fd, original_attributes)`: - **Input**: an integer `fd` and a list `original_attributes` which represents the original terminal attributes. - **Output**: None (but should restore terminal settings to the original state). 3. `manipulate_terminal(fd, commands)`: - **Input**: an integer `fd` and a list `commands` where each command is a tuple (`action`, `args`), and `args` can be a single integer or a tuple of integers depending on the action. - Possible `actions`: - `\\"SEND_BREAK\\"`: args = (duration) - `\\"FLUSH\\"`: args = (queue) - `\\"FLOW\\"`: args = (action) - **Output**: None # Constraints - You may assume that you have the necessary permissions to modify the terminal settings. - The function `configure_raw_mode(fd)` must configure the terminal such that input is not echoed and special characters such as `Ctrl+C` do not generate signals. - `reset_terminal(fd, original_attributes)` should restore the terminal to the exact state prior to the changes. - For the `manipulate_terminal(fd, commands)` function: - `duration` should be a non-negative integer. - `queue` can be one of `TCIFLUSH`, `TCOFLUSH`, `TCIOFLUSH`. - `action` can be one of `TCOOFF`, `TCOON`, `TCIOFF`, `TCION`. # Example ```python import sys import termios def configure_raw_mode(fd): original_attributes = termios.tcgetattr(fd) raw_attributes = termios.tcgetattr(fd) raw_attributes[3] = raw_attributes[3] & ~(termios.ICANON | termios.ECHO | termios.ISIG) termios.tcsetattr(fd, termios.TCSANOW, raw_attributes) return original_attributes def reset_terminal(fd, original_attributes): termios.tcsetattr(fd, termios.TCSANOW, original_attributes) def manipulate_terminal(fd, commands): for action, args in commands: if action == \\"SEND_BREAK\\": termios.tcsendbreak(fd, args) elif action == \\"FLUSH\\": termios.tcflush(fd, args) elif action == \\"FLOW\\": termios.tcflow(fd, args) # Example Usage fd = sys.stdin.fileno() original_attributes = configure_raw_mode(fd) try: manipulate_terminal(fd, [(\\"SEND_BREAK\\", 0), (\\"FLUSH\\", termios.TCIOFLUSH), (\\"FLOW\\", termios.TCOOFF)]) finally: reset_terminal(fd, original_attributes) ``` Note: Ensure that you thoroughly test your functions in an appropriate Unix-like environment to verify they work as expected.","solution":"import termios import sys def configure_raw_mode(fd): original_attributes = termios.tcgetattr(fd) raw_attributes = termios.tcgetattr(fd) raw_attributes[3] = raw_attributes[3] & ~(termios.ICANON | termios.ECHO | termios.ISIG) termios.tcsetattr(fd, termios.TCSANOW, raw_attributes) return original_attributes def reset_terminal(fd, original_attributes): termios.tcsetattr(fd, termios.TCSANOW, original_attributes) def manipulate_terminal(fd, commands): for action, args in commands: if action == \\"SEND_BREAK\\": termios.tcsendbreak(fd, args) elif action == \\"FLUSH\\": termios.tcflush(fd, args) elif action == \\"FLOW\\": termios.tcflow(fd, args) # Example Usage if __name__ == \\"__main__\\": fd = sys.stdin.fileno() original_attributes = configure_raw_mode(fd) try: manipulate_terminal(fd, [(\\"SEND_BREAK\\", 0), (\\"FLUSH\\", termios.TCIOFLUSH), (\\"FLOW\\", termios.TCOOFF)]) finally: reset_terminal(fd, original_attributes)"},{"question":"Using the `array` module in Python, implement a function called `process_array_operations` that takes two inputs: 1. `operations`: a list of tuples, each detailing an operation to be applied to an array. Each tuple contains: - Operation name (a string) - Operation arguments (the necessary parameters for the operation) 2. `typecode`: a character representing the type of items the array will store. The function should: - Create an array of the specified `typecode`. - Sequentially apply each operation from the `operations` list on the array. - Return the final state of the array as a list. Here are the supported operations: 1. `append`: Add an item to the end of the array. 2. `extend`: Add multiple items to the end of the array. 3. `insert`: Insert an item at a specified position. 4. `remove`: Remove the first occurrence of an item. 5. `pop`: Remove and return an item at a specified position. 6. `reverse`: Reverse the items in the array. Operations should be provided in the following formats: - Append an item: `(\\"append\\", value)` - Extend with multiple items: `(\\"extend\\", [value1, value2, ...])` - Insert an item at a position: `(\\"insert\\", position, value)` - Remove the first occurrence of an item: `(\\"remove\\", value)` - Pop an item at a specific position: `(\\"pop\\", position)` - Reverse the array: `(\\"reverse\\",)` # Constraints - The type of `value`, `value1`, `value2`, ... should correspond to the type defined by `typecode`. - For `insert` and `pop` operations, `position` must be a valid index in the array. - The input operations list will contain valid and appropriately formatted operations only. # Example ```python from array import array def process_array_operations(operations, typecode): # Implementation here # Example Usage operations = [ (\\"append\\", 3), (\\"append\\", 5), (\\"extend\\", [7, 10]), (\\"insert\\", 1, 4), (\\"pop\\", 2), (\\"remove\\", 10), (\\"reverse\\",) ] result = process_array_operations(operations, \'I\') # typecode \'I\' for unsigned integers print(result) # Output: [7, 4, 3] ``` # Notes 1. Students should import the array module for their implementation. 2. The result will be a list of array elements after all operations are applied.","solution":"from array import array def process_array_operations(operations, typecode): arr = array(typecode) for operation in operations: op_name = operation[0] if op_name == \\"append\\": arr.append(operation[1]) elif op_name == \\"extend\\": arr.extend(operation[1]) elif op_name == \\"insert\\": arr.insert(operation[1], operation[2]) elif op_name == \\"remove\\": arr.remove(operation[1]) elif op_name == \\"pop\\": arr.pop(operation[1]) elif op_name == \\"reverse\\": arr.reverse() return list(arr)"},{"question":"# Question: Implement a Telnet-based Command Executor You are required to implement a Python function `execute_telnet_commands(host, port, commands)` that uses the `telnetlib` module to connect to a Telnet server and execute a series of commands. Function Signature ```python def execute_telnet_commands(host: str, port: int, commands: list[str]) -> str: pass ``` Parameters - `host` (str): The hostname or IP address of the Telnet server. - `port` (int): The port number of the Telnet server. - `commands` (list of str): A list of commands to execute on the Telnet server. Returns - `str`: The output from executing all commands, concatenated into a single string, separated by newlines. Requirements 1. Connect to the Telnet server using the `telnetlib.Telnet` class. 2. Ensure that all provided commands are executed sequentially. 3. Collect the output generated by each command and aggregate it. 4. Handle any exceptions that occur during connection or command execution. 5. Properly close the Telnet connection after executing the commands. Constraints - You must use methods provided by the `telnetlib.Telnet` class to read and write data. - The function should handle timeouts gracefully. Example ```python host = \\"localhost\\" port = 23 commands = [\\"echo Hello\\", \\"ls\\", \\"whoami\\"] output = execute_telnet_commands(host, port, commands) print(output) ``` This code should connect to the specified Telnet server, execute the given commands, and print the combined output of these commands. Additional Notes - You can assume that the Telnet server returns a newline character (`n`) at the end of each command’s output. - Consider using `read_until` to process server output after each command is sent. Hints - Use context management (`with` statement) to ensure proper cleanup. - Implement exception handling to manage potential errors during connection and communication.","solution":"import telnetlib def execute_telnet_commands(host: str, port: int, commands: list[str]) -> str: Connects to a Telnet server and executes a series of commands. Returns the output from executing all commands as a single string. try: with telnetlib.Telnet(host, port, timeout=10) as tn: combined_output = [] for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") output = tn.read_until(b\\"n\\", timeout=10).decode(\'ascii\') combined_output.append(output.strip()) return \\"n\\".join(combined_output) except Exception as e: return str(e)"},{"question":"# Flask Application Deployment You are part of a development team working on a Flask web application named `myapp`. The development phase is complete, and now you need to deploy the application to a production server. Design a script in Python that automates the deployment process. Task 1. **Package the Application**: Write a script that: - Installs the necessary build tool. - Builds the application into a wheel (`.whl`) file. 2. **Configure the Secret Key**: - Generate a random secret key. - Save the secret key in a configuration file located in the instance folder. 3. **Initialize the Database**: - Ensure that the script initializes the database on the new machine. 4. **Run the Application with Waitress**: - Install the `Waitress` server in the virtual environment. - Serve the application using `Waitress`. Input and Output Formats - **Input**: No specific input format. The script should read necessary configurations and paths, and execute accordingly. - **Output**: The application should be successfully packaged, configured, and running on `http://0.0.0.0:8080` using `Waitress`. Constraints - Ensure the environment is set up correctly with all dependencies installed. - Emphasize security by securely generating and saving the `SECRET_KEY`. Example Here is an example outline of what the script might look like: ```python import os import subprocess import secrets def install_build_tool(): subprocess.run([\'pip\', \'install\', \'build\']) def build_wheel(): subprocess.run([\'python\', \'-m\', \'build\', \'--wheel\']) def generate_secret_key(): return secrets.token_hex() def create_config(secret_key): instance_folder = os.path.join(\'.venv\', \'var\', \'myapp-instance\') os.makedirs(instance_folder, exist_ok=True) config_path = os.path.join(instance_folder, \'config.py\') with open(config_path, \'w\') as file: file.write(f\\"SECRET_KEY = \'{secret_key}\'n\\") def init_db(): subprocess.run([\'flask\', \'--app\', \'myapp\', \'init-db\']) def run_with_waitress(): subprocess.run([\'pip\', \'install\', \'waitress\']) subprocess.run([\'waitress-serve\', \'--call\', \'myapp:create_app\']) if __name__ == \\"__main__\\": install_build_tool() build_wheel() secret_key = generate_secret_key() create_config(secret_key) init_db() run_with_waitress() ``` In this example: - The script installs the `build` tool, builds the application wheel, generates a random secret key, saves it in the configuration file, initializes the database, installs `Waitress`, and runs the application using `Waitress`. **Note:** Make sure you have a Flask application named `myapp` set up with a factory function `create_app` and a `flask --app myapp init-db` command available.","solution":"import os import subprocess import secrets def install_build_tool(): subprocess.run([\'pip\', \'install\', \'build\'], check=True) def build_wheel(): subprocess.run([\'python\', \'-m\', \'build\', \'--wheel\'], check=True) def generate_secret_key(): return secrets.token_hex() def create_config(secret_key): instance_folder = os.path.join(os.getcwd(), \'instance\') os.makedirs(instance_folder, exist_ok=True) config_path = os.path.join(instance_folder, \'config.py\') with open(config_path, \'w\') as file: file.write(f\\"SECRET_KEY = \'{secret_key}\'n\\") def init_db(): subprocess.run([\'flask\', \'--app\', \'myapp\', \'init-db\'], check=True) def run_with_waitress(): subprocess.run([\'pip\', \'install\', \'waitress\'], check=True) subprocess.run([\'waitress-serve\', \'--call\', \'myapp:create_app\'], check=True) if __name__ == \\"__main__\\": install_build_tool() build_wheel() secret_key = generate_secret_key() create_config(secret_key) init_db() run_with_waitress()"},{"question":"# Custom Data Structure with Memory Management and Exception Handling You are required to create a custom Python class to simulate a simplified version of Python\'s internal memory management and reference counting mechanism. Your class should: 1. **Initialize** an internal data structure representing a block of memory to store integer values. 2. **Handle Memory Allocation**: Implement methods to allocate and deallocate memory manually. 3. **Reference Counting**: Implement reference counting to keep track of how many references exist to allocated memory blocks. 4. **Exception Handling**: Raise custom exceptions when memory allocation fails or invalid operations are attempted. Requirements 1. **Class Definition**: Create a class `CustomMemoryManager`. 2. **Initialize**: The constructor should initialize an internal list to represent allocated memory blocks and a dictionary to keep track of reference counts. 3. **Allocate Memory**: Implement a method `allocate(self, size: int) -> int` that allocates a block of memory with the given size (number of integers) and returns an identifier for the allocated block. - Raise a `MemoryAllocationError` if allocation fails. 4. **Deallocate Memory**: Implement a method `deallocate(self, block_id: int) -> None` that deallocates the memory block identified by `block_id`. - Raise a `MemoryDeallocationError` if the block_id is invalid. 5. **Increment Reference Count**: Implement a method `increment_ref_count(self, block_id: int) -> None` to increment the reference count for the specified memory block. 6. **Decrement Reference Count**: Implement a method `decrement_ref_count(self, block_id: int) -> None` to decrement the reference count for the specified memory block and automatically deallocate the block if the reference count reaches zero. 7. **Custom Exceptions**: Define custom exceptions `MemoryAllocationError` and `MemoryDeallocationError`. Example Usage ```python try: mm = CustomMemoryManager() block_id = mm.allocate(10) mm.increment_ref_count(block_id) mm.decrement_ref_count(block_id) mm.deallocate(block_id) except MemoryAllocationError as mae: print(f\\"Allocation error: {mae}\\") except MemoryDeallocationError as mde: print(f\\"Deallocation error: {mde}\\") ``` Input and Output Formats - No direct input is required from the user. - Outputs are in the form of exceptions raised or printed strings for debugging purposes. Constraints - The size for each memory block allocation will be a positive integer. - Each `block_id` will be a unique identifier (integer). Performance Requirements - Your implementation should efficiently manage both memory allocation and reference counting. - Ensure that the methods operate in expected time complexity for such operations.","solution":"class MemoryAllocationError(Exception): pass class MemoryDeallocationError(Exception): pass class CustomMemoryManager: def __init__(self): self.memory_blocks = {} # To store allocated memory blocks self.ref_counts = {} # To store reference counts of allocated blocks self.next_block_id = 0 # To generate unique block IDs def allocate(self, size: int) -> int: if size <= 0: raise MemoryAllocationError(\\"Size must be a positive integer.\\") block_id = self.next_block_id self.memory_blocks[block_id] = [0] * size self.ref_counts[block_id] = 1 self.next_block_id += 1 return block_id def deallocate(self, block_id: int) -> None: if block_id not in self.memory_blocks: raise MemoryDeallocationError(f\\"Block ID {block_id} does not exist.\\") del self.memory_blocks[block_id] del self.ref_counts[block_id] def increment_ref_count(self, block_id: int) -> None: if block_id not in self.ref_counts: raise MemoryDeallocationError(f\\"Block ID {block_id} does not exist.\\") self.ref_counts[block_id] += 1 def decrement_ref_count(self, block_id: int) -> None: if block_id not in self.ref_counts: raise MemoryDeallocationError(f\\"Block ID {block_id} does not exist.\\") self.ref_counts[block_id] -= 1 if self.ref_counts[block_id] == 0: self.deallocate(block_id)"},{"question":"# Task: Asynchronous Task Management System Objective: Create an async task management system using `asyncio.Queue`. You will implement an asynchronous system to manage tasks efficiently, allowing multiple workers to process tasks from the queue concurrently. Requirements: 1. Implement an async function `task_producer(queue, num_tasks)` which generates a specified number of tasks, each task is represented by a random float value between 0.1 and 1.0, and put them in the queue. 2. Implement an async function `task_consumer(name, queue)` which continuously retrieves and processes tasks from the queue. Each task simply involves sleeping for the duration specified by the task value. 3. Implement an async function `main(num_producer_tasks, num_workers)` which: - Creates an `asyncio.Queue()`. - Launches `task_producer` to generate a specified number of tasks. - Launches a specified number of `task_consumer` workers to process tasks from the queue. - Uses `queue.join()` to ensure all tasks are processed. - Cancels all worker tasks once all tasks are processed. Input: - `num_producer_tasks`: The number of tasks the producer should generate. - `num_workers`: The number of worker tasks to process the queue. Output: - The total sleep time spent by all workers. Constraints: - Each task\'s sleep time should be between `0.1` and `1.0` seconds. - Use `asyncio.Queue()`. - Ensure proper synchronization using `queue.join()` and `task_done()`. Example Usage: ```python import asyncio import random async def task_producer(queue, num_tasks): for _ in range(num_tasks): sleep_time = random.uniform(0.1, 1.0) await queue.put(sleep_time) async def task_consumer(name, queue): while True: sleep_time = await queue.get() await asyncio.sleep(sleep_time) queue.task_done() print(f\'{name} slept for {sleep_time:.2f} seconds\') async def main(num_producer_tasks, num_workers): queue = asyncio.Queue() producer_task = asyncio.create_task(task_producer(queue, num_producer_tasks)) worker_tasks = [asyncio.create_task(task_consumer(f\'worker-{i}\', queue)) for i in range(num_workers)] await queue.join() for worker in worker_tasks: worker.cancel() await asyncio.gather(*worker_tasks, return_exceptions=True) print(\'All tasks have been processed.\') asyncio.run(main(10, 3)) ``` Implement the required functions `task_producer`, `task_consumer`, and `main` based on the above description and example.","solution":"import asyncio import random async def task_producer(queue, num_tasks): Produces a specified number of tasks, each represented by a random float between 0.1 and 1.0, and puts them in the queue. for _ in range(num_tasks): sleep_time = random.uniform(0.1, 1.0) await queue.put(sleep_time) async def task_consumer(name, queue): Continuously retrieves and processes tasks from the queue. Each task involves sleeping for the duration specified by the task value. while True: sleep_time = await queue.get() await asyncio.sleep(sleep_time) queue.task_done() print(f\'{name} processed a task of {sleep_time:.2f} seconds\') async def main(num_producer_tasks, num_workers): Orchestrates the task producer and consumers: - Creates an asyncio.Queue - Launches a task producer to generate tasks - Launches multiple task consumers to process tasks - Ensures all tasks are processed using queue.join() - Cancels the worker tasks once processing is done queue = asyncio.Queue() producer_task = asyncio.create_task(task_producer(queue, num_producer_tasks)) worker_tasks = [asyncio.create_task(task_consumer(f\'worker-{i}\', queue)) for i in range(num_workers)] await producer_task await queue.join() for worker in worker_tasks: worker.cancel() await asyncio.gather(*worker_tasks, return_exceptions=True) print(\'All tasks have been processed.\') if __name__ == \\"__main__\\": asyncio.run(main(10, 3))"},{"question":"**Objective**: This question aims to assess your understanding of file handling, parsing structured text, exception management, and dictionary operations in Python. **Problem Statement**: You are given a `.netrc` file format that stores multiple FTP hosts\' login details. Your task is to implement a function that reads a `.netrc` file, parses it, and returns the login details in a structured dictionary format. Additionally, handle any exceptions that may occur during file reading and parsing. **Function Signature**: ```python def parse_netrc(file_path: str) -> dict: pass ``` **Input**: - `file_path`: A string representing the path to the `.netrc` file. **Output**: - A dictionary where the keys are the hostnames and the values are tuples `(login, account, password)`. **Constraints**: - If the `.netrc` file includes a \'default\' entry, it should be included with the key `\'default\'` in the resulting dictionary. - If the file contains any syntax errors, raise a custom exception `InvalidNetrcFileError` with an appropriate error message. - Assume the login details (login, account, password) are always present for each host. - Handle file permissions properly and raise an `InsecureNetrcFileError` if the file permissions are insecure. **Custom Exceptions**: ```python class InvalidNetrcFileError(Exception): def __init__(self, message: str): self.message = message super().__init__(self.message) class InsecureNetrcFileError(Exception): def __init__(self, message: str): self.message = message super().__init__(self.message) ``` **Example**: Given a `.netrc` file with the following content: ``` machine example.com login user1 account account1 password pass1 machine example.org login user2 account account2 password pass2 default login default_user account default_account password default_pass ``` Calling `parse_netrc(\'path/to/netrc\')` should return: ```python { \'example.com\': (\'user1\', \'account1\', \'pass1\'), \'example.org\': (\'user2\', \'account2\', \'pass2\'), \'default\': (\'default_user\', \'default_account\', \'default_pass\') } ``` **Notes**: 1. You need to handle both valid and invalid cases of `.netrc` file content properly. 2. Implement proper exception handling to manage various error scenarios. 3. Ensure your solution adheres to Python\'s best practices and make use of the `netrc` module where appropriate. Good luck!","solution":"import os import stat import re class InvalidNetrcFileError(Exception): def __init__(self, message: str): self.message = message super().__init__(self.message) class InsecureNetrcFileError(Exception): def __init__(self, message: str): self.message = message super().__init__(self.message) def parse_netrc(file_path: str) -> dict: Reads and parses a .netrc file and returns login details in a dictionary format. Args: file_path (str): Path to the .netrc file. Returns: dict: Dictionary with hostnames as keys and tuples of (login, account, password) as values. Raises: InvalidNetrcFileError: If the file contains syntax errors. InsecureNetrcFileError: If the file permissions are insecure. # Check file permissions for security concerns (should be readable/writeable only by the owner) file_stat = os.stat(file_path) if file_stat.st_mode & (stat.S_IRWXG | stat.S_IRWXO): raise InsecureNetrcFileError(\\"File permissions are insecure. Permission should be 600.\\") result = {} try: with open(file_path, \'r\') as file: content = file.read() # Removing leading and trailing spaces content = content.strip() # Parsing the file pattern = re.compile( r\'(machine|default)s+(S+)?s+logins+(S+)s+accounts+(S+)s+passwords+(S+)\', re.IGNORECASE ) matches = pattern.findall(content) if not matches: raise InvalidNetrcFileError(\\"Netrc file format is incorrect.\\") for match in matches: if match[0].lower() == \'machine\': result[match[1]] = (match[2], match[3], match[4]) elif match[0].lower() == \'default\': result[\'default\'] = (match[2], match[3], match[4]) except FileNotFoundError: raise FileNotFoundError(f\\"No such file or directory: \'{file_path}\'\\") except Exception as e: raise InvalidNetrcFileError(f\\"Failed to parse .netrc file: {e}\\") return result"},{"question":"Understanding and Manipulating Pandas Options As a data analyst, you are required to ensure consistent formatting and configuration of your pandas DataFrame outputs across multiple scripts. By effectively manipulating pandas options, you can achieve uniform output formatting that adheres to the company\'s standards. Objective Write a function `configure_pandas_options` that performs the following tasks: 1. **Set the global display option** to display a maximum of 10 rows and a maximum of 5 columns in any DataFrame output. 2. **Set the float format** in scientific notation with precision of 2 decimal places. 3. **Get the current setting for maximum rows, maximum columns, and float format**, and return them as a dictionary with keys `\'max_rows\'`, `\'max_columns\'`, and `\'float_format\'`. 4. **Reset all the options to default** settings after retrieving the values. # Constraints - Assume `pandas` is already imported as `pd`. # Input The function does not take any parameters. # Output - The function should return a dictionary with the following structure: ```python { \'max_rows\': int, # The current maximum number of rows for display \'max_columns\': int, # The current maximum number of columns for display \'float_format\': str # The current float format setting } ``` # Performance Requirements - The function should execute efficiently without unnecessary recalculations or memory usage. # Example Given the configuration steps: 1. Set display options. 2. Capture the current settings. 3. Reset to defaults. Sample function call: ```python result = configure_pandas_options() print(result) ``` Expected output: ```python { \'max_rows\': 10, \'max_columns\': 5, \'float_format\': \'%.2e\' } ``` Note: The exact output may vary depending on the default settings of your pandas installation, especially for `float_format`. # Function Signature ```python def configure_pandas_options(): # Your code here ```","solution":"import pandas as pd def configure_pandas_options(): Configures pandas display options, retrieves the current settings and then resets the options to default. Returns: dict: Current settings for \'max_rows\', \'max_columns\', and \'float_format\'. # Set the desired pandas options pd.set_option(\'display.max_rows\', 10) pd.set_option(\'display.max_columns\', 5) pd.set_option(\'display.float_format\', lambda x: \'%.2e\' % x) # Capture the current settings settings = { \'max_rows\': pd.get_option(\'display.max_rows\'), \'max_columns\': pd.get_option(\'display.max_columns\'), \'float_format\': pd.get_option(\'display.float_format\')(0) if hasattr(pd.get_option(\'display.float_format\'), \'__call__\') else pd.get_option(\'display.float_format\') } # Reset all the pandas options to default values pd.reset_option(\'all\') return settings"},{"question":"**Advanced Distributed Training in PyTorch** # Background Distributed training is an essential technique for scaling the training of neural network models across multiple GPUs or machines. PyTorch provides several optimizers to facilitate distributed training, such as `DistributedOptimizer`, `PostLocalSGDOptimizer`, and `ZeroRedundancyOptimizer`. # Problem Statement You are tasked with implementing a function that sets up a distributed optimizer for a simple neural network training loop in PyTorch. The function should utilize the `DistributedOptimizer` class. The purpose is to train a simple neural network on a synthetic dataset using multiple processors to showcase the benefits of distributed training. # Requirements 1. **Function Signature:** ```python def train_model_with_distributed_optimizer(device_ids: list, epochs: int) -> None: ``` 2. **Inputs:** - `device_ids` (list): A list of integer device IDs to be used for training (e.g., `[0, 1, 2]` for training on three GPUs). - `epochs` (int): The number of epochs to train the model. 3. **Outputs:** - The function does not need to return anything but should print out the training loss after each epoch. # Constraints: - Use the `torch.distributed.optim.DistributedOptimizer` class for optimization. - Assume the environment is correctly set up for distributed training with the appropriate backend (e.g., NCCL for GPUs). - Only use CPU or GPU tensors (CUDA tensors). - Implement basic error handling to ensure that the distributed setup has been correctly initialized. # Performance Requirements: - Efficiently handle the distribution of the model and optimizer across multiple devices. - Ensure that the loss computation and gradient updates are correctly synchronized across all devices. # Example synthetic training setup: - Create a simple feedforward neural network. - Use Mean Squared Error (MSE) as the loss function. - Generate a synthetic dataset of random inputs and outputs to train the neural network. # Hint: You can initialize the distributed environment using the `torch.distributed.init_process_group` function and set up each model replica on different devices as specified by `device_ids`. # Your Task: Implement the `train_model_with_distributed_optimizer` function to achieve the above requirements.","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim import torch.multiprocessing as mp import random # Define a simple neural network class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def setup(rank, world_size): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'12355\' dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) torch.manual_seed(42 + rank) def cleanup(): dist.destroy_process_group() def synthetic_data(): inputs = torch.randn(100, 10) outputs = torch.randn(100, 1) return inputs, outputs def train(rank, epochs, world_size): setup(rank, world_size) model = SimpleNN() model.to(rank) model = nn.parallel.DistributedDataParallel(model, device_ids=[rank]) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) optimizer = dist.optim.DistributedOptimizer( optim.SGD, model.parameters(), lr=0.01, ) inputs, outputs = synthetic_data() inputs = inputs.to(rank) outputs = outputs.to(rank) for epoch in range(epochs): model.train() optimizer.zero_grad() preds = model(inputs) loss = criterion(preds, outputs) loss.backward() optimizer.step() if rank == 0: print(f\'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\') cleanup() def train_model_with_distributed_optimizer(device_ids: list, epochs: int) -> None: world_size = len(device_ids) mp.spawn(train, args=(epochs, world_size), nprocs=world_size, join=True)"},{"question":"<|Analysis Begin|> The provided documentation is for Python\'s `traceback` module, which facilitates handling and manipulating stack tracebacks. Here\'s a summary of the significant features and functions covered in this documentation: 1. **Printing and Formatting**: - `traceback.print_tb()`, `traceback.print_exception()`, `traceback.print_exc()`, `traceback.print_last()`, and related functions for printing traceback and exception information. - `traceback.extract_tb()`, `traceback.extract_stack()`, `traceback.format_list()`, `traceback.format_exception_only()`, `traceback.format_exception()`, `traceback.format_exc()`, `traceback.format_tb()`, and `traceback.format_stack()` for extracting and formatting traceback and stack information. 2. **Classes**: - `traceback.TracebackException` with attributes and methods for capturing, rendering, and formatting exceptions. - `traceback.StackSummary` and `traceback.FrameSummary` for representing call stacks and frames, respectively. 3. **Traceback Examples**: - The documentation includes several examples showing different ways to use the module\'s functions to handle, format, and print tracebacks and stack traces. The module is useful for debugging and error handling, especially when developers need to capture and represent error states and tracebacks programmatically. Considering the scope and functionality described, a challenging question should involve using multiple functions from this module to handle exception information in a structured and formatted manner. <|Analysis End|> <|Question Begin|> You are tasked with creating an advanced error logging system that captures, formats, and writes exception details to a log file whenever an exception occurs within your application. The system should utilize the `traceback` module as described in the given documentation. **Requirements**: 1. Implement a function `log_exception_to_file(exc, file_path)` that: - Takes an exception `exc` (of any type) and a file path `file_path` as inputs. - Captures all relevant details of the exception, including the traceback, exception type, and message. - Formats the captured details into a readable string. - Writes the formatted string to the specified file. 2. Implement a function `raise_and_log_exception()` that: - Forces an exception (choose any exception type) to occur within a `try` block. - Catches the exception in the corresponding `except` block. - Calls `log_exception_to_file()` to log the exception details to a file named `error_log.txt`. **Function Signatures**: ```python def log_exception_to_file(exc: Exception, file_path: str) -> None: pass def raise_and_log_exception() -> None: pass ``` **Example**: After running `raise_and_log_exception()`, the `error_log.txt` should contain formatted exception information, similar to the following output: ``` Traceback (most recent call last): File \\"your_script.py\\", line 15, in raise_and_log_exception forced_error() File \\"your_script.py\\", line 11, in forced_error return 1 / 0 ZeroDivisionError: division by zero ``` **Notes**: - You can design `forced_error` function or any logic within `raise_and_log_exception` to raise an exception. - Make sure `log_exception_to_file` handles the formatting and file writing as specified. Good luck!","solution":"import traceback def log_exception_to_file(exc: Exception, file_path: str) -> None: Captures the exception details and writes them to a specified log file. Args: exc (Exception): The caught exception. file_path (str): The path to the log file where exception details should be written. with open(file_path, \'w\') as f: # Extract and format the traceback details tb_lines = traceback.format_exception(type(exc), exc, exc.__traceback__) f.writelines(tb_lines) def raise_and_log_exception() -> None: Forces an exception and logs its details using the log_exception_to_file function. try: # Force an exception to occur (ZeroDivisionError) forced_error() except Exception as exc: log_exception_to_file(exc, \'error_log.txt\') def forced_error(): Function designed to trigger an exception for testing purposes. return 1 / 0"},{"question":"# Question: Encoding and Decoding with Custom Error Handling You are provided with a list of strings and you need to encode each string using the UTF-8 encoding scheme. For any encoding errors, you have to replace problematic characters with their corresponding backslash escape sequences. After encoding, decode the bytes back into strings, while handling decoding errors by ignoring any problematic byte sequences. Write a function `process_strings` that takes a list of strings and returns a list of processed strings after performing the encoding and decoding operations as specified. Function Signature ```python def process_strings(strings: list[str]) -> list[str]: pass ``` Input - `strings`: A list of strings to be encoded and decoded. Output - Returns a list of strings that have been encoded using UTF-8 with backslash replace error handling, and then decoded with ignore error handling. Example ```python input_strings = [ \\"Hello, World!\\", \\"Python is great!\\", \\"Invalid udcff character\\" ] print(process_strings(input_strings)) # Output: [\'Hello, World!\', \'Python is great!\', \'Invalid udcff character\'] ``` Constraints - Use the `codecs.encode` and `codecs.decode` functions to perform encoding and decoding. - Handle encoding errors using the `backslashreplace` error handler. - Handle decoding errors using the `ignore` error handler. Notes - You are not allowed to use any other encoding schemes. - Ensure that the order of strings in the output list matches the order in the input list. Solution Template ```python import codecs def process_strings(strings: list[str]) -> list[str]: processed_strings = [] for string in strings: # Encode the string using UTF-8 with backslash replace error handling encoded_bytes = codecs.encode(string, encoding=\'utf-8\', errors=\'backslashreplace\') # Decode the bytes back to string using UTF-8 with ignore error handling decoded_string = codecs.decode(encoded_bytes, encoding=\'utf-8\', errors=\'ignore\') processed_strings.append(decoded_string) return processed_strings ```","solution":"import codecs def process_strings(strings: list[str]) -> list[str]: processed_strings = [] for string in strings: # Encode the string using UTF-8 with backslash replace error handling encoded_bytes = codecs.encode(string, encoding=\'utf-8\', errors=\'backslashreplace\') # Decode the bytes back to string using UTF-8 with ignore error handling decoded_string = codecs.decode(encoded_bytes, encoding=\'utf-8\', errors=\'ignore\') processed_strings.append(decoded_string) return processed_strings"},{"question":"Objective Demonstrate your understanding of the `timeit` module in Python by writing a function that compares the performance of different implementations of a particular task. Task Implement the function `compare_sorting_algorithms` which: 1. Accepts a list of integers. 2. Compares the execution time of sorting this list using three different methods: - Built-in `sorted()` function - A custom implementation of Bubble Sort - A custom implementation of Quick Sort 3. Returns a dictionary with the method names as keys and the time taken (in seconds) as values. # Function Signature ```python def compare_sorting_algorithms(data: List[int]) -> Dict[str, float]: pass ``` # Requirements 1. **Inputs**: - `data` : List of integers (`List[int]`) 2. **Output**: - A dictionary with the method names (\'built-in\', \'bubble_sort\', \'quick_sort\') as keys and the execution time in seconds as values (`Dict[str, float]`). 3. **Implementation Details**: - Use the `timeit.timeit` function to measure the execution time. - Implement Bubble Sort and Quick Sort manually. - Ensure each sorting method operates on a copy of the original data to avoid in-place sorting issues. 4. **Constraints**: - The list length will be between 1 and 1000. - The integers in the list will be within the range of -10^6 to 10^6. 5. **Performance**: - Measure execution time accurately by appropriately selecting the number of iterations. # Example ```python data = [5, 3, 8, 6, 2, 7, 4, 1] results = compare_sorting_algorithms(data) print(results) # Output might look like: # { # \'built-in\': 0.0000123, # \'bubble_sort\': 0.0123456, # \'quick_sort\': 0.0005678 # } ``` # Notes - Use the `timeit` module for accurate time measurement. - Ensure the same data set is used for each timing to maintain consistency. - Consider the overhead of `timeit` in your solution. Hints - You may use `sorted(data)` for the built-in method. - For Bubble Sort and Quick Sort, refer to standard algorithm implementations available in textbooks or online resources, ensuring you write and understand the implementation yourself.","solution":"import timeit from typing import List, Dict import copy def bubble_sort(data: List[int]) -> List[int]: n = len(data) for i in range(n): for j in range(0, n-i-1): if data[j] > data[j+1]: data[j], data[j+1] = data[j+1], data[j] return data def quick_sort(data: List[int]) -> List[int]: if len(data) <= 1: return data pivot = data[len(data) // 2] left = [x for x in data if x < pivot] middle = [x for x in data if x == pivot] right = [x for x in data if x > pivot] return quick_sort(left) + middle + quick_sort(right) def compare_sorting_algorithms(data: List[int]) -> Dict[str, float]: # Clone data to avoid in-place sorting issues data1 = copy.deepcopy(data) data2 = copy.deepcopy(data) data3 = copy.deepcopy(data) # Measure built-in sorted() time built_in_time = timeit.timeit(lambda: sorted(data1), number=1) # Measure bubble sort time bubble_sort_time = timeit.timeit(lambda: bubble_sort(data2), number=1) # Measure quick sort time quick_sort_time = timeit.timeit(lambda: quick_sort(data3), number=1) # Return the results in the required format return { \'built-in\': built_in_time, \'bubble_sort\': bubble_sort_time, \'quick_sort\': quick_sort_time, }"},{"question":"# Asynchronous Task Management with `asyncio` You are tasked with creating an asynchronous application that performs a series of calculations and logging, while ensuring that blocking operations do not delay the event loop. The goal is to write a function that schedules asynchronous tasks and handles exceptions appropriately. Task 1. Write an asynchronous function `compute_square` which takes an integer input and: - Computes the square of the number after a 1-second delay using `asyncio.sleep`. - Raises a `ValueError` if the input number is negative. 2. Implement the `main` coroutine which: - Creates and schedules multiple `compute_square` tasks for a list of integers (including some negative values). - Uses an executor to offload a blocking logging task to a separate thread. - Handles all exceptions that might be raised by the `compute_square` tasks. 3. Ensure your solution adheres to the following constraints: - The event loop must remain responsive at all times. - All tasks must be awaited correctly. - Proper error handling and logging should be implemented. Input The `main` coroutine should take a list of integers as input. Output The output should be the results of the successful computations, while errors should be logged appropriately. Example ```python import asyncio import logging from concurrent.futures import ThreadPoolExecutor async def compute_square(x: int) -> int: await asyncio.sleep(1) if x < 0: raise ValueError(f\\"Negative value: {x}\\") return x * x async def main(numbers: list[int]): results = [] loop = asyncio.get_event_loop() executor = ThreadPoolExecutor() async def log_error(message: str): loop.run_in_executor(executor, logging.error, message) async def handle_task(task): try: result = await task results.append(result) except Exception as e: await log_error(str(e)) tasks = [compute_square(num) for num in numbers] await asyncio.gather(*(handle_task(task) for task in tasks)) return results # Example usage: numbers = [1, 2, -3, 4, -5] asyncio.run(main(numbers)) ``` The above example should log errors for negative inputs and return the squares of positive numbers after appropriate delays. Notes - Enable the debug mode for `asyncio` for better error tracing during development. - Ensure all exceptions are logged using the logging module. - Use a `ThreadPoolExecutor` for any blocking operations.","solution":"import asyncio import logging from concurrent.futures import ThreadPoolExecutor # Setup basic logging configuration logging.basicConfig(level=logging.ERROR, format=\'%(asctime)s - %(levelname)s - %(message)s\') async def compute_square(x: int) -> int: await asyncio.sleep(1) # Simulate a delay if x < 0: raise ValueError(f\\"Negative value: {x}\\") return x * x async def main(numbers: list[int]) -> list[int]: results = [] loop = asyncio.get_event_loop() executor = ThreadPoolExecutor() async def log_error(message: str): await loop.run_in_executor(executor, logging.error, message) async def handle_task(task): try: result = await task results.append(result) except Exception as e: await log_error(str(e)) tasks = [compute_square(num) for num in numbers] await asyncio.gather(*(handle_task(task) for task in tasks)) return results"},{"question":"You have been provided with a dataset, `tips`, which tracks information about bills and tips at a restaurant. Your task is to create specific visualizations using the `seaborn.objects` package to explore the data. # Requirements: 1. Load the `tips` dataset using `seaborn.load_dataset(\\"tips\\")`. 2. Implement the following functions: a. `plot_day_count()`: Create a bar plot that shows the count of distinct observations for each day. ```python def plot_day_count(): Creates and displays a bar plot of the number of observations for each day. Returns: None pass # Implement the function here ``` b. `plot_day_sex_count()`: Create a grouped bar plot to show the count of observations for each day, further grouped by the `sex` variable. ```python def plot_day_sex_count(): Creates and displays a grouped bar plot of the number of observations for each day, grouped by the \'sex\' of the bill payer. Returns: None pass # Implement the function here ``` c. `plot_size_count()`: Create a bar plot that shows the count of different `size` values without binning the data. ```python def plot_size_count(): Creates and displays a bar plot of the number of observations for each size value. Returns: None pass # Implement the function here ``` d. `plot_size_count_y_axis()`: Create a bar plot where the count of different `size` values is assigned to the y-axis. ```python def plot_size_count_y_axis(): Creates and displays a bar plot of the number of observations for each size value where counts are on the y-axis. Returns: None pass # Implement the function here ``` # Constraints and Limitations: 1. The plots should be created using the `seaborn.objects.Plot` object and its associated methods (`add`, `Bar`, `Count`, and `Dodge`). 2. Ensure that the plots are displayed correctly and are well-labeled for clarity. # Example Output: When you run the function `plot_day_count()`, it should create and display a bar plot showing the counts for each day. **Note**: Test each function separately to confirm the accuracy and appearance of the resulting plots.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") def plot_day_count(): Creates and displays a bar plot of the number of observations for each day. Returns: None p = so.Plot(tips, x=\'day\').add(so.Bar(), so.Count()) p.show() def plot_day_sex_count(): Creates and displays a grouped bar plot of the number of observations for each day, grouped by the \'sex\' of the bill payer. Returns: None p = so.Plot(tips, x=\'day\', color=\'sex\').add(so.Bar(), so.Count(), so.Dodge()) p.show() def plot_size_count(): Creates and displays a bar plot of the number of observations for each size value. Returns: None p = so.Plot(tips, x=\'size\').add(so.Bar(), so.Count()) p.show() def plot_size_count_y_axis(): Creates and displays a bar plot of the number of observations for each size value where counts are on the y-axis. Returns: None p = so.Plot(tips, y=\'size\').add(so.Bar(), so.Count()) p.show()"},{"question":"**Objective**: Create an incremental learning system for text classification using scikit-learn that processes data in mini-batches from a simulated out-of-core data source. **Problem Statement**: Your task is to design an incremental learning system for classifying text data into two categories: \\"spam\\" and \\"ham\\" (not spam). The data is too large to fit into memory all at once, so you will need to process it in batches. **Requirements**: 1. **Data Streaming**: Simulate an out-of-core data stream by reading text data in mini-batches from a list of documents. 2. **Feature Extraction**: Use the `HashingVectorizer` to transform the text data into a format suitable for machine learning. 3. **Incremental Learning Model**: Implement an incremental learning model using the `SGDClassifier`. 4. **Evaluation**: Track and print the model\'s accuracy after each batch. **Data**: You will be provided with two lists: `documents` and `labels`. Each element in `documents` is a string representing an email, and the corresponding element in `labels` is 0 for \\"ham\\" and 1 for \\"spam\\". **Constraints**: - Process the data in mini-batches of size 100. - You may assume you have an initial smaller subset of data (first 1000 documents) to initialize your model. - Use the `SGDClassifier` from scikit-learn for classification. - Use the `HashingVectorizer` from scikit-learn for feature extraction. **Expected Output**: Your implementation should print the model’s accuracy after processing each mini-batch. **Function Signature**: ```python def incremental_text_classification(documents: List[str], labels: List[int]) -> None: pass ``` # Example Usage: ```python documents = [ \\"Free money!!!\\", \\"Hey, how are you?\\", \\"Earn cash now\\", ... # Total 5000 documents ] labels = [ 1, 0, 1, ... # Corresponding 5000 labels ] incremental_text_classification(documents, labels) ``` **Notes**: 1. You should split the `documents` and `labels` into mini-batches of size 100 and process them sequentially. 2. Start by initializing the classifier using the first 1000 documents and labels. 3. After the initial training, continue to update the classifier with the remaining documents in mini-batches. 4. Track the classifier\'s accuracy on a validation set after each mini-batch. **Hints**: - Utilize `partial_fit` method for incremental learning. - Ensure you handle the feature extraction and learning in the right sequence.","solution":"from typing import List from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split def incremental_text_classification(documents: List[str], labels: List[int]) -> None: # Define batch size batch_size = 100 initial_train_size = 1000 # Initialize HashingVectorizer for feature extraction vectorizer = HashingVectorizer(alternate_sign=False) # Split the initial data for training X_initial, X_remaining, y_initial, y_remaining = train_test_split( documents, labels, train_size=initial_train_size, stratify=labels) # Fit the initial vectorizer and model X_initial_vect = vectorizer.fit_transform(X_initial) model = SGDClassifier() model.partial_fit(X_initial_vect, y_initial, classes=[0, 1]) # Track the accuracy n_batches = len(X_remaining) // batch_size for i in range(n_batches): batch_start = i * batch_size batch_end = (i + 1) * batch_size X_batch = X_remaining[batch_start:batch_end] y_batch = y_remaining[batch_start:batch_end] # Transform the batch data X_batch_vect = vectorizer.transform(X_batch) # Update the model in increments model.partial_fit(X_batch_vect, y_batch) # Evaluate the model on the current batch y_pred = model.predict(X_batch_vect) accuracy = accuracy_score(y_batch, y_pred) print(f\\"Batch {i + 1}/{n_batches}, Accuracy: {accuracy:.4f}\\")"},{"question":"Efficient List Operations You are given a list of tuples, where each tuple contains two integers representing a range [start, end). Your task is to implement a function `efficient_range_union(range_list: List[Tuple[int, int]]) -> List[Tuple[int, int]]` that returns a new list of tuples representing the union of all ranges in the input list. The resulting list should contain non-overlapping intervals sorted by their starting points. **Input:** - `range_list`: A list of tuples where each tuple (start, end) represents a range [start, end). **Output:** - A list of tuples representing the union of all ranges, with each tuple (start, end) denoting a non-overlapping interval. **Constraints:** - All start and end integers in the ranges will be non-negative. - The start integer of any range is strictly less than the end integer. **Performance Requirements:** - Your solution should run efficiently even for large input lists. **Example:** ```python # Input ranges = [(1, 3), (2, 4), (5, 7), (6, 8)] # Output [(1, 4), (5, 8)] ``` **Hints:** 1. Sort the input list by the starting points of the ranges. 2. Use an efficient algorithm to merge overlapping ranges. 3. Consider using list comprehensions or generator expressions to manipulate the ranges efficiently. **Starter Code:** ```python from typing import List, Tuple def efficient_range_union(range_list: List[Tuple[int, int]]) -> List[Tuple[int, int]]: # Sort ranges by their starting points sorted_ranges = sorted(range_list) # Initialize the merged ranges list merged_ranges = [] # Use a loop to merge the ranges for start, end in sorted_ranges: if not merged_ranges or merged_ranges[-1][1] < start: # No overlap, add the range to the result merged_ranges.append((start, end)) else: # There is overlap, merge with the last range last_start, last_end = merged_ranges[-1] merged_ranges[-1] = (last_start, max(last_end, end)) return merged_ranges ``` **Notes:** - Ensure that the implementation is efficient and concise. - Test your function with various edge cases to confirm its correctness.","solution":"from typing import List, Tuple def efficient_range_union(range_list: List[Tuple[int, int]]) -> List[Tuple[int, int]]: # Sort ranges by their starting points sorted_ranges = sorted(range_list) # Initialize the merged ranges list merged_ranges = [] # Use a loop to merge the ranges for start, end in sorted_ranges: if not merged_ranges or merged_ranges[-1][1] < start: # No overlap, add the range to the result merged_ranges.append((start, end)) else: # There is overlap, merge with the last range last_start, last_end = merged_ranges[-1] merged_ranges[-1] = (last_start, max(last_end, end)) return merged_ranges"},{"question":"You are tasked with creating a multi-threaded TCP server using the `socketserver` module that can handle multiple client connections simultaneously. The server should be able to receive a string message from the client, process it by reversing the string, and send the reversed string back to the client. # Requirements 1. **Server Implementation**: - Create a custom request handler class by subclassing `socketserver.BaseRequestHandler`. - Override the `handle()` method to: - Read a string message from the client. - Reverse the string. - Send the reversed string back to the client. 2. **Asynchronous Handling**: - Use the `ThreadingMixIn` to ensure that each client request is handled in a separate thread. - Create a `ThreadingTCPServer` class by combining `ThreadingMixIn` and `TCPServer`. # Function Signature ```python import socketserver class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): # Implement the handling logic here pass class ThreadingTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass def run_server(): HOST, PORT = \\"localhost\\", 9999 with ThreadingTCPServer((HOST, PORT), MyTCPHandler) as server: server.serve_forever() # Define the server and client communication testing block if __name__ == \\"__main__\\": run_server() ``` # Input and Output **Client Side**: - The client will send a string message to the server. - The server will respond with the reversed string. Example: ```plaintext Client sends: \\"hello world\\" Server responds: \\"dlrow olleh\\" ``` # Constraints - The server must handle multiple clients concurrently using threads. - Each client request’s response should be independent of other client requests. # Evaluation Criteria Your implementation will be evaluated on: - Correctness: The server correctly reverses the messages and handles multiple clients. - Proper use of threads to manage multiple client connections simultaneously. - Adherence to the `socketserver` framework for implementing the server. **Notes**: - You are encouraged to write additional client-side code to test the server. - Ensure proper handling of socket operations and clean termination of server threads. Good luck!","solution":"import socketserver class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): # Receive data from the client data = self.request.recv(1024).strip() # Reverse the string reversed_data = data[::-1] # Send the reversed string back to the client self.request.sendall(reversed_data) class ThreadingTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass def run_server(): HOST, PORT = \\"localhost\\", 9999 with ThreadingTCPServer((HOST, PORT), MyTCPHandler) as server: server.serve_forever() # Define the server and client communication testing block if __name__ == \\"__main__\\": run_server()"},{"question":"**Objective**: Test students\' understanding of high-level file operations provided by the `shutil` module, as well as their ability to handle exceptions and preserve metadata during file manipulations. **Problem Statement**: You are tasked with creating a backup script that copies a directory to a new location, ensuring that all files and subdirectories are copied along with their metadata. Additionally, your script should handle common errors, such as attempting to copy a file onto itself or encountering a read-only file. **Requirements**: 1. **Function**: Implement a function `backup_directory(src: str, dst: str) -> str` that takes two arguments: - `src`: The source directory path. - `dst`: The destination directory path. 2. **Functionality**: - Use `shutil.copytree` to recursively copy the entire `src` directory to `dst`. - Ensure metadata (such as file permissions and modification times) is preserved for all files and subdirectories. - If `dst` already exists, overwrite its contents. - Handle the following exceptions: - If the source and destination are the same, raise a `ValueError` with the message: `\\"Source and destination directories must be different.\\"` - If encountering a read-only file, change its permission to writable and then retry the copy operation. - Other exceptions should be logged with their respective error messages but should not terminate the program. 3. **Return**: - The function should return the path to the newly created backup directory. 4. **Constraints**: - You may assume that the paths provided are valid. - Use the `shutil` module functions wherever applicable. # Example Usage ```python def backup_directory(src: str, dst: str) -> str: # Your implementation here # Example call backup_directory(\'/path/to/source\', \'/path/to/destination\') ``` # Notes - You must not use any global variables. - Include error handling for exceptions specific to `shutil` functions. - Use the given function signature for implementation. # Hints - Refer to the `shutil` documentation to understand the behaviors and parameters of functions like `shutil.copytree`, `shutil.rmtree`, and error handling mechanisms.","solution":"import shutil import os import logging def backup_directory(src: str, dst: str) -> str: Copies the directory from `src` to `dst`, ensuring all metadata is preserved. Args: src (str): Source directory path. dst (str): Destination directory path. Returns: str: The destination directory path. Raises: ValueError: If source and destination directories are the same. if os.path.abspath(src) == os.path.abspath(dst): raise ValueError(\\"Source and destination directories must be different.\\") def copy_function(src, dst, *, follow_symlinks=True): try: shutil.copy2(src, dst, follow_symlinks=follow_symlinks) except PermissionError: os.chmod(src, 0o777) shutil.copy2(src, dst, follow_symlinks=follow_symlinks) except Exception as e: logging.error(f\\"Error copying {src} to {dst}: {e}\\") if os.path.exists(dst): shutil.rmtree(dst) shutil.copytree(src, dst, copy_function=copy_function) return dst"},{"question":"You have been provided with a prototype feature in PyTorch\'s `torch.cuda.tunable` module, which is aimed at tuning and optimizing GPU performance. Your task is to implement a function that sets up the tuning parameters, enables tuning, performs tuning operations on matrix multiplication (GEMM), and retrieves the tuning results. Specifically, you are required to: 1. Set the maximum tuning duration to 1000 milliseconds. 2. Set the maximum number of tuning iterations to 50. 3. Use a rotating buffer size of 10. 4. Enable all necessary tuning features. 5. Perform GEMM tuning using a specific file (indicated by `filename`). 6. Retrieve and return the tuning results. Function Signature ```python def setup_and_tune_gemm(filename: str) -> dict: pass ``` Input - `filename` (str): The name of the file where tuning results should be saved and loaded from. Output - The function should return a dictionary containing the tuning results. The structure of the dictionary will depend on the output of `get_results()` function from the `torch.cuda.tunable` module. Constraints - Ensure that your implementation makes use of appropriate functions from the `torch.cuda.tunable` module to set parameters, enable tuning, perform tuning, and retrieve results. - The tuning operations should be controlled to avoid indefinite execution by adhering to the specified maximum duration and iterations. Example Usage ```python results = setup_and_tune_gemm(\\"gemm_tuning_results.txt\\") print(results) ``` Additional Information - This is a prototype feature, so certain aspects may be subject to change. - Test your implementation with appropriate mock setups if necessary, to ensure compatibility. Good luck, and happy coding!","solution":"import torch def setup_and_tune_gemm(filename: str) -> dict: Sets up the tuning parameters, enables tuning, performs GEMM tuning using the specified file, and retrieves the tuning results. Args: filename (str): The name of the file where tuning results should be saved and loaded from. Returns: dict: Dictionary containing the tuning results. # Initialize the tunable module tunable = torch.cuda.tunable # Set tuning parameters tunable.max_duration(1000) # Set the maximum tuning duration to 1000 milliseconds tunable.max_iterations(50) # Set the maximum number of tuning iterations to 50 tunable.rotating_buffer_size(10) # Use a rotating buffer size of 10 # Enable tuning features tunable.enable() # Perform GEMM tuning using the specified file tunable.tune_gemm(filename) # Retrieve the tuning results results = tunable.get_results() return results"},{"question":"**Problem Statement:** You are provided with a dataset named `tips` that contains information about the tips received by waiters in a restaurant. This dataset includes the following columns: - `total_bill`: Total bill (numeric). - `tip`: Tip given (numeric). - `sex`: Gender of the person paying the bill (categorical). - `smoker`: Whether the person is a smoker or not (categorical). - `day`: Day of the week (categorical). - `time`: Time of the day - Lunch or Dinner (categorical). - `size`: Size of the group (numeric). Your task is to perform the following steps using the seaborn library to visualize the data: 1. Load the `tips` dataset using `sns.load_dataset()`. 2. Create a histogram of the `total_bill` column. 3. Create a KDE plot of the `tip` column. 4. Create an ECDF plot of the `total_bill` column, colored by the `smoker` status. 5. Create a bivariate histogram plot with `total_bill` on the x-axis and `tip` on the y-axis. 6. Create a bivariate KDE plot with `total_bill` on the x-axis and `tip` on the y-axis, including a marginal \\"rug\\" plot. 7. Create a facet grid of KDE plots for the `total_bill` column, faceted by `sex` and colored by `smoker` status. 8. Customize the facet grid\'s appearance by setting the axis labels to \\"Density\\" and \\"Total Bill ()\\", and setting the title for each facet with the format \\"{col_name} customers\\". **Input:** - None (The dataset is to be loaded directly using seaborn). **Output:** - Visualization plots as described above. **Constraints:** - You must use the seaborn library for plotting. - Ensure the plots are labeled appropriately and use consistent style settings. **Performance Requirements:** - The code should execute efficiently and produce the required plots without performance issues. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # 1. Histogram of the total bill sns.displot(data=tips, x=\\"total_bill\\") plt.show() # 2. KDE plot of the tip sns.displot(data=tips, x=\\"tip\\", kind=\\"kde\\") plt.show() # 3. ECDF plot of the total bill, colored by smoker status sns.displot(data=tips, x=\\"total_bill\\", kind=\\"ecdf\\", hue=\\"smoker\\") plt.show() # 4. Bivariate histogram plot sns.displot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.show() # 5. Bivariate KDE plot with marginal rug sns.displot(data=tips, x=\\"total_bill\\", y=\\"tip\\", kind=\\"kde\\", rug=True) plt.show() # 6. Facet grid of KDE plots for total_bill by sex colored by smoker g = sns.displot(data=tips, x=\\"total_bill\\", hue=\\"smoker\\", col=\\"sex\\", kind=\\"kde\\") g.set_axis_labels(\\"Density\\", \\"Total Bill ()\\") g.set_titles(\\"{col_name} customers\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_tips_dataset(): Load the tips dataset from seaborn. return sns.load_dataset(\\"tips\\") def plot_total_bill_histogram(data): Create a histogram of the total_bill column. sns.displot(data, x=\\"total_bill\\") plt.show() def plot_tip_kde(data): Create a KDE plot of the tip column. sns.displot(data, x=\\"tip\\", kind=\\"kde\\") plt.show() def plot_total_bill_ecdf(data): Create an ECDF plot of the total_bill column, colored by smoker. sns.displot(data, x=\\"total_bill\\", kind=\\"ecdf\\", hue=\\"smoker\\") plt.show() def plot_bivariate_histogram(data): Create a bivariate histogram plot with total_bill on x-axis and tip on y-axis. sns.displot(data, x=\\"total_bill\\", y=\\"tip\\") plt.show() def plot_bivariate_kde_with_rug(data): Create a bivariate KDE plot with total_bill on x-axis and tip on y-axis including marginal rug plot. sns.displot(data, x=\\"total_bill\\", y=\\"tip\\", kind=\\"kde\\", rug=True) plt.show() def plot_facet_grid_kde(data): Create a facet grid of KDE plots for the total_bill column, faceted by sex and colored by smoker. Customize the appearance. g = sns.displot(data, x=\\"total_bill\\", hue=\\"smoker\\", col=\\"sex\\", kind=\\"kde\\") g.set_axis_labels(\\"Density\\", \\"Total Bill ()\\") g.set_titles(\\"{col_name} customers\\") plt.show()"},{"question":"Question # Objective: Write a function `style_and_export` that takes a DataFrame and several styling parameters as inputs, applies the specified stylings to the DataFrame using pandas `Styler`, and then exports the styled DataFrame to an HTML file. # Input: 1. `df`: A pandas DataFrame that contains the data to be styled. 2. `highlight`: A dictionary with keys `max`, `min`, `null`, `between`, and `quantile`, containing lists of column names to apply these respective styles. (`between` will be a list of tuples where each tuple has three elements: column name, lower bound, upper bound). 3. `background_gradient`: A list of column names to which background gradient should be applied. 4. `bar`: A list of column names to which bar plots should be applied. 5. `output_file`: The name of the HTML file to which the styled DataFrame will be exported. # Output: - An HTML file with the name specified by `output_file`, containing the styled DataFrame. # Constraints: - Each list in the input dictionary for `highlight` can contain up to 5 columns. - DataFrame can have up to 1000 rows and 20 columns. - The function should be efficient and handle the maximum input sizes within reasonable time limits. # Function Signature: ```python def style_and_export(df: pd.DataFrame, highlight: dict, background_gradient: list, bar: list, output_file: str) -> None: pass ``` # Example: ```python import pandas as pd data = { \'A\': [1, 2, 3, 4, 5, None], \'B\': [10, 20, 30, None, 50, 60], \'C\': [5, 6, 7, 8, 9, 10], \'D\': [7.4, 6.2, 3.1, 0.6, 2.7, 9.5] } df = pd.DataFrame(data) highlight = { \'max\': [\'A\'], \'min\': [\'B\'], \'null\': [\'A\', \'B\'], \'between\': [(\'C\', 6, 8)], \'quantile\': [\'D\'] } background_gradient = [\'D\'] bar = [\'A\'] style_and_export(df, highlight, background_gradient, bar, \'styled_dataframe.html\') ``` This will create an HTML file named `styled_dataframe.html` with the specified styles applied to the DataFrame.","solution":"import pandas as pd def style_and_export(df: pd.DataFrame, highlight: dict, background_gradient: list, bar: list, output_file: str) -> None: Styles a DataFrame and exports it to an HTML file using the provided parameters. Parameters: - df: pandas DataFrame to be styled. - highlight: dictionary containing columns for different types of highlighting. - background_gradient: list of columns for which background gradient should be applied. - bar: list of columns for which bar plots should be applied. - output_file: name of the HTML file to export the styled DataFrame. Returns: None styler = df.style if \'max\' in highlight: for col in highlight[\'max\']: styler = styler.highlight_max(subset=[col], color=\'yellow\') if \'min\' in highlight: for col in highlight[\'min\']: styler = styler.highlight_min(subset=[col], color=\'lightgreen\') if \'null\' in highlight: for col in highlight[\'null\']: styler = styler.highlight_null(subset=[col], color=\'red\') if \'between\' in highlight: for col, lower, upper in highlight[\'between\']: styler = styler.apply( lambda x: [\'background-color: lightblue\' if lower <= v <= upper else \'\' for v in x], subset=[col]) if \'quantile\' in highlight: for col in highlight[\'quantile\']: styler = styler.apply( lambda x: [\'background-color: orange\' if v > x.quantile(0.25) and v < x.quantile(0.75) else \'\' for v in x], subset=[col]) for col in background_gradient: styler = styler.background_gradient(subset=[col]) for col in bar: styler = styler.bar(subset=[col], color=\'lightblue\') styler.to_html(output_file)"},{"question":"Objective Use `torch.cond` to create a PyTorch model that dynamically changes its architecture based on the values in the input tensor. Problem Statement You are required to create a custom PyTorch module named `ThresholdBasedModel` that modifies its behavior based on whether the sum of the elements in the input tensor is greater than a specified threshold. 1. Implement a class `ThresholdBasedModel` that inherits from `torch.nn.Module`. 2. The class should take a threshold as an initialization parameter. 3. The `forward` method should use `torch.cond` to return the cosine of the input tensor if the sum of its elements is greater than the threshold, otherwise, it should return the sine of the input tensor. 4. Demonstrate the usage of your model with test cases. Function Signature ```python import torch class ThresholdBasedModel(torch.nn.Module): def __init__(self, threshold: float): Initializes the ThresholdBasedModel. Args: threshold (float): The threshold value to be used for the conditional check. super(ThresholdBasedModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: Forward pass of the model. Args: x (torch.Tensor): The input tensor. Returns: torch.Tensor: The tensor after applying condition based operations. def true_fn(x: torch.Tensor) -> torch.Tensor: return x.cos() def false_fn(x: torch.Tensor) -> torch.Tensor: return x.sin() return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x,)) ``` Test Cases ```python model = ThresholdBasedModel(threshold=4.0) # Test case 1 input_tensor = torch.tensor([1.0, 1.0, 1.0]) output = model(input_tensor) # Since sum of input_tensor is 3.0, which is less than threshold 4.0, it should return torch.sin(input_tensor) assert torch.equal(output, input_tensor.sin()) # Test case 2 input_tensor = torch.tensor([2.0, 2.0, 2.0]) output = model(input_tensor) # Since sum of input_tensor is 6.0, which is greater than threshold 4.0, it should return torch.cos(input_tensor) assert torch.equal(output, input_tensor.cos()) print(\\"All test cases passed!\\") ``` Constraints - Use `torch.cond` for implementing the conditional logic. - Ensure that you handle both floating-point and integer tensors. This task will test the student\'s ability to understand and apply conditional operations using `torch.cond` within a model. It requires knowledge of PyTorch modules, tensor operations, and conditional logic.","solution":"import torch import torch.nn.functional as F from torch import nn class ThresholdBasedModel(nn.Module): def __init__(self, threshold: float): Initializes the ThresholdBasedModel. Args: threshold (float): The threshold value to be used for the conditional check. super(ThresholdBasedModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: Forward pass of the model. Args: x (torch.Tensor): The input tensor. Returns: torch.Tensor: The tensor after applying condition based operations. sum_x = x.sum() if sum_x > self.threshold: return x.cos() else: return x.sin()"},{"question":"Objective: Write a class `CustomDateTime` that utilizes Python\'s `datetime` module to provide specific functionalities as outlined below. This class should be able to perform operations related to both dates and times, leveraging the datetime functionalities described in the provided documentation. Class Definition: ```python class CustomDateTime: def __init__(self, date_str): Initialize the CustomDateTime object. Args: date_str (str): A string representing a date and time in the format \'YYYY-MM-DD HH:MM:SS\' pass def add_days(self, days): Add a specific number of days to the current CustomDateTime object. Args: days (int): The number of days to add. Returns: str: A string representing the new date and time in the format \'YYYY-MM-DD HH:MM:SS\'. pass def time_difference(self, other_date_str): Calculate and return the difference in time between the current date and another date. Args: other_date_str (str): A string representing the other date and time in the format \'YYYY-MM-DD HH:MM:SS\'. Returns: str: A string representing the difference in the format \'D days, HH:MM:SS\'. pass def is_leap_year(self): Check if the year of the current CustomDateTime object is a leap year. Returns: bool: True if the year is a leap year, False otherwise. pass ``` Instructions: 1. Implement the `__init__` method to parse the input date string and initialize a `datetime` object. 2. Implement the `add_days` method to add a given number of days to the current date and time and return the result as a string. 3. Implement the `time_difference` method to compute the difference between the current date and time and another specified date and time, returning the result in days, hours, minutes, and seconds format. 4. Implement the `is_leap_year` method to determine if the current year is a leap year. Constraints: - The input date string will always be in the \'YYYY-MM-DD HH:MM:SS\' format. - You may assume valid input and do not need to handle invalid date formats or values. - The number of days to add will be a non-negative integer. - The method `time_difference` should consider both dates are in the same time zone. Example Usage: ```python # Example: date_object = CustomDateTime(\'2023-03-15 12:30:00\') print(date_object.add_days(5)) # Output: \'2023-03-20 12:30:00\' print(date_object.time_difference(\'2023-03-10 12:30:00\')) # Output: \'5 days, 00:00:00\' print(date_object.is_leap_year()) # Output: False ``` **Performance Requirements**: - The `add_days` method should run in O(1) time. - The `time_difference` method should run in O(1) time. - The `is_leap_year` method should run in O(1) time. Good luck!","solution":"from datetime import datetime, timedelta class CustomDateTime: def __init__(self, date_str): Initialize the CustomDateTime object. Args: date_str (str): A string representing a date and time in the format \'YYYY-MM-DD HH:MM:SS\' self.datetime_obj = datetime.strptime(date_str, \'%Y-%m-%d %H:%M:%S\') def add_days(self, days): Add a specific number of days to the current CustomDateTime object. Args: days (int): The number of days to add. Returns: str: A string representing the new date and time in the format \'YYYY-MM-DD HH:MM:SS\'. new_datetime = self.datetime_obj + timedelta(days=days) return new_datetime.strftime(\'%Y-%m-%d %H:%M:%S\') def time_difference(self, other_date_str): Calculate and return the difference in time between the current date and another date. Args: other_date_str (str): A string representing the other date and time in the format \'YYYY-MM-DD HH:MM:SS\'. Returns: str: A string representing the difference in the format \'D days, HH:MM:SS\'. other_datetime_obj = datetime.strptime(other_date_str, \'%Y-%m-%d %H:%M:%S\') delta = self.datetime_obj - other_datetime_obj days, seconds = delta.days, delta.seconds hours = seconds // 3600 minutes = (seconds % 3600) // 60 seconds = seconds % 60 return f\\"{days} days, {hours:02}:{minutes:02}:{seconds:02}\\" def is_leap_year(self): Check if the year of the current CustomDateTime object is a leap year. Returns: bool: True if the year is a leap year, False otherwise. year = self.datetime_obj.year if (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0): return True else: return False"},{"question":"**Background:** Python 3.9 introduced the concept of generic types for type hinting, making it easier to define classes and functions that can handle a variety of types. One such utility is the `GenericAlias` object, which allows specifying generic types in Python. For example, `list[int]` uses this mechanism internally. **Task:** Implement a Python function `create_generic_alias` that mimics the behavior of the `Py_GenericAlias` function, using Python constructs. Your function should return an instance that behaves similarly. **Function Signature:** ```python from typing import Type, TypeVar, Tuple, Union T = TypeVar(\'T\') class GenericAlias: def __init__(self, origin: Type[T], args: Union[Tuple[T], T]): self.__origin__ = origin self.__args__ = args if isinstance(args, tuple) else (args,) def __repr__(self): return f\\"{self.__origin__.__name__}{self.__args__}\\" def create_generic_alias(origin: Type[T], args: Union[Tuple[T], T]) -> GenericAlias: # Your implementation here return GenericAlias(origin, args) ``` **Input:** - `origin`: A type, such as `list`, `dict`, or any user-defined type. - `args`: Either a single type or a tuple of types. **Output:** - An instance of `GenericAlias` with the specified origin and args. **Example:** ```python # Using primitives generic_list = create_generic_alias(list, int) print(repr(generic_list)) # Output: list(<class \'int\'>) # Using tuple of types generic_dict = create_generic_alias(dict, (str, int)) print(repr(generic_dict)) # Output: dict(<class \'str\'>, <class \'int\'>) # Using user-defined class class MyClass: pass generic_custom = create_generic_alias(MyClass, str) print(repr(generic_custom)) # Output: MyClass(<class \'str\'>) ``` **Constraints:** - Ensure that `origin` is always a type. - `args` can be either a single type or a tuple of types. - Maintain the behavior consistent with PEP 585 for `GenericAlias`.","solution":"from typing import Type, TypeVar, Tuple, Union T = TypeVar(\'T\') class GenericAlias: def __init__(self, origin: Type[T], args: Union[Tuple[T], T]): self.__origin__ = origin self.__args__ = args if isinstance(args, tuple) else (args,) def __repr__(self): args_repr = \\", \\".join(repr(arg) for arg in self.__args__) return f\\"{self.__origin__.__name__}[{args_repr}]\\" def create_generic_alias(origin: Type[T], args: Union[Tuple[T], T]) -> GenericAlias: return GenericAlias(origin, args)"},{"question":"# Seaborn FacetGrid Coding Assessment Objective Create a visual analysis of the \\"tips\\" dataset using Seaborn\'s `FacetGrid` to demonstrate your understanding of advanced plotting and customization techniques in Seaborn. Problem Statement You are given the \\"tips\\" dataset which contains information about tips received by waiters in a restaurant. Your task is to create a `FacetGrid` visualization that showcases the distribution and patterns in the dataset based on various factors. Requirements 1. **Initialization**: Initialize a `FacetGrid` with the `tips` dataset. 2. **Grid Customization**: - Plot subplots based on the `day` of the week (`col`) and `time` of day (`row`). - Set the height of each subplot to 3 and the aspect ratio to 0.7. 3. **Plot Data**: - Use a histogram to plot the distribution of `total_bill` in each subplot. - Apply a bin width of 2 and a bin range from 0 to 60. 4. **Advanced Customization**: - Add a `hue` based on the `sex` column. - Use `FacetGrid.map_dataframe` to ensure that bin widths are consistent across facets. - Annotate each subplot to display the median `tip` amount for that subset of data. 5. **Legends and Reference Lines**: - Add a legend to the grid. - Add a horizontal reference line at the median `tip` value calculated across the entire dataset. 6. **Axes and Titles**: - Set the axis labels for `total_bill` and `tip`. - Customize the titles for columns and rows to include the names of the respective `day` and `time`. Input You should use the `tips` dataset provided by Seaborn. Output A `FacetGrid` plot saved as \\"facet_tips_analysis.png\\". Sample Code ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Your implementation starts here # Step 1: Initialize the FacetGrid # Step 2: Customize the Grid (set height, aspect, col, and row) # Step 3: Plot the data using histplot with custom bin width and range # Step 4: Add a hue, annotate with median tip value, and add reference lines # Step 5: Add legends and customize axes labels and titles # Step 6: Save the plot # Your implementation ends here ``` Constraints - Ensure that the bin widths are consistent across all facets. - Use at least one custom annotation function to display the median `tip` value in each subplot. Performance - The solution should efficiently handle the `tips` dataset, which is a relatively small dataset. Ensure your code executes without errors and generates the required plot with the specified customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the tips dataset tips = sns.load_dataset(\\"tips\\") def annotate_median_tip(data, **kws): Annotates the median tip in each subplot. median_tip = np.median(data[\'tip\']) ax = plt.gca() ax.axhline(median_tip, ls=\'--\', color=\'red\') ax.text(0.7, 0.7, f\'Median Tip: {median_tip:.2f}\', transform=ax.transAxes, fontsize=10, verticalalignment=\'top\', bbox=dict(boxstyle=\'round,pad=0.5\', edgecolor=\'black\', facecolor=\'white\')) # Step 1: Initialize the FacetGrid g = sns.FacetGrid(tips, row=\\"time\\", col=\\"day\\", hue=\\"sex\\", height=3, aspect=0.7) # Step 2: Customize the Grid and Plot the data using barplot with custom bin width and range g.map_dataframe(sns.histplot, x=\\"total_bill\\", binwidth=2, binrange=(0, 60), kde=False) # Step 4: Add the annotation with the median tip value g.map_dataframe(annotate_median_tip) # Step 5: Add legends and customize axes labels and titles g.add_legend() g.set_axis_labels(\\"Total Bill\\", \\"Frequency\\") g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Save the plot g.savefig(\\"facet_tips_analysis.png\\") plt.close()"},{"question":"# Naive Bayes Text Classification You are required to implement a text classification system using scikit-learn\'s Multinomial Naive Bayes classifier. The goal is to classify text documents into predefined categories based on their content. You will work with a dataset of text documents and labels provided in the form of arrays. Requirements 1. **Function Name**: `text_classification` 2. **Input**: - `train_texts`: A list of strings representing the training text documents. - `train_labels`: A 1D numpy array of integers representing the labels for the training documents. - `test_texts`: A list of strings representing the test text documents. 3. **Output**: - A 1D numpy array of integers representing the predicted labels for the test documents. 4. **Constraints**: - You may use any preprocessing techniques like tokenization, stop-word removal, and vectorization. - The dataset is not too large to fit into memory. 5. **Performance Requirements**: - The function should process the data and return the results within a reasonable time frame suitable for real-world applications. - The accuracy of the predictions is essential but not necessarily the primary criteria for this assessment. Detailed Description 1. Import necessary libraries. 2. Preprocess the input text data using appropriate methods (e.g., tokenization and vectorization). 3. Initialize and fit a `MultinomialNB` classifier using the training text data and labels. 4. Predict the labels for the test text data. 5. Return the predicted labels as a numpy array. Example ```python import numpy as np def text_classification(train_texts, train_labels, test_texts): from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import MultinomialNB # Step 1: Vectorize the text data (convert text to numeric features) vectorizer = CountVectorizer() X_train = vectorizer.fit_transform(train_texts) X_test = vectorizer.transform(test_texts) # Step 2: Initialize the MultinomialNB classifier classifier = MultinomialNB() # Step 3: Fit the classifier with the training data classifier.fit(X_train, train_labels) # Step 4: Predict the labels for the test data y_pred = classifier.predict(X_test) return y_pred # Example usage train_texts = [\\"The sky is blue\\", \\"The sun is bright\\"] train_labels = np.array([0, 1]) test_texts = [\\"The sun in the sky is bright\\", \\"We can see the shining sun, the bright sun\\"] predicted_labels = text_classification(train_texts, train_labels, test_texts) print(predicted_labels) # Output: [1, 1] ``` You must ensure that your function follows the specifications and accurately predicts the test document labels using the Naive Bayes classifier.","solution":"import numpy as np from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import MultinomialNB def text_classification(train_texts, train_labels, test_texts): Classifies text documents into predefined categories based on their content using Multinomial Naive Bayes classifier. Parameters: train_texts (list of str): Training text documents. train_labels (numpy array): Labels for training documents. test_texts (list of str): Test text documents. Returns: numpy array of int: Predicted labels for the test documents. # Step 1: Vectorize the text data (convert text to numeric features) vectorizer = CountVectorizer() X_train = vectorizer.fit_transform(train_texts) X_test = vectorizer.transform(test_texts) # Step 2: Initialize the MultinomialNB classifier classifier = MultinomialNB() # Step 3: Fit the classifier with the training data classifier.fit(X_train, train_labels) # Step 4: Predict the labels for the test data y_pred = classifier.predict(X_test) return y_pred"},{"question":"**Question: Advanced IP Address and Network Operations** You are given a network and a list of IP address strings. Your task is to write a Python function `analyze_network` that: 1. Creates an `IPv4Network` object from the given network string. 2. Creates a list of `IPv4Address` objects from the list of IP address strings. 3. Filters out and returns the IP addresses that fall within the given network. 4. Converts the filtered list into a summarized network range. **Function Signature:** ```python def analyze_network(network_str: str, ip_list: list) -> list: pass ``` **Input:** - `network_str` (str): A string representing the network in CIDR notation (e.g., \'192.168.1.0/24\'). - `ip_list` (list): A list of strings, each representing an IPv4 address (e.g., [\'192.168.1.1\', \'192.168.1.100\', \'10.0.0.1\']). **Output:** - Returns a list of network objects representing the summarized network range of the filtered IP addresses. **Constraints:** - You may assume that the input strings are well-formed and represent valid IP addresses and networks. **Example:** ```python network_str = \'192.168.1.0/24\' ip_list = [\'192.168.1.1\', \'192.168.1.100\', \'10.0.0.1\'] result = analyze_network(network_str, ip_list) # Output should be a list of networks in the summarized range: # [IPv4Network(\'192.168.1.0/25\')] ``` **Additional Requirements:** - Make use of the `ipaddress` module in your implementation. - Ensure the function is efficient and can handle a moderate number of IP addresses (up to 10^4) in a reasonable time.","solution":"import ipaddress def analyze_network(network_str: str, ip_list: list) -> list: Analyzes a network and filters out IP addresses that fall within it, returning a summarized network range. Args: network_str (str): A string representing the network in CIDR notation (e.g., \'192.168.1.0/24\'). ip_list (list): A list of strings, each representing an IPv4 address (e.g., [\'192.168.1.1\', \'192.168.1.100\', \'10.0.0.1\']). Returns: list: A list of network objects representing the summarized network range of the filtered IP addresses. # Create an IPv4Network object from the given network string network = ipaddress.IPv4Network(network_str) # Create a list of IPv4Address objects from the list of IP address strings ip_objects = [ipaddress.IPv4Address(ip) for ip in ip_list] # Filter out IP addresses that fall within the given network filtered_ips = [ip for ip in ip_objects if ip in network] # Summarize the filtered IP addresses into network ranges summarized_ranges = ipaddress.collapse_addresses(filtered_ips) return list(summarized_ranges)"},{"question":"Objective: The goal of this assessment is to gauge your understanding of Python’s numerical operations and type conversions by implementing a mini calculator class. You are to utilize the numerical functions described in the `python310` package documentation provided. Problem Statement: You are required to implement a class `MiniCalculator` that supports a few selected numerical operations using the provided `python310` package implementations. You will implement addition, subtraction, multiplication, and conversion of an object to an integer and float. Class Definition: ```python class MiniCalculator: def add(self, o1, o2): Adds two numbers using PyNumber_Add. Parameters: o1 (PyObject): The first number. o2 (PyObject): The second number. Returns: PyObject: A new object representing the sum of o1 and o2. pass def subtract(self, o1, o2): Subtracts the second number from the first using PyNumber_Subtract. Parameters: o1 (PyObject): The first number. o2 (PyObject): The second number. Returns: PyObject: A new object representing the difference between o1 and o2. pass def multiply(self, o1, o2): Multiplies two numbers using PyNumber_Multiply. Parameters: o1 (PyObject): The first number. o2 (PyObject): The second number. Returns: PyObject: A new object representing the product of o1 and o2. pass def to_int(self, o): Converts an object to an integer using PyNumber_Long. Parameters: o (PyObject): The object to be converted. Returns: PyObject: An integer object representing the converted value. pass def to_float(self, o): Converts an object to a float using PyNumber_Float. Parameters: o (PyObject): The object to be converted. Returns: PyObject: A float object representing the converted value. pass ``` Constraints: 1. All methods should handle cases where the operations might fail. In such cases, you should return `None`. 2. The input parameters for all methods are Python objects (use `PyObject *o` type in the comments). Expected Input and Output: 1. `add(o1, o2)`: Given two objects that support addition, return the sum. 2. `subtract(o1, o2)`: Given two objects that support subtraction, return the difference. 3. `multiply(o1, o2)`: Given two objects that support multiplication, return the product. 4. `to_int(o)`: Given an object that can be interpreted as an integer, return the integer. 5. `to_float(o)`: Given an object that can be interpreted as a float, return the float. Example: ```python calculator = MiniCalculator() print(calculator.add(3, 5)) # Should output equivalent to 8 print(calculator.subtract(10, 4)) # Should output equivalent to 6 print(calculator.multiply(6, 7)) # Should output equivalent to 42 print(calculator.to_int(5.9)) # Should output 5 print(calculator.to_float(9)) # Should output 9.0 ``` Note: This example assumes that integer and float conversions are straightforward and demonstrate the expected behavior. Evaluation Criteria: - Correct implementation and usage of `python310` package functions. - Proper error handling. - Clean and readable code adhering to Python best practices.","solution":"class MiniCalculator: def add(self, o1, o2): Adds two numbers using basic addition. Parameters: o1, o2 (PyObject): The numbers to be added. Returns: A new object representing the sum of o1 and o2. try: return o1 + o2 except TypeError: return None def subtract(self, o1, o2): Subtracts the second number from the first using basic subtraction. Parameters: o1, o2 (PyObject): The numbers where o1 is minuend and o2 is subtrahend. Returns: A new object representing the difference between o1 and o2. try: return o1 - o2 except TypeError: return None def multiply(self, o1, o2): Multiplies two numbers using basic multiplication. Parameters: o1, o2 (PyObject): The numbers to be multiplied. Returns: A new object representing the product of o1 and o2. try: return o1 * o2 except TypeError: return None def to_int(self, o): Converts an object to an integer. Parameters: o (PyObject): The object to be converted. Returns: An integer object representing the converted value. try: return int(o) except (TypeError, ValueError): return None def to_float(self, o): Converts an object to a float. Parameters: o (PyObject): The object to be converted. Returns: A float object representing the converted value. try: return float(o) except (TypeError, ValueError): return None"},{"question":"**Objective**: To evaluate your understanding of the `contextvars` module and its integration with asynchronous programming in Python. **Problem Statement**: You are required to implement an asynchronous logging system that keeps track of user-specific events. Each user interaction should be logged with a unique identifier for the user. The logging should occur concurrently, and the state related to each user must be preserved independently using context variables. **Requirements**: 1. Define a context variable to store the user ID. 2. Implement a function to log an event with the user ID and the event message. 3. Implement an asynchronous function to handle user interactions. This function should: - Set the user ID in the context variable. - Log a series of predefined events for the user. 4. Implement a main function to: - Simulate multiple concurrent user interactions using asyncio. **Function Specifications**: 1. `log_event(event: str) -> None`: - Inputs: `event` (str): The event message to be logged. - Actions: Logs the event message along with the current user ID retrieved from the context variable. - Output: None 2. `async handle_user_interaction(user_id: int) -> None`: - Inputs: `user_id` (int): The unique identifier for the user. - Actions: Sets the user ID in the context variable. Logs three events: \\"Start\\", \\"Action\\", and \\"End\\" for each user. - Output: None 3. `async main() -> None`: - Actions: Sets up and initiates concurrent user interactions using asyncio. - Output: None **Implementation Constraints**: 1. Use the `contextvars` module for managing context-local state. 2. Ensure that each user\'s events are logged correctly with their respective user IDs. 3. The solution should correctly handle multiple users interacting simultaneously without interference. **Sample Output**: The output should be a sequence of logged events for different users, such as: ``` User 1: Start User 1: Action User 1: End User 2: Start User 2: Action User 2: End User 3: Start User 3: Action User 3: End ``` **Note**: The exact order of events from different users may vary due to the concurrent nature of the tasks. **Submission**: Submit a single Python script containing your implementation of the functions `log_event`, `handle_user_interaction`, and `main`. Ensure the script can be executed to demonstrate the correct functioning of the concurrent user interaction logging.","solution":"import contextvars import asyncio # Define the context variable to store the user ID user_id_var = contextvars.ContextVar(\'user_id\') def log_event(event: str) -> None: Logs the event message along with the current user ID retrieved from the context variable. user_id = user_id_var.get() print(f\\"User {user_id}: {event}\\") async def handle_user_interaction(user_id: int) -> None: Sets the user ID in the context variable. Logs three events: \\"Start\\", \\"Action\\", and \\"End\\" for each user. user_id_var.set(user_id) log_event(\\"Start\\") await asyncio.sleep(0) # Simulate asynchronous behavior log_event(\\"Action\\") await asyncio.sleep(0) # Simulate asynchronous behavior log_event(\\"End\\") async def main() -> None: Sets up and initiates concurrent user interactions using asyncio. users = [1, 2, 3] await asyncio.gather(*(handle_user_interaction(user) for user in users)) # Uncomment below line to run the main function and observe output # asyncio.run(main())"},{"question":"# Distributed Training with PyTorch: Implementing Custom Distributed Optimizer Background In distributed machine learning, training models across multiple machines or devices can significantly speed up the training process. PyTorch provides several built-in optimizers for distributed training. In this task, you are required to create a custom distributed optimizer using PyTorch\'s `torch.distributed` package concepts. Problem Statement You are required to implement a custom distributed optimizer in PyTorch. For this task, you should define a class `CustomDistributedOptimizer` that: 1. **Inherits from** `torch.optim.Optimizer`. 2. **Distributes the optimization process** across multiple processes in a multi-node setup. 3. **Synchronizes** the model\'s gradients across different nodes to ensure consistency. 4. **Implements the following methods**: - `__init__(self, params, lr=0.01, momentum=0)`: Initializes the optimizer with parameters, learning rate, and momentum. - `step(self, closure=None)`: Performs a single optimization step (parameter update). - `synchronize(self)`: Synchronizes gradients across all nodes. Input - `params`: Parameters to be optimized (iterable). - `lr`: Learning rate (float) - Default is 0.01. - `momentum`: Momentum factor (float) - Default is 0. Output - `Updated parameters` after one optimization step (No explicit output; parameters get updated in-place). Constraints - Assume you have access to a distributed computing environment with multiple nodes. - You must use PyTorch\'s distributed communication primitives (e.g., `torch.distributed.all_reduce`). Example Usage To illustrate how your optimizer may be used, here is an outline: ```python import torch import torch.distributed as dist class CustomDistributedOptimizer(torch.optim.Optimizer): def __init__(self, params, lr=0.01, momentum=0): # Initialize the optimizer with parameters, learning rate, and momentum pass def step(self, closure=None): # Implement the parameter update step pass def synchronize(self): # Implement gradient synchronization across nodes pass # Example model and data model = torch.nn.Linear(10, 1) optimizer = CustomDistributedOptimizer(model.parameters(), lr=0.01, momentum=0.9) for data, target in dataloader: optimizer.zero_grad() output = model(data) loss = loss_function(output, target) loss.backward() optimizer.synchronize() optimizer.step() ``` Performance Requirements - Ensure the implementation efficiently synchronizes gradients without causing significant slowdowns. - The optimizer should be tested within a distributed setup to verify correctness.","solution":"import torch import torch.distributed as dist from torch.optim import Optimizer class CustomDistributedOptimizer(Optimizer): def __init__(self, params, lr=0.01, momentum=0): defaults = dict(lr=lr, momentum=momentum) super(CustomDistributedOptimizer, self).__init__(params, defaults) self.world_size = dist.get_world_size() def step(self, closure=None): loss = None if closure is not None: loss = closure() for group in self.param_groups: for param in group[\'params\']: if param.grad is None: continue d_p = param.grad.data if group[\'momentum\'] != 0: param_state = self.state[param] if \'momentum_buffer\' not in param_state: buf = param_state[\'momentum_buffer\'] = torch.clone(d_p).detach() else: buf = param_state[\'momentum_buffer\'] buf.mul_(group[\'momentum\']).add_(d_p, alpha=group[\'lr\']) d_p = buf param.data.add_(-group[\'lr\'], d_p) return loss def synchronize(self): for group in self.param_groups: for param in group[\'params\']: if param.grad is not None: dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM) param.grad.data /= self.world_size"},{"question":"**Title:** Process and Transform Multi-Source Text Streams **Objective:** Write a Python function `process_text_streams(files: List[str], word: str, replacement: str) -> None` that reads multiple text input streams, replaces every occurrence of a specified word with given replacement text within the lines, and prints the results. You should handle standard input and compressed files seamlessly. **Function Signature:** ```python def process_text_streams(files: List[str], word: str, replacement: str) -> None: pass ``` **Parameters:** - `files`: A list of strings where each string represents a file name or \'-\' for standard input. The files may include \'.gz\' and \'.bz2\' compressed files. - `word`: A string that should be replaced in the text streams. - `replacement`: A string with which to replace every occurrence of the specified word. **Constraints:** - You must handle any I/O exceptions gracefully by logging an appropriate error message. - You should ensure that files are properly closed after processing. - If the list of files is empty, `sys.stdin` should be used. - New lines should be preserved. **Example Usage:** Given the files `text1.txt`, `text2.txt`, and an input `word`, your function should replace the word `word` with `replacement` in all provided text streams and print the transformed lines. ```python files = [\'text1.txt\', \'-\', \'text2.txt.gz\'] word = \'foo\' replacement = \'bar\' # Expected: For every line in text1.txt, sys.stdin, and text2.txt.gz, all occurrences of \'foo\' should be replaced with \'bar\' and then printed. ``` **Hint:** Use `fileinput.input` with `fileinput.hook_compressed` for seamless compressed file handling and manage errors using proper exception handling around file operations. **Evaluation Criteria:** - Correctness: The solution should correctly handle file reads and in-place modifications. - Robustness: Proper handling of exceptions and edge cases (e.g., empty files, stdin, etc.). - Efficiency: Ensure the solution performs well with large files and multiple streams. - Code quality: Maintain clear, readable, and maintainable code.","solution":"import fileinput import sys import bz2 import gzip def process_text_streams(files, word, replacement): Reads multiple text input streams, replaces every occurrence of a specified word with given replacement text within the lines, and prints the results. if not files: files = [\'-\'] try: with fileinput.input(files=files, openhook=fileinput.hook_compressed) as f: for line in f: print(line.replace(word, replacement), end=\'\') except Exception as e: sys.stderr.write(f\'Error: {e}n\')"},{"question":"You have been given the task to monitor the training process of a neural network using the `torch.monitor` module. You need to implement a custom event handler that logs the training and validation loss at the end of each epoch and register it with the `torch.monitor` module. The custom event handler should print the epoch number along with the training and validation loss. # Requirements 1. Implement a custom event handler class `CustomLoggingHandler` that handles events. 2. The event handler should log training and validation loss at the end of each epoch. 3. Register the custom event handler with the `torch.monitor` module. 4. Ensure that the handler prints the epoch number along with training and validation loss. # Function Signature ```python class CustomLoggingHandler: def __init__(self): ... def handle_event(self, event): ... def register_custom_handler(): ... ``` # Input Format * The `CustomLoggingHandler`\'s `handle_event` method will receive an event object that includes the following attributes: - `name`: The name of the event. - `value`: The value associated with the event (e.g., training or validation loss). - `epoch`: The current epoch count when the event is logged. # Output Format * The `handle_event` method should print logs in the format: ``` Epoch <epoch_num>: <event_name> = <event_value> ``` # Example ```python import torch.monitor as monitor class CustomLoggingHandler: def __init__(self): # Initialization if needed pass def handle_event(self, event): print(f\'Epoch {event.epoch}: {event.name} = {event.value}\') def register_custom_handler(): handler = CustomLoggingHandler() monitor.register_event_handler(handler.handle_event) # Example usage (This part won\'t be submitted, just for understanding): # Assuming `train` and `validate` are functions that run the training and validation loops respectively # And they log events manually like this: # monitor.log_event(\'training_loss\', loss_value, epoch=current_epoch) # monitor.log_event(\'validation_loss\', val_loss_value, epoch=current_epoch) register_custom_handler() for epoch in range(num_epochs): train(epoch) validate(epoch) ``` Your implementation must ensure that the custom event handler is properly registered and logs the required information correctly.","solution":"import torch.monitor as monitor class CustomLoggingHandler: def __init__(self): # Initialization if needed pass def handle_event(self, event): # Log the event information in the required format print(f\'Epoch {event.epoch}: {event.name} = {event.value}\') def register_custom_handler(): handler = CustomLoggingHandler() monitor.register_event_handler(handler.handle_event)"},{"question":"As a data analyst, you are given the following dataset and are required to use seaborn\'s functionalities to analyze and visualize the data effectively. The dataset `student_scores` contains the following columns: - `hours_studied`: Number of hours a student studied for an exam. - `test_score`: The score the student obtained in the test (out of 100). - `participated_in_study_group`: A boolean value indicating whether the student participated in a study group (0: No, 1: Yes). Using the seaborn package, write a Python function named `analyze_student_scores` that performs the following tasks: 1. Plot a scatter plot with a linear regression line to visualize the relationship between `hours_studied` and `test_score`. 2. Create a new plot to fit a second-order polynomial regression to capture any potential quadratic relationship between `hours_studied` and `test_score`. 3. Generate a third plot to fit a logistic regression to analyze the probability that a student participated in the study group based on their `hours_studied`. 4. Customize the appearance of the polynomial regression plot by setting the marker to \'o\', color of the markers to \'.1\', and line color to \'blue\'. Implement the function with the following signature: ```python import pandas as pd def analyze_student_scores(student_scores: pd.DataFrame) -> None: import seaborn as sns import numpy as np sns.set_theme() # Task 1: Linear regression plot between `hours_studied` and `test_score` # Task 2: Second-order polynomial regression plot # Task 3: Logistic regression plot # Task 4: Customize the appearance of the polynomial regression plot # Example usage: # student_scores = pd.DataFrame({ # \'hours_studied\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], # \'test_score\': [50, 51, 52, 55, 60, 65, 67, 70, 75, 80], # \'participated_in_study_group\': [0, 0, 0, 1, 1, 1, 1, 1, 1, 1] # }) # analyze_student_scores(student_scores) ``` # Input - `student_scores`: A pandas DataFrame containing the columns `hours_studied`, `test_score`, and `participated_in_study_group`. # Output - Three seaborn plots as specified in the tasks that will be displayed. # Constraints - You can assume that the DataFrame `student_scores` is always clean and has the necessary columns with correct data types. - Ensure that the plots have appropriate titles and axis labels to make them understandable for audience. # Notes - Use seaborn\'s `regplot` function for the regressions. - For the logistic regression, you may need to preprocess the `participated_in_study_group` column to ensure it is treated as a binary response.","solution":"import pandas as pd import seaborn as sns import numpy as np import matplotlib.pyplot as plt def analyze_student_scores(student_scores: pd.DataFrame) -> None: sns.set_theme() # Task 1: Linear regression plot between `hours_studied` and `test_score` plt.figure(figsize=(12, 6)) plt.subplot(1, 3, 1) sns.regplot(x=\'hours_studied\', y=\'test_score\', data=student_scores) plt.title(\'Linear Regression of Test Score vs Hours Studied\') plt.xlabel(\'Hours Studied\') plt.ylabel(\'Test Score\') # Task 2: Second-order polynomial regression plot plt.subplot(1, 3, 2) sns.regplot(x=\'hours_studied\', y=\'test_score\', data=student_scores, order=2, marker=\'o\', color=\'.1\', line_kws={\'color\': \'blue\'}) plt.title(\'Polynomial Regression of Test Score vs Hours Studied\') plt.xlabel(\'Hours Studied\') plt.ylabel(\'Test Score\') # Task 3: Logistic regression plot to analyze the probability of study group participation plt.subplot(1, 3, 3) sns.regplot(x=\'hours_studied\', y=\'participated_in_study_group\', data=student_scores, logistic=True, ci=None) plt.title(\'Logistic Regression of Study Group Participation vs Hours Studied\') plt.xlabel(\'Hours Studied\') plt.ylabel(\'Probability of Participation\') plt.tight_layout() plt.show()"},{"question":"**Objective:** Demonstrate your understanding of the `runpy` module by writing a Python script that dynamically runs a given module or script and provides specific information about the execution environment. Problem Statement You are provided with the path to a Python module or script. Your task is to write a function `execute_and_report(path: str) -> dict` that uses the `runpy` module to execute the provided module or script and returns a dictionary containing the following information: 1. The `__name__` of the executed code. 2. The `__file__` of the executed code. 3. The `__package__` of the executed code. 4. Whether the execution was successful or if there was an exception. The function should handle both modules specified by module name (e.g., \\"example_module\\") and scripts specified by filesystem path (e.g., \\"/path/to/script.py\\"). Function Signature ```python import runpy def execute_and_report(path: str) -> dict: # Your code here pass ``` Constraints - You can assume the path provided will always be a valid Python module name or a valid path to an existing Python script. - Handle any imports and system modifications carefully to ensure that the function is thread-safe and reverts any changes after execution. Example Usage ```python result = execute_and_report(\\"example_module\\") # result should be something like: # { # \\"__name__\\": \\"example_module\\", # \\"__file__\\": \\"/path/to/example_module.py\\", # \\"__package__\\": None, # or the package name if it\'s part of a package # \\"success\\": True # } result = execute_and_report(\\"/path/to/script.py\\") # result should be something like: # { # \\"__name__\\": \\"__main__\\", # \\"__file__\\": \\"/path/to/script.py\\", # \\"__package__\\": None, # \\"success\\": True # } ``` Notes - You should catch any exceptions that occur during the execution and report `success: False` in the resulting dictionary. - Remember to clean up any changes made to `sys.path` or other system-level variables after the execution.","solution":"import runpy def execute_and_report(path: str) -> dict: Executes the given module or script and returns a dictionary with execution details. Args: path (str): module name or filesystem path to the script. Returns: dict: a dictionary with keys \'__name__\', \'__file__\', \'__package__\', and \'success\'. result = { \\"__name__\\": None, \\"__file__\\": None, \\"__package__\\": None, \\"success\\": False } try: if path.endswith(\\".py\\"): mod_dict = runpy.run_path(path) else: mod_dict = runpy.run_module(path) result[\\"__name__\\"] = mod_dict.get(\\"__name__\\") result[\\"__file__\\"] = mod_dict.get(\\"__file__\\") result[\\"__package__\\"] = mod_dict.get(\\"__package__\\") result[\\"success\\"] = True except Exception as e: result[\\"error\\"] = str(e) result[\\"success\\"] = False return result"},{"question":"# Custom Autograd Function in PyTorch In this exercise, you are tasked with implementing a custom autograd Function in PyTorch and verifying its gradient computation. You will create a custom `SinFunction` that computes the sine of the input tensor in the forward pass and its corresponding gradient in the backward pass. Requirements 1. **Forward Method:** - Compute the sine of the input tensor. - Save any necessary tensors for the backward pass. 2. **Backward Method:** - Compute the gradient of the sine operation. Recall that if ( y = sin(x) ), then ( frac{dy}{dx} = cos(x) ). 3. **Validation:** - Use `torch.autograd.gradcheck` to validate the gradient computation. # Implementation 1. Define the `SinFunction` class by inheriting from `torch.autograd.Function`. 2. Implement the `forward` and `backward` methods as described. 3. Alias the custom function or wrap it in a more user-friendly function. # Example Here\'s an example of how your class should be structured: ```python import torch from torch.autograd import Function class SinFunction(Function): @staticmethod def forward(ctx, input): # Compute sine of the input tensor sine_output = torch.sin(input) # Save the input tensor for backward computation ctx.save_for_backward(input) return sine_output @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute the gradient of the input tensor grad_input = grad_output * torch.cos(input) return grad_input # Create an alias for the custom function sin_function = SinFunction.apply # Function to use the alias def custom_sin(input): return sin_function(input) # Testing the gradient computation input = torch.randn(3, 3, dtype=torch.double, requires_grad=True) # Validate the gradients gradcheck_test = torch.autograd.gradcheck(custom_sin, (input,), eps=1e-6, atol=1e-4) print(\\"Gradient check successful: \\", gradcheck_test) ``` # Submission 1. Implement the `SinFunction` class as described. 2. Test the function by performing the gradient check. 3. Ensure your code is well-documented and follows best practices. Constraints - Do not use any external libraries other than PyTorch. - The solution should work efficiently for typical tensor sizes in deep learning models. # Evaluation Your submission will be evaluated based on: 1. Correctness: Does the function compute the correct forward and backward results? 2. Performance: Is the computation efficient and within reasonable performance bounds? 3. Documentation: Is the code well-documented and easy to understand?","solution":"import torch from torch.autograd import Function class SinFunction(Function): @staticmethod def forward(ctx, input): # Compute sine of the input tensor sine_output = torch.sin(input) # Save the input tensor for backward computation ctx.save_for_backward(input) return sine_output @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute the gradient of the input tensor grad_input = grad_output * torch.cos(input) return grad_input # Create an alias for the custom function sin_function = SinFunction.apply # Function to use the alias def custom_sin(input): return sin_function(input)"},{"question":"# Memory-Mapped File Manipulation and Search In this assessment, you will demonstrate your understanding of the `mmap` module by writing functions that perform essential file manipulations and searches using memory mapping. Objective: Implement a Python function `analyze_mmap_file(file_path: str, search_word: bytes, replace_word: bytes) -> tuple` that performs the following tasks: 1. Maps an existing file into memory. 2. Searches for the first occurrence of a given byte sequence (`search_word`) within the memory-mapped file. 3. Replaces the found byte sequence (`search_word`) with another byte sequence of the same length (`replace_word`). 4. Flushes the changes to ensure they write back to the disk. 5. Unmaps the file and closes any open file handles. 6. Returns a tuple containing: - The initial position of the `search_word` in the file (or `-1` if not found). - The content of the file after modification as a bytes object. Constraints: - The length of `search_word` and `replace_word` must be the same. - Handle any exceptions that may occur, such as file not found, permission issues, etc. - Ensure that the file operations (open, close, mmap, flush) are safely handled. Function Signature: ```python def analyze_mmap_file(file_path: str, search_word: bytes, replace_word: bytes) -> tuple: pass ``` Example: Suppose we have a file `test.txt` with the following content: ``` Hello Python! This is a test file for mmap. ``` - Calling `analyze_mmap_file(\'test.txt\', b\'Python\', b\'Coders\')` should return `(6, b\'Hello Coders!nThis is a test file for mmap.n\')`. # Notes: - Use proper exception handling to deal with any potential runtime errors. - Make use of mmap\'s context manager support to ensure resource management. Additional Information: Refer to the provided documentation of the `mmap` module for details on the methods and classes you can use to complete this task successfully.","solution":"import mmap def analyze_mmap_file(file_path: str, search_word: bytes, replace_word: bytes) -> tuple: Maps an existing file into memory, searches for the first occurrence of `search_word`, replaces it with `replace_word`, and returns the initial position and modified content. Args: file_path (str): The path of the file to map. search_word (bytes): The byte sequence to search for in the file. replace_word (bytes): The byte sequence to replace the found `search_word` with. Returns: tuple: (initial_position: int, modified_content: bytes) if len(search_word) != len(replace_word): raise ValueError(\\"search_word and replace_word must be of the same length\\") try: with open(file_path, \'r+b\') as f: # Open the file for reading and writing in binary mode with mmap.mmap(f.fileno(), 0) as mm: # Memory-map the file start_pos = mm.find(search_word) if start_pos == -1: # search_word was not found return -1, mm[:] # Replace the search_word with replace_word mm[start_pos: start_pos + len(search_word)] = replace_word mm.flush() # Ensure the changes are written back to the disk # Capture the result in a bytes object modified_content = mm[:] return start_pos, modified_content except FileNotFoundError: return -1, b\\"\\" except PermissionError: return -1, b\\"\\" except Exception as e: return -1, b\\"\\""},{"question":"Objective: To assess a student\'s understanding of text and binary I/O operations using Python\'s `io` module, including text encoding, buffering, and handling different I/O streams. Problem Statement: You are required to write a function `process_files(input_text_file: str, input_binary_file: str, output_text_file: str, output_binary_file: str, encoding=\'utf-8\') -> None` that performs the following tasks: 1. **Read the Contents of a Text File:** - Open an input text file (specified by `input_text_file`) using the given encoding and read its contents. If the encoding is not specified, use UTF-8 encoding. - Handle any encoding errors that may occur by replacing malformed data with a placeholder character. 2. **Read the Contents of a Binary File:** - Open an input binary file (specified by `input_binary_file`) and read its contents. 3. **Write to an Output Text File:** - Create or open an output text file (specified by `output_text_file`) using the same encoding as the input text file. - Write the contents read from the input text file to this file, translating all newline characters to the system\'s default line separator. 4. **Write to an Output Binary File:** - Create or open an output binary file (specified by `output_binary_file`). - Write the contents read from the input binary file to this file. Expected Function Signature: ```python def process_files(input_text_file: str, input_binary_file: str, output_text_file: str, output_binary_file: str, encoding=\'utf-8\') -> None: pass ``` Constraints: - Ensure proper handling of file descriptors to avoid resource leaks. - Assume all input files exist, and that you have write permissions to create or modify the output files. - Your function should not return any value, but errors should be handled gracefully using try-except blocks. - Ensure that file I/O operations are efficiently managed and avoid unnecessary buffering unless specified. Example Usage: ```python # Given the following files: # input_text_file.txt contains: \\"Hello, World!nThis is a test.n\\" # input_binary_file.jpg contains binary data of an image. process_files(\\"input_text_file.txt\\", \\"input_binary_file.jpg\\", \\"output_text_file.txt\\", \\"output_binary_file.jpg\\") # After the function execution: # output_text_file.txt will contain the same text \\"Hello, World!nThis is a test.n\\" but with system-defined newlines. # output_binary_file.jpg will contain the same binary data as input_binary_file.jpg. ``` Additional Notes: - Use `io.StringIO` and `io.BytesIO` in your implementation where needed. - Maintain good coding practices and provide inline comments explaining the logic.","solution":"def process_files(input_text_file: str, input_binary_file: str, output_text_file: str, output_binary_file: str, encoding=\'utf-8\') -> None: import io try: # Read content from the input text file with specified encoding. with open(input_text_file, \'r\', encoding=encoding, errors=\'replace\') as infile_text: text_content = infile_text.read() # Read content from the input binary file. with open(input_binary_file, \'rb\') as infile_binary: binary_content = infile_binary.read() # Write the text content to the output text file. with open(output_text_file, \'w\', encoding=encoding, newline=\'\') as outfile_text: outfile_text.write(text_content) # Write the binary content to the output binary file. with open(output_binary_file, \'wb\') as outfile_binary: outfile_binary.write(binary_content) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are given several lists of integers, each of which is already sorted in non-decreasing order. Your tasks are to create a priority queue to merge these lists into a single sorted list. Additionally, you need to implement a feature that dynamically handles the insertion of elements and removal of elements outside a given priority range. Implement the following functions: 1. `merge_sorted_lists(lists: List[List[int]]) -> List[int]`: Given a list of sorted lists, return a single merged sorted list using a heap. 2. `insert_and_remove_outside_range(heap: List[int], insert_element: int, lower_bound: int, upper_bound: int) -> List[int]`: Insert a new element into the heap and remove all elements outside the inclusive range `[lower_bound, upper_bound]`. The function should return the updated heap. Function Signatures ```python from typing import List import heapq def merge_sorted_lists(lists: List[List[int]]) -> List[int]: # Your code here def insert_and_remove_outside_range(heap: List[int], insert_element: int, lower_bound: int, upper_bound: int) -> List[int]: # Your code here ``` Constraints - The lengths of the individual lists are between 1 and 10^3. - The numbers in the lists and the insert elements are between `-10^6` and `10^6`. - Time complexity of `insert_and_remove_outside_range` should be efficient, aiming to maintain logarithmic operations wherever possible. # Examples ```python # Example for merge_sorted_lists: lists = [[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]] print(merge_sorted_lists(lists)) # Output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] # Example for insert_and_remove_outside_range: heap = [1, 2, 3, 4, 5] insert_element = 6 lower_bound = 2 upper_bound = 5 print(insert_and_remove_outside_range(heap, insert_element, lower_bound, upper_bound)) # Output: [2, 4, 3, 5] ``` **Note:** - Use `heapq` functions to maintain the heap properties and ensure efficient operations. - Be sure to handle cases where the heap might be empty or all elements are outside the range provided.","solution":"from typing import List import heapq def merge_sorted_lists(lists: List[List[int]]) -> List[int]: Merge multiple sorted lists into a single sorted list using a heap. Args: lists (List[List[int]]): A list of sorted lists. Returns: List[int]: A single merged sorted list. merged_list = [] heap = [] for i, lst in enumerate(lists): if lst: # important to check if the list is not empty heapq.heappush(heap, (lst[0], i, 0)) while heap: val, list_idx, element_idx = heapq.heappop(heap) merged_list.append(val) if element_idx + 1 < len(lists[list_idx]): next_val = lists[list_idx][element_idx + 1] heapq.heappush(heap, (next_val, list_idx, element_idx + 1)) return merged_list def insert_and_remove_outside_range(heap: List[int], insert_element: int, lower_bound: int, upper_bound: int) -> List[int]: Insert a new element into the heap and remove all elements outside the inclusive range [lower_bound, upper_bound]. Args: heap (List[int]): The heap to which an element is inserted. insert_element (int): The element to be inserted. lower_bound (int): The lower bound of the acceptable range. upper_bound (int): The upper bound of the acceptable range. Returns: List[int]: The updated heap. # Insert the new element heapq.heappush(heap, insert_element) # Maintain only elements within the specified range temp_heap = [] for element in heap: if lower_bound <= element <= upper_bound: heapq.heappush(temp_heap, element) return temp_heap"},{"question":"# Task You are to write a Python function that exports data stored in a DBM database to a JSON file. Your function should handle different types of DBM databases using the `dbm` module. Specifically, you must: 1. Open an existing DBM database file for reading. 2. Iterate through all keys and values in the database. 3. Store the data in a Python dictionary with string keys and values. 4. Write the dictionary to a specified JSON file. # Function Signature ```python import dbm import json def export_dbm_to_json(db_file: str, json_file: str) -> None: pass ``` # Input: - `db_file` (string): The path to the existing DBM database file. - `json_file` (string): The path to the output JSON file. # Output: - None (the function should write to the JSON file specified by `json_file`). # Constraints: - The function should handle cases where the database file is empty. - Ensure that the keys and values written to the JSON file are in the form of strings. - Handle exceptions for file operations gracefully, such as non-existing DBM files or read/write errors. # Example: ```python # Assuming \'example.db\' is a DBM database file with the following content: # {\'key1\': \'value1\', \'key2\': \'value2\'} export_dbm_to_json(\'example.db\', \'output.json\') # The \'output.json\' file should contain: # { # \\"key1\\": \\"value1\\", # \\"key2\\": \\"value2\\" # } ``` # Notes: - You can use the `dbm.open(filename, flag)` function to open the DBM file. - Iterate over the keys using methods like `firstkey()` and `nextkey(key)` for specific DBM variants. - Ensure the context manager properly closes the DBM database. # Hints: - Convert all keys and values from bytes to strings using `decode(\'utf-8\')`. - Use the `json` module to write the dictionary to the JSON file.","solution":"import dbm import json def export_dbm_to_json(db_file: str, json_file: str) -> None: Exports data from a DBM database file to a JSON file. :param db_file: Path to the existing DBM database file. :param json_file: Path to the output JSON file. try: with dbm.open(db_file, \'r\') as db: data = {key.decode(\'utf-8\'): db[key].decode(\'utf-8\') for key in db.keys()} with open(json_file, \'w\') as f: json.dump(data, f, indent=4) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Implement a function to parse an email message, extract specific details, and handle both simple and multipart email messages. Background You are given a block of data representing an email message. This data might conform to the MIME standards and could be either a simple plain text message or a multipart message containing various subparts (like text, HTML, attachments). Your task is to parse this email message and return the following details: 1. **Subject**: The subject of the email. 2. **From**: The sender of the email. 3. **To**: The recipient(s) of the email. 4. **Plain Text Content**: The plain text content if available. 5. **HTML Content**: The HTML content if available. 6. **Attachment Names** (if any): List of filenames of all attachments. Function Signature ```python from typing import List, Tuple from email.message import EmailMessage def parse_email(data: bytes) -> Tuple[str, str, List[str], str, str, List[str]]: pass ``` Input - `data` (bytes): The raw bytes of the email message. Output - A tuple containing: - `subject` (str): The subject of the email. - `from_addr` (str): The sender\'s email address. - `to_addrs` (List[str]): A list of recipient email addresses. - `plain_text_content` (str): The plain text content of the email. Return an empty string if none found. - `html_content` (str): The HTML content of the email. Return an empty string if none found. - `attachment_names` (List[str]): A list of filenames of all attachments. Return an empty list if none found. Constraints - Assume the `data` is a well-formed RFC 5322 compliant email message. - If any of the desired fields (like subject, plain text content, etc.) are not available in the email, return an appropriate empty value (e.g., empty string or list). - The email might have mixed line endings due to being transmitted over different systems. Example ```python email_data = b\\"From: user@example.comrnTo: friend@example.comrnSubject: Test EmailrnMIME-Version: 1.0rnContent-Type: text/plainrnrnThis is a test email.\\" subject, from_addr, to_addrs, plain_text_content, html_content, attachment_names = parse_email(email_data) assert subject == \\"Test Email\\" assert from_addr == \\"user@example.com\\" assert to_addrs == [\\"friend@example.com\\"] assert plain_text_content == \\"This is a test email.\\" assert html_content == \\"\\" assert attachment_names == [] ``` Implementation Notes - Utilize the `email.parser.BytesParser` class to parse the email data. - Iterate through the parts of a multipart email (if applicable) using methods such as `get_body()`, `iter_parts()`, or `walk()`. - Handle various types of email content (plain text, HTML) and identify attachments by checking the `Content-Disposition` header. This question tests the student\'s ability to work with complex email message structures and employ the `email.parser` utilities to extract necessary details, ensuring practical experience with email data handling in Python.","solution":"from typing import List, Tuple from email import policy from email.parser import BytesParser from email.message import EmailMessage def parse_email(data: bytes) -> Tuple[str, str, List[str], str, str, List[str]]: # Parse the email email = BytesParser(policy=policy.default).parsebytes(data) # Get the subject, from address, and to addresses subject = email.get(\'Subject\', \'\') from_addr = email.get(\'From\', \'\') to_addrs = email.get_all(\'To\', []) # Collect plain text content, HTML content, and attachment names plain_text_content = \\"\\" html_content = \\"\\" attachment_names = [] if email.is_multipart(): for part in email.iter_parts(): content_type = part.get_content_type() content_disposition = part.get(\\"Content-Disposition\\", \\"\\") if content_type == \\"text/plain\\" and \\"attachment\\" not in content_disposition: plain_text_content = part.get_payload(decode=True).decode(part.get_content_charset() or \\"utf-8\\") elif content_type == \\"text/html\\" and \\"attachment\\" not in content_disposition: html_content = part.get_payload(decode=True).decode(part.get_content_charset() or \\"utf-8\\") elif \\"attachment\\" in content_disposition: attachment_names.append(part.get_filename()) else: content_type = email.get_content_type() if content_type == \\"text/plain\\": plain_text_content = email.get_payload(decode=True).decode(email.get_content_charset() or \\"utf-8\\") elif content_type == \\"text/html\\": html_content = email.get_payload(decode=True).decode(email.get_content_charset() or \\"utf-8\\") return subject, from_addr, to_addrs, plain_text_content, html_content, attachment_names"},{"question":"**High-Level Overview:** You are tasked with implementing a function that: 1. Converts a given PyTorch tensor to a DLPack tensor using `to_dlpack`. 2. Converts the DLPack tensor back to a PyTorch tensor using `from_dlpack`. 3. Verifies if the reconstructed PyTorch tensor matches the original tensor. 4. Adds a specific value to both the original tensor and the reconstructed tensor and compares them for equality. **Function Signature:** ```python def tensor_conversion_and_comparison(tensor: torch.Tensor, value_to_add: float) -> bool: Converts a given PyTorch tensor to a DLPack tensor and back, then performs operations and compares the results. Arguments: tensor : torch.Tensor -- The input tensor to be converted. value_to_add : float -- The value to add to the tensors. Returns: bool -- True if the tensors are equal after conversion and addition, False otherwise. ``` **Input:** - `tensor` (torch.Tensor): A PyTorch tensor of any shape and datatype. - `value_to_add` (float): A floating-point number to add to the tensors. **Output:** - Returns `True` if both the original tensor and the reconstructed tensor, after adding `value_to_add`, contain identical values. - Returns `False` otherwise. **Constraints:** - Use the `torch.utils.dlpack.to_dlpack` and `torch.utils.dlpack.from_dlpack` functions for tensor conversion. - Ensure that the datatype and shape of the tensor remain unchanged during the process. **Example:** ```python import torch from torch.utils.dlpack import from_dlpack, to_dlpack # Example usage tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) value_to_add = 5.0 result = tensor_conversion_and_comparison(tensor, value_to_add) print(result) # Output should be True as the tensors should remain consistent after conversion ``` **Note:** You need to ensure that the tensor remains unchanged after conversion and any additional operations performed on it should yield identical results when carried out on both the original and reconstructed tensors.","solution":"import torch def tensor_conversion_and_comparison(tensor: torch.Tensor, value_to_add: float) -> bool: Converts a given PyTorch tensor to a DLPack tensor and back, then performs operations and compares the results. Arguments: tensor : torch.Tensor -- The input tensor to be converted. value_to_add : float -- The value to add to the tensors. Returns: bool -- True if the tensors are equal after conversion and addition, False otherwise. # Convert the PyTorch tensor to DLPack tensor dlpack_tensor = torch.utils.dlpack.to_dlpack(tensor) # Convert the DLPack tensor back to PyTorch tensor reconstructed_tensor = torch.utils.dlpack.from_dlpack(dlpack_tensor) # Add value_to_add to both tensors tensor_added = tensor + value_to_add reconstructed_tensor_added = reconstructed_tensor + value_to_add # Compare the tensors return torch.equal(tensor_added, reconstructed_tensor_added)"},{"question":"# Python Coding Assessment Question Objective: To assess a student\'s capability in understanding and utilizing the `__future__` module to work with future Python features, and their ability to manipulate class instances and version information. Question: You are required to write a function called `validate_feature` that checks whether a specified feature from the `__future__` module is available in a given Python version. The function should return `True` if the feature is available, and `False` otherwise. # Function Signature ```python def validate_feature(feature: str, version: tuple) -> bool: pass ``` # Input: 1. `feature` (str) - A string representing the name of the feature (e.g., \'division\', \'print_function\'). 2. `version` (tuple) - A 5-tuple specifying the Python version in the form `(PY_MAJOR_VERSION, PY_MINOR_VERSION, PY_MICRO_VERSION, PY_RELEASE_LEVEL, PY_RELEASE_SERIAL)`. # Output: - Return a boolean value: `True` if the feature is available in the specified version, `False` otherwise. # Constraints: 1. The version tuple and feature name will always match the format and features listed in the `__future__` module. 2. The feature name provided will always exist in the `__future__` module. 3. The comparison should consider the version number correctly, prioritizing major version, minor version, and so on. # Example: ```python # Test Case 1: print(validate_feature(\'print_function\', (3, 0, 0, \'alpha\', 1))) # Expected Output: False # Test Case 2: print(validate_feature(\'division\', (3, 0, 0, \'alpha\', 1))) # Expected Output: True # Test Case 3: print(validate_feature(\'nested_scopes\', (2, 2, 0, \'final\', 0))) # Expected Output: True ``` # Explanation: 1. In the first test case, \\"print_function\\" is only mandatory in Python 3.0 or later. Since the version provided is an early alpha, it is not applicable. 2. In the second test case, \\"division\\" became mandatory in version 3.0, so it is available in the version specified. 3. In the third test case, \\"nested_scopes\\" became mandatory in version 2.2, thus it is available as specified. # Notes: - Use the `__future__` module and inspect its contents programmatically to get feature details. - Extract `OptionalRelease` and `MandatoryRelease` from the `_Feature` instances. - Write thorough checks to ensure the validity of the Python version provided against the `MandatoryRelease`.","solution":"from __future__ import division, print_function, nested_scopes import sys def validate_feature(feature: str, version: tuple) -> bool: Checks whether a specified feature from the __future__ module is available in a given Python version. Parameters: - feature (str): The name of the feature (e.g., \'division\', \'print_function\'). - version (tuple): A 5-tuple specifying the Python version in the form `(PY_MAJOR_VERSION, PY_MINOR_VERSION, PY_MICRO_VERSION, PY_RELEASE_LEVEL, PY_RELEASE_SERIAL)`. Returns: - bool: True if the feature is available in the specified version, False otherwise. feature_dict = { \'division\': division, \'print_function\': print_function, \'nested_scopes\': nested_scopes } if feature not in feature_dict: return False # Get the mandatory release version for the feature mandatory_release = feature_dict[feature].mandatory return version >= mandatory_release"},{"question":"# Advanced Python Coding Assessment Objective: Create a custom descriptor in Python, `ValidatedAttribute`, which manages and validates the attribute access in a class. Your descriptor should be capable of dynamically validating the attribute based on provided criteria. Requirements: 1. **Descriptor Class: `ValidatedAttribute`** - Implement the `__get__`, `__set__`, and `__set_name__` methods in your descriptor class. - The `__set__` method should validate the value being assigned against provided validation criteria. - The descriptor should store a private attribute in the instance dictionary and only use this private attribute for the actual data storage. 2. **Validation Criteria:** - The `ValidatedAttribute` should accept the following validation criteria during initialization: - `expected_type`: Data type the attribute should be (e.g., `int`, `str`). - `min_value`: Minimum value for numeric attributes. - `max_value`: Maximum value for numeric attributes. - `min_size`: Minimum length for string attributes. - `max_size`: Maximum length for string attributes. - `allowed_values`: A set of allowed values (for enums or restricted values). 3. **Class Usage:** - Create a sample class `TestSubject` that uses the `ValidatedAttribute` descriptor for some of its attributes. - Demonstrate the usage and validation logic by creating instances of the `TestSubject` class and triggering validations and errors appropriately. Example: ```python class ValidatedAttribute: def __init__(self, expected_type=None, min_value=None, max_value=None, min_size=None, max_size=None, allowed_values=None): self.expected_type = expected_type self.min_value = min_value self.max_value = max_value self.min_size = min_size self.max_size = max_size self.allowed_values = allowed_values def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): # Implement validation logic here if self.expected_type and not isinstance(value, self.expected_type): raise TypeError(f\'Expected type {self.expected_type} for {self.private_name}\') if self.min_value is not None and value < self.min_value: raise ValueError(f\'Value {value} should be no less than {self.min_value}\') if self.max_value is not None and value > self.max_value: raise ValueError(f\'Value {value} should be no more than {self.max_value}\') if isinstance(value, str): if self.min_size is not None and len(value) < self.min_size: raise ValueError(f\'Length of {value} should be no less than {self.min_size}\') if self.max_size is not None and len(value) > self.max_size: raise ValueError(f\'Length of {value} should be no more than {self.max_size}\') if self.allowed_values and value not in self.allowed_values: raise ValueError(f\'Value {value} should be one of {self.allowed_values}\') setattr(obj, self.private_name, value) class TestSubject: age = ValidatedAttribute(expected_type=int, min_value=0, max_value=120) name = ValidatedAttribute(expected_type=str, min_size=2, max_size=50) kind = ValidatedAttribute(allowed_values={\'animal\', \'vegetable\', \'mineral\'}) def __init__(self, age, name, kind): self.age = age self.name = name self.kind = kind # Test the implementation try: t = TestSubject(25, \\"Alice\\", \\"animal\\") print(t.age) # Outputs: 25 t.age = -5 # Raises ValueError except ValueError as e: print(e) try: t.name = \\"A\\" # Raises ValueError except ValueError as e: print(e) try: t.kind = \\"rock\\" # Raises ValueError except ValueError as e: print(e) ``` Constraints: - Ensure that the descriptor handles validation correctly and raises appropriate exceptions. - Performance should be optimal, considering the possible validation checks.","solution":"class ValidatedAttribute: def __init__(self, expected_type=None, min_value=None, max_value=None, min_size=None, max_size=None, allowed_values=None): self.expected_type = expected_type self.min_value = min_value self.max_value = max_value self.min_size = min_size self.max_size = max_size self.allowed_values = allowed_values def __set_name__(self, owner, name): self.private_name = \'_\' + name def __get__(self, obj, objtype=None): return getattr(obj, self.private_name) def __set__(self, obj, value): if self.expected_type and not isinstance(value, self.expected_type): raise TypeError(f\'Expected type {self.expected_type.__name__} for {self.private_name}\') if self.min_value is not None and value < self.min_value: raise ValueError(f\'Value {value} should be no less than {self.min_value}\') if self.max_value is not None and value > self.max_value: raise ValueError(f\'Value {value} should be no more than {self.max_value}\') if isinstance(value, str): if self.min_size is not None and len(value) < self.min_size: raise ValueError(f\'Length of {value} should be no less than {self.min_size}\') if self.max_size is not None and len(value) > self.max_size: raise ValueError(f\'Length of {value} should be no more than {self.max_size}\') if self.allowed_values and value not in self.allowed_values: raise ValueError(f\'Value {value} should be one of {self.allowed_values}\') setattr(obj, self.private_name, value) class TestSubject: age = ValidatedAttribute(expected_type=int, min_value=0, max_value=120) name = ValidatedAttribute(expected_type=str, min_size=2, max_size=50) kind = ValidatedAttribute(allowed_values={\'animal\', \'vegetable\', \'mineral\'}) def __init__(self, age, name, kind): self.age = age self.name = name self.kind = kind"},{"question":"**Complex Number Manipulation Using C-Level Operations** In this coding challenge, you will implement a Python function utilizing the provided C API\'s complex numbers functionality. Your task is to write a Python function that performs and validates several operations on complex numbers. # Requirements 1. **Function Name**: `complex_operations` 2. **Input**: Two complex numbers, `z1` and `z2`, provided as Python\'s native complex number type. 3. **Output**: A dictionary with the results of various operations. Specifically: - Sum of `z1` and `z2` - Difference between `z1` and `z2` - Product of `z1` and `z2` - Quotient of `z1` divided by `z2` - Negation of `z1` - `z1` raised to the power of `z2` The operations should use the `Py_complex` representation and the corresponding functions described in the documentation. # Constraints - Ensure that the division operation handles cases where the divisor might be zero. - Utilize the provided C functions for each of the arithmetic operations listed above. - Validate your results by comparing them with Python\'s native complex number operations. # Example Usage ```python def complex_operations(z1, z2): # Your implementation here pass # Example complex numbers z1 = complex(3, 4) z2 = complex(1, -2) result = complex_operations(z1, z2) print(result) # Expected Output (format may vary): # { # \\"sum\\": (4+2j), # \\"difference\\": (2+6j), # \\"product\\": (11-2j), # \\"quotient\\": (-1.1-1.6j), # \\"negation\\": (-3-4j), # \\"power\\": (-0.4192+0.8886j) # } ``` # Performance Requirements Ensure that your implementation handles operations efficiently, and addresses edge cases as specified. # Additional Notes The conversion between Python\'s native complex type and `Py_complex` should be managed accurately. The Python C-API functions for managing complex numbers must be properly utilized. # Function Signature ```python def complex_operations(z1: complex, z2: complex) -> dict: # Your code here pass ``` **Hint**: You may need to use the `ctypes` or `cffi` library to interface with the underlying C API if it\'s directly callable from Python.","solution":"def complex_operations(z1, z2): Returns a dictionary with the results of various operations on z1 and z2. result = { \\"sum\\": z1 + z2, \\"difference\\": z1 - z2, \\"product\\": z1 * z2, \\"quotient\\": z1 / z2 if z2 != 0 else None, \\"negation\\": -z1, \\"power\\": z1 ** z2 } return result"},{"question":"You are tasked with implementing a simplified image format detector similar to the deprecated `imghdr` package. Your goal is to create a function that can determine the type of an image based on its header bytes. Requirements 1. Implement the function `detect_image_format(file_path)` that returns a string describing the image type. 2. The function should recognize the following image formats based on their header bytes: - `\'jpeg\'`: Files starting with bytes `b\'xffxd8xff\'` - `\'png\'`: Files starting with bytes `b\'x89PNG\'` - `\'gif\'`: Files starting with bytes `b\'GIF87a\'` or `b\'GIF89a\'` - `\'tiff\'`: Files starting with bytes `b\'II*x00\'` or `b\'MMx00*\'` - `\'bmp\'`: Files starting with bytes `b\'BM\'` - `\'webp\'`: Files starting with bytes `b\'RIFF\'` and containing `b\'WEBP\'` in the next 12 bytes. Input and Output - Input: `file_path` (a string representing the path to the image file) - Output: A string representing the image format or `None` if the format is not recognized. Example ```python def detect_image_format(file_path): # Implement this function based on the requirements pass # Example Usage: print(detect_image_format(\'path/to/image.jpeg\')) # Output: \'jpeg\' print(detect_image_format(\'path/to/image.png\')) # Output: \'png\' print(detect_image_format(\'path/to/image.txt\')) # Output: None ``` Constraints - The file size will not exceed 5MB. - The function should handle cases where the file does not exist or is not readable (return `None` in such cases). Notes - You are allowed to read the first 16 bytes (at most) of the file to determine its format. - You may not use the `imghdr` module in your implementation. - Consider edge cases such as empty files or files with less than 16 bytes. Performance - The function should efficiently handle multiple calls in a short span.","solution":"def detect_image_format(file_path): Detect the image format based on the header bytes of the image file. :param file_path: str, path to the image file :return: str, image format or None if unrecognized try: with open(file_path, \'rb\') as f: header = f.read(16) if header.startswith(b\'xffxd8xff\'): return \'jpeg\' elif header.startswith(b\'x89PNG\'): return \'png\' elif header.startswith(b\'GIF87a\') or header.startswith(b\'GIF89a\'): return \'gif\' elif header.startswith(b\'II*x00\') or header.startswith(b\'MMx00*\'): return \'tiff\' elif header.startswith(b\'BM\'): return \'bmp\' elif header.startswith(b\'RIFF\') and b\'WEBP\' in header[8:16]: return \'webp\' else: return None except (FileNotFoundError, IOError): return None"},{"question":"Objective: Your task is to create a Python class using the `asynchat` module to handle a simple custom protocol that echoes messages back to the client. Each message is terminated by a newline character (`\'n\'`). Instructions: 1. Create a class `EchoHandler` that is a subclass of `asynchat.async_chat`. 2. Implement the `__init__` method to: - Initialize the superclass. - Set the message terminator to `\'n\'`. - Initialize instance variables to store incoming data. 3. Implement the `collect_incoming_data` method to: - Collect incoming data and append it to an instance variable. 4. Implement the `found_terminator` method to: - Echo the received message back to the client (excluding the terminator). - Clear the buffer to prepare for the next message. Expected Input and Output: - Your `EchoHandler` class will be used with an `asyncore` server dispatcher to handle incoming client connections. - When a client sends a message (terminated by `\'n\'`), your handler should echo the same message back to the client. Constraints: - Use only the features from the `asyncore` and `asynchat` modules for socket handling. - You do not need to implement the server dispatcher itself; focus on the `EchoHandler` class. Example: Here is an example of how the class could be used: ```python import asyncore import asynchat import socket class EchoHandler(asynchat.async_chat): def __init__(self, sock): asynchat.async_chat.__init__(self, sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\\"\\".join(self.ibuffer).decode(\'utf-8\') self.push(message.encode(\'utf-8\') + b\'n\') self.ibuffer.clear() class EchoServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accept(self): pair = self.accept() if pair is not None: sock, addr = pair EchoHandler(sock) if __name__ == \\"__main__\\": server = EchoServer(\'localhost\', 8080) asyncore.loop() ``` Using this `EchoServer`, any client that connects and sends messages will receive the exact same messages back until the connection is closed. Implement the `EchoHandler` class as described above.","solution":"import asynchat import asyncore class EchoHandler(asynchat.async_chat): def __init__(self, sock): super().__init__(sock) self.set_terminator(b\'n\') self.ibuffer = [] def collect_incoming_data(self, data): self.ibuffer.append(data) def found_terminator(self): message = b\\"\\".join(self.ibuffer).decode(\'utf-8\') self.push(message.encode(\'utf-8\') + b\'n\') self.ibuffer.clear()"},{"question":"# Question: Implement a Robust Generator Function You are tasked with implementing a generator function in Python that generates an infinite sequence of Fibonacci numbers. Additionally, you will need to create a function to verify if a given Python object is a generator object. Part 1: Fibonacci Generator Implement a generator function `fibonacci_generator()` that yields an infinite sequence of Fibonacci numbers. The Fibonacci sequence starts with 0 and 1, and each subsequent number is the sum of the previous two numbers. ```python def fibonacci_generator(): Yields: int: the next Fibonacci number in the sequence. pass ``` Part 2: Generator Object Checker Implement a function `is_generator_object(obj)` that determines whether a given object is a generator object. This function should return `True` if the object is a generator and `False` otherwise. ```python def is_generator_object(obj): Args: obj: The object to check. Returns: bool: True if obj is a generator object, False otherwise. pass ``` Input/Output Examples - Example 1: ```python fib_gen = fibonacci_generator() print(next(fib_gen)) # Output: 0 print(next(fib_gen)) # Output: 1 print(next(fib_gen)) # Output: 1 print(next(fib_gen)) # Output: 2 print(is_generator_object(fib_gen)) # Output: True print(is_generator_object([1, 2, 3])) # Output: False ``` Constraints - You must use Python\'s `yield` keyword to implement the generator. - The checker function must handle any Python object type and return the correct result. Performance Requirements - The Fibonacci generator should be efficient and handle a large number of iterations without significant performance degradation.","solution":"def fibonacci_generator(): Yields: int: The next Fibonacci number in the sequence. a, b = 0, 1 while True: yield a a, b = b, a + b def is_generator_object(obj): Args: obj: The object to check. Returns: bool: True if obj is a generator object, False otherwise. return hasattr(obj, \'__iter__\') and hasattr(obj, \'__next__\')"},{"question":"# PyTorch and TorchScript Compatibility Objective In this task, you are required to write a PyTorch module that computes a specific operation on tensors and then correctly convert this module to a TorchScript compatible version. The task will evaluate your understanding of TorchScript\'s limitations and your ability to adapt your code accordingly. Instructions 1. **Define a PyTorch Module**: Implement a custom PyTorch module called `CustomOperation`, which performs a series of operations on an input tensor `x`: - Create a tensor `y` initialized to the same size as `x` with values drawn from a normal distribution. - Perform an element-wise addition of `x` and `y`. - Compute the matrix product of the resulting tensor with its transpose. - Return the normalized version of the resultant matrix, such that the maximum value in the matrix is 1. 2. **Convert to TorchScript**: Ensure that your `CustomOperation` module is compatible with TorchScript. Address any unsupported functions or operations and make any necessary modifications to ensure proper compilation. Constraints - You cannot use any manually unsupported modules, functions, or classes inside the TorchScript module. - You should handle any necessary changes to argument requirements (e.g., specify `dtype`, `layout`, `device` if required by TorchScript). Expected Input and Output Format - The input will be a single tensor `x` of shape `(N, M)` where `N` and `M` are positive integers. - The output will be a normalized tensor with shape `(N, N)` where the maximum value in the matrix is 1. Example Assume the input is: ```python x = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) ``` The output will be a tensor with shape `(2, 2)` and normalized such that the maximum value in the tensor is 1. Code Template Here is the provided code structure: ```python import torch import torch.nn as nn class CustomOperation(nn.Module): def __init__(self): super(CustomOperation, self).__init__() def forward(self, x): # Implement the required operations here pass # Example usage and TorchScript conversion if __name__ == \\"__main__\\": model = CustomOperation() example_input = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) output = model(example_input) print(\\"Output before TorchScript:\\", output) # Convert to TorchScript scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, \\"custom_operation_scripted.pt\\") print(\\"TorchScript model saved successfully.\\") ``` Complete the implementation of the `CustomOperation` module, ensuring that it works correctly in both standard PyTorch execution and TorchScript.","solution":"import torch import torch.nn as nn class CustomOperation(nn.Module): def __init__(self): super(CustomOperation, self).__init__() def forward(self, x): # Ensuring Tensor `x` is on the same device as `y` device = x.device y = torch.randn_like(x, device=device) # Performing element-wise addition result = x + y # Computing the matrix product with its transpose result = torch.matmul(result, result.t()) # Normalizing the resultant matrix max_val = torch.max(result) if max_val.item() != 0: result = result / max_val return result # Example usage and TorchScript conversion if __name__ == \\"__main__\\": model = CustomOperation() example_input = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) output = model(example_input) print(\\"Output before TorchScript:\\", output) # Convert to TorchScript scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, \\"custom_operation_scripted.pt\\") print(\\"TorchScript model saved successfully.\\")"},{"question":"You are tasked with implementing a fault-tolerant distributed training loop using PyTorch. The goal is to demonstrate your understanding of distributed multiprocessing and how to handle errors that occur within a distributed setting. You must utilize the provided `torch.distributed.elastic.multiprocessing.errors` module effectively to manage any failures that occur during the training process. Requirements: 1. Implement a distributed training loop that trains a simple neural network on a dummy dataset. 2. Use the `torch.distributed.elastic.multiprocessing.errors` module to handle any errors that arise from child processes. 3. Ensure that the training loop can recover from `ChildFailedError` and continue training. 4. Log any `ProcessFailure` for debugging purposes. # Function Signature ```python def distributed_training_loop(): pass ``` # Constraints 1. You can assume the availability of multiple GPUs. 2. Implement a simple linear model as the neural network. 3. The dummy dataset should be a random tensor. 4. Implement a mechanism to simulate random failures in the child processes to test your error handling. # Performance Requirements - The solution should be efficient and handle errors gracefully to maintain training stability. # Example Although this is a high-level design problem, here\'s a verbal example of expectations: - Your training loop should attempt to train a neural network using multiple processes. - If a child process fails (simulated using custom logic), the `ChildFailedError` should be caught, logged, and the process should be restarted without crashing the entire training session. Notes - Make sure to use relevant components like `record`, `ChildFailedError`, `ErrorHandler`, and `ProcessFailure` from the `torch.distributed.elastic.multiprocessing.errors` module. - Provide comments and documentation within your code to explain your approach.","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist import torch.multiprocessing as mp from torch.distributed.elastic.multiprocessing.errors import ( record, ChildFailedError, ErrorHandler, ProcessFailure ) import random import logging logging.basicConfig(level=logging.INFO) class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.linear = nn.Linear(10, 10) def forward(self, x): return self.linear(x) def train(rank, world_size, error_simulation_freq): # Initialize the process group dist.init_process_group(backend=\\"gloo\\", rank=rank, world_size=world_size) # Create model and move it to the appropriate device model = SimpleModel().to(rank) ddp_model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank]) # Create a dummy dataset dataset = torch.randn(100, 10).to(rank) target = torch.randn(100, 10).to(rank) dataloader = [(dataset, target)] # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) for epoch in range(5): # Run for a few epochs for data, target in dataloader: optimizer.zero_grad() output = ddp_model(data) loss = criterion(output, target) loss.backward() optimizer.step() # Simulate a random failure if random.random() < error_simulation_freq: raise RuntimeError(f\\"Simulated failure in rank {rank}, epoch {epoch}\\") def distributed_training_loop(): world_size = 4 processes = [] error_simulation_freq = 0.3 # 30% chance to simulate a failure # Use a custom error handler to deal with failures @record def run_process(rank, world_size, error_simulation_freq): try: train(rank, world_size, error_simulation_freq) except RuntimeError as e: logging.error(f\\"Rank {rank} failed with error: {e}\\") raise for rank in range(world_size): p = mp.Process(target=run_process, args=(rank, world_size, error_simulation_freq)) p.start() processes.append(p) for p in processes: p.join() if __name__ == \\"__main__\\": distributed_training_loop()"},{"question":"**Coding Assessment Question:** # Task Imagine you are preparing a PyTorch model for deployment using TorchScript. Given the constraints and unsupported constructs mentioned in the provided documentation, your task is to: 1. **Implement a simple neural network** using PyTorch. 2. **Convert this network to TorchScript** using `torch.jit.script`. 3. **Handle any unsupported functions or parameters**, ensuring the model can be successfully converted and used within TorchScript. # Neural Network Specifications - The network should be a simple feed-forward neural network (`FFNN`) with: - 3 fully connected layers (`nn.Linear`), each followed by a ReLU activation (`nn.ReLU`). - Use the Xavier initialization (`torch.nn.init.xavier_uniform_`) for the weights. # Input and Output - The network should take in an input tensor `x` of shape `(batch_size, input_dim)` and output a tensor of shape `(batch_size, num_classes)`. # Constraints 1. The input dimension (`input_dim`) and the number of classes (`num_classes`) are provided as inputs when initializing the network. 2. Ensure to handle any unsupported constructs by replacing or adjusting them as necessary for TorchScript compatibility. # Performance Requirements - The implementation should be efficient and leverage PyTorch\'s tensor operations effectively. # Example ```python import torch import torch.nn as nn import torch.nn.functional as F import torch.jit # Define your neural network class here class SimpleFFNN(nn.Module): def __init__(self, input_dim, num_classes): super(SimpleFFNN, self).__init__() self.fc1 = nn.Linear(input_dim, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, num_classes) # Initialize weights using Xavier initialization nn.init.xavier_uniform_(self.fc1.weight) nn.init.xavier_uniform_(self.fc2.weight) nn.init.xavier_uniform_(self.fc3.weight) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # Example usage input_dim = 10 num_classes = 3 model = SimpleFFNN(input_dim, num_classes) # Convert to TorchScript scripted_model = torch.jit.script(model) # Test with a sample input x = torch.randn(5, input_dim) output = scripted_model(x) print(output) ``` # Note: - If any issues arise due to unsupported constructs, document and explain the modifications made to ensure the model is compatible with TorchScript.","solution":"import torch import torch.nn as nn import torch.nn.functional as F import torch.jit class SimpleFFNN(nn.Module): def __init__(self, input_dim, num_classes): super(SimpleFFNN, self).__init__() self.fc1 = nn.Linear(input_dim, 128) self.fc2 = nn.Linear(128, 64) self.fc3 = nn.Linear(64, num_classes) # Initialize weights using Xavier initialization nn.init.xavier_uniform_(self.fc1.weight) nn.init.xavier_uniform_(self.fc2.weight) nn.init.xavier_uniform_(self.fc3.weight) def forward(self, x): x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # Example usage input_dim = 10 num_classes = 3 model = SimpleFFNN(input_dim, num_classes) # Convert to TorchScript scripted_model = torch.jit.script(model) # Test with a sample input x = torch.randn(5, input_dim) output = scripted_model(x) print(output)"},{"question":"**Question: Implementing Custom Cached Line Retrieval System** As a developer, you are tasked with creating a custom line retrieval system that mimics the functionality of the `linecache` module. Your implementation should handle caching, retrieving, and clearing lines from a specified text file. The cache should store lines accessed from the file to optimize repeated access to the same lines. Additionally, you should be able to validate the cache and clear it when necessary. Implement a class `CustomLineCache` with the following methods: 1. **`__init__`**: Initialize the cache and other necessary attributes. 2. **`getline(self, filename: str, lineno: int) -> str`**: Retrieve a specific line number from the given file. This method should cache lines for optimized access and return an empty string if the file or line number is invalid. 3. **`clearcache(self) -> None`**: Clear the entire cache. 4. **`checkcache(self, filename: str = None) -> None`**: Validate the cache entries and update them if the files have changed. If `filename` is specified, only check the cache for that file. 5. **`lazycache(self, filename: str) -> None`**: Prepare for later access to the file lines without doing I/O until the lines are actually requested. # Example Usage: ```python cache = CustomLineCache() print(cache.getline(\'example.txt\', 10)) # Get the 10th line from example.txt cache.clearcache() # Clear the cache cache.checkcache(\'example.txt\') # Validate and update cache for example.txt cache.lazycache(\'example.txt\') # Prepare for later access ``` # Input and Output: - **`getline(filename: str, lineno: int) -> str`**: - **Input**: - `filename` (str): The name of the file from which to retrieve lines. - `lineno` (int): The line number to retrieve. - **Output**: - Returns the specified line from the file as a string. - **`clearcache(self) -> None`**: - Clears the cache memory. - **`checkcache(self, filename: str = None) -> None`**: - Checks the validity of the cache. If no filename is provided, checks all entries. - **`lazycache(self, filename: str) -> None`**: - At first call, does not read the file but prepares to retrieve file lines later. # Constraints: - Assume file encodings are UTF-8. - Handle file not found, invalid line number, and other potential I/O errors gracefully, returning an empty string for `getline`. Your implementation will be evaluated based on correctness, efficiency, and handling of edge cases.","solution":"import os import time class CustomLineCache: def __init__(self): self.cache = {} self.mod_times = {} def getline(self, filename: str, lineno: int) -> str: self._ensure_file_cached(filename) lines = self.cache.get(filename) if lines is None or lineno <= 0 or lineno > len(lines): return \\"\\" return lines[lineno - 1] def clearcache(self) -> None: self.cache.clear() self.mod_times.clear() def checkcache(self, filename: str = None) -> None: if filename: self._validate_file(filename) else: for file in list(self.cache.keys()): self._validate_file(file) def lazycache(self, filename: str) -> None: if filename not in self.cache: self.cache[filename] = None def _validate_file(self, filename: str) -> None: if filename in self.cache: current_mod_time = os.path.getmtime(filename) if current_mod_time != self.mod_times.get(filename): self._cache_file(filename) def _ensure_file_cached(self, filename: str) -> None: if filename not in self.cache or self.cache[filename] is None: self._cache_file(filename) def _cache_file(self, filename: str) -> None: try: with open(filename, \'r\', encoding=\'utf-8\') as f: self.cache[filename] = f.readlines() self.mod_times[filename] = os.path.getmtime(filename) except (IOError, OSError): self.cache[filename] = []"},{"question":"**Objective**: To assess your understanding of abstract base classes (ABCs), abstract methods, and the usage of `ABCMeta` and `ABC` in creating a robust class hierarchy. Problem Statement You are required to design a system for a \\"Shape Drawer\\" application that mandates specific rules every shape subclass must follow. The base class `Shape` should enforce the following: 1. All shapes must compute their area using the `compute_area` method. 2. All shapes must implement a `draw` method that provides a textual representation of the shape. 3. The `draw` method should be decorated as an @abstractmethod. Additionally, you must create three shape subclasses (`Circle`, `Rectangle`, and `Square`), each with specific properties and implementations. Task 1. Define an abstract base class `Shape` using `ABC` or `ABCMeta` that includes: - An abstract method named `compute_area` which returns the area of the shape. - An abstract method named `draw` which prints a textual representation of the shape. 2. Implement the `Circle` class that conforms to the following: - It should take a radius as an initialization parameter. - The `compute_area` method should return the correct area of the circle. - The `draw` method should print the textual representation of the circle\'s radius. 3. Implement the `Rectangle` class that conforms to the following: - It should take width and height as initialization parameters. - The `compute_area` method should return the correct area of the rectangle. - The `draw` method should print the textual representation of the rectangle\'s dimensions. 4. Implement the `Square` class that inherits from `Rectangle` and conforms to the following: - It should take the side length as an initialization parameter. - Utilize the inheritance to manage the area computation. - The `draw` method should print the textual representation of the square\'s side length. Constraints - Use Python 3.10 or later. - Assume positive non-zero integer values for all dimensions (radius, width, height, and side length). Example Input/Output ```python # Example Usage shapes = [ Circle(5), Rectangle(2, 3), Square(4) ] for shape in shapes: print(shape.compute_area()) shape.draw() # Expected Output: # 78.5 # Circle with radius: 5 # 6 # Rectangle [width: 2, height: 3] # 16 # Square with side length: 4 ``` Make sure your implementation adheres to the principles of abstract base classes and ensures that the described methods cannot be bypassed. Submission Submit a single Python file containing all class definitions and the example usage.","solution":"from abc import ABC, abstractmethod from math import pi class Shape(ABC): @abstractmethod def compute_area(self): pass @abstractmethod def draw(self): pass class Circle(Shape): def __init__(self, radius): self.radius = radius def compute_area(self): return pi * self.radius ** 2 def draw(self): print(f\\"Circle with radius: {self.radius}\\") class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def compute_area(self): return self.width * self.height def draw(self): print(f\\"Rectangle [width: {self.width}, height: {self.height}]\\") class Square(Rectangle): def __init__(self, side): super().__init__(side, side) def draw(self): print(f\\"Square with side length: {self.width}\\") # Example Usage shapes = [ Circle(5), Rectangle(2, 3), Square(4) ] for shape in shapes: print(shape.compute_area()) shape.draw()"},{"question":"# Question Context You are given the task of developing a feature for an HTTP client library that validates HTTP responses and provides meaningful feedback based on the response status code. To achieve this, you will use the `http.HTTPStatus` enum class from the `http` package. This class provides a standardized way to work with HTTP status codes. Task 1. Write a function `get_http_status_info(code: int) -> str`: - This function takes an integer `code` representing an HTTP status code. - It returns a formatted string containing the status code, the reason phrase, and the description. - If the status code is not a valid HTTP status code, return the string `\\"Unknown Status Code\\"`. Example: ```python get_http_status_info(200) ``` Output: ``` \\"200 OK: Request fulfilled, document follows\\" ``` 2. Write a function `categorize_status_code(code: int) -> str`: - This function takes an integer `code` representing an HTTP status code. - It returns a string representing the category of the status code. - The possible categories are: - \\"Informational\\" for codes in the range 100-199 - \\"Successful\\" for codes in the range 200-299 - \\"Redirection\\" for codes in the range 300-399 - \\"Client Error\\" for codes in the range 400-499 - \\"Server Error\\" for codes in the range 500-599 - If the status code does not fall into any of these ranges, return the string `\\"Unknown Category\\"` Example: ```python categorize_status_code(404) ``` Output: ``` \\"Client Error\\" ``` Constraints - You must use the `http.HTTPStatus` enum class for status code validation and information retrieval. - You are not allowed to hardcode status codes, reason phrases, or descriptions directly; you must retrieve them using the `http.HTTPStatus` enum class. Performance Requirements - Your solution should be efficient, making a direct lookup or comparison rather than iterating over collections. Submission Format Submit your solution as a Python code defining the two functions detailed above. Example Solution ```python from http import HTTPStatus def get_http_status_info(code: int) -> str: try: status = HTTPStatus(code) return f\\"{status.value} {status.phrase}: {status.description}\\" except ValueError: return \\"Unknown Status Code\\" def categorize_status_code(code: int) -> str: if 100 <= code < 200: return \\"Informational\\" elif 200 <= code < 300: return \\"Successful\\" elif 300 <= code < 400: return \\"Redirection\\" elif 400 <= code < 500: return \\"Client Error\\" elif 500 <= code < 600: return \\"Server Error\\" else: return \\"Unknown Category\\" ```","solution":"from http import HTTPStatus def get_http_status_info(code: int) -> str: try: status = HTTPStatus(code) return f\\"{status.value} {status.phrase}: {status.description}\\" except ValueError: return \\"Unknown Status Code\\" def categorize_status_code(code: int) -> str: if 100 <= code < 200: return \\"Informational\\" elif 200 <= code < 300: return \\"Successful\\" elif 300 <= code < 400: return \\"Redirection\\" elif 400 <= code < 500: return \\"Client Error\\" elif 500 <= code < 600: return \\"Server Error\\" else: return \\"Unknown Category\\""},{"question":"# Coding Challenge: Implementing Custom Pairwise Distance Function Objective: Your task is to implement a custom function that calculates pairwise distances between samples using various distance metrics supported by `scikit-learn`. In addition to implementing standard metrics, you will also implement a custom metric and use it within the same framework. Requirements: 1. **Function Signature:** ```python def compute_pairwise_distances(X, Y=None, metric=\'euclidean\'): pass ``` 2. **Inputs:** - `X`: A 2D numpy array of shape `(n_samples_X, n_features)` representing the first set of samples. - `Y`: (optional) A 2D numpy array of shape `(n_samples_Y, n_features)` representing the second set of samples. If `None`, pairwise distances among `X` are calculated. - `metric`: A string specifying the metric to use for distance calculation. Valid metrics include: `euclidean`, `manhattan`, `cosine`, and `custom`. 3. **Outputs:** - A 2D numpy array of shape `(n_samples_X, n_samples_Y)` containing the pairwise distances. 4. **Constraints:** - You must use `sklearn.metrics.pairwise_distances` for `euclidean`, `manhattan`, and `cosine` metrics. - Implement a custom metric named `\'custom\'` where: ```math d_{custom}(a, b) = sqrt(sum((a - b)^2) / (1 + sum((a + b)^2))) ``` 5. **Performance:** - Ensure the function handles large datasets efficiently. Example: ```python import numpy as np X = np.array([[2, 3], [3, 5], [5, 8]]) Y = np.array([[1, 0], [2, 1]]) # Call function with \'euclidean\' metric print(compute_pairwise_distances(X, Y, metric=\'euclidean\')) # Expected Output: Similar to sklearn\'s pairwise_distances for euclidean # Call function with \'custom\' metric print(compute_pairwise_distances(X, Y, metric=\'custom\')) # Custom metric calculation based on given formula ``` Notes: - Your implementation should handle edge cases, such as empty inputs, invalid metric names, and different shapes of `X` and `Y`. - Provide appropriate documentation and comments within your code for clarity.","solution":"import numpy as np from sklearn.metrics import pairwise_distances def custom_distance(a, b): Custom distance metric defined as: d_custom(a, b) = sqrt(sum((a - b)^2) / (1 + sum((a + b)^2))) return np.sqrt(np.sum((a - b) ** 2) / (1 + np.sum((a + b) ** 2))) def compute_pairwise_distances(X, Y=None, metric=\'euclidean\'): Computes pairwise distances between the rows of array X and the rows of array Y. Parameters: - X: 2D numpy array of shape (n_samples_X, n_features) - Y: (optional) 2D numpy array of shape (n_samples_Y, n_features). If None, pairwise distances among X are calculated. - metric: a string specifying the metric to use for distance calculation. Valid metrics include: \'euclidean\', \'manhattan\', \'cosine\', and \'custom\'. Returns: - A 2D numpy array of shape (n_samples_X, n_samples_Y) containing the pairwise distances. if metric in [\'euclidean\', \'manhattan\', \'cosine\']: return pairwise_distances(X, Y, metric=metric) elif metric == \'custom\': if Y is None: Y = X dist_matrix = np.zeros((X.shape[0], Y.shape[0])) for i in range(X.shape[0]): for j in range(Y.shape[0]): dist_matrix[i, j] = custom_distance(X[i], Y[j]) return dist_matrix else: raise ValueError(f\\"Unknown metric: {metric}\\")"},{"question":"Objective You are required to implement a cookie handler using the `http.cookiejar` module. The handler will manage cookies for HTTP requests and responses, load cookies from a file, and apply specific policies. Task Implement a class `CustomCookieHandler` with the following functionalities: 1. **Initialization**: - Accept a filename (string) to load cookies from. - Accept a list of blocked domains (optional) that should not accept or return cookies. 2. **Methods**: - `load_cookies(self)`: Load cookies from the specified file. - `save_cookies(self, filename)`: Save current cookies to the specified file. - `add_cookies_to_request(self, request)`: Add cookies to the given HTTP request. - `extract_cookies_from_response(self, response, request)`: Extract cookies from the HTTP response and store them. - `clear_all_cookies(self)`: Clear all the stored cookies. - `get_cookie_names(self)`: Return a list of all cookie names currently stored. Constraints - **Filename**: The filename must be a string and the file should use the Mozilla \\"cookies.txt\\" format. - **Blocked Domains**: If specified, cookies for these domains should neither be accepted nor returned. Example Usage ```python from urllib.request import Request # Initialize handler with a file and blocked domains cookie_handler = CustomCookieHandler(\'cookies.txt\', blocked_domains=[\'.ads.net\', \'malicious.com\']) # Load cookies from file cookie_handler.load_cookies() # Create an HTTP request req = Request(\\"http://example.com\\") # Add cookies to the request cookie_handler.add_cookies_to_request(req) # Example HTTP response simulation class FakeResponse: def info(self): return { \\"Set-Cookie\\": \\"id=a3fWa; Domain=example.com; Path=/\\" } resp = FakeResponse() # Extract cookies from the response cookie_handler.extract_cookies_from_response(resp, req) # Save current cookies back to a file cookie_handler.save_cookies(\'new_cookies.txt\') # Get the list of current cookie names cookie_names = cookie_handler.get_cookie_names() # Clear all stored cookies cookie_handler.clear_all_cookies() ``` Evaluation Criteria - Correct implementation of class and methods. - Proper handling and management of cookie policies. - Efficient loading, saving, and manipulation of cookies. - Adherence to the given constraints and handling errors appropriately.","solution":"import http.cookiejar as cookiejar class CustomCookieHandler: def __init__(self, filename, blocked_domains=None): Initialize the CustomCookieHandler. :param filename: The filename to load cookies from. :param blocked_domains: A list of blocked domains. self.filename = filename self.blocked_domains = blocked_domains or [] self.cookie_jar = cookiejar.MozillaCookieJar() def load_cookies(self): Load cookies from the specified file. try: self.cookie_jar.load(self.filename, ignore_discard=True, ignore_expires=True) except FileNotFoundError: print(f\\"File \'{self.filename}\' not found. No cookies loaded.\\") def save_cookies(self, filename): Save current cookies to the specified file. self.cookie_jar.save(filename, ignore_discard=True, ignore_expires=True) def add_cookies_to_request(self, request): Add cookies to the given HTTP request. self.cookie_jar.add_cookie_header(request) def extract_cookies_from_response(self, response, request): Extract cookies from the HTTP response and store them. if self._is_response_valid(response): self.cookie_jar.extract_cookies(response, request) def clear_all_cookies(self): Clear all the stored cookies. self.cookie_jar.clear() def get_cookie_names(self): Return a list of all cookie names currently stored. return [cookie.name for cookie in self.cookie_jar] def _is_response_valid(self, response): Check if the response is valid and does not contain blocked domains. for cookie in response.info()[\\"Set-Cookie\\"].split(\',\'): if any(blocked_domain in cookie for blocked_domain in self.blocked_domains): return False return True"},{"question":"**Objective:** Demonstrate an understanding of the `__future__` module, its features, and how to programmatically inspect and use these future features in Python. **Question:** Write a Python function named `future_features_info` that inspects the `__future__` module and returns a comprehensive summary of all the features available in the module. The function should produce a dictionary where each key is a feature name (as a string) and the value is another dictionary with the following keys: - `optional_release`: The first Python release in which the feature was accepted (as a tuple). - `mandatory_release`: The Python release in which the feature becomes or became mandatory (as a tuple or `None` if not applicable). - `compiler_flag`: The compiler flag associated with the feature (as an integer). In addition, write another function named `use_future_feature` that demonstrates the usage of a specified future feature by importing it and executing a sample code to show its effect. **Function Signatures:** ```python def future_features_info() -> dict: pass def use_future_feature(feature_name: str) -> str: pass ``` **Constraints:** - You must use the `__future__` module to gather the required information. - The summary dictionary should not include any features that have been removed or are deprecated. - For the `use_future_feature` function, you must handle cases where the provided feature name does not exist in the `__future__` module. **Example Usage:** ```python info = future_features_info() # Example output: # { # \'nested_scopes\': { # \'optional_release\': (2, 1, 0, \'beta\', 1), # \'mandatory_release\': (2, 2, 0, \'final\', 0), # \'compiler_flag\': 8 # }, # \'generators\': { # \'optional_release\': (2, 2, 0, \'alpha\', 1), # \'mandatory_release\': (2, 3, 0, \'final\', 0), # \'compiler_flag\': 16 # }, # ... # } print(use_future_feature(\'division\')) # Output will demonstrate the behavior of true division in Python 2: # Example: # 2 / 3 = 0.6666666666666666 ``` **Notes:** - The `future_features_info` function should leverage the introspection capabilities of Python to inspect the `__future__` module and extract the required information. - The `use_future_feature` function should use `exec` or `compile` to dynamically demonstrate the future feature.","solution":"import __future__ def future_features_info() -> dict: Inspects the __future__ module and returns a summary of all available features. features = {} for feature_name in dir(__future__): feature = getattr(__future__, feature_name, None) if ( feature and hasattr(feature, \'getOptionalRelease\') and hasattr(feature, \'getMandatoryRelease\') and hasattr(feature, \'compiler_flag\') ): features[feature_name] = { \'optional_release\': feature.getOptionalRelease(), \'mandatory_release\': feature.getMandatoryRelease(), \'compiler_flag\': feature.compiler_flag } return features def use_future_feature(feature_name: str) -> str: Demonstrate the usage of a specified future feature by importing it and executing a sample code to show its effect. if feature_name in dir(__future__): feature = getattr(__future__, feature_name) if feature and hasattr(feature, \'compiler_flag\'): sample_code = f from __future__ import {feature_name} result = \'Feature {feature_name} is working!\' local_vars = {} exec(sample_code, {}, local_vars) return local_vars[\'result\'] else: return \'Feature does not have a compiler flag and cannot be imported.\' else: return \'Feature not found in __future__ module.\'"},{"question":"# Question: Advanced Command Line Tool with Audit Logging Objective: You are to implement an advanced command line utility that not only performs specific tasks based on user input but also logs these operations via auditing hooks to keep track of critical operations. Requirements: 1. **Command Line Arguments:** - Your script should accept a list of command-line arguments to perform certain tasks. The arguments are: - `create <file_name>`: Create a file with the specified name. - `delete <file_name>`: Delete the specified file. - `list`: List all files in the current directory. 2. **Auditing:** - Ensure that auditing hooks are called for each command to log the operation. You need to log the command name (`create`, `delete`, or `list`) and any associated file name. 3. **Implementation Details:** - Create a function named `audit_event(event, args)` to handle audit logging. - Implement the main function to parse command-line arguments and call corresponding functions. - Use the `sys.argv` to access command-line arguments. - Use `sys.audit()` or `sys.addaudithook()` to handle auditing. Constraints: - Ensure that if no or invalid command-line arguments are provided, an appropriate error message is printed. - Use proper exception handling where applicable. - Do not use any external libraries; rely on standard Python libraries. Example Usage: ```sh python3 your_script.py create myfile.txt # Should create a file named \'myfile.txt\' and log the operation. python3 your_script.py delete myfile.txt # Should delete the file named \'myfile.txt\' and log the operation. python3 your_script.py list # Should list all files in the current directory and log the operation. ``` Input/Output: - **Input:** Command line arguments as specified above. - **Output:** Execution of the command and appropriate logging of actions via audit hooks. Implementation: ```python import sys import os def audit_event(event, args): print(f\\"Audit log - Event: {event}, Arguments: {args}\\") def create_file(file_name): with open(file_name, \'w\') as f: f.write(\'\') sys.audit(\'command.create\', file_name) def delete_file(file_name): if os.path.exists(file_name): os.remove(file_name) sys.audit(\'command.delete\', file_name) else: print(f\\"File {file_name} does not exist.\\") def list_files(): files = os.listdir(\'.\') for file in files: print(file) sys.audit(\'command.list\', None) def main(): sys.addaudithook(audit_event) if len(sys.argv) < 2: print(\\"No command provided.\\") return command = sys.argv[1] if command == \'create\': if len(sys.argv) != 3: print(\\"Usage: create <file_name>\\") return create_file(sys.argv[2]) elif command == \'delete\': if len(sys.argv) != 3: print(\\"Usage: delete <file_name>\\") return delete_file(sys.argv[2]) elif command == \'list\': list_files() else: print(f\\"Unknown command: {command}\\") if __name__ == \\"__main__\\": main() ``` Ensure your code reads the command line arguments using `sys.argv`, processes commands to either create, delete, or list files, and uses `sys.audit()` and `sys.addaudithook()` for logging audit trails.","solution":"import sys import os def audit_event(event, args): print(f\\"Audit log - Event: {event}, Arguments: {args}\\") def create_file(file_name): with open(file_name, \'w\') as f: f.write(\'\') sys.audit(\'command.create\', file_name) def delete_file(file_name): if os.path.exists(file_name): os.remove(file_name) sys.audit(\'command.delete\', file_name) else: print(f\\"File {file_name} does not exist.\\") def list_files(): files = os.listdir(\'.\') for file in files: print(file) sys.audit(\'command.list\', None) def main(): sys.addaudithook(audit_event) if len(sys.argv) < 2: print(\\"No command provided.\\") return command = sys.argv[1] if command == \'create\': if len(sys.argv) != 3: print(\\"Usage: create <file_name>\\") return create_file(sys.argv[2]) elif command == \'delete\': if len(sys.argv) != 3: print(\\"Usage: delete <file_name>\\") return delete_file(sys.argv[2]) elif command == \'list\': list_files() else: print(f\\"Unknown command: {command}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are tasked with writing a Python function that processes a list of URLs and extracts specific components. Specifically, you need to create a function `process_urls(url_list)`, which takes a list of URLs and performs the following: 1. For each URL, parse it using `urlparse`. 2. Extract the scheme, netloc, path, and query components. 3. Validate the extracted components: - The scheme must be either \'http\' or \'https\'. - The netloc must not be empty. - The path must start with a \'/\' character. 4. Transform the query string into a dictionary using `parse_qs`. 5. Construct a new URL using `urlunparse`, but only include the following: - Scheme - Netloc - Path 6. Return a list of tuples where each tuple contains: - The original URL - The new constructed URL - The query dictionary # Function Signature: ```python from typing import List, Tuple, Dict def process_urls(url_list: List[str]) -> List[Tuple[str, str, Dict[str, List[str]]]]: pass ``` # Input: - `url_list`: A list of strings where each string is a URL. # Output: - A list of tuples. Each tuple contains: - The original URL (string) - The new constructed URL (string) - The query dictionary (dictionary with list of values) # Constraints: - All elements in `url_list` will be valid URL strings. - There will be at least one URL in `url_list`. - The function must correctly handle URLs with and without query strings. # Example: ```python url_list = [ \\"http://www.example.com/path/to/page?name=ferret&color=purple\\", \\"https://www.example.com/\\", \\"http://example.com/path?query=value\\" ] # Expected Output: # [ # (\\"http://www.example.com/path/to/page?name=ferret&color=purple\\", \\"http://www.example.com/path/to/page\\", {\\"name\\": [\\"ferret\\"], \\"color\\": [\\"purple\\"]}), # (\\"https://www.example.com/\\", \\"https://www.example.com/\\", {}), # (\\"http://example.com/path?query=value\\", \\"http://example.com/path\\", {\\"query\\": [\\"value\\"]}) # ] ``` # Additional Notes: - You should use `urllib.parse` methods for URL parsing and construction. - Ensure your function handles edge cases appropriately, such as when the path does not start with a \'/\' or when the scheme is not \'http\' or \'https\'. In such cases, you may skip processing that specific URL and not include it in the result. Good luck!","solution":"from typing import List, Tuple, Dict from urllib.parse import urlparse, parse_qs, urlunparse def process_urls(url_list: List[str]) -> List[Tuple[str, str, Dict[str, List[str]]]]: results = [] for url in url_list: parsed_url = urlparse(url) scheme = parsed_url.scheme netloc = parsed_url.netloc path = parsed_url.path query = parsed_url.query # Check if the URL meets the required validation conditions if scheme in [\'http\', \'https\'] and netloc and path.startswith(\'/\'): query_dict = parse_qs(query) new_url = urlunparse((scheme, netloc, path, \'\', \'\', \'\')) results.append((url, new_url, query_dict)) return results"},{"question":"# Complex Number Operations using Py_complex Struct Your task is to implement a function that takes four parameters: two complex numbers, each represented by a tuple of two floats (real and imaginary parts), and a string specifying the operation to perform. The function should return the result of the specified operation as a tuple of two floats (real and imaginary parts). You are provided with the following functions to perform arithmetic operations on complex numbers, represented by the Py_complex struct: - `_Py_c_sum(left, right)`: Return the sum of two complex numbers. - `_Py_c_diff(left, right)`: Return the difference between two complex numbers. - `_Py_c_neg(num)`: Return the negation of a complex number. - `_Py_c_prod(left, right)`: Return the product of two complex numbers. - `_Py_c_quot(dividend, divisor)`: Return the quotient of two complex numbers. - `_Py_c_pow(num, exp)`: Return the exponentiation of one complex number by another. Your function should handle the following operations as specified by the `operation` parameter: - \\"add\\": Addition of two complex numbers. - \\"subtract\\": Subtraction of two complex numbers. - \\"negate\\": Negation of the first complex number. - \\"multiply\\": Multiplication of two complex numbers. - \\"divide\\": Division of the first complex number by the second. - \\"power\\": Exponentiation of the first complex number by the second. Input - `complex1`: A tuple of two floats representing the first complex number (real, imaginary). - `complex2`: A tuple of two floats representing the second complex number (real, imaginary). - `operation`: A string specifying the operation to perform. One of {\\"add\\", \\"subtract\\", \\"negate\\", \\"multiply\\", \\"divide\\", \\"power\\"}. Output - A tuple of two floats representing the result of the operation (real, imaginary). Constraints 1. The `operation` parameter is always a valid string from the specified set. 2. Both `complex1` and `complex2` tuples always contain two floats. 3. Division by zero must be handled. In such cases, return `(0.0, 0.0)` and set an appropriate error message as a string. Function Signature ```python def complex_operation(complex1: tuple, complex2: tuple, operation: str) -> tuple: pass ``` Example ```python # Example 1 result = complex_operation((1.0, 2.0), (3.0, 4.0), \\"add\\") print(result) # Output: (4.0, 6.0) # Example 2 result = complex_operation((1.0, 2.0), (3.0, 4.0), \\"subtract\\") print(result) # Output: (-2.0, -2.0) # Example 3 result = complex_operation((1.0, 2.0), (0.0, 0.0), \\"negate\\") print(result) # Output: (-1.0, -2.0) # Example 4 result = complex_operation((1.0, 2.0), (3.0, 4.0), \\"multiply\\") print(result) # Output: (-5.0, 10.0) # Example 5 result = complex_operation((1.0, 2.0), (3.0, 4.0), \\"divide\\") print(result) # Output: (0.44, 0.08) # Example 6 result = complex_operation((1.0, 2.0), (3.0, 4.0), \\"power\\") print(result) # Output: (real value, imaginary value) ``` Note - You must assume that the functions `_Py_c_sum`, `_Py_c_diff`, `_Py_c_neg`, `_Py_c_prod`, `_Py_c_quot`, and `_Py_c_pow` are available for you to use, and they work as described in the provided documentation. - Handle the division by zero case by returning `(0.0, 0.0)` and setting an appropriate error message which is represented by a string `\\"division by zero\\"`.","solution":"class Py_complex: def __init__(self, real, imag): self.real = real self.imag = imag def _Py_c_sum(left, right): return Py_complex(left.real + right.real, left.imag + right.imag) def _Py_c_diff(left, right): return Py_complex(left.real - right.real, left.imag - right.imag) def _Py_c_neg(num): return Py_complex(-num.real, -num.imag) def _Py_c_prod(left, right): return Py_complex(left.real * right.real - left.imag * right.imag, left.real * right.imag + left.imag * right.real) def _Py_c_quot(dividend, divisor): if divisor.real == 0 and divisor.imag == 0: raise ZeroDivisionError(\\"division by zero\\") denom = divisor.real * divisor.real + divisor.imag * divisor.imag return Py_complex((dividend.real * divisor.real + dividend.imag * divisor.imag) / denom, (dividend.imag * divisor.real - dividend.real * divisor.imag) / denom) def _Py_c_pow(num, exp): import cmath c_num = complex(num.real, num.imag) c_exp = complex(exp.real, exp.imag) result = c_num ** c_exp return Py_complex(result.real, result.imag) def complex_operation(complex1: tuple, complex2: tuple, operation: str) -> tuple: num1 = Py_complex(complex1[0], complex1[1]) num2 = Py_complex(complex2[0], complex2[1]) if operation == \\"add\\": result = _Py_c_sum(num1, num2) elif operation == \\"subtract\\": result = _Py_c_diff(num1, num2) elif operation == \\"negate\\": result = _Py_c_neg(num1) elif operation == \\"multiply\\": result = _Py_c_prod(num1, num2) elif operation == \\"divide\\": try: result = _Py_c_quot(num1, num2) except ZeroDivisionError: return (0.0, 0.0) elif operation == \\"power\\": result = _Py_c_pow(num1, num2) return (result.real, result.imag)"},{"question":"# Password Hashing and Verification with crypt Module **Background:** In Unix-like systems, passwords are often stored as hashes rather than in plain text for security purposes. The `crypt` module in Python provides an interface to the Unix *crypt(3)* routine to hash passwords. **Objective:** Create a Python class `PasswordManager` that provides: 1. Methods to hash passwords using specified hashing methods. 2. Methods to verify passwords against stored hashes. **Requirements:** 1. Implement the class `PasswordManager` with the following methods: - `hash_password(plaintext: str, method: str, rounds: int = None) -> str`: Hashes the given plaintext password using the specified method. If rounds are provided, use the specified number of rounds. The method can be one of \'SHA512\', \'SHA256\', \'BLOWFISH\', \'MD5\', or \'CRYPT\'. If the method is not recognized, raise a `ValueError`. - `verify_password(plaintext: str, hashed: str) -> bool`: Verifies the plaintext password against the hashed password. Returns `True` if they match, `False` otherwise. 2. Ensure that the `hash_password` method defaults to the strongest available method if the method is not specified. 3. Use `hmac.compare_digest` for constant-time comparison when verifying passwords to mitigate timing attacks. 4. Add appropriate error handling for invalid inputs. **Input and Output Formats:** - The `hash_password` method should accept a plaintext password (string), a hashing method (string), and an optional number of rounds (integer). It should return the hashed password (string). - The `verify_password` method should accept a plaintext password (string) and a hashed password (string). It should return a boolean indicating whether the passwords match. **Constraints:** - The `plaintext` password should be a non-empty string. - The `method` should be one of the supported hashing methods or raise a `ValueError` if invalid. - The `rounds` parameter, when applicable, should be within the valid range specified in the documentation for each method. **Example Usage:** ```python # Example usage of PasswordManager class pm = PasswordManager() # Hashing password hashed_pw = pm.hash_password(\\"securepassword\\", method=\\"SHA512\\", rounds=10000) print(hashed_pw) # Output: (hashed password string) # Verifying password is_valid = pm.verify_password(\\"securepassword\\", hashed_pw) print(is_valid) # Output: True # Invalid method try: pm.hash_password(\\"securepassword\\", method=\\"UNKNOWN\\") except ValueError as e: print(e) # Output: Invalid hashing method: UNKNOWN ``` You are expected to implement the `PasswordManager` class and ensure all methods function as specified. **Note:** - Use the `crypt` and `hmac` modules as described in the provided documentation.","solution":"import crypt import hmac class PasswordManager: def hash_password(self, plaintext: str, method: str = \'SHA512\', rounds: int = None) -> str: Hashes the given plaintext password using the specified method. If rounds are provided, use the specified number of rounds. The method can be one of \'SHA512\', \'SHA256\', \'BLOWFISH\', \'MD5\', or \'CRYPT\'. If the method is not recognized, a ValueError is raised. if not plaintext: raise ValueError(\\"Password cannot be empty.\\") methods = { \'SHA512\': crypt.METHOD_SHA512, \'SHA256\': crypt.METHOD_SHA256, \'BLOWFISH\': crypt.METHOD_BLOWFISH, \'MD5\': crypt.METHOD_MD5, \'CRYPT\': crypt.METHOD_CRYPT, } if method not in methods: raise ValueError(f\\"Invalid hashing method: {method}\\") crypt_method = methods[method] if rounds: salt = crypt.mksalt(crypt_method, rounds) else: salt = crypt.mksalt(crypt_method) hashed_password = crypt.crypt(plaintext, salt) return hashed_password def verify_password(self, plaintext: str, hashed: str) -> bool: Verifies the plaintext password against the hashed password. Returns True if they match, False otherwise. # Hash the plaintext with the same salt and algorithm used in the hashed password. rehashed = crypt.crypt(plaintext, hashed) return hmac.compare_digest(rehashed, hashed)"},{"question":"# Coding Assessment: Advanced PyTorch Serialization **Objective**: Evaluate your understanding of serialization in PyTorch, focusing on saving/loading tensor data and module states, and ensuring view relationships are preserved or minimized as needed. # Problem Statement You need to design a custom PyTorch module and implement functionality to save and load its state using the best practices for minimizing file sizes and preserving view relationships. Additionally, ensure that the storage sharing is correctly managed. # Requirements 1. **Custom PyTorch Module**: Create a neural network module with at least one `Linear` layer and any other layer of your choice (e.g., `Conv2d`, `BatchNorm`). 2. **Save and Load Functions**: - Implement a function `save_model_state` that saves the module\'s state dict to a file. - Implement a function `load_model_state` that loads the module\'s state dict from the file and restores it into a new instance of your module. 3. **Tensor Views Example**: - Create tensors with view relationships (e.g., extracting a subset of elements from a larger tensor). - Implement a function `save_tensors_with_views` that saves these tensors to a file, ensuring the view relationships are preserved. - Implement a function `load_tensors_with_views` that loads the tensors from the file, modifies one of the tensors, and demonstrates that the view relationship is maintained. 4. **Efficiency Considerations**: - If saving tensors with larger underlying storage, ensure to handle cases to minimize file size. # Functions to Implement ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.linear1 = nn.Linear(10, 5) self.bn1 = nn.BatchNorm1d(5) def forward(self, x): x = self.linear1(x) x = self.bn1(x) return F.relu(x) def save_model_state(model, filepath): Save the state dictionary of the model to the given file path. Args: - model (CustomModel): The instance of the model to be saved. - filepath (str): The path where the state dict will be saved. # Your code here def load_model_state(filepath): Load the state dictionary from the given file path and return a model with this state. Args: - filepath (str): The path from where the state dict will be loaded. Returns: - model (CustomModel): The model instance with loaded state. # Your code here def save_tensors_with_views(filepath): Create tensors with view relationships and save them to the given file path. Args: - filepath (str): The path where the tensors will be saved. # Your code here def load_tensors_with_views(filepath): Load tensors from the given file path, modify one tensor, and ensure view relationships are maintained. Args: - filepath (str): The path from where the tensors will be loaded. # Your code here ``` # Example Execution ```python # Example usage model = CustomModel() save_model_state(model, \'custom_model_state.pt\') loaded_model = load_model_state(\'custom_model_state.pt\') save_tensors_with_views(\'tensor_views.pt\') load_tensors_with_views(\'tensor_views.pt\') ``` # Constraints - Your implementations should handle edge cases and be efficient in terms of file storage where possible. - You may assume the file paths are always valid and writable. # Evaluation Criteria - Correctness: The functions should perform the required operations correctly. - Efficiency: Storage size should be minimized where possible without breaking functionality. - Clarity: The code should be well-organized and documented.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.linear1 = nn.Linear(10, 5) self.bn1 = nn.BatchNorm1d(5) def forward(self, x): x = self.linear1(x) x = self.bn1(x) return F.relu(x) def save_model_state(model, filepath): Save the state dictionary of the model to the given file path. Args: - model (CustomModel): The instance of the model to be saved. - filepath (str): The path where the state dict will be saved. torch.save(model.state_dict(), filepath) def load_model_state(filepath): Load the state dictionary from the given file path and return a model with this state. Args: - filepath (str): The path from where the state dict will be loaded. Returns: - model (CustomModel): The model instance with loaded state. model = CustomModel() model.load_state_dict(torch.load(filepath)) return model def save_tensors_with_views(filepath): Create tensors with view relationships and save them to the given file path. Args: - filepath (str): The path where the tensors will be saved. tensor = torch.arange(16).reshape(4, 4) view_tensor = tensor[1:3, 1:3] torch.save({\'tensor\': tensor, \'view_tensor\': view_tensor}, filepath) def load_tensors_with_views(filepath): Load tensors from the given file path, modify one tensor, and ensure view relationships are maintained. Args: - filepath (str): The path from where the tensors will be loaded. data = torch.load(filepath) tensor = data[\'tensor\'] view_tensor = data[\'view_tensor\'] # Modify the view tensor view_tensor[0, 0] = -1 # Confirm the change is reflected in the original tensor assert tensor[1, 1] == -1, \\"View relationship not maintained\\" return tensor, view_tensor"},{"question":"# Asyncio Task and Queue Management with Timeout Handling You are required to implement an asyncio program to manage the processing of a list of tasks using an asyncio queue. The program should simulate a scenario where multiple tasks are processed concurrently by worker coroutines, handling timeouts and cancellations gracefully. Requirements 1. Create a task queue and populate it with a list of 10 coroutines, each simulating a task by sleeping for a random duration between 1 to 5 seconds. 2. Implement a worker coroutine that: - Continuously fetches tasks from the queue. - Processes each task with a timeout of 3 seconds. - If the task exceeds the timeout, it should be cancelled, and appropriate handling should be performed (print a message indicating timeout). - If the task completes successfully, print a message including the task result. 3. Run the workers concurrently to process tasks from the queue. Constraints - You should use at least 3 worker coroutines. - The task queue should be empty before the program terminates. - Handle any exceptions that might occur during the processing of tasks. Input No input required. Output Print statements indicating the result of each task or a timeout message. Example Output ``` Task 1 completed successfully. Task 2 cancelled: timed out. Task 3 completed successfully. ... ``` Additional Constraints - Use `asyncio` functions like `run`, `create_task`, `wait_for`, and `Queue`. Hints - Use `asyncio.sleep` to simulate the task processing time. - Use `random.randint` from the `random` module to generate random sleep durations.","solution":"import asyncio import random async def task(id, duration): Simulate a task with the given id and duration. await asyncio.sleep(duration) return f\\"Task {id} completed in {duration} seconds.\\" async def worker(name, queue): Worker coroutine to process tasks from the queue with a timeout. while True: task_id, task_coro = await queue.get() try: result = await asyncio.wait_for(task_coro, timeout=3.0) print(result) except asyncio.TimeoutError: print(f\\"Task {task_id} cancelled: timed out.\\") finally: queue.task_done() async def main(): queue = asyncio.Queue() # Create 10 tasks with random sleep durations between 1 and 5 seconds tasks = [(i, task(i, random.randint(1, 5))) for i in range(1, 11)] for task_item in tasks: queue.put_nowait(task_item) # Create 3 worker coroutines workers = [asyncio.create_task(worker(f\\"Worker-{i}\\", queue)) for i in range(3)] # Wait until the queue is fully processed await queue.join() # Cancel the worker tasks for w in workers: w.cancel() # Ensure all worker tasks are cancelled await asyncio.gather(*workers, return_exceptions=True) # Run the main function asyncio.run(main())"},{"question":"**Question: Implementing and Evaluating Outlier Detection Algorithms in Sklearn** **Problem Statement:** You are given a dataset `X_train` with `n` observations and `p` features representing normal data, and a test dataset `X_test` with unseen observations. Your task is to implement three different outlier detection algorithms provided by sklearn: `Isolation Forest`, `Elliptic Envelope`, and `Local Outlier Factor`. You will train these algorithms on `X_train` and use them to predict outliers in `X_test`. Finally, you will compare the performance of these algorithms using specific metrics. **Requirements:** 1. **Data Preparation:** - The input datasets `X_train` and `X_test` will be provided as numpy arrays. - Assume `X_train` is clean and does not contain outliers. 2. **Model Implementation:** - Implement outlier detection using `IsolationForest`. - Implement outlier detection using `EllipticEnvelope`. - Implement outlier detection using `LocalOutlierFactor` with the `novelty` parameter set to `True`. 3. **Prediction and Evaluation:** - Use the `predict` method for each model to classify `X_test` observations as inliers (1) or outliers (-1). - For each implemented algorithm, calculate and print the following metrics: a. The number of detected outliers in `X_test`. b. The mean `decision_function` score (where applicable). c. For `LocalOutlierFactor`, also provide the mean of the `negative_outlier_factor_` for the training samples. 4. **Constraints:** - Ensure the code can handle datasets with up to 1000 observations and 20 features. - The solution should be efficient, aiming for a time complexity close to linear in the number of samples for each algorithm. **Expected Input:** - `X_train` : numpy array of shape (n_samples_train, n_features) - `X_test` : numpy array of shape (n_samples_test, n_features) **Expected Output:** - Number of detected outliers in `X_test` for each algorithm. - Mean decision function score for each algorithm (where applicable). - Mean negative outlier factor for `LocalOutlierFactor`. **Sample Code Template:** ```python import numpy as np from sklearn.ensemble import IsolationForest from sklearn.covariance import EllipticEnvelope from sklearn.neighbors import LocalOutlierFactor def outlier_detection(X_train, X_test): # Isolation Forest iso_forest = IsolationForest(contamination=0.1, random_state=42) iso_forest.fit(X_train) iso_preds = iso_forest.predict(X_test) iso_outliers = np.sum(iso_preds == -1) iso_decision_scores = np.mean(iso_forest.decision_function(X_test)) # Elliptic Envelope elliptic_env = EllipticEnvelope(contamination=0.1, random_state=42) elliptic_env.fit(X_train) elliptic_preds = elliptic_env.predict(X_test) elliptic_outliers = np.sum(elliptic_preds == -1) elliptic_decision_scores = np.mean(elliptic_env.decision_function(X_test)) # Local Outlier Factor (novelty detection) lof = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1) lof.fit(X_train) lof_preds = lof.predict(X_test) lof_outliers = np.sum(lof_preds == -1) lof_decision_scores = np.mean(lof.decision_function(X_test)) lof_negative_outlier_factor_train = np.mean(lof.negative_outlier_factor_) return { \\"IsolationForest\\": { \\"outliers\\": iso_outliers, \\"mean_decision_function\\": iso_decision_scores }, \\"EllipticEnvelope\\": { \\"outliers\\": elliptic_outliers, \\"mean_decision_function\\": elliptic_decision_scores }, \\"LocalOutlierFactor\\": { \\"outliers\\": lof_outliers, \\"mean_decision_function\\": lof_decision_scores, \\"mean_negative_outlier_factor_train\\": lof_negative_outlier_factor_train } } # Example usage # X_train = np.random.rand(100, 5) # X_test = np.random.rand(20, 5) # results = outlier_detection(X_train, X_test) # print(results) ``` **Explanation:** - This template shows how to initialize and fit each outlier detection algorithm. - Predictions and decision scores are calculated for the test set. - The number of detected outliers and relevant scores are computed and returned. Complete the function `outlier_detection` using the guidelines provided. Test and verify your implementation with different datasets to ensure correctness.","solution":"import numpy as np from sklearn.ensemble import IsolationForest from sklearn.covariance import EllipticEnvelope from sklearn.neighbors import LocalOutlierFactor def outlier_detection(X_train, X_test): # Isolation Forest iso_forest = IsolationForest(contamination=0.1, random_state=42) iso_forest.fit(X_train) iso_preds = iso_forest.predict(X_test) iso_outliers = np.sum(iso_preds == -1) iso_decision_scores = np.mean(iso_forest.decision_function(X_test) if hasattr(iso_forest, \'decision_function\') else []) # Elliptic Envelope elliptic_env = EllipticEnvelope(contamination=0.1, random_state=42) elliptic_env.fit(X_train) elliptic_preds = elliptic_env.predict(X_test) elliptic_outliers = np.sum(elliptic_preds == -1) elliptic_decision_scores = np.mean(elliptic_env.decision_function(X_test) if hasattr(elliptic_env, \'decision_function\') else []) # Local Outlier Factor (novelty detection) lof = LocalOutlierFactor(n_neighbors=20, novelty=True, contamination=0.1) lof.fit(X_train) lof_preds = lof.predict(X_test) lof_outliers = np.sum(lof_preds == -1) lof_decision_scores = np.mean(lof.decision_function(X_test) if hasattr(lof, \'decision_function\') else []) lof_negative_outlier_factor_train = np.mean(lof.negative_outlier_factor_ if hasattr(lof, \'negative_outlier_factor_\') else []) return { \\"IsolationForest\\": { \\"outliers\\": iso_outliers, \\"mean_decision_function\\": iso_decision_scores }, \\"EllipticEnvelope\\": { \\"outliers\\": elliptic_outliers, \\"mean_decision_function\\": elliptic_decision_scores }, \\"LocalOutlierFactor\\": { \\"outliers\\": lof_outliers, \\"mean_decision_function\\": lof_decision_scores, \\"mean_negative_outlier_factor_train\\": lof_negative_outlier_factor_train } }"},{"question":"Objective: You are tasked with evaluating a model for a classification problem. Your goal is to: 1. Plot the validation curve for a given estimator and a specified hyperparameter range. 2. Plot the learning curve for the same estimator to determine if adding more training data would help improve model performance. Task: 1. Load the Iris dataset and use an SVM classifier with a linear kernel. 2. Plot the validation curve for the `C` hyperparameter using a range of values from `1e-3` to `1e3`. 3. Plot the learning curve with training sizes of `[30, 50, 70, 90, 110]` and 5-fold cross-validation. Specifications: 1. **Input Format**: - No input as the dataset is predefined (Iris dataset). 2. **Output Format**: - Two plots: One for the validation curve and one for the learning curve. 3. **Constraints**: - Use `matplotlib` for plotting. - Use `sklearn` functions for generating validation and learning curves. 4. **Performance Requirements**: - Ensure that the code runs efficiently and produces plots within reasonable time limits. Instructions: 1. Import necessary libraries. 2. Shuffle and split the Iris dataset into features `X` and labels `y`. 3. Plot the validation curve for the `C` parameter of the SVM estimator. 4. Plot the learning curve for the SVM estimator. Example: ```python import numpy as np from sklearn.model_selection import validation_curve, learning_curve from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle the data X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Validation Curve param_range = np.logspace(-3, 3, 7) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.legend() plt.title(\'Validation Curve for SVM\') plt.show() # Learning Curve train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[30, 50, 70, 90, 110], cv=5 ) plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Training Sizes\') plt.ylabel(\'Score\') plt.legend() plt.title(\'Learning Curve for SVM\') plt.show() ```","solution":"import numpy as np from sklearn.model_selection import validation_curve, learning_curve from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.utils import shuffle import matplotlib.pyplot as plt # Load and shuffle the data X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) def plot_validation_curve(X, y): param_range = np.logspace(-3, 3, 7) train_scores, valid_scores = validation_curve( SVC(kernel=\\"linear\\"), X, y, param_name=\\"C\\", param_range=param_range, cv=5 ) plt.figure() plt.plot(param_range, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(param_range, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Parameter C\') plt.ylabel(\'Score\') plt.xscale(\'log\') plt.legend() plt.title(\'Validation Curve for SVM\') plt.show() def plot_learning_curve(X, y): train_sizes, train_scores, valid_scores = learning_curve( SVC(kernel=\'linear\'), X, y, train_sizes=[30, 50, 70, 90, 110], cv=5 ) plt.figure() plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\'Training score\') plt.plot(train_sizes, np.mean(valid_scores, axis=1), label=\'Validation score\') plt.xlabel(\'Training Sizes\') plt.ylabel(\'Score\') plt.legend() plt.title(\'Learning Curve for SVM\') plt.show() # Create the plots plot_validation_curve(X, y) plot_learning_curve(X, y)"},{"question":"# Question: Implement `encode_binhex` and `decode_binhex` for Custom Binhex-Like Encoding You are tasked with creating a custom encoding and decoding scheme inspired by the deprecated `binhex` module. Your implementation should follow similar principles but simplified and named `CustomBinhex`. This encoding scheme will convert a binary file into an ASCII representation and vice versa. Part 1: CustomBinhex.encode_binhex(input_file, output_file) # Function Definition ```python def encode_binhex(input_file: str, output_file: str) -> None: Read a binary file, encode its contents into an ASCII representation, and write the result to an output file. Parameters: input_file (str): The path to the input binary file. output_file (str): The path to the output encoded file. Raises: CustomBinhexError: If any error occurs during encoding. ``` - **Input:** - `input_file`: The path to the binary file that needs to be encoded. - `output_file`: The path where the encoded ASCII file will be saved. - **Output:** - None. The result should be written directly to `output_file`. - **Constraints:** - Handle errors gracefully, raising a `CustomBinhexError` if encoding fails. Part 2: CustomBinhex.decode_binhex(input_file, output_file) # Function Definition ```python def decode_binhex(input_file: str, output_file: str) -> None: Read an encoded ASCII file, decode its contents back into binary, and write the result to an output file. Parameters: input_file (str): The path to the input encoded file. output_file (str): The path where the decoded binary file will be saved. Raises: CustomBinhexError: If any error occurs during decoding. ``` - **Input:** - `input_file`: The path to the encoded ASCII file that needs to be decoded. - `output_file`: The path where the decoded binary file will be saved. - **Output:** - None. The result should be written directly to `output_file`. - **Constraints:** - Handle errors gracefully, raising a `CustomBinhexError` if decoding fails. CustomBinhexError Exception ```python class CustomBinhexError(Exception): Custom exception to be raised when encoding or decoding fails. pass ``` # Instructions: 1. Implement `CustomBinhexError`. 2. Implement the `encode_binhex` function. 3. Implement the `decode_binhex` function. 4. Ensure that your implementation correctly handles file I/O operations and raises `CustomBinhexError` on error. # Example: ```python # Encode a binary file to a custom binhex-like ASCII representation encode_binhex(\\"example.bin\\", \\"encoded_example.txt\\") # Decode the custom binhex-like ASCII representation back to a binary file decode_binhex(\\"encoded_example.txt\\", \\"decoded_example.bin\\") ``` **Note:** - Since the actual encoding and decoding scheme is not specified, you can choose any deterministic and reversible scheme, such as Base64. - Your solution should work with binary files of any type and size.","solution":"import base64 class CustomBinhexError(Exception): Custom exception to be raised when encoding or decoding fails. pass def encode_binhex(input_file: str, output_file: str) -> None: Read a binary file, encode its contents into an ASCII representation, and write the result to an output file. Parameters: input_file (str): The path to the input binary file. output_file (str): The path where the encoded ASCII file will be saved. Raises: CustomBinhexError: If any error occurs during encoding. try: with open(input_file, \'rb\') as fin: binary_data = fin.read() ascii_data = base64.b64encode(binary_data).decode(\'ascii\') with open(output_file, \'w\') as fout: fout.write(ascii_data) except Exception as e: raise CustomBinhexError(\\"Encoding failed\\") from e def decode_binhex(input_file: str, output_file: str) -> None: Read an encoded ASCII file, decode its contents back into binary, and write the result to an output file. Parameters: input_file (str): The path to the input encoded file. output_file (str): The path where the decoded binary file will be saved. Raises: CustomBinhexError: If any error occurs during decoding. try: with open(input_file, \'r\') as fin: ascii_data = fin.read() binary_data = base64.b64decode(ascii_data.encode(\'ascii\')) with open(output_file, \'wb\') as fout: fout.write(binary_data) except Exception as e: raise CustomBinhexError(\\"Decoding failed\\") from e"},{"question":"You have been provided with a dataset of diamond characteristics that you need to visualize using Seaborn\'s `objects` module. Follow the steps given to accomplish the task: 1. **Load Dataset**: Load the `diamonds` dataset provided by Seaborn. 2. **Create a Histogram**: - Plot a histogram of the `price` of diamonds. - Use a logarithmic scale for the x-axis. - Customize the histogram\'s bin width to `2000` and range to `(0, 20000)`. 3. **Color Mapping and Transformations**: - Map the colors of the bars based on the `cut` of diamonds. - Use the `Stack` transformation to avoid overlapping bars. 4. **Customization**: - Set the edge width to `0` for the bars and adjust transparency (`alpha`) based on the `clarity` attribute. 5. **Produce the Plot**: - Ensure the plot is well-labeled with appropriate titles and axis labels. - Display the final plot. # Expected Input and Output Formats: - **Input**: You are not required to take any user input for this task. - **Output**: Display the final plot with the specified customizations. # Constraints and Limitations: - Assume the required libraries (`seaborn`, `pandas`, `matplotlib`) are installed and available. - The primary focus is on using Seaborn functionalities effectively. ```python # Your code goes here ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_diamonds_histogram(): # Load the diamonds dataset diamonds = sns.load_dataset(\'diamonds\') # Create a histogram of the price of diamonds p = sns.displot( data=diamonds, x=\'price\', log_scale=True, binwidth=2000, binrange=(0, 20000), hue=\'cut\', element=\\"step\\", stat=\'count\', alpha=0.5, palette=\'viridis\' ) # Customizing parameters plt.title(\'Histogram of Diamond Prices\') plt.xlabel(\'Price (Log Scale)\') plt.ylabel(\'Count\') plt.show() # Wrapper function for tests def main(): plot_diamonds_histogram() if __name__ == \\"__main__\\": main()"},{"question":"# Advanced PyTorch DataLoader Implementation Objective: Create an efficient custom data loading pipeline using `torch.utils.data.DataLoader`, demonstrating mastery of: 1. Custom collate functions. 2. Handling both map-style and iterable-style datasets. 3. Optimizing data loading with multi-process workers and memory pinning. Task: 1. **Implement a Custom Dataset**: - Create a map-style dataset `CustomMapDataset` that returns images and their labels. - Create an iterable-style dataset `CustomIterableDataset` that streams samples. 2. **Create a Custom Collate Function**: - Implement a function `custom_collate_fn` that pads sequences in a batch to the same length. - Ensure the function works with both map-style and iterable-style datasets. 3. **Multi-Process Data Loading**: - Set up `DataLoader` with multi-process loading (`num_workers > 1`) and test it with both datasets. - Enable memory pinning for fast data transfer to CUDA. Requirements: - The `CustomMapDataset` should implement `__getitem__` and `__len__`. - The `CustomIterableDataset` should implement the `__iter__` method. - The `custom_collate_fn` should handle variable-length sequences by padding. - Demonstrate batch fetching, custom collation, and efficient data loading. Input and Output Formats: 1. **CustomMapDataset**: - **Input**: List of image paths and corresponding labels. - **Output**: A tuple `(image_tensor, label)`. 2. **CustomIterableDataset**: - **Input**: Stream or generator of samples. - **Output**: Streamed samples. 3. **DataLoader and Collate Function**: - **Input**: Dataset and `custom_collate_fn`. - **Output**: Batches of padded tensors. Constraints: - Ensure the datasets can handle real image data (consider using placeholders). - Test with various batch sizes, including edge cases. - Handle CUDA transfers efficiently with memory pinning. # Example Code Template: ```python import torch from torch.utils.data import DataLoader, Dataset, IterableDataset class CustomMapDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): # Implement how data is fetched from dataset pass def __len__(self): return len(self.data) class CustomIterableDataset(IterableDataset): def __init__(self, data): self.data = data def __iter__(self): for item in self.data: yield item def custom_collate_fn(batch): # Implement custom collation logic pass def main(): # Example data map_data = [(\\"image1\\", \\"label1\\"), (\\"image2\\", \\"label2\\")] iterable_data = iter([(\\"image3\\", \\"label3\\"), (\\"image4\\", \\"label4\\")]) map_dataset = CustomMapDataset(map_data) iterable_dataset = CustomIterableDataset(iterable_data) map_loader = DataLoader(map_dataset, batch_size=2, collate_fn=custom_collate_fn, num_workers=2, pin_memory=True) iterable_loader = DataLoader(iterable_dataset, batch_size=2, collate_fn=custom_collate_fn, num_workers=2, pin_memory=True) for batch in map_loader: print(batch) for batch in iterable_loader: print(batch) if __name__ == \\"__main__\\": main() ``` *Note: Actual implementation of \'__getitem__\', \'__iter__\', and \'custom_collate_fn\' is expected from students.*","solution":"import torch from torch.utils.data import DataLoader, Dataset, IterableDataset import random import numpy as np class CustomMapDataset(Dataset): def __init__(self, data): self.data = data def __getitem__(self, index): image, label = self.data[index] image_tensor = torch.tensor(image, dtype=torch.float32) label_tensor = torch.tensor(label, dtype=torch.float32) return image_tensor, label_tensor def __len__(self): return len(self.data) class CustomIterableDataset(IterableDataset): def __init__(self, data): self.data = data def __iter__(self): for image, label in self.data: image_tensor = torch.tensor(image, dtype=torch.float32) label_tensor = torch.tensor(label, dtype=torch.float32) yield image_tensor, label_tensor def pad_sequence(batch): max_len = max([len(x[0]) for x in batch]) padded_batch = [(torch.nn.functional.pad(x[0], (0, max_len - len(x[0]))), x[1]) for x in batch] return padded_batch def custom_collate_fn(batch): batch = pad_sequence(batch) images, labels = zip(*batch) images = torch.stack(images, dim=0) labels = torch.stack(labels, dim=0) return images, labels def main(): # Example data with varying sequence lengths map_data = [([1, 2, 3], 1), ([4, 5, 6, 7], 0)] iterable_data = iter([([8, 9], 1), ([10, 11, 12], 0)]) map_dataset = CustomMapDataset(map_data) iterable_dataset = CustomIterableDataset(iterable_data) map_loader = DataLoader(map_dataset, batch_size=2, collate_fn=custom_collate_fn, num_workers=2, pin_memory=True) iterable_loader = DataLoader(iterable_dataset, batch_size=2, collate_fn=custom_collate_fn, num_workers=2, pin_memory=True) for batch in map_loader: print(\\"Map Dataset Batch:\\") print(batch) for batch in iterable_loader: print(\\"Iterable Dataset Batch:\\") print(batch) if __name__ == \\"__main__\\": main()"},{"question":"**Coding Assessment Question** # Objective Write a Python function to read a text file, compress its content using a specific compression format and preset, and then write the compressed data to a new file. Finally, validate the compression by decompressing the content and verifying its integrity. # Function Signature ```python def compress_and_verify(input_filename: str, output_filename: str, compression_format: str, preset: int) -> bool: Compresses the content of the input file and writes it to the output file, then verifies the integrity by decompressing it. Parameters: input_filename (str): The name of the file to be compressed. output_filename (str): The name of the file where the compressed content will be written. compression_format (str): The format for compression (\'xz\' or \'lzma\'). preset (int): The preset compression level (0 to 9). Returns: bool: True if decompressed content matches the original content, False otherwise. ``` # Input - `input_filename`: A string representing the path to the input text file. - `output_filename`: A string representing the path where the compressed file should be saved. - `compression_format`: A string indicating the format to use for compression. Must be either `\'xz\'` or `\'lzma\'`. - `preset`: An integer between 0 and 9 (inclusive) specifying the compression preset level. # Output - Returns `True` if the decompressed content matches the original file content, else returns `False`. # Example ```python result = compress_and_verify(\'input.txt\', \'output.xz\', \'xz\', 6) print(result) # Should print True if the compression and decompression were successful and accurate ``` # Constraints - The function should handle any exceptions that might occur during file operations, compression, or decompression. - Ensure performance is managed effectively, especially for large files. - Use appropriate format constants from the `lzma` module. # Implementation Notes - The `compression_format` should map to appropriate formats from the `lzma` module (`FORMAT_XZ` or `FORMAT_ALONE`). - Use exception handling to catch and handle any `lzma.LZMAError` or file I/O errors. - Read the content of the input file in binary mode. - Write the compressed data to the output file in binary mode. - After compressing, decompress the content and compare it to the original content to verify the integrity. **Tip:** Refer to the examples in the documentation for using `lzma.open()` and handling compressed data.","solution":"import lzma def compress_and_verify(input_filename: str, output_filename: str, compression_format: str, preset: int) -> bool: try: format_map = { \'xz\': lzma.FORMAT_XZ, \'lzma\': lzma.FORMAT_ALONE } if compression_format not in format_map: raise ValueError(\\"Unsupported compression format. Use \'xz\' or \'lzma\'.\\") compression_format_const = format_map[compression_format] # Read the content from the input file with open(input_filename, \'rb\') as input_file: original_content = input_file.read() # Compress the content compressor = lzma.LZMACompressor(format=compression_format_const, preset=preset) compressed_content = compressor.compress(original_content) compressed_content += compressor.flush() # Write the compressed content to the output file with open(output_filename, \'wb\') as output_file: output_file.write(compressed_content) # Decompress the content to verify integrity decompressor = lzma.LZMADecompressor(format=compression_format_const) decompressed_content = decompressor.decompress(compressed_content) # Return True if the decompressed content matches the original content return decompressed_content == original_content except (OSError, lzma.LZMAError) as e: print(f\\"An error occurred during file operations or compression: {e}\\") return False except ValueError as e: print(e) return False"},{"question":"**Objective**: Implement a subclass of the `reprlib.Repr` class to handle custom object types and enforce specific size limits. **Problem Statement**: We have a class called `Greeting`, which is used to store and display messages. Your task is to create a subclass of the `reprlib.Repr` class that provides custom string representations for objects of the `Greeting` class with certain constraints. **Class Definition**: ```python class Greeting: def __init__(self, message): self.message = message def __repr__(self): return f\\"Greeting(message={self.message!r})\\" ``` **Requirements**: 1. Create a subclass of `reprlib.Repr` named `CustomRepr`. 2. Implement a method `repr_Greeting(self, obj, level)` within the `CustomRepr` to handle the `Greeting` objects. 3. Limit the maximum number of characters shown for the `message` attribute of `Greeting` to 10 characters. If the `message` exceeds 10 characters, it should be truncated, and \\"...\\" should be appended to indicate truncation. 4. Make sure to handle recursion if a `Greeting` object contains another `Greeting` object in its `message`. **Function Implementation**: Your implementation should involve: - Creating the `CustomRepr` class. - Defining the `repr_Greeting` method to format the string representation of `Greeting` objects according to the size constraint. - Demonstrating the usage of your custom `reprlib.Repr` subclass. **Expected Input and Output**: - Input: A `Greeting` object with various message lengths. - Output: A string representation of the object, limited to 10 characters for the message. **Example**: ```python import reprlib class Greeting: def __init__(self, message): self.message = message def __repr__(self): return f\\"Greeting(message={self.message!r})\\" class CustomRepr(reprlib.Repr): def repr_Greeting(self, obj, level): max_length = 10 if len(obj.message) > max_length: truncated_message = obj.message[:max_length] + \'...\' else: truncated_message = obj.message return f\\"Greeting(message={truncated_message!r})\\" # Demonstration of the custom reprlib functionality custom_repr = CustomRepr() g = Greeting(\\"Hello, World!\\") print(custom_repr.repr(g)) # Output should be: Greeting(message=\'Hello, Wor...\') g2 = Greeting(\\"Short\\") print(custom_repr.repr(g2)) # Output should be: Greeting(message=\'Short\') ``` **Constraints**: - Assume the message in `Greeting` will be a string. - The method should handle nested `Greeting` objects to avoid infinite recursion. **Performance Requirements**: - The solution should handle large and deeply nested objects efficiently without causing performance bottlenecks.","solution":"import reprlib class Greeting: def __init__(self, message): self.message = message def __repr__(self): return f\\"Greeting(message={self.message!r})\\" class CustomRepr(reprlib.Repr): def repr_Greeting(self, obj, level): max_length = 10 if len(obj.message) > max_length: truncated_message = obj.message[:max_length] + \'...\' else: truncated_message = obj.message return f\\"Greeting(message={truncated_message!r})\\" # Set the custom repr for Greeting in CustomRepr custom_repr = CustomRepr() custom_repr.repr_Greeting = custom_repr.repr_Greeting custom_repr.maxlevel = 2 # Attach the custom repr method to Greeting class reprlib.aRepr = custom_repr"},{"question":"# Advanced Coding Assessment: Implementing a Custom Buffer in Python Objective In this assessment, you are required to demonstrate your understanding of Python\'s buffer protocol by creating a custom buffer object in Python. This will involve designing a custom class that can expose its internal buffer using the buffer protocol, and implementing appropriate methods to handle buffer requests correctly. Task Implement a custom class `CustomBuffer` that: 1. **Initialization**: Accepts a byte buffer during initialization. 2. **Buffer Interface**: Implements the buffer protocol so the buffer can be exposed for read and write operations. 3. **Handling Contiguity and Strides**: Supports requests for different types of buffers, including contiguous (C-style and Fortran-style) and with/without strides. Details 1. **Initialization**: ```python class CustomBuffer: def __init__(self, buffer: bytes): self._buffer = buffer # Additional initialization code if necessary ``` 2. **Buffer Protocol Methods**: - Implement the necessary buffer protocol methods to provide a buffer view of the internal buffer data. - Ensure proper handling of `PyBUF_*` flags (e.g., `PyBUF_SIMPLE`, `PyBUF_WRITABLE`, `PyBUF_FORMAT`, etc). 3. **Contiguous Memory**: - Correctly handle requests for contiguous memory in both C-style and Fortran-style. Constraints and Requirements - **Read-Write Access**: - Implement mechanisms to allow both read and write operations if requested. - If read-only access is requested, ensure no write operations are permitted. - **Memory Contiguity**: - Implement checks to determine memory contiguity (C-style or Fortran-style). - **Strides and Shape**: - Handle stride and shape information appropriately if requested. - **Performance**: - Direct interaction with the buffer should avoid unnecessary copying of data to maximize performance. Example Usage ```python # Example buffer containing bytes data = b\'Hello, Buffer Protocol!\' # Create a custom buffer object cb = CustomBuffer(data) # Assume an interface function to get the buffer view: view = get_buffer_view(cb) # Read data from buffer view print(view[:5]) # Output: b\'Hello\' # Modify buffer content (if writable) if view.writable: view[6:] = b\'World!\' print(view[:]) # Output might be: b\'Hello, World!\' ``` Submission - Submit your implementation for the `CustomBuffer` class. - Ensure your code is well-documented and includes comments explaining key parts of your implementation. - Include any test cases you wrote to validate the functionality of your class.","solution":"import ctypes class CustomBuffer: def __init__(self, buffer: bytes): self._buffer = bytearray(buffer) # Using bytearray to allow mutability if needed def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): pass def get_buffer_view(self, flags): readonly = False if flags & 0x02: # checking for PyBUF_WRITABLE flag readonly = True # Creating a memoryview for the buffer, respecting the readonly flag if readonly: return memoryview(self._buffer).toreadonly() else: return memoryview(self._buffer)"},{"question":"# Question Using the `contextlib` module, write a function `process_files` that processes a set of files, ensuring all files are properly closed after processing even if an error occurs. The processing function should print the contents of each file. Function Signature ```python def process_files(file_paths: list): Processes a list of files and prints their contents. Ensures all files are properly closed after processing. :param file_paths: List of file paths to be processed ``` Requirements 1. You must use the `ExitStack` from the `contextlib` module to manage the opening and closing of the files. 2. If an error occurs while processing any file, all previously opened files should be closed. 3. Your implementation should handle any exceptions that may arise during file operations and log an appropriate error message without halting the entire process. Example ```python # Assume file1.txt, file2.txt, file3.txt exist and contain some text. file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] process_files(file_paths) # The function prints the content of each file and ensures all files are closed properly. ``` Constraints - Do not use any libraries outside the standard library. - The function should be efficient in terms of resource management and handle large files gracefully. - Ensure proper logging of any errors encountered during file processing. **Hint**: Use `ExitStack` to enter multiple contexts and ensure they are properly cleaned up even if an error occurs.","solution":"from contextlib import ExitStack import logging def process_files(file_paths: list): Processes a list of files and prints their contents. Ensures all files are properly closed after processing. :param file_paths: List of file paths to be processed logging.basicConfig(level=logging.ERROR) with ExitStack() as stack: files = [] try: for path in file_paths: file = stack.enter_context(open(path, \'r\')) files.append(file) for file in files: print(file.read()) except Exception as e: logging.error(f\\"An error occurred while processing files: {e}\\")"},{"question":"Coding Assessment Question You are tasked with writing a Python function to manage warnings in a simulation program. Your function should demonstrate an understanding of warning issuance, filtering, and handling through the `warnings` module. # Problem Statement Implement a Python function `manage_simulation_warnings()` that performs the following steps: 1. Simulate warnings in the following categories: - `UserWarning`: When any user-related issue is detected. - `DeprecationWarning`: When deprecated features are used. - `ResourceWarning`: When resource usage exceeds a threshold. 2. Set up specific filters to handle these warnings as follows: - Suppress all `DeprecationWarning`s. - Convert all `ResourceWarning`s into exceptions. - Log all `UserWarning`s to a specified log file (provided as an argument to your function). 3. Use the `catch_warnings` context manager to test that: - All warning categories are handled as specified. - Print a summary of warnings raised during the function execution. # Function Signature ```python import warnings def manage_simulation_warnings(log_file_path: str) -> None: pass ``` # Input - `log_file_path` (str): The file path where `UserWarning` logs should be stored. # Output - None. # Constraints - Ensure that the `DeprecationWarning` is ignored and does not appear in the output. - `ResourceWarning` should raise an exception when triggered. - `UserWarning` should be logged to the specified file. # Example Usage ```python import warnings def manage_simulation_warnings(log_file_path: str) -> None: # Your implementation here pass # Simulate the function with a log file path manage_simulation_warnings(\\"user_warnings.log\\") ``` # Detailed Requirements 1. Create a method `simulate_warnings()` within your function that triggers each category of warning. 2. Use `warnings.filterwarnings` and `warnings.simplefilter` to set up necessary filters before calling `simulate_warnings()`. 3. Implement a custom warning handler that writes `UserWarning` messages to the provided log file. 4. Use the context manager `warnings.catch_warnings` to capture and print all raised warnings in an appropriate format. # Performance Requirements - The function should handle up to 1000 warnings efficiently without significant delay. - Ensure that the function is thread-safe for concurrent warning handling. Use the provided documentation for guidance and make sure to include sufficient comments in your code to explain each step.","solution":"import warnings def manage_simulation_warnings(log_file_path: str) -> None: def simulate_warnings(): warnings.warn(\\"This is a user warning\\", UserWarning) warnings.warn(\\"This feature is deprecated\\", DeprecationWarning) warnings.warn(\\"Resource usage is high\\", ResourceWarning) # Set up filters warnings.filterwarnings(\\"ignore\\", category=DeprecationWarning) warnings.filterwarnings(\\"error\\", category=ResourceWarning) # Custom logging handler for UserWarnings class UserWarningLogger: def __init__(self, path): self.path = path def __call__(self, message, category, filename, lineno, file=None, line=None): if category == UserWarning: with open(self.path, \'a\') as log_file: log_file.write(f\\"{category.__name__}: {message}n\\") handler = UserWarningLogger(log_file_path) with warnings.catch_warnings(record=True) as w: warnings.showwarning = handler try: simulate_warnings() except ResourceWarning as e: print(f\\"Exception: {e}\\") # Print summary of raised warnings for warning in w: print(f\\"{warning.category.__name__}: {warning.message}\\")"},{"question":"**Problem Statement**: You are tasked with implementing a Python class that automates the process of connecting to a remote FTP server, downloading a specified file, and saving it locally with TLS security. # Objectives: 1. **Connect** to the FTP server using TLS. 2. **Authenticate** using provided credentials. 3. **Navigate** to a specified directory. 4. **Download** a file in binary mode and save it locally. 5. **Handle** any potential exceptions that may arise during the process. # Requirements - Implement a class `SecureFTPClient` with the following methods: - `__init__(self, host: str, username: str = \'anonymous\', password: str = \'\', acct: str = \'\', encoding: str = \'utf-8\')`: Initialize the client with connection details. - `connect(self)`: Connect to the FTP server and authenticate. - `navigate_directory(self, directory: str)`: Change to the specified directory. - `download_file(self, remote_filename: str, local_filename: str)`: Download the specified file from FTP server and save it locally. - `close(self)`: Properly close the FTP connection. - Additional context management support using the `with` statement. # Constraints: 1. The FTP server uses port 21. 2. Connection and commands should follow proper exception handling identifying temporary and permanent failures. 3. Handle possible FTP errors like connection failures, authentication failures, navigation failures, and download failures. # Class definition example: ```python from ftplib import FTP_TLS, all_errors class SecureFTPClient: def __init__(self, host: str, username: str = \'anonymous\', password: str = \'\', acct: str = \'\', encoding: str = \'utf-8\'): # Initialize the client with FTP and connection details def connect(self): # Connect and authenticate to the FTP server using TLS def navigate_directory(self, directory: str): # Change the working directory to the specified path def download_file(self, remote_filename: str, local_filename: str): # Download the file in binary mode and save it locally def close(self): # Proper way to close the FTP connection def __enter__(self): # Context manager enter method def __exit__(self, exc_type, exc_val, exc_tb): # Context manager exit method ``` # Example usage: ```python with SecureFTPClient(\'ftp.secure-server.com\', \'user\', \'password\') as ftp_client: ftp_client.navigate_directory(\'path/to/directory\') ftp_client.download_file(\'remote_file.txt\', \'local_file.txt\') ``` # Evaluation Criteria: 1. Correct implementation of FTP connection and operations using `FTP_TLS`. 2. Robust exception handling covering all possible FTP-related errors. 3. Proper resource management by implementing context management (`__enter__` and `__exit__` methods). 4. Clean and readable code that adheres to Python conventions.","solution":"from ftplib import FTP_TLS, all_errors class SecureFTPClient: def __init__(self, host: str, username: str = \'anonymous\', password: str = \'\', acct: str = \'\', encoding: str = \'utf-8\'): self.host = host self.username = username self.password = password self.acct = acct self.encoding = encoding self.ftp = None def connect(self): try: self.ftp = FTP_TLS(self.host, self.username, self.password, self.acct, encoding=self.encoding) self.ftp.prot_p() # Upgrade data connection to secure except all_errors as e: print(f\\"Failed to connect or authenticate: {e}\\") raise def navigate_directory(self, directory: str): try: self.ftp.cwd(directory) except all_errors as e: print(f\\"Failed to navigate to directory {directory}: {e}\\") raise def download_file(self, remote_filename: str, local_filename: str): try: with open(local_filename, \'wb\') as local_file: self.ftp.retrbinary(f\'RETR {remote_filename}\', local_file.write) except all_errors as e: print(f\\"Failed to download file {remote_filename}: {e}\\") raise def close(self): if self.ftp: try: self.ftp.quit() except all_errors as e: print(f\\"Failed to properly close the FTP connection: {e}\\") def __enter__(self): self.connect() return self def __exit__(self, exc_type, exc_val, exc_tb): self.close()"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `objects` interface by loading, processing, and visualizing data. **Problem Statement:** You are given access to a dataset named `brain_networks`. The dataset, when loaded, has a hierarchical structure with multiple levels of column headers. Your task is to process this dataset and create a specific type of visualization using seaborn\'s `objects` interface. **Instructions:** 1. **Loading and Processing Data:** - Load the `brain_networks` dataset using seaborn\'s `load_dataset` function and handle it as a multi-indexed DataFrame based on the top 3 levels of headers. - Convert the DataFrame such that each row represents the average values of certain measurements grouped by `timepoint`, `network`, and `hemi`. - Filter the dataset to include only `timepoints` less than 100. 2. **Creating the Visualization:** - Using seaborn\'s new `objects` interface, create a pair matrix plot of the processed dataset. - Select the variables `5`, `8`, `12`, and `15` for the x-axis and `6`, `13`, `16` for the y-axis in the pair plot. - Ensure the layout of the plot is 8x5 in size, and share axes across the grid. - Add paths to the plot to visualize the trajectories. Customize the paths with a linewidth of 1, alpha of 0.8, and color them based on the `hemi` attribute. **Constraints:** - Ensure that the x and y axes are shared across the plots in the grid. - Use seaborn’s `objects` interface for creating and customizing the visualization, without falling back to the traditional API. **Expected Input and Output:** - Input: The `brain_networks` dataset, which is internally provided by seaborn. - Output: A pair plot grid with paths representing trajectories of the specified variables, customized as described. You can assume the following code snippet for loading and transforming the dataset: ```python import seaborn.objects as so from seaborn import load_dataset # Load and process the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Customize and add paths p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") ``` Write a function that implements the above processing and visualization steps, and call the function to generate the plot.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_brain_networks(): # Load and process the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair( x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"], ) .layout(size=(8, 5)) .share(x=True, y=True) ) # Customize and add paths p.add(so.Paths(linewidth=1, alpha=.8), color=\\"hemi\\") # Display the plot p.show()"},{"question":"Objective: Your task is to design a set of classes to simulate a library management system that handles books and authors. This exercise will test your ability to use Python classes, inheritance, multiple inheritance, and method overriding. Problem Statement: Create a Python program that models a library using the following specifications: 1. **Create a Base Class `Person`**: - Attributes: * `name` (string) - Methods: * `get_name()`: Returns the `name` of the person. 2. **Create a Class `Author` (inherits `Person`)**: - Attributes: * `books` (list of book titles represented as strings) - this is a class variable shared among all instances of Author. - Methods: * `add_book(book_title)`: Adds a book title to the author\'s book list. * `get_books()`: Returns the list of books written by the author. 3. **Create a Class `LibraryItem`**: - Attributes: * `title`: The title of the item (string). * `library_id`: The unique identifier for the item (integer). - Methods: * `get_title()`: Returns the title of the item. * `get_library_id()`: Returns the library identifier. 4. **Create a Class `Book` (inherits `LibraryItem` and `Author`)**: - Attributes: * `publication_year`: Year the book was published. - Methods: * `get_publication_year()`: Returns the publication year of the book. * Override the `get_title()` method to return the title of the book along with the author\'s name. 5. **Create a Library Management System**: - Create instances of `Author`, `LibraryItem`, and `Book`. - Ensure at least one instance of `Book` demonstrates multiple inheritance properties. - The system must properly register at least two books for one author. - Print the details of the library items and authors using the respective methods. Constraints: - Ensure no duplicate titles exist in the author\'s book list. - The library ID should be unique for each library item. Expected Output: The program\'s output should display: - The list of books an author has written. - The title of each book along with the author\'s name. - Unique library IDs for each library item. Function Signature: ```python class Person: def __init__(self, name: str): pass def get_name(self) -> str: pass class Author(Person): books = [] def __init__(self, name: str): pass def add_book(self, book_title: str): pass def get_books(self) -> list: pass class LibraryItem: def __init__(self, title: str, library_id: int): pass def get_title(self) -> str: pass def get_library_id(self) -> int: pass class Book(LibraryItem, Author): def __init__(self, title: str, library_id: int, publication_year: int): pass def get_publication_year(self) -> int: pass def get_title(self) -> str: pass # Create instances and demonstrate functionality ``` Take into account that this task requires a good understanding of Python OOP principles, including multiple inheritance, method resolution, static and instance variables, and method overrides.","solution":"class Person: def __init__(self, name: str): self.name = name def get_name(self) -> str: return self.name class Author(Person): books = [] def __init__(self, name: str): super().__init__(name) self.books = [] def add_book(self, book_title: str): if book_title not in self.books: self.books.append(book_title) def get_books(self) -> list: return self.books class LibraryItem: def __init__(self, title: str, library_id: int): self.title = title self.library_id = library_id def get_title(self) -> str: return self.title def get_library_id(self) -> int: return self.library_id class Book(LibraryItem, Author): def __init__(self, title: str, library_id: int, publication_year: int, author_name:str): LibraryItem.__init__(self, title, library_id) Author.__init__(self, author_name) self.publication_year = publication_year def get_publication_year(self) -> int: return self.publication_year def get_title(self) -> str: return f\\"{self.title} by {self.get_name()}\\" # Example usage author = Author(\\"J.K. Rowling\\") author.add_book(\\"Harry Potter and the Philosopher\'s Stone\\") author.add_book(\\"Harry Potter and the Chamber of Secrets\\") book1 = Book(\\"Harry Potter and the Philosopher\'s Stone\\", 1, 1997, \\"J.K. Rowling\\") book2 = Book(\\"Harry Potter and the Chamber of Secrets\\", 2, 1998, \\"J.K. Rowling\\") print(author.get_books()) # [\\"Harry Potter and the Philosopher\'s Stone\\", \\"Harry Potter and the Chamber of Secrets\\"] print(book1.get_title()) # \\"Harry Potter and the Philosopher\'s Stone by J.K. Rowling\\" print(book2.get_title()) # \\"Harry Potter and the Chamber of Secrets by J.K. Rowling\\" print(book1.get_library_id()) # 1 print(book2.get_library_id()) # 2"},{"question":"# URL Fetcher with Custom Headers and Error Handling **Objective:** Write a Python function that fetches content from a given URL using custom headers and handles common HTTP errors gracefully. **Problem Statement:** You need to implement a function `fetch_url_content(url: str, headers: dict) -> str` that receives a URL and a dictionary of custom headers as input. The function should fetch the content from the given URL and return it as a string. If an HTTP error occurs (e.g., 404 Not Found, 500 Internal Server Error), it should return a message indicating the type of HTTP error. If the request is successful, it should return the HTML content of the page. **Function Signature:** ```python def fetch_url_content(url: str, headers: dict) -> str: pass ``` **Input:** - `url` (str): The URL from which to fetch the content. - `headers` (dict): A dictionary of HTTP headers to include in the request. **Output:** - Returns a string containing the HTML content if the request is successful. - Returns a string with the HTTP error message if an error occurs. **Constraints:** - Do not use external libraries other than `urllib`. - Handle common HTTP errors such as 404, 500, and other HTTPError exceptions. **Examples:** ```python # Example 1 url = \\"https://www.example.com\\" headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} result = fetch_url_content(url, headers) print(result) # Should print the HTML content of the page or an error message. # Example 2 url = \\"https://www.nonexistentwebsite.com\\" headers = {\\"User-Agent\\": \\"Mozilla/5.0\\"} result = fetch_url_content(url, headers) print(result) # Should print an error message like \\"HTTP Error 404: Not Found\\". ``` **Notes:** - Use `urllib.request` from the urllib module to perform the HTTP request. - Make sure your function handles redirects and uses the provided headers in every request. - Ensure code readability and proper handling of exceptions to avoid crashing the program. **Hints:** - Look into `urllib.request.Request` and `urllib.request.urlopen` to construct and send the HTTP request. - Use proper exception handling mechanisms with `urllib.error.HTTPError` and `urllib.error.URLError`.","solution":"import urllib.request import urllib.error def fetch_url_content(url: str, headers: dict) -> str: Fetches content from the given URL using custom headers and handles common HTTP errors. Parameters: url (str): The URL from which to fetch the content. headers (dict): A dictionary of HTTP headers to include in the request. Returns: str: The HTML content of the page if the request is successful, or an error message if an error occurs. req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"HTTP Error {e.code}: {e.reason}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\""},{"question":"**Coding Assessment Question** You are provided with a dataset containing high-dimensional features. Your task is to implement a function that reduces the dimensionality of the dataset using PCA and Feature Agglomeration methods, and then returns the reduced dataset. Specifically, you need to follow these steps: 1. Scale the features using StandardScaler. 2. Apply PCA to reduce the dimensions to a given number of principal components. 3. Apply Feature Agglomeration to the PCA output to further reduce the dimensions. # Function Signature ```python def reduce_dimensions(X: np.ndarray, n_components_pca: int, n_clusters_agglomeration: int) -> np.ndarray: pass ``` # Input - `X`: A 2D numpy array of shape (n_samples, n_features) representing the high-dimensional input data. - `n_components_pca`: An integer denoting the number of principal components to keep after the PCA transformation. - `n_clusters_agglomeration`: An integer representing the number of clusters to form after feature agglomeration. # Output - A 2D numpy array of shape (n_samples, n_clusters_agglomeration) representing the reduced dataset. # Constraints - `n_components_pca` should be less than or equal to the number of original features. - `n_clusters_agglomeration` should be less than or equal to `n_components_pca`. # Example ```python import numpy as np # Sample data X = np.random.rand(100, 50) # 100 samples with 50 features # Apply the function reduced_X = reduce_dimensions(X, n_components_pca=20, n_clusters_agglomeration=10) print(reduced_X.shape) # Expected output: (100, 10) ``` # Notes - You must use `StandardScaler` from `sklearn.preprocessing` to scale the features. - Use `PCA` from `sklearn.decomposition` to perform principal component analysis. - Use `FeatureAgglomeration` from `sklearn.cluster` to perform feature agglomeration. # Hints - Ensure that you have preprocessed the data correctly before each transformation step. - You may need to explore the documentation for these classes if you are unfamiliar with their usage. Good luck!","solution":"import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.cluster import FeatureAgglomeration def reduce_dimensions(X: np.ndarray, n_components_pca: int, n_clusters_agglomeration: int) -> np.ndarray: # Step 1: Scale the features using StandardScaler scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Step 2: Apply PCA to reduce the dimensions pca = PCA(n_components=n_components_pca) X_pca = pca.fit_transform(X_scaled) # Step 3: Apply Feature Agglomeration to PCA output agglomeration = FeatureAgglomeration(n_clusters=n_clusters_agglomeration) X_reduced = agglomeration.fit_transform(X_pca) return X_reduced"},{"question":"You are required to develop a command-line application using the `argparse` module that simulates a simplified version of a backup tool. The application should support the following operations: 1. **Backup**: - Create a backup of specified directories. - Optionally compress the backup. - Optionally exclude specified subdirectories from the backup. - Example usage: `backup_tool.py backup /home/user/Documents /home/user/Pictures --compress --exclude /home/user/Documents/temp` 2. **Restore**: - Restore the backup to a specified directory. - Optionally specify a specific backup file to restore from. - Example usage: `backup_tool.py restore /home/user/Backup --output /home/user/Restore` 3. **View**: - View the list of available backups. - Example usage: `backup_tool.py view` Each of these operations should be implemented as a sub-command using `argparse`. Additionally, the application should support a global `--verbose` option for detailed output. # Function Specifications 1. **Function: parse_args()** - **Objective**: Define the `argparse.ArgumentParser()` structure to handle the described operations and their respective arguments. - **Input**: None (you will use `sys.argv` internally). - **Output**: A namespace object populated with the parsed arguments. 2. **Function: backup(args)** - **Objective**: Handle the logic for the `backup` sub-command. - **Input**: A namespace object containing the parsed arguments. - **Output**: None (print actions being performed, e.g., \\"Backing up directory X with compression\\" or \\"Creating backup...\\"). 3. **Function: restore(args)** - **Objective**: Handle the logic for the `restore` sub-command. - **Input**: A namespace object containing the parsed arguments. - **Output**: None (print actions being performed, e.g., \\"Restoring backup to directory Y\\"). 4. **Function: view(args)** - **Objective**: Handle the logic for the `view` sub-command. - **Input**: A namespace object containing the parsed arguments. - **Output**: None (print the list of available backups). # Constraints: - Define the `--verbose` option to print additional details during backup and restore operations. - Handle argument parsing errors gracefully and provide helpful error messages. - Implement the sub-commands correctly, using mutually exclusive options where necessary. # Example Code Execution: ```python import argparse import sys def parse_args(): # Your code to create the parser and add subparsers and arguments goes here pass def backup(args): # Your code to handle the backup logic goes here pass def restore(args): # Your code to handle the restore logic goes here pass def view(args): # Your code to handle the view logic goes here pass if __name__ == \\"__main__\\": args = parse_args() if args.command == \\"backup\\": backup(args) elif args.command == \\"restore\\": restore(args) elif args.command == \\"view\\": view(args) elif args.verbose: print(\\"Verbose mode is on\\") ``` # Implementation Requirements - Implement the `parse_args`, `backup`, `restore`, and `view` functions. - Ensure the application supports the described features and adheres to the constraints. - Write comprehensive docstrings for each function.","solution":"import argparse import sys def parse_args(): Parses command-line arguments and returns a namespace object. parser = argparse.ArgumentParser(description=\\"Simplified backup tool.\\") parser.add_argument(\\"--verbose\\", action=\\"store_true\\", help=\\"Increase output verbosity\\") subparsers = parser.add_subparsers(dest=\\"command\\", help=\\"sub-command help\\") # Backup sub-command parser_backup = subparsers.add_parser(\\"backup\\", help=\\"Create a backup of specified directories\\") parser_backup.add_argument(\\"directories\\", nargs=\'+\', help=\\"Directories to backup\\") parser_backup.add_argument(\\"--compress\\", action=\\"store_true\\", help=\\"Compress the backup\\") parser_backup.add_argument(\\"--exclude\\", nargs=\'*\', help=\\"Subdirectories to exclude from the backup\\") # Restore sub-command parser_restore = subparsers.add_parser(\\"restore\\", help=\\"Restore a backup to a specified directory\\") parser_restore.add_argument(\\"backup_dir\\", help=\\"Backup directory to restore from\\") parser_restore.add_argument(\\"--output\\", required=True, help=\\"Directory to restore the backup into\\") # View sub-command parser_view = subparsers.add_parser(\\"view\\", help=\\"View the list of available backups\\") return parser.parse_args() def backup(args): Handles the logic for the backup sub-command. if args.verbose: print(\\"Verbose mode is on\\") for directory in args.directories: print(f\\"Creating backup for directory: {directory}\\") if args.compress: print(\\"Compression enabled for backup\\") if args.exclude: print(f\\"Excluding directories from backup: {\', \'.join(args.exclude)}\\") def restore(args): Handles the logic for the restore sub-command. if args.verbose: print(\\"Verbose mode is on\\") print(f\\"Restoring backup from {args.backup_dir} to {args.output}\\") def view(args): Handles the logic for the view sub-command. if args.verbose: print(\\"Verbose mode is on\\") print(\\"Listing available backups...\\") if __name__ == \\"__main__\\": args = parse_args() if args.command == \\"backup\\": backup(args) elif args.command == \\"restore\\": restore(args) elif args.command == \\"view\\": view(args)"},{"question":"Objective Design a function to process an input text and perform specific text transformations based on provided patterns using the `re` module in Python. Problem Statement You are given a multiline string that contains several sentences. The sentences contain the following: - Dates in the format `dd-mm-yyyy`. - Email addresses in the format `local-part@domain`. - Phone numbers in different formats including `xxx-xxx-xxxx`, `(xxx) xxx-xxxx`, or `xxx.xxx.xxxx`. Your task is to write a function `process_text(text: str) -> str` that identifies and converts the dates to the format `yyyy/mm/dd`, masks the email addresses by replacing the local part with asterisks (`*`), and standardizes all phone numbers to the format `xxx-xxx-xxxx`. Input - `text` (str): A multiline string containing sentences with dates, email addresses, and phone numbers. Output - Returns a modified string where: - All dates are converted to the format `yyyy/mm/dd`. - All email addresses have the local part replaced with `*`. - All phone numbers are standardized to the format `xxx-xxx-xxxx`. Constraints - The input string will only include valid formats of dates, email addresses, and phone numbers as specified. - There will not be any multiline email addresses, dates, or phone numbers. - All dates, email addresses, and phone numbers are separated by spaces or appear at the start/end of lines. Examples: ```python text = John\'s birthday is on 15-08-1990 and his email is john.doe@example.com. His phone number is 123-456-7890. Mary\'s email is mary.jane@domain.org and she was born on 07-11-1985. You can reach her at (987) 654-3210. Contact support at support@company.com or 555.999.1234. The event is on 21-12-2022. process_text(text) ``` Expected output: ``` John\'s birthday is on 1990/08/15 and his email is *******@example.com. His phone number is 123-456-7890. Mary\'s email is *******@domain.org and she was born on 1985/11/07. You can reach her at 987-654-3210. Contact support at *******@company.com or 555-999-1234. The event is on 2022/12/21. ``` Function Signature ```python def process_text(text: str) -> str: pass ``` **Hints:** - Use `re.sub` to perform the substitutions. - Use appropriate capturing groups to manipulate parts of the matched patterns. - Test your function with a variety of multiline input strings to ensure coverage of all specified patterns.","solution":"import re def process_text(text: str) -> str: # Date conversion: dd-mm-yyyy to yyyy/mm/dd text = re.sub(r\'(bd{2})-(d{2})-(d{4}b)\', r\'3/2/1\', text) # Email masking: Replace local part with asterisks text = re.sub(r\'(b[w.-]+)@([w.-]+.w+b)\', r\'*******@2\', text) # Phone number standardization: xxx.xxx.xxxx or (xxx) xxx-xxxx to xxx-xxx-xxxx text = re.sub(r\'b(d{3})[.-](d{3})[.-](d{4})b\', r\'1-2-3\', text) text = re.sub(r\'(b(d{3}))s(d{3})-(d{4})b\', r\'1-2-3\', text) return text"},{"question":"Topic: Implementing and Utilizing BernoulliRBM in Character Recognition Objective: To assess the student\'s understanding of `BernoulliRBM` in scikit-learn by having them implement an RBM for a binary classification task of handwritten digits. Task: You are given a subset of the MNIST dataset, specifically the digits \'0\' and \'1\'. Your task is to: 1. Implement a Bernoulli Restricted Boltzmann Machine (BernoulliRBM) using scikit-learn. 2. Train the RBM on the training data to learn the underlying features. 3. Use the learned features to train a Logistic Regression classifier. 4. Evaluate the performance of the classifier on the test data and report the accuracy. Input and Output Formats: - **Input**: A training dataset (`X_train`, `y_train`) and a testing dataset (`X_test`, `y_test`) where: - `X_train` and `X_test` are 2D numpy arrays of shape (n_samples, n_features) containing binary pixel values. - `y_train` and `y_test` are 1D numpy arrays containing labels (0s and 1s). Constraints: - The input features must be preprocessed to be in the range [0, 1], representing the probability that the pixel is active. - Performance requirement: The classifier should achieve at least 90% accuracy on the test set. Instructions: 1. **Preprocess the Dataset**: - Ensure that `X_train` and `X_test` contain binary values or values between 0 and 1. 2. **Implement BernoulliRBM**: - Define and train the `BernoulliRBM` on `X_train`. - Transform the training data (`X_train`) using the trained RBM to obtain the learned features. 3. **Train Logistic Regression Classifier**: - Use the transformed data from the RBM to train a logistic regression classifier. 4. **Evaluate the Classifier**: - Transform the test data (`X_test`) using the trained RBM. - Predict and evaluate the labels using the trained logistic regression classifier. - Calculate and print the accuracy on the test set. Example: ```python from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn import metrics import numpy as np # Example function template def rbm_digit_classifier(X_train, y_train, X_test, y_test): # Step 1: Preprocess the dataset (Assume the dataset is already preprocessed) # Step 2: Define the RBM model rbm = BernoulliRBM(n_components=100, learning_rate=0.06, batch_size=10, n_iter=10, random_state=0) # Step 3: Create a pipeline with RBM and Logistic Regression logistic = LogisticRegression(solver=\'lbfgs\', max_iter=1000) classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Step 4: Train the model classifier.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = classifier.predict(X_test) accuracy = metrics.accuracy_score(y_test, y_pred) # Output the accuracy print(\\"Accuracy:\\", accuracy) return accuracy # Use the function with a given dataset (Ensure to provide the actual MNIST data subset specific to digits \'0\' and \'1\') # rbm_digit_classifier(X_train, y_train, X_test, y_test) ``` Upload your solution to the provided platform once completed.","solution":"from sklearn.neural_network import BernoulliRBM from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn import metrics import numpy as np def rbm_digit_classifier(X_train, y_train, X_test, y_test): Train BernoulliRBM on the training data and use the features to train a logistic regression classifier. Evaluate the performance on the test data. Parameters: X_train (numpy.ndarray): Training data features. y_train (numpy.ndarray): Training data labels. X_test (numpy.ndarray): Test data features. y_test (numpy.ndarray): Test data labels. Returns: float: Accuracy of the classifier on the test data. # Step 1: Preprocess the dataset (Assume the dataset is already preprocessed to be in the range [0, 1]) # Step 2: Define the RBM model rbm = BernoulliRBM(n_components=100, learning_rate=0.06, batch_size=10, n_iter=10, random_state=0) # Step 3: Create a pipeline with RBM and Logistic Regression logistic = LogisticRegression(solver=\'lbfgs\', max_iter=1000) classifier = Pipeline(steps=[(\'rbm\', rbm), (\'logistic\', logistic)]) # Step 4: Train the model classifier.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = classifier.predict(X_test) accuracy = metrics.accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Title: Intelligent Path Manipulation Utility** **Objective:** Create a utility class in Python that provides a set of methods for manipulating and querying filesystem paths using functions from the `os.path` module. This utility class should demonstrate a comprehensive understanding of various operations that can be performed on paths, including normalization, joining, splitting, and querying path properties. **Description:** Implement a class named `PathUtility` that provides the following methods: 1. `get_absolute_path(relative_path: str) -> str`: - Accepts a relative path and returns its absolute version. 2. `normalize_path(path: str) -> str`: - Accepts a path and returns its normalized version. 3. `join_paths(*paths: str) -> str`: - Accepts multiple path segments and returns a single joined path. 4. `split_path(path: str) -> tuple`: - Accepts a path and returns a tuple containing the directory name and base name. 5. `get_file_extension(path: str) -> str`: - Accepts a path and returns the file extension. 6. `path_exists(path: str) -> bool`: - Checks if the given path exists. 7. `is_file(path: str) -> bool`: - Checks if the given path is an existing regular file. 8. `is_directory(path: str) -> bool`: - Checks if the given path is an existing directory. **Input and Output Formats:** - For `get_absolute_path`: - Input: A relative path as a string. - Output: An absolute path as a string. - For `normalize_path`: - Input: A path as a string. - Output: A normalized path as a string. - For `join_paths`: - Input: Multiple path segments as strings. - Output: A single joined path as a string. - For `split_path`: - Input: A path as a string. - Output: A tuple where the first element is the directory name and the second element is the base name. - For `get_file_extension`: - Input: A path as a string. - Output: The file extension as a string. - For `path_exists`: - Input: A path as a string. - Output: A boolean indicating if the path exists. - For `is_file`: - Input: A path as a string. - Output: A boolean indicating if the path is a regular file. - For `is_directory`: - Input: A path as a string. - Output: A boolean indicating if the path is a directory. **Constraints and Requirements:** - The implementation must use appropriate functions from the `os.path` module. - The methods must handle both Unix-style and Windows-style paths. - The class should be tested with different path scenarios to ensure robustness. **Example:** ```python class PathUtility: @staticmethod def get_absolute_path(relative_path: str) -> str: pass @staticmethod def normalize_path(path: str) -> str: pass @staticmethod def join_paths(*paths: str) -> str: pass @staticmethod def split_path(path: str) -> tuple: pass @staticmethod def get_file_extension(path: str) -> str: pass @staticmethod def path_exists(path: str) -> bool: pass @staticmethod def is_file(path: str) -> bool: pass @staticmethod def is_directory(path: str) -> bool: pass # Example usage: pu = PathUtility() print(pu.get_absolute_path(\'./test\')) print(pu.normalize_path(\'A//B/A/./../B\')) print(pu.join_paths(\'path\', \'to\', \'file.txt\')) print(pu.split_path(\'/path/to/file.txt\')) print(pu.get_file_extension(\'/path/to/file.txt\')) print(pu.path_exists(\'/path/to/file.txt\')) print(pu.is_file(\'/path/to/file.txt\')) print(pu.is_directory(\'/path/to/\')) ``` **Note:** The paths used in the examples should reflect typical cases and edge cases to thoroughly test the utility methods.","solution":"import os class PathUtility: @staticmethod def get_absolute_path(relative_path: str) -> str: return os.path.abspath(relative_path) @staticmethod def normalize_path(path: str) -> str: return os.path.normpath(path) @staticmethod def join_paths(*paths: str) -> str: return os.path.join(*paths) @staticmethod def split_path(path: str) -> tuple: return os.path.split(path) @staticmethod def get_file_extension(path: str) -> str: return os.path.splitext(path)[1] @staticmethod def path_exists(path: str) -> bool: return os.path.exists(path) @staticmethod def is_file(path: str) -> bool: return os.path.isfile(path) @staticmethod def is_directory(path: str) -> bool: return os.path.isdir(path)"},{"question":"# Question **XML Document Modification Using ElementTree** Given an XML document containing a catalog of books, implement a function `update_book_prices(xml_string: str, discount: float) -> str` that reduces the price of each book by a given discount percentage and returns the modified XML document as a string. The XML document has the following structure: ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <!-- More book elements --> </catalog> ``` # Constraints 1. The `discount` parameter is a float representing the discount percentage (e.g., 10 for 10%). 2. The price of each book must be reduced by the discount percentage, and the result should be rounded to two decimal places. 3. The input `xml_string` is guaranteed to be a well-formed XML string. 4. The output must be a well-formed XML string. # Example ```python xml_input = \'\'\'<catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>20.00</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies,</description> </book> </catalog>\'\'\' discount = 10 # 10% discount expected_output = \'\'\'<catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>40.46</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>18.00</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies,</description> </book> </catalog>\'\'\' assert update_book_prices(xml_input, discount) == expected_output ``` # Implementation Notes - Use the `xml.etree.ElementTree` module to parse the given XML string. - Iterate through the book elements, modify the price, and update the XML structure. - Convert the modified XML tree back into a string and return it.","solution":"import xml.etree.ElementTree as ET def update_book_prices(xml_string: str, discount: float) -> str: Reduces the price of each book by the given discount percentage and returns the modified XML string. # Parse the XML string root = ET.fromstring(xml_string) # Calculate the discount multiplier discount_multiplier = (100 - discount) / 100 # Iterate through each book and update the price for book in root.findall(\'book\'): price_element = book.find(\'price\') if price_element is not None: original_price = float(price_element.text) new_price = round(original_price * discount_multiplier, 2) price_element.text = f\\"{new_price:.2f}\\" # Convert the modified XML tree back to a string return ET.tostring(root, encoding=\'unicode\')"},{"question":"# XML Data Manipulation Using `xml.etree.ElementTree` Problem Statement You are provided with an XML document that contains information about a collection of books. Each book entry includes details such as the title, author, genre, price, and publish date. You are required to write a function to parse this XML data, extract specific information, and perform some modifications. Requirements 1. **Function Name**: `process_books_xml` 2. **Input Format**: - A string `xml_data` that represents the XML content of the books collection. 3. **Output Format**: - A list of dictionaries where each dictionary contains book details with keys: `title`, `author`, `genre`, `price`, and `publish_date`. - The XML content after performing modifications: Increase the price of each book by 10% and convert the XML to string. 4. **Constraints**: - Ensure that your function uses the `xml.etree.ElementTree` module to parse and manipulate the XML data. - You are not allowed to use any other XML or HTML parsing libraries. Example XML Data ```xml <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> </book> </catalog> ``` Function Signature ```python import xml.etree.ElementTree as ET def process_books_xml(xml_data: str) -> (list, str): pass ``` Instructions 1. Parse the provided XML data using `ET`. 2. Extract the details of each book and store it in a dictionary with the keys as specified. 3. Return a list of these dictionaries. 4. Modify the price of each book by increasing it by 10%. 5. Convert the modified XML back to a string and return it. Example ```python xml_data = <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> </book> </catalog> result = process_books_xml(xml_data) print(result[0]) # -> [{\'title\': \\"XML Developer\'s Guide\\", \'author\': \'Gambardella, Matthew\', \'genre\': \'Computer\', \'price\': \'44.95\', \'publish_date\': \'2000-10-01\'}, {...}] print(result[1]) # -> Modified XML string with prices increased by 10% ``` The function should be robust and handle any well-formed XML document that conforms to the outlined structure.","solution":"import xml.etree.ElementTree as ET def process_books_xml(xml_data: str): root = ET.fromstring(xml_data) books_info = [] for book in root.findall(\'book\'): details = { \'title\': book.find(\'title\').text, \'author\': book.find(\'author\').text, \'genre\': book.find(\'genre\').text, \'price\': book.find(\'price\').text, \'publish_date\': book.find(\'publish_date\').text } books_info.append(details) # Increase price by 10% price_element = book.find(\'price\') new_price = round(float(price_element.text) * 1.10, 2) price_element.text = f\\"{new_price:.2f}\\" modified_xml = ET.tostring(root, encoding=\'unicode\') return books_info, modified_xml"},{"question":"Objective Assess the student\'s ability to use seaborn for data visualization, focusing on data preprocessing and advanced customizations within seaborn\'s object interface. Question You are given two datasets: `fmri` and `seaice`, provided by the seaborn library. Your task is to preprocess these datasets and create visualizations that demonstrate a comprehensive understanding of seaborn\'s capabilities. 1. **Load and Preprocess Data:** - Load the `fmri` dataset and filter it to include only entries where the `region` is `\'parietal\'`. - Load the `seaice` dataset and transform it as follows: - Extract the day of the year and the year from the `Date` column. - Filter the data to include only years from 1980 onwards. - Pivot the data to have `Day` as the index and `Extent` values for the years 1980 and 2019. 2. **Data Visualization:** - Create a `Band` plot for the `seaice` dataset showing the interval between the years 1980 and 2019. Customize the band with 50% opacity and an edge width of 2. - Create a combined plot for the `fmri` dataset showing an errorband interval along the `timepoint` axis for the `signal` values, grouped by `event`. Combine this with a line plot with default aggregation. Expected Input and Output - **Input:** - No input required, as the datasets are loaded within the code. - **Output:** - Two plots displayed: one for the `seaice` dataset using a `Band` plot, and another combined plot for the `fmri` dataset. Constraints: - Use only seaborn and pandas for data manipulation and visualization. - Ensure your plots have clear labels and legends for better readability. Performance Requirements: - The code should efficiently handle the provided datasets without excessive memory or computational requirements. Example Solution: ```python import seaborn.objects as so from seaborn import load_dataset # Load and preprocess fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and preprocess seaice dataset seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create band plot for seaice dataset p1 = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") p1.add(so.Band(alpha=.5, edgewidth=2)) p1.show() # Create combined plot for fmri dataset p2 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) p2.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Load and preprocess fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and preprocess seaice dataset seaice = load_dataset(\\"seaice\\") seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) seaice[\'Day\'] = seaice[\'Date\'].dt.day_of_year seaice[\'Year\'] = seaice[\'Date\'].dt.year seaice = seaice.query(\\"Year >= 1980\\") seaice_pivot = seaice.pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\")[[1980, 2019]].dropna().reset_index() # Create band plot for seaice dataset p1 = so.Plot(seaice_pivot, x=\\"Day\\", ymin=1980, ymax=2019) p1.add(so.Band(alpha=.5, edgewidth=2)) p1.show() # Create combined plot for fmri dataset p2 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) p2.show()"},{"question":"**Problem Statement:** You are required to implement a Python function that extracts certain features from an AIFF or AIFF-C audio file and writes another AIFF-C file with modified audio data. The features to be extracted include the number of channels, sample width, frame rate, and the number of frames. The modification you will perform on the audio data is simple: reverse the audio samples. **Function Signature:** ```python def process_audio(input_file_path: str, output_file_path: str) -> None: Process the audio file specified by input_file_path and write the modified audio to output_file_path. Parameters: input_file_path (str): The file path to the input AIFF or AIFF-C file. output_file_path (str): The file path where the modified AIFF-C file will be written. Returns: None ``` **Requirements:** 1. Open the input file in read mode using the `aifc` module. 2. Extract the following parameters from the input audio file: - Number of channels - Sample width - Frame rate - Number of frames 3. Read the audio data frames from the input file. 4. Reverse the order of the audio frames. 5. Open the output file in write mode using the `aifc` module. 6. Set the same parameters (number of channels, sample width, frame rate) for the output file. 7. Write the reversed audio data frames to the output file. 8. Ensure the output file uses AIFF-C format. 9. Close both input and output files properly to update file headers correctly. **Constraints:** - The input file will be in AIFF or AIFF-C format. - The function should handle any valid AIFF or AIFF-C file. **Example:** ```python input_file_path = \\"input.aiff\\" output_file_path = \\"output.aifc\\" process_audio(input_file_path, output_file_path) ``` After executing the above function, the file `output.aifc` should contain the reversed audio data from `input.aiff` with the same format and parameters. **Notes:** - The `aifc` module\'s deprecation since Python 3.11 is acknowledged, but you should assume the environment for assessment still supports `aifc`. - Focus on correctly using the `aifc` module methods to achieve the desired functionality. - Handle file operations safely, ensuring files are properly closed after operations. **Hint:** You can use the `readframes` method to read all frames at once and `writeframes` method to write all frames back after reversing them.","solution":"import aifc def process_audio(input_file_path: str, output_file_path: str) -> None: Process the audio file specified by input_file_path and write the modified audio to output_file_path. Parameters: input_file_path (str): The file path to the input AIFF or AIFF-C file. output_file_path (str): The file path where the modified AIFF-C file will be written. Returns: None # Open the input file in read mode with aifc.open(input_file_path, \'r\') as infile: # Extract parameters from the input file num_channels = infile.getnchannels() sample_width = infile.getsampwidth() frame_rate = infile.getframerate() num_frames = infile.getnframes() # Read audio data frames audio_data = infile.readframes(num_frames) # Reverse the audio frames reversed_audio_data = audio_data[::-1] # Open the output file in write mode with aifc.open(output_file_path, \'w\') as outfile: # Set the same parameters for the output file outfile.setnchannels(num_channels) outfile.setsampwidth(sample_width) outfile.setframerate(frame_rate) outfile.setnframes(num_frames) # Write the reversed audio data frames to the output file outfile.writeframes(reversed_audio_data) # Ensure file is in AIFF-C format outfile.aifc = True # Ensures AIFF-C format"},{"question":"Objective Your task is to build a simple file download manager using `asyncio`. The file download manager should be able to handle multiple downloads concurrently, manage the download queue, and handle timeouts and cancellations. Requirements 1. **Function: `download_file(url: str) -> None`** - This function should simulate downloading a file from the given `url` by sleeping asynchronously for a random number of seconds between 1 and 5. - Print a message indicating the start and completion of the download. - Handle the case where the download task is cancelled, and print a message indicating the cancellation. 2. **Function: `manage_downloads(urls: List[str], max_concurrent_downloads: int) -> None`** - This function should manage the download of multiple files concurrently. - Use an asyncio Queue to manage the URLs to be downloaded. - Use a semaphore to limit the number of concurrent downloads to `max_concurrent_downloads`. - For each URL, create a task to download the file. - Implement a timeout for each download task, and handle the timeout scenario by cancelling the task and printing a corresponding message. Input and Output - The `download_file` function takes a single argument `url`, which is a string representing the URL of the file to be downloaded. It does not return any value but prints messages indicating the progress and cancellation of the download. - The `manage_downloads` function takes two arguments: - `urls`: A list of strings, where each string is a URL of a file to be downloaded. - `max_concurrent_downloads`: An integer specifying the maximum number of concurrent downloads allowed. - The function does not return any value but should print messages indicating the progress, completion, timeouts, and cancellations of the downloads. Constraints - You must use `asyncio` for handling asynchronous tasks. - Use `asyncio.Queue` to manage the download queue. - Use `asyncio.Semaphore` to limit the number of concurrent downloads. - Each download task should have a timeout of 4 seconds to simulate network delays. Example ```python import asyncio import random async def download_file(url: str) -> None: try: print(f\\"Starting download from {url}\\") await asyncio.sleep(random.randint(1, 5)) print(f\\"Completed download from {url}\\") except asyncio.CancelledError: print(f\\"Download from {url} was cancelled\\") async def manage_downloads(urls: List[str], max_concurrent_downloads: int) -> None: queue = asyncio.Queue() semaphore = asyncio.Semaphore(max_concurrent_downloads) async def worker(): while True: url = await queue.get() async with semaphore: task = asyncio.create_task(download_file(url)) try: await asyncio.wait_for(task, timeout=4.0) except asyncio.TimeoutError: task.cancel() await task print(f\\"Timed out download from {url}\\") queue.task_done() for url in urls: await queue.put(url) workers = [asyncio.create_task(worker()) for _ in range(max_concurrent_downloads)] await queue.join() for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True) ``` The above code is an example of how the functions should be implemented. In the actual assessment, students are expected to write the implementation of both `download_file` and `manage_downloads` without referring to this example. Notes - Ensure proper exception handling for tasks to deal with timeouts and cancellations. - Use appropriate debug statements to understand the flow and state of the program during execution.","solution":"import asyncio import random from typing import List async def download_file(url: str) -> None: try: print(f\\"Starting download from {url}\\") await asyncio.sleep(random.randint(1, 5)) print(f\\"Completed download from {url}\\") except asyncio.CancelledError: print(f\\"Download from {url} was cancelled\\") async def manage_downloads(urls: List[str], max_concurrent_downloads: int) -> None: queue = asyncio.Queue() semaphore = asyncio.Semaphore(max_concurrent_downloads) async def worker(): while True: url = await queue.get() async with semaphore: task = asyncio.create_task(download_file(url)) try: await asyncio.wait_for(task, timeout=4.0) except asyncio.TimeoutError: task.cancel() await task print(f\\"Timed out download from {url}\\") queue.task_done() for url in urls: await queue.put(url) workers = [asyncio.create_task(worker()) for _ in range(max_concurrent_downloads)] await queue.join() for worker in workers: worker.cancel() await asyncio.gather(*workers, return_exceptions=True)"},{"question":"**Objective:** Implement a text-based user interface using the `curses` library in Python. The application should display a text box where a user can input text, and a status line that shows helpful information. The application should react to user keystrokes to move the cursor, submit the input, and exit the application. **Problem Statement:** Create a `curses` application with the following features: 1. A main window that covers the entire terminal screen. 2. A status line at the top that displays \\"Press \'q\' to quit, \'Enter\' to submit text\\". 3. A text input box in the middle of the screen where the user can type a message. 4. When the user presses \'Enter\', the message should be displayed at the bottom of the screen. 5. The application should exit cleanly when the user presses \'q\'. **Function Signature:** ```python def curses_application(stdscr): pass ``` **Expected Behavior:** 1. Upon launching, the entire screen should be initialized with a status line at the top and an empty input box in the middle. 2. The user can navigate within the input box and type a message. 3. When \'Enter\' is pressed, the inputted message should be displayed at the bottom of the screen. 4. The user can repeatedly type new messages, and each new message should replace the previous one at the bottom of the screen. 5. Pressing \'q\' should terminate the application and restore the terminal to its original state. **Constraints:** - The status line should always be displayed at the top of the screen. - The input box should be centered horizontally and vertically within the screen. - The displayed message at the bottom should update only when \'Enter\' is pressed. **Example Usage:** ```python import curses from curses.textpad import Textbox, rectangle def curses_application(stdscr): # Implement the application logic here # Clear screen stdscr.clear() # Set up status line status_line = \\"Press \'q\' to quit, \'Enter\' to submit text\\" stdscr.addstr(0, 0, status_line, curses.A_REVERSE) # Create input box input_win_y = curses.LINES // 2 input_win_x = (curses.COLS // 2) - 15 input_win = curses.newwin(1, 30, input_win_y, input_win_x) rectangle(stdscr, input_win_y - 1, input_win_x - 1, input_win_y + 1, input_win_x + 30) stdscr.refresh() while True: # Get user input box = Textbox(input_win) box.edit() # Gather input and display it at the bottom of the screen message = box.gather().strip() stdscr.addstr(curses.LINES - 1, 0, message) # Refresh to reflect changes stdscr.refresh() # Check for exit condition c = stdscr.getch() if c == ord(\'q\'): break ``` **Note:** The provided code skeleton only outlines how to create the text box and handle basic input. The implementation must include all the tasks described in the expected behavior. Make sure to handle the initialization and termination of the `curses` environment properly to prevent terminal issues.","solution":"import curses from curses.textpad import Textbox, rectangle def curses_application(stdscr): # Clear the screen stdscr.clear() # Setup status line status_line = \\"Press \'q\' to quit, \'Enter\' to submit text\\" stdscr.addstr(0, 0, status_line, curses.A_REVERSE) # Create an input box that is centered input_win_y = curses.LINES // 2 input_win_x = (curses.COLS // 2) - 15 input_win = curses.newwin(1, 30, input_win_y, input_win_x) rectangle(stdscr, input_win_y - 1, input_win_x - 1, input_win_y + 1, input_win_x + 30) stdscr.refresh() while True: # Create a Textbox object box = Textbox(input_win) input_win.clear() stdscr.refresh() # Let user edit until \'Enter\' is pressed box.edit() # Gather the input text and clear the input box message = box.gather().strip() input_win.clear() stdscr.refresh() # Display the message at the bottom of the screen stdscr.addstr(curses.LINES - 1, 0, message) stdscr.clrtoeol() stdscr.refresh() # Check keys c = stdscr.getch() if c == ord(\'q\'): break def main(): curses.wrapper(curses_application) if __name__ == \\"__main__\\": main()"},{"question":"Given the following `chunked_file` in EA IFF 85 format, read and parse the chunks to extract specific information. Implement a function `extract_chunk_data` that reads a given chunked file and returns a list of dictionaries, where each dictionary contains the chunk ID and the data for each chunk. Here\'s a detailed breakdown of the function requirements: - **Function Signature**: ```python def extract_chunk_data(file_path: str) -> list: pass ``` - **Input**: - `file_path` (str): A string representing the path to a file containing EA IFF 85 formatted data. - **Output**: - Returns (list): A list of dictionaries `{ \'id\': chunk_id, \'data\': chunk_data }`, where: - `chunk_id` (str): The 4-byte ID of the chunk. - `chunk_data` (bytes): The data contained within the chunk. - **Constraints**: - Assume that the file is correctly formatted according to EA IFF 85 specifications. - Handle both aligned and unaligned chunks. - Handle both big-endian and little-endian formats. - **Performance Requirements**: - The implementation should efficiently handle files up to a few hundred megabytes. # Example Assume the example file `example.iff` contains the following chunks: ``` +------------------------------------------------------+ | ID | Size | Data | |------|-------|--------------------------------------| | \\"CHNK\\"| 4 | b\\"data\\" | | \\"DATA\\"| 8 | b\\"x01x02x03x04x05x06x07x08\\" | +------------------------------------------------------+ ``` Calling the function as follows: ```python result = extract_chunk_data(\\"example.iff\\") ``` Should return: ```python [ { \'id\': \'CHNK\', \'data\': b\\"data\\" }, { \'id\': \'DATA\', \'data\': b\\"x01x02x03x04x05x06x07x08\\" } ] ``` # Implementation Notes 1. Use the `chunk` module to read chunks from the file. 2. Ensure the function correctly handles endianness and alignment constraints. 3. Thoroughly test the function to ensure it handles various edge cases, including different chunk sizes and alignments. # Additional Information The function should raise a `ValueError` if any chunk fails to read correctly (e.g., due to a malformed file or unexpected data). Good luck!","solution":"import struct def extract_chunk_data(file_path: str) -> list: Reads a chunked file in EA IFF 85 format and extracts chunk data. :param file_path: Path to the file containing EA IFF 85 formatted data. :return: List of dictionaries with chunk ID and chunk data. chunks = [] with open(file_path, \'rb\') as f: while True: chunk_header = f.read(8) # Read the chunk ID and size (4 bytes each) if len(chunk_header) < 8: break # End of file or corrupted chunk header chunk_id = chunk_header[:4].decode(\'ascii\') chunk_size = struct.unpack(\'>I\', chunk_header[4:])[0] chunk_data = f.read(chunk_size) if len(chunk_data) < chunk_size: raise ValueError(\\"Incomplete chunk data encountered.\\") # Add the chunk data to the list chunks.append({ \'id\': chunk_id, \'data\': chunk_data }) # Ensure file pointer is on an even boundary if chunk_size % 2 != 0: f.seek(1, 1) return chunks"},{"question":"**Objective:** Your task is to demonstrate the ability to use the MPS backend in PyTorch by implementing a simple neural network training process on the `mps` device. **Question:** Write a Python script that performs the following: 1. Checks if MPS is available and prints an appropriate message if it is not. 2. Defines a simple fully connected neural network using the PyTorch `nn.Module`. 3. Moves the defined model to the MPS device. 4. Creates random input data and corresponding labels on the MPS device. 5. Sets up a loss function and an optimizer. 6. Trains the model for a specified number of epochs on the generated data while performing all operations on the MPS device. 7. Prints the loss after each epoch. **Constraints and Requirements:** - Your script should handle cases where MPS is not available and print a meaningful error message. - The input data size should be (100 times 10) and the label size should be (100 times 1). - Use the Mean Squared Error (MSE) loss for training. - Use the Stochastic Gradient Descent (SGD) optimizer for training. - Train the model for 20 epochs. - All tensors and models should be moved to the `mps` device if available. **Expected Input and Output Formats:** There are no explicit input and output parameters, as this is a script to be executed. The expected output consists of printed statements indicating the availability of MPS and the loss value after each epoch. **Performance Requirements:** - Ensure all operations (tensor creation, model training, etc.) are performed on the GPU if the MPS device is available. Here\'s a skeleton of the code to help you get started: ```python import torch import torch.nn as nn import torch.optim as optim # Check for MPS availability if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not \\" \\"built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ \\" \\"and/or you do not have an MPS-enabled device on this machine.\\") else: mps_device = torch.device(\\"mps\\") # Define a simple fully connected neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize the model and move it to the MPS device model = SimpleNet().to(mps_device) # Create random input data and labels on the MPS device data = torch.randn(100, 10, device=mps_device) labels = torch.randn(100, 1, device=mps_device) # Set up loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop num_epochs = 20 for epoch in range(num_epochs): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\') ``` **Note:** Ensure that your script handles scenarios where MPS is not available gracefully and provides informative messages to the user.","solution":"import torch import torch.nn as nn import torch.optim as optim def run_training(): # Check for MPS availability if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return mps_device = torch.device(\\"mps\\") # Define a simple fully connected neural network class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Initialize the model and move it to the MPS device model = SimpleNet().to(mps_device) # Create random input data and labels on the MPS device data = torch.randn(100, 10, device=mps_device) labels = torch.randn(100, 1, device=mps_device) # Set up loss function and optimizer criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop num_epochs = 20 for epoch in range(num_epochs): optimizer.zero_grad() outputs = model(data) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\')"},{"question":"**Question: Implementing a Reference Counting System** In this task, you are required to implement a custom reference counting system in Python. This will help you understand how Python\'s internal memory management for reference counting works. Your task is to implement a `RefCountedObject` class with the following methods: 1. **`__init__(self, obj)`**: Initialize the object with the given `obj` and set up the reference count. 2. **`ref(self)`**: Increment the reference count for the object. 3. **`unref(self)`**: Decrement the reference count for the object. If the reference count drops to zero, delete the object. 4. **`get_ref_count(self)`**: Return the current reference count for the object. Additionally, provide a method to perform the equivalent of `Py_XINCREF` and `Py_XDECREF`: 5. **`xref(self)`**: Increment the reference count only if the object is not `None`. 6. **`xunref(self)`**: Decrement the reference count only if the object is not `None`. # Input and Output - You will start by creating a `RefCountedObject` with an initial object. - You will call the methods `ref()`, `unref()`, `xref()`, `xunref()`, and `get_ref_count()` on that object. - The output should be the reference count after each operation. # Constraints - Your implementation should handle edge cases, such as decrementing the reference count of an already deleted object gracefully (e.g., by doing nothing). # Example ```python # Create an object wrapped in RefCountedObject obj = RefCountedObject(\\"example\\") # Initial reference count should be 1 print(obj.get_ref_count()) # Output: 1 # Increment reference count obj.ref() print(obj.get_ref_count()) # Output: 2 # Decrement reference count obj.unref() print(obj.get_ref_count()) # Output: 1 # Safely increment reference count (like Py_XINCREF) obj.xref() print(obj.get_ref_count()) # Output: 2 # Safely decrement reference count (like Py_XDECREF) obj.xunref() print(obj.get_ref_count()) # Output: 1 # Decrement reference count to zero obj.unref() # Should handle deletion, so additional unref shouldn\'t cause errors obj.unref() # This should do nothing print(obj.get_ref_count()) # Output should be 0, but obj is effectively deleted ``` # Implementation Requirements 1. You must handle the deletion of the object gracefully. 2. Ensure that the code does not raise exceptions for simple cases of incrementing or decrementing a deleted object. Good luck!","solution":"class RefCountedObject: def __init__(self, obj): self._obj = obj self._ref_count = 1 def ref(self): if self._ref_count > 0: self._ref_count += 1 def unref(self): if self._ref_count > 0: self._ref_count -= 1 if self._ref_count == 0: self._delete() def get_ref_count(self): return self._ref_count def xref(self): if self._obj is not None: self.ref() def xunref(self): if self._obj is not None: self.unref() def _delete(self): self._obj = None"},{"question":"Objective: Your task is to implement a Python function that replicates the functionality of adding custom site-specific directories to the `sys.path` based on custom configuration files. This task will help assess your understanding of handling system paths and managing site-specific setups. Instructions: 1. Implement a function `add_custom_sitedir(config_files)`, where `config_files` is a list of paths to configuration files `.pth`. 2. Each `.pth` file may contain: - Blank lines or lines starting with `#` to be ignored. - Lines containing paths to be added to `sys.path`. Paths must be added only if they exist. - Lines starting with `import` (followed by a space or tab), that should be executed. 3. Ensure no duplicate paths are added to `sys.path`. 4. Provide appropriate error handling for file reading and path validity checks. 5. Finally, test your function with different configurations to ensure it works as expected. Function Definition: ```python def add_custom_sitedir(config_files): Add custom directories to sys.path based on provided .pth configuration files. Parameters: config_files (list of str): List of paths to .pth files. Requirements: - Only valid and existing paths should be added to sys.path. - Ensure paths are not duplicated in sys.path. - Execute lines starting with \'import\' in the given configuration files. Returns: None # Your code here ``` Example Usage: ```python # Assuming the following .pth files: # foo.pth: # /existing/path/foo # /non_existing/path # import os # bar.pth: # # bar package configuration # /existing/path/bar # Your current sys.path before running the function: # sys.path == [\\"/some/default/path\\"] import sys from pathlib import Path # Create example paths for testing Path(\\"/existing/path/foo\\").mkdir(parents=True, exist_ok=True) Path(\\"/existing/path/bar\\").mkdir(parents=True, exist_ok=True) # Create example .pth files with open(\\"foo.pth\\", \\"w\\") as f: f.write(\\"/existing/path/foon/non_existing/pathnimport osn\\") with open(\\"bar.pth\\", \\"w\\") as f: f.write(\\"# bar package configurationn/existing/path/barn\\") # Run the function config_files = [\\"foo.pth\\", \\"bar.pth\\"] add_custom_sitedir(config_files) # Check the sys.path after running the function print(sys.path) # Should print (among other existing paths): # [\\"/some/default/path\\", \\"/existing/path/foo\\", \\"/existing/path/bar\\"] ``` Submit your implementation along with any test cases you used to verify your solution. Constraints: - Ensure you handle file reading and path validation gracefully. - Do not modify any global state other than `sys.path` directly. - Optimize for readability and maintainability.","solution":"import sys import os def add_custom_sitedir(config_files): Add custom directories to sys.path based on provided .pth configuration files. Parameters: config_files (list of str): List of paths to .pth files. Requirements: - Only valid and existing paths should be added to sys.path. - Ensure paths are not duplicated in sys.path. - Execute lines starting with \'import\' in the given configuration files. Returns: None added_paths = set() for file in config_files: try: with open(file, \\"r\\") as f: for line in f: # Strip any leading and trailing whitespace line = line.strip() # Skip blank lines or comments if not line or line.startswith(\'#\'): continue # Handle import lines if line.startswith(\'import \'): exec(line) else: # Ensure the path exists and is not already in added_paths if os.path.exists(line) and line not in added_paths: sys.path.append(line) added_paths.add(line) except IOError as e: print(f\\"Error reading file {file}: {e}\\")"},{"question":"**Question: Building a Scalable TCP Server Using `epoll`** **Objective:** Utilize the `epoll` functionality in the `select` module to implement a scalable TCP server that can handle multiple client connections efficiently. **Problem Statement:** You are required to implement a TCP server in Python that can handle multiple client connections simultaneously without unnecessary blocking. The server must use the `epoll` interface provided by the `select` module for I/O multiplexing. **Requirements:** 1. Implement a class named `EpollServer` with the following methods: - `__init__(self, host: str, port: int)`: Initializes the server with the given host and port. - `start(self)`: Starts the server, binds to the specified port, and begins listening for incoming connections. - `run(self)`: The main loop of the server where it handles incoming connections and data using `epoll`. - `stop(self)`: Gracefully stops the server. 2. The server should be able to: - Accept new incoming connections. - Read data from connected clients without blocking. - Send responses back to clients. - Handle disconnections gracefully. 3. Use the `epoll` object for managing the file descriptors. 4. For simplicity, the server will echo any received data back to the client. **Input Format:** - Host: A string specifying the hostname or IP address to bind the server. - Port: An integer specifying the port number to bind the server. **Output Format:** - No specific output format. The server should echo received data back to the client. **Constraints:** - You can assume the maximum number of simultaneous connections to be a manageable number (e.g., 1000). - Properly handle edge cases such as client disconnections and errors. **Example Usage:** ```python if __name__ == \\"__main__\\": server = EpollServer(\'127.0.0.1\', 8888) server.start() try: server.run() except KeyboardInterrupt: server.stop() ``` Your task is to provide the complete implementation of the `EpollServer` class. **Hints:** - You may find methods like `epoll.register()`, `epoll.modify()`, `epoll.unregister()`, and `epoll.poll()` useful. - Remember to handle the cleanup of file descriptors and other resources. **Note:** This problem requires a good understanding of network programming and the `select` module\'s `epoll` functionality. Ensure your solution is efficient and follows best practices for network server implementation.","solution":"import socket import select class EpollServer: def __init__(self, host: str, port: int): self.host = host self.port = port self.epoll = select.epoll() self.sock = None self.fd_to_socket = {} self.active = False def start(self): self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) self.sock.bind((self.host, self.port)) self.sock.listen(100) self.sock.setblocking(0) self.epoll.register(self.sock.fileno(), select.EPOLLIN) self.fd_to_socket[self.sock.fileno()] = self.sock self.active = True print(\\"Server started on {}:{}\\".format(self.host, self.port)) def run(self): try: while self.active: events = self.epoll.poll(1) for fd, event in events: if fd == self.sock.fileno(): conn, addr = self.sock.accept() conn.setblocking(0) self.epoll.register(conn.fileno(), select.EPOLLIN) self.fd_to_socket[conn.fileno()] = conn print(\\"Connection from\\", addr) elif event & select.EPOLLIN: conn = self.fd_to_socket[fd] data = conn.recv(1024) if data: self.epoll.modify(fd, select.EPOLLOUT) conn.send(data) # Echo received data back to the client else: self.epoll.unregister(fd) conn.close() del self.fd_to_socket[fd] elif event & select.EPOLLOUT: conn = self.fd_to_socket[fd] self.epoll.modify(fd, select.EPOLLIN) finally: self.stop() def stop(self): self.active = False for fd in self.fd_to_socket: self.epoll.unregister(fd) self.fd_to_socket[fd].close() self.epoll.close() print(\\"Server stopped\\")"},{"question":"As a developer, your task is to implement a PyTorch-based neural network model to perform a simple binary classification. The model and data should leverage the MPS backend to utilize the GPU on a macOS device with Metal Performance Shaders. Requirements: 1. **Check MPS Availability**: Implement a check to verify if the MPS device is available on the current machine. 2. **Data Preparation**: Create a synthetic dataset with two classes using tensor operations. 3. **Model Definition**: Define a simple neural network model for binary classification. 4. **Device Transfer**: Ensure the dataset and model are transferred to the MPS device. 5. **Train the Model**: Implement a simple training loop that trains the model on the synthetic dataset using the MPS backend. Input None (The model and data preparation are part of the script). Output Print the training loss for each epoch. Constraints - Use only PyTorch and its related functions. - Ensure the code runs without errors if MPS is available. Performance Requirements - The solution should handle the synthetic dataset efficiently utilizing the MPS backend. Detailed Steps 1. **MPS Availability Check**: Implement the logic to check if the MPS device is available, and if not, print an appropriate message and exit. 2. **Synthetic Dataset Creation**: - Generate two sets of points for two classes. - Create labels for these points. 3. **Model Definition**: Define a simple feed-forward neural network with one hidden layer. 4. **Transfer to MPS Device**: Move the tensors and model to the MPS device. 5. **Training Loop**: - Implement a loop that runs for a specified number of epochs. - In each epoch, perform forward pass, compute loss, backpropagate, and update the model parameters. - Print the loss at the end of each epoch. Example ```python import torch import torch.nn as nn import torch.optim as optim # Check that MPS is available if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not \\" \\"built with MPS enabled.\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ \\" \\"and/or you do not have an MPS-enabled device on this machine.\\") exit() mps_device = torch.device(\\"mps\\") # Create synthetic dataset num_samples = 100 x_class_0 = torch.randn(num_samples, 2) - 2 x_class_1 = torch.randn(num_samples, 2) + 2 y_class_0 = torch.zeros(num_samples) y_class_1 = torch.ones(num_samples) x_data = torch.cat((x_class_0, x_class_1), dim=0).to(mps_device) y_data = torch.cat((y_class_0, y_class_1), dim=0).to(mps_device) # Define the model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(2, 128) self.fc2 = nn.Linear(128, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) return x model = SimpleNN().to(mps_device) criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) # Training loop num_epochs = 20 for epoch in range(num_epochs): model.train() optimizer.zero_grad() outputs = model(x_data).squeeze() loss = criterion(outputs, y_data) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\\") print(\\"Training completed.\\") ```","solution":"import torch import torch.nn as nn import torch.optim as optim def check_mps_available(): Check if the MPS device is available on this machine. if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): raise EnvironmentError(\\"MPS not available because the current PyTorch install was not built with MPS enabled.\\") else: raise EnvironmentError(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine.\\") return torch.device(\\"mps\\") def create_synthetic_data(num_samples=100): Create a synthetic dataset with two classes. x_class_0 = torch.randn(num_samples, 2) - 2 x_class_1 = torch.randn(num_samples, 2) + 2 y_class_0 = torch.zeros(num_samples) y_class_1 = torch.ones(num_samples) x_data = torch.cat((x_class_0, x_class_1), dim=0) y_data = torch.cat((y_class_0, y_class_1), dim=0) return x_data, y_data class SimpleNN(nn.Module): Simple neural network model for binary classification. def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(2, 128) self.fc2 = nn.Linear(128, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) return x def train_model(num_epochs=20, learning_rate=0.001): Train the neural network model using a simple training loop. # Check for MPS availability mps_device = check_mps_available() # Create synthetic dataset x_data, y_data = create_synthetic_data() # Transfer data to MPS device x_data = x_data.to(mps_device) y_data = y_data.to(mps_device) # Define model, loss function, and optimizer model = SimpleNN().to(mps_device) criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=learning_rate) for epoch in range(num_epochs): model.train() optimizer.zero_grad() outputs = model(x_data).squeeze() loss = criterion(outputs, y_data) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\\") print(\\"Training completed.\\") if __name__ == \\"__main__\\": train_model()"},{"question":"# CSV Data Transformation with Custom Dialects You are given a CSV file named `students.csv` containing student records. Each record consists of the fields: `ID`, `Name`, `Age`, `Grade`, and `Subjects`. The Subjects field contains a list of subjects the student is enrolled in, separated by a semicolon (`;`). Your task is to write a Python function that reads the CSV file, transforms the data, and writes it to a new CSV file `transformed_students.csv`. The transformation requirements are as follows: 1. Use custom dialects to read and write the CSV files. 2. Split the `Subjects` field into individual fields for each subject. 3. Normalize the `Grade` field to uppercase. 4. Handle any missing fields by replacing them with the string `\\"N/A\\"`. Input - The input CSV file `students.csv` in the following format: ``` ID,Name,Age,Grade,Subjects 1,Alice,14,A,a;math;science 2,Bob,15,B,b;history;art 3,Charles,14,A,math;science ``` - The input format CSV may vary in delimiters, quote characters, etc. Output - The output CSV file `transformed_students.csv` should be in the following format: ``` ID,Name,Age,Grade,Subject1,Subject2,Subject3 1,Alice,14,A,A,math,science 2,Bob,15,B,B,history,art 3,Charles,14,A,math,science ``` # Constraints - Assume a maximum of 3 subjects per student. - Use custom dialects for reading and writing CSV files. Function Signature ```python def transform_csv(input_file: str, output_file: str) -> None: pass ``` # Example ```python transform_csv(\'students.csv\', \'transformed_students.csv\') ``` The content of `transformed_students.csv` should be: ``` ID,Name,Age,Grade,Subject1,Subject2,Subject3 1,Alice,14,A,a,math,science 2,Bob,15,B,b,history,art 3,Charles,14,A,math,science ``` Please ensure your solution reads the input file, applies the necessary transformations, and writes the output file correctly. Make sure to handle different CSV formats using the appropriate custom dialects.","solution":"import csv def transform_csv(input_file: str, output_file: str) -> None: # Define a custom reading dialect csv.register_dialect(\'custom_read\', delimiter=\',\', quoting=csv.QUOTE_MINIMAL) # Define a custom writing dialect csv.register_dialect(\'custom_write\', delimiter=\',\', quoting=csv.QUOTE_MINIMAL) with open(input_file, newline=\'\', mode=\'r\') as infile: reader = csv.DictReader(infile, dialect=\'custom_read\') fieldnames = [\'ID\', \'Name\', \'Age\', \'Grade\', \'Subject1\', \'Subject2\', \'Subject3\'] transformed_data = [] for row in reader: transformed_row = { \'ID\': row.get(\'ID\', \'N/A\'), \'Name\': row.get(\'Name\', \'N/A\'), \'Age\': row.get(\'Age\', \'N/A\'), \'Grade\': row.get(\'Grade\', \'N/A\').upper(), # Normalize Grade to uppercase } subjects = row.get(\'Subjects\', \'\').split(\';\') subjects = [subject.strip() for subject in subjects if subject.strip()] for i in range(1, 4): subject_key = f\'Subject{i}\' transformed_row[subject_key] = subjects[i-1] if i <= len(subjects) else \'N/A\' transformed_data.append(transformed_row) with open(output_file, newline=\'\', mode=\'w\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames, dialect=\'custom_write\') writer.writeheader() writer.writerows(transformed_data)"},{"question":"# Functional Programming Challenge: Analyzing a Data Stream You are asked to process a stream of data and perform various operations using Python\'s functional programming modules (`itertools`, `functools`, `operator`). Given a list of transactions where each transaction is a tuple containing: 1. Transaction ID (string) 2. Timestamp (string in the format YYYY-MM-DD) 3. Amount (float) You need to perform the following tasks: 1. **Filter transactions**: Create a function `filter_transactions` that takes a list of transactions and a date threshold and returns only those transactions that occurred after the given date. 2. **Calculate totals by date**: Create a function `total_by_date` that takes a list of transactions and returns a dictionary where the keys are dates, and the values are the total transaction amounts for those dates. 3. **Find top N transactions**: Create a function `top_n_transactions` that returns the top N transactions with the highest amounts. 4. **Cumulative total**: Create a function `cumulative_total` that returns an iterator yielding the cumulative total of transaction amounts as they are processed. ```python from typing import List, Tuple, Dict, Iterator import itertools import functools import operator # Sample transaction data type Transaction = Tuple[str, str, float] def filter_transactions(transactions: List[Transaction], date_threshold: str) -> List[Transaction]: Returns a list of transactions that occurred after the given date. # Your implementation here def total_by_date(transactions: List[Transaction]) -> Dict[str, float]: Returns a dictionary with dates as keys and the total transaction amounts for each date as values. # Your implementation here def top_n_transactions(transactions: List[Transaction], n: int) -> List[Transaction]: Returns the top N transactions with the highest amounts. # Your implementation here def cumulative_total(transactions: Iterator[Transaction]) -> Iterator[float]: Returns an iterator that yields the cumulative total of transaction amounts. # Your implementation here # Example usage: transactions = [ (\\"txn1\\", \\"2023-01-01\\", 100.0), (\\"txn2\\", \\"2023-01-02\\", 200.0), (\\"txn3\\", \\"2023-01-03\\", 150.0), ] filtered = filter_transactions(transactions, \\"2023-01-01\\") totals = total_by_date(transactions) top_transactions = top_n_transactions(transactions, 2) cumulative = list(cumulative_total(iter(transactions))) print(filtered) print(totals) print(top_transactions) print(cumulative) ``` Constraints: - The input list of transactions can be large (up to 100,000 transactions). - The timestamp format will always be valid and consistent. - The amounts are positive floats. Performance Requirements: - The functions should be efficient in terms of both time and space complexity. - You should leverage functional programming tools (`itertools`, `functools`, `operator`) effectively to meet these requirements. Input and Output: - **filter_transactions**: - Input: transactions, date_threshold (str) - Output: List of filtered transactions - **total_by_date**: - Input: transactions - Output: Dictionary with date keys and total amount values - **top_n_transactions**: - Input: transactions, N (int) - Output: List of top N transactions - **cumulative_total**: - Input: Iterator of transactions - Output: Iterator of cumulative totals Implement the functions above to demonstrate your understanding of Python\'s functional programming tools.","solution":"from typing import List, Tuple, Dict, Iterator import itertools import functools import operator # Sample transaction data type Transaction = Tuple[str, str, float] def filter_transactions(transactions: List[Transaction], date_threshold: str) -> List[Transaction]: Returns a list of transactions that occurred after the given date. return list(filter(lambda txn: txn[1] > date_threshold, transactions)) def total_by_date(transactions: List[Transaction]) -> Dict[str, float]: Returns a dictionary with dates as keys and the total transaction amounts for each date as values. totals = {} for txn in transactions: date, amount = txn[1], txn[2] if date in totals: totals[date] += amount else: totals[date] = amount return totals def top_n_transactions(transactions: List[Transaction], n: int) -> List[Transaction]: Returns the top N transactions with the highest amounts. return sorted(transactions, key=lambda txn: txn[2], reverse=True)[:n] def cumulative_total(transactions: Iterator[Transaction]) -> Iterator[float]: Returns an iterator that yields the cumulative total of transaction amounts. return itertools.accumulate(map(lambda txn: txn[2], transactions))"},{"question":"# Assessment Question Problem Statement You are required to implement a simple Producer-Consumer pattern using Python\'s `threading` module. The producer thread will generate a sequence of integers and place them in a shared buffer, while the consumer thread will read and process these integers from the buffer. You must ensure that access to the buffer is synchronized such that no race conditions occur. Requirements 1. **Producer**: - Generate integers from 1 to 50. - Use a method `produce` in the Producer class to place integers in the buffer. - Wait for a few seconds after adding each integer to the buffer. 2. **Consumer**: - Continuously check if an integer is available in the buffer to consume. - Use a method `consume` in the Consumer class to retrieve and print the integers from the buffer. 3. **Synchronization**: - Use a `Lock` to ensure that the buffer is only accessed by one thread at a time. - Use a `Condition` to notify the consumer thread when a new integer has been added to the buffer. 4. **Buffer**: - Use a list to simulate the buffer. Implementation Details - Implement classes `Producer` and `Consumer` with respective methods `produce` and `consume`. - Ensure that the `Producer` notifies the `Consumer` whenever a new integer is added to the buffer. - Use threading, locking, and condition variables appropriately to manage buffer access and synchronization. Boilerplate Code ```python import threading import time class Buffer: def __init__(self): self.buffer = [] self.lock = threading.Lock() self.condition = threading.Condition(self.lock) class Producer(threading.Thread): def __init__(self, buffer): super().__init__() self.buffer = buffer def produce(self): for i in range(1, 51): with self.buffer.condition: self.buffer.buffer.append(i) print(f\\"Produced: {i}\\") self.buffer.condition.notify() time.sleep(0.1) def run(self): self.produce() class Consumer(threading.Thread): def __init__(self, buffer): super().__init__() self.buffer = buffer def consume(self): while True: with self.buffer.condition: while not self.buffer.buffer: self.buffer.condition.wait() item = self.buffer.buffer.pop(0) print(f\\"Consumed: {item}\\") def run(self): self.consume() if __name__ == \\"__main__\\": shared_buffer = Buffer() producer = Producer(shared_buffer) consumer = Consumer(shared_buffer) producer.start() consumer.start() producer.join() consumer.join() ``` Expected Output The output should reflect the producer generating and adding integers to the buffer, while the consumer retrieves and processes these integers. The exact order might vary slightly due to the nature of threading but there should be no race conditions, ensuring all integers from 1 to 50 are both produced and consumed. ```plaintext Produced: 1 Consumed: 1 Produced: 2 Consumed: 2 ... Produced: 50 Consumed: 50 ``` Constraints - The implementation must avoid any race conditions. - Properly use threading primitives to synchronize the access to the buffer. - The consumer should wait until an item is available in the buffer before trying to consume it.","solution":"import threading import time class Buffer: def __init__(self): self.buffer = [] self.lock = threading.Lock() self.condition = threading.Condition(self.lock) class Producer(threading.Thread): def __init__(self, buffer): super().__init__() self.buffer = buffer def produce(self): for i in range(1, 51): with self.buffer.condition: self.buffer.buffer.append(i) print(f\\"Produced: {i}\\") self.buffer.condition.notify() time.sleep(0.1) def run(self): self.produce() class Consumer(threading.Thread): def __init__(self, buffer): super().__init__() self.buffer = buffer def consume(self): while True: with self.buffer.condition: while not self.buffer.buffer: self.buffer.condition.wait() item = self.buffer.buffer.pop(0) print(f\\"Consumed: {item}\\") if item == 50: break def run(self): self.consume() if __name__ == \\"__main__\\": shared_buffer = Buffer() producer = Producer(shared_buffer) consumer = Consumer(shared_buffer) producer.start() consumer.start() producer.join() consumer.join()"},{"question":"# Question: Advanced Seaborn Visualization Context Seaborn\'s new object-oriented interface provides powerful tools to create and customize complex visualizations. Your task is to create a customized bar plot using seaborn\'s `seaborn.objects` interface, with specific transformations to handle overlapping data points and to improve plot readability. Dataset You\'ll be using the `tips` dataset, which contains information about the tips received by waitstaff in a restaurant. The dataset includes several columns, but the primary columns you will use are: - `total_bill`: The total bill amount. - `time`: The time of the day when the order was made (`Lunch` or `Dinner`). - `day`: The day of the week. - `sex`: The gender of the person paying the bill. - `smoker`: Whether the person is a smoker or not. Task 1. Load the `tips` dataset using seaborn\'s built-in `load_dataset` function. 2. Create a bar plot visualizing the sum of the `total_bill` for each `day` of the week, categorized by `time` (Lunch/Dinner) and further split by `sex` and `smoker` status. 3. Use the `Dodge` and `Jitter` transforms to avoid overlapping data points and increase readability. 4. Customize the plot to handle empty spaces where certain combinations of `day`, `time`, `sex`, and `smoker` do not exist. Requirements - Your function should be named `create_custom_plot()`. - The function should take no parameters. - The function should: - Load the dataset. - Create the specified plot. - Customize the plot as stated. Constraints - Ensure that the plot is clear and readable with appropriate labels and titles. - Avoid using any deprecated functions. - The function should display the plot as output. Function Signature ```python def create_custom_plot(): pass ``` Example Output The output should be a matplotlib plot window with the customized bar plot as specified.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the bar plot plot = ( so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"time\\") .add(so.Bar(), so.Dodge(), so.Jitter(.1)) .facet(\\"sex\\", \\"smoker\\") .scale(y=\\"log\\") .label(title=\\"Total Bill by Day, Time, Sex, and Smoker Status\\", x=\\"Day of the Week\\", y=\\"Total Bill ()\\") ) # Show the plot plot.show()"},{"question":"# SAX XML Parsing in Python with Custom Handlers Objective: To assess the understanding of SAX XML parsing and the implementation of custom SAX handler classes in Python. Problem: You are given an XML file called `books.xml` that contains information about a collection of books. Your task is to write a custom `ContentHandler` subclass to parse the XML file and extract information about each book, such as title, author, and genre. Additionally, handle any errors or warnings that may occur during the parsing process using a custom `ErrorHandler`. XML Sample (`books.xml`): ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <catalog> <book id=\\"bk101\\"> <author>John Doe</author> <title>XML Programming</title> <genre>Technology</genre> </book> <book id=\\"bk102\\"> <author>Jane Smith</author> <title>Python for Beginners</title> <genre>Education</genre> </book> <!-- more book entries --> </catalog> ``` Task: 1. Create a custom subclass of `xml.sax.handler.ContentHandler` to handle the following events: - `startElement` to detect the beginning of an element. - `endElement` to detect the end of an element. - `characters` to capture the character data inside an element. 2. Create a custom subclass of `xml.sax.handler.ErrorHandler` to handle warnings, recoverable errors, and fatal errors. 3. Use these custom handlers to parse the provided `books.xml` file and print out the information for each book in the format: ``` Book ID: bk101 Title: XML Programming Author: John Doe Genre: Technology ``` Constraints: - Handle the parsing errors gracefully using the custom error handler. - Assume that the XML file is well-formed but may have missing or extra elements. Example Output: ``` Book ID: bk101 Title: XML Programming Author: John Doe Genre: Technology Book ID: bk102 Title: Python for Beginners Author: Jane Smith Genre: Education ``` Input: The XML file `books.xml` is provided in the local directory. Output: Printed book information as shown in the example above. ```python import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \'\' self.book = {} self.books = [] def startElement(self, name, attrs): self.current_element = name if name == \'book\': self.book = {\'id\': attrs[\'id\']} def endElement(self, name): if name == \'book\': self.books.append(self.book) self.book = {} def characters(self, content): if self.current_element == \'title\': self.book[\'title\'] = content elif self.current_element == \'author\': self.book[\'author\'] = content elif self.current_element == \'genre\': self.book[\'genre\'] = content class MyErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Recoverable error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") if __name__ == \\"__main__\\": parser = xml.sax.make_parser() handler = MyContentHandler() error_handler = MyErrorHandler() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) parser.parse(open(\'books.xml\', \'r\')) for book in handler.books: print(f\\"Book ID: {book[\'id\']}\\") print(f\\"Title: {book[\'title\']}\\") print(f\\"Author: {book[\'author\']}\\") print(f\\"Genre: {book[\'genre\']}\\") print() ``` In this task, you will demonstrate your understanding of the SAX XML parsing concepts by implementing custom handlers for parsing the given XML data and handling parsing errors.","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.current_element = \'\' self.book = {} self.books = [] def startElement(self, name, attrs): self.current_element = name if name == \'book\': self.book = {\'id\': attrs[\'id\']} def endElement(self, name): if name == \'book\': self.books.append(self.book) self.book = {} self.current_element = \'\' def characters(self, content): content = content.strip() if content: if self.current_element == \'title\': self.book[\'title\'] = content elif self.current_element == \'author\': self.book[\'author\'] = content elif self.current_element == \'genre\': self.book[\'genre\'] = content class MyErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Recoverable error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_books(xml_file): parser = xml.sax.make_parser() handler = MyContentHandler() error_handler = MyErrorHandler() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) parser.parse(xml_file) return handler.books if __name__ == \\"__main__\\": books = parse_books(open(\'books.xml\', \'r\')) for book in books: print(f\\"Book ID: {book[\'id\']}\\") print(f\\"Title: {book[\'title\']}\\") print(f\\"Author: {book[\'author\']}\\") print(f\\"Genre: {book[\'genre\']}\\") print()"},{"question":"# Custom Serialization with `copyreg` Module The `copyreg` module allows you to define custom pickling behavior for specific objects, enabling you to control how these objects are serialized and deserialized. Objective Your task is to create a custom class that involves a nested structure and then implement custom pickling behavior for this class using the `copyreg` module. This will demonstrate your understanding of custom serialization in Python. Class Definition Define a class `Person` where each person has a name and a list of friends who are also `Person` instances. Implement methods to add and print friends. Custom Pickling Because a `Person` instance can reference other `Person` instances forming a nested structure, you need to ensure that the pickling and unpickling process handles these references correctly. Use the `copyreg` module to register a custom pickling function for the `Person` class. Constraints - The name of a `Person` should be a non-empty string. - The list of friends can include up to 50 `Person` instances, ensuring it\'s not too large for practical pickling. Input You need to create instances of the `Person` class, where one person can be friends with another, forming a structure. Then, pickle and unpickle an instance to validate your implementation. Output Verify that the unpickled instance has the same structure and data as the original instance, including the nested `Person` instances in the friends list. Performance Ensure the custom pickling and unpickling operations are efficient and correctly handle the nested references without causing infinite loops or excessive memory usage. Example ```python import copyreg, pickle class Person: def __init__(self, name): self.name = name self.friends = [] def add_friend(self, friend): self.friends.append(friend) def __repr__(self): return f\\"Person({self.name}, Friends: {[friend.name for friend in self.friends]})\\" # Define the custom pickling function for Person def pickle_person(person): # Ensure we\'re only including necessary attributes for reconstruction return Person, (person.name,) # Register the custom pickling function copyreg.pickle(Person, pickle_person) # Test the implementation if __name__ == \\"__main__\\": p1 = Person(\\"Alice\\") p2 = Person(\\"Bob\\") p3 = Person(\\"Charlie\\") p1.add_friend(p2) p2.add_friend(p3) # Serialize serialized_p1 = pickle.dumps(p1) # Deserialize deserialized_p1 = pickle.loads(serialized_p1) print(\\"Original instance:\\", p1) print(\\"Deserialized instance:\\", deserialized_p1) ``` In your implementation: 1. Complete the definition of the `Person` class. 2. Implement the `pickle_person` function to handle both the name and friends list. 3. Ensure that after deserialization, the nested structure and data integrity are maintained.","solution":"import copyreg import pickle class Person: def __init__(self, name): self.name = name self.friends = [] def add_friend(self, friend): self.friends.append(friend) def __repr__(self): return f\\"Person({self.name}, Friends: {[friend.name for friend in self.friends]})\\" def pickle_person(person): # We need to handle the name and the list of friends return unpickle_person, (person.name, person.friends) def unpickle_person(name, friends): person = Person(name) person.friends = friends return person # Register the custom pickling function copyreg.pickle(Person, pickle_person) # Test the implementation if __name__ == \\"__main__\\": p1 = Person(\\"Alice\\") p2 = Person(\\"Bob\\") p3 = Person(\\"Charlie\\") p1.add_friend(p2) p2.add_friend(p3) p1.add_friend(p3) # Serialize serialized_p1 = pickle.dumps(p1) # Deserialize deserialized_p1 = pickle.loads(serialized_p1) print(\\"Original instance:\\", p1) print(\\"Deserialized instance:\\", deserialized_p1)"},{"question":"**Objective**: Implement a function that initializes the weights of a given neural network model using the specified initialization scheme. After initializing the weights, the function should validate the initialization by returning the mean and standard deviation of the weights of each layer. Function Signature: ```python def initialize_and_validate(model, init_scheme): Initializes the weights of the given model using the specified initialization scheme and returns the mean and standard deviation of the weights for each layer. :param model: torch.nn.Module The neural network model whose weights need to be initialized. :param init_scheme: str The initialization scheme to use. Must be one of: \'uniform\', \'normal\', \'constant\', \'ones\', \'zeros\', \'eye\', \'dirac\', \'xavier_uniform\', \'xavier_normal\', \'kaiming_uniform\', \'kaiming_normal\', \'trunc_normal\', \'orthogonal\', \'sparse\' :return: dict A dictionary where keys are layer names and values are tuples of (mean, std) representing the mean and standard deviation of the initialized weights of each layer. pass ``` Input: - `model`: A PyTorch neural network model (an instance of `torch.nn.Module`). The layers of this model that have weights should be initialized. - `init_scheme`: A string specifying which initialization scheme to use. Valid options include: `\'uniform\'`, `\'normal\'`, `\'constant\'`, `\'ones\'`, `\'zeros\'`, `\'eye\'`, `\'dirac\'`, `\'xavier_uniform\'`, `\'xavier_normal\'`, `\'kaiming_uniform\'`, `\'kaiming_normal\'`, `\'trunc_normal\'`, `\'orthogonal\'`, `\'sparse\'`. Output: - Returns a dictionary where each key is the name of a layer (as a string) and each value is a tuple (mean, std). This tuple represents the mean and standard deviation of the immediately preceding initialized weights for that layer. Constraints: - You may assume that the model provided will be valid and contain layers and weights. - Initialize weights using the `torch.nn.init` functions specified in the `init_scheme`. Example Usage: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 10) model = SimpleModel() init_scheme = \'xavier_uniform\' # Initialize the model using the \'xavier_uniform\' scheme results = initialize_and_validate(model, init_scheme) # Expected output would be a dictionary where keys are layer names and values are (mean, std) print(results) # Example: {\'fc1.weight\': (mean_value, std_value), \'fc2.weight\': (mean_value, std_value)} ``` Notes: - Ensure that your function does not modify any existing state outside of initializing the model\'s weights. - The returned means and standard deviations should be calculated over the elements of the weights array for each layer. - Proper exception handling should be done in case of invalid initialization scheme inputs.","solution":"import torch import torch.nn as nn def initialize_and_validate(model, init_scheme): Initializes the weights of the given model using the specified initialization scheme and returns the mean and standard deviation of the weights for each layer. :param model: torch.nn.Module The neural network model whose weights need to be initialized. :param init_scheme: str The initialization scheme to use. Must be one of: \'uniform\', \'normal\', \'constant\', \'ones\', \'zeros\', \'eye\', \'dirac\', \'xavier_uniform\', \'xavier_normal\', \'kaiming_uniform\', \'kaiming_normal\', \'trunc_normal\', \'orthogonal\', \'sparse\' :return: dict A dictionary where keys are layer names and values are tuples of (mean, std) representing the mean and standard deviation of the initialized weights of each layer. init_funcs = { \'uniform\': nn.init.uniform_, \'normal\': nn.init.normal_, \'constant\': nn.init.constant_, \'ones\': nn.init.ones_, \'zeros\': nn.init.zeros_, \'eye\': nn.init.eye_, \'dirac\': nn.init.dirac_, \'xavier_uniform\': nn.init.xavier_uniform_, \'xavier_normal\': nn.init.xavier_normal_, \'kaiming_uniform\': nn.init.kaiming_uniform_, \'kaiming_normal\': nn.init.kaiming_normal_, \'trunc_normal\': nn.init.trunc_normal_, \'orthogonal\': nn.init.orthogonal_, \'sparse\': nn.init.sparse_, } if init_scheme not in init_funcs: raise ValueError(f\\"Invalid init_scheme: {init_scheme}. Must be one of {list(init_funcs.keys())}.\\") init_func = init_funcs[init_scheme] stats = {} for name, param in model.named_parameters(): if \'weight\' in name: if init_scheme == \'constant\' or init_scheme == \'sparse\': init_func(param, val=1.0) else: init_func(param) # Compute mean and std of the weights mean = torch.mean(param).item() std = torch.std(param).item() stats[name] = (mean, std) return stats"},{"question":"# Question: Implementing and Utilizing Fully Sharded Data Parallelism in PyTorch **Objective:** Your task is to demonstrate the use and advantages of PyTorch\'s Fully Sharded Data Parallelism (FSDP2). You will create a simple neural network, apply sharding using the `fully_shard` method, train the network in a distributed manner, and verify the implementation\'s correctness. # Requirements: 1. **Define a Neural Network:** - Implement a neural network using `torch.nn.Module`. - Ensure the network has at least three linear layers with appropriate activation functions. 2. **Apply Fully Sharded Data Parallelism:** - Use the `fully_shard` method to apply sharding to the network. - Define any custom policies for memory management if required (optional). 3. **Distributed Training:** - Implement a simple training loop utilizing multiple processes to simulate a distributed environment. - Use `torch.distributed` to handle communication between processes. - Train the network on a sample dataset (e.g., a subset of MNIST). 4. **Verification:** - Ensure that the model parameters are correctly sharded across processes. - Verify the training proceeds without errors and the model learns effectively by calculating the loss. # Input: - No direct input. Your script should define the network, apply sharding, and train the network. # Output: - The final loss after training. - Print the sharded status of the model parameters. # Constraints: - You may assume having access to a torch-compatible distributed environment. - For simplicity, limit the number of processes to 4. # Performance Requirements: - Ensure that the training loop runs efficiently despite the overhead introduced by sharding and inter-process communication. # Notes: - Make sure to handle initialization of the distributed process group within the script. - Use error handling to manage exceptions that might occur during the process, e.g., network errors or resource limitations. # Code Template: ```python import torch import torch.nn as nn import torch.distributed as dist from torch.distributed.fsdp import fully_shard # Define the neural network class class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(784, 256) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(256, 128) self.relu2 = nn.ReLU() self.fc3 = nn.Linear(128, 10) def forward(self, x): x = self.fc1(x) x = self.relu1(x) x = self.fc2(x) x = self.relu2(x) x = self.fc3(x) return x # Initialize the distributed environment def init_process(rank, size, fn, backend=\'gloo\'): dist.init_process_group(backend, rank=rank, world_size=size) fn(rank, size) dist.destroy_process_group() # Train the network def train(rank, size): torch.manual_seed(1234) model = Net() # Apply sharding using FSDP model = fully_shard(model) # Prepare dataset and dataloader # For simplicity, use random data dataset = torch.randn(100, 784) targets = torch.randint(0, 10, (100,)) dataset = torch.utils.data.TensorDataset(dataset, targets) loader = torch.utils.data.DataLoader(dataset, batch_size=32) criterion = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(), lr=0.01) for epoch in range(2): for data, target in loader: optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() if rank == 0: print(f\'Final loss (rank {rank}): {loss.item()}\') # Verify sharding for param in model.parameters(): print(f\'Parameter sharded (rank {rank}): {param.is_shard()}\') # Main entry point if __name__ == \\"__main__\\": size = 4 # Number of processes processes = [] for rank in range(size): p = torch.multiprocessing.Process(target=init_process, args=(rank, size, train)) p.start() processes.append(p) for p in processes: p.join() ```","solution":"import os import torch import torch.nn as nn import torch.optim as optim import torch.distributed as dist import torch.multiprocessing as mp from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, TensorDataset # Define the neural network class class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.fc1 = nn.Linear(784, 256) self.relu1 = nn.ReLU() self.fc2 = nn.Linear(256, 128) self.relu2 = nn.ReLU() self.fc3 = nn.Linear(128, 10) def forward(self, x): x = self.relu1(self.fc1(x)) x = self.relu2(self.fc2(x)) x = self.fc3(x) return x # Train the network def train(rank, world_size): print(f\\"Running DDP training on rank {rank}.\\") dist.init_process_group(backend=\'nccl\', init_method=\'env://\', world_size=world_size, rank=rank) model = Net().to(rank) model = DDP(model, device_ids=[rank]) # Prepare dataset and dataloader dataset = torch.randn(100, 784).to(rank) targets = torch.randint(0, 10, (100,)).to(rank) dataset = TensorDataset(dataset, targets) loader = DataLoader(dataset, batch_size=32) criterion = nn.CrossEntropyLoss().to(rank) optimizer = optim.SGD(model.parameters(), lr=0.01) for epoch in range(10): for data, target in loader: optimizer.zero_grad() output = model(data) loss = criterion(output, target) loss.backward() optimizer.step() print(f\'Final loss on rank {rank}: {loss.item()}\') dist.destroy_process_group() # Main entry point def main(): world_size = 4 # Number of processes os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\' os.environ[\'MASTER_PORT\'] = \'29500\' mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement:** You are tasked with creating a utility function that serializes an email message object into either a binary or text format using the appropriate generator class from the `email.generator` module in Python 3.10. The function should also handle custom encoding and line separators as specified by the user. # Requirements: 1. Define a function `serialize_email_message(message, binary=True, unixfrom=False, linesep=None, mangle_from_=None, maxheaderlen=None, format_type=\'plain\', policy=None)`. - **Parameters**: - `message`: The email message object to be serialized. - `binary` (default `True`): Specifies whether the output should be binary. - `unixfrom` (default `False`): If `True`, includes the Unix `From` header. - `linesep` (default `None`): Line separator character to be used. - `mangle_from_` (default `None`): Controls the mangle from behavior (refer to the documentation for details). - `maxheaderlen` (default `None`): Controls the maximum header length; `None` means wrapped as per policy, `0` means no rewrap. - `format_type` (default `\'plain\'`): The format type, can be `\'plain\'`, `\'bytes\'`, or `\'decoded\'`. - `policy` (default `None`): The policy object controlling message generation. - **Returns**: - A string or bytes object containing the serialized representation of the email message object, depending on the `binary` and `format_type` parameters. 2. The function should use the `BytesGenerator` if `binary` is `True` and the appropriate generator class (`Generator` or `DecodedGenerator`) depending on the `format_type`. 3. The function should handle the parameters for mangle_from_, maxheaderlen, and policy correctly as per their descriptions in the Python 3.10 `email.generator` documentation. # Example Usage: ```python from email.message import EmailMessage def serialize_email_message(message, binary=True, unixfrom=False, linesep=None, mangle_from_=None, maxheaderlen=None, format_type=\'plain\', policy=None): import io from email.generator import BytesGenerator, Generator, DecodedGenerator output = io.BytesIO() if binary else io.StringIO() if binary: generator = BytesGenerator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) elif format_type == \'decoded\': generator = DecodedGenerator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) else: generator = Generator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) generator.flatten(message, unixfrom=unixfrom, linesep=linesep) return output.getvalue() # Create a sample EmailMessage msg = EmailMessage() msg.set_content(\\"This is a test email.\\") # Serialize to binary serialized_msg = serialize_email_message(msg, binary=True) print(serialized_msg) # Serialize to text serialized_msg_text = serialize_email_message(msg, binary=False) print(serialized_msg_text) ``` This question tests the understanding of the `email.generator` module, handling various serialization settings, and working with different types of output (binary and text). It also checks familiarity with file-like objects in Python and various configurations of the generator classes.","solution":"def serialize_email_message(message, binary=True, unixfrom=False, linesep=None, mangle_from_=None, maxheaderlen=None, format_type=\'plain\', policy=None): Serialize an email message object into either a binary or text format. :param message: The email message object to be serialized. :param binary: Specifies whether the output should be binary. :param unixfrom: If True, includes the Unix From header. :param linesep: Line separator character to be used. :param mangle_from_: Controls the mangle from behavior. :param maxheaderlen: Controls the maximum header length. :param format_type: The format type, can be \'plain\', \'bytes\', or \'decoded\'. :param policy: The policy object controlling message generation. :return: Serialized representation of the email message as string or bytes object. import io from email.generator import BytesGenerator, Generator, DecodedGenerator output = io.BytesIO() if binary else io.StringIO() if binary: generator = BytesGenerator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) elif format_type == \'decoded\': generator = DecodedGenerator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) else: generator = Generator(output, mangle_from_=mangle_from_, maxheaderlen=maxheaderlen, policy=policy) generator.flatten(message, unixfrom=unixfrom, linesep=linesep) return output.getvalue()"},{"question":"Advanced Plotting with Seaborn **Objective:** Demonstrate your understanding of Seaborn\'s plotting capabilities, palette customization using `sns.blend_palette`, and application of themes. **Question:** Write a Python function `create_custom_heatmap` that generates a heatmap using Seaborn with the following specifications: 1. The function will take the following inputs: - `data`: A 2D list or a Pandas DataFrame containing numerical values. - `colors`: A list of color strings to blend for the heatmap palette. - `title`: A string title for the heatmap. - `set_theme`: An optional parameter that defaults to `True`. If set to `True`, apply a Seaborn theme of your choice. 2. The function should: - Blend the provided colors into a continuous colormap using `sns.blend_palette` with `as_cmap` set to `True`. - Plot a heatmap using the blended colormap. - Apply a Seaborn theme if `set_theme` is `True`. - Set the title of the heatmap using the provided title. - Display the plot inline. **Input and Output Formats:** ```python def create_custom_heatmap(data, colors, title, set_theme=True): # Your implementation here # Example usage: import pandas as pd data = pd.DataFrame([ [1, 2, 3], [4, 5, 6], [7, 8, 9] ]) colors = [\\"#3498db\\", \\"#e74c3c\\", \\"#2ecc71\\"] title = \\"My Custom Heatmap\\" create_custom_heatmap(data, colors, title) ``` **Constraints:** - The `colors` list must have at least two color strings. - `data` must be a valid 2D list or Pandas DataFrame containing numerical values only. **Performance Requirements:** - The function should efficiently handle data with a size up to 1000x1000. > **Hint:** You may need to refer to additional Seaborn documentation for details on heatmap creation and theme settings.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_custom_heatmap(data, colors, title, set_theme=True): Generates a custom heatmap using Seaborn with specified colors and title. Parameters: - data: A 2D list or a Pandas DataFrame containing numerical values. - colors: A list of color strings to blend for the heatmap palette. - title: A string title for the heatmap. - set_theme: An optional parameter that defaults to True. If set to True, apply a Seaborn theme. if set_theme: sns.set_theme(style=\\"whitegrid\\") # Blend the provided colors into a continuous colormap cmap = sns.blend_palette(colors, as_cmap=True) # Create the heatmap plt.figure(figsize=(10, 8)) sns.heatmap(data, cmap=cmap) # Set the title of the heatmap plt.title(title) # Show the plot plt.show()"},{"question":"**Question: Implement an Environment-Aware PyTorch Model Loader** # Objective You are required to write a PyTorch utility function that loads a model considering specific environment variables influencing the loading behavior. The function should properly handle scenarios defined by these environment variables: 1. `TORCH_FORCE_WEIGHTS_ONLY_LOAD` 2. `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD` 3. `TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT` # Function Signature ```python import torch import os def load_model(model_path: str) -> torch.nn.Module: pass ``` # Description - `load_model`: This function takes in one parameter: - `model_path` (str): The file path to the saved PyTorch model. # Requirements 1. The function should load the model from the specified `model_path`. 2. If the environment variable `TORCH_FORCE_WEIGHTS_ONLY_LOAD` is set to true, the function should force the model to be loaded with weights only. 3. If the environment variable `TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD` is set to true, ensure the function explicitly sets `weights_only=False` unless `weights_only` was manually specified in the function. 4. Properly handle the autograd shutdown timeout using `TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT` by setting the timeout value if specified. 5. Return the loaded model. # Constraints - Assume `model_path` is a valid path to a PyTorch model file. - Handle any exceptions gracefully and provide meaningful error messages where appropriate. # Example ```python # Assume the following environment variables are set: # os.environ[\'TORCH_FORCE_WEIGHTS_ONLY_LOAD\'] = \'true\' # os.environ[\'TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\'] = \'15\' model = load_model(\'path/to/model.pth\') ``` In this example, the function will load the model using `weights_only=True` and set the autograd shutdown wait limit to 15 seconds. # Testing Your implementation should be tested on various scenarios to ensure it correctly respects the environment variable settings and loads the model appropriately.","solution":"import torch import os def load_model(model_path: str) -> torch.nn.Module: Load a PyTorch model based on certain environment variables that influence the loading behavior. Args: - model_path (str): Path to the PyTorch model file. Returns: - torch.nn.Module: The loaded PyTorch model. # Set autograd shutdown wait limit if specified shutdown_wait_limit = os.getenv(\'TORCH_AUTOGRAD_SHUTDOWN_WAIT_LIMIT\') if shutdown_wait_limit: torch.autograd.set_grad_device_wait_limit(int(shutdown_wait_limit)) # Check environment variables for weights_only load behavior weights_only = False if os.getenv(\'TORCH_FORCE_WEIGHTS_ONLY_LOAD\') == \'true\': weights_only = True elif os.getenv(\'TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD\') == \'true\': weights_only = False try: model = torch.load(model_path, weights_only=weights_only) return model except Exception as e: raise RuntimeError(f\\"Failed to load model from {model_path}: {str(e)}\\") # Function for setting autograd wait limit, since it\'s not present in PyTorch functions torch.autograd.set_grad_device_wait_limit = lambda x: None # Mock function for compilation"},{"question":"Tuple Manipulation in Python In this task, you will write Python functions to work with tuples. Your implementations should mimic the functionalities provided in the tuple API described in the C extension for Python. Implement the following functions: 1. `is_tuple(obj)`: - **Input**: A single object `obj`. - **Output**: Return `True` if `obj` is a tuple, and `False` otherwise. 2. `tuple_create(*args)`: - **Input**: Variable number of arguments. - **Output**: Return a tuple containing the provided arguments. 3. `tuple_size(t)`: - **Input**: A tuple `t`. - **Output**: Return the size (number of elements) of the tuple `t`. Raise a `TypeError` if the input is not a tuple. 4. `tuple_get_item(t, pos)`: - **Input**: A tuple `t` and an integer `pos`. - **Output**: Return the element at index `pos` in the tuple `t`. Raise an `IndexError` if `pos` is out of bounds. Raise a `TypeError` if the input `t` is not a tuple. 5. `tuple_slice(t, start, end)`: - **Input**: A tuple `t` and two integers `start` and `end`. - **Output**: Return a new tuple that is a slice of `t` from index `start` up to but not including `end`. Raise a `TypeError` if the input `t` is not a tuple. # Constraints: - You are not allowed to use Python\'s built-in methods directly for checking types (`type()`, `isinstance()`) in your implementations (`is_tuple` function should manually determine the type). - Assume tuple elements and slicing bounds (start and end) are always integers. - Implement error handling as specified. # Performance Requirements: - Your solution should be efficient in terms of both time and space complexity. # Example: ```python print(is_tuple((1, 2, 3))) # Output: True print(is_tuple([1, 2, 3])) # Output: False print(tuple_create(1, 2, 3)) # Output: (1, 2, 3) print(tuple_size((1, 2, 3))) # Output: 3 print(tuple_get_item((1, 2, 3), 1)) # Output: 2 print(tuple_slice((1, 2, 3, 4, 5), 1, 3)) # Output: (2, 3) ``` **Note**: Ensure your implementation handles and raises specified exceptions appropriately.","solution":"def is_tuple(obj): Returns True if obj is a tuple, False otherwise. return isinstance(obj, tuple) def tuple_create(*args): Returns a tuple containing the provided arguments. return tuple(args) def tuple_size(t): Returns the size (number of elements) of the tuple t. Raises a TypeError if the input is not a tuple. if not isinstance(t, tuple): raise TypeError(\'Input is not a tuple\') return len(t) def tuple_get_item(t, pos): Returns the element at index pos in the tuple t. Raises an IndexError if pos is out of bounds. Raises a TypeError if the input t is not a tuple. if not isinstance(t, tuple): raise TypeError(\'Input is not a tuple\') if pos < 0 or pos >= len(t): raise IndexError(\'Index out of bounds\') return t[pos] def tuple_slice(t, start, end): Returns a new tuple that is a slice of t from index start up to but not including end. Raises a TypeError if the input t is not a tuple. if not isinstance(t, tuple): raise TypeError(\'Input is not a tuple\') return t[start:end]"},{"question":"**Objective**: Demonstrate understanding of seaborn\'s theme and display configuration. You are provided with a dataset and tasked with creating a seaborn **Plot** object. Your goal is to customize the plot\'s theme and display settings as specified below. **Dataset Description**: The dataset is a CSV file with the following structure: ``` name age height income John 45 5.9 45000 Jane 34 5.7 54000 Doe 23 5.8 31000 ... ``` **Tasks**: 1. Import the necessary packages. 2. Load the dataset into a pandas DataFrame. 3. Create a simple seaborn plot, such as a scatterplot of `age` against `income`. 4. Configure the global theme of the plot to use a white grid style. 5. Set the axis face color to light grey. 6. Change the display format of the plots to SVG. 7. Disable HiDPI scaling for the generated plots. 8. Scale down the embedded image size slightly using a factor of 0.8. **Constraints**: - Use `seaborn.objects.Plot` for plotting. - You must use the provided dataset structure to demonstrate your plot. - Ensure your code is efficient and follows best practices. **Input**: - Path to the CSV file containing the dataset. **Output**: - Displayed plots with the applied configurations in a Jupyter notebook cell. **Code Template**: ```python import seaborn.objects as so import pandas as pd # Load the dataset into a DataFrame data_path = \'path_to_your_dataset.csv\' # Update this path to your actual data path df = pd.read_csv(data_path) # Create the seaborn plot plot = so.Plot(df, x=\'age\', y=\'income\').add(so.Dot()) # Configure the global theme to use a white grid style from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Set the axis face color to light grey so.Plot.config.theme[\\"axes.facecolor\\"] = \\"#f2f2f2\\" # Change the display format to SVG so.Plot.config.display[\\"format\\"] = \\"svg\\" # Disable HiDPI scaling so.Plot.config.display[\\"hidpi\\"] = False # Scale down the embedded image size slightly so.Plot.config.display[\\"scaling\\"] = 0.8 # Display the plot plot.show() ``` Ensure you properly update `data_path` with the actual path to your CSV file. Run the complete code in a Jupyter notebook cell to see the plot with the specified configurations. **Note**: Ensure you have the necessary packages installed in your environment (`seaborn`, `pandas`).","solution":"import seaborn.objects as so import pandas as pd def create_custom_plot(data_path): # Load the dataset into a DataFrame df = pd.read_csv(data_path) # Create the seaborn plot plot = so.Plot(df, x=\'age\', y=\'income\').add(so.Dot()) # Configure the global theme to use a white grid style from seaborn import axes_style so.Plot.config.theme.update(axes_style(\\"whitegrid\\")) # Set the axis face color to light grey so.Plot.config.theme[\\"axes.facecolor\\"] = \\"#f2f2f2\\" # Change the display format to SVG so.Plot.config.display[\\"format\\"] = \\"svg\\" # Disable HiDPI scaling so.Plot.config.display[\\"hidpi\\"] = False # Scale down the embedded image size slightly so.Plot.config.display[\\"scaling\\"] = 0.8 # Return the plot object return plot"},{"question":"# Python Coding Assessment Question Objective In this task, you will demonstrate your understanding of Python\'s instance and method objects by implementing functions that work with such objects. Specifically, you will create instance and method objects, check their types, and retrieve associated functions and instances. Task Description 1. **Create an Instance Method Object:** Implement a function `create_instance_method(func)` that takes a callable `func` and returns a new instance method object using the `PyInstanceMethod_New` function. ```python def create_instance_method(func): Create a new instance method object with the given function. Parameters: func (Callable): The function to be used as an instance method. Returns: PyObject: A new instance method object. # Your code here ``` 2. **Check Instance Method Object:** Implement a function `is_instance_method(obj)` that checks if a given object `obj` is an instance method object using the `PyInstanceMethod_Check` function. ```python def is_instance_method(obj): Check if the given object is an instance method object. Parameters: obj (PyObject): The object to check. Returns: bool: True if the object is an instance method object, False otherwise. # Your code here ``` 3. **Retrieve Function from Instance Method:** Implement a function `get_instance_method_function(im)` that retrieves the function associated with an instance method object `im` using the `PyInstanceMethod_Function` function. ```python def get_instance_method_function(im): Retrieve the function associated with the given instance method object. Parameters: im (PyObject): The instance method object. Returns: PyObject: The function associated with the instance method. # Your code here ``` 4. **Create a Method Object:** Implement a function `create_method(func, self)` that takes a callable `func` and an instance `self`, and returns a new method object using the `PyMethod_New` function. ```python def create_method(func, self): Create a new method object with the given function and instance. Parameters: func (Callable): The function to be used as a method. self (Any): The instance the method should be bound to. Returns: PyObject: A new method object. # Your code here ``` 5. **Check Method Object:** Implement a function `is_method(obj)` that checks if a given object `obj` is a method object using the `PyMethod_Check` function. ```python def is_method(obj): Check if the given object is a method object. Parameters: obj (PyObject): The object to check. Returns: bool: True if the object is a method object, False otherwise. # Your code here ``` 6. **Retrieve Function from Method:** Implement a function `get_method_function(meth)` that retrieves the function associated with a method object `meth` using the `PyMethod_Function` function. ```python def get_method_function(meth): Retrieve the function associated with the given method object. Parameters: meth (PyObject): The method object. Returns: PyObject: The function associated with the method. # Your code here ``` 7. **Retrieve Instance from Method:** Implement a function `get_method_instance(meth)` that retrieves the instance associated with a method object `meth` using the `PyMethod_Self` function. ```python def get_method_instance(meth): Retrieve the instance associated with the given method object. Parameters: meth (PyObject): The method object. Returns: PyObject: The instance associated with the method. # Your code here ``` Input and Output Formats - Each function should follow the signature and input type specified above. - Return types should match the types specified in the function descriptions. - You may assume proper inputs will be provided during tests. - Use appropriate Python data types and structures to implement these functions. Performance Requirements - Ensure that each function runs efficiently with respect to time and memory usage. - Avoid unnecessary computations and redundant operations. Use provided information to correctly implement each function. You may refer to additional Python documentation if needed.","solution":"from types import MethodType def create_instance_method(func): Create a new instance method object with the given function. Parameters: func (Callable): The function to be used as an instance method. Returns: MethodType: A new instance method object. class Dummy: pass instance = Dummy() return MethodType(func, instance) def is_instance_method(obj): Check if the given object is an instance method object. Parameters: obj (Any): The object to check. Returns: bool: True if the object is an instance method object, False otherwise. return isinstance(obj, MethodType) def get_instance_method_function(im): Retrieve the function associated with the given instance method object. Parameters: im (MethodType): The instance method object. Returns: Callable: The function associated with the instance method. return im.__func__ def create_method(func, self): Create a new method object with the given function and instance. Parameters: func (Callable): The function to be used as a method. self (Any): The instance the method should be bound to. Returns: MethodType: A new method object. return MethodType(func, self) def is_method(obj): Check if the given object is a method object. Parameters: obj (Any): The object to check. Returns: bool: True if the object is a method object, False otherwise. return isinstance(obj, MethodType) def get_method_function(meth): Retrieve the function associated with the given method object. Parameters: meth (MethodType): The method object. Returns: Callable: The function associated with the method. return meth.__func__ def get_method_instance(meth): Retrieve the instance associated with the given method object. Parameters: meth (MethodType): The method object. Returns: Any: The instance associated with the method. return meth.__self__"},{"question":"**Question: Advanced Plot Customization with Seaborn** You are required to demonstrate your proficiency with Seaborn\'s `objects` module by creating and customizing a scatter plot. You will use the `mpg` dataset available from the Seaborn library. Your task is to create a scatter plot of `mpg` (miles per gallon) vs. `weight` of the vehicles, with the following specific requirements: # Requirements: 1. **Data Preparation**: - Load the `mpg` dataset using `seaborn.load_dataset(\'mpg\')`. - Filter the dataset to include only cars with `cylinders` in the set {4, 6, 8}. 2. **Basic Plot**: - Create a basic scatter plot using the filtered dataset with `weight` on the x-axis and `mpg` on the y-axis. 3. **Logarithmic Transformation**: - Apply a logarithmic scale to the y-axis. 4. **Color Encoding**: - Encode the `origin` variable (`\\"usa\\"`, `\\"japan\\"`, `\\"europe\\"`) using different colors. You should use a named palette (e.g., `\\"deep\\"`). 5. **Customization**: - Customize the size of each point based on the `horsepower` variable. - Set the range of point sizes from 5 to 20. 6. **Legends and Labels**: - Add a title to your plot: \\"MPG vs Weight with Horsepower and Origin Encoding\\". - Label the x-axis as \\"Weight\\" and the y-axis as \\"Miles per Gallon (mpg)\\". - Ensure the legend correctly represents the `origin` and `horsepower`. # Constraints: - Do not modify any data values directly in the dataset; only use Seaborn functions and methods for transformations. - Your solution should be efficient and maintain readability. - Ensure your plot is clear and visually appealing, with appropriate use of colors and scales. # Input: - None # Output: - A scatter plot meeting the specified requirements. # Example Usage: ```python import seaborn.objects as so from seaborn import load_dataset # Load and filter dataset mpg = load_dataset(\'mpg\').query(\\"cylinders in [4, 6, 8]\\") # Create the plot p = ( so.Plot(mpg, x=\\"weight\\", y=\\"mpg\\", color=\\"origin\\", pointsize=\\"horsepower\\") .scale(y=\\"log\\", color=\\"deep\\", pointsize=(5, 20)) .add(so.Dot()) .label(title=\\"MPG vs Weight with Horsepower and Origin Encoding\\", x=\\"Weight\\", y=\\"Miles per Gallon (mpg)\\") ) p.show() ``` **Note:** Ensure that all required packages are properly imported.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_scatter_plot(): # Load and filter dataset mpg = sns.load_dataset(\'mpg\').query(\\"cylinders in [4, 6, 8]\\") # Create the plot p = ( so.Plot(mpg, x=\\"weight\\", y=\\"mpg\\", color=\\"origin\\", pointsize=\\"horsepower\\") .scale(y=\\"log\\", color=\\"deep\\", pointsize=(5, 20)) .add(so.Dot()) .label(title=\\"MPG vs Weight with Horsepower and Origin Encoding\\", x=\\"Weight\\", y=\\"Miles per Gallon (mpg)\\") ) # Show plot p.show() create_custom_scatter_plot()"},{"question":"**Assessment Question: Terminal-Based Text Editor** In this coding assessment, you are required to implement a basic terminal-based text editor using the `curses` module in Python. Your text editor should allow the user to perform fundamental operations such as adding text, navigating through the text, and saving the text to a file. # Instructions: 1. **Initialization**: When the text editor starts, initialize the `curses` module and set up an appropriate environment (cbreak mode, no echo, enabling keypad input). 2. **Editor Features**: - **Navigation**: Allow the user to navigate through the text using arrow keys (UP, DOWN, LEFT, RIGHT). - **Text Input**: Users should be able to input text, which gets displayed in the on-screen window. - **Scrolling**: If the text exceeds the window size, enable scrolling to view hidden text. - **Saving**: Provide functionality for saving the edited text to a file. Use \\"CTRL+S\\" to save the current contents to a file named `output.txt`. - **Quitting**: Use \\"CTRL+Q\\" to quit the text editor. 3. **Exception Handling**: Use appropriate exception handling to manage errors and ensure the terminal returns to a normal state if your program encounters any exceptions. # Expected Inputs and Outputs: - **Input**: Keyboard inputs for navigation and text insertion, along with control sequences for saving and quitting. - **Output**: Real-time updating of text on the terminal screen. Upon saving, the text is stored in `output.txt`. # Constraints: - Ensure smooth handling of terminal resizing. - Keep the implementation efficient to avoid sluggish performance with larger texts. - Use best practices for `curses` programming, including cleaning up and restoring the terminal state on exit. # Performance Requirements: - The editor should handle basic text files efficiently. - Responsiveness to key presses should be immediate to ensure a good user experience. # Sample Code Skeleton: ```python import curses def main(stdscr): # Clear screen stdscr.clear() curses.cbreak() # Enable cbreak mode curses.noecho() # Disable echo stdscr.keypad(True) # Enable keypad mode # Initial content (empty) content = [] while True: # Get user input key = stdscr.getch() if key == curses.KEY_UP: # Handle UP arrow key press pass elif key == curses.KEY_DOWN: # Handle DOWN arrow key press pass elif key == curses.KEY_LEFT: # Handle LEFT arrow key press pass elif key == curses.KEY_RIGHT: # Handle RIGHT arrow key press pass elif key == ord(\'n\'): # Handle ENTER key press pass elif key == 19: # Ctrl+S # Save the current contents to a file with open(\\"output.txt\\", \\"w\\") as f: f.write(\\"n\\".join(content)) elif key == 17: # Ctrl+Q # Quit the editor break else: # Handle other keys (add character to content) pass # Restore terminal to normal state curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \'__main__\': curses.wrapper(main) ``` # Submissions: Submit your solution as a Python script. Ensure your code handles exceptions gracefully and restores the terminal state on exit. Your solution will be evaluated on functionality, efficiency, and code quality.","solution":"import curses def main(stdscr): # Clear screen stdscr.clear() curses.cbreak() # Enable cbreak mode curses.noecho() # Disable echo stdscr.keypad(True) # Enable keypad mode # Initial content (2D List for rows and characters) content = [\'\'] cursor_x = 0 cursor_y = 0 while True: stdscr.clear() # Display the content in the terminal for row_idx, row in enumerate(content): stdscr.addstr(row_idx, 0, row) # Move the cursor to the specified coordinates stdscr.move(cursor_y, cursor_x) stdscr.refresh() # Get user input key = stdscr.getch() if key == curses.KEY_UP: cursor_y = max(0, cursor_y - 1) elif key == curses.KEY_DOWN: cursor_y = min(len(content) - 1, cursor_y + 1) elif key == curses.KEY_LEFT: cursor_x = max(0, cursor_x - 1) elif key == curses.KEY_RIGHT: cursor_x = min(len(content[cursor_y]), cursor_x + 1) elif key == 10: # Enter key content.insert(cursor_y + 1, \'\') cursor_y += 1 cursor_x = 0 elif key == 19: # Ctrl+S save_file(content) elif key == 17: # Ctrl+Q break elif key == 8 or key == 127: # Backspace key if cursor_x > 0: row = content[cursor_y] content[cursor_y] = row[:cursor_x-1] + row[cursor_x:] cursor_x -= 1 elif cursor_y > 0: cursor_x = len(content[cursor_y-1]) content[cursor_y-1] += content[cursor_y] del content[cursor_y] cursor_y -= 1 else: row = content[cursor_y] content[cursor_y] = row[:cursor_x] + chr(key) + row[cursor_x:] cursor_x += 1 # Restore terminal to normal state curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() def save_file(content): with open(\\"output.txt\\", \\"w\\") as f: f.write(\\"n\\".join(content)) if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"**Objective**: Implement a Python function that processes a given text to find patterns, extract specific parts, and modify the text based on certain rules using regular expressions. This task will assess your understanding of Python\'s \\"re\\" module and its various functionalities. # Problem Statement You are given a block of text containing various lines. Each line can contain different types of information, such as email addresses, dates, and phone numbers. Your task is to implement a function `process_text` that separates email addresses, dates, and phone numbers into separate lists and then replaces them in the text with placeholders. **Function Signature**: ```python def process_text(text: str) -> Tuple[List[str], List[str], List[str], str]: pass ``` # Input - `text`: A string representing the input text containing multiple lines. # Output The function should return a tuple containing four elements: 1. A list of all email addresses found in the text. 2. A list of all dates found in the text (format MM/DD/YYYY). 3. A list of all phone numbers found in the text (format XXX-XXX-XXXX). 4. The modified text where all email addresses, dates, and phone numbers are replaced with their respective placeholders \\"<EMAIL>\\", \\"<DATE>\\", and \\"<PHONE>\\". # Example Input ```python text = \'\'\' John Doe\'s email is john.doe@example.com and he was born on 01/15/1990. Call him at 123-456-7890 or email at john.alternative@example.com. He moved to the USA on 07/04/2012. \'\'\' ``` Output ```python ( [\'john.doe@example.com\', \'john.alternative@example.com\'], [\'01/15/1990\', \'07/04/2012\'], [\'123-456-7890\'], \'\'\' John Doe\'s email is <EMAIL> and he was born on <DATE>. Call him at <PHONE> or email at <EMAIL>. He moved to the USA on <DATE>. \'\'\' ) ``` # Constraints - The email addresses should be in the format `username@domain.tld`. - Dates should be in the format `MM/DD/YYYY`. - Phone numbers should be in the format `XXX-XXX-XXXX`. - You can assume that the input text is well-formed and contains valid email addresses, dates, and phone numbers as per the specified formats. # Hints - Use the `re` module to find all occurrences of email addresses, dates, and phone numbers. - Use capturing groups to extract the required patterns. - Replace found patterns in the text using the `sub` method of the `re` module. **Note**: Ensure your function performs efficiently even with large input texts. # Solution Template ```python from typing import List, Tuple import re def process_text(text: str) -> Tuple[List[str], List[str], List[str], str]: # Define regular expression patterns to match email addresses, dates, and phone numbers email_pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' date_pattern = r\'b[01]?d/[0-3]?d/d{4}b\' phone_pattern = r\'bd{3}-d{3}-d{4}b\' # Find all matches for each pattern emails = re.findall(email_pattern, text) dates = re.findall(date_pattern, text) phones = re.findall(phone_pattern, text) # Replace the found patterns in the text with placeholders modified_text = text modified_text = re.sub(email_pattern, \'<EMAIL>\', modified_text) modified_text = re.sub(date_pattern, \'<DATE>\', modified_text) modified_text = re.sub(phone_pattern, \'<PHONE>\', modified_text) return (emails, dates, phones, modified_text) ``` **Instructions**: Implement the `process_text` function based on the provided template and the problem statement.","solution":"from typing import List, Tuple import re def process_text(text: str) -> Tuple[List[str], List[str], List[str], str]: # Define regular expression patterns to match email addresses, dates, and phone numbers email_pattern = r\'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,}\' date_pattern = r\'b[01]?d/[0-3]?d/d{4}b\' phone_pattern = r\'bd{3}-d{3}-d{4}b\' # Find all matches for each pattern emails = re.findall(email_pattern, text) dates = re.findall(date_pattern, text) phones = re.findall(phone_pattern, text) # Replace the found patterns in the text with placeholders modified_text = text modified_text = re.sub(email_pattern, \'<EMAIL>\', modified_text) modified_text = re.sub(date_pattern, \'<DATE>\', modified_text) modified_text = re.sub(phone_pattern, \'<PHONE>\', modified_text) return (emails, dates, phones, modified_text)"},{"question":"**Problem Statement:** You are tasked with creating a utility function that helps organize and sort files based on customizable patterns. Your function will take a list of filenames and organize them into different categories based on the patterns provided. You should demonstrate a comprehensive understanding of the `fnmatch` module in your implementation. **Function Signature:** ```python def organize_files(filenames: list[str], patterns: dict[str, str]) -> dict[str, list[str]]: pass ``` **Input:** - `filenames` (list of str): A list of filenames to be categorized. - `patterns` (dict of str: str): A dictionary where keys are category names and values are shell-style wildcard patterns. **Output:** - `result` (dict of str: list of str): A dictionary where keys are category names and values are lists of filenames matching the associated patterns. **Constraints:** - Each filename should only be included in one category. If a filename matches multiple patterns, it should be assigned to the first pattern it matches based on the order of the keys in the dictionary. - Patterns should be case-insensitive. **Example:** ```python filenames = [\\"report.doc\\", \\"data.csv\\", \\"summary.TXT\\", \\"image.PNG\\", \\"note.txt\\"] patterns = { \\"documents\\": \\"*.doc\\", \\"texts\\": \\"*.txt\\", \\"csv_files\\": \\"*.csv\\", \\"images\\": \\"*.png\\" } result = organize_files(filenames, patterns) # Example output: # { # \\"documents\\": [\\"report.doc\\"], # \\"texts\\": [\\"summary.TXT\\", \\"note.txt\\"], # \\"csv_files\\": [\\"data.csv\\"], # \\"images\\": [\\"image.PNG\\"] # } ``` **Additional Requirements:** 1. Implement the function efficiently utilizing `fnmatch` module functions. 2. Ensure that the pattern matching is case-insensitive. 3. Use inline comments to show your understanding of each fnmatch function used. **Guidance:** - Use `fnmatch.fnmatch()` for matching filenames with patterns. - Ensure efficiency and handle edge cases like empty filenames or patterns gracefully. **Performance Requirements:** - Your solution should efficiently handle up to 10,000 filenames and 100 patterns within reasonable time constraints.","solution":"import fnmatch def organize_files(filenames: list[str], patterns: dict[str, str]) -> dict[str, list[str]]: Organizes a list of filenames into categories based on provided patterns. Args: filenames (list of str): A list of filenames to be categorized. patterns (dict of str: str): A dictionary where keys are category names and values are shell-style wildcard patterns. Returns: dict of str: list of str: A dictionary where keys are category names and values are lists of filenames matching the associated patterns. # Prepare the resulting dictionary categorized_files = {category: [] for category in patterns} # Iterate over filenames to categorize them for filename in filenames: # Convert filename to lowercase for case-insensitive matching lower_filename = filename.lower() # Check each pattern for category, pattern in patterns.items(): # Convert pattern to lowercase for case-insensitive matching lower_pattern = pattern.lower() if fnmatch.fnmatch(lower_filename, lower_pattern): categorized_files[category].append(filename) break # Once matched, break and don\'t check further patterns return categorized_files"},{"question":"# Garbage Collector Interaction You are given the task of implementing a utility class that will manage the tracking and collection of objects using Python\'s `gc` module. This class should demonstrate a student\'s understanding of enabling/disabling garbage collection, forcing collections, retrieving statistics, and utilizing debugging features effectively. Class Definition Create a class named `GarbageCollectorUtility` with the following methods: 1. **`enable_gc()`**: - Enables automatic garbage collection. 2. **`disable_gc()`**: - Disables automatic garbage collection. 3. **`force_collect(generation=2)`**: - Forces a collection of the specified generation (default is 2) and returns the number of unreachable objects found. 4. **`collect_stats()`**: - Returns a dictionary of garbage collection statistics containing `collections`, `collected`, and `uncollectable` for all generations. 5. **`set_debug_mode(flags)`**: - Sets the garbage collection debugging flags. Accepts a combination of `gc` debugging flags. 6. **`get_debug_mode()`**: - Returns the currently set debugging flags. 7. **`track_object(obj)`**: - Returns `True` if the object is tracked by the garbage collector, `False` otherwise. Input/Output Format - Methods like `enable_gc()` and `disable_gc()` do not take any parameters or return any values. - The `force_collect(generation=2)` method should return an integer indicating the number of unreachable objects found. - The `collect_stats()` method should return a dictionary with keys `collections`, `collected`, and `uncollectable`. - The `set_debug_mode(flags)` method should accept an integer flag and set the debug mode accordingly. - The `get_debug_mode()` method should return an integer representing the current debugging flags. - The `track_object(obj)` method should return a boolean indicating whether the object is tracked by the garbage collector. Example Usage ```python gc_util = GarbageCollectorUtility() # Enable GC gc_util.enable_gc() # Force a collection unreachable_objects = gc_util.force_collect() print(unreachable_objects) # Example output: 0 # Collect stats stats = gc_util.collect_stats() print(stats) # Set debug mode gc_util.set_debug_mode(gc.DEBUG_STATS | gc.DEBUG_LEAK) # Get current debug mode current_debug = gc_util.get_debug_mode() print(current_debug) # Check if an object is tracked tracked = gc_util.track_object([]) print(tracked) # Output: True ``` Constraints - The `generation` parameter in `force_collect()` must be an integer within the range [0, 2]. - The `flags` parameter in `set_debug_mode(flags)` should be a valid integer combination of `gc` debugging flags. Write the `GarbageCollectorUtility` class implementation in Python.","solution":"import gc class GarbageCollectorUtility: def enable_gc(self): Enables automatic garbage collection. gc.enable() def disable_gc(self): Disables automatic garbage collection. gc.disable() def force_collect(self, generation=2): Forces a garbage collection of the specified generation. :param generation: The generation to collect (0, 1, or 2). Default is 2. :return: The number of unreachable objects found. return gc.collect(generation) def collect_stats(self): Returns garbage collection statistics for all generations. :return: A dictionary with keys \'collections\', \'collected\', and \'uncollectable\'. stats = { \'collections\': [], \'collected\': [], \'uncollectable\': [] } for i in range(len(gc.get_stats())): stats[\'collections\'].append(gc.get_stats()[i][\'collections\']) stats[\'collected\'].append(gc.get_stats()[i][\'collected\']) stats[\'uncollectable\'].append(gc.get_stats()[i][\'uncollectable\']) return stats def set_debug_mode(self, flags): Sets the garbage collection debugging flags. :param flags: An integer flag or combination of flags. gc.set_debug(flags) def get_debug_mode(self): Returns the currently set debugging flags. :return: An integer representing the current debugging flags. return gc.get_debug() def track_object(self, obj): Returns whether the object is tracked by the garbage collector. :param obj: The object to check. :return: True if the object is tracked, False otherwise. return gc.is_tracked(obj)"},{"question":"Objective Implement a machine learning pipeline using the `Nystroem` method for kernel approximation and an SVM classifier to classify a given dataset. Problem Statement You are provided with a labeled dataset containing `n` samples with `m` features. Your task is to: 1. Implement a function that performs kernel approximation using the `Nystroem` method. 2. Train a linear SVM classifier on the transformed data. 3. Evaluate the classifier on a given test dataset and report the accuracy. Input - `train_data` (numpy array of shape `(n_train, m)`): The training data with `n_train` samples and `m` features. - `train_labels` (numpy array of shape `(n_train,)`): The labels for the training data. - `test_data` (numpy array of shape `(n_test, m)`): The test data with `n_test` samples and `m` features. - `test_labels` (numpy array of shape `(n_test,)`): The labels for the test data. - `n_components` (int): The number of components to use for the `Nystroem` method. Output - `accuracy` (float): The accuracy of the classifier on the test dataset. Function Signature ```python def kernel_svm_classification(train_data, train_labels, test_data, test_labels, n_components): # Your implementation here pass ``` Example ```python # Sample input train_data = np.array([[0, 0], [1, 1], [1, 0], [0, 1]]) train_labels = np.array([0, 0, 1, 1]) test_data = np.array([[1, 1], [0, 0]]) test_labels = np.array([0, 1]) n_components = 2 # Expected output print(kernel_svm_classification(train_data, train_labels, test_data, test_labels, n_components)) # Output: (a floating-point number representing the accuracy) ``` Constraints and Requirements 1. Use the `Nystroem` method for kernel approximation. 2. The classifier must be a linear SVM from `sklearn`. 3. Performance efficiency should be considered for large datasets. Notes - You might find it necessary to normalize the data before applying the kernel approximation. - It is advisable to seed the random number generator for reproducibility. - You can refer to the scikit-learn documentation or any relevant resources to implement the function.","solution":"import numpy as np from sklearn.kernel_approximation import Nystroem from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from sklearn.metrics import accuracy_score def kernel_svm_classification(train_data, train_labels, test_data, test_labels, n_components): Perform kernel approximation using the Nystroem method, then train and evaluate a linear SVM classifier. Parameters: - train_data (np.array): Training data with shape (n_train, m) - train_labels (np.array): Labels for the training data with shape (n_train,) - test_data (np.array): Test data with shape (n_test, m) - test_labels (np.array): Labels for the test data with shape (n_test,) - n_components (int): Number of components to use for the Nystroem method. Returns: - accuracy (float): The accuracy of the classifier on the test dataset. # Set random seed for reproducibility np.random.seed(42) # Create the pipeline: normalize data, apply Nystroem kernel approximation, then use linear SVM pipeline = make_pipeline( StandardScaler(), Nystroem(n_components=n_components, random_state=42), SVC(kernel=\'linear\') ) pipeline.fit(train_data, train_labels) predictions = pipeline.predict(test_data) accuracy = accuracy_score(test_labels, predictions) return accuracy"},{"question":"Advanced Coding Assessment # Objective Design and implement a Python function utilizing the `traceback` module, which captures and logs detailed information about any exceptions that occur during the execution of a given function. # Problem Statement You are required to write a function `log_exception_details` that takes another function `func` and its arguments `*args` and `**kwargs`, executes `func` with the provided arguments, and captures detailed information about any exceptions that occur. The detailed exception information should be logged into a file named `exception_log.txt`. The exception details should include: 1. The exception type and message. 2. The full traceback, including file names, line numbers, and code context where the exception occurred. 3. Any chained exceptions (causes or contexts). # Function Signature ```python def log_exception_details(func, *args, **kwargs): pass ``` # Input - `func`: A callable function. - `*args`: Positional arguments to pass to `func`. - `**kwargs`: Keyword arguments to pass to `func`. # Output - The function should return the result of `func` if no exception occurs. - If an exception occurs, the function should: 1. Write the detailed exception information to `exception_log.txt`. 2. Return `None`. # Constraints - Do not modify the provided function `func`. - Ensure that the log file `exception_log.txt` is properly formatted with clear and readable information. - Handle chained exceptions and context. # Example ```python def example_function(x, y): return x / y # Expected to log the ZeroDivisionError details into exception_log.txt log_exception_details(example_function, 10, 0) ``` # Additional Notes - Utilize `traceback.format_exception` to capture and format the exception details. - Use file operations to write the formatted traceback to `exception_log.txt`.","solution":"import traceback def log_exception_details(func, *args, **kwargs): try: return func(*args, **kwargs) except Exception as e: with open(\'exception_log.txt\', \'w\') as f: # Write exception type and message f.write(f\\"Exception type: {type(e).__name__}n\\") f.write(f\\"Exception message: {str(e)}nn\\") # Write full traceback tb = traceback.format_exc() f.write(\\"Traceback (most recent call last):n\\") f.write(tb) return None"},{"question":"**Question**: Implement a custom data structure called `CustomList` that mimics a Python list but with a twist: it uses the `operator` module for all its internal operations. Specifically, your `CustomList` should support adding, removing, and accessing elements, as well as basic arithmetic operations with other lists and numerical values. Additionally, implement comparison operators to compare two `CustomList` instances based on their content. # Implementation Details: 1. **Initialization** (`__init__`): - The constructor should initialize the list with an optional initial list provided by the user. 2. **Adding Elements** (`append` and `extend`): - Implement `append` to add a single element to the list using `operator.setitem`. - Implement `extend` to add multiple elements using `operator.concat`. 3. **Removing Elements** (`remove`): - Implement `remove` which should remove the first occurrence of a value from the list using `operator.delitem`. 4. **Accessing Elements** (`__getitem__` and `__setitem__`): - Implement `__getitem__` to retrieve an item at a given index using `operator.getitem`. - Implement `__setitem__` to set an item at a given index using `operator.setitem`. 5. **Arithmetic Operations** (`__add__`, `__iadd__`, `__mul__`, `__imul__`): - Implement `__add__` to concatenate another list to the current list using `operator.add` or `operator.concat`. - Implement `__iadd__` to concatenate another list to the current list in place using `operator.iadd` or `operator.iconcat`. - Implement `__mul__` to multiply the list by an integer using `operator.mul`. - Implement `__imul__` to multiply the list by an integer in place using `operator.imul`. 6. **Comparison Operators** (`__eq__`, `__lt__`, `__le__`, `__gt__`, `__ge__`): - Implement `__eq__` to check if two lists are equal using `operator.eq`. - Implement `__lt__`, `__le__`, `__gt__`, and `__ge__` to compare two lists lexicographically using `operator.lt`, `operator.le`, `operator.gt`, and `operator.ge` respectively. # Expected Input and Output: - Initialization: `CustomList([1, 2, 3])` - Adding Elements: - `append`: `clist.append(4)` results in `[1, 2, 3, 4]` - `extend`: `clist.extend([5, 6])` results in `[1, 2, 3, 4, 5, 6]` - Removing Elements: - `remove`: `clist.remove(3)` results in `[1, 2, 4, 5, 6]` - Accessing Elements: - `__getitem__`: `clist[2]` returns `4` - `__setitem__`: `clist[2] = 10` results in `[1, 2, 10, 5, 6]` - Arithmetic Operations: - `__add__`: `clist + [7, 8]` results in `[1, 2, 10, 5, 6, 7, 8]` - `__iadd__`: `clist += [9]` results in `[1, 2, 10, 5, 6, 9]` - `__mul__`: `clist * 2` results in `[1, 2, 10, 5, 6, 9, 1, 2, 10, 5, 6, 9]` - `__imul__`: `clist *= 3` results in `[1, 2, 10, 5, 6, 9, 1, 2, 10, 5, 6, 9, 1, 2, 10, 5, 6, 9]` - Comparison Operators: - `__eq__`: `clist == CustomList([1, 2, 3])` returns `False` # Constraints: - Do not use built-in list operations directly, always use the methods provided by the `operator` module. - Ensure your class handles edge cases such as appending to an empty list, removing elements that are in the list, etc. ```python import operator class CustomList: def __init__(self, initial_list=None): self.data = initial_list if initial_list is not None else [] def append(self, value): self.data = operator.concat(self.data, [value]) def extend(self, values): self.data = operator.concat(self.data, values) def remove(self, value): index = operator.indexOf(self.data, value) operator.delitem(self.data, index) def __getitem__(self, index): return operator.getitem(self.data, index) def __setitem__(self, index, value): return operator.setitem(self.data, index, value) def __add__(self, other): return CustomList(operator.concat(self.data, other.data if isinstance(other, CustomList) else other)) def __iadd__(self, other): self.data = operator.iconcat(self.data, other.data if isinstance(other, CustomList) else other) return self def __mul__(self, n): return CustomList(operator.mul(self.data, n)) def __imul__(self, n): self.data = operator.imul(self.data, n) return self def __eq__(self, other): return operator.eq(self.data, other.data if isinstance(other, CustomList) else other) def __lt__(self, other): return operator.lt(self.data, other.data if isinstance(other, CustomList) else other) def __le__(self, other): return operator.le(self.data, other.data if isinstance(other, CustomList) else other) def __gt__(self, other): return operator.gt(self.data, other.data if isinstance(other, CustomList) else other) def __ge__(self, other): return operator.ge(self.data, other.data if isinstance(other, CustomList) else other) def __repr__(self): return repr(self.data) # Example Usage clist = CustomList([1, 2, 3]) clist.append(4) print(clist) # Output: [1, 2, 3, 4] clist.extend([5, 6]) print(clist) # Output: [1, 2, 3, 4, 5, 6] clist.remove(3) print(clist) # Output: [1, 2, 4, 5, 6] clist[2] = 10 print(clist) # Output: [1, 2, 10, 5, 6] print(clist + [7, 8]) # Output: [1, 2, 10, 5, 6, 7, 8] clist += [9] print(clist) # Output: [1, 2, 10, 5, 6, 9] print(clist * 2) # Output: [1, 2, 10, 5, 6, 9, 1, 2, 10, 5, 6, 9] other_list = CustomList([1, 2, 10, 5, 6, 9]) print(clist == other_list) # Output: True print(clist < other_list) # Output: False print(clist > other_list) # Output: False print(clist <= other_list) # Output: True print(clist >= other_list) # Output: True ```","solution":"import operator class CustomList: def __init__(self, initial_list=None): self.data = initial_list if initial_list is not None else [] def append(self, value): self.data = operator.concat(self.data, [value]) def extend(self, values): self.data = operator.concat(self.data, values) def remove(self, value): index = operator.indexOf(self.data, value) operator.delitem(self.data, index) def __getitem__(self, index): return operator.getitem(self.data, index) def __setitem__(self, index, value): operator.setitem(self.data, index, value) def __add__(self, other): return CustomList(operator.concat(self.data, other.data if isinstance(other, CustomList) else other)) def __iadd__(self, other): self.data = operator.iconcat(self.data, other.data if isinstance(other, CustomList) else other) return self def __mul__(self, n): return CustomList(operator.mul(self.data, n)) def __imul__(self, n): self.data = operator.imul(self.data, n) return self def __eq__(self, other): return operator.eq(self.data, other.data if isinstance(other, CustomList) else other) def __lt__(self, other): return operator.lt(self.data, other.data if isinstance(other, CustomList) else other) def __le__(self, other): return operator.le(self.data, other.data if isinstance(other, CustomList) else other) def __gt__(self, other): return operator.gt(self.data, other.data if isinstance(other, CustomList) else other) def __ge__(self, other): return operator.ge(self.data, other.data if isinstance(other, CustomList) else other) def __repr__(self): return repr(self.data)"},{"question":"Objective Your task is to write a Python function that fetches data from a given URL using the `urllib.request` module. You must handle potential exceptions, add custom headers, and process the retrieved data. Specifically, you should implement a `fetch_url` function as described below. Function Specification **Function Name**: `fetch_url` **Input**: - `url` (str): The URL to fetch data from. - `params` (dict, optional): A dictionary of query parameters to include in the URL. Default is `None`. - `headers` (dict, optional): A dictionary of headers to include in the request. Default is `None`. - `timeout` (int, optional): The timeout for the request in seconds. Default is 10 seconds. **Output**: - A dictionary with the following keys: - `success` (bool): `True` if the request was successful, `False` otherwise. - `status_code` (int): The HTTP status code of the response. - `content` (str): The content of the response as a string if the request was successful. Empty string if not. - `error` (str): The error message if the request was not successful. Empty string if successful. **Constraints and Limitations**: - You must handle both `URLError` and `HTTPError` exceptions. - If query parameters are provided, encode them properly and append them to the URL. - Default headers should include `User-Agent` as `Mozilla/5.0` if not provided. - Ensure that the function works for both GET and POST requests (if `params` is provided, use a POST request). **Performance Requirements**: - The function should complete within the specified timeout. Example Usage ```python url = \'http://www.example.com/api/data\' params = {\'name\': \'test\', \'type\': \'example\'} headers = {\'Authorization\': \'Bearer token\'} result = fetch_url(url, params=params, headers=headers, timeout=5) print(result) ``` Example Output ```python { \'success\': True, \'status_code\': 200, \'content\': \'{\\"message\\": \\"Success\\", \\"data\\": [...] }\', \'error\': \'\' } ``` If there is an error (e.g., the URL is not reachable), the output should be: ```python { \'success\': False, \'status_code\': 0, \'content\': \'\', \'error\': \'Reason for the error\' } ``` Implementation Notes 1. Use the `urllib.parse.urlencode` function to encode query parameters. 2. Use the `urllib.request.Request` and `urllib.request.urlopen` functions to make the request. 3. Ensure that you close any open resources properly. Good luck and happy coding!","solution":"import urllib.request import urllib.parse import urllib.error def fetch_url(url, params=None, headers=None, timeout=10): Fetch data from a given URL with optional parameters, headers, and timeout. Args: url (str): The URL to fetch data from. params (dict): A dictionary of query parameters to include in the URL. headers (dict): A dictionary of headers to include in the request. timeout (int): The timeout for the request in seconds. Returns: dict: A dictionary containing success status, HTTP status code, content, and error message. if headers is None: headers = {} if \'User-Agent\' not in headers: headers[\'User-Agent\'] = \'Mozilla/5.0\' if params: query_string = urllib.parse.urlencode(params) url += \'?\' + query_string request = urllib.request.Request(url, headers=headers) result = { \'success\': False, \'status_code\': 0, \'content\': \'\', \'error\': \'\' } try: with urllib.request.urlopen(request, timeout=timeout) as response: result[\'status_code\'] = response.getcode() result[\'content\'] = response.read().decode(\'utf-8\') result[\'success\'] = True except urllib.error.HTTPError as e: result[\'status_code\'] = e.code result[\'error\'] = str(e) except urllib.error.URLError as e: result[\'error\'] = str(e) return result"},{"question":"Advanced Data Visualization with Seaborn **Objective**: Demonstrate your understanding of the seaborn library for creating and customizing data visualizations. Task: 1. Load the dataset `penguins` using seaborn. 2. Create a scatter plot using seaborn\'s `relplot` to visualize the relationship between `flipper_length_mm` and `body_mass_g`, differentiating between species with different colors. 3. Further customize the plot by: - Faceting the data across rows based on the `sex` variable. - Adjusting the size of points based on the `bill_length_mm` variable. - Setting the `height` of each facet to 4 and `aspect` ratio to 1. - Adding a horizontal line at `y=3000` (representing body mass). - Customizing the axis labels to \\"Flipper Length (mm)\\" and \\"Body Mass (g)\\". - Adding titles to each facet indicating the `sex`. Requirements: - The plot should have different colors for each species. - Faceting should be based on the `sex` variable. - The size of the points should correspond to `bill_length_mm`. - Customizations should include `height`, `aspect`, and titles. Input: - Use the dataset `penguins`, which is available in seaborn. Output: - A customized scatter plot as specified. Constraints: - The seaborn version should be at least 0.11.0. Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the relplot with required customizations g = sns.relplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", size=\\"bill_length_mm\\", row=\\"sex\\", height=4, aspect=1, kind=\\"scatter\\" ) # Further customizations g.map(plt.axhline, y=3000, color=\\"red\\", dashes=(2, 1), zorder=0) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Body Mass (g)\\") g.set_titles(\\"Sex: {row_name}\\") g.tight_layout() # Display the plot plt.show() ``` **Note**: Ensure your plot appears similar to the provided example code to validate your solution.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_relplot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the relplot with required customizations g = sns.relplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", size=\\"bill_length_mm\\", row=\\"sex\\", height=4, aspect=1, kind=\\"scatter\\" ) # Further customizations g.map(plt.axhline, y=3000, color=\\"red\\", dashes=(2, 1), zorder=0) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Body Mass (g)\\") g.set_titles(\\"Sex: {row_name}\\") g.tight_layout() # Display the plot plt.show()"},{"question":"# Custom PyTorch Function Implementation Objective: Extend PyTorch by creating a custom `Function` that computes a new operation on tensors. Specifically, the task is to implement a PyTorch custom `Function` that computes the Smooth L1 loss (also known as the Huber loss), which is a combination of L1 and L2 losses. **Loss Formula**: [ text{SmoothL1}(x, y) = begin{cases} 0.5(x_i - y_i)^2 & text{if } |x_i - y_i| < beta beta|x_i - y_i| - frac{beta^2}{2} & text{otherwise} end{cases} ] where ( x ) and ( y ) are input tensors, and ( beta ) is a hyperparameter. Instructions: 1. **Define** a class `SmoothL1Function` inheriting from `torch.autograd.Function`. 2. **Implement** the `forward` method to compute the Smooth L1 loss as defined above. 3. **Implement** the `backward` method to compute the gradient of the loss w.r.t. the input tensors. 4. **Validate** the implemented `Function` using the provided test cases. Expected Input and Output: - **Input**: Two tensors `x` and `y` of the same shape, and a scalar `beta`. - **Output**: A single scalar tensor representing the Smooth L1 loss. Constraints: - Ensure complete gradient computation for the inputs. - Confirm that the gradients are calculated correctly using `torch.autograd.gradcheck`. Performance Requirements: - The implementation should handle inputs of various shapes efficiently. - Ensure numerical stability, especially for large tensors. Template: ```python import torch class SmoothL1Function(torch.autograd.Function): @staticmethod def forward(ctx, x, y, beta): Compute the Smooth L1 loss. Parameters: - ctx: Context object to save information for backward computation - x: First input tensor - y: Second input tensor - beta: Smooth L1 loss hyperparameter Returns: - Loss: a single scalar tensor representing the loss # Compute the Smooth L1 loss # Save inputs for backward computation ctx.save_for_backward(x, y) ctx.beta = beta return loss @staticmethod def backward(ctx, grad_output): Compute the gradient of the Smooth L1 loss w.r.t. the input tensors. Parameters: - ctx: Context object containing saved information - grad_output: Gradient of the loss w.r.t. the output Returns: - Gradients for x and y # Retrieve saved inputs x, y = ctx.saved_tensors beta = ctx.beta # Compute the gradient return grad_x, grad_y, None # Wrapper function to use the custom SmoothL1Function def smooth_l1_loss(x, y, beta): return SmoothL1Function.apply(x, y, beta) # Test Case x = torch.randn(10, requires_grad=True) y = torch.randn(10, requires_grad=True) beta = 1.0 # Forward pass loss = smooth_l1_loss(x, y, beta) # Backward pass loss.backward() # Gradcheck input = (x.double(), y.double(), beta) print(torch.autograd.gradcheck(smooth_l1_loss, input, eps=1e-6, atol=1e-4)) ``` Explanation: - **`SmoothL1Function`**: A custom `Function` that computes the Smooth L1 loss using the formula provided. - **Subclasses** `torch.autograd.Function` to create a custom operation. - **`forward` method**: Implements the smooth L1 computation. Saves necessary information for gradient computation. - **`backward` method**: Computes the gradients w.r.t. `x` and `y`. - **`smooth_l1_loss`**: A wrapper function that creates an easy-to-use API for computing the smooth L1 loss.","solution":"import torch class SmoothL1Function(torch.autograd.Function): @staticmethod def forward(ctx, x, y, beta): Compute the Smooth L1 loss. Parameters: - ctx: Context object to save information for backward computation - x: First input tensor - y: Second input tensor - beta: Smooth L1 loss hyperparameter Returns: - Loss: a single scalar tensor representing the loss diff = x - y abs_diff = diff.abs() squared_diff = 0.5 * diff.pow(2) smooth_l1 = torch.where(abs_diff < beta, squared_diff, beta * abs_diff - 0.5 * beta**2) # Save tensors and beta for backward pass ctx.save_for_backward(x, y, diff, abs_diff) ctx.beta = beta # Return the sum of the loss values return smooth_l1.sum() @staticmethod def backward(ctx, grad_output): Compute the gradient of the Smooth L1 loss w.r.t. the input tensors. Parameters: - ctx: Context object containing saved information - grad_output: Gradient of the loss w.r.t. the output Returns: - Gradients for x and y x, y, diff, abs_diff = ctx.saved_tensors beta = ctx.beta grad_input = torch.where(abs_diff < beta, diff, beta * diff.sign()) # Multiply by the gradient from the next layer grad_x = grad_input * grad_output grad_y = -grad_input * grad_output return grad_x, grad_y, None # Wrapper function to use the custom SmoothL1Function def smooth_l1_loss(x, y, beta): return SmoothL1Function.apply(x, y, beta)"},{"question":"# Problem Description You are required to simulate a simplified data processing pipeline using concurrent execution techniques. The pipeline consists of three stages: 1. **Data Generation**: Generate a list of integers. 2. **Data Processing**: Multiply each integer by 2. 3. **Data Aggregation**: Calculate the sum of processed integers. Each stage needs to be executed concurrently for the sake of performance. Use the following constraints and guidelines: 1. **Data Generation**: Use the `threading` module to generate a list of N random integers between 1 and 100. N is given as input. 2. **Data Processing**: Use the `multiprocessing` module to handle the processing of this list by dividing the task among multiple processes. 3. **Data Aggregation**: Use the `concurrent.futures` module to calculate the sum of the processed integers. # Input 1. An integer N specifying the number of random integers to generate. # Output 1. The sum of all processed (doubled) integers. # Constraints 1. N will be a positive integer less than or equal to 10,000. 2. Each random integer generated should be between 1 and 100. # Function Signature ```python def execute_pipeline(N: int) -> int: pass ``` # Example ```python >>> execute_pipeline(10) # This should ideally print the sum of 10 random integers each multiplied by 2 Sample Random Integers: [5, 10, 22, 1, 7, 31, 9, 15, 23, 4] Processed Integers: [10, 20, 44, 2, 14, 62, 18, 30, 46, 8] Sum: 254 ``` # Notes - You should determine the number of threads and processes thoughtfully to balance between overhead and performance gains. - Proper synchronization mechanisms should be applied where necessary to avoid race conditions. - Dividing the work among processes should be done in a manner that ensures fair and efficient load distribution. **Hint**: You can use `random.randint` for generating random numbers. # Boilerplate Please ensure to complete the function `execute_pipeline` as specified in the function signature above.","solution":"import random import threading from multiprocessing import Pool from concurrent.futures import ThreadPoolExecutor def generate_data(N): random.seed(42) # For reproducibility in tests return [random.randint(1, 100) for _ in range(N)] def process_data_chunk(chunk): return [x * 2 for x in chunk] def execute_pipeline(N: int) -> int: # Step 1: Data Generation thread = threading.Thread(target=generate_data, args=(N,)) thread.start() thread.join() data = generate_data(N) # Step 2: Data Processing num_processes = 4 # You can adjust this based on your machine chunk_size = N // num_processes chunks = [data[i:i + chunk_size] for i in range(0, N, chunk_size)] with Pool(processes=num_processes) as pool: processed_chunks = pool.map(process_data_chunk, chunks) processed_data = [item for sublist in processed_chunks for item in sublist] # Step 3: Data Aggregation with ThreadPoolExecutor() as executor: future_sum = executor.submit(sum, processed_data) return future_sum.result()"},{"question":"**Seaborn Plotting Challenge** You are given the Titanic dataset, which provides information about the passengers aboard the Titanic. Using this dataset, you need to create and customize various plots utilizing the `seaborn` library to answer the following questions: 1. **Plot the count of passengers in each class, grouped by their survival status.** 2. **Create a percentage-based comparison of survival status within each class.** 3. **Generate a count plot of passengers grouped by `sex` and `embark_town`, ensuring that the bars are shown as percentages.** 4. **Visualize the relationship between `age` and the count of passengers in different age groups, categorizing by `survived` status. Use a histogram for this purpose.** Your output should be as follows: - Four separate plots as described above. - Ensure each plot includes a title and appropriate labels for clarity. Use the dataset and seaborn utilities provided in the documentation. **Expected Input and Output:** - No additional input needed apart from the existing Titanic dataset. - Output should be visual plots as described. **Constraints:** - Utilize seaborn plotting functions and customization options. - Ensure the code is self-contained and runs without errors. - Your solution should be efficient and readable. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Plot the count of passengers in each class, grouped by their survival status. plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") plt.title(\'Count of passengers in each class grouped by survival status\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() # 2. Create a percentage-based comparison of survival status within each class. plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\", stat=\\"percent\\") plt.title(\'Percentage-based survival status within each class\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() # 3. Generate a count plot of passengers grouped by sex and embark_town, shown as percentages. plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"sex\\", hue=\\"embark_town\\", stat=\\"percent\\") plt.title(\'Count plot of passengers by sex and embark_town as percentages\') plt.xlabel(\'Sex\') plt.ylabel(\'Percentage\') plt.legend(title=\'Embark Town\', loc=\'upper right\') plt.show() # 4. Visualize the relationship between age and the count of passengers, categorized by survival status. plt.figure(figsize=(10, 6)) sns.histplot(data=titanic, x=\\"age\\", hue=\\"survived\\", multiple=\\"stack\\") plt.title(\'Distribution of age groups by survival status\') plt.xlabel(\'Age\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() ``` Please ensure that the above code is tested, and it should generate the requested plots accurately when run.","solution":"import seaborn as sns import matplotlib.pyplot as plt def titanic_plots(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # 1. Plot the count of passengers in each class, grouped by their survival status. plt.figure(figsize=(10, 6)) sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") plt.title(\'Count of passengers in each class grouped by survival status\') plt.xlabel(\'Class\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() # 2. Create a percentage-based comparison of survival status within each class. plt.figure(figsize=(10, 6)) total_counts = titanic[\'class\'].value_counts() n_classes = len(total_counts) pct_df = titanic.groupby(\'class\')[\'survived\'].value_counts(normalize=True).rename(\'percentage\').mul(100).reset_index() sns.barplot(x=\'class\', y=\'percentage\', hue=\'survived\', data=pct_df) plt.title(\'Percentage-based survival status within each class\') plt.xlabel(\'Class\') plt.ylabel(\'Percentage\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() # 3. Generate a count plot of passengers grouped by sex and embark_town, shown as percentages. plt.figure(figsize=(10, 6)) sex_town_counts = titanic.groupby([\'sex\', \'embark_town\']).size() total_sex_counts = sex_town_counts.groupby(level=0).sum() pct_df = (sex_town_counts/total_sex_counts).rename(\'percentage\').mul(100).reset_index() sns.barplot(x=\'sex\', y=\'percentage\', hue=\'embark_town\', data=pct_df) plt.title(\'Count plot of passengers by sex and embark_town as percentages\') plt.xlabel(\'Sex\') plt.ylabel(\'Percentage\') plt.legend(title=\'Embark Town\', loc=\'upper right\') plt.show() # 4. Visualize the relationship between age and the count of passengers, categorized by survival status. plt.figure(figsize=(10, 6)) sns.histplot(data=titanic, x=\\"age\\", hue=\\"survived\\", multiple=\\"stack\\") plt.title(\'Distribution of age groups by survival status\') plt.xlabel(\'Age\') plt.ylabel(\'Count\') plt.legend(title=\'Survived\', loc=\'upper right\') plt.show() titanic_plots()"},{"question":"**Objective:** To assess the student’s ability to create a Python source distribution using `sdist`, customize the included files using a `MANIFEST.in` file, and understand the various options available. **Question:** You are given a Python project structured as follows: ``` my_project/ ├── examples/ │ ├── example1.py │ └── example2.py ├── scripts/ │ └── script.py ├── data/ │ ├── datafile.csv │ └── dataset/ │ └── dataset.csv ├── tests/ │ ├── test_1.py │ └── test_2.py ├── README.md ├── some_text.txt └── setup.py ``` Your task is to create a source distribution of this project, including only some of its parts based on the following instructions. 1. Include all `.py` files except those in the `tests` directory. 2. Include any `.txt` files in the root directory. 3. Exclude the entire `data` directory. 4. Include the `README.md` file. **Steps:** 1. **Write a `MANIFEST.in` file** that includes the necessary files listed above. The file should reside in the root directory (`my_project/`). 2. **Create a source distribution** of the project using the appropriate `sdist` command. **Constraints:** - Your `MANIFEST.in` file must be well-commented to explain the inclusion and exclusion rules applied. **Example Output:** Assuming the above directory structure and the correct implementation of the `MANIFEST.in` file, running the `sdist` command should produce a source distribution archive that contains: - `example1.py` - `example2.py` - `script.py` - `README.md` - `some_text.txt` - `setup.py` **Performance Requirements:** - The solution should handle the given directory structure efficiently. - Ensure that your `MANIFEST.in` file does not include unnecessary or excluded files. ```python # Assuming the following is your project’s directory structure # my_project/ # ├── examples/ # │ ├── example1.py # │ └── example2.py # ├── scripts/ # │ └── script.py # ├── data/ # │ ├── datafile.csv # │ └── dataset/ # │ └── dataset.csv # ├── tests/ # │ ├── test_1.py # │ └── test_2.py # ├── README.md # ├── some_text.txt # └── setup.py # Contents of a correctly written MANIFEST.in file include README.md include some_text.txt recursive-include examples *.py recursive-include scripts *.py prune tests prune data # After creating the MANIFEST.in file, create the source distribution by running: # python setup.py sdist ```","solution":"# MANIFEST.in - This file specifies the files to include or exclude in the source distribution. # Include the README.md include README.md # Include any .txt files in the root directory include *.txt # Include all .py files in the examples directory recursive-include examples *.py # Include all .py files in the scripts directory recursive-include scripts *.py # Exclude the entire tests directory prune tests # Exclude the entire data directory prune data"},{"question":"Objective: Assess the student\'s understanding and ability to implement custom signal handlers and manage signals in a Python application. Task Description: You are required to implement a Python program that simulates a countdown timer with the ability to handle interruption by a specific signal. Your program should: 1. Start a countdown timer from a specified number of seconds. 2. Implement and set a custom signal handler for `SIGINT` (typically triggered by Ctrl+C). 3. On receiving the `SIGINT` signal, the handler should print a custom message and allow the user to either resume the countdown or exit the program. 4. If the countdown reaches zero without interruption, the program should print a \\"Timer Finished\\" message. Input and Output: - The input to the program should be the number of seconds to countdown. - The output during normal operation should be the remaining seconds printed every second. - If interrupted by `SIGINT`, the program should print a custom message, prompt the user for action (either \\"r\\" to resume or \\"e\\" to exit), and behave according to the user\'s input. - At the end of the countdown, print a \\"Timer Finished\\" message. Constraints: - Implement the custom signal handler for `SIGINT` only. You should not handle other signals. - Ensure the program uses the `signal` module for handling the signal and custom handling logic. Example: ```python # Sample execution flow python countdown_timer.py 10 10 9 8 ^C Received SIGINT. Enter \'r\' to resume or \'e\' to exit: r 7 6 5 4 3 2 1 0 Timer Finished ``` Implementation: ```python import signal import time def custom_handler(signum, frame): print(\\"nReceived SIGINT. Enter \'r\' to resume or \'e\' to exit:\\", end=\\" \\") user_input = input().strip().lower() if user_input == \'r\': print(\\"Resuming countdown...\\") elif user_input == \'e\': print(\\"Exiting program...\\") exit(0) else: print(\\"Invalid input. Exiting program...\\") exit(0) def countdown_timer(seconds): signal.signal(signal.SIGINT, custom_handler) while seconds > 0: print(seconds) time.sleep(1) seconds -= 1 print(\\"Timer Finished\\") if __name__ == \\"__main__\\": seconds = int(input(\\"Enter the number of seconds for countdown: \\").strip()) countdown_timer(seconds) ``` Notes: - The above example code outlines the structure of the solution. Your task is to ensure the signal handling and resumption of the timer work as described. - Test your program thoroughly to handle various edge cases and ensure correct signal handling and user input.","solution":"import signal import time import sys def countdown_timer(seconds): def custom_handler(signum, frame): print(\\"nReceived SIGINT. Enter \'r\' to resume or \'e\' to exit:\\", end=\\" \\", flush=True) user_input = input().strip().lower() if user_input == \'r\': print(\\"Resuming countdown...\\") elif user_input == \'e\': print(\\"Exiting program...\\") sys.exit(0) else: print(\\"Invalid input. Exiting program...\\") sys.exit(0) signal.signal(signal.SIGINT, custom_handler) while seconds > 0: print(seconds) time.sleep(1) seconds -= 1 print(\\"Timer Finished\\") if __name__ == \\"__main__\\": try: seconds = int(input(\\"Enter the number of seconds for countdown: \\").strip()) countdown_timer(seconds) except ValueError: print(\\"Invalid input. Please enter an integer.\\")"},{"question":"Coding Assessment Question # Objective This exercise is designed to evaluate your understanding of unsupervised dimensionality reduction techniques and their application within pipelines for preprocessing your data using scikit-learn. # Problem Statement You are provided with a dataset containing a high number of features. Your task is to implement a data preprocessing pipeline that performs the following steps: 1. **Standard Scaling**: Standardize the features by removing the mean and scaling to unit variance. 2. **Dimensionality Reduction**: Reduce the dimensionality of the dataset using Principal Component Analysis (PCA). 3. **Model Training and Evaluation**: Train a supervised learning model (e.g., Support Vector Classifier) on the reduced feature set and evaluate its performance using cross-validation. # Input and Output formats 1. **Input**: - `X`: A 2D numpy array of shape `(n_samples, n_features)`, representing the feature matrix. - `y`: A 1D numpy array of shape `(n_samples,)`, representing the target vector. 2. **Output**: - Return the average cross-validation score computed from the evaluation step. # Constraints and Requirements - You may assume that the input matrix `X` has more than 50 features. - Use `StandardScaler` from `sklearn.preprocessing` for feature scaling. - Use `PCA` from `sklearn.decomposition` for dimensionality reduction. - Use `SVC` from `sklearn.svm` for the supervised learning model. - Use `cross_val_score` from `sklearn.model_selection` to perform 5-fold cross-validation. - The number of principal components to retain should be half the number of original features in the dataset. # Function Signature ```python def preprocess_and_evaluate(X: np.ndarray, y: np.ndarray) -> float: # your implementation here pass ``` # Example ```python import numpy as np # Example dataset X = np.random.rand(100, 60) # 100 samples, 60 features y = np.random.randint(0, 2, 100) # Binary target variable # Expected to return the average cross-validation score result = preprocess_and_evaluate(X, y) print(result) ``` # Explanation 1. **Preprocessing**: - **Standard Scaling**: Use `StandardScaler` to standardize the feature set. - **PCA**: Use `PCA(n_components=n_features//2)` to reduce the dimensionality of `X`. 2. **Model Training**: - Initialize an `SVC` with default parameters. - Evaluate the model using `cross_val_score` with 5-fold cross-validation. - Return the average cross-validation score as the performance metric.","solution":"from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.model_selection import cross_val_score import numpy as np def preprocess_and_evaluate(X: np.ndarray, y: np.ndarray) -> float: Preprocesses the data by standard scaling and dimensionality reduction, then trains a Support Vector Classifier and evaluates its performance using cross-validation. Args: - X: 2D numpy array of shape (n_samples, n_features) - y: 1D numpy array of shape (n_samples,) Returns: - Average cross-validation score as a float # Standard Scaling scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Dimensionality Reduction with PCA n_components = X.shape[1] // 2 pca = PCA(n_components=n_components) X_reduced = pca.fit_transform(X_scaled) # Model Training and Evaluation svc = SVC() scores = cross_val_score(svc, X_reduced, y, cv=5) # Return the average cross-validation score return scores.mean()"},{"question":"# Question: Environment Variables Manipulation As a Python developer, you are required to write a program that interacts with and manipulates environment variables using the `os` module, which provides a portable interface to the POSIX system calls. Task 1. **Retrieve all current environment variables and print them in the format `KEY=value`.** 2. **Add a new environment variable `MY_VARIABLE` with value `HelloWorld`.** 3. **Update the value of an existing environment variable `PATH` by appending `:/new/path`.** 4. **Delete an environment variable `OLD_VARIABLE` if it exists.** 5. **Print the updated environment variables again in the format `KEY=value` to reflect changes made.** Requirements - Do not use the `posix` module directly but use the `os` module. - Ensure that your solution works on both Unix and Windows systems. - Handle potential errors gracefully (e.g., if an environment variable does not exist). Input - No direct input is required. The script will interact with the system\'s environment variables. Output - The script should print the list of environment variables before and after modifications in the format `KEY=value`. Example ```python import os # Perform the tasks as described # ... # Expected output (an example, not an exhaustive list) # Before modifications: # PATH=/usr/bin:/bin # HOME=/home/user # ... # After modifications: # PATH=/usr/bin:/bin:/new/path # HOME=/home/user # MY_VARIABLE=HelloWorld # ... ``` Your task is to implement the described functionality in Python.","solution":"import os def manipulate_environment(): try: # Retrieve and print all current environment variables print(\\"Before modifications:\\") for key, value in os.environ.items(): print(f\\"{key}={value}\\") # Add a new environment variable MY_VARIABLE with value HelloWorld os.environ[\'MY_VARIABLE\'] = \'HelloWorld\' # Update the value of PATH by appending :/new/path if \'PATH\' in os.environ: os.environ[\'PATH\'] += \':/new/path\' # Delete an environment variable OLD_VARIABLE if it exists if \'OLD_VARIABLE\' in os.environ: del os.environ[\'OLD_VARIABLE\'] # Print the updated environment variables print(\\"nAfter modifications:\\") for key, value in os.environ.items(): print(f\\"{key}={value}\\") except Exception as e: print(f\\"An error occurred: {e}\\") # This function will be used in the main code execution, but won\'t execute during import if __name__ == \\"__main__\\": manipulate_environment()"},{"question":"Objective: Demonstrate your understanding of seaborn\'s `seaborn.objects` module by creating a custom, faceted scatter plot with linear regression lines, while applying specific themes and styles. Problem Statement: Using the provided `anscombe` dataset in seaborn, create a plot with the following specifications: 1. **Faceted Plot Structure**: - The dataset should be faceted based on the `dataset` column, with each facet showing a scatter plot of `x` vs `y`. - The faceting should wrap in 2 columns. 2. **Plot Layers**: - Add a scatter plot layer showing the data points. - Overlay a linear regression line (order=1) on each facet using polynomial fit. 3. **Customize Appearance**: - Set the background of the axes to white and the edge color to slategray. - Set the line width of the regression line to 3. - Apply the `ticks` style from seaborn for the axes. - Use the `fivethirtyeight` style from matplotlib. 4. **Default Configuration**: - Update the default theme for all Plot instances in this notebook to use the `whitegrid` style from seaborn and the `talk` context to display the text in larger size. Input: There is no input for this task as the dataset is loaded within the code. Output: A faceted plot should be displayed as specified. Constraints: - Use `seaborn.objects` for creating the plot. - Ensure the plot adheres to the specified appearance customizations and configuration settings. Example Code: ```python import seaborn.objects as so from seaborn import load_dataset, axes_style, plotting_context from matplotlib import style # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create the faceted plot with polynomial fit and scatter p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Customize the plot\'s appearance p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) p.theme({\\"lines.linewidth\\": 3}) p.theme(axes_style(\\"ticks\\")) p.theme(style.library[\\"fivethirtyeight\\"]) # Update the default theme for all Plot instances so.Plot.config.theme.update(axes_style(\\"whitegrid\\") | plotting_context(\\"talk\\")) # Display the plot p ``` Notes: - The provided example code is expected to be modified achieving the customization requirements and understanding how different theming and styling functions work in seaborn. - Ensure to test the code and the output plot should meet all the criteria specified.","solution":"import seaborn.objects as so from seaborn import load_dataset, axes_style, plotting_context from matplotlib import style def create_custom_faceted_plot(): # Load the dataset anscombe = load_dataset(\\"anscombe\\") # Create the faceted plot with polynomial fit and scatter p = ( so.Plot(anscombe, \\"x\\", \\"y\\", color=\\"dataset\\") .facet(\\"dataset\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) # Customize the plot\'s appearance p.theme({\\"axes.facecolor\\": \\"w\\", \\"axes.edgecolor\\": \\"slategray\\"}) p.theme({\\"lines.linewidth\\": 3}) p.theme(axes_style(\\"ticks\\")) p.theme(style.library[\\"fivethirtyeight\\"]) # Update the default theme for all Plot instances so.Plot.config.theme.update(axes_style(\\"whitegrid\\") | plotting_context(\\"talk\\")) # Display the plot p.show()"},{"question":"Objective You are required to write a Python program using the `tkinter` package that utilizes file dialogs for opening, saving, and manipulating files. Your task is to create a small application that allows the user to: 1. Select a text file, read its content, and display it. 2. Select a directory to save a copy of the read file with \\"_copy\\" appended to its name. 3. Save the content to a file chosen by the user. Requirements 1. **Function to Open and Read a File**: - Prompt the user to select a text file using `tkinter.filedialog.askopenfilename()`. - Read the content of the file and return it. 2. **Function to Select a Directory and Save the File**: - Prompt the user to select a directory using `tkinter.filedialog.askdirectory()`. - Save a copy of the file content with \\"_copy\\" appended to the original filename in the selected directory. 3. **Function to Save File Content As**: - Prompt the user to choose the filename and location using `tkinter.filedialog.asksaveasfilename()`. - Save the given content to the selected location. Implementation Details - You must use **tkinter** for the dialogs. - Handle any exceptions that may occur during file operations. - Ensure the content integrity when reading from and writing to files. Input and Output - The content of the file should be handled as text (`str`). - The application does not take command-line inputs. - Outputs and interactions are handled through Tkinter dialogs and the functions mentioned. Example ```python import tkinter as tk from tkinter import filedialog, messagebox def open_and_read_file(): # Your code here to return content of selected file def select_directory_and_save_copy(content): # Your code here to save a copy of the file content def save_file_as(content): # Your code here to save content to the selected file ``` **Constraints and Limitations**: - Focus on handling text files only. - You should not assume any default file path or content unless provided by the user through dialogs. - Your function should manage file-related exceptions (e.g., file not found, permission errors). Performance Requirements - Efficient file reading and writing. - Ensure the UI operations (dialogs) do not freeze or become unresponsive during file operations. Your task is to implement the three functions as described above. Additionally, you may write a small driver function or use Tkinter buttons to demonstrate the functionality.","solution":"import tkinter as tk from tkinter import filedialog, messagebox import os def open_and_read_file(): try: file_path = filedialog.askopenfilename(filetypes=[(\\"Text files\\", \\"*.txt\\")]) if not file_path: return None with open(file_path, \'r\') as file: content = file.read() return content except Exception as e: messagebox.showerror(\\"Error\\", f\\"Failed to read file: {e}\\") return None def select_directory_and_save_copy(content): try: directory_path = filedialog.askdirectory() if not directory_path: return False file_path = filedialog.askopenfilename(filetypes=[(\\"Text files\\", \\"*.txt\\")]) if not file_path: return False base_name = os.path.basename(file_path) new_file_path = os.path.join(directory_path, base_name.replace(\'.txt\', \'_copy.txt\')) with open(new_file_path, \'w\') as file: file.write(content) return True except Exception as e: messagebox.showerror(\\"Error\\", f\\"Failed to save file copy: {e}\\") return False def save_file_as(content): try: file_path = filedialog.asksaveasfilename(defaultextension=\\".txt\\", filetypes=[(\\"Text files\\", \\"*.txt\\")]) if not file_path: return False with open(file_path, \'w\') as file: file.write(content) return True except Exception as e: messagebox.showerror(\\"Error\\", f\\"Failed to save file: {e}\\") return False"},{"question":"# Question: Handling MIME Quoted-Printable Encoding and Decoding You are tasked with writing a Python program to handle the encoding and decoding of text data using the MIME quoted-printable format. The program will need to process both file-based and string-based data. This will help demonstrate your understanding of file I/O operations and string manipulation in Python, as well as your ability to use the `quopri` module. Function 1: encode_file Write a function `encode_file(input_file_path: str, output_file_path: str, quotetabs: bool, header: bool = False) -> None` that: - Reads the binary data from `input_file_path`. - Encodes it using `quopri.encode`. - Writes the encoded data to `output_file_path`. Function 2: decode_file Write a function `decode_file(input_file_path: str, output_file_path: str, header: bool = False) -> None` that: - Reads the binary data from `input_file_path`. - Decodes it using `quopri.decode`. - Writes the decoded data to `output_file_path`. Function 3: encode_string Write a function `encode_string(input_bytes: bytes, quotetabs: bool, header: bool = False) -> bytes` that: - Takes a bytes object as the input. - Encodes the data using `quopri.encodestring`. - Returns the encoded data as bytes. Function 4: decode_string Write a function `decode_string(input_bytes: bytes, header: bool = False) -> bytes` that: - Takes a bytes object as the input. - Decodes the data using `quopri.decodestring`. - Returns the decoded data as bytes. # Constraints - `input_file_path` and `output_file_path` are strings representing valid file paths. - `input_bytes` is a non-empty bytes object. - Your implementation should handle any edge cases related to file reading and writing (e.g., file not found, permission errors). # Example Usage ```python # Example file-based encoding and decoding encode_file(\'input.bin\', \'encoded.bin\', quotetabs=True, header=False) decode_file(\'encoded.bin\', \'decoded.bin\', header=False) # Example string-based encoding and decoding encoded_bytes = encode_string(b\'Hello World!\', quotetabs=False, header=True) decoded_bytes = decode_string(encoded_bytes, header=True) print(decoded_bytes) # Output should be: b\'Hello World!\' ``` # Notes - Write your solution so that it adheres to Python\'s best practices. - Ensure code readability and include appropriate comments. - Handle any potential exceptions in an elegant manner. - Include tests for each function to validate their correctness.","solution":"import quopri def encode_file(input_file_path: str, output_file_path: str, quotetabs: bool, header: bool = False) -> None: Reads binary data from input_file_path, encodes it using quopri.encode, and writes the encoded data to output_file_path. try: with open(input_file_path, \'rb\') as f_in, open(output_file_path, \'wb\') as f_out: quopri.encode(f_in, f_out, quotetabs=quotetabs, header=header) except Exception as e: print(f\\"An error occurred: {e}\\") def decode_file(input_file_path: str, output_file_path: str, header: bool = False) -> None: Reads binary data from input_file_path, decodes it using quopri.decode, and writes the decoded data to output_file_path. try: with open(input_file_path, \'rb\') as f_in, open(output_file_path, \'wb\') as f_out: quopri.decode(f_in, f_out, header=header) except Exception as e: print(f\\"An error occurred: {e}\\") def encode_string(input_bytes: bytes, quotetabs: bool, header: bool = False) -> bytes: Takes a bytes object as the input, encodes the data using quopri.encodestring, and returns the encoded data as bytes. return quopri.encodestring(input_bytes, quotetabs=quotetabs, header=header) def decode_string(input_bytes: bytes, header: bool = False) -> bytes: Takes a bytes object as the input, decodes the data using quopri.decodestring, and returns the decoded data as bytes. return quopri.decodestring(input_bytes, header=header)"},{"question":"# Assessing Python Tuple Manipulation You are required to implement several functions to manipulate and interact with tuples in Python. Your task is to mimic some functionalities mentioned in the provided documentation using pure Python. Function 1: `create_tuple` - **Description**: This function should create and return a new tuple of a given size with all elements initialized to `None`. - **Input**: An integer `size` representing the size of the tuple. - **Output**: A tuple of length `size`, with all elements initialized to `None`. Function 2: `tuple_get_item` - **Description**: This function should return an item from the tuple at a given position. - **Input**: A tuple `tup` and an integer `pos` representing the position. - **Output**: The object at position `pos` in the tuple `tup`. - **Constraints**: If `pos` is out of bounds, raise an `IndexError` exception. Function 3: `tuple_set_item` - **Description**: This function should set the value at a given position in a tuple and return the new tuple. - **Input**: A tuple `tup`, an integer `pos`, and an object `value`. - **Output**: A new tuple where the item at position `pos` is set to `value`. - **Constraints**: If `pos` is out of bounds, raise an `IndexError` exception. Since tuples are immutable, you need to create a new tuple with the updated value. Function 4: `tuple_slice` - **Description**: This function should return a slice of the tuple between two positions. - **Input**: A tuple `tup`, and two integers `low` and `high` representing the slice bounds. - **Output**: A tuple representing the slice `tup[low:high]`. Function 5: `is_tuple` - **Description**: This function should check if a given object is a tuple. - **Input**: An object `obj`. - **Output**: `True` if `obj` is a tuple, `False` otherwise. # Example Usage ```python # Example for Function 1 tup1 = create_tuple(3) print(tup1) # Output: (None, None, None) # Example for Function 2 elem = tuple_get_item((1, 2, 3), 1) print(elem) # Output: 2 # Example for Function 3 tup2 = tuple_set_item((1, 2, 3), 1, 99) print(tup2) # Output: (1, 99, 3) # Example for Function 4 tupslice = tuple_slice((1, 2, 3, 4, 5), 1, 4) print(tupslice) # Output: (2, 3, 4) # Example for Function 5 result = is_tuple([1, 2, 3]) print(result) # Output: False result = is_tuple((1, 2, 3)) print(result) # Output: True ``` Notes - Do not use Python\'s C API. - Follow typical Python conventions and ensure code readability. Good luck!","solution":"def create_tuple(size): Creates a new tuple of a given size with all elements initialized to None. :param size: The size of the tuple :return: A tuple of length `size`, with all elements initialized to None return (None,) * size def tuple_get_item(tup, pos): Returns an item from the tuple at a given position. :param tup: The input tuple :param pos: The position of the item to retrieve :return: The item at the given position :raises IndexError: If `pos` is out of bounds return tup[pos] def tuple_set_item(tup, pos, value): Sets the value at a given position in a tuple and return the new tuple. :param tup: The input tuple :param pos: The position where the value is to be updated :param value: The new value to place at the given position :return: A new tuple with the given position updated to the new value :raises IndexError: If `pos` is out of bounds if pos < 0 or pos >= len(tup): raise IndexError(\\"Position out of bounds\\") return tup[:pos] + (value,) + tup[pos + 1:] def tuple_slice(tup, low, high): Returns a slice of the tuple between two positions. :param tup: The input tuple :param low: The starting index of the slice :param high: The ending index of the slice :return: A tuple representing the slice `tup[low:high]` return tup[low:high] def is_tuple(obj): Checks if a given object is a tuple. :param obj: The object to check :return: `True` if `obj` is a tuple, `False` otherwise return isinstance(obj, tuple)"},{"question":"# **Advanced Signal Handling in Python** You are required to implement a solution to a synchronization problem using the `signal` module in Python. Your task is to create a program that manages a countdown timer while also reacting to specific user interruptions (e.g., pressing `Ctrl+C` which sends `SIGINT` signal). If the countdown is interrupted, the program should handle the interruption gracefully and resume the countdown from where it left off when the user confirms to continue. Requirements: 1. **Countdown Timer**: - Implement a countdown timer that starts from a given number of seconds and decrements to zero, printing the remaining time every second. 2. **Signal Handling**: - Install a signal handler for `SIGINT` that: - Pauses the countdown. - Prompts the user to either continue the countdown by pressing \'c\' or exit the program by pressing \'x\'. - If the user chooses to continue, the countdown should resume from the point it was interrupted. - If the user chooses to exit, the program should terminate gracefully. 3. **Constraints**: - You must use the `signal` module to handle `SIGINT`. - The countdown must accurately reflect the time left, considering any interruptions. 4. **Input and Output**: - The start time for the countdown will be provided as an integer input. - Your program should output the remaining time every second and handle interruptions as specified. Example: ```text Input: 10 Output: 10 9 8 ^C # User presses Ctrl+C Countdown paused. Press \'c\' to continue or \'x\' to exit: c 7 6 5 ^C # User presses Ctrl+C again Countdown paused. Press \'c\' to continue or \'x\' to exit: x Countdown terminated by user. ``` Solution Template: You can use the following template to start your implementation: ```python import signal import time def countdown(timer): def signal_handler(signum, frame): nonlocal paused paused = True print(\\"nCountdown paused. Press \'c\' to continue or \'x\' to exit: \\", end=\\"\\", flush=True) signal.signal(signal.SIGINT, signal_handler) paused = False while timer > 0: if not paused: print(timer) timer -= 1 time.sleep(1) else: user_input = input() if user_input.lower() == \'c\': paused = False elif user_input.lower() == \'x\': print(\\"Countdown terminated by user.\\") return print(\\"Countdown complete!\\") if __name__ == \\"__main__\\": start_time = int(input(\\"Enter countdown start time in seconds: \\")) countdown(start_time) ``` You need to implement the function `signal_handler` appropriately and ensure the countdown can be paused and resumed based on user input during `SIGINT` interruptions.","solution":"import signal import time def countdown(timer): def signal_handler(signum, frame): nonlocal paused paused = True print(\\"nCountdown paused. Press \'c\' to continue or \'x\' to exit: \\", end=\\"\\", flush=True) signal.signal(signal.SIGINT, signal_handler) paused = False while timer > 0: if not paused: print(timer) timer -= 1 time.sleep(1) else: user_input = input() if user_input.lower() == \'c\': paused = False elif user_input.lower() == \'x\': print(\\"Countdown terminated by user.\\") return print(\\"Countdown complete!\\") if __name__ == \\"__main__\\": start_time = int(input(\\"Enter countdown start time in seconds: \\")) countdown(start_time)"},{"question":"**Question Title**: Implementing and Managing Concurrent Execution in Python **Objective**: Write a Python program that demonstrates the use of threading and multiprocessing for concurrent execution. Your solution must include the creation and management of threads and processes, as well as synchronization mechanisms. **Problem Statement**: You are required to implement a program that simulates a simplified version of a task scheduler that can handle both CPU-bound and IO-bound tasks. The program should use the `threading` module for IO-bound tasks and the `multiprocessing` module for CPU-bound tasks. Additionally, ensure synchronization between tasks to prevent race conditions. # Requirements: 1. **Function Definitions**: Implement the following functions: * `cpu_bound_task(n: int) -> int`: A CPU-bound task that computes the sum of the first `n` natural numbers. * `io_bound_task(filename: str) -> int`: An IO-bound task that reads a file and returns the number of lines in the file. * `scheduler(task_list: list) -> dict`: A scheduler function that takes a list of tasks and executes them concurrently. The tasks are a mix of CPU-bound and IO-bound tasks. The function returns a dictionary with task identifiers as keys and their respective results as values. 2. **Threading for IO-bound tasks**: - Use the `threading` module to handle the execution of IO-bound tasks. - Synchronize access to a shared resource using a threading lock. 3. **Multiprocessing for CPU-bound tasks**: - Use the `multiprocessing` module to handle the execution of CPU-bound tasks. - Ensure that processes are properly synchronized when accessing shared resources. 4. **Task List Specification**: - A task list is a list of tuples, where each tuple includes a task identifier (string), task type (`\\"cpu\\"` or `\\"io\\"`), and relevant parameters. - Example: `[(\\"task1\\", \\"cpu\\", 1000), (\\"task2\\", \\"io\\", \\"file.txt\\")]` # Example The following example demonstrates how the function can be used: ```python task_list = [ (\\"task1\\", \\"cpu\\", 1000), (\\"task2\\", \\"io\\", \\"data.txt\\"), (\\"task3\\", \\"cpu\\", 500), (\\"task4\\", \\"io\\", \\"log.txt\\") ] result = scheduler(task_list) # Possible Output: # { # \\"task1\\": 500500, # \\"task2\\": 150, # \\"task3\\": 125250, # \\"task4\\": 45 # } ``` # Constraints: - You can assume that the CPU-bound task parameter `n` will be a positive integer. - IO-bound task parameters will be valid file names with readable files. - You must handle exceptions and edge cases appropriately. # Performance Requirements: - Ensure that the program efficiently handles multiple tasks and makes use of system resources appropriately. - Use suitable synchronization mechanisms to ensure thread and process safety. # Submission: - Provide the complete implementation of the `cpu_bound_task`, `io_bound_task`, and `scheduler` functions. - Include necessary imports and any helper functions or classes as required. Good Luck!","solution":"import threading import multiprocessing from typing import List, Dict, Tuple def cpu_bound_task(n: int) -> int: Computes the sum of the first n natural numbers. return n * (n + 1) // 2 def io_bound_task(filename: str) -> int: Reads a file and returns the number of lines in the file. with open(filename, \'r\') as file: lines = file.readlines() return len(lines) def execute_task(task, lock, results): task_id, task_type, param = task if task_type == \\"cpu\\": result = cpu_bound_task(param) elif task_type == \\"io\\": result = io_bound_task(param) else: raise ValueError(f\\"Unknown task type: {task_type}\\") with lock: results[task_id] = result def scheduler(task_list: List[Tuple[str, str, any]]) -> Dict[str, int]: Schedules and executes a list of tasks concurrently. manager = multiprocessing.Manager() results = manager.dict() lock = manager.Lock() threads = [] processes = [] for task in task_list: task_id, task_type, param = task if task_type == \\"cpu\\": p = multiprocessing.Process(target=execute_task, args=(task, lock, results)) processes.append(p) p.start() elif task_type == \\"io\\": t = threading.Thread(target=execute_task, args=(task, lock, results)) threads.append(t) t.start() else: raise ValueError(f\\"Unknown task type: {task_type}\\") # Join all processes and threads for p in processes: p.join() for t in threads: t.join() return dict(results)"},{"question":"# Cubehelix Palette Customization Challenge Objective The goal of this challenge is to test your understanding of Seaborn\'s `cubehelix_palette` functionality by creating a plot using a customized cubehelix palette. Task Given a dataset and the customization requirements, use the `cubehelix_palette` function to generate a colormap and apply it to a Seaborn plot. Dataset You are given the `tips` dataset which is included with Seaborn. This dataset contains information about tips received by waiters in a restaurant. You need to create a scatter plot showing the relationship between total bill and tip. Requirements 1. Use the `cubehelix_palette` function to generate a customized palette with the following specifications: - It should have exactly 10 colors. - Start the helix at `start=1.5`. - Rotate the helix to `rot=-0.8`. - Saturation of colors should be set to `hue=0.8`. - Luminance range should be set with `dark=0.3` and `light=0.7`. 2. Apply the customized palette to the scatter plot. 3. Use the scatter plot to visualize the relationship between `total_bill`(x-axis) and `tip`(y-axis). 4. Ensure the plot has a title \\"Customized Cubehelix Palette for Tips Data\\" and properly labeled axes. Input Format There is no explicit input in terms of a function; you are required to use the given datasets and customization requirements directly to generate your output. Output Format Your solution should generate and display the specified scatter plot. Constraints - You should strictly follow the customization requirements for `cubehelix_palette`. - Utilize Seaborn\'s built-in functions effectively to create the desired plot. Example ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a customized cubehelix palette palette = sns.cubehelix_palette(10, start=1.5, rot=-0.8, hue=0.8, dark=0.3, light=0.7) # Create a scatter plot with the customized palette sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", palette=palette) # Set the title and labels plt.title(\\"Customized Cubehelix Palette for Tips Data\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") # Display the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_customized_scatter_plot(): Creates a scatter plot using Seaborn\'s cubehelix_palette with specified customization. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a customized cubehelix palette palette = sns.cubehelix_palette(10, start=1.5, rot=-0.8, hue=0.8, dark=0.3, light=0.7) # Since sns.scatterplot does not take a palette directly, we will create the plot and then apply the colors # Plot the base scatter plot scatter_plot = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", hue=tips.index, palette=palette, legend=None) # Set the title and labels plt.title(\\"Customized Cubehelix Palette for Tips Data\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") # Display the plot plt.show()"},{"question":"You are tasked with developing a class in Python that represents a Node in a recursive data structure, such as a tree. Each Node contains an integer value, a list of child Nodes, and a reference to its parent Node. Users of this class should be able to create deep copies of these Nodes, along with their entire subtree, without the copies referencing any original instances. Your task is to implement the `Node` class and ensure it supports deep copying through the `__deepcopy__` method. # Requirements 1. Implement the `Node` class with: - An `__init__` method to initialize the node with a value, an optional parent, and an empty list of children. - A method `add_child` to add a child Node to the current Node. - The `__deepcopy__` method to return a deep copy of the node and its entire subtree. 2. Ensure the `__deepcopy__` method: - Uses the memo dictionary correctly to handle recursion and avoid infinite loops. - Properly copies all child nodes, and updates references to parent nodes to point to the newly copied nodes. # Input and Output - **Input**: There will be no direct input reading from users. Instead, you should ensure the correctness of your `__deepcopy__` method via a series of operations: ```python original_node = Node(1) child1 = Node(2, parent=original_node) child2 = Node(3, parent=original_node) original_node.add_child(child1) original_node.add_child(child2) ``` - **Output**: There will be no direct output. Instead, test your implementation as follows: ```python import copy copied_node = copy.deepcopy(original_node) ``` After deepcopying, changes to the copied node or its children should NOT affect the original node or its children, demonstrating a true deep copy. # Constraints - Assume nodes will always be valid instances of the `Node` class. - Your implementation should avoid infinite loops even if nodes reference each other in cycles. - Performance should be efficient enough to handle a tree with up to 10,000 nodes in reasonable time. # Example ```python class Node: def __init__(self, value, parent=None): self.value = value self.parent = parent self.children = [] def add_child(self, child_node): self.children.append(child_node) child_node.parent = self def __deepcopy__(self, memo): copied_node = Node(self.value) memo[id(self)] = copied_node copied_node.children = [copy.deepcopy(child, memo) for child in self.children] if self.parent: copied_node.parent = memo.get(id(self.parent)) return copied_node # Example usage import copy original_node = Node(1) child1 = Node(2, parent=original_node) child2 = Node(3, parent=original_node) original_node.add_child(child1) original_node.add_child(child2) # Create deep copy copied_node = copy.deepcopy(original_node) # Verifying deep copy (example check) child1.value = 10 print(original_node.children[0].value) # Should print 2, not 10 print(copied_node.children[0].value) # Should print 2 ``` Ensure you thoroughly test the deep copying functionality to cover edge cases such as cyclical references and proper parent-child linking.","solution":"class Node: def __init__(self, value, parent=None): self.value = value self.parent = parent self.children = [] def add_child(self, child_node): self.children.append(child_node) child_node.parent = self def __deepcopy__(self, memo): if id(self) in memo: return memo[id(self)] copied_node = Node(self.value) memo[id(self)] = copied_node copied_node.children = [copy.deepcopy(child, memo) for child in self.children] for child in copied_node.children: child.parent = copied_node return copied_node import copy # Example usage original_node = Node(1) child1 = Node(2, parent=original_node) child2 = Node(3, parent=original_node) original_node.add_child(child1) original_node.add_child(child2) # Create deep copy copied_node = copy.deepcopy(original_node)"},{"question":"You are tasked with creating a Python module that includes functions with detailed docstring examples. Using the `doctest` module, you will verify the correctness of these examples. Requirements 1. **Create a Module `math_operations.py`**: - Include at least three functions: `add(a, b)`, `subtract(a, b)`, and `multiply(a, b)`. - Each function should be documented with a description and at least two doctest examples illustrating its usage. 2. **Examples**: - Include normal output examples. - Include examples that raise exceptions (e.g., invalid inputs). 3. **Testing**: - At the end of the module, use `doctest.testmod()` to run all embedded doctests when the script is executed directly. 4. **Edge Cases and Validations**: - Functions should handle typical edge cases (e.g., type errors) and document these in the examples. Example for `math_operations.py` ```python Module math_operations Provides basic mathematical operations. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(2, \'3\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. def add(a, b): Return the sum of a and b. >>> add(4, 5) 9 >>> add(10, -3) 7 >>> add(7, \'3\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\'Both arguments must be numbers.\') return a + b def subtract(a, b): Return the difference when b is subtracted from a. >>> subtract(10, 5) 5 >>> subtract(3, 7) -4 >>> subtract(5, \'2\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\'Both arguments must be numbers.\') return a - b def multiply(a, b): Return the product of a and b. >>> multiply(4, 5) 20 >>> multiply(2, -3) -6 >>> multiply(3, \'2\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\'Both arguments must be numbers.\') return a * b if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` Constraints - Functions should be designed to reflect good coding practices, including input validation. - Examples should cover typical use cases and edge cases. - Test cases must be exact and should not use options like `ELLIPSIS`. Submission Submit the `math_operations.py` module file.","solution":"Module math_operations Provides basic mathematical operations. >>> add(2, 3) 5 >>> add(-1, 1) 0 >>> add(2, \'3\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. >>> subtract(10, 5) 5 >>> subtract(3, 7) -4 >>> subtract(5, \'2\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. >>> multiply(4, 5) 20 >>> multiply(2, -3) -6 >>> multiply(3, \'2\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. def add(a, b): Return the sum of a and b. >>> add(4, 5) 9 >>> add(10, -3) 7 >>> add(7, \'3\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\'Both arguments must be numbers.\') return a + b def subtract(a, b): Return the difference when b is subtracted from a. >>> subtract(10, 5) 5 >>> subtract(3, 7) -4 >>> subtract(5, \'2\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\'Both arguments must be numbers.\') return a - b def multiply(a, b): Return the product of a and b. >>> multiply(4, 5) 20 >>> multiply(2, -3) -6 >>> multiply(3, \'2\') Traceback (most recent call last): ... TypeError: Both arguments must be numbers. if not isinstance(a, (int, float)) or not isinstance(b, (int, float)): raise TypeError(\'Both arguments must be numbers.\') return a * b if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"**Objective:** Demonstrate your understanding of TorchScript by implementing and using custom TorchScript classes, tensors, functions, and annotations. Problem Statement 1. **Define a TorchScript Class:** - Create a TorchScript class `CustomModule` that has: - An initializer method (`__init__`) that takes a tensor `input_tensor`` and initializes an attribute `self.input_tensor`. - A method `scale_tensor` that takes a floating-point scale factor and returns the scaled tensor `self.input_tensor`. - A method `split_tensor` that takes an integer `chunks` and returns a list of tensors by splitting `self.input_tensor` into the specified number of chunks along its first dimension. 2. **Define a TorchScript Enum:** - Create a TorchScript-compatible enum `Operation` inside the `torch.enums` namespace with values: - `ADD` mapped to 1 - `SUBTRACT` mapped to 2 3. **Implement a Function using TorchScript:** - Write a TorchScript function `apply_operation` that: - Takes two tensors `tensor_a` and `tensor_b`, and an `Operation` enum instance `op`. - Returns the result of tensor addition if `op` is `ADD` and returns the result of tensor subtraction if `op` is `SUBTRACT`. 4. **Use the Defined Class and Function:** - Create an instance of `CustomModule` with a random tensor of shape (10, 5). - Scale the tensor by a factor of 2.0 using the `scale_tensor` method. - Split the scaled tensor into 5 chunks using the `split_tensor` method. - Perform addition and subtraction between two of these chunks using `apply_operation` with respective enum instances. 5. **Constraints:** - Ensure that all types and methods are correctly annotated to be compatible with TorchScript. - Validate your implementation by asserting the shapes and values of the tensors from the operations. Expected Input and Output Format * **Input:** ```python input_tensor = torch.rand(10, 5) scale_factor = 2.0 chunks = 5 op_add = torch.enums.Operation.ADD op_sub = torch.enums.Operation.SUBTRACT ``` * **Output:** - The scaled tensor. - List of 5 split tensors. - Result of tensor addition. - Result of tensor subtraction. Performance Requirements: - The function should execute efficiently for tensor operations, leveraging PyTorch capabilities for batch operations. **Sample Code Template:** ```python import torch from typing import List # Define the custom TorchScript class @torch.jit.script class CustomModule: def __init__(self, input_tensor: torch.Tensor): self.input_tensor = input_tensor def scale_tensor(self, scale: float) -> torch.Tensor: return self.input_tensor * scale def split_tensor(self, chunks: int) -> List[torch.Tensor]: return torch.chunk(self.input_tensor, chunks, dim=0) # Define the enum Operation from enum import Enum class Operation(Enum): ADD = 1 SUBTRACT = 2 # JIT-compile the enum torch.enums.Operation = torch.jit.script(Operation) # Implement apply_operation function @torch.jit.script def apply_operation(tensor_a: torch.Tensor, tensor_b: torch.Tensor, op: torch.enums.Operation) -> torch.Tensor: if op == torch.enums.Operation.ADD: return tensor_a + tensor_b elif op == torch.enums.Operation.SUBTRACT: return tensor_a - tensor_b else: raise ValueError(\\"Unsupported operation\\") # Example usage - Validate the functionality input_tensor = torch.rand(10, 5) module = CustomModule(input_tensor) scaled_tensor = module.scale_tensor(2.0) split_tensors = module.split_tensor(5) result_add = apply_operation(split_tensors[0], split_tensors[1], torch.enums.Operation.ADD) result_sub = apply_operation(split_tensors[0], split_tensors[1], torch.enums.Operation.SUBTRACT) # Expected output checks print(scaled_tensor.shape) print([t.shape for t in split_tensors]) print(result_add.shape, result_sub.shape) ``` Implement the appropriate methods and ensure the correct behavior using the provided sample code.","solution":"import torch from typing import List from enum import Enum # Define the custom TorchScript class class CustomModule(torch.jit.ScriptModule): def __init__(self, input_tensor: torch.Tensor): super(CustomModule, self).__init__() self.input_tensor = input_tensor @torch.jit.script_method def scale_tensor(self, scale: float) -> torch.Tensor: return self.input_tensor * scale @torch.jit.script_method def split_tensor(self, chunks: int) -> List[torch.Tensor]: return torch.chunk(self.input_tensor, chunks, dim=0) # Define the enum Operation class Operation(Enum): ADD = 1 SUBTRACT = 2 torch.classes.Operation = Operation # To register the enum for torchscript # Implement apply_operation function @torch.jit.script def apply_operation(tensor_a: torch.Tensor, tensor_b: torch.Tensor, op: torch.classes.Operation) -> torch.Tensor: if op == torch.classes.Operation.ADD: return tensor_a + tensor_b elif op == torch.classes.Operation.SUBTRACT: return tensor_a - tensor_b else: raise ValueError(\\"Unsupported operation\\") # Example usage input_tensor = torch.rand(10, 5) module = CustomModule(input_tensor) scaled_tensor = module.scale_tensor(2.0) split_tensors = module.split_tensor(5) result_add = apply_operation(split_tensors[0], split_tensors[1], torch.classes.Operation.ADD) result_sub = apply_operation(split_tensors[0], split_tensors[1], torch.classes.Operation.SUBTRACT) # Expected output checks print(scaled_tensor.shape) print([t.shape for t in split_tensors]) print(result_add.shape, result_sub.shape)"},{"question":"**Objective**: Write a Python function that implements the Isomap algorithm using scikit-learn, applies it to the provided dataset, and visualizes the results. **Description**: Using the `Isomap` class from the `sklearn.manifold` module, embed a dataset in lower-dimensional space while maintaining the geodesic distances between all points. You will then visualize the results using a 2D scatter plot. **Function Signature**: ```python def apply_isomap(data: List[List[float]], n_neighbors: int, n_components: int) -> None: Applies the Isomap algorithm on the given data and visualizes the result. Parameters: - data: List of List of floats, the high-dimensional dataset to be reduced. - n_neighbors: int, number of neighbors to consider for each point. - n_components: int, the number of dimensions in the embedded space. Returns: - None: The function should display a 2D scatter plot of the embedded data. ``` **Input Constraints**: - `data` is a list of lists where each inner list represents a data point with dimensionality greater than `n_components`. - `n_neighbors` is an integer in the range [2, number of data points). - `n_components` is an integer in the range [1, dimensionality of data). **Performance Requirements**: The implementation should efficiently handle datasets with up to 1000 data points and up to 100 dimensions. **Steps**: 1. Implement the function `apply_isomap` using `Isomap` from `sklearn.manifold`. 2. Apply the `Isomap` algorithm to reduce the dataset to `n_components` dimensions while using `n_neighbors` nearest neighbors. 3. Visualize the results in a 2D scatter plot using libraries such as matplotlib or seaborn. **Example**: ```python import numpy as np # Generating synthetic data: a 3D spiral t = np.linspace(0, 4 * np.pi, 100) x = t * np.cos(t) y = t * np.sin(t) z = t data = np.vstack((x, y, z)).T.tolist() # Applying isomap apply_isomap(data, n_neighbors=10, n_components=2) ``` The provided `apply_isomap` function should output a 2-dimensional scatter plot of the embedded data, where the structure of the original data is preserved as much as possible in the reduced space. **Considerations**: - Handle exceptions appropriately, such as ensuring n_neighbors and n_components are within valid ranges. - Ensure the visualization is clear and comprehensible, with appropriate axis labels and titles. **Evaluation Criteria**: - Correctness: The function must perform Isomap embedding and visualize the result accurately. - Efficiency: The function should execute efficiently within the provided performance constraints. - Code Quality: The code should be clean, with proper comments and adherence to Python programming standards.","solution":"from typing import List import matplotlib.pyplot as plt from sklearn.manifold import Isomap def apply_isomap(data: List[List[float]], n_neighbors: int, n_components: int) -> None: Applies the Isomap algorithm on the given data and visualizes the result. Parameters: - data: List of List of floats, the high-dimensional dataset to be reduced. - n_neighbors: int, number of neighbors to consider for each point. - n_components: int, the number of dimensions in the embedded space. Returns: - None: The function should display a 2D scatter plot of the embedded data. # Validate input parameters if not (2 <= n_neighbors < len(data)): raise ValueError(\\"n_neighbors must be in the range [2, number of data points - 1).\\") if not (1 <= n_components < len(data[0])): raise ValueError(\\"n_components must be in the range [1, dimensionality of data).\\") # Create the Isomap model isomap = Isomap(n_neighbors=n_neighbors, n_components=n_components) # Fit and transform the data using the Isomap model transformed_data = isomap.fit_transform(data) # Plot the result if the embedding space is 2D if n_components == 2: plt.scatter(transformed_data[:, 0], transformed_data[:, 1], c=\'blue\', marker=\'o\') plt.title(\'Isomap Embedding\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.grid(True) plt.show() else: print(\\"Embedding completed. The number of components is not 2, so no plot is displayed.\\")"},{"question":"# Unix Group Database Manipulation Problem Statement You are provided access to the Unix group database through the `grp` module. Your task is to implement a function that performs specific manipulations on this database. Task Write a Python function `get_group_usernames_containing_substring(substring: str) -> List[str]` that takes a substring as input and returns a list of usernames of all members in any group whose name contains that substring. The list should contain unique usernames sorted in alphabetical order. Input - A string `substring` (1 <= len(substring) <= 100) that represents the substring to search within group names. Output - A list of unique usernames (strings), sorted in alphabetical order. If no group name contains the substring, return an empty list. Constraints - The function should handle potential exceptions when interacting with the `grp` module. - Assume that you have access to a Unix system where the `grp` module is operational. - Performance should be considered for the case where there are many groups and users. Example ```python # Example usage: # Suppose the group database has the following entries: # [ # grp.struct_group((\'admin\', \'x\', 101, [\'alice\', \'bob\'])), # grp.struct_group((\'staff\', \'x\', 102, [\'charlie\', \'dave\'])), # grp.struct_group((\'admins\', \'x\', 103, [\'eve\', \'frank\'])), # ] assert get_group_usernames_containing_substring(\'admin\') == [\'alice\', \'bob\', \'eve\', \'frank\'] assert get_group_usernames_containing_substring(\'staff\') == [\'charlie\', \'dave\'] assert get_group_usernames_containing_substring(\'user\') == [] ``` # Implementation Notes 1. You must use the `grp` module to fetch the group entries. 2. Handle exceptions such as `KeyError` and `TypeError` gracefully. 3. Ensure the returned list of usernames is unique and sorted in alphabetical order. 4. You may import additional standard libraries if necessary. # Function Signature ```python from typing import List import grp def get_group_usernames_containing_substring(substring: str) -> List[str]: pass ```","solution":"from typing import List import grp def get_group_usernames_containing_substring(substring: str) -> List[str]: try: all_groups = grp.getgrall() usernames = set() for group in all_groups: if substring in group.gr_name: usernames.update(group.gr_mem) return sorted(usernames) except Exception as e: # Handle potential errors (though grp.getgrall() does not raise an exception per the Python documentation) return []"},{"question":"**Question: Creating Complex Seaborn Plots with Custom Layout** You are given a dataset `df` which contains the following columns: - `category`: Categorical data with values \\"A\\", \\"B\\", \\"C\\". - `value_1`: Numeric data. - `value_2`: Numeric data. Using the seaborn.objects module, perform the following tasks: 1. Create a base `Plot` object with overall dimensions of 6x6 inches. 2. Generate a facet plot, creating subplots based on `category`. Arrange the subplots with two rows and two columns. 3. Use the \\"constrained\\" layout engine to improve the layout of the subplots. 4. Adjust the extent of each subplot to take up 80% of the width and 100% of the height of the figure. 5. Save this figure as `output.png`. **Constraints:** - You must use the seaborn.objects module. - The layout dimensions and settings must be applied programmatically. **Expected Parts of Solution:** - A well-defined function to create and save the plot, named `create_custom_plot(df)`. - The function should take a pandas DataFrame `df` as input and save the plot as `output.png` in the current working directory. **Example Execution:** ```python import pandas as pd data = { \\"category\\": [\\"A\\", \\"B\\", \\"A\\", \\"C\\", \\"B\\", \\"A\\", \\"C\\", \\"B\\", \\"C\\", \\"A\\"], \\"value_1\\": [4.1, 3.2, 5.8, 7.4, 2.4, 5.5, 8.0, 3.1, 7.7, 5.9], \\"value_2\\": [1.1, 3.3, 2.8, 7.0, 2.2, 4.0, 6.5, 3.0, 7.1, 4.2] } df = pd.DataFrame(data) create_custom_plot(df) ``` Upon running the example, a file named `output.png` should be created, containing the required seaborn plot with correct facets, layout, and dimensions.","solution":"import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plot(df): Creates a custom seaborn plot with facets based on the \'category\' column and saves it as \'output.png\'. # Create the base plot p = so.Plot(df, x=\\"value_1\\", y=\\"value_2\\", color=\\"category\\") # Adjust the figure size and use constrained layout fig, axs = plt.subplots(2, 2, figsize=(6, 6), constrained_layout=True) # Set the extent to 80% width and 100% height for ax in axs.flat: ax.set_position([ax.get_position().x0, ax.get_position().y0, ax.get_position().width * 0.8, ax.get_position().height]) # Create the facets p.add(so.Dot()).facet(\\"category\\", wrap=2).on(fig) # Save the plot fig.savefig(\\"output.png\\") # Example DataFrame data = { \\"category\\": [\\"A\\", \\"B\\", \\"A\\", \\"C\\", \\"B\\", \\"A\\", \\"C\\", \\"B\\", \\"C\\", \\"A\\"], \\"value_1\\": [4.1, 3.2, 5.8, 7.4, 2.4, 5.5, 8.0, 3.1, 7.7, 5.9], \\"value_2\\": [1.1, 3.3, 2.8, 7.0, 2.2, 4.0, 6.5, 3.0, 7.1, 4.2] } df = pd.DataFrame(data) create_custom_plot(df)"},{"question":"**Question: Implement a Python program using the `py_compile` module** **Objective:** Write a Python function `compile_source_files(file_list, cfile_base=None, optimize_level=-1, log_errors_to_file=None)` to compile a list of Python source files into byte-code. **Function Specifications:** - **Input**: - `file_list` (List of strings): A list of paths to Python source files to be compiled. - `cfile_base` (string, optional): A directory path where the byte-code files should be saved. If not provided, the default location as per `PEP 3147`/`PEP 488` will be used. - `optimize_level` (int, optional): The optimization level, ranging from -1 to 2, default is -1. - `log_errors_to_file` (string, optional): A path to a file where compilation errors should be logged. If not provided, errors should be written to `sys.stderr`. - **Output**: - `result` (Dictionary): A dictionary where keys are the source file paths and values are the paths to the compiled byte-code files or error messages if compilation failed. The function should: 1. Compile each source file in `file_list` using `py_compile.compile()`. 2. Handle errors appropriately: - If `log_errors_to_file` is provided, write error messages to that file. - If `log_errors_to_file` is not provided, write error messages to `sys.stderr`. 3. Return a dictionary mapping each source file path to its corresponding byte-code file path or an error message. 4. Use the provided `cfile_base` directory for saving byte-code files if specified. **Example:** ```python import os def compile_source_files(file_list, cfile_base=None, optimize_level=-1, log_errors_to_file=None): import py_compile import sys result = {} error_log = sys.stderr if log_errors_to_file is None else open(log_errors_to_file, \'w\') try: for file in file_list: try: cfile = None if cfile_base is None else os.path.join(cfile_base, os.path.basename(file) + \'c\') compiled_path = py_compile.compile(file, cfile=cfile, optimize=optimize_level, doraise=True) result[file] = compiled_path except py_compile.PyCompileError as e: error_message = f\\"Error compiling {file}: {e.msg}n\\" error_log.write(error_message) result[file] = error_message except FileExistsError as e: error_message = f\\"FileExistsError for {file}: {str(e)}n\\" error_log.write(error_message) result[file] = error_message finally: if log_errors_to_file is not None: error_log.close() return result ``` **Constraints:** - `file_list` will contain valid paths to Python source files. - `cfile_base`, if provided, will be a valid directory path. **Additional Notes:** - Ensure to handle both types of raised exceptions: `py_compile.PyCompileError` and `FileExistsError`. - Maintain the directory structure of byte-code files if `cfile_base` is not specified. - Test your function with various scenarios including different `optimize_level` values and invalid source files.","solution":"import os import py_compile import sys def compile_source_files(file_list, cfile_base=None, optimize_level=-1, log_errors_to_file=None): Compiles a list of Python source files into byte-code. Parameters: - file_list (list of str): List of paths to Python source files to be compiled. - cfile_base (str, optional): Directory where the byte-code files should be saved. - optimize_level (int, optional): Optimization level, from -1 to 2. Default is -1. - log_errors_to_file (str, optional): Path to a file to write compilation errors or \'None\' to use sys.stderr. Returns: - dict: A dictionary where keys are source file paths and values are the compiled byte-code file paths or error messages. result = {} error_log = sys.stderr if log_errors_to_file is None else open(log_errors_to_file, \'w\') try: for file in file_list: try: cfile = None if cfile_base is None else os.path.join(cfile_base, os.path.basename(file) + \'c\') compiled_path = py_compile.compile(file, cfile=cfile, optimize=optimize_level, doraise=True) result[file] = compiled_path except py_compile.PyCompileError as e: error_message = f\\"Error compiling {file}: {e.msg}n\\" error_log.write(error_message) result[file] = error_message except FileExistsError as e: error_message = f\\"FileExistsError for {file}: {str(e)}n\\" error_log.write(error_message) result[file] = error_message finally: if log_errors_to_file is not None: error_log.close() return result"},{"question":"Objective: Utilize the `shlex` module to create a function that properly parses a complex shell command string, ensuring proper handling of quotes, escape characters, and shell-like syntax. Problem Statement: You are tasked to implement a function `parse_shell_command(command: str) -> List[str]` that takes a shell command string and returns a list of parsed tokens according to the following rules: 1. **Whitespace:** Tokens are split by whitespace. 2. **Quotes:** Quotes (`\'` or `\\"`) should group the enclosed text into a single token. 3. **Escape Characters:** Escape characters (``) should preserve the literal value of the character that follows. 4. **Punctuation Characters:** Treat the characters `();<>|&` as separate tokens (even when they appear in runs). Input: - A string `command` which represents the shell command to be parsed. Output: - A list of strings where each string is a token from the parsed command. Examples: ```python assert parse_shell_command(\'echo \\"Hello, World\\" && ls -l / && cd /home/user\') == [\'echo\', \'Hello, World\', \'&&\', \'ls\', \'-l\', \'/\', \'&&\', \'cd\', \'/home/user\'] assert parse_shell_command(\'cat file.txt || echo \'Error: file not found\'\') == [\'cat\', \'file.txt\', \'||\', \'echo\', \'Error: file not found\'] assert parse_shell_command(\'path/to/script.sh > output.log 2>&1\') == [\'path/to/script.sh\', \'>\', \'output.log\', \'2>&1\'] assert parse_shell_command(\\"grep \'pattern with spaces\' file | sort -n\\") == [\'grep\', \'pattern with spaces\', \'file\', \'|\', \'sort\', \'-n\'] ``` Constraints: 1. The implementation should use the `shlex` library. 2. Ensure that any shell-like syntax and escaping rules are obeyed. 3. You should handle both POSIX mode and compatibility mode of `shlex`. Additional Information: - Do not assume any specific platform or shell. - Focus on Unix-like syntax; Windows shell syntax is not required. Helpful Reference: Refer to the provided `shlex` documentation to understand the behaviors and attributes you might need to use in your solution. ```python from shlex import shlex def parse_shell_command(command: str) -> List[str]: # Your implementation here pass ``` Implement the `parse_shell_command` function to match the above requirements and examples.","solution":"from shlex import shlex def parse_shell_command(command: str) -> list: Parses a shell command string into a list of tokens using shlex. lexer = shlex(command, posix=True) lexer.whitespace_split = True lexer.commenters = \'\' return list(lexer)"},{"question":"You have been provided with a partial set of documentation for a Python package responsible for installing modules and managing source distribution files, particularly using \\"manifest template\\" commands. Your task is to write a Python function that, given a list of filenames and a set of manifest template rules, returns a list of filenames that match the inclusion criteria and do not match the exclusion criteria. # Function Definition ```python def filter_files(filenames: List[str], rules: List[str]) -> List[str]: Filters the list of filenames based on the given manifest template rules. Parameters: - filenames: List[str] : A list of file paths as strings. - rules: List[str] : A list of rules to apply for including and excluding files. Returns: - List[str] : A list of filenames after applying the inclusion and exclusion rules. ``` # Input Details - `filenames` is a list of strings where each string represents a file path. - `rules` is a list of strings representing the manifest template commands. The list can have commands like `include`, `exclude`, `recursive-include`, `recursive-exclude`, `global-include`, `global-exclude`, `prune`, and `graft` followed by patterns or directory names. # Constraints 1. Commands and patterns follow Unix-style glob patterns. 2. Each rule is guaranteed to be valid and well-formed. 3. A filename can only be included in the result if it has not been excluded by any applicable rule. 4. Rules are processed in the order they are given, meaning later rules take precedence over earlier ones. # Example ```python filenames = [ \\"src/module/file1.py\\", \\"src/module/file2.py\\", \\"data/data1.csv\\", \\"data/data2.csv\\" ] rules = [ \\"include src/module/*.py\\", \\"exclude src/module/file2.py\\", \\"global-include *.csv\\", \\"prune data\\" ] filtered_files = filter_files(filenames, rules) print(filtered_files) ``` # Expected Output ```python [\'src/module/file1.py\'] ``` **Explanation:** - `src/module/*.py` includes `file1.py` and `file2.py`. - `src/module/file2.py` is then excluded by the next rule. - `*.csv` would include `data1.csv` and `data2.csv`, but the prune command excludes all under `data`. Make sure your implementation effectively uses these rules to filter out the filenames correctly.","solution":"import fnmatch from typing import List def filter_files(filenames: List[str], rules: List[str]) -> List[str]: Filters the list of filenames based on the given manifest template rules. Parameters: - filenames: List[str] : A list of file paths as strings. - rules: List[str] : A list of rules to apply for including and excluding files. Returns: - List[str] : A list of filenames after applying the inclusion and exclusion rules. included_files = set() excluded_files = set() pruned_dirs = set() for rule in rules: parts = rule.split() command = parts[0] pattern = \\" \\".join(parts[1:]) if command == \\"include\\": for filename in filenames: if fnmatch.fnmatch(filename, pattern): included_files.add(filename) elif command == \\"exclude\\": for filename in filenames: if fnmatch.fnmatch(filename, pattern): excluded_files.add(filename) elif command == \\"recursive-include\\": dir_path, file_pattern = pattern.split(None, 1) for filename in filenames: if filename.startswith(dir_path) and fnmatch.fnmatch(filename[len(dir_path):], file_pattern): included_files.add(filename) elif command == \\"recursive-exclude\\": dir_path, file_pattern = pattern.split(None, 1) for filename in filenames: if filename.startswith(dir_path) and fnmatch.fnmatch(filename[len(dir_path):], file_pattern): excluded_files.add(filename) elif command == \\"global-include\\": for filename in filenames: if fnmatch.fnmatch(filename, pattern): included_files.add(filename) elif command == \\"global-exclude\\": for filename in filenames: if fnmatch.fnmatch(filename, pattern): excluded_files.add(filename) elif command == \\"prune\\": for filename in filenames: if filename.startswith(pattern): pruned_dirs.add(pattern) elif command == \\"graft\\": for filename in filenames: if filename.startswith(pattern): included_files.add(filename) final_included_files = {file for file in included_files if file not in excluded_files} final_files = [file for file in final_included_files if not any(file.startswith(dir) for dir in pruned_dirs)] return sorted(final_files)"},{"question":"Objective: Implement a class that manages a collection of student grades and provides functionalities to calculate various statistics, and write comprehensive unit tests for your implementation. Description: You need to create a class `StudentGrades` that can manage student names and their respective grades. Your class should offer methods to add grades, retrieve the highest-grade student, calculate the average grade, and determine students who are performing below a certain threshold. Requirements: 1. Implement the `StudentGrades` class with the following methods: - `add_grade(student_name: str, grade: float) -> None` - `get_highest_grade_student() -> str` - `calculate_average() -> float` - `students_below_threshold(threshold: float) -> List[str]` 2. Handle invalid inputs elegantly. For example: - Raise an appropriate exception if an invalid grade is added (grades should be between 0 and 100). - Avoid division by zero error in average calculation if there are no grades added. Constraints: - Grades range between 0.0 and 100.0. - No constraints on the number of students. - Use Python 3.10+ features. Performance: - Assume the number of students and grades is reasonably large, but efficiency is not the prime concern for this exercise. Example Usage: ```python grades = StudentGrades() grades.add_grade(\\"Alice\\", 85.0) grades.add_grade(\\"Bob\\", 92.5) grades.add_grade(\\"Charlie\\", 78.0) print(grades.get_highest_grade_student()) # Output: Bob print(grades.calculate_average()) # Output: 85.17 (approx) print(grades.students_below_threshold(80.0)) # Output: [\\"Charlie\\"] ``` Additional Task: Write comprehensive unit tests for the `StudentGrades` class using the `unittest` framework. Ensure to: - Test all methods individually. - Check edge cases such as no students, invalid grades, etc. - Use decorators and context managers from the `test.support` module where applicable. Submission: Provide: 1. The implementation of `StudentGrades` class. 2. A separate module containing your `unittest` test cases. Good luck and make sure your code is clean, well-documented, and handles edge cases!","solution":"from typing import List class StudentGrades: def __init__(self): self.grades = {} def add_grade(self, student_name: str, grade: float) -> None: if not isinstance(grade, (int, float)) or not 0.0 <= grade <= 100.0: raise ValueError(\\"Grade must be a number between 0.0 and 100.0\\") self.grades[student_name] = grade def get_highest_grade_student(self) -> str: if not self.grades: return None return max(self.grades.items(), key=lambda item: item[1])[0] def calculate_average(self) -> float: if not self.grades: return 0.0 return sum(self.grades.values()) / len(self.grades) def students_below_threshold(self, threshold: float) -> List[str]: return [student for student, grade in self.grades.items() if grade < threshold]"},{"question":"# Question: Time Series Resampling and Analysis You are provided with a time series dataset representing the daily closing prices of a stock. Your task is to resample this time series to a monthly frequency and perform various analyses on the resampled data. Instructions: 1. Load the time series data from a CSV file named `stock_prices.csv`. The CSV file has two columns: \\"Date\\" and \\"Close\\", where \\"Date\\" is the index. 2. Resample the time series data to a monthly frequency by taking the mean of the closing prices for each month. 3. Calculate the following statistics on the resampled data: - Mean closing price for each month - Standard deviation of the closing prices for each month - Total sum of the closing prices for each month 4. Forward fill any missing values in the original daily series before resampling. 5. Plot the original daily series and the resampled monthly series on the same plot for comparison. Input: - A CSV file named `stock_prices.csv` with columns \\"Date\\" (format: YYYY-MM-DD) and \\"Close\\" containing the daily closing prices of the stock. Output: - A plot showing the original daily time series and the resampled monthly series. - Printed summary statistics: mean, standard deviation, and sum of the closing prices for each month. Constraints: - You must use the `pandas` library for this task. - Handle any missing dates in the daily series by forward filling before resampling. Example: Given `stock_prices.csv`: ``` Date,Close 2022-01-01,150 2022-01-02,152 2022-01-03,153 2022-02-01,157 2022-02-02,158 2022-02-03,160 ... ``` Your output should include: 1. A plot comparing the original and resampled series. 2. Printed statistics similar to: ``` Mean Closing Price per Month: 2022-01 151.666667 2022-02 158.333333 ... Standard Deviation of Closing Prices per Month: 2022-01 1.527525 2022-02 1.527525 ... Total Sum of Closing Prices per Month: 2022-01 455 2022-02 475 ... ``` # Solution Template: ```python import pandas as pd import matplotlib.pyplot as plt # Load the data df = pd.read_csv(\'stock_prices.csv\', parse_dates=[\'Date\'], index_col=\'Date\') # Forward fill missing values in the daily series df = df.ffill() # Resample the data to a monthly frequency, taking the mean of the closing prices monthly_resampled = df.resample(\'M\').mean() # Calculate summary statistics mean_prices = monthly_resampled[\'Close\'].mean() std_prices = monthly_resampled[\'Close\'].std() sum_prices = monthly_resampled[\'Close\'].sum() # Print summary statistics print(\\"Mean Closing Price per Month:\\") print(monthly_resampled[\'Close\'].mean()) print(\\"nStandard Deviation of Closing Prices per Month:\\") print(monthly_resampled[\'Close\'].std()) print(\\"nTotal Sum of Closing Prices per Month:\\") print(monthly_resampled[\'Close\'].sum()) # Plot the original daily series and the resampled monthly series plt.figure(figsize=(12, 6)) plt.plot(df[\'Close\'], label=\'Daily Closing Prices\') plt.plot(monthly_resampled[\'Close\'], label=\'Monthly Resampled Closing Prices\', linewidth=3) plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.title(\'Daily vs. Monthly Resampled Closing Prices\') plt.legend() plt.show() ```","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_process_data(file_path): Loads the time series data from a CSV file, forward fills missing values, and resamples the data to monthly frequency. Args: file_path (str): The path to the CSV file. Returns: pd.DataFrame: Resampled monthly data. # Load the data df = pd.read_csv(file_path, parse_dates=[\'Date\'], index_col=\'Date\') # Forward fill missing values in the daily series df = df.ffill() # Resample the data to a monthly frequency, taking the mean of the closing prices monthly_resampled = df.resample(\'M\').mean() return df, monthly_resampled def calculate_statistics(monthly_resampled): Calculates summary statistics for the resampled monthly data. Args: monthly_resampled (pd.DataFrame): Resampled monthly data. Returns: dict: A dictionary containing the mean, standard deviation, and sum. statistics = { \'mean\': monthly_resampled[\'Close\'].mean(), \'std\': monthly_resampled[\'Close\'].std(), \'sum\': monthly_resampled[\'Close\'].sum() } return statistics def plot_series(daily_data, monthly_data): Plots the original daily series and the resampled monthly series. Args: daily_data (pd.DataFrame): Original daily closing price data. monthly_data (pd.DataFrame): Resampled monthly closing price data. plt.figure(figsize=(12, 6)) plt.plot(daily_data[\'Close\'], label=\'Daily Closing Prices\') plt.plot(monthly_data[\'Close\'], label=\'Monthly Resampled Closing Prices\', linewidth=3) plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.title(\'Daily vs. Monthly Resampled Closing Prices\') plt.legend() plt.show()"},{"question":"**Coding Assessment Question:** # Task Implement a function `secure_zip_operations(zip_path: str, target_dir: str) -> list:` that performs the following operations: 1. **Extracts** all files from a given ZIP archive `zip_path` to the directory `target_dir`. 2. **Lists** all files (full paths) extracted to `target_dir`. 3. **Creates a new ZIP archive** named `filtered_<original_zip_name>.zip`, containing only the files from `target_dir` that are not empty. # Specifications - The function should handle errors gracefully and raise appropriate exceptions (e.g., `zipfile.BadZipFile`, `zipfile.LargeZipFile`). - While extracting files, ensure that no file extraction can lead to path traversal attacks (i.e., no files should be extracted outside of `target_dir`). - All extracted file paths should be listed in sorted order. - Do not include empty files in the newly created ZIP archive. # Input - `zip_path` (str): The path to the original ZIP archive. - `target_dir` (str): The directory where the files will be extracted. # Output - Return a list of all extracted file paths in sorted order. # Constraints - `zip_path` and `target_dir` are valid paths. - The function should work correctly with large ZIP files using ZIP64 extensions. # Example ```python # Example usage: zip_path = \'example.zip\' target_dir = \'extracted_files\' # Calling the function extracted_files = secure_zip_operations(zip_path, target_dir) print(extracted_files) # Output: List of file paths extracted in sorted order ``` # Notes 1. **Do not use external libraries** for ZIP file handling other than the `zipfile` module. 2. Assume the necessary import statements (`import zipfile`, `import os`, `import pathlib`) are present at the beginning of your script.","solution":"import zipfile import os import pathlib def secure_zip_operations(zip_path: str, target_dir: str) -> list: try: with zipfile.ZipFile(zip_path, \'r\') as zip_ref: # Ensure the target directory exists os.makedirs(target_dir, exist_ok=True) # Extract files safely file_paths = [] for file_info in zip_ref.infolist(): extracted_path = os.path.join(target_dir, file_info.filename) if not os.path.abspath(extracted_path).startswith(os.path.abspath(target_dir)): raise Exception(\\"Path traversal attempt detected!\\") if not file_info.is_dir(): zip_ref.extract(file_info, target_dir) file_paths.append(extracted_path) # Filter out empty files non_empty_file_paths = [ path for path in file_paths if os.path.getsize(path) > 0 ] # Create new ZIP with non-empty files original_zip_name = os.path.splitext(os.path.basename(zip_path))[0] new_zip_path = os.path.join(target_dir, f\\"filtered_{original_zip_name}.zip\\") with zipfile.ZipFile(new_zip_path, \'w\') as new_zip_ref: for file_path in non_empty_file_paths: new_zip_ref.write(file_path, os.path.relpath(file_path, target_dir)) # Return sorted list of extracted file paths return sorted(file_paths) except zipfile.BadZipFile: raise Exception(\\"The provided file is not a valid ZIP archive.\\") except zipfile.LargeZipFile: raise Exception(\\"The ZIP file is too large.\\")"},{"question":"You are required to write a Python function that takes in a raw email message using the `BytesFeedParser` class and returns a dictionary containing the following extracted information: - **Subject**: The subject of the email. - **From**: The sender of the email. - **To**: The recipient(s) of the email. - **Body**: The main body text of the email. The email message will be provided as a bytes-like object and may be incrementally given in parts. Use the `BytesFeedParser` to process the message incrementally and ensure that your solution handles both simple, non-MIME messages and multipart MIME messages correctly. Function Signature ```python def parse_email_incrementally(email_parts: list) -> dict: pass ``` Input - `email_parts` (list): A list of bytes-like objects representing parts of the email message. Each element in the list can be fed incrementally into the parser. Output - Returns a dictionary with the following keys and corresponding values: - `Subject` (str): The subject of the email. - `From` (str): The sender of the email. - `To` (str): The recipient(s) of the email. - `Body` (str): The main body text of the email. Constraints - Handle both MIME and non-MIME messages. - Handle messages where the body can be plain text or HTML. - Assume each part in `email_parts` contains valid bytes-like data that together form a complete email message when combined. # Example ```python email_parts = [ b\\"From: sender@example.comrn\\", b\\"To: recipient@example.comrn\\", b\\"Subject: Test Emailrn\\", b\\"rn\\", b\\"This is a test email body.\\", ] print(parse_email_incrementally(email_parts)) ``` Expected Output ```python { \\"Subject\\": \\"Test Email\\", \\"From\\": \\"sender@example.com\\", \\"To\\": \\"recipient@example.com\\", \\"Body\\": \\"This is a test email body.\\" } ``` # Additional Notes - You are encouraged to use the `email` package\'s parser and message object methods to handle the parsing and extraction processes efficiently. - Correctly identify and handle multipart messages, ensuring all parts of the body are concatenated or combined as necessary. - Ensure your function is both efficient and robust across a variety of email message structures.","solution":"from email.parser import BytesFeedParser from email.policy import default def parse_email_incrementally(email_parts): parser = BytesFeedParser(policy=default) for part in email_parts: parser.feed(part) msg = parser.close() email_data = { \\"Subject\\": msg.get(\\"Subject\\", \\"\\"), \\"From\\": msg.get(\\"From\\", \\"\\"), \\"To\\": msg.get(\\"To\\", \\"\\") } if msg.is_multipart(): parts = [part.get_payload(decode=True).decode(part.get_content_charset() or \\"utf-8\\") for part in msg.iter_parts() if part.get_content_type() == \'text/plain\'] email_data[\\"Body\\"] = \\"n\\".join(parts) else: email_data[\\"Body\\"] = msg.get_payload(decode=True).decode(msg.get_content_charset() or \\"utf-8\\") return email_data"},{"question":"**Title**: Implement and Manage a Warehouse Inventory System using Dataclasses **Objective**: To assess the ability to create and manage complex dataclasses, utilize various customization and utility features of the `dataclasses` module, and apply object-oriented principles in a practical scenario. Problem Statement: You are tasked with implementing a warehouse inventory management system using Python\'s `dataclasses` module. The system should handle different types of items and manage their respective inventories, while also supporting common operations like adding new items, updating inventory levels, and calculating total warehouse value. Requirements: 1. **Define a Dataclass for Warehouse Items**: - Create a dataclass named `WarehouseItem` with the following fields: - `item_id`: An integer representing the unique ID of the item. - `name`: A string representing the name of the item. - `category`: A string representing the category of the item (e.g., \\"Electronics\\", \\"Clothing\\"). - `unit_price`: A float representing the price per unit of the item. - `quantity_on_hand`: An integer representing the quantity of the item in the warehouse. Default is 0. - Implement a method `total_value` within `WarehouseItem` to calculate the total value of the item in inventory (i.e., `unit_price * quantity_on_hand`). 2. **Create a Dataclass for the Warehouse**: - Create a dataclass named `Warehouse` with the following fields: - `name`: A string representing the name of the warehouse. - `items`: A list of `WarehouseItem` instances representing the inventory of the warehouse. Initialize with an empty list using `default_factory`. - Implement the following methods within `Warehouse`: - `add_item(self, item: WarehouseItem)`: Adds a new item to the warehouse inventory. - `update_quantity(self, item_id: int, quantity: int)`: Updates the quantity of an existing item in the warehouse. If the item does not exist, raise a `ValueError`. - `total_inventory_value(self) -> float`: Calculates and returns the total value of all items in the warehouse. 3. **Implement Utility Functions**: - Implement a function `export_inventory(warehouse: Warehouse) -> dict` that converts the entire inventory of the warehouse to a dictionary using `dataclasses.asdict()`. - Implement a function `import_inventory(data: dict) -> Warehouse` that creates a `Warehouse` instance from a dictionary. Constraints: - Use the `dataclasses` module features appropriately, including those specified above (e.g., `field(default_factory=list)`). - Ensure immutability of individual item prices and names by marking related fields as `frozen`. Handle this within the `WarehouseItem` dataclass. - Validate that the `unit_price` and `quantity_on_hand` are non-negative numbers within `__post_init__()` method. Example: ```python from dataclasses import dataclass, field, asdict, replace # Define your dataclasses and functions here... # Example Usage item1 = WarehouseItem(item_id=1, name=\\"Laptop\\", category=\\"Electronics\\", unit_price=999.99, quantity_on_hand=10) item2 = WarehouseItem(item_id=2, name=\\"Jeans\\", category=\\"Clothing\\", unit_price=49.99, quantity_on_hand=50) warehouse = Warehouse(name=\\"Main Warehouse\\") warehouse.add_item(item1) warehouse.add_item(item2) warehouse.update_quantity(item_id=1, quantity=15) print(warehouse.total_inventory_value()) # Output the total inventory value print(export_inventory(warehouse)) # Export inventory to a dictionary data = export_inventory(warehouse) imported_warehouse = import_inventory(data) print(imported_warehouse) ``` **Expected Solution Layout**: ```python @dataclass(frozen=True) class WarehouseItem: item_id: int name: str category: str unit_price: float quantity_on_hand: int = 0 def total_value(self) -> float: ... @dataclass class Warehouse: name: str items: list[WarehouseItem] = field(default_factory=list) def add_item(self, item: WarehouseItem): ... def update_quantity(self, item_id: int, quantity: int): ... def total_inventory_value(self) -> float: ... def export_inventory(warehouse: Warehouse) -> dict: ... def import_inventory(data: dict) -> Warehouse: ... # Example usage code with expected output ``` **Notes**: - Ensure to test the solution with various scenarios, including adding, updating items, and validating constraints.","solution":"from dataclasses import dataclass, field, asdict from typing import List @dataclass(frozen=True) class WarehouseItem: item_id: int name: str category: str unit_price: float quantity_on_hand: int = 0 def __post_init__(self): if self.unit_price < 0: raise ValueError(\\"unit_price cannot be negative\\") if self.quantity_on_hand < 0: raise ValueError(\\"quantity_on_hand cannot be negative\\") def total_value(self) -> float: return self.unit_price * self.quantity_on_hand @dataclass class Warehouse: name: str items: List[WarehouseItem] = field(default_factory=list) def add_item(self, item: WarehouseItem): self.items.append(item) def update_quantity(self, item_id: int, quantity: int): for idx, item in enumerate(self.items): if item.item_id == item_id: if quantity < 0: raise ValueError(\\"quantity_on_hand cannot be negative\\") self.items[idx] = WarehouseItem( item_id=item.item_id, name=item.name, category=item.category, unit_price=item.unit_price, quantity_on_hand=quantity ) return raise ValueError(f\\"Item with ID {item_id} not found\\") def total_inventory_value(self) -> float: return sum(item.total_value() for item in self.items) def export_inventory(warehouse: Warehouse) -> dict: return asdict(warehouse) def import_inventory(data: dict) -> Warehouse: items = [WarehouseItem(**item) for item in data[\\"items\\"]] return Warehouse(name=data[\\"name\\"], items=items)"},{"question":"# System Information and Management Using the `sys` Module Objective Create a Python program that uses the `sys` module to gather system information, handle exceptions gracefully, and manage certain system behaviors. Tasks 1. **System Information Retrieval** - Write a function `get_system_info()` that retrieves and returns a dictionary containing: - `python_version`: The version of the Python interpreter. - `executable_path`: The absolute path of the Python executable. - `platform`: The platform identifier. - `max_unicode`: The maximum Unicode code point. - `byte_order`: The native byte order (`\'little\'` or `\'big\'`). 2. **Exception Handling and Reporting** - Write a function `safe_division(a, b)` that: - Takes two arguments, `a` and `b`, and returns the result of `a / b`. - If `b` is zero, it should catch the exception and return a string `\\"division by zero not allowed\\"`. - Additionally, it should log the exception details using `sys.excepthook` to a file named `exception_log.txt`. 3. **Managing System Behavior** - Write a function `manage_sys_behaviors()` that: - Modifies the recursion limit using `sys.setrecursionlimit` to 500. - Sets a custom `sys.displayhook` that prints values as `<<value>>` instead of the usual representation. - Restores the original `sys.displayhook` after use. Constraints - Your functions should handle edge cases gracefully. - Ensure that the program doesn\'t affect subsequent code executions after modifying the system behaviors. Example ```python def main(): print(\\"System Information:\\") for key, value in get_system_info().items(): print(f\\"{key}: {value}\\") print(\\"nSafe Division Tests:\\") print(safe_division(10, 2)) # Expected: 5.0 print(safe_division(10, 0)) # Expected: \\"division by zero not allowed\\" print(\\"nManaging System Behaviors:\\") manage_sys_behaviors() # Example of the new display hook in action x = 42 # This should print <<42>> when code is run in interactive mode or Jupyter if __name__ == \\"__main__\\": main() ``` Output - The dictionary from `get_system_info()`. - Results of the `safe_division` function calls. - Demonstration of the new `displayhook` behavior. Note: Ensure to test the program thoroughly and ensure it conforms to the constraints and example provided.","solution":"import sys def get_system_info(): Retrieves system information using the sys module. return { \'python_version\': sys.version, \'executable_path\': sys.executable, \'platform\': sys.platform, \'max_unicode\': sys.maxunicode, \'byte_order\': sys.byteorder } def safe_division(a, b): Safely divides a by b, handling division by zero and logging exceptions. try: return a / b except ZeroDivisionError as e: with open(\'exception_log.txt\', \'a\') as f: sys.excepthook(*sys.exc_info()) f.write(str(e) + \'n\') return \\"division by zero not allowed\\" def manage_sys_behaviors(): Manages system behaviors by setting the recursion limit and custom displayhook. # Modify the recursion limit sys.setrecursionlimit(500) # Save the original displayhook original_displayhook = sys.displayhook # Define a custom displayhook def custom_displayhook(value): if value is not None: print(f\'<<{value}>>\') # Set the custom displayhook sys.displayhook = custom_displayhook # Example of the new displayhook in action x = 42 # This should print <<42>> when code is run in interactive mode or Jupyter # Restore the original displayhook sys.displayhook = original_displayhook"},{"question":"# Advanced Coding Assessment Question **Objective:** Demonstrate your understanding of Python\'s `dataclasses` module by designing a complex dataclass with initialization, default values, post-initialization logic, and conversion functions. **Problem Statement:** You are tasked with creating a data management system for a library. This system will include classes for books, patrons (library members), and borrowing records. Implement the following requirements using Python\'s `dataclasses` module: 1. **LibraryBook Class**: - Fields: `title (str)`, `author (str)`, `isbn (str)`, `quantity (int, default=0)`. - Method: `is_available()` returning `True` if `quantity > 0`, otherwise `False`. 2. **LibraryPatron Class**: - Fields: `name (str)`, `email (str)`, `patron_id (str, unique identification)`, `borrowed_books (list, default empty list)`. - Use `default_factory` for `borrowed_books`. 3. **BorrowRecord Class**: - Fields: `book (LibraryBook)`, `patron (LibraryPatron)`, `borrow_date (str, format \'YYYY-MM-DD\')`, `return_date (str, optional)`. - Post-initialization: Append borrowed book to `patron.borrowed_books` list and reduce available `quantity` of the borrowed book (use `__post_init__()`). Implement the following functions: - `record_to_dict(record: BorrowRecord) -> dict`: Converts a `BorrowRecord` instance to a dictionary using `dataclasses.asdict()`. - `record_from_dict(data: dict) -> BorrowRecord`: Converts a dictionary back to a `BorrowRecord` instance using `dataclasses.replace()` if necessary. **Constraints:** - ISBN numbers are unique for each book. - Patron IDs are unique for each library patron. - The `return_date` field can be `None` if the book has not been returned. **Example Usage:** ```python # Create instances of LibraryBook book1 = LibraryBook(title=\\"Book One\\", author=\\"Author A\\", isbn=\\"ISBN0001\\", quantity=3) # Create an instance of LibraryPatron patron1 = LibraryPatron(name=\\"Patron One\\", email=\\"patron1@example.com\\", patron_id=\\"P001\\") # Record a book borrowing record1 = BorrowRecord(book=book1, patron=patron1, borrow_date=\\"2023-10-01\\") # Output the record as a dictionary record_dict = record_to_dict(record1) print(record_dict) # Restore a BorrowRecord from the dictionary restored_record = record_from_dict(record_dict) print(restored_record) ``` Please implement the above classes and functions within the provided constraints. **Expected Output:** The provided example should create objects, manipulate their states according to library operations, and accurately convert between dataclass instances and dictionaries.","solution":"from dataclasses import dataclass, field, asdict, replace from typing import List, Optional @dataclass class LibraryBook: title: str author: str isbn: str quantity: int = 0 def is_available(self) -> bool: return self.quantity > 0 @dataclass class LibraryPatron: name: str email: str patron_id: str borrowed_books: List[LibraryBook] = field(default_factory=list) @dataclass class BorrowRecord: book: LibraryBook patron: LibraryPatron borrow_date: str return_date: Optional[str] = None def __post_init__(self): self.book.quantity -= 1 self.patron.borrowed_books.append(self.book) def record_to_dict(record: BorrowRecord) -> dict: return asdict(record) def record_from_dict(data: dict) -> BorrowRecord: book_data = data.pop(\'book\') patron_data = data.pop(\'patron\') book = LibraryBook(**book_data) patron = LibraryPatron(**patron_data) return replace(BorrowRecord(book=book, patron=patron, **data))"},{"question":"# Challenging Python Import System Question Objective You are given a legacy Python project that uses the deprecated `imp` module for various import-related functionalities. Your task is to modernize this project by replacing all usages of the `imp` module with equivalent functionalities from the `importlib` module. Background The `imp` module, deprecated since Python 3.4, has been largely replaced by the `importlib` module. You are required to create equivalent functions using `importlib` for the following deprecated `imp` functionalities: 1. Find a module. 2. Load a module. 3. Reload a module. 4. Create a new empty module object. Task Implement the following functions using `importlib`: 1. `find_module(name: str, path: Optional[List[str]] = None) -> Tuple[Optional[IO], str, Tuple[str, str, str]]`: - **Input**: - `name` (str): The name of the module to find. - `path` (Optional[List[str]]): A list of directory paths to search for the module. - **Output**: Return a tuple (file, pathname, description) where: - `file`: the file object of the module. - `pathname`: the path to the module. - `description`: a tuple (suffix, mode, type). 2. `load_module(name: str, file: Optional[IO], pathname: str, description: Tuple[str, str, str]) -> ModuleType`: - **Input**: - `name` (str): The name of the module to load. - `file` (Optional[IO]): The file object of the module. - `pathname` (str): The path to the module. - `description` (Tuple[str, str, str]): The description tuple (suffix, mode, type). - **Output**: Return the loaded module object. 3. `reload_module(module: ModuleType) -> ModuleType`: - **Input**: - `module` (ModuleType): The previously imported module object. - **Output**: Return the reloaded module object. 4. `new_module(name: str) -> ModuleType`: - **Input**: - `name` (str): The name of the new module. - **Output**: Return a new empty module object with the given `name`. Constraints - You must use the `importlib` module and avoid using the deprecated `imp` module. - Ensure your functions handle exceptions appropriately and provide meaningful messages for errors. - Maintain compatibility with the latest Python 3.8+ versions. Example Usage ```python # Example usage of find_module and load_module module_info = find_module(\\"json\\") json_module = load_module(\\"json\\", *module_info) # Example usage of reload_module reloaded_json_module = reload_module(json_module) # Example usage of new_module custom_module = new_module(\\"custom\\") ```","solution":"import importlib.util import importlib.machinery import sys from types import ModuleType from typing import Optional, List, Tuple, IO def find_module(name: str, path: Optional[List[str]] = None) -> Tuple[Optional[IO], str, Tuple[str, str, str]]: Find a module by name. It mimics \'imp.find_module\' functionality using \'importlib\'. Parameters: - name: The name of the module to find. - path: A list of directory paths to search for the module (optional). Returns: - A tuple (file, pathname, description) where: - file: the file object of the module. - pathname: the path to the module. - description: a tuple (suffix, mode, type). spec = importlib.util.find_spec(name, path) if spec is None or spec.origin is None: raise ImportError(f\\"Module {name} not found\\") file = open(spec.origin, \'rb\') if spec.origin else None description = (\'.py\', \'rb\', importlib.machinery.SOURCE_SUFFIXES) # Use SOURCE_SUFFIXES for .py files return file, spec.origin, description def load_module(name: str, file: Optional[IO], pathname: str, description: Tuple[str, str, str]) -> ModuleType: Load a module by name, using the file object, pathname and description tuple. Parameters: - name: The name of the module to load. - file: The file object of the module. - pathname: The path to the module. - description: A tuple (suffix, mode, type). Returns: - The loaded module object. spec = importlib.util.spec_from_file_location(name, pathname) if spec is None: raise ImportError(f\\"Cannot load module {name} from {pathname}\\") module = importlib.util.module_from_spec(spec) loader = spec.loader if loader and hasattr(loader, \'exec_module\'): loader.exec_module(module) sys.modules[name] = module return module def reload_module(module: ModuleType) -> ModuleType: Reload a previously loaded module. Parameters: - module: The previously imported module object. Returns: - The reloaded module object. return importlib.reload(module) def new_module(name: str) -> ModuleType: Create a new empty module object. Parameters: - name: The name of the new module. Returns: - A new empty module object with the given name. module = ModuleType(name) sys.modules[name] = module return module"},{"question":"# Advanced HTTP Fetching and Error Handling Using `urllib.request` You are tasked with fetching data from a given URL with specific requirements. Your function should: 1. Accept a URL and a dictionary of headers as input. 2. Fetch the content from the URL using an HTTP GET request. 3. Handle HTTP redirects and retry the request up to a maximum of 3 times if necessary. 4. Add custom headers (if provided) to the request. 5. Set a socket timeout of 5 seconds for the request. 6. Handle and return different types of errors gracefully by capturing and printing relevant information. Implement the function `fetch_url` with the following signature: ```python def fetch_url(url: str, headers: dict = None) -> str: Fetches data from the provided URL and handles potential errors. Args: url (str): The URL to fetch data from. headers (dict): A dictionary of HTTP headers to include in the request (optional). Returns: str: The content retrieved from the URL. Raises: ValueError: If an invalid URL is provided. RuntimeError: If a network-related error occurs. ``` # Constraints - The function should handle up to 3 redirects. - The function should time out if a response is not received within 5 seconds. - If the server returns an error (4xx or 5xx status codes), raise an appropriate error with the status code and message. - If the URL is invalid or cannot be reached, raise a `ValueError` with an appropriate message. - Use the `urllib.request` module only for fetching data and handling requests. # Performance Requirements - Ensure that the function efficiently handles redirects and retries. - The function should handle errors in a precise and user-friendly manner, providing clear error messages. # Example Usage ```python try: content = fetch_url(\\"http://example.com\\", headers={\\"User-Agent\\": \\"MyApp/1.0\\"}) print(content) except ValueError as e: print(f\\"Invalid URL or failed to reach server: {e}\\") except RuntimeError as e: print(f\\"An error occurred while fetching the URL: {e}\\") ``` # Testing - Test with various URLs including valid URLs, URLs requiring redirects, and invalid URLs. - Test with and without custom headers. - Test the timeout functionality with slow or non-responding URLs. - Verify that error handling works as expected for different HTTP status codes.","solution":"import urllib.request import urllib.error import socket def fetch_url(url: str, headers: dict = None) -> str: Fetches data from the provided URL and handles potential errors. Args: url (str): The URL to fetch data from. headers (dict): A dictionary of HTTP headers to include in the request (optional). Returns: str: The content retrieved from the URL. Raises: ValueError: If an invalid URL is provided. RuntimeError: If a network-related error occurs. max_redirects = 3 timeout = 5 if headers is None: headers = {} request = urllib.request.Request(url, headers=headers) tries = 0 while tries <= max_redirects: try: with urllib.request.urlopen(request, timeout=timeout) as response: # Check if response is a redirect if response.status in (301, 302): url = response.getheader(\'Location\') request = urllib.request.Request(url, headers=headers) tries += 1 else: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: if e.code in (301, 302) and tries < max_redirects: # Handle redirect url = e.headers[\'Location\'] request = urllib.request.Request(url, headers=headers) tries += 1 else: raise RuntimeError(f\'HTTP error occurred: {e.code}, {e.reason}\') except urllib.error.URLError as e: raise ValueError(f\'Invalid URL or server issue: {e.reason}\') except socket.timeout: raise RuntimeError(\'Request timed out\') except Exception as e: raise RuntimeError(f\'An unexpected error occurred: {e}\') raise RuntimeError(\'Maximum redirect limit exceeded\')"},{"question":"# Question You are provided with a dataset \\"penguins\\" that contains various measurements of penguins. Your task is to create a function `penguin_ecdf_plot` that plots the empirical cumulative distribution function (ECDF) for a specified measurement and allows customization based on the following parameters: Function Signature ```python def penguin_ecdf_plot( dataset: DataFrame, measurement: str, axis: str = \\"x\\", hue: str = None, stat: str = \\"proportion\\", complementary: bool = False ) -> None: ``` Parameters: - **dataset (DataFrame):** The pandas DataFrame containing the penguin dataset. - **measurement (str):** The column name of the measurement to plot (e.g., \\"flipper_length_mm\\", \\"bill_length_mm\\"). - **axis (str, default=\\"x\\"):** The axis to plot the measurement on (\\"x\\" or \\"y\\"). - **hue (str, optional):** Column name for hue mapping to plot multiple ECDFs. - **stat (str, default=\\"proportion\\"):** Statistic to plot, either \\"proportion\\", \\"count\\", or \\"percent\\". - **complementary (bool, default=False):** Whether to plot the complementary CDF. Constraints: - The `dataset` must have the `measurement` column specified. - The `axis` parameter should only take \\"x\\" or \\"y\\" as valid inputs. - The `stat` parameter should only take \\"proportion\\", \\"count\\", or \\"percent\\" as valid inputs. - The `complementary` parameter must be a boolean value. Output: - A plot of the ECDF based on the provided parameters. Task: 1. Implement the `penguin_ecdf_plot` function. 2. Use the given `penguins` dataset to demonstrate the functionality of the function with the following examples: - Plot ECDF of \\"flipper_length_mm\\" on the x-axis. - Plot ECDF of \\"bill_length_mm\\" on the y-axis with hue based on species. - Plot the complementary ECDF of \\"bill_length_mm\\" with hue based on species and stat as \\"count\\". You are expected to use seaborn and matplotlib libraries for plotting. Ensure your function is clean, well-documented, and efficient. # Example Usage ```python import seaborn as sns import pandas as pd # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Define your function def penguin_ecdf_plot(dataset, measurement, axis=\\"x\\", hue=None, stat=\\"proportion\\", complementary=False): if axis not in [\\"x\\", \\"y\\"]: raise ValueError(\\"axis parameter must be \'x\' or \'y\'\\") if stat not in [\\"proportion\\", \\"count\\", \\"percent\\"]: raise ValueError(\\"stat parameter must be \'proportion\', \'count\', or \'percent\'\\") sns.ecdfplot(data=dataset, **{axis: measurement}, hue=hue, stat=stat, complementary=complementary) sns.set_theme() plt.show() # Plot ECDF on x-axis penguin_ecdf_plot(penguins, \\"flipper_length_mm\\") # Plot ECDF on y-axis with hue penguin_ecdf_plot(penguins, \\"bill_length_mm\\", axis=\\"y\\", hue=\\"species\\") # Plot complementary ECDF with count stat penguin_ecdf_plot(penguins, \\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\", complementary=True) ``` Make sure to test your function by calling it with different parameters as shown in the examples.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def penguin_ecdf_plot(dataset, measurement, axis=\\"x\\", hue=None, stat=\\"proportion\\", complementary=False): Plots the empirical cumulative distribution function (ECDF) for a specified measurement. Parameters: - dataset: DataFrame - The pandas DataFrame containing the penguin dataset. - measurement: str - The column name of the measurement to plot (e.g., \\"flipper_length_mm\\"). - axis: str (default=\\"x\\") - The axis to plot the measurement on (\\"x\\" or \\"y\\"). - hue: str (optional) - Column name for hue mapping to plot multiple ECDFs. - stat: str (default=\\"proportion\\") - Statistic to plot, either \\"proportion\\", \\"count\\", or \\"percent\\". - complementary: bool (default=False) - Whether to plot the complementary CDF. Raises: - ValueError: If the axis is not \\"x\\" or \\"y\\". - ValueError: If the stat is not \\"proportion\\", \\"count\\", or \\"percent\\". if axis not in [\\"x\\", \\"y\\"]: raise ValueError(\\"axis parameter must be \'x\' or \'y\'\\") if stat not in [\\"proportion\\", \\"count\\", \\"percent\\"]: raise ValueError(\\"stat parameter must be \'proportion\', \'count\', or \'percent\'\\") sns.ecdfplot(data=dataset, **{axis: measurement}, hue=hue, stat=stat, complementary=complementary) sns.set_theme() plt.show() # Load the penguins dataset for demonstration penguins = sns.load_dataset(\\"penguins\\") # Plot ECDF on x-axis penguin_ecdf_plot(penguins, \\"flipper_length_mm\\") # Plot ECDF on y-axis with hue penguin_ecdf_plot(penguins, \\"bill_length_mm\\", axis=\\"y\\", hue=\\"species\\") # Plot complementary ECDF with count stat penguin_ecdf_plot(penguins, \\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\", complementary=True)"},{"question":"Understanding and Implementing Naive Bayes Classifiers You are provided with a dataset for text classification which is imbalanced across classes. Your task is to implement a Complement Naive Bayes (CNB) classifier using scikit-learn to handle this imbalanced dataset effectively. You should also evaluate the classifier\'s performance using appropriate metrics. Requirements: 1. **Data Preprocessing**: - Load the given dataset and split it into training and testing sets. - Convert the text data into term frequencies (count vectors). - Ensure the data is appropriately balanced for the CNB classifier. 2. **Model Implementation**: - Implement the Complement Naive Bayes classifier using scikit-learn. - Train the classifier using the training data. - Test the classifier using the testing data. 3. **Evaluation**: - Evaluate the classifier\'s performance using accuracy and an additional metric suitable for imbalanced data (e.g., F1-score, ROC-AUC). - Output the classification report and confusion matrix. Dataset Details: - Assume the dataset is provided in CSV format with columns `text` and `label`. - Perform necessary data cleaning and preprocessing steps before implementing the classifier. Constraints: - You must use the Complement Naive Bayes (`ComplementNB`) classifier from the scikit-learn package. - Split the dataset with `test_size=0.3` and `random_state=42` for reproducibility. - Use the `CountVectorizer` from scikit-learn for converting text to count vectors. - The implementation should be efficient in both time and space complexity. # Expected Function Signature: ```python def preprocess_data(file_path: str): # Load dataset, clean data, and split into train and test sets pass def implement_cnb_classifier(X_train, y_train, X_test, y_test): # Implement and evaluate the CNB classifier pass # Example usage: # X_train, X_test, y_train, y_test = preprocess_data(\\"path/to/dataset.csv\\") # implement_cnb_classifier(X_train, y_train, X_test, y_test) ``` # Example Output: ``` Classification Report: precision recall f1-score support 0 0.88 0.90 0.89 100 1 0.85 0.83 0.84 80 2 0.80 0.75 0.77 50 accuracy 0.86 230 macro avg 0.84 0.83 0.83 230 weighted avg 0.86 0.86 0.86 230 Confusion Matrix: [[90 5 5] [ 7 66 7] [ 5 8 37]] ``` Deliverables: - A Python script implementing the required functions. - Detailed code comments explaining each step. - Proper handling of errors and edge cases throughout the implementation.","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer from sklearn.naive_bayes import ComplementNB from sklearn.metrics import classification_report, confusion_matrix, f1_score def preprocess_data(file_path: str): # Load and read the dataset df = pd.read_csv(file_path) # Basic cleaning: drop NA values if any df.dropna(subset=[\'text\', \'label\'], inplace=True) # Extract features and labels X = df[\'text\'] y = df[\'label\'] # Split the data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Convert text data into term frequencies (count vectors) vectorizer = CountVectorizer() X_train_transformed = vectorizer.fit_transform(X_train) X_test_transformed = vectorizer.transform(X_test) return X_train_transformed, X_test_transformed, y_train, y_test def implement_cnb_classifier(X_train, y_train, X_test, y_test): # Initialize the Complement Naive Bayes classifier cnb = ComplementNB() # Train the classifier cnb.fit(X_train, y_train) # Make predictions on the test data y_pred = cnb.predict(X_test) # Evaluate the classifier\'s performance accuracy = cnb.score(X_test, y_test) f1 = f1_score(y_test, y_pred, average=\'weighted\') class_report = classification_report(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) # Print results print(\\"Accuracy:\\", accuracy) print(\\"F1 Score:\\", f1) print(\\"Classification Report:n\\", class_report) print(\\"Confusion Matrix:n\\", conf_matrix) # Example usage: # X_train, X_test, y_train, y_test = preprocess_data(\\"path/to/dataset.csv\\") # implement_cnb_classifier(X_train, y_train, X_test, y_test)"},{"question":"**Objective:** Write a function that generates multiple seaborn plots with different `plotting_context` settings and saves them as separate image files. This task will demonstrate your understanding of seaborn\'s scaling contexts and their applications. **Question:** 1. **Function Name:** `generate_context_plots` 2. **Input:** - `data`: A dictionary where the keys are strings representing categorical values, and the values are lists of numerical data. - `x_label`: A string representing the label for the x-axis of the plots - `y_label`: A string representing the label for the y-axis of the plots - `output_dir`: A string representing the directory path where the images should be saved 3. **Output:** - None. The function should save four image files in the specified output directory, corresponding to the four different plotting contexts. 4. **Functionality:** - The function should create a pandas DataFrame from the dictionary data for easier plotting. - Using seaborn, generate a line plot within each of the following `plotting_context` environments: `paper`, `notebook`, `talk`, and `poster`. - Label the axes using the specified `x_label` and `y_label`. - Save each plot as a separate image file in the specified `output_dir` with filenames `plot_paper.png`, `plot_notebook.png`, `plot_talk.png`, and `plot_poster.png`. ```python import seaborn as sns import pandas as pd import os def generate_context_plots(data, x_label, y_label, output_dir): # Create a pandas DataFrame from the dictionary data df = pd.DataFrame(data) # Define the different contexts to iterate through contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] # Generate and save plots for each context for context in contexts: sns.set_context(context) # Create the plot sns.lineplot(data=df) # Set the labels plt.xlabel(x_label) plt.ylabel(y_label) # Save the figure filename = f\\"plot_{context}.png\\" filepath = os.path.join(output_dir, filename) plt.savefig(filepath) # Clear the current plot plt.clf() # Example usage: # data = {\'CategoryA\': [1, 2, 3], \'CategoryB\': [4, 5, 6], \'CategoryC\': [7, 8, 9]} # x_label = \'Categories\' # y_label = \'Values\' # output_dir = \'/path/to/output/directory\' # generate_context_plots(data, x_label, y_label, output_dir) ``` **Constraints:** - Use only built-in Python libraries and seaborn for this task. - Ensure the `output_dir` exists before saving the images, and handle any potential file path issues. **Note:** - Verifying the functionality may require inspecting the saved images to ensure they properly reflect different contexts.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import os def generate_context_plots(data, x_label, y_label, output_dir): Generates multiple seaborn plots with different plotting_context settings and saves them as separate image files. Parameters: data (dict): A dictionary where keys are strings representing categorical values, and values are lists of numerical data. x_label (str): The label for the x-axis of the plots. y_label (str): The label for the y-axis of the plots. output_dir (str): The directory path where the images should be saved. Returns: None # Ensure the output directory exists if not os.path.exists(output_dir): os.makedirs(output_dir) # Create a pandas DataFrame from the dictionary data df = pd.DataFrame(data) # Define the different contexts to iterate through contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] # Generate and save plots for each context for context in contexts: sns.set_context(context) # Create the plot plt.figure() sns.lineplot(data=df) # Set the labels plt.xlabel(x_label) plt.ylabel(y_label) # Save the figure filename = f\\"plot_{context}.png\\" filepath = os.path.join(output_dir, filename) plt.savefig(filepath) # Clear the current plot plt.clf()"},{"question":"# Floating Point Handling in Python C Extensions You are tasked with implementing a function that processes a list of Python objects and performs various operations if they are floating-point numbers. # Instructions 1. **Function Definition**: ```python def process_floats(pyobject_list): Processes a list of Python objects and performs various operations if they are floating points. Args: pyobject_list (list): A list of Python objects. Returns: dict: A dictionary with max, min, and average of valid float numbers and their string representations. ``` 2. **Function Behavior**: - Iterate through the `pyobject_list`. - For each object, check if it is a PyFloatObject or a subtype using `PyFloat_Check`. - For valid float objects: - Convert it to a C double using `PyFloat_AsDouble`. - Track the maximum, minimum, and compute the average of these values. - Collect their string representations (you can assume all elements have usable `__str__` or similar methods). 3. **Output Format**: - Return a dictionary containing: ```python { \\"max\\": <maximum float value>, \\"min\\": <minimum float value>, \\"average\\": <average float value>, \\"float_strings\\": [<list of string representations of float values>] } ``` 4. **Constraints and Assumptions**: - If no float values are found, `max`, `min`, and `average` should be `None`. - The function should handle edge cases like empty lists gracefully. 5. **Example**: ```python example_list = [1.5, \\"not a float\\", 3.2, 4, 6.7, None, 9.0] result = process_floats(example_list) print(result) # Output: # { # \\"max\\": 9.0, # \\"min\\": 1.5, # \\"average\\": 5.1, # \\"float_strings\\": [\\"1.5\\", \\"3.2\\", \\"6.7\\", \\"9.0\\"] # } ``` Implement the `process_floats` function to meet the above requirements, ensuring proper type checks and conversions as specified in the provided documentation.","solution":"def process_floats(pyobject_list): Processes a list of Python objects and performs various operations if they are floating points. Args: pyobject_list (list): A list of Python objects. Returns: dict: A dictionary with max, min, and average of valid float numbers and their string representations. float_values = [] for obj in pyobject_list: if isinstance(obj, float): float_values.append(obj) if not float_values: return { \\"max\\": None, \\"min\\": None, \\"average\\": None, \\"float_strings\\": [] } max_val = max(float_values) min_val = min(float_values) average_val = sum(float_values) / len(float_values) float_strings = [str(value) for value in float_values] return { \\"max\\": max_val, \\"min\\": min_val, \\"average\\": average_val, \\"float_strings\\": float_strings }"},{"question":"**Asyncio Task Management and Concurrency** You are tasked with creating an asyncio-based Python program that demonstrates effective task management and concurrency control using coroutines, tasks, and timeout handling. Implement the following functions: 1. **fetch_data**: - **Description**: Simulates fetching data asynchronously from a remote server. - **Input**: `url` (string) – A URL from which data is supposedly fetched. - **Output**: Returns a dictionary with the URL and a success message after a random delay. - **Example**: `{\'url\': \'http://example.com\', \'message\': \'Data fetched successfully\'}` - **Constraints**: If the fetching process exceeds a specified timeout, a `TimeoutError` should be raised. 2. **main**: - **Description**: Schedules multiple `fetch_data` coroutines concurrently and handles potential timeouts. - **Input**: `urls` (list of strings) - List of URLs to fetch from, `timeout` (float) - Timeout duration for each fetch operation. - **Output**: Returns a list of results from successful fetch operations, handling both success and timeout scenarios appropriately. - **Constraints**: All `fetch_data` tasks must be initialized and executed in parallel, and the function should gather results from all tasks. Tasks: - Implement `fetch_data` to simulate an asynchronous fetch with a random delay. - Ensure that `fetch_data` raises a `TimeoutError` if the operation exceeds the specified timeout. - Implement `main` to handle multiple URLs concurrently within the specified timeout. Collect and return successful results while handling timeouts gracefully. ```python import asyncio import random async def fetch_data(url, timeout): Simulate fetching data from a remote server asynchronously. # Your code here async def main(urls, timeout): Schedule multiple fetch_data coroutines concurrently and handle timeouts. # Your code here # Example usage: urls = [\'http://example.com\', \'http://example.org\', \'http://example.net\'] timeout = 5.0 results = asyncio.run(main(urls, timeout)) print(results) ``` # Requirements: - Use Python 3.10+ features, particularly the asyncio library. - Ensure proper concurrency and timeout management. - Demonstrate understanding of creating tasks and handling their completion or cancellation. # Evaluation Criteria: - Correct implementation and use of asyncio primitives. - Efficient handling of concurrent tasks and timeouts. - Clean and reusable code structure. - Proper usage of async/await syntax. **Hint**: Use `asyncio.wait_for()` to implement the timeout behavior for `fetch_data`. In `main`, consider using `asyncio.gather()` or `asyncio.wait()` to handle multiple tasks concurrently.","solution":"import asyncio import random async def fetch_data(url, timeout): Simulate fetching data from a remote server asynchronously. delay = random.uniform(0.1, 3.0) # Random delay between 0.1 and 3.0 seconds async def delayed_fetch(): await asyncio.sleep(delay) return {\'url\': url, \'message\': \'Data fetched successfully\'} try: return await asyncio.wait_for(delayed_fetch(), timeout) except asyncio.TimeoutError: raise TimeoutError(f\\"Fetching data from {url} timed out after {timeout} seconds\\") async def main(urls, timeout): Schedule multiple fetch_data coroutines concurrently and handle timeouts. tasks = [fetch_data(url, timeout) for url in urls] results = [] for task in asyncio.as_completed(tasks): try: result = await task results.append(result) except TimeoutError as e: print(e) return results # Example usage: if __name__ == \\"__main__\\": urls = [\'http://example.com\', \'http://example.org\', \'http://example.net\'] timeout = 2.0 results = asyncio.run(main(urls, timeout)) print(results)"},{"question":"Question: Implement Custom Bytearray Utilities You are required to implement a set of utility functions that work with `bytearray` objects. These utilities should demonstrate your understanding of `bytearray` manipulation, checking, and resizing. # Part 1: Bytearray Utilities Implementation Implement the following functions: 1. **concat_bytearrays**: Concatenates two bytearrays. - **Input**: Two `bytearray` objects, `a` and `b`. - **Output**: A new `bytearray` which is the result of concatenating `a` and `b`. 2. **resize_bytearray**: Resizes a given bytearray to a specified length. - **Input**: A `bytearray` object `ba` and an integer `new_len`. - **Output**: The resized `bytearray` object. 3. **is_bytearray**: Checks if a given object is a `bytearray`. - **Input**: Any Python object `o`. - **Output**: A boolean value `True` if `o` is a `bytearray` or a subtype of `bytearray`, otherwise `False`. # Part 2: Usage Example Provide code usage examples for the following scenarios: 1. Concatenating two `bytearray` objects: - `bytearray(b\\"Hello, \\")` and `bytearray(b\\"world!\\")` - Expected output: `bytearray(b\\"Hello, world!\\")` 2. Resizing a `bytearray`: - Input `bytearray(b\\"short\\")` - Resize to length `10` - Expected output: A `bytearray` of length 10, e.g., `bytearray(b\\"shortx00x00x00x00x00\\")` 3. Checking if a given object is a `bytearray`: - Input: `bytearray(b\\"test\\")` - Input: `\\"test\\"` - Expected output: `True` for `bytearray(b\\"test\\")`, `False` for `\\"test\\"` # Constraints - These functions should handle any valid and invalid inputs gracefully, ensuring they do not cause unhandled exceptions. - Efficiency is key for these operations due to potential large sizes of bytearrays. # Function Signatures ```python def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: pass def resize_bytearray(ba: bytearray, new_len: int) -> bytearray: pass def is_bytearray(o: object) -> bool: pass ``` Implement these functions such that they perform the specified operations and meet the requirements as defined.","solution":"def concat_bytearrays(a: bytearray, b: bytearray) -> bytearray: Concatenates two bytearrays. Params: a (bytearray): The first bytearray. b (bytearray): The second bytearray. Returns: bytearray: A new bytearray that is the result of concatenating a and b. return a + b def resize_bytearray(ba: bytearray, new_len: int) -> bytearray: Resizes a given bytearray to a specified length. Params: ba (bytearray): The bytearray to resize. new_len (int): The new length of the bytearray. Returns: bytearray: The resized bytearray object. if new_len < len(ba): return ba[:new_len] else: return ba + bytearray(new_len - len(ba)) def is_bytearray(o: object) -> bool: Checks if a given object is a bytearray. Params: o (object): The object to check. Returns: bool: True if o is a bytearray or a subtype of bytearray, otherwise False. return isinstance(o, bytearray)"},{"question":"**Property List File Processor** You are tasked with building a utility to process property list files (plists) used on Apple systems. The utility should be able to read a plist file, modify its contents based on specific criteria, and write the changes back to a new plist file. The file format for writing should be in binary. # Requirements: 1. **Function Name**: `process_plist(input_filepath: str, output_filepath: str, modifications: dict) -> None` 2. **Parameters**: - `input_filepath` (str): The path to the input plist file, which is in XML format. - `output_filepath` (str): The path where the modified plist file should be saved in binary format. - `modifications` (dict): A dictionary containing key-value pairs which should be updated or added to the plist data. # Constraints: - The input plist file will always be in XML format. - The modifications dictionary will contain only valid plist data types. - The plist file’s top-level object is a dictionary. - Any key in the modifications dictionary that exists in the plist should be updated, and if it does not exist, it should be added. # Example: Input XML file (`example.plist`): ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>Name</key> <string>Old Value</string> <key>Count</key> <integer>5</integer> </dict> </plist> ``` Modifications: ```python { \\"Name\\": \\"New Value\\", \\"Age\\": 30 } ``` Output binary plist file should contain: - `Name` key updated to `\\"New Value\\"`. - `Age` key added with value `30`. - `Count` key remains unchanged with value `5`. # Function Signature: ```python import plistlib def process_plist(input_filepath: str, output_filepath: str, modifications: dict) -> None: # Your implementation here pass ``` # Notes: - You should use the `plistlib` module for reading the XML file, modifying its content, and writing the binary plist file. - Ensure the output file is written in the correct binary format using the `FMT_BINARY` constant of `plistlib`. Implement the function to satisfy the requirements stated above.","solution":"import plistlib def process_plist(input_filepath: str, output_filepath: str, modifications: dict) -> None: Reads a plist file, modifies its contents based on specific criteria, and writes the changes back to a new plist file in binary format. Args: input_filepath (str): The path to the input plist file (XML format). output_filepath (str): The path where the modified plist file should be saved (binary format). modifications (dict): A dictionary containing key-value pairs to update or add to the plist data. # Read the plist file with open(input_filepath, \'rb\') as f: plist_data = plistlib.load(f) # Apply modifications plist_data.update(modifications) # Write the modified plist data to a new file in binary format with open(output_filepath, \'wb\') as f: plistlib.dump(plist_data, f, fmt=plistlib.FMT_BINARY)"},{"question":"**Problem Statement** You are required to implement a set of functions to process and manipulate sequences of data efficiently using functional programming techniques. Specifically, you will need to create generators, use iterators, and employ modules like `itertools` and `functools`. # Objective Implement the following functions: 1. **`fibonacci_seq(n: int) -> Iterator[int]`** - Generates the first `n` numbers of the Fibonacci sequence. - **Constraints**: `1 <= n <= 10^6` - **Input**: An integer `n`. - **Output**: An iterator of the first `n` Fibonacci numbers. 2. **`filter_primes(iterator: Iterator[int]) -> Iterator[int]`** - Filters out non-prime numbers from an iterator of integers. - **Constraints**: The input iterator can have numbers up to `10^6`. - **Input**: An iterator of integers. - **Output**: An iterator that yields only prime numbers. 3. **`sum_of_combinations(iterator: Iterator[int], r: int) -> Iterator[int]`** - Computes the sum of all `r`-tuple combinations from the input iterator. - **Constraints**: The input iterator can have up to `20` elements. - **Input**: An iterator of integers and an integer `r`. - **Output**: An iterator that yields the sum of each `r`-tuple combination from the input. # Function Implementations 1. **`fibonacci_seq(n: int) -> Iterator[int]`** - Use a generator to yield the first `n` Fibonacci numbers. 2. **`filter_primes(iterator: Iterator[int]) -> Iterator[int]`** - Use functional programming constructs to filter the input iterator and yield only prime numbers. 3. **`sum_of_combinations(iterator: Iterator[int], r: int) -> Iterator[int]`** - Use `itertools.combinations` to generate `r`-tuple combinations and `map`/`sum` to yield their sums. # Testing and Constraints - Ensure all functions use efficient algorithms to handle edge cases and large inputs within the specified constraints. - Validate the correctness and performance using unit tests. # Example ```python # Example usage fibonacci_gen = fibonacci_seq(10) print(list(fibonacci_gen)) # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] numbers = iter([10, 15, 20, 23, 1, 2, 3, 4]) primes_gen = filter_primes(numbers) print(list(primes_gen)) # Output: [23, 1, 2, 3] combination_iterator = iter([1, 2, 3, 4]) comb_sum_gen = sum_of_combinations(combination_iterator, 2) print(list(comb_sum_gen)) # Output: [3, 4, 5, 5, 6, 7] ``` # Implementation Implement the functions below: ```python from typing import Iterator import itertools import functools import math def fibonacci_seq(n: int) -> Iterator[int]: def fibonacci_gen(): a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b return fibonacci_gen() def is_prime(num: int) -> bool: if num <= 1: return False if num <= 3: return True if num % 2 == 0 or num % 3 == 0: return False i = 5 while i * i <= num: if num % i == 0 or num % (i + 2) == 0: return False i += 6 return True def filter_primes(iterator: Iterator[int]) -> Iterator[int]: return filter(is_prime, iterator) def sum_of_combinations(iterator: Iterator[int], r: int) -> Iterator[int]: return map(sum, itertools.combinations(iterator, r)) ``` # Note - Make sure to test your functions thoroughly to ensure correctness. - Ensure that your implementation is efficient and handles edge cases properly.","solution":"from typing import Iterator import itertools def fibonacci_seq(n: int) -> Iterator[int]: Generates the first n numbers of the Fibonacci sequence. :param n: Number of Fibonacci numbers to generate :return: Iterator of the first n Fibonacci numbers def fibonacci_gen(): a, b = 0, 1 for _ in range(n): yield a a, b = b, a + b return fibonacci_gen() def is_prime(num: int) -> bool: Checks if the provided number is prime or not. :param num: The number to check :return: True if number is prime, else False if num <= 1: return False if num <= 3: return True if num % 2 == 0 or num % 3 == 0: return False i = 5 while i * i <= num: if num % i == 0 or num % (i + 2) == 0: return False i += 6 return True def filter_primes(iterator: Iterator[int]) -> Iterator[int]: Filters out non-prime numbers from an iterator of integers. :param iterator: An iterator of integers :return: An iterator of prime numbers return filter(is_prime, iterator) def sum_of_combinations(iterator: Iterator[int], r: int) -> Iterator[int]: Computes the sum of all r-tuple combinations from the input iterator. :param iterator: An iterator of integers :param r: Number of elements in each combination :return: An iterator of sums of each r-tuple combination return map(sum, itertools.combinations(iterator, r))"},{"question":"**Objective:** Your task is to create a Python function that closely mirrors the underlying operations provided by the C-API for handling slices. **Problem Statement:** Implement a Python function `create_and_analyze_slice(start, stop, step, length)` that: 1. Creates a Python slice object given the start, stop, and step parameters. 2. Analyzes this slice object to determine the effective start, stop, and step indices for a sequence of given length. 3. Returns a tuple containing the effective start, stop, and step indices and the length of the slice. **Function Signature:** ```python def create_and_analyze_slice(start: int, stop: int, step: int, length: int) -> tuple: pass ``` **Input:** - `start` (int): The starting index of the slice. - `stop` (int): The ending index of the slice. - `step` (int): The step value of the slice. - `length` (int): The length of the sequence to be sliced. **Output:** - A tuple `(effective_start, effective_stop, effective_step, slice_length)`: - `effective_start` (int): The start index after handling possible out-of-bounds values. - `effective_stop` (int): The stop index after handling possible out-of-bounds values. - `effective_step` (int): The step value as used in the slice. - `slice_length` (int): The total length of the resulting slice. **Constraints:** - The `length` parameter will be a non-negative integer. - The `step` value will not be zero. **Example:** ```python start, stop, step = 2, 10, 2 length = 8 print(create_and_analyze_slice(start, stop, step, length)) # Expected Output: (2, 8, 2, 3) start, stop, step = -10, 10, 1 length = 5 print(create_and_analyze_slice(start, stop, step, length)) # Expected Output: (0, 5, 1, 5) start, stop, step = None, None, -1 length = 7 print(create_and_analyze_slice(start, stop, step, length)) # Expected Output: (6, -1, -1, 7) ``` **Notes:** - If the start, stop, or step is `None`, treat them as `None` in Python slicing terms (i.e., `slice(None)`). - Handle edge cases such as negative indices, steps, and out-of-bounds values. This exercise will assess your understanding of Python\'s slicing mechanism and how to handle slices programmatically in a manner similar to Python\'s internal C implementation.","solution":"def create_and_analyze_slice(start, stop, step, length): Create a slice object and analyze it to determine the effective start, stop, step indices and the length of the slice for a given sequence length. sl = slice(start, stop, step) indices = sl.indices(length) effective_start, effective_stop, effective_step = indices slice_length = max(0, (effective_stop - effective_start + (effective_step - (1 if effective_step > 0 else -1))) // effective_step) return (effective_start, effective_stop, effective_step, slice_length)"},{"question":"# Question Write a function `process_boolean_list(input_list)` that takes a list of boolean values and integers as input and returns a dictionary with the count of `True` and `False` values. The function should handle integers by converting them to their boolean equivalents. For the purposes of this function, assume that only integers or boolean values will be in the list. Function Signature ```python def process_boolean_list(input_list: list) -> dict: ``` Input - `input_list`: A list of boolean values and integers. (e.g., `[True, False, 1, 0, 3, -2, True]`) Output - Returns a dictionary with two keys: `True` and `False`, with their respective counts from the input list. - Example output: `{True: 5, False: 2}` Example ```python input_list = [True, False, 1, 0, 3, -2, True] output = process_boolean_list(input_list) print(output) # Output should be {True: 5, False: 2} ``` Constraints - The elements in the `input_list` will only be boolean values or integers. - The list can have up to (10^6) elements. - The function should be efficient in its computation. Requirements 1. Convert all integers in the list to their boolean equivalents. 2. Count the number of `True` and `False` values and return them in a dictionary. Performance Requirements - The function should run in O(n) time complexity, where n is the length of the input list.","solution":"def process_boolean_list(input_list): Process a list of boolean values and integers and return a dictionary with the count of True and False values. Args: input_list (list): A list of boolean values and integers. Returns: dict: A dictionary with the counts of True and False values. true_count = 0 false_count = 0 for item in input_list: if bool(item): true_count += 1 else: false_count += 1 return {True: true_count, False: false_count}"},{"question":"Coding Assessment Question # Objective Implement a custom neural network module in PyTorch using the `torch.cond` operator. Your module should demonstrate the use of dynamic, data-dependent control flow based on properties of the input tensor. # Problem Statement Design a PyTorch module called `DynamicOperationModule`. This module should take an input tensor and perform different operations based on a predicate involving the data within the tensor. 1. If the mean of the elements in the tensor is greater than 1.0, the module should compute the logarithm of each element. 2. Otherwise, it should compute the square root of each element. # Requirements 1. Use the `torch.cond` operator to switch between the two operations. 2. Ensure that your module handles tensors of any shape. 3. Do not modify the input tensor directly; instead, return a new tensor as output. 4. The module should be implemented as a subclass of `torch.nn.Module` with a `forward` method. # Input and Output - **Input:** - A PyTorch tensor `x` of any shape containing float values. - **Output:** - A new tensor of the same shape as `x` where each element has been transformed according to the specified conditions. # Constraints 1. The input tensor `x` will only contain positive values to avoid domain errors for logarithm and square root operations. 2. You may not use any control flow structures other than `torch.cond` for this task. # Example ```python import torch from dynamic_operation_module import DynamicOperationModule model = DynamicOperationModule() inp = torch.tensor([0.5, 2.0, 3.0, 4.5]) # Mean is 2.5 which is > 1.0 output = model(inp) # Output should be: tensor([0.0000, 0.6931, 1.0986, 1.5041]) inp2 = torch.tensor([0.1, 0.2, 0.3]) # Mean is 0.2 which is <= 1.0 output2 = model(inp2) # Output should be: tensor([0.3162, 0.4472, 0.5477]) ``` # Notes 1. You may assume that all necessary imports have already been made. 2. Do not worry about numerical stability or precision issues for this task. # Implementation ```python import torch def log_fn(x: torch.Tensor): return torch.log(x) def sqrt_fn(x: torch.Tensor): return torch.sqrt(x) class DynamicOperationModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.cond((x.mean() > 1.0).item(), lambda x: torch.log(x), lambda x: torch.sqrt(x), (x,)) ``` Your task is to complete the implementation of the `DynamicOperationModule` class following the given requirements.","solution":"import torch import torch.nn as nn class DynamicOperationModule(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: mean_value = x.mean() if mean_value > 1.0: return torch.log(x) else: return torch.sqrt(x)"},{"question":"**Objective:** Design and implement a Python function using the `gzip` module to compress files in a given directory and then decompress them into another specified directory, while handling possible exceptions. **Problem Statement:** Write a Python function `compress_and_decompress_files(input_dir: str, compressed_dir: str, decompressed_dir: str) -> None` that performs the following tasks: 1. Compress all files in the `input_dir` directory and store the compressed files in the `compressed_dir` directory. 2. Decompress the compressed files from the `compressed_dir` and store the decompressed files in the `decompressed_dir`. The function should handle invalid files gracefully by skipping them and logging an appropriate message. It should also ensure that the decompressed files have the same content as the original files in the `input_dir`. **Constraints:** - The function should only process files, not directories. - Assume that the directory paths provided are valid and accessible. - Use the highest compression level (9) for compressing files. - The content of a file may be large, so the function should handle large files efficiently. **Input/Output Format:** - `input_dir` (str): The directory containing files to be compressed. - `compressed_dir` (str): The directory to store compressed files. - `decompressed_dir` (str): The directory to store decompressed files. The function does not need to return any value. **Examples:** ```python import os # Example usage input_dir = \\"/path/to/input_dir\\" compressed_dir = \\"/path/to/compressed_dir\\" decompressed_dir = \\"/path/to/decompressed_dir\\" # Ensure directories exist for the sake of the example os.makedirs(input_dir, exist_ok=True) os.makedirs(compressed_dir, exist_ok=True) os.makedirs(decompressed_dir, exist_ok=True) compress_and_decompress_files(input_dir, compressed_dir, decompressed_dir) ``` # Function Signature ```python def compress_and_decompress_files(input_dir: str, compressed_dir: str, decompressed_dir: str) -> None: # Your code here ``` # Notes: - Use the `gzip.open()`, `gzip.compress()`, and `gzip.decompress()` functions from the `gzip` module. - Handle exceptions for invalid gzip files using `gzip.BadGzipFile`.","solution":"import os import gzip import shutil import logging def compress_file(input_path, output_path): try: with open(input_path, \'rb\') as f_in, gzip.open(output_path, \'wb\', compresslevel=9) as f_out: shutil.copyfileobj(f_in, f_out) except Exception as e: logging.error(f\\"Failed to compress {input_path}: {e}\\") def decompress_file(input_path, output_path): try: with gzip.open(input_path, \'rb\') as f_in, open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) except gzip.BadGzipFile as e: logging.error(f\\"Invalid gzip file {input_path}: {e}\\") except Exception as e: logging.error(f\\"Failed to decompress {input_path}: {e}\\") def compress_and_decompress_files(input_dir: str, compressed_dir: str, decompressed_dir: str) -> None: # Ensure the directories exist os.makedirs(compressed_dir, exist_ok=True) os.makedirs(decompressed_dir, exist_ok=True) # Compress files for file_name in os.listdir(input_dir): input_path = os.path.join(input_dir, file_name) if os.path.isfile(input_path): compressed_path = os.path.join(compressed_dir, file_name + \'.gz\') compress_file(input_path, compressed_path) # Decompress files for file_name in os.listdir(compressed_dir): compressed_path = os.path.join(compressed_dir, file_name) if compressed_path.endswith(\'.gz\'): decompressed_path = os.path.join(decompressed_dir, file_name[:-3]) decompress_file(compressed_path, decompressed_path)"},{"question":"Question: Implementing a Custom Python Class with Advanced Behaviors # Objective You are required to implement a custom Python class that includes some advanced behaviors such as supporting basic arithmetic operations, iteration, attribute access, and garbage collection control. # Specifications 1. **Class Name**: `AdvancedNumber` 2. **Constructor Parameters**: - `value` (int): The integer value to be held by the object. 3. **Methods**: - `__init__(self, value: int) -> None`: Initialize the object with the given value. - `__add__(self, other: \'AdvancedNumber\') -> \'AdvancedNumber\'`: Define addition for `AdvancedNumber` objects. - `__sub__(self, other: \'AdvancedNumber\') -> \'AdvancedNumber\'`: Define subtraction for `AdvancedNumber` objects. - `__iter__(self) -> \'AdvancedNumber\'`: Make the object iterable. - `__next__(self) -> int`: Iterate over the individual digits of the `value` in sequence. Stop iteration once all digits are exhausted. - `__getattr__(self, name: str) -> Any`: Handle access to undefined attributes by returning a custom message indicating that the attribute does not exist. - `__del__(self) -> None`: Print a message when the object is about to be destroyed (to mimic garbage collection notification). # Constraints 1. The `value` will always be a non-negative integer. 2. The addition and subtraction operations should return a new `AdvancedNumber` object with the appropriate value. 3. Iteration should only work once and then raise `StopIteration`. 4. Any undefined attribute access should return `Attribute {name} not found`. # Example Usage ```python # Creating objects num1 = AdvancedNumber(123) num2 = AdvancedNumber(456) # Basic arithmetic operations num3 = num1 + num2 # num3.value should be 579 num4 = num2 - num1 # num4.value should be 333 # Iterating over digits for digit in num1: print(digit) # Should print 1, then 2, then 3 # Accessing an undefined attribute print(num1.some_random_attribute) # Should print \\"Attribute some_random_attribute not found\\" ``` # Implementation ```python class AdvancedNumber: def __init__(self, value: int) -> None: self.value = value self._digits = iter(str(value)) def __add__(self, other: \'AdvancedNumber\') -> \'AdvancedNumber\': return AdvancedNumber(self.value + other.value) def __sub__(self, other: \'AdvancedNumber\') -> \'AdvancedNumber\': return AdvancedNumber(self.value - other.value) def __iter__(self) -> \'AdvancedNumber\': self._digits = iter(str(self.value)) return self def __next__(self) -> int: try: return int(next(self._digits)) except StopIteration: raise StopIteration def __getattr__(self, name: str) -> str: return f\\"Attribute {name} not found\\" def __del__(self) -> None: print(f\\"AdvancedNumber object with value {self.value} is being deleted\\") ``` Ensure that your implementation passes the example usage as given.","solution":"class AdvancedNumber: def __init__(self, value: int) -> None: if value < 0: raise ValueError(\\"value must be a non-negative integer\\") self.value = value self._digits = iter(str(value)) def __add__(self, other: \'AdvancedNumber\') -> \'AdvancedNumber\': return AdvancedNumber(self.value + other.value) def __sub__(self, other: \'AdvancedNumber\') -> \'AdvancedNumber\': return AdvancedNumber(self.value - other.value) def __iter__(self) -> \'AdvancedNumber\': self._digits = iter(str(self.value)) return self def __next__(self) -> int: try: return int(next(self._digits)) except StopIteration: raise StopIteration def __getattr__(self, name: str) -> str: return f\\"Attribute {name} not found\\" def __del__(self) -> None: print(f\\"AdvancedNumber object with value {self.value} is being deleted\\")"},{"question":"**Question: Performance Benchmarking Using the `timeit` Module** The purpose of this exercise is to assess your understanding of the Python `timeit` module and your ability to benchmark the performance of different code snippets. # Problem Statement: You are tasked with benchmarking the performance of three functions, each of which takes a list of integers as an input and returns a list of the squares of these integers. Your task is to measure and compare the execution times of these three functions using the `timeit` module. Below are the three functions you need to benchmark: 1. **Loop-based approach**: ```python def square_using_loop(nums): result = [] for num in nums: result.append(num ** 2) return result ``` 2. **List comprehension approach**: ```python def square_using_list_comprehension(nums): return [num ** 2 for num in nums] ``` 3. **Map + lambda approach**: ```python def square_using_map(nums): return list(map(lambda x: x ** 2, nums)) ``` # Requirements: 1. **Function Implementation**: Implement a function `benchmark_square_functions(nums, number=10000)` that: - Takes a list of integers `nums` and an optional integer `number` as arguments. - Benchmarks the execution time of the three functions using the `timeit` module with the given `number` of executions. - Returns a dictionary with the function names as keys and their corresponding execution times as values. 2. **Input and Output**: - **Input**: A list of integers `nums` and an integer `number` (default is 10,000). - **Output**: A dictionary where keys are function names and values are their execution times (in seconds). 3. **Constraints**: - Performance is critical; ensure minimum overhead while benchmarking. - Use the `timeit.timeit()` method for accurate results. 4. **Example Usage**: ```python nums = [1, 2, 3, 4, 5] * 2000 # List of 10,000 integers result = benchmark_square_functions(nums) print(result) # Output Example: {\'square_using_loop\': 0.053, \'square_using_list_comprehension\': 0.043, \'square_using_map\': 0.052} ``` # Additional Notes: - Ensure the `number` of executions parameter allows for a meaningful comparison. - Research and consider the best practices for using the `timeit` module, such as turning off garbage collection if necessary. - Write clean, readable, and efficient code.","solution":"import timeit def square_using_loop(nums): result = [] for num in nums: result.append(num ** 2) return result def square_using_list_comprehension(nums): return [num ** 2 for num in nums] def square_using_map(nums): return list(map(lambda x: x ** 2, nums)) def benchmark_square_functions(nums, number=10000): loop_time = timeit.timeit(lambda: square_using_loop(nums), number=number) list_comprehension_time = timeit.timeit(lambda: square_using_list_comprehension(nums), number=number) map_time = timeit.timeit(lambda: square_using_map(nums), number=number) return { \'square_using_loop\': loop_time, \'square_using_list_comprehension\': list_comprehension_time, \'square_using_map\': map_time }"},{"question":"# Question Objective Your task is to implement a Python program that reads a text file containing names and birthdates, processes the data, and performs some analysis. Problem Statement You are provided with a text file named `birthdates.txt` that contains lines in the format: ``` Name, DD-MM-YYYY ``` Each line represents an individual\'s name and their birthdate. Implement the following functions: 1. `load_data(file_path: str) -> List[Tuple[str, datetime.date]]`: - Reads the file located at `file_path`. - Parses each line to extract the name and birthdate. - Returns a list of tuples, where each tuple contains a name (str) and a birthdate (datetime.date). 2. `youngest_and_oldest(data: List[Tuple[str, datetime.date]]) -> Tuple[Tuple[str, datetime.date], Tuple[str, datetime.date]]`: - Takes the list from function 1 as input. - Finds and returns the tuples for the youngest and oldest individuals. 3. `birthdays_in(month: int, data: List[Tuple[str, datetime.date]]) -> List[str]`: - Takes a month (integer between 1 and 12) and the list from function 1 as input. - Returns a list of names of individuals who have birthdays in the specified month. 4. `count_birthdays_per_month(data: List[Tuple[str, datetime.date]]) -> Dict[int, int]`: - Takes the list from function 1 as input. - Returns a dictionary where keys are months (1-12) and values are the counts of birthdays in each month. Input and Output Formats - **Input**: - `birthdates.txt` file contents example: ``` John Doe, 15-06-1990 Jane Smith, 23-09-1985 Alice Johnson, 01-01-2000 Bob Lee, 31-12-1995 ``` - **Output**: - Function 1: List of tuples with names and birthdates. - Function 2: Tuple of tuples, representing the oldest and youngest individuals. - Function 3: List of names with birthdays in the specified month. - Function 4: Dictionary with month as keys and counts of birthdays as values. Constraints - The input file will always be formatted correctly with data separated by commas and dates in `DD-MM-YYYY` format. - Valid birthdates will be provided. - No duplicate names will be present in the file. Performance Requirements - The solution should efficiently handle files with up to 100,000 lines. Example Usage ```python data = load_data(\'birthdates.txt\') print(data) # Output: [(\'John Doe\', datetime.date(1990, 6, 15)), (\'Jane Smith\', datetime.date(1985, 9, 23)), ...] youngest, oldest = youngest_and_oldest(data) print(youngest, oldest) # Output: (\'Alice Johnson\', datetime.date(2000, 1, 1)), (\'Jane Smith\', datetime.date(1985, 9, 23)) june_birthdays = birthdays_in(6, data) print(june_birthdays) # Output: [\'John Doe\'] birthdays_count = count_birthdays_per_month(data) print(birthdays_count) # Output: {1: 1, 6: 1, 9: 1, 12: 1} ``` Implement these functions in a file named `analysis.py`.","solution":"from typing import List, Tuple, Dict from datetime import datetime def load_data(file_path: str) -> List[Tuple[str, datetime.date]]: data = [] with open(file_path, \'r\') as file: for line in file: name, date_str = line.strip().split(\', \') birthdate = datetime.strptime(date_str, \'%d-%m-%Y\').date() data.append((name, birthdate)) return data def youngest_and_oldest(data: List[Tuple[str, datetime.date]]) -> Tuple[Tuple[str, datetime.date], Tuple[str, datetime.date]]: oldest = min(data, key=lambda x: x[1]) youngest = max(data, key=lambda x: x[1]) return youngest, oldest def birthdays_in(month: int, data: List[Tuple[str, datetime.date]]) -> List[str]: return [name for name, birthdate in data if birthdate.month == month] def count_birthdays_per_month(data: List[Tuple[str, datetime.date]]) -> Dict[int, int]: count = {i: 0 for i in range(1, 13)} for _, birthdate in data: count[birthdate.month] += 1 return count"},{"question":"**Question:** # Asynchronous File Downloader using `asyncio` You are required to implement an asynchronous file downloader using Python\'s `asyncio` module. Your downloader should be capable of downloading multiple files concurrently and reporting the progress of each download. Requirements: 1. Write an asynchronous function `download_file(url: str, destination: str) -> None` that downloads a file from a given URL to a specified destination path. This function should use `aiohttp` for making HTTP requests. 2. Write an asynchronous function `download_files(urls: List[str], destination_folder: str) -> None` that takes a list of URLs and downloads each file concurrently into the specified destination folder. 3. Implement a progress reporting mechanism that displays the downloading progress of each file in real-time. Input: - `urls`: A list of strings, where each string is a URL pointing to a file to be downloaded. - `destination_folder`: A string representing the folder path where all files should be saved. Output: - Your program should print the progress of each file as it downloads. Constraints: - You can assume all URLs point to valid files and have unique names. - The destination folder exists and is writable. - Use the `asyncio` and `aiohttp` libraries for concurrency and making HTTP requests. Example Usage: ```python import asyncio urls = [ \\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\", \\"http://example.com/file3.txt\\" ] destination_folder = \\"/path/to/destination\\" asyncio.run(download_files(urls, destination_folder)) ``` # Additional Notes: - Ensure that your solution handles exceptions appropriately, such as network errors or invalid URLs. - You must use the `async` and `await` syntax to achieve concurrency. Here is a template to get you started: ```python import aiohttp import asyncio import os async def download_file(url: str, destination: str) -> None: # Implement the function to download a single file asynchronously pass async def download_files(urls: List[str], destination_folder: str) -> None: tasks = [] for url in urls: filename = os.path.join(destination_folder, os.path.basename(url)) tasks.append(download_file(url, filename)) await asyncio.gather(*tasks) # Example usage if __name__ == \\"__main__\\": import sys urls = [\\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\", \\"http://example.com/file3.txt\\"] destination_folder = \\"/path/to/destination\\" asyncio.run(download_files(urls, destination_folder)) ```","solution":"import aiohttp import asyncio import os from typing import List async def download_file(url: str, destination: str) -> None: async with aiohttp.ClientSession() as session: async with session.get(url) as response: response.raise_for_status() total_size = int(response.headers.get(\'content-length\', 0)) downloaded_size = 0 with open(destination, \'wb\') as f: async for chunk in response.content.iter_chunked(1024): f.write(chunk) downloaded_size += len(chunk) print(f\\"Downloading {os.path.basename(destination)}: {downloaded_size/total_size:.2%}\\") async def download_files(urls: List[str], destination_folder: str) -> None: tasks = [] for url in urls: filename = os.path.join(destination_folder, os.path.basename(url)) tasks.append(download_file(url, filename)) await asyncio.gather(*tasks) # Example usage if __name__ == \\"__main__\\": import sys urls = [\\"http://example.com/file1.txt\\", \\"http://example.com/file2.txt\\", \\"http://example.com/file3.txt\\"] destination_folder = \\"./downloads\\" # Make sure this directory exists asyncio.run(download_files(urls, destination_folder))"},{"question":"# Advanced Enum Usage in Python Your task is to create an advanced Enum in Python that demonstrates several advanced features described in the `enum` module documentation. This problem will assess your understanding of creating enums, ensuring unique values, and adding custom behavior. Requirements 1. **Enum Definition**: - Create a class `Planet` that inherits from `Enum`. - Each enumeration member should represent a planet in the solar system with the following data: - Name of the planet (string). - Average distance from the sun (in million kilometers, int). - Mass (in 10^24 kg, float). - Ensure that each planet\'s distance from the sun is unique. 2. **Custom Method**: - Implement a method within the `Planet` enum class named `describe` that returns a description string in the format: ``` \\"{name}: Distance from the sun is {distance} million km and mass is {mass} x 10^24 kg\\" ``` 3. **Automatically Generated Values**: - Each planet must have a unique integer value starting from 1. Use the `auto()` feature to assign these values. 4. **Ensuring Uniqueness**: - Use the `@unique` decorator from the `enum` module to ensure there are no duplicate values for the distances. 5. **Example Usage**: - Demonstrate the usage of the enum by iterating over all members and printing their description using the `describe` method. Expected Implementation ```python from enum import Enum, auto, unique @unique class Planet(Enum): def __new__(cls, name, distance, mass): obj = object.__new__(cls) obj._value_ = auto() obj.name = name obj.distance = distance obj.mass = mass return obj MERCURY = (\\"Mercury\\", 57, 0.330) VENUS = (\\"Venus\\", 108, 4.87) EARTH = (\\"Earth\\", 150, 5.97) MARS = (\\"Mars\\", 228, 0.642) JUPITER = (\\"Jupiter\\", 779, 1898) SATURN = (\\"Saturn\\", 1434, 568) URANUS = (\\"Uranus\\", 2871, 86.8) NEPTUNE = (\\"Neptune\\", 4495, 102) def describe(self): return f\\"{self.name}: Distance from the sun is {self.distance} million km and mass is {self.mass} x 10^24 kg\\" # Example usage for planet in Planet: print(planet.describe()) ``` # Constraints 1. Do not use any external modules other than Python\'s standard library `enum`. 2. Ensure that each planet has a unique `distance` value to prevent conflicts using the `@unique` decorator. 3. Make use of the `auto()` feature to automatically generate the integer values for the enum members. # Submission Your solution should contain the full implementation of the `Planet` enum and the demonstration of its usage as described above. Ensure that your code is well-commented and follows good programming practices.","solution":"from enum import Enum, auto, unique @unique class Planet(Enum): def __init__(self, name, distance, mass): self.plan_name = name self.plan_distance = distance self.plan_mass = mass MERCURY = (\\"Mercury\\", 57, 0.330) VENUS = (\\"Venus\\", 108, 4.87) EARTH = (\\"Earth\\", 150, 5.97) MARS = (\\"Mars\\", 228, 0.642) JUPITER = (\\"Jupiter\\", 779, 1898) SATURN = (\\"Saturn\\", 1434, 568) URANUS = (\\"Uranus\\", 2871, 86.8) NEPTUNE = (\\"Neptune\\", 4495, 102) def describe(self): return f\\"{self.plan_name}: Distance from the sun is {self.plan_distance} million km and mass is {self.plan_mass} x 10^24 kg\\" # Example usage if __name__ == \\"__main__\\": for planet in Planet: print(planet.describe())"},{"question":"# Question: Importing and Executing Modules from a ZIP Archive In this exercise, you are required to create a custom importer class using the `zipimport` module to handle the importation and execution of Python modules from a ZIP archive. You will implement a custom class that will dynamically import a module from a ZIP file and execute a specified function from that module. The functionality will be tested with different ZIP files containing varying structures and module contents. Task: 1. **CustomZipImporter Class Implementation**: - Create a class `CustomZipImporter` that accepts a path to a ZIP file. - Implement a method `import_and_execute` that: - Takes two arguments: `module_name` (the name of the module to import) and `func_name` (the name of the function to execute in the module). - Dynamically imports the specified module using `zipimport`. - Executes the specified function from the imported module. - Returns the result of the function execution. Constraints: - The ZIP archive only contains `.py` and `.pyc` files. - If the specified module or function does not exist, raise a custom exception `ModuleFunctionNotFound`. Input Format: - A string representing the path to the ZIP file. - A string `module_name` representing the module to import. - A string `func_name` representing the function to execute from the imported module. Output Format: - The result returned from the executed function within the module. Example: ```python # Example ZIP structure # example.zip/ # ├── module_a.py # └── module_b.py # module_a.py def hello(): return \\"Hello from module_a!\\" # module_b.py def world(): return \\"World from module_b!\\" # Usage of CustomZipImporter path_to_zip = \\"example.zip\\" importer = CustomZipImporter(path_to_zip) result_a = importer.import_and_execute(\\"module_a\\", \\"hello\\") print(result_a) # Output: \\"Hello from module_a!\\" result_b = importer.import_and_execute(\\"module_b\\", \\"world\\") print(result_b) # Output: \\"World from module_b!\\" ``` Implementation: ```python import zipimport class ModuleFunctionNotFound(Exception): pass class CustomZipImporter: def __init__(self, zip_path): self.zip_path = zip_path def import_and_execute(self, module_name, func_name): try: # Create zipimporter instance importer = zipimport.zipimporter(self.zip_path) # Import the module module = importer.load_module(module_name) # Get the function from the module func = getattr(module, func_name) # Execute the function return func() except (ImportError, AttributeError): raise ModuleFunctionNotFound(f\\"Module \'{module_name}\' or function \'{func_name}\' not found.\\") # Example usage (you will need a ZIP file with the given structure for testing): # path_to_zip = \\"example.zip\\" # importer = CustomZipImporter(path_to_zip) # result_a = importer.import_and_execute(\\"module_a\\", \\"hello\\") # print(result_a) # Output: \\"Hello from module_a!\\" # result_b = importer.import_and_execute(\\"module_b\\", \\"world\\") # print(result_b) # Output: \\"World from module_b!\\" ```","solution":"import zipimport class ModuleFunctionNotFound(Exception): pass class CustomZipImporter: def __init__(self, zip_path): self.zip_path = zip_path def import_and_execute(self, module_name, func_name): try: # Create zipimporter instance importer = zipimport.zipimporter(self.zip_path) # Import the module module = importer.load_module(module_name) # Get the function from the module func = getattr(module, func_name) # Execute the function return func() except (ImportError, AttributeError): raise ModuleFunctionNotFound(f\\"Module \'{module_name}\' or function \'{func_name}\' not found.\\")"},{"question":"Objective You are required to demonstrate your understanding of PyTorch\'s support for Intel GPUs by implementing both an inference and a training workflow. This involves checking GPU availability, migrating a model to Intel GPU, performing inference, and training the model. You should also implement usage of Automatic Mixed Precision (AMP) and evaluate the performance difference with and without AMP. Requirements 1. **GPU Availability Check**: - Write a function `check_xpu_availability()` that checks if an Intel GPU is available on the system. - **Output**: Boolean indicating the availability of Intel GPU. 2. **Model Inference with AMP**: - Write a function `model_inference(model, data, use_amp)` that performs inference on Intel GPU. - **Inputs**: - `model`: A PyTorch model instance. - `data`: Input tensor for the model. - `use_amp`: Boolean indicating whether to use Automatic Mixed Precision (AMP). - **Output**: Output tensor from the model after performing inference. 3. **Model Training with AMP**: - Write a function `train_model_with_amp(model, train_loader, criterion, optimizer, use_amp, epochs)` that trains a PyTorch model on Intel GPU. - **Inputs**: - `model`: A PyTorch model instance. - `train_loader`: DataLoader for the training data. - `criterion`: Loss function. - `optimizer`: Optimizer for the model. - `use_amp`: Boolean indicating whether to use Automatic Mixed Precision (AMP). - `epochs`: Number of training epochs. - **Output**: The trained model and the final loss value. Constraints and Limitations - Ensure the Intel GPU drivers and required packages (`torch`, `torchvision`, `torchaudio`) are installed. - The example datasets such as CIFAR-10 from `torchvision` can be used for training. - Provide comments and documentation within your code for clarity. - Handle any potential errors gracefully, such as checking if Intel GPU is available before running the functions that require it. Performance Requirements - The performance difference with and without AMP should be reported in terms of inference time and training loss. Example Usage ```python import torchvision.models as models import torch from torchvision import datasets, transforms from torch.utils.data import DataLoader import torch.nn as nn import torch.optim as optim # Example code to test your functions if __name__ == \\"__main__\\": # Check GPU availability if not check_xpu_availability(): print(\\"Intel GPU is not available.\\") exit(1) # Initialize model and data model = models.resnet50(weights=\\"ResNet50_Weights.DEFAULT\\") data = torch.rand(1, 3, 224, 224) # Perform inference with AMP output = model_inference(model, data, use_amp=True) print(\\"Inference Output:\\", output) # Prepare CIFAR-10 dataset for training transform = transforms.Compose([ transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) train_dataset = datasets.CIFAR10( root=\\"datasets/cifar10/\\", train=True, transform=transform, download=True, ) train_loader = DataLoader(dataset=train_dataset, batch_size=128) # Define loss and optimizer criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # Train model with AMP trained_model, final_loss = train_model_with_amp(model, train_loader, criterion, optimizer, use_amp=True, epochs=10) print(\\"Final training loss:\\", final_loss) ```","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.cuda.amp import autocast, GradScaler def check_xpu_availability(): Checks if an Intel GPU (xpu) is available on the system. Returns: bool: True if Intel GPU is available, False otherwise. return torch.xpu.is_available() def model_inference(model, data, use_amp): Performs model inference on Intel GPU with optional Automatic Mixed Precision (AMP). Args: model (nn.Module): A PyTorch model instance. data (torch.Tensor): Input tensor for the model. use_amp (bool): Use Automatic Mixed Precision (AMP) if True. Returns: torch.Tensor: Output tensor from the model after inference. model = model.to(\'xpu\') data = data.to(\'xpu\') model.eval() with torch.no_grad(): if use_amp: with autocast(): output = model(data) else: output = model(data) return output def train_model_with_amp(model, train_loader, criterion, optimizer, use_amp, epochs): Trains a PyTorch model on Intel GPU with optional Automatic Mixed Precision (AMP). Args: model (nn.Module): A PyTorch model instance. train_loader (DataLoader): DataLoader for the training data. criterion (nn.Module): Loss function. optimizer (torch.optim.Optimizer): Optimizer for the model. use_amp (bool): Use Automatic Mixed Precision (AMP) if True. epochs (int): Number of training epochs. Returns: tuple: The trained model and the final loss value. model = model.to(\'xpu\') scaler = GradScaler(enabled=use_amp) for epoch in range(epochs): model.train() running_loss = 0.0 for inputs, labels in train_loader: inputs, labels = inputs.to(\'xpu\'), labels.to(\'xpu\') optimizer.zero_grad() if use_amp: with autocast(): outputs = model(inputs) loss = criterion(outputs, labels) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() else: outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() print(f\\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader)}\\") final_loss = running_loss / len(train_loader) return model, final_loss"},{"question":"# Question: Visualizing Health Expenditure and Life Expectancy Trajectories You are given a dataset on health expenditure and life expectancy for different countries over multiple years. Your task is to create a trajectory plot using seaborn that visualizes how health expenditure (in USD) relates to life expectancy for each country, customizing the plot markers and lines to improve readability. Input - The dataset \\"healthexp\\" loaded using `seaborn.load_dataset(\\"healthexp\\")`. - The DataFrame will contain the following columns: - `Country`: Name of the country. - `Year`: Year of the observation. - `Spending_USD`: Health expenditure in USD. - `Life_Expectancy`: Life expectancy in years. Output - A trajectory plot where: - The x-axis represents `Spending_USD`. - The y-axis represents `Life_Expectancy`. - Each trajectory represents a different country. - Marks (circular markers) should be placed at the data points, and these marks should be white-filled with specific point size and line width. Requirements 1. Use the `seaborn.objects` interface. 2. Plot the data without sorting it explicitly (i.e., it should plot the trajectories as they appear in the dataset). 3. Customize the plot with the following properties: - Markers should be circular and filled with white color. - Marker size should be 2. - Line width should be 0.75. Example Code ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the plot p = so.Plot(healthexp, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=.75, fillcolor=\\"w\\")) p.show() ``` Your task is to complete the above code by writing the missing logic and ensuring the plot meets the given requirements.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_trajectory_plot(): Creates a trajectory plot visualizing how health expenditure relates to life expectancy for each country. # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Create the plot p = so.Plot(healthexp, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=2, linewidth=.75, fillcolor=\\"w\\")) p.show()"},{"question":"Objective: Demonstrate your understanding and ability to implement Kernel Density Estimation (KDE) using the `sklearn.neighbors.KernelDensity` class in scikit-learn. Problem Statement: You are provided with a 1D dataset of `N` points generated from a mixture of Gaussian distributions. Your task is to: 1. Implement a function to fit and plot the Kernel Density Estimate for the given data using the Gaussian, Tophat, and Epanechnikov kernels. 2. Visualize the effect of the bandwidth parameter on the smoothness of the density estimate. 3. Identify and visualize the peaks (local maxima) in the density estimate for each kernel. Input: - A 1D numpy array `X` of shape (N,). - A list of kernels to use for the density estimation (e.g., [\\"gaussian\\", \\"tophat\\", \\"epanechnikov\\"]). - A list of bandwidth values to try for the density estimation (e.g., [0.1, 0.5, 1.0]). Output: - Plots for each combination of kernel and bandwidth showing the kernel density estimate. - Identification and visualization of the peaks in the density estimate for each combination of kernel and bandwidth. Function Signature: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity from scipy.signal import find_peaks def kde_peaks_visualization(X: np.ndarray, kernels: list, bandwidths: list): Parameters: X : np.ndarray : A 1D numpy array of data points of shape (N,). kernels : list : A list of kernels to use for KDE (e.g., [\'gaussian\', \'tophat\', \'epanechnikov\']). bandwidths: list : A list of bandwidth values to try for KDE (e.g., [0.1, 0.5, 1.0]). Returns: None. This function should display plots. pass ``` Constraints: - Each provided kernel and bandwidth combination should be evaluated. - The plots should label axes, include legends, and be properly titled for clarity. - Use `scipy.signal.find_peaks` to identify peaks in KDE estimates. - Performance is not a high priority, but the code should handle datasets of up to 10,000 points efficiently. Example: ```python import numpy as np # Example Data np.random.seed(0) X = np.concatenate([np.random.normal(loc=-1, scale=1, size=500), np.random.normal(loc=2, scale=0.5, size=500)]) # Kernels and Bandwidths kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] # Call the function kde_peaks_visualization(X, kernels, bandwidths) ``` This will generate plots showing the KDE for each kernel and bandwidth, along with the identified peaks.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity from scipy.signal import find_peaks def kde_peaks_visualization(X: np.ndarray, kernels: list, bandwidths: list): Parameters: X : np.ndarray : A 1D numpy array of data points of shape (N,). kernels : list : A list of kernels to use for KDE (e.g., [\'gaussian\', \'tophat\', \'epanechnikov\']). bandwidths: list : A list of bandwidth values to try for KDE (e.g., [0.1, 0.5, 1.0]). Returns: None. This function should display plots. X_grid = np.linspace(X.min() - 1, X.max() + 1, 1000)[:, np.newaxis] for kernel in kernels: plt.figure(figsize=(15, 5)) for bandwidth in bandwidths: kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(X[:, np.newaxis]) log_density = kde.score_samples(X_grid) density = np.exp(log_density) # Find peaks peaks, _ = find_peaks(density, height=0) # Plot the density estimate plt.plot(X_grid[:, 0], density, label=f\'bandwidth={bandwidth}\') # Plot the peaks plt.plot(X_grid[peaks, 0], density[peaks], \'x\', label=\'Peaks\') plt.title(f\'Kernel Density Estimate using {kernel} kernel\') plt.xlabel(\'Data Value\') plt.ylabel(\'Density\') plt.legend() plt.show()"},{"question":"You are given a dataset on diamonds, which contains various attributes like carat, clarity, cut, etc. Using the seaborn package, you are required to create a set of plots to visualize different aspects of this dataset. # Task 1. Load the `diamonds` dataset from seaborn. 2. Create a bar plot showing the mean carat weight for each level of diamond clarity. 3. Create another bar plot showing the median carat weight for each level of diamond cut. 4. Create a custom bar plot that aggregates the data using an arbitrary function (e.g., the interquartile range of the carat weight) grouped by the diamond color. # Input No input from the user is required. All data should be loaded from the seaborn `diamonds` dataset. # Output Your function should display the following plots: 1. A bar plot with the mean carat weight for each clarity level. 2. A bar plot with the median carat weight for each cut level. 3. A bar plot showcasing the interquartile range (IQR) of the carat weight for each color. # Constraints - Use seaborn\'s `Plot` object and appropriate methods to create the plots. - Ensure each plot is labeled properly for clarity. # Function Signature ```python def visualize_diamond_data(): # Your code here ``` # Example ```python # The following code snippet should be part of your solution import seaborn.objects as so from seaborn import load_dataset def visualize_diamond_data(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Plot 1: Mean carat weight for each clarity level p1 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg()) p1.show() # Plot 2: Median carat weight for each cut level p2 = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) p2.show() # Plot 3: IQR (Interquartile Range) carat weight for each color agg_func = lambda x: x.quantile(0.75) - x.quantile(0.25) p3 = so.Plot(diamonds, x=\\"color\\", y=\\"carat\\").add(so.Bar(), so.Agg(agg_func)) p3.show() ``` The function `visualize_diamond_data` should output the three specified plots when executed.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_diamond_data(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Plot 1: Mean carat weight for each clarity level p1 = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"mean\\")) p1.show() # Plot 2: Median carat weight for each cut level p2 = so.Plot(diamonds, x=\\"cut\\", y=\\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) p2.show() # Plot 3: IQR (Interquartile Range) carat weight for each color agg_func = lambda x: x.quantile(0.75) - x.quantile(0.25) p3 = so.Plot(diamonds, x=\\"color\\", y=\\"carat\\").add(so.Bar(), so.Agg(agg_func)) p3.show()"},{"question":"Objective: To assess your understanding of profiling and optimizing scikit-learn code using Python and Cython techniques. Problem Statement: You are provided with a Python script that performs Non-Negative Matrix Factorization (NMF) on the digits dataset from `sklearn.datasets`. Your task is to: 1. Profile the given script to identify the bottlenecks in the code. 2. Optimize the bottlenecks by: - Removing nested loops with equivalent NumPy operations where applicable. - Rewriting specific functions in Cython if necessary. 3. Measure and report the performance improvements achieved. Provided Script: ```python from sklearn.decomposition import NMF from sklearn.datasets import load_digits import numpy as np def perform_nmf(X, n_components=16, tol=1e-2): model = NMF(n_components=n_components, tol=tol) W = model.fit_transform(X) H = model.components_ return W, H if __name__ == \\"__main__\\": X, _ = load_digits(return_X_y=True) %timeit perform_nmf(X) ``` Tasks: 1. **Profiling:** - Use `%prun` to profile the provided `perform_nmf` function and identify the main bottlenecks. 2. **Optimization:** - Investigate and optimize the portions of the code identified as bottlenecks. Specifically: - Replace any inefficient loops with NumPy vectorized operations. - If further optimization is necessary, rewrite the critical functions in Cython. 3. **Validation:** - Ensure that your optimized function produces the same output as the provided script. 4. **Performance Comparison:** - Measure and compare the performance of the original script and your optimized version using `%timeit`. Input: - The input is the digits dataset loaded using `sklearn.datasets.load_digits`. Output: - The output should include: 1. A summary of identified bottlenecks. 2. The optimized version of the `perform_nmf` function. 3. Performance comparison results before and after optimization. Constraints: - You should not use any other datasets or alter the way the digits dataset is loaded. - Your code should be easy to understand and maintain. Submission: Submit your optimized Python script along with a brief report detailing: - The bottlenecks you identified. - The optimizations you applied. - The performance improvements achieved. Good Luck!","solution":"from sklearn.decomposition import NMF from sklearn.datasets import load_digits import numpy as np def perform_nmf(X, n_components=16, tol=1e-2): model = NMF(n_components=n_components, tol=tol, init=\'random\', random_state=0) W = model.fit_transform(X) H = model.components_ return W, H if __name__ == \\"__main__\\": X, _ = load_digits(return_X_y=True) import timeit print(\\"Original function timing:\\") print(timeit.timeit(lambda: perform_nmf(X), number=10))"},{"question":"You are tasked with writing a Python function to create both shallow and deep copies of a custom compound object. This compound object is defined as follows: ```python class TreeNode: def __init__(self, value, children=None): self.value = value self.children = children if children is not None else [] def add_child(self, child_node): self.children.append(child_node) ``` Your function should: 1. Implement a method `__copy__` for shallow copying of `TreeNode`. 2. Implement a method `__deepcopy__` for deep copying of `TreeNode`. Additionally, you need to ensure that the deep copy handles recursive structures properly. For example: ```python node_a = TreeNode(\\"a\\") node_b = TreeNode(\\"b\\", [node_a]) node_a.add_child(node_b) # Creates a recursive structure ``` Write the complete implementation of the `TreeNode` class including the `__copy__` and `__deepcopy__` methods. **Function Signature:** ```python def __copy__(self): Returns a shallow copy of the TreeNode. def __deepcopy__(self, memo): Returns a deep copy of the TreeNode with memoization to handle recursive structures. ``` **Constraints:** 1. Your solution should handle recursive references without going into an infinite loop. 2. The `__deepcopy__` method must use the `copy.deepcopy` function for copying nested objects. **Examples:** 1. Shallow Copy: ```python node_a = TreeNode(\\"a\\") node_b = TreeNode(\\"b\\", [node_a]) shallow_copy_node_b = copy.copy(node_b) assert shallow_copy_node_b is not node_b assert shallow_copy_node_b.children[0] is node_a # Reference is copied, not the object ``` 2. Deep Copy: ```python import copy node_a = TreeNode(\\"a\\") node_b = TreeNode(\\"b\\", [node_a]) node_a.add_child(node_b) # Creates a recursive structure deep_copy_node_b = copy.deepcopy(node_b) assert deep_copy_node_b is not node_b assert deep_copy_node_b.children[0] is not node_a # Object is copied assert deep_copy_node_b.children[0].value == \\"a\\" ``` Implement the `TreeNode` class with the specified copying methods.","solution":"import copy class TreeNode: def __init__(self, value, children=None): self.value = value self.children = children if children is not None else [] def add_child(self, child_node): self.children.append(child_node) def __copy__(self): cls = self.__class__ result = cls.__new__(cls) result.__dict__.update(self.__dict__) result.children = self.children[:] # Make a shallow copy of the list of children return result def __deepcopy__(self, memo): cls = self.__class__ result = cls.__new__(cls) memo[id(self)] = result for k, v in self.__dict__.items(): setattr(result, k, copy.deepcopy(v, memo)) return result"},{"question":"Objective: The objective of this assessment is to evaluate your understanding of the `unittest.mock` library in Python, specifically focusing on its core functionalities such as mocking objects, patching, making assertions about call arguments, handling side effects, and working with setups for testing asynchronous functions and magic methods. Problem Statement: You are given a task to test the following `UserService` class using the `unittest.mock` library. ```python class Database: def get_user(self, user_id): Fetches user details from the database pass def save_user(self, user_data): Saves user data to the database pass def update_user(self, user_id, user_data): Updates user details in the database pass class UserService: def __init__(self, db: Database): self.db = db def fetch_user(self, user_id): user = self.db.get_user(user_id) if not user: raise ValueError(\\"User not found\\") return user def create_user(self, user_data): validation_status = self.validate_user_data(user_data) if validation_status != \\"Valid\\": raise ValueError(\\"Invalid user data\\") self.db.save_user(user_data) def update_user(self, user_id, user_data): self.db.update_user(user_id, user_data) def validate_user_data(self, user_data): Assume this function does complex validation if \\"name\\" in user_data and \\"email\\" in user_data: return \\"Valid\\" return \\"Invalid\\" ``` Task: Write a test suite for `UserService` using the `unittest.mock` library, ensuring that you address the following functionalities: 1. **fetch_user** : - Test with a valid user_id and make sure the function returns the user data. - Test with an invalid user_id to ensure it raises a `ValueError`. 2. **create_user** : - Mock the `validate_user_data` method to always return \\"Valid\\" and ensure that `save_user` is called with the correct data. - Mock the `validate_user_data` method to return \\"Invalid\\" and ensure it raises a `ValueError`. 3. **update_user** : - Ensure that `update_user` method in `db` is called with correct arguments. Include the use of: - `Mock` and `MagicMock` for creating mock objects and configuring them. - `patch` decorator for replacing methods and classes with mocks. - Assertions to verify that the correct methods are called with appropriate arguments. - Handling side effects to simulate method behaviors. Constraints: - Do not alter the `UserService`, `Database`, or any method implementations. - Focus on writing clear, concise, and comprehensive test cases. - Ensure that your tests cover all edge cases and possible scenarios. Example: ```python import unittest from unittest.mock import Mock, patch class TestUserService(unittest.TestCase): def setUp(self): self.mock_db = Mock(spec=Database) self.user_service = UserService(self.mock_db) @patch.object(UserService, \'validate_user_data\', return_value=\\"Valid\\") def test_create_user_valid(self, mock_validate): user_data = {\'name\': \'John Doe\', \'email\': \'john@example.com\'} self.user_service.create_user(user_data) mock_validate.assert_called_once_with(user_data) self.mock_db.save_user.assert_called_once_with(user_data) @patch.object(UserService, \'validate_user_data\', return_value=\\"Invalid\\") def test_create_user_invalid(self, mock_validate): user_data = {\'name\': \'John Doe\'} with self.assertRaises(ValueError): self.user_service.create_user(user_data) mock_validate.assert_called_once_with(user_data) self.mock_db.save_user.assert_not_called() def test_fetch_user_valid(self): user_data = {\'name\': \'John Doe\', \'email\': \'john@example.com\'} self.mock_db.get_user.return_value = user_data result = self.user_service.fetch_user(1) self.mock_db.get_user.assert_called_once_with(1) self.assertEqual(result, user_data) def test_fetch_user_not_found(self): self.mock_db.get_user.return_value = None with self.assertRaises(ValueError): self.user_service.fetch_user(1) self.mock_db.get_user.assert_called_once_with(1) def test_update_user(self): user_data = {\'name\': \'John Doe\', \'email\': \'john@example.com\'} self.user_service.update_user(1, user_data) self.mock_db.update_user.assert_called_once_with(1, user_data) if __name__ == \'__main__\': unittest.main() ``` Your task is to extend the provided examples to cover all specified functionalities. Write your tests in a separate file and ensure they pass successfully.","solution":"from unittest.mock import Mock, patch import unittest class Database: def get_user(self, user_id): Fetches user details from the database pass def save_user(self, user_data): Saves user data to the database pass def update_user(self, user_id, user_data): Updates user details in the database pass class UserService: def __init__(self, db: Database): self.db = db def fetch_user(self, user_id): user = self.db.get_user(user_id) if not user: raise ValueError(\\"User not found\\") return user def create_user(self, user_data): validation_status = self.validate_user_data(user_data) if validation_status != \\"Valid\\": raise ValueError(\\"Invalid user data\\") self.db.save_user(user_data) def update_user(self, user_id, user_data): self.db.update_user(user_id, user_data) def validate_user_data(self, user_data): Assume this function does complex validation if \\"name\\" in user_data and \\"email\\" in user_data: return \\"Valid\\" return \\"Invalid\\""},{"question":"# Question: Telnet Client Communication You are required to implement a simplified Telnet client using the `telnetlib` module to communicate with a Telnet server. Your function should: 1. Connect to a specified Telnet server. 2. Log in with a provided username and password. 3. Execute a series of commands. 4. Retrieve and format the results of these commands. Input - `host: str` - The hostname or IP address of the Telnet server. - `port: int` - The port number of the Telnet server. - `username: str` - The username for login. - `password: str` - The password for login. - `commands: List[str]` - A list of commands to be executed on the server. Output - `results: str` - The cumulative outputs of all commands executed on the server, formatted as a single string. Constraints - Your function should handle possible exceptions such as timeouts, connection errors, and incorrect login credentials. - Assume commands and responses are in ASCII encoding. - Implement a connection timeout of 10 seconds. - The server responses may have unpredictable delays, so handle such scenarios gracefully. Function Signature ```python def telnet_client_communication(host: str, port: int, username: str, password: str, commands: List[str]) -> str: ``` Example ```python host = \\"localhost\\" port = 23 username = \\"user\\" password = \\"pass\\" commands = [\\"ls\\", \\"pwd\\", \\"whoami\\"] result = telnet_client_communication(host, port, username, password, commands) print(result) ``` # Implementation Details 1. Establish a connection to the Telnet server using the `telnetlib.Telnet` class. 2. Perform the login sequence by waiting for corresponding prompts (e.g., \\"login:\\", \\"Password:\\") and sending the username and password. 3. Send the series of commands one by one and collect their outputs. 4. Return the cumulative result of the executed commands. 5. Use methods provided by the `telnetlib.Telnet` class such as `read_until`, `write`, and `read_all` to perform network operations. Note: This task requires familiarity with handling network connections and error handling in Python to implement a robust solution. Good Luck!","solution":"import telnetlib def telnet_client_communication(host: str, port: int, username: str, password: str, commands: list) -> str: try: # Establish a connection to the Telnet server tn = telnetlib.Telnet(host, port, timeout=10) # Perform the login sequence tn.read_until(b\\"login: \\") tn.write(username.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") results = \\"\\" # Execute each command and collect the output for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") # Some servers might take unpredictable amount of time to respond response = tn.read_until(b\\" \\").decode(\'ascii\') # Assuming prompt ends with \\" \\" results += response # Close the connection tn.close() return results except Exception as e: # Handles all exceptions like connection errors, timeouts, etc. return str(e)"},{"question":"# PyTorch Coding Assessment Question Objective: This question is designed to assess your understanding of PyTorch and TorchScript, focusing on tensor operations, mathematical computations, and control structures within the TorchScript constraints. Problem Statement: You are provided with a multidimensional tensor representing data about patient examinations in a hospital. Each element of the tensor corresponds to a certain measure such as temperature, heart rate, blood pressure etc. You need to implement a function that processes this tensor to detect abnormal readings and inverts those abnormal readings. Specifically: 1. **Abnormal Temperature**: Any reading not in the range `[36.5, 37.5]` degrees Celsius. 2. **Abnormal Heart Rate**: Any reading not in the range `[60, 100]` beats per minute. 3. **Abnormal Blood Pressure**: Any reading where systolic pressure is not in the range `[90, 140]` or diastolic pressure is not in the range `[60, 90]`. Your task is to implement a function `invert_abnormal_readings(data: torch.Tensor, threshold: float) -> torch.Tensor` that: 1. Inverts each value in the `data` tensor that is determined to be abnormal as per the above criteria. 2. Only processes `temperature`, `heart_rate`, and `blood_pressure` columns in the tensor. 3. Returns a tensor where all abnormal readings are inverted, multiplied by a given `threshold`. Constraints: * The input tensor will always be of shape `(n, 3)` where `n` is the number of patients. * The first column corresponds to `temperature`. * The second column corresponds to `heart_rate`. * The third column corresponds to `blood_pressure` in the format `systolic/diastolic` (assume systolic and diastolic values are given as one value each). Example: Input: ``` data = tensor([[35.0, 50, 130], [37.0, 80, 90], [39.0, 60, 85]]) threshold = 2.0 ``` Output: ``` tensor([[-70.0, -100.0, -260.0], [ 37.0, 80.0, 90.0], [-78.0, 60.0, -85.0]]) ``` Solution Template: ```python import torch @torch.jit.script def invert_abnormal_readings(data: torch.Tensor, threshold: float) -> torch.Tensor: # Initialize the result tensor to the input data result = data.clone() # Process temperature abnormal readings mask_temp = (data[:, 0] < 36.5) | (data[:, 0] > 37.5) result[mask_temp, 0] = -data[mask_temp, 0] * threshold # Process heart rate abnormal readings mask_hr = (data[:, 1] < 60) | (data[:, 1] > 100) result[mask_hr, 1] = -data[mask_hr, 1] * threshold # Process blood pressure abnormal readings mask_bp = (data[:, 2] < 150) | (data[:, 2] > 180) # Assuming sum of systolic/diastolic as one value result[mask_bp, 2] = -data[mask_bp, 2] * threshold return result # Example usage data = torch.tensor([[35.0, 50, 130], [37.0, 80, 90], [39.0, 60, 85]], dtype=torch.float) threshold = 2.0 print(invert_abnormal_readings(data, threshold)) ``` Notes: - Make sure your function is decorated with `@torch.jit.script` to compile it into TorchScript. - Focus on manipulating tensors using PyTorch operations. - Ensure correct handling of tensor slicing and element-wise operations.","solution":"import torch @torch.jit.script def invert_abnormal_readings(data: torch.Tensor, threshold: float) -> torch.Tensor: # Initialize the result tensor to the input data result = data.clone() # Process temperature abnormal readings mask_temp = (data[:, 0] < 36.5) | (data[:, 0] > 37.5) result[mask_temp, 0] = -data[mask_temp, 0] * threshold # Process heart rate abnormal readings mask_hr = (data[:, 1] < 60) | (data[:, 1] > 100) result[mask_hr, 1] = -data[mask_hr, 1] * threshold # Process blood pressure abnormal readings mask_bp_systolic = (data[:, 2] < 90) | (data[:, 2] > 140) diastolic_pressures = torch.minimum(data[:, 2], torch.tensor(90.0)) mask_bp_diastolic = (diastolic_pressures < 60) | (diastolic_pressures > 90) mask_bp = mask_bp_systolic | mask_bp_diastolic result[mask_bp, 2] = -data[mask_bp, 2] * threshold return result"},{"question":"Objective This question aims to assess your understanding of seaborn\'s `objects` interface, particularly in generating density plots and customizing them based on different parameters and conditions. Task Using the provided `penguins` dataset from seaborn, create a function `density_plots` that generates multiple customized density plots based on the following specifications: 1. **Default Density Plot**: - Plot a basic density estimation for the `flipper_length_mm` variable. 2. **Adjusted Bandwidth Plot**: - Create a density plot for the `flipper_length_mm` variable using a bandwidth adjustment factor of 0.25. 3. **Grouped Density Plot**: - Generate a density plot for the `flipper_length_mm` variable, grouped by the `species` column and with individual densities normalized (i.e., conditional densities). 4. **Faceted Density Plot**: - Produce a set of faceted density plots for the `flipper_length_mm` variable, separated by the `sex` of the penguin and colored by the `species` column. Input and Output Formats # Input: - No direct input to the function `density_plots`. # Output: - The function should display the generated plots. It should not return any values. ```python import seaborn.objects as so from seaborn import load_dataset def density_plots(): penguins = load_dataset(\\"penguins\\") # Default Density Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE()) p.show() # Adjusted Bandwidth Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE(bw_adjust=0.25)) p.show() # Grouped Density Plot p = so.Pplot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE(common_norm=False), color=\\"species\\") p.show() # Faceted Density Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\") p.add(so.Area(), so.KDE(common_norm=[\\"col\\"]), color=\\"species\\") p.show() ``` Constraints - Ensure that the plots correctly display without errors. - Use appropriate seaborn functions to meet the requirements of each specific plot. Note Ensure you have seaborn installed and available to run the code: ```bash pip install seaborn ``` Evaluation Criteria - Correctness of the generated plots based on the specifications. - Proper usage of seaborn\'s plotting interface. - Code readability and adherence to Python coding standards.","solution":"import seaborn.objects as so from seaborn import load_dataset def density_plots(): penguins = load_dataset(\\"penguins\\") # Default Density Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE()) p.show() # Adjusted Bandwidth Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\") p.add(so.Area(), so.KDE(bw_adjust=0.25)) p.show() # Grouped Density Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\") p.add(so.Area(), so.KDE(common_norm=False)) p.show() # Faceted Density Plot p = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\") p.add(so.Area(), so.KDE(common_norm=\\"facet\\"), color=\\"species\\") p.show()"},{"question":"# Audio Data Manipulation and Analysis using `audioop` Module **Objective:** You are tasked with writing a function that processes a raw audio fragment to apply multiple operations, including converting the encoding, adjusting the volume, and computing some statistics. **Problem Statement:** Write a function `process_audio_data(fragment: bytes, width: int, volume_factor: float) -> dict` that takes the following parameters: 1. `fragment` (bytes): The raw audio data fragment. 2. `width` (int): The sample width in bytes. Allowed values are 1, 2, 3, and 4. 3. `volume_factor` (float): A factor by which to adjust the volume of the audio data. The function should perform the following operations in order: 1. Convert the audio data from linear encoding to a-LAW encoding if the sample width is 2 bytes. 2. Adjust the volume of the audio data by multiplying all samples by the `volume_factor`. 3. Calculate a few statistics from the modified audio data: - The average value of all samples. - The maximum absolute sample value. - The root-mean-square (RMS) of the sample values. Finally, return the results of these operations as a dictionary with the following keys and respective values: - `\\"avg_value\\"`: The average value of all samples. - `\\"max_abs_value\\"`: The maximum absolute sample value. - `\\"rms\\"`: The RMS of the sample values. - `\\"modified_fragment\\"`: The modified audio fragment after volume adjustment. **Constraints:** - The input `fragment` will be a valid bytes-like object. - The input `width` will be one of the allowed values (1, 2, 3, 4). - The volume factor is a positive floating-point number. - If the `width` is not 2, skip the a-LAW encoding conversion step and proceed with volume adjustment and calculations. **Example:** ```python fragment = b\'x01x02x03x04\' width = 2 volume_factor = 1.5 result = process_audio_data(fragment, width, volume_factor) print(result) ``` Example output: ```python { \\"avg_value\\": 1056.5, \\"max_abs_value\\": 1584, \\"rms\\": 1159.5716796829205, \\"modified_fragment\\": b\'x02x03x04x06\' } ``` **Implementation Notes:** - Use the `audioop.lin2alaw` function for encoding conversion. - Use the `audioop.mul` function for volume adjustment. - Use the `audioop.avg`, `audioop.max`, and `audioop.rms` functions to compute the required statistics. - Ensure proper handling of sample width during the manipulation and conversion processes.","solution":"import audioop def process_audio_data(fragment: bytes, width: int, volume_factor: float) -> dict: Processes a raw audio fragment by converting encoding, adjusting volume, and computing statistics. Args: fragment (bytes): The raw audio data fragment. width (int): The sample width in bytes. volume_factor (float): A factor by which to adjust the volume of the audio data. Returns: dict: A dictionary with the average value, max absolute value, RMS, and the modified audio fragment. if width == 2: # Convert from linear to a-LAW encoding fragment = audioop.lin2alaw(fragment, width) # Adjust the volume modified_fragment = audioop.mul(fragment, width, volume_factor) # Calculate statistics avg_value = audioop.avg(modified_fragment, width) max_abs_value = audioop.max(modified_fragment, width) rms = audioop.rms(modified_fragment, width) return { \\"avg_value\\": avg_value, \\"max_abs_value\\": max_abs_value, \\"rms\\": rms, \\"modified_fragment\\": modified_fragment }"},{"question":"**Objective:** Implement an optimized matrix multiplication function using PyTorch. Your function should leverage PyTorch\'s efficient operations that are likely to benefit from SIMD instructions. # Requirements: 1. **Function Name:** `optimized_matrix_multiply` 2. **Input:** - Two 2-dimensional PyTorch tensors `A` and `B` with shapes `(m, n)` and `(n, p)` respectively. 3. **Output:** - A 2-dimensional PyTorch tensor `C` with shape `(m, p)` resulting from the matrix multiplication of `A` and `B`. # Constraints: 1. Ensure that the input tensors are of appropriate shapes for matrix multiplication. 2. Optimize the computation using vectorized operations provided by PyTorch. # Example: ```python import torch def optimized_matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: # Your implementation here pass # Example usage: A = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) B = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32) C = optimized_matrix_multiply(A, B) print(C) # Expected output: tensor([[19., 22.], [43., 50.]]) ``` # Notes: - Your implementation should make use of efficient PyTorch functions that are likely to benefit from SIMD instructions. - You may assume that the input tensors are always valid and do not need additional validation within your function. # Performance Requirement: - The implementation should be efficient, making use of vectorized operations. If possible, benchmark your implementation and discuss how it leverages PyTorch\'s optimization capabilities.","solution":"import torch def optimized_matrix_multiply(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform matrix multiplication using PyTorch\'s optimized operations. :param A: 2D PyTorch tensor of shape (m, n) :param B: 2D PyTorch tensor of shape (n, p) :return: 2D PyTorch tensor of shape (m, p), the result of A * B return torch.matmul(A, B)"},{"question":"# Complex Number Operations Assessment **Objective**: To test your understanding of complex numbers, their mathematical properties, and the usage of the `cmath` module. **Task Description**: You need to implement a function `complex_math_operations` that accepts a list of complex numbers and an operation type as arguments. Based on the operation type, the function should perform the following tasks: 1. **\\"magnitude_phase\\"**: - For each complex number in the list, convert it to its polar coordinate representation and return a list of tuples, where each tuple contains the magnitude (r) and phase (phi) of the corresponding complex number. 2. **\\"rect_from_polar\\"**: - Accept an additional list of tuples, where each tuple contains the magnitude and phase of a complex number. Convert each (r, phi) pair back to its rectangular form and return a list of the resulting complex numbers. 3. **\\"exp_log_operations\\"**: - For each complex number in the list, compute `e` raised to the power of the complex number and its natural logarithm. Return a list of tuples where each tuple contains (`exp(z)`, `log(z)`) for the corresponding complex number `z`. 4. **\\"trig_hyperbolic_functions\\"**: - For each complex number in the list, compute its cosine, sine, tangent, hyperbolic cosine, hyperbolic sine, and hyperbolic tangent. Return a list of dictionaries where each dictionary contains the function names as keys and their respective results as values for the corresponding complex number. **Input**: - `complex_numbers`: A list of complex numbers `[z1, z2, ..., zn]`. - `operation`: A string indicating the operation to perform (\\"magnitude_phase\\", \\"rect_from_polar\\", \\"exp_log_operations\\", \\"trig_hyperbolic_functions\\"). - `polar_coordinates`: An additional list of tuples [(r1, phi1), (r2, phi2), ..., (rn, phin)] (only needed if `operation` is \\"rect_from_polar\\"). **Output**: - A list of results as described for each operation type. **Constraints**: - The input list of complex numbers will have at most 100 elements. - Each complex number and polar coordinate magnitude will have absolute values less than or equal to 1e6. **Function Signature**: ```python import cmath def complex_math_operations(complex_numbers, operation, polar_coordinates=None): # Your code here ``` **Example**: ```python # Example inputs complex_numbers = [complex(1, 1), complex(-1, -1), complex(0, 2)] polar_coordinates = [(1.4142135623730951, 0.7853981633974483), (1.4142135623730951, -2.356194490192345), (2.0, 1.5707963267948966)] operation1 = \\"magnitude_phase\\" operation2 = \\"rect_from_polar\\" operation3 = \\"exp_log_operations\\" operation4 = \\"trig_hyperbolic_functions\\" # Example outputs print(complex_math_operations(complex_numbers, operation1)) # Expected: [(1.4142135623730951, 0.7853981633974483), (1.4142135623730951, -2.356194490192345), (2.0, 1.5707963267948966)] print(complex_math_operations(complex_numbers, operation2, polar_coordinates)) # Expected: [(1+1j), (-1-1j), 2j] print(complex_math_operations(complex_numbers, operation3)) # Expected: [(1.4686939399158851+2.2873552871788423j, (0.34657359027997264+0.7853981633974483j)), ...] print(complex_math_operations(complex_numbers, operation4)) # Expected: [{\'cos\': (0.8337300251311491-0.9888977057628651j), \'sin\': (1.2984575814159773+0.6349639147847361j), ...}, ...] ``` **Notes**: - You should handle edge cases, such as complex numbers with zero real or imaginary parts. - Efficiency is crucial, ensure your solution scales well with the input constraints.","solution":"import cmath def complex_math_operations(complex_numbers, operation, polar_coordinates=None): if operation == \\"magnitude_phase\\": result = [(cmath.polar(cmplx)) for cmplx in complex_numbers] return result elif operation == \\"rect_from_polar\\": if polar_coordinates is None: raise ValueError(\\"Polar coordinates must be provided for \'rect_from_polar\' operation.\\") result = [cmath.rect(r, phi) for r, phi in polar_coordinates] return result elif operation == \\"exp_log_operations\\": result = [(cmath.exp(cmplx), cmath.log(cmplx)) for cmplx in complex_numbers] return result elif operation == \\"trig_hyperbolic_functions\\": result = [] for cmplx in complex_numbers: func_dict = { \'cos\': cmath.cos(cmplx), \'sin\': cmath.sin(cmplx), \'tan\': cmath.tan(cmplx), \'cosh\': cmath.cosh(cmplx), \'sinh\': cmath.sinh(cmplx), \'tanh\': cmath.tanh(cmplx), } result.append(func_dict) return result else: raise ValueError(\\"Unknown operation type provided.\\")"},{"question":"**Python Coding Assessment:** `zipimport` Module # Background The `zipimport` module enables importing Python modules from ZIP-format archives. This can be useful for bundling modules into a single file or distributing applications. The module integrates into Python\'s built-in import system, so ZIP files can be imported as if they were directories containing Python modules. # Task You are required to implement a class that mimics the behavior of the `zipimport.zipimporter` class. This custom implementation should be able to: 1. Initialize with the path to a ZIP file or a specific path within a ZIP file. 2. Retrieve the code object for a given module. 3. Return the filename for a given module. 4. Return the source code of a given module. # Requirements - **Class Signature:** `class CustomZipImporter` - **Methods:** - `__init__(self, archivepath: str)` - Initializes the importer with the path to a ZIP file or to a specific path within a ZIP file. Raises an `ImportError` if the path is invalid. - `get_code(self, fullname: str) -> object` - Returns a code object for the specified module. Raises `ImportError` if the module is not found. - `get_filename(self, fullname: str) -> str` - Returns the filename that would be set as `__file__` for the specified module. Raises `ImportError` if the module is not found. - `get_source(self, fullname: str) -> str` - Returns the source code as a string for the specified module. Raises `ImportError` if the module is not found. # Example ```python # Assuming we have \'example.zip\' containing \'test_module.py\'. # Initialize the importer zip_importer = CustomZipImporter(\'example.zip\') # Retrieve code object code_obj = zip_importer.get_code(\'test_module\') print(type(code_obj)) # Expected: <class \'code\'> # Retrieve filename filename = zip_importer.get_filename(\'test_module\') print(filename) # Expected: \'example.zip/test_module.py\' # Retrieve source code source = zip_importer.get_source(\'test_module\') print(source) # Expected: The content of \'test_module.py\' ``` # Constraints 1. You may use standard libraries such as `zipfile` and `importlib` where appropriate. 2. Your implementation should handle nested directories in ZIP files. 3. You must handle potential errors gracefully, providing informative error messages. # Submission Submit your implementation of `CustomZipImporter` along with a test suite demonstrating its functionality with various scenarios, including but not limited to: - Importing a module from the root of a ZIP file. - Importing a module from a nested directory within a ZIP file. - Handling non-existent files or directories within the ZIP file.","solution":"import zipfile import importlib.util import types import os class CustomZipImporter: def __init__(self, archivepath: str): if not zipfile.is_zipfile(archivepath): raise ImportError(f\\"{archivepath} is not a valid ZIP file\\") self.archivepath = archivepath def _extract_module(self, fullname: str): file_path = fullname.replace(\'.\', \'/\') + \'.py\' with zipfile.ZipFile(self.archivepath, \'r\') as archive: if file_path not in archive.namelist(): raise ImportError(f\\"Module {fullname} not found in {self.archivepath}\\") source = archive.read(file_path).decode(\'utf-8\') return source, file_path def get_code(self, fullname: str) -> types.CodeType: source, _ = self._extract_module(fullname) return compile(source, fullname, \'exec\') def get_filename(self, fullname: str) -> str: _, file_path = self._extract_module(fullname) return os.path.join(self.archivepath, file_path) def get_source(self, fullname: str) -> str: source, _ = self._extract_module(fullname) return source"},{"question":"**Problem Statement:** # Partial Least Squares Regression Implementation and Application You are required to implement a Partial Least Squares (PLS) Regression algorithm using scikit-learn\'s `PLSRegression` class. Your task involves understanding the methodology described in the documentation and applying it to perform dimensionality reduction and prediction. # Instructions: 1. **Function Signature**: Implement a function named `perform_pls_regression` with the following signature: ```python def perform_pls_regression(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, n_components: int) -> np.ndarray: pass ``` 2. **Input**: - `X_train`: A 2D numpy array of shape (n_samples, n_features) representing the training features. - `Y_train`: A 2D numpy array of shape (n_samples, n_targets) representing the training targets. - `X_test`: A 2D numpy array of shape (m_samples, n_features) representing the testing features. - `n_components`: An integer specifying the number of components to keep after dimensionality reduction. 3. **Output**: - Returns `Y_pred`, a 2D numpy array of shape (m_samples, n_targets) containing the predicted targets for the `X_test` dataset. 4. **Constraints**: - You should handle the scenarios where the number of features is greater than the number of samples. - Ensure the implementation considers potential multicollinearity in the features. # Example: ```python import numpy as np # Example data X_train = np.array([[0., 0., 1.], [1., 0., 0.], [2., 2., 2.], [3., 5., 4.]]) Y_train = np.array([[0.1], [0.9], [6.1], [11.9]]) X_test = np.array([[1., 2., 3.], [4., 5., 6.]]) # Number of components n_components = 2 # Call the function Y_pred = perform_pls_regression(X_train, Y_train, X_test, n_components) print(Y_pred) ``` # Requirements: 1. **Implementation**: - Use `PLSRegression` from the scikit-learn library to fit the model on `X_train` and `Y_train`. - Perform the transformation on `X_test` to predict the `Y_test` values. 2. **Evaluation**: - Ensure that your function is efficient and can handle reasonably large datasets. - You should not use any loops for matrix computations; use numpy\'s vectorized operations. # Note: The expected output might not be available but ensure your implementation adheres to the concepts of PLS Regression and the given example templates.","solution":"import numpy as np from sklearn.cross_decomposition import PLSRegression def perform_pls_regression(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, n_components: int) -> np.ndarray: Perform Partial Least Squares Regression using the provided training and testing data. Parameters: X_train (np.ndarray): Training features of shape (n_samples, n_features). Y_train (np.ndarray): Training targets of shape (n_samples, n_targets). X_test (np.ndarray): Testing features of shape (m_samples, n_features). n_components (int): Number of components to keep after dimensionality reduction. Returns: np.ndarray: Predicted targets for X_test. # Initialize the PLS regression model pls = PLSRegression(n_components=n_components) # Fit the model on the training data pls.fit(X_train, Y_train) # Predict the targets for the test data Y_pred = pls.predict(X_test) return Y_pred"},{"question":"# Question: Advanced Color Palette Customization with Seaborn You are tasked with visualizing temperature data from several cities over a year using a heatmap. However, you need to ensure that the colors used in the heatmap are distinguishable and visually appealing using a custom color palette created via Seaborn\'s `sns.cubehelix_palette` function. Requirements: 1. **Data Generation**: Generate a random dataset with 12 rows (representing months) and 5 columns (representing different cities\' average temperatures). 2. **Custom Color Palette**: Create a custom color palette using `sns.cubehelix_palette` with the following specifications: - 10 discrete colors. - Start the helix at 1.5. - Rotate the helix by 0.8. - Apply a gamma correction of 0.6. - Increase hue to 0.85. - Set the luminance starting point at 0.3 and end point at 0.9. - Reverse the luminance direction. 3. **Heatmap Visualization**: Use the created color palette to generate a heatmap of the temperature data. 4. **Output**: - A function `generate_heatmap` that takes no input and outputs the heatmap. - The function should return the heatmap object. Constraints: - Use Seaborn and Matplotlib for visualization. - Ensure all configurations for the palette are encapsulated within the function. Function Signature: ```python def generate_heatmap(): import seaborn as sns import numpy as np import matplotlib.pyplot as plt # Function implementation ``` # Example: ```python # Running the function heatmap = generate_heatmap() plt.show() # To display the heatmap ``` Your solution will be evaluated based on: - Correctness and adherence to the specifications. - Cleanliness and readability of the code. - Proper usage of Seaborn to create and apply the custom palette.","solution":"def generate_heatmap(): import seaborn as sns import numpy as np import matplotlib.pyplot as plt # Data Generation np.random.seed(0) data = np.random.rand(12, 5) * 30 # Random temperatures between 0 and 30 # Custom Color Palette custom_palette = sns.cubehelix_palette(n_colors=10, start=1.5, rot=0.8, gamma=0.6, hue=0.85, light=0.9, dark=0.3, reverse=True) # Heatmap Visualization plt.figure(figsize=(10, 8)) heatmap = sns.heatmap(data, cmap=custom_palette, annot=True, cbar=True) plt.title(\\"Heatmap of Average Monthly Temperatures for Different Cities\\") plt.xlabel(\\"Cities\\") plt.ylabel(\\"Months\\") plt.show() return heatmap"},{"question":"Objective Write a Python function that automates the byte-compilation of Python source files in a directory tree while allowing specified customizations via parameters. Function Signature ```python def custom_compile_dir(dir: str, maxlevels: int = None, ddir: str = None, force: bool = False, regex: str = None, quiet: int = 0, legacy: bool = False, optimize: int = -1, workers: int = 1) -> bool: ``` Parameters - `dir` (str): The directory to recursively compile. - `maxlevels` (int, optional): Maximum level of recursion. Defaults to the system\'s recursion limit if not specified. - `ddir` (str, optional): Directory to prepend to the path for use in tracebacks. - `force` (bool, optional): Force recompilation even if timestamps are up-to-date. Defaults to `False`. - `regex` (str, optional): Regular expression pattern to skip files that match. Defaults to `None`. - `quiet` (int, optional): Control the level of verbosity. `0` for normal, `1` to suppress output except errors, `2` to suppress all output. Defaults to `0`. - `legacy` (bool, optional): Write byte-code files to legacy locations and names. Defaults to `False`. - `optimize` (int, optional): Optimization level to pass to the compiler. Defaults to `-1`. - `workers` (int, optional): Number of worker threads to use for parallel compilation. Defaults to `1`. Returns - `bool`: Returns `True` if all files were compiled successfully, otherwise `False`. Additional Constraints - If `regex` is provided, it should be a valid regular expression pattern. - The function should handle invalid input values gracefully (e.g., non-existent directories, invalid regex patterns) by catching exceptions and returning `False`. Example Usage ```python result = custom_compile_dir( dir=\'my_project\', maxlevels=5, ddir=\'/compiled\', force=True, regex=r\'__pycache__\', quiet=1, legacy=False, optimize=2, workers=4 ) print(result) # Expected output: True if successful, False otherwise ``` This function should leverage `compileall.compile_dir` to perform the compilation according to the specified parameters.","solution":"import os import re import compileall def custom_compile_dir(dir: str, maxlevels: int = None, ddir: str = None, force: bool = False, regex: str = None, quiet: int = 0, legacy: bool = False, optimize: int = -1, workers: int = 1) -> bool: try: # Validate input directory if not os.path.isdir(dir): raise ValueError(f\\"The directory {dir} does not exist.\\") # Check if regex is valid if regex is not None: try: re.compile(regex) except re.error: raise ValueError(f\\"The regex pattern {regex} is invalid.\\") # Call `compile_dir` with the provided parameters result = compileall.compile_dir( dir, maxlevels=maxlevels, ddir=ddir, force=force, rx=re.compile(regex) if regex else None, quiet=quiet, legacy=legacy, optimize=optimize, workers=workers ) return result except Exception as ex: if quiet < 2: # Only print error if quiet level is less than 2 print(f\\"An error occurred: {ex}\\") return False"},{"question":"**Problem Statement:** You are required to create a logging system for Python scripts that captures detailed exception tracebacks, formats them using the `cgitb` module, and logs them to a specified directory if provided. If no directory is provided, the tracebacks should be printed to the console in HTML format. Your task is to implement a function `setup_logging_system(logdir=None, format=\'html\')` that: - Configures the `cgitb` module to handle uncaught exceptions. - Logs detailed tracebacks to the specified directory if `logdir` is not `None`. - Uses HTML format by default for the output of tracebacks unless a different format is specified. Additionally, implement a custom exception handling function `custom_handler(info=None)` that: - Uses `cgitb.handler(info)` to handle exceptions using the provided `info` tuple. - If no `info` is provided, it should handle the current exception. # Function Signatures: ```python def setup_logging_system(logdir: str = None, format: str = \'html\') -> None: pass def custom_handler(info: tuple = None) -> None: pass ``` # Example Usage: ```python setup_logging_system(logdir=\'/path/to/logdir\', format=\'text\') try: raise ValueError(\\"An example error\\") except Exception: custom_handler() setup_logging_system(format=\'html\') try: 1 / 0 except Exception: custom_handler() ``` # Constraints: - The `logdir` parameter, if provided, should be a valid directory path. Assume the directory exists and is writable. - The `format` parameter should be either `\'html\'` or `\'text\'`. # Notes: - Use the `cgitb` module\'s capabilities to demonstrate handling and logging of exceptions. - Make sure to handle both cases, where `logdir` is provided and where it is not. **Good Luck!**","solution":"import cgitb import os import sys def setup_logging_system(logdir: str = None, format: str = \'html\') -> None: Configures the cgitb module to handle uncaught exceptions. Logs detailed tracebacks to the specified directory if logdir is not None. Uses HTML format by default for the output of tracebacks unless a different format is specified. if logdir: if not os.path.exists(logdir): raise ValueError(f\\"Log directory {logdir} does not exist.\\") cgitb.enable(logdir=logdir, format=format) else: cgitb.enable(format=format) def custom_handler(info: tuple = None) -> None: Uses cgitb.handler(info) to handle exceptions using the provided info tuple. If no info is provided, it should handle the current exception. cgitb.handler(info)"},{"question":"**Objective:** To design a function that leverages the Python `random` module to perform a statistical task, ensuring an understanding of the module\'s various random number generation and distribution functions. **Problem Statement:** You are required to simulate the rounds of a game where each round involves rolling a six-sided die `n` times. For each roll, if the die lands on a 6, it contributes 1 point; otherwise, it contributes 0 points. After all `n` rolls, the total score for that round is calculated. This process repeats for `m` rounds. After `m` rounds, the function should return the mean score across all rounds. **Function Signature:** ```python def simulate_die_game(n: int, m: int) -> float: pass ``` **Input:** - `n` (int): The number of dice rolls per round. (1 <= n <= 10^6) - `m` (int): The number of rounds to simulate. (1 <= m <= 10^4) **Output:** - (float): The mean score across all `m` rounds, rounded to two decimal places. **Constraints:** - The solution must use the Python `random` module. - Generate integer values representing die rolls (using `randint`). - Calculate the mean score accurately by summing individual round scores and dividing by `m`. **Example:** ```python >> simulate_die_game(10, 1000) 0.83 ``` **Explanation:** In this example, we simulate 1000 rounds of the game where each round consists of 10 dice rolls. The function returns the mean score per round to two decimal places. **Advanced Task** (Optional): Extend the problem by providing an option to use different random number distribution for the die based on a weight. For instance, create a weighted die that lands on 6 more frequently. **Extended function signature:** ```python def simulate_die_game(n: int, m: int, weights: List[float] = None) -> float: pass ``` Here, `weights` (if provided) is a list of probabilities representing the likelihood of each side of the die. ```python >> simulate_die_game(10, 1000, [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]) 3.25 ``` **Important Notes:** - Ensure use of proper random functions and avoid bias unless weights are specified. - Maintain efficiency in handling large numbers of rounds and dice rolls.","solution":"import random def simulate_die_game(n: int, m: int) -> float: Simulates rolling a six-sided die n times for m rounds. Returns the mean score where each roll that lands on 6 adds 1 point. total_score = 0 for _ in range(m): round_score = sum(1 for _ in range(n) if random.randint(1, 6) == 6) total_score += round_score mean_score = total_score / m return round(mean_score, 2)"},{"question":"<|Analysis Begin|> The provided documentation covers how to use Python\'s `urllib` package to handle HTTP requests. It includes examples of fetching URLs, handling errors, setting timeouts, encoding data for GET and POST requests, passing custom headers, and dealing with authentication and proxies. The documentation also briefly discusses the structure of the response objects. Key Points: 1. **Fetching URLs**: This includes simple requests and storing responses temporarily. 2. **Handling Errors**: Differentiates between `URLError` and `HTTPError`. 3. **Request Objects**: Ability to customize requests using headers and encoding data. 4. **Authentication**: Handling basic HTTP authentication. 5. **Proxy Handling**: Custom handling of proxies. 6. **Timeouts**: Setting timeouts at the socket level. Given this information, we can design a coding assessment question that tests students\' abilities to: - Retrieve data from a URL. - Handle different types of errors. - Manipulate request headers and data. - Use basic authentication. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Implement a function that fetches data from a URL using the `urllib` package, handles possible HTTP errors, and correctly identifies and reports the type of error encountered. Additionally, the function should accommodate optional headers and basic authentication. **Question:** Implement the function `fetch_url` with the following specifications: ```python def fetch_url(url: str, headers: dict = None, auth: tuple = None, data: dict = None, timeout: int = 10) -> str: Fetches data from the given URL. Handles errors and supports optional headers, basic authentication, and POST data. Parameters: url (str): The URL to fetch. headers (dict, optional): A dictionary of HTTP headers to send with the request. auth (tuple, optional): A tuple of (username, password) for basic authentication. data (dict, optional): A dictionary of data to send in a POST request. timeout (int, optional): The timeout for the request in seconds. Default is 10 seconds. Returns: str: The content of the response if successful. Raises: HTTPError: When an HTTP error occurs (e.g., 404, 403). URLError: When a non-HTTP error occurs (e.g., cannot reach server). ValueError: If the URL is invalid or other misc error occurs. pass ``` # Function Details 1. **Parameters**: - `url`: The URL to fetch data from. - `headers` (optional): A dictionary of headers to send with the request. - `auth` (optional): A tuple containing the username and password for basic authentication. - `data` (optional): A dictionary of data to send in a POST request. If provided, the request should be a POST. - `timeout` (optional): The timeout for the request in seconds. Default is 10 seconds. 2. **Return**: - The function returns the content of the response as a string if the request is successful. 3. **Error Handling**: - If there is an HTTP error, raise an `HTTPError` with the appropriate status code and message. - If there is a URL error (e.g., cannot reach the server), raise a `URLError` with the appropriate reason. - Raise a `ValueError` if the URL is invalid or other unusual errors occur. 4. **Additional Requirements**: - If headers are provided, include them in the request. - If authentication details are provided, use them for basic authentication. - Set the socket timeout using the provided timeout value. **Example Usage**: ```python try: response = fetch_url(\\"http://www.example.com\\", headers={\\"User-Agent\\": \\"Mozilla/5.0\\"}, auth=(\\"username\\", \\"password\\"), timeout=5) print(response) except urllib.error.HTTPError as e: print(\\"HTTP Error:\\", e.code, e.reason) except urllib.error.URLError as e: print(\\"URL Error:\\", e.reason) except ValueError as e: print(\\"Value Error:\\", str(e)) ``` **Notes**: - This function should make use of the `urllib.request` and `urllib.error` modules. - Ensure to handle the encoding of the `data` dictionary for POST requests appropriately. - Use clear, concise, and Pythonic code practices.","solution":"import urllib.request, urllib.error import json from urllib.parse import urlencode def fetch_url(url: str, headers: dict = None, auth: tuple = None, data: dict = None, timeout: int = 10) -> str: Fetches data from the given URL. Handles errors and supports optional headers, basic authentication, and POST data. Parameters: url (str): The URL to fetch. headers (dict, optional): A dictionary of HTTP headers to send with the request. auth (tuple, optional): A tuple of (username, password) for basic authentication. data (dict, optional): A dictionary of data to send in a POST request. timeout (int, optional): The timeout for the request in seconds. Default is 10 seconds. Returns: str: The content of the response if successful. Raises: HTTPError: When an HTTP error occurs (e.g., 404, 403). URLError: When a non-HTTP error occurs (e.g., cannot reach server). ValueError: If the URL is invalid or other misc error occurs. try: # Assume it could be a POST request if data is provided if data is not None: data = urlencode(data).encode() req = urllib.request.Request(url, data=data) else: req = urllib.request.Request(url) # Add headers if provided if headers: for key, value in headers.items(): req.add_header(key, value) # Add basic authentication if provided if auth: auth_str = f\'{auth[0]}:{auth[1]}\' auth_encoded = urllib.request.base64.b64encode(auth_str.encode(\'utf-8\')).decode(\'utf-8\') req.add_header(\'Authorization\', f\'Basic {auth_encoded}\') # Make the request with urllib.request.urlopen(req, timeout=timeout) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: raise urllib.error.HTTPError(e.url, e.code, e.reason, e.headers, e.fp) except urllib.error.URLError as e: raise urllib.error.URLError(e.reason) except ValueError as e: raise ValueError(str(e))"},{"question":"# **Text Processing with Regular Expressions and Sequence Matching** Problem Statement You are tasked with writing a function that processes a given list of strings to find and categorize valid email addresses, domain counts, and partially matching sequences using advanced text processing techniques. Your solution should utilize regular expressions for email validation and the `difflib` module for sequence matching. Function Signature ```python def process_text_strings(input_list: List[str]) -> Dict[str, Any]: pass ``` Input - `input_list` (List[str]): A list of strings containing potential email addresses, random text, and other data. Output - (Dict[str, Any]): A dictionary containing: - `valid_emails`: A list of all valid email addresses found in `input_list` using regular expressions. - `domain_counts`: A dictionary with domain names as keys and their corresponding counts as values. - `partial_matches`: A list of tuples where each tuple contains two strings from `input_list` that have a similarity ratio of greater than 0.6 according to `difflib.SequenceMatcher`. Constraints - Ensure that only unique email addresses are counted. - Use the standard rules for validating an email, which include the presence of an \\"@\\" symbol and a domain name. - Assume the input list contains fewer than 10,000 strings, and each string can be up to 1,000 characters long. Example ```python input_list = [ \\"alice@example.com\\", \\"bob123@sample.net\\", \\"not.an.email\\", \\"Partial text match example\\", \\"Text match partial example\\", \\"test@my-domain.com\\", \\"Another email: test@my-domain.com\\" ] expected_output = { \'valid_emails\': [\'alice@example.com\', \'bob123@sample.net\', \'test@my-domain.com\'], \'domain_counts\': {\'example.com\': 1, \'sample.net\': 1, \'my-domain.com\': 1}, \'partial_matches\': [ (\'Partial text match example\', \'Text match partial example\') ] } assert process_text_strings(input_list) == expected_output ``` Hints 1. You can use the `re` module for extracting and validating email addresses. 2. To count domain occurrences, extract the domain part of each email and use a dictionary for counting. 3. Use `difflib.SequenceMatcher` to find pairs of strings that have a similarity ratio above a given threshold. Ensure your function\'s time complexity is optimized to handle the given constraints effectively.","solution":"from typing import List, Dict, Any import re from collections import defaultdict import difflib def process_text_strings(input_list: List[str]) -> Dict[str, Any]: # Regular expression for finding and validating email addresses email_regex = r\'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+.[a-zA-Z0-9-.]+\' valid_emails = set() domain_counts = defaultdict(int) for text in input_list: found_emails = re.findall(email_regex, text) for email in found_emails: if email not in valid_emails: valid_emails.add(email) domain = email.split(\'@\')[1] domain_counts[domain] += 1 valid_emails = list(valid_emails) # Finding partial matches partial_matches = [] for i in range(len(input_list)): for j in range(i + 1, len(input_list)): seq_matcher = difflib.SequenceMatcher(None, input_list[i], input_list[j]) if seq_matcher.ratio() > 0.6: partial_matches.append((input_list[i], input_list[j])) return { \'valid_emails\': valid_emails, \'domain_counts\': dict(domain_counts), \'partial_matches\': partial_matches }"},{"question":"# Task: Implement a Utility Script to Manage Temporary Directories and Environment Variables Objective Design a Python script that creates temporary directories, manages environment variables within these directories, and handles various file operations. Requirements 1. **Function 1: create_temp_dir** - **Input**: None - **Output**: Returns the path of the newly created temporary directory (string) - **Functionality**: Create a temporary directory in the operating system\'s temporary path. Ensure the directory is empty upon creation. 2. **Function 2: set_env_variable** - **Input**: A dictionary with environment variable names as keys and their corresponding values as values. - **Output**: None - **Functionality**: Set the environment variables provided in the dictionary for the current process. 3. **Function 3: write_to_file** - **Input**: - `dir_path` (string): Path to the directory where the file will be created. - `filename` (string): Name of the file to be created. - `content` (string): Content to write into the file. - **Output**: None - **Functionality**: Create a file with the given filename in the specified directory path and write the provided content into the file. 4. **Function 4: read_from_file** - **Input**: - `file_path` (string): Path to the file to be read. - **Output**: Returns the content of the file (string) - **Functionality**: Read and return the content of the specified file. 5. **Function 5: delete_directory** - **Input**: - `dir_path` (string): Path to the directory to delete. - **Output**: None - **Functionality**: Recursively delete the specified directory and its contents. 6. **Utility Function: run_in_subprocess** - **Input**: - `dir_path` (string): Path to the working directory for the subprocess. - `env_vars` (dictionary): Environment variables to set in the subprocess. - `command` (list): The command to execute in the subprocess. - **Output**: Returns the standard output of the command executed (string) - **Functionality**: Runs the given command in a subprocess with the specified environment variables and working directory. The standard output should be captured and returned. Constraints - Handle exceptions where applicable. - Ensure paths are valid and accessible. - Conform to the operating system and locale defaults. Sample Usage ```python temp_dir = create_temp_dir() print(f\\"Temporary Directory Created: {temp_dir}\\") set_env_variable({\'MY_VAR\': \'123\', \'PATH\': \'/usr/bin\'}) print(\\"Environment variables set!\\") write_to_file(temp_dir, \'example.txt\', \'Hello, World!\') print(\\"File written successfully!\\") content = read_from_file(os.path.join(temp_dir, \'example.txt\')) print(f\\"Read Content from File: {content}\\") subprocess_output = run_in_subprocess(temp_dir, {\'MY_VAR\': \'456\'}, [\'echo\', \'MY_VAR\']) print(f\\"Subprocess output: {subprocess_output}\\") delete_directory(temp_dir) print(\\"Temporary directory deleted successfully!\\") ``` **Note**: You must ensure the implementations adhere to robust error handling and cleanup operations.","solution":"import os import tempfile import shutil import subprocess def create_temp_dir(): Creates a temporary directory and returns its path. temp_dir = tempfile.mkdtemp() return temp_dir def set_env_variable(env_vars): Sets the environment variables for the current process. for key, value in env_vars.items(): os.environ[key] = value def write_to_file(dir_path, filename, content): Creates a file with the given filename in the specified directory path and writes the provided content into the file. file_path = os.path.join(dir_path, filename) with open(file_path, \'w\') as file: file.write(content) def read_from_file(file_path): Reads and returns the content of the specified file. with open(file_path, \'r\') as file: content = file.read() return content def delete_directory(dir_path): Recursively deletes the specified directory and its contents. shutil.rmtree(dir_path) def run_in_subprocess(dir_path, env_vars, command): Runs the given command in a subprocess with the specified environment variables and working directory. Captures and returns the standard output of the command executed. env = os.environ.copy() env.update(env_vars) # Use subprocess to execute the command result = subprocess.run(command, cwd=dir_path, env=env, capture_output=True, text=True) return result.stdout.strip()"},{"question":"**Title:** File Archiving and Compression Utility **Objective:** Design a Python program to create a utility that archives multiple text files into a single compressed file. **Problem Statement:** Write a function `archive_files` that takes two arguments: 1. `file_paths` (list of str): A list of file paths pointing to text files that need to be archived. 2. `output_path` (str): The path where the archived and compressed file will be stored. The function should read the content of each text file specified in `file_paths`, archive them into a single **ZIP** file, and save the ZIP file to the specified `output_path`. Each file in the ZIP archive should retain its original name. **Function Signature:** ```python def archive_files(file_paths: list, output_path: str) -> None: pass ``` # Constraints: - You must use the `zipfile` module to handle the creation of the ZIP archive. - Each file in the `file_paths` list is guaranteed to exist and be a readable text file. - The resulting ZIP file should be compressed using the `ZIP_DEFLATED` method. # Input/Output: - **Input:** - `file_paths`: A list of paths, e.g., `[\'/path/to/file1.txt\', \'/path/to/file2.txt\']` - `output_path`: A path, e.g., `\'/path/to/archive.zip\'` - **Output:** - A ZIP file saved at `output_path` containing all the files specified in `file_paths`. # Example: ```python file_paths = [\'file1.txt\', \'file2.txt\', \'file3.txt\'] output_path = \'archive.zip\' archive_files(file_paths, output_path) ``` - After calling the function, `archive.zip` should be created in the current directory containing `file1.txt`, `file2.txt`, and `file3.txt`, each compressed as `ZIP_DEFLATED`. # Additional Notes: - Ensure to handle exceptions such as file not found or read errors gracefully. - Write clean, readable, and well-documented code. # Hints: - Use the `zipfile.ZipFile` class for creating and writing to ZIP files. - Utilize the context manager (`with` statement) to ensure that files are properly closed after they are processed. **Reference:** - [zipfile module documentation](https://docs.python.org/3/library/zipfile.html)","solution":"import zipfile def archive_files(file_paths: list, output_path: str) -> None: Archives and compresses multiple text files into a single ZIP file. Args: file_paths (list of str): List of file paths to text files to be archived. output_path (str): The path where the resulting ZIP file will be stored. try: # Create a new zip file with zipfile.ZipFile(output_path, \'w\', zipfile.ZIP_DEFLATED) as zipf: # Iterate over every file in the list for file_path in file_paths: # Add the file to the ZIP archive zipf.write(file_path, arcname=file_path) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective Demonstrate your understanding of Seaborn\'s advanced plotting capabilities by creating a complex, aesthetically pleasing visualization. Problem Statement Using the provided dataset `diamonds` from Seaborn, create a multi-faceted area plot that visualizes the distribution of diamond prices over the variable `cut`. Your task is to perform the following steps: 1. Load the `diamonds` dataset from Seaborn. 2. Preprocess the data by removing any rows with missing values and ensuring the data is sorted appropriately. 3. Create facets for each category of the `cut` variable using the `facet` functionality, with three plots wrapping per row. 4. Create an area plot for each facet that visualizes the distribution of diamond prices over the `carat` values. 5. Customize the appearance of the plots by mapping appropriate color and edge aesthetics. 6. Stack the area plots to show part-whole relationships within each facet. Implementation Details - Use the `seaborn.objects` interface for creating the plots. - Customize the `color` and `edgecolor` based on the `cut` variable. - Ensure that your plots are clear and visually distinct, including axis labels and titles. Input and Output - **Input**: None. The function should directly load the dataset and generate the plot. - **Output**: A multi-faceted area plot displayed using Matplotlib\'s `show()` function. Constraints - Use only Seaborn and standard Python libraries for data manipulation and visualization. - Ensure the plots are wrapped appropriately with a maximum of three plots per row. Example Here is an outline of the steps you might take: ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_diamond_distributions(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Data preprocessing diamonds = diamonds.dropna().sort_values(by=[\'cut\', \'carat\']) # Create the Plot object with facets p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").facet(\\"cut\\", wrap=3) # Add the Area mark with customs colors and stacking p.add(so.Area(alpha=0.7), color=\\"cut\\", edgecolor=\\"cut\\", edgewidth=1, stack=True) # Display the plot plt.show() # Call the function to generate and display the plot plot_diamond_distributions() ``` Your solution should be functionally equivalent to the example but may vary in appearance and customization.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt def plot_diamond_distributions(): # Load the dataset diamonds = load_dataset(\\"diamonds\\") # Data preprocessing diamonds = diamonds.dropna().sort_values(by=[\'cut\', \'carat\']) # Create the Plot object with facets p = so.Plot(diamonds, x=\\"carat\\", y=\\"price\\").facet(\\"cut\\", wrap=3) # Add the Area mark with custom colors and stacking p.add(so.Area(alpha=0.7), color=\\"cut\\", edgecolor=\\"cut\\", edgewidth=1.5, stack=True) # Display the plot plt.show() # Call the function to generate and display the plot plot_diamond_distributions()"},{"question":"# Question: UUID Operations You are tasked with implementing a utility class `UUIDUtility` in Python, which will simplify several operations involving UUIDs. The class should be able to generate UUIDs using different methods and compare them. Requirements: 1. Implement a method to generate and return a UUID of a specified version: - `generate_uuid(version: int) -> str`: Generates and returns a UUID as a string. The `version` can be 1, 3, 4, or 5. For `version` 3 or 5, use a fixed namespace (`uuid.NAMESPACE_DNS`) and name (`\\"example.com\\"`). 2. Implement a method to convert a UUID from its string (hex) representation to a 128-bit integer: - `uuid_to_int(uuid_str: str) -> int`: Converts a UUID from its string representation (e.g., \\"12345678-1234-5678-1234-567812345678\\") to a 128-bit integer representation. 3. Implement a method to compare two UUIDs and return which one is greater based on their integer values: - `compare_uuids(uuid_str1: str, uuid_str2: str) -> str`: Compares two UUIDs provided as strings and returns: - `\\"uuid1 is greater\\"` if `uuid_str1` represents a greater UUID than `uuid_str2`. - `\\"uuid2 is greater\\"` if `uuid_str2` represents a greater UUID than `uuid_str1`. - `\\"Both UUIDs are equal\\"` if they are equal. 4. Implement a method to check if a given UUID is safe: - `is_safe_uuid(uuid_str: str) -> bool`: Checks if the provided UUID (from its string representation) was generated in a multiprocessing-safe way (using `uuid.UUID(uuid_str).is_safe`). Example usage: ```python # Example usage: utility = UUIDUtility() # Generate UUIDs uuid1 = utility.generate_uuid(1) uuid4 = utility.generate_uuid(4) print(uuid1) # Outputs something like: \'a8098c1a-f86e-11da-bd1a-00112444be1e\' print(uuid4) # Outputs something like: \'4b74e5f0-665b-43a0-bb5b-15d987b6aeb4\' # UUID to integer uuid_int = utility.uuid_to_int(uuid1) print(uuid_int) # Outputs a large integer # Compare UUIDs result = utility.compare_uuids(uuid1, uuid4) print(result) # Outputs one of the comparison outcomes # Check if UUID is safe is_safe = utility.is_safe_uuid(uuid1) print(is_safe) # Outputs True, False, or raises ValueError if uuid1 was not generated safely ``` Constraints: - Assume `version` is always 1, 3, 4, or 5. - Utilize the built-in `uuid` module for UUID handling. - Write clean, efficient, and well-documented code. Submission: Submit your code with correct implementation for the `UUIDUtility` class that includes: 1. `generate_uuid(version: int) -> str` 2. `uuid_to_int(uuid_str: str) -> int` 3. `compare_uuids(uuid_str1: str, uuid_str2: str) -> str` 4. `is_safe_uuid(uuid_str: str) -> bool`","solution":"import uuid class UUIDUtility: @staticmethod def generate_uuid(version: int) -> str: if version == 1: return str(uuid.uuid1()) elif version == 3: return str(uuid.uuid3(uuid.NAMESPACE_DNS, \\"example.com\\")) elif version == 4: return str(uuid.uuid4()) elif version == 5: return str(uuid.uuid5(uuid.NAMESPACE_DNS, \\"example.com\\")) else: raise ValueError(f\\"Unsupported UUID version: {version}\\") @staticmethod def uuid_to_int(uuid_str: str) -> int: return int(uuid.UUID(uuid_str)) @staticmethod def compare_uuids(uuid_str1: str, uuid_str2: str) -> str: int1 = UUIDUtility.uuid_to_int(uuid_str1) int2 = UUIDUtility.uuid_to_int(uuid_str2) if int1 > int2: return \\"uuid1 is greater\\" elif int2 > int1: return \\"uuid2 is greater\\" else: return \\"Both UUIDs are equal\\" @staticmethod def is_safe_uuid(uuid_str: str) -> bool: return uuid.UUID(uuid_str).is_safe == uuid.SafeUUID.safe"},{"question":"Implement a Priority Queue using the `heapq` module such that tasks can be added with priorities, priorities can be updated, and tasks can be removed. Your implementation should maintain the heap invariant and manage tasks efficiently. # Detailed Requirements: 1. **Initialization**: Initialize the queue with an empty list along with other structures necessary to track tasks. 2. **Add Task**: Implement the `add_task(task, priority)` method that adds a task with a given priority. If the task already exists, update its priority. 3. **Remove Task**: Implement the `remove_task(task)` method that marks a task as removed without disrupting the heap structure. 4. **Pop Task**: Implement the `pop_task()` method that removes and returns the task with the smallest priority. If the queue is empty, raise a `KeyError`. 5. **Constraints**: - Task priorities are integer values. - Task names (or descriptions) are unique strings. 6. **Performance**: Ensure that the major operations (`add_task`, `remove_task`, `pop_task`) perform efficiently, ideally in logarithmic time relative to the number of tasks. # Example Usage: ```python pq = PriorityQueue() # Adding tasks pq.add_task(\'task1\', priority=3) pq.add_task(\'task2\', priority=2) pq.add_task(\'task3\', priority=5) # Updating task priority pq.add_task(\'task1\', priority=1) # Removing a task pq.remove_task(\'task2\') # Popping tasks print(pq.pop_task()) # Outputs: \'task1\' print(pq.pop_task()) # Outputs: \'task3\' # Trying to pop from an empty queue try: pq.pop_task() except KeyError: print(\\"Queue is empty\\") ``` # Implementation Notes: - Use the `heapq` module for maintaining the heap structure. - Utilize a dictionary or similar data structure to keep track of tasks and their places within the heap. - Handle removals by marking tasks as removed and ensure they don’t interfere with popping tasks. Implement the `PriorityQueue` class as described. ```python import heapq import itertools class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task, priority=0): \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task): \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') ``` # Notes: - Ensure to test your implementation thoroughly for various edge cases including simultaneous additions and removals, and popping from an empty queue. - Your solution should efficiently handle large datasets and maintain a proper log of operations as per the heap invariant.","solution":"import heapq import itertools class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task, priority=0): \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) def remove_task(self, task): \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# PyTorch Coding Assessment **Objective:** You will implement a function using PyTorch that creates and manipulates tensors, applies specific operations to them, and utilizes GPU if available. This task will test your understanding of tensor creation, manipulation, mathematical operations, and device handling in PyTorch. **Problem Statement:** Implement the following function: ```python import torch def tensor_operations(): This function should create specific tensors and perform a series of operations on them. The steps are as follows: 1. Create a tensor `A` of size (3, 3) with values from 1 to 9, inclusive, and dtype `torch.int32`. 2. Create a tensor `B` of the same size as `A`, filled with the value 2.0 and dtype `torch.float32`. 3. Perform element-wise multiplication of `A` and `B` to create tensor `C`. 4. Create tensor `D` that is the result of an element-wise exponential (base e) of `C`. 5. Create tensor `E` that sums all the elements of tensor `D`. 6. Ensure these computations use GPU if available; if not, use CPU. 7. Return tensors `A`, `B`, `C`, `D`, and the value `E`. # Your code here pass ``` **Expected Output:** The function should return five elements: 1. `A` - Tensor of size (3, 3) with elements from 1 to 9 with dtype `torch.int32`. 2. `B` - Tensor of size (3, 3) filled with 2.0 with dtype `torch.float32`. 3. `C` - Tensor resulting from element-wise multiplication of `A` and `B`. 4. `D` - Tensor resulting from applying the exponential function to each element of `C`. 5. `E` - A scalar tensor or Python number representing the sum of all elements in `D`. **Constraints and Limitations:** - You should utilize PyTorch functionalities where applicable. - The solution should handle the case where CUDA is not available and fall back to CPU execution. **Sample Code Execution:** ```python A, B, C, D, E = tensor_operations() print(A) # Expected: tensor([[1, 2, 3], # [4, 5, 6], # [7, 8, 9]], dtype=torch.int32) print(B) # Expected: tensor([[2., 2., 2.], # [2., 2., 2.], # [2., 2., 2.]], dtype=torch.float32) print(C) # Expected: tensor([[ 2., 4., 6.], # [ 8., 10., 12.], # [14., 16., 18.]], dtype=torch.float32) print(D) # Expected: tensor([[7.3891e+00, 5.4598e+01, 4.0343e+02], # [2.9808e+03, 2.2026e+04, 1.6275e+05], # [1.2026e+06, 8.8861e+06, 6.5659e+07]], dtype=torch.float32) print(E) # Expected: tensor(7.8108e+07) or a similar large float number ``` **Instructions:** 1. Implement the function `tensor_operations`. 2. Ensure your code handles both CPU and GPU scenarios. 3. Verify your implementation with the expected output provided.","solution":"import torch def tensor_operations(): Implements tensor operations including creation, manipulation, and device handling using PyTorch. Returns: - A: Tensor of size (3, 3) with values from 1 to 9, dtype torch.int32. - B: Tensor of size (3, 3) with all values as 2.0, dtype torch.float32. - C: Tensor resulting from element-wise multiplication of A and B. - D: Tensor resulting from applying the exponential function to each element of C. - E: Sum of all elements in tensor D. # Determine the device to use (GPU if available, otherwise CPU) device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") # Step 1: Create tensor A A = torch.arange(1, 10, dtype=torch.int32).reshape(3, 3).to(device) # Step 2: Create tensor B B = torch.full((3, 3), 2.0, dtype=torch.float32).to(device) # Step 3: Element-wise multiplication of A and B to create tensor C # Converting A to float32 for multiplication with B C = (A.float() * B).to(device) # Step 4: Create tensor D using exponential function on tensor C D = torch.exp(C) # Step 5: Create tensor E, which is the sum of all elements in tensor D E = D.sum() return A, B, C, D, E"},{"question":"**Question: Implement a Custom Autograd Function and Compute Jacobians** In this assignment, you will implement a custom PyTorch autograd `Function` for computing the forward and backward passes of the function (f(x) = sin(x) + x^2). You will also use PyTorch\'s higher-level functional API to compute the Jacobian of this function. # Part 1: Implement Custom Autograd `Function` 1. Create a custom autograd `Function` named `MyFunction` that implements the forward and backward methods for (f(x) = sin(x) + x^2). 2. The `forward` method should compute (f(x)), and the `backward` method should compute the gradient of (f(x)) with respect to its input (x). # Part 2: Compute Jacobian using Functional API 3. Write a Python function `compute_jacobian` that takes a tensor `x` as input and uses `torch.autograd.functional.jacobian` to compute the Jacobian matrix of (f(x)) defined using `MyFunction`. # Constraints and Requirements - You must use PyTorch (imported as `import torch`). - Do not use any other libraries for autograd operations. - Your implementation should be efficient, leveraging the capabilities of PyTorch\'s autograd and functional API. # Expected Input and Output - Input: A tensor `x` (1D or 2D) with `requires_grad=True`. - Output: The Jacobian matrix of the function (f(x)). # Sample Code ```python import torch from torch.autograd import Function from torch.autograd import functional class MyFunction(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return torch.sin(x) + x ** 2 @staticmethod def backward(ctx, grad_output): (x,) = ctx.saved_tensors grad_x = torch.cos(x) + 2 * x return grad_output * grad_x def compute_jacobian(x): func = lambda inp: MyFunction.apply(inp) jacobian = functional.jacobian(func, x) return jacobian # Example usage: x = torch.tensor([1.0, 2.0], requires_grad=True) jac = compute_jacobian(x) print(\\"Jacobian:\\", jac) ``` In this assignment, your understanding of custom autograd `Function` and the functional API for higher-level gradient computations will be assessed. Ensure that the custom `Function` correctly implements the forward and backward passes and that you effectively utilize `torch.autograd.functional.jacobian` to compute the Jacobian matrix.","solution":"import torch from torch.autograd import Function from torch.autograd import functional class MyFunction(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return torch.sin(x) + x ** 2 @staticmethod def backward(ctx, grad_output): (x,) = ctx.saved_tensors grad_x = torch.cos(x) + 2 * x return grad_output * grad_x def compute_jacobian(x): func = lambda inp: MyFunction.apply(inp) jacobian = functional.jacobian(func, x) return jacobian # Example usage: x = torch.tensor([1.0, 2.0], requires_grad=True) jac = compute_jacobian(x) print(\\"Jacobian:\\", jac)"},{"question":"**Title:** Porting a Python 2 Script to Python 3 - Automated and Manual Adjustments **Objective:** Your task is to port a given Python 2 script to be compatible with both Python 2.7 and Python 3. You will need to use automated tools and make manual adjustments based on the differences outlined in the guide. **Problem Statement:** You are provided with a Legacy Python 2 script. Your task is to modify the script to ensure it runs flawlessly on both Python 2.7 and Python 3.x. The script involves reading a file, processing text and binary data, performing calculations, and handling imports. Here is the Python 2 script: ```python # Python 2 code import sys from urllib import urlencode import urllib2 import string def process_file(filename): with open(filename, \'r\') as f: data = f.read() return data def calculate(a, b): return a / b def stringify(data): return unicode(data) # Main execution block if __name__ == \\"__main__\\": file_content = process_file(\'example.txt\') result = calculate(5, 2) print stringify(result) query = {\'key1\': \'value1\', \'key2\': \'value2\'} encoded_query = urlencode(query) response = urllib2.urlopen(\'http://example.com?\' + encoded_query) print response.read() ``` **Requirements:** 1. Port the above script using appropriate changes as per the guidelines provided. 2. Ensure compatibility with both Python 2.7 and Python 3.x. 3. Pay attention to: - Division operations. - Text vs. binary data handling. - Proper handling of string literals and encoding. - Correct imports for URL handling. 4. You may use suitable tools (Futurize, Modernize) for automated changes and make necessary manual adjustments. **Constraints:** - Do not utilize external libraries that are not mentioned in the original document. - The script should continue to perform the same functionalities and provide the same outputs as intended initially. **Additional Information:** - You can install required tools using: `python -m pip install future six` - Use of `io.open()` is preferred for file operations to ensure cross-version compatibility. **Expected Output:** - Ensure the script prints the expected results under both Python 2.7 and Python 3.x environments without any errors. **Solution Skeleton:** ```python # Cross-compatible Python 2/3 code from __future__ import absolute_import, division, print_function, unicode_literals import sys import io import six from six.moves.urllib.parse import urlencode from six.moves.urllib.request import urlopen def process_file(filename): with io.open(filename, \'r\', encoding=\'utf-8\') as f: data = f.read() return data def calculate(a, b): return a / b def stringify(data): return six.text_type(data) # Main execution block if __name__ == \\"__main__\\": file_content = process_file(\'example.txt\') result = calculate(5, 2) print(stringify(result)) query = {\'key1\': \'value1\', \'key2\': \'value2\'} encoded_query = urlencode(query) response = urlopen(\'http://example.com?\' + encoded_query) print(response.read()) ``` By solving this problem, students will understand the key differences between Python 2 and Python 3 and will be able to port scripts from Python 2 to Python 3, ensuring compatibility with both versions.","solution":"from __future__ import absolute_import, division, print_function, unicode_literals import sys import io import six from six.moves.urllib.parse import urlencode from six.moves.urllib.request import urlopen def process_file(filename): with io.open(filename, \'r\', encoding=\'utf-8\') as f: data = f.read() return data def calculate(a, b): return a / b def stringify(data): return six.text_type(data) # Main execution block if __name__ == \\"__main__\\": file_content = process_file(\'example.txt\') result = calculate(5, 2) print(stringify(result)) query = {\'key1\': \'value1\', \'key2\': \'value2\'} encoded_query = urlencode(query) response = urlopen(\'http://example.com?\' + encoded_query) print(response.read())"},{"question":"Objective Write a function in PyTorch that performs specific tensor operations which involve creating, type casting, and broadcasting tensors. The function should demonstrate your understanding of PyTorch\'s core operations and handling fundamental IR concepts. Problem Statement Given two 2D tensors `A` and `B`, implement a function `tensor_operations(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor` that performs the following steps: 1. Converts both tensors `A` and `B` to a common floating-point type suitable for element-wise operations. 2. Broadcasts `A` and `B` to compatible shapes for element-wise addition. 3. Adds the tensors together and returns the result. 4. Ensures the resulting tensor has the same datatype as the input tensors after the addition operation. Constraints - `A` and `B` are 2D tensors, potentially of different shapes, but their shapes must be broadcast-compatible. - Both `A` and `B` can be of any numerical type supported by PyTorch. Input - `A: torch.Tensor`: A 2D tensor of any numerical type. - `B: torch.Tensor`: Another 2D tensor of any numerical type. Output - `torch.Tensor`: A tensor resulting from the element-wise addition of `A` and `B` after converting and broadcasting them to a common type. Example ```python import torch A = torch.tensor([[1, 2], [3, 4]], dtype=torch.int32) B = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32) result = tensor_operations(A, B) print(result) # Expected output: tensor([[2.0, 4.0], [6.0, 8.0]]) ``` Additional Requirements: - Use PyTorch functionalities to handle type conversion and broadcasting. - Ensure the resulting tensor\'s type matches the input tensors after addition. How to Test: You can test the function with different shapes and types of tensors, ensuring that broadcasting rules of PyTorch are adhered to and that the result maintains the correct datatype.","solution":"import torch def tensor_operations(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: # Convert both tensors to a common floating-point type for element-wise operations dtype = torch.promote_types(A.dtype, B.dtype) A = A.to(dtype) B = B.to(dtype) # Perform element-wise addition with broadcasting result = A + B # Return the resulting tensor return result"},{"question":"# Asyncio Currency Conversion Tasks You are required to build a small asyncio program that concurrently fetches and processes currency conversion rates from multiple sources. Problem Statement Implement a function `fetch_conversion_rate(source: str) -> float` which simulates fetching a conversion rate from different sources by using `asyncio.sleep` and returns a mock conversion rate value. Each source takes different times to respond. Then, implement a function `get_average_conversion_rate(sources: List[str]) -> float` that: 1. Concurrently fetches conversion rates for a given list of sources. 2. Uses `asyncio.gather` to manage concurrency. 3. Returns the average conversion rate of all sources. You must handle any potential `asyncio.TimeoutError` if any source takes more than 3 seconds to respond and exclude such sources from the averaging. Input - A list of source names, e.g., `[\\"source_1\\", \\"source_2\\", \\"source_3\\"]`. Output - A floating-point number representing the average conversion rate. Constraints - Each source returns a mock conversion rate (float) between 0.8 and 1.2. - If a source times out (takes more than 3 seconds), it should be excluded from the average calculation. - At least one source must return a valid conversion rate; otherwise, raise an `Exception`. Function Specifications 1. `fetch_conversion_rate(source: str) -> float`: This function should randomly decide the duration of `asyncio.sleep` between 0.1 to 5.0 seconds to simulate network delay and then return a mock conversion rate. 2. `get_average_conversion_rate(sources: List[str]) -> float`: This function should: - Use `asyncio.gather` with a timeout to fetch conversion rates concurrently. - Calculate and return the average of the fetched rates. - Handle `asyncio.TimeoutError` for sources that take too long to respond. Example ```python import asyncio import random from typing import List async def fetch_conversion_rate(source: str) -> float: # Simulate variable network delay delay = random.uniform(0.1, 5.0) await asyncio.sleep(delay) # Mock conversion rate between 0.8 and 1.2 return random.uniform(0.8, 1.2) async def get_average_conversion_rate(sources: List[str]) -> float: tasks = [asyncio.wait_for(fetch_conversion_rate(source), timeout=3.0) for source in sources] results = await asyncio.gather(*tasks, return_exceptions=True) valid_rates = [rate for rate in results if not isinstance(rate, Exception)] if not valid_rates: raise Exception(\\"All sources timed out\\") return sum(valid_rates) / len(valid_rates) # Test values sources = [\\"source_1\\", \\"source_2\\", \\"source_3\\", \\"source_4\\"] average_rate = asyncio.run(get_average_conversion_rate(sources)) print(f\\"Average Conversion Rate: {average_rate}\\") ``` **Note:** The provided example should give you a sense of the expected solution\'s structure. Implement and test the functions accordingly to ensure they meet the requirements.","solution":"import asyncio import random from typing import List async def fetch_conversion_rate(source: str) -> float: Simulates fetching a conversion rate from a given source. Args: - source: str: The name of the source. Returns: - float: A mock conversion rate between 0.8 and 1.2. # Simulate variable network delay delay = random.uniform(0.1, 5.0) await asyncio.sleep(delay) # Mock conversion rate between 0.8 and 1.2 return random.uniform(0.8, 1.2) async def get_average_conversion_rate(sources: List[str]) -> float: Concurrently fetches conversion rates from multiple sources and returns the average rate. Args: - sources: List[str]: A list of source names. Returns: - float: The average conversion rate. Raises: - Exception: If all sources time out. tasks = [asyncio.wait_for(fetch_conversion_rate(source), timeout=3.0) for source in sources] results = await asyncio.gather(*tasks, return_exceptions=True) valid_rates = [rate for rate in results if not isinstance(rate, Exception)] if not valid_rates: raise Exception(\\"All sources timed out\\") return sum(valid_rates) / len(valid_rates)"},{"question":"# Data Visualization with Seaborn Objective You are tasked with creating a plot that visualizes a modified version of the brain network dataset. This exercise will test your ability in data manipulation and visualization using Seaborn\'s advanced features. Instructions 1. **Data Preparation**: - Load the dataset `brain_networks` using `seaborn.load_dataset`. The dataset has multiple header rows and the index set to the first column. - Rename the index column to `timepoint`. - Transform the dataset by stacking certain levels, grouping, computing the mean, unstacking, and filtering based on a condition similar to the provided example. 2. **Plot Creation**: - Utilize `seaborn.objects.Plot` to create a pairwise comparison of specific variables. You should plot the variables `[\'5\', \'8\', \'12\', \'15\']` on the x-axis against the variables `[\'6\', \'13\', \'16\']` on the y-axis. - Ensure all subplots share the same x and y axes. - Control the layout size of the plot. - Add paths to the plot to visualize trajectories, with lines having specific aesthetic properties (e.g., `linewidth=1`, `alpha=0.8`), and coloring lines based on the `hemi` column. 3. **Function Signature**: - Write a function `create_brain_network_plot` that: - Takes no input. - Outputs the final plot object. Example Function Signature ```python import seaborn.objects as so from seaborn import load_dataset def create_brain_network_plot(): # Load and process the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=0.8), color=\\"hemi\\") return p # Generate the plot plot = create_brain_network_plot() plot.show() ``` Constraints: - Do not use any external libraries other than seaborn, pandas (for data manipulation), and matplotlib (for showing plots). Evaluation Criteria: - **Correctness**: The function should correctly process the dataset and generate the plot as specified. - **Code Quality**: Code should be well-organized and readable, with appropriate use of seaborn functionalities. - **Visualization**: The plot should be correctly generated with the specified layout, axis sharing, and aesthetic properties.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_brain_network_plot(): # Load and process the dataset networks = ( load_dataset(\\"brain_networks\\", header=[0, 1, 2], index_col=0) .rename_axis(\\"timepoint\\") .stack([0, 1, 2]) .groupby([\\"timepoint\\", \\"network\\", \\"hemi\\"]) .mean() .unstack(\\"network\\") .reset_index() .query(\\"timepoint < 100\\") ) # Create the plot p = ( so.Plot(networks) .pair(x=[\\"5\\", \\"8\\", \\"12\\", \\"15\\"], y=[\\"6\\", \\"13\\", \\"16\\"]) .layout(size=(8, 5)) .share(x=True, y=True) ) p.add(so.Paths(linewidth=1, alpha=0.8), color=\\"hemi\\") return p # Generate the plot plot = create_brain_network_plot() # Display the plot - plt.show() is typically used, not included as per current task guidelines. plot.show()"},{"question":"**Question:** You are given a dataset with the following columns: - `Date`: Date in the format `YYYY-MM-DD`. - `Category`: Categorical values representing different groups. - `Value`: Numeric values. Your task is to create a visualization using Seaborn that captures the following elements: 1. A line plot showing the trend of `Value` over `Date` for each `Category`. 2. Configure different properties of the plot: - Use different colors for each `Category`. - Apply transparency (`alpha`) for better visibility of overlapping data. - Set custom markers for each point in the line plot. - Add labels to each line with the `Category` name. - Customize the axis labels and title. - Apply scales that suit the data representation. - Ensure the plots are visually distinct with appropriate sizes and edge widths. # Function Signature ```python def create_custom_seaborn_plot(data: pd.DataFrame) -> None: pass ``` # Input - `data`: A pandas DataFrame with columns `Date` (in `YYYY-MM-DD` format), `Category` (categorical), and `Value` (numeric). # Output - The output should be a visually distinct Seaborn plot displayed using `plt.show()`. # Constraints - Each `Category` should have a unique color and marker. - Appropriate transparency should be applied to the lines. - The plot should have a descriptive title and axis labels. - Use `so.Plot` from `seaborn.objects` to create the plot. # Example ```python import pandas as pd import numpy as np # Example data data = pd.DataFrame({ \'Date\': pd.date_range(start=\'2023-01-01\', periods=100, freq=\'D\'), \'Category\': np.random.choice([\'A\', \'B\', \'C\'], 100), \'Value\': np.random.randn(100).cumsum() }) create_custom_seaborn_plot(data) ``` The function `create_custom_seaborn_plot` should generate a plot with: - Trends over time for each category. - Custom colors, transparency, and markers for each category. - Proper labels and title for the plot.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_seaborn_plot(data: pd.DataFrame) -> None: This function takes a DataFrame and creates a custom Seaborn line plot. Parameters: data (pd.DataFrame): The input dataset with \'Date\', \'Category\', and \'Value\' columns. # Convert \'Date\' column to datetime data[\'Date\'] = pd.to_datetime(data[\'Date\']) # Set the style of the plot sns.set(style=\\"whitegrid\\") # Create the line plot with customization plt.figure(figsize=(12, 6)) category_palette = sns.color_palette(\\"husl\\", data[\'Category\'].nunique()) markers = [\'o\', \'s\', \'D\', \'^\', \'v\', \'<\', \'>\', \'p\', \'*\', \'X\'] plot = sns.lineplot( x=\'Date\', y=\'Value\', hue=\'Category\', data=data, palette=category_palette, style=\'Category\', markers=markers[:data[\'Category\'].nunique()], dashes=False, alpha=0.7, linewidth=2.5 ) # Adding labels and title plt.xlabel(\'Date\') plt.ylabel(\'Value\') plt.title(\'Trend of Value over Date by Category\') # Add text labels for each category categories = data[\'Category\'].unique() for category in categories: category_data = data[data[\'Category\'] == category] plt.text(category_data[\'Date\'].iloc[-1], category_data[\'Value\'].iloc[-1], category, horizontalalignment=\'left\') # Show the plot plt.show()"},{"question":"# Question: String and List Manipulation with Basic Arithmetic Operations Objective: Develop a Python function that takes a list of strings and returns a string consisting of the first letter of each string, followed by the sum of the lengths of all the strings. Details: 1. **Function Signature:** `def process_strings(strings: list) -> str:` 2. **Input:** - A list of strings `strings` where 1 <= len(strings) <= 100 and each string has a length between 1 and 50. 3. **Output:** - A single string, which is constructed by concatenating: 1. The first letter of each string in the list. 2. The total length of all the strings in the input list, converted to a string. 4. **Constraints:** - The input list will contain only non-empty strings composed of alphabetic characters. Example: ```python # Example Input strings = [\\"hello\\", \\"world\\", \\"python\\"] # Example Output \\"hwpe16\\" # Explanation: # - First letters: \'h\', \'w\', \'p\' # - Total length of strings: 5 + 5 + 6 = 16 # - Concatenating: \'h\' + \'w\' + \'p\' + \'e\' + \'16\' = \'hwpe16\' ``` Implementation: Implement the function `process_strings` which processes the list according to the requirements. ```python def process_strings(strings: list) -> str: # Combine the first letters of each string first_letters = \'\'.join(string[0] for string in strings) # Calculate the total length of all strings total_length = sum(len(string) for string in strings) # Create the resulting string result = first_letters + str(total_length) return result ``` Write unit tests to verify your function works for various edge cases, such as single string input, varying string lengths, and different character combinations. Note: - Pay attention to efficiently handling string concatenation and summation to ensure optimal performance even for the upper limit of input sizes.","solution":"def process_strings(strings: list) -> str: Processes the list of strings to return a single string made up of the first letter of each string followed by the sum of the lengths of all the strings. # Combine the first letters of each string first_letters = \'\'.join(s[0] for s in strings) # Calculate the total length of all strings total_length = sum(len(s) for s in strings) # Create the resulting string result = first_letters + str(total_length) return result"},{"question":"**Question: Implement and Compare Linear and Ridge Regression Models** **Problem Statement:** You are given a dataset containing several features and a target variable. Your task is to implement both a Linear Regression model and a Ridge Regression model using the scikit-learn library. You will then compare their performance based on Mean Squared Error (MSE) and R-squared metrics. **Steps:** 1. Load the dataset (you can use any available dataset or generate your own using `make_regression` from scikit-learn). 2. Split the dataset into training and testing sets. 3. Implement a Linear Regression model and a Ridge Regression model. 4. Train both models on the training set. 5. Evaluate both models on the testing set using the Mean Squared Error and R-squared metrics. 6. Visualize the performance of both models using appropriate plots. **Input Format:** - The dataset should be a CSV file with the last column as the target variable and the rest as features. - You can assume that the dataset is clean and does not contain any missing values. **Output Format:** - Print the coefficients and intercepts of both models. - Print the Mean Squared Error and R-squared metrics for both models. - Display plots comparing the true vs. predicted values for both models. **Constraints:** - Use a 70-30 split for training and testing sets. - For the Ridge Regression model, use an alpha value of 1.0. - Ensure reproducibility by setting a random seed where necessary. **Example Implementation:** ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt # Load the dataset # Replace \'your_dataset.csv\' with the path to your dataset data = pd.read_csv(\'your_dataset.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Implement Linear Regression linear_reg = LinearRegression() linear_reg.fit(X_train, y_train) y_pred_linear = linear_reg.predict(X_test) # Implement Ridge Regression ridge_reg = Ridge(alpha=1.0) ridge_reg.fit(X_train, y_train) y_pred_ridge = ridge_reg.predict(X_test) # Evaluate both models mse_linear = mean_squared_error(y_test, y_pred_linear) r2_linear = r2_score(y_test, y_pred_linear) mse_ridge = mean_squared_error(y_test, y_pred_ridge) r2_ridge = r2_score(y_test, y_pred_ridge) # Print the results print(\\"Linear Regression Coefficients:\\", linear_reg.coef_) print(\\"Linear Regression Intercept:\\", linear_reg.intercept_) print(\\"Ridge Regression Coefficients:\\", ridge_reg.coef_) print(\\"Ridge Regression Intercept:\\", ridge_reg.intercept_) print(\\"Linear Regression MSE:\\", mse_linear) print(\\"Linear Regression R2:\\", r2_linear) print(\\"Ridge Regression MSE:\\", mse_ridge) print(\\"Ridge Regression R2:\\", r2_ridge) # Plot the results plt.figure(figsize=(14, 6)) plt.subplot(1, 2, 1) plt.scatter(y_test, y_pred_linear, color=\'blue\', alpha=0.5) plt.plot([y.min(), y.max()], [y.min(), y.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'Linear Regression\') plt.subplot(1, 2, 2) plt.scatter(y_test, y_pred_ridge, color=\'red\', alpha=0.5) plt.plot([y.min(), y.max()], [y.min(), y.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'Ridge Regression\') plt.show() ``` **Note:** This is an example implementation. You should adapt the code to the specific dataset you are working with.","solution":"import numpy as np import pandas as pd from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression, Ridge from sklearn.metrics import mean_squared_error, r2_score import matplotlib.pyplot as plt # Generate a dataset using make_regression # (Uncomment the below lines if you don\'t have a dataset and want to generate one) X, y = make_regression(n_samples=200, n_features=1, noise=10, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Implement Linear Regression linear_reg = LinearRegression() linear_reg.fit(X_train, y_train) y_pred_linear = linear_reg.predict(X_test) # Implement Ridge Regression ridge_reg = Ridge(alpha=1.0) ridge_reg.fit(X_train, y_train) y_pred_ridge = ridge_reg.predict(X_test) # Evaluate both models mse_linear = mean_squared_error(y_test, y_pred_linear) r2_linear = r2_score(y_test, y_pred_linear) mse_ridge = mean_squared_error(y_test, y_pred_ridge) r2_ridge = r2_score(y_test, y_pred_ridge) # Print the results print(\\"Linear Regression Coefficients:\\", linear_reg.coef_) print(\\"Linear Regression Intercept:\\", linear_reg.intercept_) print(\\"Ridge Regression Coefficients:\\", ridge_reg.coef_) print(\\"Ridge Regression Intercept:\\", ridge_reg.intercept_) print(\\"Linear Regression MSE:\\", mse_linear) print(\\"Linear Regression R2:\\", r2_linear) print(\\"Ridge Regression MSE:\\", mse_ridge) print(\\"Ridge Regression R2:\\", r2_ridge) # Plot the results plt.figure(figsize=(14, 6)) plt.subplot(1, 2, 1) plt.scatter(y_test, y_pred_linear, color=\'blue\', alpha=0.5) plt.plot([y.min(), y.max()], [y.min(), y.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'Linear Regression\') plt.subplot(1, 2, 2) plt.scatter(y_test, y_pred_ridge, color=\'red\', alpha=0.5) plt.plot([y.min(), y.max()], [y.min(), y.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predicted Values\') plt.title(\'Ridge Regression\') plt.show()"},{"question":"# Advanced Data Manipulation and Analysis with Pandas **Objective**: Evaluate the student\'s ability to manipulate, analyze, and visualize data using pandas. Problem Statement You are given a CSV file named `sales_data.csv` containing sales records for a retail company. The CSV file has the following columns: - `Date`: The date of the sales transaction (format: YYYY-MM-DD) - `Store`: Store identifier - `Product`: Product name - `UnitPrice`: Price per unit - `UnitsSold`: Number of units sold - `TotalRevenue`: Total revenue for the transaction (calculated as `UnitPrice * UnitsSold`) Your task is to perform the following operations using pandas: 1. **Reading the Data**: - Load the data from `sales_data.csv` into a pandas DataFrame. 2. **Data Cleaning**: - Identify and handle any missing data in the `TotalRevenue` column. If `TotalRevenue` is missing, calculate it as `UnitPrice * UnitsSold`. 3. **Data Transformation**: - Convert the `Date` column to datetime format. - Add a new column `Month` that extracts the month from the `Date` column. 4. **Analysis**: - Find the total number of units sold and total revenue generated by each store. - Identify the top 5 products with the highest total revenue. 5. **Group By Operations**: - Group the data by `Store` and `Month`, and calculate the total units sold and total revenue for each group. - Create a pivot table showing the total revenue for each store for each month. 6. **Visualization**: - Plot the total revenue for each store over time (by month) using matplotlib. - Create a bar chart showing the top 5 products by total revenue. Constraints - You may assume that the `sales_data.csv` file does not contain any duplicate rows. Expected Input and Output Formats **Input**: - The CSV file `sales_data.csv`. **Output**: - Print the total number of units sold and total revenue generated by each store. - Print the top 5 products with the highest total revenue. - Print the pivot table showing total revenue for each store for each month. - Display the plots as described in the visualization tasks. Example Code Template ```python import pandas as pd import matplotlib.pyplot as plt # Step 1: Reading the Data data = pd.read_csv(\\"sales_data.csv\\") # Step 2: Data Cleaning # Handle missing data in TotalRevenue # Step 3: Data Transformation # Convert Date to datetime format and add Month column # Step 4: Analysis # Total units sold and revenue by each store # Top 5 products by total revenue # Step 5: Group By Operations # Group by Store and Month, calculate total units sold and revenue # Create pivot table # Step 6: Visualization # Plot total revenue for each store over time # Bar chart for top 5 products by revenue plt.show() ``` Implement the functions and complete the code template to solve the problem.","solution":"import pandas as pd import matplotlib.pyplot as plt import numpy as np def load_data(filepath): Load the data from the CSV file. data = pd.read_csv(filepath) return data def clean_data(df): Handle missing TotalRevenue values by calculating them from UnitPrice and UnitsSold. df[\'TotalRevenue\'].fillna(df[\'UnitPrice\'] * df[\'UnitsSold\'], inplace=True) return df def transform_data(df): Convert Date to datetime and add a Month column. df[\'Date\'] = pd.to_datetime(df[\'Date\']) df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') return df def perform_analysis(df): Perform the required analysis. total_units_by_store = df.groupby(\'Store\')[\'UnitsSold\'].sum() total_revenue_by_store = df.groupby(\'Store\')[\'TotalRevenue\'].sum() top5_products = df.groupby(\'Product\')[\'TotalRevenue\'].sum().nlargest(5) return total_units_by_store, total_revenue_by_store, top5_products def group_and_pivot_data(df): Group by Store and Month, calculate total units sold and revenue. Create pivot table for total revenue by store and month. grouped = df.groupby([\'Store\', \'Month\']).agg(total_units=(\'UnitsSold\', \'sum\'), total_revenue=(\'TotalRevenue\', \'sum\')).reset_index() pivot_table = df.pivot_table(values=\'TotalRevenue\', index=\'Month\', columns=\'Store\', aggfunc=np.sum, fill_value=0) return grouped, pivot_table def visualize_data(grouped, top5_products): Visualize the total revenue for each store over time and the top 5 products by total revenue. fig, ax = plt.subplots(1, 2, figsize=(15, 5)) # Line plot of total revenue for each store over time for store in grouped[\'Store\'].unique(): store_data = grouped[grouped[\'Store\'] == store] ax[0].plot(store_data[\'Month\'].astype(str), store_data[\'total_revenue\'], label=f\'Store {store}\') ax[0].set_title(\'Total Revenue by Store Over Time\') ax[0].set_xlabel(\'Month\') ax[0].set_ylabel(\'Total Revenue\') ax[0].legend() # Bar chart of top 5 products by total revenue top5_products.plot(kind=\'bar\', ax=ax[1]) ax[1].set_title(\'Top 5 Products by Total Revenue\') ax[1].set_xlabel(\'Product\') ax[1].set_ylabel(\'Total Revenue\') plt.tight_layout() plt.show() def main(filepath): # Step 1: Reading the Data data = load_data(filepath) # Step 2: Data Cleaning data = clean_data(data) # Step 3: Data Transformation data = transform_data(data) # Step 4: Analysis total_units_by_store, total_revenue_by_store, top5_products = perform_analysis(data) print(\\"Total units sold by store:\\") print(total_units_by_store) print(\\"nTotal revenue by store:\\") print(total_revenue_by_store) print(\\"nTop 5 products by total revenue:\\") print(top5_products) # Step 5: Group By Operations grouped, pivot_table = group_and_pivot_data(data) print(\\"nPivot table of total revenue by store and month:\\") print(pivot_table) # Step 6: Visualization visualize_data(grouped, top5_products) # main(\'sales_data.csv\') # Uncomment this line to run the analysis with the actual sales_data.csv file"},{"question":"**Objective:** The goal of this assessment is to test your ability to manipulate raw audio data using the `audioop` module in Python. You are required to implement a function that processes an audio fragment, performs various operations, and produces a modified fragment according to specified parameters. **Problem Statement:** You are given a bytes-like object `audio_fragment` containing audio samples with a specified bit width. Your task is to implement a function `process_audio_fragment` that performs the following operations in sequence: 1. Convert the audio fragment from a-LAW encoding to linear PCM. 2. Add a bias of magnitude `bias_value` to each sample. 3. Reverse the samples in the biased fragment. 4. Change the frame rate of the reversed fragment from `in_rate` to `out_rate`. 5. Convert the modified fragment back to a-LAW encoding. **Function Signature:** ```python def process_audio_fragment(audio_fragment: bytes, width: int, bias_value: int, in_rate: int, out_rate: int) -> bytes: pass ``` **Input:** 1. `audio_fragment` (bytes): The input audio fragment encoded in a-LAW format. 2. `width` (int): The bit-width of the output fragment, which can be 1, 2, 3, or 4 bytes. 3. `bias_value` (int): The value of the bias to be added to each sample. 4. `in_rate` (int): The input frame rate of the audio fragment. 5. `out_rate` (int): The desired output frame rate of the audio fragment. **Output:** - Returns a bytes object containing the final processed audio fragment in a-LAW encoding. **Constraints:** - The input `audio_fragment` is guaranteed to be non-empty. - The `width` parameter will always be a valid value (1, 2, 3, or 4). - Both `in_rate` and `out_rate` will be positive integers. **Example:** ```python input_audio = b\'xD5x05xA5xC5\' width = 2 bias_value = 50 in_rate = 44100 out_rate = 48000 result = process_audio_fragment(input_audio, width, bias_value, in_rate, out_rate) print(result) ``` **Notes:** - Use the `audioop.alaw2lin` and `audioop.lin2alaw` functions to convert between a-LAW and linear PCM encoding. - Use the `audioop.bias` function to add a bias to the samples. - Use the `audioop.reverse` function to reverse the samples in the fragment. - Use the `audioop.ratecv` function to change the frame rate of the audio fragment. You are advised to refer to the `audioop` module documentation for details on the functions you will use.","solution":"import audioop def process_audio_fragment(audio_fragment: bytes, width: int, bias_value: int, in_rate: int, out_rate: int) -> bytes: # Step 1: Convert the audio fragment from a-LAW encoding to linear PCM linear_pcm = audioop.alaw2lin(audio_fragment, width) # Step 2: Add a bias of magnitude `bias_value` to each sample biased_fragment = audioop.bias(linear_pcm, width, bias_value) # Step 3: Reverse the samples in the biased fragment reversed_fragment = audioop.reverse(biased_fragment, width) # Step 4: Change the frame rate of the reversed fragment from `in_rate` to `out_rate` converted_fragment, _ = audioop.ratecv(reversed_fragment, width, 1, in_rate, out_rate, None) # Step 5: Convert the modified fragment back to a-LAW encoding alaw_fragment = audioop.lin2alaw(converted_fragment, width) return alaw_fragment"},{"question":"# HTTP Request Handling with `urllib.request` Objective In this task, you will demonstrate your understanding of the `urllib.request` module by writing a function that performs various types of HTTP requests (GET and POST) and handles responses and exceptions appropriately. Problem Statement Write a function `http_request_handler(url: str, request_type: str, data: dict = None, headers: dict = None) -> dict` that sends an HTTP request to a given URL. - The function should handle both `GET` and `POST` requests based on the `request_type` parameter. - If the `request_type` is \\"POST\\", the function should encode the `data` dictionary and send it as part of the POST request. - If the `request_type` is \\"GET\\", the function should append the `data` dictionary to the URL as query parameters. - Optionally, the function should accept a dictionary of HTTP headers to be added to the request. - The function should handle `URLError` and `HTTPError` exceptions and return a dictionary with appropriate keys and values to indicate any errors encountered. - The function should also return useful information such as the response code, the final URL (after any redirects), and the content of the response. Input - `url` (str): The URL to which the request is to be sent. - `request_type` (str): The type of HTTP request to be made (\\"GET\\" or \\"POST\\"). - `data` (dict, optional): A dictionary of key-value pairs to be sent with the request. Defaults to `None`. - `headers` (dict, optional): A dictionary of HTTP headers to be added to the request. Defaults to `None`. Output - A dictionary containing: - `status` (str): \\"success\\" if the request was successful, or \\"error\\" if an exception was raised. - `response_code` (int): The HTTP status code of the response. - `final_url` (str): The final URL after any redirects. - `content` (str): The content of the response as a string. - `error` (str, optional): The error message if an exception was raised. Example ```python def http_request_handler( url: str, request_type: str, data: dict = None, headers: dict = None ) -> dict: # Your code here # Example usage: result = http_request_handler( \'http://www.example.com\', \'GET\', data={\'param1\': \'value1\'}, headers={\'User-Agent\': \'Mozilla/5.0\'} ) print(result) ``` Notes - You may use the `urllib.parse` and `urllib.request` modules for encoding data and making requests. - Properly handle exceptions and return meaningful error messages in the output dictionary. - Ensure the function is robust and can handle various types of HTTP requests and responses.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError def http_request_handler(url: str, request_type: str, data: dict = None, headers: dict = None) -> dict: try: if data: encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') else: encoded_data = None if request_type.upper() == \'GET\' and data: url = f\\"{url}?{urllib.parse.urlencode(data)}\\" request = urllib.request.Request(url, headers=headers or {}, data=(encoded_data if request_type.upper() == \'POST\' else None), method=request_type.upper()) with urllib.request.urlopen(request) as response: result = { \'status\': \'success\', \'response_code\': response.getcode(), \'final_url\': response.geturl(), \'content\': response.read().decode(\'utf-8\') } return result except HTTPError as e: return { \'status\': \'error\', \'response_code\': e.code, \'final_url\': \'\', \'content\': \'\', \'error\': str(e) } except URLError as e: return { \'status\': \'error\', \'response_code\': 0, \'final_url\': \'\', \'content\': \'\', \'error\': str(e) }"},{"question":"Objective: To demonstrate your understanding of PyTorch Hub by implementing and using a custom pre-trained model entrypoint. Task: 1. Create a custom `hubconf.py` in a hypothetical repository that publishes a simple pre-trained neural network model. 2. Simulate how you would load, initialize, and use this model using the `torch.hub` interface. Details: - Your `hubconf.py` should define an entrypoint for a simple fully connected neural network model (`MySimpleNet`) suitable for classification. - Ensure that your `hubconf.py` can optionally load pre-trained weights from a specified URL. - Write a script that uses `torch.hub` to load this model and demonstrate running a forward pass with dummy data. Requirements: 1. **Define `MySimpleNet` in `hubconf.py`:** ```python import torch.nn as nn class MySimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MySimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out ``` 2. **Implement the entrypoint in `hubconf.py`:** ```python import torch dependencies = [\'torch\'] def mysimplenet(pretrained=False, input_size=10, hidden_size=5, output_size=2): MySimpleNet model pretrained (bool): load pretrained weights into the model input_size (int): size of the input layer hidden_size (int): size of the hidden layer output_size (int): size of the output layer model = MySimpleNet(input_size, hidden_size, output_size) if pretrained: checkpoint = \'https://path_to_pretrained_weights/mysimplenet.pth\' model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=True)) return model ``` 3. **Write a script to load & run the model using `torch.hub`:** ```python import torch import torch.nn as nn import torch.hub # Load the model from the repository model = torch.hub.load(\'your_github_repo\', \'mysimplenet\', pretrained=False, input_size=10, hidden_size=5, output_size=2) # Set the model in evaluation mode model.eval() # Create a dummy input tensor dummy_input = torch.randn(1, 10) # Run a forward pass with torch.no_grad(): output = model(dummy_input) print(f\\"Model Output: {output}\\") ``` Submission: - Submit the `hubconf.py` file content. - Submit the script to load and run the model using `torch.hub`. Constraints: - Ensure all steps are encapsulated in the provided files. - The use of other external libraries for neural network layers beyond PyTorch is prohibited. - Ensure that your script does not permanently alter any files or require extensive configurations. Performance: - The model definition and the script should run without any errors. - Ensure that your `hubconf.py` file contains clear docstrings for user guidance.","solution":"# hubconf.py import torch import torch.nn as nn class MySimpleNet(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(MySimpleNet, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) return out dependencies = [\'torch\'] def mysimplenet(pretrained=False, input_size=10, hidden_size=5, output_size=2): MySimpleNet model pretrained (bool): load pretrained weights into the model input_size (int): size of the input layer hidden_size (int): size of the hidden layer output_size (int): size of the output layer model = MySimpleNet(input_size, hidden_size, output_size) if pretrained: checkpoint = \'https://path_to_pretrained_weights/mysimplenet.pth\' model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=True)) return model"},{"question":"# Streaming Encoder for a Custom Format You are required to implement a custom streaming encoder and decoder using the `codecs` module in Python. Specifically, you need to create a codec that performs a custom transformation called \\"reverse_encode\\". This transformation encodes text by reversing each word in a string and retains non-word characters (such as spaces and punctuations) in their original positions. For instance, \\"Hello, World!\\" becomes \\"olleH, dlroW!\\". Requirements: 1. **Custom Codec Class**: - Implement a class `ReverseEncoder` that defines `encode()`, `decode()`, and any necessary methods. - `encode(input, errors=\'strict\')` should encode text using the reverse encoding transformation described above. - `decode(input, errors=\'strict\')` should decode text back to its original form by reversing the word reversal. 2. **StreamWriter and StreamReader**: - Implement `ReverseStreamWriter` and `ReverseStreamReader` classes that use the `ReverseEncoder` class for stream-based encoding and decoding. - Support `read()`, `write()`, and `reset()` methods appropriately. 3. **Codec Registration**: - Register the custom codec using `codecs.register()` so that it can be accessed via the `codecs.lookup()` function. 4. **Usage**: - Demonstrate how to use the registered codec to encode and decode a text string using both stateless and stream-based modes. Input and Output Format: - Input: Strings containing alphanumeric characters and punctuation, with words separated by spaces. - Output: - For `encode()`: The input string with each word reversed but positions of non-word characters preserved. - For `decode()`: The original string before encoding. Constraints: - The implementation must be efficient, and handle large text streams. - Error handling should follow the standard practices demonstrated in the `codecs` module documentation. Example Usage: ```python # Stateless usage encoded = codecs.encode(\\"Hello, World!\\", \\"reverse_encode\\") print(encoded) # Output: \\"olleH, dlroW!\\" decoded = codecs.decode(encoded, \\"reverse_encode\\") print(decoded) # Output: \\"Hello, World!\\" # Stream-based usage with codecs.open(\'example.txt\', \'w\', encoding=\'reverse_encode\') as f: f.write(\\"Hello, World!\\") with codecs.open(\'example.txt\', \'r\', encoding=\'reverse_encode\') as f: print(f.read()) # Output: \\"Hello, World!\\" ``` Implementation Notes: - Ensure to define the necessary error handling as specified in the `codecs` base interfaces. - You might need to implement additional helper classes/methods depending on your approach.","solution":"import codecs import re class ReverseEncoder: def encode(self, input, errors=\'strict\'): def reverse_word(match): return match.group(0)[::-1] encoded = re.sub(r\'bw+b\', reverse_word, input) return (encoded, len(input)) def decode(self, input, errors=\'strict\'): # Since the operation is symmetric, decode is the same as encode return self.encode(input, errors) class ReverseStreamWriter(codecs.StreamWriter): def __init__(self, stream, errors=\'strict\'): self.stream = stream self.errors = errors self.encoder = ReverseEncoder() def write(self, object): data, done = self.encoder.encode(object, self.errors) self.stream.write(data) def reset(self): pass class ReverseStreamReader(codecs.StreamReader): def __init__(self, stream, errors=\'strict\'): self.stream = stream self.errors = errors self.decoder = ReverseEncoder() def read(self, size=-1, chars=-1, firstline=False): data = self.stream.read(size) decoded, _ = self.decoder.decode(data, self.errors) return decoded def reset(self): pass def reverse_search_function(encoding): if encoding == \'reverse_encode\': return codecs.CodecInfo( name=\'reverse_encode\', encode=ReverseEncoder().encode, decode=ReverseEncoder().decode, incrementalencoder=None, incrementaldecoder=None, streamreader=ReverseStreamReader, streamwriter=ReverseStreamWriter, ) return None # Register the custom codec codecs.register(reverse_search_function) # Example usage if __name__ == \\"__main__\\": sample_text = \\"Hello, World!\\" encoded = codecs.encode(sample_text, \'reverse_encode\') print(encoded) # Output: \\"olleH, dlroW!\\" decoded = codecs.decode(encoded, \'reverse_encode\') print(decoded) # Output: \\"Hello, World!\\" with codecs.open(\'example.txt\', \'w\', encoding=\'reverse_encode\') as f: f.write(sample_text) with codecs.open(\'example.txt\', \'r\', encoding=\'reverse_encode\') as f: print(f.read()) # Output: \\"Hello, World!\\""},{"question":"# Question: Pandas Index Manipulations and Computations Objective: You are provided with several datasets that you must merge and analyze using pandas Index objects. The goal is to practice various Index operations, including but not limited to creating, transforming, and computing over Index objects. Task: 1. **Creating Indexes:** - You will be given two lists of dates and two lists of corresponding values. Create two `DatetimeIndex` objects from these lists. - Create two `Index` objects from the value lists. 2. **Merging Data:** - Merge the two `DatetimeIndex` objects to form a single union index. - Join the corresponding values based on the merged `DatetimeIndex`, ensuring alignment. If any date does not have a corresponding value, it should be replaced by `NaN`. 3. **Computations:** - Calculate the difference between the two value lists for the aligned dates using the merged index. - Identify and print the dates where the difference is maximum and minimum. 4. **Details:** - Input lists for dates and values will be as follows: - `dates1` and `values1` representing the first dataset. - `dates2` and `values2` representing the second dataset. - Dates will be strings in \'YYYY-MM-DD\' format, and values will be floating-point numbers. Input: - `dates1` (List[str]): List of date strings for the first dataset. - `values1` (List[float]): List of corresponding values for the first dataset. - `dates2` (List[str]): List of date strings for the second dataset. - `values2` (List[float]): List of corresponding values for the second dataset. Output: - Print the merged `DatetimeIndex` object. - Print the DataFrame with aligned values. - Print the maximum difference date and value. - Print the minimum difference date and value. Constraints: - Assume there are no duplicate dates within each list. - Both lists may have different lengths, and there may be dates that do not appear in both lists. Example: **Input:** ```python dates1 = [\\"2023-01-01\\", \\"2023-01-03\\", \\"2023-01-05\\"] values1 = [10.5, 20.3, 30.7] dates2 = [\\"2023-01-02\\", \\"2023-01-03\\", \\"2023-01-06\\"] values2 = [12.5, 22.1, 33.9] ``` **Output:** ``` Merged DatetimeIndex: DatetimeIndex([\'2023-01-01\', \'2023-01-02\', \'2023-01-03\', \'2023-01-05\', \'2023-01-06\'], dtype=\'datetime64[ns]\', freq=None) Aligned DataFrame: values1 values2 2023-01-01 10.5 NaN 2023-01-02 NaN 12.5 2023-01-03 20.3 22.1 2023-01-05 30.7 NaN 2023-01-06 NaN 33.9 Max Difference: (date: 2023-01-03, value: -1.8) Min Difference: (date: 2023-01-03, value: -1.8) ``` Implement a function `analyze_data(dates1, values1, dates2, values2)` to accomplish this task. ```python def analyze_data(dates1, values1, dates2, values2): # Implementation goes here pass ```","solution":"import pandas as pd def analyze_data(dates1, values1, dates2, values2): # Create DatetimeIndex from lists dti1 = pd.to_datetime(dates1) dti2 = pd.to_datetime(dates2) # Create Index from value lists (no operation needed as lists are already usable) idx1 = pd.Index(values1) idx2 = pd.Index(values2) # Merge DatetimeIndex objects to form a single union index merged_index = dti1.union(dti2) print(f\\"Merged DatetimeIndex: {merged_index}\\") # Create DataFrames df1 = pd.DataFrame(values1, index=dti1, columns=[\'values1\']) df2 = pd.DataFrame(values2, index=dti2, columns=[\'values2\']) # Join DataFrames on the merged index and align values df = df1.reindex(merged_index).join(df2.reindex(merged_index)) print(f\\"Aligned DataFrame:n{df}\\") # Calculate the difference between the two value lists df[\'difference\'] = df[\'values1\'] - df[\'values2\'] # Identify the dates with the maximum and minimum difference max_diff_date = df[\'difference\'].idxmax() max_diff_value = df[\'difference\'].max() min_diff_date = df[\'difference\'].idxmin() min_diff_value = df[\'difference\'].min() print(f\\"Max Difference: (date: {max_diff_date}, value: {max_diff_value})\\") print(f\\"Min Difference: (date: {min_diff_date}, value: {min_diff_value})\\")"},{"question":"# Python Coding Assessment Objective: To assess your understanding of the Python `warnings` module, including issuing warnings, handling them through filtering mechanisms, and using context managers to manage the behavior of warnings. Problem Statement: You are required to implement a set of functions to work with Python\'s warnings system. The goal is to: 1. Issue warnings of different types (e.g., `DeprecationWarning`, `UserWarning`). 2. Create custom filters for handling these warnings in different ways. 3. Use a context manager to temporarily suppress specific warnings during a block of code. Requirements: 1. **Function 1: issue_warnings()** - **Input**: None - **Output**: None - **Functionality**: This function should: - Issue a `DeprecationWarning` with the message \\"Deprecated feature\\". - Issue a `UserWarning` with the message \\"User alert\\". 2. **Function 2: set_warning_filter(action, category)** - **Input**: - `action` (str): The action to be taken on the warning. One of: \\"default\\", \\"error\\", \\"ignore\\", \\"always\\", \\"module\\", \\"once\\". - `category` (str): The category of warning to be filtered. One of: \\"DeprecationWarning\\", \\"UserWarning\\". - **Output**: None - **Functionality**: This function should: - Set a filter on warnings using `warnings.filterwarnings()` such that warnings of the specified category are handled according to the specified action. 3. **Function 3: suppress_warnings_temporarily(category, code_block)** - **Input**: - `category` (str): The category of warning to be suppressed temporarily. - `code_block` (Callable): A no-argument callable representing the code block to be executed with warnings suppressed. - **Output**: None - **Functionality**: This function should: - Use the `warnings.catch_warnings()` context manager to suppress the specified warning category while executing the `code_block` callable. 4. **Function 4: test_warnings()** - **Input**: None - **Output**: `bool` - Returns True if all tests pass; otherwise, returns False. - **Functionality**: This function should: - Call `issue_warnings()` and verify two warnings are issued. - Use `suppress_warnings_temporarily()` to suppress `UserWarning` and verify that only `DeprecationWarning` is issued. - Use `set_warning_filter()` to set `DeprecationWarning` as \\"ignore\\" and `UserWarning` as \\"error\\", call `issue_warnings()`, and check that only `UserWarning` raises an exception. Constraints: - Use provided documentation for reference. - The `test_warnings` function must pass successfully demonstrating that all required functionalities are correctly implemented. - Do not use any external libraries. # Note: - Implement proper exception handling as required. - Provide docstrings for all functions. - Ensure the code is clean and follows Python coding standards. # Example Usage: ```python # Function calls issue_warnings() # Setting filters set_warning_filter(\'ignore\', \'DeprecationWarning\') # Suppressing warnings temporarily def risky_code(): # Code that raises UserWarning warnings.warn(\\"This is a user warning.\\", UserWarning) suppress_warnings_temporarily(\'UserWarning\', risky_code) # Testing result = test_warnings() print(result) # Output should be True if all tests pass. ```","solution":"import warnings def issue_warnings(): Issue a DeprecationWarning and a UserWarning. warnings.warn(\\"Deprecated feature\\", DeprecationWarning) warnings.warn(\\"User alert\\", UserWarning) def set_warning_filter(action, category): Set a filter on warnings using warnings.filterwarnings(). Parameters: - action (str): The action to be taken on the warning. - category (str): The category of warning to be filtered. if category == \\"DeprecationWarning\\": warnings.filterwarnings(action, category=DeprecationWarning) elif category == \\"UserWarning\\": warnings.filterwarnings(action, category=UserWarning) else: raise ValueError(\\"Unsupported warning category.\\") def suppress_warnings_temporarily(category, code_block): Suppress specific warnings temporarily during the execution of a code block. Parameters: - category (str): The category of warning to be suppressed temporarily. - code_block (Callable): A no-argument callable representing the code block to be executed with warnings suppressed. with warnings.catch_warnings(): if category == \\"DeprecationWarning\\": warnings.simplefilter(\\"ignore\\", DeprecationWarning) elif category == \\"UserWarning\\": warnings.simplefilter(\\"ignore\\", UserWarning) else: raise ValueError(\\"Unsupported warning category.\\") code_block() def test_warnings(): Test the warnings functionalities. Returns: True if all tests pass; False otherwise. # Check if issue_warnings issues both warnings with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") issue_warnings() if not any(isinstance(warning.message, DeprecationWarning) for warning in w): return False if not any(isinstance(warning.message, UserWarning) for warning in w): return False # Check suppressing UserWarning def code_with_warning(): warnings.warn(\\"User alert\\", UserWarning) with warnings.catch_warnings(record=True) as w: warnings.simplefilter(\\"always\\") suppress_warnings_temporarily(\\"UserWarning\\", code_with_warning) if any(isinstance(warning.message, UserWarning) for warning in w): return False # Check filter settings with warnings.catch_warnings(record=True) as w: set_warning_filter(\\"ignore\\", \\"DeprecationWarning\\") set_warning_filter(\\"error\\", \\"UserWarning\\") try: issue_warnings() return False # If we get here, UserWarning didn\'t raise an error except UserWarning: # Expected exception pass if any(isinstance(warning.message, DeprecationWarning) for warning in w): return False return True return False # Something went wrong if we reach this line"},{"question":"# Python Coding Assessment Question Problem: Advanced Rational Number Operations You are tasked with enhancing the behavior of the `Fraction` class from Python\'s `fractions` module by implementing a new class called `AdvancedFraction`. This class extends `Fraction` and introduces additional functionality for arithmetic operations and approximations. # Requirements 1. **Class `AdvancedFraction`**: - Inherit from `fractions.Fraction`. - Implement a method `to_decimal(self)` that returns the decimal representation of the fraction as a `decimal.Decimal` instance. - Implement a method `is_integral(self)` that checks if the fraction represents an integer. - Implement a method `approximate_value(self, tolerance: float)` that returns a simplified floating point representation of the fraction within a given tolerance. # Implementation Details - Your `AdvancedFraction` class should have the following methods and properties: ```python class AdvancedFraction(Fraction): def to_decimal(self) -> Decimal: Return the decimal representation of the fraction. pass def is_integral(self) -> bool: Check if the fraction is an integer. pass def approximate_value(self, tolerance: float) -> float: Return a floating point approximation of the fraction within a given tolerance. pass ``` # Example Usage ```python from decimal import Decimal from fractions import Fraction class AdvancedFraction(Fraction): def to_decimal(self) -> Decimal: Return the decimal representation of the fraction. # Your implementation here def is_integral(self) -> bool: Check if the fraction is an integer. # Your implementation here def approximate_value(self, tolerance: float) -> float: Return a floating point approximation of the fraction within a given tolerance. # Your implementation here # Example instantiation and method calls f = AdvancedFraction(22, 7) print(f.to_decimal()) # Output: Decimal instance equivalent to 22 / 7 print(f.is_integral()) # Output: False print(f.approximate_value(0.01)) # Output: A floating point number close to 3.142857 ``` # Constraints - You must use the `fractions.Fraction` class as the base class. - The `approximate_value` method should ensure the returned floating point number differs from the actual fraction by no more than the given tolerance. # Notes - The `to_decimal` method should handle large fractions correctly and return a precise decimal representation. - The `is_integral` method should correctly determine whether the fraction\'s value is an integer. - Carefully handle edge cases, including very small or very large fractions for the `approximate_value` method. **Performance Requirements:** Your implementation should efficiently handle fractions with reasonably large numerators and denominators.","solution":"from decimal import Decimal from fractions import Fraction class AdvancedFraction(Fraction): def to_decimal(self) -> Decimal: Return the decimal representation of the fraction. return Decimal(self.numerator) / Decimal(self.denominator) def is_integral(self) -> bool: Check if the fraction is an integer. return self.denominator == 1 def approximate_value(self, tolerance: float) -> float: Return a floating point approximation of the fraction within a given tolerance. return round(float(self), len(str(tolerance).split(\\".\\")[1]))"},{"question":"Objective Implement a Python module that simulates data marshalling and unmarshalling operations. Specifically, write functions to serialize and deserialize Python objects to and from binary files, while handling potential errors. Task You need to implement the following functions: 1. `marshal_write_object_to_file(obj, filename, version)` - **Input**: - `obj` (Python object): The object to serialize. - `filename` (str): The name of the binary file to write the serialized object. - `version` (int): The version of the marshalling format. - **Output**: None - **Description**: This function serializes the given Python object `obj` to a binary file specified by `filename` using the provided marshalling format `version`. Handle any potential errors that may occur during the file write operation. 2. `marshal_read_object_from_file(filename)` - **Input**: - `filename` (str): The name of the binary file from which to read the serialized object. - **Output**: - Returns the deserialized Python object. - **Description**: This function reads a serialized Python object from the binary file specified by `filename` and returns the deserialized object. Handle any potential errors that may occur during the file read operation. Constraints - The functions should manage typical file I/O errors properly and raise appropriate exceptions when errors occur (e.g., `FileNotFoundError` when the file does not exist, `EOFError` when an unexpected end of file is encountered, etc.). - Assume `obj` can be any Python object that can be marshalled (e.g., int, str, list, dict). - Use Python\'s built-in `marshal` module for serialization and deserialization. Example ```python import marshal def marshal_write_object_to_file(obj, filename, version): try: with open(filename, \'wb\') as file: marshal.dump(obj, file, version) except Exception as e: raise e def marshal_read_object_from_file(filename): try: with open(filename, \'rb\') as file: return marshal.load(file) except Exception as e: raise e # Example Usage: data = {\\"key\\": \\"value\\"} file = \\"example.dat\\" version = 2 # Serialize the data marshal_write_object_to_file(data, file, version) # Deserialize the data deserialized_data = marshal_read_object_from_file(file) print(deserialized_data) # Output: {\'key\': \'value\'} ``` Ensure your implementation correctly handles: - Writing a Python object to a binary file. - Reading a Python object from a binary file. - Appropriate error handling for file operations.","solution":"import marshal def marshal_write_object_to_file(obj, filename, version): Serialize the given Python object to a binary file using the marshalling format version. Parameters: obj (object): The object to serialize. filename (str): The name of the binary file to write the serialized object. version (int): The version of the marshalling format. Raises: Exception: If an error occurs during the file write operation. try: with open(filename, \'wb\') as file: marshal.dump(obj, file, version) except Exception as e: raise e def marshal_read_object_from_file(filename): Read a serialized Python object from the binary file and return the deserialized object. Parameters: filename (str): The name of the binary file from which to read the serialized object. Returns: object: The deserialized Python object. Raises: Exception: If an error occurs during the file read operation. try: with open(filename, \'rb\') as file: return marshal.load(file) except Exception as e: raise e"},{"question":"# **Sequence Operability Test** You are required to write a Python class `CustomSequence` that replicates some functionality for managing sequences with additional constraints and optimizations. The class should: 1. Implement an initializer that accepts any iterable and stores it as an internal list. 2. Implement a method `concat_and_repeat(other, count)` that: - Concatenates the current sequence with another sequence. - Repeats the resulting concatenated sequence `count` times. - Returns the new sequence. 3. Implement a method `get_slice(i1, i2)` that: - Returns a slice from the internal list from index `i1` to `i2`. 4. Implement a method `count_value(value)` that: - Returns the number of occurrences of `value` in the sequence. 5. Implement a method `get_index(value)` that: - Returns the first index of `value` in the sequence, or raises a `ValueError` if the value is not found. # **Constraints** - You may assume that given sequences and values are always of types that support the described operations. - You must handle cases where indices might be out of bounds in a way that mimics Python’s default behavior for lists. - The order of elements must be maintained as in the original sequence. # **Function Signature** ```python class CustomSequence: def __init__(self, iterable): pass def concat_and_repeat(self, other, count): pass def get_slice(self, i1, i2): pass def count_value(self, value): pass def get_index(self, value): pass ``` # **Example Usage** ```python seq1 = CustomSequence([1, 2, 3]) seq2 = CustomSequence([4, 5]) # Concatenating two sequences and repeating 2 times result1 = seq1.concat_and_repeat(seq2, 2) print(result1) # Output: [1, 2, 3, 4, 5, 1, 2, 3, 4, 5] # Slicing elements from index 1 to 4 result2 = seq1.get_slice(1, 4) print(result2) # Output: [2, 3] # Counting occurrences of the value 2 result3 = seq1.count_value(2) print(result3) # Output: 1 # Getting index of the value 3 result4 = seq1.get_index(3) print(result4) # Output: 2 ``` Implement the `CustomSequence` class and ensure your class methods pass the provided example cases and constraints.","solution":"class CustomSequence: def __init__(self, iterable): self.sequence = list(iterable) def concat_and_repeat(self, other, count): new_sequence = self.sequence + list(other) return new_sequence * count def get_slice(self, i1, i2): return self.sequence[i1:i2] def count_value(self, value): return self.sequence.count(value) def get_index(self, value): return self.sequence.index(value)"},{"question":"# Advanced Curses Window Management in Python Let\'s create a text-based user interface that mimics a basic text viewer/editor: **Task**: Implement a Python program using the `curses` module to create a scrollable text viewer. **Requirements**: 1. Initialize the `curses` library and create a window (`stdscr`) that occupies the entire terminal screen. 2. Load a large block of text (can be hard-coded or read from a file) that is too large to fit on the screen at once. 3. Display the text within a window, and allow the user to scroll up and down using the arrow keys. 4. Ensure proper cleanup on exit, restoring the terminal to its original state. **Additional Constraints**: - Ensure your program robustly handles user inputs, including window resizing or unexpected shutdowns. - Your implementation should handle at least 100 lines of text efficiently. - Implement proper error handling to capture and report any exceptions. **Example Input and Output**: Given an input file `example_text.txt` with 150 lines of text: ```plaintext Line 1: This is a sample text viewer using Python curses. Line 2: Each line of text will be displayed in the window. ... Line 150: This is the last line of the sample text. ``` **Expected Behavior**: - The user should be able to scroll through the text using the `UP` and `DOWN` arrow keys. - The program should display the lines, adjusting the display as the user scrolls. - Proper formatting and edge-case handling (e.g., when the user reaches the top or bottom of the text). **Implementation Guidance**: - Use `curses.wrapper()` to initialize and finalize the curses environment. - Utilize `newwin()` method to create the main window and manage the `y` and `x` coordinates for displaying text. - Implement scrolling functionality by adjusting the displayed portion based on user input. **Sample Code Skeleton**: ```python import curses def main(stdscr): # Clear screen stdscr.clear() # Load a large block of text (could be from a file) lines = [f\\"Line {i}: This is a sample text viewer using Python curses.\\" for i in range(1, 151)] # Initialize variables for scrolling max_y, max_x = stdscr.getmaxyx() start_line = 0 while True: stdscr.clear() current_lines = lines[start_line:start_line + max_y - 1] for idx, line in enumerate(current_lines): stdscr.addstr(idx, 0, line) stdscr.refresh() # Get user input key = stdscr.getch() # Scroll up if key == curses.KEY_UP and start_line > 0: start_line -= 1 # Scroll down elif key == curses.KEY_DOWN and (start_line + max_y - 1) < len(lines): start_line += 1 # Exit on \'q\' if key == ord(\'q\'): break # Properly end curses application curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() # Start curses application curses.wrapper(main) ``` Extend this skeleton by filling in the required details and ensuring robust error handling. **Note**: This example is provided as a starting point; your actual implementation may differ and should handle edge cases and user interactions as required.","solution":"import curses def main(stdscr): Main function to run the scrollable text viewer using curses. # Turn off cursor blinking curses.curs_set(0) # Load a large block of text (hardcoded for this example) lines = [f\\"Line {i}: This is a sample text viewer using Python curses.\\" for i in range(1, 151)] # Initialize variables for scrolling max_y, max_x = stdscr.getmaxyx() start_line = 0 while True: stdscr.clear() current_lines = lines[start_line:start_line + max_y - 1] for idx, line in enumerate(current_lines): stdscr.addstr(idx, 0, line[:max_x-1]) # Print line truncated to window width stdscr.refresh() # Get user input key = stdscr.getch() # Scroll up if key == curses.KEY_UP and start_line > 0: start_line -= 1 # Scroll down elif key == curses.KEY_DOWN and (start_line + max_y - 1) < len(lines): start_line += 1 # Exit on \'q\' elif key == ord(\'q\'): break def run_curses_program(): Function to properly start the curses application. try: curses.wrapper(main) except Exception as e: print(str(e))"},{"question":"# Question: Implementing Custom Generic Types using GenericAlias In this task, you will create a custom generic class using Python’s type hinting system. Your goal is to develop a function that constructs a `GenericAlias` object similar to how the `List` and `Dict` generics work in Python. # Task 1. Create a class `CustomList` that represents a list-like structure but only stores elements of a specific type. 2. Implement a method `make_generic_alias` which uses Python\'s `types.GenericAlias` to create a generic alias for this `CustomList` class. 3. The `make_generic_alias` function should accept a type (e.g., `int`, `str`) and should return the `GenericAlias` object. # Requirements - **Input:** The `make_generic_alias` function will receive a single parameter: a type (`T`). - **Output:** The function should return a `GenericAlias` object that represents a generic `CustomList` for the type `T`. # Constraints 1. You cannot use existing Python generics like `List` or `Dict` directly. 2. Your implementation should only use the built-in `types` module to create the `GenericAlias`. 3. The `CustomList` class only needs to support basic list operations like adding an item and retrieving an item, but it should enforce type checking for the provided type `T`. # Example Usage ```python from types import GenericAlias class CustomList: def __init__(self): self._data = [] def add(self, item): self._data.append(item) def get(self, index): return self._data[index] def make_generic_alias(T): return GenericAlias(CustomList, (T,)) # Example of using the make_generic_alias function IntList = make_generic_alias(int) int_list = IntList() int_list.add(1) assert isinstance(int_list, CustomList) assert int_list.get(0) == 1 StrList = make_generic_alias(str) str_list = StrList() str_list.add(\\"example\\") assert isinstance(str_list, CustomList) assert str_list.get(0) == \\"example\\" ``` In this example, `IntList` and `StrList` are instances of the `GenericAlias` for `CustomList` that only accept `int` and `str` types, respectively. # Notes - You can assume the type `T` provided will always be a valid type in Python. - Focus on correctly implementing the type hinting mechanism and ensuring type consistency within your `CustomList` class.","solution":"from types import GenericAlias class CustomList: def __init__(self): self._data = [] self._type = None def add(self, item): if self._type is not None and not isinstance(item, self._type): raise TypeError(f\\"Item must be of type {self._type.__name__}\\") if self._type is None: self._type = type(item) self._data.append(item) def get(self, index): return self._data[index] def make_generic_alias(T): return GenericAlias(CustomList, (T,))"},{"question":"# Exercise: Visualizing Categorical Data with Seaborn Objective The goal of this exercise is to assess your understanding of Seaborn\'s capabilities for visualizing categorical data. You will need to demonstrate your ability to create various plots and customize them based on given requirements. Problem Statement You are given a dataset containing information about tips received by waiters in a restaurant. Your task is to create different types of categorical plots using Seaborn and customize them as per the requirements outlined below. Dataset You will use the built-in dataset `tips` provided by Seaborn. You can load this dataset using the following command: ```python import seaborn as sns tips = sns.load_dataset(\\"tips\\") ``` Tasks 1. **Categorical Scatterplot: Strip Plot** - Create a strip plot showing the total bill (`total_bill`) for each day (`day`). - Ensure there is no jitter applied in this plot. 2. **Categorical Scatterplot: Swarm Plot** - Create a swarm plot showing the total bill (`total_bill`) for each day (`day`), and differentiate the points based on the time of day (`time`) using the `hue` parameter. 3. **Box Plot** - Create a box plot showing the total bill (`total_bill`) for each day (`day`). - Use the `hue` parameter to differentiate the box plots based on whether the customer is a smoker (`smoker`). 4. **Violin Plot** - Create a violin plot showing the distribution of the total bill (`total_bill`) for each day (`day`), and split the violins based on the time of day (`time`). - Customize the plot to show individual observations as well. 5. **Bar Plot** - Create a bar plot showing the average tip (`tip`) for each day (`day`). - Include error bars representing 95% confidence intervals. 6. **Faceted Plot** - Create a faceted plot showing the total bill (`total_bill`) for each day (`day`), with separate plots for each category of the `time` variable (lunch and dinner). - Use a swarm plot within the faceted grid. Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_categorical_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Strip Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", kind=\\"strip\\", jitter=False) plt.title(\\"Strip Plot without Jitter\\") # Task 2: Swarm Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"swarm\\") plt.title(\\"Swarm Plot with Hue\\") # Task 3: Box Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"box\\") plt.title(\\"Box Plot with Hue\\") # Task 4: Violin Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"violin\\", split=True, inner=\\"stick\\") plt.title(\\"Violin Plot with Split and Inner Observations\\") # Task 5: Bar Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"tip\\", kind=\\"bar\\", ci=95) plt.title(\\"Bar Plot with 95% CI\\") # Task 6: Faceted Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"swarm\\", col=\\"time\\", aspect=.7) plt.title(\\"Faceted Swarm Plot\\") plt.show() ``` Constraints - The dataset can be directly loaded from Seaborn without any external file handling. - Each plot must be created using Seaborn\'s `catplot` function. - Proper titles must be added to each plot for clarity. Expected Output Upon calling the `visualize_categorical_data()` function, all six plots should be displayed sequentially in the output.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_categorical_data(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Strip Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", kind=\\"strip\\", jitter=False) plt.title(\\"Strip Plot without Jitter\\") # Task 2: Swarm Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"swarm\\") plt.title(\\"Swarm Plot with Hue\\") # Task 3: Box Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", kind=\\"box\\") plt.title(\\"Box Plot with Hue\\") # Task 4: Violin Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", kind=\\"violin\\", split=True, inner=\\"stick\\") plt.title(\\"Violin Plot with Split and Inner Observations\\") # Task 5: Bar Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"tip\\", kind=\\"bar\\", ci=95) plt.title(\\"Bar Plot with 95% CI\\") # Task 6: Faceted Plot plt.figure() sns.catplot(data=tips, x=\\"day\\", y=\\"total_bill\\", kind=\\"swarm\\", col=\\"time\\", aspect=.7) plt.suptitle(\\"Faceted Swarm Plot\\") plt.subplots_adjust(top=0.9) plt.show()"},{"question":"Coding Assessment Question # Objective Implement a custom class `PySet` that mimics the behavior of Python\'s built-in set using the provided API functions to perform operations such as adding elements, removing elements, checking membership, and getting the size. # Description You are required to implement a class `PySet` using the provided Python C API functions for sets. The class should support the following methods: 1. `__init__(self, iterable)`: Initialize the set with elements from the iterable. 2. `add(self, element)`: Add an element to the set. 3. `discard(self, element)`: Remove the element from the set if present. 4. `contains(self, element)`: Check if an element is in the set. 5. `size(self)`: Return the number of elements in the set. 6. `clear(self)`: Remove all elements from the set. # Method Specifications - `__init__(self, iterable)` - **Input**: An iterable object with initial elements. - **Output**: None. - **Behavior**: Initialize an empty set or with elements provided by the iterable. - `add(self, element)` - **Input**: An element to be added to the set. - **Output**: None. - **Behavior**: Add the element to the set. If the element is unhashable, raise a `TypeError`. - `discard(self, element)` - **Input**: An element to be removed from the set. - **Output**: None. - **Behavior**: Remove the element from the set if it exists. If the element is unhashable, raise a `TypeError`. - `contains(self, element)` - **Input**: An element to check for presence in the set. - **Output**: Boolean (`True` if the element exists, `False` otherwise). - **Behavior**: Return `True` if the element is present in the set, `False` otherwise. If the element is unhashable, raise a `TypeError`. - `size(self)` - **Input**: None. - **Output**: Integer (size of the set). - **Behavior**: Return the number of elements in the set. - `clear(self)` - **Input**: None. - **Output**: None. - **Behavior**: Remove all elements from the set. # Constraints and Limitations - The `element` provided to any method must be hashable. - The `iterable` provided to `__init__` must be an iterable type containing hashable elements. - The class should employ the appropriate error handling as specified in the method behaviors. # Performance Requirements - Each method should aim for optimal performance considering the underlying implementation based on the API functions. # Example Usage ```python # Initialize with an iterable ps = PySet([1, 2, 3, 4]) # Add an element ps.add(5) # Check the size print(ps.size()) # Expected output: 5 # Check if an element exists print(ps.contains(3)) # Expected output: True # Remove an element ps.discard(3) print(ps.contains(3)) # Expected output: False # Clear the set ps.clear() print(ps.size()) # Expected output: 0 ``` # Notes - Your implementation should directly utilize the corresponding API functions detailed in the documentation. - Ensure appropriate exception handling and error messages as specified.","solution":"class PySet: def __init__(self, iterable=None): self._set = set() if iterable is not None: for item in iterable: self.add(item) def add(self, element): if not isinstance(element, (int, float, str, tuple)): raise TypeError(\\"Unhashable type: \'{}\'\\".format(type(element).__name__)) self._set.add(element) def discard(self, element): if not isinstance(element, (int, float, str, tuple)): raise TypeError(\\"Unhashable type: \'{}\'\\".format(type(element).__name__)) self._set.discard(element) def contains(self, element): if not isinstance(element, (int, float, str, tuple)): raise TypeError(\\"Unhashable type: \'{}\'\\".format(type(element).__name__)) return element in self._set def size(self): return len(self._set) def clear(self): self._set.clear()"},{"question":"# Explanation The `atexit` module allows defining cleanup functions that are automatically executed upon normal termination of the Python interpreter. These functions are executed in the reverse order of their registration. # Task You are required to: 1. Create a class `ResourceHandler` that manages resources and automatically cleans them up upon program termination. 2. The class should have: - An `__init__` method that takes a `resource_name` as an argument and stores it. - A `register_action` method that registers a cleanup function using the `atexit` module. - An `unregister_action` method that unregisters a previously registered cleanup function. # Implementation Details 1. The `register_action` method should register a function that prints a message indicating the resource associated with an instance of `ResourceHandler` is being cleaned up. For example, \\"Cleaning up resource: <resource_name>\\". 2. Demonstrate the registration and unregistration of cleanup functions by showing that registered cleanup actions are executed when the program finishes, and that unregistered actions are not executed. # Example Usage ```python r1 = ResourceHandler(\\"Resource1\\") r2 = ResourceHandler(\\"Resource2\\") r1.register_action() r2.register_action() # Suppose we decide that Resource1 does not need to be cleaned up anymore r1.unregister_action() # The program will run its main logic here # Upon program termination, \\"Cleaning up resource: Resource2\\" should be printed. # \\"Cleaning up resource: Resource1\\" should not be printed since it was unregistered. ``` # Constraints - Assume the program will terminate normally. - The `resource_name` will always be a string. Write your solution as a complete Python script below: ```python import atexit class ResourceHandler: def __init__(self, resource_name): self.resource_name = resource_name self.cleanup_func = lambda: print(f\\"Cleaning up resource: {self.resource_name}\\") def register_action(self): atexit.register(self.cleanup_func) def unregister_action(self): atexit.unregister(self.cleanup_func) # Example usage: r1 = ResourceHandler(\\"Resource1\\") r2 = ResourceHandler(\\"Resource2\\") r1.register_action() r2.register_action() # Unregister the cleanup action for Resource1 r1.unregister_action() # Placeholder for the main logic of the program print(\\"Program is running\\") # Upon normal termination, \\"Cleaning up resource: Resource2\\" should be printed. # \\"Cleaning up resource: Resource1\\" should not be printed. ```","solution":"import atexit class ResourceHandler: def __init__(self, resource_name): self.resource_name = resource_name self.cleanup_func = lambda: print(f\\"Cleaning up resource: {self.resource_name}\\") def register_action(self): atexit.register(self.cleanup_func) def unregister_action(self): atexit.unregister(self.cleanup_func) # Example usage: r1 = ResourceHandler(\\"Resource1\\") r2 = ResourceHandler(\\"Resource2\\") r1.register_action() r2.register_action() # Unregister the cleanup action for Resource1 r1.unregister_action() # Placeholder for the main logic of the program print(\\"Program is running\\") # Upon normal termination, \\"Cleaning up resource: Resource2\\" should be printed. # \\"Cleaning up resource: Resource1\\" should not be printed."},{"question":"**Coding Assessment Question: Advanced Visualization with seaborn.objects** # Objective Demonstrate your ability to create complex and customizable visualizations using the `seaborn.objects` interface. # Task You are provided with a dataset called `iris` which contains information about different species of iris flowers. Using the `seaborn.objects` interface, create a multi-faceted visualization that highlights the relationships between different numerical features of the dataset, while applying various customizations to enhance readability and aesthetics. # Dataset The `iris` dataset has the following columns: - `sepal_length`: Sepal length (in cm) - `sepal_width`: Sepal width (in cm) - `petal_length`: Petal length (in cm) - `petal_width`: Petal width (in cm) - `species`: Species of the iris flower (Setosa, Versicolor, Virginica) # Requirements 1. **Load the dataset** using seaborn\'s utility function. 2. **Create a pair plot** to visualize pairwise relationships between `sepal_length`, `sepal_width`, `petal_length`, and `petal_width`. Each subplot should display these relationships for every species. 3. **Facet the plot by species** using the `facet` method to create different subplots for each species. 4. **Customize the appearance** by: - Adding different marks (dots and regression lines) to the subplots. - Setting a color palette for the species. - Adjusting scales, limits, and labels for both axes. - Applying a theme to the plot for a clean and distinctive look. # Constraints - Ensure that the plots do not overlap excessively and that each facet is clearly distinguishable. - The customizations should improve the readability of the plots without making the visualization overly complex. # Expected Output A multi-faceted visualization consisting of subplots for each species, with pairwise plots of the numerical features. Customizations should enhance the plot\'s readability and aesthetics. ```python import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the \'iris\' dataset iris = sns.load_dataset(\'iris\') # Create a pair plot with faceting by species plot = ( so.Plot(iris, y=\'species\', color=\'species\') .pair(x=[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\']) .facet(row=\'species\') .add(so.Dots()) .add(so.Line(), so.PolyFit()) .scale(color=\'Set3\') .label( x=\'Measurement (cm)\', color=str.capitalize, title=\'Pairwise Relationships of Iris Measurements\' ) .theme({ \'axes.grid\': True, \'grid.linestyle\': \'--\', \'axes.titlesize\': \'large\', \'axes.labelsize\': \'medium\', \'axes.labelweight\': \'bold\' }) ) plot.show() ``` # Note - The above code provided is a template for you to start with. Make sure to apply appropriate customizations and adjustments as required. - The final visualization should effectively communicate the relationships between the numerical features across different species of the iris plant.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load the \'iris\' dataset iris = sns.load_dataset(\'iris\') # Create a pair plot with faceting by species plot = ( so.Plot(iris, y=\'species\', color=\'species\') .pair(x=[\'sepal_length\', \'sepal_width\', \'petal_length\', \'petal_width\']) .facet(row=\'species\') .add(so.Dots(), so.PolyFit()) .scale(color=\'Set2\') .label( x=\'Measurement (cm)\', color=str.capitalize, title=\\"Pairwise Relationships of Iris Measurements\\" ) .theme({ \'axes.grid\': True, \'grid.linestyle\': \'--\', \'axes.titlesize\': \'large\', \'axes.labelsize\': \'medium\', \'axes.labelweight\': \'bold\' }) ) plot.show()"},{"question":"Custom Color Palette Creation with Seaborn Objective Demonstrate your understanding of the seaborn package by creating and customizing color palettes using the `husl_palette` function. Problem Statement You are tasked with creating a function that generates and visualizes custom color palettes using seaborn\'s `husl_palette` function. Your function should allow users to specify various parameters to customize the palette. Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(num_colors: int=6, lightness: float=0.5, saturation: float=0.8, hue_start: float=0.0, as_cmap: bool=False) -> None: pass ``` Parameters 1. **num_colors (int)**: The number of colors to generate in the palette. Must be a positive integer. Default is 6. 2. **lightness (float)**: A float value between 0 and 1 representing the lightness of the colors. Default is 0.5. 3. **saturation (float)**: A float value between 0 and 1 representing the saturation of the colors. Default is 0.8. 4. **hue_start (float)**: A float value representing the hue start point for sampling, in the range [0, 1). Default is 0.0. 5. **as_cmap (bool)**: If True, return the palette as a continuous colormap instead of discrete colors. Default is False. Returns - The function does not return any value. Functionality 1. Use the `sns.husl_palette` function to create a color palette based on the parameters provided. 2. If `as_cmap` is `True`, the function should generate and display a colormap. 3. If `as_cmap` is `False`, the function should generate and display a bar plot where each bar is colored with a different color from the palette. Example ```python # Example 1: Generate and display a palette with 8 colors, lightness 0.4, and saturation 0.7 create_custom_palette(num_colors=8, lightness=0.4, saturation=0.7) # Example 2: Generate and display a continuous colormap create_custom_palette(as_cmap=True) ``` Constraints - `num_colors` should be between 1 and 100. - `lightness` and `saturation` should be in the range [0, 1]. - `hue_start` should be in the range [0, 1). Notes - Ensure you have the latest version of seaborn and matplotlib installed. - The function should appropriately handle edge cases and raise appropriate errors for invalid input parameters. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette(num_colors: int=6, lightness: float=0.5, saturation: float=0.8, hue_start: float=0.0, as_cmap: bool=False) -> None: Generate and display a custom color palette using seaborn\'s husl_palette function. Parameters: num_colors (int): The number of colors to generate in the palette. Must be a positive integer. Default is 6. lightness (float): A float value between 0 and 1 representing the lightness of the colors. Default is 0.5. saturation (float): A float value between 0 and 1 representing the saturation of the colors. Default is 0.8. hue_start (float): A float value representing the hue start point for sampling, in the range [0, 1). Default is 0.0. as_cmap (bool): If True, return the palette as a continuous colormap instead of discrete colors. Default is False. Returns: None if not (1 <= num_colors <= 100): raise ValueError(\\"num_colors must be between 1 and 100\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation must be between 0 and 1\\") if not (0 <= hue_start < 1): raise ValueError(\\"hue_start must be between 0 and 1\\") palette = sns.husl_palette(n_colors=num_colors, l=lightness * 100, s=saturation * 100, h=hue_start * 360) if as_cmap: cmap = sns.color_palette(palette, as_cmap=True) sns.palplot(sns.color_palette(palette)) plt.title(\\"Continuous Colormap\\") else: sns.palplot(palette) plt.title(f\\"HUSL Palette with {num_colors} colors\\") plt.show()"},{"question":"# Data Compression and Archiving Challenge Objective: Implement a Python function that compresses all files in a specified directory into a compressed archive using a specified algorithm and then decompresses the archive into another directory. The function should verify that the decompressed files are identical to the original files. Function Signature: ```python import os from typing import List def compress_and_verify( directory: str, archive_name: str, algorithm: str, output_dir: str ) -> bool: Compress all files in the given directory into a specified archive format and decompress them into another directory to verify integrity. Parameters: directory (str): The path of the directory containing files to be compressed. archive_name (str): The name of the resulting compressed archive (without extension). algorithm (str): The compression algorithm (\'gzip\', \'bz2\', \'lzma\', \'zip\', \'tar\'). output_dir (str): The directory where decompressed files will be placed. Returns: bool: True if decompressed files are identical to original files, else False. pass ``` Input: 1. `directory`: The path to a directory containing files to be compressed. 2. `archive_name`: The desired name of the output archive file (without extension). 3. `algorithm`: The algorithm to use for compression. Should be one of \'gzip\', \'bz2\', \'lzma\', \'zip\', or \'tar\'. 4. `output_dir`: A directory where decompressed files will be extracted. Output: - Return `True` if the decompressed files match the original files, `False` otherwise. Constraints: 1. The function should handle typical file I/O errors gracefully. 2. Handle both text and binary files. 3. Assume file names in the directory are unique. Example: ```python # Example directory structure: # /original_dir/ # file1.txt # file2.bin # Compress using the \'zip\' algorithm: success = compress_and_verify(\\"/original_dir\\", \\"archive\\", \\"zip\\", \\"/output_dir\\") print(success) # Should be True if the decompressed files match the originals. ``` Notes: - You will need to use appropriate Python modules: `gzip`, `bz2`, `lzma`, `zipfile`, and `tarfile`. - Ensure the function creates any necessary directories. - Implement verification by comparing the original and decompressed files byte-by-byte.","solution":"import os import filecmp import shutil from typing import List import gzip import bz2 import lzma import zipfile import tarfile def compress_and_verify(directory: str, archive_name: str, algorithm: str, output_dir: str) -> bool: Compress all files in the given directory into a specified archive format and decompress them into another directory to verify integrity. Parameters: directory (str): The path of the directory containing files to be compressed. archive_name (str): The name of the resulting compressed archive (without extension). algorithm (str): The compression algorithm (\'gzip\', \'bz2\', \'lzma\', \'zip\', \'tar\'). output_dir (str): The directory where decompressed files will be placed. Returns: bool: True if decompressed files are identical to original files, else False. # Ensure the output directory exists os.makedirs(output_dir, exist_ok=True) # Create the archive archive_path = f\\"{archive_name}.{algorithm}\\" try: if algorithm == \'gzip\': with tarfile.open(f\\"{archive_path}.tar.gz\\", \\"w:gz\\") as tar: tar.add(directory, arcname=os.path.basename(directory)) elif algorithm == \'bz2\': with tarfile.open(f\\"{archive_path}.tar.bz2\\", \\"w:bz2\\") as tar: tar.add(directory, arcname=os.path.basename(directory)) elif algorithm == \'lzma\': with tarfile.open(f\\"{archive_path}.tar.xz\\", \\"w:xz\\") as tar: tar.add(directory, arcname=os.path.basename(directory)) elif algorithm == \'zip\': with zipfile.ZipFile(f\\"{archive_path}.zip\\", \'w\', zipfile.ZIP_DEFLATED) as zipf: for root, dirs, files in os.walk(directory): for file in files: zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), os.path.join(directory, \'..\'))) elif algorithm == \'tar\': with tarfile.open(f\\"{archive_path}.tar\\", \\"w\\") as tar: tar.add(directory, arcname=os.path.basename(directory)) else: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") except Exception as e: return False # Decompress the archive try: if algorithm == \'gzip\': with tarfile.open(f\\"{archive_path}.tar.gz\\", \\"r:gz\\") as tar: tar.extractall(path=output_dir) elif algorithm == \'bz2\': with tarfile.open(f\\"{archive_path}.tar.bz2\\", \\"r:bz2\\") as tar: tar.extractall(path=output_dir) elif algorithm == \'lzma\': with tarfile.open(f\\"{archive_path}.tar.xz\\", \\"r:xz\\") as tar: tar.extractall(path=output_dir) elif algorithm == \'zip\': with zipfile.ZipFile(f\\"{archive_path}.zip\\", \'r\') as zipf: zipf.extractall(output_dir) elif algorithm == \'tar\': with tarfile.open(f\\"{archive_path}.tar\\", \\"r\\") as tar: tar.extractall(path=output_dir) else: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") except Exception as e: return False # Verify the integrity of the decompressed files against the original files original_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(directory) for f in filenames] for file in original_files: relative_path = os.path.relpath(file, directory) original = file decompressed = os.path.join(output_dir, os.path.basename(directory), relative_path) if not filecmp.cmp(original, decompressed, shallow=False): return False return True"},{"question":"**Task: Implement and Apply Causal Bias in an Attention Mechanism using PyTorch** In this task, you will work with concepts related to causal attention biases, a typical requirement in models handling sequence data (such as Transformer-based models for NLP tasks). Implement a custom neural network module in PyTorch that uses causal biases in its attention mechanism. Specifically, you need to apply a type of causal bias to a matrix, enhancing the functionality given in the provided module `torch.nn.attention.bias`. # Instructions 1. **Implement a Custom Causal Attention Module:** - Create a subclass of `torch.nn.Module` named `CustomCausalAttention` which initializes with the following parameters: - `input_dim` (int): The dimension of the input vectors. - `hidden_dim` (int): The dimension of the hidden state in the attention mechanism. - `bias_type` (str): The type of causal bias, either \\"lower_right\\" or \\"upper_left\\". 2. **Define the forward Method:** - This should take an input tensor `x` of shape `(batch_size, seq_length, input_dim)`. - Apply a causal bias to the attention weights based on the `bias_type` specified during initialization. - Return the resulting tensor after the attention and bias application. 3. **Use Existing APIs from `torch.nn.attention.bias`:** - Leverage `causal_lower_right` and `causal_upper_left` functions appropriately based on the chosen bias type. # Constraints - Assume the input tensor `x` is a batch of sequences with each sequence being `input_dim`-dimensional. - You can assume `causal_lower_right` and `causal_upper_left` functions are available from `torch.nn.attention.bias`. # Expected Output - The output tensor should have the same shape as the input tensor `(batch_size, seq_length, input_dim)` but with the specified causal bias applied to the attention weights. # Example Usage ```python import torch from torch.nn.attention.bias import causal_lower_right, causal_upper_left class CustomCausalAttention(torch.nn.Module): def __init__(self, input_dim, hidden_dim, bias_type): super(CustomCausalAttention, self).__init__() # ... (initialize necessary layers and parameters) def forward(self, x): # ... (implement forward pass with causal bias) pass # Example usage: input_dim = 64 hidden_dim = 128 bias_type = \'lower_right\' batch_size = 32 seq_length = 10 model = CustomCausalAttention(input_dim, hidden_dim, bias_type) input_tensor = torch.randn(batch_size, seq_length, input_dim) output_tensor = model(input_tensor) print(output_tensor.shape) # Expected: torch.Size([32, 10, 64]) ``` Your implementation should ensure that `output_tensor` correctly reflects the causal bias applied within the attention mechanism.","solution":"import torch import torch.nn as nn import torch.nn.functional as F # Placeholder functions for causal biases def causal_lower_right(matrix): seq_len = matrix.size(-1) causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool() matrix.masked_fill_(causal_mask, float(\'-inf\')) return matrix def causal_upper_left(matrix): seq_len = matrix.size(-1) causal_mask = torch.tril(torch.ones(seq_len, seq_len), diagonal=-1).bool() matrix.masked_fill_(causal_mask, float(\'-inf\')) return matrix class CustomCausalAttention(nn.Module): def __init__(self, input_dim, hidden_dim, bias_type): super(CustomCausalAttention, self).__init__() self.input_dim = input_dim self.hidden_dim = hidden_dim self.bias_type = bias_type self.query = nn.Linear(input_dim, hidden_dim) self.key = nn.Linear(input_dim, hidden_dim) self.value = nn.Linear(input_dim, hidden_dim) self.output = nn.Linear(hidden_dim, input_dim) def forward(self, x): batch_size, seq_length, _ = x.size() queries = self.query(x) # (batch_size, seq_length, hidden_dim) keys = self.key(x) # (batch_size, seq_length, hidden_dim) values = self.value(x) # (batch_size, seq_length, hidden_dim) # Scaled Dot-Product Attention scores = torch.matmul(queries, keys.transpose(-2, -1)) / self.hidden_dim ** 0.5 # (batch_size, seq_length, seq_length) if self.bias_type == \'lower_right\': scores = causal_lower_right(scores) elif self.bias_type == \'upper_left\': scores = causal_upper_left(scores) attention_weights = F.softmax(scores, dim=-1) # (batch_size, seq_length, seq_length) attended = torch.matmul(attention_weights, values) # (batch_size, seq_length, hidden_dim) output = self.output(attended) # (batch_size, seq_length, input_dim) return output"},{"question":"**Problem Statement:** You are part of a software development team working with large datasets. Your task is to implement a utility using the `gzip` module that efficiently handles the compression and decompression of data. The utility should be able to read large text files, compress them to save space, and later decompress them for use. Using the `gzip` module, implement the following functions: 1. `compress_file(input_file_path: str, output_file_path: str, compresslevel: int = 9) -> None`: - This function should read the content from `input_file_path`, compress it, and write the compressed data to `output_file_path`. - Parameters: - `input_file_path`: A string specifying the path to the input text file. - `output_file_path`: A string specifying the path where the compressed output file should be saved. - `compresslevel`: An integer from 0 to 9 indicating the level of compression (default is 9). - The input file is a plain text file. - The output should be a gzip-compressed file. 2. `decompress_file(input_file_path: str, output_file_path: str) -> None`: - This function should read the compressed data from `input_file_path`, decompress it, and write the decompressed data to `output_file_path`. - Parameters: - `input_file_path`: A string specifying the path to the input compressed file. - `output_file_path`: A string specifying the path where the decompressed output file should be saved. - The input file is a gzip-compressed file. - The output should be a plain text file. 3. `compress_data(data: bytes, compresslevel: int = 9) -> bytes`: - This function should take in a bytes object, compress it using gzip compression, and return the compressed bytes. - Parameters: - `data`: A bytes object to be compressed. - `compresslevel`: An integer from 0 to 9 indicating the level of compression (default is 9). 4. `decompress_data(data: bytes) -> bytes`: - This function should take in compressed bytes, decompress it using gzip decompression, and return the decompressed bytes. - Parameters: - `data`: A bytes object of compressed data. **Constraints:** - The input and output paths provided will always be valid and accessible. - You may assume the string lengths (for file paths) and sizes of data will be within manageable limits for the memory available in your development environment. **Performance Requirements:** - Your solution should handle large files efficiently without excessive memory usage. **Example Usage:** ```python # Example usage of compress_file and decompress_file compress_file(\'input.txt\', \'output.gz\', compresslevel=6) decompress_file(\'output.gz\', \'output.txt\') # Example usage of compress_data and decompress_data data = b\\"Example data to be compressed\\" compressed_data = compress_data(data, compresslevel=6) decompressed_data = decompress_data(compressed_data) print(decompressed_data) # Output should be: b\'Example data to be compressed\' ``` Ensure you adhere to the constraints and performance requirements. Think about the edge cases and provide solutions that are robust and efficient.","solution":"import gzip def compress_file(input_file_path: str, output_file_path: str, compresslevel: int = 9) -> None: with open(input_file_path, \'rb\') as input_file: with gzip.open(output_file_path, \'wb\', compresslevel=compresslevel) as output_file: output_file.write(input_file.read()) def decompress_file(input_file_path: str, output_file_path: str) -> None: with gzip.open(input_file_path, \'rb\') as input_file: with open(output_file_path, \'wb\') as output_file: output_file.write(input_file.read()) def compress_data(data: bytes, compresslevel: int = 9) -> bytes: return gzip.compress(data, compresslevel=compresslevel) def decompress_data(data: bytes) -> bytes: return gzip.decompress(data)"},{"question":"# Advanced Python Programming Task You are assigned to manage and debug a legacy CGI application that uses Python\'s `cgitb` module. Given that this module provides enhanced traceback information, your task is to extend the functionality of `cgitb` to provide additional logging capabilities. # Objectives 1. **Exception Wrapper Function**: - Implement a function `enhanced_traceback(func)` that serves as a decorator to wrap any function with enhanced exception reporting using the `cgitb` module. - This decorator should enable `cgitb` with specific settings (`display=0`, `logdir=\'log\', context=3, format=\'text\'`), ensuring that the tracebacks are logged into a specified directory (`log`). 2. **Exception Logging**: - The decorator should catch any uncaught exceptions raised by the decorated function and log detailed tracebacks to a file in plain text format. - The log file should be named based on the function\'s name and the timestamp of the exception occurrence (e.g., `function_name_YYYYMMDD_HHMMSS.log`). # Input and Output Formats 1. **Function Signature**: ```python def enhanced_traceback(func: Callable) -> Callable: ``` 2. **Input**: - `func` (Callable): A function that will be wrapped for enhanced exception logging. 3. **Output**: - A callable function that, when executed, will log detailed tracebacks on any uncaught exceptions to the `log` directory in plain text. # Constraints and Requirements - The provided function `func` could be any standard Python function that may or may not raise exceptions. - Ensure the `log` directory exists before attempting to write logs. - The log file must contain comprehensive traceback details as provided by `cgitb`. - Include necessary imports and error handling to manage file operations and directory creation. - Ensure the solution is compatible with Python versions where `cgitb` is not yet deprecated (i.e., <= Python 3.10). # Example Usage ```python import os import cgitb from datetime import datetime from typing import Callable def enhanced_traceback(func: Callable) -> Callable: def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except Exception as e: # Ensure log directory exists log_dir = \'log\' os.makedirs(log_dir, exist_ok=True) # Generate log file name timestamp = datetime.now().strftime(\'%Y%m%d_%H%M%S\') log_file = os.path.join(log_dir, f\'{func.__name__}_{timestamp}.log\') # Enable cgitb with the required settings cgitb.enable(display=0, logdir=log_dir, context=3, format=\'text\') # Log the exception traceback with open(log_file, \'w\') as f: f.write(cgitb.text(sys.exc_info(), context=3)) raise # Re-raise the exception after logging return wrapper # Example function to test the decorator @enhanced_traceback def test_function(x): return 10 / x # This call will create a log entry in the log directory due to ZeroDivisionError try: test_function(0) except: pass ``` In this example, invoking `test_function(0)` triggers a `ZeroDivisionError`, which is caught by the decorator and logged to the `log` directory with detailed traceback information.","solution":"import os import cgitb import sys from datetime import datetime from typing import Callable def enhanced_traceback(func: Callable) -> Callable: def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except Exception as e: # Ensure log directory exists log_dir = \'log\' os.makedirs(log_dir, exist_ok=True) # Generate log file name timestamp = datetime.now().strftime(\'%Y%m%d_%H%M%S\') log_file = os.path.join(log_dir, f\'{func.__name__}_{timestamp}.log\') # Enable cgitb with the required settings cgitb.enable(display=0, logdir=log_dir, context=3, format=\'text\') # Log the exception traceback with open(log_file, \'w\') as f: # Generate the cgitb formatted traceback string traceback_info = cgitb.text(sys.exc_info(), context=3) f.write(traceback_info) raise # Re-raise the exception after logging return wrapper"},{"question":"**Question: Implement a Comprehensive Robots.txt Analyzer** You are required to implement a function, `fetch_analysis(robots_url, user_agent, url_list)`, that uses the `RobotFileParser` class from the `urllib.robotparser` module to analyze the provided robots.txt file and retrieve detailed information for a specific user agent and a list of URLs. # Function Signature ```python def fetch_analysis(robots_url: str, user_agent: str, url_list: List[str]) -> Dict[str, Any]: ``` # Input - `robots_url`: A string representing the URL of the robots.txt file. Example: `\\"http://www.example.com/robots.txt\\"` - `user_agent`: A string representing the user agent making the requests. Example: `\\"MyBot\\"` - `url_list`: A list of strings, each representing URLs to be checked against the robots.txt file. Example: `[\\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\"]` # Output - A dictionary with the following structure: ```python { \\"can_fetch\\": {url_1: bool, url_2: bool, ...}, # A dictionary mapping each URL in the url_list to a boolean indicating if the user agent can fetch the URL \\"crawl_delay\\": int or None, # The crawl delay for the user agent, or None if not specified \\"request_rate\\": (int, int) or None, # A tuple representing the request rate (requests, seconds) for the user agent, or None if not specified \\"sitemaps\\": List[str] or None # A list of sitemap URLs specified in the robots.txt file, or None if not specified } ``` # Constraints - The robots.txt file may not exist or may be inaccessible. - Some of the parameters (crawl delay, request rate, sitemaps) may not be present in the robots.txt file. - The URL list can vary in size but will have a maximum of 100 URLs. # Example ```python robots_url = \\"http://www.example.com/robots.txt\\" user_agent = \\"MyBot\\" url_list = [\\"http://www.example.com/page1\\", \\"http://www.example.com/page2\\"] result = fetch_analysis(robots_url, user_agent, url_list) print(result) ``` Expected output (example): ```python { \\"can_fetch\\": { \\"http://www.example.com/page1\\": True, \\"http://www.example.com/page2\\": False }, \\"crawl_delay\\": 10, \\"request_rate\\": (5, 20), \\"sitemaps\\": [\\"http://www.example.com/sitemap1.xml\\", \\"http://www.example.com/sitemap2.xml\\"] } ``` # Notes - Make sure the function is robust to handle network issues or invalid robots.txt files gracefully. - Refer to the `urllib.robotparser` documentation to understand the different methods available and how to use them.","solution":"from typing import List, Dict, Any from urllib.robotparser import RobotFileParser from urllib.error import URLError def fetch_analysis(robots_url: str, user_agent: str, url_list: List[str]) -> Dict[str, Any]: # Initialize the RobotFileParser object rp = RobotFileParser() rp.set_url(robots_url) try: rp.read() except URLError as e: # If robots.txt cannot be read, return a default result indicating failure return { \\"can_fetch\\": {url: False for url in url_list}, \\"crawl_delay\\": None, \\"request_rate\\": None, \\"sitemaps\\": None } # Prepare the can_fetch dictionary can_fetch = {url: rp.can_fetch(user_agent, url) for url in url_list} # Get crawl delay, request rate, and sitemaps crawl_delay = rp.crawl_delay(user_agent) request_rate = rp.request_rate(user_agent) sitemaps = rp.site_maps() return { \\"can_fetch\\": can_fetch, \\"crawl_delay\\": crawl_delay, \\"request_rate\\": request_rate, \\"sitemaps\\": sitemaps }"},{"question":"# **Python Socket Programming Assessment** **Objective:** Implement a Python program to create a client-server application using sockets. Use non-blocking sockets and `select` to manage multiple connections efficiently. The server should handle multiple clients by echoing back any messages it receives. **Problem Description:** Implement two Python functions: `start_server(host, port)` for the server and `start_client(host, port, message)` for the client. **1. Function: `start_server(host, port)`** This function should: - Create and bind a server socket to the specified `host` and `port`. - Set the socket to non-blocking mode. - Use `select` to manage multiple clients. - Accept new client connections. - Receive data from clients and echo back the same data. - Properly handle client disconnections and clean up resources. **2. Function: `start_client(host, port, message)`** This function should: - Create a client socket and connect to the server at the specified `host` and `port`. - Send the specified `message` to the server. - Wait to receive the echoed message from the server. - Print the received message. - Handle potential connection errors. **Constraints:** - The server should handle at least 5 simultaneous client connections. - Ensure proper handling of network buffer and incomplete message scenarios. - The server should run indefinitely until manually stopped. - The client should gracefully handle server unavailability. **Performance Requirements:** - The server should efficiently handle incoming connections and data using the `select` function for I/O multiplexing. - Implement proper error handling and clean-up to avoid resource leaks. **Expected Input and Output:** - Input for `start_server(host, port)`: - `host`: A string representing the host name or IP address. - `port`: An integer representing the port number. - Input for `start_client(host, port, message)`: - `host`: A string representing the server hostname or IP address. - `port`: An integer representing the server port number. - `message`: A string representing the message to be sent. - Output for `start_client(host, port, message)`: - Print the echoed message received from the server. **Example Usage:** ```python # Start the server (run this in one terminal or script) start_server(\'localhost\', 12345) # Start clients (run these in separate terminals or scripts) start_client(\'localhost\', 12345, \'Hello, Server!\') start_client(\'localhost\', 12345, \'Another Message\') ``` **Note:** You may use the following pseudocode to get started with the server: ```python import socket import select def start_server(host, port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) server_socket.setblocking(False) sockets_list = [server_socket] clients = {} while True: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address else: message = notified_socket.recv(1024) if not message: sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() else: notified_socket.send(message) for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() def start_client(host, port, message): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.connect((host, port)) client_socket.send(message.encode()) response = client_socket.recv(1024) print(f\'Received from server: {response.decode()}\') client_socket.close() ``` Make sure to add error handling and clean up resources appropriately.","solution":"import socket import select def start_server(host, port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.bind((host, port)) server_socket.listen(5) server_socket.setblocking(False) sockets_list = [server_socket] clients = {} print(f\\"Server started on {host}:{port}\\") try: while True: read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for notified_socket in read_sockets: if notified_socket == server_socket: client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"Accepted new connection from {client_address[0]}:{client_address[1]}\\") else: message = notified_socket.recv(1024) if not message: print(f\\"Closed connection from {clients[notified_socket][0]}:{clients[notified_socket][1]}\\") sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() else: print(f\\"Received message from {clients[notified_socket][0]}:{clients[notified_socket][1]}: {message.decode()}\\") notified_socket.sendall(message) for notified_socket in exception_sockets: sockets_list.remove(notified_socket) del clients[notified_socket] notified_socket.close() finally: for sock in sockets_list: sock.close() def start_client(host, port, message): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: client_socket.connect((host, port)) client_socket.sendall(message.encode()) response = client_socket.recv(1024) print(f\\"Received from server: {response.decode()}\\") except Exception as e: print(f\\"Client error: {e}\\") finally: client_socket.close()"},{"question":"You are required to implement a multithreaded program using Python\'s `threading` module. Your task is to simulate a simple producer-consumer problem where multiple producer threads generate random numbers and put them into a shared buffer, and multiple consumer threads take numbers from the buffer and compute their sum. # Requirements: 1. **Shared Buffer**: Use a thread-safe queue (using the `queue` module). 2. **Producer Threads**: - There should be N producers (N is an input to the program). - Each producer generates M random numbers (M is an input to the program) and puts them into the shared buffer. 3. **Consumer Threads**: - There should be P consumers (P is an input to the program). - Each consumer waits for a number to be available in the buffer, removes it, and adds it to a running sum. 4. **Synchronization**: - Use threading synchronization primitives such as `Lock` or `Semaphore` to ensure that access to the shared sum is thread-safe. - Ensure that the producers and consumers do not run indefinitely. Each producer should stop after producing M items, and each consumer should stop after consuming a total of M*N items divided by P. # Input: - `N` (number of producer threads) - `M` (number of items each producer generates) - `P` (number of consumer threads) # Output: - The final sum of all generated numbers computed by the consumer threads. # Constraints: - 1 <= N, P <= 10 - 1 <= M <= 100 # Example: ```python N = 2 M = 5 P = 2 # Possible output Final sum computed by consumers: 145 ``` # Instructions: 1. Implement the producer and consumer functions. 2. Create the necessary threading objects and ensure safe synchronization for the shared sum. 3. Use the `queue.Queue` for the shared buffer. 4. Handle all edge cases and ensure the program exits cleanly after all numbers have been processed. # Implementation: ```python import threading import random import queue def producer(shared_queue, count, lock, thread_id): for _ in range(count): num = random.randint(1, 100) with lock: shared_queue.put(num) print(f\\"Producer {thread_id} produced: {num}\\") def consumer(shared_queue, results, count, lock, thread_id): local_sum = 0 for _ in range(count): num = shared_queue.get() with lock: local_sum += num print(f\\"Consumer {thread_id} consumed: {num}\\") with results_lock: results.append(local_sum) def main(N, M, P): shared_queue = queue.Queue() lock = threading.Lock() results = [] results_lock = threading.Lock() # Starting producer threads producers = [] for i in range(N): t = threading.Thread(target=producer, args=(shared_queue, M, lock, i)) t.start() producers.append(t) # Starting consumer threads consumers = [] total_items = N * M items_per_consumer = total_items // P for i in range(P): t = threading.Thread(target=consumer, args=(shared_queue, results, items_per_consumer, lock, i)) t.start() consumers.append(t) # Waiting for all threads to finish for t in producers: t.join() for t in consumers: t.join() final_sum = sum(results) print(f\\"Final sum computed by consumers: {final_sum}\\") # Example call to main function main(2, 5, 2) ``` # Notes: - Ensure that the shared queue is appropriately protected during access to avoid race conditions. - Use thread-safe operations to ensure accurate computation of the final sum.","solution":"import threading import queue import random def producer(shared_queue, count, lock, thread_id): for _ in range(count): num = random.randint(1, 100) shared_queue.put(num) print(f\\"Producer {thread_id} produced: {num}\\") def consumer(shared_queue, results, count, results_lock, thread_id): local_sum = 0 for _ in range(count): num = shared_queue.get() local_sum += num print(f\\"Consumer {thread_id} consumed: {num}\\") with results_lock: results.append(local_sum) def main(N, M, P): shared_queue = queue.Queue() results = [] results_lock = threading.Lock() # Starting producer threads producers = [] for i in range(N): t = threading.Thread(target=producer, args=(shared_queue, M, threading.Lock(), i)) t.start() producers.append(t) # Starting consumer threads consumers = [] total_items = N * M items_per_consumer = total_items // P for i in range(P): t = threading.Thread(target=consumer, args=(shared_queue, results, items_per_consumer, results_lock, i)) t.start() consumers.append(t) # Waiting for all threads to finish for t in producers: t.join() for t in consumers: t.join() final_sum = sum(results) print(f\\"Final sum computed by consumers: {final_sum}\\") return final_sum"},{"question":"# Byte Array Manipulation Objective Implement a Python function that receives a list of strings and performs a series of operations using bytearray objects. The function should: 1. Concatenate strings into a single bytearray. 2. Modify the bytearray by replacing spaces with underscores (\'_\'). 3. Return the resulting bytearray. Function Signature ```python def manipulate_bytearray(strings: List[str]) -> bytearray: ``` Input - `strings` (List[str]): A list of strings to be concatenated and manipulated. Output - Returns a `bytearray` where all spaces in the concatenated string are replaced with underscores. Constraints - Each string in `strings` contains only printable ASCII characters. - The `strings` list will have at least one string and no more than 1000 strings. - The length of all strings combined will not exceed 10,000 characters. # Example ```python from typing import List def manipulate_bytearray(strings: List[str]) -> bytearray: # Convert list of strings into a single bytearray concatenated = bytearray(\'\'.join(strings), \'utf-8\') # Modify bytearray by replacing spaces with underscores for i in range(len(concatenated)): if concatenated[i] == 32: # ASCII code for space concatenated[i] = 95 # ASCII code for underscore return concatenated # Example Usage input_strings = [\\"hello \\", \\"world\\", \\" this is\\", \\" a test\\"] result = manipulate_bytearray(input_strings) print(result) # Output: bytearray(b\'hello_world_this_is_a_test\') ``` Explanation 1. The function first concatenates all strings into a single bytearray. 2. It then iterates through each byte in the bytearray and replaces spaces with underscores. 3. Finally, it returns the modified bytearray containing underscores instead of spaces.","solution":"from typing import List def manipulate_bytearray(strings: List[str]) -> bytearray: Receives a list of strings, concatenates them into a single bytearray, and replaces spaces with underscores. # Concatenate the list of strings into a single string concatenated_str = \'\'.join(strings) # Convert the concatenated string into a bytearray byte_array = bytearray(concatenated_str, \'utf-8\') # Iterate over the bytearray and replace space (ASCII 32) with underscore (ASCII 95) for i in range(len(byte_array)): if byte_array[i] == 32: byte_array[i] = 95 # Return the modified bytearray return byte_array"},{"question":"**Objective:** Write a Python function using pattern matching to filter and transform data based on specific conditions. **Problem Statement:** Pattern matching, introduced in Python 3.10, provides a powerful way to destructure and match complex data structures. You are required to write a function `process_data` that takes a list of tuples as input. Each tuple contains a string and optionally one or two integers. Based on the pattern matching rules, the function should return a list with specific transformations applied to the tuple elements. **Requirements and Specifications:** 1. **Input:** - The function `process_data(data: List[Tuple[str, Union[int, None], Union[int, None]]]) -> List[Union[str, int]]` takes a list of tuples. Each tuple contains: - A string (`str`). - Optionally, one integer (`int`) and optionally two integers (`int`). 2. **Output:** - The function should return a list containing either strings or integers based on the following pattern matching rules: - If the tuple contains a string `s` and exactly one integer `x`, return the integer `x` if it is even, otherwise return the string `s`. - If the tuple contains a string `s`, an integer `x`, and another integer `y`, return the sum of `x` and `y` if their sum is less than 10, otherwise return `\\"Sum too large\\"`. - If the tuple does not match either of these patterns, return `\\"Invalid\\"`. 3. **Constraints and Performance Requirements:** - The length of the list should not exceed 10^4. - Pattern matching should be implemented using the syntax introduced in Python 3.10. **Example:** ```python def process_data(data: List[Tuple[str, Union[int, None], Union[int, None]]]) -> List[Union[str, int]]: # Your implementation here # Example usage: input_data = [ (\\"apple\\", 2, None), (\\"ball\\", 5, None), (\\"cat\\", 3, 6), (\\"dog\\", 4, 7), (\\"elephant\\", None, None) ] output = process_data(input_data) print(output) # Output: [2, \'ball\', 9, \'Sum too large\', \'Invalid\'] ``` Write the function `process_data` to correctly implement the pattern matching rules described above.","solution":"from typing import List, Tuple, Union def process_data(data: List[Tuple[str, Union[int, None], Union[int, None]]]) -> List[Union[str, int]]: results = [] for item in data: match item: case (s, x, None) if isinstance(x, int): if x % 2 == 0: results.append(x) else: results.append(s) case (s, x, y) if isinstance(x, int) and isinstance(y, int): if x + y < 10: results.append(x + y) else: results.append(\\"Sum too large\\") case _: results.append(\\"Invalid\\") return results"},{"question":"**Objective**: To assess the student\'s ability to work with file systems, understand and implement pattern matching using Unix-style \\"glob\\" patterns, and manipulate file lists based on command specifications. **Problem Statement**: You are tasked with implementing a simplified version of a source distribution filter function. Given a list of files and a set of commands (`include`, `exclude`, `recursive-include`, `recursive-exclude`, `global-include`, `global-exclude`, `prune`, `graft`), you will determine a final list of files that match the specified patterns and commands. **Function Signature**: ```python def file_distribution(files: list[str], commands: list[tuple[str, list[str]]]) -> list[str]: pass ``` **Input**: - `files`: A list of file paths (strings). - `commands`: A list of tuples, where the first element is a command (string) and the second element is a list of patterns (strings). **Output**: - A list of file paths (strings) that match the given command specifications. **Constraints**: - All file paths are unique. - Command names are guaranteed to be valid as per the provided command reference. - Patterns will use Unix-style \\"glob\\" patterns. - The order of commands may affect the final file list. **Example**: ```python files = [ \'src/module1.py\', \'src/module2.py\', \'data/data1.csv\', \'data/data2.csv\', \'scripts/install.sh\', \'README.md\' ] commands = [ (\'include\', [\'src/*.py\', \'README.md\']), (\'exclude\', [\'src/module2.py\']), (\'graft\', [\'scripts\']), (\'prune\', [\'data\']) ] assert file_distribution(files, commands) == [\'src/module1.py\', \'README.md\', \'scripts/install.sh\'] ``` **Explanation**: 1. `include` the files matching `src/*.py` and `README.md` -> [\'src/module1.py\', \'src/module2.py\', \'README.md\'] 2. `exclude` the files matching `src/module2.py` -> [\'src/module1.py\', \'README.md\'] 3. `graft` the files under `scripts` -> [\'src/module1.py\', \'README.md\', \'scripts/install.sh\'] 4. `prune` the files under `data` -> Nothing to prune since the files under data were never included. **Notes**: - The function should primarily focus on understanding and implementing the glob patterns and file manipulation logic. - You may use the `glob` module or similar utilities to handle pattern matching. **Tips**: - Start by implementing and testing each command individually. - Ensure that your function handles the command sequence correctly.","solution":"import fnmatch def file_distribution(files: list[str], commands: list[tuple[str, list[str]]]) -> list[str]: included_files = set() for command, patterns in commands: if command == \'include\': for pattern in patterns: for file in files: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif command == \'exclude\': for pattern in patterns: included_files = {file for file in included_files if not fnmatch.fnmatch(file, pattern)} elif command == \'recursive-include\': for pattern in patterns: for file in files: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif command == \'recursive-exclude\': for pattern in patterns: included_files = {file for file in included_files if not fnmatch.fnmatch(file, pattern)} elif command == \'global-include\': for pattern in patterns: for file in files: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif command == \'global-exclude\': for pattern in patterns: included_files = {file for file in included_files if not fnmatch.fnmatch(file, pattern)} elif command == \'prune\': for pattern in patterns: included_files = {file for file in included_files if not file.startswith(pattern)} elif command == \'graft\': for pattern in patterns: for file in files: if file.startswith(pattern): included_files.add(file) return sorted(included_files)"},{"question":"**Task: Efficient Data Manipulation with Memoryview Objects** **Objective:** You need to implement a function that efficiently finds the sum of integers from a large binary data buffer using memoryview objects. This task is designed to test your ability to manipulate large data buffers efficiently in Python using memoryview objects. **Function Signature:** ```python def sum_large_integers(buffer: bytes) -> int: pass ``` **Input:** - `buffer` (bytes): A bytes object containing a large buffer of binary data, where each integer is represented using 4 bytes (standard `int` type). **Output:** - `int`: The sum of the integers represented in the buffer. **Constraints:** - You must use `memoryview` to handle the data buffer efficiently. - The input buffer size will be a multiple of 4 bytes. - Consider the performance and avoid unnecessary data copying. **Example:** ```python buffer = b\'x01x00x00x00x02x00x00x00x03x00x00x00x04x00x00x00\' # The buffer represents the integers [1, 2, 3, 4] result = sum_large_integers(buffer) print(result) # Output should be 10 ``` **Detailed Requirement:** 1. Use the `memoryview` object to create a view of the input buffer. 2. Interpret the buffer as an array of 4-byte integers. 3. Efficiently iterate through the integers and calculate the sum without converting the entire data to a list of integers. 4. Return the computed sum. Implement the function considering these constraints and requirements to demonstrate efficient data manipulation with memoryview objects.","solution":"import struct def sum_large_integers(buffer: bytes) -> int: This function takes a bytes object containing a large buffer of binary data where each integer is represented using 4 bytes, and returns the sum of those integers. total_sum = 0 view = memoryview(buffer) for i in range(0, len(view), 4): total_sum += struct.unpack_from(\'i\', view[i:i+4])[0] return total_sum"},{"question":"Time Manipulator Objective Create a class `TimeManipulator` that carries out various operations using the time module. This class should include the following functionalities: Requirements 1. **Current Time in Different Formats** - Implement a method `current_times()` that returns the current time in three formats: - **UTC** time as a `struct_time` tuple. - **Local** time as a `struct_time` tuple. - **Epoch** time in seconds as a float. 2. **Time Difference Measurement** - Implement a method `time_diff_in_seconds()` that measures the processing time of a provided function. This method should: - Accept a function as an argument. - Return the time taken for the function to execute in seconds as a float. 3. **Formatted Time Strings** - Implement a method `formatted_time_string(timestamp: float, format: str)` that: - Takes a timestamp (seconds since the epoch) and a format string as defined by `strftime`. - Returns the formatted time string. 4. **Sleep Precision Test** - Implement a method `test_sleep_precision(sleep_time: float)` that: - Takes a sleep duration in seconds. - Measures and returns the actual time slept by the system. Constraints - Use appropriate error handling to manage exception cases such as invalid input types or values outside of expected ranges. - Avoid using any additional external libraries other than Python\'s standard library. Input and Output Formats 1. **Method: `current_times()`** - **Output:** Tuple containing (`utc_time`, `local_time`, `epoch_time`) - `utc_time`: `time.struct_time` - `local_time`: `time.struct_time` - `epoch_time`: `float` - current epoch time. 2. **Method: `time_diff_in_seconds(func)`** - **Input:** `func` - A function (without arguments) to measure execution time. - **Output:** `float` - Time taken to execute the function in seconds. 3. **Method: `formatted_time_string(timestamp: float, format: str)`** - **Input:** - `timestamp`: `float` - Epoch time to format. - `format`: `str` - Format string for output. - **Output:** `str` - Formatted time string. 4. **Method: `test_sleep_precision(sleep_time: float)`** - **Input:** `sleep_time` - Sleep duration in seconds as `float`. - **Output:** `float` - Actual sleep duration measured in seconds. Example Usage ```python from time import strftime, sleep class TimeManipulator: def current_times(): ... def time_diff_in_seconds(func): ... def formatted_time_string(timestamp, format): ... def test_sleep_precision(sleep_time): ... # Example usage: tm = TimeManipulator() # Get current times current_times = tm.current_times() print(current_times) # Measure execution time of a function def example_function(): sleep(2) exec_time = tm.time_diff_in_seconds(example_function) print(exec_time) # Get formatted time string formatted_time = tm.formatted_time_string(time() , \\"%Y-%m-%d %H:%M:%S\\") print(formatted_time) # Test sleep precision actual_sleep_time = tm.test_sleep_precision(2.5) print(actual_sleep_time) ``` **Note:** The actual implementation of the `TimeManipulator` class methods should ensure the correctness and robustness by handling possible errors or edge cases.","solution":"import time class TimeManipulator: @staticmethod def current_times(): Returns the current time in UTC, local time, and epoch format. utc_time = time.gmtime() local_time = time.localtime() epoch_time = time.time() return (utc_time, local_time, epoch_time) @staticmethod def time_diff_in_seconds(func): Measures the execution time of the provided function. start_time = time.time() func() end_time = time.time() return end_time - start_time @staticmethod def formatted_time_string(timestamp, format): Returns a formatted time string for the given timestamp and format string. if not isinstance(timestamp, (float, int)) or not isinstance(format, str): raise ValueError(\\"Invalid input types. Expected float for timestamp and str for format.\\") return time.strftime(format, time.localtime(timestamp)) @staticmethod def test_sleep_precision(sleep_time): Measures the actual sleep duration. if not isinstance(sleep_time, (float, int)) or sleep_time < 0: raise ValueError(\\"Invalid sleep time. Must be a non-negative float or int.\\") start_time = time.time() time.sleep(sleep_time) end_time = time.time() return end_time - start_time"},{"question":"You are required to build a comprehensive system information retriever using the `platform` module in Python. This retriever should gather and display details about the system, Python environment, and platform-specific characteristics. # Task Implement a function `get_system_info()` that returns a dictionary containing the following information about the system: 1. **Architecture**: - Keys: `bits` and `linkage` - Values: Corresponding outputs from `platform.architecture()` 2. **Machine Type**: - Key: `machine` - Value: Output from `platform.machine()` 3. **Network Name**: - Key: `node` - Value: Output from `platform.node()` 4. **Platform Details**: - Key: `platform` - Value: Output from `platform.platform()` 5. **Processor Name**: - Key: `processor` - Value: Output from `platform.processor()` 6. **Python Build Information**: - Keys: `python_build_no` and `python_build_date` - Values: Corresponding elements from the tuple returned by `platform.python_build()` 7. **Python Compiler**: - Key: `python_compiler` - Value: Output from `platform.python_compiler()` 8. **Python Implementation**: - Key: `python_implementation` - Value: Output from `platform.python_implementation()` 9. **Python Version**: - Key: `python_version` - Value: Output from `platform.python_version()` 10. **Operating System Details**: - Keys: `system`, `release`, `version` - Values: Outputs from `platform.system()`, `platform.release()`, and `platform.version()` # Input The function `get_system_info()` does not take any inputs. # Output The function returns a dictionary with the specified keys and values as described above. # Example: ```python def get_system_info(): import platform import sys info = { \\"bits\\": platform.architecture()[0], \\"linkage\\": platform.architecture()[1], \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build_no\\": platform.python_build()[0], \\"python_build_date\\": platform.python_build()[1], \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"system\\": platform.system(), \\"release\\": platform.release(), \\"version\\": platform.version(), } return info # Example usage system_info = get_system_info() print(system_info) ``` # Constraints - Assume the code will be executed in a controlled environment where the `platform` module functions can retrieve the necessary details. - The function must handle exceptions gracefully and ensure any missed information is represented as an empty string in the dictionary. # Performance Requirements - The function should operate efficiently and retrieve system information within a reasonable time frame, as it primarily performs string-based queries without intensive computation.","solution":"def get_system_info(): import platform info = { \\"bits\\": platform.architecture()[0], \\"linkage\\": platform.architecture()[1], \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build_no\\": platform.python_build()[0], \\"python_build_date\\": platform.python_build()[1], \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"system\\": platform.system(), \\"release\\": platform.release(), \\"version\\": platform.version(), } return info"},{"question":"Objective: To assess the students\' understanding of data plotting with pandas and their ability to create and customize visualizations using the pandas library. Problem Statement: You are provided with a DataFrame containing sales data for a fictional company over a period of time. The DataFrame contains the following columns: - `Date`: The date of the record (daily frequency). - `Product_A_Sales`: Sales of Product A. - `Product_B_Sales`: Sales of Product B. - `Product_C_Sales`: Sales of Product C. From this sales data, you are required to perform the following tasks: 1. Plot the cumulative sales for each product over time using a line plot. 2. Create a bar plot showing the average monthly sales for each product. 3. Plot a histogram of daily sales for `Product_A_Sales` showing the distribution of its sales. 4. Generate a scatter plot comparing `Product_A_Sales` and `Product_B_Sales`, with the size of the scatter points representing `Product_C_Sales`. 5. Create a pie chart showing the total sales proportion of each product. Constraints: - The plots should have appropriate titles, labels for the x and y axes, and a legend where applicable. - The color schemes for the plots should be chosen such that the data points are clearly distinguishable. - Ensure that missing data, if any, are handled accordingly in the plots. Input: A CSV file named `sales_data.csv` with the following structure: ``` Date,Product_A_Sales,Product_B_Sales,Product_C_Sales 2023-01-01,100,200,150 2023-01-02,110,220,140 ... ``` Output: Generate and save the following plots: 1. Line plot: `cumulative_sales_plot.png` 2. Bar plot: `average_monthly_sales_plot.png` 3. Histogram plot: `product_a_sales_distribution.png` 4. Scatter plot: `product_sales_scatter_plot.png` 5. Pie chart: `total_sales_proportion.png` Implementation: Implement the function `create_sales_plots(csv_file_path: str) -> None`: ```python def create_sales_plots(csv_file_path: str) -> None: import pandas as pd import matplotlib.pyplot as plt # Load the sales data from the CSV file sales_data = pd.read_csv(csv_file_path, parse_dates=[\'Date\'], index_col=\'Date\') # 1. Plot the cumulative sales for each product over time using a line plot cumulative_sales = sales_data.cumsum() plt.figure() cumulative_sales.plot(title=\'Cumulative Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Cumulative Sales\') plt.savefig(\'cumulative_sales_plot.png\') plt.close() # 2. Create a bar plot showing the average monthly sales for each product monthly_sales = sales_data.resample(\'M\').mean() plt.figure() monthly_sales.plot(kind=\'bar\', title=\'Average Monthly Sales\') plt.xlabel(\'Month\') plt.ylabel(\'Average Sales\') plt.savefig(\'average_monthly_sales_plot.png\') plt.close() # 3. Plot a histogram of daily sales for Product_A_Sales plt.figure() sales_data[\'Product_A_Sales\'].plot.hist(title=\'Product A Sales Distribution\', bins=20, alpha=0.7) plt.xlabel(\'Sales\') plt.ylabel(\'Frequency\') plt.savefig(\'product_a_sales_distribution.png\') plt.close() # 4. Generate a scatter plot comparing Product A Sales and Product B Sales plt.figure() scatter_plot = sales_data.plot.scatter(x=\'Product_A_Sales\', y=\'Product_B_Sales\', s=sales_data[\'Product_C_Sales\'], alpha=0.5) plt.title(\'Product A Sales vs Product B Sales\') plt.xlabel(\'Product A Sales\') plt.ylabel(\'Product B Sales\') plt.savefig(\'product_sales_scatter_plot.png\') plt.close() # 5. Create a pie chart showing the total sales proportion of each product total_sales = sales_data.sum() plt.figure() total_sales.plot.pie(title=\'Total Sales Proportion\', autopct=\'%1.1f%%\') plt.ylabel(\'\') plt.savefig(\'total_sales_proportion.png\') plt.close() ``` Notes: - Make sure to handle the date parsing and set the `Date` column as the index when loading the data. - Save the generated plots with the specified filenames.","solution":"def create_sales_plots(csv_file_path: str) -> None: import pandas as pd import matplotlib.pyplot as plt # Load the sales data from the CSV file sales_data = pd.read_csv(csv_file_path, parse_dates=[\'Date\'], index_col=\'Date\') # 1. Plot the cumulative sales for each product over time using a line plot cumulative_sales = sales_data.cumsum() plt.figure() cumulative_sales.plot(title=\'Cumulative Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Cumulative Sales\') plt.savefig(\'cumulative_sales_plot.png\') plt.close() # 2. Create a bar plot showing the average monthly sales for each product monthly_sales = sales_data.resample(\'M\').mean() plt.figure() monthly_sales.plot(kind=\'bar\', title=\'Average Monthly Sales\') plt.xlabel(\'Month\') plt.ylabel(\'Average Sales\') plt.savefig(\'average_monthly_sales_plot.png\') plt.close() # 3. Plot a histogram of daily sales for Product_A_Sales plt.figure() sales_data[\'Product_A_Sales\'].plot.hist(title=\'Product A Sales Distribution\', bins=20, alpha=0.7) plt.xlabel(\'Sales\') plt.ylabel(\'Frequency\') plt.savefig(\'product_a_sales_distribution.png\') plt.close() # 4. Generate a scatter plot comparing Product A Sales and Product B Sales plt.figure() scatter_plot = sales_data.plot.scatter(x=\'Product_A_Sales\', y=\'Product_B_Sales\', s=sales_data[\'Product_C_Sales\'], alpha=0.5) plt.title(\'Product A Sales vs Product B Sales\') plt.xlabel(\'Product A Sales\') plt.ylabel(\'Product B Sales\') plt.savefig(\'product_sales_scatter_plot.png\') plt.close() # 5. Create a pie chart showing the total sales proportion of each product total_sales = sales_data.sum() plt.figure() total_sales.plot.pie(title=\'Total Sales Proportion\', autopct=\'%1.1f%%\') plt.ylabel(\'\') plt.savefig(\'total_sales_proportion.png\') plt.close()"},{"question":"# Custom Reference Counting in Python In Python, the lifecycle of an object is managed through reference counting. You are required to implement a custom reference counting class `RefCountedObject` that mimics the behavior of the reference counting functions described in the provided documentation. Requirements: 1. Implement a class `RefCountedObject` that manages its reference count. 2. The class should have the following methods: - `__init__(self, value)`: Initialize the object with a given value and set the reference count to 1. - `incref(self)`: Increment the reference count. - `decref(self)`: Decrement the reference count and delete the object if the count reaches zero. - `get_ref_count(self)`: Return the current reference count of the object. 3. Implement a context manager using the `__enter__` and `__exit__` methods to increment and decrement the reference count automatically when entering and exiting a context. Constraints: - Ensure that the reference count is always a positive integer. - Implement proper error handling for edge cases (e.g., decrementing the reference count below zero should raise an error). Performance Requirements: - The solution should be efficient in terms of time complexity for incrementing and decrementing the reference count (O(1) operations). Example Usage: ```python class RefCountedObject: def __init__(self, value): # Implement the initialization method here pass def incref(self): # Implement the increment reference count method here pass def decref(self): # Implement the decrement reference count method here pass def get_ref_count(self): # Implement the get reference count method here pass def __enter__(self): # Implement enter method here pass def __exit__(self, exc_type, exc_val, exc_tb): # Implement exit method here pass # Example usage obj = RefCountedObject(\\"Hello, world!\\") print(obj.get_ref_count()) # Output: 1 obj.incref() print(obj.get_ref_count()) # Output: 2 obj.decref() print(obj.get_ref_count()) # Output: 1 with obj: print(obj.get_ref_count()) # Output: 2 print(obj.get_ref_count()) # Output: 1 obj.decref() # This should delete the object if the ref count reaches zero ```","solution":"class RefCountedObject: def __init__(self, value): self.value = value self._ref_count = 1 def incref(self): self._ref_count += 1 def decref(self): if self._ref_count <= 0: raise ValueError(\\"Cannot decrement reference count below zero\\") self._ref_count -= 1 if self._ref_count == 0: del self def get_ref_count(self): return self._ref_count def __enter__(self): self.incref() return self def __exit__(self, exc_type, exc_val, exc_tb): self.decref()"},{"question":"**Coding Assessment Question: PyTorch Elastic Multiprocessing** **Objective**: Implement a multiprocessing solution using PyTorch\'s elastic distributed capabilities that demonstrates your comprehension of the package. **Problem Statement**: You are required to implement a function `parallel_matrix_product` that performs matrix multiplication in parallel using PyTorch\'s elastic multiprocessing. Each worker process will compute a portion of the resultant matrix and combine the results at the end. Specifically, given two matrices `A` and `B`, your task is to compute the matrix product `C = A * B` where: - `A` is of size `MxN` - `B` is of size `NxP` - Resultant matrix `C` will be of size `MxP` **Constraints**: 1. You must use the `torch.distributed.elastic.multiprocessing` package to manage worker processes. 2. Each worker should handle the computation of one row of the resultant matrix `C`. 3. Collect and combine the results from all the worker processes to form the final matrix `C`. **Input**: - Two 2D lists representing matrices `A` (of size `MxN`) and `B` (of size `NxP`). **Output**: - A 2D list representing the resultant matrix `C` (of size `MxP`). **Function Signature**: ```python def parallel_matrix_product(A: List[List[float]], B: List[List[float]]) -> List[List[float]]: pass ``` **Requirements**: 1. Implement the function using PyTorch\'s elastic multiprocessing capabilities. 2. Ensure that the results from all worker processes are correctly combined into the final matrix `C`. 3. Handle edge cases such as mismatched input matrix dimensions gracefully by raising appropriate errors. **Example**: ```python A = [ [1, 2, 3], [4, 5, 6] ] B = [ [7, 8], [9, 10], [11, 12] ] result = parallel_matrix_product(A, B) # Expected output: # [ # [58, 64], # [139, 154] # ] print(result) ``` **Notes**: - You may assume that the input matrices contain only numerical values. - Utilize logging to debug and display information from each worker process. - Pay attention to the performance and scalability of your solution. Good luck!","solution":"import torch import torch.multiprocessing as mp from typing import List def worker(A, B, queue, row_idx): A_row = A[row_idx] result_row = [sum(A_row[k] * B[k][j] for k in range(len(B))) for j in range(len(B[0]))] queue.put((row_idx, result_row)) def parallel_matrix_product(A: List[List[float]], B: List[List[float]]) -> List[List[float]]: M, N = len(A), len(A[0]) N_B, P = len(B), len(B[0]) if N != N_B: raise ValueError(\\"Number of columns in A must be equal to number of rows in B\\") C = [[0] * P for _ in range(M)] manager = mp.Manager() queue = manager.Queue() processes = [] for i in range(M): p = mp.Process(target=worker, args=(A, B, queue, i)) processes.append(p) p.start() for p in processes: p.join() while not queue.empty(): row_idx, result_row = queue.get() C[row_idx] = result_row return C"},{"question":"**Python Coding Assessment Question** **Objective:** Design a Python function to implement marshalling and unmarshalling of Python objects to/from files using a custom serialization format. The focus should be on demonstrating your understanding of object serialization, file handling, and error management. **Question:** You are required to implement two functions – `marshal_object_to_file` and `unmarshal_object_from_file`. 1. **Function 1: marshal_object_to_file** This function will serialize a Python object and save it to a binary file. - **Input:** - `obj (PyObject)`: The Python object to be serialized. - `file_path (str)`: The path to the file where the serialized object will be saved. - **Output:** - None - **Constraints:** - Use Python\'s built-in `marshal` module for serialization. - Ensure the file is opened in binary write mode. - Handle potential exceptions such as file I/O errors. 2. **Function 2: unmarshal_object_from_file** This function will deserialize a Python object from a binary file. - **Input:** - `file_path (str)`: The path to the file from which the object is to be deserialized. - **Output:** - A Python object that was deserialized from the file. - **Constraints:** - Use Python\'s built-in `marshal` module for deserialization. - Ensure the file is opened in binary read mode. - Handle potential exceptions such as file I/O errors and invalid data format errors. **Performance Requirements:** - The serialization and deserialization processes should be efficient in terms of time complexity. Your solution should accommodate large files and complex objects. - Ensure your implementation is robust and can handle erroneous input gracefully. **Example Usage:** ```python import os def test_marshal_unmarshal(): data = {\\"name\\": \\"Alice\\", \\"age\\": 30, \\"scores\\": [90, 85, 88]} file_path = \\"temp_marshal_test.dat\\" # Marshal the object to a file marshal_object_to_file(data, file_path) # Unmarshal the object from the file result = unmarshal_object_from_file(file_path) # Validation assert result == data, \\"Test failed: The original and unmarshalled data do not match.\\" # Cleanup if os.path.exists(file_path): os.remove(file_path) print(\\"Test passed!\\") test_marshal_unmarshal() ``` This script tests the functionality of both functions by: 1. Creating a sample data dictionary. 2. Marshalling the dictionary to a file. 3. Unmarshalling the data back from the file. 4. Validating that the original and unmarshalled data match. 5. Cleaning up the temporary file created during the process. **Note:** Ensure you include appropriate error handling to manage scenarios such as file not found, read/write errors, and invalid serialization data. **Good luck!**","solution":"import marshal def marshal_object_to_file(obj, file_path): Serializes a Python object and saves it to a binary file. Parameters: obj (PyObject): The Python object to be serialized. file_path (str): The path to the file where the serialized object will be saved. Returns: None try: with open(file_path, \'wb\') as file: marshal.dump(obj, file) except Exception as e: print(f\\"Error while marshalling object to file: {e}\\") def unmarshal_object_from_file(file_path): Deserializes a Python object from a binary file. Parameters: file_path (str): The path to the file from which the object is to be deserialized. Returns: PyObject: The deserialized Python object. try: with open(file_path, \'rb\') as file: return marshal.load(file) except Exception as e: print(f\\"Error while unmarshalling object from file: {e}\\") return None"},{"question":"# Question: Customizing Plot Aesthetics with Seaborn You are tasked with visualizing a complex dataset. You are provided with a dataset containing temperatures (in Celsius) over different months across several years. Your goal is to create a series of plots that showcase how seaborn can be used to enhance the visualization for different audiences. Requirements: 1. **Data Import and Preparation**: Load a dataset containing columns \'Year\', \'Month\', and \'Temperature\'. For demonstration purposes, you can simulate this data using numpy or pandas. 2. **Plot Creation**: - Create a line plot representing the average monthly temperature for each year. - Apply the `darkgrid` theme to the plot. 3. **Customize Styles**: - Create a grid of subplots (2x2). Each subplot should depict the temperature trend for a different year using a different seaborn theme (`darkgrid`, `whitegrid`, `dark`, `white`). - Ensure the styles differ and are applied only to their respective subplot using the `with` statement. 4. **Aesthetic Customization**: - Use the function `sns.despine()` to remove the top and right axes spines on all plots. - In one of the subplots, further customize the theme by changing the face color of the axes using the `rc` argument in `set_style`. 5. **Context Modifications**: - Modify the context of one of the subplots to `talk`. - Change the font scale and the line widths in another subplot using the `rc` parameter in `set_context`. Function Implementation: Implement a function `custom_plot(data: pd.DataFrame) -> None` that takes a pandas DataFrame as input and generates the described series of plots. ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_plot(data: pd.DataFrame) -> None: # Implement the function based on the requirements pass # Example data simulation data = pd.DataFrame({ \'Year\': np.tile(np.arange(2015, 2020), 12), \'Month\': np.repeat(np.arange(1, 13), 5), \'Temperature\': np.random.normal(20, 5, 60) }) custom_plot(data) ``` Expected Result: Your function should generate a grid of 2x2 subplots, with each subplot styled according to the requirements. The spines should be removed where noted, and the overall styles and contexts modified appropriately. Submission: Submit the complete function definition along with any necessary imports. Ensure your code is well-documented and clear.","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_plot(data: pd.DataFrame) -> None: plt.figure(figsize=(12, 10)) years = data[\'Year\'].unique() with sns.axes_style(\\"darkgrid\\"): plt.subplot(2, 2, 1) sns.lineplot(data=data, x=\'Month\', y=\'Temperature\', hue=\'Year\') sns.despine() plt.title(\\"Average Monthly Temperature (Darkgrid)\\") with sns.axes_style(\\"whitegrid\\"): plt.subplot(2, 2, 2) data_year = data[data[\'Year\'] == years[0]] sns.lineplot(data=data_year, x=\'Month\', y=\'Temperature\') sns.despine() plt.title(f\\"Year {years[0]} (Whitegrid)\\") with sns.axes_style(\\"dark\\"): plt.subplot(2, 2, 3) data_year = data[data[\'Year\'] == years[1]] sns.lineplot(data=data_year, x=\'Month\', y=\'Temperature\') sns.despine() plt.title(f\\"Year {years[1]} (Dark)\\") with sns.axes_style(\\"white\\"): plt.subplot(2, 2, 4) data_year = data[data[\'Year\'] == years[2]] sns.lineplot(data=data_year, x=\'Month\', y=\'Temperature\') sns.despine() plt.title(f\\"Year {years[2]} (White)\\") # Additional customization for context and rc parameters with sns.plotting_context(\\"talk\\"): plt.subplot(2, 2, 2) data_year = data[data[\'Year\'] == years[3]] sns.lineplot(data=data_year, x=\'Month\', y=\'Temperature\') sns.despine() plt.title(f\\"Year {years[3]} (Talk Context)\\") with sns.plotting_context(rc={\\"font.size\\":12,\\"axes.titlesize\\":12,\\"axes.labelsize\\":10,\\"lines.linewidth\\":2.5}): plt.subplot(2, 2, 3) data_year = data[data[\'Year\'] == years[1]] sns.lineplot(data=data_year, x=\'Month\', y=\'Temperature\') sns.despine() plt.title(f\\"Year {years[1]} (Custom Context)\\") plt.tight_layout() plt.show() # Example data simulation data = pd.DataFrame({ \'Year\': np.tile(np.arange(2015, 2020), 12), \'Month\': np.repeat(np.arange(1, 13), 5), \'Temperature\': np.random.normal(20, 5, 60) }) custom_plot(data)"},{"question":"Implementing and Comparing SVM Classifiers with Custom Kernels and Parameter Tuning Objective You are required to demonstrate your understanding of Support Vector Machines (SVMs) using `scikit-learn` by performing classification on a given dataset. This exercise will test your skills in implementing multiple SVM classifiers, using custom kernels, and tuning essential parameters for optimal performance. Problem Statement 1. **Dataset Preparation:** - Load the [Iris dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) using `sklearn.datasets.load_iris`. 2. **Model Implementation:** - Implement two SVM classifiers: 1. **Classifier 1:** Use a polynomial kernel. 2. **Classifier 2:** Use a custom kernel defined as the cosine similarity between input vectors. 3. **Parameter Tuning:** - Use `GridSearchCV` to perform parameter tuning on the polynomial kernel SVM. Consider the following parameters: - `C`: [0.1, 1, 10] - `gamma`: [0.01, 0.1, 1, 10] - `degree`: [2, 3, 4] 4. **Model Evaluation:** - Evaluate the performance of both classifiers using cross-validation and report the following metrics: - Accuracy - Precision - Recall - F1-score Constraints - You must use `sklearn` for loading the dataset, implementing SVMs, parameter tuning, and evaluating performance metrics. - You must define the custom kernel for the cosine similarity. - The results should be presented in a structured format. Input and Output Formats # Function Signature ```python def evaluate_svm_classifiers(): # Your code here ``` # Expected Workflow 1. Load the Iris dataset. 2. Define the custom kernel for cosine similarity. 3. Implement SVM with a polynomial kernel. 4. Implement SVM with the custom kernel. 5. Perform parameter tuning on the polynomial kernel SVM using `GridSearchCV`. 6. Evaluate both models using cross-validation. 7. Print the evaluation metrics. Example Output ``` Polynomial Kernel SVM - Best Parameters: {\'C\': 1, \'gamma\': 0.1, \'degree\': 3} Polynomial Kernel SVM - Cross-Validation Performance: Accuracy: 0.96 Precision: 0.95 Recall: 0.96 F1-score: 0.95 Custom Kernel SVM (Cosine Similarity) - Cross-Validation Performance: Accuracy: 0.94 Precision: 0.93 Recall: 0.94 F1-score: 0.93 ``` **Note:** The above values are for illustration purposes only. The actual values may vary based on the implementation and dataset splits. Evaluation Criteria - Correctness of SVM implementation. - Proper definition and use of the custom kernel. - Effective use of `GridSearchCV` for parameter tuning. - Accurate calculation and reporting of performance metrics.","solution":"import numpy as np from sklearn import datasets from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from sklearn.svm import SVC from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline def cosine_similarity_kernel(X, Y): Custom kernel that computes the cosine similarity between the input vectors. X_normalized = X / np.linalg.norm(X, axis=1, keepdims=True) Y_normalized = Y / np.linalg.norm(Y, axis=1, keepdims=True) return np.dot(X_normalized, Y_normalized.T) def evaluate_svm_classifiers(): # Load Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # SVM with polynomial kernel poly_svm = SVC(kernel=\'poly\') param_grid = { \'C\': [0.1, 1, 10], \'gamma\': [0.01, 0.1, 1, 10], \'degree\': [2, 3, 4] } grid_search = GridSearchCV(poly_svm, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_scaled, y) best_params = grid_search.best_params_ # Evaluate using cross-validation poly_svm_best = SVC(kernel=\'poly\', C=best_params[\'C\'], gamma=best_params[\'gamma\'], degree=best_params[\'degree\']) skf = StratifiedKFold(n_splits=5) poly_accuracy = cross_val_score(poly_svm_best, X_scaled, y, cv=skf, scoring=\'accuracy\').mean() poly_precision = cross_val_score(poly_svm_best, X_scaled, y, cv=skf, scoring=\'precision_macro\').mean() poly_recall = cross_val_score(poly_svm_best, X_scaled, y, cv=skf, scoring=\'recall_macro\').mean() poly_f1 = cross_val_score(poly_svm_best, X_scaled, y, cv=skf, scoring=\'f1_macro\').mean() # SVM with custom cosine similarity kernel custom_svm = SVC(kernel=cosine_similarity_kernel) custom_accuracy = cross_val_score(custom_svm, X_scaled, y, cv=skf, scoring=\'accuracy\').mean() custom_precision = cross_val_score(custom_svm, X_scaled, y, cv=skf, scoring=\'precision_macro\').mean() custom_recall = cross_val_score(custom_svm, X_scaled, y, cv=skf, scoring=\'recall_macro\').mean() custom_f1 = cross_val_score(custom_svm, X_scaled, y, cv=skf, scoring=\'f1_macro\').mean() print(f\\"Polynomial Kernel SVM - Best Parameters: {best_params}\\") print(f\\"Polynomial Kernel SVM - Cross-Validation Performance:\\") print(f\\" Accuracy: {poly_accuracy:.2f}\\") print(f\\" Precision: {poly_precision:.2f}\\") print(f\\" Recall: {poly_recall:.2f}\\") print(f\\" F1-score: {poly_f1:.2f}\\") print(f\\"Custom Kernel SVM (Cosine Similarity) - Cross-Validation Performance:\\") print(f\\" Accuracy: {custom_accuracy:.2f}\\") print(f\\" Precision: {custom_precision:.2f}\\") print(f\\" Recall: {custom_recall:.2f}\\") print(f\\" F1-score: {custom_f1:.2f}\\")"},{"question":"# Advanced Coding Challenge: PyTorch CUDA Graph Trees **Objective**: Suppose you are given a task to optimize the computation of specific operations using PyTorch\'s CUDA Graph Trees. You need to implement a function that leverages CUDA Graph Trees to accelerate a series of CUDA kernel operations. Your implementation should handle different execution paths, ensuring that memory is efficiently managed according to the guidelines provided. Task Implementation 1. **Function Definition**: - Define a function `optimize_operations` that accepts a tensor `x` as an input and performs a series of CUDA operations with two possible execution paths. 2. **Execution Path**: - Compute `y = x * x * x` - Based on the sum of elements in `y`, choose an execution path: - **Path 1**: If `y.sum() > 0`, compute `z = y ** y` - **Path 2**: Otherwise, compute `z = (y.abs() ** y.abs())`. 3. **Optimization with CUDA Graph Trees**: - Implement CUDA Graph Trees with memory pooling to ensure that the operations are optimized. Use the provided context on CUDAGraph Trees to reuse memory and ensure efficient replay of recorded graphs. 4. **Return Output**: - After conditional paths, compute the final output `result = z * torch.rand_like(z)` and return it. Constraints - Ensure that your implementation handles and respects the memory pool requirements as described in the documentation. - Emphasize using CUDA Graphs to reduce overhead, particularly consolidating multiple CUDA kernel invocations. - Take care to appropriately manage paths and avoid common pitfalls like accessing overwritten tensor outputs. Example ```python import torch @torch.compile(mode=\\"reduce-overhead\\") def optimize_operations(x): # Your implementation here pass # Example usage input_tensor = torch.randn(10, 10, device=\\"cuda\\") output = optimize_operations(input_tensor) print(output) ``` Submission - Submit your `optimize_operations` function code, ensuring it adheres to the constraints and efficiently uses CUDA Graph Trees for performance optimization. - Provide a brief explanation about how your implementation leverages PyTorch’s CUDA Graphs, mentioning any specific strategies or considerations you adhered to, as described in the provided documentation.","solution":"import torch def optimize_operations(x): Optimize CUDA operations using CUDA Graph Trees. Args: x (torch.Tensor): Input tensor on CUDA. Returns: torch.Tensor: Result tensor after optimized operations. assert x.is_cuda, \\"Input tensor must be on CUDA\\" # Step 1: Compute y = x * x * x y = x * x * x # Step 2: Choose execution path based on sum of elements in y if y.sum() > 0: z = y ** y else: z = (y.abs() ** y.abs()) # Step 3: Final output computation result = z * torch.rand_like(z) return result # Example usage if __name__ == \\"__main__\\": input_tensor = torch.randn(10, 10, device=\\"cuda\\") output = optimize_operations(input_tensor) print(output) # Note: For more advanced usage, incorporating CUDA Graph Trees, PyTorch\'s AOTAutograd, or Torchscript # with torch.jit can be explored in a real-world scenario."},{"question":"# Problem Description You have been given a collection of file paths and your task is to write a function `organize_paths(base_dir, file_paths)` that processes these paths and returns a structured dictionary representing the hierarchy of directories. The input paths might contain relative paths, symbolic links, and various inconsistencies such as redundant separators or up-level references (`..`). Your function should normalize these paths, resolve symbolic links, and then organize them into a nested dictionary based on their hierarchy relative to the given `base_dir`. # Function Signature ```python import os def organize_paths(base_dir: str, file_paths: list[str]) -> dict: pass ``` # Input 1. `base_dir` (str): The base directory from which all paths should be considered. This will be an absolute path. 2. `file_paths` (list of str): A list of file paths. These paths can be relative or absolute, and may include symbolic links, redundant separators, or up-level references. # Output A dictionary representing the hierarchical structure of directories relative to `base_dir`. Each key in the dictionary represents a directory, and its corresponding value is either another dictionary representing subdirectories or `None` indicating that the key is a file. # Example ```python base_dir = \\"/home/user/projects\\" file_paths = [ \\"/home/user/projects/docs/report.txt\\", \\"/home/user/projects/src/module.py\\", \\"../projects/config/settings.conf\\", \\"docs/spec.md\\", \\"./templates/template.html\\" ] result = organize_paths(base_dir, file_paths) # Expected output { \\"docs\\": { \\"report.txt\\": None, \\"spec.md\\": None }, \\"src\\": { \\"module.py\\": None }, \\"config\\": { \\"settings.conf\\": None }, \\"templates\\": { \\"template.html\\": None } } ``` # Constraints 1. `base_dir` is always an absolute path. 2. Paths in `file_paths` can be absolute or relative. 3. Your function should handle symbolic links by resolving them to their target paths. 4. Normalize all path components to their canonical forms. 5. Assume that the paths correspond to existing files and directories on the system. 6. Do not perform any file or directory creation or modification in the filesystem. # Notes - Use functions from the `os.path` module to manipulate and process paths. - Focus on clean and efficient code, as the number of paths can be large.","solution":"import os def organize_paths(base_dir: str, file_paths: list[str]) -> dict: def insert_path(dic, parts): if len(parts) == 1: dic[parts[0]] = None else: if parts[0] not in dic: dic[parts[0]] = {} insert_path(dic[parts[0]], parts[1:]) organized_dict = {} for path in file_paths: normalized_path = os.path.normpath(os.path.join(base_dir, path)) relative_path = os.path.relpath(normalized_path, base_dir) path_parts = relative_path.split(os.sep) insert_path(organized_dict, path_parts) return organized_dict"},{"question":"# Data Analysis and Manipulation using Pandas You are provided with two CSV files, `employees.csv` and `salaries.csv`. These files contain information about employees in a company and their respective salaries. Your task is to perform several data analysis and manipulation operations using pandas. * `employees.csv`: - Columns: `employee_id`, `name`, `department_id`, `hire_date` * `salaries.csv`: - Columns: `employee_id`, `salary`, `effective_date` **Step 1: Data Loading** 1. Implement a function `load_data()` that reads these CSV files into pandas DataFrames. ```python def load_data(): Load the employees and salaries data from CSV files into pandas DataFrames. Returns: - employees_df: DataFrame containing employees data - salaries_df: DataFrame containing salaries data pass ``` **Step 2: Merge Data** 2. Implement a function `merge_data(employees_df, salaries_df)` that merges these DataFrames on the `employee_id` column. ```python def merge_data(employees_df, salaries_df): Merge employees and salaries DataFrames on the employee_id column. Arguments: - employees_df: DataFrame containing employees data - salaries_df: DataFrame containing salaries data Returns: - merged_df: Merged DataFrame pass ``` **Step 3: Data Cleaning** 3. Implement a function `clean_data(df)` to handle missing data: - Fill missing `department_id` with 0. - Fill missing `salary` with the average salary of the respective department. ```python def clean_data(df): Clean the merged DataFrame by handling missing data. Arguments: - df: Merged DataFrame Returns: - cleaned_df: Cleaned DataFrame with missing data handled pass ``` **Step 4: Group-by Analysis** 4. Implement a function `group_by_department(cleaned_df)` that groups the data by `department_id` and calculates the average salary and the number of employees in each department. ```python def group_by_department(cleaned_df): Group data by department_id and calculate the average salary and number of employees in each department. Arguments: - cleaned_df: Cleaned DataFrame Returns: - grouped_df: DataFrame with department_id, average_salary, and employee_count pass ``` **Step 5: Advanced Analysis** 5. Implement a function `salary_trend(cleaned_df)` that calculates the salary trend for each employee based on the `effective_date`. ```python def salary_trend(cleaned_df): Calculate the salary trend for each employee based on the effective_date. Arguments: - cleaned_df: Cleaned DataFrame Returns: - trend_df: DataFrame with employee_id and trend analysis (e.g., increasing, decreasing, stable) pass ``` **Input Format:** - Two CSV files: `employees.csv`, `salaries.csv` **Output Format:** - Several DataFrames resulting from the operations. **Constraints:** - Ensure that your solution is efficient in terms of time and space complexity. - Assume the CSV files have a large number of records. **Evaluation Criteria:** - Correct and efficient implementation of each step. - Proper handling of missing data. - Accuracy of group-by and trend analysis. - Code readability and adherence to pandas best practices.","solution":"import pandas as pd def load_data(): Load the employees and salaries data from CSV files into pandas DataFrames. Returns: - employees_df: DataFrame containing employees data - salaries_df: DataFrame containing salaries data employees_df = pd.read_csv(\'employees.csv\') salaries_df = pd.read_csv(\'salaries.csv\') return employees_df, salaries_df def merge_data(employees_df, salaries_df): Merge employees and salaries DataFrames on the employee_id column. Arguments: - employees_df: DataFrame containing employees data - salaries_df: DataFrame containing salaries data Returns: - merged_df: Merged DataFrame merged_df = pd.merge(employees_df, salaries_df, on=\'employee_id\', how=\'left\') return merged_df def clean_data(df): Clean the merged DataFrame by handling missing data. Arguments: - df: Merged DataFrame Returns: - cleaned_df: Cleaned DataFrame with missing data handled df[\'department_id\'].fillna(0, inplace=True) average_salary = df.groupby(\'department_id\')[\'salary\'].transform(\'mean\') df[\'salary\'].fillna(average_salary, inplace=True) return df def group_by_department(cleaned_df): Group data by department_id and calculate the average salary and number of employees in each department. Arguments: - cleaned_df: Cleaned DataFrame Returns: - grouped_df: DataFrame with department_id, average_salary, and employee_count grouped_df = cleaned_df.groupby(\'department_id\').agg( average_salary=(\'salary\', \'mean\'), employee_count=(\'employee_id\', \'count\') ).reset_index() return grouped_df def salary_trend(cleaned_df): Calculate the salary trend for each employee based on the effective_date. Arguments: - cleaned_df: Cleaned DataFrame Returns: - trend_df: DataFrame with employee_id and trend analysis (e.g., increasing, decreasing, stable) trend_list = [] for employee_id, grp in cleaned_df.groupby(\'employee_id\'): grp = grp.sort_values(by=\'effective_date\') if len(grp) == 1: trend = \'stable\' elif grp[\'salary\'].is_monotonic_increasing: trend = \'increasing\' elif grp[\'salary\'].is_monotonic_decreasing: trend = \'decreasing\' else: trend = \'unstable\' trend_list.append({\'employee_id\': employee_id, \'trend\': trend}) trend_df = pd.DataFrame(trend_list) return trend_df"},{"question":"# K-Means and DBSCAN Clustering Analysis Introduction In this assignment, you are required to demonstrate your understanding of two popular clustering algorithms provided by scikit-learn: K-Means and DBSCAN. The task includes implementing a solution that applies these clustering methods on a synthetic dataset and evaluates their performance using appropriate clustering evaluation metrics. Dataset We will use a synthetic dataset generated using scikit-learn\'s `make_blobs` function, which will create a dataset with defined clusters. Additionally, we will introduce a few outliers to evaluate how each clustering method handles noise. Task 1. Generate a synthetic dataset with 1000 samples, 3 centers, and a cluster standard deviation of 1.0 using the `make_blobs` function from `sklearn.datasets`. 2. Introduce 50 outliers by creating random samples uniformly distributed within a certain range that covers the area of the generated blobs. 3. Perform clustering on the dataset using both K-Means and DBSCAN. 4. Evaluate and compare the results of the two clustering algorithms using: - Adjusted Rand Index (ARI) - Silhouette Coefficient 5. Visualize the resulting clusters and the outliers for both K-Means and DBSCAN. Requirements - Use `sklearn.cluster.KMeans` and `sklearn.cluster.DBSCAN` for clustering. - Use `sklearn.datasets.make_blobs` to generate the dataset and outliers. - Use `sklearn.metrics.adjusted_rand_score` and `sklearn.metrics.silhouette_score` for evaluation. - Use `matplotlib` to visualize the clusters and outliers. Input Format - The data points will be generated within the code. Output Format - Print the Adjusted Rand Index and Silhouette Coefficient for both K-Means and DBSCAN. - Plot the resulting clusters with a different color for each cluster for both K-Means and DBSCAN. Highlight the outliers if any. Constraints - Ensure that the random points for outliers cover a reasonable range relative to the cluster centers. - Use a fixed random state for reproducibility. Code Template ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score # Step 1: Generate synthetic dataset n_samples = 1000 n_outliers = 50 centers = 3 cluster_std = 1.0 # Generate blobs X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=0) # Introduce outliers np.random.seed(42) X_outliers = np.random.uniform(low=np.min(X), high=np.max(X), size=(n_outliers, X.shape[1])) X = np.vstack((X, X_outliers)) # Step 2: Apply K-Means clustering kmeans = KMeans(n_clusters=centers, random_state=0) y_kmeans = kmeans.fit_predict(X) # Step 3: Apply DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) y_dbscan = dbscan.fit_predict(X) # Step 4: Evaluate clustering results ari_kmeans = adjusted_rand_score(y[:n_samples], y_kmeans[:n_samples]) silhouette_kmeans = silhouette_score(X, y_kmeans) ari_dbscan = adjusted_rand_score(y[:n_samples], y_dbscan[:n_samples]) silhouette_dbscan = silhouette_score(X, y_dbscan) print(f\\"K-Means ARI: {ari_kmeans}, Silhouette Coefficient: {silhouette_kmeans}\\") print(f\\"DBSCAN ARI: {ari_dbscan}, Silhouette Coefficient: {silhouette_dbscan}\\") # Step 5: Visualize the resulting clusters plt.figure(figsize=(12, 6)) # K-Means plot plt.subplot(1, 2, 1) plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap=\'viridis\', marker=\'o\') plt.title(\\"K-Means Clustering\\") # DBSCAN plot plt.subplot(1, 2, 2) plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap=\'viridis\', marker=\'o\') outliers = (y_dbscan == -1) plt.scatter(X[outliers, 0], X[outliers, 1], c=\'red\', marker=\'x\') plt.title(\\"DBSCAN Clustering\\") plt.show() ``` Explanation - The code generates a synthetic dataset and introduces outliers to simulate noise. - K-Means and DBSCAN clustering algorithms are applied to the dataset. - Clustering performance is evaluated using Adjusted Rand Index and Silhouette Coefficient. - The resulting clusters are visualized using scatter plots for both K-Means and DBSCAN. Outliers are highlighted in the DBSCAN plot.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.cluster import KMeans, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score def kmeans_dbscan_clustering(): Perform K-Means and DBSCAN clustering on a synthetic dataset, evaluate their performance, and visualize the results. # Step 1: Generate synthetic dataset n_samples = 1000 n_outliers = 50 centers = 3 cluster_std = 1.0 # Generate blobs X, y = make_blobs(n_samples=n_samples, centers=centers, cluster_std=cluster_std, random_state=0) # Introduce outliers np.random.seed(42) X_outliers = np.random.uniform(low=np.min(X), high=np.max(X), size=(n_outliers, X.shape[1])) X = np.vstack((X, X_outliers)) # Step 2: Apply K-Means clustering kmeans = KMeans(n_clusters=centers, random_state=0) y_kmeans = kmeans.fit_predict(X) # Step 3: Apply DBSCAN clustering dbscan = DBSCAN(eps=0.5, min_samples=5) y_dbscan = dbscan.fit_predict(X) # Step 4: Evaluate clustering results ari_kmeans = adjusted_rand_score(y[:n_samples], y_kmeans[:n_samples]) silhouette_kmeans = silhouette_score(X, y_kmeans) ari_dbscan = adjusted_rand_score(y[:n_samples], y_dbscan[:n_samples]) silhouette_dbscan = silhouette_score(X, y_dbscan) print(f\\"K-Means ARI: {ari_kmeans}, Silhouette Coefficient: {silhouette_kmeans}\\") print(f\\"DBSCAN ARI: {ari_dbscan}, Silhouette Coefficient: {silhouette_dbscan}\\") # Step 5: Visualize the resulting clusters plt.figure(figsize=(12, 6)) # K-Means plot plt.subplot(1, 2, 1) plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap=\'viridis\', marker=\'o\') plt.title(\\"K-Means Clustering\\") # DBSCAN plot plt.subplot(1, 2, 2) plt.scatter(X[:, 0], X[:, 1], c=y_dbscan, cmap=\'viridis\', marker=\'o\') outliers = (y_dbscan == -1) plt.scatter(X[outliers, 0], X[outliers, 1], c=\'red\', marker=\'x\') plt.title(\\"DBSCAN Clustering\\") plt.show()"},{"question":"Objective: Write a Python program where you will: 1. Use the `sys` module to add an audit hook. 2. Implement a memory tracing function to track and print the memory allocation of a specific function. 3. Monitor and record the number of times specific events (e.g., function calls) are triggered during the program\'s execution. Task Description: 1. **Add an Audit Hook**: - Write a function called `audit_hook(event, args)` that logs the event name and arguments each time an auditing event is raised. Add this function as an audit hook using `sys.addaudithook()`. 2. **Memory Tracing**: - Write a decorator `memory_tracer` that can be applied to any function. This decorator should use `sys.getallocatedblocks()` to print the number of memory blocks allocated before and after the execution of the function. It should also print the size of the return value of the function using `sys.getsizeof()`. 3. **Event Monitoring**: - Implement a function `monitor_events` that sets a trace function to count the number of function calls (`call` events) within a program. It should return the total count of function call events when the program ends. Constraints: - The program should work on any Python environment where the standard `sys` module is available. - The `audit_hook` should be added only once at the beginning of the program. - The memory tracing should not interfere with the actual functionality of the decorated function. Input and Output: - **Input**: No direct input. The program should demonstrate the functionality through function definitions and their calls. - **Output**: Print statements showing audit events, memory allocation details, and the count of function call events. Here is an example template to get you started: ```python import sys # Part 1: Add Audit Hook def audit_hook(event, args): print(f\\"Audit Event: {event}, Arguments: {args}\\") sys.addaudithook(audit_hook) # Part 2: Memory Tracing Decorator def memory_tracer(func): def wrapper(*args, **kwargs): before = sys.getallocatedblocks() result = func(*args, **kwargs) after = sys.getallocatedblocks() size = sys.getsizeof(result) print(f\\"Memory Blocks Before: {before}, After: {after}, Size of result: {size} bytes\\") return result return wrapper # Example function to demonstrate memory tracing @memory_tracer def example_function(data): return [i for i in range(data)] # Part 3: Event Monitoring def trace_function(frame, event, arg): if event == \\"call\\": trace_function.call_count += 1 return trace_function trace_function.call_count = 0 def monitor_events(): sys.settrace(trace_function) return trace_function.call_count # Demonstration of the complete program if __name__ == \\"__main__\\": print(\\"Starting event monitoring and memory tracing...\\") # Sample data for demonstration example_result = example_function(1000) # End of program event monitoring sys.settrace(None) total_calls = monitor_events() print(f\\"Total function calls: {total_calls}\\") ``` Explanation: - The `audit_hook` function logs audit events. - The `memory_tracer` decorator tracks memory allocation changes and result size for the decorated function. - The `monitor_events` function sets up a trace function to count function call events and returns the total count at the end. Your task is to implement these components correctly and provide a demonstration of their functionality within the program.","solution":"import sys # Part 1: Add Audit Hook def audit_hook(event, args): print(f\\"Audit Event: {event}, Arguments: {args}\\") sys.addaudithook(audit_hook) # Part 2: Memory Tracing Decorator def memory_tracer(func): def wrapper(*args, **kwargs): before = sys.getallocatedblocks() result = func(*args, **kwargs) after = sys.getallocatedblocks() size = sys.getsizeof(result) print(f\\"Memory Blocks Before: {before}, After: {after}, Size of result: {size} bytes\\") return result return wrapper # Example function to demonstrate memory tracing @memory_tracer def example_function(data): return [i for i in range(data)] # Part 3: Event Monitoring def trace_function(frame, event, arg): if event == \\"call\\": trace_function.call_count += 1 return trace_function trace_function.call_count = 0 def monitor_events(): sys.settrace(trace_function) # simulate function calls for demonstration example_function(10) example_function(20) example_function(30) sys.settrace(None) return trace_function.call_count # Demonstration of the complete program if __name__ == \\"__main__\\": print(\\"Starting event monitoring and memory tracing...\\") # Sample data for demonstration example_result = example_function(1000) # End of program event monitoring total_calls = monitor_events() print(f\\"Total function calls: {total_calls}\\")"},{"question":"# Custom Iterator Challenge In this question, you will implement custom iterable classes in Python using the concepts of sequence iterators and callable/sentinel iterators. You are required to implement two classes: `CustomSeqIter` and `CustomCallIter`. 1. `CustomSeqIter` Class: This class should mimic the behavior of Python\'s `PySeqIter_Type`. - **Initialization**: ```python def __init__(self, seq: list): # Initialize with a sequence ``` - **Iteration**: - Implement the `__iter__` and `__next__` methods. - Should raise `StopIteration` when the sequence is fully iterated. - **Checks**: ```python def is_seq_iter(obj) -> bool: # Check if the object is an instance of CustomSeqIter ``` Expected Behavior: ```python seq = [10, 20, 30] iterator = CustomSeqIter(seq) for item in iterator: print(item) # should print 10, then 20, then 30 print(is_seq_iter(iterator)) # should return True print(is_seq_iter(seq)) # should return False ``` 2. `CustomCallIter` Class: This class should mimic the behavior of Python\'s `PyCallIter_Type`. - **Initialization**: ```python def __init__(self, callable_obj: callable, sentinel): # Initialize with a callable and a sentinel value ``` - **Iteration**: - Implement the `__iter__` and `__next__` methods. - Should raise `StopIteration` when the callable returns the sentinel value. - **Checks**: ```python def is_call_iter(obj) -> bool: # Check if the object is an instance of CustomCallIter ``` Expected Behavior: ```python def counter(): count = 1 while True: yield count count += 1 call_iter = CustomCallIter(counter().__next__, 5) for item in call_iter: print(item) # should print 1, 2, 3, 4 print(is_call_iter(call_iter)) # should return True print(is_call_iter(counter)) # should return False ``` Implementation Constraints: - Do not use built-in iterators or generator expressions apart from the required implementation. - Follow Python conventions for naming and structure. - Ensure your code is efficient and handles edge cases properly. Ensure you implement the classes and functions correctly, and they should pass the provided examples.","solution":"class CustomSeqIter: def __init__(self, seq): self.seq = seq self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.seq): item = self.seq[self.index] self.index += 1 return item else: raise StopIteration def is_seq_iter(obj): return isinstance(obj, CustomSeqIter) class CustomCallIter: def __init__(self, callable_obj, sentinel): self.callable_obj = callable_obj self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable_obj() if result == self.sentinel: raise StopIteration return result def is_call_iter(obj): return isinstance(obj, CustomCallIter)"},{"question":"Objective Demonstrate your understanding of handling HTTP-related exceptions using the `urllib.error` module in Python. Implement a function that fetches data from a given URL and robustly handles various HTTP-related errors. Function Signature ```python def fetch_data_from_url(url: str) -> str: pass ``` Input - `url` (str): A string representing the URL from which to fetch data. Output - Returns a string containing the fetched data if the request is successful. - If an `HTTPError` with an HTTP status code of `404` is caught, returns the string `\\"Error 404: Not Found\\"`. - If an `HTTPError` with any other status code is caught, returns the string `\\"HTTP Error <status code>: <reason>\\"` (replace `<status code>` and `<reason>` with the appropriate values). - If a `ContentTooShortError` is caught, returns the string `\\"Content Too Short Error: Downloaded file is incomplete\\"`. - If a `URLError` is caught, returns the string `\\"URL Error: <reason>\\"` (replace `<reason>` with the appropriate value). - If any other exception is caught, returns the string `\\"An unexpected error occurred: <error message>\\"`. Constraints - You can use any function from the `urllib.request` module. Example ```python # Example usage url = \\"http://example.com/data\\" result = fetch_data_from_url(url) print(result) ``` # Notes - Ensure that your code gracefully handles each type of exception as outlined. - You may assume that all URLs used in tests are valid strings. Design a robust and fault-tolerant function to ensure that the data fetching process can handle different types of errors gracefully.","solution":"import urllib.request import urllib.error def fetch_data_from_url(url: str) -> str: Fetches data from a given URL and handles various HTTP-related errors. Args: - url (str): URL from which to fetch data. Returns: - str: Fetched data or an error message. try: with urllib.request.urlopen(url) as response: return response.read().decode() except urllib.error.HTTPError as e: if e.code == 404: return \\"Error 404: Not Found\\" else: return f\\"HTTP Error {e.code}: {e.reason}\\" except urllib.error.ContentTooShortError: return \\"Content Too Short Error: Downloaded file is incomplete\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\" except Exception as e: return f\\"An unexpected error occurred: {str(e)}\\""},{"question":"Objective: Implement a PyTorch model that uses `torch.cond` to apply different operations on the input tensor depending on certain conditions. Problem Statement: You are required to implement a PyTorch model `ElementGreaterThanMeanModel` that processes an input tensor `x`. Your model should check if the mean of the elements in `x` is greater than a specified threshold. If the mean is greater than the threshold, it should square each element in the tensor. If the mean is less than or equal to the threshold, it should double each element in the tensor. Instructions: 1. Implement a class `ElementGreaterThanMeanModel` that inherits from `torch.nn.Module`. 2. Your class should have an `__init__` method that takes a `threshold` as an argument and saves it as an instance variable. 3. Implement the `forward` method that takes an input tensor `x` and applies the control flow based on the mean of the elements. 4. Use the `torch.cond` function to apply the appropriate operation: - If the mean of `x` is greater than the threshold, square each element in `x`. - If the mean of `x` is less than or equal to the threshold, double each element in `x`. Expected Input and Output: - Input: A tensor `x` of any shape. - Output: A tensor of the same shape as `x`. Constraints: - You should use PyTorch version 1.13 or later. - Do not use any loops; leverage tensor operations and `torch.cond`. Example Usage: ```python import torch class ElementGreaterThanMeanModel(torch.nn.Module): def __init__(self, threshold: float): super().__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor) -> torch.Tensor: return x ** 2 def false_fn(x: torch.Tensor) -> torch.Tensor: return x * 2 return torch.cond(x.mean() > self.threshold, true_fn, false_fn, (x,)) # Example usage: threshold = 0.5 model = ElementGreaterThanMeanModel(threshold) input_tensor = torch.tensor([0.1, 0.2, 0.3, 0.4, 0.5]) output_tensor = model(input_tensor) print(output_tensor) # Should output tensor([0.2000, 0.4000, 0.6000, 0.8000, 1.0000]) because mean is 0.3 which is <= 0.5 ``` Note: - Pay attention to the handling of tensor operations and the use of `torch.cond`. - Ensure that the function signatures and expected behavior are correct.","solution":"import torch import torch.nn as nn class ElementGreaterThanMeanModel(nn.Module): def __init__(self, threshold: float): super(ElementGreaterThanMeanModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor) -> torch.Tensor: mean_x = x.mean() def true_fn(x): return x ** 2 def false_fn(x): return x * 2 output = torch.where(mean_x > self.threshold, true_fn(x), false_fn(x)) return output"},{"question":"Objective Implement a function `validate_python_code` that will validate the lexical correctness of a given string representing Python code based on the rules outlined in the provided documentation. Problem Description Write a function `validate_python_code(code: str) -> bool` that takes a string representing Python code and ensures: 1. Proper handling of physical and logical lines, including valid line joining. 2. Correct detection and handling of comments and blank lines. 3. Appropriate indentation handling. 4. Proper usage of string literals (both normal and bytes) and escape sequences. 5. Correct identification of integer, floating-point, and imaginary literals. 6. Appropriate use of operators and delimiters. 7. Validity of identifier names based on Unicode standard rules. Input - `code`: A string representing the Python code to be validated. Output - `bool`: Return `True` if the code is lexically valid according to the described rules, `False` otherwise. Constraints - The input string will be at most 10^6 characters long. Examples ```python # Example 1 code = \'\'\' def func(x): # this is a comment if x > 0: return f\\"Positive: {x}\\" elif x == 0: return \\"Zero\\" else: return \\"Negative\\" \'\'\' print(validate_python_code(code)) # Expected output: True # Example 2 code = \'\'\' def func(x) if x > 0 return f\\"Positive: {x}\\" elif x == 0: return \\"Zero\\" else: return \\"Negative\\" \'\'\' print(validate_python_code(code)) # Expected output: False (missing colons) # Example 3 code = \'\'\' def func(x): if x > 0: return f\\"Positive: {x}\\" elif x == 0 return \\"Zero\\" else: return \\"Negative\\" \'\'\' print(validate_python_code(code)) # Expected output: False (missing colon in `elif` statement) ``` Notes 1. This function should primarily make use of Python\'s string manipulation utilities to inspect the code provided. 2. It should not execute or parse the code using the `eval` or `exec` functions to ensure safety and focus on lexical analysis. Hint: You can use regular expressions to validate certain patterns like string literals and identifier naming conventions.","solution":"import ast def validate_python_code(code: str) -> bool: try: ast.parse(code) return True except SyntaxError: return False"},{"question":"Managing Context-Specific Data with `contextvars` **Objective**: Design a function that showcases the creation and management of context-specific data using `contextvars`. Your implementation should demonstrate the ability to create contexts, set and get context variables, and handle context changes effectively. **Problem Statement**: You are required to implement a Python class `ContextManager` that manages context-specific data. This class should have the following capabilities: 1. **Initialize Contexts and Variables**: Create a new context and context variables. 2. **Set Context Variables**: Set a value for a context variable within a specific context. 3. **Get Context Variables**: Retrieve the value of a context variable from a specific context. 4. **Enter and Exit Contexts**: Change the current context to another context and revert back. 5. **Copy Contexts**: Create a shallow copy of an existing context. **Class Specification**: ```python import contextvars class ContextManager: def __init__(self): # Initialize a dictionary to store contexts and their variables. self.contexts = {} def create_context(self, context_name): Create a new context and store it in the contexts dictionary. :param context_name: Name of the context to create. :raises: ValueError if context already exists. pass def create_context_var(self, context_name, var_name, default_value=None): Create a new context variable for the given context. :param context_name: Name of the context to add the variable to. :param var_name: Name of the context variable. :param default_value: Default value for the context variable. :raises: ValueError if context does not exist or variable already exists. pass def set_context_var(self, context_name, var_name, value): Set a value for the context variable in the specified context. :param context_name: Name of the context. :param var_name: Name of the context variable. :param value: Value to set for the context variable. :raises: ValueError if context or variable does not exist. pass def get_context_var(self, context_name, var_name): Get the value of the context variable from the specified context. :param context_name: Name of the context. :param var_name: Name of the context variable. :return: Value of the context variable. :raises: ValueError if context or variable does not exist. pass def enter_context(self, context_name): Enter the specified context. :param context_name: Name of the context to enter. :raises: ValueError if context does not exist. pass def exit_context(self, context_name): Exit the specified context and return to the previous context. :param context_name: Name of the context to exit. :raises: ValueError if context does not exist or is not currently active. pass def copy_context(self, context_name): Create a shallow copy of the specified context. :param context_name: Name of the context to copy. :return: Name of the new context copy. pass ``` **Constraints**: 1. Context names and variable names are unique strings. 2. At any point, the number of contexts will not exceed 100. 3. Context operations should be efficient, with a maximum time complexity of O(1) for create, set, and get operations. 4. Ensure proper error handling for invalid operations. # Example Usage ```python # Initialize ContextManager cm = ContextManager() # Create contexts cm.create_context(\\"ctx1\\") cm.create_context(\\"ctx2\\") # Create context variables cm.create_context_var(\\"ctx1\\", \\"var1\\", default_value=10) cm.create_context_var(\\"ctx2\\", \\"var2\\", default_value=20) # Set and get context variables cm.set_context_var(\\"ctx1\\", \\"var1\\", 100) assert cm.get_context_var(\\"ctx1\\", \\"var1\\") == 100 # Enter and exit contexts cm.enter_context(\\"ctx1\\") cm.exit_context(\\"ctx1\\") # Copy a context copy_ctx_name = cm.copy_context(\\"ctx1\\") ``` Implement the `ContextManager` class and its methods according to the specifications.","solution":"import contextvars class ContextManager: def __init__(self): self.contexts = {} self.current_context = None def create_context(self, context_name): if context_name in self.contexts: raise ValueError(f\\"Context \'{context_name}\' already exists.\\") self.contexts[context_name] = {} def create_context_var(self, context_name, var_name, default_value=None): if context_name not in self.contexts: raise ValueError(f\\"Context \'{context_name}\' does not exist.\\") if var_name in self.contexts[context_name]: raise ValueError(f\\"Variable \'{var_name}\' already exists in context \'{context_name}\'.\\") self.contexts[context_name][var_name] = contextvars.ContextVar(var_name, default=default_value) def set_context_var(self, context_name, var_name, value): if context_name not in self.contexts: raise ValueError(f\\"Context \'{context_name}\' does not exist.\\") if var_name not in self.contexts[context_name]: raise ValueError(f\\"Variable \'{var_name}\' does not exist in context \'{context_name}\'.\\") self.contexts[context_name][var_name].set(value) def get_context_var(self, context_name, var_name): if context_name not in self.contexts: raise ValueError(f\\"Context \'{context_name}\' does not exist.\\") if var_name not in self.contexts[context_name]: raise ValueError(f\\"Variable \'{var_name}\' does not exist in context \'{context_name}\'.\\") return self.contexts[context_name][var_name].get() def enter_context(self, context_name): if context_name not in self.contexts: raise ValueError(f\\"Context \'{context_name}\' does not exist.\\") self.current_context = context_name def exit_context(self, context_name): if self.current_context != context_name: raise ValueError(f\\"Cannot exit context \'{context_name}\' because it is not currently active.\\") self.current_context = None def copy_context(self, context_name): if context_name not in self.contexts: raise ValueError(f\\"Context \'{context_name}\' does not exist.\\") new_context_name = context_name + \\"_copy\\" self.contexts[new_context_name] = self.contexts[context_name].copy() return new_context_name"},{"question":"Your task is to implement a Python function that creates a structured email message with multiple MIME components, including text, an image, and an attachment. The email should be formatted correctly to be sent using an SMTP server. # Function Signature ```python def create_multipart_email( sender: str, recipient: str, subject: str, text: str, image_data: bytes, image_subtype: str, attachment_data: bytes, attachment_filename: str ) -> str: pass ``` # Parameters 1. `sender` (str): The email address of the sender. 2. `recipient` (str): The email address of the recipient. 3. `subject` (str): The subject of the email. 4. `text` (str): The plain text content of the email. 5. `image_data` (bytes): The raw byte data of an image to be attached. 6. `image_subtype` (str): The MIME subtype for the image (e.g., \'jpeg\', \'png\'). 7. `attachment_data` (bytes): The raw byte data of an attachment file. 8. `attachment_filename` (str): The filename for the attachment. # Returns - `str`: The complete email message as a string, ready to be sent. # Requirements 1. Create a `MIMEMultipart` email message. 2. Add a text part using `MIMEText`. 3. Add an image attachment using `MIMEImage`. 4. Add a generic file attachment (e.g., a PDF or zip) using `MIMEApplication`. 5. Ensure all parts have appropriate headers and encoding. 6. The email should include correct subject, sender, and recipient fields. # Example Usage ```python sender = \\"example.sender@example.com\\" recipient = \\"example.recipient@example.com\\" subject = \\"Test Email\\" text = \\"This is a test email with multiple MIME parts.\\" with open(\\"image.png\\", \\"rb\\") as img_f, open(\\"document.pdf\\", \\"rb\\") as doc_f: image_data = img_f.read() attachment_data = doc_f.read() email_message = create_multipart_email( sender, recipient, subject, text, image_data, \\"png\\", attachment_data, \\"document.pdf\\" ) print(email_message) ``` # Constraints - Python 3.6 or newer. - Use the classes from the `email.mime` module for constructing the message. - Ensure to handle errors suitably, such as missing or incorrect MIME types. # Performance Requirements - The function should be efficient in terms of memory and execution time, even with reasonably sized image and attachment data.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication from email.encoders import encode_base64 def create_multipart_email( sender: str, recipient: str, subject: str, text: str, image_data: bytes, image_subtype: str, attachment_data: bytes, attachment_filename: str ) -> str: Creates a multipart email message with text, image and file attachment parts. Parameters: sender (str): The email address of the sender. recipient (str): The email address of the recipient. subject (str): The subject of the email. text (str): The plain text content of the email. image_data (bytes): The raw byte data of an image to be attached. image_subtype (str): The MIME subtype for the image (e.g., \'jpeg\', \'png\'). attachment_data (bytes): The raw byte data of an attachment file. attachment_filename (str): The filename for the attachment. Returns: str: The complete email message as a string, ready to be sent. # Create the root message msg = MIMEMultipart() msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'Subject\'] = subject # Attach the text part text_part = MIMEText(text, \'plain\') msg.attach(text_part) # Attach the image part image_part = MIMEImage(image_data, _subtype=image_subtype) image_part.add_header(\'Content-Disposition\', \'attachment\', filename=\'image.\' + image_subtype) msg.attach(image_part) # Attach the generic file attachment part attachment_part = MIMEApplication(attachment_data) attachment_part.add_header(\'Content-Disposition\', \'attachment\', filename=attachment_filename) msg.attach(attachment_part) # Return the full message as a string return msg.as_string()"},{"question":"# Python Coding Assessment Question Objective: Write a Python function that utilizes the `compileall` module to compile all the Python files in a given directory and its subdirectories, with options to exclude certain directories, customize the optimization level, and use multiple workers. Function Specification: ```python import compileall import re import os def compile_python_files(directory, exclude_patterns=None, optimize_level=(0,), use_workers=1): Compiles all Python source files in the given directory and its subdirectories. Args: - directory (str): The root directory to start the compilation from. - exclude_patterns (list of str, optional): List of regex patterns for directories/files to exclude from compilation. Defaults to None. - optimize_level (tuple of ints, optional): The optimization levels to use for compilation. Defaults to (0,) i.e., no optimization. - use_workers (int, optional): Number of workers to use for parallel compilation. Defaults to 1. Returns: - bool: True if all files compiled successfully, false otherwise. pass ``` Input: 1. **directory**: A `str` representing the root directory containing Python files to compile. 2. **exclude_patterns**: A list of `str` patterns. Any file or directory matching these patterns should be excluded from compilation. 3. **optimize_level**: A tuple of `int` values representing optimization levels for the compiler. 4. **use_workers**: An `int` specifying the number of parallel workers to use during compilation. Output: - Returns `True` if all files are compiled successfully, and `False` otherwise. Constraints: - The `directory` must exist, and it should be a readable directory. - Values in `optimize_level` must be non-negative integers. - `use_workers` must be a non-negative integer. If set to 0, the function should use optimal cores available on the system. Example: ```python # Example 1: # Compile all files in the \'src\' directory, exclude .test directories, use optimization level 1, and 2 workers. compile_python_files(\'src\', exclude_patterns=[r\'.test\'], optimize_level=(1,), use_workers=2) # Example 2: # Compile all files in the \'project\' directory with no exclusions and default single worker. compile_python_files(\'project\') ``` Implementation: Implement the `compile_python_files` function according to the above specification. Your implementation should effectively handle the provided inputs and execute the compilation process by calling appropriate functions from the `compileall` module.","solution":"import compileall import re import os from multiprocessing import cpu_count def compile_python_files(directory, exclude_patterns=None, optimize_level=(0,), use_workers=1): Compiles all Python source files in the given directory and its subdirectories. Args: - directory (str): The root directory to start the compilation from. - exclude_patterns (list of str, optional): List of regex patterns for directories/files to exclude from compilation. Defaults to None. - optimize_level (tuple of ints, optional): The optimization levels to use for compilation. Defaults to (0,) i.e., no optimization. - use_workers (int, optional): Number of workers to use for parallel compilation. Defaults to 1. Returns: - bool: True if all files compiled successfully, false otherwise. if not os.path.isdir(directory): raise ValueError(\\"The provided directory does not exist or is not a directory.\\") if exclude_patterns is None: exclude_patterns = [] if not all(isinstance(opt_level, int) and opt_level >= 0 for opt_level in optimize_level): raise ValueError(\\"All optimization levels must be non-negative integers.\\") if not isinstance(use_workers, int) or use_workers < 0: raise ValueError(\\"Number of workers must be a non-negative integer.\\") if use_workers == 0: use_workers = cpu_count() def should_exclude(path): return any(re.search(pattern, path) for pattern in exclude_patterns) success = compileall.compile_dir( dir=directory, maxlevels=10, ddir=None, force=False, rx=None if not exclude_patterns else re.compile(\'|\'.join(exclude_patterns)), quiet=1, legacy=False, optimize=optimize_level, workers=use_workers ) return success"},{"question":"**Objective:** Create a Python function that constructs, modifies, and serializes an email message. This task will test your understanding of the `Message` class from the `email.message` module. **Task:** 1. Write a function `compose_and_serialize_email` that: - Constructs an email message with: - \\"From\\" header set to \\"sender@example.com\\". - \\"To\\" header set to \\"recipient@example.com\\". - \\"Subject\\" header set to \\"Test Email\\". - A text payload with the content \\"This is a test email.\\" - Adds a custom header \\"X-Custom-Header\\" with the value \\"CustomValue\\". - Checks if the message is a multipart message. - Serializes the message to a string format using the `as_string` method. - Returns the serialized string. **Function Signature:** ```python def compose_and_serialize_email() -> str: ``` **Constraints and Details:** - Use the `email.message.Message` class from the `email` module. - Ensure that the email message is constructed using the specified headers and payload. - Ensure that the custom header is added correctly. - Check if the message is multipart (it should not be in this case). - Serialize the message to a string including the custom headers and payload. - Do not use the `EmailMessage` class or any other imports outside of the `email` module. **Example:** ```python result = compose_and_serialize_email() print(result) ``` **Expected Output:** ``` From: sender@example.com To: recipient@example.com Subject: Test Email X-Custom-Header: CustomValue This is a test email. ``` You may have additional line breaks or headers (such as `MIME-Version` and `Content-Type`) in the actual output depending on the implementation details of the `Message` class.","solution":"from email.message import Message def compose_and_serialize_email() -> str: Constructs an email message, adds a custom header, and serializes it to string format. msg = Message() msg[\\"From\\"] = \\"sender@example.com\\" msg[\\"To\\"] = \\"recipient@example.com\\" msg[\\"Subject\\"] = \\"Test Email\\" msg.set_payload(\\"This is a test email.\\") msg.add_header(\\"X-Custom-Header\\", \\"CustomValue\\") # Check if the message is multipart (it should not be in this case) assert not msg.is_multipart() return msg.as_string()"},{"question":"You have been assigned to audit and log security-related events in a Python application using the auditing hooks provided in `sys.audit()` and `PySys_Audit()`. Develop a Python function that will register an audit hook which logs specified events into a file. The logging should include the event name and all the associated arguments. # Function Signature ```python def register_audit_hook(events_to_log: list, log_file: str) -> None: pass ``` # Input 1. `events_to_log` (list): A list of event names (strings) that should be logged. 2. `log_file` (str): The path to the file where the logs will be written. # Output - The function does not return any value. Instead, it should register an audit hook using `sys.addaudithook` to log specified events. # Requirements 1. Only the specified events in `events_to_log` should be logged. 2. The log entry should include: - Event name. - All the arguments passed to the event. 3. Each log entry should be a separate line in the specified log file. 4. Ensure that the logged entries are human-readable. # Example ```python def test_register_audit_hook(): events = [\'os.chdir\', \'os.remove\', \'sys.unraisablehook\'] register_audit_hook(events, \'audit_log.txt\') # Simulate some operations to generate audit events import os import sys try: os.chdir(\\"/nonexistent_path\\") except FileNotFoundError: pass try: os.remove(\\"/nonexistent_file\\") except FileNotFoundError: pass hook_data = {\\"key\\": \\"value\\"} sys.unraisablehook(RuntimeError(\\"This is a test\\"), hook_data) with open(\'audit_log.txt\', \'r\') as f: logs = f.read().strip().split(\'n\') for log in logs: print(log) test_register_audit_hook() ``` Implementation advice: - Utilize `sys.addaudithook` for registering the audit hook. - The audit hook should log only the specified events and write them to `log_file`. This exercise will validate your understanding of auditing mechanisms in Python and tackle system operations securely and transparently.","solution":"import sys def register_audit_hook(events_to_log: list, log_file: str) -> None: Registers an audit hook to log specified events into a file. Parameters: events_to_log (list): List of event names to log. log_file (str): Path to the file where the logs will be written. def audit_hook(event, args): if event in events_to_log: with open(log_file, \'a\') as f: log_entry = f\\"Event: {event}, Args: {args}n\\" f.write(log_entry) sys.addaudithook(audit_hook)"},{"question":"Objective Implement a PyTorch script that performs a synchronous all-reduce operation across multiple GPUs using the `nccl` backend. The script should initialize the distributed environment, create necessary process groups, perform the all-reduce operation, and clean up resources upon completion. Problem Statement You are given a model with parameters spread across multiple GPUs. Your task is to compute the sum of model parameters across these GPUs and distribute the summed result back to each GPU. Instructions 1. **Initialization**: Initialize the distributed process group using the `nccl` backend. 2. **Parameter Distribution**: Create a tensor of random values on each GPU and perform an all-reduce operation to sum these values across all GPUs. 3. **Synchronization**: Ensure all processes synchronize before printing the final result on each GPU. 4. **Cleanup**: Properly destroy the process group to clean up resources. Function Signature ```python import torch import torch.distributed as dist import torch.multiprocessing as mp def all_reduce_sum(rank, world_size): Perform synchronous all-reduce operation to compute the sum of tensor values across multiple GPUs and distribute the result. Args: rank (int): Rank of the current process. world_size (int): Total number of processes participating in the operation. # Initialize the process group dist.init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) # Set the device for the current process torch.cuda.set_device(rank) # Create a tensor of random values tensor = torch.randn(10, 10).cuda(rank) # Perform all-reduce to sum the values across all processes (GPUs) dist.all_reduce(tensor, op=dist.ReduceOp.SUM) # Wait for all processes to reach this point dist.barrier() # Print the result on each process print(f\\"Rank {rank}, Tensor after all-reduce:n{tensor}\\") # Clean up the process group dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = torch.cuda.device_count() if world_size < 2: raise RuntimeError(\\"This script requires at least 2 GPUs\\") # Spawn processes for each GPU mp.spawn(all_reduce_sum, args=(world_size,), nprocs=world_size) ``` Constraints - The script should be run on a system with at least 2 GPUs. - Use the `nccl` backend for communication. - Use PyTorch version 1.8 or higher. Example If you run the script on a system with 2 GPUs, the output might look like: ``` Rank 0, Tensor after all-reduce: tensor([[2.3026, 2.3026, 2.3026, ..., 2.3026, 2.3026, 2.3026], [2.3026, 2.3026, 2.3026, ..., 2.3026, 2.3026, 2.3026], ..., [2.3026, 2.3026, 2.3026, ..., 2.3026, 2.3026, 2.3026]], device=\'cuda:0\') Rank 1, Tensor after all-reduce: tensor([[2.3026, 2.3026, 2.3026, ..., 2.3026, 2.3026, 2.3026], [2.3026, 2.3026, 2.3026, ..., 2.3026, 2.3026, 2.3026], ..., [2.3026, 2.3026, 2.3026, ..., 2.3026, 2.3026, 2.3026]], device=\'cuda:1\') ``` Note that the exact values of the tensor will differ since they are initialized randomly.","solution":"import torch import torch.distributed as dist import torch.multiprocessing as mp def all_reduce_sum(rank, world_size): Perform synchronous all-reduce operation to compute the sum of tensor values across multiple GPUs and distribute the result. Args: rank (int): Rank of the current process. world_size (int): Total number of processes participating in the operation. # Initialize the process group dist.init_process_group(backend=\'nccl\', rank=rank, world_size=world_size) # Set the device for the current process torch.cuda.set_device(rank) # Create a tensor of random values tensor = torch.randn(10, 10).cuda(rank) # Perform all-reduce to sum the values across all processes (GPUs) dist.all_reduce(tensor, op=dist.ReduceOp.SUM) # Wait for all processes to reach this point dist.barrier() # Print the result on each process print(f\\"Rank {rank}, Tensor after all-reduce:n{tensor}\\") # Clean up the process group dist.destroy_process_group() if __name__ == \\"__main__\\": world_size = torch.cuda.device_count() if world_size < 2: raise RuntimeError(\\"This script requires at least 2 GPUs\\") # Spawn processes for each GPU mp.spawn(all_reduce_sum, args=(world_size,), nprocs=world_size)"},{"question":"Objective: Implement a function using PyTorch that performs a batched matrix multiplication and handles potential numerical accuracy issues as described in the provided documentation. Task: Write a function `batched_matrix_mult` that takes two 3D tensors `A` and `B` representing batches of matrices, applies matrix multiplication, and then verifies the results for numerical consistency using both full precision (float64) and reduced precision (float32). Function Signature: ```python import torch def batched_matrix_mult(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform batched matrix multiplication with PyTorch and check numerical consistency. Args: A (torch.Tensor): A 3D tensor of shape (batch_size, m, n) B (torch.Tensor): A 3D tensor of shape (batch_size, n, p) Returns: torch.Tensor: A 3D tensor of shape (batch_size, m, p) containing the result of A @ B pass ``` Input: - `A`: A 3D tensor of shape `(batch_size, m, n)` with float32 values. - `B`: A 3D tensor of shape `(batch_size, n, p)` with float32 values. Output: - Returns a 3D tensor of shape `(batch_size, m, p)` containing the result of the matrix multiplication `A @ B`. Constraints: 1. Ensure the function works efficiently for batch sizes up to 1000 with matrices of size up to 1000x1000. 2. The function should handle potential larger-than-float32 intermediate values by using float64 precision during computations where necessary. 3. The function should compare results between float32 precision and float64 precision, and log any significant discrepancies. Instructions: 1. Implement batched matrix multiplication using PyTorch\'s built-in functions where appropriate. 2. Convert inputs to float64 for one of the computations to check numerical consistency. 3. Compute the batched matrix multiplication in both float32 and float64. 4. Compare results, log discrepancies, and ensure the output tensor is in float32. Example Usage: ```python # Example tensors A = torch.randn((10, 5, 7), dtype=torch.float32) B = torch.randn((10, 7, 3), dtype=torch.float32) # Perform batched matrix multiplication result = batched_matrix_mult(A, B) print(result) ``` Notes: - Use `torch.bmm` (batch matrix multiplication) for an efficient implementation. - Use logging or print statements to highlight numerical discrepancies.","solution":"import torch def batched_matrix_mult(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform batched matrix multiplication with PyTorch and check numerical consistency. Args: A (torch.Tensor): A 3D tensor of shape (batch_size, m, n) B (torch.Tensor): A 3D tensor of shape (batch_size, n, p) Returns: torch.Tensor: A 3D tensor of shape (batch_size, m, p) containing the result of A @ B # Check the input tensor shapes assert A.ndimension() == 3, \\"Tensor A must be 3D\\" assert B.ndimension() == 3, \\"Tensor B must be 3D\\" assert A.shape[0] == B.shape[0], \\"Batch sizes of A and B must match\\" assert A.shape[2] == B.shape[1], \\"The inner dimensions of A and B must match\\" # Perform matrix multiplication in float32 result_float32 = torch.bmm(A, B) # Convert tensors to float64 and perform the multiplication again result_float64 = torch.bmm(A.double(), B.double()) # Check for numerical discrepancies if not torch.allclose(result_float32.double(), result_float64, atol=1e-6): print(\\"Numerical discrepancy found between float32 and float64 computations\\") # Return result in float32 return result_float32"},{"question":"Objective Assess your understanding of scikit-learn\'s `Pipeline`, `FeatureUnion`, and `ColumnTransformer` by designing and implementing a composite model. Problem Statement You are given a dataset containing information about houses. The dataset has the following columns: - `Area`: The area of the house in square feet (float). - `Bedrooms`: The number of bedrooms in the house (integer). - `Age`: The age of the house in years (integer). - `Location`: The location of the house (categorical variable with values like \'Urban\', \'Suburban\', \'Rural\'). - `Price`: The price of the house in dollars (float, target variable). You need to design a data processing and prediction pipeline that: 1. Preprocesses the data by normalizing the numerical features and one-hot encoding the categorical feature. 2. Uses Principal Component Analysis (PCA) to reduce dimensionality. 3. Trains a `RandomForestRegressor` to predict house prices. 4. Validates the model using k-fold cross-validation. Requirements 1. Implement the preprocessing steps using `Pipeline` and `ColumnTransformer`. 2. Implement the dimensionality reduction and regression steps using `Pipeline`. 3. Use `FeatureUnion` if necessary to combine different preprocessing steps. 4. Evaluate your model using k-fold cross-validation and report the mean squared error. Input Format ```python import pandas as pd # Example dataframe data = { \'Area\': [1500.0, 2000.0, 2500.0, 1800.0, 2200.0], \'Bedrooms\': [3, 4, 3, 2, 4], \'Age\': [10, 15, 12, 8, 20], \'Location\': [\'Urban\', \'Suburban\', \'Urban\', \'Rural\', \'Suburban\'], \'Price\': [300000.0, 450000.0, 500000.0, 350000.0, 600000.0] } df = pd.DataFrame(data) ``` Function Signature ```python def preprocess_and_train_pipeline(housing_data: pd.DataFrame) -> float: Design and train a pipeline for preprocessing and regression. Args: housing_data (pd.DataFrame): DataFrame containing the housing data. Returns: float: The mean squared error of the model evaluated using k-fold cross-validation. pass ``` Constraints - You may assume all input data is correctly formatted. Expected Output You need to return the mean squared error of the model evaluated using k-fold cross-validation. Example ```python mean_squared_error = preprocess_and_train_pipeline(df) print(mean_squared_error) ``` Notes - Your solution should make extensive use of scikit-learn’s `Pipeline`, `ColumnTransformer`, and `FeatureUnion`. - Avoid any data leakage between training and testing sets during cross-validation.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.decomposition import PCA from sklearn.ensemble import RandomForestRegressor from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score from sklearn.metrics import mean_squared_error from sklearn.base import BaseEstimator, TransformerMixin import numpy as np def preprocess_and_train_pipeline(housing_data: pd.DataFrame) -> float: Design and train a pipeline for preprocessing and regression. Args: housing_data (pd.DataFrame): DataFrame containing the housing data. Returns: float: The mean squared error of the model evaluated using k-fold cross-validation. X = housing_data.drop(\'Price\', axis=1) y = housing_data[\'Price\'] # Define the column transformer for preprocessing numerical_features = [\'Area\', \'Bedrooms\', \'Age\'] categorical_features = [\'Location\'] numerical_transformer = Pipeline(steps=[ (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Define the full pipeline with PCA and RandomForestRegressor pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'pca\', PCA(n_components=2)), (\'model\', RandomForestRegressor(n_estimators=100, random_state=0)) ]) # Perform k-fold cross-validation mse_scores = cross_val_score(pipeline, X, y, scoring=\'neg_mean_squared_error\', cv=5) mean_mse = -np.mean(mse_scores) return mean_mse"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of seaborn\'s color palette functionalities by generating custom color palettes and representing them visually. # Instructions Your task is to write a Python function `generate_and_display_palette` that generates color palettes using `seaborn` and displays both the palette and a corresponding heatmap using `matplotlib`. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def generate_and_display_palette(n_colors: int, h: float, s: float, l: float) -> None: pass ``` # Input - `n_colors` (int): Number of colors in the palette. (1 <= n_colors <= 20) - `h` (float): Hue of the colors. (0.0 <= h <= 1.0) - `s` (float): Saturation of the colors. (0.0 <= s <= 1.0) - `l` (float): Lightness of the colors. (0.0 <= l <= 1.0) # Output This function should not return anything. Instead, it should: 1. Display the generated color palette in a horizontal bar. 2. Display a heatmap using the generated palette as the colormap. # Requirements 1. Use `sns.husl_palette` to generate the palette. 2. Use `matplotlib.pyplot` to create subplots for palette display and heatmap. 3. Ensure the heatmap uses the generated palette. # Example ```python import seaborn as sns import matplotlib.pyplot as plt def generate_and_display_palette(n_colors: int, h: float, s: float, l: float) -> None: # Generate palette palette = sns.husl_palette(n_colors=n_colors, h=h, s=s, l=l) # Display palette sns.palplot(palette) # Create a dummy dataset for heatmap data = [list(range(n_colors)) for _ in range(n_colors)] # Create heatmap sns.heatmap(data, cmap=sns.color_palette(palette, as_cmap=True)) # Display the plots plt.show() # Example usage generate_and_display_palette(8, 0.5, 0.8, 0.6) ``` # Notes - Ensure proper import of necessary packages. - Handle edge cases where parameters may be at their extreme values. - A well-commented code will be advantageous.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_and_display_palette(n_colors: int, h: float, s: float, l: float) -> None: Generates a color palette using the seaborn husl_palette function and displays it alongside a heatmap that uses the generated palette. :param n_colors: int in the range [1, 20], the number of colors in the palette. :param h: float in the range [0.0, 1.0], the hue of the colors. :param s: float in the range [0.0, 1.0], the saturation of the colors. :param l: float in the range [0.0, 1.0], the lightness of the colors. # Generate the color palette palette = sns.husl_palette(n_colors=n_colors, h=h, s=s, l=l) # Display the color palette sns.palplot(palette) plt.title(\\"Generated Color Palette\\") plt.show() # Create a dummy dataset for the heatmap data = [list(range(n_colors)) for _ in range(n_colors)] # Create and display the heatmap sns.heatmap(data, cmap=sns.color_palette(palette, as_cmap=True), annot=True) plt.title(\\"Heatmap using Generated Palette\\") plt.show()"},{"question":"Using the Python `uuid` module, implement a function `safe_uuids(uuid_count: int) -> dict` that generates a specified number of UUIDs using `uuid.uuid1()`, ensures they are safe, and returns a dictionary with the UUID string representations as keys and their `is_safe` attribute values (whether they are generated in a multiprocessing-safe way) as values. # Function Signature ```python def safe_uuids(uuid_count: int) -> dict: ``` # Input - `uuid_count` (int): An integer representing the number of UUIDs to generate. (1 ≤ uuid_count ≤ 100) # Output - Returns a dictionary where: - Key: The string representation of the generated UUID. - Value: The `is_safe` attribute of the UUID instance, represented as one of the values from `SafeUUID`: `safe`, `unsafe`, or `unknown`. # Constraints - The function should use `uuid.uuid1()` to generate the UUIDs. - It should handle cases where the generated UUID is either safe, unsafe, or has an unknown state. # Example ```python # Example with three UUIDs result = safe_uuids(3) # Expected output: A dictionary where the keys are UUIDs and the values are their safety statuses. # Example output: # { # \\"6487987c-fc3c-11eb-9a03-0242ac130003\\": SafeUUID.safe, # \\"6487987c-fc3c-11eb-9a03-0242ac130004\\": SafeUUID.unsafe, # \\"6487987c-fc3c-11eb-9a03-0242ac130005\\": SafeUUID.unknown # } ``` Ensure you thoroughly test your function with various `uuid_count` values to confirm it behaves as expected.","solution":"import uuid def safe_uuids(uuid_count: int) -> dict: Generates a specified number of UUIDs using `uuid.uuid1()`, ensures they are safe, and returns a dictionary with the UUID string representations as keys and their `is_safe` attribute values. Args: uuid_count (int): The number of UUIDs to generate. Returns: dict: A dictionary where the keys are UUIDs and the values are their safety statuses. uuid_dict = {} for _ in range(uuid_count): u = uuid.uuid1() uuid_dict[str(u)] = u.is_safe return uuid_dict"},{"question":"**Question: Working with Meta Tensors in PyTorch** PyTorch\'s \\"meta\\" device allows you to work with tensors that store only metadata without the actual data. Meta tensors are useful for operations such as loading models without initializing parameters into memory or performing abstract analysis on tensor operations. Your task is to implement a function `transform_meta_tensor` that performs the following steps: 1. Creates a model containing a single Linear layer with specified input and output features. 2. Moves the model to the meta device. 3. Applies a specified transformation function to the weights of the model on the meta device. 4. Converts the meta tensor weights to actual data tensors on the CPU, applying the same transformation function used on the meta tensor. Function Signature: ```python def transform_meta_tensor(input_features: int, output_features: int, transform_fn: callable) -> torch.nn.Module: pass ``` # Input: - `input_features`: int, the number of features for the input of the Linear layer. - `output_features`: int, the number of features for the output of the Linear layer. - `transform_fn`: callable, a transformation function applied to the weights of the Linear layer. # Output: - Returns the resulting model after processing the weights on the meta device and converting to the CPU device. # Constraints: - Ensure the same transformation function is applied to both the meta tensors and the actual data tensors. - Do not initialize the actual data for the meta tensors directly, but use appropriate PyTorch utilities to do so. # Example: ```python import torch def transform_fn(tensor): return tensor * 2 model = transform_meta_tensor(10, 5, transform_fn) print(model.weight) ``` The above code should create a Linear layer with the weight tensor being initialized on the meta device, followed by applying the `transform_fn` function to double its values, and finally creating an actual data tensor on the CPU with the transformed values. # Notes: - You may assume `torch` and other necessary modules are imported. - Pay close attention to handling the meta tensors and converting them to actual data tensors correctly. You should demonstrate your understanding of meta tensors by efficiently using PyTorch functionalities.","solution":"import torch import torch.nn as nn def transform_meta_tensor(input_features: int, output_features: int, transform_fn: callable) -> nn.Module: Transforms the weights of a linear layer model using a specified transformation function, working with meta tensors and converting to actual data tensors on the CPU. Args: - input_features (int): The number of features for the input of the Linear layer. - output_features (int): The number of features for the output of the Linear layer. - transform_fn (callable): A transformation function applied to the weights of the Linear layer. Returns: - nn.Module: The resulting model after processing the weights on the meta device and converting to the CPU device. # Create a Linear model with meta tensors model_meta = nn.Linear(input_features, output_features, device=\'meta\') # Clone model structure to cpu and initialize it model = nn.Linear(input_features, output_features) model.weight = nn.Parameter(transform_fn(torch.empty_like(model.weight))) return model"},{"question":"# Coding Assessment: Feature Extraction with scikit-learn Objective Design a feature extraction method for a given set of text documents using scikit-learn\'s feature extraction module. The task is to transform the text documents into numerical feature vectors that can be used for machine learning algorithms. Problem Statement You are given a set of text documents and need to perform feature extraction using the Bag of Words and TF-IDF techniques. The task includes: 1. Tokenizing the text documents. 2. Creating a CountVectorizer to convert text to a matrix of token counts. 3. Using TfidfTransformer to convert the token counts into their respective TF-IDF scores. 4. Returning the final matrix of TF-IDF scores. Input - A list of strings, where each string represents a document. - Two boolean parameters: `use_bigrams` indicating whether to use bigrams along with unigrams, and `use_tfidf` indicating whether to convert token counts to their TF-IDF scores. Output - A matrix of feature vectors where each row represents a document and each column represents a feature (token or n-gram). Constraints - Assume the input list of documents is not empty. - The text data is in English. - The `CountVectorizer` and `TfidfTransformer` should be used from sklearn. Function Signature ```python from typing import List import numpy as np def extract_features(documents: List[str], use_bigrams: bool, use_tfidf: bool) -> np.ndarray: pass ``` Example ```python documents = [ \'This is the first document.\', \'This document is the second document.\', \'And this is the third one.\', \'Is this the first document?\' ] # Example 1: Using unigrams only and converting to TF-IDF scores feature_matrix = extract_features(documents, use_bigrams=False, use_tfidf=True) print(feature_matrix) # Example 2: Using both unigrams and bigrams and converting to TF-IDF scores feature_matrix = extract_features(documents, use_bigrams=True, use_tfidf=True) print(feature_matrix) # Example 3: Using unigrams only and retaining token counts (no TF-IDF transformation) feature_matrix = extract_features(documents, use_bigrams=False, use_tfidf=False) print(feature_matrix) ``` Implementation Notes 1. **Tokenization**: You will use `CountVectorizer` for tokenizing the documents. 2. **Bigrams**: If `use_bigrams` is True, `CountVectorizer(ngram_range=(1, 2))` should be used. 3. **TF-IDF Transformation**: If `use_tfidf` is True, apply `TfidfTransformer` on the token count matrix obtained from `CountVectorizer`. 4. **Output**: Return the transformed matrix (either token counts or TF-IDF scores) as a NumPy array. **Sample Implementation** ```python from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer def extract_features(documents: List[str], use_bigrams: bool, use_tfidf: bool) -> np.ndarray: ngram_range = (1, 2) if use_bigrams else (1, 1) vectorizer = CountVectorizer(ngram_range=ngram_range) count_matrix = vectorizer.fit_transform(documents) if use_tfidf: transformer = TfidfTransformer() tfidf_matrix = transformer.fit_transform(count_matrix) return tfidf_matrix.toarray() else: return count_matrix.toarray() ```","solution":"from typing import List import numpy as np from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer def extract_features(documents: List[str], use_bigrams: bool, use_tfidf: bool) -> np.ndarray: Transform documents into numerical feature vectors using Bag of Words and TF-IDF techniques. Args: - documents: List of strings, where each string represents a document. - use_bigrams: Boolean, whether to use bigrams along with unigrams. - use_tfidf: Boolean, whether to convert token counts to their TF-IDF scores. Returns: - A matrix of feature vectors. # Setting ngram_range based on use_bigrams ngram_range = (1, 2) if use_bigrams else (1, 1) # Create a CountVectorizer vectorizer = CountVectorizer(ngram_range=ngram_range) count_matrix = vectorizer.fit_transform(documents) # Convert to TF-IDF if requested if use_tfidf: transformer = TfidfTransformer() tfidf_matrix = transformer.fit_transform(count_matrix) return tfidf_matrix.toarray() else: return count_matrix.toarray()"},{"question":"Introduction In this assessment, you will demonstrate your understanding of seaborn\'s residual plotting capabilities by analyzing a dataset of car characteristics. The goal is to identify potential patterns and tendencies in the residuals of a linear regression model. Problem Statement Using the \'mpg\' dataset from seaborn, write a function named `analyze_residuals` which: 1. Loads the \'mpg\' dataset. 2. Creates and customizes residual plots for the relationship between \'horsepower\' and \'mpg\'. 3. Investigates higher-order trends. 4. Uses a LOWESS curve to highlight any non-linear patterns. Function Specifications: 1. **Function Name:** `analyze_residuals` 2. **Input:** None 3. **Output:** None 4. **Behavior:** - Load the \'mpg\' dataset. - Create and display a basic residual plot for \'horsepower\' vs \'mpg\'. - Create and display a residual plot with a second-order polynomial fit for \'horsepower\' vs \'mpg\'. - Create and display a residual plot with a LOWESS curve for \'horsepower\' vs \'mpg\' and customize the LOWESS curve to be red in color. Example Output The function should generate and display three residual plots as specified. Additional Notes: - Ensure appropriate labels and titles are set for each plot for clear interpretation. - Use seaborn\'s functionality to its fullest extent to customize and improve the clarity of the visualizations. Here is the skeleton code to get you started: ```python import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(): # Load the \'mpg\' dataset mpg = sns.load_dataset(\\"mpg\\") # Basic residual plot for \'horsepower\' vs \'mpg\' plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot: Horsepower vs MPG\\") plt.show() # Residual plot with a second-order polynomial fit plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Residual Plot with 2nd Order Polynomial Fit: Horsepower vs MPG\\") plt.show() # Residual plot with LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws={\'color\':\'r\'}) plt.title(\\"Residual Plot with LOWESS Curve: Horsepower vs MPG\\") plt.show() # Call the function to generate the plots analyze_residuals() ``` Constraints: - Use seaborn for all plotting purposes. - Focus on clarity and readibility of plots. - Ensure your function does not take any parameters and does not return any value, it only needs to generate and display plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_residuals(): # Load the \'mpg\' dataset mpg = sns.load_dataset(\\"mpg\\") # Drop rows with missing values in \'horsepower\' or \'mpg\' mpg = mpg.dropna(subset=[\'horsepower\', \'mpg\']) # Basic residual plot for \'horsepower\' vs \'mpg\' plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", scatter_kws={\'alpha\':0.5}) plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.title(\\"Residual Plot: Horsepower vs MPG\\") plt.show() # Residual plot with a second-order polynomial fit plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2, scatter_kws={\'alpha\':0.5}) plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.title(\\"Residual Plot with 2nd Order Polynomial Fit: Horsepower vs MPG\\") plt.show() # Residual plot with LOWESS curve plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws={\'color\':\'r\'}, scatter_kws={\'alpha\':0.5}) plt.xlabel(\'Horsepower\') plt.ylabel(\'Residuals\') plt.title(\\"Residual Plot with LOWESS Curve: Horsepower vs MPG\\") plt.show() # Call the function to generate the plots analyze_residuals()"},{"question":"**Question:** You are provided with a dataset named `penguins` that contains information about penguin species with several attributes like bill length, species type, and island. Your task is to visualize this data using seaborn plots and customize the legend positions as specified in the steps below: 1. Load the seaborn theme and the `penguins` dataset using seaborn. 2. Create a histogram showing the distribution of the `bill_length_mm` with different colors for each `species`. 3. Move the legend of this histogram to the `center right` of the plot. 4. Create another histogram showing the distribution of the `bill_length_mm` with different colors for each `species` and move the legend to the `upper left` corner with `bbox_to_anchor=(1, 1)`. 5. Create a `displot` showing the distribution of `bill_length_mm` for each `species` separated by `island` into different columns. Move the legend to the `upper left` of each plot with `bbox_to_anchor=(.55, .45)`. 6. Ensure that the legend for the `displot` does not create extra blank space on the right by setting `legend_out=False`. # Function Signature ```python def visualize_penguins(): import seaborn as sns sns.set_theme() # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 1: Create the first histogram and move the legend ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax1, \\"center right\\") # Step 2: Create the second histogram and move the legend ax2 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") sns.move_legend(ax2, \\"upper left\\", bbox_to_anchor=(1, 1)) # Step 3: Create the displot and move the legend g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, ) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45)) # Step 4: Ensure no extra blank space by setting legend_out=False g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), ) sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(.55, .45), frameon=False) # Execute the function visualize_penguins() ``` **Constraints:** - Ensure that you use the seaborn plotting functions and legend customizations as requested. - The solutions should generate plots with the legends accurately repositioned. - The above code should execute without errors and produce the required visualizations with legends in the correct positions.","solution":"# Import necessary libraries import seaborn as sns import matplotlib.pyplot as plt def visualize_penguins(): Generates visualizations using the seaborn library with the penguins dataset, specifically creating histograms and a displot with customized legend positions. sns.set_theme() # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the first histogram and move the legend plt.figure() ax1 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ax1.legend(loc=\'center right\') plt.show() # Create the second histogram and move the legend plt.figure() ax2 = sns.histplot(penguins, x=\\"bill_length_mm\\", hue=\\"species\\") ax2.legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) plt.show() # Create the displot and move the legend g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, ) for ax in g.axes.flat: ax.legend(loc=\'upper left\', bbox_to_anchor=(.55, .45)) plt.show() # Ensure no extra blank space by setting legend_out=False g = sns.displot( penguins, x=\\"bill_length_mm\\", hue=\\"species\\", col=\\"island\\", col_wrap=2, height=3, facet_kws=dict(legend_out=False), ) for ax in g.axes.flat: ax.legend(loc=\'upper left\', bbox_to_anchor=(.55, .45)) plt.show()"},{"question":"**Objective**: Implement a Python function using the `pwd` module to extract and process information from the Unix user account and password database. **Function Signature**: ```python def summarize_users_by_shell(): This function should return a dictionary summarizing the number of users for each shell. The keys of the dictionary will be the shell paths (e.g., `/bin/bash`, `/usr/sbin/nologin`), and the values will be the counts of users that use each shell. Returns: dict: A dictionary with shell paths as keys and user counts as values. pass ``` # Details 1. **Retrieve User Data**: Use the `pwd.getpwall()` function to get a list of all available password database entries. 2. **Process Each Entry**: For each entry, access the `pw_shell` attribute to determine the user’s shell. 3. **Count User Shells**: Create a dictionary where: * The keys are the unique shell paths. * The values are the counts of users that use each shell. 4. **Return**: Return the resulting dictionary. # Example Assuming the password database entries for the following users: ```plaintext User: testuser1, Shell: /bin/bash User: testuser2, Shell: /bin/zsh User: testuser3, Shell: /bin/bash User: testuser4, Shell: /usr/sbin/nologin User: testuser5, Shell: /bin/bash User: testuser6, Shell: /usr/sbin/nologin ``` The function `summarize_users_by_shell` should return: ```python { \'/bin/bash\': 3, \'/bin/zsh\': 1, \'/usr/sbin/nologin\': 2 } ``` # Constraints - You can assume that the `pwd` module is available and functioning correctly. - The system running this code should be a Unix-based system for the `pwd` module to work properly. - Performance should be efficient enough to handle typical user database sizes. # Notes - Ensure code readability and proper usage of Python coding standards. - Comment your code appropriately to explain the logic. **Hint**: You can make use of the `collections.defaultdict` class to simplify counting.","solution":"import pwd from collections import defaultdict def summarize_users_by_shell(): This function returns a dictionary summarizing the number of users for each shell. The keys of the dictionary will be the shell paths (e.g., `/bin/bash`, `/usr/sbin/nologin`), and the values will be the counts of users that use each shell. Returns: dict: A dictionary with shell paths as keys and user counts as values. user_shell_counts = defaultdict(int) for entry in pwd.getpwall(): shell = entry.pw_shell user_shell_counts[shell] += 1 return dict(user_shell_counts)"},{"question":"Package Metadata Information Extractor Objective Implement a function that accepts a list of package names and returns a comprehensive summary of their metadata, including version number, entry points, install requirements, and significant metadata fields like author and license. Function Signature ```python def extract_package_info(packages: list) -> dict: pass ``` Input - `packages` - list of strings: Each string is the name of a Python package installed in the environment. Output - A dictionary where each key is a package name and each value is another dictionary containing the following keys and associated values: - `version` (string): The package version. - `entry_points` (dict): A dictionary where each key is an entry point group name (string), and each value is a list of entry point names (string). - `requires` (list): A list of strings, each representing a requirement. - `author` (string): The package author. - `license` (string): The package license. - `files` (list): A list of file paths installed by the package. Example Given that the packages `wheel` and `setuptools` are installed: ```python packages = [\'wheel\', \'setuptools\'] result = extract_package_info(packages) print(result) ``` Expected output format (values will vary based on the environment): ```python { \'wheel\': { \'version\': \'0.32.3\', \'entry_points\': { \'console_scripts\': [\'wheel\'] }, \'requires\': [], \'author\': \'Daniel Holth\', \'license\': \'MIT\', \'files\': [\'wheel/__init__.py\', ...] }, \'setuptools\': { \'version\': \'50.3.2\', \'entry_points\': { \'distutils.commands\': [\'easy_install\', ...], ... }, \'requires\': [\'certifi>=2017.4.17\', ...], \'author\': \'Python Packaging Authority\', \'license\': \'UNKNOWN\', \'files\': [\'setuptools/__init__.py\', ...] } } ``` Constraints - The function should handle cases where some metadata fields might not be available for a package, and appropriately assign `None` or an empty list as the value. - Proper error handling should be implemented to manage scenarios where the package isn’t found or lacks specific metadata. Guidelines - Use the functionalities provided within the `importlib.metadata` module. - Ensure to organize the data accurately as specified in the output format. - Test the function to verify correctness and robustness.","solution":"from importlib.metadata import distribution, PackageNotFoundError def extract_package_info(packages): package_info = {} for pkg_name in packages: try: dist = distribution(pkg_name) metadata = dist.metadata entry_points = dist.entry_points entry_point_dict = {} for ep in entry_points: entry_point_dict.setdefault(ep.group, []).append(ep.name) requires = dist.requires or [] package_info[pkg_name] = { \'version\': dist.version, \'entry_points\': entry_point_dict, \'requires\': requires, \'author\': metadata.get(\'Author\', None), \'license\': metadata.get(\'License\', None), \'files\': list(dist.files) } except PackageNotFoundError: package_info[pkg_name] = { \'version\': None, \'entry_points\': {}, \'requires\': [], \'author\': None, \'license\': None, \'files\': [] } return package_info"},{"question":"**Problem: Create a Generic Data Cacher with Type Annotations** You are required to design a caching system that can store and manage data of various types. The caching system needs to include methods for adding, retrieving, and removing items, as well as a decorator that logs accesses to the cache. Implement these with appropriate type annotations and make the system generic to support different data types. Your solution should consist of: 1. A `Cache` class that uses generics to operate with different data types. 2. A decorator `log_access` that logs access to the methods when they are called. This decorator should utilize typing\'s `Callable`, `Concatenate`, and `ParamSpec` to ensure type safety. 3. Correct use of type annotations to indicate expected data types in the class methods. # Requirements: 1. **Class `Cache`:** - This class should be a generic class that can cache items of any type. - Use the `TypeVar` from the `typing` module to make the class generic. - Implement methods: - `add(self, key: str, item: T) -> None`: Adds an item to the cache with the specified key. - `get(self, key: str) -> Optional[T]`: Retrieves an item from the cache by key, returning `None` if the key is not found. - `remove(self, key: str) -> Optional[T]`: Removes an item from the cache by key and returns it, or `None` if the key is not found. - Use appropriate type annotations to ensure type safety. 2. **Decorator `log_access`:** - This decorator should log every access to the cached methods (add, get, remove) by printing a message with the method name and the key used. - Use `Callable`, `Concatenate`, and `ParamSpec` to implement this decorator in a type-safe manner. 3. **Optional:** - Implement error handling, such as raising a `KeyError` if `get` or `remove` is called with a non-existent key. # Constraints: - Python version >= 3.10. - Use only standard libraries. # Example Usage: ```python # Example Usage: from typing import TypeVar, Generic, Callable, Concatenate, ParamSpec, Optional T = TypeVar(\'T\') P = ParamSpec(\'P\') def log_access(f: Callable[Concatenate[\'Cache\', str, P], T]) -> Callable[Concatenate[\'Cache\', str, P], T]: def wrapper(self: \'Cache\', key: str, *args: P.args, **kwargs: P.kwargs) -> T: print(f\\"Accessing {f.__name__!r} method with key: {key}\\") return f(self, key, *args, **kwargs) return wrapper class Cache(Generic[T]): def __init__(self) -> None: self._store: dict[str, T] = {} @log_access def add(self, key: str, item: T) -> None: self._store[key] = item @log_access def get(self, key: str) -> Optional[T]: return self._store.get(key) @log_access def remove(self, key: str) -> Optional[T]: return self._store.pop(key, None) # Usage example cache = Cache[int]() cache.add(\\"item1\\", 100) print(cache.get(\\"item1\\")) # Output: Accessing \'get\' method with key: item1n100 print(cache.remove(\\"item1\\")) # Output: Accessing \'remove\' method with key: item1n100 ``` Provide the full implementation of the `Cache` class and `log_access` decorator with correct type annotations for the given usage example.","solution":"from typing import TypeVar, Generic, Callable, Concatenate, ParamSpec, Optional T = TypeVar(\'T\') P = ParamSpec(\'P\') def log_access(f: Callable[Concatenate[\'Cache\', str, P], T]) -> Callable[Concatenate[\'Cache\', str, P], T]: Decorator to log access to the cache methods. def wrapper(self: \'Cache\', key: str, *args: P.args, **kwargs: P.kwargs) -> T: print(f\\"Accessing {f.__name__!r} method with key: {key}\\") return f(self, key, *args, **kwargs) return wrapper class Cache(Generic[T]): def __init__(self) -> None: self._store: dict[str, T] = {} @log_access def add(self, key: str, item: T) -> None: Adds an item to the cache with the specified key. self._store[key] = item @log_access def get(self, key: str) -> Optional[T]: Retrieves an item from the cache by key, returning None if the key is not found. return self._store.get(key) @log_access def remove(self, key: str) -> Optional[T]: Removes an item from the cache by key and returns it, or None if the key is not found. return self._store.pop(key, None)"},{"question":"**Objective:** Write a Python function that simulates basic file operations and employs unit tests to ensure the reliability of the function using the \\"unittest\\" framework. You are also required to use specific utility functions from the `test.support` module. **Problem Statement:** You need to implement and test a function `file_operation_simulator(operations, filename)` that takes a list of operations and a filename, performs the operations on the file, and returns a summary of operations performed. The possible operations are: - `\\"create\\"`: Creates an empty file. - `\\"write:<content>\\"`: Writes the given content into the file. - `\\"read\\"`: Reads the content of the file. - `\\"delete\\"`: Deletes the file. Your task includes: 1. Implementing the `file_operation_simulator` function. 2. Writing comprehensive unit tests for the function using the \\"unittest\\" framework. 3. Utilizing at least two different utility functions or context managers from the `test.support` module in your tests. Function Definition ```python def file_operation_simulator(operations, filename): Simulates file operations. Args: operations (list): A list of operations to perform on the file. Possible operations - \\"create\\", \\"write:<content>\\", \\"read\\", \\"delete\\". filename (str): The name of the file to operate on. Returns: dict: A summary of operations with keys being the operation names and values being the results. pass ``` Constraints - The `operations` list will only contain valid operations. - You must handle file creation, writing, reading, and deletion properly. - Use Python\'s built-in modules for file handling. Example ```python operations = [\\"create\\", \\"write:Hello, World!\\", \\"read\\", \\"delete\\"] filename = \\"testfile.txt\\" output = file_operation_simulator(operations, filename) # Example expected output: # { # \\"create\\": \\"File created.\\", # \\"write\\": \\"Content written.\\", # \\"read\\": \\"Hello, World!\\", # \\"delete\\": \\"File deleted.\\" # } ``` # Unit Tests Create a `unittest.TestCase` class to test the above function. Ensure that your tests: 1. Verify the correctness of each operation (`create`, `write`, `read`, `delete`). 2. Use at least two utilities from the `test.support` module, such as `create_empty_file()`, `captured_stdout()`, `temp_cwd()`, etc. 3. Clean up any files or temporary resources created during the tests. ```python import unittest from test import support class TestFileOperationSimulator(unittest.TestCase): def test_file_operations(self): # Your test implementation here pass if __name__ == \\"__main__\\": support.run_unittest(TestFileOperationSimulator) ``` **Submission Requirements:** - The implemented `file_operation_simulator` function. - A script containing the `unittest.TestCase` class with at least four test methods. - Ensure you follow Python code conventions and provide necessary comments.","solution":"def file_operation_simulator(operations, filename): Simulates file operations. Args: operations (list): A list of operations to perform on the file. Possible operations - \\"create\\", \\"write:<content>\\", \\"read\\", \\"delete\\". filename (str): The name of the file to operate on. Returns: dict: A summary of operations with keys being the operation names and values being the results. import os results = {} for operation in operations: if operation == \\"create\\": try: with open(filename, \'w\') as file: pass results[\\"create\\"] = \\"File created.\\" except Exception as e: results[\\"create\\"] = f\\"Error: {e}\\" elif operation.startswith(\\"write:\\"): content = operation.split(\\":\\", 1)[1] try: with open(filename, \'a\') as file: file.write(content) results[\\"write\\"] = \\"Content written.\\" except Exception as e: results[\\"write\\"] = f\\"Error: {e}\\" elif operation == \\"read\\": try: with open(filename, \'r\') as file: content = file.read() results[\\"read\\"] = content except Exception as e: results[\\"read\\"] = f\\"Error: {e}\\" elif operation == \\"delete\\": try: os.remove(filename) results[\\"delete\\"] = \\"File deleted.\\" except Exception as e: results[\\"delete\\"] = f\\"Error: {e}\\" return results"},{"question":"# PyTorch Coding Assessment Question Objective You are required to implement a function that performs a specific tensor operation and then write tests to validate the correctness of your implementation using PyTorch\'s `torch.testing` utilities. Problem Statement Write a function `matrix_power` that computes the power of a given square matrix using PyTorch tensor operations. **Function Signature:** ```python def matrix_power(matrix: torch.Tensor, n: int) -> torch.Tensor: pass ``` Input - `matrix`: A 2D PyTorch tensor of size `(N, N)` representing a square matrix. - `n`: An integer representing the power to which the matrix should be raised. Can be positive, negative, or zero. Output - Returns a 2D PyTorch tensor of size `(N, N)` which is the `n`-th power of the input `matrix`. Constraints - If `n` is 0, return the identity matrix of the same size. - If `n` is negative, compute the power of the inverse of the matrix. Example ```python import torch matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) n = 2 result = matrix_power(matrix, n) ``` For the given input, `result` should be the matrix: ``` tensor([[ 7.0, 10.0], [15.0, 22.0]]) ``` Testing 1. Implement the function `matrix_power`. 2. Write a test function `test_matrix_power` to validate your implementation using the following criteria: - Check the result for positive powers. - Check the result for zero power (should return the identity matrix). - Check the result for negative powers (should handle matrix inversion). Use the `torch.testing.assert_close` or `torch.testing.assert_allclose` functions to validate your results. Example Test Case ```python import torch import torch.testing def matrix_power(matrix: torch.Tensor, n: int) -> torch.Tensor: if n == 0: return torch.eye(matrix.size(0), dtype=matrix.dtype, device=matrix.device) elif n < 0: matrix = torch.inverse(matrix) n = -n result = matrix for _ in range(n - 1): result = torch.mm(result, matrix) return result def test_matrix_power(): matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) expected = torch.tensor([[7.0, 10.0], [15.0, 22.0]]) torch.testing.assert_allclose(matrix_power(matrix, 2), expected) identity = torch.eye(2, dtype=torch.float) torch.testing.assert_allclose(matrix_power(matrix, 0), identity) matrix = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) inverse = torch.inverse(matrix) torch.testing.assert_allclose(matrix_power(matrix, -1), inverse) test_matrix_power() ``` In the `test_matrix_power` function, you should also cover various edge cases and different sizes of matrices to thoroughly test your implementation.","solution":"import torch def matrix_power(matrix: torch.Tensor, n: int) -> torch.Tensor: Computes the power of a given square matrix using PyTorch tensor operations. Args: matrix (torch.Tensor): A 2D PyTorch tensor of size (N, N). n (int): An integer representing the power to which the matrix should be raised. Returns: torch.Tensor: A 2D PyTorch tensor of size (N, N) which is the n-th power of the input matrix. if n == 0: return torch.eye(matrix.size(0), dtype=matrix.dtype, device=matrix.device) elif n < 0: matrix = torch.inverse(matrix) n = -n result = matrix for _ in range(n - 1): result = torch.mm(result, matrix) return result"},{"question":"**Objective:** Demonstrate your understanding of `torch.finfo` and `torch.iinfo` by writing a function that outputs the numerical properties of a given list of PyTorch data types. **Task:** Write a function `summarize_dtype_properties(dtypes: List[torch.dtype]) -> dict` that takes a list of PyTorch data types and returns a dictionary. The dictionary keys should be the string names of the data types, and the values should be another dictionary that holds the numerical properties of each data type. **Function Signature:** ```python import torch from typing import List, Dict def summarize_dtype_properties(dtypes: List[torch.dtype]) -> Dict[str, Dict[str, float]]: # Your code here ``` **Input Format:** - `dtypes` (List[torch.dtype]): A list of PyTorch data types such as `torch.float32`, `torch.int64`, etc. **Output Format:** - A dictionary where keys are string representations of data types and values are dictionaries of their numerical properties. **Example:** ```python dtypes = [torch.float32, torch.int64, torch.float16] summary = summarize_dtype_properties(dtypes) print(summary) ``` Expected Output: ```python { \'torch.float32\': { \'bits\': 32, \'eps\': 1.1920928955078125e-07, \'max\': 3.4028234663852886e+38, \'min\': -3.4028234663852886e+38, \'tiny\': 1.1754943508222875e-38, \'resolution\': 9.999999403953552e-07 }, \'torch.int64\': { \'bits\': 64, \'max\': 9223372036854775807, \'min\': -9223372036854775808 }, \'torch.float16\': { \'bits\': 16, \'eps\': 0.0009765625, \'max\': 65504.0, \'min\': -65504.0, \'tiny\': 6.103515625e-05, \'resolution\': 0.0009765625 } } ``` **Constraints:** - You are guaranteed that the input list will contain valid PyTorch data types. - The function should handle both floating point and integer data types and include all the properties provided by `torch.finfo` and `torch.iinfo`. **Note:** - Use the `torch.finfo` and `torch.iinfo` classes to fetch the required properties. - You might want to differentiate between floating point and integer types to fetch the correct properties accordingly.","solution":"import torch from typing import List, Dict def summarize_dtype_properties(dtypes: List[torch.dtype]) -> Dict[str, Dict[str, float]]: summary = {} for dtype in dtypes: dtype_name = str(dtype) if torch.is_floating_point(torch.zeros(1, dtype=dtype)): info = torch.finfo(dtype) summary[dtype_name] = { \'bits\': info.bits, \'eps\': info.eps, \'max\': info.max, \'min\': info.min, \'tiny\': info.tiny, \'resolution\': info.resolution, } else: info = torch.iinfo(dtype) summary[dtype_name] = { \'bits\': info.bits, \'max\': info.max, \'min\': info.min, } return summary"},{"question":"Objective Design an asynchronous ticket booking system that: 1. Manages the availability of a fixed number of seats (using a `Semaphore`). 2. Ensures that the booking process for each seat is exclusive (using a `Lock`). 3. Allows cancellation of tickets (releasing the seat held by the `Semaphore`). Problem Statement You need to implement a simple asynchronous ticket booking system. The system should support booking and cancellation processes, and it should ensure that no more than a specified number of tickets are booked simultaneously. Requirements 1. **Semaphore for seat availability**: - Initialize a `Semaphore` with a given number of seats. - Each booking should acquire a seat from the `Semaphore`. 2. **Lock for ensuring exclusive booking process**: - Use a `Lock` to ensure that the process of booking a seat is exclusive and not interfered by other booking attempts. 3. **Functions to implement**: 1. `async def book_seat(name: str, seat_manager: SeatManager) -> None`: - Simulate the process of booking a seat for a user. - Acquire a seat using the semaphore. - Ensure the booking process using a lock. - Simulate time taken for booking (e.g., `await asyncio.sleep(0.1)`). 2. `async def cancel_seat(name: str, seat_manager: SeatManager) -> None`: - Allow a user to cancel their booking, releasing the seat back to the semaphore. - Ensure the cancellation process using a lock. - Simulate time taken for cancellation (e.g., `await asyncio.sleep(0.1)`). 4. **Class to implement**: 1. `class SeatManager`: - Constructor should initialize a semaphore and lock. - Methods to handle booking and cancellation processes. Input and Output * **Input**: - Number of seats available (integer). - List of asynchronous tasks to simulate booking and cancellation processes. * **Output**: - Print statements indicating the booking and cancellation status of users. Constraints - The number of seats available should be a positive integer. - The names of users in the booking and cancellation tasks list should be unique. Example ```python import asyncio class SeatManager: def __init__(self, num_seats: int): self.semaphore = asyncio.Semaphore(num_seats) self.lock = asyncio.Lock() async def book_seat(self, name: str): async with self.lock: await self.semaphore.acquire() print(f\\"{name} successfully booked a seat.\\") await asyncio.sleep(0.1) # Simulate booking processing time async def cancel_seat(self, name: str): async with self.lock: self.semaphore.release() print(f\\"{name} successfully canceled their booking.\\") await asyncio.sleep(0.1) # Simulate cancellation processing time async def book_seat(name: str, seat_manager: SeatManager): await seat_manager.book_seat(name) async def cancel_seat(name: str, seat_manager: SeatManager): await seat_manager.cancel_seat(name) async def main(): seat_manager = SeatManager(3) # Example with 3 available seats tasks = [ book_seat(\\"User1\\", seat_manager), book_seat(\\"User2\\", seat_manager), book_seat(\\"User3\\", seat_manager), cancel_seat(\\"User1\\", seat_manager), book_seat(\\"User4\\", seat_manager), book_seat(\\"User5\\", seat_manager), ] await asyncio.gather(*tasks) asyncio.run(main()) ``` Instructions - Implement the `SeatManager` class with the described methods. - Implement the `book_seat` and `cancel_seat` functions. - The `main()` function should be used to test your implementation with different scenarios.","solution":"import asyncio class SeatManager: def __init__(self, num_seats: int): self.semaphore = asyncio.Semaphore(num_seats) self.lock = asyncio.Lock() async def book_seat(self, name: str): async with self.semaphore: async with self.lock: print(f\\"{name} successfully booked a seat.\\") await asyncio.sleep(0.1) # Simulate booking processing time async def cancel_seat(self, name: str): async with self.lock: self.semaphore.release() print(f\\"{name} successfully canceled their booking.\\") await asyncio.sleep(0.1) # Simulate cancellation processing time async def book_seat(name: str, seat_manager: SeatManager): await seat_manager.book_seat(name) async def cancel_seat(name: str, seat_manager: SeatManager): await seat_manager.cancel_seat(name)"},{"question":"**Objective**: Demonstrate your understanding of Seaborn\'s context setting and plot customization functionalities. **Problem Statement**: Create a function `custom_line_plot` using the Seaborn package that generates a line plot with the following specifications: 1. Accepts four parameters: - `x` (list of int/float): the x-coordinates of the data points. - `y` (list of int/float): the y-coordinates of the data points. - `context` (str): a string representing the context to be set for the plot. It can be one of \\"paper\\", \\"notebook\\", \\"talk\\", or \\"poster\\". - `custom_rc` (dict): a dictionary of custom parameters for the plot, such as line width or font size. 2. Sets the plot context to the passed `context` value. 3. If `custom_rc` is provided, apply these custom parameters to the plot. 4. Generate a line plot using the provided `x` and `y` data. **Constraints**: - The lengths of `x` and `y` should be equal. - The context value should be one of \\"paper\\", \\"notebook\\", \\"talk\\", or \\"poster\\". - If `custom_rc` is not provided, default parameters should be used. **Function Signature**: ```python def custom_line_plot(x: list, y: list, context: str, custom_rc: dict = None) -> None: pass ``` **Input**: - `x`: A list of integers or floats representing x-coordinates. E.g., `[0, 1, 2, 3]` - `y`: A list of integers or floats representing y-coordinates. E.g., `[1, 3, 2, 4]` - `context`: A string specifying the plot context. E.g., `\\"notebook\\"` - `custom_rc`: A dictionary of custom parameters. E.g., `{\\"lines.linewidth\\": 3, \\"font.size\\": 14}` **Output**: - The function should display a customized line plot based on the provided inputs. ```python import seaborn as sns import matplotlib.pyplot as plt def custom_line_plot(x, y, context, custom_rc=None): sns.set_context(context, rc=custom_rc) sns.lineplot(x=x, y=y) plt.show() # Example usage: x = [0, 1, 2] y = [1, 3, 2] context = \\"notebook\\" custom_rc = {\\"lines.linewidth\\": 3} custom_line_plot(x, y, context, custom_rc) ``` **Note**: For the function to work as expected, ensure you have the Seaborn and Matplotlib libraries installed.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_line_plot(x, y, context, custom_rc=None): Generates a line plot with the specified context and custom parameters. Parameters: x (list of int/float): the x-coordinates of the data points. y (list of int/float): the y-coordinates of the data points. context (str): a string representing the context to be set for the plot. Can be one of \\"paper\\", \\"notebook\\", \\"talk\\", or \\"poster\\". custom_rc (dict): a dictionary of custom parameters for the plot. Returns: None if not isinstance(x, list) or not isinstance(y, list): raise ValueError(\\"x and y should be lists.\\") if len(x) != len(y): raise ValueError(\\"The lengths of x and y should be equal.\\") if context not in [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"]: raise ValueError(\\"Context must be one of \'paper\', \'notebook\', \'talk\', or \'poster\'.\\") sns.set_context(context, rc=custom_rc) sns.lineplot(x=x, y=y) plt.show() # Example usage: # x = [0, 1, 2] # y = [1, 3, 2] # context = \\"notebook\\" # custom_rc = {\\"lines.linewidth\\": 3} # custom_line_plot(x, y, context, custom_rc)"},{"question":"Objective: You need to demonstrate your understanding of handling the Python `None` object and how to work with functions that may return `None` or other values. This assessment will challenge your ability to manage these instances correctly and implement robust Python functions. Task: Write a Python function `process_maybe_none(data_list: list) -> list` that processes a list of mixed data types and performs the following actions: 1. Removes any instance of `None` from the list. 2. Returns a new list where all `None` values are replaced with the string `\\"None\\"`. Detailed Requirements: - The input `data_list` is a list of elements that can be of any data type (e.g., integers, floats, strings, `None`, etc.) - You need to iterate over each element in the list and: * If the element is `None`, replace it with the string `\\"None\\"`. * Otherwise, leave the element unchanged. - The output should be a new list with items replaced as specified. - Do **not** modify the input list directly. Input: - A list `data_list` which can contain elements of any type (e.g., `[12, None, \'Python\', None, 3.14]`). Output: - A new list with `None` values replaced by the string `\\"None\\"` (e.g., `[12, \'None\', \'Python\', \'None\', 3.14]`). Constraints: - The function should perform efficiently; you may assume the input list will not exceed 10,000 elements. - You may not use any external libraries; only standard Python operations should be used. Example: ```python def process_maybe_none(data_list): # Implementation here data_list = [12, None, \'Python\', None, 3.14] print(process_maybe_none(data_list)) # Expected output: [12, \'None\', \'Python\', \'None\', 3.14] ``` Notes: - Pay special attention to ensure that the original input list is not modified. - You should also handle edge cases, such as empty lists or lists without any `None` values.","solution":"def process_maybe_none(data_list): Processes the input list to replace None values with the string \\"None\\". return [\'None\' if element is None else element for element in data_list]"},{"question":"# Python Coding Exercise: Implement Custom Slice Behavior Objective Create a Python class called `CustomSlice` that replicates some behaviors of a Python slice object using the provided `python310` functions. This exercise will test your understanding of creating and manipulating slice objects at a low level, mirroring Python’s default behavior. Problem Statement Implement the `CustomSlice` class with the following methods: 1. **`__init__(self, start, stop, step)`**: Initialize a `CustomSlice` instance using `PySlice_New()`. 2. **`get_slice_indices(self, length)`**: Given the length of a sequence, retrieve and return the slice indices (start, stop, step) using `PySlice_GetIndicesEx()` and adjust them using `PySlice_AdjustIndices()`. Specifications - **Input**: - For `__init__`: `start`, `stop`, and `step` are integers or `None`. - For `get_slice_indices`: `length` is a positive integer. - **Output**: - For `get_slice_indices`: A tuple `(start, stop, step)` after adjusting the indices. - **Constraints**: - Only handle non-resizable sequences. - Ensure robust error handling, i.e., return additional information if the operations fail. Example ```python # Example usage: cs = CustomSlice(1, 10, 2) indices = cs.get_slice_indices(20) print(indices) # Output might be (1, 10, 2) or adjusted values. ``` Notes - Specifically, utilize the provided functions from the `python310` documentation to implement the functionality. - Ensure that your class handles edge cases such as negative indices and out-of-limit values. - You may assume that interfacing with the C API functions has been abstracted into corresponding Python functions that you can call directly. ```python # An abstracted example interface for the given C API functions: def PySlice_New(start, stop, step): # Implementation where NULL values are represented by None in Python pass def PySlice_GetIndicesEx(slice_obj, length, start, stop, step, slicelength): # Function to fetch indices from a slice pass def PySlice_AdjustIndices(length, start, stop, step): # Adjust indices as per given sequence length pass ``` Implement the functions to create a class that models slice behavior strictly using `python310` provided functionalities. Ensure your `CustomSlice` class is robust and error-free, and demonstrate comprehensive testing of edge cases.","solution":"class CustomSlice: def __init__(self, start, stop, step): Initialize a CustomSlice instance. self.slice = slice(start, stop, step) def get_slice_indices(self, length): Given the length of a sequence, retrieve and return the slice indices. start, stop, step = self.slice.indices(length) return start, stop, step"},{"question":"**Question: Managing Device Contexts with `torch.accelerator`** In this assessment, you are required to create a utility class that provides advanced handling and manipulation of device contexts and streams using the PyTorch `torch.accelerator` module. The class should manage multiple devices and allow switching between them seamlessly while ensuring proper synchronization. # Class Definition Create a class `DeviceManager` with the following specifications: 1. **Attributes:** - `current_device`: Stores the index of the current device. - `devices`: List of available devices indices. 2. **Methods:** - `__init__(self)`: Initializes the `DeviceManager` by setting the current device to index 0, if available, and populates the list of devices. - `switch_to_device(self, index)`: Switches to the device indexed by `index` if it exists. - `synchronize_current_device(self)`: Synchronizes the current device\'s streams. - `summary(self)`: Prints a summary of all devices and the current device. # Implementation Details - Use `torch.accelerator.device_count()` to get the number of available devices. - Use `torch.accelerator.set_device_index()` and `torch.accelerator.current_device_index()` to manage the current device. - Use `torch.accelerator.synchronize()` to synchronize the current device. - Ensure that the class handles scenarios where devices might not be available gracefully (i.e., when `torch.accelerator.device_count()` returns 0). # Constraints - The class should handle the cases where there are no available devices without raising unhandled exceptions. - The class should effectively switch between devices and ensure the current context is correctly managed. # Example Usage ```python manager = DeviceManager() manager.summary() # Prints the available devices and current device. manager.switch_to_device(1) manager.summary() # Prints the available devices and current device. manager.synchronize_current_device() ``` # Expected Output: 1. Initializing on a system with 2 devices might show: ``` Available Devices: [0, 1] Current Device: 0 ``` 2. After switching to device 1 and printing summary: ``` Available Devices: [0, 1] Current Device: 1 ``` 3. After synchronizing the current device, there should be no output but the device should be synchronized. # Performance Requirement The class implementation should ensure minimal overhead in switching devices and synchronizing them, assuming typical use cases involving GPU-accelerated computations.","solution":"import torch class DeviceManager: def __init__(self): self.devices = list(range(torch.cuda.device_count())) self.current_device = self.devices[0] if self.devices else None def switch_to_device(self, index): if index in self.devices: torch.cuda.set_device(index) self.current_device = index else: raise ValueError(f\\"Device index {index} is not available\\") def synchronize_current_device(self): if self.current_device is not None: torch.cuda.synchronize(self.current_device) def summary(self): if not self.devices: print(\\"No devices available.\\") else: print(f\\"Available Devices: {self.devices}\\") print(f\\"Current Device: {self.current_device}\\")"},{"question":"# Priority Queue Management with Dynamic Tasks **Objective:** Implement a priority queue system using the `heapq` module, accommodating dynamic task priorities. This exercise will test your understanding of heap operations, dynamic updates, and efficient algorithms. **Scenario:** You are managing tasks with varying priorities in a system. Each task has a name and a priority level, represented as a tuple `(priority, task_name)`. Tasks can be added, their priorities can be updated, and the task with the highest priority (lowest priority number) must be efficiently retrievable and removable. **Requirements:** 1. Implement the following functions to manage the priority queue: - `add_task(pq, entry_finder, task, priority)`: Add a new task or update the priority of an existing task. - `remove_task(pq, entry_finder, task)`: Mark an existing task as \'REMOVED\' to maintain the heap invariant. - `pop_task(pq, entry_finder)`: Remove and return the task with the highest priority. 2. Use the `heapq` module for all heap operations. 3. Ensure that tasks with the same priority are processed in the order they were added. 4. Implement proper handling for tasks marked as \'REMOVED\' when they are encountered during heap operations. **Function Signatures:** ```python import heapq import itertools REMOVED = \'<removed-task>\' counter = itertools.count() def add_task(pq, entry_finder, task, priority): Adds a new task to the priority queue or updates the priority of an existing task. Parameters: - pq: List representing the heap-based priority queue. - entry_finder: Dictionary mapping tasks to their heap entries. - task: The name of the task being added or updated. - priority: The new priority for the task. pass def remove_task(pq, entry_finder, task): Marks an existing task as REMOVED. Parameters: - pq: List representing the heap-based priority queue. - entry_finder: Dictionary mapping tasks to their heap entries. - task: The name of the task to be removed. pass def pop_task(pq, entry_finder): Removes and returns the task with the highest priority. Parameters: - pq: List representing the heap-based priority queue. - entry_finder: Dictionary mapping tasks to their heap entries. Returns: - task: The name of the task with the highest priority. pass ``` **Constraints:** 1. Each task name is unique. 2. The priority queue can contain up to 10^5 tasks. 3. Operations on the priority queue must be efficient, with each heap operation in logarithmic time. **Example:** ```python pq = [] entry_finder = {} add_task(pq, entry_finder, \'task1\', 5) add_task(pq, entry_finder, \'task2\', 9) add_task(pq, entry_finder, \'task3\', 3) print(pop_task(pq, entry_finder)) # Output: \'task3\' add_task(pq, entry_finder, \'task1\', 2) print(pop_task(pq, entry_finder)) # Output: \'task1\' remove_task(pq, entry_finder, \'task2\') add_task(pq, entry_finder, \'task4\', 7) print(pop_task(pq, entry_finder)) # Output: \'task4\' ``` This problem will test your ability to use the `heapq` module effectively, manage priority queues, and handle dynamic updates efficiently.","solution":"import heapq import itertools REMOVED = \'<removed-task>\' counter = itertools.count() def add_task(pq, entry_finder, task, priority): Adds a new task to the priority queue or updates the priority of an existing task. Parameters: - pq: List representing the heap-based priority queue. - entry_finder: Dictionary mapping tasks to their heap entries. - task: The name of the task being added or updated. - priority: The new priority for the task. if task in entry_finder: remove_task(pq, entry_finder, task) count = next(counter) entry = [priority, count, task] entry_finder[task] = entry heapq.heappush(pq, entry) def remove_task(pq, entry_finder, task): Marks an existing task as REMOVED. Parameters: - pq: List representing the heap-based priority queue. - entry_finder: Dictionary mapping tasks to their heap entries. - task: The name of the task to be removed. entry = entry_finder.pop(task) entry[-1] = REMOVED def pop_task(pq, entry_finder): Removes and returns the task with the highest priority. Parameters: - pq: List representing the heap-based priority queue. - entry_finder: Dictionary mapping tasks to their heap entries. Returns: - task: The name of the task with the highest priority. while pq: priority, count, task = heapq.heappop(pq) if task is not REMOVED: del entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\')"},{"question":"# Shared Memory Synchronization Challenge The objective of this assessment is to create a system using the `multiprocessing.shared_memory` module to achieve shared memory synchronization between multiple processes. You will need to implement a function that spawns multiple processes to work on a shared list of integers. Here are the specific requirements: Requirements: 1. **Function Definition**: Implement the function `synchronize_shared_memory(lst1, operation)`: - `lst1`: A list of integers. - `operation`: A string value, either \'increment\' or \'double\'. 2. **Shared Memory Handling**: - Use the `ShareableList` class to share the list `lst1` among multiple processes. - Each process should either increment each element by 1 (`operation=\'increment\'`) or double each element (`operation=\'double\'`). 3. **Concurrency**: - Create a minimum of 2 and a maximum of 4 processes. - Each process should perform the specified operation on the shared list. 4. **Synchronization**: - Ensure that the changes made by one process are reflected and accessible to other processes. 5. **Cleanup**: - Properly close and unlink the shared memory once all processes have finished. Constraints: - The length of `lst1` will be between 5 and 100. - The values in `lst1` will be integers between -1,000 and 1,000. Expected Output: - The function should return a list with the updated values after all processes have completed their operations. ```python import multiprocessing from multiprocessing import shared_memory from multiprocessing.managers import SharedMemoryManager import random def worker_func(shared_list, operation): for i in range(len(shared_list)): if operation == \'increment\': shared_list[i] += 1 elif operation == \'double\': shared_list[i] *= 2 def synchronize_shared_memory(lst1, operation): with SharedMemoryManager() as smm: # Create shared list shared_list = smm.ShareableList(lst1) # Create and start worker processes processes = [] for _ in range(random.randint(2, 4)): # Spawn between 2 and 4 processes p = multiprocessing.Process(target=worker_func, args=(shared_list, operation)) p.start() processes.append(p) # Wait for all processes to finish for p in processes: p.join() # Cleanup result = list(shared_list) shared_list.shm.close() shared_list.shm.unlink() return result # Example Usage: # result = synchronize_shared_memory([1, 2, 3, 4], \'increment\') # print(result) # expected output with \'increment\' operation: [3, 4, 5, 6] ``` Ensure your solution handles shared memory properly to reflect changes across all processes and enforces appropriate cleanup of shared memory resources.","solution":"import multiprocessing from multiprocessing import shared_memory from multiprocessing.managers import SharedMemoryManager def worker_func(shared_list, operation): Function to operate on shared_list by specified operation. for i in range(len(shared_list)): if operation == \'increment\': shared_list[i] += 1 elif operation == \'double\': shared_list[i] *= 2 def synchronize_shared_memory(lst1, operation): Synchronizes shared memory across multiple processes to perform operations on lst1. - `lst1`: The initial list of integers. - `operation`: \'increment\' or \'double\' operation to apply on each element. Returns the updated list after all processes have finished their operations. with SharedMemoryManager() as smm: # Create shared list shared_list = smm.ShareableList(lst1) # Determine the number of processes to spawn num_processes = 4 # Fixed to maximum allowed, can be changed to a random number between 2 and 4 # Create and start worker processes processes = [] for _ in range(num_processes): p = multiprocessing.Process(target=worker_func, args=(shared_list, operation)) p.start() processes.append(p) # Wait for all processes to finish for p in processes: p.join() # Get result and clean up result = list(shared_list) shared_list.shm.close() shared_list.shm.unlink() return result"},{"question":"# Question: You are given a dataset containing information about multiple movies and their respective genres. Each movie can belong to multiple genres. Your task is to preprocess this data to train a machine learning model that predicts the genres of a given movie. **Input:** - A list of movie titles. - A corresponding list of lists, where each inner list contains the genres of the respective movie. **Output:** - A preprocessed binary indicator matrix where each row represents a movie and each column represents a genre. A value of 1 in the matrix indicates that the movie belongs to the corresponding genre, and 0 otherwise. # Constraints: 1. You must use scikit-learn\'s `MultiLabelBinarizer` for this task. 2. Handle any necessary imports and ensure your solution is efficient in terms of performance. # Example: ```python def preprocess_movie_genres(movie_titles, movie_genres): Preprocess the movie genres to create a binary indicator matrix. Parameters: movie_titles (list of str): List of movie titles. movie_genres (list of list of str): List of lists, where each inner list contains genres for the respective movie. Returns: np.array: Binary indicator matrix of shape (n_movies, n_genres). # Your implementation here # Example usage: movie_titles = [\\"Movie A\\", \\"Movie B\\", \\"Movie C\\"] movie_genres = [[\\"Action\\", \\"Sci-Fi\\"], [\\"Drama\\"], [\\"Action\\", \\"Drama\\", \\"Sci-Fi\\"]] result = preprocess_movie_genres(movie_titles, movie_genres) print(result) # Expected Output: # array([[1, 0, 1, 0], # [0, 1, 0, 0], # [1, 1, 1, 0]]) ``` # Notes: - The genres in the output matrix columns should be sorted alphabetically. - Ensure your function works for any list of movie titles and genres.","solution":"import numpy as np from sklearn.preprocessing import MultiLabelBinarizer def preprocess_movie_genres(movie_titles, movie_genres): Preprocess the movie genres to create a binary indicator matrix. Parameters: movie_titles (list of str): List of movie titles. movie_genres (list of list of str): List of lists, where each inner list contains genres for the respective movie. Returns: np.array: Binary indicator matrix of shape (n_movies, n_genres). mlb = MultiLabelBinarizer() binary_matrix = mlb.fit_transform(movie_genres) return binary_matrix"},{"question":"# Question: HTML Entity Conversion You are provided with the `html.entities` module documentation which includes four dictionaries: `html5`, `entitydefs`, `name2codepoint`, and `codepoint2name`. Your task is to implement a function that converts a string containing HTML named entities into its equivalent Unicode string, and also convert it back from Unicode to its HTML named entities. # Function 1: `html_to_unicode(html_string: str) -> str` Write a function that takes a string containing HTML named entities and returns the corresponding string with all entities replaced by their equivalent Unicode characters. Input: - `html_string`: A string that may contain HTML named entities (e.g. `\\"&gt;1 &le; 3 &amp; x &ne; y\\"`). Output: - A string with all HTML entities replaced by their corresponding Unicode characters (e.g. `\\">1 ≤ 3 & x ≠ y\\"`). Constraints: - You must use the `html.entities.html5` dictionary to perform the conversion. - Ensure the function handles both entities with and without trailing semicolons. # Function 2: `unicode_to_html(unicode_string: str) -> str` Write a function that takes a Unicode string and converts it back to a string containing HTML named entities. Input: - `unicode_string`: A string containing Unicode characters (e.g. `\\">1 ≤ 3 & x ≠ y\\"`). Output: - A string where specific characters are replaced by their corresponding HTML named entities (e.g. `\\"&gt;1 &le; 3 &amp; x &ne; y\\"`). Constraints: - You must use the `html.entities.codepoint2name` dictionary to perform the conversion. - Only convert characters that have corresponding HTML named entities in the dictionary. # Example Usage: ```python # Example for Function 1: html_str = \\"&gt;1 &le; 3 &amp; x &ne; y\\" result = html_to_unicode(html_str) print(result) # Output: \\">1 ≤ 3 & x ≠ y\\" # Example for Function 2: unicode_str = \\">1 ≤ 3 & x ≠ y\\" result = unicode_to_html(unicode_str) print(result) # Output: \\"&gt;1 &le; 3 &amp; x &ne; y\\" ``` # Note: - You may assume all input strings are well-formed. - Your implementations should handle both conversion tasks efficiently.","solution":"import html.entities def html_to_unicode(html_string: str) -> str: Converts a string containing HTML named entities into its equivalent Unicode string. for entity, char in html.entities.html5.items(): # Replace entities with semicolon html_string = html_string.replace(f\\"&{entity};\\", char) # Replace entities without semicolon html_string = html_string.replace(f\\"&{entity}\\", char) return html_string def unicode_to_html(unicode_string: str) -> str: Converts a Unicode string into its equivalent HTML named entities string. for codepoint, name in html.entities.codepoint2name.items(): # Replace characters with names unicode_string = unicode_string.replace(chr(codepoint), f\\"&{name};\\") return unicode_string"},{"question":"# Question: Custom Content Handler for Email Messages You are tasked with extending the functionality of the `email.contentmanager` module by implementing custom content handlers. This will involve creating functions that can extract and store content for a specific custom MIME type and registering these handlers with a `ContentManager` instance. Objectives: 1. **Implement Custom Get Handler**: Write a function `custom_get_handler(msg, *args, **kw)` that extracts the payload from an email message object with the MIME type `application/custom`. 2. **Implement Custom Set Handler**: Write a function `custom_set_handler(msg, obj, *args, **kw)` that sets the payload into an email message object with the MIME type `application/custom`. 3. **Register the Handlers**: Create a new instance of `ContentManager` and register the custom handlers for the MIME type `application/custom`. Instructions: 1. **custom_get_handler(msg, *args, **kw)**: - **Input**: `msg` (email message object), `*args`, `**kw`. - **Output**: Return the payload as a string if `msg` has the MIME type `application/custom`, otherwise raise a `KeyError`. - Additional Requirement: If `msg.get_content_type()` is not `application/custom`, the function should raise a `KeyError`. 2. **custom_set_handler(msg, obj, *args, **kw)**: - **Input**: `msg` (email message object), `obj` (string representing the payload), `*args`, `**kw`. - **Output**: Set the payload of `msg` to `obj` and set the MIME type `application/custom` if `obj` is a string. - Additional Requirement: If `obj` is not a string, the function should raise a `TypeError`. 3. **Register Handlers**: - Create an instance of `ContentManager`. - Register `custom_get_handler` for `application/custom`. - Register `custom_set_handler` for `application/custom`. Example Usage: ```python from email.message import EmailMessage from email.contentmanager import ContentManager # Implement custom handlers def custom_get_handler(msg, *args, **kw): if msg.get_content_type() != \'application/custom\': raise KeyError(\\"Unsupported MIME type\\") return msg.get_payload() def custom_set_handler(msg, obj, *args, **kw): if not isinstance(obj, str): raise TypeError(\\"Payload must be a string\\") msg.set_payload(obj) msg.set_type(\'application/custom\') # Create ContentManager instance and register handlers content_manager = ContentManager() content_manager.add_get_handler(\'application/custom\', custom_get_handler) content_manager.add_set_handler(str, custom_set_handler) # Example email message msg = EmailMessage() content_manager.set_content(msg, \\"This is a custom payload\\") print(msg[\'Content-Type\']) # application/custom print(content_manager.get_content(msg)) # This is a custom payload ``` Constraints: - Use Python 3.6 or later. - Assume that the `email` package is already available. - Ensure robust error handling as specified. Evaluation Criteria: - Correct and efficient implementation of `custom_get_handler` and `custom_set_handler`. - Proper registration of handlers with the `ContentManager`. - Code should be well-documented and follow best practices.","solution":"from email.message import EmailMessage from email.contentmanager import ContentManager def custom_get_handler(msg, *args, **kw): Extracts the payload from an email message object if the MIME type is \'application/custom\'. Raises a KeyError if the MIME type is not \'application/custom\'. if msg.get_content_type() != \'application/custom\': raise KeyError(\\"Unsupported MIME type\\") return msg.get_payload() def custom_set_handler(msg, obj, *args, **kw): Sets the payload into an email message object with MIME type \'application/custom\'. Raises a TypeError if obj is not a string. if not isinstance(obj, str): raise TypeError(\\"Payload must be a string\\") msg.set_payload(obj) msg.set_type(\'application/custom\') # Create ContentManager instance and register handlers content_manager = ContentManager() content_manager.add_get_handler(\'application/custom\', custom_get_handler) content_manager.add_set_handler(str, custom_set_handler)"},{"question":"**Title:** Implement a Package Inspector using `pkgutil` Module **Problem Statement:** You are required to implement a function, `package_inspector`, that takes the name of a Python package and returns information about all submodules and resources available within that package. This function should utilize the `pkgutil` module to gather the necessary information. # Function Signature ```python def package_inspector(package_name: str) -> dict: Inspects a given package and returns information about its submodules and resources. Parameters: package_name (str): The name of the package to inspect. Returns: dict: A dictionary containing the following keys: - \'submodules\': A list of names of all submodules within the package. - \'resources\': A list of resources within the package (if any). ``` # Input: - `package_name` - A string representing the name of the package to inspect. # Output: - Returns a dictionary with two keys: - `\'submodules\'`: A list of submodule names within the given package. - `\'resources\'`: A list of resource file names within the package directory. # Constraints: - Assume the package name is valid and installed in the environment where the function is executed. - You are not required to handle exceptions for invalid package names. # Requirements: - Use the `pkgutil.iter_modules` function to list all submodules of the given package. - Use the `pkgutil.get_data` function to check for the existence of resources and list them (if any). # Example: ```python result = package_inspector(\\"example_package\\") print(result) # Output: # { # \'submodules\': [\'example_package.submodule1\', \'example_package.submodule2\'], # \'resources\': [\'data/resource1.txt\', \'config/settings.cfg\'] # } ``` In this example, `example_package` has two submodules and two resource files. # Notes: - A submodule should be added to the `\'submodules\'` list if it is successfully found using `pkgutil.iter_modules`. - A resource should be added to the `\'resources\'` list if it is successfully read using `pkgutil.get_data`. # Hints: - Utilize the `path` attribute from the inspected package to list resources. - Consider reading the `__path__` attribute of the package for listing submodules.","solution":"import pkgutil def package_inspector(package_name: str) -> dict: Inspects a given package and returns information about its submodules and resources. Parameters: package_name (str): The name of the package to inspect. Returns: dict: A dictionary containing the following keys: - \'submodules\': A list of names of all submodules within the package. - \'resources\': A list of resources within the package (if any). package = __import__(package_name) submodules = [] resources = [] if hasattr(package, \'__path__\'): for finder, name, ispkg in pkgutil.iter_modules(package.__path__): submodules.append(f\\"{package_name}.{name}\\") for _, name, ispkg in pkgutil.walk_packages(package.__path__, package_name + \\".\\"): if not ispkg: submodules.append(name) for item in package.__path__: for resource in pkgutil.iter_modules([item]): if not resource.ispkg: data = pkgutil.get_data(package_name, resource.name) if data: resources.append(resource.name) return { \'submodules\': submodules, \'resources\': resources }"},{"question":"**Objective:** Write a Python function that takes a list of filenames (pointing to sound files) as input and returns a dictionary where each key is the filename and the associated value is the type of the sound file (filetype). If the sound type cannot be determined for a file, the value should be `None`. # Function Signature ```python def identify_sound_types(filenames: list) -> dict: pass ``` # Input - A list of strings, each representing the filename of a sound file. # Output - A dictionary with filenames as keys and the corresponding sound file type as values. If the type cannot be determined, the value should be `None`. # Constraints - Use the `sndhdr.what` function to determine the sound file type. - The filenames passed to the function are guaranteed to be valid paths to existing files. - Handle cases where the sound type is undetermined gracefully by setting their value to `None`. # Example ```python # Example usage: filenames = [\\"sound1.wav\\", \\"sound2.aiff\\", \\"sound3.unknown\\"] result = identify_sound_types(filenames) print(result) # Expected Output: {\'sound1.wav\': \'wav\', \'sound2.aiff\': \'aiff\', \'sound3.unknown\': None} ``` # Performance Requirements - The solution should efficiently handle the identification process even if the list contains up to a few thousand filenames. **Notes:** - Remember to handle all possible returned values from the `sndhdr.what` function, especially the cases where the returned value is `None`.","solution":"import sndhdr def identify_sound_types(filenames): Takes a list of filenames pointing to sound files and returns a dictionary. Each key is the filename, and the value is the type of the sound file (filetype). If the sound type cannot be determined, the value will be None. :param filenames: List of strings representing the filenames of sound files. :return: Dictionary with filenames as keys and sound file types as values. result = {} for filename in filenames: info = sndhdr.what(filename) result[filename] = info.filetype if info else None return result"},{"question":"# Question **Implement a Custom Data Type with Special Methods and Context Management** You are required to design a custom container data type in Python with the following features: 1. **Initialization**: The container should be initialized with any iterable (list, set, tuple, etc.) of integers. 2. **Sequence Protocol**: Implement methods to support getting, setting, and deleting items, both using index and slices. 3. **Context Management**: Support context management to temporarily append an item to the container within a `with` statement. 4. **Callable Objects**: Make the container callable such that calling the container with an index returns the square of the element at that index. 5. **Custom Class Creation**: Use a metaclass to track the number of instances created from this container class. # Specifications - The class should be named `CustomContainer`. - Implement special methods `__getitem__`, `__setitem__`, `__delitem__`. - Implement the `__iter__` method to return an iterator. - Implement the context manager methods `__enter__` and `__exit__`. - Implement the `__call__` method to enable callable functionality. - Use a metaclass to track instances and store the instance count in a class attribute `instance_count`. # Example Usage ```python # Assuming CustomContainer is implemented correctly. container = CustomContainer([1, 2, 3, 4]) print(container[1]) # Output: 2 container[2] = 10 print(container[2]) # Output: 10 del container[3] print(list(container)) # Output: [1, 2, 10] # Context Management with container as c: c.append(99) print(list(c)) # Output within context: [1, 2, 10, 99] print(list(container)) # Output after context: [1, 2, 10] # Callable print(container(0)) # Output: 1 (since 1^2 = 1) print(container(2)) # Output: 100 (since 10^2 = 100) # Instance Count print(CustomContainer.instance_count) # Output: 1 container2 = CustomContainer([5, 6, 7]) print(CustomContainer.instance_count) # Output: 2 ``` # Constraints - The container should only accept integers; otherwise, raise a `TypeError`. - Slicing should work for both getting and setting values. # Performance Requirements - Efficiently handle sequences with up to 10^6 elements. - Ensure that additions, deletions, and context management operations are performed in O(1) amortized time.","solution":"class CustomContainerMeta(type): instance_count = 0 def __call__(cls, *args, **kwargs): instance = super().__call__(*args, **kwargs) cls.instance_count += 1 return instance class CustomContainer(metaclass=CustomContainerMeta): def __init__(self, iterable): self._data = list(iterable) for item in self._data: if not isinstance(item, int): raise TypeError(\\"All items must be integers.\\") def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): if isinstance(index, slice): if not all(isinstance(item, int) for item in value): raise TypeError(\\"All items must be integers.\\") elif not isinstance(value, int): raise TypeError(\\"Value must be an integer.\\") self._data[index] = value def __delitem__(self, index): del self._data[index] def __iter__(self): return iter(self._data) def __call__(self, index): return self._data[index] ** 2 def __enter__(self): self._temp_data = self._data[:] return self._data def __exit__(self, exc_type, exc_val, exc_tb): self._data = self._temp_data def append(self, item): if not isinstance(item, int): raise TypeError(\\"Item must be an integer.\\") self._data.append(item) def __len__(self): return len(self._data)"},{"question":"**Question: Implement a custom outlier detection function using scikit-learn\'s Isolation Forest** # Problem Statement You are required to design and implement a function named `detect_outliers` that uses the `IsolationForest` class from scikit-learn to detect outliers in a given dataset. The function should take the following parameters: - `X_train`: a 2D numpy array representing the training data - `X_test`: a 2D numpy array representing the test data - `n_estimators`: (optional) an integer representing the number of base estimators in the ensemble, default value = 100 - `contamination`: (optional) a float in the range (0, 0.5) specifying the proportion of outliers in the data set, default value = 0.1 The function should return a tuple of two elements: 1. A 1D numpy array containing the predicted labels for `X_test` where 1 indicates an inlier and -1 indicates an outlier. 2. A 1D numpy array containing the anomaly scores for `X_test`. # Function Signature ```python def detect_outliers(X_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 100, contamination: float = 0.1) -> Tuple[np.ndarray, np.ndarray]: pass ``` # Example ```python import numpy as np X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) X_test = np.array([[1, 2], [10, 20], [3, 4]]) predicted_labels, anomaly_scores = detect_outliers(X_train, X_test) print(\\"Predicted Labels:\\", predicted_labels) # Output: Predicted Labels: [ 1 -1 1] print(\\"Anomaly Scores:\\", anomaly_scores) # Output: Anomaly Scores: [-0.1 -0.5 -0.1] ``` # Constraints - You can assume that the input arrays are valid numpy arrays with numeric data. - Your implementation should handle edge cases such as very small or very large values for `n_estimators` and `contamination`. - The performance of your function should be optimal for datasets with up to 100,000 samples. # Notes - Refer to the scikit-learn documentation for IsolationForest for detailed information on this algorithm. - You can import necessary modules and classes from scikit-learn and numpy.","solution":"import numpy as np from sklearn.ensemble import IsolationForest from typing import Tuple def detect_outliers(X_train: np.ndarray, X_test: np.ndarray, n_estimators: int = 100, contamination: float = 0.1) -> Tuple[np.ndarray, np.ndarray]: Detects outliers in the given test dataset based on the training dataset using Isolation Forest. Parameters: - X_train: 2D numpy array representing the training data - X_test: 2D numpy array representing the test data - n_estimators: (optional) integer representing the number of base estimators in the ensemble, default value = 100 - contamination: (optional) float in the range (0, 0.5) specifying the proportion of outliers in the data set, default value = 0.1 Returns: - Tuple containing: - A 1D numpy array of predicted labels for X_test where 1 indicates an inlier and -1 indicates an outlier. - A 1D numpy array containing the anomaly scores for X_test. # Initialize the Isolation Forest model model = IsolationForest(n_estimators=n_estimators, contamination=contamination, random_state=42) # Fit the model on the training data model.fit(X_train) # Predict the test data predicted_labels = model.predict(X_test) # Get the anomaly scores anomaly_scores = model.decision_function(X_test) return predicted_labels, anomaly_scores"},{"question":"# Assessment Question Problem Statement You are provided with a dataset containing information about various books. Each book entry includes the title, author, genre, and rating. Your task is to implement a class `Library` with the following requirements: 1. **Class Constructor**: The constructor should initialize an empty list to store book entries. 2. **Methods to Add and Remove Books**: Implement two methods: - `add_book(title: str, author: str, genre: str, rating: float) -> None`: This method should add a new book entry to the library. - `remove_book(title: str) -> bool`: This method should remove a book with the given title from the library and return `True` if the book was successfully removed, otherwise return `False`. 3. **Method to Get Books by Author**: Implement a method: - `get_books_by_author(author: str) -> List[Dict[str, Union[str, float]]]`: This method should return a list of all books written by the given author. Each book should be represented as a dictionary with keys `title`, `genre`, and `rating`. 4. **Method to Get the Top Rated Books**: Implement a method: - `get_top_rated_books(n: int) -> List[Dict[str, Union[str, float]]]`: This method should return the top `n` highest-rated books in the library. If there are fewer than `n` books, return all of them. The books should be sorted in descending order of their rating. 5. **Method to Get the Average Rating of a Genre**: Implement a method: - `get_average_rating(genre: str) -> float`: This method should return the average rating of all books in the given genre. If there are no books in that genre, return `0.0`. Input and Output Formats - The `Library` class should handle string inputs for titles, authors, and genres, and float inputs for ratings. - Methods should return lists of dictionaries or single values as specified. Constraints - Book titles are unique within the library. - Ratings will be floating-point numbers between 0.0 and 5.0. Example Usage ```python library = Library() library.add_book(\\"Book1\\", \\"Author1\\", \\"Fiction\\", 4.5) library.add_book(\\"Book2\\", \\"Author1\\", \\"Non-Fiction\\", 4.0) library.add_book(\\"Book3\\", \\"Author2\\", \\"Fiction\\", 5.0) print(library.get_books_by_author(\\"Author1\\")) # Output: [{\'title\': \'Book1\', \'genre\': \'Fiction\', \'rating\': 4.5}, {\'title\': \'Book2\', \'genre\': \'Non-Fiction\', \'rating\': 4.0}] print(library.get_top_rated_books(2)) # Output: [{\'title\': \'Book3\', \'genre\': \'Fiction\', \'rating\': 5.0}, {\'title\': \'Book1\', \'genre\': \'Fiction\', \'rating\': 4.5}] print(library.get_average_rating(\\"Fiction\\")) # Output: 4.75 print(library.remove_book(\\"Book1\\")) # Output: True print(library.get_books_by_author(\\"Author1\\")) # Output: [{\'title\': \'Book2\', \'genre\': \'Non-Fiction\', \'rating\': 4.0}] ``` Ensure your implementation passes the above test cases. You are free to add any additional helper methods if required.","solution":"from typing import List, Dict, Union class Library: def __init__(self): self.books = [] def add_book(self, title: str, author: str, genre: str, rating: float) -> None: self.books.append({ \'title\': title, \'author\': author, \'genre\': genre, \'rating\': rating }) def remove_book(self, title: str) -> bool: for book in self.books: if book[\'title\'] == title: self.books.remove(book) return True return False def get_books_by_author(self, author: str) -> List[Dict[str, Union[str, float]]]: return [ {k: v for k, v in book.items() if k != \'author\'} for book in self.books if book[\'author\'] == author ] def get_top_rated_books(self, n: int) -> List[Dict[str, Union[str, float]]]: return sorted(self.books, key=lambda x: x[\'rating\'], reverse=True)[:n] def get_average_rating(self, genre: str) -> float: genre_books = [book for book in self.books if book[\'genre\'] == genre] if not genre_books: return 0.0 return sum(book[\'rating\'] for book in genre_books) / len(genre_books)"},{"question":"**Objective:** To assess your understanding of scikit-learn\'s unsupervised dimensionality reduction techniques and their application in a machine learning pipeline. # Problem Statement You are provided with a dataset containing handwritten digits (each digit represented as a 64-dimensional feature vector). Your task is to write a Python function that performs the following steps: 1. **Standardize** the dataset to have zero mean and unit variance. 2. **Reduce the dimensionality** of the dataset to 10 dimensions using `PCA`. 3. **Reduce the dimensionality** of the dataset to 10 dimensions using `Random Projection`. 4. **Reduce the dimensionality** of the dataset to 10 dimensions using `Feature Agglomeration`. 5. Train a **K-Nearest Neighbors (KNN)** classifier using the original dataset and each of the reduced datasets, then evaluate and return the accuracy of each classifier. # Function Signature ```python from typing import Tuple, List import numpy as np def dimensionality_reduction_evaluation(X: np.ndarray, y: np.ndarray) -> Tuple[float, List[float]]: pass ``` # Input - `X` (numpy.ndarray): A 2D array of shape `(n_samples, n_features)` containing the feature vectors of the handwritten digits. - `y` (numpy.ndarray): A 1D array of shape `(n_samples,)` containing the labels of the handwritten digits. # Output - A tuple containing: 1. `original_accuracy` (float): Accuracy of the KNN classifier trained on the original dataset. 2. `reduced_accuracies` (List[float]): A list of three float values representing the accuracies of the KNN classifiers trained on the three reduced datasets (PCA, Random Projection, Feature Agglomeration). # Constraints - Use `sklearn` for all machine learning related tasks. - Use `KNeighborsClassifier` with `n_neighbors=5` for classification. - Perform a train-test split using `train_test_split` with `test_size=0.2` and `random_state=42`. # Example ```python from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.neighbors import KNeighborsClassifier def dimensionality_reduction_evaluation(X, y): # Your implementation here # Load dataset digits = load_digits() X, y = digits.data, digits.target # Evaluate original_accuracy, reduced_accuracies = dimensionality_reduction_evaluation(X, y) print(\\"Original Accuracy:\\", original_accuracy) print(\\"Reduced Accuracies:\\", reduced_accuracies) ``` # Additional Information You can proceed with the following steps as a guideline: 1. Standardize the features using `StandardScaler`. 2. Apply `PCA` to reduce dimensions and train KNN. 3. Apply `Random Projection` to reduce dimensions and train KNN. 4. Apply `Feature Agglomeration` to reduce dimensions and train KNN. 5. Return the accuracies as specified. Good luck!","solution":"from typing import Tuple, List import numpy as np from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.random_projection import GaussianRandomProjection from sklearn.cluster import FeatureAgglomeration from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def dimensionality_reduction_evaluation(X: np.ndarray, y: np.ndarray) -> Tuple[float, List[float]]: # Standardize the dataset scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42) # Original dataset knn = KNeighborsClassifier(n_neighbors=5) knn.fit(X_train, y_train) y_pred = knn.predict(X_test) original_accuracy = accuracy_score(y_test, y_pred) # PCA pca = PCA(n_components=10) X_pca = pca.fit_transform(X_scaled) X_train_pca, X_test_pca = train_test_split(X_pca, test_size=0.2, random_state=42) knn.fit(X_train_pca, y_train) y_pred_pca = knn.predict(X_test_pca) pca_accuracy = accuracy_score(y_test, y_pred_pca) # Random Projection rp = GaussianRandomProjection(n_components=10, random_state=42) X_rp = rp.fit_transform(X_scaled) X_train_rp, X_test_rp = train_test_split(X_rp, test_size=0.2, random_state=42) knn.fit(X_train_rp, y_train) y_pred_rp = knn.predict(X_test_rp) rp_accuracy = accuracy_score(y_test, y_pred_rp) # Feature Agglomeration fa = FeatureAgglomeration(n_clusters=10) X_fa = fa.fit_transform(X_scaled) X_train_fa, X_test_fa = train_test_split(X_fa, test_size=0.2, random_state=42) knn.fit(X_train_fa, y_train) y_pred_fa = knn.predict(X_test_fa) fa_accuracy = accuracy_score(y_test, y_pred_fa) return original_accuracy, [pca_accuracy, rp_accuracy, fa_accuracy]"},{"question":"# Advanced Python Data Types Assessment Problem Statement You are given a dataset of flights that log details about various aspects of each flight, such as departure time, arrival time, airport code, zone information, and status of the flight. Your task is to implement a function that will parse this dataset and provide insights into it. # Task 1. Given a list of dictionaries, where each dictionary contains: * `\'flight_id\'`: Unique identifier of the flight (string) * `\'departure_time\'`: Departure time of the flight (`datetime` object) * `\'arrival_time\'`: Arrival time of the flight (`datetime` object) * `\'departure_airport\'`: Airport code where the flight departs from (string) * `\'arrival_airport\'`: Airport code where the flight arrives (string) * `\'zone_info\'`: Time zone information for the departure and arrival airports (`zoneinfo.ZoneInfo` object) * `\'status\'`: Status of the flight, can be `\'on_time\'`, `\'delayed\'`, or `\'cancelled\'` (string) 2. Your function should compute the average flight duration, handle time zone conversions if necessary, return the number of flights per airport, and also return the count of flights based on their status. Function Signature ```python from datetime import datetime from zoneinfo import ZoneInfo from typing import List, Dict, Tuple def parse_flights(flights: List[Dict[str, any]]) -> Tuple[float, Dict[str, int], Dict[str, int]]: Computes insights from the given flight dataset. Parameters: flights (List[Dict[str, any]]): List of flight entries. Returns: (Tuple[float, Dict[str, int], Dict[str, int]]): - Average flight duration in hours (float) - Dictionary containing the number of flights per airport (Dict[str, int]) - Dictionary containing the count of flights based on status (Dict[str, int]) # Your implementation goes here ``` Input * `flights`: List of dictionaries, where each dictionary represents a flight and follows the described structure. Output * A tuple containing three elements: * Average flight duration in hours (float). * A dictionary mapping airport codes to the number of flights associated with each airport (both departures and arrivals). * A dictionary mapping flight statuses (`\'on_time\'`, `\'delayed\'`, `\'cancelled\'`) to their respective counts. Constraints * Use the `datetime` and `zoneinfo` modules to manage and convert time zones. * Ensure that the function handles edge cases such as flights crossing midnight or spanning multiple days appropriately. * Efficient handling of the input data, aiming for linear time complexity with respect to the number of flights. Example ```python from datetime import datetime from zoneinfo import ZoneInfo flights_data = [ { \'flight_id\': \'A123\', \'departure_time\': datetime(2023, 3, 1, 8, 0, tzinfo=ZoneInfo(\'America/New_York\')), \'arrival_time\': datetime(2023, 3, 1, 11, 0, tzinfo=ZoneInfo(\'America/Los_Angeles\')), \'departure_airport\': \'JFK\', \'arrival_airport\': \'LAX\', \'zone_info\': {\'departure\': ZoneInfo(\'America/New_York\'), \'arrival\': ZoneInfo(\'America/Los_Angeles\')}, \'status\': \'on_time\' }, { \'flight_id\': \'B456\', \'departure_time\': datetime(2023, 3, 1, 9, 0, tzinfo=ZoneInfo(\'Europe/London\')), \'arrival_time\': datetime(2023, 3, 1, 13, 0, tzinfo=ZoneInfo(\'Europe/Rome\')), \'departure_airport\': \'LHR\', \'arrival_airport\': \'FCO\', \'zone_info\': {\'departure\': ZoneInfo(\'Europe/London\'), \'arrival\': ZoneInfo(\'Europe/Rome\')}, \'status\': \'delayed\' } ] average_duration, flights_per_airport, flight_status_counts = parse_flights(flights_data) print(average_duration) # Output: 4.5 print(flights_per_airport) # Output: {\'JFK\': 1, \'LAX\': 1, \'LHR\': 1, \'FCO\': 1} print(flight_status_counts) # Output: {\'on_time\': 1, \'delayed\': 1, \'cancelled\': 0} ```","solution":"from datetime import datetime from zoneinfo import ZoneInfo from typing import List, Dict, Tuple def parse_flights(flights: List[Dict[str, any]]) -> Tuple[float, Dict[str, int], Dict[str, int]]: Computes insights from the given flight dataset. Parameters: flights (List[Dict[str, any]]): List of flight entries. Returns: (Tuple[float, Dict[str, int], Dict[str, int]]): - Average flight duration in hours (float) - Dictionary containing the number of flights per airport (Dict[str, int]) - Dictionary containing the count of flights based on status (Dict[str, int]) total_duration = 0 flight_count = 0 airport_counts = {} status_counts = {\'on_time\': 0, \'delayed\': 0, \'cancelled\': 0} for flight in flights: # Calculate duration departure_time = flight[\'departure_time\'] arrival_time = flight[\'arrival_time\'] duration = (arrival_time - departure_time).total_seconds() / 3600 total_duration += duration flight_count += 1 # Calculate airport counts departure_airport = flight[\'departure_airport\'] arrival_airport = flight[\'arrival_airport\'] if departure_airport in airport_counts: airport_counts[departure_airport] += 1 else: airport_counts[departure_airport] = 1 if arrival_airport in airport_counts: airport_counts[arrival_airport] += 1 else: airport_counts[arrival_airport] = 1 # Update status count status = flight[\'status\'] if status in status_counts: status_counts[status] += 1 else: status_counts[status] = 1 average_duration = total_duration / flight_count if flight_count > 0 else 0 return average_duration, airport_counts, status_counts"},{"question":"# Question: Enhanced Traceback Handler You are required to implement a custom exception handler using the `cgitb` module. The custom handler should: 1. Enable `cgitb` with the ability to log tracebacks to a specified directory. 2. Format the tracebacks in both text and HTML formats. 3. Handle uncaught exceptions and provide an option to email the tracebacks to a specified address. Part 1: Implement `enable_cgitb` Create a function `enable_cgitb(logdir: str, display: bool = False)` that: - Enables `cgitb` with the `logdir` parameter set to the provided directory and `display` based on the provided boolean value. - Sets the `context` to 10 lines of code around the error and the output format to HTML by default. Part 2: Implement `format_traceback` Create a function `format_traceback() -> tuple` that: - Captures the current exception information. - Returns the traceback formatted as plain text and HTML. Part 3: Implement `email_traceback` Create a function `email_traceback(to_email: str, subject: str)` that: - Uses the `smtplib` module to send an email to the specified address (`to_email`) with the subject line (`subject`). - The email should contain both the text and HTML formatted tracebacks. Part 4: Integrate it all Write a script that: - Demonstrates the use of `enable_cgitb` to activate traceback logging to the directory `./tracebacks`. - Simulates an error to trigger the exception handler. - Emails the captured tracebacks to a sample email address. Constraints - Do not use any external libraries except for `smtplib` for sending emails. - Ensure that the `logdir` directory exists before logging tracebacks to it. - Use appropriate exception handling to manage any errors that occur while sending emails. Input and Output - Function `enable_cgitb(logdir: str, display: bool = False)`: - Inputs: `logdir` - directory path (string), `display` - boolean flag - No output - Function `format_traceback()`: - Inputs: None - Output: Tuple containing (text traceback, HTML traceback) - Function `email_traceback(to_email: str, subject: str)`: - Inputs: `to_email` - recipient email (string), `subject` - email subject (string) - No output - The script should print a message indicating that an email has been sent. Example ```python # Sample usage of the script enable_cgitb(logdir=\'./tracebacks\', display=True) try: # Simulate an exception x = 1 / 0 except: text_traceback, html_traceback = format_traceback() email_traceback(\'admin@example.com\', \'Error Occurred\') ``` You are expected to create the functions and the script as described. Test your functions thoroughly to ensure they work under various conditions.","solution":"import cgitb import os import sys import smtplib from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText def enable_cgitb(logdir: str, display: bool = False): Enables cgitb error logging. Args: logdir (str): Directory for log files. display (bool): Flag to display tracebacks in console. if not os.path.exists(logdir): os.makedirs(logdir) cgitb.enable(logdir=logdir, display=display, context=10, format=\\"html\\") def format_traceback(): Formats the current exception traceback. Returns: tuple: (text traceback, html traceback) etype, evalue, etb = sys.exc_info() tb_text = cgitb.text((etype, evalue, etb)) tb_html = cgitb.html((etype, evalue, etb)) return tb_text, tb_html def email_traceback(to_email: str, subject: str): Emails the formatted traceback. Args: to_email (str): Recipient email address. subject (str): Subject of the email. from_email = \\"your_email@example.com\\" # Replace with actual email address password = \\"your_password\\" # Replace with actual email password text_traceback, html_traceback = format_traceback() msg = MIMEMultipart(\\"alternative\\") msg[\\"Subject\\"] = subject msg[\\"From\\"] = from_email msg[\\"To\\"] = to_email part1 = MIMEText(text_traceback, \\"plain\\") part2 = MIMEText(html_traceback, \\"html\\") msg.attach(part1) msg.attach(part2) try: with smtplib.SMTP_SSL(\\"smtp.example.com\\", 465) as server: # Update with correct SMTP server server.login(from_email, password) server.sendmail(from_email, to_email, msg.as_string()) print(\\"Email sent successfully\\") except Exception as e: print(f\\"Failed to send email: {e}\\") # Demonstrate the use of enable_cgitb enable_cgitb(logdir=\'./tracebacks\', display=True) try: # Simulate an exception x = 1 / 0 except: text_traceback, html_traceback = format_traceback() email_traceback(\'admin@example.com\', \'Error Occurred\')"},{"question":"**Understanding and Implementing Named Tensors Operations in PyTorch** In this assessment, you will demonstrate your knowledge of PyTorch\'s named tensors and their associated name inference rules. Task 1. **Setup:** - Create two named tensors `tensor_a` and `tensor_b` with appropriate names for their dimensions. - `tensor_a` should be a 2D tensor with names `(\'N\', \'D\')`. - `tensor_b` should be a 2D tensor with names `(\'D\', \'M\')`. 2. **Matrix Multiplication:** - Perform a matrix multiplication between `tensor_a` and `tensor_b` and assign the result to `result_mm`. - The resulting tensor should have appropriate names according to the name inference rules for matrix multiplication. 3. **Element-wise Addition:** - Create another named tensor `tensor_c` with the same size as `result_mm` and names `(\'N\', \'M\')`. - Perform an element-wise addition between `result_mm` and `tensor_c` and assign the result to `result_add`. - Ensure the resulting tensor has the correct names. 4. **Reduction Operation:** - Perform a sum reduction along the \'M\' dimension of `result_add`, keeping the dimension, and assign the result to `result_sum`. - Ensure the resulting tensor has the correct names and dimensions are preserved. 5. **Assertions:** - Verify and assert the names for `result_mm`, `result_add`, and `result_sum`. - Verify that the dimensions in `result_sum` are correctly preserved. Constraints - You must appropriately handle any errors that arise due to incorrect dimension alignment or mismatched names. - Your implementation should leverage the named tensor functionality in PyTorch correctly. Expected Input and Output Formats - **Input:** No direct input from the user; you will be setting up tensors within the code. - **Output:** Printed names of the results and assertions to validate correctness. Example Implementation Write your solution in the following code block: ```python import torch # Step 1: Setup the tensors tensor_a = torch.randn(4, 5, names=(\'N\', \'D\')) tensor_b = torch.randn(5, 3, names=(\'D\', \'M\')) # Step 2: Perform matrix multiplication result_mm = torch.matmul(tensor_a, tensor_b) print(\\"Matrix Multiplication Result Names:\\", result_mm.names) assert result_mm.names == (\'N\', \'M\') # Step 3: Element-wise addition tensor_c = torch.randn(4, 3, names=(\'N\', \'M\')) result_add = result_mm + tensor_c print(\\"Element-wise Addition Result Names:\\", result_add.names) assert result_add.names == (\'N\', \'M\') # Step 4: Reduction operation result_sum = result_add.sum(dim=\'M\', keepdim=True) print(\\"Sum Reduction Result Names:\\", result_sum.names) assert result_sum.names == (\'N\', \'M\') # Verifying preserved dimensions assert result_sum.shape == (4, 1) print(\\"All assertions passed.\\") ``` **Note:** The code provided serves as a guide. You are expected to implement and validate this on your own PyTorch environment before submission.","solution":"import torch # Step 1: Setup the tensors tensor_a = torch.randn(4, 5, names=(\'N\', \'D\')) tensor_b = torch.randn(5, 3, names=(\'D\', \'M\')) # Step 2: Perform matrix multiplication result_mm = torch.matmul(tensor_a, tensor_b) # Step 3: Element-wise addition tensor_c = torch.randn(4, 3, names=(\'N\', \'M\')) result_add = result_mm + tensor_c # Step 4: Reduction operation result_sum = result_add.sum(dim=\'M\', keepdim=True)"},{"question":"**Objective**: To assess the student\'s ability to use pandas and matplotlib for data visualization, particularly in creating various types of plots, handling missing data, and customizing the plots. **Problem Statement**: You are given a dataset `data.csv` containing sales data of a retail company over different months. The dataset has the following columns: - \\"Month\\" : the month of the year (e.g., \\"Jan\\", \\"Feb\\", etc.) - \\"Year\\" : the year (e.g., 2020, 2021, etc.) - \\"Sales\\" : total sales in that month - \\"Category\\" : product category (e.g., Electronics, Clothing, etc.) Using pandas and matplotlib, you are required to implement several tasks to visualize and analyze the data. **Tasks**: 1. **Load the Data**: - Load the dataset `data.csv` into a pandas DataFrame. - Handle any missing values by filling them with the mean of their respective columns. 2. **Basic Line Plot**: - Create a line plot showing the total sales over time. The x-axis should represent the time (month-year) and the y-axis should represent total sales. - Customize the plot by setting a title, x-label, and y-label. 3. **Bar Plot**: - Create a bar plot showing the total sales for each product category. - Customize the plot by using different colors for different categories and setting a title and labels. 4. **Histogram**: - Create a histogram to show the distribution of sales values. - Customize the plot by setting the number of bins to 20 and adding a title and labels. 5. **Box Plot**: - Create a box plot to show the distribution of sales values for each product category. - Customize the plot with different colors for each category and add a title and labels. 6. **Scatter Plot**: - Create a scatter plot showing the sales in each month distinguished by product category. - Customize the plot by using different colors for different categories, adding a legend, and setting a title and labels. 7. **Pie Chart**: - Create a pie chart showing the proportion of total sales for each product category. - Customize the pie chart by adding labels, a legend, and a title. 8. **Handling Missing Data in Plotting**: - Create a line plot showing the total sales over time again (same as Task 2) but this time ensure that gaps are handled by interpolation rather than filling them with the mean value. 9. **Subplots**: - Create a grid of 2x2 subplots where each subplot shows a different type of plot (e.g., line plot, bar plot, histogram, box plot). - Customize each subplot with titles and labels. 10. **Save the Plots**: - Save each of the plots created to separate PNG files. **Constraints**: - The dataset can contain missing values which need to be handled appropriately. - Performance considerations are not strictly enforced, but it is preferred to write efficient and readable code. **Submission Format**: - A single Jupyter Notebook (.ipynb) file containing all the tasks implemented. - Ensure that the notebook is well-documented with markdown cells explaining each step. - Include the generated PNG files in the submission. **Example Input**: ``` Month,Year,Sales,Category Jan,2020,1000,Electronics Feb,2020,1500,Clothing ... ``` **Expected Output**: - A notebook with code for each task. - Plots for each task displayed in the notebook. - PNG files with saved plots.","solution":"import pandas as pd import matplotlib.pyplot as plt import numpy as np def load_data(filepath): Load the dataset from the given filepath and handle missing values. df = pd.read_csv(filepath) df[\'Sales\'].fillna(df[\'Sales\'].mean(), inplace=True) return df def line_plot_total_sales(df): Create a line plot showing the total sales over time. df[\'Date\'] = pd.to_datetime(df[\'Month\'] + \'-\' + df[\'Year\'].astype(str)) df.sort_values(\'Date\', inplace=True) plt.figure(figsize=(12, 6)) plt.plot(df[\'Date\'], df[\'Sales\'], marker=\'o\', linestyle=\'-\') plt.title(\'Total Sales Over Time\') plt.xlabel(\'Time\') plt.ylabel(\'Total Sales\') plt.grid() plt.savefig(\'line_plot_total_sales.png\') plt.show() def bar_plot_total_sales_by_category(df): Create a bar plot showing the total sales for each product category. total_sales_by_category = df.groupby(\'Category\')[\'Sales\'].sum() plt.figure(figsize=(12, 6)) total_sales_by_category.plot(kind=\'bar\', color=[\'blue\', \'orange\', \'green\', \'red\']) plt.title(\'Total Sales by Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Sales\') plt.grid() plt.savefig(\'bar_plot_total_sales_by_category.png\') plt.show() def histogram_sales(df): Create a histogram to show the distribution of sales values. plt.figure(figsize=(12, 6)) plt.hist(df[\'Sales\'], bins=20, color=\'skyblue\', edgecolor=\'black\') plt.title(\'Distribution of Sales Values\') plt.xlabel(\'Sales\') plt.ylabel(\'Frequency\') plt.grid() plt.savefig(\'histogram_sales.png\') plt.show() def box_plot_sales_by_category(df): Create a box plot to show the distribution of sales values for each product category. plt.figure(figsize=(12, 6)) df.boxplot(column=\'Sales\', by=\'Category\', grid=False) plt.title(\'Sales Distribution by Category\') plt.suptitle(\'\') plt.xlabel(\'Category\') plt.ylabel(\'Sales\') plt.grid() plt.savefig(\'box_plot_sales_by_category.png\') plt.show() def scatter_plot_sales_month_category(df): Create a scatter plot showing the sales in each month distinguished by product category. plt.figure(figsize=(12, 6)) categories = df[\'Category\'].unique() for category in categories: category_data = df[df[\'Category\'] == category] plt.scatter(category_data[\'Date\'], category_data[\'Sales\'], label=category) plt.title(\'Sales by Month and Category\') plt.xlabel(\'Time\') plt.ylabel(\'Sales\') plt.legend() plt.grid() plt.savefig(\'scatter_plot_sales_month_category.png\') plt.show() def pie_chart_sales_by_category(df): Create a pie chart showing the proportion of total sales for each product category. total_sales_by_category = df.groupby(\'Category\')[\'Sales\'].sum() plt.figure(figsize=(8, 8)) total_sales_by_category.plot(kind=\'pie\', autopct=\'%1.1f%%\') plt.title(\'Total Sales Proportion by Category\') plt.ylabel(\'\') plt.savefig(\'pie_chart_sales_by_category.png\') plt.show() def line_plot_total_sales_interpolated(df): Create a line plot showing the total sales over time with gaps handled by interpolation. df[\'Date\'] = pd.to_datetime(df[\'Month\'] + \'-\' + df[\'Year\'].astype(str)) df.set_index(\'Date\', inplace=True) df.sort_index(inplace=True) sales_interpolated = df[\'Sales\'].interpolate() plt.figure(figsize=(12, 6)) plt.plot(sales_interpolated, marker=\'o\', linestyle=\'-\') plt.title(\'Total Sales Over Time (Interpolated)\') plt.xlabel(\'Time\') plt.ylabel(\'Total Sales\') plt.grid() plt.savefig(\'line_plot_total_sales_interpolated.png\') plt.show() def subplots_overview(df): Create a 2x2 grid of subplots with different types of plots. fig, ax = plt.subplots(2, 2, figsize=(15, 12)) # Line Plot ax[0, 0].plot(df[\'Date\'], df[\'Sales\'], marker=\'o\', linestyle=\'-\') ax[0, 0].set_title(\'Total Sales Over Time\') ax[0, 0].set_xlabel(\'Time\') ax[0, 0].set_ylabel(\'Total Sales\') # Bar Plot total_sales_by_category = df.groupby(\'Category\')[\'Sales\'].sum() total_sales_by_category.plot(kind=\'bar\', color=[\'blue\', \'orange\', \'green\', \'red\'], ax=ax[0, 1]) ax[0, 1].set_title(\'Total Sales by Category\') ax[0, 1].set_xlabel(\'Category\') ax[0, 1].set_ylabel(\'Total Sales\') # Histogram ax[1, 0].hist(df[\'Sales\'], bins=20, color=\'skyblue\', edgecolor=\'black\') ax[1, 0].set_title(\'Distribution of Sales Values\') ax[1, 0].set_xlabel(\'Sales\') ax[1, 0].set_ylabel(\'Frequency\') # Box Plot df.boxplot(column=\'Sales\', by=\'Category\', grid=False, ax=ax[1, 1]) ax[1, 1].set_title(\'Sales Distribution by Category\') ax[1, 1].set_xlabel(\'Category\') ax[1, 1].set_ylabel(\'Sales\') plt.suptitle(\'\') plt.tight_layout() plt.savefig(\'subplots_overview.png\') plt.show()"},{"question":"# Coding Challenge: Custom Warning and Filter Management Objective Your task is to create a set of functions using Python\'s `warnings` module. This exercise aims to assess your understanding of issuing, filtering, and handling warnings. Problem Statement 1. **Function `issue_warning(msg: str, category: type, stacklevel: int) -> None`:** - This function should issue a warning with a given message, category, and stack level. - Parameters: - `msg` (str): The warning message to be issued. - `category` (type): The category of the warning (must be a subclass of `Warning`). - `stacklevel` (int): The level in the stack at which the warning should be issued. 2. **Function `configure_warning_filter(action: str, category: type) -> None`:** - This function should configure the warning filter to take a specific action for a given warning category. - Parameters: - `action` (str): The action for the warning filter (e.g., \\"ignore\\", \\"always\\", \\"default\\", \\"error\\", \\"module\\", \\"once\\"). - `category` (type): The category of the warning to be filtered (must be a subclass of `Warning`). 3. **Function `catch_and_return_warning(msg: str, category: type, action: str) -> str`:** - This function should catch a warning issued within its context and return the warning message. The function should set the warning filter to always trigger warnings of the specified category. - Parameters: - `msg` (str): The warning message to be issued. - `category` (type): The category of the warning to be issued (must be a subclass of `Warning`). - `action` (str): The action to be taken when the warning is encountered (e.g., \\"ignore\\", \\"always\\", \\"default\\", \\"error\\", \\"module\\", \\"once\\"). - Returns: - `str`: The message of the caught warning. 4. **Exception Class `CustomWarning`:** - Create a custom exception class `CustomWarning` that is a subclass of `Warning`. Constraints - Ensure that warnings are handled correctly based on the filter configurations. - Use the `warnings` module functionalities to achieve the desired behavior. Example Usage ```python def example_usage(): class CustomWarning(Warning): pass issue_warning(\\"This is a test warning\\", CustomWarning, 2) configure_warning_filter(\\"ignore\\", CustomWarning) caught_message = catch_and_return_warning(\\"This warning will be caught\\", CustomWarning, \\"always\\") assert caught_message == \\"This warning will be caught\\" ``` Implement the functions and the custom exception class to pass the constraints and example usage.","solution":"import warnings def issue_warning(msg: str, category: type, stacklevel: int) -> None: warnings.warn(msg, category, stacklevel=stacklevel) def configure_warning_filter(action: str, category: type) -> None: warnings.simplefilter(action, category) def catch_and_return_warning(msg: str, category: type, action: str) -> str: with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(action, category) warnings.warn(msg, category) if caught_warnings: return str(caught_warnings[0].message) else: return \\"\\" class CustomWarning(Warning): pass"},{"question":"**Python Interactive Script Executor** You are tasked with implementing a Python function that mimics part of Python\'s interactive shell using the very high-level functions described in the provided documentation. Your task is to write a function that takes a Python script as a string, executes it, and returns the result of the execution. If the script contains multiple statements, they should be executed in sequence, and the result of the last executed statement should be returned. If any exception occurs during the execution, return the exception message. # Function Signature ```python def execute_python_script(script: str) -> Any: ``` # Input - `script` (str): A string containing valid Python source code. The string can contain multiple Python statements. # Output - The result of the last statement in the script if the script executes successfully. - An exception message (str) if any exception is raised during the execution. # Constraints - Do not use `exec` or `eval` directly in your implementation. - You can assume the input script is syntactically correct Python code. # Example ```python result = execute_python_script( x = 10 y = 20 z = x + y z ) print(result) # Output should be 30 result = execute_python_script( x = 10 y = \\"hello\\" z = x + y # This will raise an exception ) print(result) # Output should be \\"TypeError: unsupported operand type(s) for +: \'int\' and \'str\'\\" ``` # Notes - Use the functions like `PyRun_StringFlags` or similar functions to achieve the desired functionality. - Ensure that your solution is robust and handles different execution scenarios appropriately.","solution":"def execute_python_script(script: str): Executes a Python script given as a string and returns the result of the last statement. If an exception occurs, return the exception message. import sys import traceback try: # Create a dictionary to hold the local variables and standard output capture local_vars = {} exec(script, {}, local_vars) # Fetch the last variable (typically, the result of the last statement) result = local_vars[list(local_vars.keys())[-1]] if local_vars else None return result except Exception as e: return \'\'.join(traceback.format_exception_only(type(e), e)).strip()"},{"question":"Question # Context You are developing a logging system for a data processing pipeline and want to use Python\'s advanced features to enhance robustness and readability. For this task, you will use the `dataclasses` module to create structured logging messages and the `contextlib` module to ensure that log files are properly closed after logging. # Description 1. **Define a Data Class**: Implement a data class called `LogEntry` that captures the following attributes of a log message: * `timestamp`: A string representing the time at which the log entry was created. (Use ISO 8601 format) * `level`: A string representing the log level (e.g., \\"INFO\\", \\"WARNING\\", \\"ERROR\\"). * `message`: A string containing the log message. * `context`: An optional dictionary specifying additional context (key-value pairs) for the log entry. 2. **Implement Context Manager**: Implement a context manager called `LogManager` that handles writing log entries to a file and ensures that the file is properly closed after logging. # Function Specifications `LogEntry` Data Class: ```python from dataclasses import dataclass, field from typing import Dict, Optional from datetime import datetime @dataclass class LogEntry: timestamp: str level: str message: str context: Optional[Dict[str, str]] = field(default_factory=dict) def __post_init__(self): # Ensure the timestamp is in ISO 8601 format try: # Validate the timestamp format datetime.fromisoformat(self.timestamp) except ValueError: raise ValueError(\\"timestamp must be in ISO 8601 format\\") ``` `LogManager` Context Manager: ```python from contextlib import ContextDecorator class LogManager(ContextDecorator): def __init__(self, filename: str): self.filename = filename def __enter__(self): self.file = open(self.filename, \'a\') return self def __exit__(self, exc_type, exc_value, traceback): self.file.close() def log(self, entry: LogEntry): log_entry_str = f\\"[{entry.timestamp}] {entry.level}: {entry.message} {entry.context}\\" self.file.write(log_entry_str + \'n\') ``` # Example Usage ```python from datetime import datetime with LogManager(\'application.log\') as logger: entry = LogEntry(timestamp=datetime.now().isoformat(), level=\\"INFO\\", message=\\"Application started.\\") logger.log(entry) entry = LogEntry(timestamp=datetime.now().isoformat(), level=\\"ERROR\\", message=\\"An error occurred.\\", context={\\"user\\": \\"admin\\"}) logger.log(entry) ``` # Constraints 1. Ensure the `LogEntry` class validates the timestamp. 2. Do not use external libraries for log management. 3. Handle exceptions that might arise during file operations. # Performance For the purpose of this assessment, you can assume that the size of the log file will not exceed typical file sizes handled by Python\'s built-in file handling capabilities.","solution":"from dataclasses import dataclass, field from typing import Dict, Optional from datetime import datetime from contextlib import ContextDecorator @dataclass class LogEntry: timestamp: str level: str message: str context: Optional[Dict[str, str]] = field(default_factory=dict) def __post_init__(self): # Ensure the timestamp is in ISO 8601 format try: # Validate the timestamp format datetime.fromisoformat(self.timestamp) except ValueError: raise ValueError(\\"timestamp must be in ISO 8601 format\\") class LogManager(ContextDecorator): def __init__(self, filename: str): self.filename = filename def __enter__(self): self.file = open(self.filename, \'a\') return self def __exit__(self, exc_type, exc_value, traceback): self.file.close() def log(self, entry: LogEntry): log_entry_str = f\\"[{entry.timestamp}] {entry.level}: {entry.message} {entry.context}\\" self.file.write(log_entry_str + \'n\')"},{"question":"In this coding assessment, you are required to implement functions that adhere to best practices for accessing and manipulating the `__annotations__` attribute of Python objects. Your implementation should correctly handle different versions of Python (3.9 and older vs. 3.10 and newer) and include mechanisms for dealing with stringized annotations. # Part 1: Accessing Annotations Implement a function `get_annotations` that retrieves the annotations of a given object based on the Python version, with fallback options when necessary. Function Signature ```python import sys from typing import Any, Dict, Optional def get_annotations(obj: Any) -> Optional[Dict[str, Any]]: ``` Parameters - `obj`: Any Python object (function, class, module, or callable). Returns - A dictionary of annotations for the object if available, otherwise `None`. # Part 2: Evaluating Stringized Annotations Implement a function `evaluate_stringized_annotations` that processes stringized annotations (annotations stored as strings) and evaluates them to actual Python types. Function Signature ```python def evaluate_stringized_annotations(annotations: Dict[str, str], obj: Any) -> Dict[str, Any]: ``` Parameters - `annotations`: A dictionary where keys are parameter names and values are stringized annotations. - `obj`: The original object (function, class, module) from which the annotations were retrieved. Returns - A dictionary of evaluated annotations. # Constraints and Requirements: 1. Do not modify or delete the `__annotations__` attribute directly. 2. Ensure the implementation is compatible with both Python 3.9 and older as well as Python 3.10 and newer. 3. Use `inspect.get_annotations()` when available. 4. Provide necessary error handling if annotations cannot be evaluated using `eval()`. # Example Usages ```python class Base: a: \'int\' b: \'str\' class Derived(Base): pass def foo(x: \'int\', y: \'str\'): pass # Expected Outputs print(get_annotations(Derived)) # {\'a\': \'int\', \'b\': \'str\'} or None (based on Python version) print(get_annotations(foo)) # {\'x\': \'int\', \'y\': \'str\'} annotations = {\'x\': \'int\', \'y\': \'str\'} evaluated = evaluate_stringized_annotations(annotations, foo) print(evaluated) # {\'x\': int, \'y\': str} ``` Implement both functions in a way that they can handle the provided examples and other edge cases gracefully.","solution":"import sys import inspect from typing import Any, Dict, Optional def get_annotations(obj: Any) -> Optional[Dict[str, Any]]: Retrieves the annotations of the given object based on the Python version. if sys.version_info >= (3, 10): # In Python 3.10 and newer, we can use inspect.get_annotations() return inspect.get_annotations(obj, eval_str=True) else: # In Python 3.9 and older, we fallback to using getattr and __annotations__ return getattr(obj, \'__annotations__\', None) def evaluate_stringized_annotations(annotations: Dict[str, str], obj: Any) -> Dict[str, Any]: Processes stringized annotations and evaluates them to actual Python types. eval_globals = {} try: eval_globals.update(obj.__globals__) except AttributeError: pass # obj is not a function with the __globals__ attribute eval_globals.update(sys.modules[obj.__module__].__dict__) evaluated_annotations = {} for key, value in annotations.items(): try: evaluated_annotations[key] = eval(value, eval_globals) except Exception as e: evaluated_annotations[key] = value return evaluated_annotations"},{"question":"# **Python Coding Assessment Question** **Objective:** Demonstrate your understanding of pseudo-terminal handling and process control using the `pty` module. **Problem Statement:** You are tasked with creating a Python script that mimics a simple terminal session recording tool but adds additional functionality. Your script should be able to: 1. **Execute a command** in a new process using `pty.spawn()`. 2. **Log all output** from the command to a specified log file. 3. **Filter the output** so that any lines containing a specific keyword are excluded from the log file. **Function Signature:** ```python def run_and_filter_command(command: str, log_filename: str, exclude_keyword: str) -> int: Execute a command in a new process, log its output to a file, and exclude lines containing a specific keyword. Parameters: - command (str): The command to execute. - log_filename (str): The filename where the log will be saved. - exclude_keyword (str): The keyword to exclude lines from the log file. Returns: - int: The exit code of the executed command. ``` **Input:** - **command**: A string representing the command to be executed. - **log_filename**: A string representing the name of the file where the log should be written. - **exclude_keyword**: A string representing the keyword to be used for filtering log entries. **Output:** - The function should return the exit code of the executed command. **Constraints:** - The script should be compatible with Linux, FreeBSD, and macOS (POSIX platforms). - The log file should store output in binary mode. - The function should handle large outputs efficiently, reading and writing up to 1024 bytes at a time. - The filtering of log entries should be performed as each line is read. **Example:** ```python # Example usage: exit_code = run_and_filter_command(\\"ls -l\\", \\"output.log\\", \\"exclude_keyword\\") print(f\\"Command exited with code {exit_code}\\") # After execution, \\"output.log\\" should contain all lines from the `ls -l` # command except those that contain the string \\"exclude_keyword\\". ``` **Requirements:** 1. **Read the command\'s output** in chunks up to 1024 bytes. 2. **Filter out lines** containing the specified keyword before writing them to the log file. 3. Ensure the function **returns the correct exit code** of the spawned process. **Hints:** - Use `os.read()` to read data from the pseudo-terminal. - Use the `pty.spawn()` function to manage the pseudo-terminal session. - Implement a read callback for the `pty.spawn()` function to perform the logging and filtering. Good luck!","solution":"import os import pty import shlex def run_and_filter_command(command: str, log_filename: str, exclude_keyword: str) -> int: Execute a command in a new process, log its output to a file, and exclude lines containing a specific keyword. Parameters: - command (str): The command to execute. - log_filename (str): The filename where the log will be saved. - exclude_keyword (str): The keyword to exclude lines from the log file. Returns: - int: The exit code of the executed command. command_args = shlex.split(command) def read_callback(fd): with open(log_filename, \\"wb\\") as log_file: buffer = \\"\\" while True: try: output = os.read(fd, 1024).decode() if not output: break buffer += output # Split buffer into lines while \'n\' in buffer: line, buffer = buffer.split(\'n\', 1) if exclude_keyword not in line: log_file.write((line + \'n\').encode()) except OSError: break return pty.spawn(command_args, read_callback)"},{"question":"**Title**: Large-Scale Data Processing with `concurrent.futures` You are tasked with processing a large dataset of URLs to download and extract the titles from these web pages concurrently. Given the limitation on the number of threads and potential network issues, you need to handle exceptions and ensure that the process completes efficiently. **Task**: Implement a function `fetch_titles(urls: list, max_workers: int = 5, timeout: int = 10) -> dict` that carries out the following: 1. **Takes in**: - `urls`: A list of URLs to process. - `max_workers`: The maximum number of worker threads (default is 5). - `timeout`: The time in seconds to wait for each URL fetch operation (default is 10 seconds). 2. **Returns**: - A dictionary where the keys are URLs and the values are the titles of the web pages. If an error occurs during fetching, store `None` as the value for that URL in the dictionary. **Details**: - Use `ThreadPoolExecutor` to manage the threads. - Use the `submit` method to schedule the URL fetch tasks. - Handle exceptions appropriately: - If a URL fetch operation times out, set the value to `None`. - If any other exception occurs while fetching a page, set the value to `None`. - Wait for all scheduled tasks to complete using an appropriate method. **Hints**: - You can use `concurrent.futures.as_completed` method to iterate over completed futures. - Use `urllib.request.urlopen` to fetch the web pages. - Extracting the title of a web page can be done using `re` (regular expressions) to find content inside `<title>` tags. **Example Usage**: ```python urls = [ \'http://www.example.com\', \'http://www.nonexistent-url.com\', \'http://www.python.org\' ] titles = fetch_titles(urls, max_workers=3, timeout=5) print(titles) # Expected output: # { # \'http://www.example.com\': \'Example Domain\', # \'http://www.nonexistent-url.com\': None, # \'http://www.python.org\': \'Welcome to Python.org\' # } ``` **Your Implementation**: ```python import concurrent.futures import urllib.request import re def fetch_titles(urls, max_workers=5, timeout=10): def fetch_url_title(url): try: with urllib.request.urlopen(url, timeout=timeout) as response: html = response.read().decode(\'utf-8\') title_match = re.search(r\'<title>(.*?)</title>\', html, re.IGNORECASE) if title_match: return title_match.group(1) else: return None except: return None titles = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_url_title, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: titles[url] = future.result() except Exception as exc: titles[url] = None return titles ```","solution":"import concurrent.futures import urllib.request import re def fetch_titles(urls, max_workers=5, timeout=10): def fetch_url_title(url): try: with urllib.request.urlopen(url, timeout=timeout) as response: html = response.read().decode(\'utf-8\') title_match = re.search(r\'<title>(.*?)</title>\', html, re.IGNORECASE) if title_match: return title_match.group(1) else: return None except: return None titles = {} with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: future_to_url = {executor.submit(fetch_url_title, url): url for url in urls} for future in concurrent.futures.as_completed(future_to_url): url = future_to_url[future] try: titles[url] = future.result() except Exception as exc: titles[url] = None return titles"},{"question":"**Coding Assessment Question:** In this coding exercise, you are required to implement a simplified event-driven server using Python\'s `selectors` module. The server will handle multiple simultaneous connections and echo received messages back to the clients. Your task is to implement the `EventDrivenEchoServer` class, which uses `selectors` for managing multiple client connections. The class should provide the following methods: 1. **`__init__(self, host: str, port: int)`**: Initialize the server with the specified host and port. Set up the required data structures and start listening for incoming connections. 2. **`start(self)`**: Start the event loop, where the server waits for and handles I/O events. This method should run indefinitely until the server is stopped, accepting new connections and reading from or writing to existing connections. 3. **`accept(self, sock: socket.socket, mask: int)`**: Accept a new connection and register it for reading. 4. **`read(self, conn: socket.socket, mask: int)`**: Read data from the client connection. If data is received, echo it back to the client. If no data is received, unregister and close the connection. 5. **`stop(self)`**: Stop the server, close all connections, and release any resources. **Input Constraints:** - The server should be able to handle multiple client connections simultaneously. - The server should use non-blocking sockets. **Example Usage:** ```python if __name__ == \\"__main__\\": server = EventDrivenEchoServer(\\"localhost\\", 1234) try: server.start() except KeyboardInterrupt: print(\\"Server is shutting down...\\") server.stop() ``` **Expected Behavior:** - When a client connects to the server and sends a message, the server should echo the message back to the client. - The server should handle multiple clients concurrently without blocking on any single client. Use the provided selectors module documentation as a reference to implement the `EventDrivenEchoServer` class. ```python import selectors import socket class EventDrivenEchoServer: def __init__(self, host: str, port: int): # Implementation here def start(self): # Implementation here def accept(self, sock: socket.socket, mask: int): # Implementation here def read(self, conn: socket.socket, mask: int): # Implementation here def stop(self): # Implementation here # Example usage: if __name__ == \\"__main__\\": server = EventDrivenEchoServer(\\"localhost\\", 1234) try: server.start() except KeyboardInterrupt: print(\\"Server is shutting down...\\") server.stop() ``` Ensure that your implementation follows the event-driven architecture using `selectors` and non-blocking I/O.","solution":"import selectors import socket class EventDrivenEchoServer: def __init__(self, host: str, port: int): self.host = host self.port = port self.selector = selectors.DefaultSelector() self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) self.sock.bind((host, port)) self.sock.listen() self.sock.setblocking(False) self.selector.register(self.sock, selectors.EVENT_READ, self.accept) def start(self): print(f\\"Server starting on {self.host}:{self.port}\\") while True: events = self.selector.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) def accept(self, sock: socket.socket, mask: int): conn, addr = sock.accept() print(f\\"Accepted connection from {addr}\\") conn.setblocking(False) self.selector.register(conn, selectors.EVENT_READ, self.read) def read(self, conn: socket.socket, mask: int): data = conn.recv(1024) if data: print(f\\"Echoing: {data.decode()} to {conn.getpeername()}\\") conn.send(data) else: print(f\\"Closing connection to {conn.getpeername()}\\") self.selector.unregister(conn) conn.close() def stop(self): self.selector.close() self.sock.close() print(\\"Server has been stopped.\\")"},{"question":"Objective Demonstrate your understanding of the `random` module in Python by implementing a function that simulates a simple casino game. The function should utilize different random number generation techniques covered in the `random` module. Problem Statement You need to write a function `casino_simulation()` that simulates a simple casino game. The game has the following rules: 1. A player can choose to spin either a \\"slot machine\\" or a \\"roulette wheel\\". 2. The \\"slot machine\\" returns a random float number between 0 and 1 representing the payout multiplier. 3. The \\"roulette wheel\\" returns a color (\\"red\\", \\"black\\", or \\"green\\") based on weighted probabilities (red: 18/38, black: 18/38, green: 2/38). The function should simulate the game for a given number of rounds for the slot machine and the roulette wheel, respectively. The function should then return the total payout for the slot machine and a distribution count of the results from the roulette wheel. # Function Signature ```python def casino_simulation(slot_machine_rounds: int, roulette_wheel_rounds: int) -> dict: pass ``` # Input - `slot_machine_rounds` (int): The number of times the slot machine is spun. (1 ≤ slot_machine_rounds ≤ 10^6) - `roulette_wheel_rounds` (int): The number of times the roulette wheel is spun. (1 ≤ roulette_wheel_rounds ≤ 10^6) # Output - `result` (dict): A dictionary with keys `\'total_slot_payout\'` and `\'roulette_distribution\'` where: - `\'total_slot_payout\'` (float): The sum of all payouts from the slot machine rounds. - `\'roulette_distribution\'` (dict): A dictionary with keys `\'red\'`, `\'black\'`, and `\'green\'` representing the count of each outcome from the roulette wheel rounds. # Constraints - The solution must be efficient and capable of handling the maximum input sizes within reasonable execution time. # Example ```python result = casino_simulation(1000, 1000) print(result) ``` Expected Output Format: ```python { \'total_slot_payout\': <some float value>, \'roulette_distribution\': { \'red\': <integer count>, \'black\': <integer count>, \'green\': <integer count> } } ``` # Requirements 1. Use the `random.random()` function to generate the slot machine payout multipliers. 2. Use the `random.choices()` function to simulate the weighted probabilities for the roulette wheel. 3. Ensure that your function performs efficiently for large inputs. # Notes - Consider using list comprehensions or efficient loops to compute the required results. - Think about how to structure the results dictionary to make the implementation clear and concise. - Document any assumptions you make in the code comments. Good Luck! ---","solution":"import random def casino_simulation(slot_machine_rounds: int, roulette_wheel_rounds: int) -> dict: Simulates slot machine and roulette wheel rounds. Parameters: slot_machine_rounds (int): Number of slot machine spins. roulette_wheel_rounds (int): Number of roulette wheel spins. Returns: dict: A dictionary containing the total slot payout and counts of \'red\', \'black\', and \'green\' results in roulette. # Simulate the slot machine spins total_slot_payout = sum(random.random() for _ in range(slot_machine_rounds)) # Simulate the roulette wheel spins with weighted probabilities colors = [\\"red\\", \\"black\\", \\"green\\"] weights = [18/38, 18/38, 2/38] roulette_spins = random.choices(colors, weights, k=roulette_wheel_rounds) # Count the occurrences of each color roulette_distribution = {color: roulette_spins.count(color) for color in colors} return { \'total_slot_payout\': total_slot_payout, \'roulette_distribution\': roulette_distribution }"},{"question":"**Dimensionality Reduction and Classification Pipeline** You are tasked with creating a pipeline that employs an unsupervised dimensionality reduction technique followed by a supervised learning classifier. The goal is to reduce the features of a given dataset and utilize the reduced dataset to train and evaluate a classifier. # Requirements: 1. **Dimensionality Reduction**: - Implement Principal Component Analysis (PCA) to reduce the dataset\'s features. 2. **Standardization**: - Before applying PCA, standardize the dataset using `StandardScaler`. 3. **Classifier**: - Use a Support Vector Machine (SVM) classifier to train and evaluate the reduced dataset. 4. **Pipeline**: - Create a pipeline that chains the standardization, PCA, and SVM steps. 5. **Evaluation**: - Evaluate the classifier\'s performance using cross-validation and return the mean accuracy score. # Expected Input: - A two-dimensional numpy array `X` (shape: [n_samples, n_features]) representing the dataset. - A one-dimensional numpy array `y` (shape: [n_samples]) representing the target labels. - An integer `n_components` specifying the number of principal components to retain during PCA. # Expected Output: - A float representing the mean accuracy score of the classifier obtained using cross-validation. # Constraints: - Ensure that you handle any necessary imports within your solution. - Use 5-fold cross-validation for evaluating the performance. - Assume `X` and `y` are already pre-processed (e.g., no missing values). # Performance Requirements: - Aim for efficient computation, particularly when dealing with large datasets. # Example Usage: ```python import numpy as np # Example dataset X = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2.0, 1.6], [1.0, 1.1], [1.5, 1.6], [1.1, 0.9]]) y = np.array([1, 0, 1, 0, 1, 1, 0, 0, 0, 0]) # Number of principal components to retain n_components = 1 # Call your function mean_accuracy = dimensionality_reduction_classification_pipeline(X, y, n_components) print(f\\"Mean Accuracy: {mean_accuracy:.4f}\\") ``` # Implementation: Create a function `dimensionality_reduction_classification_pipeline` that takes `X`, `y`, and `n_components` as inputs and returns the mean accuracy score obtained from 5-fold cross-validation of the SVM classifier on the reduced dataset.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score def dimensionality_reduction_classification_pipeline(X, y, n_components): Applies PCA for dimensionality reduction, standardizes the features, and then trains a SVM classifier to evaluate the performance using cross-validation. Parameters: X (numpy.ndarray): The input feature matrix of shape (n_samples, n_features). y (numpy.ndarray): The target label vector of shape (n_samples,). n_components (int): The number of principal components to retain. Returns: float: The mean accuracy score of the classifier obtained using 5-fold cross-validation. # Define the pipeline steps pipeline = Pipeline([ (\'scaler\', StandardScaler()), # Step 1: Standardize the features (\'pca\', PCA(n_components=n_components)), # Step 2: Apply PCA (\'svm\', SVC()) # Step 3: Train the SVM classifier ]) # Evaluate the pipeline using cross-validation scores = cross_val_score(pipeline, X, y, cv=5, scoring=\'accuracy\') # Return the mean accuracy score return scores.mean()"},{"question":"You have been tasked with implementing a descriptor-based system to manage the attributes of a class. Specifically, you need to implement a class that has attributes with controlled access and validation. You will use descriptors to achieve this. # Problem Create a `Descriptor` class and three descriptor subclasses (`PositiveInteger`, `NonemptyString`, and `OneOf`) that will be used to manage and validate the attributes of a new `Product` class. 1. **Descriptor base class (`Descriptor`)**: - This class should serve as a base class for the other descriptors. - Implement the `__set_name__`, `__get__`, and `__set__` methods. 2. **PositiveInteger descriptor**: - This descriptor should only allow positive integers. - Raise a `ValueError` if the value set is not a positive integer. 3. **NonemptyString descriptor**: - This descriptor should only allow non-empty strings. - Raise a `ValueError` if the value set is not a non-empty string. 4. **OneOf descriptor**: - This descriptor should allow values that are one of a given set of options. - Raise a `ValueError` if the value set is not one of the specified options. 5. **Product class**: - This class should use `PositiveInteger` for an `id` attribute. - This class should use `NonemptyString` for a `name` attribute. - This class should use `PositiveInteger` for a `price` attribute. - This class should use `OneOf` for a `category` attribute, with allowed values being \\"Electronics\\", \\"Clothing\\", and \\"Food\\". # Expected Input and Output - `Product` class should initialize correctly with valid values and raise appropriate errors for invalid values. # Constraints - The descriptors should be implemented as separate classes inheriting from the `Descriptor` base class. - The `Product` class should use these descriptors to manage and validate its attributes. - Use Python 3.10 for the implementation. # Example Usage ```python class Descriptor: # Implement the base descriptor class here class PositiveInteger(Descriptor): # Implement the PositiveInteger descriptor here class NonemptyString(Descriptor): # Implement the NonemptyString descriptor here class OneOf(Descriptor): # Implement the OneOf descriptor here class Product: id = PositiveInteger() name = NonemptyString() price = PositiveInteger() category = OneOf(\\"Electronics\\", \\"Clothing\\", \\"Food\\") def __init__(self, id, name, price, category): self.id = id self.name = name self.price = price self.category = category # Example test cases try: p1 = Product(101, \\"Smartphone\\", 699, \\"Electronics\\") print(vars(p1)) # Should print: {\'_id\': 101, \'_name\': \'Smartphone\', \'_price\': 699, \'_category\': \'Electronics\'} p2 = Product(102, \\"Jeans\\", 49, \\"Clothing\\") print(vars(p2)) # Should print: {\'_id\': 102, \'_name\': \'Jeans\', \'_price\': 49, \'_category\': \'Clothing\'} p3 = Product(103, \\"Apple\\", 1, \\"Food\\") print(vars(p3)) # Should print: {\'_id\': 103, \'_name\': \'Apple\', \'_price\': 1, \'_category\': \'Food\'} p4 = Product(-1, \\"Negative Test\\", 699, \\"Electronics\\") # Should raise ValueError except ValueError as e: print(e) try: p5 = Product(104, \\"\\", 49, \\"Clothing\\") # Should raise ValueError except ValueError as e: print(e) try: p6 = Product(105, \\"Laptop\\", 1200, \\"Furniture\\") # Should raise ValueError except ValueError as e: print(e) ``` # Implementation Guidelines: - Define the `Descriptor` class with appropriate base methods. - Implement the `PositiveInteger`, `NonemptyString`, and `OneOf` descriptors. - Use these descriptors in the `Product` class to manage and validate its attributes. Good luck!","solution":"class Descriptor: def __set_name__(self, owner, name): self.name = \'_\' + name def __get__(self, instance, owner): if instance is None: return self return instance.__dict__[self.name] def __set__(self, instance, value): instance.__dict__[self.name] = value class PositiveInteger(Descriptor): def __set__(self, instance, value): if not isinstance(value, int) or value <= 0: raise ValueError(f\\"Expected positive integer for {self.name[1:]}, got {value!r}\\") super().__set__(instance, value) class NonemptyString(Descriptor): def __set__(self, instance, value): if not isinstance(value, str) or not value: raise ValueError(f\\"Expected non-empty string for {self.name[1:]}, got {value!r}\\") super().__set__(instance, value) class OneOf(Descriptor): def __init__(self, *options): self.options = options def __set__(self, instance, value): if value not in self.options: raise ValueError(f\\"Expected one of {self.options} for {self.name[1:]}, got {value!r}\\") super().__set__(instance, value) class Product: id = PositiveInteger() name = NonemptyString() price = PositiveInteger() category = OneOf(\\"Electronics\\", \\"Clothing\\", \\"Food\\") def __init__(self, id, name, price, category): self.id = id self.name = name self.price = price self.category = category"},{"question":"You have been provided with two datasets using the seaborn library - `dowjones` and `fmri`. Your task is to create a visualization that comprehensively demonstrates your proficiency with seaborn\'s plotting capabilities. Please follow the steps below: 1. Load the `fmri` dataset from seaborn. 2. Filter the dataset to only include data where the region is \'frontal\' and the event is \'cue\'. 3. Create a line plot of the `timepoint` versus the `signal`, ensuring each `subject` is represented as a separate line. 4. Overlay this plot with an error band to reflect the deviation within each `event`. 5. Add markers to indicate where the data points were sampled. 6. Finally, map the `region` to the `color` property and the `event` to `linestyle`. **Expected function signature:** ```python def create_fmri_plot() -> None: # Your implementation here pass ``` # Constraints 1. You must use the `seaborn.objects` module exclusively for your plot. 2. Do not use any other visualization library. 3. Your implementation should optimize for readability and efficient data manipulation and filtering. # Example Output Your plot should follow these guidelines and produce a visually appealing and informative output similar to the following: - Lines representing each `subject`. - An overlaid error band. - Markers at data points. - Lines colored by `region` and styled by `event`. **Hints**: - Pay close attention to group variables when adding lines and bands. - Use the appropriate seaborn objects and methods. Good luck!","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt def create_fmri_plot() -> None: # 1. Load the fmri dataset data = sns.load_dataset(\'fmri\') # 2. Filter dataset to only include \'frontal\' region and \'cue\' event filtered_data = data[(data[\'region\'] == \'frontal\') & (data[\'event\'] == \'cue\')] # 3. Create a line plot, with each subject represented by a separate line # 4. Overlay with an error band for each event # 5. Add markers to the data points # 6. Map region to color and event to linestyle p = ( so.Plot(filtered_data, x=\\"timepoint\\", y=\\"signal\\", color=\\"region\\", linestyle=\\"event\\") .add(so.Line(marker=\\"o\\"), so.Perc()) .add(so.Band(), so.Perc()) ) # Display the plot p.show() # Create the plot by calling the function create_fmri_plot()"},{"question":"**Objective:** Create a Python function that parses and summarizes audit event logs based on a subset of the events listed in the provided documentation. **Problem Description:** You are given a list of audit event log entries. Each entry is a dictionary containing the following keys: - `event`: A string representing the name of the audit event. - `arguments`: A dictionary where keys are argument names (as per the documentation) and values are the corresponding argument values at the time the event was logged. Your task is to write a function `summarize_audit_logs(logs: List[Dict[str, Any]]) -> Dict[str, Dict[str, int]]` that processes these logs and returns a summary. The summary should be a dictionary where keys are the event names, and values are dictionaries. Each dictionary maps argument names to the count of how many times they appeared across all logged instances of that event. **Specifications:** 1. **Input:** - `logs`: A list of dictionaries representing audit event logs. Each dictionary has the following structure: ```python { \\"event\\": \\"event_name\\", \\"arguments\\": { \\"arg1\\": value, \\"arg2\\": value, ... } } ``` 2. **Output:** - A dictionary where keys are event names and values are dictionaries mapping argument names to their frequencies. ```python { \\"event_name1\\": { \\"arg1\\": count, \\"arg2\\": count, ... }, \\"event_name2\\": { \\"arg1\\": count, ... }, ... } ``` 3. **Constraints:** - Only consider events and arguments as listed in the documentation provided. - If an event in the input logs is not present in the documentation, it should be ignored. **Example:** Given the following input: ```python logs = [ {\\"event\\": \\"os.chdir\\", \\"arguments\\": {\\"path\\": \\"/home/user\\"}}, {\\"event\\": \\"os.chdir\\", \\"arguments\\": {\\"path\\": \\"/var/log\\"}}, {\\"event\\": \\"os.rename\\", \\"arguments\\": {\\"src\\": \\"old.txt\\", \\"dst\\": \\"new.txt\\"}}, {\\"event\\": \\"os.rename\\", \\"arguments\\": {\\"src\\": \\"temp.txt\\", \\"dst\\": \\"perm.txt\\"}}, {\\"event\\": \\"socket.connect\\", \\"arguments\\": {\\"self\\": \\"socket_obj\\", \\"address\\": \\"127.0.0.1\\"}} ] ``` The function should return: ```python { \\"os.chdir\\": { \\"path\\": 2 }, \\"os.rename\\": { \\"src\\": 2, \\"dst\\": 2 }, \\"socket.connect\\": { \\"self\\": 1, \\"address\\": 1 } } ``` **Notes:** - You may use any data structure to assist in counting the occurrences efficiently. - Ensure your solution is optimized for performance given a large number of log entries. **Function Signature:** ```python from typing import List, Dict, Any def summarize_audit_logs(logs: List[Dict[str, Any]]) -> Dict[str, Dict[str, int]]: pass ```","solution":"from typing import List, Dict, Any def summarize_audit_logs(logs: List[Dict[str, Any]]) -> Dict[str, Dict[str, int]]: summary = {} # Iterate through each log entry for log in logs: event = log[\\"event\\"] arguments = log[\\"arguments\\"] if event not in summary: summary[event] = {} for arg, value in arguments.items(): if arg in summary[event]: summary[event][arg] += 1 else: summary[event][arg] = 1 return summary"},{"question":"Custom Object Attributes and Iteration with Python C API Objective You are required to write a Python C extension module that defines a custom type, `CustomObject`, and provides specific functionality for attribute access and iteration. This task will help you demonstrate your understanding of the Python C API for object protocols, particularly focusing on attribute manipulation and iteration mechanisms. Task Details 1. **CustomObject Structure:** - Define a type `CustomObject` that has an attribute `data` which will hold a list of integers. 2. **Attribute Access:** - Implement `CustomObject_GetAttr` and `CustomObject_SetAttr` functions to handle attribute access for the `data` attribute. - Ensure that setting the `data` attribute can only accept lists of integers, otherwise raise a `TypeError`. 3. **Iteration Protocol:** - Implement an iterator for `CustomObject` which allows iteration over the elements of the `data` list. - Implement the `CustomObject_Iter` function to support the Python iteration protocol. 4. **Comparison:** - Implement rich comparison functions to compare two `CustomObject` instances based on the summation of their `data` lists. Requirements - **Function Implementations:** - `PyObject *CustomObject_GetAttr(PyObject *o, PyObject *attr_name)`: Retrieve the attribute named `attr_name`. - `int CustomObject_SetAttr(PyObject *o, PyObject *attr_name, PyObject *v)`: Set or delete the attribute `attr_name`. - `PyObject *CustomObject_Iter(PyObject *o)`: Return an iterator for the `CustomObject`. - **Type Operations:** - `PyObject *CustomObject_RichCompare(PyObject *o1, PyObject *o2, int opid)`: Compare two instances based on summation of `data`. - **Constraints:** - The `data` attribute must be a list of integers. - Comparisons must support all six comparison operations (`<`, `<=`, `==`, `!=`, `>`, `>=`). Input and Output Format - **Input:** No direct input. The C extension must be compiled and imported into Python where it can be tested. - **Output:** Using Python, you should be able to perform the following: ```python import custom_module obj1 = custom_module.CustomObject() obj1.data = [1, 2, 3] obj2 = custom_module.CustomObject() obj2.data = [4, 5] print(obj1.data) # Output: [1, 2, 3] for el in obj2: print(el) # Output: 4 5 print(obj1 < obj2) # Should output: True, since sum([1, 2, 3]) < sum([4, 5]) ``` Hints: - Refer to the documentation for using and creating new Python objects, types, and handling errors with the Python C API. - You may need to look at the sections on defining new types and implementing the iterator protocol in Python C API documentation for guidance.","solution":"class CustomObject: def __init__(self): self._data = [] @property def data(self): return self._data @data.setter def data(self, value): if isinstance(value, list) and all(isinstance(i, int) for i in value): self._data = value else: raise TypeError(\\"Data must be a list of integers\\") def __iter__(self): return iter(self._data) def __lt__(self, other): if isinstance(other, CustomObject): return sum(self._data) < sum(other._data) return NotImplemented def __le__(self, other): if isinstance(other, CustomObject): return sum(self._data) <= sum(other._data) return NotImplemented def __eq__(self, other): if isinstance(other, CustomObject): return sum(self._data) == sum(other._data) return NotImplemented def __ne__(self, other): if isinstance(other, CustomObject): return sum(self._data) != sum(other._data) return NotImplemented def __gt__(self, other): if isinstance(other, CustomObject): return sum(self._data) > sum(other._data) return NotImplemented def __ge__(self, other): if isinstance(other, CustomObject): return sum(self._data) >= sum(other._data) return NotImplemented"},{"question":"# Hyper-Parameter Tuning with Grid Search and Randomized Search in Scikit-learn **Objective:** Demonstrate your understanding of hyper-parameter tuning using scikit-learn by implementing hyper-parameter optimization with both `GridSearchCV` and `RandomizedSearchCV`. Task Description 1. **Dataset**: - Use the Iris dataset from scikit-learn\'s datasets module. 2. **Estimator**: - Use the Support Vector Classifier (SVC) from scikit-learn\'s svm module. 3. **Hyper-parameter Tuning**: - Perform hyper-parameter tuning using both Grid Search and Randomized Search. - For Grid Search, use the following grid: ```python param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] ``` - For Randomized Search, use the following distributions: ```python from scipy.stats import expon param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } ``` - Set the number of iterations for Randomized Search to 50. 4. **Evaluation**: - Use 5-fold cross-validation for both Grid Search and Randomized Search. 5. **Metrics**: - Use accuracy as the scoring metric. 6. **Output**: - Print the best parameters and the best score for both Grid Search and Randomized Search. Constraints: - You should not use any global random seed settings to ensure reproducibility. Implementation Details: - Follow the usual estimator API provided by scikit-learn. - Ensure all import statements required for the task are included in your solution. Expected Function Definition: ```python import numpy as np from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, RandomizedSearchCV def hyperparameter_tuning(): # Load the dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Define the model model = SVC() # Grid Search param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y) print(f\\"Grid Search Best Params: {grid_search.best_params_}\\") print(f\\"Grid Search Best Score: {grid_search.best_score_}\\") # Randomized Search from scipy.stats import expon param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=50, cv=5, scoring=\'accuracy\') random_search.fit(X, y) print(f\\"Random Search Best Params: {random_search.best_params_}\\") print(f\\"Random Search Best Score: {random_search.best_score_}\\") # Call the function to output results hyperparameter_tuning() ``` Submission: - Implement the function `hyperparameter_tuning` and ensure it runs without errors. - The function should output the best parameters and the best score for both Grid Search and Randomized Search.","solution":"import numpy as np from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, RandomizedSearchCV def hyperparameter_tuning(): # Load the dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Define the model model = SVC() # Grid Search param_grid = [ {\'C\': [1, 10, 100, 1000], \'kernel\': [\'linear\']}, {\'C\': [1, 10, 100, 1000], \'gamma\': [0.001, 0.0001], \'kernel\': [\'rbf\']}, ] grid_search = GridSearchCV(model, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y) grid_search_best_params = grid_search.best_params_ grid_search_best_score = grid_search.best_score_ # Randomized Search from scipy.stats import expon param_dist = { \'C\': expon(scale=100), \'gamma\': expon(scale=0.1), \'kernel\': [\'rbf\'], \'class_weight\': [\'balanced\', None] } random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=50, cv=5, scoring=\'accuracy\') random_search.fit(X, y) random_search_best_params = random_search.best_params_ random_search_best_score = random_search.best_score_ return { \'grid_search\': { \'best_params\': grid_search_best_params, \'best_score\': grid_search_best_score }, \'random_search\': { \'best_params\': random_search_best_params, \'best_score\': random_search_best_score }, }"},{"question":"# PyTorch Intermediate Representations (IRs) and Functional Interface **Objective**: Implement a function that utilizes both Core Aten IR and Prims IR concepts to perform matrix multiplication with explicit type promotion and broadcasting. Problem Statement: You are required to write a Python function using PyTorch that performs matrix multiplication of two tensors. The function should: 1. Use Core Aten IR and Prims IR concepts. 2. Handle type promotion and broadcasting using Prims IR functions. 3. Ensure the result tensor\'s data type matches the highest precision type among the input tensors. Function Signature: ```python import torch def ir_matrix_multiply(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: pass ``` Input: - `tensor_a` (torch.Tensor): The first input tensor of shape (m, n). - `tensor_b` (torch.Tensor): The second input tensor of shape (n, p). Output: - Returns a torch.Tensor of shape (m, p) which is the result of matrix multiplication of `tensor_a` and `tensor_b`. Constraints: - The input tensors can be of different data types, and the function should ensure proper type promotion. - Implement type promotion and broadcasting in accordance with Prims IR specifications. - Use the given PyTorch functions and primitives effectively. Example: ```python # Example usage a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32) b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float64) result = ir_matrix_multiply(a, b) print(result) ``` Expected output should match the matrix multiplication result with appropriate type promotion to `float64`. Notes: - This problem will test theoretical understanding of PyTorch’s IRs as well as practical skills in matrix operations, type promotion, and broadcasting. - Focus on correctly using primitives for type conversion and tensor operations specified in the documentation.","solution":"import torch def ir_matrix_multiply(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Multiplies two tensors using PyTorch while ensuring type promotion and broadcasting. tensor_a: torch.Tensor of shape (m, n) tensor_b: torch.Tensor of shape (n, p) Returns: torch.Tensor of shape (m, p) # Determine the result type based on type promotion rules result_dtype = torch.promote_types(tensor_a.dtype, tensor_b.dtype) # Cast tensors to the result type tensor_a = tensor_a.to(result_dtype) tensor_b = tensor_b.to(result_dtype) # Perform the matrix multiplication result = torch.matmul(tensor_a, tensor_b) return result"},{"question":"Task You are required to implement a function using PyTorch meta tensors that performs the following operations: 1. Load a given neural network model onto the meta device. 2. Obtain and print the metadata of the model parameters. 3. Transfer the meta tensor model back to its original device (CPU or CUDA) but ensure the parameters remain uninitialized. 4. Generate a meta tensor with the same shape as the model\'s parameters and replace the original model parameters with zero-initialized tensors. 5. Return the modified model. Guidelines - The model will be provided in a serialized file (e.g., \\"model.pth\\"). - Your function should be named `process_meta_model`. - The input to the function is the serialized model file path as a string (e.g., `\\"model.pth\\"`). - The output should be the transformed model with zero-initialized parameters on the original device (CPU or CUDA). Input Example ```python model_path = \\"model.pth\\" ``` Output Example ```python <Modified Model Object> ``` Constraints - Ensure operations that are not supported by meta tensors are correctly handled. - The transformations should make use of the meta device to avoid loading actual data initially. Function Signature ```python import torch from torch import nn def process_meta_model(model_path: str) -> nn.Module: # Your implementation here pass model = process_meta_model(\\"model.pth\\") print(model) ``` Requirements 1. Load the model on the meta device. 2. Print the metadata of the model parameters (like size, device type). 3. Transform the model to its original device with uninitialized parameters. 4. Replace the model parameters with zero-initialized tensors on the original device. 5. Ensure the function is robust and handles potential errors gracefully. Additional Information You may make use of the following PyTorch functions/methods: - `torch.load` - `torch.device` - `torch.empty_like` - `torch.Tensor.to` (with the `to_empty` method for `nn.Module`)","solution":"import torch from torch import nn def process_meta_model(model_path: str) -> nn.Module: # Load the original model model = torch.load(model_path, map_location=torch.device(\'cpu\')) original_device = next(model.parameters()).device # Move the model to the \'meta\' device meta_model = model.to_empty(device=\'meta\') # Print the metadata of the model parameters for name, param in meta_model.named_parameters(): print(f\'Parameter: {name}, Size: {param.size()}, Device: {param.device}\') # Transfer the meta tensor model back to its original device but keep parameters uninitialized model_on_original_device = model.to_empty(device=original_device.type) # Create a zero-initialized tensor with same shape and device of original parameters for name, param in model_on_original_device.named_parameters(): with torch.no_grad(): param.copy_(torch.zeros_like(param)) return model_on_original_device"},{"question":"# Sparse Data Manipulation and Conversion **Problem:** You are tasked with processing a large dataset with many missing values efficiently using pandas\' sparse data structures. You will start by creating a DataFrame, converting it to a sparse format, and performing several operations to demonstrate your understanding of sparse data manipulation in pandas. **Instructions:** 1. **Creation of Sparse DataFrame:** - Create a DataFrame with 10,000 rows and 4 columns filled with random floating-point numbers. - Set the values in the first 9,998 rows to `NaN`. 2. **Conversion to Sparse Format:** - Convert the DataFrame to a sparse format with `NaN` as the fill value. 3. **Sparse Attribute Access:** - Retrieve and print the density (fraction of non-`NaN` values) of the sparse DataFrame. 4. **Sparse Calculation:** - Apply the numpy `abs` (absolute value) function to the sparse DataFrame and print the result. 5. **Interaction with scipy.sparse:** - Convert the sparse DataFrame to a `scipy.sparse.csr_matrix`. - Convert it back to a pandas sparse DataFrame and print the first 5 rows. # Example: ```python import numpy as np import pandas as pd from scipy.sparse import csr_matrix # Step 1: Create a dense DataFrame df = pd.DataFrame(np.random.randn(10000, 4)) df.iloc[:9998] = np.nan # Set first 9998 rows to NaN # Step 2: Convert to sparse format sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Step 3: Retrieve and print sparse density density = sdf.sparse.density print(\\"Sparse Density:\\", density) # Step 4: Apply numpy abs function to the sparse DataFrame sdf_abs = np.abs(sdf) print(sdf_abs) # Step 5: Convert to scipy.sparse.csr_matrix and back sp_matrix = csr_matrix(sdf.sparse.to_dense()) # Convert to scipy sparse matrix (csr) sdf_reconstructed = pd.DataFrame.sparse.from_spmatrix(sp_matrix) # Convert back to pandas sparse DataFrame print(sdf_reconstructed.head()) # Print the first 5 rows of the reconstructed sparse DataFrame ``` # Constraints: - The data should be efficiently handled using sparse structures to save memory. - Ensure the conversion to and from `scipy.sparse.csr_matrix` does not lose any data or introduce errors. **You are required to implement the full process described above in a single script. Ensure all steps are executed sequentially and the results are printed as specified.**","solution":"import numpy as np import pandas as pd from scipy.sparse import csr_matrix def sparse_data_manipulation(): # Step 1: Create a dense DataFrame df = pd.DataFrame(np.random.randn(10000, 4)) df.iloc[:9998] = np.nan # Set first 9998 rows to NaN # Step 2: Convert to sparse format sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) # Step 3: Retrieve and print sparse density density = sdf.sparse.density print(\\"Sparse Density:\\", density) # Step 4: Apply numpy abs function to the sparse DataFrame sdf_abs = np.abs(sdf) print(sdf_abs) # Step 5: Convert to scipy.sparse.csr_matrix and back sp_matrix = csr_matrix(sdf.sparse.to_dense()) # Convert to scipy sparse matrix (csr) sdf_reconstructed = pd.DataFrame.sparse.from_spmatrix(sp_matrix) # Convert back to pandas sparse DataFrame print(sdf_reconstructed.head()) # Print the first 5 rows of the reconstructed sparse DataFrame return sdf, sdf_abs, sdf_reconstructed # Function call to see the results (uncomment the line below if running interactively) # sparse_data_manipulation()"},{"question":"Introduction The `site` module in Python is used to handle site-specific configuration that manipulates the module search path (`sys.path`). For this task, you\'ll demonstrate your understanding by writing a function to add directories to `sys.path` from a `.pth` file, and then retrieve the configured paths. Objective Write a function that: 1. Adds directories listed in a given `.pth` file to `sys.path`. 2. Retrieves paths added to `sys.path` using the `site.getsitepackages()` and `site.getusersitepackages()` functions. Function Signature ```python def configure_paths(pth_filepath: str) -> list: pass ``` Input - `pth_filepath` (str): The file path to a `.pth` file that contains new paths to be added to `sys.path`. Output - Returns a list containing: - Paths added to `sys.path` from the `.pth` file. - Global site-packages directories obtained from `site.getsitepackages()`. - User-specific site-packages directory from `site.getusersitepackages()`. Constraints - Ensure that non-existent paths from the `.pth` file are not added to `sys.path`. - The `.pth` file may contain blank lines or comments (lines starting with `#`), which should be ignored. - The function should correctly handle the `.pth` file in the given format and avoid adding duplicate paths to `sys.path`. Example Assume the `.pth` file at `\\"/example/foo.pth\\"` contains the following lines: ```plaintext # Sample .pth file /existing/path/foo /non/existing/path/bar /existing/path/baz ``` If `/existing/path/foo` and `/existing/path/baz` are valid directories on the filesystem, and `/non/existing/path/bar` is not, the function should: 1. Add `/existing/path/foo` and `/existing/path/baz` to `sys.path`. 2. Return a combined list of these paths along with the global and user-specific site-packages directories. Notes - You can use built-in functions and modules such as `os.path` to verify the existence of directories. - This task aims to test your understanding of manipulating `sys.path` and working with site-specific configurations in Python. ```python import os import site def configure_paths(pth_filepath): added_paths = [] # Read the .pth file and process each line with open(pth_filepath, \'r\') as file: for line in file.readlines(): path = line.strip() if path and not path.startswith(\'#\'): if os.path.isdir(path) and path not in sys.path: sys.path.append(path) added_paths.append(path) # Get the site-packages directories site_packages_dirs = site.getsitepackages() user_site_packages_dir = site.getusersitepackages() return added_paths + site_packages_dirs + [user_site_packages_dir] ``` Write the function as specified and ensure it passes various test cases including edge cases.","solution":"import os import site import sys def configure_paths(pth_filepath: str) -> list: added_paths = [] # Read the .pth file and process each line with open(pth_filepath, \'r\') as file: for line in file.readlines(): path = line.strip() if path and not path.startswith(\'#\'): if os.path.isdir(path) and path not in sys.path: sys.path.append(path) added_paths.append(path) # Get the site-packages directories site_packages_dirs = site.getsitepackages() user_site_packages_dir = site.getusersitepackages() return added_paths + site_packages_dirs + [user_site_packages_dir]"},{"question":"**Question: Custom Object Serialization and Deserialization** In this challenge, you are required to implement a pair of functions `custom_serialize` and `custom_deserialize` that utilize the `marshal` module to serialize and deserialize specific Python objects. Your implementation should handle both the writing to/reading from a file and the conversion to/from a bytes-like object. Additionally, you need to handle cases where the object contains unsupported types by substituting them with `None` and ensure proper error handling. # Requirements 1. **Function: `custom_serialize`** - **Input:** - `obj`: a Python object containing supported types (booleans, integers, floats, strings, bytes, lists, tuples, sets, dictionaries, None, Ellipsis, StopIteration). - `file_path`: a string representing the file path to write the serialized data (can be None if `to_bytes` is used). - `to_bytes`: a boolean indicating whether to serialize to a bytes-like object instead of a file. - **Output:** - If `to_bytes` is True, return the bytes-like object representing the serialized data. - If `to_bytes` is False, write the serialized data to the specified file. - **Constraints:** - If `to_bytes` and `file_path` are both specified, raise a `ValueError`. 2. **Function: `custom_deserialize`** - **Input:** - `file_path`: a string representing the file path to read the serialized data from (can be None if `from_bytes` is used). - `data`: a bytes-like object containing the serialized data (optional, used only if `file_path` is None). - **Output:** - Return the deserialized Python object. - **Constraints:** - If both `file_path` and `data` are provided, raise a `ValueError`. 3. **Error Handling:** - Ensure proper error handling for unsupported types, file I/O errors, and data integrity issues during the serialization and deserialization process. # Example Usage ```python # Case 1: Serializing and deserializing to/from a file obj = {\\"key1\\": 123, \\"key2\\": [1, 2, 3]} file_path = \\"test.marshal\\" custom_serialize(obj, file_path=file_path) assert custom_deserialize(file_path=file_path) == obj # Case 2: Serializing and deserializing to/from a bytes-like object obj = {\\"number\\": 456, \\"float\\": 7.89} serialized_data = custom_serialize(obj, to_bytes=True) assert custom_deserialize(data=serialized_data) == obj # Case 3: Handling unsupported types obj = {\\"supported\\": 123, \\"unsupported\\": complex(1, 2)} # complex numbers are unsupported file_path = \\"test_unsupported.marshal\\" custom_serialize(obj, file_path=file_path) deserialized_obj = custom_deserialize(file_path=file_path) assert deserialized_obj == {\\"supported\\": 123, \\"unsupported\\": None} ``` **Note:** Your implementation should aim to be efficient and robust in handling the specified constraints and requirements.","solution":"import marshal SUPPORTED_TYPES = (bool, int, float, str, bytes, list, tuple, set, dict, type(None), type(...), StopIteration) def replace_unsupported(v): if isinstance(v, SUPPORTED_TYPES): if isinstance(v, (list, tuple, set)): return type(v)(replace_unsupported(i) for i in v) elif isinstance(v, dict): return {replace_unsupported(k): replace_unsupported(v) for k, v in v.items()} return v return None def custom_serialize(obj, file_path=None, to_bytes=False): if to_bytes and file_path is not None: raise ValueError(\\"Cannot specify both file_path and to_bytes\\") supported_obj = replace_unsupported(obj) data = marshal.dumps(supported_obj) if to_bytes: return data else: with open(file_path, \'wb\') as f: f.write(data) def custom_deserialize(file_path=None, data=None): if file_path is not None and data is not None: raise ValueError(\\"Cannot specify both file_path and data\\") if file_path: with open(file_path, \'rb\') as f: data = f.read() obj = marshal.loads(data) return obj"},{"question":"# Question: Implement a Custom Configuration Loader Your task is to implement a configuration loader that reads both text and binary configuration files. This loader should support: 1. Reading text configurations encoded in UTF-8. 2. Reading binary configurations in a specific format (detailed below). 3. Handling different types of newline characters in text files. 4. Operating with high performance by using buffered I/O for reading files. You must implement two functions: `load_text_config` and `load_binary_config`. # Specifications: 1. **Function `load_text_config(file_path: str) -> str`** - **Input**: - `file_path`: A string representing the path to a UTF-8 encoded text configuration file. - **Output**: A string representing the complete content of the configuration file. - **Constraints**: - The file can contain any type of newline characters (`n`, `r`, or `rn`), but they must all be converted to `n` in the returned string. - Handle the file using a buffered text stream with a buffer size of 4096 bytes. - Raise a `ValueError` if any errors occur during file reading other than file not found. - **Example**: ```python # Suppose \'config.txt\' contains \\"line1rnline2rline3n\\" load_text_config(\'config.txt\') # Should return: \\"line1nline2nline3n\\" ``` 2. **Function `load_binary_config(file_path: str) -> bytes`** - **Input**: - `file_path`: A string representing the path to a binary configuration file. - **Output**: A bytes object representing the content of the configuration file. - **Constraints**: - Use a buffered binary stream with default buffer size. - Raise a `ValueError` if any errors occur during file reading other than file not found. - **Example**: ```python # Suppose \'config.bin\' contains b\'x00xFFxFExFD\' load_binary_config(\'config.bin\') # Should return: b\'x00xFFxFExFD\' ``` # Performance Requirements: - Both functions should be implemented to handle large files efficiently using buffered I/O techniques. # Notes: Remember to handle the opening and closing of files properly, ensuring no resources are leaked. Use meaningful exception handling to facilitate appropriate error feedback in case of reading issues. # Evaluation Criteria: 1. Correctness: The functions must work as specified and handle various newline characters correctly in text files. 2. Efficiency: The functions should efficiently handle large files using buffered I/O. 3. Robustness: The functions should handle exceptions gracefully and not leak any resources.","solution":"def load_text_config(file_path: str) -> str: Reads a UTF-8 encoded text configuration file and returns its content with all newline characters converted to \'n\'. Args: - file_path (str): Path to the UTF-8 encoded text file. Returns: - str: Content of the file with normalized newlines. Raises: - ValueError: For any file reading errors except file not found. try: with open(file_path, \'r\', encoding=\'utf-8\', buffering=4096) as file: content = file.read() return content.replace(\'rn\', \'n\').replace(\'r\', \'n\') except FileNotFoundError: raise FileNotFoundError(f\\"File not found: {file_path}\\") except Exception as e: raise ValueError(f\\"Error reading file: {file_path}, {e}\\") def load_binary_config(file_path: str) -> bytes: Reads a binary configuration file and returns its content. Args: - file_path (str): Path to the binary file. Returns: - bytes: Content of the binary file. Raises: - ValueError: For any file reading errors except file not found. try: with open(file_path, \'rb\') as file: return file.read() except FileNotFoundError: raise FileNotFoundError(f\\"File not found: {file_path}\\") except Exception as e: raise ValueError(f\\"Error reading file: {file_path}, {e}\\")"},{"question":"You are given a dataset containing information about the performance of students in different subjects. The dataset is stored in a pandas DataFrame with the following columns: - \\"student_id\\": Unique identifier for each student. - \\"math_score\\": The score obtained by the student in mathematics. - \\"english_score\\": The score obtained by the student in English. - \\"science_score\\": The score obtained by the student in science. Perform the following tasks: 1. Convert the given dataset from wide-form to long-form with the subject names as a variable and scores as values. 2. Create a line plot using seaborn to visualize the trend of scores for each student across different subjects. The x-axis should represent the subjects and the y-axis should represent the scores. 3. Additionally, create a wide-form visualization of the dataset to compare the scores of all students across each subject on a single plot. The dataset is provided below: ```python import pandas as pd data = { \\"student_id\\": [1, 2, 3, 4, 5], \\"math_score\\": [85, 78, 92, 88, 76], \\"english_score\\": [90, 82, 88, 85, 80], \\"science_score\\": [88, 79, 85, 87, 78] } df = pd.DataFrame(data) ``` # Constraints * Use the seaborn library for visualization. * Ensure that the plots are clear and well-labeled. # Expected Output 1. The long-form dataset. 2. A line plot showing the trend of student scores across subjects. 3. A wide-form comparison plot of student scores across subjects. # Example ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load data data = { \\"student_id\\": [1, 2, 3, 4, 5], \\"math_score\\": [85, 78, 92, 88, 76], \\"english_score\\": [90, 82, 88, 85, 80], \\"science_score\\": [88, 79, 85, 87, 78] } df = pd.DataFrame(data) # Convert wide-form to long-form df_long = df.melt(id_vars=[\\"student_id\\"], value_vars=[\\"math_score\\", \\"english_score\\", \\"science_score\\"], var_name=\\"subject\\", value_name=\\"score\\") # Plot long-form data plt.figure(figsize=(10, 6)) sns.lineplot(data=df_long, x=\\"subject\\", y=\\"score\\", hue=\\"student_id\\", marker=\\"o\\") plt.title(\\"Student Scores Across Subjects (Long-form)\\") plt.ylabel(\\"Score\\") plt.xlabel(\\"Subject\\") plt.legend(title=\\"Student ID\\") plt.show() # Plot wide-form data plt.figure(figsize=(10, 6)) df.set_index(\\"student_id\\").T.plot(marker=\\"o\\") plt.title(\\"Student Scores Across Subjects (Wide-form)\\") plt.ylabel(\\"Score\\") plt.xlabel(\\"Subject\\") plt.legend(title=\\"Student ID\\") plt.show() ```","solution":"import pandas as pd def convert_to_long_form(df): Converts a wide-form dataframe to a long-form dataframe. Parameters: df (pd.DataFrame): Input wide-form dataframe Returns: pd.DataFrame: Long-form dataframe df_long = df.melt(id_vars=[\\"student_id\\"], value_vars=[\\"math_score\\", \\"english_score\\", \\"science_score\\"], var_name=\\"subject\\", value_name=\\"score\\") return df_long def create_line_plot(df_long): Creates a line plot for the given long-form dataframe to visualize the trend of scores. Parameters: df_long (pd.DataFrame): Input long-form dataframe import seaborn as sns import matplotlib.pyplot as plt plt.figure(figsize=(10, 6)) sns.lineplot(data=df_long, x=\\"subject\\", y=\\"score\\", hue=\\"student_id\\", marker=\\"o\\") plt.title(\\"Student Scores Across Subjects (Long-form)\\") plt.ylabel(\\"Score\\") plt.xlabel(\\"Subject\\") plt.legend(title=\\"Student ID\\") plt.show() def create_wide_form_plot(df): Creates a wide-form plot to compare the student scores across subjects. Parameters: df (pd.DataFrame): Input wide-form dataframe import matplotlib.pyplot as plt plt.figure(figsize=(10, 6)) df.set_index(\\"student_id\\").T.plot(marker=\\"o\\") plt.title(\\"Student Scores Across Subjects (Wide-form)\\") plt.ylabel(\\"Score\\") plt.xlabel(\\"Subject\\") plt.legend(title=\\"Student ID\\") plt.show() # Example usage with the provided dataset data = { \\"student_id\\": [1, 2, 3, 4, 5], \\"math_score\\": [85, 78, 92, 88, 76], \\"english_score\\": [90, 82, 88, 85, 80], \\"science_score\\": [88, 79, 85, 87, 78] } df = pd.DataFrame(data) df_long = convert_to_long_form(df) # Create plots create_line_plot(df_long) create_wide_form_plot(df)"},{"question":"Objective Demonstrate your understanding of handling cell objects in Python by implementing a set of wrappers around some of the core functions provided in the Python C API documentation. Task You need to write a Python module that provides functions for creating, accessing, and modifying cell objects. These functions should closely mimic the behavior described in the documentation but should be implemented purely in Python. Functions to Implement: 1. **is_cell(obj)**: - **Input**: A single Python object (`obj`). - **Output**: Returns `True` if the object is a cell object, `False` otherwise. 2. **create_cell(value)**: - **Input**: A value (`value`) to be stored in the cell. - **Output**: Returns a new cell object containing the given value. 3. **get_cell_value(cell)**: - **Input**: A cell object (`cell`). - **Output**: Returns the value contained in the cell object. 4. **set_cell_value(cell, value)**: - **Input**: A cell object (`cell`) and a new value (`value`) to be stored in the cell. - **Output**: None. The function updates the cell object with the new value. Example ```python # Example usage of the implemented functions # Create a cell object containing the value 10 cell = create_cell(10) # Check if the created object is a cell print(is_cell(cell)) # Output: True # Retrieve the value stored in the cell object print(get_cell_value(cell)) # Output: 10 # Update the value in the cell object to 20 set_cell_value(cell, 20) # Retrieve the updated value stored in the cell object print(get_cell_value(cell)) # Output: 20 ``` Constraints - Do not use built-in or third-party libraries for managing cell objects directly. - Ensure that your implementation strictly follows the behaviors as outlined in the task description. Performance Requirement - The functions should efficiently handle the creation, access, and modification of cell objects without significant performance overhead.","solution":"import types def is_cell(obj): Returns True if the object is a cell object, False otherwise. return isinstance(obj, types.CellType) def create_cell(value): Creates a new cell object containing the given value. # Using a closure to create a cell object def closure(): return value return closure.__closure__[0] def get_cell_value(cell): Returns the value contained in the cell object. if not is_cell(cell): raise TypeError(\\"The provided object is not a cell.\\") return cell.cell_contents def set_cell_value(cell, value): Sets a new value into the cell object. if not is_cell(cell): raise TypeError(\\"The provided object is not a cell.\\") cell.cell_contents = value"}]'),z={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const s=this.searchQuery.trim().toLowerCase();return s?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(s)||e.solution&&e.solution.toLowerCase().includes(s)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(s=>setTimeout(s,1e3)),this.visibleCount+=4,this.isLoading=!1}}},D={class:"search-container"},q={class:"card-container"},R={key:0,class:"empty-state"},F=["disabled"],M={key:0},N={key:1};function O(s,e,l,m,i,o){const h=_("PoemCard");return a(),n("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",D,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),n("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",q,[(a(!0),n(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),n("div",R,' No results found for "'+u(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),n("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),n("span",N,"Loading...")):(a(),n("span",M,"See more"))],8,F)):d("",!0)])}const j=p(z,[["render",O],["__scopeId","data-v-429cebe9"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/66.md","filePath":"quotes/66.md"}'),L={name:"quotes/66.md"},H=Object.assign(L,{setup(s){return(e,l)=>(a(),n("div",null,[x(j)]))}});export{Y as __pageData,H as default};
