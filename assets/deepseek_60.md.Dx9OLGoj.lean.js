import{_ as p,o as a,c as s,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},S={class:"review-content"};function E(n,e,l,m,r,i){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",S,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",E],["__scopeId","data-v-c78b150d"]]),D=JSON.parse('[{"question":"**Objective:** Assess the students\' ability to handle and manipulate data using pandas, apply various data transformation, and aggregation techniques. # Problem Statement: You have been given a dataset containing information about different products sold by a company. The dataset includes the following columns: - `product_id`: A unique identifier for each product. - `category`: Category of the product (e.g., electronics, furniture, etc.). - `price`: The price of the product. - `quantity_sold`: The quantity of the product sold. - `date_of_sale`: The date when the product was sold. You need to perform various data manipulations and analysis using pandas. Specifically, you need to: 1. **Read the Dataset:** - Read the dataset from a CSV file named `sales_data.csv`. 2. **Basic Data Cleaning:** - Handle any missing values in the `price` and `quantity_sold` columns by replacing them with the mean value of the respective columns. 3. **Data Transformation and Analysis:** - Create a new column `total_revenue` which is calculated as the product of `price` and `quantity_sold`. - Group the data by `category` and compute the total revenue and average price for each category. - Filter the data to include only those products that were sold after January 1, 2021. 4. **Pivot Table and Reshape:** - Create a pivot table that shows the total revenue for each `category` per month. - Reshape the pivot table to stack the months into a single column. 5. **Visualization:** - Plot a bar chart showing the total revenue for each category. # Function Signature ```python import pandas as pd def analyze_sales_data(file_path: str) -> None: # Function to read the dataset, perform data cleaning, transformation, analysis, # create pivot table, reshape, and plot the results. pass ``` # Input: - `file_path` (str): The path to the `sales_data.csv` file. # Expected Output: - The function should perform the above operations and plot a bar chart displaying the total revenue for each product category. # Constraints: - You are required to use pandas for all data manipulations. - Ensure that all necessary imports are included in your solution. # Example Usage: ```python # Assuming the file sales_data.csv exists in the current directory analyze_sales_data(\\"sales_data.csv\\") ``` # Notes: - For the bar chart, you can use pandas plotting capabilities or use matplotlib. - Ensure that your solution handles edge cases, such as missing values or dates in different formats. # Dataset Example (`sales_data.csv`): ``` product_id,category,price,quantity_sold,date_of_sale 1,electronics,500,4,2021-02-15 2,furniture,1500,2,2021-03-20 3,clothing,100,10,2020-12-25 4,electronics,,5,2021-01-05 5,furniture,1250,3,2021-02-10 ```","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path: str) -> None: # Read the dataset from the CSV file df = pd.read_csv(file_path) # Basic Data Cleaning - Handle missing values in \'price\' and \'quantity_sold\' df[\'price\'].fillna(df[\'price\'].mean(), inplace=True) df[\'quantity_sold\'].fillna(df[\'quantity_sold\'].mean(), inplace=True) # Data Transformation - Add \'total_revenue\' column df[\'total_revenue\'] = df[\'price\'] * df[\'quantity_sold\'] # Group by \'category\' and compute total revenue and average price category_group = df.groupby(\'category\').agg(total_revenue=(\'total_revenue\', \'sum\'), average_price=(\'price\', \'mean\')).reset_index() # Filter data to include only sales after January 1, 2021 df[\'date_of_sale\'] = pd.to_datetime(df[\'date_of_sale\']) filtered_df = df[df[\'date_of_sale\'] > \'2021-01-01\'] # Pivot table showing total revenue for each \'category\' per month filtered_df[\'month\'] = filtered_df[\'date_of_sale\'].dt.to_period(\'M\') pivot_table = pd.pivot_table(filtered_df, values=\'total_revenue\', index=\'month\', columns=\'category\', aggfunc=\'sum\', fill_value=0) pivot_table = pivot_table.reset_index() # Reshape pivot table to stack months into a single column reshaped_pivot = pivot_table.melt(id_vars=[\'month\'], value_vars=pivot_table.columns[1:], var_name=\'category\', value_name=\'total_revenue\') # Visualization - Plot a bar chart showing total revenue for each category category_total_revenue = df.groupby(\'category\')[\'total_revenue\'].sum() category_total_revenue.plot(kind=\'bar\') plt.title(\'Total Revenue for Each Category\') plt.xlabel(\'Category\') plt.ylabel(\'Total Revenue\') plt.show()"},{"question":"**Problem Statement:** Write a Python program that sets up custom signal handlers for different types of signals. Your program should demonstrate the ability to: 1. Set and handle the `SIGINT` signal (typically sent using Ctrl+C). 2. Set and handle the `SIGALRM` signal using a timer. 3. Set and handle the `SIGUSR1` signal. 4. Utilize `signal.pause` to wait for a signal to be received. **Function Signatures:** ```python def handle_sigint(signum, frame): pass def handle_sigalrm(signum, frame): pass def handle_sigusr1(signum, frame): pass def setup_signal_handlers(): pass def main(): setup_signal_handlers() # Set an alarm for 5 seconds signal.alarm(5) # Wait for a signal to be received signal.pause() ``` **Detailed Requirements:** 1. Implement the `handle_sigint` function to print \\"SIGINT received, exiting.\\" and exit the program using `sys.exit(0)`. 2. Implement the `handle_sigalrm` function to print \\"SIGALRM received, alarm triggered.\\" 3. Implement the `handle_sigusr1` function to print \\"SIGUSR1 received.\\" 4. Implement the `setup_signal_handlers` function to register the above handlers for their respective signals using `signal.signal`. 5. In the `main` function, call `setup_signal_handlers`, set an alarm to trigger in 5 seconds, and use `signal.pause` to wait for signals. **Constraints:** - The implemented signal handlers should properly print the messages described above. - The `SIGINT` handler should exit the program gracefully. - The code should be executable in a Unix environment where `SIGUSR1` and `SIGALRM` signals are available. **Example Execution:** When the program is run: 1. If the user sends a `SIGINT` using Ctrl+C, it should print \\"SIGINT received, exiting.\\" and terminate the program. 2. If the `SIGALRM` fires after 5 seconds, it should print \\"SIGALRM received, alarm triggered.\\" 3. If a `SIGUSR1` is sent to the process, it should print \\"SIGUSR1 received.\\" **Testing Guide:** - Run the program and use `Ctrl+C` to ensure the `SIGINT` handler is triggered. - Run the program and wait for 5 seconds to ensure the `SIGALRM` handler is triggered. - Send a `SIGUSR1` signal to the process using the `kill` command from another terminal: ```bash kill -USR1 <pid> ``` Replace `<pid>` with the program\'s process ID. Ensure the `SIGUSR1` handler is triggered and prints the appropriate message. ```python import signal import sys def handle_sigint(signum, frame): print(\\"SIGINT received, exiting.\\") sys.exit(0) def handle_sigalrm(signum, frame): print(\\"SIGALRM received, alarm triggered.\\") def handle_sigusr1(signum, frame): print(\\"SIGUSR1 received.\\") def setup_signal_handlers(): signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGALRM, handle_sigalrm) signal.signal(signal.SIGUSR1, handle_sigusr1) def main(): setup_signal_handlers() # Set an alarm for 5 seconds signal.alarm(5) # Wait for a signal to be received signal.pause() if __name__ == \'__main__\': main() ```","solution":"import signal import sys def handle_sigint(signum, frame): print(\\"SIGINT received, exiting.\\") sys.exit(0) def handle_sigalrm(signum, frame): print(\\"SIGALRM received, alarm triggered.\\") def handle_sigusr1(signum, frame): print(\\"SIGUSR1 received.\\") def setup_signal_handlers(): signal.signal(signal.SIGINT, handle_sigint) signal.signal(signal.SIGALRM, handle_sigalrm) signal.signal(signal.SIGUSR1, handle_sigusr1) def main(): setup_signal_handlers() # Set an alarm for 5 seconds signal.alarm(5) # Wait for a signal to be received signal.pause() if __name__ == \'__main__\': main()"},{"question":"You are provided with a log file containing temperature readings taken every 10 minutes throughout the day. Your task is to find the top N highest temperatures over the course of the day. Each temperature reading contains a timestamp and the temperature value. Implement a function `find_top_n_temperatures(log_file: str, n: int) -> List[Tuple[str, float]]` that reads the log file, processes the temperature data, and returns a list of the top N highest temperature readings along with their timestamps. # Input: - `log_file`: A string representing the filename of the log file. Each line in the log file contains a timestamp and a temperature reading separated by a comma. - `n`: An integer representing the number of highest temperatures to find. # Output: - A list of tuples where each tuple contains a timestamp (string) and a temperature (float). The list should be sorted in descending order by temperature. # Constraints: - The log file is guaranteed to have at least `n` temperature readings. - The format of each line in the log file is \'timestamp,temperature\'. For example: ``` 2023-10-05 08:00,23.4 2023-10-05 08:10,24.1 2023-10-05 08:20,22.8 ``` # Requirements: - The function should efficiently handle large log files using the heapq module. - The implementation should prioritize performance and operate in O(N log n) time complexity, where N is the total number of temperature readings in the log file. # Example: Given the following log file contents: ``` 2023-10-05 08:00,23.4 2023-10-05 08:10,24.1 2023-10-05 08:20,22.8 2023-10-05 08:30,25.6 2023-10-05 08:40,24.9 2023-10-05 08:50,26.3 2023-10-05 09:00,23.5 ``` And `n = 3`, calling `find_top_n_temperatures(\\"temperature_log.txt\\", 3)` should return: ``` [ (\\"2023-10-05 08:50\\", 26.3), (\\"2023-10-05 08:30\\", 25.6), (\\"2023-10-05 08:40\\", 24.9) ] ``` Make sure your code handles reading from the log file and correctly uses the `heapq` module to find the top N highest temperatures.","solution":"import heapq from typing import List, Tuple def find_top_n_temperatures(log_file: str, n: int) -> List[Tuple[str, float]]: Finds the top N highest temperatures from the log file. Args: log_file (str): The name of the log file containing temperature readings. n (int): The number of top highest temperatures to return. Returns: List[Tuple[str, float]]: A list of tuples where each tuple contains a timestamp and a temperature reading. The list is sorted in descending order by temperature. with open(log_file, \'r\') as file: lines = file.readlines() min_heap = [] for line in lines: timestamp, temp_str = line.strip().split(\',\') temperature = float(temp_str) if len(min_heap) < n: heapq.heappush(min_heap, (temperature, timestamp)) else: heapq.heappushpop(min_heap, (temperature, timestamp)) largest_readings = heapq.nlargest(n, min_heap) result = [(ts, temp) for temp, ts in largest_readings] return result"},{"question":"**Custom SMTP Server Implementation** You are tasked with implementing a custom SMTP server using the deprecated `smtpd` module in Python. Your server should process incoming email messages and perform a specific action based on the content of the message. # Requirements: 1. **Custom SMTP Server Class**: - Create a class `CustomSMTPServer` inheriting from `smtpd.SMTPServer`. - Override the `process_message` method to read the content of the email message. 2. **Process Email Messages**: - If the email message contains the word \\"hello\\" (case-insensitive) in its body, log the message to a file named `greetings.log`. - If the message size exceeds a given limit (e.g., 1024 bytes), return an error response to the client. 3. **Server Initialization**: - Instantiate the server with local address `(\'localhost\', 1025)` and remote address `None`. # Implementation: - Define the `CustomSMTPServer` class as described. - Implement the `process_message` method with the specified conditions. - Ensure the server runs and is capable of handling incoming SMTP connections. # Input: The `process_message` method will receive the following parameters: - `peer` (tuple): The remote host\'s address. - `mailfrom` (str): The envelope originator. - `rcpttos` (list): A list of envelope recipients. - `data` (str or bytes): The contents of the email message. - `**kwargs`: Additional keyword arguments. # Output: The `process_message` method should: - Return \\"250 Ok\\" if the message is successfully processed. - Return an appropriate error message in RFC 5321 format if the message size exceeds the limit. # Example: ```python import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): # Implement the message processing logic here. pass # Initialize and run the server server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop() ``` **Note**: Ensure you handle the case when the message data is in bytes or string format depending on the `decode_data` parameter.","solution":"import smtpd import asyncore import logging class CustomSMTPServer(smtpd.SMTPServer): MAX_MESSAGE_SIZE = 1024 def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) logging.basicConfig(filename=\'greetings.log\', level=logging.INFO) def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): # Check message size if len(data) > self.MAX_MESSAGE_SIZE: return \'552 Message size exceeds maximum limit\' # Convert data to string if it is in bytes if isinstance(data, bytes): data = data.decode(\'utf-8\') # Check for the word \'hello\' in the message content if \'hello\' in data.lower(): logging.info(f\\"From: {mailfrom}, To: {rcpttos}, Data: {data}\\") return \'250 Ok\' # Initialize and run the server if __name__ == \\"__main__\\": server = CustomSMTPServer((\'localhost\', 1025), None) asyncore.loop()"},{"question":"# Question: Custom Plotting with Jitter using Seaborn You are tasked with analyzing the penguins dataset using Seaborn\'s `objects` module. Specifically, you need to create a customized scatter plot with jitter applied to both axes. You will use the provided `penguins` dataset for this task. Write a function `custom_penguin_plot` that takes the following parameters: - `width_jitter`: A floating-point number representing the width jitter relative to the spacing between the marks. - `x_jitter`: A floating-point number representing the jitter applied along the x-axis in data units. - `y_jitter`: A floating-point number representing the jitter applied along the y-axis in data units. The function should: 1. Load the `penguins` dataset using `seaborn`. 2. Round the `body_mass_g` to the nearest 1000 and `flipper_length_mm` to the nearest 10. 3. Create a scatter plot of `body_mass_g` vs. `flipper_length_mm` using the `so.Plot` object, with jitter applied using the provided parameters. 4. Uncomment the final plotting line when ready to visualize. Input: - `width_jitter`: float (e.g., 0.5) - `x_jitter`: float (e.g., 200) - `y_jitter`: float (e.g., 5) Output: - Save the plot to a file named `penguin_jitter_plot.png`. Constraints: - Ensure that you round the columns as specified before plotting. - Use Seaborn\'s `so.Plot` and `so.Jitter` objects for plotting. - Assume the `penguins` dataset has already been loaded. # Example Usage: ```python custom_penguin_plot(0.5, 200, 5) ``` Here is an outline of how the function should be implemented: ```python import seaborn.objects as so from seaborn import load_dataset def custom_penguin_plot(width_jitter, x_jitter, y_jitter): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Round columns body_mass_rounded = penguins[\\"body_mass_g\\"].round(-3) flipper_length_rounded = penguins[\\"flipper_length_mm\\"].round(-1) # Create the plot plot = ( so.Plot(body_mass_rounded, flipper_length_rounded) .add(so.Dots(), so.Jitter(width=width_jitter, x=x_jitter, y=y_jitter)) ) # Uncomment the following line to display/save the plot # plot.save(\\"penguin_jitter_plot.png\\") ```","solution":"import seaborn.objects as so from seaborn import load_dataset def custom_penguin_plot(width_jitter, x_jitter, y_jitter): Creates a scatter plot of body mass vs flipper length of penguins with jitter applied. Parameters: - width_jitter: float, width jitter relative to the spacing between the marks. - x_jitter: float, jitter applied along the x-axis in data units. - y_jitter: float, jitter applied along the y-axis in data units. # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Round the necessary columns penguins[\\"body_mass_g\\"] = penguins[\\"body_mass_g\\"].round(-3) penguins[\\"flipper_length_mm\\"] = penguins[\\"flipper_length_mm\\"].round(-1) # Create the plot with jitter plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\") .add(so.Dots(), so.Jitter(width=width_jitter, x=x_jitter, y=y_jitter)) ) # Save the plot to a file plot.save(\\"penguin_jitter_plot.png\\")"},{"question":"Objective: The task is to implement and work with several methods and functionalities of the `email.charset` package to handle character set conversions and encoding for email components. Problem Statement: You are required to create a utility that processes email messages by encoding their headers and bodies according to the character set specified. Implement the following functionalities using the `email.charset` module: 1. **Character Set Handler:** - Define a function `charset_handler(input_charset: str) -> Tuple[str, str, str]` that takes an `input_charset` as an argument. This function should return a tuple with the following information using the `Charset` class: - The encoding method for the email header. - The encoding method for the email body. - The output character set used. 2. **Email Encoder:** - Define a function `email_encoder(input_charset: str, header: str, body: str) -> Tuple[str, str]` that takes an `input_charset`, `header`, and `body` as arguments. This function should: - Create an instance of the `Charset` class with the given `input_charset`. - Encode the `header` and `body` using the appropriate encoding methods based on the charset properties. - Return a tuple containing the encoded header and encoded body as strings. # Input: - `charset_handler`: - `input_charset`: A string representing the character set of the email. - `email_encoder`: - `input_charset`: A string representing the character set of the email. - `header`: A string representing the content of the email header. - `body`: A string representing the content of the email body. # Output: - `charset_handler`: - Returns a tuple of three strings representing the header encoding, body encoding, and output charset used. - `email_encoder`: - Returns a tuple of two strings representing the encoded header and encoded body. # Constraints: - You can assume that the input character set is always valid and the relevant codecs for conversion are available. - All strings provided for header and body will be in valid formats suitable for encoding. # Example: ```python # Example usage of charset_handler header_enc, body_enc, output_cs = charset_handler(\\"iso-8859-1\\") print(header_enc) # Expected output: \\"quoted-printable\\" print(body_enc) # Expected output: \\"quoted-printable\\" print(output_cs) # Expected output: \\"iso-8859-1\\" # Example usage of email_encoder encoded_header, encoded_body = email_encoder(\\"iso-8859-1\\", \\"Hello World\\", \\"This is a body text.\\") print(encoded_header) # Expected output: Encoded version of \\"Hello World\\" using quoted-printable. print(encoded_body) # Expected output: Encoded version of \\"This is a body text.\\" using quoted-printable. ``` Make sure to thoroughly test your implementation with various character sets and email content to ensure correctness and compliance with the RFC standards.","solution":"from email.charset import Charset, QP def charset_handler(input_charset: str) -> tuple[str, str, str]: Takes an input_charset and returns a tuple of (header encoding, body encoding, output charset). charset = Charset(input_charset) header_encoding = charset.header_encoding body_encoding = charset.body_encoding output_charset = charset.output_charset return (header_encoding, body_encoding, output_charset) def email_encoder(input_charset: str, header: str, body: str) -> tuple[str, str]: Takes an input_charset, header and body strings, encodes them according to the given charset, and returns a tuple of (encoded_header, encoded_body). charset = Charset(input_charset) if charset.header_encoding == QP: encoded_header = header.encode(charset.output_charset).decode(\'utf-8\') else: encoded_header = charset.header_encode(header) if charset.body_encoding == QP: encoded_body = body.encode(charset.output_charset).decode(\'utf-8\') else: encoded_body = charset.body_encode(body) return (encoded_header, encoded_body)"},{"question":"# Coding Assessment: Advanced Dataclasses Design Objective Implement a complex dataclass with multiple advanced features, including default factories, frozen instances, inherited fields, and custom post-initialization behavior. Problem Statement You are required to design a dataclass-based system to manage a library with different types of books. Follow the specifications below to create your solution. 1. **Base Class - Book**: - Fields: - `title`: `str` - `author`: `str` - `published_year`: `int` - `isbn`: `str` - `copies_available`: `int` (default: 1) - Requirements: - This class should be immutable (frozen). - Implement a method `borrow_book` that reduces `copies_available` by 1 if more than 0 copies are available, raising a custom exception `OutOfStockError` if no copies are left. 2. **Derived Class - SpecialEditionBook (inherits from Book)**: - Additional Fields: - `features`: `list[str]` (use `default_factory` to initialize an empty list) - `signed`: `bool` (default: False) - Requirements: - Override the `__repr__` method to include all fields from `Book` and the additional fields in `SpecialEditionBook`. 3. **Utility Functions**: - Implement a function `library_summary(books: list[Book]) -> dict` that takes a list of `Book` objects (could include `SpecialEditionBook` objects) and returns a dictionary summarizing: - Total number of books. - Total copies available. - A breakdown of counts of each type of book (by class name). Constraints - Use Type Hints as indicated in the problem statement. - The solution must use dataclasses and their advanced features where applicable. - Immutable classes should raise an exception if an attempt is made to modify them. - Ensure performance is optimal even for a large number of book entries. Example Usage: ```python from typing import List from dataclasses import dataclass, field, FrozenInstanceError import dataclasses class OutOfStockError(Exception): pass @dataclass(frozen=True) class Book: title: str author: str published_year: int isbn: str copies_available: int = 1 def borrow_book(self) -> None: if self.copies_available > 0: object.__setattr__(self, \'copies_available\', self.copies_available - 1) else: raise OutOfStockError(f\\"No copies left of book {self.title}\\") @dataclass(frozen=True) class SpecialEditionBook(Book): features: List[str] = field(default_factory=list) signed: bool = False def __repr__(self): return (f\\"SpecialEditionBook(title={self.title}, author={self.author}, \\" f\\"published_year={self.published_year}, isbn={self.isbn}, \\" f\\"copies_available={self.copies_available}, features={self.features}, signed={self.signed})\\") def library_summary(books: List[Book]) -> dict: summary = { \'total_books\': len(books), \'total_copies_available\': sum(book.copies_available for book in books), \'book_type_counts\': {} } for book in books: book_type = type(book).__name__ summary[\'book_type_counts\'][book_type] = summary[\'book_type_counts\'].get(book_type, 0) + 1 return summary # Example books book1 = Book(title=\\"1984\\", author=\\"George Orwell\\", published_year=1949, isbn=\\"1234567890123\\", copies_available=3) book2 = SpecialEditionBook(title=\\"The Hobbit\\", author=\\"J.R.R. Tolkien\\", published_year=1937, isbn=\\"9876543210987\\", features=[\\"Illustrated\\", \\"Gold Leaf\\"], signed=True) # Example usage of the borrow_book method try: book1.borrow_book() print(f\\"Borrowed a copy of {book1.title}, remaining copies: {book1.copies_available}\\") except OutOfStockError as e: print(e) # Example library summary library_books = [book1, book2] summary = library_summary(library_books) print(summary) ``` Write your implementations within the outlined specifications and ensure test cases cover different scenarios.","solution":"from dataclasses import dataclass, field, FrozenInstanceError from typing import List class OutOfStockError(Exception): Exception raised when trying to borrow a book with no available copies. pass @dataclass(frozen=True) class Book: title: str author: str published_year: int isbn: str copies_available: int = 1 def borrow_book(self) -> None: Reduce the number of available copies by 1 if there are any available. if self.copies_available > 0: # Bypass the immutability using object.__setattr__ object.__setattr__(self, \'copies_available\', self.copies_available - 1) else: raise OutOfStockError(f\\"No copies left of book {self.title}\\") @dataclass(frozen=True) class SpecialEditionBook(Book): features: List[str] = field(default_factory=list) signed: bool = False def __repr__(self): return (f\\"SpecialEditionBook(title={self.title}, author={self.author}, \\" f\\"published_year={self.published_year}, isbn={self.isbn}, \\" f\\"copies_available={self.copies_available}, features={self.features}, signed={self.signed})\\") def library_summary(books: List[Book]) -> dict: Generate a summary of the library\'s books. summary = { \'total_books\': len(books), \'total_copies_available\': sum(book.copies_available for book in books), \'book_type_counts\': {} } for book in books: book_type = type(book).__name__ if book_type in summary[\'book_type_counts\']: summary[\'book_type_counts\'][book_type] += 1 else: summary[\'book_type_counts\'][book_type] = 1 return summary"},{"question":"Objective: To test the student\'s understanding of various input handling mechanisms in Python 3.10, including complete programs, file input, interactive input, and expression input using the `eval()` function. Problem Statement: You are to write a function `execute_input(input_data: str, input_type: str) -> Any` that processes and executes Python code based on the specified input type. The function should accept two arguments: - `input_data` (a string): The Python code or expression to be executed. - `input_type` (a string): The type of input provided. It can be one of the following: - `\\"complete_program\\"`: Represents a complete Python program. - `\\"file_input\\"`: Represents Python code as if it\'s coming from a non-interactive file. - `\\"interactive_input\\"`: Represents Python code as if typed interactively in the interpreter. - `\\"expression_input\\"`: Represents an expression to be evaluated using `eval()`. The function should return: - The output of the executed code if `input_type` is `\\"complete_program\\"`, `\\"file_input\\"`, or `\\"interactive_input\\"`. - The result of the evaluated expression if `input_type` is `\\"expression_input\\"`. Constraints: 1. For complete programs, file input, and interactive input types, assume that the code to be executed is syntactically correct and can be executed without causing side effects like file I/O or network operations. 2. For expression input, assume that the expression is a valid Python expression that can be evaluated using `eval()`. Example Usage: ```python def execute_input(input_data: str, input_type: str) -> Any: # Your implementation goes here # Example 1: result = execute_input(\\"a = 5nb = 10nprint(a + b)\\", \\"complete_program\\") # Expected: 15 # Example 2: result = execute_input(\\"for i in range(3):n print(i)\\", \\"file_input\\") # Expected: 0n1n2 # Example 3: result = execute_input(\\"x = 10ny = 20nprint(x * y)\\", \\"interactive_input\\") # Expected: 200 # Example 4: result = execute_input(\\"3 + 7\\", \\"expression_input\\") # Expected: 10 ``` Notes: - You may use `exec()` for executing complete programs, file input, and interactive input. - You may use `eval()` for evaluating expression input. - The function should correctly handle the specified input type and execute the corresponding Python code or expression appropriately. Implement the function `execute_input` that meets the above requirements.","solution":"def execute_input(input_data: str, input_type: str): Executes the given Python code or expression based on the specified input type. :param input_data: The Python code or expression to be executed. :param input_type: The type of input provided. Can be \\"complete_program\\", \\"file_input\\", \\"interactive_input\\", or \\"expression_input\\". :return: The output of the executed code or the result of the evaluated expression. if input_type == \\"complete_program\\" or input_type == \\"file_input\\" or input_type == \\"interactive_input\\": # For these types, we need to capture the printed output import io import sys # Create a new stream to capture the output new_stdout = io.StringIO() old_stdout = sys.stdout sys.stdout = new_stdout # Execute the code exec(input_data) # Restore the original stdout and get the value from the new stream sys.stdout = old_stdout output = new_stdout.getvalue() return output.strip() # Strip any extra whitespace/newlines elif input_type == \\"expression_input\\": # For expression input, we return the evaluated result return eval(input_data) else: raise ValueError(\\"Invalid input type\\")"},{"question":"**Question: Implement Kernel Approximation using Nystroem method** Your task is to implement a class named `CustomNystroem` that replicates the functionality of the `Nystroem` method for kernel approximation as described in the documentation. The `CustomNystroem` class should have at least the following methods: - `__init__(self, kernel=\'rbf\', n_components=100, gamma=None, coef0=1, degree=3, random_state=None)`: Constructor method. - `fit(self, X, y=None)`: Fits the model using the training data. - `transform(self, X)`: Transform the data to the new feature space. - `fit_transform(self, X, y=None)`: Fits the model and transforms the data in one step. # Expected Input and Output: **Input:** - `X`: A 2D list or numpy array of shape (n_samples, n_features) representing the training data. - `y`: The target values (optional). **Output:** - Transformed feature space data of shape (n_samples, n_components). # Constraints: - You should not use the `Nystroem` class from scikit-learn but can refer to numpy or scipy for matrix computations. - The implementation should be efficient and able to handle large datasets. # Performance Requirements: - The complexity of the approximation should be O(n^2_components * n_samples) as described, and it should scale well with increasing components and samples. **Example:** ```python import numpy as np from custom_nystroem import CustomNystroem # Sample Data X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) # Create an instance of CustomNystroem nystroem_transformer = CustomNystroem(kernel=\'rbf\', n_components=2, gamma=0.5, random_state=1) # Fit and transform the data X_transformed = nystroem_transformer.fit_transform(X) print(X_transformed) ``` The output should be a 2D array with transformed features based on the Nystroem approximation.","solution":"import numpy as np from scipy.linalg import svd class CustomNystroem: def __init__(self, kernel=\'rbf\', n_components=100, gamma=None, coef0=1, degree=3, random_state=None): self.kernel = kernel self.n_components = n_components self.gamma = gamma self.coef0 = coef0 self.degree = degree self.random_state = np.random.RandomState(random_state) self.components_ = None self.normalization_ = None def _rbf_kernel(self, X, Y=None): if Y is None: Y = X if self.gamma is None: self.gamma = 1.0 / X.shape[1] dist_sq = -2 * np.dot(X, Y.T) + np.sum(X ** 2, axis=1)[:, np.newaxis] + np.sum(Y ** 2, axis=1) return np.exp(-self.gamma * dist_sq) def _polynomial_kernel(self, X, Y=None): if Y is None: Y = X return (np.dot(X, Y.T) + self.coef0) ** self.degree def _sigmoid_kernel(self, X, Y=None): if Y is None: Y = X return np.tanh(self.gamma * np.dot(X, Y.T) + self.coef0) def _linear_kernel(self, X, Y=None): if Y is None: Y = X return np.dot(X, Y.T) def _get_kernel(self): if self.kernel == \'rbf\': return self._rbf_kernel elif self.kernel == \'polynomial\': return self._polynomial_kernel elif self.kernel == \'sigmoid\': return self._sigmoid_kernel elif self.kernel == \'linear\': return self._linear_kernel else: raise ValueError(f\\"Unknown kernel \'{self.kernel}\'.\\") def fit(self, X, y=None): n_samples, n_features = X.shape if self.n_components > n_samples: self.n_components = n_samples indices = self.random_state.choice(n_samples, self.n_components, replace=False) self.components_ = X[indices] kernel_func = self._get_kernel() K_mm = kernel_func(self.components_) U, S, Vt = svd(K_mm, full_matrices=False) S = np.maximum(S, 1e-12) self.normalization_ = np.dot(U / np.sqrt(S), Vt) return self def transform(self, X): if self.components_ is None: raise ValueError(\\"The CustomNystroem instance is not fitted yet.\\") kernel_func = self._get_kernel() K_nm = kernel_func(X, self.components_) return np.dot(K_nm, self.normalization_.T) def fit_transform(self, X, y=None): return self.fit(X).transform(X)"},{"question":"Coding Assessment Question # Problem Description You are tasked with analyzing a 2D spatial signal using the Fast Fourier Transform (FFT) in PyTorch. Your goal is to implement a function that performs the following steps: 1. Compute the 2D FFT of the input signal. 2. Shift the zero-frequency component to the center of the spectrum using `fftshift`. 3. Apply a Gaussian filter in the frequency domain. 4. Shift the zero-frequency component back to the original position using `ifftshift`. 5. Compute the inverse 2D FFT to obtain the filtered signal. # Function Signature ```python import torch def apply_frequency_filter(signal: torch.Tensor, sigma: float) -> torch.Tensor: Apply a Gaussian filter in the frequency domain to a 2D input signal. Args: - signal (torch.Tensor): A 2D tensor representing the input signal. Shape (H, W). - sigma (float): The standard deviation of the Gaussian filter. Returns: - torch.Tensor: The filtered 2D signal. Shape (H, W). pass ``` # Input - `signal`: A 2D tensor of shape (H, W) representing the input signal. - `sigma`: A positive float representing the standard deviation of the Gaussian filter. # Output - The function should return a 2D tensor of shape (H, W) representing the filtered signal. # Additional Constraints - You must use the `torch.fft` module to perform Fourier transforms. - The input signal tensor will have a shape where both H and W are powers of 2, ensuring optimal performance for FFT operations. - The standard deviation `sigma` will be a positive float less than the smallest dimension of the input signal. # Example ```python import torch # Example signal: a 2D tensor with shape (8, 8) signal = torch.randn(8, 8) # Apply the frequency filter with a given sigma filtered_signal = apply_frequency_filter(signal, sigma=1.0) print(filtered_signal) ``` # Hints 1. Use `torch.fft.fft2` to compute the 2D FFT of the input signal. 2. Use `torch.fft.fftshift` and `torch.fft.ifftshift` for shifting zero-frequency components. 3. To create the Gaussian filter, generate a 2D grid of frequencies using `torch.fft.fftfreq` and compute the Gaussian function. 4. Use `torch.fft.ifft2` to compute the inverse 2D FFT of the filtered frequency domain signal.","solution":"import torch def apply_frequency_filter(signal: torch.Tensor, sigma: float) -> torch.Tensor: Apply a Gaussian filter in the frequency domain to a 2D input signal. Args: - signal (torch.Tensor): A 2D tensor representing the input signal. Shape (H, W). - sigma (float): The standard deviation of the Gaussian filter. Returns: - torch.Tensor: The filtered 2D signal. Shape (H, W). # Compute the 2D FFT of the input signal fft_signal = torch.fft.fft2(signal) # Shift the zero-frequency component to the center shifted_fft_signal = torch.fft.fftshift(fft_signal) # Generate frequency grid H, W = signal.shape u = torch.fft.fftfreq(H)[:, None] v = torch.fft.fftfreq(W)[None, :] # Compute the Gaussian filter in the frequency domain gaussian_filter = torch.exp(-0.5 * (u**2 + v**2) / sigma**2) # Apply the Gaussian filter to the shifted FFT signal filtered_shifted_fft_signal = shifted_fft_signal * gaussian_filter # Shift the zero-frequency component back to the original position filtered_fft_signal = torch.fft.ifftshift(filtered_shifted_fft_signal) # Compute the inverse 2D FFT to get the filtered signal filtered_signal = torch.fft.ifft2(filtered_fft_signal) # Return the real part of the filtered signal return torch.real(filtered_signal)"},{"question":"# Descriptor Implementation and Usage in Python **Problem Statement:** You are required to create a Python class that uses a descriptor to manage the access to a sensitive attribute. This descriptor must: - Log every access and modification to the attribute. - Ensure that the attribute being managed is an integer and is always non-negative. - Please provide validation logic to handle incorrect types and negative values. Implement the following components: 1. **LoggedAttrDescriptor:** - A descriptor class that logs attribute accesses and modifications. - It ensures the attribute is an integer and non-negative. - Raises appropriate exceptions if the constraints are not met. 2. **ProtectedClass:** - A class containing an attribute `protected_value` managed by the LoggedAttrDescriptor. - It should initialize and use the descriptor for the attribute. Input and Output - **No direct inputs/outputs**: the classes themselves are the core of this task, showcasing descriptor functionality and constraints management. Constraints - The managed attribute should always be an integer and non-negative. - Every access and modification should be logged to the console using `logging`. Example ```python if __name__ == \\"__main__\\": import logging logging.basicConfig(level=logging.INFO) obj = ProtectedClass(10) obj.protected_value # Should log access and return 10 obj.protected_value = 20 # Should log the update obj.protected_value = -5 # Should raise ValueError obj.protected_value = \\"not an int\\" # Should raise TypeError ``` Complete below this line <Your Implementation Here>","solution":"import logging logging.basicConfig(level=logging.INFO) class LoggedAttrDescriptor: def __init__(self): self._value = None def __get__(self, obj, objtype=None): logging.info(f\\"Accessing protected value, current value: {self._value}\\") return self._value def __set__(self, obj, value): if not isinstance(value, int): raise TypeError(\\"Value must be an integer\\") if value < 0: raise ValueError(\\"Value must be non-negative\\") logging.info(f\\"Updating protected value from {self._value} to {value}\\") self._value = value class ProtectedClass: protected_value = LoggedAttrDescriptor() def __init__(self, initial_value): self.protected_value = initial_value"},{"question":"<|Analysis Begin|> The provided documentation details the functionalities and configuration options available in the `torch.utils.data` module, specifically focusing on the `DataLoader` class and the different types of datasets (`Dataset` and `IterableDataset`). The `DataLoader` class is central to PyTorch\'s data loading process, supporting various features such as map-style and iterable-style datasets, custom sampling, automatic batching, multi-process data loading, and memory pinning. Key points from the documentation: 1. **Dataset Types**: - Map-style datasets implement `__getitem__` and `__len__`. - Iterable-style datasets implement `__iter__`. 2. **Data Loading Order and `Sampler`**: - Custom samplers can control the sequence of indices/keys. - `Sampler` and `batch_sampler` are used for map-style datasets. 3. **Automatic Batching**: - Batching can be enabled/disabled. - The `collate_fn` parameter can customize how samples are collated into batches. 4. **Single- and Multi-Process Data Loading**: - Default is single-process. - Multi-process is enabled by setting `num_workers` > 0. 5. **Memory Pinning**: - `pin_memory=True` can speed up host to GPU copies. Based on this, a challenging question can involve creating a custom dataset and a custom `collate_fn`, using both map-style and iterable-style datasets, leveraging multi-process data loading, and enabling memory pinning. <|Analysis End|> <|Question Begin|> # Advanced Data Loading with PyTorch You are tasked with developing a custom data loading solution that demonstrates your understanding of PyTorch\'s `torch.utils.data` module. Your solution must meet the following requirements: 1. **Custom Dataset**: - Create a custom map-style dataset that reads data from a list of file paths and their corresponding labels. Implement the `__getitem__` and `__len__` methods. - Create a custom iterable-style dataset that generates data samples on-the-fly. Implement the `__iter__` method. 2. **Custom DataLoader**: - Use PyTorch\'s `DataLoader` to load data from both custom datasets. - Enable multi-process data loading with a configurable number of worker processes. - Use memory pinning to optimize data transfer to CUDA-enabled GPUs. 3. **Custom `collate_fn`**: - Implement a custom `collate_fn` that pads sequences to the maximum length within a batch and converts NumPy arrays to PyTorch tensors. # Detailed Requirements 1. Custom Map-Style Dataset - Class name: `CustomMapDataset` - Methods to implement: - `__init__(self, file_paths, labels)`: Initialize with a list of file paths and their corresponding labels. - `__getitem__(self, idx)`: Return the sample (data, label) at index `idx`. - `__len__(self)`: Return the length of the dataset. 2. Custom Iterable-Style Dataset - Class name: `CustomIterableDataset` - Methods to implement: - `__init__(self, data_generator)`: Initialize with a data generator (an iterable). - `__iter__(self)`: Yield data samples from the generator. 3. Custom DataLoader - Create `DataLoader` instances for both datasets with the following configurations: - Use batch size of 4. - Enable shuffling for the map-style dataset. - Set `num_workers` to 2. - Enable `pin_memory`. 4. Custom `collate_fn` - Function name: `custom_collate_fn` - Parameters: `batch` - Functionality: - Pad sequences to the maximum length within a batch. - Convert NumPy arrays to PyTorch tensors. # Input and Output Formats Sample Input - A list of file paths and labels for `CustomMapDataset`. - A data generator function for `CustomIterableDataset`. Sample Output - Batches of padded sequences and labels as PyTorch tensors. # Constraints - Ensure no duplicate data is generated in multi-process mode for the iterable-style dataset. - The batch dimension should be the first dimension in the tensors. - The collate function must handle variable-length sequences correctly. # Performance Requirements - The solution should efficiently load and batch data utilizing multiple processes. - Data transfer to CUDA-enabled GPUs should be optimized by using pinned memory. # Example Code Here\'s a starting template for your solution: ```python import torch from torch.utils.data import DataLoader, Dataset, IterableDataset import numpy as np class CustomMapDataset(Dataset): def __init__(self, file_paths, labels): self.file_paths = file_paths self.labels = labels def __getitem__(self, idx): data = np.load(self.file_paths[idx]) label = self.labels[idx] return data, label def __len__(self): return len(self.file_paths) class CustomIterableDataset(IterableDataset): def __init__(self, data_generator): self.data_generator = data_generator def __iter__(self): return iter(self.data_generator) def custom_collate_fn(batch): data = [item[0] for item in batch] labels = [item[1] for item in batch] data = torch.nn.utils.rnn.pad_sequence([torch.tensor(d) for d in data], batch_first=True) labels = torch.tensor(labels) return data, labels # Example usage file_paths = [\'data1.npy\', \'data2.npy\', \'data3.npy\', \'data4.npy\'] labels = [0, 1, 0, 1] map_dataset = CustomMapDataset(file_paths, labels) map_loader = DataLoader(map_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn) def data_gen(): for _ in range(100): yield np.random.rand(10), np.random.randint(0, 2) iter_dataset = CustomIterableDataset(data_gen) iter_loader = DataLoader(iter_dataset, batch_size=4, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn) for data, labels in map_loader: print(data, labels) for data, labels in iter_loader: print(data, labels) ```","solution":"import torch from torch.utils.data import DataLoader, Dataset, IterableDataset import numpy as np class CustomMapDataset(Dataset): def __init__(self, file_paths, labels): self.file_paths = file_paths self.labels = labels def __getitem__(self, idx): data = np.load(self.file_paths[idx]) label = self.labels[idx] return data, label def __len__(self): return len(self.file_paths) class CustomIterableDataset(IterableDataset): def __init__(self, data_generator): self.data_generator = data_generator def __iter__(self): return iter(self.data_generator) def custom_collate_fn(batch): data = [item[0] for item in batch] labels = [item[1] for item in batch] data = torch.nn.utils.rnn.pad_sequence([torch.tensor(d) for d in data], batch_first=True) labels = torch.tensor(labels) return data, labels # Example usage file_paths = [\'data1.npy\', \'data2.npy\', \'data3.npy\', \'data4.npy\'] labels = [0, 1, 0, 1] map_dataset = CustomMapDataset(file_paths, labels) map_loader = DataLoader(map_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn) def data_gen(): for _ in range(100): yield np.random.rand(10), np.random.randint(0, 2) iter_dataset = CustomIterableDataset(data_gen) iter_loader = DataLoader(iter_dataset, batch_size=4, num_workers=2, pin_memory=True, collate_fn=custom_collate_fn)"},{"question":"# Asynchronous Iterator Challenge Objective Implement a function `async_even_generator()` that provides an asynchronous iterator. The iterator should generate even numbers up to a given limit asynchronously. Additionally, implement a synchronous function `collect_even_numbers(n)` that utilizes this asynchronous iterator to collect all even numbers up to the specified limit into a list. Details **Function 1: `async_even_generator(limit)`** This is an asynchronous generator function that: - Takes an integer `limit` as an argument. - Yields even numbers starting from 0 up to (and including) the specified `limit` asynchronously. **Function 2: `collect_even_numbers(n)`** This function: - Takes an integer `n` as an argument. - Uses an event loop to run the asynchronous generator `async_even_generator`. - Collects and returns the list of even numbers up to `n` generated by `async_even_generator`. Constraints - You must use the `anext()`, `aiter()`, or similar async functions appropriately in your implementation. - The value of `n` for `collect_even_numbers` will be a positive integer and within the range of [2, 10000]. Example ```python import asyncio async def async_even_generator(limit): # Your implementation here def collect_even_numbers(n): # Your implementation here # Example Usage # This should output: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] print(collect_even_numbers(18)) ``` Notes - Ensure that the `collect_even_numbers` function can handle the asynchronous nature properly and can correctly collect the output from the `async_even_generator`. - Handle edge cases such as when `n` is 0 or 1 by returning an empty list as there are no even numbers in this range. Submission Submit your implementation of both `async_even_generator(limit)` and `collect_even_numbers(n)` in a Python script.","solution":"import asyncio async def async_even_generator(limit): Asynchronous generator function that yields even numbers up to the specified limit. for number in range(0, limit + 1, 2): await asyncio.sleep(0) # Simulate asynchronous behavior yield number def collect_even_numbers(n): Synchronous function that collects all even numbers up to n using the async_even_generator. async def collect(): even_numbers = [] async for number in async_even_generator(n): even_numbers.append(number) return even_numbers return asyncio.run(collect())"},{"question":"**Question: Constructing and Manipulating a MIME Email Message** You are tasked with writing a Python function to create a MIME email message which includes text, an image, and an audio file as attachments. Your function should leverage the `email.mime` module and its various classes to construct a well-structured MIME email. The email should have a text body with a plain text attachment, and also include an image and an audio file as attachments. # Function Signature ```python def create_mime_email(text_content: str, text_attachment: str, image_data: bytes, audio_data: bytes) -> email.mime.multipart.MIMEMultipart: ``` # Parameters - **text_content (str)**: The main text content of the email. - **text_attachment (str)**: The text content to be attached as a text file. - **image_data (bytes)**: The binary data of the image to be attached. - **audio_data (bytes)**: The binary data of the audio to be attached. # Returns - **email.mime.multipart.MIMEMultipart**: The constructed MIME email message object. # Requirements 1. Create a `MIMEMultipart` message object as the root message. 2. Add `text_content` as the main body of the email using the `MIMEText` class. 3. Attach `text_attachment` as a plain text file named \\"attachment.txt\\" using the `MIMEText` class. 4. Attach the `image_data` as an image, guessing the MIME subtype automatically, using the `MIMEImage` class. 5. Attach the `audio_data` as an audio file, guessing the MIME subtype automatically, using the `MIMEAudio` class. 6. Ensure all attachments and the main message part are added correctly to the `MIMEMultipart` object. # Example ```python from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio def create_mime_email(text_content: str, text_attachment: str, image_data: bytes, audio_data: bytes) -> MIMEMultipart: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Add the main text content main_text = MIMEText(text_content, \'plain\') msg.attach(main_text) # Add text attachment attachment = MIMEText(text_attachment, \'plain\') attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\\"attachment.txt\\") msg.attach(attachment) # Add image attachment img = MIMEImage(image_data) msg.attach(img) # Add audio attachment audio = MIMEAudio(audio_data) msg.attach(audio) return msg # Example usage: # create_mime_email(\\"Hello, this is the email body\\", \\"This is the content of the text attachment\\", image_data, audio_data) ``` Notes - You may assume `image_data` and `audio_data` are provided in the correct binary format. - The function should not send the email, only construct and return the MIME message object.","solution":"from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.audio import MIMEAudio def create_mime_email(text_content: str, text_attachment: str, image_data: bytes, audio_data: bytes) -> MIMEMultipart: # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = \'Test Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'receiver@example.com\' # Add the main text content main_text = MIMEText(text_content, \'plain\') msg.attach(main_text) # Add text attachment attachment = MIMEText(text_attachment, \'plain\') attachment.add_header(\'Content-Disposition\', \'attachment\', filename=\\"attachment.txt\\") msg.attach(attachment) # Add image attachment img = MIMEImage(image_data) msg.attach(img) # Add audio attachment audio = MIMEAudio(audio_data) msg.attach(audio) return msg"},{"question":"**Abstract Base Classes and Custom Iterables** **Objective:** Design an abstract base class using the `abc` module and demonstrate its usage by creating concrete subclasses that fulfill the specified conditions. This will assess your understanding of abstract base classes, abstract methods, and their usage in defining interfaces for class hierarchies. **Problem Statement:** You are required to implement an abstract base class `BaseStack` using the `abc` module. This class should define a stack-like interface. Then, implement two concrete classes `ListStack` and `TupleStack` that inherit from `BaseStack`. Ensure that these concrete classes adhere to the interface defined by `BaseStack`. 1. **Abstract Class: `BaseStack`** - This abstract base class should inherit from `abc.ABC`. - Include the following abstract methods: - `push(self, item)`: To add an item to the stack. - `pop(self)`: To remove and return the top item from the stack. - `is_empty(self)`: To check if the stack is empty. - `__len__(self)`: To return the number of items in the stack. - Include a concrete method: - `clear(self)`: To clear all items from the stack. 2. **Concrete Class: `ListStack`** - This class should inherit from `BaseStack`. - Implement the stack using a Python list. - Implement all abstract methods defined in `BaseStack`. 3. **Concrete Class: `TupleStack`** - This class should inherit from `BaseStack`. - Implement the stack using a Python tuple (hint: tuples are immutable, so consider how you might manage stack operations efficiently). - Implement all abstract methods defined in `BaseStack`. **Expected Input and Output:** Your implementation should be able to handle the following: - Create instances of `ListStack` and `TupleStack`. - Use the `push`, `pop`, `is_empty`, and `__len__` methods. - Clear the stack using the `clear` method. **Constraints:** - Ensure type consistency within your stack implementations. - Do not use any external libraries for stack operations. **Performance Requirements:** - Your stack operations should have an expected time complexity of O(1) for `push`, `pop`, and `is_empty`. However, ensure `TupleStack` is designed efficiently given tuple immutability. **Example Usage:** ```python # Define the abstract base class from abc import ABC, abstractmethod class BaseStack(ABC): @abstractmethod def push(self, item): pass @abstractmethod def pop(self): pass @abstractmethod def is_empty(self): pass @abstractmethod def __len__(self): pass def clear(self): while not self.is_empty(): self.pop() # Define concrete ListStack class class ListStack(BaseStack): def __init__(self): self._stack = [] def push(self, item): self._stack.append(item) def pop(self): if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self._stack.pop() def is_empty(self): return len(self._stack) == 0 def __len__(self): return len(self._stack) # Define concrete TupleStack class class TupleStack(BaseStack): def __init__(self): self._stack = () def push(self, item): self._stack = self._stack + (item,) def pop(self): if self.is_empty(): raise IndexError(\\"pop from empty stack\\") item = self._stack[-1] self._stack = self._stack[:-1] return item def is_empty(self): return len(self._stack) == 0 def __len__(self): return len(self._stack) # Test the classes list_stack = ListStack() tuple_stack = TupleStack() list_stack.push(1) list_stack.push(2) assert len(list_stack) == 2 tuple_stack.push(1) tuple_stack.push(2) assert len(tuple_stack) == 2 assert list_stack.pop() == 2 assert tuple_stack.pop() == 2 assert list_stack.is_empty() is False assert tuple_stack.is_empty() is False list_stack.clear() tuple_stack.clear() assert list_stack.is_empty() is True assert tuple_stack.is_empty() is True ``` Ensure that your implementation correctly passes the above test cases.","solution":"from abc import ABC, abstractmethod class BaseStack(ABC): @abstractmethod def push(self, item): pass @abstractmethod def pop(self): pass @abstractmethod def is_empty(self): pass @abstractmethod def __len__(self): pass def clear(self): while not self.is_empty(): self.pop() class ListStack(BaseStack): def __init__(self): self._stack = [] def push(self, item): self._stack.append(item) def pop(self): if self.is_empty(): raise IndexError(\\"pop from empty stack\\") return self._stack.pop() def is_empty(self): return len(self._stack) == 0 def __len__(self): return len(self._stack) class TupleStack(BaseStack): def __init__(self): self._stack = () def push(self, item): self._stack = self._stack + (item,) def pop(self): if self.is_empty(): raise IndexError(\\"pop from empty stack\\") item = self._stack[-1] self._stack = self._stack[:-1] return item def is_empty(self): return len(self._stack) == 0 def __len__(self): return len(self._stack)"},{"question":"**Python Coding Assessment Question: Advanced Dictionary Operations** In this task, you will implement a class `AdvancedDict` that mimics some behavior and utility functions of Python dictionaries. Your implementation will include methods to add, get, and delete items; check for item existence; and merge another dictionary into it. To demonstrate your understanding, you will also implement custom iteration over the dictionary items. # Problem Statement: Implement a class `AdvancedDict` in Python with the following methods: 1. **`__init__(self)`**: Initializes an empty dictionary. 2. **`add_item(self, key, value)`**: Adds the key-value pair to the dictionary. 3. **`get_item(self, key)`**: Returns the value associated with the key. If the key does not exist, return `None`. 4. **`delete_item(self, key)`**: Deletes the key-value pair from the dictionary if the key exists. 5. **`contains(self, key)`**: Returns `True` if the dictionary contains the key, `False` otherwise. 6. **`merge(self, other_dict, override=True)`**: Merges `other_dict` into the current dictionary. If `override` is `True`, existing keys will be overwritten by the values from `other_dict`. If `override` is `False`, existing keys will not be overridden. 7. **`as_list(self)`**: Returns a list of tuples representing the dictionary\'s items (key-value pairs). 8. **`__iter__(self)`**: Custom iterator method that yields dictionary items one by one. # Input: - There is no direct input to the class. Use the provided methods to interact with the class. # Output: - The output will be based on the returned values from the methods implemented. # Constraints: - Keys in the dictionary are hashable. - You must not use the built-in Python dictionary methods directly (e.g., `dict.add()`) but instead implement the logic yourself using core data structures. # Example Usage: ```python adv_dict = AdvancedDict() adv_dict.add_item(\'a\', 1) adv_dict.add_item(\'b\', 2) adv_dict.add_item(\'c\', 3) print(adv_dict.get_item(\'b\')) # Output: 2 adv_dict.delete_item(\'b\') print(adv_dict.contains(\'b\')) # Output: False other_dict = {\'d\': 4, \'e\': 5} adv_dict.merge(other_dict, override=False) print(adv_dict.as_list()) # Output: [(\'a\', 1), (\'c\', 3), (\'d\', 4), (\'e\', 5)] for item in adv_dict: print(item) # Output: (\'a\', 1) (\'c\', 3) (\'d\', 4) (\'e\', 5) ``` Your task is to implement the `AdvancedDict` class with the specified methods and functionality.","solution":"class AdvancedDict: def __init__(self): self._data = {} def add_item(self, key, value): self._data[key] = value def get_item(self, key): return self._data.get(key, None) def delete_item(self, key): if key in self._data: del self._data[key] def contains(self, key): return key in self._data def merge(self, other_dict, override=True): for key, value in other_dict.items(): if override or key not in self._data: self._data[key] = value def as_list(self): return list(self._data.items()) def __iter__(self): for key, value in self._data.items(): yield (key, value)"},{"question":"<|Analysis Begin|> The \\"runpy\\" module is designed to locate and run Python modules without importing them first. It provides functionalities to implement the \\"-m\\" command line switch, which allows scripts to be located using the Python module namespace rather than the filesystem. The module consists of two primary functions: 1. `runpy.run_module(mod_name, init_globals=None, run_name=None, alter_sys=False)`: This function executes the code of a specified module and returns the resulting module globals dictionary. It uses an absolute module name and provides customization options like initial global variables, run name, and system alterations. 2. `runpy.run_path(path_name, init_globals=None, run_name=None)`: This function executes code located at a specific filesystem path and returns the module globals dictionary. It handles various path inputs like Python source files, compiled bytecode files, or valid sys.path entries. Both functions provide flexibility in handling module execution and manipulating global variables and system arguments, though they come with certain limitations and precautions, such as thread-safety concerns. To design an assessment question, we should focus on these core functionalities, requiring students to demonstrate their understanding and ability to use these functions effectively. The question could involve implementing a script that makes use of these functions to dynamically execute specified modules or scripts, prepopulate global variables, and appropriately handle system state changes. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective You are tasked with implementing a utility script that dynamically executes Python modules or scripts based on given inputs. The script will utilize the functionalities provided by the `runpy` module to achieve this. This exercise will test your understanding of module execution, global variable management, and handling system state changes. Instructions 1. Implement the function `execute_module_or_script(source: str, globals_dict: dict) -> dict`: - The `source` parameter is a string that can either be a module name (e.g., \\"module.submodule\\") or a filesystem path to a Python script. - The `globals_dict` parameter is a dictionary used to prepopulate the module\'s globals before execution. - The function should determine if the `source` is a module name or a filesystem path. - If `source` is a module name, use `runpy.run_module` to execute it. - If `source` is a filesystem path, use `runpy.run_path` to execute it. - Ensure that the returned result is the module\'s globals dictionary resulting from the execution. 2. Handle any exceptions that may arise during execution, providing meaningful error messages. 3. Write a test case to demonstrate the usage of your function. Requirements - Use the `runpy` module functions appropriately based on the type of `source` provided. - Prepopulate the module\'s globals dictionary with the provided `globals_dict` without modifying the original input dictionary. - The function should return the resulting module\'s globals dictionary after execution. - Ensure thread safety by avoiding any unsafe manipulations of `sys`. Example Usage ```python def execute_module_or_script(source: str, globals_dict: dict) -> dict: # Your implementation here # Test case if __name__ == \\"__main__\\": globals_dict = {\\"example_variable\\": \\"hello world!\\"} result = execute_module_or_script(\\"example_module\\", globals_dict) print(result) # Should include the globals of the executed \'example_module\' ``` Notes - You can distinguish between a module name and a filesystem path by checking if `source` ends with \\".py\\". - Make sure to handle cases where the module or script specified in the `source` does not exist or cannot be executed. - Your function should not alter the input `globals_dict`. Good luck, and may your code run smoothly!","solution":"import runpy import os def execute_module_or_script(source: str, globals_dict: dict) -> dict: Executes a module or script and returns the module globals dictionary. Parameters: source (str): Module name or filesystem path to a Python script. globals_dict (dict): Dictionary to prepopulate the module\'s globals. Returns: dict: The globals dictionary resulting from the execution. if not isinstance(globals_dict, dict): raise TypeError(\\"globals_dict must be a dictionary\\") # Make a copy of the globals_dict to avoid altering the original init_globals = globals_dict.copy() try: if source.endswith(\'.py\'): if not os.path.isfile(source): raise ValueError(f\\"The script \'{source}\' does not exist.\\") result_globals = runpy.run_path(source, init_globals=init_globals) else: result_globals = runpy.run_module(source, init_globals=init_globals) except Exception as e: raise RuntimeError(f\\"Failed to execute \'{source}\': {e}\\") return result_globals"},{"question":"# Task You are required to demonstrate your understanding of Seaborn\'s `jointplot` functionality by visualizing a given dataset with various customizations. Follow the instructions below to implement the solution. # Instructions 1. Load the \\"penguins\\" dataset from Seaborn. 2. Create a scatter plot with marginal histograms of \\"bill_length_mm\\" vs. \\"bill_depth_mm\\" for the penguins dataset. 3. Color code the scatter plot by the \\"species\\" column. 4. Use the kernel density estimate (KDE) plot for both the scatter plot and the marginal plots. 5. Customize the joint plot: - Set the size of the plot to a height of 8. - Set the aspect ratio of the marginal plots to 3. - Display ticks on the marginal plots. 6. Add a red KDE layer and rug plot to the joint plot: - The KDE layer should have 6 levels. - The rug plot should be added to the marginals, with height proportional to -0.15 and `clip_on=False`. 7. Return the final plot. # Expected Output A visual plot showcasing the specified customizations, with \\"bill_length_mm\\" on the x-axis and \\"bill_depth_mm\\" on the y-axis, color-coded by \\"species\\" and including the customized KDE and rug plots. # Constraints - Use only Seaborn for visualization. - Follow the given instructions strictly without omitting any step. - Document your code with appropriate comments. # Performance Requirements - The code should execute efficiently and render the plot without delays. Example Code Implementation ```python import seaborn as sns def custom_jointplot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the initial joint plot with KDE and color coding by species g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", height=8, ratio=3, marginal_ticks=True) # Add an additional red KDE layer and a red rug plot to the marginals g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) return g # Generate the plot custom_jointplot() ```","solution":"import seaborn as sns def custom_jointplot(): Creates a scatter plot with marginal histograms of \'bill_length_mm\' vs. \'bill_depth_mm\' for the penguins dataset, color-coded by \'species\', and including customized KDE and rug plots. # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create the initial joint plot with KDE and color coding by species g = sns.jointplot(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"scatter\\", height=8, ratio=3, marginal_ticks=True) # Add an additional red KDE layer g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) # Add a red rug plot to the marginals g.plot_marginals(sns.rugplot, color=\\"r\\", height=-.15, clip_on=False) return g # Generate the plot # This is a visualization function and is intended to be run in an environment that can display plots. # custom_jointplot()"},{"question":"# Custom Asynchronous HTTP Server In this task, you will implement a custom asynchronous HTTP server using the `asyncio` library\'s low-level transports and protocols. Your server will accept HTTP GET requests and respond with a simple HTML page. # Requirements: 1. **Custom HTTP Protocol**: - Implement a protocol class `HTTPProtocol` that inherits from `asyncio.Protocol`. - Handle incoming data to parse HTTP GET requests. - Respond with a simple HTML page content for valid GET requests. - Use proper HTTP headers to indicate the content type and length. 2. **Server Setup**: - Use the `loop.create_server()` method to create a server. - The server should bind to `127.0.0.1` and port `8080`. # Custom HTTP Protocol Specification: - **GET Request Handling** - Parse the incoming HTTP GET request. - If the request is valid and targets the URL `/`, respond with the following HTML content: ```html <!DOCTYPE html> <html> <head> <title>Sample Page</title> </head> <body> <h1>Hello, World!</h1> </body> </html> ``` - Respond with a `400 Bad Request` status for invalid requests or unsupported methods. - **HTTP Response Format**: - Use the HTTP/1.1 version. - Include the `Content-Type` header set to `text/html`. - Include the `Content-Length` header with the size of the response body. # Implementation Details: 1. Define the `HTTPProtocol` class with appropriate methods: - `connection_made(transport)`: Save the transport instance. - `data_received(data)`: Parse the data and construct responses based on the request type. - `connection_lost(exc)`: Log the connection loss. 2. Setup and start the server using asyncio event loop in an `async main()` function. 3. Run the `main()` function. # Example Code Structure: ```python import asyncio class HTTPProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): request = data.decode() response = self.handle_request(request) self.transport.write(response.encode()) self.transport.close() def handle_request(self, request): # Your implementation here to parse request and return appropriate response pass def connection_lost(self, exc): print(\'Connection lost\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: HTTPProtocol(), \'127.0.0.1\', 8080 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Constraints: - You may use only the standard `asyncio` library (no external HTTP libraries). - Ensure proper handling of different client connections. - The server should close the client connection after the response is sent. # Expected Outcome: - The server correctly handles HTTP GET requests to `/` and serves the predefined HTML page. - The server handles invalid requests gracefully with appropriate error responses. - Efficient and proper use of transports and protocols demonstrates understanding of the `asyncio` library\'s low-level APIs.","solution":"import asyncio class HTTPProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport def data_received(self, data): request = data.decode() response = self.handle_request(request) self.transport.write(response.encode()) self.transport.close() def handle_request(self, request): lines = request.split(\'rn\') if len(lines) > 0: request_line = lines[0].split() if len(request_line) >= 2 and request_line[0] == \'GET\' and request_line[1] == \'/\': html_content = <!DOCTYPE html> <html> <head> <title>Sample Page</title> </head> <body> <h1>Hello, World!</h1> </body> </html> response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/htmlrn\\" f\\"Content-Length: {len(html_content.encode())}rn\\" \\"rn\\" + html_content ) return response return \\"HTTP/1.1 400 Bad RequestrnContent-Length: 0rnrn\\" def connection_lost(self, exc): print(\'Connection lost\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: HTTPProtocol(), \'127.0.0.1\', 8080 ) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Objective Demonstrate your understanding of the `xml.sax` package by creating a SAX parser to process a given XML string. Implement handlers to perform specific tasks based on XML content and manage parsing errors appropriately using exception handling. Instructions 1. **Create a SAX parser** using the `xml.sax` package. 2. **Define a ContentHandler** that: - Extracts and prints the name and text content of each XML element. - Counts and prints the number of elements processed. 3. **Define an ErrorHandler** that: - Logs any parsing errors encountered. 4. **Use the SAX parser** to parse the following XML string: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> </book> <book> <title>Learning XML</title> <author>Jane Smith</author> </book> </library> ``` 5. **Handle exceptions** using the `SAXException` class and its subclasses. Requirements - **Input**: A string containing the XML data. - **Output**: Printed XML element names, their text content, the total number of elements, and any error messages. - **Constraints**: Ensure that the solution gracefully handles invalid XML data and logs appropriate error messages. Example Output ``` Element: library Start of element: book Element: title Text: Python Programming Element: author Text: John Doe End of element: book Start of element: book Element: title Text: Learning XML Element: author Text: Jane Smith End of element: book Total elements processed: 8 SAXParseException: <Descriptive error message if any> ``` Hints - Use `xml.sax.make_parser()` to create a parser instance. - Implement `xml.sax.handler.ContentHandler` and override methods like `startElement`, `endElement`, and `characters`. - Implement `xml.sax.handler.ErrorHandler` and override methods like `error`, `fatalError`, and `warning`. - Use `xml.sax.parseString()` to parse the XML string.","solution":"import xml.sax class SimpleContentHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.elements_count = 0 def startElement(self, name, attrs): self.current_data = name self.elements_count += 1 print(f\\"Start of element: {name}\\") def endElement(self, name): print(f\\"End of element: {name}\\") self.current_data = \\"\\" def characters(self, content): if self.current_data and content.strip(): print(f\\"Element: {self.current_data}\\") print(f\\"Text: {content.strip()}\\") def endDocument(self): print(f\\"Total elements processed: {self.elements_count}\\") class SimpleErrorHandler(xml.sax.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_xml_string(xml_string): handler = SimpleContentHandler() error_handler = SimpleErrorHandler() parser = xml.sax.make_parser() parser.setContentHandler(handler) parser.setErrorHandler(error_handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXException as e: print(f\\"SAXParseException: {e}\\") # Example XML input xml_data = <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> </book> <book> <title>Learning XML</title> <author>Jane Smith</author> </book> </library> parse_xml_string(xml_data)"},{"question":"# Objective: Using the seaborn library, create a Python script to perform the following tasks: # Tasks: 1. Load the built-in \'tips\' dataset from seaborn. 2. Create a scatter plot to visualize the relationship between the total bill and tip amount. 3. Create a residual plot to check the residuals for a linear regression model between total bill and tip amount. 4. Create a second residual plot to check the residuals for a quadratic model (order=2) between the total bill and tip amount. 5. Add a LOWESS smoother to the residual plots to highlight the structure, and use red color for the LOWESS line. # Requirements: 1. Your script should import seaborn and set the default theme. 2. Ensure that your plots have appropriate titles and axis labels to clearly convey the information. # Input format: There is no input required from the user; the script should directly load the dataset and generate the specified plots. # Output format: The script should output three plots: 1. A scatter plot showing the relationship between total bill and tip amount. 2. A residual plot for the linear regression model. 3. A residual plot for the quadratic regression model with a LOWESS smoother in red. # Example Usage: ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create a scatter plot sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # Step 3: Create a residual plot for a linear model sns.residplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Residual plot of Linear Model (Total Bill vs Tip)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 4: Create a residual plot for a quadratic model sns.residplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", order=2, lowess=True, line_kws={\\"color\\": \\"red\\"}) plt.title(\\"Residual plot of Quadratic Model with LOWESS (Total Bill vs Tip)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Residuals\\") plt.show() ``` Your task is to implement this script and ensure all the steps are completed as described.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the default theme sns.set_theme() # Step 2: Create a scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Scatter plot of Total Bill vs Tip\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.show() # Step 3: Create a residual plot for a linear model plt.figure(figsize=(10, 6)) sns.residplot(data=tips, x=\\"total_bill\\", y=\\"tip\\") plt.title(\\"Residual plot of Linear Model (Total Bill vs Tip)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Residuals\\") plt.show() # Step 4: Create a residual plot for a quadratic model with LOWESS plt.figure(figsize=(10, 6)) sns.residplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", order=2, lowess=True, line_kws={\\"color\\": \\"red\\"}) plt.title(\\"Residual plot of Quadratic Model with LOWESS (Total Bill vs Tip)\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"# PyTorch Compatibility Feature Implementation You are given the task of implementing a utility function that ensures certain properties of module parameter conversions in a neural network training context using PyTorch. Specifically, you need to control whether module parameters should be overwritten or swapped during neural network conversions. Your function should take a configuration dictionary as its input and set the appropriate behaviors using functions from `torch.__future__`. Function Signature: ```python def configure_conversion_behavior(config: dict) -> dict: Configures the behavior of module parameter conversions based on the given configuration. Args: config (dict): A dictionary containing flags for \'overwrite\' and \'swap\'. Returns: dict: A dictionary containing the current states of \'overwrite\' and \'swap\' after configuration. pass ``` Input: - `config`: A dictionary with two keys: - `\'overwrite\'`: A boolean value indicating whether to overwrite module parameters during conversion. - `\'swap\'`: A boolean value indicating whether to swap module parameters during conversion. Output: - A dictionary containing the current states of the configurations: - `\'overwrite\'`: A boolean value representing the current state of module parameter overwriting. - `\'swap\'`: A boolean value representing the current state of module parameter swapping. Constraints: - You must use `torch.__future__` functions to set and get the module parameter conversion behaviors. - Assume that `torch.__future__` module is already imported and available in the scope. Example: ```python config = {\'overwrite\': True, \'swap\': False} current_states = configure_conversion_behavior(config) assert current_states[\'overwrite\'] == True assert current_states[\'swap\'] == False ``` Implement the `configure_conversion_behavior` function to fulfill the requirements described above.","solution":"import torch def configure_conversion_behavior(config: dict) -> dict: Configures the behavior of module parameter conversions based on the given configuration. Args: config (dict): A dictionary containing flags for \'overwrite\' and \'swap\'. Returns: dict: A dictionary containing the current states of \'overwrite\' and \'swap\' after configuration. if \'overwrite\' in config: torch.__future__.set_overwrite_module_params_on_conversion(config[\'overwrite\']) if \'swap\' in config: torch.__future__.set_swap_module_params_on_conversion(config[\'swap\']) current_states = { \'overwrite\': torch.__future__.get_overwrite_module_params_on_conversion(), \'swap\': torch.__future__.get_swap_module_params_on_conversion() } return current_states"},{"question":"**Memory Management and Object-Oriented Programming in Python** In this exercise, you will demonstrate your understanding of object-oriented programming, memory management, and efficient data handling in Python. You will design a custom class in Python that simulates a simple memory management system. Problem Statement: You are required to implement a class called `MemoryManager` which will manage a fixed-size heap memory allocation for objects. Each object has a unique identifier, a specific size, and can be deleted to free space. The class should be able to handle the following operations: 1. **Initialize the memory manager** with a given total memory size. 2. **Allocate memory** for a new object with a unique identifier and a given size. 3. **Free memory** from a previously allocated object using its unique identifier. 4. **Check the current state** of allocated memory and free memory. Requirements and Constraints: 1. **Initialization**: - `__init__(self, total_size: int)`: - `total_size`: Integer representing the total size of the memory in bytes. 2. **Allocate Method**: - `allocate(self, object_id: str, size: int) -> bool`: - `object_id`: String representing the unique identifier of the object. - `size`: Integer representing the size of the object in bytes. - Returns `True` if the allocation is successful, `False` if there is not enough free memory. 3. **Free Method**: - `free(self, object_id: str) -> bool`: - `object_id`: String representing the unique identifier of the object to free. - Returns `True` if the object is successfully freed, `False` if the object id is not found. 4. **State Check Method**: - `state(self) -> dict`: - Returns a dictionary with two keys: `allocated` and `free`, representing the currently allocated memory size and the free memory size, respectively. 5. **Constraints**: - Memory size values are in bytes and will be non-negative integers. - Object identifiers are unique strings. - You need to ensure that there are no memory leaks or invalid memory accesses. Example Usage: ```python memory_manager = MemoryManager(1000) # Initialize memory manager with 1000 bytes assert memory_manager.allocate(\\"obj1\\", 200) == True assert memory_manager.allocate(\\"obj2\\", 500) == True assert memory_manager.allocate(\\"obj3\\", 400) == False # Not enough memory assert memory_manager.state() == {\\"allocated\\": 700, \\"free\\": 300} assert memory_manager.free(\\"obj1\\") == True assert memory_manager.state() == {\\"allocated\\": 500, \\"free\\": 500} assert memory_manager.allocate(\\"obj3\\", 400) == True assert memory_manager.state() == {\\"allocated\\": 900, \\"free\\": 100} ``` Your task is to implement the `MemoryManager` class in Python which satisfies the above requirements. Ensure your implementation is efficient and handles edge cases gracefully.","solution":"class MemoryManager: def __init__(self, total_size): self.total_size = total_size self.allocated_memory = {} self.used_memory = 0 def allocate(self, object_id, size): if object_id in self.allocated_memory: return False if self.used_memory + size > self.total_size: return False self.allocated_memory[object_id] = size self.used_memory += size return True def free(self, object_id): if object_id not in self.allocated_memory: return False size = self.allocated_memory.pop(object_id) self.used_memory -= size return True def state(self): return { \\"allocated\\": self.used_memory, \\"free\\": self.total_size - self.used_memory }"},{"question":"**Title**: Managing and Utilizing `__annotations__` in Python 3.10 **Objective**: Design and implement a function that safely accesses and processes the `__annotations__` attribute of a given object, providing compatibility with both Python 3.10 and newer versions as well as with older versions. The function should demonstrate understanding of `__annotations__` access patterns, the usage of `inspect.get_annotations()`, and handling of stringized annotations. **Problem Statement**: Write a function `process_annotations(obj: object) -> dict:` that: 1. Accesses the `__annotations__` attribute of the given object `obj` safely. 2. If the annotations are stringized (i.e., in string format), it should resolve them into their corresponding Python objects using appropriate context. 3. Handles all possible types of objects that can have annotations including functions, classes, and modules. 4. Ensures compatibility with both Python 3.10 and newer and older versions down to Python 3.6. 5. Returns the resolved annotations as a dictionary. **Function Signature**: ```python def process_annotations(obj: object) -> dict: ``` **Constraints**: - Do not directly assign or modify the `__annotations__` attribute. - Ensure that the function correctly handles objects with no annotations, returning an empty dictionary in such cases. - Use `inspect.get_annotations()` if available and fall back to manual processing for older Python versions. - Handle any exceptions or edge cases such as objects without `__annotations__` gracefully. **Expected Input and Output**: - Input: Any Python object (`obj`) that can potentially have annotations. - Output: A dictionary containing resolved annotations. **Example**: ```python from typing import Any class Base: x: int y: str class Derived(Base): z: \\"list[Any]\\" def wrapper_fn(): pass wrapper_fn.__annotations__ = {\'a\': \'int\', \'return\': \'None\'} print(process_annotations(Base)) # Expected: {\'x\': int, \'y\': str} print(process_annotations(Derived)) # Expected: {\'z\': list[Any]} print(process_annotations(wrapper_fn)) # Expected: {\'a\': int, \'return\': None} print(process_annotations(int)) # Expected: {} ``` **Description to Students**: Ensure you are familiar with handling annotations in Python and understand the differences in accessing them between newer and older Python versions as highlighted in the provided documentation. Your function should demonstrate robust handling of the `__annotations__` attribute, making use of `inspect.get_annotations()` when available and provide backward compatibility where necessary. This question tests your ability to work with annotations in a safe and Pythonic manner.","solution":"import inspect import sys from types import ModuleType from typing import Any, get_type_hints def process_annotations(obj: object) -> dict: Processes the __annotations__ attribute of the given object. Parameters: obj (object): The object to process annotations from. Returns: dict: A dictionary containing resolved annotations. try: if hasattr(inspect, \'get_annotations\'): annotations = inspect.get_annotations(obj, eval_str=True) else: annotations = getattr(obj, \'__annotations__\', {}) if isinstance(annotations, dict): if any(isinstance(v, str) for v in annotations.values()): frame = sys._getframe(1) globalns, localns = frame.f_globals, frame.f_locals annotations = get_type_hints(obj, globalns=globalns, localns=localns) return annotations except Exception: return {} # Example usage: # from typing import Any # # class Base: # x: int # y: str # # class Derived(Base): # z: \\"list[Any]\\" # # def wrapper_fn(): # pass # # wrapper_fn.__annotations__ = {\'a\': \'int\', \'return\': \'None\'} # # print(process_annotations(Base)) # Expected: {\'x\': int, \'y\': str} # print(process_annotations(Derived)) # Expected: {\'z\': list[Any]} # print(process_annotations(wrapper_fn)) # Expected: {\'a\': int, \'return\': None} # print(process_annotations(int)) # Expected: {}"},{"question":"# Naive Bayes Classifier Implementation and Evaluation You are given a dataset, `spam_data.csv`, which contains email text and labels indicating whether each email is spam (1) or not spam (0). Your task is to implement and compare the performance of different Naive Bayes classifiers on this dataset. Dataset Description: - The dataset contains two columns: `text` and `label`. - `text`: The content of the email. - `label`: The target variable, where 1 represents spam and 0 represents not spam. Tasks: 1. **Data Loading and Preprocessing:** - Load the dataset using pandas. - Convert the email text into a numerical format using the `CountVectorizer` from `sklearn.feature_extraction.text`. - Split the dataset into training and testing sets (80% training, 20% testing). 2. **Model Implementation:** - Implement the following Naive Bayes classifiers using scikit-learn: - Gaussian Naive Bayes - Multinomial Naive Bayes - Bernoulli Naive Bayes 3. **Model Training and Evaluation:** - Train each classifier on the training set. - Evaluate the performance on the testing set using accuracy, precision, recall, and F1-score. 4. **Result Interpretation:** - Compare the performance metrics of the three classifiers. - Discuss which classifier performs the best and why that might be the case. Requirements: - The implementation should be done in Python. - Use scikit-learn for implementing the Naive Bayes classifiers. - Use appropriate functions from scikit-learn for calculating performance metrics. - Include comments in your code to explain each step. Input: - `spam_data.csv` (This file should be provided separately in your working environment) Output: - Accuracy, Precision, Recall, and F1-Score for each Naive Bayes classifier. - A brief discussion (1-2 paragraphs) on the performance comparison. Example Output: ```python GaussianNB Accuracy: 0.85, Precision: 0.80, Recall: 0.75, F1-Score: 0.77 MultinomialNB Accuracy: 0.90, Precision: 0.85, Recall: 0.88, F1-Score: 0.86 BernoulliNB Accuracy: 0.88, Precision: 0.83, Recall: 0.85, F1-Score: 0.84 Discussion: The MultinomialNB classifier performs the best in this scenario with the highest accuracy, precision, recall, and F1-score. This is likely due to the nature of the text data where the word counts provide crucial information for classification which MultinomialNB handles well. The GaussianNB, while useful for normally distributed data, may not be the best fit for text classification tasks. Similarly, BernoulliNB, which deals with binary features, performs well but not as effectively as MultinomialNB. ```","solution":"import pandas as pd from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Load the dataset def load_data(file_path): data = pd.read_csv(file_path) return data # Preprocess the data def preprocess_data(data): vectorizer = CountVectorizer() X = vectorizer.fit_transform(data[\'text\']).toarray() y = data[\'label\'] return train_test_split(X, y, test_size=0.2, random_state=42) # Train Naive Bayes classifiers and evaluate them def evaluate_classifiers(X_train, X_test, y_train, y_test): classifiers = { \\"GaussianNB\\": GaussianNB(), \\"MultinomialNB\\": MultinomialNB(), \\"BernoulliNB\\": BernoulliNB() } results = {} for name, clf in classifiers.items(): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) f1 = f1_score(y_test, y_pred) results[name] = { \\"accuracy\\": accuracy, \\"precision\\": precision, \\"recall\\": recall, \\"f1_score\\": f1 } return results # Main function to execute task def main(file_path): data = load_data(file_path) X_train, X_test, y_train, y_test = preprocess_data(data) results = evaluate_classifiers(X_train, X_test, y_train, y_test) for clf, metrics in results.items(): print(f\\"{clf} Accuracy: {metrics[\'accuracy\']:.4f}, Precision: {metrics[\'precision\']:.4f}, Recall: {metrics[\'recall\']:.4f}, F1-Score: {metrics[\'f1_score\']:.4f}\\") # Discussion: # Among the classifiers, MultinomialNB generally performs better with text data due to the # natural structure of the frequency of words in text which it handles optimally. # BernoulliNB also performs well with binary features but might not capture the word frequency distribution as effectively. # GaussianNB, being more suited for continuous normally-distributed data, may underperform in text data classification tasks. # Uncomment the following line to run the main function with the spam_data.csv file # main(\\"spam_data.csv\\")"},{"question":"**Question: Unsupervised Dimensionality Reduction with PCA and Feature Agglomeration** You are working on a project where you need to preprocess a high-dimensional dataset before applying any supervised learning algorithm. To achieve this, you decide to use unsupervised dimensionality reduction techniques provided by scikit-learn. Specifically, you will use PCA (Principal Component Analysis) and Feature Agglomeration to understand their impact on the dataset. **Task:** 1. Implement a function `reduce_dimensionality_PCA` that performs PCA on a given dataset. The function should return the transformed dataset, where the number of dimensions is reduced to the top `n_components` principal components. 2. Implement a function `reduce_dimensionality_agglomeration` that performs Feature Agglomeration on the same dataset. This function should also return the transformed dataset, but the number of features should be reduced to `n_clusters`. **Function Signatures:** ```python def reduce_dimensionality_PCA(data: np.ndarray, n_components: int) -> np.ndarray: pass def reduce_dimensionality_agglomeration(data: np.ndarray, n_clusters: int) -> np.ndarray: pass ``` **Input:** - `data`: A 2D NumPy array of shape (num_samples, num_features) representing the high-dimensional dataset. - `n_components`: An integer representing the number of principal components for PCA. - `n_clusters`: An integer representing the number of clusters for Feature Agglomeration. **Output:** - The functions should return a transformed 2D NumPy array of shape (num_samples, n_components) for `reduce_dimensionality_PCA`. - For `reduce_dimensionality_agglomeration`, the function should return a transformed 2D NumPy array of shape (num_samples, n_clusters). **Constraints:** - You may assume `n_components` and `n_clusters` will always be less than or equal to the number of original features. **Example:** ```python import numpy as np data = np.random.rand(100, 50) # 100 samples, 50 features # PCA Transformation reduced_data_pca = reduce_dimensionality_PCA(data, n_components=10) print(reduced_data_pca.shape) # Expected shape: (100, 10) # Feature Agglomeration Transformation reduced_data_agg = reduce_dimensionality_agglomeration(data, n_clusters=10) print(reduced_data_agg.shape) # Expected shape: (100, 10) ``` **Requirements:** - Use `sklearn.decomposition.PCA` for PCA implementation. - Use `sklearn.cluster.FeatureAgglomeration` for Feature Agglomeration implementation. - Ensure the dataset is properly scaled before applying Feature Agglomeration. Provide rigorous documentation and comments within the code to explain your implementation steps and logic.","solution":"import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler from sklearn.cluster import FeatureAgglomeration def reduce_dimensionality_PCA(data: np.ndarray, n_components: int) -> np.ndarray: Perform PCA on a given dataset to reduce its dimensionality. Parameters: - data (np.ndarray): A 2D NumPy array of shape (num_samples, num_features) representing the dataset. - n_components (int): The number of principal components for PCA. Returns: - np.ndarray: Transformed dataset of shape (num_samples, n_components). pca = PCA(n_components=n_components) transformed_data = pca.fit_transform(data) return transformed_data def reduce_dimensionality_agglomeration(data: np.ndarray, n_clusters: int) -> np.ndarray: Perform Feature Agglomeration on a given dataset to reduce its dimensionality. Parameters: - data (np.ndarray): A 2D NumPy array of shape (num_samples, num_features) representing the dataset. - n_clusters (int): The number of clusters for Feature Agglomeration. Returns: - np.ndarray: Transformed dataset of shape (num_samples, n_clusters). scaler = StandardScaler() scaled_data = scaler.fit_transform(data) agglomeration = FeatureAgglomeration(n_clusters=n_clusters) transformed_data = agglomeration.fit_transform(scaled_data) return transformed_data"},{"question":"# **Asyncio Futures Simulation with Task Management** In this assessment, you are tasked with demonstrating your understanding of `asyncio` futures by implementing a simple asynchronous task management system using `asyncio.Future`. **Instructions**: 1. **Create a Function `delayed_result(delay, value)`**: - This asynchronous function takes a delay (in seconds) and a value. - It should await `asyncio.sleep(delay)` and then return the value. 2. **Implement `run_tasks` Function**: - This function should create a list of `Future` objects using the event loop’s `create_future()` method. - Use `ensure_future` to wrap `delayed_result` calls and set their results to the corresponding `Future`. - Await all futures using `asyncio.gather`. - Finally, return the results once all tasks are done. 3. **Implement `main` function**: - The `main` function should create a set of tasks with varying delays and gather their results. **Function Signatures**: ```python async def delayed_result(delay: int, value: any) -> any: Returns the value after a specified delay. pass async def run_tasks(tasks: list[(int, any)]) -> list[any]: Runs multiple `delayed_result` tasks and returns their results. Args: tasks: A list of tuples where each tuple contains a delay and a value. Returns: A list of results from the executed tasks. pass def main() -> None: Main function to create and run tasks. pass ``` **Example**: ```python # Example usage import asyncio async def delayed_result(delay, value): await asyncio.sleep(delay) return value async def run_tasks(tasks): loop = asyncio.get_running_loop() futures = [loop.create_future() for _ in tasks] for future, task in zip(futures, tasks): asyncio.ensure_future(delayed_result(task[0], task[1])).add_done_callback(lambda t, f=future: f.set_result(t.result())) results = await asyncio.gather(*futures) return results def main(): tasks = [(1, \'task1\'), (2, \'task2\'), (3, \'task3\')] results = asyncio.run(run_tasks(tasks)) print(results) if __name__ == \\"__main__\\": main() ``` **Constraints**: - The tasks should not exceed a delay of 5 seconds. - Ensure that the event loop is managed correctly and that futures are cleaned up after completion. **Assessment Criteria**: - Correct implementation of asynchronous delays. - Proper use and management of `asyncio.Future` and `asyncio.ensure_future`. - Accurate result collection and error handling (if the execution of tasks fails).","solution":"import asyncio async def delayed_result(delay: int, value: any) -> any: Returns the value after a specified delay. Args: delay: The wait time in seconds. value: The value to be returned after the delay. await asyncio.sleep(delay) return value async def run_tasks(tasks: list[tuple[int, any]]) -> list[any]: Runs multiple `delayed_result` tasks and returns their results. Args: tasks: A list of tuples where each tuple contains a delay and a value. Returns: A list of results from the executed tasks. loop = asyncio.get_running_loop() futures = [loop.create_future() for _ in tasks] for future, task in zip(futures, tasks): asyncio.ensure_future(delayed_result(task[0], task[1])).add_done_callback(lambda t, f=future: f.set_result(t.result())) results = await asyncio.gather(*futures) return results def main() -> None: Main function to create and run tasks. tasks = [(1, \'task1\'), (2, \'task2\'), (3, \'task3\')] results = asyncio.run(run_tasks(tasks)) print(results) if __name__ == \\"__main__\\": main()"},{"question":"**Question: Reproducibility in PyTorch** In this task, you are required to set up a reproducible environment in PyTorch. You will implement a function that ensures reproducibility by setting seeds and configuring the necessary components. Additionally, you will create a DataLoader with reproducible behavior and demonstrate running a small neural network training loop with a deterministic configuration. **Requirements:** 1. Implement a function `set_reproducibility()` that: - Sets the global seed for PyTorch, NumPy, and Python’s random module. - Configures PyTorch to use deterministic algorithms where available. 2. Implement a function `get_dataloader(dataset, batch_size, num_workers)` that: - Creates a PyTorch DataLoader for the given dataset. - Ensures the DataLoader has reproducible random behavior using `worker_init_fn` and `generator`. 3. Write a script to: - Create a small dataset and a simple neural network model. - Use the `set_reproducibility()` function and the `get_dataloader()` function. - Train the model for a few epochs and demonstrate that model training is reproducible. **Input and Output:** 1. **Function `set_reproducibility()`**: - **Input**: `seed` (an integer for setting the seed). - **Output**: None. 2. **Function `get_dataloader(dataset, batch_size, num_workers)`**: - **Input**: `dataset` (a PyTorch Dataset object), `batch_size` (int), `num_workers` (int). - **Output**: A PyTorch DataLoader object. 3. **Script**: - Demonstrates reproducible model training in a clear and concise manner. **Constraints and Limitations:** - Your implementation should work with CUDA if available, but must also function correctly on CPU-only setups. - Ensure deterministic operations might have performance impacts, but they are necessary for reproducibility. Example: ```python import torch import torch.nn as nn import torch.optim as optim import numpy as np import random from torch.utils.data import DataLoader, TensorDataset # Part 1: Set Reproducibility def set_reproducibility(seed): torch.manual_seed(seed) np.random.seed(seed) random.seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True) # Part 2: Get DataLoader def get_dataloader(dataset, batch_size, num_workers): def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) g = torch.Generator() g.manual_seed(0) return DataLoader( dataset, batch_size=batch_size, num_workers=num_workers, worker_init_fn=seed_worker, generator=g ) # Part 3: Main Script if __name__ == \\"__main__\\": # Set Seeds set_reproducibility(42) # Create a simple dataset data = torch.randn(100, 10) labels = torch.randint(0, 2, (100,)) dataset = TensorDataset(data, labels) # Create DataLoader dataloader = get_dataloader(dataset, batch_size=10, num_workers=2) # Define a simple model model = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2)) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(3): for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1} completed.\\") ``` In your submission, ensure that your code implements the functions and demonstrates reproducibility as specified.","solution":"import torch import torch.nn as nn import torch.optim as optim import numpy as np import random from torch.utils.data import DataLoader, TensorDataset # Function to set reproducibility def set_reproducibility(seed): torch.manual_seed(seed) np.random.seed(seed) random.seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False torch.use_deterministic_algorithms(True) # Only available in PyTorch 1.8.0 or later # Function to get DataLoader def get_dataloader(dataset, batch_size, num_workers): def seed_worker(worker_id): worker_seed = torch.initial_seed() % 2**32 np.random.seed(worker_seed) random.seed(worker_seed) g = torch.Generator() g.manual_seed(0) return DataLoader( dataset, batch_size=batch_size, num_workers=num_workers, worker_init_fn=seed_worker, generator=g ) # Main Script if __name__ == \\"__main__\\": # Set reproducibility seed set_reproducibility(42) # Create a simple dataset data = torch.randn(100, 10) labels = torch.randint(0, 2, (100,)) dataset = TensorDataset(data, labels) # Create DataLoader dataloader = get_dataloader(dataset, batch_size=10, num_workers=2) # Define a simple model model = nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2)) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(), lr=0.01) # Training loop for epoch in range(3): for inputs, targets in dataloader: optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\\"Epoch {epoch+1} completed.\\")"},{"question":"# Question: Parallel Data Processing with `concurrent.futures` You are given a list of URLs pointing to various JSON data files on the web. Your task is to write a Python function that downloads and processes these JSON files concurrently to extract specific information. The information you\'re interested in is the value associated with a specific key in each JSON file. Function Specification **Function Name**: `process_json_urls` **Inputs**: 1. `urls` (List[str]): A list of strings where each string is a URL pointing to a JSON file. 2. `key` (str): The key whose value needs to be extracted from each JSON file. **Outputs**: - A dictionary where each key is the URL and the corresponding value is the value extracted from the JSON file for the specified key. **Constraints**: - Each JSON file is expected to have a unique URL. - If a URL cannot be reached or does not contain valid JSON data, or the specified key is not present in the JSON, the value for that URL in the output dictionary should be `None`. Example ```python urls = [ \\"https://example.com/data1.json\\", \\"https://example.com/data2.json\\", \\"https://example.com/data3.json\\" ] key = \\"desired_key\\" output = process_json_urls(urls, key) print(output) # Sample output: { # \\"https://example.com/data1.json\\": \\"value1\\", # \\"https://example.com/data2.json\\": None, # \\"https://example.com/data3.json\\": \\"value3\\" # } ``` Additional Requirements: - Utilize the `concurrent.futures` module to download and process JSON files concurrently. - Include comprehensive exception handling to address issues like unreachable URLs, invalid JSON data, and missing keys. Performance: - The function should efficiently handle up to 1000 URLs with response times of individual URLs potentially varying. Notes: - Use the `requests` module to handle HTTP requests. - Make sure to close any open resources and manage thread/process pools properly.","solution":"import concurrent.futures import requests def fetch_json(url): try: response = requests.get(url) response.raise_for_status() return response.json() except (requests.RequestException, ValueError): return None def process_json_urls(urls, key): result = {} def process_url(url): json_data = fetch_json(url) if json_data is not None and key in json_data: return url, json_data[key] else: return url, None with concurrent.futures.ThreadPoolExecutor() as executor: futures = {executor.submit(process_url, url): url for url in urls} for future in concurrent.futures.as_completed(futures): url, value = future.result() result[url] = value return result"},{"question":"# Custom Exception Handling and Exception Context in Python You are tasked with implementing a custom file parser that raises specific exceptions based on various error conditions. The main objective is to write a function that reads content from a file, processes it, and handles any issues that arise gracefully using custom exceptions. Additionally, your program should demonstrate proper usage of exception context and chaining. Requirements 1. **Custom Exceptions**: - Create a custom exception class `FileParsingError` for general parsing errors. - Create subclasses `EmptyFileError`, `MalformedDataError`, and `UnsupportedFileTypeError` derived from `FileParsingError`. 2. **Function Definition**: - Define a function `parse_file(filename: str) -> dict` that reads the content of a file. - The function should: - Raise `EmptyFileError` if the file is empty. - Raise `UnsupportedFileTypeError` if the file extension is not `.txt`. - Attempt to parse the file content into key-value pairs (assume each line contains `key:value` pairs). - Raise `MalformedDataError` if any line is not in the correct `key:value` format. 3. **Exception Handling**: - Demonstrate handling of `FileParsingError` and its subclasses. - Implement exception chaining to provide context when re-raising exceptions. - Use `from None` to suppress context when converting one type of exception to another. 4. **Performance**: - The function should handle large files efficiently without crashing. Input and Output Formats - **Input**: - `filename`: A string representing the path to the file. - **Output**: - A dictionary containing the parsed key-value pairs if no exceptions are raised. Constraints - The file should only contain UTF-8 encoded text. - Handle both Unix (`n`) and Windows (`rn`) newline characters. Example ```python # Example content of a file \\"example.txt\\" name:John Doe age:30 city:New York ``` - If `example.txt` exists and contains the above content, `parse_file(\'example.txt\')` should return: ```python {\'name\': \'John Doe\', \'age\': \'30\', \'city\': \'New York\'} ``` - If the file is empty, raise `EmptyFileError`. - If the filename does not end with `.txt`, raise `UnsupportedFileTypeError`. - If any line is not in the `key:value` format, raise `MalformedDataError`. Implementation Implement your solution below: ```python class FileParsingError(Exception): Base class for exceptions in file parsing. pass class EmptyFileError(FileParsingError): Raised when the file being parsed is empty. pass class MalformedDataError(FileParsingError): Raised when the file contains malformed data. pass class UnsupportedFileTypeError(FileParsingError): Raised when the file type is unsupported. pass def parse_file(filename: str) -> dict: Parses a given file and returns a dictionary of key-value pairs. Raises custom exceptions for various error conditions. # Implement your solution here pass # Include test cases to validate your solution if __name__ == \\"__main__\\": # Test cases go here pass ```","solution":"class FileParsingError(Exception): Base class for exceptions in file parsing. pass class EmptyFileError(FileParsingError): Raised when the file being parsed is empty. pass class MalformedDataError(FileParsingError): Raised when the file contains malformed data. pass class UnsupportedFileTypeError(FileParsingError): Raised when the file type is unsupported. pass def parse_file(filename: str) -> dict: Parses a given file and returns a dictionary of key-value pairs. Raises custom exceptions for various error conditions. if not filename.endswith(\'.txt\'): raise UnsupportedFileTypeError(\\"Only \'.txt\' files are supported\\") try: with open(filename, \'r\', encoding=\'utf-8\') as file: lines = file.readlines() except IOError as e: raise FileParsingError(\\"An error occurred while reading the file\\") from e if not lines: raise EmptyFileError(\\"The file is empty\\") data = {} for line in lines: line = line.strip() if not line: continue # Skip empty lines if \':\' not in line: raise MalformedDataError(f\\"Malformed line: {line}\\") key, value = line.split(\':\', 1) data[key.strip()] = value.strip() return data"},{"question":"# Task You are required to implement a function that connects to a Telnet server, logs in with given credentials, executes a specific command, and returns the output of that command. To achieve this, you will use the `telnetlib` module. # Function Signature ```python def run_telnet_command(host: str, user: str, password: str, command: str, timeout: int = 10) -> str: pass ``` # Input 1. `host` (str): The Telnet server host. 2. `user` (str): The username for authentication. 3. `password` (str): The password for authentication. 4. `command` (str): The command to be executed on the Telnet server. 5. `timeout` (int): The connection timeout in seconds (default is 10 seconds). # Output - Returns the output (str) from the executed command on the Telnet server. # Constraints - Handle exceptions such as connection errors, authentication failures, and command execution errors. - Ensure the connection is properly closed after the operation. - The command execution should handle timeouts gracefully. # Example ```python try: output = run_telnet_command(\\"localhost\\", \\"test_user\\", \\"test_pass\\", \\"ls\\", timeout=5) print(\\"Command Output:n\\", output) except Exception as e: print(f\\"Error: {e}\\") ``` # Note - The function should use the methods from the `telnetlib` module to interact with the Telnet server. - Consider edge cases like incorrect login credentials and unreachable host.","solution":"import telnetlib def run_telnet_command(host: str, user: str, password: str, command: str, timeout: int = 10) -> str: Connects to a Telnet server, logs in with given credentials, executes a specific command, and returns the output of that command. try: # Establish connection to the host tn = telnetlib.Telnet(host, timeout=timeout) # Read until \'login:\' prompt and send the username tn.read_until(b\\"login: \\") tn.write(user.encode(\'ascii\') + b\\"n\\") # Read until \'Password:\' prompt and send the password tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") # Wait for command execution readiness tn.read_until(b\\" \\") # Send the command to be executed tn.write(command.encode(\'ascii\') + b\\"n\\") # Read the output until the next prompt or a timeout occurs output = tn.read_until(b\\" \\", timeout=timeout) # Close the Telnet connection tn.close() # Decode the output to a string return output.decode(\'ascii\') except Exception as e: raise e"},{"question":"**Problem Statement: Environment Variable Manager** You are tasked with creating a Python program to manage environment variables for different scenarios in a software development environment. The program should allow setting, getting, and unsetting environment variables. Additionally, it should provide functionalities to create a backup of current environment variables and restore them as needed. **Requirements:** 1. Implement a class `EnvManager` that provides the following functionalities: - `set_var(key: str, value: str) -> None`: Sets the environment variable `key` to the value `value`. - `get_var(key: str, default: str = None) -> str`: Retrieves the value of the environment variable `key`. If the variable does not exist, returns the `default` value. - `unset_var(key: str) -> None`: Unsets (removes) the environment variable `key`. - `backup_env() -> Dict[str, str]`: Creates and returns a backup of all current environment variables in a dictionary. - `restore_env(backup: Dict[str, str]) -> None`: Restores the environment variables from the given `backup`. This should reset the existing environment with the variables provided in the backup. 2. Ensure that any modifications to the environment variables are reflected immediately in the process environment. **Constraints:** - The `key` and `value` for environment variables should be strings. - The methods should handle invalid inputs gracefully. - The implementation should be compatible with both Unix and Windows systems. **Example Usage:** ```python env_manager = EnvManager() # Setting environment variables env_manager.set_var(\'MY_VAR\', \'my_value\') assert env_manager.get_var(\'MY_VAR\') == \'my_value\' # Backup current environment variables backup = env_manager.backup_env() # Unset an environment variable env_manager.unset_var(\'MY_VAR\') assert env_manager.get_var(\'MY_VAR\') is None # Restore environment variables from backup env_manager.restore_env(backup) assert env_manager.get_var(\'MY_VAR\') == \'my_value\' ``` **Performance Requirements:** - The operations related to getting, setting, and unsetting environment variables should run in constant time. - The backup and restore operations should be efficient enough to handle a large number of environment variables with minimal performance overhead. Write the implementation for the `EnvManager` class with the functionalities described above.","solution":"import os class EnvManager: def set_var(self, key: str, value: str) -> None: Sets the environment variable `key` to the value `value`. os.environ[key] = value def get_var(self, key: str, default: str = None) -> str: Retrieves the value of the environment variable `key`. If the variable does not exist, returns the `default` value. return os.getenv(key, default) def unset_var(self, key: str) -> None: Unsets (removes) the environment variable `key`. if key in os.environ: del os.environ[key] def backup_env(self) -> dict: Creates and returns a backup of all current environment variables in a dictionary. return dict(os.environ) def restore_env(self, backup: dict) -> None: Restores the environment variables from the given `backup`. This should reset the existing environment with the variables provided in the backup. os.environ.clear() os.environ.update(backup)"},{"question":"# Data Class Customer Order Management System You are required to create a customer order management system using Python\'s `dataclasses` module. Your task is to define several data classes to handle customer information, product details, and order processing. 1. Customer Class Define a `Customer` data class with the following attributes: - `customer_id`: an `int` representing the unique ID of the customer. - `name`: a `str` for the customer\'s name. - `email`: a `str` for the customer\'s email address. - `premium_member`: a `bool` indicating whether the customer is a premium member. Ensure that the class also includes a validation check in the `__post_init__` method to verify that the `email` attribute contains an \\"@\\" symbol. Raise a `ValueError` if this condition is not met. 2. Product Class Define a `Product` data class with the following attributes: - `product_id`: an `int` representing the unique ID of the product. - `name`: a `str` for the product name. - `price`: a `float` for the product\'s price. - `stock`: an `int` for the number of items available in stock. 3. Order Class Define an `Order` data class with the following attributes: - `order_id`: an `int` representing the unique ID of the order. - `customer`: a `Customer` instance representing the customer who placed the order. - `products`: a list of `Product` instances representing the products in the order. Include the following additional functionalities: - An `__post_init__` method to compute the total price of the order, taking into account that premium members receive a 10% discount on the total order value. - A method `add_product(self, product: Product, quantity: int) -> None` to add a specified quantity of a product to the order. Ensure that the product is still in stock; otherwise, raise a `ValueError` indicating insufficient stock. 4. Order Management System Implement functions to: 1. Create a new customer and ensure the email validation is enforced. 2. Create a new product with a given stock quantity. 3. Place an order and compute the total price with discounts applied for premium members. Example Usage ```python # Creating customers cus1 = Customer(customer_id=1, name=\\"Alice\\", email=\\"alice@example.com\\", premium_member=True) cus2 = Customer(customer_id=2, name=\\"Bob\\", email=\\"bobexample.com\\", premium_member=False) # should raise ValueError # Creating products prod1 = Product(product_id=101, name=\\"Smartphone\\", price=699.99, stock=10) prod2 = Product(product_id=102, name=\\"Laptop\\", price=999.99, stock=5) # Creating an order order1 = Order(order_id=1001, customer=cus1, products=[]) order1.add_product(prod1, 3) order1.add_product(prod2, 1) print(order1.total_price) # Should reflect 10% discount as Alice is a premium member # Attempting to add more products than available in stock order1.add_product(prod1, 20) # Should raise ValueError ``` Constraints - Assume all IDs are positive integers. - Prices are positive floating-point numbers. - The stock quantity is a positive integer. - No two orders should have the same order ID. Implement these classes and methods diligently, ensuring all edge cases are handled appropriately.","solution":"from dataclasses import dataclass, field from typing import List @dataclass class Customer: customer_id: int name: str email: str premium_member: bool def __post_init__(self): if \\"@\\" not in self.email: raise ValueError(\\"Invalid email address\\") @dataclass class Product: product_id: int name: str price: float stock: int @dataclass class Order: order_id: int customer: Customer products: List[Product] = field(default_factory=list) def __post_init__(self): self.total_price = 0.0 self.compute_total_price() def compute_total_price(self): self.total_price = sum(p.price for p in self.products) if self.customer.premium_member: self.total_price *= 0.9 # Apply 10% discount def add_product(self, product: Product, quantity: int) -> None: if product.stock < quantity: raise ValueError(\\"Insufficient stock for the product\\") for _ in range(quantity): self.products.append(product) product.stock -= quantity self.compute_total_price()"},{"question":"# Custom Command Line Interpreter with Logging You are tasked with creating a custom command line interpreter that performs basic file operations. The goal is to utilize the `cmd` module to create an interactive shell which supports the following commands: 1. **read <filename>** : Reads the content of the specified file and prints it to the console. 2. **write <filename> <content>** : Writes the provided content to the specified file. 3. **delete <filename>** : Deletes the specified file. 4. **exit** : Closes the interpreter. 5. **p_log <filename>** : Plays back commands from a log file, executing them as if they were typed in at the command prompt. 6. **r_log <filename>** : Starts recording commands to a specified log file. 7. **stop_log** : Stops the recording of commands. Additionally, you need to implement the command completion feature and ensure that the logging occurs properly. # Requirements - Implement a class `FileManagerShell` inherited from `cmd.Cmd`. - A method to handle each of the commands listed above (`do_read`, `do_write`, `do_delete`, `do_exit`, `do_p_log`, `do_r_log`, `do_stop_log`). - Command completion support for the commands using `complete_` methods. - Proper logging of commands to a file when recording is active. - Reading and executing commands from a log file when playback is initiated. # Input For commands entered in the custom shell: - `read <filename>` - `write <filename> <content>` - `delete <filename>` - `exit` - `p_log <filename>` - `r_log <filename>` - `stop_log` # Output The expected output: - Contents of the file for the `read` command. - Confirmation message after writing or deleting a file. - Playback of commands from a log file. - Recording of commands to a log file. # Constraints - Ensure that the logging feature does not log the `p_log` command to avoid recursive playback. - Handle file errors gracefully (e.g., file not found, permission errors). # Example Usage ```python (shell) r_log commands.txt (shell) write test.txt Hello, World! (shell) read test.txt Hello, World! (shell) delete test.txt (shell) stop_log (shell) p_log commands.txt ``` # Your Task Implement the `FileManagerShell` class below: ```python import cmd import os class FileManagerShell(cmd.Cmd): intro = \'Welcome to the file manager shell. Type help or ? to list commands.n\' prompt = \'(shell) \' file = None # Add your methods here if __name__ == \'__main__\': FileManagerShell().cmdloop() ``` Ensure your implementation fulfills the requirements and handles all the commands appropriately.","solution":"import cmd import os class FileManagerShell(cmd.Cmd): intro = \'Welcome to the file manager shell. Type help or ? to list commands.n\' prompt = \'(shell) \' file = None log_file = None def do_read(self, arg): \'Read the content of a file: read <filename>\' try: with open(arg, \'r\') as f: print(f.read()) except FileNotFoundError: print(f\\"Error: File \'{arg}\' not found.\\") except Exception as e: print(f\\"Error: {e}\\") def do_write(self, arg): \'Write content to a file: write <filename> <content>\' parts = arg.split(\' \', 1) if len(parts) < 2: print(\\"Usage: write <filename> <content>\\") return filename, content = parts try: with open(filename, \'w\') as f: f.write(content) print(f\\"Written to {filename}\\") except Exception as e: print(f\\"Error: {e}\\") def do_delete(self, arg): \'Delete a file: delete <filename>\' try: os.remove(arg) print(f\\"Deleted {arg}\\") except FileNotFoundError: print(f\\"Error: File \'{arg}\' not found.\\") except Exception as e: print(f\\"Error: {e}\\") def do_exit(self, arg): \'Exit the file manager shell\' print(\'Goodbye!\') return True def do_p_log(self, arg): \'Play back commands from a log file: p_log <filename>\' try: with open(arg, \'r\') as f: for line in f: print(f\\"(log) {line.strip()}\\") self.onecmd(line.strip()) except FileNotFoundError: print(f\\"Error: File \'{arg}\' not found.\\") except Exception as e: print(f\\"Error: {e}\\") def do_r_log(self, arg): \'Start recording commands to a specified log file: r_log <filename>\' self.log_file = open(arg, \'w\') print(f\\"Recording to {arg}\\") def do_stop_log(self, arg): \'Stop recording commands\' if self.log_file: self.log_file.close() self.log_file = None print(\\"Stopped logging\\") def precmd(self, line): if self.log_file and not line.startswith(\\"p_log\\"): self.log_file.write(line + \'n\') return line def complete_read(self, text, line, begidx, endidx): return self._complete_path(text) def complete_write(self, text, line, begidx, endidx): return self._complete_path(text) def complete_delete(self, text, line, begidx, endidx): return self._complete_path(text) def _complete_path(self, text): return [f for f in os.listdir(\'.\') if f.startswith(text)] if __name__ == \'__main__\': FileManagerShell().cmdloop()"},{"question":"# Objective: To evaluate the understanding and application of Python’s profiling tools (`cProfile`, `profile`, and `pstats`) for analyzing program performance. # Problem Statement: Write a Python script to profile the performance of different sorting algorithms. You need to implement the following sorting algorithms, profile their execution, and format the results using the `pstats` module. Sorting Algorithms to Implement: - Bubble Sort - Merge Sort Requirements: 1. Implement functions for Bubble Sort and Merge Sort. 2. Write a function `profile_sorting_algorithms` to: - Run each sorting algorithm with the same list of random integers. - Profile the execution time of each algorithm using `cProfile`. - Save the profiling results to a file named `sorting_stats`. 3. Write another function `analyze_profile_data` to: - Load the profiling data using the `pstats` module. - Sort the profiling results by cumulative time. - Print the top 5 functions that consumed the most cumulative time. # Input: - List of random integers to be sorted. - Number of integers, N (e.g., 1000) # Output: - Print profiling statistics sorted by the time each function took. # Constraints: - Use Python’s `cProfile` and `pstats` modules. - Implement Bubble Sort and Merge Sort as separate functions. - Ensure the profiling results are analyzed and printed correctly. # Example: ```python import random import cProfile import pstats from pstats import SortKey # Implement Bubble Sort def bubble_sort(arr): n = len(arr) for i in range(n): for j in range(0, n-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] # Implement Merge Sort def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2 L = arr[:mid] R = arr[mid:] merge_sort(L) merge_sort(R) i = j = k = 0 while i < len(L) and j < len(R): if L[i] < R[j]: arr[k] = L[i] i += 1 else: arr[k] = R[j] j += 1 k += 1 while i < len(L): arr[k] = L[i] i += 1 k += 1 while j < len(R): arr[k] = R[j] j += 1 k += 1 # Profiling Function def profile_sorting_algorithms(N): arr = random.sample(range(N*10), N) with cProfile.Profile() as pr: bubble_sort(arr.copy()) merge_sort(arr.copy()) pr.dump_stats(\'sorting_stats\') # Analyze Profiling Data def analyze_profile_data(): p = pstats.Stats(\'sorting_stats\') p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(5) # Driver Code if __name__ == \\"__main__\\": N = 1000 profile_sorting_algorithms(N) analyze_profile_data() ``` **Note:** Make sure the functions are implemented correctly, profile the sorting functions, save the results, and then analyze and print the profiling data. # Explanation: - Implement `bubble_sort` and `merge_sort` functions. - Implement `profile_sorting_algorithms` to profile the sorting functions and save profiling data. - Implement `analyze_profile_data` to load and sort profiling data and print the top 5 results by cumulative time.","solution":"import random import cProfile import pstats from pstats import SortKey def bubble_sort(arr): n = len(arr) for i in range(n): for j in range(0, n-i-1): if arr[j] > arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] def merge_sort(arr): if len(arr) > 1: mid = len(arr) // 2 L = arr[:mid] R = arr[mid:] merge_sort(L) merge_sort(R) i = j = k = 0 while i < len(L) and j < len(R): if L[i] < R[j]: arr[k] = L[i] i += 1 else: arr[k] = R[j] j += 1 k += 1 while i < len(L): arr[k] = L[i] i += 1 k += 1 while j < len(R): arr[k] = R[j] j += 1 k += 1 def profile_sorting_algorithms(N): arr = random.sample(range(N*10), N) with cProfile.Profile() as pr: bubble_sort(arr.copy()) merge_sort(arr.copy()) pr.dump_stats(\'sorting_stats\') def analyze_profile_data(): p = pstats.Stats(\'sorting_stats\') p.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(5) if __name__ == \\"__main__\\": N = 1000 profile_sorting_algorithms(N) analyze_profile_data()"},{"question":"# Python Version Number Handling Given the macros used in CPython to store version numbers, you are required to implement two functions that handle Python version strings and their encoded integer representations. **Task 1: Parse Version String** Write a function `parse_version_string(version_str: str) -> dict` that takes a version string (e.g., \\"3.4.1a2\\") and returns a dictionary containing the individual components: major, minor, micro, release level, and release serial. - Input: A version string in the format \\"X.Y.ZRL\\" where: - `X` is the major version. - `Y` is the minor version. - `Z` is the micro version. - `R` is the release level (one of \'a\', \'b\', \'c\', \'f\' representing alpha, beta, release candidate, or final respectively). - `L` is the release serial. - Output: A dictionary with the keys: \\"major\\", \\"minor\\", \\"micro\\", \\"release_level\\", and \\"release_serial\\". Example: ```python parse_version_string(\\"3.4.1a2\\") # Output: {\\"major\\": 3, \\"minor\\": 4, \\"micro\\": 1, \\"release_level\\": \\"a\\", \\"release_serial\\": 2} ``` **Task 2: Construct Version String** Write a function `construct_version_string(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> str` that constructs a version string from the given components. - Input: Components of the version: - `major`: An integer for the major version. - `minor`: An integer for the minor version. - `micro`: An integer for the micro version. - `release_level`: A character (\'a\', \'b\', \'c\', \'f\'). - `release_serial`: An integer for the release serial. - Output: A version string in the format \\"X.Y.ZRL\\". Example: ```python construct_version_string(3, 4, 1, \\"a\\", 2) # Output: \\"3.4.1a2\\" ``` **Constraints:** - The major, minor, and micro versions are non-negative integers. - The release level is guaranteed to be one of \'a\', \'b\', \'c\', \'f\'. - The release serial is a non-negative integer. **Performance Requirements:** - Your function should handle any valid version string and its components within a reasonable (linear) time complexity. Implement these functions to demonstrate your understanding of string manipulation, dictionary handling, and conditional logic in Python.","solution":"def parse_version_string(version_str): Parses a version string in the format \\"X.Y.ZRL\\" and returns a dictionary containing the individual components: major, minor, micro, release level, and release serial. Args: version_str (str): The version string to parse. Returns: dict: A dictionary with keys: \\"major\\", \\"minor\\", \\"micro\\", \\"release_level\\", and \\"release_serial\\". import re pattern = r\\"(d+).(d+).(d+)([a-z])(d+)\\" match = re.match(pattern, version_str) if match: major, minor, micro, release_level, release_serial = match.groups() return { \\"major\\": int(major), \\"minor\\": int(minor), \\"micro\\": int(micro), \\"release_level\\": release_level, \\"release_serial\\": int(release_serial) } else: raise ValueError(\\"Invalid version string format\\") def construct_version_string(major, minor, micro, release_level, release_serial): Constructs a version string from given components. Args: major (int): Major version number. minor (int): Minor version number. micro (int): Micro version number. release_level (str): Release level (one of \'a\', \'b\', \'c\', \'f\'). release_serial (int): Release serial number. Returns: str: A version string in the format \\"X.Y.ZRL\\". return f\\"{major}.{minor}.{micro}{release_level}{release_serial}\\""},{"question":"Objective Your task is to demonstrate your understanding of the scikit-learn library by loading a dataset, pre-processing the data, and applying a machine learning model to classify the data. You will be using the `load_breast_cancer` dataset for this task. Instructions 1. **Dataset Loading**: - Load the Breast Cancer dataset using the `load_breast_cancer` function from scikit-learn. 2. **Data Pre-processing**: - Split the dataset into training and testing sets. Use 80% of the data for training and 20% for testing. - Standardize the features by removing the mean and scaling to unit variance. 3. **Model Implementation**: - Implement a Support Vector Machine (SVM) classifier using the standardized training data. - Use the classifier to predict the classes on the testing data. 4. **Evaluation**: - Evaluate and print the classification accuracy of the model on the testing set. - Additionally, print the confusion matrix of the classification results. Requirements - Your solution should correctly load and preprocess the dataset. - The SVM model should be applied appropriately. - Your solution should generate and print the accuracy and confusion matrix as specified. Expected Input and Output Format - **Input**: No input from the user is required; the required dataset should be loaded directly from scikit-learn. - **Output**: The final output should include: - Classification accuracy on the testing set. - Confusion matrix representing the classification results on the testing set. Constraints - You must use scikit-learn for loading the dataset, preprocessing, and modeling. Example ```python from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score, confusion_matrix # Load dataset data = load_breast_cancer() X = data.data y = data.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement SVM model svm_model = SVC() svm_model.fit(X_train, y_train) # Predict and evaluate y_pred = svm_model.predict(X_test) # Output results accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) print(\\"Classification accuracy: {:.2f}%\\".format(accuracy * 100)) print(\\"Confusion Matrix:n\\", conf_matrix) ``` Note Make sure your code is well-documented with comments explaining each step.","solution":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score, confusion_matrix def breast_cancer_classification(): # Load dataset data = load_breast_cancer() X = data.data y = data.target # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Implement SVM model svm_model = SVC() svm_model.fit(X_train, y_train) # Predict and evaluate y_pred = svm_model.predict(X_test) # Output results accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) results = { \\"accuracy\\": accuracy, \\"confusion_matrix\\": conf_matrix } return results # If you want to print the results results = breast_cancer_classification() print(\\"Classification accuracy: {:.2f}%\\".format(results[\\"accuracy\\"] * 100)) print(\\"Confusion Matrix:n\\", results[\\"confusion_matrix\\"])"},{"question":"# Advanced Python Configuration and Customization **Objective:** The goal of this exercise is to assess your understanding and ability to work with the `site` module for site-specific configuration and customization of Python environments. **Problem Statement:** You are tasked with creating a set of utility functions to configure and inspect the site-specific and user-specific directories for Python packages. Your functions will be utilized to set up a Python environment during initialization and to inspect existing configurations for debugging purposes. **Requirements:** 1. **Function: `initialize_site_configuration()`** - **Description:** This function should call necessary functions from the `site` module to add the standard site-specific directories to the module search path. - **Input/Output:** None (Configures the environment internally). 2. **Function: `add_directory_to_sys_path(directory: str) -> bool`** - **Description:** This function adds a specified directory to `sys.path` if it is not already added and processes any `.pth` files in it. - **Input:** `directory (str)` - The directory path to be added. - **Output:** `bool` - Returns `True` if the directory was added, `False` if it was already present. 3. **Function: `list_global_site_packages() -> list`** - **Description:** This function returns a list of global site-packages directories. - **Output:** `list` - A list of paths to global site-packages directories. 4. **Function: `get_user_base_directory() -> str`** - **Description:** This function returns the path to the user base directory. - **Output:** `str` - The path to the user base directory. 5. **Function: `get_user_site_packages_directory() -> str`** - **Description:** This function returns the path to the user-specific site-packages directory. - **Output:** `str` - The path to the user-specific site-packages directory. **Constraints:** - Each function should be implemented using the corresponding functions and attributes from the `site` module. - Ensure that your implementations handle exceptional cases gracefully and log appropriate errors or messages. **Example Usage:** ```python # Initialize site configuration initialize_site_configuration() # Add directory to sys.path result = add_directory_to_sys_path(\\"/my/custom/path\\") print(result) # Outputs: True if added, False if already present # List global site-packages directories global_packages = list_global_site_packages() print(global_packages) # Example output: [\'/usr/local/lib/python3.10/site-packages\'] # Get user base directory user_base = get_user_base_directory() print(user_base) # Example output: \'/home/user/.local\' # Get user-specific site-packages directory user_site = get_user_site_packages_directory() print(user_site) # Example output: \'/home/user/.local/lib/python3.10/site-packages\' ``` **Note:** Your solution should be tested thoroughly to ensure it works correctly in different environments and configurations.","solution":"import site import sys import os def initialize_site_configuration(): Calls necessary functions from the `site` module to add the standard site-specific directories to the module search path. site.addsitedir(site.getsitepackages()[0]) site.addsitedir(site.getusersitepackages()) def add_directory_to_sys_path(directory: str) -> bool: Adds a specified directory to `sys.path` if it is not already added and processes any `.pth` files in it. Args: - directory (str): The directory path to be added. Returns: - bool: True if the directory was added, False if it was already present. if directory not in sys.path: site.addsitedir(directory) return True return False def list_global_site_packages() -> list: Returns a list of global site-packages directories. Returns: - list: A list of paths to global site-packages directories. return site.getsitepackages() def get_user_base_directory() -> str: Returns the path to the user base directory. Returns: - str: The path to the user base directory. return site.USER_BASE def get_user_site_packages_directory() -> str: Returns the path to the user-specific site-packages directory. Returns: - str: The path to the user-specific site-packages directory. return site.getusersitepackages()"},{"question":"# Advanced Logging Configuration with Python **Objective:** Create a custom logging handler in Python that extends the `HTTPHandler` from the `logging.handlers` module to send log messages to a web server using different HTTP methods and handle potential errors gracefully. **Task:** 1. Implement a class `CustomHTTPHandler` that inherits from `logging.handlers.HTTPHandler`. 2. Override the `emit` method to handle logging records and send them to the server using either `POST` or `PUT` methods based on an initialization parameter. 3. Implement error handling within the `emit` method to retry sending the log message up to three times if a connection error occurs. 4. Demonstrate configuring the logging system to use your `CustomHTTPHandler` with sample code. **Requirements:** 1. **Class `CustomHTTPHandler`** - **Constructor Parameters:** - `host`: The web server address in the format \'host:port\'. - `url`: The URL path to send the logging data to. - `method`: The HTTP method to use for sending the log record (\'POST\' or \'PUT\'). - `secure`: Boolean to specify whether to use HTTPS. - **Methods:** - `emit(record)`: Override this method to send the log record to the server using the specified HTTP method. Implement retry logic for connection errors. 2. **Logging Configuration:** - Configure logging to use `CustomHTTPHandler`. - Redirect log messages from different levels (e.g., INFO, ERROR) to the handler. 3. **Sample Code:** - Provide sample code that configures the logging system and demonstrates sending a few test log messages via the `CustomHTTPHandler`. **Constraints:** - Do not use any external libraries except for standard Python modules. - Handle exceptions gracefully, ensuring that the application does not crash due to logging errors. **Example:** ```python import logging from logging.handlers import HTTPHandler class CustomHTTPHandler(HTTPHandler): def __init__(self, host, url, method=\'POST\', secure=False): supported_methods = [\'POST\', \'PUT\'] if method not in supported_methods: raise ValueError(f\\"Unsupported method: {method}. Choose from {supported_methods}.\\") self.retry_count = 3 super().__init__(host, url, method, secure=secure) def emit(self, record): for attempt in range(self.retry_count): try: super().emit(record) break except Exception as e: # Handle exception and retry logging.error(f\\"Failed to send log record: {e}. Attempt {attempt + 1} of {self.retry_count}.\\") if attempt == self.retry_count - 1: raise # Sample configuration and testing if __name__ == \\"__main__\\": logger = logging.getLogger(\\"CustomLogger\\") logger.setLevel(logging.DEBUG) # Replace below with correct web server details custom_handler = CustomHTTPHandler(\'localhost:8000\', \'/log\', method=\'POST\', secure=False) logger.addHandler(custom_handler) # Sending test messages logger.info(\\"This is an info message\\") logger.error(\\"This is an error message\\") ``` **Expected Output:** The sample code should successfully send log messages to the specified web server using the HTTP method configured for the `CustomHTTPHandler`. Any connection errors or other issues should be logged appropriately, without causing the application to crash.","solution":"import logging from logging.handlers import HTTPHandler from urllib.error import URLError, HTTPError import time class CustomHTTPHandler(HTTPHandler): def __init__(self, host, url, method=\'POST\', secure=False): supported_methods = [\'POST\', \'PUT\'] if method not in supported_methods: raise ValueError(f\\"Unsupported method: {method}. Choose from {supported_methods}.\\") self.retry_count = 3 super().__init__(host, url, method, secure=secure) def emit(self, record): for attempt in range(self.retry_count): try: super().emit(record) break except (URLError, HTTPError) as e: logging.error(f\\"Failed to send log record: {e}. Attempt {attempt + 1} of {self.retry_count}.\\") if attempt == self.retry_count - 1: raise time.sleep(2) # Backoff before retrying"},{"question":"Coding Assessment Question # Objective Implement a series of functions in Python that recreate several operations provided by the C-API for dictionaries described in the documentation. This exercise is designed to test your understanding of dictionary manipulations, type checking, and error handling. # Requirements 1. **Function: `dict_check(obj)`** - **Input**: `obj` - Any Python object. - **Output**: Return `True` if `obj` is a dictionary (`dict`) or an instance of a subtype of `dict`. Otherwise, return `False`. - **Constraints**: Use the `isinstance` function in Python. 2. **Function: `dict_set_item(d, key, value)`** - **Input**: - `d` - A dictionary object. - `key` - A hashable object to be used as a key. - `value` - The value to be associated with `key`. - **Output**: Update the dictionary `d` such that `key` maps to `value`. Return `0` on success. - **Constraints**: Handle exceptions such as `TypeError` if the key is not hashable. 3. **Function: `dict_get_item(d, key)`** - **Input**: - `d` - A dictionary object. - `key` - The key whose value needs to be retrieved. - **Output**: Return the value associated with `key` if it exists. If `key` does not exist, return `None`. - **Constraints**: Do not raise exceptions if the key does not exist. 4. **Function: `dict_del_item(d, key)`** - **Input**: - `d` - A dictionary object. - `key` - The key that should be removed from the dictionary `d`. - **Output**: Remove the `key` and associated value from `d`. Return `0` on success, `-1` if the key does not exist. - **Constraints**: Proper error handling if the key is not found. 5. **Function: `dict_size(d)`** - **Input**: - `d` - A dictionary object. - **Output**: Return the number of key-value pairs in the dictionary `d`. - **Constraints**: None. # Example ```python # Example usage of the functions: # dict_check example print(dict_check({\\"a\\": 1})) # Output: True print(dict_check([1, 2, 3])) # Output: False # dict_set_item example d = {} print(dict_set_item(d, \\"key\\", \\"value\\")) # Output: 0 print(d) # Output: {\\"key\\": \\"value\\"} # dict_get_item example d = {\\"name\\": \\"Alice\\"} print(dict_get_item(d, \\"name\\")) # Output: \\"Alice\\" print(dict_get_item(d, \\"age\\")) # Output: None # dict_del_item example d = {\\"a\\": 1, \\"b\\": 2} print(dict_del_item(d, \\"a\\")) # Output: 0 print(d) # Output: {\\"b\\": 2} print(dict_del_item(d, \\"c\\")) # Output: -1 # dict_size example d = {\\"a\\": 1, \\"b\\": 2, \\"c\\": 3} print(dict_size(d)) # Output: 3 ``` # Notes - Ensure your functions handle errors gracefully and match the expected behavior as described. - Do not import any additional libraries; use built-in Python functions and error handling mechanisms. Good luck!","solution":"def dict_check(obj): Check if the given object is a dictionary or an instance of a subtype of dict. Parameters: obj (any): The object to check. Returns: bool: True if the object is a dictionary, False otherwise. return isinstance(obj, dict) def dict_set_item(d, key, value): Set a key-value pair in a dictionary. Parameters: d (dict): The dictionary to update. key (hashable): The key to set. value (any): The value to set. Returns: int: 0 on success, raises TypeError if key is not hashable. try: d[key] = value return 0 except TypeError: raise def dict_get_item(d, key): Retrieve the value associated with a key in a dictionary. Parameters: d (dict): The dictionary to query. key (hashable): The key to look up. Returns: any: The value associated with the key, or None if the key does not exist. return d.get(key) def dict_del_item(d, key): Remove a key-value pair from a dictionary. Parameters: d (dict): The dictionary to update. key (hashable): The key to remove. Returns: int: 0 on success, -1 if the key does not exist. if key in d: del d[key] return 0 else: return -1 def dict_size(d): Get the number of key-value pairs in a dictionary. Parameters: d (dict): The dictionary to check. Returns: int: The number of key-value pairs in the dictionary. return len(d)"},{"question":"**Objective:** Demonstrate your proficiency in using the `sklearn.feature_extraction` module by implementing a solution that extracts and transforms features from a given dataset of text documents and categorical attributes into a numerical format suitable for machine learning models. **Problem Statement:** You are given the following dataset of textual documents and a separate dictionary of categorical attributes: Textual Documents ```python documents = [ \'The quick brown fox jumps over the lazy dog\', \'Never jump over the lazy dog quickly\', \'A fast brown fox leaps over a lazy cat\' ] ``` Categorical Attributes ```python attributes = [ {\'category\': \'animal\', \'sound\': \'bark\', \'speed\': \'slow\'}, {\'category\': \'human\', \'sound\': \'shout\', \'speed\': \'fast\'}, {\'category\': \'animal\', \'sound\': \'meow\', \'speed\': \'fast\'} ] ``` **Requirements:** 1. **Text Vectorization:** - Use `CountVectorizer` to convert the list of textual documents into a numerical array. - Use both unigrams and bigrams for this transformation. 2. **Categorical Attribute Vectorization:** - Use `DictVectorizer` to convert the list of dictionaries representing categorical attributes into a numerical array. 3. **TF-IDF Transformation:** - Apply `TfidfTransformer` to the numerical array obtained from text vectorization to normalize the term frequencies. 4. **Combination:** - Combine the TF-IDF transformed text features with the vectorized categorical attributes into a single feature matrix suitable for input into a machine learning model. **Function Signature:** ```python def process_data(documents: list, attributes: list) -> tuple: Transforms text documents and categorical attributes into numerical arrays suitable for machine learning models. Parameters: documents (list): List of text documents. attributes (list): List of dictionaries representing categorical attributes. Returns: tuple: Tuple containing the following elements: - tfidf_matrix (numpy.ndarray): The TF-IDF transformed text features. - cat_matrix (numpy.ndarray): The vectorized categorical attributes. - combined_matrix (numpy.ndarray): The combined feature matrix. pass ``` **Expected Input and Output:** - **Input:** - `documents` (List of strings): The textual documents. - `attributes` (List of dictionaries): The categorical attributes. - **Output:** - A tuple containing: - `tfidf_matrix` (numpy.ndarray): The TF-IDF transformed text features. - `cat_matrix` (numpy.ndarray): The vectorized categorical attributes. - `combined_matrix` (numpy.ndarray): The combined feature matrix. **Example:** ```python documents = [ \'The quick brown fox jumps over the lazy dog\', \'Never jump over the lazy dog quickly\', \'A fast brown fox leaps over a lazy cat\' ] attributes = [ {\'category\': \'animal\', \'sound\': \'bark\', \'speed\': \'slow\'}, {\'category\': \'human\', \'sound\': \'shout\', \'speed\': \'fast\'}, {\'category\': \'animal\', \'sound\': \'meow\', \'speed\': \'fast\'} ] tfidf_matrix, cat_matrix, combined_matrix = process_data(documents, attributes) # Example shapes of the output matrices # tfidf_matrix.shape -> (3, n) where n is the number of unique tokens (unigrams + bigrams) # cat_matrix.shape -> (3, m) where m is the number of possible categorical features (one-hot encoded) # combined_matrix.shape -> (3, n + m) ``` **Constraints:** 1. You may assume that all input documents and attributes are well-formed. 2. The implementation should handle transformation efficiently for the given dataset size. 3. Make sure your code follows best practices for readability and efficiency. **Instructions to students:** 1. Implement the `process_data` function as per the requirements outlined above. 2. Write clean and efficient code. 3. Add appropriate comments to explain the logic and flow of your implementation.","solution":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer from sklearn.feature_extraction import DictVectorizer import numpy as np def process_data(documents: list, attributes: list) -> tuple: # Step 1: Text Vectorization using CountVectorizer count_vectorizer = CountVectorizer(ngram_range=(1, 2)) text_counts = count_vectorizer.fit_transform(documents) # Step 2: Convert counts to TF-IDF tfidf_transformer = TfidfTransformer() tfidf_matrix = tfidf_transformer.fit_transform(text_counts).toarray() # Step 3: Categorical Attribute Vectorization using DictVectorizer dict_vectorizer = DictVectorizer() cat_matrix = dict_vectorizer.fit_transform(attributes).toarray() # Step 4: Combine TF-IDF matrix and categorical matrix combined_matrix = np.hstack((tfidf_matrix, cat_matrix)) return tfidf_matrix, cat_matrix, combined_matrix # Example usage of the function documents = [ \'The quick brown fox jumps over the lazy dog\', \'Never jump over the lazy dog quickly\', \'A fast brown fox leaps over a lazy cat\' ] attributes = [ {\'category\': \'animal\', \'sound\': \'bark\', \'speed\': \'slow\'}, {\'category\': \'human\', \'sound\': \'shout\', \'speed\': \'fast\'}, {\'category\': \'animal\', \'sound\': \'meow\', \'speed\': \'fast\'} ] tfidf_matrix, cat_matrix, combined_matrix = process_data(documents, attributes) print(\\"TF-IDF Matrix:n\\", tfidf_matrix) print(\\"Categorical Matrix:n\\", cat_matrix) print(\\"Combined Matrix:n\\", combined_matrix)"},{"question":"**XML Parsing Challenge Using xml.parsers.expat** # Objective: Write a Python function using the `xml.parsers.expat` module to parse an XML document and collect specific data from it. Your function should demonstrate the ability to handle XML parsing events, handle errors, and process complex XML structures. # Problem Statement: You are given an XML document containing information about books in a library. Each book has attributes like title, author, publication date, and genre. Your task is to parse this XML document and produce a summary of the books categorized by genre. # Details: 1. **Input**: - The function will receive a single string, `xml_data`, containing the XML document. 2. **Expected Actions**: - Parse the XML document. - Collect and categorize books based on their genre. - Handle any potential errors during parsing and report them appropriately. 3. **Output**: - Return a dictionary where the keys are genres, and the values are lists of tuples. Each tuple should contain (title, author, publication_date) of a book. # Example XML: ```xml <?xml version=\\"1.0\\"?> <library> <book genre=\\"Fiction\\"> <title>The Great Gatsby</title> <author>F. Scott Fitzgerald</author> <publication_date>1925</publication_date> </book> <book genre=\\"Science Fiction\\"> <title>Dune</title> <author>Frank Herbert</author> <publication_date>1965</publication_date> </book> <book genre=\\"Fiction\\"> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> <publication_date>1960</publication_date> </book> </library> ``` # Expected Output: Given the above XML as input, the expected output would be: ```python { \\"Fiction\\": [ (\\"The Great Gatsby\\", \\"F. Scott Fitzgerald\\", \\"1925\\"), (\\"To Kill a Mockingbird\\", \\"Harper Lee\\", \\"1960\\") ], \\"Science Fiction\\": [ (\\"Dune\\", \\"Frank Herbert\\", \\"1965\\") ] } ``` # Implementation Constraints: - You must use the `xml.parsers.expat` module to parse the XML. - Implement error handling for malformed XML using the `ExpatError` exception. - Ensure your function adheres to the given input and output format. # Performance Requirements: - The function should efficiently handle XML documents up to a size of 1MB. - Ensure minimal memory footprint by handling the XML elements incrementally. # Function Signature: ```python def parse_books(xml_data: str) -> dict: pass ``` # Additional Notes: - You are encouraged to write helper functions if needed. - Include appropriate comments and docstrings in your code for better readability.","solution":"import xml.parsers.expat def parse_books(xml_data: str) -> dict: Parse the given XML document and categorize books based on their genre. Args: xml_data (str): The XML document as a string. Returns: dict: A dictionary where the keys are genres and the values are lists of tuples containing (title, author, publication_date) of books. books_by_genre = {} current_elements = [] current_book = {} def start_element(name, attrs): nonlocal current_elements, current_book current_elements.append(name) if name == \'book\': current_book[\'genre\'] = attrs.get(\'genre\') def end_element(name): nonlocal current_elements, current_book if name == \'book\': genre = current_book.get(\'genre\') if genre: book_tuple = ( current_book.get(\'title\'), current_book.get(\'author\'), current_book.get(\'publication_date\') ) if genre not in books_by_genre: books_by_genre[genre] = [] books_by_genre[genre].append(book_tuple) current_book = {} current_elements.pop() def char_data(data): nonlocal current_elements, current_book if current_elements[-1] in [\'title\', \'author\', \'publication_date\']: current_book[current_elements[-1]] = data parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data try: parser.Parse(xml_data, True) except xml.parsers.expat.ExpatError as e: print(f\\"An error occurred while parsing the XML: {e}\\") return {} return books_by_genre"},{"question":"# Problem: Mathematical Sequence Analysis Implement a function `analyze_sequence(sequence: List[float]) -> Tuple[float, float, float, float, float, int]` that takes a list of floating-point numbers and returns a tuple containing: 1. The sum of all elements (`math.fsum` should be used). 2. The product of all elements (`math.prod` should be used). 3. The mean (average) of the elements. 4. The standard deviation of the elements. Use the formula: [ text{std} = sqrt{frac{1}{N} sum_{i=1}^{N} (x_i - text{mean})^2} ] 5. The median of the elements. If there is an even number of elements, the median is the average of the two middle numbers. 6. The greatest common divisor (GCD) of the integer parts of all elements (`math.gcd` should be used). **Input:** - `sequence` (List[float]): A non-empty list of floating-point numbers. **Output:** - A tuple of six elements, as specified above: (sum, product, mean, standard deviation, median, gcd). **Constraints:** - You can assume that the input list is non-empty. - `sequence` may contain both positive and negative numbers. **Example:** ```python import math from typing import List, Tuple def analyze_sequence(sequence: List[float]) -> Tuple[float, float, float, float, float, int]: # Implementation goes here # Example usage: sequence = [1.5, 2.5, 3.5, 4.5] print(analyze_sequence(sequence)) # Output could be (12.0, 59.0625, 3.0, 1.118033988749895, 3.0, 1) ``` In this problem, students are required to: 1. Use the `math.fsum`, `math.prod`, and `math.gcd` functions appropriately. 2. Calculate the mean and median of the list. 3. Compute the standard deviation. 4. Handle list manipulation and understanding of statistical formulas. **Guidelines:** - Ensure accuracy by avoiding loss of precision using the mentioned functions. - The greatest common divisor should be calculated from the integer parts of the elements of the sequence.","solution":"import math from typing import List, Tuple from math import sqrt def analyze_sequence(sequence: List[float]) -> Tuple[float, float, float, float, float, int]: N = len(sequence) # Sum of all elements total_sum = math.fsum(sequence) # Product of all elements total_product = math.prod(sequence) # Mean (average) of the elements mean = total_sum / N # Standard deviation variance = math.fsum((x - mean) ** 2 for x in sequence) / N std_dev = sqrt(variance) # Median of the elements sorted_sequence = sorted(sequence) if N % 2 == 1: median = sorted_sequence[N // 2] else: median = (sorted_sequence[N // 2 - 1] + sorted_sequence[N // 2]) / 2 # Greatest Common Divisor (GCD) of the integer parts of all elements int_parts = [int(x) for x in sequence] gcd_result = int_parts[0] for num in int_parts[1:]: gcd_result = math.gcd(gcd_result, num) return (total_sum, total_product, mean, std_dev, median, gcd_result)"},{"question":"**Objective:** Implement a function that takes a list of tuples representing an inventory of items, performs various operations using the `operator` module, and returns a summarized inventory report. **Problem Description:** You need to create a function `summarize_inventory` that accepts a list of tuples `(item_name, quantity, price_per_item)` and performs the following operations: 1. Calculate the total value of each item by multiplying the quantity and the price per item. 2. Sort the list of items by their total value in descending order. 3. Return a list of item names ordered by their total value. **Requirements:** - You must use functions from the `operator` module to perform operations such as multiplication and sorting. - The input list can contain 0 to 10,000 items. - The `item_name` is a string, `quantity` and `price_per_item` are integers or floats. **Function Signature:** ```python def summarize_inventory(inventory: list) -> list: pass ``` **Example:** ```python # Sample Input inventory = [ (\\"apple\\", 10, 1.5), (\\"banana\\", 5, 2.0), (\\"orange\\", 7, 1.8) ] # Expected Output [\\"orange\\", \\"banana\\", \\"apple\\"] ``` **Explanation:** 1. The total value for each item is calculated: - apple: 10 * 1.5 = 15.0 - banana: 5 * 2.0 = 10.0 - orange: 7 * 1.8 = 12.6 2. Sorted by total value in descending order, the list becomes: - orange: 12.6 - banana: 10.0 - apple: 15.0 3. The function returns the sorted list based on total values. **Constraints:** - The function should handle and return results within a reasonable time for up to 10,000 items. Make sure your implementation leverages the `operator` module for operations where applicable.","solution":"import operator def summarize_inventory(inventory): Summarizes an inventory of items by calculating total values, sorting by total value in descending order, and returning the ordered list of item names. Parameters: inventory (list): A list of tuples (item_name, quantity, price_per_item). Returns: list: A list of item names sorted by total value in descending order. # Calculate the total value for each item inventory_with_total_value = [ (item_name, operator.mul(quantity, price_per_item)) for item_name, quantity, price_per_item in inventory ] # Sort the list by total value in descending order sorted_inventory = sorted(inventory_with_total_value, key=operator.itemgetter(1), reverse=True) # Return the list of item names ordered by their total value return [item[0] for item in sorted_inventory]"},{"question":"**Question: Email Message Manipulation with Python \'email\' Package** You are tasked with creating a Python function that demonstrates your understanding of the `email` package. The function should create an email message, parse it, modify it, and then serialize it back to a string. # Function Signature: ```python def manipulate_email_message(from_addr: str, to_addr: str, subject: str, body: str) -> str: pass ``` # Input: - `from_addr` (str): The email address of the sender. - `to_addr` (str): The email address of the recipient. - `subject` (str): The subject of the email. - `body` (str): The body content of the email. # Output: - `str`: The serialized string version of the modified email message. # Requirements: 1. **Create**: Construct an email message using the provided `from_addr`, `to_addr`, `subject`, and `body`. 2. **Parse**: Take the serialized email message (in string format) and parse it back into an `EmailMessage` object. 3. **Modify**: Add a new header (\\"X-Modified-By: YourName\\") to the parsed email message. 4. **Serialize**: Convert the modified `EmailMessage` object back into a serialized string. # Constraints: - Ensure that the email message is well-formed and adheres to standard practices. - You may assume the inputs will always be valid email addresses and non-empty strings for subject and body. # Example: Given: ```python from_addr = \\"sender@example.com\\" to_addr = \\"recipient@example.com\\" subject = \\"Meeting Reminder\\" body = \\"Don\'t forget about our meeting at 10 AM tomorrow.\\" ``` Your function should return a string representing the serialized version of the modified email including the new header \\"X-Modified-By: YourName\\". # Implementation: ```python from email.message import EmailMessage from email.parser import Parser from email.policy import default def manipulate_email_message(from_addr: str, to_addr: str, subject: str, body: str) -> str: # Step 1: Create the email message msg = EmailMessage() msg[\'From\'] = from_addr msg[\'To\'] = to_addr msg[\'Subject\'] = subject msg.set_content(body) # Serialize the email message to a string serialized_email = msg.as_string() # Step 2: Parse the serialized email message back to an EmailMessage object parsed_msg = Parser(policy=default).parsestr(serialized_email) # Step 3: Modify the parsed email message by adding a new header parsed_msg[\'X-Modified-By\'] = \'YourName\' # Step 4: Serialize the modified email message back to a string modified_serialized_email = parsed_msg.as_string() return modified_serialized_email ``` **Note**: Ensure you have included the necessary `import` statements and the function meets the specified requirements.","solution":"from email.message import EmailMessage from email.parser import Parser from email.policy import default def manipulate_email_message(from_addr: str, to_addr: str, subject: str, body: str) -> str: # Step 1: Create the email message msg = EmailMessage() msg[\'From\'] = from_addr msg[\'To\'] = to_addr msg[\'Subject\'] = subject msg.set_content(body) # Serialize the email message to a string serialized_email = msg.as_string() # Step 2: Parse the serialized email message back to an EmailMessage object parsed_msg = Parser(policy=default).parsestr(serialized_email) # Step 3: Modify the parsed email message by adding a new header parsed_msg[\'X-Modified-By\'] = \'YourName\' # Step 4: Serialize the modified email message back to a string modified_serialized_email = parsed_msg.as_string() return modified_serialized_email"},{"question":"**Question: Handling Custom Exceptions in Asyncio** **Objective:** Demonstrate your understanding of the asyncio package and its custom exceptions by implementing a set of asynchronous functions and handling different exceptions effectively. **Task:** 1. Implement two asynchronous functions `read_data_with_timeout` and `send_data`. Both functions simulate reading and sending data asynchronously using the `asyncio` library. 2. The `read_data_with_timeout` function should: - Accept a `timeout` parameter and simulate an operation that occasionally surpasses the given deadline by raising an `asyncio.TimeoutError`. - Handle the `asyncio.TimeoutError` exception when it occurs and return a specific string: `\\"TimeoutError: The operation timed out\\"`. 3. The `send_data` function should: - Simulate an operation that may be cancelled by raising an `asyncio.CancelledError` and another operation that might raise an `asyncio.SendfileNotAvailableError`. - Appropriately handle each exception and return respective strings: `\\"CancelledError: Operation cancelled\\"` for `CancelledError` and `\\"SendfileNotAvailableError: sendfile syscall not available\\"` for `SendfileNotAvailableError`. 4. Combine these functions into a main coroutine `main` that demonstrates usage and error handling by invoking both functions with various scenarios. **Constraints:** - Use the async/await syntax for coroutine definitions and asyncio functionality. - Ensure that your functions handle errors gracefully and return the appropriate messages as specified above. - Simulate scenarios where exceptions may occur to demonstrate comprehensive error handling. - You are not required to implement real I/O operations; you can mock the behavior where necessary. **Performance Requirements:** - The solution should be efficient and avoid unnecessary complexity. **Expected Function Signatures:** ```python import asyncio async def read_data_with_timeout(timeout: int) -> str: # Simulate reading data pass async def send_data() -> str: # Simulate sending data pass async def main() -> None: # Demonstrate the usage of read_data_with_timeout and send_data pass # Example usage: # asyncio.run(main()) ``` Write your implementations below and ensure your solution handles all specified scenarios gracefully.","solution":"import asyncio async def read_data_with_timeout(timeout: int) -> str: try: await asyncio.sleep(timeout + 1) return \\"Data read successfully\\" except asyncio.TimeoutError: return \\"TimeoutError: The operation timed out\\" async def send_data() -> str: try: await asyncio.sleep(2) raise asyncio.CancelledError return \\"Data sent successfully\\" except asyncio.CancelledError: return \\"CancelledError: Operation cancelled\\" except asyncio.SendfileNotAvailableError: return \\"SendfileNotAvailableError: sendfile syscall not available\\" async def main() -> None: print(await read_data_with_timeout(1)) print(await send_data())"},{"question":"**Objective:** Create a Python script that automates the process of building a source distribution, ensuring that only specific files are included or excluded based on the provided patterns. # Problem Statement You are required to write a Python function `create_sdist()` that creates a source distribution for a given project directory based on specified inclusion and exclusion patterns. This function will use the concepts of the `install` command family and the `sdist` command from setuptools. # Function Signature ```python def create_sdist(project_dir: str, include_patterns: list, exclude_patterns: list) -> str: pass ``` # Input - `project_dir` (str): The path to the project directory. - `include_patterns` (list): A list of Unix-style \\"glob\\" patterns for files to include in the source distribution. - `exclude_patterns` (list): A list of Unix-style \\"glob\\" patterns for files to exclude from the source distribution. # Output - Returns (str): The path to the created source distribution file. # Constraints 1. The `project_dir` should be a valid directory path. 2. `include_patterns` and `exclude_patterns` should be lists containing valid Unix-style \\"glob\\" patterns. # Example ```python project_dir = \\"/path/to/my_project\\" include_patterns = [\\"*.py\\", \\"README.md\\"] exclude_patterns = [\\"*.pyc\\", \\"__pycache__\\"] result = create_sdist(project_dir, include_patterns, exclude_patterns) print(result) # Output: \\"/path/to/my_project/dist/my_project-0.1.tar.gz\\" ``` # Notes 1. Ensure the function handles errors gracefully, such as invalid directory paths or invalid patterns. 2. You may use any relevant libraries or modules to implement the functionality (e.g., `setuptools`). 3. Document the steps involved in the `create_sdist` function, detailing how files are included or excluded. # Hints - You can use the `setuptools` library and its functions to create the source distribution. - Utilize the manifest template commands (`include`, `exclude`, `recursive-include`, `recursive-exclude`, etc.) to achieve the desired file inclusion and exclusion. Good luck and happy coding!","solution":"import os from setuptools import sandbox from glob import glob import shutil def create_sdist(project_dir: str, include_patterns: list, exclude_patterns: list) -> str: Create a source distribution for a given project directory based on specified inclusion and exclusion patterns. Args: - project_dir (str): The path to the project directory. - include_patterns (list): A list of Unix-style \\"glob\\" patterns for files to include in the source distribution. - exclude_patterns (list): A list of Unix-style \\"glob\\" patterns for files to exclude from the source distribution. Returns: - str: The path to the created source distribution file. if not os.path.isdir(project_dir): raise ValueError(f\\"{project_dir} is not a valid directory.\\") # Create a MANIFEST.in file that setuptools will use manifest_path = os.path.join(project_dir, \'MANIFEST.in\') with open(manifest_path, \'w\') as manifest_file: for pattern in include_patterns: manifest_file.write(f\\"include {pattern}n\\") for pattern in exclude_patterns: manifest_file.write(f\\"exclude {pattern}n\\") # Run the setup script to create the source distribution setup_py = os.path.join(project_dir, \'setup.py\') if not os.path.exists(setup_py): raise ValueError(f\\"No setup.py found in {project_dir}\\") sandbox.run_setup(setup_py, [\'sdist\']) # Find the created sdist file dist_dir = os.path.join(project_dir, \'dist\') sdist_files = glob(os.path.join(dist_dir, \'*.tar.gz\')) if not sdist_files: raise ValueError(\\"No source distribution file created.\\") # Clean up the generated MANIFEST.in os.remove(manifest_path) return sdist_files[0]"},{"question":"# Dynamic Class Creation and Type Handling in Python In this coding challenge, you are required to utilize the `types` module to dynamically create and manipulate classes and their instances. Problem Statement You are to write a function `dynamic_class_manager` that dynamically creates a class based on given specifications and handles instances of this class. The function should take three parameters: 1. `class_name` (str): The name of the class to be created. 2. `base_classes` (tuple): A tuple of base classes (can be empty). 3. `methods` (dict): A dictionary where keys are method names (str) and values are functions that define the methods of the class. The function must: 1. Dynamically create a class with the provided `class_name`, `base_classes`, and `methods` using `types.new_class`. 2. Instantiate an object of this dynamically created class. 3. Use the `types.FunctionType` to verify if the class methods have been correctly assigned. 4. Return the created instance and the verified method names as a dictionary. Expected Output - An instance of the dynamically created class. - A dictionary with method names as keys and a boolean indicating if they are of `types.FunctionType` as values. Constraints - The methods dictionary must have function values only. - The base classes, if provided, should be valid classes. Performance Requirements - The solution should handle the creation of classes with up to 10 methods efficiently. Example Usage ```python import types def dynamic_class_manager(class_name, base_classes, methods): # Your code here # Example methods def example_method(self): return \\"This is an example method\\" class BaseClass: def base_method(self): return \\"This is a base class method\\" # Example call to dynamic_class_manager methods = { \\"example_method\\": example_method, \\"another_method\\": lambda self: \\"Another method\\" } instance, method_checks = dynamic_class_manager(\\"DynamicClass\\", (BaseClass,), methods) print(instance.example_method()) # Output: This is an example method print(instance.base_method()) # Output: This is a base class method print(method_checks) # Output: {\'example_method\': True, \'another_method\': True} ``` # Implementation Details 1. Define the `dynamic_class_manager` function. 2. Use `types.new_class` to create the class dynamically. 3. Instantiate the created class. 4. Use `types.FunctionType` to verify the methods. 5. Return the instance and method verification results. # Hints - Use the `types.new_class` to handle dynamic class creation. - Leverage `types.FunctionType` for type checks on methods. **Note:** Ensure your code is clean, well-documented, and handles edge cases effectively.","solution":"import types def dynamic_class_manager(class_name, base_classes, methods): Dynamically creates a class with the specified name, base classes, and methods. Args: - class_name (str): The name of the class to be created. - base_classes (tuple): A tuple of base classes (can be empty). - methods (dict): A dictionary where keys are method names (str) and values are functions. Returns: - instance: An instance of the dynamically created class. - method_checks (dict): A dictionary with method names as keys and a boolean indicating if they are of types.FunctionType as values. # Create a new class new_class = types.new_class(class_name, base_classes, {}, lambda ns: ns.update(methods)) # Instantiate the new class instance = new_class() # Verify if methods are assigned correctly and are of type FunctionType method_checks = {name: isinstance(getattr(instance, name), types.MethodType) for name in methods} return instance, method_checks"},{"question":"Question You are given a PyTorch tensor with named dimensions, and you are required to perform several operations without losing these dimension names. You need to implement the following function: ```python import torch def tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> dict: Perform multiple tensor operations ensuring the dimensional names are correctly propagated or handled. Args: - tensor_a (torch.Tensor): A named tensor with dimensions `(\'N\', \'C\')`. - tensor_b (torch.Tensor): A named tensor that will operate with tensor_a, having appropriate dimension names. Returns: - dict: A dictionary containing the following key-value pairs: - \\"sum_result\\": Tensor obtained by summing over specified dimension(s) from tensor_a. - \\"prod_result\\": Tensor obtained by multiplying `tensor_a` and `tensor_b`. - \\"aligned_result\\": Tensor obtained by aligning `tensor_a` with `tensor_b` and adding them element-wise. Constraints: - You must ensure that the tensor operations do not lose or alter the dimension names improperly. - tensor_a and tensor_b are always provided with valid names that adhere to the function definitions given in the provided documentation. # Your implementation here ``` # Example Suppose you are given the following tensors: ```python tensor_a = torch.randn(3, 3, names=(\'N\', \'C\')) tensor_b = torch.randn(3, names=(\'N\',)) ``` Calling `tensor_operations(tensor_a, tensor_b)` should return results with correctly handled names for each operation. # Explanation 1. **Sum Result**: - Sum `tensor_a` over the `\'N\'` dimension resulting in a tensor of dimension `\'C\'`. 2. **Prod Result**: - Multiply `tensor_a` by `tensor_b` using an element-wise multiplication after aligning both tensors as necessary by their dimension names. 3. **Aligned Result**: - First align `tensor_a` to the necessary dimensions of `tensor_b` (or vice versa) and perform an element-wise addition ensuring the dimension names are maintained correctly. You must ensure that the implemented operations share the correct dimension names according to the PyTorch named tensors guidelines provided.","solution":"import torch def tensor_operations(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> dict: Perform multiple tensor operations ensuring the dimensional names are correctly propagated or handled. Args: - tensor_a (torch.Tensor): A named tensor with dimensions `(\'N\', \'C\')`. - tensor_b (torch.Tensor): A named tensor that will operate with tensor_a, having appropriate dimension names. Returns: - dict: A dictionary containing the following key-value pairs: - \\"sum_result\\": Tensor obtained by summing over specified dimension(s) from tensor_a. - \\"prod_result\\": Tensor obtained by multiplying `tensor_a` and `tensor_b`. - \\"aligned_result\\": Tensor obtained by aligning `tensor_a` with `tensor_b` and adding them element-wise. Constraints: - You must ensure that the tensor operations do not lose or alter the dimension names improperly. - tensor_a and tensor_b are always provided with valid names that adhere to the function definitions given in the provided documentation. # Summing over the \'N\' dimension of tensor_a sum_result = tensor_a.sum(dim=\'N\') # Multiplying tensor_a and tensor_b # Align tensor_b to the shape of tensor_a prod_result = tensor_a * tensor_b.align_to(\'N\', \'C\') # Summing along the dimension \'N\' to produce aligned_result aligned_result = tensor_a + tensor_b.align_to(\'N\', \'C\') return { \\"sum_result\\": sum_result, \\"prod_result\\": prod_result, \\"aligned_result\\": aligned_result }"},{"question":"Question: Advanced Data Visualization with Seaborn You are given a dataset `healthexp` that consists of healthcare spending data across different countries over several years. Your task is to create a series of visualizations using seaborn\'s advanced features to provide insightful analysis of the data. **Dataset Description**: - `Year`: The year of the recorded spending. - `Spending_USD`: The healthcare spending in USD. - `Country`: The country in which the spending was recorded. # Requirements 1. **Load the Dataset**: Load the `healthexp` dataset using the `seaborn.load_dataset` function. 2. **Create a Normalized Line Plot**: - Plot the healthcare spending over years for each country. - Normalize the spending data relative to each country\'s maximum spending. - Label the Y-axis as \\"Spending relative to maximum amount\\". 3. **Create a Percent Change Line Plot**: - Reuse the data to plot the percent change in spending over years from a specific baseline year (e.g., 1970). - Use the first available year in the dataset as the baseline. - Label the Y-axis as \\"Percent change in spending from baseline\\". 4. **Compare Countries**: - Create a plot comparing the spending patterns across countries. - Highlight any significant trends or patterns observed from the data. # Input and Output Formats - **Input**: This will be provided using the seaborn.load_dataset method. - **Output**: Visual plots in a Jupyter notebook or Python script format. # Constraints - Ensure readability of plots with appropriate labels and legends. - Handle missing data gracefully. - Code should be efficient and well-documented. # Example Here is an example snippet to get you started with loading the dataset and creating a basic plot: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Basic plot example ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) ``` Your task is to extend and enhance this example to meet all the requirements above. Good luck!","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt import pandas as pd # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # 1. Create a Normalized Line Plot def normalized_line_plot(data): # Normalize the spending data relative to each country\'s maximum spending data[\'Normalized_Spending\'] = data.groupby(\'Country\')[\'Spending_USD\'].transform(lambda x: x / x.max()) # Plot the normalized spending over years for each country plt.figure(figsize=(12, 8)) ( so.Plot(data, x=\\"Year\\", y=\\"Normalized_Spending\\", color=\\"Country\\") .add(so.Lines()) .label(y=\\"Spending relative to maximum amount\\") .show() ) plt.title(\'Healthcare Spending Normalized to Maximum Spending Amount\') plt.show() # 2. Create a Percent Change Line Plot def percent_change_line_plot(data): # First available year in the dataset baseline_year = data[\'Year\'].min() # Calculate the percent change in spending from the baseline year baseline_spending = data[data[\'Year\'] == baseline_year].set_index(\'Country\')[\'Spending_USD\'] data = data.set_index(\'Country\') data[\'Baseline_Spending\'] = data.index.map(baseline_spending) data[\'Percent_Change_Spending\'] = ((data[\'Spending_USD\'] - data[\'Baseline_Spending\']) / data[\'Baseline_Spending\']) * 100 data = data.reset_index() # Plot the percent change in spending over years from the baseline year plt.figure(figsize=(12, 8)) ( so.Plot(data, x=\\"Year\\", y=\\"Percent_Change_Spending\\", color=\\"Country\\") .add(so.Lines()) .label(y=\\"Percent change in spending from baseline\\") .show() ) plt.title(f\'Percent Change in Healthcare Spending from {baseline_year}\') plt.show() # 3. Compare Countries def compare_countries_plot(data): # Plot the spending patterns across countries plt.figure(figsize=(12, 8)) sns.lineplot(data=data, x=\\"Year\\", y=\\"Spending_USD\\", hue=\\"Country\\") plt.title(\'Health Expenditure Comparison Across Countries\') plt.ylabel(\'Healthcare Spending in USD\') plt.xlabel(\'Year\') plt.legend(title=\'Country\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Run all the plots normalized_line_plot(healthexp) percent_change_line_plot(healthexp) compare_countries_plot(healthexp)"},{"question":"**Coding Assessment Question** You are tasked to implement an XML-RPC server using the `xmlrpc.server` module. Your server should register various functions, some as standalone functions, others encapsulated within an instance. Additionally, your server should be capable of handling multicall requests. Ensure to enable the necessary features for handling dotted names and self-documenting capabilities. # Requirements: 1. **Server Setup:** - Create a class `CustomXMLRPCServer` derived from `DocXMLRPCServer`. - Use address `(\'localhost\', 9000)` for your server. - Ensure your server is self-documenting. - Enable logging of requests. 2. **Function Registration:** - Register a function `power(base, exponent)` that returns the result of `base**exponent`. - Register a function `addition(x, y)` under the name `sum` which returns the result of `x + y`. 3. **Instance with Dotted Names:** - Create a class `AdvancedOperations` with a method `multiply(x, y)` that returns `x * y`. - Within `AdvancedOperations`, add a nested class `NestedOperations` with a method `subtract(a, b)` that returns `a - b`. - Register an instance of `AdvancedOperations` and allow dotted names. 4. **Multicall Registration:** - Register the multicall function for batch processing of multiple calls. 5. **Server Execution:** - Ensure the server runs indefinitely and handles requests as specified. # Additional Information: - The server should generate appropriate pydoc-style HTML documentation for all the registered functions. - Make sure to make the necessary imports and handle any exceptions gracefully. # Expected Python Code ```python from xmlrpc.server import DocXMLRPCServer class CustomXMLRPCServer(DocXMLRPCServer): def __init__(self, addr=(\'localhost\', 9000)): super().__init__(addr) self.set_server_title(\\"Custom XML-RPC Server\\") self.set_server_name(\\"Custom XML-RPC\\") self.set_server_documentation(\\"This server provides several XML-RPC methods including mathematical operations.\\") # Function Definitions def power(base, exponent): return base ** exponent # Instance Definitions class AdvancedOperations: def multiply(self, x, y): return x * y class NestedOperations: @staticmethod def subtract(a, b): return a - b # Server Configuration and Execution if __name__ == \\"__main__\\": server = CustomXMLRPCServer() server.register_introspection_functions() server.register_function(power, \'power\') server.register_function(lambda x, y: x + y, \'sum\') server.register_instance(AdvancedOperations(), allow_dotted_names=True) server.register_multicall_functions() print(\\"Serving XML-RPC on localhost port 9000\\") try: server.serve_forever() except KeyboardInterrupt: print(\\"nKeyboard interrupt received, exiting.\\") ``` # Constraints: - **Security:** Ensure you are running this server within a secure and closed network environment due to the potential security vulnerabilities, specifically related to enabling dotted names. # Performance: - The server must handle at least 100 concurrent clients without crashing. Explain your implementation, especially the key components and why certain features were enabled or disabled in your setup.","solution":"from xmlrpc.server import DocXMLRPCServer class CustomXMLRPCServer(DocXMLRPCServer): def __init__(self, addr=(\'localhost\', 9000)): super().__init__(addr, logRequests=True) self.set_server_title(\\"Custom XML-RPC Server\\") self.set_server_name(\\"Custom XML-RPC\\") self.set_server_documentation(\\"This server provides several XML-RPC methods including mathematical operations.\\") # Function Definitions def power(base, exponent): return base ** exponent def addition(x, y): return x + y # Instance Definitions class AdvancedOperations: def multiply(self, x, y): return x * y class NestedOperations: @staticmethod def subtract(a, b): return a - b # Server Configuration and Execution if __name__ == \\"__main__\\": server = CustomXMLRPCServer() server.register_introspection_functions() # Enable self-documentation server.register_function(power, \'power\') server.register_function(addition, \'sum\') # Register addition under the name \'sum\' server.register_instance(AdvancedOperations(), allow_dotted_names=True) # Allow dotted names server.register_multicall_functions() # Register multicall for batch processing print(\\"Serving XML-RPC on localhost port 9000\\") try: server.serve_forever() except KeyboardInterrupt: print(\\"nKeyboard interrupt received, exiting.\\")"},{"question":"**Objective:** Implement a functionality that reads a large file, compresses its contents using `zlib`, and then decompresses it to verify the integrity of the compression process. **Problem Statement:** You are required to implement two functions: `compress_file(input_file_path, output_file_path, compression_level=6)` and `decompress_file(input_file_path, output_file_path)`. 1. `compress_file(input_file_path, output_file_path, compression_level=6)`: - **Inputs:** - `input_file_path` (string): The path to the file that needs to be compressed. - `output_file_path` (string): The path where the compressed file should be written. - `compression_level` (int, optional): Compression level (0-9) as described in `zlib.compress()`. Default is 6. - **Outputs:** None - **Constraints:** - The input file can be very large, potentially tens of MB. - The function should handle reading and writing of binary data. - If any errors occur during compression, it should raise a `zlib.error`. - **Functionality:** - Read the contents of the input file. - Compress the data using `zlib.compress()` with the specified compression level. - Write the compressed data to the output file. 2. `decompress_file(input_file_path, output_file_path)`: - **Inputs:** - `input_file_path` (string): The path to the compressed file that needs to be decompressed. - `output_file_path` (string): The path where the decompressed file should be written. - **Outputs:** None - **Constraints:** - The compressed file can be very large, potentially tens of MB. - The function should handle reading and writing of binary data. - If any errors occur during decompression, it should raise a `zlib.error`. - **Functionality:** - Read the contents of the compressed file. - Decompress the data using `zlib.decompress()`. - Write the decompressed data to the output file. **Example Usage:** ```python # Compress a file compress_file(\'large_text.txt\', \'compressed_data.zlib\', compression_level=9) # Decompress the file decompress_file(\'compressed_data.zlib\', \'recovered_text.txt\') ``` **Evaluation Criteria:** - Correctness: The functions should correctly compress and decompress files and handle errors gracefully. - Efficiency: The functions should efficiently handle large files without excessive memory usage. - Code Quality: The code should be well-structured, clear, and follow standard Python coding conventions.","solution":"import zlib def compress_file(input_file_path, output_file_path, compression_level=6): Compresses the contents of a file using zlib and writes the compressed data to an output file. :param input_file_path: Path to the file to be compressed. :param output_file_path: Path to the output file where compressed data will be written. :param compression_level: Compression level (0-9). try: with open(input_file_path, \'rb\') as input_file: data = input_file.read() compressed_data = zlib.compress(data, level=compression_level) with open(output_file_path, \'wb\') as output_file: output_file.write(compressed_data) except zlib.error as e: raise zlib.error(f\\"Error during compression: {e}\\") def decompress_file(input_file_path, output_file_path): Decompresses the contents of a compressed file using zlib and writes the decompressed data to an output file. :param input_file_path: Path to the compressed file. :param output_file_path: Path to the output file where decompressed data will be written. try: with open(input_file_path, \'rb\') as input_file: compressed_data = input_file.read() decompressed_data = zlib.decompress(compressed_data) with open(output_file_path, \'wb\') as output_file: output_file.write(decompressed_data) except zlib.error as e: raise zlib.error(f\\"Error during decompression: {e}\\")"},{"question":"**Sparse Data Analysis with Pandas** **Objective:** The goal of this exercise is to assess your understanding of pandas\' sparse data structures. You will be required to perform several tasks to manipulate and analyze sparse data efficiently using pandas. **Problem Statement:** You are given a large dataset representing sensor readings from various locations. Many of the readings are missing (`NaN`). Your task is to read the data, convert it to a sparse representation, perform some calculations, and finally convert it back to a dense representation. **Tasks:** 1. **Data Initialization:** Create a DataFrame with the following artificially generated data: ```python import numpy as np import pandas as pd np.random.seed(0) data = np.random.randn(1000, 5) data[data < 0.7] = np.nan df = pd.DataFrame(data, columns=[\'Sensor1\', \'Sensor2\', \'Sensor3\', \'Sensor4\', \'Sensor5\']) ``` 2. **Sparse Conversion:** Convert the DataFrame `df` into a sparse DataFrame with `NaN` as the fill value. Validate the conversion by printing the first 5 rows and the data types of the sparse DataFrame. 3. **Density Calculation:** Calculate and print the density of the sparse DataFrame. 4. **Sparse Calculation:** Using the sparse DataFrame, calculate the absolute values of all the sensor readings and store the result in a new sparse DataFrame. Print the first 5 rows of the resulting sparse DataFrame. 5. **Conversion to Dense:** Convert the resulting sparse DataFrame back to a dense DataFrame and print its first 5 rows. **Expected Input/Output:** - **Input:** No explicit input provided. - **Output:** - Sparse DataFrame with `NaN` as fill value. - Density of sparse DataFrame. - Sparse DataFrame with absolute values. - Dense DataFrame after conversion from the sparse DataFrame. **Code Constraints:** - Use `pandas` and `numpy` only. - Ensure the implementation is efficient in terms of memory usage. ```python # Task 1: Data Initialization import numpy as np import pandas as pd np.random.seed(0) data = np.random.randn(1000, 5) data[data < 0.7] = np.nan df = pd.DataFrame(data, columns=[\'Sensor1\', \'Sensor2\', \'Sensor3\', \'Sensor4\', \'Sensor5\']) # Task 2: Sparse Conversion sparse_df = df.astype(pd.SparseDtype(np.float64, np.nan)) print(sparse_df.head()) print(sparse_df.dtypes) # Task 3: Density Calculation density = sparse_df.sparse.density print(\\"Density of sparse DataFrame:\\", density) # Task 4: Sparse Calculation sparse_abs_df = sparse_df.apply(np.abs) print(sparse_abs_df.head()) # Task 5: Conversion to Dense dense_abs_df = sparse_abs_df.sparse.to_dense() print(dense_abs_df.head()) ``` **Explanation:** - **Task 1:** Initializes a DataFrame `df` with random data, setting values below 0.7 to `NaN`. - **Task 2:** Converts `df` to a sparse DataFrame with `NaN` as the fill value and prints its data types and first 5 rows. - **Task 3:** Calculates and prints the density of the sparse DataFrame. - **Task 4:** Applies the absolute value function to all elements of the sparse DataFrame and prints the first 5 rows of the result. - **Task 5:** Converts the sparse DataFrame with absolute values back to a dense format and prints the first 5 rows. By completing this question, students will demonstrate their ability to work with pandas sparse data structures, perform calculations, and handle conversions between sparse and dense representations.","solution":"import numpy as np import pandas as pd def initialize_data(): Initializes a DataFrame with random data, setting values below 0.7 to NaN. np.random.seed(0) data = np.random.randn(1000, 5) data[data < 0.7] = np.nan df = pd.DataFrame(data, columns=[\'Sensor1\', \'Sensor2\', \'Sensor3\', \'Sensor4\', \'Sensor5\']) return df def convert_to_sparse(df): Converts a DataFrame to a sparse DataFrame with NaN as the fill value. Prints the first 5 rows and the data types of the sparse DataFrame. sparse_df = df.astype(pd.SparseDtype(np.float64, np.nan)) print(sparse_df.head()) print(sparse_df.dtypes) return sparse_df def calculate_density(sparse_df): Calculates and returns the density of the sparse DataFrame. density = sparse_df.sparse.density print(\\"Density of sparse DataFrame:\\", density) return density def calculate_absolute_values(sparse_df): Calculates the absolute values of all sensor readings in the sparse DataFrame. Prints the first 5 rows of the result. sparse_abs_df = sparse_df.apply(np.abs) print(sparse_abs_df.head()) return sparse_abs_df def convert_to_dense(sparse_df): Converts the sparse DataFrame back to a dense DataFrame. Prints the first 5 rows of the dense DataFrame. dense_df = sparse_df.sparse.to_dense() print(dense_df.head()) return dense_df"},{"question":"# Advanced Pandas Extension Question **Objective:** Design a custom extension array and dtype to support a new type of array in pandas, `IPAddressArray`, which will store IPv4 addresses. **Required Tasks:** 1. Implement an `IPAddressDtype` class that extends `pandas.api.extensions.ExtensionDtype`. 2. Implement an `IPAddressArray` class that extends `pandas.api.extensions.ExtensionArray`. **Requirements:** 1. **IPAddressDtype Class:** - Must return a descriptive name. - Must implement `type`, `name`, and `na_value` attributes according to pandas’ `ExtensionDtype` guidelines. 2. **IPAddressArray Class:** - Must store and manage the array of IPv4 addresses. - Must implement methods such as: - `_from_sequence(cls, scalars)`: Creates an `IPAddressArray` from a sequence of scalars. - `_from_factorized(cls, values, original)`: Reconstructs the `IPAddressArray` from a factorized array. - `isna`: Identifies missing values in the array. - `take`: Supports indexing and selecting values from the array. - `concat_same_type`: Concatenates arrays. - `unique`: Returns unique addresses. - Additional methods as needed to support common array operations (you can refer to methods listed in the documentation). **Constraints:** - The `IPAddressArray` should handle the storage and retrieval of addresses efficiently. - Implement validation to ensure that only valid IPv4 addresses are stored. **Input and Output Formats:** - `IPAddressArray` should initialize from a list-like sequence of IPv4 strings. - For demonstration, implement the following: ```python arr = IPAddressArray([\'192.168.1.1\', \'192.168.1.2\', \'192.168.1.1\', None]) print(arr.isna()) # Expected output: array([False, False, False, True]) print(arr.unique()) # Expected output: array([\'192.168.1.1\', \'192.168.1.2\', None]) ``` **Performance Requirements:** - Ensure basic operations like indexing and checking for missing values are performed in an efficient manner. **Note:** - You can use Python’s `ipaddress` module for handling IP address manipulations. - You can assume input will be well-formed for basic validation. This question requires deep understanding and practical implementation of pandas extensions, challenging students to integrate custom data types smoothly with pandas.","solution":"import pandas as pd import numpy as np from pandas.api.extensions import ExtensionDtype, ExtensionArray from pandas.api.types import is_scalar import ipaddress class IPAddressDtype(ExtensionDtype): name = \'ipaddress\' type = ipaddress.IPv4Address na_value = None @classmethod def construct_from_string(cls, string): if string == cls.name: return cls() raise TypeError(f\\"Cannot construct a \'{cls.__name__}\' from \'{string}\'\\") def __from_arrow__(self, array): # implement if using Arrow conversion pass class IPAddressArray(ExtensionArray): def __init__(self, values): self._data = np.asarray(values, dtype=object) for value in self._data: if value is not None: ipaddress.IPv4Address(value) # This will raise if the address is invalid @classmethod def _from_sequence(cls, scalars): return cls(scalars) @classmethod def _from_factorized(cls, values, original): return cls(values) @property def dtype(self): return IPAddressDtype() def __getitem__(self, item): if is_scalar(item): return self._data[item] return self._data[item] def __len__(self): return len(self._data) def isna(self): return np.array([x is None for x in self._data]) def take(self, indices, allow_fill=False, fill_value=None): if fill_value is None: fill_value = self.dtype.na_value taken = np.array(self._data.take(indices, mode=\'wrap\' if not allow_fill else \'clip\')) if allow_fill and fill_value is not None: mask = indices == -1 taken[mask] = fill_value return self.__class__(taken) def copy(self): return self.__class__(self._data.copy()) def unique(self): unique_values = pd.unique(self._data) return self.__class__(unique_values) def append(self, other): return self.__class__(np.concatenate([self._data, other._data])) @classmethod def _concat_same_type(cls, to_concat): return cls(np.concatenate([x._data for x in to_concat])) def __eq__(self, other): if isinstance(other, IPAddressArray): return np.array(self._data == other._data) else: return np.array(self._data == other)"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s theme customization and parameter setting capabilities by creating a series of customized plots. # Problem Statement You are provided with a dataset representing the sales of three products (Product A, Product B, and Product C) over six months. Using seaborn and matplotlib, you are required to visualize this data in several ways, demonstrating your ability to customize plots using themes and rc parameters. # Dataset The dataset is as follows (expressed as a dictionary): ```python data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\"], \\"Product_A\\": [10, 15, 7, 8, 13, 9], \\"Product_B\\": [13, 9, 11, 15, 12, 14], \\"Product_C\\": [8, 7, 12, 11, 9, 10] } ``` # Requirements 1. **Default Theme Plot** - Create a bar plot for the sales of the three products over six months using seaborn\'s default theme. 2. **Custom Theme Plot** - Set a custom theme using `sns.set_theme()` with: - `style=\\"whitegrid\\"` - `palette=\\"muted\\"` - Create a bar plot for the given dataset using this theme. 3. **Overriding Parameters** - Define custom parameters to remove the top and right spines of the plot axes. - Update the theme to include these custom parameters. - Create a bar plot for the given dataset with the updated theme. 4. **Final Combined Plot** - Create a single figure with three subplots, each demonstrating the different themes/parameter settings (default, custom with theme, custom with overridden parameters). - Ensure that each subplot is clearly labeled with the theme/setting used. # Input and Output - **Input:** - None (the dataset is hardcoded within the script). - **Output:** - A figure with three subplots as described above. # Constraints - Use seaborn and matplotlib libraries only. - Ensure that the plots are clearly labeled with titles and axes. # Example Code ```python import seaborn as sns import matplotlib.pyplot as plt # Dataset data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\"], \\"Product_A\\": [10, 15, 7, 8, 13, 9], \\"Product_B\\": [13, 9, 11, 15, 12, 14], \\"Product_C\\": [8, 7, 12, 11, 9, 10] } # Convert the dataset into a DataFrame import pandas as pd df = pd.DataFrame(data) # Create the required plots with proper themes and parameters # Your implementation here... ``` Ensure that you provide the necessary code to generate plots as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Dataset data = { \\"Month\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\", \\"Apr\\", \\"May\\", \\"Jun\\"], \\"Product_A\\": [10, 15, 7, 8, 13, 9], \\"Product_B\\": [13, 9, 11, 15, 12, 14], \\"Product_C\\": [8, 7, 12, 11, 9, 10] } # Convert the dataset into a DataFrame df = pd.DataFrame(data) df_melted = df.melt(\'Month\', var_name=\'Product\', value_name=\'Sales\') def plot_default_theme(df): sns.barplot(data=df, x=\\"Month\\", y=\\"Sales\\", hue=\\"Product\\") plt.title(\\"Default Theme Plot\\") def plot_custom_theme(df): sns.set_theme(style=\\"whitegrid\\", palette=\\"muted\\") sns.barplot(data=df, x=\\"Month\\", y=\\"Sales\\", hue=\\"Product\\") plt.title(\\"Custom Theme Plot\\") def plot_custom_params(df): sns.set_theme(style=\\"whitegrid\\", palette=\\"muted\\") sns.set(rc={\\"axes.spines.right\\": False, \\"axes.spines.top\\": False}) sns.barplot(data=df, x=\\"Month\\", y=\\"Sales\\", hue=\\"Product\\") plt.title(\\"Custom Parameters Plot\\") # Create a single figure with three subplots fig, axes = plt.subplots(3, 1, figsize=(10, 15)) plt.sca(axes[0]) plot_default_theme(df_melted) axes[0].legend(loc=\'upper left\') plt.sca(axes[1]) plot_custom_theme(df_melted) axes[1].legend(loc=\'upper left\') plt.sca(axes[2]) plot_custom_params(df_melted) axes[2].legend(loc=\'upper left\') plt.tight_layout() plt.show()"},{"question":"**Question:** You are provided with a data distribution consisting of two components: a Gaussian (Normal) distribution and a Bernoulli distribution. Your task is to implement a function that models this data using PyTorch\'s `torch.distributions` package and performs specific statistical computations. # Function Signature ```python def gaussian_bernoulli_model(data: torch.Tensor, mu: float, sigma: float, p: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Parameters: data (torch.Tensor): A 1D tensor containing the observed data. mu (float): Mean of the Gaussian distribution. sigma (float): Standard deviation of the Gaussian distribution. p (float): Probability parameter of the Bernoulli distribution. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: - sampled_data: Tensor containing samples drawn from the combined model. - log_probs: Tensor containing the log probabilities of the observed data. - likelihood: Tensor containing the likelihood of the observed data. ``` # Requirements 1. **Model the Data Distribution**: - The Gaussian distribution should be parameterized by `mu` and `sigma`. - The Bernoulli distribution should be parameterized by `p`. 2. **Sampling**: - Sample from the Gaussian distribution and the Bernoulli distribution independently. - Construct a combined model where each data point is drawn from the Gaussian distribution if the corresponding sample from the Bernoulli distribution is 1. Otherwise, set the data point to zero. 3. **Computations**: - Calculate the log probabilities of the observed data. - Calculate the likelihood of the observed data. # Example Assume you have the following sample data: ```python data = torch.tensor([0.5, -1.0, 0.0, 1.5, 0.0]) mu = 0.0 sigma = 1.0 p = 0.7 ``` Then, calling `gaussian_bernoulli_model(data, mu, sigma, p)` should return: 1. `sampled_data` - tensor of the same shape as `data` with values sampled according to the combined model. 2. `log_probs` - tensor containing the log probabilities of the observed data. 3. `likelihood` - tensor containing the likelihood of the observed data. **Constraints:** - You must use PyTorch\'s `torch.distributions` package to model the distributions and perform the computations. - Ensure that the function is efficient and can handle large data tensors. **Performance Requirements**: - The function should be optimized for both memory and speed, ensuring the ability to handle large datasets in a reasonable timeframe.","solution":"import torch from torch.distributions import Normal, Bernoulli from typing import Tuple def gaussian_bernoulli_model(data: torch.Tensor, mu: float, sigma: float, p: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Parameters: data (torch.Tensor): A 1D tensor containing the observed data. mu (float): Mean of the Gaussian distribution. sigma (float): Standard deviation of the Gaussian distribution. p (float): Probability parameter of the Bernoulli distribution. Returns: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: - sampled_data: Tensor containing samples drawn from the combined model. - log_probs: Tensor containing the log probabilities of the observed data. - likelihood: Tensor containing the likelihood of the observed data. # Define the distributions gaussian_dist = Normal(mu, sigma) bernoulli_dist = Bernoulli(p) # Sample from distributions gaussian_samples = gaussian_dist.sample(data.shape) bernoulli_samples = bernoulli_dist.sample(data.shape) # Combine the samples sampled_data = gaussian_samples * bernoulli_samples # Calculate log probabilities and likelihoods of observed data log_probs = (bernoulli_dist.log_prob((data != 0).float()) + gaussian_dist.log_prob(data) * (data != 0).float()) # Calculate likelihoods likelihood = torch.exp(log_probs) return sampled_data, log_probs, likelihood"},{"question":"# Question: Custom Data Processing with Iterators, Generators, and Functional Programming Objective Implement a sequence data processing pipeline utilizing custom iterators, generators, and functional programming constructs using `itertools` and `functools` modules to process and analyze a stream of numerical data. Problem Statement You are given a stream of sensor data collected over a period of time. Each data point represents a measurement from the sensor. Your objective is to implement a pipeline that processes this data through a series of operations in a functional programming style. Pipeline Operations 1. **Input Data Stream**: Implement a custom iterator `SensorDataIterator` that yields data points from a given list of numerical data. 2. **Filtering**: Use a generator to filter out data points below a certain threshold. 3. **Transformation**: Apply a transformation function to the filtered data points using a generator expression. 4. **Aggregation**: Use `itertools.accumulate` to provide a running total of the transformed data points. 5. **Output**: Create a function to output the final processed results. Implementation Details 1. **SensorDataIterator** - Implement a custom Python iterator class `SensorDataIterator` that takes a list of numerical data points and yields one data point at a time. ```python class SensorDataIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopIteration ``` 2. **Filter Generator** - Implement a generator function `filter_above_threshold` that takes an iterator and a threshold value, and yields only those values from the iterator that are above the threshold. ```python def filter_above_threshold(data_iter, threshold): for data in data_iter: if data > threshold: yield data ``` 3. **Transformation Generator Expression** - Create a generator expression that squares each data point from the filtered data. ```python squared_data = (x ** 2 for x in filter_above_threshold(data_iter, threshold)) ``` 4. **Running Total Using `itertools.accumulate`** - Use `itertools.accumulate` to compute the running total of the squared data points. ```python import itertools running_total = itertools.accumulate(squared_data) ``` 5. **Final Output Function** - Implement a function `process_sensor_data` that integrates all the steps and returns the final list of processed sensor data. ```python def process_sensor_data(data, threshold): sensor_data_iter = SensorDataIterator(data) filtered_data = filter_above_threshold(sensor_data_iter, threshold) squared_data = (x ** 2 for x in filtered_data) running_total = itertools.accumulate(squared_data) return list(running_total) ``` Input and Output - **Input** - `data`: List of numerical sensor data points (e.g., `[10, 5, 3, 7, 9]`) - `threshold`: Numerical value to filter data points (e.g., `5`) - **Output** - List of cumulative sum of squared data points that are above the threshold. Example ```python data = [10, 5, 3, 7, 9] threshold = 5 result = process_sensor_data(data, threshold) print(result) # Output: [100, 149, 230, 361] ``` **Explanation**: 1. Filtered data points above threshold 5: `[10, 7, 9]` 2. Squared data points: `[100, 49, 81]` 3. Running total: `[100, 149, 230, 361]` Use the above components to write a Python program that correctly processes the sensor data as described. Ensure your implementation adheres to functional programming principles.","solution":"class SensorDataIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.data): result = self.data[self.index] self.index += 1 return result else: raise StopIteration def filter_above_threshold(data_iter, threshold): for data in data_iter: if data > threshold: yield data import itertools def process_sensor_data(data, threshold): sensor_data_iter = SensorDataIterator(data) filtered_data = filter_above_threshold(sensor_data_iter, threshold) squared_data = (x ** 2 for x in filtered_data) running_total = itertools.accumulate(squared_data) return list(running_total)"},{"question":"**Objective:** Implement a Python script that utilizes advanced logging concepts to handle logging in a multi-threaded application. The script should demonstrate the use of custom handlers, structured logging, and logging configuration via a dictionary. **Task:** 1. Create a logging configuration dictionary that: - Logs DEBUG and higher messages to a file (`app.log`) with a detailed formatter. - Logs ERROR and higher messages to the console with a simpler formatter. 2. Implement a custom logging handler that: - Buffers log messages and flushes them to the file every 5 messages or when an ERROR or higher level message is logged. 3. Use structured logging to add custom context information (e.g., thread name and an additional context-specific ID) to the log messages. 4. Implement a function `worker()` that runs in multiple threads and logs messages at different levels with context-specific information. **Requirements:** - Use a logging configuration dictionary to set up the loggers and handlers. - Implement a custom logging handler for buffering messages as specified. - Demonstrate structured logging with additional contextual information. - Use the `Thread` class from the `threading` module to start multiple threads executing the `worker()` function. **Input/Output:** - The logging output should be written to `app.log` and to the console as per the configuration. - The `worker()` function should log starting and completing messages at different levels. **Constraints:** - Do not use global variables for logging configuration. - Ensure threads log their messages independently. **Performance:** - The solution should appropriately handle logging from multiple threads without data loss or corruption. ```python import logging import threading import time # Implement the custom handler, worker function, and logging setup here class BufferingCustomHandler(logging.Handler): def __init__(self, capacity, flush_level=logging.ERROR): super().__init__() self.capacity = capacity self.flush_level = flush_level self.buffer = [] def emit(self, record): self.buffer.append(record) if len(self.buffer) >= self.capacity or record.levelno >= self.flush_level: self.flush() def flush(self): with open(\'app.log\', \'a\') as f: for record in self.buffer: f.write(self.format(record) + \'n\') self.buffer = [] def worker(context_id): logger = logging.getLogger(\'worker\') logger.info(\'Worker started\', extra={\'context_id\': context_id}) logger.debug(\'Worker is processing data\', extra={\'context_id\': context_id}) # Simulate some processing time.sleep(2) logger.warning(\'Worker encountered a minor error\', extra={\'context_id\': context_id}) logger.error(\'Worker encountered a major error\', extra={\'context_id\': context_id}) logger.info(\'Worker finished\', extra={\'context_id\': context_id}) def main(): logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(threadName)-12s %(context_id)s %(levelname)-8s %(message)s\', }, \'simple\': { \'format\': \'%(levelname)s: %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'ERROR\', \'formatter\': \'simple\', }, \'file\': { \'class\': \'logging.FileHandler\', \'filename\': \'app.log\', \'mode\': \'a\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', }, }, \'root\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', } } logging.config.dictConfig(logging_config) # Add custom buffering handler to root logger buffering_handler = BufferingCustomHandler(capacity=5) buffering_handler.setFormatter(logging.Formatter(\'%(asctime)s %(threadName)-12s %(context_id)s %(levelname)-8s %(message)s\')) logging.getLogger().addHandler(buffering_handler) # Start multiple threads threads = [] for i in range(5): thread = threading.Thread(target=worker, args=(f\'context-{i}\',)) threads.append(thread) thread.start() for thread in threads: thread.join() if __name__ == \'__main__\': main() ``` Implement the script by filling in the missing functions and classes. After implementing the script, it should create log entries for each thread, written with structured data and appropriately handled by custom handlers.","solution":"import logging import logging.config import threading import time class BufferingCustomHandler(logging.Handler): def __init__(self, capacity, flush_level=logging.ERROR): super().__init__() self.capacity = capacity self.flush_level = flush_level self.buffer = [] def emit(self, record): self.buffer.append(record) if len(self.buffer) >= self.capacity or record.levelno >= self.flush_level: self.flush() def flush(self): if not self.buffer: return with open(\'app.log\', \'a\') as f: for record in self.buffer: f.write(self.format(record) + \'n\') self.buffer = [] def worker(context_id): logger = logging.getLogger(\'worker\') logger.info(\'Worker started\', extra={\'context_id\': context_id}) logger.debug(\'Worker is processing data\', extra={\'context_id\': context_id}) # Simulate some processing time.sleep(2) logger.warning(\'Worker encountered a minor error\', extra={\'context_id\': context_id}) logger.error(\'Worker encountered a major error\', extra={\'context_id\': context_id}) logger.info(\'Worker finished\', extra={\'context_id\': context_id}) def main(): logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(threadName)-12s %(context_id)s %(levelname)-8s %(message)s\', }, \'simple\': { \'format\': \'%(levelname)s: %(message)s\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'ERROR\', \'formatter\': \'simple\', }, \'file\': { \'class\': \'logging.FileHandler\', \'filename\': \'app.log\', \'mode\': \'a\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', }, }, \'loggers\': { \'worker\': { \'handlers\': [\'console\', \'file\'], \'level\': \'DEBUG\', \'propagate\': False, }, } } logging.config.dictConfig(logging_config) # Add custom buffering handler to worker logger buffering_handler = BufferingCustomHandler(capacity=5) buffering_handler.setFormatter(logging.Formatter(\'%(asctime)s %(threadName)-12s %(context_id)s %(levelname)-8s %(message)s\')) logging.getLogger(\'worker\').addHandler(buffering_handler) # Start multiple threads threads = [] for i in range(5): thread = threading.Thread(target=worker, args=(f\'context-{i}\',)) threads.append(thread) thread.start() for thread in threads: thread.join() if __name__ == \'__main__\': main()"},{"question":"**Objective:** Write a Python function that reads the content of a file, replaces all occurrences of a specified word with another word, and returns the modified content. Use the `builtins` module to read the file. **Instructions:** 1. Define a function `replace_word_in_file` that takes three arguments: - `file_path` (str): The path to the file. - `old_word` (str): The word to be replaced. - `new_word` (str): The word to replace with. 2. The function should use the `builtins.open` function to read and close the file. 3. The function should return the modified content as a string with all occurrences of `old_word` replaced by `new_word`. 4. Ensure your solution handles large files efficiently and is robust against exceptions (e.g., file not found, read errors). **Function Signature:** ```python def replace_word_in_file(file_path: str, old_word: str, new_word: str) -> str: pass ``` **Example:** ```python # Assume \'sample.txt\' contains the following content: # \\"Hello world, welcome to the world of programming.\\" # Calling the function: modified_content = replace_word_in_file(\'sample.txt\', \'world\', \'universe\') # Output: # \\"Hello universe, welcome to the universe of programming.\\" print(modified_content) ``` **Constraints:** - You can assume that `file_path` is a valid path to a readable text file. - The strings `old_word` and `new_word` will not be empty. **HINT:** You can use the `str.replace` method to replace all occurrences of a substring in a string.","solution":"def replace_word_in_file(file_path: str, old_word: str, new_word: str) -> str: Reads the content of the file at file_path, replaces all occurrences of old_word with new_word, and returns the modified content. :param file_path: str : The path to the file to be read :param old_word: str : The word to be replaced :param new_word: str : The word to replace the old word with :return: str : The modified content of the file try: with open(file_path, \'r\', encoding=\'utf-8\') as file: content = file.read() modified_content = content.replace(old_word, new_word) return modified_content except FileNotFoundError: return \\"The file was not found.\\" except IOError: return \\"An error occurred while reading the file.\\""},{"question":"Coding Assessment Question: Custom Python Type Implementation # Objective Design and implement a custom Python class named `CustomObject` that simulates some of the features typically defined in a C extension type (`PyTypeObject`). The class should focus on managing specific attributes, including initialization, representation, hashing, and custom comparison. # Problem Statement 1. Define a class `CustomObject` with the following properties: - An `id` attribute that initializes with a unique identifier for each instance. - A `data` attribute to store string data. - A `metadata` attribute to store additional dictionary-based metadata for the instance. 2. Implement the following methods: - `__init__(self, id: int, data: str, **metadata)`: Initialize a new instance with the given `id`, `data`, and any additional keyword arguments as metadata. - `__repr__(self)`: Return a string in the format `<CustomObject id={id} data={data}>`. - `__hash__(self)`: Compute a hash based on the `id` attribute. - `__eq__(self, other)`: Define equality comparison to compare `CustomObject` instances based on their `id`. 3. Add the following functionalities: - A method `update_metadata(self, key: str, value: Any)`: which allows updating the metadata dictionary with a new key-value pair. - Ensure that the `CustomObject` instances are usable as dictionary keys. # Additional Requirements - Ensure proper documentation for each method. - Write test cases to verify the functionality of your `CustomObject` class. # Example Usage ```python obj1 = CustomObject(id=1, data=\\"sample data\\", info=\\"metadata\\", status=\\"active\\") obj2 = CustomObject(id=2, data=\\"another sample\\", info=\\"more metadata\\") print(repr(obj1)) # Output: <CustomObject id=1 data=sample data> print(hash(obj2)) # Outputs a hash based on the id of the object obj1.update_metadata(\\"new_key\\", \\"new_value\\") # Using CustomObject instances as dictionary keys custom_dict = {obj1: \\"First Object\\", obj2: \\"Second Object\\"} print(custom_dict[obj1]) # Output: First Object print(obj1 == obj2) # Output: False, since their ids are different ``` # Constraints - The `id` attribute will always be an integer. - The `data` attribute will always be a string. - Metadata should handle any type of values. - The `__hash__` and `__eq__` methods must be correctly implemented to ensure instances can be used as dictionary keys. # Submission Instructions Submit a Python file named `custom_object.py` containing the `CustomObject` class implementation and test cases demonstrating its functionality.","solution":"class CustomObject: def __init__(self, id: int, data: str, **metadata): Initialize a new CustomObject instance. Args: id (int): Unique identifier for the instance. data (str): String data for the instance. **metadata (dict): Additional keyword arguments as metadata. self.id = id self.data = data self.metadata = metadata def __repr__(self): Return a string representation of the CustomObject instance. Returns: str: String representation in the format <CustomObject id={id} data={data}>. return f\\"<CustomObject id={self.id} data={self.data}>\\" def __hash__(self): Compute a hash based on the id attribute. Returns: int: Hash value of the instance. return hash(self.id) def __eq__(self, other): Define equality comparison based on the id attribute. Args: other (CustomObject): Other instance to compare. Returns: bool: True if the id attributes are equal, False otherwise. if not isinstance(other, CustomObject): return NotImplemented return self.id == other.id def update_metadata(self, key: str, value: any): Update the metadata dictionary with a new key-value pair. Args: key (str): Key for the metadata. value (any): Value for the metadata. self.metadata[key] = value"},{"question":"**Coding Assessment Question** In this coding assessment, you will demonstrate your understanding of custom object types and attribute management in Python. You are required to implement a Python class that mimics some behaviors of Python extension types as described in the provided documentation, focusing on attribute management and object comparison. # Task: 1. **Define a class `CustomType`:** - It should have an initializer that takes two parameters: `size` (an integer) and `data` (optional, defaulting to `None`). - The class should maintain a private attribute `_size` and an attribute `data`. 2. **Implement the following methods:** - `__repr__()`: Return a string in the format `CustomType(size=<size>, data=<data>)`. - `__str__()`: Return a string in the format `CustomType with size <size> and data <data>`. 3. **Manage attributes explicitly:** - Implement a `__getattr__()` method to handle attribute access. - Implement a `__setattr__()` method to handle attribute assignments. - If an undefined attribute is accessed, raise an `AttributeError`. 4. **Implement the comparison methods:** - Define rich comparison methods (`__eq__`, `__lt__`, `__le__`, `__gt__`, `__ge__`) based on the `size` attribute only. # Constraints: 1. The `size` attribute must always be an integer. 2. Raising appropriate exceptions where necessary. 3. Code readability and proper documentation are essential. 4. You should not use any external libraries for these implementations. # Sample Input/Output: ```python # Creating an instance of CustomType obj1 = CustomType(10, \'sample data\') obj2 = CustomType(5) # Testing __repr__ and __str__ print(repr(obj1)) # Output: CustomType(size=10, data=sample data) print(str(obj1)) # Output: CustomType with size 10 and data sample data # Accessing attributes print(obj1.data) # Output: sample data print(obj2.data) # Output: None # Setting attributes obj1.data = \'new data\' print(obj1.data) # Output: new data # Comparing objects print(obj1 > obj2) # Output: True print(obj1 == obj2) # Output: False # Attempting to access a non-existing attribute try: print(obj1.non_existent) except AttributeError as e: print(e) # Output: \'CustomType\' object has no attribute \'non_existent\' ``` # Implementation: ```python class CustomType: def __init__(self, size, data=None): if not isinstance(size, int): raise TypeError(\\"size must be an integer\\") self._size = size self.data = data def __repr__(self): return f\\"CustomType(size={self._size}, data={self.data})\\" def __str__(self): return f\\"CustomType with size {self._size} and data {self.data}\\" def __getattr__(self, name): if name in self.__dict__: return self.__dict__[name] raise AttributeError(f\\"\'CustomType\' object has no attribute \'{name}\'\\") def __setattr__(self, name, value): if name == \'size\': if not isinstance(value, int): raise TypeError(\\"size must be an integer\\") self.__dict__[\'_size\'] = value else: self.__dict__[name] = value def __eq__(self, other): if isinstance(other, CustomType): return self._size == other._size return NotImplemented def __lt__(self, other): if isinstance(other, CustomType): return self._size < other._size return NotImplemented def __le__(self, other): if isinstance(other, CustomType): return self._size <= other._size return NotImplemented def __gt__(self, other): if isinstance(other, CustomType): return self._size > other._size return NotImplemented def __ge__(self, other): if isinstance(other, CustomType): return self._size >= other._size return NotImplemented ``` **Note:** Ensure your implementation handles all the described functionality correctly, including type checking and attribute handling.","solution":"class CustomType: def __init__(self, size, data=None): if not isinstance(size, int): raise TypeError(\\"size must be an integer\\") self._size = size self.data = data def __repr__(self): return f\\"CustomType(size={self._size}, data={self.data})\\" def __str__(self): return f\\"CustomType with size {self._size} and data {self.data}\\" def __getattr__(self, name): if name in self.__dict__: return self.__dict__[name] raise AttributeError(f\\"\'CustomType\' object has no attribute \'{name}\'\\") def __setattr__(self, name, value): if name == \'size\': if not isinstance(value, int): raise TypeError(\\"size must be an integer\\") self.__dict__[\'_size\'] = value else: self.__dict__[name] = value def __eq__(self, other): if isinstance(other, CustomType): return self._size == other._size return NotImplemented def __lt__(self, other): if isinstance(other, CustomType): return self._size < other._size return NotImplemented def __le__(self, other): if isinstance(other, CustomType): return self._size <= other._size return NotImplemented def __gt__(self, other): if isinstance(other, CustomType): return self._size > other._size return NotImplemented def __ge__(self, other): if isinstance(other, CustomType): return self._size >= other._size return NotImplemented"},{"question":"# Question: Advanced Serialization with Persistent IDs In this task, you will work with the `pickle` module to serialize and deserialize complex data structures involving external references. You need to implement functionality to store a list of objects with references to another collection that is external to the main list. Requirements: 1. **Create two classes `DataItem` and `ExternalResource`:** - `DataItem`: Represents an item with an identifier (string), a value (any data type), and an optional reference to an `ExternalResource`. - `ExternalResource`: Represents an external object with an identifier (string) and content (any data type). 2. **Implement Custom Pickling:** - Implement custom pickling and unpickling methods for both classes to handle the serialization of references using persistent IDs. - Use the `pickle` module\'s `persistent_id` and `persistent_load` methods to manage the external resource references. Input and Output: - **Input**: A list of `DataItem` objects with potential references to `ExternalResource` objects. - **Output**: A serialized byte stream (pickle format) and the ability to deserialize it back into the list of `DataItem` objects with correct references. Constraints: - The `ExternalResource` objects must be serialized separately from the `DataItem` objects, referenced using persistent IDs. - After deserialization, the references between `DataItem` and `ExternalResource` must be maintained correctly. - Your solution should handle the latest pickle protocol (protocol 5). Performance Requirements: - Efficient serialization and deserialization of potentially large data structures. - Use of out-of-band buffers if applicable. Example: ```python import pickle class DataItem: def __init__(self, identifier, value, resource=None): self.identifier = identifier self.value = value self.resource = resource class ExternalResource: def __init__(self, identifier, content): self.identifier = identifier self.content = content class CustomPickler(pickle.Pickler): def __init__(self, file, resources): super().__init__(file, protocol=pickle.HIGHEST_PROTOCOL) self.resources = resources def persistent_id(self, obj): if isinstance(obj, ExternalResource): return (\'ExternalResource\', obj.identifier) return None class CustomUnpickler(pickle.Unpickler): def __init__(self, file, resources): super().__init__(file) self.resources = resources def persistent_load(self, pid): type_tag, identifier = pid if type_tag == \'ExternalResource\': return self.resources[identifier] raise pickle.UnpicklingError(f\\"unsupported persistent object: {pid}\\") def serialize_data(data_items, resources): buffers = [] file = io.BytesIO() pickler = CustomPickler(file, resources) pickler.dump(data_items) return file.getvalue() def deserialize_data(serialized_data, resources): file = io.BytesIO(serialized_data) unpickler = CustomUnpickler(file, resources) data_items = unpickler.load() return data_items # Example Usage resource1 = ExternalResource(\'res1\', \'external content 1\') resource2 = ExternalResource(\'res2\', \'external content 2\') item1 = DataItem(\'item1\', \'value1\', resource1) item2 = DataItem(\'item2\', \'value2\', resource2) item3 = DataItem(\'item3\', \'value3\') # No resource reference resources_dict = { \'res1\': resource1, \'res2\': resource2 } data_items = [item1, item2, item3] serialized_data = serialize_data(data_items, resources_dict) deserialized_items = deserialize_data(serialized_data, resources_dict) assert deserialized_items[0].resource.content == \'external content 1\' assert deserialized_items[1].resource.content == \'external content 2\' assert deserialized_items[2].resource is None ``` Write the complete code to define the classes `DataItem`, `ExternalResource`, and the custom pickler and unpickler (`CustomPickler` and `CustomUnpickler`). Ensure that your solution handles the serialization of external references correctly.","solution":"import pickle import io class DataItem: def __init__(self, identifier, value, resource=None): self.identifier = identifier self.value = value self.resource = resource def __eq__(self, other): return (self.identifier == other.identifier and self.value == other.value and self.resource == other.resource) class ExternalResource: def __init__(self, identifier, content): self.identifier = identifier self.content = content def __eq__(self, other): return (self.identifier == other.identifier and self.content == other.content) class CustomPickler(pickle.Pickler): def __init__(self, file, resources): super().__init__(file, protocol=pickle.HIGHEST_PROTOCOL) self.resources = resources def persistent_id(self, obj): if isinstance(obj, ExternalResource): return (\'ExternalResource\', obj.identifier) return None class CustomUnpickler(pickle.Unpickler): def __init__(self, file, resources): super().__init__(file) self.resources = resources def persistent_load(self, pid): type_tag, identifier = pid if type_tag == \'ExternalResource\': return self.resources[identifier] raise pickle.UnpicklingError(f\\"unsupported persistent object: {pid}\\") def serialize_data(data_items, resources): buffers = [] file = io.BytesIO() pickler = CustomPickler(file, resources) pickler.dump(data_items) return file.getvalue() def deserialize_data(serialized_data, resources): file = io.BytesIO(serialized_data) unpickler = CustomUnpickler(file, resources) data_items = unpickler.load() return data_items"},{"question":"# Pandas Advanced DataFrame Manipulation **Objective**: Assess your ability to work with multiple DataFrame objects and perform complex data manipulations including merging, joining, and concatenating. # Scenario You are working as a data analyst for a retail company. The company has sales and customer information stored in different DataFrames. You have been asked to compile a comprehensive report combining sales transactions with customer data, and identify discrepancies between the sales records from different regions. # Datasets 1. **Sales Data**: ```python import pandas as pd sales_data = pd.DataFrame({ \'transaction_id\': [101, 102, 103, 104, 105, 106, 107, 108], \'customer_id\': [1, 2, 2, 3, 4, 4, 5, 5], \'region\': [\'North\', \'South\', \'North\', \'East\', \'West\', \'West\', \'North\', \'East\'], \'amount\': [200, 150, 100, 300, 500, 700, 250, 400], \'date\': pd.to_datetime([ \'2023-01-01\', \'2023-01-02\', \'2023-01-02\', \'2023-01-03\', \'2023-01-03\', \'2023-01-04\', \'2023-01-05\', \'2023-01-05\' ]) }) ``` 2. **Customer Data**: ```python customer_data = pd.DataFrame({ \'customer_id\': [1, 2, 3, 4, 5], \'customer_name\': [\'Alice\', \'Bob\', \'Charlie\', \'David\', \'Eve\'], \'email\': [\'alice@example.com\', \'bob@example.com\', \'charlie@example.com\', \'david@example.com\', \'eve@example.com\'] }) ``` 3. **Sales Data from another region (e.g., Remote) which might have discrepancies**: ```python remote_sales_data = pd.DataFrame({ \'transaction_id\': [101, 102, 109, 110], \'customer_id\': [1, 2, 6, 7], \'region\': [\'North\', \'South\', \'Remote\', \'Remote\'], \'amount\': [200, 150, 350, 450], \'date\': pd.to_datetime([\'2023-01-01\', \'2023-01-02\', \'2023-01-06\', \'2023-01-07\']) }) ``` # Task 1. **Concatenate Sales Data**: Combine `sales_data` and `remote_sales_data` into a single DataFrame without losing any data. Use appropriate index handling to ensure complete records. 2. **Merge with Customer Data**: Merge the concatenated sales data with `customer_data` to include customer information for each transaction. Make sure to handle any missing customer details gracefully. 3. **Identify Discrepancies**: Write a function `identify_discrepancies` that compares `sales_data` and `remote_sales_data`. The function should return a DataFrame highlighting discrepancies in transaction amounts for the same transaction IDs. 4. **Generate Report**: Summarize the data by calculating the total sales amount per region and the total sales amount per customer. Provide the summary as two separate DataFrames. # Input and Output Specifications 1. **Concatenate Sales Data**: - Input: `sales_data`, `remote_sales_data` - Output: `combined_sales_data` (DataFrame) 2. **Merge with Customer Data**: - Input: `combined_sales_data`, `customer_data` - Output: `sales_with_customer` (DataFrame) 3. **Identify Discrepancies**: - Input: `sales_data`, `remote_sales_data` - Output: `discrepancies` (DataFrame) 4. **Generate Report**: - Input: `sales_with_customer` - Output: `total_sales_per_region` (DataFrame), `total_sales_per_customer` (DataFrame) # Example call ```python # Assuming you have already created the DataFrames sales_data, remote_sales_data, and customer_data # Task 1: Concatenate Sales Data combined_sales_data = concat_sales_data(sales_data, remote_sales_data) # Task 2: Merge with Customer Data sales_with_customer = merge_sales_customer(combined_sales_data, customer_data) # Task 3: Identify Discrepancies discrepancies = identify_discrepancies(sales_data, remote_sales_data) # Task 4: Generate Report total_sales_per_region, total_sales_per_customer = generate_report(sales_with_customer) ``` # Constraints - Ensure proper handling of missing data without dropping any essential records. - Maintain column integrity during merges and concatenations. # Evaluation Criteria - Correctness of concatenation and merging operations. - Accurate identification of discrepancies. - Comprehensive and accurate summary report generation. - Code readability and use of appropriate pandas functions.","solution":"import pandas as pd def concat_sales_data(sales_data, remote_sales_data): Concatenate the sales data from different sources. combined_sales_data = pd.concat([sales_data, remote_sales_data], ignore_index=True) return combined_sales_data def merge_sales_customer(combined_sales_data, customer_data): Merge the combined sales data with customer information. sales_with_customer = pd.merge(combined_sales_data, customer_data, on=\'customer_id\', how=\'left\') return sales_with_customer def identify_discrepancies(sales_data, remote_sales_data): Identify discrepancies in transaction amounts for the same transaction IDs between sales_data and remote_sales_data. discrepancies = sales_data.merge(remote_sales_data, on=\'transaction_id\', suffixes=(\'_local\', \'_remote\')) discrepancies = discrepancies[discrepancies[\'amount_local\'] != discrepancies[\'amount_remote\']] return discrepancies[[\'transaction_id\', \'customer_id_local\', \'region_local\', \'amount_local\', \'region_remote\', \'amount_remote\']] def generate_report(sales_with_customer): Generate sales report summarizing total sales amount per region and per customer. total_sales_per_region = sales_with_customer.groupby(\'region\')[\'amount\'].sum().reset_index() total_sales_per_customer = sales_with_customer.groupby([\'customer_id\', \'customer_name\'])[\'amount\'].sum().reset_index() return total_sales_per_region, total_sales_per_customer"},{"question":"**Objective**: Implement a logging system using the `syslog` module that simulates different logging scenarios in a Unix-based system. **Task**: You are required to implement a Python class `SysLogger` that utilizes the Unix \\"syslog\\" library routines to log messages with various priorities and facilities. The class should support the capabilities to set options, log messages, change the logging mask, and manage the logging session. # Class `SysLogger`: 1. **Initialization Method**: - `__init__(self, ident=None, logoption=0, facility=syslog.LOG_USER)`: Initializes the logger with optional identifier, log options, and facility. If the identifier is not provided, default to \\"SysLogger\\". 2. **Setup Method**: - `setup(self, logoption=0, facility=syslog.LOG_USER)`: Configures the logging options and facility. Opens the log with the provided settings. 3. **Log Method**: - `log(self, priority, message)`: Logs a message with the given priority. Only logs messages that conform to the current logging mask. 4. **Set Log Mask Method**: - `set_mask(self, mask)`: Configures the logging mask. Log messages below the mask priority should be ignored. 5. **Close Method**: - `close(self)`: Closes the logging session, resets the module values. # Usage Example: ```python import syslog class SysLogger: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): if ident is None: ident = \\"SysLogger\\" self.ident = ident self.logoption = logoption self.facility = facility self.setup(logoption, facility) def setup(self, logoption=0, facility=syslog.LOG_USER): syslog.openlog(ident=self.ident, logoption=logoption, facility=facility) def log(self, priority, message): syslog.syslog(priority, message) def set_mask(self, mask): syslog.setlogmask(mask) def close(self): syslog.closelog() ``` # Instructions: 1. **Initialization**: Create an instance of `SysLogger` with an identifier \\"MyApp\\" and log options including `syslog.LOG_PID` and `syslog.LOG_CONS`. 2. **Setup**: Configure the logger to log messages to the mail facility. 3. **Logging**: Log messages with different priorities and observe the output. 4. **Change Log Mask**: Set the logging mask to ignore debug level messages and test this by trying to log a debug message. 5. **Close**: Close the logging session and reset the module. Confirm no further messages are logged. # Expected Outcome: - A class `SysLogger` with defined behavior to manage syslog-based logging. - Demonstrated understanding of setting, logging with priorities, modifying log masks, and proper session management. Constraints: - You must use the syslog module. - Ensure logging parameters are correctly used as specified. - Handle exceptions where necessary, ensuring the logging system is robust. Implement your solution and demonstrate the use of the `SysLogger` class as described above.","solution":"import syslog class SysLogger: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): if ident is None: ident = \\"SysLogger\\" self.ident = ident self.logoption = logoption self.facility = facility self.setup(logoption, facility) def setup(self, logoption=0, facility=syslog.LOG_USER): syslog.openlog(ident=self.ident, logoption=logoption, facility=facility) def log(self, priority, message): syslog.syslog(priority, message) def set_mask(self, mask): syslog.setlogmask(mask) def close(self): syslog.closelog()"},{"question":"You are tasked with visualizing customer tipping habits based on a restaurant dataset using the seaborn library in Python. You will utilize seaborn\'s `stripplot` method along with advanced visual customizations to analyze and display the data. Your goal is to create a comprehensive plot that displays tipping patterns across different days, times, and customer party sizes. Instructions: 1. **Load the `tips` dataset** using seaborn\'s `load_dataset` function. 2. **Customize the plot** to meet the following requirements: - Plot the `total_bill` variable along the x-axis. - Plot the `day` variable along the y-axis. - Use the `time` variable as the hue to distinguish between Lunch and Dinner. - Set `dodge=True` to split the levels in the same `day` category. - Disable the jitter to avoid random displacement of data points. - Customize the markers to diamonds (`marker=\'D\'`), with a size of 20 (`s=20`), and set the transparency to 0.4 (`alpha=0.4`). - Add a legend to explain the hue variable. 3. **Display the plot** and ensure all the customizations are applied correctly. Constraints: - Ensure that the seaborn library is properly imported. - Add necessary matplotlib settings for improved display aesthetics (such as setting the theme). # Expected Function Signature ```python def plot_customized_tipping_data(): pass ``` Example Output: The function `plot_customized_tipping_data()` should display a seaborn plot with all the required customizations. # Performance Considerations: - Ensure the function loads the dataset and plots the data efficiently. - Maintain an aesthetically pleasing layout, legible markings, and appropriate legend placement.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_customized_tipping_data(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the theme for the plot sns.set_theme(style=\\"whitegrid\\") # Create the stripplot with the required customizations plt.figure(figsize=(10, 6)) sns.stripplot( x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", data=tips, dodge=True, jitter=False, marker=\'D\', s=20, alpha=0.4 ) # Add a legend to explain the hue variable plt.legend(title=\'Time\') # Display the plot plt.show()"},{"question":"You are provided with a dataset consisting of features `X` and targets `Y`. Your task is to implement a function that trains a PLSRegression model using Scikit-Learn\'s `PLSRegression` class and evaluates its performance by computing the Root Mean Square Error (RMSE) on a test set. # Function Signature ```python from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error import numpy as np def train_and_evaluate_pls(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, n_components: int) -> float: Trains a PLSRegression model and evaluates its performance on the test set using RMSE. Parameters: - X_train (np.ndarray): Training feature matrix of shape (n_samples, n_features). - Y_train (np.ndarray): Training target matrix of shape (n_samples, n_targets). - X_test (np.ndarray): Testing feature matrix of shape (m_samples, n_features). - Y_test (np.ndarray): Testing target matrix of shape (m_samples, n_targets). - n_components (int): Number of components to keep for the PLS model. Returns: - float: The RMSE of the model predictions on the test set. # Your implementation here pass ``` # Requirements 1. **Model Training**: - Use the `PLSRegression` class from Scikit-Learn to fit the model on the provided training data `X_train` and `Y_train`. - Ensure you set the `n_components` parameter to the provided argument. 2. **Prediction**: - Use the trained model to predict the targets of the test features `X_test`. 3. **Evaluation**: - Compute the Root Mean Square Error (RMSE) between the predicted targets and the actual targets `Y_test`. # Example Usage ```python # Example data X_train = np.array([[0.1, 0.2], [0.4, 0.5], [0.7, 0.8]]) Y_train = np.array([[0.1], [0.4], [0.7]]) X_test = np.array([[0.3, 0.3], [0.6, 0.7]]) Y_test = np.array([[0.3], [0.6]]) # Number of components n_components = 2 # Call your function rmse = train_and_evaluate_pls(X_train, Y_train, X_test, Y_test, n_components) print(f\\"RMSE: {rmse}\\") ``` # Constraints - `X_train`, `Y_train`, `X_test`, and `Y_test` will always be valid numpy arrays with appropriate shapes. - `n_components` will be a positive integer less than or equal to the minimum of `(n_samples, n_features)` in the training data. # Notes - You are not required to perform any data preprocessing (like scaling or centering), assume the inputs are already preprocessed. - Ensure you handle the model fitting, prediction, and RMSE computation efficiently.","solution":"from sklearn.cross_decomposition import PLSRegression from sklearn.metrics import mean_squared_error import numpy as np def train_and_evaluate_pls(X_train: np.ndarray, Y_train: np.ndarray, X_test: np.ndarray, Y_test: np.ndarray, n_components: int) -> float: Trains a PLSRegression model and evaluates its performance on the test set using RMSE. Parameters: - X_train (np.ndarray): Training feature matrix of shape (n_samples, n_features). - Y_train (np.ndarray): Training target matrix of shape (n_samples, n_targets). - X_test (np.ndarray): Testing feature matrix of shape (m_samples, n_features). - Y_test (np.ndarray): Testing target matrix of shape (m_samples, n_targets). - n_components (int): Number of components to keep for the PLS model. Returns: - float: The RMSE of the model predictions on the test set. # Initialize and train the PLSRegression model pls = PLSRegression(n_components=n_components) pls.fit(X_train, Y_train) # Predict the targets for the test data Y_pred = pls.predict(X_test) # Calculate the RMSE rmse = np.sqrt(mean_squared_error(Y_test, Y_pred)) return rmse"},{"question":"Objective Implement a function to securely manage passwords using Python\'s cryptographic services. Your function should perform the following tasks: 1. **Hash a password** using a secure hashing algorithm. 2. **Generate a secure random salt** using the `secrets` module. 3. **Combine the salt and the hashed password** for storage. 4. **Verify a password** by comparing the stored hash with a newly hashed password input, using the stored salt. Function Signature ```python def store_password(password: str) -> tuple: Hashes a password and returns a tuple containing the salt and the hashed password. Args: - password (str): The plaintext password to hash. Returns: - tuple: A tuple containing the generated salt and the hashed password. def verify_password(stored_salt: str, stored_hash: str, password: str) -> bool: Verifies a password by comparing the stored hash with a newly hashed password input. Args: - stored_salt (str): The salt that was stored alongside the hashed password. - stored_hash (str): The hashed password that was stored. - password (str): The plaintext password to verify. Returns: - bool: True if the password is correct, False otherwise. ``` Constraints and Requirements 1. **Hashing Algorithm**: Use the `hashlib` library and a secure algorithm like `SHA256`. 2. **Salt**: Generate a secure random salt using the `secrets` module. Ensure the salt is of a reasonable length (e.g., 16 bytes). 3. **Combination**: When storing the result, combine the salt and the hashed password in a way that they can be easily separated later for verification. 4. **Verification**: The verification function should re-compute the hash using the provided password and stored salt and then compare it with the stored hash to check if they match. Example Usage ```python # Storing a password salt, hashed_password = store_password(\\"my_secure_password\\") # Verifying a password assert verify_password(salt, hashed_password, \\"my_secure_password\\") == True assert verify_password(salt, hashed_password, \\"wrong_password\\") == False ``` Performance Requirements - The functions should be efficient enough to handle multiple calls within a reasonable time frame, given typical password lengths that do not exceed 256 characters.","solution":"import hashlib import secrets def store_password(password: str) -> tuple: Hashes a password and returns a tuple containing the salt and the hashed password. Args: - password (str): The plaintext password to hash. Returns: - tuple: A tuple containing the generated salt and the hashed password. # Generate a secure random salt salt = secrets.token_hex(16) # Create the hash using SHA256 hashed_password = hashlib.sha256((salt + password).encode()).hexdigest() return (salt, hashed_password) def verify_password(stored_salt: str, stored_hash: str, password: str) -> bool: Verifies a password by comparing the stored hash with a newly hashed password input. Args: - stored_salt (str): The salt that was stored alongside the hashed password. - stored_hash (str): The hashed password that was stored. - password (str): The plaintext password to verify. Returns: - bool: True if the password is correct, False otherwise. # Re-create the hash using the stored salt and the provided password new_hash = hashlib.sha256((stored_salt + password).encode()).hexdigest() # Compare the newly created hash with the stored hash return new_hash == stored_hash"},{"question":"**Coding Question** # Objective: You are tasked with writing a Python function that dynamically adds a list of directories to the Python module search path (`sys.path`), processes `.pth` files in the given directories to include their paths, and ensures specific custom site-packages directories are properly handled. # Problem Statement: Implement a function `customize_site(directories: List[str], custom_site_packages: Optional[str] = None) -> List[str]` which: 1. Takes a list of directory paths (`directories`) to be added to the Python module search path. 2. Optional: Takes a custom site-packages directory (`custom_site_packages`). 3. Adds each directory in `directories` to `sys.path`. 4. Processes `.pth` files found in the new directories to add their paths to `sys.path`. 5. If `custom_site_packages` is provided, adds it to `sys.path` and processes its `.pth` files. 6. Returns the updated `sys.path`. Input Format: - `directories`: A list of strings, where each string is an absolute path to a directory. - `custom_site_packages`: An optional string, which is an absolute path to a custom site-packages directory. Output Format: - A list of strings representing the updated `sys.path`. Constraints: - Ensure that directories and paths added to `sys.path` are unique. - The `.pth` files may not exist, and the function should handle such cases gracefully. - If any directory in `directories` or `custom_site_packages` does not exist, it should not be added to `sys.path`. # Example: ```python def customize_site(directories: List[str], custom_site_packages: Optional[str] = None) -> List[str]: # Your implementation here # Example usage: directories = [ \\"/example/custom/python/lib/python3.10/site-packages\\", \\"/example/another/path\\" ] custom_site_packages = \\"/example/custom/site-packages\\" updated_sys_path = customize_site(directories, custom_site_packages) # The function returns the updated sys.path and ensures the correct processing of .pth files. print(updated_sys_path) ``` # Notes: - You should use `site.addsitedir(sitedir, known_paths=None)` to add directories and process `.pth` files. - Use built-in libraries such as `os` and `sys` to handle file system checks and `sys.path` manipulation. - Ensure that the function is robust and handles edge cases like non-existing directories and invalid paths gracefully.","solution":"import os import sys from typing import List, Optional import site def customize_site(directories: List[str], custom_site_packages: Optional[str] = None) -> List[str]: Adds given directories and custom site-packages to sys.path and processes .pth files. Parameters: - directories (List[str]): List of directory paths to be added to sys.path. - custom_site_packages (Optional[str]): Optional custom site-packages directory path. Returns: - List[str]: The updated sys.path. added_paths = set() def add_directory_to_sys_path(directory: str): if os.path.exists(directory) and directory not in added_paths: site.addsitedir(directory) added_paths.add(directory) for directory in directories: add_directory_to_sys_path(directory) if custom_site_packages: add_directory_to_sys_path(custom_site_packages) return sys.path"},{"question":"# Abstract Base Class Implementation and Usage **Objective:** Design a class hierarchy using the `abc` module to create and manage abstract base classes. This question will test your understanding of ABCs, abstract methods, and the `abc` module. **Problem Statement:** You are to design a class hierarchy for a simple shape drawing application. The base class will be an abstract class named `Shape` which defines the following abstract methods and properties: 1. **Abstract Methods:** - `calculate_area()`: Should be implemented by subclasses to calculate the area of the shape. - `calculate_perimeter()`: Should be implemented by subclasses to calculate the perimeter of the shape. 2. **Abstract Property:** - `name`: Should be implemented by subclasses to return the name of the shape as a string. Subclasses of `Shape` will include: - `Circle`: Requires a radius for initialization. - `Square`: Requires a side length for initialization. Each subclass should implement the abstract methods and properties. **Instructions:** 1. Define the abstract base class `Shape` using the `ABCMeta` metaclass from the abc module. 2. Define the subclasses `Circle` and `Square` that inherit from `Shape`. 3. Each subclass should provide concrete implementations of the abstract methods and property. **Constraints:** - The radius and side length will be positive floating-point numbers. - Use the value of π (pi) approximately as 3.14 for calculations in `Circle`. **Expected Function and Property Signatures:** ```python from abc import ABC, abstractmethod class Shape(ABC): @property @abstractmethod def name(self): pass @abstractmethod def calculate_area(self): pass @abstractmethod def calculate_perimeter(self): pass class Circle(Shape): def __init__(self, radius: float): self.radius = radius @property def name(self): return \'Circle\' def calculate_area(self): return 3.14 * (self.radius ** 2) def calculate_perimeter(self): return 2 * 3.14 * self.radius class Square(Shape): def __init__(self, side_length: float): self.side_length = side_length @property def name(self): return \'Square\' def calculate_area(self): return self.side_length ** 2 def calculate_perimeter(self): return 4 * self.side_length ``` **Example Usage:** ```python circle = Circle(2.5) print(circle.name) # Output: \'Circle\' print(circle.calculate_area()) # Output: 19.625 print(circle.calculate_perimeter()) # Output: 15.7 square = Square(3.0) print(square.name) # Output: \'Square\' print(square.calculate_area()) # Output: 9.0 print(square.calculate_perimeter()) # Output: 12.0 ``` Ensure your implementation adheres to the above requirements and signatures. **Evaluation Criteria:** - Correct implementation of the abstract base class and abstract methods. - Proper use of the `abc` module and its functionality. - Accurate calculations of area and perimeter for each shape. - Proper use of properties and methods. Submit your implementation of `Shape`, `Circle`, and `Square` classes.","solution":"from abc import ABC, abstractmethod class Shape(ABC): @property @abstractmethod def name(self): pass @abstractmethod def calculate_area(self): pass @abstractmethod def calculate_perimeter(self): pass class Circle(Shape): def __init__(self, radius: float): self.radius = radius @property def name(self): return \'Circle\' def calculate_area(self): return 3.14 * (self.radius ** 2) def calculate_perimeter(self): return 2 * 3.14 * self.radius class Square(Shape): def __init__(self, side_length: float): self.side_length = side_length @property def name(self): return \'Square\' def calculate_area(self): return self.side_length ** 2 def calculate_perimeter(self): return 4 * self.side_length"},{"question":"# Question: Classifying Iris Species Using Scikit-Learn Objective Use the `scikit-learn` library to build a machine learning model for classifying iris species from the classic Iris dataset. Implement a function `iris_classifier_pipeline()` that creates and evaluates a machine learning pipeline. Function Signature ```python def iris_classifier_pipeline(random_state: int) -> dict: pass ``` Input - `random_state` (int): An integer to set the seed for random number generation, ensuring reproducibility of the results. Output - Returns a dictionary, with the following keys: - `model`: The trained machine learning model. - `accuracy`: The accuracy of the model on a held-out test set as a float. - `classification_report`: A detailed classification report as a dictionary. Constraints - You must use the Iris dataset provided by `scikit-learn`. - Use a train-test split ratio of 80:20. - The random state provided must be passed to the train-test split and model (if applicable) to ensure reproducibility. Requirements 1. Load the Iris dataset using `load_iris()`. 2. Split the dataset into an 80:20 train-test split. 3. Implement a machine learning pipeline using `scikit-learn`. The pipeline should include: - Standardization of features using `StandardScaler`. - A classification model (you can choose any model: Logistic Regression, Decision Tree, Random Forest, etc.). 4. Train the pipeline on the training data. 5. Evaluate the pipeline on the test data and calculate the accuracy. 6. Generate a detailed classification report using `classification_report` from `scikit-learn`. # Example ```python result = iris_classifier_pipeline(random_state=42) assert \'model\' in result assert \'accuracy\' in result assert \'classification_report\' in result ``` The `classification_report` key in the returned dictionary should have values similar to the output of `sklearn.metrics.classification_report` method.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report def iris_classifier_pipeline(random_state: int) -> dict: # Load the Iris dataset iris = load_iris() X, y = iris.data, iris.target # Split the dataset into an 80:20 train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state) # Create a pipeline with StandardScaler and RandomForestClassifier pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'classifier\', RandomForestClassifier(random_state=random_state)) ]) # Train the pipeline on the training data pipeline.fit(X_train, y_train) # Make predictions on the test data y_pred = pipeline.predict(X_test) # Calculate the accuracy accuracy = accuracy_score(y_test, y_pred) # Generate a detailed classification report report = classification_report(y_test, y_pred, output_dict=True) return { \'model\': pipeline, \'accuracy\': accuracy, \'classification_report\': report }"},{"question":"**Objective:** Demonstrate the ability to create and manage capsules in Python, encapsulating C pointers and providing Python-accessible APIs to interact with the encapsulated data. **Task:** Write a Python C extension module that: 1. **Creates a Capsule:** - Implement a function `create_capsule` that takes a number (representing an integer pointer) and a name as arguments and returns a `PyCapsule` encapsulating this integer pointer. 2. **Accesses the Capsule Data:** - Implement a function `get_pointer_value` that takes a `PyCapsule` and retrieves the value stored in it. - Implement a function `get_capsule_name` that takes a `PyCapsule` and retrieves the name stored in it. 3. **Validates the Capsule:** - Implement a function `is_capsule_valid` that takes a `PyCapsule` and a name, and returns whether the capsule is valid and the name matches. **Input/Output Format:** 1. `create_capsule(pointer_value: int, name: str) -> PyCapsule` - **Input:** `pointer_value`: An integer value to be stored in capsule, `name`: A string representing the capsule\'s name. - **Output:** A `PyCapsule` object encapsulating the integer pointer. 2. `get_pointer_value(capsule: PyCapsule) -> int` - **Input:** `capsule`: A `PyCapsule` object. - **Output:** The integer value stored in the capsule. 3. `get_capsule_name(capsule: PyCapsule) -> str` - **Input:** `capsule`: A `PyCapsule` object. - **Output:** The name stored in the capsule. 4. `is_capsule_valid(capsule: PyCapsule, name: str) -> bool` - **Input:** `capsule`: A `PyCapsule` object, `name`: A string to validate against the capsule\'s name. - **Output:** `True` if the capsule is valid and name matches; `False` otherwise. **Constraints:** - The capsule\'s pointer should represent a non-negative integer. - The capsule name should not be an empty string. **Example:** ```python # Assume these functions are implemented in a Python C extension module named \'capsulemod\' import capsulemod # Create a capsule capsule = capsulemod.create_capsule(42, \\"test.capsule\\") # Access capsule data pointer_value = capsulemod.get_pointer_value(capsule) assert pointer_value == 42 # Access capsule name name = capsulemod.get_capsule_name(capsule) assert name == \\"test.capsule\\" # Validate capsule is_valid = capsulemod.is_capsule_valid(capsule, \\"test.capsule\\") assert is_valid == True # Validate with wrong name is_valid = capsulemod.is_capsule_valid(capsule, \\"wrong.name\\") assert is_valid == False ``` **Performance Requirements:** - The implementation should handle capsule creation and access operations in constant time, O(1).","solution":"import ctypes def create_capsule(pointer_value, name): Creates a capsule containing the given integer pointer and name. Args: pointer_value (int): The integer value to encapsulate. name (str): The name of the capsule. Returns: ctypes.py_object: The capsule object. if not isinstance(pointer_value, int) or not isinstance(name, str): raise TypeError(\\"pointer_value must be int and name must be str\\") if pointer_value < 0: raise ValueError(\\"pointer_value must be non-negative\\") if not name: raise ValueError(\\"name must not be empty\\") capsule = ctypes.py_object((pointer_value, name)) return capsule def get_pointer_value(capsule): Retrieves the integer value stored in the capsule. Args: capsule (ctypes.py_object): The capsule object. Returns: int: The integer value stored in the capsule. return capsule.value[0] def get_capsule_name(capsule): Retrieves the name stored in the capsule. Args: capsule (ctypes.py_object): The capsule object. Returns: str: The name stored in the capsule. return capsule.value[1] def is_capsule_valid(capsule, name): Validates whether the given name matches the capsule\'s name. Args: capsule (ctypes.py_object): The capsule object. name (str): The name to validate. Returns: bool: True if the name is valid; False otherwise. return capsule.value[1] == name"},{"question":"Implement a function `broadcast_and_add` that takes two tensors `tensor_a` and `tensor_b` as input and returns their element-wise sum if they are broadcastable. If the tensors are not broadcastable according to the broadcasting semantics explained below, the function should raise a `ValueError` with an appropriate message. # Input Format - `tensor_a` (torch.Tensor): The first input tensor. - `tensor_b` (torch.Tensor): The second input tensor. # Output Format - torch.Tensor: The element-wise sum of the broadcasted tensors if they are broadcastable. # Constraints 1. Each tensor has at least one dimension. 2. The dimensions of the tensors must either be equal, one of them must be 1, or one of them must not exist for all dimensions starting from the trailing dimension. # Examples Example 1 ```python import torch tensor_a = torch.tensor([[1,2,3], [4,5,6]]) tensor_b = torch.tensor([[1], [1]]) result = broadcast_and_add(tensor_a, tensor_b) print(result) ``` **Output:** ``` tensor([[2, 3, 4], [5, 6, 7]]) ``` Example 2 ```python import torch tensor_a = torch.tensor([[1,2,3], [4,5,6]]) tensor_b = torch.tensor([[1, 2]]) result = broadcast_and_add(tensor_a, tensor_b) ``` **Output:** ``` ValueError: Tensors are not broadcastable ``` # Function Signature ```python import torch def broadcast_and_add(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: # Your code here pass ``` # Explanation The function `broadcast_and_add` should: 1. Examine if the input tensors `tensor_a` and `tensor_b` are broadcastable. 2. If they are broadcastable, calculate and return the result of their element-wise addition. 3. If they are not broadcastable, raise a `ValueError` with the message \\"Tensors are not broadcastable\\". Utilize the rules of broadcasting as defined in the provided documentation section to implement the function.","solution":"import torch def broadcast_and_add(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor: Returns the element-wise sum of the broadcasted tensors if they are broadcastable. Parameters: tensor_a: torch.Tensor - The first input tensor. tensor_b: torch.Tensor - The second input tensor. Returns: torch.Tensor - The element-wise sum of the broadcasted tensors. Raises: ValueError - If the tensors are not broadcastable. try: # Use torch\'s broadcasting rules to add the tensors result = tensor_a + tensor_b return result except RuntimeError: raise ValueError(\\"Tensors are not broadcastable\\")"},{"question":"# Question: Implementing File Compression and Decompression Using Custom Filters You are tasked with implementing a Python function to compress a text file using LZMA compression with custom filters, and another function to decompress the resulting file. Specifications 1. Implement the function `compress_file_with_filters(input_file: str, output_file: str, filters: list) -> None`: - **Input:** - `input_file`: a string representing the path to the input text file to be compressed. - `output_file`: a string representing the path to the output compressed file. - `filters`: a list of dictionaries representing the custom filter chain to be used for compression. - **Output:** - None. The function should read the contents of `input_file`, compress it using `lzma` with the specified filters, and write the compressed data to `output_file`. 2. Implement the function `decompress_file(input_file: str, output_file: str) -> None`: - **Input:** - `input_file`: a string representing the path to the input compressed file. - `output_file`: a string representing the path to the decompressed output file. - **Output:** - None. The function should read the compressed contents of `input_file`, decompress it using `lzma`, and write the decompressed data to `output_file`. Constraints - The `filters` parameter in `compress_file_with_filters` cannot be empty and must contain valid filter configurations as described in the documentation. - The input text file size should not exceed 10 MB. - Assume the input text file contains plain text data. Example Usage ```python import lzma def compress_file_with_filters(input_file, output_file, filters): data = \\"\\" # Read the input file with open(input_file, \'rt\', encoding=\'utf-8\') as f: data = f.read() # Compress the data with custom filters compressed_data = None with lzma.open(output_file, \'wb\', filters=filters) as f: compressed_data = f.write(data.encode(\'utf-8\')) def decompress_file(input_file, output_file): decompressed_data = \\"\\" # Decompress the input file with lzma.open(input_file, \'rb\') as f: decompressed_data = f.read() # Write the decompressed data to output file with open(output_file, \'wt\', encoding=\'utf-8\') as f: f.write(decompressed_data.decode(\'utf-8\')) # Example filter chain filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 1}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 7 | lzma.PRESET_EXTREME} ] # Example calls compress_file_with_filters(\'sample_input.txt\', \'compressed_output.xz\', filters) decompress_file(\'compressed_output.xz\', \'decompressed_output.txt\') ``` # Your Implementation Implement the functions `compress_file_with_filters` and `decompress_file` according to the specifications above. Ensure that your code handles edge cases and errors appropriately.","solution":"import lzma def compress_file_with_filters(input_file: str, output_file: str, filters: list) -> None: with open(input_file, \'rt\', encoding=\'utf-8\') as f: data = f.read() with lzma.open(output_file, \'wb\', filters=filters) as f: f.write(data.encode(\'utf-8\')) def decompress_file(input_file: str, output_file: str) -> None: with lzma.open(input_file, \'rb\') as f: decompressed_data = f.read() with open(output_file, \'wt\', encoding=\'utf-8\') as f: f.write(decompressed_data.decode(\'utf-8\'))"},{"question":"Objective: Demonstrate your understanding of the `warnings` module to control and handle different kinds of warnings generated in a Python program. Problem Statement: Write a Python function `manage_warnings` that receives a list of tuples and a test function, where each tuple contains: 1. A string `action` specifying what action to take on a matching warning (e.g., \\"ignore\\", \\"error\\", \\"always\\"). 2. A string `message` containing a regular expression that the warning message must match. 3. A subclass of `Warning` specifying the category of the warning (e.g., `UserWarning`, `DeprecationWarning`). 4. A string `module` containing a regular expression that the module name must match. Additionally, the test function `test_warnings` should be executed within a context that applies these warning filters. The `manage_warnings` function should return a list of all the warnings generated during the execution of `test_warnings`. Constraints: - The `test_warnings` function, when called, will not take any arguments and may generate warnings using the `warnings.warn` function. - You must use the `warnings.catch_warnings` context manager to capture and return the warnings. - The list of tuples will have at most 10 filters. - Each warning can match multiple filters; however, filters listed later take precedence over those listed before them. Input: - `filters`: A list of tuples, where each tuple contains four elements: `action` (str), `message` (str), `category` (Warning subclass), and `module` (str). - `test_warnings`: A function that generates warnings when executed. Output: - A list of caught warning records, where each record is an object with attributes `message`, `category`, `filename`, `lineno`, `file`, and `line`. Example: ```python import warnings def test_warnings(): warnings.warn(\\"This is a user warning\\", UserWarning) warnings.warn(\\"This is a deprecation warning\\", DeprecationWarning) filters = [ (\\"error\\", \\".*deprecation.*\\", DeprecationWarning, \\".*\\"), (\\"ignore\\", \\".*user.*\\", UserWarning, \\"test_module\\") ] result = manage_warnings(filters, test_warnings) # Expected output: a list of warning record(s) # Note: Exception should be captured for DeprecationWarning, and UserWarning should be ignored ``` Implementation: Write the function `manage_warnings` to fulfill the above requirements. ```python import warnings def manage_warnings(filters, test_warnings): with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(\\"default\\") for action, message, category, module in filters: warnings.filterwarnings(action, message=message, category=category, module=module) try: test_warnings() except Warning as w: caught_warnings.append(w) return caught_warnings ```","solution":"import warnings def manage_warnings(filters, test_warnings): with warnings.catch_warnings(record=True) as caught_warnings: warnings.simplefilter(\\"default\\") for action, message, category, module in filters: warnings.filterwarnings(action, message=message, category=category, module=module) test_warnings() return caught_warnings"},{"question":"**Problem: Advanced Logging Configuration** You are tasked with setting up a complex logging configuration for an application that consists of multiple modules. Your objective is to write a logging setup script that configures different loggers for each module, routes log messages to various destinations based on their severity, applies custom formats, and includes demonstration code to showcase the logging in action. # Objectives: 1. **Configure Loggers**: - Configure a root logger that logs messages of level `DEBUG` and higher to the console, formatting the messages to include the timestamp and severity level. - Create additional loggers for two modules: `module_a` and `module_b`, each logging to separate files (`module_a.log` and `module_b.log`). - `module_a` should log messages of level `INFO` and above, while `module_b` should log messages of level `WARNING` and above. 2. **Use Custom Formats**: - Customize the log format for the root logger to `\\"%(asctime)s - %(levelname)s - %(message)s\\"`. - Customize the log format for `module_a` and `module_b` to include the logger name, timestamp, and log message as `\\"%(name)s - %(asctime)s - %(message)s\\"`. 3. **Implement Example Code**: - Write example code in `module_a.py` and `module_b.py` that demonstrates logging at various levels: `DEBUG`, `INFO`, `WARNING`, `ERROR`, and `CRITICAL`. # Input: - **No direct input**: Your script should set up the logging configuration purely using Python code. # Output: - **Log Files and Console Output**: Verify the configuration through console output and the content of the log files `module_a.log` and `module_b.log`. # Constraints: - Use the Python `logging` module for all configurations. - Ensure that no log messages from `module_a.py` and `module_b.py` appear in the console output but are correctly routed to their respective log files. # Instructions: 1. **Create the Main Logging Configuration**: ```python import logging def setup_logging(): # Root logger configuration logging.basicConfig( format=\'%(asctime)s - %(levelname)s - %(message)s\', level=logging.DEBUG ) # Logger for module_a logger_a = logging.getLogger(\'module_a\') logger_a.setLevel(logging.INFO) handler_a = logging.FileHandler(\'module_a.log\', mode=\'w\') formatter_a = logging.Formatter(\'%(name)s - %(asctime)s - %(message)s\') handler_a.setFormatter(formatter_a) logger_a.addHandler(handler_a) # Logger for module_b logger_b = logging.getLogger(\'module_b\') logger_b.setLevel(logging.WARNING) handler_b = logging.FileHandler(\'module_b.log\', mode=\'w\') formatter_b = logging.Formatter(\'%(name)s - %(asctime)s - %(message)s\') handler_b.setFormatter(formatter_b) logger_b.addHandler(handler_b) setup_logging() ``` 2. **Implement logging in `module_a.py`**: ```python import logging logger = logging.getLogger(\'module_a\') def perform_task(): logger.debug(\'This is a DEBUG message\') logger.info(\'This is an INFO message\') logger.warning(\'This is a WARNING message\') logger.error(\'This is an ERROR message\') logger.critical(\'This is a CRITICAL message\') if __name__ == \'__main__\': perform_task() ``` 3. **Implement logging in `module_b.py`**: ```python import logging logger = logging.getLogger(\'module_b\') def perform_task(): logger.debug(\'This is a DEBUG message\') logger.info(\'This is an INFO message\') logger.warning(\'This is a WARNING message\') logger.error(\'This is an ERROR message\') logger.critical(\'This is a CRITICAL message\') if __name__ == \'__main__\': perform_task() ``` # Verification: - Run the main script containing `setup_logging()` and then run `module_a.py` and `module_b.py` separately to verify the log outputs. - Check `module_a.log`, `module_b.log`, and console output to ensure the logging configuration works as expected.","solution":"import logging def setup_logging(): # Root logger configuration logging.basicConfig( format=\'%(asctime)s - %(levelname)s - %(message)s\', level=logging.DEBUG ) # Create a null handler for the root logger to suppress propagation to console null_handler = logging.NullHandler() logging.getLogger().addHandler(null_handler) # Logger for module_a logger_a = logging.getLogger(\'module_a\') logger_a.setLevel(logging.INFO) handler_a = logging.FileHandler(\'module_a.log\', mode=\'w\') formatter_a = logging.Formatter(\'%(name)s - %(asctime)s - %(message)s\') handler_a.setFormatter(formatter_a) logger_a.addHandler(handler_a) # Logger for module_b logger_b = logging.getLogger(\'module_b\') logger_b.setLevel(logging.WARNING) handler_b = logging.FileHandler(\'module_b.log\', mode=\'w\') formatter_b = logging.Formatter(\'%(name)s - %(asctime)s - %(message)s\') handler_b.setFormatter(formatter_b) logger_b.addHandler(handler_b) setup_logging()"},{"question":"# PairGrid Visualization of a Custom Dataset You are provided with a dataset stored in a CSV file named `student_scores.csv`, which contains the following columns: - `Gender` (Categorical, with values \\"Male\\" or \\"Female\\") - `Math` (Numerical) - `Reading` (Numerical) - `Writing` (Numerical) Write a Python function using `seaborn.PairGrid` to visualize the relationships between the `Math`, `Reading`, and `Writing` scores. Your function should do the following: 1. Load the `student_scores.csv` dataset into a pandas DataFrame. 2. Use the `seaborn.PairGrid` class to set up a grid with `Math`, `Reading`, and `Writing`. 3. On the diagonal, plot a histogram of each score. 4. On the lower triangle, plot kernel density estimates. 5. On the upper triangle, plot scatter plots colored by `Gender`. 6. Include a legend to distinguish between the `Gender` categories. The function should not return anything but should display the resulting PairGrid plot. **Function Signature:** ```python def plot_student_scores(filepath: str) -> None: pass ``` **Expected Input:** - `filepath` (str): The file path of the `student_scores.csv`. **Example:** ```python plot_student_scores(\'path/to/student_scores.csv\') ``` **Note:** - Ensure that all necessary libraries are imported and handle any potential file reading errors. - Your code should be efficient and make use of seaborn\'s capabilities fully.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_student_scores(filepath: str) -> None: try: # Load the dataset df = pd.read_csv(filepath) # Set up the PairGrid g = sns.PairGrid(df, vars=[\\"Math\\", \\"Reading\\", \\"Writing\\"], hue=\\"Gender\\") # Map the plots to the grid g.map_diag(sns.histplot, kde=False) g.map_lower(sns.kdeplot, fill=True) g.map_upper(sns.scatterplot) # Add a legend g.add_legend() # Show the plot plt.show() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"**Question: Encoding and Binary Manipulation with Codecs and Struct** You are tasked with writing a function `process_data` that takes in three parameters: - `data` (str): A string of text that needs to be encoded. - `encoding` (str): The name of the encoding to use (e.g., \'utf-8\', \'utf-16\'). - `format_string` (str): A format string for interpreting binary data using the `struct` module (e.g., \'I 2s f\'). The function should: 1. Encode the input string `data` using the provided `encoding`. 2. Interpret the encoded bytes according to the provided `format_string` using the `struct` module. 3. Return the result as a tuple of decoded data. # Function Signature ```python def process_data(data: str, encoding: str, format_string: str) -> tuple: pass ``` # Input - `data`: A string of text (1 ≤ len(data) ≤ 100). - `encoding`: A string representing the encoding type (\'utf-8\', \'utf-16\', etc.). - `format_string`: A format string for the `struct` module (valid format characters). # Output - A tuple containing the unpacked data according to the format string. # Example ```python data = \\"Hello\\" encoding = \\"utf-8\\" format_string = \\"5s\\" output = process_data(data, encoding, format_string) print(output) # Expected: (b\'Hello\',) ``` # Constraints 1. The encoding provided will be a valid encoding type supported by Python\'s `codecs` module. 2. The `format_string` will be a valid format string supported by the `struct` module. 3. The encoded data size will match the specifications of the provided `format_string`. # Notes - Utilize the `codecs` module for encoding the data. - Use the `struct` module to unpack the encoded bytes according to the format string. - Ensure the implementation handles various encodings and format strings effectively. # Hints - You might find `codecs.encode` useful for encoding the string. - The `struct.unpack` method will help in interpreting the bytes as per the format string.","solution":"import struct def process_data(data: str, encoding: str, format_string: str) -> tuple: Encodes the input string using the provided encoding and interprets the encoded bytes according to the provided format string. Parameters: data (str): The input string to encode. encoding (str): The encoding to use for the string. format_string (str): The struct module format string for interpreting the encoded bytes. Returns: tuple: A tuple of unpacked data as per the format string. # Encode the string data using the specified encoding encoded_data = data.encode(encoding) # Unpack the encoded bytes using the struct format string unpacked_data = struct.unpack(format_string, encoded_data[:struct.calcsize(format_string)]) return unpacked_data"},{"question":"# Seaborn Coding Assessment Question **Objective:** Create a customized scatter plot using Seaborn\'s objects interface, incorporating various properties as detailed below. **Task:** Write a function `custom_scatter_plot(data)` that generates a scatter plot with the following specifications: 1. **Plot Configuration:** - Use the `seaborn.objects` interface. - Horizontal axis (x-axis) should be a continuous variable. - Vertical axis (y-axis) should be a continuous variable. 2. **Coordinate Properties:** - Specify which columns in the `data` DataFrame to use for the x and y coordinates. Use columns `x_var` and `y_var`. 3. **Color Properties:** - Distinguish data points by a categorical variable using a nominal color scale. Use the `category_var` column in the DataFrame. 4. **Alpha Properties:** - Set a fixed transparency level of `0.6` for all points. 5. **Marker and Size Properties:** - Use different markers for different categories in the `category_var`. - Size of the markers should correspond to the `size_var` column in the DataFrame. 6. **Text Annotations:** - Add text annotations corresponding to `label_var` column at the location of each point. 7. **Use of Styles:** - Customize the markers to enhance visual clarity. 8. **Additional Customizations:** - Configure axis labels and title for clarity. - Apply any other necessary styles using Seaborn\'s theming options. **Input:** - `data` (DataFrame): A pandas DataFrame containing at least five columns named `x_var`, `y_var`, `category_var`, `size_var`, and `label_var`. **Output:** - The function should plot the graph inline if running in a Jupyter environment or display it using `matplotlib`\'s `show` function. # Constraints: - Ensure your function is efficient and leverages the functionalities provided by the Seaborn library. - Handle any potential exceptions or errors gracefully. Here is an example function definition: ```python import numpy as np import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def custom_scatter_plot(data): # Step 1: Create a plot object plot = so.Plot(data, x=\'x_var\', y=\'y_var\') # Step 2: Add scatter marks based on the given properties plot.add(so.Dot(marker=\\"o\\", pointsize=\\"size_var\\", color=\\"category_var\\", alpha=0.6)) # Step 3: Add text annotations for i, row in data.iterrows(): plt.text(row[\'x_var\'], row[\'y_var\'], row[\'label_var\'], fontsize=9, va=\'top\') # Step 4: Apply style modifications for readability plot.theme({\'axes.labelsize\': 14, \'axes.titlesize\': 16}) plot.label(x=\'X Axis Label\', y=\'Y Axis Label\', title=\'Scatter Plot with Custom Styling\') # Step 5: Render plot plot.plot() # Example usage: # df = pd.DataFrame({ # \'x_var\': np.random.rand(20), # \'y_var\': np.random.rand(20), # \'category_var\': np.random.choice([\'A\', \'B\', \'C\'], size=20), # \'size_var\': np.random.randint(10, 100, size=20), # \'label_var\': list(\'abcdefghijklmnopqrst\') # }) # custom_scatter_plot(df) ``` **Notes:** - Ensure you have `seaborn` (version 0.11.0 or later) installed in your environment. - Validate the functionality with a sample DataFrame before submission.","solution":"import numpy as np import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def custom_scatter_plot(data): Generates a customized scatter plot using Seaborn\'s objects interface. Parameters: data (DataFrame): A pandas DataFrame containing at least five columns named `x_var`, `y_var`, `category_var`, `size_var`, and `label_var`. # Step 1: Create a Seaborn plot object with specified data and axes plot = so.Plot(data, x=\'x_var\', y=\'y_var\') # Step 2: Add scatter marks with custom properties scatter = so.Dot(marker={\\"category_var\\": \\"category_var\\"}, pointsize=\\"size_var\\", color=\\"category_var\\", alpha=0.6) plot.add(scatter) # Step 3: Add text annotations for each point for i, row in data.iterrows(): plt.text(row[\'x_var\'], row[\'y_var\'], row[\'label_var\'], fontsize=9, ha=\'right\') # Step 4: Apply style modifications for better readability plot.theme({\'axes.labelsize\': 14, \'axes.titlesize\': 16}) plot.label(x=\'X Axis Label\', y=\'Y Axis Label\', title=\'Customized Scatter Plot\') # Step 5: Render and display the plot plot.show()"},{"question":"**Question:** You are given a dataset of diamond characteristics with columns like `carat`, `cut`, `color`, `clarity`, `depth`, `table`, `price`, `x`, `y`, and `z`. Using Seaborn\'s `objects` module, create a plot that compares the average carat weight of diamonds across different clarity grades. Additionally, include 95% confidence intervals for these estimates and ensure the plot is reproducible by setting a random seed. Your final plot should: 1. Display the clarity grades along the x-axis and the average carat weight along the y-axis. 2. Include 95% confidence intervals for the mean estimates. 3. Be reproducible by setting the random seed to 42. **Input:** - A Pandas DataFrame `diamonds` with attributes of diamonds. **Output:** - A plot that meets the above specifications. **Constraints:** - Use Seaborn\'s `objects` module. - Ensure 95% confidence intervals using bootstrapping. - Set the random seed to `42` to ensure reproducibility. **Example:** You can start by loading the dataset and creating the necessary plot. ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset diamonds = load_dataset(\\"diamonds\\") # Create plot p = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") p.add(so.Range(), so.Est(seed=42)) p.show() ``` This code will generate a plot where the x-axis represents different clarity grades, the y-axis shows the average carat weight, and error bars represent the 95% confidence intervals for these averages.","solution":"import seaborn.objects as so import pandas as pd import matplotlib.pyplot as plt def plot_diamond_clarity_vs_carat(diamonds: pd.DataFrame): Plots the average carat weight of diamonds across different clarity grades. Includes 95% confidence intervals for these estimates. Ensures reproducibility by setting a random seed. # Ensure reproducibility seed_value = 42 # Create the plot p = so.Plot(diamonds, x=\\"clarity\\", y=\\"carat\\") p.add(so.Range(), so.Est(seed=seed_value)) # Show the plot plt.figure(figsize=(10, 6)) p.show()"},{"question":"Coding Assessment Question # Objective Design a Python program that allows storing and retrieving Python objects into/from an SQLite database. This will require implementing custom serialization and deserialization functions using the `pickle` module and registering these functions appropriately. The SQLite database should store the serialized objects. # Problem Statement Your task is to implement a class `PersistentObjectStore` that supports storing and retrieving Python objects. The class should interact with an SQLite database and handle object serialization and deserialization using the `pickle` module. # Class Definition ```python class PersistentObjectStore: def __init__(self, db_path: str): Initialize the PersistentObjectStore with the given SQLite database file path. :param db_path: The file path to the SQLite database. pass def store_object(self, key: str, obj: object) -> None: Store the given object in the database with the associated key. :param key: The key under which the object is stored. :param obj: The Python object to store. pass def retrieve_object(self, key: str) -> object: Retrieve the object associated with the given key from the database. :param key: The key of the object to retrieve. :return: The retrieved Python object. pass ``` # Requirements 1. **Initialization (`__init__` method):** - The constructor accepts a string `db_path` which specifies the path to the SQLite database file. - The database should have a single table `objects` with two columns `key` (TEXT) and `value` (BLOB). 2. **Storing Objects (`store_object` method):** - This method accepts a string `key` and a Python `obj`. - Serialize the object using the `pickle` module. - Store the serialized object in the `objects` table with the associated key. 3. **Retrieving Objects (`retrieve_object` method):** - This method accepts a string `key`. - Retrieve the associated serialized object from the `objects` table. - Deserialize the object using the `pickle` module. - Return the deserialized Python object. # Constraints 1. Only standard and built-in Python objects should be supported for serialization and deserialization. 2. Handle exceptions appropriately, such as when attempting to retrieve an object that does not exist. # Example Usage ```python # Example usage of PersistentObjectStore store = PersistentObjectStore(\'persistent_store.db\') # Storing an object store.store_object(\'my_list\', [1, 2, 3, 4, 5]) # Retrieving the stored object retrieved_list = store.retrieve_object(\'my_list\') print(retrieved_list) # Output: [1, 2, 3, 4, 5] ``` # Additional Information 1. Make sure to handle database connections efficiently, ensuring they are closed when not in use. 2. Consider edge cases, such as inserting duplicate keys or retrieving non-existing keys. # Performance Considerations - Ensure that storing and retrieving objects are time-efficient operations. - The solution should handle large objects gracefully without significant performance degradation.","solution":"import sqlite3 import pickle class PersistentObjectStore: def __init__(self, db_path: str): Initialize the PersistentObjectStore with the given SQLite database file path. Create the database table if it does not exist. :param db_path: The file path to the SQLite database. self.db_path = db_path self._initialize_db() def _initialize_db(self): Initialize the database by creating the \'objects\' table if it does not exist. with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS objects ( key TEXT PRIMARY KEY, value BLOB ) \'\'\') conn.commit() def store_object(self, key: str, obj: object) -> None: Store the given object in the database with the associated key by serializing the object using pickle. :param key: The key under which the object is stored. :param obj: The Python object to store. serialized_obj = pickle.dumps(obj) with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'\'\' REPLACE INTO objects (key, value) VALUES (?, ?) \'\'\', (key, serialized_obj)) conn.commit() def retrieve_object(self, key: str) -> object: Retrieve the object associated with the given key from the database by deserializing the stored blob. :param key: The key of the object to retrieve. :return: The retrieved Python object. with sqlite3.connect(self.db_path) as conn: cursor = conn.cursor() cursor.execute(\'SELECT value FROM objects WHERE key = ?\', (key,)) result = cursor.fetchone() if result is not None: return pickle.loads(result[0]) else: raise KeyError(f\\"No object found with key: {key}\\")"},{"question":"# Memory Mapping Assignment In this assignment, you are required to demonstrate your understanding and ability to use Python\'s `mmap` module to handle memory-mapped file operations. Specifically, you are to implement a function that reads, modifies, and writes to a memory-mapped file. # Objective To assess the fundamental and advanced comprehension of the `mmap` module by performing various operations on a memory-mapped file. # Task Implement a function `process_memory_mapped_file(file_path)`, which does the following: 1. **Creates and maps** the entire given file (`file_path`) into memory. 2. **Reads and prints** the first line of the file. 3. **Finds** and prints the position of a given substring (passed as a parameter). 4. **Replaces** all occurrences of a given substring with another given substring and ensures the file is updated. 5. **Prints** the updated content of the entire file. 6. **Ensures** all changes are flushed and the memory mapping is properly closed. # Constraints - The file is assumed to be a text file with enough content to perform meaningful operations. - The replace operation should handle varying lengths of search and replacement substrings. # Inputs and Outputs **Input**: - `file_path` (string): The path to the file that will be memory-mapped. - `search_substring` (string): The substring to search for in the file. - `replace_substring` (string): The substring to replace the search substring with. **Output**: - Prints: - The first line of the file. - The position(s) of the search substring in the file. - The entire content of the file after replacements. # Example Usage ```python def process_memory_mapped_file(file_path, search_substring, replace_substring): # Your implementation here # Example usage: process_memory_mapped_file(\'example.txt\', \'Hello\', \'Hi\') ``` This will map \'example.txt\' file, replace all occurrences of \'Hello\' with \'Hi\', and print the relevant information as specified. # Notes - Make sure to handle exceptions and edge cases (e.g., file not found, substring not in file). - Ensure the memory-mapped file is correctly flushed and closed after operations.","solution":"import mmap def process_memory_mapped_file(file_path, search_substring, replace_substring): with open(file_path, \'r+\') as f: # Memory-map the file, size 0 means whole file mm = mmap.mmap(f.fileno(), 0) try: # Read the first line of the file mm.seek(0) first_line_end = mm.find(b\'n\') + 1 first_line = mm[:first_line_end].decode(\'utf-8\') print(\\"First line: \\", first_line.strip()) # Find and print the position of the search substring positions = [] search_bytes = search_substring.encode(\'utf-8\') pos = mm.find(search_bytes) while pos != -1: positions.append(pos) pos = mm.find(search_bytes, pos + 1) print(f\\"Positions of \'{search_substring}\': \\", positions) # Replace all occurrences of the search substring with the replace substring replace_bytes = replace_substring.encode(\'utf-8\') content = mm[:].replace(search_bytes, replace_bytes) mm.resize(len(content)) mm[:] = content mm.flush() # Print the updated content of the entire file mm.seek(0) updated_content = mm.read().decode(\'utf-8\') print(\\"Updated content:n\\", updated_content) finally: mm.close() # Example usage: # process_memory_mapped_file(\'example.txt\', \'Hello\', \'Hi\')"},{"question":"**Objective**: To assess students\' understanding and ability to implement and use descriptor objects in Python. **Problem Statement**: You are required to implement a custom class in Python that utilizes descriptors to manage its attributes. You will be using the functions described in the provided documentation to achieve this. # Requirements 1. **Custom Descriptor Class**: - Implement a descriptor class `MyDescriptor` with `__get__`, `__set__`, and `__delete__` methods to manage an attribute\'s access and modification. 2. **Custom Class `Person`**: - Create a class `Person` that uses your `MyDescriptor` class to manage the `name` and `age` attributes. - Ensure that the `name` attribute is a string and the `age` attribute is an integer. - Implement proper validation in the descriptors to ensure `name` cannot be empty and `age` must be a positive integer. 3. **Descriptor Creation Functions**: - Use `PyDescr_NewGetSet` and other relevant functions provided in the documentation to create custom descriptors for the `name` and `age` attributes. - Demonstrate the use of these descriptors within your `Person` class. 4. **Testing**: - Write test cases to demonstrate the behavior of the `Person` class, including validation, getting, setting, and deleting attributes. - Ensure that invalid values raise appropriate exceptions. # Input/Output **Input**: - There is no direct input for the function; instead, you will write a class with methods and demonstrate their usage via test cases. **Output**: - Your output will be the results of the test cases, demonstrating the functionality and validation of your descriptors. # Constraints - You must use descriptors to manage `name` and `age` attributes. - Ensure proper exception handling for invalid inputs. - You are only allowed to use standard Python libraries. # Example ```python class MyDescriptor: def __init__(self): self.value = None def __get__(self, instance, owner): return self.value def __set__(self, instance, value): if isinstance(value, str) and value: self.value = value else: raise ValueError(\\"Name must be a non-empty string\\") def __delete__(self, instance): self.value = None class Person: name = MyDescriptor() age = MyDescriptor() def __init__(self, name, age): self.name = name self.age = age # Test Cases try: p = Person(\\"John\\", 30) print(p.name) # Should print \\"John\\" print(p.age) # Should print 30 p.name = \\"Doe\\" print(p.name) # Should print \\"Doe\\" p.age = 25 print(p.age) # Should print 25 p.name = \\"\\" # Should raise ValueError except ValueError as e: print(e) # Should print \\"Name must be a non-empty string\\" ``` Implement the above requirements with proper use of the provided documentation functions.","solution":"class MyDescriptor: def __init__(self, name=None): self.value = None self.name = name def __get__(self, instance, owner): return self.value def __set__(self, instance, value): if self.name == \\"name\\" and isinstance(value, str) and value: self.value = value elif self.name == \\"age\\" and isinstance(value, int) and value > 0: self.value = value else: if self.name == \\"name\\": raise ValueError(\\"Name must be a non-empty string\\") elif self.name == \\"age\\": raise ValueError(\\"Age must be a positive integer\\") def __delete__(self, instance): self.value = None class Person: name = MyDescriptor(\\"name\\") age = MyDescriptor(\\"age\\") def __init__(self, name, age): self.name = name self.age = age"},{"question":"**Problem Statement:** You are developing an inventory management system for a store. Your system needs to efficiently handle inventory updates and provide statistical insights on the inventory items using Python\'s `functools` module. **Task:** 1. Create a class `InventoryItem` that manages an item in the inventory. 2. Implement a method to update the inventory stock. 3. Calculate and cache the average price of an item over time using the `cached_property` decorator. 4. Allow comparison of inventory items based on their names using `total_ordering`. 5. Provide functionality to apply discounts to an item using the `partial` function. **Requirements:** 1. The `InventoryItem` class should have: - Attributes: `name` (string), `price` (list of floats), `stock` (int). - A method `update_price` to update the price of the item. - A method `update_stock` to update the stock of the item. - A method `apply_discount` that applies a given discount to the most recent price of the item. - A cached property `average_price` to compute and store the average price of the item. 2. You should be able to compare `InventoryItem` objects based on their `name`. **Example:** ```python from functools import cached_property, total_ordering, partial @total_ordering class InventoryItem: def __init__(self, name, price, stock): self.name = name self.price = price self.stock = stock def update_price(self, new_price): self.price.append(new_price) def update_stock(self, new_stock): self.stock = new_stock @cached_property def average_price(self): return sum(self.price) / len(self.price) def apply_discount(self, discount): self.price[-1] -= self.price[-1] * discount def __eq__(self, other): if not isinstance(other, InventoryItem): return NotImplemented return self.name.lower() == other.name.lower() def __lt__(self, other): if not isinstance(other, InventoryItem): return NotImplemented return self.name.lower() < other.name.lower() # Example Usage item1 = InventoryItem(\'Laptop\', [1500], 10) item1.update_price(1400) item1.update_price(1300) print(item1.average_price) # Should print the average price item1.apply_discount(0.1) # Apply a 10% discount to the latest price ``` **Instructions:** 1. Implement the `InventoryItem` class as described. 2. Make sure your code handles comparison of items correctly. 3. Use `functools.partial` to create a method that applies a given discount to an inventory item. 4. Optimize the calculation of the average price using `cached_property` to avoid recomputation. **Note:** - Ensure your solution follows best practices for code structure and readability. - Add docstrings to your methods to explain the functionality of each method. **Constraints:** - The `name` of an item is a non-empty string. - The `price` list contains at least one float value. - The `stock` is a non-negative integer. - The discount applied cannot exceed 100%.","solution":"from functools import cached_property, total_ordering, partial @total_ordering class InventoryItem: def __init__(self, name, price, stock): Initialize an inventory item with a name, a list of prices, and stock count. :param name: The name of the item :param price: A list of prices for the item :param stock: The stock count of the item self.name = name self.price = price self.stock = stock def update_price(self, new_price): Update the price of the item by appending a new price to the price list. :param new_price: The new price to be added self.price.append(new_price) # Invalidate cached average price if hasattr(self, \'average_price\'): del self.__dict__[\'average_price\'] def update_stock(self, new_stock): Update the stock count of the item. :param new_stock: The new stock count self.stock = new_stock @cached_property def average_price(self): Calculate and cache the average price of the item. :return: The average price return sum(self.price) / len(self.price) def apply_discount(self, discount): Apply a given discount to the most recent price of the item. :param discount: The discount to be applied (as a fraction, e.g., 0.1 for 10%) self.price[-1] -= self.price[-1] * discount # Invalidate cached average price if hasattr(self, \'average_price\'): del self.__dict__[\'average_price\'] def __eq__(self, other): if not isinstance(other, InventoryItem): return NotImplemented return self.name.lower() == other.name.lower() def __lt__(self, other): if not isinstance(other, InventoryItem): return NotImplemented return self.name.lower() < other.name.lower() # Example Usage item1 = InventoryItem(\'Laptop\', [1500], 10) item1.update_price(1400) item1.update_price(1300) print(item1.average_price) # Should print the average price item1.apply_discount(0.1) # Apply a 10% discount to the latest price print(item1.average_price) # Check the new average price after discount"},{"question":"You are provided with a dataset containing features and target variables. Your task is to implement a Gaussian Process model to solve a regression problem using `scikit-learn`. You are required to: 1. Preprocess the dataset by splitting it into training and testing sets. 2. Implement the Gaussian Process Regressor with two different kernels: `RBF` and `Matérn`. 3. Optimize the hyperparameters of the kernels using log-marginal-likelihood (LML). 4. Evaluate the performance of both models on the test set using appropriate metrics (e.g., Mean Squared Error). 5. Plot the predicted values against the actual values for both kernels. # Input Format - A CSV file with `n` rows and `d` columns where the last column is the target variable and the others are features. - Example: ```csv feature1, feature2, ..., feature(d-1), target 5.1, 3.5, ..., 0.2, 1.4 4.9, 3.0, ..., 0.2, 1.4 ... ``` # Output Format - Print the mean squared error for both kernels. - Generate plots showing predicted vs actual values for both kernels. # Constraints - Ensure that the `test_size` for train-test split is 20%. - Use random state `42` for reproducibility. - Optimize hyperparameters using a maximum of 10 restarts (`n_restarts_optimizer=10`). # Performance Requirements - Your implementation should efficiently handle datasets with up to 10,000 rows and 50 features. # Libraries Allowed - `scikit-learn` - `numpy` - `pandas` - `matplotlib` # Example Here is an example code structure to get you started: ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt # Load the dataset data = pd.read_csv(\'path_to_csv.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define and train GPR with RBF kernel kernel_rbf = RBF() gpr_rbf = GaussianProcessRegressor(kernel=kernel_rbf, n_restarts_optimizer=10, random_state=42) gpr_rbf.fit(X_train, y_train) # Predict and evaluate y_pred_rbf = gpr_rbf.predict(X_test) mse_rbf = mean_squared_error(y_test, y_pred_rbf) print(f\'Mean Squared Error (RBF): {mse_rbf}\') # Define and train GPR with Matern kernel kernel_matern = Matern() gpr_matern = GaussianProcessRegressor(kernel=kernel_matern, n_restarts_optimizer=10, random_state=42) gpr_matern.fit(X_train, y_train) # Predict and evaluate y_pred_matern = gpr_matern.predict(X_test) mse_matern = mean_squared_error(y_test, y_pred_matern) print(f\'Mean Squared Error (Matern): {mse_matern}\') # Plot results plt.figure(figsize=(14, 7)) plt.subplot(1, 2, 1) plt.scatter(y_test, y_pred_rbf) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'RBF Kernel\') plt.subplot(1, 2, 2) plt.scatter(y_test, y_pred_matern) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Matern Kernel\') plt.show() ``` You need to: 1. Complete the example code structure where necessary. 2. Ensure proper handling of edge cases such as missing values in the dataset. # Submission Submit your code in a Jupyter notebook, ensuring all cells execute without error.","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF, Matern from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt def load_and_preprocess_data(filepath): Loads the dataset from a CSV file and preprocess it by splitting into training and testing sets. Parameters: filepath (str): Path to the CSV file. Returns: X_train, X_test, y_train, y_test (numpy arrays): Split and preprocessed data. data = pd.read_csv(filepath) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) return X_train, X_test, y_train, y_test def train_and_evaluate_gpr(X_train, X_test, y_train, y_test, kernel, kernel_name): Trains and evaluates a Gaussian Process Regressor model with the given kernel. Parameters: X_train, X_test, y_train, y_test (numpy arrays): Split and preprocessed data. kernel (Kernel object): The kernel to be used for the GPR model. kernel_name (str): Name of the kernel, used for printing and plotting. Returns: mse (float): Mean squared error on the test set. y_pred (numpy array): Predicted values on the test set. gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42) gpr.fit(X_train, y_train) y_pred = gpr.predict(X_test) mse = mean_squared_error(y_test, y_pred) print(f\'Mean Squared Error ({kernel_name}): {mse:.4f}\') return mse, y_pred if __name__ == \\"__main__\\": # Load and preprocess data filepath = \'path_to_csv.csv\' # Replace with correct path X_train, X_test, y_train, y_test = load_and_preprocess_data(filepath) # Train and evaluate GPR with RBF kernel kernel_rbf = RBF() mse_rbf, y_pred_rbf = train_and_evaluate_gpr(X_train, X_test, y_train, y_test, kernel_rbf, \'RBF\') # Train and evaluate GPR with Matern kernel kernel_matern = Matern() mse_matern, y_pred_matern = train_and_evaluate_gpr(X_train, X_test, y_train, y_test, kernel_matern, \'Matern\') # Plot results plt.figure(figsize=(14, 7)) plt.subplot(1, 2, 1) plt.scatter(y_test, y_pred_rbf) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'RBF Kernel\') plt.subplot(1, 2, 2) plt.scatter(y_test, y_pred_matern) plt.xlabel(\'Actual\') plt.ylabel(\'Predicted\') plt.title(\'Matern Kernel\') plt.show()"},{"question":"Coding Assessment Question # Objective: Write a Python function that demonstrates the creation and management of context variables and context objects using the `contextvars` module. Your task requires implementing a function to create a new context, set multiple context variables, retrieve their values, and finally reset them when needed. # Function Signature: ```python def manage_context_variables(variables: dict, default_values: dict, operations: list) -> dict: Manages context variables based on the provided inputs. :param variables: A dictionary where keys are variable names and values are their initial values. :param default_values: A dictionary where keys are variable names and values are their default values. :param operations: A list of operations to perform, where each operation is a tuple of (\'set\'|\'get\'|\'reset\', var_name, value|token). :return: A dictionary of results for \'get\' operations, with variable names as keys and their fetched values. pass ``` # Input: 1. `variables` (dict): A dictionary where keys are variable names (strings) and values are their initial values. 2. `default_values` (dict): A dictionary where keys are variable names (strings) and values are their default values. 3. `operations` (list): A list of operations to perform, where each operation is a tuple: - `(\'set\', var_name, value)`: Set the context variable `var_name` to `value`. - `(\'get\', var_name, None)`: Get the value of the context variable `var_name`. - `(\'reset\', var_name, token)`: Reset the context variable `var_name` to its previous state using the provided `token`. # Output: - A dictionary of results for \'get\' operations, with the variable names as keys and their fetched values as values. # Constraints: - You should handle errors gracefully, returning an appropriate error message in the results dictionary if an operation fails. - Assume `PyContextVar_New`, `PyContextVar_Set`, `PyContextVar_Get`, and `PyContextVar_Reset` functions are wrapped for Python use. # Example: ```python variables = {\'var1\': 10, \'var2\': 20} default_values = {\'var1\': 5, \'var2\': 15} operations = [ (\'set\', \'var1\', 100), (\'get\', \'var1\', None), (\'reset\', \'var1\', token), (\'get\', \'var1\', None) ] result = manage_context_variables(variables, default_values, operations) print(result) # Output should be {\'var1\': [100, 10]} ``` # Notes: - `token` in the operations list refers to a token returned by a previous \'set\' operation. - Implement the function ensuring to use context management functionalities. - The final output dictionary should contain the results of \'get\' operations corresponding to their variable names. # Hints: - You may use a dictionary to map variable names to their context variables. - Use the provided context and variable functions to manage the context and context variables. - Maintain lists of values for each variable for \'get\' operations to capture their states at different points.","solution":"import contextvars def manage_context_variables(variables: dict, default_values: dict, operations: list) -> dict: Manages context variables based on the provided inputs. :param variables: A dictionary where keys are variable names and values are their initial values. :param default_values: A dictionary where keys are variable names and values are their default values. :param operations: A list of operations to perform, where each operation is a tuple of (\'set\'|\'get\'|\'reset\', var_name, value|token). :return: A dictionary of results for \'get\' operations, with variable names as keys and their fetched values. context_vars = {} results = {} # Initialize context variables and set initial values for var_name, value in variables.items(): context_var = contextvars.ContextVar(var_name, default=default_values[var_name]) context_var.set(value) context_vars[var_name] = context_var # Perform operations token_store = {} # To store tokens for reset operations for op, var_name, value in operations: context_var = context_vars.get(var_name) if not context_var: continue if op == \'set\': token_store[var_name] = context_var.set(value) elif op == \'get\': results[var_name] = results.get(var_name, []) + [context_var.get()] elif op == \'reset\': token = token_store.get(var_name) if token: context_var.reset(token) return results"},{"question":"In this task, you will create a Python class that uses the `gc` (garbage collection) module to manage memory usage by monitoring and collecting garbage at specified intervals. Your class should perform the following functionalities: 1. **Initialization**: - Accept a threshold for the number of uncollectable objects allowed before triggering a forced garbage collection. - Set the appropriate garbage collection thresholds for managing collections of different generations. 2. **Custom Garbage Collection**: - Implement a method to trigger garbage collection automatically if the number of uncollectable objects exceeds the specified threshold. - Print information regarding the number of objects collected and the number of uncollectable objects found during the collection process. 3. **Monitoring and Reporting**: - Set up callbacks to monitor the start and stop phases of garbage collection, printing relevant debug information. - Implement a method that returns a report of garbage collection statistics, including the number of collections, collected objects, and uncollectable objects per generation. # Expected Input and Output **Input**: - Various method calls to the class controlling the garbage collection based on the functionalities described above. **Output**: - Printed debug information regarding garbage collections. - A report of garbage collection statistics. # Constraints - The solution should leverage the `gc` module\'s functionalities. - Ensure that the garbage collection thresholds are appropriately set to avoid excessive collections. # Performance Requirements - The implemented methods should be efficient and avoid introducing significant overhead to the garbage collection process. ```python import gc from typing import Dict class CustomGarbageCollector: def __init__(self, uncollectable_threshold: int, thresholds: (int, int, int)): Initialize the CustomGarbageCollector with a threshold for uncollectable objects and garbage collection thresholds for generations. :param uncollectable_threshold: int - Threshold for uncollectable objects :param thresholds: tuple(int, int, int) - Thresholds for generations (generation 0, 1, and 2) # Implement the initialization logic here pass def automatic_collection(self): Trigger garbage collection if the number of uncollectable objects exceeds the threshold. Print the number of objects collected and uncollectable objects found. # Implement the automatic collection logic here pass def report_stats(self) -> Dict[int, Dict[str, int]]: Return the garbage collection statistics. :return: dict(int, dict(str, int)) - Collection statistics per generation # Implement the reporting logic here pass # Example usage: # collector = CustomGarbageCollector(uncollectable_threshold=10, thresholds=(700, 10, 10)) # collector.automatic_collection() # stats = collector.report_stats() # print(stats) ``` **Notes**: - Fill in the implementation details for the class methods to meet the specifications. - Ensure you print relevant debugging information during garbage collection as described. - Write your code considering the performance requirements and constraints mentioned.","solution":"import gc from typing import Dict, Tuple class CustomGarbageCollector: def __init__(self, uncollectable_threshold: int, thresholds: Tuple[int, int, int]): Initialize the CustomGarbageCollector with a threshold for uncollectable objects and garbage collection thresholds for generations. :param uncollectable_threshold: int - Threshold for uncollectable objects :param thresholds: tuple(int, int, int) - Thresholds for generations (generation 0, 1, and 2) self.uncollectable_threshold = uncollectable_threshold gc.set_threshold(*thresholds) # Add callbacks for garbage collection start and stop events. gc.callbacks.append(self.gc_notify) def gc_notify(self, phase, info): print(f\\"[GC {phase}] {info}\\") def automatic_collection(self): Trigger garbage collection if the number of uncollectable objects exceeds the threshold. Print the number of objects collected and uncollectable objects found. uncollectable_count = len(gc.garbage) if uncollectable_count > self.uncollectable_threshold: collected = gc.collect() print(f\\"Garbage Collection triggered:\\") print(f\\" Collected: {collected} objects\\") print(f\\" Uncollectable: {uncollectable_count} objects\\") def report_stats(self) -> Dict[int, Dict[str, int]]: Return the garbage collection statistics. :return: dict(int, dict(str, int)) - Collection statistics per generation stats = {} for gen in range(3): stats[gen] = { \\"collections\\": gc.get_count()[gen], \\"collected\\": gc.get_stats()[gen][\'collected\'], \\"uncollectable\\": gc.get_stats()[gen][\'uncollectable\'], } return stats # Example usage: # collector = CustomGarbageCollector(uncollectable_threshold=10, thresholds=(700, 10, 10)) # collector.automatic_collection() # stats = collector.report_stats() # print(stats)"},{"question":"**Question: Advanced Bar Plot with Seaborn** # Problem Statement Using the `seaborn` library, you are required to create a customized bar plot that displays the count of total bills categorized by the day of the week and further grouped by gender. Additionally, you should demonstrate how to count numeric data (i.e., tips) without binning and compare this against the counts assigned to different days of the week. Your plot should adhere to the following constraints and specifications: Input: - Use the `tips` dataset from Seaborn\'s built-in datasets. Expected Output: - A grouped bar plot showing the count of total bills (`total_bill`) categorized by the `day` and grouped by `sex`. - A bar plot counting the occurrences of `size` without binning. - A bar plot where the counts of `size` are assigned to the `y` axis (i.e., flipped axes in contrast to the previous plot). Detailed Steps: 1. Import the necessary libraries and load the `tips` dataset. 2. Create a grouped bar plot where: - `x` axis represents the `day`. - `color` represents `sex`. - Each bar shows the count of total bills. 3. Create a bar plot counting the occurrences of `size` without binning. 4. Create a bar plot where the counts of `size` are assigned to the `y` axis, demonstrating the flipped axis concept. Constraints: 1. Ensure that your code follows a clear and logical structure. 2. Use `seaborn.objects` interface for plot creation. 3. Document your code with appropriate comments. # Performance Requirements: - The plots should be generated efficiently using Seaborn. - The code should execute without errors and display the plots correctly. Example: Your output should resemble the structure given here: ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\") # Step 2: Grouped bar plot by day and sex plot1 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot1.show() # Step 3: Bar plot counting occurrences of size without binning plot2 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot2.show() # Step 4: Bar plot with counts of size on y-axis plot3 = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot3.show() ``` # Submission: Submit your Python code in a Jupyter notebook or a Python script file that satisfies the requirements above. Ensure that the plots render correctly within the file upon execution.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plots(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Step 2: Grouped bar plot by day and sex plot1 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot1.show() # Step 3: Bar plot counting occurrences of size without binning plot2 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) plot2.show() # Step 4: Bar plot with counts of size on y-axis plot3 = so.Plot(tips, y=\\"size\\").add(so.Bar(), so.Count()) plot3.show() # Calling the function to generate the plots create_plots()"},{"question":"Objective To assess your understanding of seaborn and its capabilities for visualizing statistical estimation and error bars, demonstrate your comprehension by implementing the following task. Task Write a function named `plot_with_errorbars` that takes a pandas DataFrame and the names of two columns as inputs. Use the provided DataFrame to create a plot that demonstrates different types of error bars around a central tendency estimate of a given numerical column, grouped by a categorical column. Requirements 1. The function should take the following parameters: - `df`: A pandas DataFrame containing the data. - `categorical_col`: The name of the categorical column to group by. - `numerical_col`: The name of the numerical column to estimate and visualize. 2. The function should: - Compute and plot the mean of the `numerical_col` for each category in `categorical_col` using a seaborn point plot. - Add error bars representing standard deviation, standard error, percentile intervals, and confidence intervals to each point. - Clearly annotate and differentiate between these error bars. 3. Customize the plot for clarity: - Include a legend that distinguishes the types of error bars used. - Label the axes and title the plot appropriately. 4. Ensure compatibility with a wide range of sample sizes and data distributions, keeping performance considerations for large datasets in mind. Input Format - `df` (pandas.DataFrame): The input dataframe containing data. - `categorical_col` (str): The name of the categorical column. - `numerical_col` (str): The name of the numerical column. Output - A seaborn plot displayed using matplotlib. Constraints - The provided DataFrame will contain at least one categorical and one numerical column. - Assume proper column names will be provided for the given dataset. Example ```python import pandas as pd # Sample data data = { \'Category\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\', \'C\'], \'Value\': [10, 12, 9, 11, 5, 7, 6] } df = pd.DataFrame(data) # Example function call plot_with_errorbars(df, \'Category\', \'Value\') ``` Expected Output A plot showing the mean of the values for each category with error bars indicating standard deviation, standard error, percentile intervals, and confidence intervals. Notes - You may use the `sns.pointplot` function along with appropriate error bar arguments described in the documentation. - Ensure the plot is clear and easily interpretable.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def plot_with_errorbars(df, categorical_col, numerical_col): Plots the mean of a numerical column grouped by a categorical column with various types of error bars representing standard deviation, standard error, percentile intervals, and confidence intervals. Parameters: df (pandas.DataFrame): The input dataframe containing data. categorical_col (str): The name of the categorical column to group by. numerical_col (str): The name of the numerical column to estimate and visualize. # Calculate the required statistics means = df.groupby(categorical_col)[numerical_col].mean().reset_index() std_devs = df.groupby(categorical_col)[numerical_col].std().reset_index() std_errors = df.groupby(categorical_col)[numerical_col].sem().reset_index() conf_intervals = df.groupby(categorical_col)[numerical_col].apply( lambda x: x.mean() + 1.96*x.std()/len(x)**0.5).reset_index() # Merging for easy plotting data = means data = data.merge(std_devs, on=categorical_col, suffixes=(\'\', \'_std\')) data = data.merge(std_errors, on=categorical_col, suffixes=(\'\', \'_sem\')) data = data.merge(conf_intervals, on=categorical_col, suffixes=(\'\', \'_ci\')) # Plotting plt.figure(figsize=(12, 8)) plot = sns.pointplot( data=data, x=categorical_col, y=numerical_col, join=False, ci=None, errwidth=1) # Adding error bars manually for i in range(data.shape[0]): plt.errorbar( i, data[numerical_col].iloc[i], yerr=data[numerical_col + \'_std\'].iloc[i], fmt=\'none\', c=\'b\', label=\'Standard Deviation\' if i==0 else \\"\\") plt.errorbar( i, data[numerical_col].iloc[i], yerr=data[numerical_col + \'_sem\'].iloc[i], fmt=\'none\', c=\'r\', label=\'Standard Error\' if i==0 else \\"\\") plt.errorbar( i, data[numerical_col].iloc[i], yerr=data[numerical_col + \'_ci\'].iloc[i], fmt=\'none\', c=\'g\', label=\'Confidence Interval\' if i==0 else \\"\\") # Plot customizations plt.title(f\'{numerical_col} by {categorical_col} with Error Bars\') plt.xlabel(categorical_col) plt.ylabel(numerical_col) plt.legend() plt.show()"},{"question":"Seaborn Styling and Context Management **Objective:** Assess the student’s ability to manipulate and temporarily change styling of visualizations in Seaborn using `sns.axes_style`. **Question:** You are provided with a dataset containing sales figures over three years. Your task is to create a bar plot comparing these sales figures across the years. Additionally, you should demonstrate different Seaborn styling mechanisms. 1. Retrieve and print the current default Seaborn style parameters. 2. Display the style parameters for the predefined `darkgrid` style. 3. Create a bar plot for the given sales data using the default style. 4. Create another bar plot using the `whitegrid` style by temporarily changing the style within a context manager without affecting the default style setting. Data: ```python years = [\\"Year 1\\", \\"Year 2\\", \\"Year 3\\"] sales = [100, 150, 130] ``` Expected Steps: 1. Initialize the data for the years and sales. 2. Print the current default style parameters using `sns.axes_style()`. 3. Print the style parameters for the `darkgrid` style using `sns.axes_style(\\"darkgrid\\")`. 4. Create a bar plot with default styling for the given years and sales data. 5. Use a context manager to temporarily set the `whitegrid` style and create another bar plot with the same data. Constraints: - Use the Seaborn and Matplotlib libraries only. - Ensure the style changes using the context manager do not affect other plots outside its scope. The output should include the printed style parameters and the visual plots as described. ```python import seaborn as sns import matplotlib.pyplot as plt # Your code here ``` **Input/Output Example:** - **Input:** {Predefined in problem statement} - **Output:** (Print statements for style), Plots (Two bar plots displaying the specified data with different styles)","solution":"import seaborn as sns import matplotlib.pyplot as plt def seaborn_styling_and_context_management(years, sales): # 1. Retrieve and print the current default Seaborn style parameters. current_style = sns.axes_style() print(\\"Current default Seaborn style parameters:\\") print(current_style) # 2. Display the style parameters for the predefined `darkgrid` style. darkgrid_style = sns.axes_style(\\"darkgrid\\") print(\\"nDarkgrid style parameters:\\") print(darkgrid_style) # 3. Create a bar plot for the given sales data using the default style. plt.figure(figsize=(8, 6)) sns.barplot(x=years, y=sales) plt.title(\'Sales Figures Over Three Years (Default Style)\') plt.show() # 4. Use a context manager to temporarily set the `whitegrid` style and create another bar plot. with sns.axes_style(\\"whitegrid\\"): plt.figure(figsize=(8, 6)) sns.barplot(x=years, y=sales) plt.title(\'Sales Figures Over Three Years (Whitegrid Style)\') plt.show()"},{"question":"You are given a compute-intensive task that needs to be executed on a machine with multiple GPU devices. Your goal is to implement a function using PyTorch\'s `torch.accelerator` module to distribute the task across multiple GPUs and ensure that the computation is synchronized. # Task 1. Write a function `distribute_computation(task_function, n_iterations)` that accepts a compute function, `task_function`, and the number of iterations, `n_iterations`. The function should: - Check if any accelerator devices are available. - If available, retrieve the number of devices and distribute the iterations equally across the available devices. - Use the device index to set the currently active GPU. - Use streams to manage concurrent execution if necessary. - Ensure that all computations are fully synchronized before returning. 2. The `task_function` should be executed on each device in separate streams to optimize computation across multiple GPUs. 3. Your implementation must handle scenarios where no GPUs are available by printing an appropriate message. # Input - `task_function`: A function that performs a compute-intensive task. - `n_iterations`: An integer specifying the number of iterations the `task_function` should be executed. # Output - The function should print messages indicating the start and end of the computation on each device, and when the computation is synchronized. # Example Usage ```python import torch def example_task(): # Simulate compute-intensive task for _ in range(1000000): pass def distribute_computation(task_function, n_iterations): if not torch.accelerator.is_available(): print(\\"No accelerator devices available.\\") return device_count = torch.accelerator.device_count() iter_per_device = n_iterations // device_count for device_idx in range(device_count): torch.accelerator.set_device_idx(device_idx) stream = torch.cuda.Stream(device=device_idx) with torch.cuda.stream(stream): print(f\\"Starting computation on device {device_idx}\\") for _ in range(iter_per_device): task_function() print(f\\"Completed computation on device {device_idx}\\") torch.accelerator.synchronize() # Example call distribute_computation(example_task, 10) ``` # Constraints - The function should handle erroneous or edge cases gracefully, such as when the number of iterations is less than the number of devices. - Ensure that all necessary PyTorch functions from `torch.accelerator` are utilized effectively. # Notes - Make sure to use `torch.cuda.set_device`, `torch.cuda.Stream`, and `torch.cuda.synchronize` where necessary. - This question tests understanding of device management, parallel execution with streams, and synchronization in PyTorch.","solution":"import torch def distribute_computation(task_function, n_iterations): if not torch.cuda.is_available(): print(\\"No GPU devices are available.\\") return device_count = torch.cuda.device_count() iter_per_device = n_iterations // device_count remainder_iterations = n_iterations % device_count streams = [torch.cuda.Stream(device=i) for i in range(device_count)] for device_idx in range(device_count): torch.cuda.set_device(device_idx) with torch.cuda.stream(streams[device_idx]): print(f\\"Starting computation on device {device_idx}\\") iterations = iter_per_device + (1 if device_idx < remainder_iterations else 0) for _ in range(iterations): task_function() print(f\\"Completed computation on device {device_idx}\\") torch.cuda.synchronize() print(\\"All computations are synchronized.\\")"},{"question":"# HMAC-Based File Integrity Verification **Objective:** Implement a Python function to verify the integrity of files using HMAC. Given a list of file paths and a corresponding list of expected HMAC digest values, the function should verify that each file\'s content matches its expected HMAC digest. Return a dictionary where keys are file paths and values are boolean indicating whether the file matches its expected digest. **Function Signature:** ```python def verify_files_integrity(file_paths: list, expected_digests: list, key: bytes, digestmod: str) -> dict: Verifies the integrity of files using HMAC. Parameters: - file_paths: List of strings where each string is a file path. - expected_digests: List of strings where each string is the expected HMAC digest of the corresponding file in hexadecimal form. - key: A bytes object that represents the secret key for HMAC. - digestmod: A string representing the name of the hash algorithm to use (e.g., \'sha256\'). Returns: - A dictionary where keys are file paths and values are booleans indicating whether the file\'s HMAC digest matches the expected digest. ``` **Input:** - `file_paths`: List of strings representing paths to files. - `expected_digests`: List of strings representing the expected HMAC digest for each file (in hexadecimal). - `key`: A bytes object representing the secret key for HMAC. - `digestmod`: A string representing the hash algorithm to use (e.g., \'sha256\'). **Output:** - A dictionary with file paths as keys and booleans as values. A value should be `True` if the file matches its expected HMAC digest, `False` otherwise. **Example:** ```python file_paths = [\'file1.txt\', \'file2.txt\'] expected_digests = [ \'5d41402abc4b2a76b9719d911017c592\', # MD5 digest for \\"hello\\" \'7d793037a0760186574b0282f2f435e7\' # MD5 digest for \\"world\\" ] key = b\'secret\' digestmod = \'md5\' print(verify_files_integrity(file_paths, expected_digests, key, digestmod)) # Output: {\'file1.txt\': True, \'file2.txt\': True} ``` **Constraints:** - The files may be large, so reading the contents incrementally is recommended to avoid memory issues. - Ensure the function handles exceptions gracefully (e.g., file not found). **Notes:** - Use the `hmac` module for HMAC computation. - Use `hmac.compare_digest` for secure comparison of digests.","solution":"import hmac import hashlib def verify_files_integrity(file_paths, expected_digests, key, digestmod): results = {} for file_path, expected_digest in zip(file_paths, expected_digests): try: h = hmac.new(key, digestmod=getattr(hashlib, digestmod)) with open(file_path, \'rb\') as file: while chunk := file.read(4096): h.update(chunk) computed_digest = h.hexdigest() results[file_path] = hmac.compare_digest(computed_digest, expected_digest) except FileNotFoundError: results[file_path] = False return results"},{"question":"**Coding Assessment Question:** # Task Create a function `plot_penguins_body_mass` that visualizes the body mass distribution of the different penguin species (`species`), separated by sex (`sex`). The function should use the `seaborn.objects` module to create the following plot: 1. A bar plot showing the average body mass (`body_mass_g`) for each species. 2. Different colors for different sexes. 3. Properly handle overlapping bars using the `Dodge` transformation. 4. Add error bars to indicate the standard deviation of the body mass within each species and sex category. 5. Customize the plot by setting properties such as `alpha` transparency and `edgewidth` for the bars. # Input There are no direct inputs to the function. The function should use the `penguins` dataset from seaborn. # Output The function should display the constructed plot using the provided `seaborn.objects` module. # Code Requirements - You must use the `seaborn.objects` module and the `penguins` dataset. - You should not use global variables; the code must work as part of the function. # Example ```python def plot_penguins_body_mass(): import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.5, edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .show() ) # Call the function to display the plot plot_penguins_body_mass() ``` This function should encapsulate the entire process of creating the desired plot and adhere to the provided guidelines effectively.","solution":"def plot_penguins_body_mass(): import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.5, edgewidth=2), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .show() ) # Call the function to display the plot plot_penguins_body_mass()"},{"question":"**Objective:** To assess your understanding and application of the \\"xdrlib\\" module in packing and unpacking data using both `Packer` and `Unpacker` classes. **Problem Statement:** You are required to implement two functions: `serialize_person(person)` and `deserialize_person(data)`. These functions will handle the serialization and deserialization of a custom data structure named `Person`. A `Person` is represented by the following attributes: - `name` (string): A variable-length string representing the person\'s name. - `age` (integer): An integer representing the person\'s age. - `height` (float): A floating-point number representing the person\'s height. - `is_student` (boolean): A boolean indicating whether the person is a student. # Function 1: `serialize_person(person)` - **Input:** A dictionary representing a person, e.g., ```python { \'name\': \'Alice\', \'age\': 30, \'height\': 5.5, \'is_student\': False } ``` - **Output:** A binary string representing the serialized person data. # Function 2: `deserialize_person(data)` - **Input:** A binary string obtained from the `serialize_person` function. - **Output:** A dictionary resembling the original person structure. # Constraints: - The `name` string should be a maximum of 50 characters. - The `age` should be a non-negative integer. - The `height` should be a non-negative float. - The `is_student` is a boolean. # Example Usage: ```python person = { \'name\': \'Alice\', \'age\': 30, \'height\': 5.5, \'is_student\': False } # Serialize the person serialized_data = serialize_person(person) # Deserialize the data back to a person deserialized_person = deserialize_person(serialized_data) assert deserialized_person == person ``` # Implementation Details: 1. **serialize_person(person)**: - Use the `Packer` class to serialize each attribute of the person into XDR format. - Make sure to handle appropriate padding and data alignment as per XDR standards. 2. **deserialize_person(data)**: - Use the `Unpacker` class to deserialize the binary string back into a person dictionary. - Ensure that the unpacked values match the original data types and constraints. **Note:** Proper exception handling should be implemented to manage potential conversion errors or buffer misalignments while packing or unpacking data. **Grading Criteria:** - Correct implementation of the packing and unpacking methods. - Adherence to XDR standards for serialization. - Robust handling of edge cases and exceptions. - Code readability and use of Pythonic conventions.","solution":"import xdrlib MAX_NAME_LENGTH = 50 def serialize_person(person): packer = xdrlib.Packer() # Ensure the name is not longer than MAX_NAME_LENGTH name = person[\'name\'][:MAX_NAME_LENGTH] packer.pack_string(name.encode(\'utf-8\')) packer.pack_int(person[\'age\']) packer.pack_float(person[\'height\']) packer.pack_bool(person[\'is_student\']) return packer.get_buffer() def deserialize_person(data): unpacker = xdrlib.Unpacker(data) name = unpacker.unpack_string().decode(\'utf-8\') age = unpacker.unpack_int() height = unpacker.unpack_float() is_student = unpacker.unpack_bool() return { \'name\': name, \'age\': age, \'height\': height, \'is_student\': is_student }"},{"question":"Objective Your task is to write Python functions that demonstrate the use of the `struct` module for packing and unpacking binary data. The question will cover the usage of different format characters, byte ordering, and optimized data processing using the `Struct` class. Problem Description 1. Write a function `pack_data` that takes a format string and a list of values, and returns the packed binary data. - **Input**: A string `format` and a list of values `values`. - **Output**: A bytes object containing the packed data. - **Example**: ```python data = pack_data(\'>I4sh\', [1, b\'test\', 2]) # Expected output: b\'x00x00x00x01testx00x00x00x02\' ``` - **Constraints**: - The format string will be valid and compatible with the given values. - The values list length matches the number of format characters specified (accounting for any implicit repetitions). 2. Write a function `unpack_data` that takes a format string and a bytes object containing packed data, and returns the unpacked values as a tuple. - **Input**: A string `format` and a bytes object `data`. - **Output**: A tuple containing the unpacked values. - **Example**: ```python values = unpack_data(\'>I4sh\', b\'x00x00x00x01testx00x00x00x02\') # Expected output: (1, b\'test\', 2) ``` - **Constraints**: - The format string will be valid and compatible with the given data. - The data length matches the size required by the format string. 3. Write a function `optimized_pack_and_unpack` that demonstrates the efficient usage of the `Struct` class for packing and unpacking binary data. This function should: - Take a format string and a list of values. - Create a `Struct` object with the given format string. - Pack the values using the `Struct` object. - Unpack the resulting binary data using the `Struct` object. - Return a tuple containing the packed data and the unpacked values. - **Input**: A string `format` and a list of values `values`. - **Output**: A tuple `(packed_data, unpacked_values)`. - **Example**: ```python result = optimized_pack_and_unpack(\'>I4sh\', [1, b\'test\', 2]) # Expected output: (b\'x00x00x00x01testx00x00x00x02\', (1, b\'test\', 2)) ``` - **Constraints**: - The format string will be valid and compatible with the given values. - The values list length matches the number of format characters specified (accounting for any implicit repetitions). Performance Requirements - The function `optimized_pack_and_unpack` should be optimized for repeated packing and unpacking of the same format string using the `Struct` class. - Ensure the functions can handle typical use cases under a reasonable performance constraint. Your Implementation Implement the given functions in Python, adhering to the input and output formats, constraints, and performance requirements specified.","solution":"import struct def pack_data(format, values): Packs the given data into a bytes object based on the format. Parameters: format (str): The format string as per the struct module. values (list): The list of values to pack. Returns: bytes: The packed bytes object. return struct.pack(format, *values) def unpack_data(format, data): Unpacks the given binary data into values as per the format. Parameters: format (str): The format string as per the struct module. data (bytes): The packed binary data to unpack. Returns: tuple: The unpacked values. return struct.unpack(format, data) def optimized_pack_and_unpack(format, values): Demonstrates efficient usage of the Struct class for packing and unpacking. Parameters: format (str): The format string as per the struct module. values (list): The list of values to pack. Returns: tuple: A tuple containing the packed data and the unpacked values. s = struct.Struct(format) packed_data = s.pack(*values) unpacked_values = s.unpack(packed_data) return (packed_data, unpacked_values)"},{"question":"# Objective: To assess your understanding of creating and manipulating color palettes using the `seaborn` package. # Task: Your task is to write a function `generate_dark_palettes` that generates various dark palettes and visualizes them using `seaborn`. Specifically, you need to implement the following: 1. Generate a sequential dark palette ranging from dark gray to a provided color. 2. Generate a palette where the color is specified using a hex code. 3. Generate a palette using the husl color system. 4. Generate a palette with a specified number of colors. 5. Generate and return a continuous colormap. # Input: Your function will take the following arguments: - `color1` (str): The name of the color for the first palette (e.g., \\"seagreen\\"). - `color2` (str): The hex code of the color for the second palette (e.g., \\"#79C\\"). - `husl_color` (tuple): A tuple representing the color in husl system for the third palette (e.g., (20, 60, 50)). - `num_colors` (int): The number of colors in the fourth palette (e.g., 8). - `color3` (str): The hex code of the color for the continuous colormap (e.g., \\"#b285bc\\"). # Output: The function should return five items: 1. A list of the color palette generated for `color1`. 2. A list of the color palette generated for `color2`. 3. A list of the color palette generated for `husl_color`. 4. A list of the color palette generated with `num_colors`. 5. A continuous colormap for `color3`. # Constraints: - Use seaborn\'s `dark_palette` function for all palette generations. - Ensure your code is optimized for readability and simplicity. # Example: ```python def generate_dark_palettes(color1, color2, husl_color, num_colors, color3): import seaborn as sns palette1 = sns.dark_palette(color1) palette2 = sns.dark_palette(color2) palette3 = sns.dark_palette(husl_color, input=\\"husl\\") palette4 = sns.dark_palette(color1, num_colors) cmap = sns.dark_palette(color3, as_cmap=True) return palette1, palette2, palette3, palette4, cmap # Example usage result = generate_dark_palettes(\\"seagreen\\", \\"#79C\\", (20, 60, 50), 8, \\"#b285bc\\") # \'result\' should contain the specified color palettes and colormap ``` # Notes: - Test your function with different colors and numbers of colors to ensure robustness. - You can visualize the palettes using seaborn\'s `sns.palplot()` function if you want to check the output manually.","solution":"def generate_dark_palettes(color1, color2, husl_color, num_colors, color3): import seaborn as sns palette1 = sns.dark_palette(color=color1, reverse=False, as_cmap=False) palette2 = sns.dark_palette(color=color2, reverse=False, as_cmap=False) palette3 = sns.dark_palette(color=husl_color, input=\\"husl\\", reverse=False, as_cmap=False) palette4 = sns.dark_palette(color=color1, n_colors=num_colors, reverse=False, as_cmap=False) cmap = sns.dark_palette(color=color3, as_cmap=True) return palette1, palette2, palette3, palette4, cmap"},{"question":"Implementing and Loading a Custom Model with PyTorch Hub Objective To assess your understanding of PyTorch Hub, you are required to implement an entry point for a custom neural network model and then load it using the PyTorch Hub API. Description You will create a simple neural network model in PyTorch and publish it in a `hubconf.py` file. The model should be a basic feedforward neural network with one hidden layer. Task 1: Define the Model Implement a PyTorch neural network model with the following specifications: - Input layer: 10 units - Hidden layer: 5 units - Output layer: 1 unit - Activation function: ReLU for the hidden layer - Output function: Sigmoid for the output layer Task 2: Create the `hubconf.py` File Create an entry point in a `hubconf.py` file to publish the model. The entry point should: 1. Define a function named `simple_nn` that initializes the model. 2. Specify `torch` as a dependency. 3. Provide optional keyword arguments to the `simple_nn` function to allow customization of the model\'s configuration. Task 3: Load the Model using PyTorch Hub Write a script to load the model using the PyTorch Hub API. The script should: 1. List all available models using `torch.hub.list(\'repo_owner/repo_name\')`. 2. Display the help message for the `simple_nn` entry point. 3. Load the model using `torch.hub.load(\'repo_owner/repo_name\', \'simple_nn\')`. Input and Output Formats - **Input**: No direct input from the user. - **Output**: The model should be printed after loading. Constraints - Use only PyTorch and torchvision packages. - The model should be simple but functional, adhering to the specifications given. Performance Requirements The code should be efficient and free of any redundant computations. The initialization and loading processes should be optimized. Example Here\'s an example of how the `hubconf.py` file should look: ```python dependencies = [\'torch\'] import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_size=10, hidden_size=5, output_size=1): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) self.sigmoid = nn.Sigmoid() def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) out = self.sigmoid(out) return out def simple_nn(input_size=10, hidden_size=5, output_size=1): model = SimpleNN(input_size, hidden_size, output_size) return model ``` And an example script to load the model: ```python import torch.hub # List all available models print(torch.hub.list(\'repo_owner/repo_name\')) # Display the help message for the simple_nn entry point print(torch.hub.help(\'repo_owner/repo_name\', \'simple_nn\')) # Load the model model = torch.hub.load(\'repo_owner/repo_name\', \'simple_nn\') print(model) ``` Submission Guidelines Submit your solution as a Jupyter notebook or a Python script with appropriate explanations for each task.","solution":"import torch import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_size=10, hidden_size=5, output_size=1): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_size, output_size) self.sigmoid = nn.Sigmoid() def forward(self, x): out = self.fc1(x) out = self.relu(out) out = self.fc2(out) out = self.sigmoid(out) return out def simple_nn(input_size=10, hidden_size=5, output_size=1): model = SimpleNN(input_size, hidden_size, output_size) return model"},{"question":"# Email Header Manipulation with `email.headerregistry` Background: In this task, you will be working with the `email.headerregistry` module, which is used to handle email headers in compliance with various RFC specifications. The core concept of this module revolves around creating and manipulating customized header objects. Task: Implement a function `process_email_headers` that takes a raw email header string as input and returns a dictionary with the following keys: - `subject`: The subject of the email. - `date`: The datetime object representing the date of the email. - `from_addresses`: A list of `email.headerregistry.Address` objects representing the sender’s addresses. - `to_addresses`: A list of `email.headerregistry.Address` objects representing the recipient’s addresses. - `cc_addresses`: A list of `email.headerregistry.Address` objects representing the CC’d addresses. - `defects`: A list of defects found during processing. If the header string does not contain a field, set the corresponding value in the dictionary to `None` (or an empty list for address fields). Be sure to correctly handle any compliance issues by populating the defects list. Function Signature: ```python def process_email_headers(header_str: str) -> dict: pass ``` Input: - `header_str` (str): A string representing the raw email headers. Output: - A dictionary with keys `subject`, `date`, `from_addresses`, `to_addresses`, `cc_addresses`, `defects`. The values should be as described above. Constraints: - You are only allowed to use the classes and methods provided by the `email.headerregistry` module. Example: ```python from datetime import datetime header_string = \'\'\'Subject: Test Email Date: Sat, 13 Mar 2021 16:41:00 -0800 From: John Doe <john.doe@example.com> To: jane.doe@example.com CC: ccuser@example.com \'\'\' result = process_email_headers(header_string) print(result[\'subject\']) # Output: \'Test Email\' print(result[\'date\']) # Output: datetime(2021, 3, 13, 16, 41, tzinfo=datetime.timezone(datetime.timedelta(days=-1, seconds=57600))) print(result[\'from_addresses\'][0].username) # Output: \'john.doe\' print(result[\'to_addresses\'][0].username) # Output: \'jane.doe\' print(result[\'cc_addresses\'][0].username) # Output: \'ccuser\' print(result[\'defects\']) # Output: [] ``` Notes: - Pay close attention to the types of objects and encoding specified in the documentation. - Ensure your function can handle various edge cases in compliance with the appropriate RFC specifications.","solution":"from email import policy from email.parser import BytesParser from email.headerregistry import Address from datetime import datetime import email def process_email_headers(header_str: str) -> dict: # Creating an empty dictionary to hold the result result = { \'subject\': None, \'date\': None, \'from_addresses\': [], \'to_addresses\': [], \'cc_addresses\': [], \'defects\': [] } # Parsing the header string to get the EmailMessage object msg = BytesParser(policy=policy.default).parsebytes(header_str.encode()) # Extracting the subject result[\'subject\'] = msg[\'subject\'] # Extracting the date if msg[\'date\']: result[\'date\'] = email.utils.parsedate_to_datetime(msg[\'date\']) # Extracting from addresses if msg[\'from\']: result[\'from_addresses\'] = [address for address in msg[\'from\'].addresses] # Extracting to addresses if msg[\'to\']: result[\'to_addresses\'] = [address for address in msg[\'to\'].addresses] # Extracting cc addresses if msg[\'cc\']: result[\'cc_addresses\'] = [address for address in msg[\'cc\'].addresses] # Extracting defects result[\'defects\'] = list(msg.defects) return result"},{"question":"# Question: You have been provided with a dataset containing information about different species of penguins. Your task is to thoroughly examine the distribution of certain quantitative variables, compare it among various species, and visualize these distributions using seaborn. Dataset Description: The dataset `penguins` contains several features related to penguins, such as: - `species`: Species of the penguin. - `bill_length_mm`: The length of the penguin\'s bill. - `bill_depth_mm`: The depth of the penguin\'s bill. - `flipper_length_mm`: The length of the penguin\'s flipper. - `body_mass_g`: The body mass of the penguin. Tasks: 1. **Univariate Distribution**: - Plot the distribution of `flipper_length_mm` and adjust the bin size appropriately to reveal any noticeable features in the distribution. - Include a kernel density estimate (KDE) for better visualization. 2. **Conditional Distribution on Species**: - Plot the distribution of `flipper_length_mm` conditioned on the `species` variable using different colors for each species. - Adjust the visual representation to ensure that the distributions are distinguishable. 3. **Bivariate Distribution**: - Create a bivariate plot visualizing the relationship between `bill_length_mm` and `bill_depth_mm`. - Make use of a KDE plot for this purpose to show the density contours. 4. **Joint Distribution**: - Using `jointplot`, visualize the joint distribution of `bill_length_mm` and `bill_depth_mm` and include their marginal distributions. 5. **Comparative Distribution**: - Compare the distribution of `flipper_length_mm` among different species using a normalized histogram that represents the probability density. Submission: Please submit the fully functional code using seaborn that accomplishes the above tasks. Ensure your plots are clearly labeled and appropriately adjusted for clear interpretation. **Hint**: Make sure to import all necessary libraries and load the `penguins` dataset correctly at the start of your code. ```python # Sample Code for Loading Dataset (Do not include in your submission) import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") def analyze_penguins(penguins): Analyzes the penguins dataset and performs various visualizations as described. # Univariate Distribution plt.figure(figsize=(10, 6)) sns.histplot(penguins[\'flipper_length_mm\'], bins=30, kde=True) plt.title(\'Distribution of Flipper Length (mm)\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # Conditional Distribution on Species plt.figure(figsize=(10, 6)) sns.histplot(penguins, x=\'flipper_length_mm\', hue=\'species\', element=\'step\', stat=\'density\', common_norm=False) plt.title(\'Distribution of Flipper Length (mm) Conditioned on Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() # Bivariate Distribution plt.figure(figsize=(10, 6)) sns.kdeplot(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', fill=True) plt.title(\'Bivariate Density Plot of Bill Length vs. Bill Depth\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Bill Depth (mm)\') plt.show() # Joint Distribution sns.jointplot(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', kind=\'scatter\', marginal_kws=dict(bins=20, fill=True)) plt.show() # Comparative Distribution plt.figure(figsize=(10, 6)) sns.histplot(penguins, x=\'flipper_length_mm\', hue=\'species\', element=\'step\', stat=\'density\', common_norm=True) plt.title(\'Comparative Distribution of Flipper Length (mm) Among Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Probability Density\') plt.show()"},{"question":"**Objective:** The task is to demonstrate your ability to manipulate datasets into the appropriate forms (long-form or wide-form) and create effective visualizations using seaborn. **Question:** You have been provided two datasets, representing monthly sales figures (`sales_data`) from different regions and years, and psychological experiment results (`experiment_data`) of subjects\' response times under different conditions. 1. The `sales_data` dataset contains columns: `Year`, `Month`, `Region`, and `Sales`. 2. The `experiment_data` dataset contains columns: `SubjectID`, `Condition`, `Task1_ResponseTime`, `Task2_ResponseTime`. **Datasets:** ```python import pandas as pd data1 = { \'Year\': [2019, 2019, 2020, 2020], \'Month\': [\'Jan\', \'Feb\', \'Jan\', \'Feb\'], \'Region\': [\'North\', \'South\', \'North\', \'South\'], \'Sales\': [200, 150, 300, 200] } sales_data = pd.DataFrame(data1) data2 = { \'SubjectID\': [1, 2, 3, 4], \'Condition\': [\'Focused\', \'Divided\', \'Focused\', \'Divided\'], \'Task1_ResponseTime\': [10, 20, 15, 25], \'Task2_ResponseTime\': [11, 22, 14, 23] } experiment_data = pd.DataFrame(data2) ``` **Tasks:** 1. **Visualization 1**: Create a line plot for `sales_data` by reshaping it to wide-form where each line represents a region, and the X-axis is the `Month`, and the Y-axis is the `Sales`. The data should be split by `Year`. **Input:** `sales_data` DataFrame **Output:** A seaborn line plot as described. 2. **Visualization 2**: Transform the `experiment_data` into a long-form dataset where the variable columns `Task1_ResponseTime` and `Task2_ResponseTime` are melted into one column. Create a point plot with `Condition` on the x-axis, `ResponseTime` on the y-axis, and different colors representing the `Task`. **Input:** `experiment_data` DataFrame **Output:** A seaborn point plot as described. **Expected Function Signature:** ```python import seaborn as sns import pandas as pd def plot_sales_data(sales_data: pd.DataFrame): # Your code goes here pass def plot_experiment_data(experiment_data: pd.DataFrame): # Your code goes here pass ``` **Constraints:** 1. Use seaborn for plotting. 2. Use appropriate pandas functions to reshape the data. 3. Ensure plots are clear and appropriately labeled. **Performance Requirements:** - Your code should be efficient and run within a reasonable time for large datasets. - Visualizations should be accurate and easily understandable. **Note:** You do not need to return any value from the functions. The primary goal is to generate and show the plots.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_sales_data(sales_data: pd.DataFrame): Create a line plot for the sales data by reshaping it to wide-form where each line represents a region, the X-axis is the Month, and the Y-axis is the Sales, split by Year. pivot_df = sales_data.pivot_table(index=\'Month\', columns=[\'Year\', \'Region\'], values=\'Sales\') pivot_df.plot(kind=\'line\', marker=\'o\') plt.title(\'Monthly Sales Figures by Region\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.legend(title=\'Year, Region\') plt.show() def plot_experiment_data(experiment_data: pd.DataFrame): Transform the experiment data into a long-form dataset where the variable columns Task1_ResponseTime and Task2_ResponseTime are melted into one column. Create a point plot with Condition on the x-axis, ResponseTime on the y-axis, and different colors representing the Task. long_form = experiment_data.melt(id_vars=[\'SubjectID\', \'Condition\'], value_vars=[\'Task1_ResponseTime\', \'Task2_ResponseTime\'], var_name=\'Task\', value_name=\'ResponseTime\') plt.figure() sns.pointplot(data=long_form, x=\'Condition\', y=\'ResponseTime\', hue=\'Task\', markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"]) plt.title(\'Response Time under Different Conditions\') plt.show()"},{"question":"# Question: Task Scheduling Using Topological Sort You\'re given a set of tasks and their dependencies, where a task may depend on multiple other tasks being completed first. Your goal is to implement a function that determines a valid order to complete the tasks, or identifies if it\'s not possible due to cyclic dependencies. Function Signature ```python from graphlib import TopologicalSorter, CycleError def task_scheduling(tasks: dict) -> list or str: Determine a valid order to complete the tasks or detect if cyclic dependencies exist. :param tasks: A dictionary where the keys are task identifiers (strings) and values are sets of task identifiers that must be completed before the key task. :return: A list of tasks in a valid completion order, or the string \\"Cycle detected\\" if a cycle exists in the task dependencies. pass ``` Input - `tasks`: A dictionary representing the graph of tasks. Keys are task identifiers (strings), and values are sets of tasks (strings) that must precede the key task. - Example: `{\\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}}` Output - A list of task identifiers in a valid completion order - Example: `[\'A\', \'C\', \'B\', \'D\']` - The string `\\"Cycle detected\\"` if cyclic dependencies are present in the input graph. Constraints 1. All task identifiers are strings. 2. The input graph may have up to 1000 tasks with valid dependencies. 3. If there are multiple valid orderings, any one of them is acceptable. Example ```python from graphlib import TopologicalSorter, CycleError def task_scheduling(tasks: dict) -> list or str: try: ts = TopologicalSorter(tasks) return list(ts.static_order()) except CycleError: return \\"Cycle detected\\" # Example usage: tasks = {\\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"B\\": {\\"A\\"}} print(task_scheduling(tasks)) # Output: [\'A\', \'C\', \'B\', \'D\'] tasks_with_cycle = {\\"D\\": {\\"B\\", \\"C\\"}, \\"C\\": {\\"A\\"}, \\"A\\": {\\"D\\"}} print(task_scheduling(tasks_with_cycle)) # Output: \\"Cycle detected\\" ``` Explanation - The function initializes a `TopologicalSorter` with the given tasks. - It tries to extract a static order of tasks. - If a cycle is detected, it returns \\"Cycle detected\\". - Otherwise, it returns the list of tasks in a valid topological order. Implement the function `task_scheduling` to achieve the desired behavior.","solution":"from graphlib import TopologicalSorter, CycleError def task_scheduling(tasks: dict) -> list or str: Determine a valid order to complete the tasks or detect if cyclic dependencies exist. :param tasks: A dictionary where the keys are task identifiers (strings) and values are sets of task identifiers that must be completed before the key task. :return: A list of tasks in a valid completion order, or the string \\"Cycle detected\\" if a cycle exists in the task dependencies. try: ts = TopologicalSorter(tasks) return list(ts.static_order()) except CycleError: return \\"Cycle detected\\""},{"question":"**Objective:** Demonstrate your understanding of Python\'s interaction with the operating system at a low level by using functions from the `sys` module and handling process control through forking operations. **Description:** You are required to implement a Python function that manages subprocesses and interacts with the file system using the C API functions exposed above. Your task is to: 1. **Prepare for Process Forking:** - Call the appropriate function to prepare the internal state before forking a new process. 2. **Fork the Process:** - Use the Python `os.fork()` function to fork the current process. Ensure that the fork operation is executed from the main thread. 3. **Handle Parent and Child Process States:** - In the parent process: - Call the function that updates the internal state using the appropriate function after forking. - Write a message to `sys.stdout` indicating successful forking and the child\'s PID. - Terminate the parent process using an appropriate exit status. - In the child process: - Call the function to update the internal state after forking in the child process. - Write a message to `sys.stdout` from the child process indicating that it is the child process. - Terminate the child process using an appropriate exit status. 4. **Ensure Clean Exit:** - Register a cleanup function that will be executed when the child process exits. **Function Signature:** ```python def manage_subprocess(): pass ``` **Constraints:** - The function must handle `TypeError` or any other exceptions as specified in the documentation. - The fork operation must be synchronized to avoid race conditions. - Ensure to manage the file system paths using `PyOS_FSPath`. **Expected Output:** - The function should print messages to `sys.stdout` indicating the state transitions and process information. - Ensure the cleanup function runs on process exit. - Parents should exit with status 0 and children with status 1. **Performance Requirements:** - Handle potential forking and exiting efficiently to avoid memory leaks. - Properly use internal state functions before and after forking. **Example:** ```python if __name__ == \\"__main__\\": manage_subprocess() # Expected stdout messages: # Parent process forked successfully, child PID: <actual_child_pid> # Child process running, PID: <actual_child_pid> # Cleanup function called on child process exit ``` Use the provided documentation to guide your implementation. Ensure all C API interactions are properly handled. If an error occurs at any step, an appropriate exception must be raised and handled gracefully.","solution":"import os import sys def manage_subprocess(): try: # Preparing for forking os.fork() pid = os.fork() if pid > 0: # Parent process sys.stdout.write(f\\"Parent process forked successfully, child PID: {pid}n\\") sys.exit(0) elif pid == 0: # Child process sys.stdout.write(f\\"Child process running, PID: {os.getpid()}n\\") sys.exit(1) except OSError as e: sys.stderr.write(f\\"Fork failed: {e.strerror}n\\") sys.exit(1) # Cleanup function call def cleanup(): sys.stdout.write(\\"Cleanup function called on child process exitn\\") if __name__ == \\"__main__\\": import atexit atexit.register(cleanup) manage_subprocess()"},{"question":"You have been tasked with implementing a class EmailEncoder that provides functionality to encode email message payloads using different encoding schemes. This class will use the functions provided in the `email.encoders` module. Requirements 1. The class should be named `EmailEncoder`. 2. The class should have a method `encode_message` that accepts two arguments: - `message`: An instance of the `email.message.Message` class. - `encoding`: A string indicating the encoding scheme to use. Valid values are `\\"quoted-printable\\"`, `\\"base64\\"`, `\\"7bit\\"`, `\\"8bit\\"`, and `\\"noop\\"`. 3. The `encode_message` method should apply the specified encoding to the message\'s payload using the corresponding function from the `email.encoders` module. 4. If the `message` is multipart, the method should raise a `TypeError` with the message \\"Cannot encode a multipart message\\". 5. The method should return the modified message object after encoding. Implementation You will have access to the following functions from the `email.encoders` module: - `encode_quopri(message)` - `encode_base64(message)` - `encode_7or8bit(message)` - `encode_noop(message)` Example Usage ```python from email.message import EmailMessage import email.encoders class EmailEncoder: def encode_message(self, message: EmailMessage, encoding: str) -> EmailMessage: if message.is_multipart(): raise TypeError(\\"Cannot encode a multipart message\\") if encoding == \\"quoted-printable\\": email.encoders.encode_quopri(message) elif encoding == \\"base64\\": email.encoders.encode_base64(message) elif encoding == \\"7bit\\" or encoding == \\"8bit\\": email.encoders.encode_7or8bit(message) elif encoding == \\"noop\\": email.encoders.encode_noop(message) else: raise ValueError(\\"Unknown encoding type\\") return message # Usage Example message = EmailMessage() message.set_payload(\\"This is a test message with special characters: ñ, ü, å.\\") encoder = EmailEncoder() encoded_message = encoder.encode_message(message, \\"base64\\") print(encoded_message.get_payload(decode=True)) ``` This example demonstrates how to encode an email message using base64 encoding. Constraints - Do not modify the EmailMessage class or the encoders module directly. - Assume all inputs are valid except for the constraints mentioned. - You must use the provided functions from the `email.encoders` module for encoding. Your implementation should focus on using the functionality provided by the `email.encoders` module to achieve the required encoding of email messages.","solution":"from email.message import Message import email.encoders class EmailEncoder: def encode_message(self, message: Message, encoding: str) -> Message: if message.is_multipart(): raise TypeError(\\"Cannot encode a multipart message\\") if encoding == \\"quoted-printable\\": email.encoders.encode_quopri(message) elif encoding == \\"base64\\": email.encoders.encode_base64(message) elif encoding == \\"7bit\\" or encoding == \\"8bit\\": email.encoders.encode_7or8bit(message) elif encoding == \\"noop\\": email.encoders.encode_noop(message) else: raise ValueError(\\"Unknown encoding type\\") return message"},{"question":"You are tasked with creating a custom color palette using seaborn\'s `sns.blend_palette` function and demonstrating the difference between discrete palettes and continuous colormaps. To achieve this, you need to implement a function `generate_custom_palette` that will generate both types of palettes and visualize them. Function Signature ```python def generate_custom_palette(colors: list, as_cmap: bool = False): Generate and visualize a custom color palette using seaborn. Parameters: colors (list): A list of color specifications to blend. as_cmap (bool): If true, return a continuous colormap; otherwise, return a discrete palette. Returns: None: The function should plot the generated palette/colormap. ``` Input - `colors`: A list of strings specifying the colors to be blended. The list should contain at least two colors. - Example: `[\\"#45a872\\", \\".8\\", \\"xkcd:golden\\"]` - `as_cmap`: A boolean indicating whether to generate a continuous colormap (`True`) or a discrete palette (`False`). Output - The function should produce a visual output (plot) showing the generated palette or colormap. Constraints - You can use any format for colors (named colors, hex codes, etc.). - The number of colors in the list should be at least two. Requirements 1. **Function Implementation**: Implement the `generate_custom_palette` function using seaborn. 2. **Visualization**: Show the generated palette or colormap visually using appropriate seaborn or matplotlib functions. Example ```python generate_custom_palette([\\"#bdc\\", \\"#7b9\\", \\"#47a\\"], as_cmap=False) generate_custom_palette([\\"#bdc\\", \\"#7b9\\", \\"#47a\\"], as_cmap=True) ``` The first call should produce a discrete palette plot, while the second call should produce a continuous colormap plot. **Performance Requirements**: There are no stringent performance requirements, but the function should generate and plot the palettes efficiently. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_custom_palette(colors: list, as_cmap: bool = False): Generate and visualize a custom color palette using seaborn. Parameters: colors (list): A list of color specifications to blend. as_cmap (bool): If true, return a continuous colormap; otherwise, return a discrete palette. Returns: None: The function should plot the generated palette/colormap. if as_cmap: palette = sns.blend_palette(colors, as_cmap=True) sns.heatmap([list(range(100))], cmap=palette, cbar=False) plt.title(\\"Continuous Colormap\\") else: palette = sns.blend_palette(colors, as_cmap=False) sns.palplot(palette) plt.title(\\"Discrete Palette\\") plt.show()"},{"question":"URL Examination Tool You are tasked with implementing a function `examine_url(url: str) -> dict` that performs the following operations: 1. **Open and Read the URL Content:** - Use `urllib.request` to open the given URL and read its contents. 2. **Handle Errors:** - Catch and handle any errors that may occur during the URL opening and reading process using `urllib.error`. 3. **Parse URL Components:** - Parse the provided URL into its components (scheme, netloc, path, params, query, fragment) using `urllib.parse`. 4. **Check robots.txt:** - Extract the netloc from the parsed URL and fetch the `robots.txt` file for the domain. Parse the file to check if the URL is allowed to be fetched according to the rules specified. The function should return a dictionary with the following keys: - `\\"content_length\\"`: The length of the content read from the URL. - `\\"is_fetchable\\"`: A boolean indicating whether the URL is fetchable according to the `robots.txt` rules. - `\\"url_components\\"`: A dictionary with the components of the URL (scheme, netloc, path, params, query, fragment). Input - `url` (str): A string representing the URL to be examined. Output - (dict): A dictionary containing: - `\\"content_length\\"` (int): Length of the URL content (number of characters). - `\\"is_fetchable\\"` (bool): Indicates if the URL can be fetched according to the `robots.txt` rules. - `\\"url_components\\"` (dict): Components of the URL with the keys `scheme`, `netloc`, `path`, `params`, `query`, and `fragment`. Example ```python url = \\"http://example.com/index.html\\" result = examine_url(url) print(result) ``` Expected Output: ```python { \\"content_length\\": 1270, # Example value, actual length might vary \\"is_fetchable\\": True, \\"url_components\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"/index.html\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" } } ``` Constraints - If an error occurs during the URL reading, `\\"content_length\\"` should be `0` and `is_fetchable` should be `False`. Implement the `examine_url` function with the described functionality.","solution":"import urllib.request import urllib.error import urllib.parse from urllib.robotparser import RobotFileParser def examine_url(url: str) -> dict: result = { \\"content_length\\": 0, \\"is_fetchable\\": False, \\"url_components\\": {} } try: # Parse URL components parsed_url = urllib.parse.urlparse(url) result[\\"url_components\\"] = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } # Get the content length with urllib.request.urlopen(url) as response: content = response.read() result[\\"content_length\\"] = len(content) # Check robots.txt robot_parser = RobotFileParser() robot_parser.set_url(f\\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\\") robot_parser.read() result[\\"is_fetchable\\"] = robot_parser.can_fetch(\\"*\\", url) except urllib.error.URLError as e: result[\\"content_length\\"] = 0 result[\\"is_fetchable\\"] = False result[\\"url_components\\"] = {} return result"},{"question":"# Task You are tasked with implementing a WSGI application that can handle multiple HTTP methods (GET and POST) and manage response headers dynamically. The application will run on a simple WSGI server, and you are required to ensure it conforms to the WSGI specification using the validation tools from the `wsgiref` module. # Requirements 1. **WSGI Application `my_app`:** - The application should accept HTTP GET and POST requests. - For GET requests, respond with a form that allows users to submit their name. - For POST requests, respond with a personalized greeting message using the name submitted. - Dynamically set response headers including content type and a custom header (`X-Custom-Header`). 2. **Server Implementation:** - Use `wsgiref.simple_server.make_server` to create and run the server on `localhost` at port `8080`. 3. **Validation:** - Wrap your WSGI application with `wsgiref.validate.validator` to ensure it conforms to the WSGI specification. # Input and Output Formats - **GET Request Input:** Retrieves a form. - **GET Request Output:** Respond with an HTML form. - **POST Request Input:** Name entered in the HTML form. - **POST Request Output:** Personalized greeting message. # Example When a GET request is made to the root URL, the application should respond with an HTML form: ```html <!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\"> <title>Submit Name</title> </head> <body> <form action=\\"/\\" method=\\"POST\\"> <label for=\\"name\\">Enter your name:</label> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"> <button type=\\"submit\\">Submit</button> </form> </body> </html> ``` When a POST request is made with the name `Alice`, the application should respond with: ``` Hello, Alice! ``` # Constraints - Use `wsgiref` utilities and classes as appropriate. - Ensure the application can process both GET and POST requests. - Implement a custom header (`X-Custom-Header`) in the response. # Performance Requirements - Start and serve the application on `localhost` at port `8080`. - Ensure the application is compliant with the WSGI specification. # Code Template ```python from wsgiref.simple_server import make_server from wsgiref.validate import validator from wsgiref.util import setup_testing_defaults def my_app(environ, start_response): # YOUR CODE HERE if __name__ == \'__main__\': # Wrap the application in the validator validated_app = validator(my_app) # Create the server server = make_server(\'localhost\', 8080, validated_app) print(\\"Serving on port 8080...\\") # Run the server server.serve_forever() ``` Implement the function `my_app` to satisfy the requirements above.","solution":"from wsgiref.simple_server import make_server from wsgiref.validate import validator from urllib.parse import parse_qs def my_app(environ, start_response): path = environ.get(\'PATH_INFO\', \'\') method = environ.get(\'REQUEST_METHOD\', \'GET\') if method == \'GET\' and path == \'/\': status = \'200 OK\' headers = [ (\'Content-type\', \'text/html; charset=utf-8\'), (\'X-Custom-Header\', \'GET Request Received\') ] start_response(status, headers) return [b <!DOCTYPE html> <html lang=\\"en\\"> <head> <meta charset=\\"UTF-8\\"> <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1.0\\"> <title>Submit Name</title> </head> <body> <form action=\\"/\\" method=\\"POST\\"> <label for=\\"name\\">Enter your name:</label> <input type=\\"text\\" id=\\"name\\" name=\\"name\\"> <button type=\\"submit\\">Submit</button> </form> </body> </html> ] elif method == \'POST\' and path == \'/\': try: size = int(environ.get(\'CONTENT_LENGTH\', 0)) except (ValueError): size = 0 post_body = environ[\'wsgi.input\'].read(size).decode(\'utf-8\') post_data = parse_qs(post_body) name = post_data.get(\'name\', [\'\'])[0] if not name: status = \'400 Bad Request\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [b\'Name is required!\'] message = f\'Hello, {name}!\' status = \'200 OK\' headers = [ (\'Content-type\', \'text/plain; charset=utf-8\'), (\'X-Custom-Header\', \'POST Request Received\') ] start_response(status, headers) return [message.encode(\'utf-8\')] else: status = \'404 Not Found\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) return [b\'404 Not Found\'] if __name__ == \'__main__\': validated_app = validator(my_app) server = make_server(\'localhost\', 8080, validated_app) print(\\"Serving on port 8080...\\") server.serve_forever()"},{"question":"# Question: You have been tasked with creating an audio processing Python script that involves reading a WAV file, performing some basic operations, and saving the modified audio back to a WAV file. Task: 1. Write a function `invert_audio(input_filename: str, output_filename: str) -> None` that reads a WAV file, inverts the audio signal, and writes the inverted audio back to a new WAV file. 2. Implement a color conversion utility using the \\"colorsys\\" module. Write a function `rgb_to_hsv(r: int, g: int, b: int) -> tuple` that converts RGB color values to the HSV color system. Specifications: 1. **invert_audio** - **Input:** - `input_filename` (str): The name of the input WAV file. - `output_filename` (str): The name of the output WAV file where the inverted audio will be saved. - **Output:** None - **Functionality:** - The function should read the input WAV file. - Invert the audio signal (e.g., multiplying the amplitude by -1). - Write the inverted audio signal to the output WAV file. - **Constraints:** - The input WAV file will be a valid mono audio file. - You should handle potential file I/O errors gracefully. 2. **rgb_to_hsv** - **Input:** - `r` (int): Red color component (0-255). - `g` (int): Green color component (0-255). - `b` (int): Blue color component (0-255). - **Output:** tuple - A tuple representing the color in HSV format, where each component (h, s, v) is a float in the range [0.0, 1.0]. - **Functionality:** - The function should use the `colorsys` module to perform the conversion. - **Constraints:** - The input values for `r`, `g`, and `b` are guaranteed to be within the range [0, 255]. Example Usage: ```python # Example for invert_audio invert_audio(\'input.wav\', \'output.wav\') # Example for rgb_to_hsv hsv = rgb_to_hsv(255, 0, 0) # Expected output: (0.0, 1.0, 1.0) for red color ``` You are expected to handle the reading and writing of WAV files using the \\"wave\\" module and to use the \\"colorsys\\" module for the RGB to HSV conversion.","solution":"import wave import numpy as np import colorsys def invert_audio(input_filename: str, output_filename: str) -> None: try: with wave.open(input_filename, \'rb\') as infile: params = infile.getparams() frames = infile.readframes(params.nframes) audio_signal = np.frombuffer(frames, dtype=np.int16) # Invert the audio signal inverted_signal = -audio_signal # Convert back to bytes inverted_frames = inverted_signal.tobytes() # Write to output file with wave.open(output_filename, \'wb\') as outfile: outfile.setparams(params) outfile.writeframes(inverted_frames) except Exception as e: print(f\\"An error occurred: {e}\\") def rgb_to_hsv(r: int, g: int, b: int) -> tuple: r_norm = r / 255.0 g_norm = g / 255.0 b_norm = b / 255.0 return colorsys.rgb_to_hsv(r_norm, g_norm, b_norm)"},{"question":"# Advanced Asynchronous Programming with asyncio **Objective:** Implement an asynchronous function using Python\'s `asyncio` library that demonstrates the ability to manage multiple asynchronous tasks, handle potential pitfalls, and effectively use debugging and logging. Problem Statement: Write an asynchronous function named `process_urls` that accepts a list of URLs. The function should: 1. **Fetch content asynchronously** from each URL using HTTP GET requests. 2. **Process the content** of each URL. We\'ll simulate processing by sleeping asynchronously for 1 second. 3. **Ensure non-blocking execution**: Fetching and processing should not block the event loop. 4. **Handle exceptions gracefully** during fetching and processing, ensuring that all errors are logged. 5. **Use asyncio\'s debug mode** to log various execution details during execution (e.g., slow callbacks, unawaited tasks). 6. **Return a list** of tuples containing the URL and the length of the fetched content, or an appropriate error message in case of a failure. **Input:** - A list of URLs (strings). **Output:** - A list of tuples. Each tuple should contain a URL and the length of the content fetched from that URL, or an error message if the fetch or processing fails. Constraints and Important Notes: 1. Assume all URLs are valid and the server responds within a reasonable time frame. 2. You must use `asyncio` functionalities; synchronous code might result in a zero score. 3. Use an `aiohttp` client session for making HTTP requests asynchronously. 4. Log all fetch and processing errors using Python\'s `logging` module. 5. Ensure that the execution uses asyncio\'s debug mode. 6. The logging level should be set to `DEBUG`. Example Usage: ```python import asyncio async def fetch_content(url: str) -> str: # Function to fetch content from the URL (dummy function, actual implementation required) ... async def process_content(content: str) -> int: # Function to process the content (dummy function, actual implementation required) ... async def process_urls(urls: list) -> list: # Your implementation here ... # Example list of URLs: Replace with actual URLs for real use urls = [\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"] # Running the asyncio event loop if __name__ == \\"__main__\\": result = asyncio.run(process_urls(urls), debug=True) print(result) ``` **HINTS:** 1. Use `asyncio.create_task` to schedule fetching and processing tasks concurrently. 2. Use a separate function for fetching and processing to handle exceptions locally. 3. Make sure to handle tasks using `asyncio.gather()` to ensure they are all awaited and to handle any exceptions. --- Implement the `fetch_content`, `process_content`, and `process_urls` functions to achieve the described behavior.","solution":"import asyncio import aiohttp import logging # Configure logging logging.basicConfig(level=logging.DEBUG) logger = logging.getLogger(__name__) async def fetch_content(url: str) -> str: Fetch content from the given URL asynchronously. try: async with aiohttp.ClientSession() as session: async with session.get(url) as response: response.raise_for_status() return await response.text() except Exception as e: logger.error(f\\"Error fetching {url}: {e}\\") raise async def process_content(content: str) -> int: Simulate processing the content asynchronously. try: await asyncio.sleep(1) return len(content) except Exception as e: logger.error(f\\"Error processing content: {e}\\") raise async def process_single_url(url: str) -> tuple: Fetch and process a single URL, returning the result as a tuple. try: content = await fetch_content(url) length = await process_content(content) return (url, length) except Exception as e: return (url, str(e)) async def process_urls(urls: list) -> list: Process a list of URLs asynchronously. tasks = [process_single_url(url) for url in urls] results = await asyncio.gather(*tasks, return_exceptions=True) return results # Example list of URLs: Replace with actual URLs for real use urls = [\\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\"] # Running the asyncio event loop if __name__ == \\"__main__\\": asyncio.run(process_urls(urls), debug=True)"},{"question":"Using Seaborn, styles can significantly enhance the readability and aesthetics of your plots. Your task is to create a function that generates two different types of plots with customized styles and saves them to disk. # Function Signature ```python def generate_customized_plots(): pass ``` # Requirements 1. **Bar Plot with Whitegrid Style:** - Create a bar plot with the following data points: - x: [\\"Category A\\", \\"Category B\\", \\"Category C\\"] - y: [10, 20, 15] - Use the `\\"whitegrid\\"` style for this plot. - The x-axis should be labeled \\"Categories\\" and the y-axis should be labeled \\"Values\\". - Save this plot as \\"bar_plot.png\\". 2. **Line Plot with Darkgrid Style:** - Create a line plot with the following data points: - x: [\\"Point 1\\", \\"Point 2\\", \\"Point 3\\"] - y: [5, 15, 10] - Use the `\\"darkgrid\\"` style but customize `grid.color` to \\".6\\" and `grid.linestyle` to \\":\\". - The x-axis should be labeled \\"Points\\" and the y-axis should be labeled \\"Values\\". - Save this plot as \\"line_plot.png\\". # Constraints - Use only the Seaborn and matplotlib libraries for plotting. - Ensure that the plots are saved as PNG files in the current directory. # Example When you run the function `generate_customized_plots()`, it should generate and save two PNG files: - `bar_plot.png`: A bar plot with whitegrid style. - `line_plot.png`: A line plot with customized darkgrid style. You do not need to return anything from the function. # Notes - Pay attention to the aesthetics and ensure labels are readable. - The colors and line styles should be customized according to the instructions. Implement the function below: ```python def generate_customized_plots(): import seaborn as sns import matplotlib.pyplot as plt # Bar Plot with Whitegrid Style sns.set_style(\\"whitegrid\\") plt.figure() sns.barplot(x=[\\"Category A\\", \\"Category B\\", \\"Category C\\"], y=[10, 20, 15]) plt.xlabel(\\"Categories\\") plt.ylabel(\\"Values\\") plt.title(\\"Bar Plot with Whitegrid Style\\") plt.savefig(\\"bar_plot.png\\") plt.close() # Line Plot with Darkgrid Style sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}) plt.figure() sns.lineplot(x=[\\"Point 1\\", \\"Point 2\\", \\"Point 3\\"], y=[5, 15, 10]) plt.xlabel(\\"Points\\") plt.ylabel(\\"Values\\") plt.title(\\"Line Plot with Customized Darkgrid Style\\") plt.savefig(\\"line_plot.png\\") plt.close() ```","solution":"def generate_customized_plots(): import seaborn as sns import matplotlib.pyplot as plt # Bar Plot with Whitegrid Style sns.set_style(\\"whitegrid\\") plt.figure(figsize=(8, 6)) sns.barplot(x=[\\"Category A\\", \\"Category B\\", \\"Category C\\"], y=[10, 20, 15]) plt.xlabel(\\"Categories\\") plt.ylabel(\\"Values\\") plt.title(\\"Bar Plot with Whitegrid Style\\") plt.savefig(\\"bar_plot.png\\") plt.close() # Line Plot with Darkgrid Style sns.set_style(\\"darkgrid\\", {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"}) plt.figure(figsize=(8, 6)) sns.lineplot(x=[\\"Point 1\\", \\"Point 2\\", \\"Point 3\\"], y=[5, 15, 10]) plt.xlabel(\\"Points\\") plt.ylabel(\\"Values\\") plt.title(\\"Line Plot with Customized Darkgrid Style\\") plt.savefig(\\"line_plot.png\\") plt.close()"},{"question":"Objective: Implement a class that manages a system-like environment, including file handling, process creation, and signal handling in a simulated environment. Description: Create a class `SystemManager` with the following methods: 1. **`__init__(self)`**: - Initialize an internal state for the class, including a list to store registered signal handlers and a dictionary for environment variables. 2. **`set_env(self, key: str, value: str) -> None`**: - Set an environment variable with the provided key and value. 3. **`get_env(self, key: str) -> str`**: - Retrieve the value of an environment variable with the provided key. 4. **`register_signal_handler(self, signal_number: int, handler: callable) -> None`**: - Register a signal handler for the given signal number. The handler is a callable that takes one argument (the signal number). 5. **`simulate_fork(self) -> int`**: - Simulate a fork operation. It should return `0` to simulate child process behavior and a positive integer to simulate parent process behavior. In real applications, this would involve actual process creation (`os.fork()`). 6. **`path_operations(self, path: str) -> dict`**: - Perform path operations using `pathlib`. Given a string path, this method should return a dictionary with: - The absolute path. - The file name. - The parent directory. Constraints: - Do not use actual `os.fork()` in `simulate_fork`. - Use `pathlib` for path manipulations. - Assumptions may include only POSIX-compliant file paths. Example Usage: ```python sys_manager = SystemManager() sys_manager.set_env(\\"PATH\\", \\"/usr/bin\\") assert sys_manager.get_env(\\"PATH\\") == \\"/usr/bin\\" def my_handler(signal_number): print(f\\"Received signal: {signal_number}\\") sys_manager.register_signal_handler(2, my_handler) # Simulate fork pid = sys_manager.simulate_fork() if pid == 0: print(\\"Child process\\") else: print(f\\"Parent process with child pid: {pid}\\") path_info = sys_manager.path_operations(\\"/usr/local/bin/python\\") print(path_info) # Output example: # { # \'absolute_path\': \'/usr/local/bin/python\', # \'file_name\': \'python\', # \'parent_directory\': \'/usr/local/bin\' # } ``` Your Task: Implement the `SystemManager` class according to the above specifications and ensure it behaves as described in the example usage.","solution":"import os from pathlib import Path class SystemManager: def __init__(self): self.signal_handlers = {} self.env_vars = {} def set_env(self, key: str, value: str) -> None: self.env_vars[key] = value def get_env(self, key: str) -> str: return self.env_vars.get(key) def register_signal_handler(self, signal_number: int, handler: callable) -> None: self.signal_handlers[signal_number] = handler def simulate_fork(self) -> int: import random return random.choice([0, 1]) def path_operations(self, path: str) -> dict: p = Path(path) return { \'absolute_path\': str(p.resolve()), \'file_name\': p.name, \'parent_directory\': str(p.parent) }"},{"question":"# PyTorch with MPS Backend Assessment You are tasked with implementing a function that demonstrates the use of the MPS backend in PyTorch. Your function should: 1. Check if the MPS backend is available. 2. Create two tensors on the MPS device and perform basic tensor operations. 3. Create a simple neural network model, move it to the MPS device, and run a forward pass with one of the tensors. Function Signature ```python def use_mps_backend(): pass ``` Detailed Requirements 1. **Check MPS Availability:** - Print \\"MPS is available\\" if the MPS backend is available. - Otherwise, print an appropriate message indicating why MPS is not available (either \\"MPS not available because the current PyTorch install was not built with MPS enabled\\" or \\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine\\"). 2. **Tensor Operations:** - Create a tensor of shape (3, 3) filled with ones on the MPS device. - Create another tensor of shape (3, 3) filled with random values on the MPS device. - Perform element-wise addition of the two tensors and store the result in a new tensor. - Print the resulting tensor. 3. **Model Operations:** - Define a simple neural network model with one linear layer. - Move the model to the MPS device. - Pass the tensor filled with ones through the model and print the output. Example Output ``` MPS is available Tensor on MPS device: tensor([[1.4345, 1.6690, 1.0402], [1.1650, 1.5354, 1.4566], [1.5423, 1.2366, 1.5467]], device=\'mps:0\') Model output: tensor([[0.3143], [0.3143], [0.3143]], device=\'mps:0\', grad_fn=<AddmmBackward>) ``` Constraints - Use PyTorch\'s `torch` package. - Ensure your solution works on macOS 12.3+ with an MPS-enabled GPU. Notes - The random values in the example output will vary. - The simple neural network should use an `nn.Linear` layer, for instance, `torch.nn.Linear(3, 1)`. Implement the function to clearly demonstrate your understanding of utilizing the MPS backend in PyTorch.","solution":"import torch import torch.nn as nn def use_mps_backend(): if not torch.backends.mps.is_available(): if not torch.backends.mps.is_built(): print(\\"MPS not available because the current PyTorch install was not built with MPS enabled\\") else: print(\\"MPS not available because the current MacOS version is not 12.3+ and/or you do not have an MPS-enabled device on this machine\\") return print(\\"MPS is available\\") # Creating tensors on MPS device tensor_ones = torch.ones((3, 3), device=\'mps\') tensor_random = torch.rand((3, 3), device=\'mps\') result_tensor = tensor_ones + tensor_random print(\\"Tensor on MPS device:\\") print(result_tensor) # Defining a simple neural network and moving it to MPS model = nn.Linear(3, 1).to(\'mps\') # Forward pass model_output = model(tensor_ones) print(\\"Model output:\\") print(model_output)"},{"question":"# Advanced File Handling with Python C API Wrappers Problem Statement You are working on integrating some low-level file handling functionalities in Python. Your task is to implement a Python class that uses the provided C API wrappers to perform several file operations. You\'ll need to use `ctypes` to interact with these C functions from Python. # Requirements 1. Implement a class `AdvancedFileHandler` that: - Initializes with a file path and mode (`\'r\'`, `\'w\'`, etc.). - Uses the C function `PyFile_FromFd` to open the file and create a Python file object. - Provides a method `read_line` that uses `PyFile_GetLine` to read a single line from the file. - Provides a method `write_string` that uses `PyFile_WriteString` to write a string to the file. - Provides a method `write_object` that uses `PyFile_WriteObject` to write a given Python object to the file. - Implements error handling such that if any of the C function calls fail, a Python exception is raised. # Constraints - You must use Python\'s `ctypes` library to load and call the C functions described in the documentation. - File operations should handle different modes (`\'r\'`, `\'w\'`, `\'a\'`, etc.) correctly. - Ensure proper handling of resources, such as closing files and cleaning up properly. # Example ```python handler = AdvancedFileHandler(\\"example.txt\\", \\"w\\") handler.write_string(\\"Hello, world!\\") handler.write_object({\\"key\\": \\"value\\"}) handler = AdvancedFileHandler(\\"example.txt\\", \\"r\\") print(handler.read_line()) # Should print \\"Hello, world!\\" ``` Input and Output Formats - Input: The class methods would take appropriate parameters as described: - `AdvancedFileHandler(file_path: str, mode: str)` - `read_line() -> str` - `write_string(content: str) -> None` - `write_object(obj: Any) -> None` - Output: The methods should perform I/O operations as specified and handle any errors appropriately by raising exceptions. Performance - Ensure that file operations are efficient and handle large files gracefully. - Properly use buffered I/O if applicable to avoid performance pitfalls. # Constraints - The `AdvancedFileHandler` class should work with text files only. - Ensure the correct use of file descriptors and proper error reporting according to the C API documentation provided. Implement the class `AdvancedFileHandler` as specified.","solution":"import ctypes import os # Load the C standard library libc = ctypes.CDLL(\\"libc.so.6\\") # Define ctypes function prototypes libc.fdopen.argtypes = [ctypes.c_int, ctypes.c_char_p] libc.fdopen.restype = ctypes.c_void_p libc.fgets.argtypes = [ctypes.c_char_p, ctypes.c_int, ctypes.c_void_p] libc.fgets.restype = ctypes.c_char_p libc.fputs.argtypes = [ctypes.c_char_p, ctypes.c_void_p] libc.fputs.restype = ctypes.c_int class AdvancedFileHandler: def __init__(self, file_path, mode): self.file_path = file_path self.mode = mode.encode(\'utf-8\') self.fd = os.open(file_path, self._mode_to_flags()) self.file = libc.fdopen(self.fd, self.mode) if not self.file: raise OSError(\\"Failed to open file\\") def _mode_to_flags(self): if b\'w\' in self.mode: return os.O_WRONLY | os.O_CREAT | os.O_TRUNC elif b\'r\' in self.mode: return os.O_RDONLY elif b\'a\' in self.mode: return os.O_WRONLY | os.O_CREAT | os.O_APPEND else: raise ValueError(f\\"Unsupported mode: {self.mode.decode()}\\") def read_line(self): if b\'r\' not in self.mode: raise OSError(\\"File not opened in read mode\\") buffer = ctypes.create_string_buffer(1024) result = libc.fgets(buffer, 1024, self.file) if not result: raise OSError(\\"Failed to read line from file\\") return buffer.value.decode(\'utf-8\') def write_string(self, content): if b\'w\' not in self.mode and b\'a\' not in self.mode: raise OSError(\\"File not opened in write mode\\") result = libc.fputs(content.encode(\'utf-8\'), self.file) if result == -1: raise OSError(\\"Failed to write string to file\\") def write_object(self, obj): self.write_string(str(obj)) def __del__(self): if hasattr(self, \'file\') and self.file: libc.fclose(self.file)"},{"question":"Coding Assessment Question # Objective To test your understanding of Seaborn\'s `displot` function and its customization options to create different types of visualizations. # Problem Statement You are provided with a dataset `penguins` which contains information about penguins\' flipper lengths, bill lengths, species, and sex. Your task is to create a series of visualizations using Seaborn\'s `displot` function to display and compare this data in various ways. # Instructions 1. Write a function `plot_penguin_data()` that takes no parameters and produces the following visualizations: a. **Histogram**: Plot a histogram of the `flipper_length_mm` column with KDE curve added. b. **Bivariate KDE**: Plot a bivariate KDE of `flipper_length_mm` and `bill_length_mm`. c. **Faceted KDE**: Create KDE plots of `flipper_length_mm` for each species, separated into different subplots (facets) for each sex. 2. Each plot should include appropriate labels and titles. Customize the titles to include the name of facets where applicable. # Expected Output The function should display the following plots: 1. **Histogram with KDE**: A single histogram plot showing the distribution of `flipper_length_mm` with an overlaid KDE curve, labeled appropriately. 2. **Bivariate KDE**: A single bivariate KDE plot showing the relationship between `flipper_length_mm` and `bill_length_mm`. 3. **Faceted KDE**: A grid of KDE plots of `flipper_length_mm` for each species faceted by sex, each facet with customized titles. # Constraints - Ensure that all plots include clear labels for axes and titles that reflect the data being visualized. - Customize the visualizations as specified using appropriate Seaborn parameters and methods. # Performance Requirements - The solution should efficiently handle the provided dataset without unnecessary computations or excessive memory usage. # Example Code ```python def plot_penguin_data(): import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Part (a) - Plot Histogram with KDE sns.displot(data=penguins, x=\\"flipper_length_mm\\", kde=True) plt.title(\\"Histogram of Flipper Length with KDE\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # Part (b) - Plot Bivariate KDE sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\") plt.title(\\"Bivariate KDE of Flipper and Bill Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Bill Length (mm)\\") plt.show() # Part (c) - Plot Faceted KDE g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\") g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") g.set_titles(\\"{col_name} penguins: {col_var}\\") plt.show() ``` # Note Make sure to import necessary libraries and set an appropriate theme for Seaborn before creating the plots.","solution":"def plot_penguin_data(): import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Part (a) - Plot Histogram with KDE sns.displot(data=penguins, x=\\"flipper_length_mm\\", kde=True) plt.title(\\"Histogram of Flipper Length with KDE\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Density\\") plt.show() # Part (b) - Plot Bivariate KDE sns.displot(data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", kind=\\"kde\\") plt.title(\\"Bivariate KDE of Flipper and Bill Length\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Bill Length (mm)\\") plt.show() # Part (c) - Plot Faceted KDE g = sns.displot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"kde\\") g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Density\\") g.set_titles(\\"{col_name} penguins\\") plt.show()"},{"question":"# Question: # Memory Buffer Management in Python You are working with a binary data processing application, and you need to handle large datasets that require efficient memory management. In this task, you will need to implement a function that processes an input string by obtaining a buffer view and then modifying its content. Requirements: 1. Implement a function `process_buffer(input_data: bytes) -> bytes` that: - Takes a byte string (`input_data`) as input. - Uses the `memoryview` object to get a buffer view over the input data. - Modifies the buffer content by reversing it. - Returns the modified byte string. 2. Ensure that the buffer management is handled efficiently, and all resources are properly released after usage. Constraints: - The input byte string can be of any length but will not exceed 10^7 bytes. - You should use the `memoryview` object and appropriate buffer management methods provided in Python 3. Example: ```python def process_buffer(input_data: bytes) -> bytes: # Your code here # Example usage: input_data = b\\"Hello, world!\\" output_data = process_buffer(input_data) print(output_data) # Should output: b\\"!dlrow ,olleH\\" ``` **Note:** The solution must showcase proper usage of memory management techniques using memory views in Python 3.","solution":"def process_buffer(input_data: bytes) -> bytes: Processes the input byte string by reversing its content using memoryview. :param input_data: A byte string to be processed. :return: A new byte string with the content reversed. with memoryview(input_data) as view: reversed_data = view[::-1].tobytes() return reversed_data"},{"question":"**Title:** Implementing a Distributed Optimizer in PyTorch **Objective:** To assess your understanding of distributed optimizers in PyTorch by implementing a function that sets up and runs a simple distributed training procedure using CPU tensors. **Problem Statement:** You are given the task of implementing a distributed training setup for a simple linear model using PyTorch\'s distributed optimizers. Specifically, you\'ll use `torch.distributed.optim.DistributedOptimizer` to manage the optimization process across multiple processes. **Requirements:** 1. Implement a function `distributed_training` that performs the following tasks: - Initializes a PyTorch `DistributedOptimizer` for a linear model. - Sets up a distributed environment using `torch.distributed`. - Trains the model on dummy data for a given number of epochs. - Ensures that the optimizer works correctly across the specified number of worker processes. 2. The linear model should have a single layer: `torch.nn.Linear(input_size, output_size)`. 3. The function should use dummy data (e.g., random tensors) for training data and targets. **Input:** - Number of worker processes (int): Number of processes to run concurrently in a distributed fashion. - Number of epochs (int): Number of epochs to train the model. - Input size (int): Size of the input features. - Output size (int): Size of the output features. **Output:** - The final model parameters after training (list of `torch.Tensor`). **Constraints:** - Ensure the function runs exclusively on CPU, as CUDA tensors are not supported with the distributed optimizers. - Use random tensors for input data and corresponding targets. - Synchronize processes correctly to reflect distributed optimization. **Example Usage:** ```python def distributed_training(num_workers: int, epochs: int, input_size: int, output_size: int): # Implementation here pass final_params = distributed_training(num_workers=4, epochs=10, input_size=5, output_size=2) print(final_params) ``` Implement the missing `distributed_training` function keeping these constraints in mind. You need to define all necessary synchronization and process handling using PyTorch\'s distributed package. **Note:** You might need to configure the environment properly to run multiprocessing code in Jupyter Notebook or other environments.","solution":"import torch import torch.distributed as dist from torch.multiprocessing import Process import torch.nn as nn import torch.optim as optim def train(rank, size, input_size, output_size, epochs, model, optimizer): # Dummy data inputs = torch.randn(100, input_size) targets = torch.randn(100, output_size) criterion = nn.MSELoss() for epoch in range(epochs): optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, targets) loss.backward() optimizer.step() print(f\'Rank {rank}, Epoch [{epoch}/{epochs}], Loss: {loss.item()}\') def init_process(rank, size, input_size, output_size, epochs, fn, backend=\'gloo\'): Initialize the distributed environment. dist.init_process_group(backend, rank=rank, world_size=size) model = nn.Linear(input_size, output_size) optimizer = dist.optim.DistributedOptimizer( optim.SGD, model.parameters(), lr=0.01 ) fn(rank, size, input_size, output_size, epochs, model, optimizer) def distributed_training(num_workers, epochs, input_size, output_size): processes = [] for rank in range(num_workers): p = Process(target=init_process, args=(rank, num_workers, input_size, output_size, epochs, train)) p.start() processes.append(p) for p in processes: p.join() # Dummy model to return the parameters (real implementation should sync parameters) model = nn.Linear(input_size, output_size) return list(model.parameters())"},{"question":"**Problem Statement: Complex Assignment and Exception Handling** **Objective:** You are required to implement a Python function `process_data(data: str) -> dict` that: 1. Takes a single input, `data`, which is a string. 2. Extracts assignment operations from the string and evaluates them. 3. Returns a dictionary representing the final state of variables after evaluating all assignments. 4. Properly handles any exceptions that occur during the evaluation of assignments. **Input:** - `data`: A string containing multiple lines of assignments, each on a new line. - Each assignment is in the form of `var = value` where `var` is a string (variable name) and `value` is an integer or another variable name that has already been defined. - The string can also contain comments which should be ignored (comments start with `#`). **Output:** - A dictionary containing the final values of all variables defined in the input string. **Constraints:** - The input string will contain valid syntax for assignments. - Assume all variables will be valid Python identifiers. - Integer values are within the range of -1000 to 1000. **Example:** ```python data = \'\'\' a = 5 b = 10 c = a + b # comment d = c - b e = d # another comment \'\'\' output = process_data(data) # output: {\'a\': 5, \'b\': 10, \'c\': 15, \'d\': 5, \'e\': 5} ``` **Performance Requirements:** - The solution should efficiently process the input string with multiple lines, ensuring that each assignment is evaluated in order and dependencies between variables are correctly resolved. **Function Signature:** ```python def process_data(data: str) -> dict: pass ``` **Advanced Requirements:** 1. You should handle simple assertions using `assert` statements within the function where necessary. 2. Use Python\'s built-in exception handling constructs to manage any errors in variable assignments. 3. Ensure that all valid Python identifiers comply with Python variable naming rules. **Notes:** - This task assesses your understanding of Python syntax rules, exception handling, and code execution flow through different forms of assignments. - Incorporate augmented assignments (like `+=`, `-=`, etc.) where possible in the input data, ensuring the function handles them correctly. - Ignore any lines that contain comments or are empty. Good luck and ensure your solution is both correct and optimized!","solution":"def process_data(data: str) -> dict: Process a string of assignments and return a dictionary of variable values. variables = {} for line in data.split(\'n\'): line = line.split(\'#\')[0].strip() # Remove comments and whitespace if not line: continue try: var, expr = line.split(\'=\') var = var.strip() expr = expr.strip() # Evaluate the expression in the context of the variables dictionary # Using eval is risky, however we assume input is given as problem constraints. variables[var] = eval(expr, {}, variables) except Exception as e: print(f\\"Error processing line: {line}n{e}\\") return variables"},{"question":"**Objective:** Demonstrate your understanding of scikit-learn by designing a machine-learning pipeline from data preprocessing to model evaluation. **Background:** Given a dataset containing various features of houses, you are tasked with predicting the house prices. The dataset contains both numerical and categorical features, and some of the entries are missing values. **Dataset Description:** * `house_data.csv` - Each row represents a house with columns: - `id`: Unique identifier for each house - `price`: Price of the house (target variable) - `bedrooms`: Number of bedrooms (numerical) - `bathrooms`: Number of bathrooms (numerical) - `sqft_living`: Square footage of living area (numerical) - `sqft_lot`: Square footage of the lot area (numerical) - `floors`: Number of floors (numerical) - `waterfront`: Whether the house has a waterfront (categorical, \'yes\' or \'no\') - `view`: Quality of the view (categorical, \'excellent\', \'good\', \'average\', \'bad\') - `condition`: Condition of the house (categorical, \'excellent\', \'good\', \'average\', \'bad\') - `grade`: Overall grade of the house (numerical) - `yr_built`: Year the house was built (numerical) **Tasks:** 1. **Load the data:** - Read `house_data.csv` into a Pandas DataFrame. 2. **Data Preprocessing:** - Handle missing values: Fill missing numerical values with the median of the respective column, and fill missing categorical values with the mode of the respective column. - Convert categorical features into numerical using one-hot encoding. - Split the data into training (80%) and testing (20%) sets. 3. **Model Selection and Training:** - Select a suitable regression model from scikit-learn. - Train the model on the training data. 4. **Evaluation:** - Evaluate the model using Root Mean Squared Error (RMSE) on the test data. - Output the RMSE score. 5. **Performance Consideration:** - Ensure transformations and model fitting are done efficiently, considering computation time and memory usage. **Input:** - `house_data.csv` **Output:** - Print the RMSE score of your model on the test data. **Constraints:** - Use scikit-learn for data preprocessing and modeling. - Ensure reproducibility by setting a random seed for data splitting. - Assume the dataset fits in memory. **Example:** ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import numpy as np # Sample code structure (without full implementation): # Step 1: Load the data df = pd.read_csv(\'house_data.csv\') # Step 2: Data Preprocessing # ... (fill missing values, one-hot encode, split data) # Step 3: Model Selection and Training # ... (setup pipeline, train model) # Step 4: Evaluation # ... (predict and evaluate RMSE) # Print RMSE score print(\\"RMSE:\\", rmse_score) ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import numpy as np def predict_house_prices(file_path): # Load the data df = pd.read_csv(file_path) # Separate target variable and features X = df.drop(columns=[\'id\', \'price\']) y = df[\'price\'] # Identifying numerical and categorical features numerical_features = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_features = [\'waterfront\', \'view\', \'condition\'] # Data preprocessing pipelines numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Creating the model pipeline model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', RandomForestRegressor(n_estimators=100, random_state=0)) ]) # Splitting data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the model model.fit(X_train, y_train) # Predict and evaluate y_pred = model.predict(X_test) rmse = np.sqrt(mean_squared_error(y_test, y_pred)) return rmse # Example usage # rmse_score = predict_house_prices(\'house_data.csv\') # print(\\"RMSE:\\", rmse_score)"},{"question":"**Objective:** Implement and optimize a machine learning pipeline using scikit-learn to understand the influence of model complexity, data sparsity, and feature extraction latency on prediction latency and throughput. **Problem Statement:** You are required to build a machine learning model using the Scikit-learn package and perform several configurations to observe their impacts on prediction latency and throughput. The task involves the following steps: 1. **Generate Synthetic Data:** - Create a synthetic dataset with a specific number of features and instances. Represent the data in both dense and sparse matrix formats. 2. **Model Training:** - Train a linear regression model and a decision tree model using the synthetic data. 3. **Evaluate and Compare Prediction Latency and Throughput:** - Measure the prediction latency and throughput for the trained models on dense and sparse data formats. - Optimize the model by tuning parameters such as the number of features and using sparse input representation. Measure how these optimizations affect prediction latency. 4. **Report and Analysis:** - Provide a detailed report summarizing the impact of the different configurations on the model\'s performance. **Implementation Steps and Constraints:** 1. **Input Data:** - `n_features`: Number of features in the synthetic dataset. - `n_instances`: Number of instances in the synthetic dataset. - Ensure data is generated in both dense and sparse formats. 2. **Model Training and Configuration:** - Implement linear regression using `sklearn.linear_model.SGDRegressor` and decision tree using `sklearn.tree.DecisionTreeRegressor`. - Allow the model to be trained and predict using both dense and sparse data formats. 3. **Performance Measurement:** - Develop a function to measure prediction latency (time taken to make predictions). - Develop a function to measure prediction throughput (number of predictions per second). 4. **Optimization:** - Modify the number of features and adjust the model settings to improve latency and throughput. - Implement sparse input representation and assess the impact on prediction latency and throughput. **Expected Functions:** ```python import time import numpy as np from scipy.sparse import csr_matrix from sklearn.linear_model import SGDRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error def generate_synthetic_data(n_features, n_instances, sparse=False): Generates synthetic data for testing. Parameters: - n_features: int, number of features. - n_instances: int, number of instances. - sparse: bool, default=False, Data return format. Returns: - X: Features matrix. - y: Target vector. X = np.random.randn(n_instances, n_features) y = np.random.randn(n_instances) if sparse: X = csr_matrix(X) return X, y def measure_prediction_latency(model, X, y): Measures prediction latency of the model. Parameters: - model: Trained model. - X: Features matrix. - y: Target vector. Returns: - latency: float, prediction time in seconds. start_time = time.time() predictions = model.predict(X) end_time = time.time() latency = end_time - start_time mse = mean_squared_error(y, predictions) return latency, mse def measure_prediction_throughput(model, X, batch_size): Measures prediction throughput of the model. Parameters: - model: Trained model. - X: Features matrix. - batch_size: int, number of instances per batch. Returns: - throughput: float, predictions per second. start_time = time.time() for i in range(0, len(X), batch_size): model.predict(X[i:i + batch_size]) end_time = time.time() total_time = end_time - start_time throughput = len(X) / total_time return throughput def main(): # Generate synthetic data n_features = 1000 n_instances = 10000 X_dense, y = generate_synthetic_data(n_features, n_instances) X_sparse, _ = generate_synthetic_data(n_features, n_instances, sparse=True) # Train models linear_regressor = SGDRegressor() decision_tree = DecisionTreeRegressor() linear_regressor.fit(X_dense, y) decision_tree.fit(X_dense, y) # Evaluate models latency_dense_lr, mse_dense_lr = measure_prediction_latency(linear_regressor, X_dense, y) latency_sparse_lr, mse_sparse_lr = measure_prediction_latency(linear_regressor, X_sparse, y) throughput_dense_lr = measure_prediction_throughput(linear_regressor, X_dense, batch_size=10) throughput_sparse_lr = measure_prediction_throughput(linear_regressor, X_sparse, batch_size=10) latency_dense_dt, mse_dense_dt = measure_prediction_latency(decision_tree, X_dense, y) latency_sparse_dt, mse_sparse_dt = measure_prediction_latency(decision_tree, X_sparse, y) throughput_dense_dt = measure_prediction_throughput(decision_tree, X_dense, batch_size=10) throughput_sparse_dt = measure_prediction_throughput(decision_tree, X_sparse, batch_size=10) print(\\"Linear Regressor - Dense Data: Latency:\\", latency_dense_lr, \\"MSE:\\", mse_dense_lr, \\"Throughput:\\", throughput_dense_lr) print(\\"Linear Regressor - Sparse Data: Latency:\\", latency_sparse_lr, \\"MSE:\\", mse_sparse_lr, \\"Throughput:\\", throughput_sparse_lr) print(\\"Decision Tree - Dense Data: Latency:\\", latency_dense_dt, \\"MSE:\\", mse_dense_dt, \\"Throughput:\\", throughput_dense_dt) print(\\"Decision Tree - Sparse Data: Latency:\\", latency_sparse_dt, \\"MSE:\\", mse_sparse_dt, \\"Throughput:\\", throughput_sparse_dt) # Call the main function to execute main() ``` **Constraints:** - Use only scikit-learn, numpy, scipy, and standard Python libraries. - Make sure the implementation is optimized for performance. **Expected Output:** Provide a summary of the prediction latency and throughput under different configurations and optimization techniques along with any observations or conclusions regarding the impact of these changes on the model\'s performance.","solution":"import time import numpy as np from scipy.sparse import csr_matrix from sklearn.linear_model import SGDRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error def generate_synthetic_data(n_features, n_instances, sparse=False): Generates synthetic data for testing. Parameters: - n_features: int, number of features. - n_instances: int, number of instances. - sparse: bool, default=False, Data return format. Returns: - X: Features matrix. - y: Target vector. X = np.random.randn(n_instances, n_features) y = np.random.randn(n_instances) if sparse: X = csr_matrix(X) return X, y def measure_prediction_latency(model, X, y): Measures prediction latency of the model. Parameters: - model: Trained model. - X: Features matrix. - y: Target vector. Returns: - latency: float, prediction time in seconds. start_time = time.time() predictions = model.predict(X) end_time = time.time() latency = end_time - start_time mse = mean_squared_error(y, predictions) return latency, mse def measure_prediction_throughput(model, X, batch_size): Measures prediction throughput of the model. Parameters: - model: Trained model. - X: Features matrix. - batch_size: int, number of instances per batch. Returns: - throughput: float, predictions per second. start_time = time.time() for i in range(0, len(X), batch_size): model.predict(X[i:i + batch_size]) end_time = time.time() total_time = end_time - start_time throughput = len(X) / total_time return throughput def main(): # Generate synthetic data n_features = 1000 n_instances = 10000 X_dense, y = generate_synthetic_data(n_features, n_instances) X_sparse, _ = generate_synthetic_data(n_features, n_instances, sparse=True) # Train models linear_regressor = SGDRegressor() decision_tree = DecisionTreeRegressor() linear_regressor.fit(X_dense, y) decision_tree.fit(X_dense, y) # Evaluate models latency_dense_lr, mse_dense_lr = measure_prediction_latency(linear_regressor, X_dense, y) latency_sparse_lr, mse_sparse_lr = measure_prediction_latency(linear_regressor, X_sparse, y) throughput_dense_lr = measure_prediction_throughput(linear_regressor, X_dense, batch_size=10) throughput_sparse_lr = measure_prediction_throughput(linear_regressor, X_sparse, batch_size=10) latency_dense_dt, mse_dense_dt = measure_prediction_latency(decision_tree, X_dense, y) latency_sparse_dt, mse_sparse_dt = measure_prediction_latency(decision_tree, X_sparse, y) throughput_dense_dt = measure_prediction_throughput(decision_tree, X_dense, batch_size=10) throughput_sparse_dt = measure_prediction_throughput(decision_tree, X_sparse, batch_size=10) print(\\"Linear Regressor - Dense Data: Latency:\\", latency_dense_lr, \\"MSE:\\", mse_dense_lr, \\"Throughput:\\", throughput_dense_lr) print(\\"Linear Regressor - Sparse Data: Latency:\\", latency_sparse_lr, \\"MSE:\\", mse_sparse_lr, \\"Throughput:\\", throughput_sparse_lr) print(\\"Decision Tree - Dense Data: Latency:\\", latency_dense_dt, \\"MSE:\\", mse_dense_dt, \\"Throughput:\\", throughput_dense_dt) print(\\"Decision Tree - Sparse Data: Latency:\\", latency_sparse_dt, \\"MSE:\\", mse_sparse_dt, \\"Throughput:\\", throughput_sparse_dt) # Call the main function to execute if __name__ == \\"__main__\\": main()"},{"question":"# PyTorch Coding Assessment: Advanced Tensor Operations Objective: Implement a function that performs a series of advanced tensor operations. This task will test your understanding of tensor creation, manipulation, and mathematical operations using PyTorch. Problem Statement: Write a function `advanced_tensor_operations` that takes in a list of tensors and performs the following operations in sequence: 1. Create a new tensor by stacking the input tensors along a new dimension. 2. Compute the mean of this stacked tensor along the new dimension. 3. Subtract this mean tensor from each tensor in the original input list. 4. Square the result from the previous step. 5. Sum these squared tensors to get a final tensor. The function should return this final tensor and the mean tensor. ```python import torch from typing import List def advanced_tensor_operations(tensor_list: List[torch.Tensor]) -> (torch.Tensor, torch.Tensor): Perform a series of advanced tensor operations on a list of tensors: Args: tensor_list (List[torch.Tensor]): List of input tensors (all are of the same shape). Returns: final_tensor (torch.Tensor): The resulting tensor after performing specified operations. mean_tensor (torch.Tensor): The mean tensor computed along the new dimension. # Stack tensors along a new dimension stacked_tensor = torch.stack(tensor_list) # Compute the mean along the new dimension mean_tensor = torch.mean(stacked_tensor, dim=0) # Subtract the mean tensor from each tensor in the original list subtracted_tensors = [tensor - mean_tensor for tensor in tensor_list] # Square the result squared_tensors = [torch.square(tensor) for tensor in subtracted_tensors] # Sum the squared tensors final_tensor = torch.sum(torch.stack(squared_tensors), dim=0) return final_tensor, mean_tensor ``` Input Constraints: 1. The input list `tensor_list` will contain at least one tensor. 2. Each tensor in the list will have the same shape and datatype. Example: ```python t1 = torch.tensor([[1., 2.], [3., 4.]]) t2 = torch.tensor([[5., 6.], [7., 8.]]) t3 = torch.tensor([[9., 10.], [11., 12.]]) tensor_list = [t1, t2, t3] final_tensor, mean_tensor = advanced_tensor_operations(tensor_list) print(\\"Final Tensor:\\", final_tensor) print(\\"Mean Tensor:\\", mean_tensor) ``` Expected Output: ``` Final Tensor: tensor([[32., 32.], [32., 32.]]) Mean Tensor: tensor([[5., 6.], [7., 8.]]) ``` This question will assess your knowledge of tensor operations, how to leverage PyTorch\'s broadcasting, and basic understanding of tensor arithmetic and reduction functions.","solution":"import torch from typing import List def advanced_tensor_operations(tensor_list: List[torch.Tensor]) -> (torch.Tensor, torch.Tensor): Perform a series of advanced tensor operations on a list of tensors: Args: tensor_list (List[torch.Tensor]): List of input tensors (all are of the same shape). Returns: final_tensor (torch.Tensor): The resulting tensor after performing specified operations. mean_tensor (torch.Tensor): The mean tensor computed along the new dimension. # Stack tensors along a new dimension stacked_tensor = torch.stack(tensor_list) # Compute the mean along the new dimension mean_tensor = torch.mean(stacked_tensor, dim=0) # Subtract the mean tensor from each tensor in the original list subtracted_tensors = [tensor - mean_tensor for tensor in tensor_list] # Square the result squared_tensors = [torch.square(tensor) for tensor in subtracted_tensors] # Sum the squared tensors final_tensor = torch.sum(torch.stack(squared_tensors), dim=0) return final_tensor, mean_tensor"},{"question":"Python\'s `pydoc` module is a comprehensive tool for generating documentation from Python modules. For this assessment, you will implement a simplified version of some functionalities described in the `pydoc` documentation. Objectives 1. **Generate Documentation**: Create a function that generates documentation for a given module, class, or function. 2. **Console Output**: Implement functionality to print this documentation to the console. Requirements 1. **Documentation Function**: - Create a function `generate_doc(target: str) -> str` that takes a target string representing the name of a module, class, or function. - The function should return a string containing documentation derived from the docstring or comments of the specified target. - If the target does not have a docstring or comments, return `\\"No documentation available.\\"`. 2. **Console Output Function**: - Create a function `print_doc(target: str) -> None` that takes a target string and prints the documentation to the console. - This should use the `generate_doc` function to get the documentation. You are allowed to use Python\'s built-in `inspect` module to fetch information about objects if needed. Input and Output - **Input**: A valid Python target string (module, class, or function name). - **Output**: The appropriate documentation string or a message signifying lack of documentation. Constraints - Assume the target will be importable in your Python environment. - The target string should be properly formatted as `module`, `module.class`, `module.function`, etc. Example ```python # Example module: example.py Example module demonstrating a basic documentation. class ExampleClass: This is an example class. def example_method(self): This is an example method. pass def example_function(): This is an example function. pass ``` ```python # Using the functions print(generate_doc(\'example.ExampleClass\')) # Output: \\"This is an example class.\\" print(generate_doc(\'example.example_function\')) # Output: \\"This is an example function.\\" print_doc(\'example.ExampleClass.example_method\') # Output to console: \\"This is an example method.\\" ``` Implement the functions according to the given objectives and requirements. Note - This assessment is designed to test your understanding of modules, docstrings, and the `inspect` module in Python, as well as basic string manipulation and function implementation.","solution":"import importlib import inspect def generate_doc(target: str) -> str: Generates the documentation for a specified module, class, or function. Args: target (str): The name of the module, class, or function as a string. Returns: str: The documentation string if available, otherwise \\"No documentation available.\\" try: components = target.split(\'.\') module_name = components[0] module = importlib.import_module(module_name) obj = module for comp in components[1:]: obj = getattr(obj, comp) doc = inspect.getdoc(obj) return doc if doc else \\"No documentation available.\\" except Exception as e: return str(e) def print_doc(target: str) -> None: Prints the documentation for a specified module, class, or function to the console. Args: target (str): The name of the module, class, or function as a string. doc = generate_doc(target) print(doc)"},{"question":"Objective Your task is to implement a function that organizes a list of filenames by matching them against provided patterns, using the `fnmatch` module. The function will segregate filenames into categories based on these patterns. Function Signature ```python def organize_files(filenames: list[str], patterns: dict[str, str]) -> dict[str, list[str]]: pass ``` Input - `filenames`: A list of strings, where each string represents a filename. - `patterns`: A dictionary where keys are category names (strings) and values are patterns (strings) that filenames must match to fall into that category. Output - A dictionary where keys are the category names and values are lists of filenames that match the category\'s pattern. If no filenames match a pattern, the corresponding list should be empty. Constraints - Filenames will be case-sensitive (use `fnmatchcase` function). - Each filename should only appear in one category — the first category (order based on the provided dictionary) that its pattern matches. - Patterns will be valid Unix shell-style patterns. - The length of `filenames` list and `patterns` dictionary will not exceed 1000. Example ```python filenames = [\\"report1.txt\\", \\"REPORT2.TXT\\", \\"summary.docx\\", \\"image.png\\", \\"presentation.pptx\\"] patterns = { \\"Text Files\\": \\"*.txt\\", \\"Documents\\": \\"*.docx\\", \\"Presentations\\": \\"*.pptx\\", \\"Images\\": \\"*.png\\" } result = organize_files(filenames, patterns) ``` Expected output: ```python { \\"Text Files\\": [\\"report1.txt\\"], \\"Documents\\": [\\"summary.docx\\"], \\"Presentations\\": [\\"presentation.pptx\\"], \\"Images\\": [\\"image.png\\"] } ``` In this example, \\"REPORT2.TXT\\" is not in the \\"Text Files\\" category because the comparison is case-sensitive. Notes - Utilize functions from the `fnmatch` module to perform pattern matching. - Ensure the function checks patterns in the order they are provided in the dictionary. - Your implementation should maintain efficient performance given the constraints. Good luck, and happy coding!","solution":"import fnmatch def organize_files(filenames, patterns): Organizes a list of filenames into categories based on provided patterns. Parameters: - filenames: a list of filename strings. - patterns: a dictionary with category names as keys and patterns as values. Returns: - A dictionary with category names as keys and lists of filenames that match the patterns as values. categorized_files = {category: [] for category in patterns} for filename in filenames: for category, pattern in patterns.items(): if fnmatch.fnmatchcase(filename, pattern): categorized_files[category].append(filename) break return categorized_files"},{"question":"# Background TorchScript is an intermediate representation of a PyTorch model that can be run in a high-performance environment. However, not all PyTorch constructs are supported in TorchScript or may have divergent behavior. # Task You are to implement a custom neural network model and a forward propagation method that is compatible with TorchScript, considering the limitations and unsupported constructs. # Requirements: - Design a simple feedforward neural network using PyTorch that includes at least: 1. Two fully connected layers (`torch.nn.Linear`). 2. A ReLU activation function between the layers. 3. An output layer with a softmax activation function (`torch.nn.Softmax`). - Use `torch.jit.script` to convert your model to TorchScript. - Implement a custom forward method that uses `torch.nn.functional.relu` and `torch.nn.functional.softmax` instead of relying on `torch.nn.ReLU` and `torch.nn.Softmax` modules. # Input: - The model should accept a tensor of shape `(batch_size, input_dim)` where `input_dim` is the size of the input features. # Output: - The output should be a tensor of shape `(batch_size, output_dim)` where `output_dim` is the number of classes for the classification task. # Constraints: - Do not use any classes or functions listed as unsupported in the provided documentation (e.g., `torch.nn.RNN`, `torch.autograd.Function`). - Ensure your model is compatible with TorchScript. # Performance Requirements: - Your implementation should be efficient and handle typical input sizes for a feedforward neural network. # Example: ```python import torch import torch.nn as nn import torch.nn.functional as F class FeedForwardNN(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(FeedForwardNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = F.relu(self.fc1(x)) x = F.softmax(self.fc2(x), dim=1) return x # Example Usage input_dim = 10 hidden_dim = 5 output_dim = 3 model = FeedForwardNN(input_dim, hidden_dim, output_dim) # Convert to TorchScript scripted_model = torch.jit.script(model) # Test with random input input_tensor = torch.randn(8, input_dim) output_tensor = scripted_model(input_tensor) print(output_tensor) ``` # Notes: 1. Pay special attention to the supported methods and avoid using any unsupported features as per the provided documentation. 2. Ensure the model and the scripting run without errors and produce the expected output.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class FeedForwardNN(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(FeedForwardNN, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = F.relu(self.fc1(x)) x = F.softmax(self.fc2(x), dim=1) return x # Example Usage input_dim = 10 hidden_dim = 5 output_dim = 3 model = FeedForwardNN(input_dim, hidden_dim, output_dim) # Convert to TorchScript scripted_model = torch.jit.script(model) # Test with random input input_tensor = torch.randn(8, input_dim) output_tensor = scripted_model(input_tensor) print(output_tensor)"},{"question":"You are required to parse an XML document containing information about books in a library. The XML structure includes tags for `library`, `book`, `title`, `author`, and `year`. Your task is to use the `xml.sax` package to create a custom SAX `ContentHandler` to read and print out the titles and authors of all books published after the year 2000. # Expected Input: The input will be an XML string with the following structure: ```xml <library> <book> <title>The Da Vinci Code</title> <author>Dan Brown</author> <year>2003</year> </book> <book> <title>The Hobbit</title> <author>J.R.R. Tolkien</author> <year>1937</year> </book> <!-- Additional book entries --> </library> ``` # Expected Output: Titles and authors of all books published after the year 2000, printed in the format: ``` Title: The Da Vinci Code, Author: Dan Brown ``` # Requirements: 1. Create a SAX `ContentHandler` subclass that correctly processes the XML data. 2. Ensure your handler captures book titles and authors and only prints those published after the year 2000. 3. Implement error handling using `SAXParseException`. # Constraints: - You may assume the XML data is well-formed, but you must handle potential parsing errors. - Your solution must use the `xml.sax` package for XML parsing. # Example: Here is a sample example to demonstrate the expected functionality: ```python import xml.sax class LibraryContentHandler(xml.sax.ContentHandler): def __init__(self): self.currentData = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = 0 def startElement(self, tag, attributes): self.currentData = tag def endElement(self, tag): if tag == \\"book\\": if self.year > 2000: print(f\\"Title: {self.title}, Author: {self.author}\\") self.title = \\"\\" self.author = \\"\\" self.year = 0 self.currentData = \\"\\" def characters(self, content): if self.currentData == \\"title\\": self.title = content elif self.currentData == \\"author\\": self.author = content elif self.currentData == \\"year\\": self.year = int(content) def main(xml_string): parser = xml.sax.make_parser() handler = LibraryContentHandler() parser.setContentHandler(handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: print(f\\"Error parsing XML: {e.getMessage()}\\") # Sample XML input xml_data = <library> <book> <title>The Da Vinci Code</title> <author>Dan Brown</author> <year>2003</year> </book> <book> <title>The Hobbit</title> <author>J.R.R. Tolkien</author> <year>1937</year> </book> </library> # Execute main function with the provided XML data main(xml_data) ``` Your task is to complete the implementation of the `LibraryContentHandler` class and ensure it works as described.","solution":"import xml.sax class LibraryContentHandler(xml.sax.ContentHandler): def __init__(self): self.currentData = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = 0 def startElement(self, tag, attributes): self.currentData = tag def endElement(self, tag): if tag == \\"book\\": if self.year > 2000: print(f\\"Title: {self.title}, Author: {self.author}\\") self.title = \\"\\" self.author = \\"\\" self.year = 0 self.currentData = \\"\\" def characters(self, content): if self.currentData == \\"title\\": self.title = content elif self.currentData == \\"author\\": self.author = content elif self.currentData == \\"year\\": self.year = int(content) def parse_books(xml_string): parser = xml.sax.make_parser() handler = LibraryContentHandler() parser.setContentHandler(handler) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: print(f\\"Error parsing XML: {e.getMessage()}\\") # Sample XML input xml_data = <library> <book> <title>The Da Vinci Code</title> <author>Dan Brown</author> <year>2003</year> </book> <book> <title>The Hobbit</title> <author>J.R.R. Tolkien</author> <year>1937</year> </book> </library> # Execute parse_books function with the provided XML data parse_books(xml_data)"},{"question":"# Unicode String Processing in Python Problem Statement You are provided with a text file containing various Unicode characters. The file may include characters from multiple languages and symbols such as emojis. Your task is to implement a Python function that performs the following operations: 1. **Read** the contents of the file and decode it using UTF-8 encoding. 2. **Normalize** the Unicode strings to a consistent form using Normalization Form Canonical Composition (NFC). 3. **Extract and Print** all distinct characters in the file along with their Unicode code points. 4. **Sort** the characters based on their Unicode code points. 5. **Count** the occurrences of each character in the file and print the count. 6. **Handle Errors** gracefully by ignoring invalid characters. Function Signature ```python def process_unicode_file(file_path: str) -> None: # Implement your function here ``` Input - `file_path` (str): The path to the UTF-8 encoded text file. Output - Print the following information: - Each unique character along with its Unicode code point. - The characters sorted by their Unicode code points. - The occurrence count for each character. Constraints - Ensure that your function can handle large files efficiently. - Any invalid characters should be ignored without causing the program to crash. Example Suppose the content of the file is as follows: ``` Smile 😊, and the world smiles with you. Smile 😊! ``` Your function should process this and output: ``` Character: \' \' | Code Point: U+0020 Character: \'!\' | Code Point: U+0021 Character: \',\' | Code Point: U+002C Character: \'.\' | Code Point: U+002E Character: \'S\' | Code Point: U+0053 Character: \'a\' | Code Point: U+0061 Character: \'d\' | Code Point: U+0064 Character: \'e\' | Code Point: U+0065 Character: \'h\' | Code Point: U+0068 Character: \'i\' | Code Point: U+0069 Character: \'l\' | Code Point: U+006C Character: \'m\' | Code Point: U+006D Character: \'n\' | Code Point: U+006E Character: \'o\' | Code Point: U+006F Character: \'r\' | Code Point: U+0072 Character: \'s\' | Code Point: U+0073 Character: \'t\' | Code Point: U+0074 Character: \'u\' | Code Point: U+0075 Character: \'w\' | Code Point: U+0077 Character: \'😊\' | Code Point: U+1F60A Sorted Characters by Code Point: \' \' (U+0020) \'!\' (U+0021) \',\' (U+002C) \'.\' (U+002E) \'S\' (U+0053) \'a\' (U+0061) \'d\' (U+0064) \'e\' (U+0065) \'h\' (U+0068) \'i\' (U+0069) \'l\' (U+006C) \'m\' (U+006D) \'n\' (U+006E) \'o\' (U+006F) \'r\' (U+0072) \'s\' (U+0073) \'t\' (U+0074) \'u\' (U+0075) \'w\' (U+0077) \'😊\' (U+1F60A) Character Occurrences: \' \' : 10 \'!\' : 2 \',\' : 1 \'.\' : 2 \'S\' : 2 \'a\' : 2 \'d\' : 1 \'e\' : 2 \'h\' : 1 \'i\' : 3 \'l\' : 2 \'m\' : 2 \'n\' : 1 \'o\' : 1 \'r\' : 1 \'s\' : 3 \'t\' : 1 \'u\' : 1 \'w\' : 1 \'😊\' : 2 ``` - Remember to handle the invalid characters gracefully, and ensure your function works efficiently for large files.","solution":"import unicodedata def process_unicode_file(file_path: str) -> None: try: with open(file_path, \'r\', encoding=\'utf-8\', errors=\'ignore\') as file: content = file.read() normalized_content = unicodedata.normalize(\'NFC\', content) # Extract distinct characters and their code points characters = set(normalized_content) char_code_points = [(char, ord(char)) for char in characters] # Sort characters by their Unicode code points sorted_chars = sorted(char_code_points, key=lambda x: x[1]) # Count occurrences of each character char_count = {char: normalized_content.count(char) for char in characters} # Print unique characters and their code points print(\\"Unique Characters and their Unicode code points:\\") for char, code_point in sorted_chars: print(f\\"Character: \'{char}\' | Code Point: U+{code_point:04X}\\") # Print sorted characters by code point print(\\"nSorted Characters by Code Point:\\") for char, code_point in sorted_chars: print(f\\"\'{char}\' (U+{code_point:04X})\\") # Print character occurrences print(\\"nCharacter Occurrences:\\") for char, count in char_count.items(): print(f\\"\'{char}\' : {count}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Extended Manual Polynomial Regression in PyTorch In this assessment, you are required to extend the functionality of PyTorch by implementing a custom polynomial regression layer and the corresponding autograd function. You will demonstrate your understanding of both PyTorch modules and custom autograd functions. Your task is divided into multiple steps: # Step 1: Create a Custom Autograd Function You need to implement a custom autograd function named `PolynomialFunction` to compute the forward pass and the gradient for a polynomial transformation of input data. 1. Subclass `torch.autograd.Function`. 2. Define three methods: - `forward(ctx, input, coefficients)` to compute the polynomial transformation using the coefficients. - `setup_context(ctx, inputs, output)` to save the input and coefficients for the backward pass. - `backward(ctx, grad_output)` to compute the gradient of the inputs and coefficients. **Input:** - `input`: A tensor of shape `(N, D)` representing the input features. - `coefficients`: A tensor of shape `(P,)` where `P` is the order of the polynomial plus one. **Output:** - A tensor of shape `(N, D)` representing the transformed features. **Constraints:** - Ensure the forward pass computes the polynomial transformation as `output = sum(c_i * x^i)` where `c_i` are the coefficients. - The backward method should return gradients for both the input and the coefficients. # Step 2: Create a Custom PyTorch Module Implement a PyTorch module named `PolynomialRegression` that uses the custom autograd function defined in Step 1. This module should behave like a linear layer but instead apply a polynomial transformation to the input features. 1. Subclass `torch.nn.Module`. 2. Implement the `__init__` method to initialize the coefficients as a learnable parameter. 3. Define the `forward` method to use the `PolynomialFunction`. **Input:** - `input`: A tensor of shape `(N, D)`. **Output:** - A tensor of shape `(N, 1)` representing the regression output. **Constraints:** - Initialize the coefficients randomly (using a normal or uniform distribution). - Ensure the coefficients require gradients. # Step 3: Polynomial Regression Training Write a training loop to fit the polynomial regression model to a simple dataset. Use Mean Squared Error (MSE) as the loss function and stochastic gradient descent (SGD) as the optimizer. **Dataset:** - Generate a dataset `(X, y)` where `X` is a tensor of shape `(N, D)` uniformly sampled from the range `[0, 1]`. - Define `y` as a polynomial function of `X` plus some Gaussian noise. **Requirements:** - Implement the data generation. - Train the `PolynomialRegression` model to fit the generated dataset. - Plot the loss curve showing the training process. **Example:** ```python import torch from torch import nn import torch.optim as optim import matplotlib.pyplot as plt import numpy as np # Define the dataset N, D = 100, 1 X = torch.rand(N, D) true_coefficients = torch.tensor([1.5, -2.3, 0.7]) y = sum(true_coefficients[i] * X**i for i in range(len(true_coefficients))) + 0.1 * torch.randn(N, D) # Define custom autograd function class PolynomialFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, coefficients): # Implement the forward pass to compute polynomial transformation output = sum(coefficients[i] * input**i for i in range(len(coefficients))) ctx.save_for_backward(input, coefficients) return output @staticmethod def setup_context(ctx, inputs, output): input, coefficients = inputs ctx.save_for_backward(input, coefficients) @staticmethod def backward(ctx, grad_output): input, coefficients = ctx.saved_tensors grad_input = grad_coefficients = None if ctx.needs_input_grad[0]: grad_input = sum(grad_output * i * coefficients[i] * input**(i-1) for i in range(1, len(coefficients))) if ctx.needs_input_grad[1]: grad_coefficients = torch.stack([torch.sum(grad_output * input**i) for i in range(len(coefficients))]) return grad_input, grad_coefficients # Define the custom module class PolynomialRegression(nn.Module): def __init__(self, poly_order): super().__init__() self.coefficients = nn.Parameter(torch.randn(poly_order + 1)) def forward(self, input): return PolynomialFunction.apply(input, self.coefficients) # Define and train the model model = PolynomialRegression(poly_order=2) criterion = nn.MSELoss() optimizer = optim.SGD(model.parameters(), lr=0.1) losses = [] for epoch in range(500): optimizer.zero_grad() outputs = model(X) loss = criterion(outputs, y) loss.backward() optimizer.step() losses.append(loss.item()) # Plot the loss curve plt.plot(losses) plt.xlabel(\'Epoch\') plt.ylabel(\'Loss\') plt.title(\'Training Loss Curve\') plt.show() ``` This example serves as a reference. You need to implement your own version following the steps provided. Make sure to thoroughly test your implementation for correctness. **Note:** Make sure your implementation works with higher polynomial orders and different datasets.","solution":"import torch from torch import nn class PolynomialFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, coefficients): output = sum(coefficients[i] * input**i for i in range(len(coefficients))) ctx.save_for_backward(input, coefficients) return output @staticmethod def backward(ctx, grad_output): input, coefficients = ctx.saved_tensors grad_input = sum(grad_output * i * coefficients[i] * input**(i-1) for i in range(1, len(coefficients))) grad_coefficients = torch.stack([torch.sum(grad_output * input**i) for i in range(len(coefficients))]) return grad_input, grad_coefficients class PolynomialRegression(nn.Module): def __init__(self, poly_order): super().__init__() self.coefficients = nn.Parameter(torch.randn(poly_order + 1)) def forward(self, input): return PolynomialFunction.apply(input, self.coefficients)"},{"question":"# Question Background: The Python `ast` (Abstract Syntax Tree) module is a powerful toolkit for analyzing and modifying Python code. By transforming code into an abstract syntax tree, we can inspect and manipulate its components programmatically. Objective: Write a function `find_function_calls` that takes the source code of a Python program as a string input and returns a list of function names that are called within this code. Function Signature: ```python def find_function_calls(source_code: str) -> list: pass ``` Input: - `source_code` (str): A string representing Python source code. Output: - A list of function names (str) that are called within the given source code. The functions should be listed in the order they appear in the code. Constraints: - The source code will syntactically be a valid Python program. - The function names should be unique within the list of function calls. - Performance is not the primary concern, but the solution should not be overly inefficient. Requirements: 1. Use the `ast` module to parse and analyze the Python source code. 2. Traverse the AST to find function call nodes. 3. Collect and return the names of the functions that are called. Example: ```python source_code = \'\'\' def foo(): bar() baz() return qux() foo() \'\'\' print(find_function_calls(source_code)) # Output should be [\'bar\', \'baz\', \'qux\', \'foo\'] ``` Notes: - You may assume that any function defined within the provided source code does not have the same name as any function from imported modules. - The output list should maintain the order of first appearance of each function call. Additional Guidance: - Refer to the [`ast` documentation](https://docs.python.org/3/library/ast.html) for a better understanding of how to work with Abstract Syntax Trees. - You might find `ast.NodeVisitor` and `ast.walk` helpful for traversing the tree.","solution":"import ast def find_function_calls(source_code: str) -> list: Finds and returns a list of unique function names that are called within the given source code. class FunctionCallVisitor(ast.NodeVisitor): def __init__(self): self.calls = [] def visit_Call(self, node): if isinstance(node.func, ast.Name): if node.func.id not in self.calls: self.calls.append(node.func.id) self.generic_visit(node) tree = ast.parse(source_code) visitor = FunctionCallVisitor() visitor.visit(tree) return visitor.calls"},{"question":"# Hashing and Secure Password Storage System Your task is to create a simple application that performs secure password storage using Python\'s `hashlib` module. The application will include functionalities to create user accounts, store hashed passwords, and verify login credentials. You will use the `pbkdf2_hmac` function for password hashing to ensure security. Requirements: 1. **User Registration**: - Develop a function `register_user(username: str, password: str) -> str` that takes a username and a password, hashes the password using `hashlib.pbkdf2_hmac`, and returns a confirmation message. - Save the hashed password along with a random salt in a dictionary named `user_data` with the username as the key. 2. **User Login**: - Develop a function `login_user(username: str, password: str) -> bool` that takes a username and a password, hashes the password using the stored salt, and compares it with the stored hashed password. - Return `True` if the credentials match, otherwise return `False`. 3. **Security Constraints**: - Use SHA-256 as the hash function. - Use a salt of 16 bytes (generated using `os.urandom`). - Perform 100,000 iterations for the hashing process. - Store both the salt and the hashed password. Input/Output Formats: - The input to `register_user` is two strings: `username` and `password`. - The return value of `register_user` is a string message confirming the creation of a user. - The input to `login_user` is two strings: `username` and `password`. - The return value of `login_user` is a boolean indicating whether the login was successful. Constraints: - You can assume that usernames are unique and consist of alphanumeric characters. - Passwords are non-empty and can contain any printable characters. Example: ```python # Example usage register_user(\\"alice\\", \\"alice_password\\") register_user(\\"bob\\", \\"bob_password\\") login_user(\\"alice\\", \\"wrong_password\\") # Returns: False login_user(\\"alice\\", \\"alice_password\\") # Returns: True ``` # Your Solution: ```python import hashlib import os # Dictionary to store user data user_data = {} def register_user(username: str, password: str) -> str: salt = os.urandom(16) hashed_password = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000) user_data[username] = (salt, hashed_password) return f\\"User \'{username}\' registered successfully.\\" def login_user(username: str, password: str) -> bool: if username not in user_data: return False salt, stored_hashed_password = user_data[username] hashed_password = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000) return hashed_password == stored_hashed_password # Example usage print(register_user(\\"alice\\", \\"alice_password\\")) print(register_user(\\"bob\\", \\"bob_password\\")) print(login_user(\\"alice\\", \\"wrong_password\\")) # False print(login_user(\\"alice\\", \\"alice_password\\")) # True ``` Implement the `register_user` and `login_user` functions to meet the specified requirements, ensuring the secure storage and verification of passwords.","solution":"import hashlib import os # Dictionary to store user data user_data = {} def register_user(username: str, password: str) -> str: salt = os.urandom(16) hashed_password = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000) user_data[username] = (salt, hashed_password) return f\\"User \'{username}\' registered successfully.\\" def login_user(username: str, password: str) -> bool: if username not in user_data: return False salt, stored_hashed_password = user_data[username] hashed_password = hashlib.pbkdf2_hmac(\'sha256\', password.encode(), salt, 100000) return hashed_password == stored_hashed_password"},{"question":"Objective The goal of this exercise is to test your understanding of Python\'s bytes objects and your ability to work with them. You will need to implement a function that processes data using bytes objects. Problem Statement You need to implement a function `process_data`, which accepts a list of strings and performs the following operations: 1. Converts each string into a bytes object. 2. Concatenates all bytes objects into a single bytes object. 3. Returns the concatenated bytes object and its size. Function Signature ```python def process_data(data: list) -> tuple: :param data: List of strings to be processed. :return: A tuple containing the concatenated bytes object and its size. ``` Input - `data`: A list of strings (`List[str]`). Each string in the list will contain printable ASCII characters. Output - Returns a tuple: - The first element is a single bytes object that is the result of concatenating the bytes representation of all strings from the input list. - The second element is an integer representing the size of the final concatenated bytes object. Constraints - The input list will contain at most 1000 strings. - Each string will have a maximum length of 1000 characters. Example ```python data = [\\"Hello\\", \\"World\\"] result = process_data(data) print(result) # Output: (b\'HelloWorld\', 10) ``` Notes - You can use Python\'s built-in functions for bytes conversion and concatenation. - You need to handle all operations using the bytes type and ensure the final size is correctly calculated.","solution":"def process_data(data: list) -> tuple: Processes a list of strings by converting them to bytes, concatenating them, and returning the results. :param data: List of strings to be processed. :return: A tuple containing the concatenated bytes object and its size. bytes_objects = [s.encode(\'utf-8\') for s in data] concatenated_bytes = b\'\'.join(bytes_objects) return concatenated_bytes, len(concatenated_bytes)"},{"question":"# Advanced Python Coding Assessment Question: Implementing a Custom Data Structure with Advanced Features **Objective:** Design and implement a custom data structure `AdvancedDeque` that combines the functionality of a double-ended queue (deque) with additional features. This data structure should allow elements to be added or removed from either end efficiently, and it should support advanced operations such as reversing, rotation, and slicing. **Requirements:** 1. **Initialization:** The `AdvancedDeque` should be initialized with an optional iterable to populate the deque. 2. **Basic Operations:** - `append_left(x)`: Add an element `x` to the left end of the deque. - `append_right(x)`: Add an element `x` to the right end of the deque. - `pop_left()`: Remove and return an element from the left end of the deque. Raise an exception if the deque is empty. - `pop_right()`: Remove and return an element from the right end of the deque. Raise an exception if the deque is empty. 3. **Advanced Operations:** - `reverse()`: Reverse the elements in the deque. - `rotate(n)`: Rotate the deque `n` steps to the right. If `n` is negative, rotate to the left. - `slice(start, end, step)`: Return a list of elements from the deque following Python\'s list slicing rules with `start`, `end`, and `step`. 4. **Magic Methods:** - Implement the `__len__` method to return the number of elements in the deque. - Implement the `__str__` method to return a string representation of the deque. **Constraints:** - The performance of the basic operations (`append_left`, `append_right`, `pop_left`, `pop_right`) should be O(1) in average time complexity. - The `reverse`, `rotate`, and `slice` operations should be implemented efficiently. **Example Usage:** ```python # Initialization dq = AdvancedDeque([1, 2, 3, 4, 5]) # Basic Operations dq.append_left(0) # Deque: [0, 1, 2, 3, 4, 5] dq.append_right(6) # Deque: [0, 1, 2, 3, 4, 5, 6] dq.pop_left() # Returns 0, Deque: [1, 2, 3, 4, 5, 6] dq.pop_right() # Returns 6, Deque: [1, 2, 3, 4, 5] # Advanced Operations dq.reverse() # Deque: [5, 4, 3, 2, 1] dq.rotate(2) # Deque: [2, 1, 5, 4, 3] dq.rotate(-3) # Deque: [5, 4, 3, 2, 1] sliced = dq.slice(1, 4, 1) # Returns [4, 3, 2] # Magic Methods length = len(dq) # Returns 5 print(dq) # Output: AdvancedDeque([5, 4, 3, 2, 1]) ``` **Implementation:** You need to write the `AdvancedDeque` class with all the required methods and functionality as described. ```python from collections import deque class AdvancedDeque: def __init__(self, iterable=None): self.deque = deque(iterable) if iterable else deque() def append_left(self, x): self.deque.appendleft(x) def append_right(self, x): self.deque.append(x) def pop_left(self): if not self.deque: raise IndexError(\\"pop from an empty deque\\") return self.deque.popleft() def pop_right(self): if not self.deque: raise IndexError(\\"pop from an empty deque\\") return self.deque.pop() def reverse(self): self.deque.reverse() def rotate(self, n): self.deque.rotate(n) def slice(self, start, end, step): return list(self.deque)[start:end:step] def __len__(self): return len(self.deque) def __str__(self): return f\\"AdvancedDeque({list(self.deque)})\\" ``` **Notes:** - Ensure your implementation handles edge cases, such as operations on an empty deque. - You can use the `collections.deque` from Python\'s standard library to assist with the implementation, focusing on adding the advanced functionalities.","solution":"from collections import deque class AdvancedDeque: def __init__(self, iterable=None): Initialize the AdvancedDeque with an optional iterable. self.deque = deque(iterable) if iterable else deque() def append_left(self, x): Add an element x to the left end of the deque. self.deque.appendleft(x) def append_right(self, x): Add an element x to the right end of the deque. self.deque.append(x) def pop_left(self): Remove and return an element from the left end of the deque. Raise an exception if the deque is empty. if not self.deque: raise IndexError(\\"pop from an empty deque\\") return self.deque.popleft() def pop_right(self): Remove and return an element from the right end of the deque. Raise an exception if the deque is empty. if not self.deque: raise IndexError(\\"pop from an empty deque\\") return self.deque.pop() def reverse(self): Reverse the elements in the deque. self.deque.reverse() def rotate(self, n): Rotate the deque n steps to the right. If n is negative, rotate to the left. self.deque.rotate(n) def slice(self, start, end, step): Return a list of elements from the deque following Python\'s list slicing rules with start, end, and step. return list(self.deque)[start:end:step] def __len__(self): Return the number of elements in the deque. return len(self.deque) def __str__(self): Return a string representation of the deque. return f\\"AdvancedDeque({list(self.deque)})\\""},{"question":"**Question: Implementing a Multi-Format Compression Utility** You are tasked with creating a compression utility that can handle multiple compression formats (`gzip`, `bz2`, and `lzma`). Your utility should provide the following functionalities: 1. **Compress**: Accept a file path and a compression format (either \\"gzip\\", \\"bz2\\", or \\"lzma\\"), compress the file, and save the compressed file with an appropriate extension (e.g., `.gz` for gzip, `.bz2` for bzip2, `.xz` for lzma). 2. **Decompress**: Accept a compressed file path (either `.gz`, `.bz2`, or `.xz`), decompress the file, and save the decompressed file with the same name, removing the compression extension. # Function 1: compress_file **Input:** - `file_path` (str): The path to the file to be compressed. - `compression_format` (str): The format to use for compression. Supported values are \\"gzip\\", \\"bz2\\", and \\"lzma\\". **Output:** - None. The compressed file should be saved in the same directory as the original file, with the appropriate extension added. # Function 2: decompress_file **Input:** - `compressed_file_path` (str): The path to the compressed file. **Output:** - None. The decompressed file should be saved in the same directory as the original file, with the compression extension removed from the file name. # Constraints: - You can assume that the input file paths are valid and the files are accessible. - Handle any potential errors that might arise from file operations or unsupported compression formats. # Example Usage: ```python compress_file(\'example.txt\', \'gzip\') # This will create a compressed file \'example.txt.gz\' in the same directory. decompress_file(\'example.txt.gz\') # This will decompress \'example.txt.gz\' to \'example.txt\' in the same directory. ``` # Implementation Notes: - Make use of the `gzip`, `bz2`, and `lzma` modules for compression and decompression. - Verify the compression format before proceeding with the operation. - Handle cases where the provided compression format or file extension is not supported by raising appropriate errors. Write your implementation of the two functions `compress_file` and `decompress_file`.","solution":"import gzip import bz2 import lzma import os def compress_file(file_path, compression_format): if compression_format not in [\'gzip\', \'bz2\', \'lzma\']: raise ValueError(\\"Unsupported compression format. Use \'gzip\', \'bz2\', or \'lzma\'.\\") if compression_format == \'gzip\': compressed_file_path = file_path + \'.gz\' with open(file_path, \'rb\') as f_in, gzip.open(compressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) elif compression_format == \'bz2\': compressed_file_path = file_path + \'.bz2\' with open(file_path, \'rb\') as f_in, bz2.open(compressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) elif compression_format == \'lzma\': compressed_file_path = file_path + \'.xz\' with open(file_path, \'rb\') as f_in, lzma.open(compressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) def decompress_file(compressed_file_path): extension = os.path.splitext(compressed_file_path)[1] if extension == \'.gz\': decompressed_file_path = os.path.splitext(compressed_file_path)[0] with gzip.open(compressed_file_path, \'rb\') as f_in, open(decompressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) elif extension == \'.bz2\': decompressed_file_path = os.path.splitext(compressed_file_path)[0] with bz2.open(compressed_file_path, \'rb\') as f_in, open(decompressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) elif extension == \'.xz\': decompressed_file_path = os.path.splitext(compressed_file_path)[0] with lzma.open(compressed_file_path, \'rb\') as f_in, open(decompressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) else: raise ValueError(\\"Unsupported file extension. Use \'.gz\', \'.bz2\', or \'.xz\'.\\")"},{"question":"**Objective:** Demonstrate the use of PyTorch\'s testing utilities to construct tensors and validate their properties. **Task:** You are given two functions from the `torch.testing` module: `make_tensor` for creating tensors and `assert_close` for asserting the similarity of two tensors. Your task is to implement a function that generates tensors following specific properties, performs operations on them, and uses `assert_close` to validate these operations. **Requirements:** 1. Implement a function `generate_and_test_tensors()` that performs the following: - Generates a tensor `A` of shape (3, 3) filled with random values. - Generates a tensor `B` which is the element-wise addition of `A` and a tensor of ones of shape (3, 3). - Validates that `B` is close to `C`, where `C` is manually constructed to be `A + 1` for each element, using `assert_close`. 2. The function signature should be: ```python def generate_and_test_tensors(): pass ``` **Input and Output:** - The function `generate_and_test_tensors` does not take any inputs. - The function should not return any values but should raise an appropriate error if the tensors `B` and `C` are not close enough. **Constraints:** - Use PyTorch functions only. - Ensure tensor operations leverage GPU acceleration if available. **Performance Requirements:** - The implementation should be efficient and take advantage of PyTorch’s optimization for tensor operations. **Example Implementation:** ```python import torch import torch.testing as testing def generate_and_test_tensors(): # Generate tensor A of shape (3, 3) with random values A = testing.make_tensor(size=(3, 3)) # Generate tensor B as the addition of A and a tensor of ones B = A + torch.ones((3, 3)) # Manually create tensor C to be A + 1 C = A + 1 # Element-wise addition of 1 to each element of A # Use assert_close to validate B and C are close enough testing.assert_close(B, C) ``` Ensure your implementation meets the requirements and that `assert_close` correctly validates the tensor operations.","solution":"import torch import torch.testing as testing def generate_and_test_tensors(): Generates tensors A and B, and validates if B is close to C. # Generate tensor A of shape (3, 3) with random values A = torch.randn((3, 3)) # Generate tensor B as the addition of A and a tensor of ones B = A + torch.ones((3, 3)) # Manually create tensor C to be A + 1 C = A + 1 # Element-wise addition of 1 to each element of A # Use assert_close to validate B and C are close enough testing.assert_close(B, C)"},{"question":"Objective Design a program to effectively analyze and optimize a given pickle file using the `pickletools` module. Problem Statement You are provided with multiple pickle files containing various Python objects. Your task is to: 1. Disassemble each pickle file and print the detailed opcode information to an output file. 2. Optimize each pickle file to remove unnecessary opcodes and save the optimized pickle to a new file. 3. Verify that the original and optimized pickles produce the same Python object upon unpickling. Requirements: 1. Write a function `analyze_and_optimize_pickle(input_files, output_dir)` where: - `input_files` is a list of paths to the input pickle files. - `output_dir` is the directory path where you should save the analysis files and optimized pickles. 2. For each pickle file in `input_files`, perform the following steps: - Disassemble and write the detailed opcode information to a file named `analysis_<filename>.txt` in the `output_dir`. Ensure annotations are included. - Optimize the pickle and save it to a file named `optimized_<filename>` in the `output_dir`. - Verify that unpickling the original and optimized pickles results in the same Python object. If they do not match, raise an exception. Constraints: - Assume all input filenames are unique and valid. - The output directory exists and is writable. - Performance should be considered if processing large pickles. Sample Usage ```python input_files = [\\"data1.pickle\\", \\"data2.pickle\\"] output_dir = \\"output\\" analyze_and_optimize_pickle(input_files, output_dir) ``` Expected Output For each input file `data1.pickle`, `data2.pickle`, etc.: - Files `analysis_data1.pickle.txt`, `analysis_data2.pickle.txt`, etc., containing detailed disassembly with annotations. - Optimized pickle files `optimized_data1.pickle`, `optimized_data2.pickle`, etc. - An exception is raised if verification fails. Notes - Utilize `pickletools.dis()` for disassembly. - Utilize `pickletools.optimize()` for optimization. - Use the `pickle` module for unpickling and verifying objects.","solution":"import pickle import pickletools import os def analyze_and_optimize_pickle(input_files, output_dir): Analyzes and optimizes pickle files. Args: input_files (list): List of input pickle file paths. output_dir (str): Directory path to save analysis and optimized pickles. for file in input_files: with open(file, \'rb\') as f: data = f.read() # Generate analysis file analysis_filename = os.path.join(output_dir, f\'analysis_{os.path.basename(file)}.txt\') with open(analysis_filename, \'w\') as analysis_file: pickletools.dis(data, out=analysis_file) # Optimize pickle file optimized_data = pickletools.optimize(data) # Save optimized pickle file optimized_filename = os.path.join(output_dir, f\'optimized_{os.path.basename(file)}\') with open(optimized_filename, \'wb\') as optimized_file: optimized_file.write(optimized_data) # Verify optimization original_object = pickle.loads(data) optimized_object = pickle.loads(optimized_data) if original_object != optimized_object: raise ValueError(f\\"Verification failed for file: {file}\\")"},{"question":"# **Custom Autograd Function and Gradient Computation with PyTorch** Objective: The objective of this exercise is to implement a custom autograd function in PyTorch and illustrate the computation of gradients using both the custom function and PyTorch\'s built-in utilities. Problem Statement: You are required to implement a custom autograd function in PyTorch for the following mathematical operation: [ y = sin(x) + x^2 ] Follow the steps below to complete this task: 1. **Custom Autograd Function**: - Define a custom autograd function `MySinSquare` for the mathematical operation ( y = sin(x) + x^2 ). - Implement the `forward` and `backward` methods of the custom function. 2. **Gradient Computation**: - Write a function `compute_gradient` that: - Takes a tensor input (x) with `requires_grad=True`. - Applies the custom autograd function on (x). - Computes the gradient of (y) with respect to (x) using `backward()`. - Returns the gradient. 3. **Forward-Mode AD**: - Implement another function `compute_jvp` that: - Takes the same tensor input (x) and a vector `v` for computing the Jacobian-vector product (JVP). - Uses `torch.autograd.functional.jvp` to compute the JVP. - Returns the result of the JVP. Input: - A tensor `x` with `requires_grad=True` and a random initial value. - A vector `v` for computing JVP, with the same shape as `x`. Output: - Gradient of (y) with respect to (x). - The result of the Jacobian-vector product (JVP). Constraints: - Do not use external libraries other than PyTorch. - Ensure the custom autograd function is correctly registered and used. Example Usage: ```python import torch # Sample input x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) v = torch.tensor([0.1, 0.2, 0.3]) # Compute gradient grad = compute_gradient(x) print(\\"Gradient:\\", grad) # Compute JVP jvp_result = compute_jvp(x, v) print(\\"JVP Result:\\", jvp_result) ``` # Solution Template: ```python import torch from torch.autograd import Function class MySinSquare(Function): @staticmethod def forward(ctx, input): # Save input for backward pass ctx.save_for_backward(input) return torch.sin(input) + input ** 2 @staticmethod def backward(ctx, grad_output): # Retrieve saved input input, = ctx.saved_tensors # Compute the gradient using chain rule grad_input = grad_output * (torch.cos(input) + 2 * input) return grad_input def compute_gradient(x): # Apply custom autograd function y = MySinSquare.apply(x) # Compute backward pass to get gradients y.backward(torch.ones_like(x)) return x.grad def compute_jvp(x, v): # Define the function for which we want to compute JVP def func(input): return torch.sin(input) + input ** 2 # Compute JVP _, jvp_result = torch.autograd.functional.jvp(func, x, v) return jvp_result ``` Complete the implementation of the above template to achieve the objective.","solution":"import torch from torch.autograd import Function class MySinSquare(Function): @staticmethod def forward(ctx, input): # Save input for backward pass ctx.save_for_backward(input) return torch.sin(input) + input ** 2 @staticmethod def backward(ctx, grad_output): # Retrieve saved input input, = ctx.saved_tensors # Compute the gradient using chain rule grad_input = grad_output * (torch.cos(input) + 2 * input) return grad_input def compute_gradient(x): # Apply custom autograd function y = MySinSquare.apply(x) # Compute backward pass to get gradients y.backward(torch.ones_like(x)) return x.grad def compute_jvp(x, v): # Define the function for which we want to compute JVP def func(input): return torch.sin(input) + input ** 2 # Compute JVP _, jvp_result = torch.autograd.functional.jvp(func, x, v) return jvp_result"},{"question":"Objective: To assess your understanding of the `urllib` package and its modules: `urllib.request`, `urllib.error`, and `urllib.parse`. Problem Statement: You are required to write a function `fetch_and_parse_url(url: str) -> dict`. This function should: 1. Fetch the content of the given URL using `urllib.request`. 2. Handle potential errors using `urllib.error`. 3. Parse and return specific components from the URL using `urllib.parse`. Expected Input and Output: - **Input:** A single string `url` representing the URL to be processed. - **Output:** A dictionary containing the following keys: - `\\"status_code\\"`: The HTTP status code of the fetched URL. - `\\"content\\"`: The first 200 characters of the webpage content as a string. - `\\"hostname\\"`: The hostname component of the URL. - `\\"scheme\\"`: The scheme component of the URL (e.g., http, https). - `\\"path\\"`: The path component of the URL. Function Signature: ```python def fetch_and_parse_url(url: str) -> dict: ``` Constraints: - You must use the `urllib` package for fetching, error handling, and parsing the URL. - You should handle the following exceptions: `URLError`, `HTTPError` from `urllib.error`. - The function should handle redirects and follow them if possible. - If an error occurs, return an empty dictionary `{}`. - Assume `url` is a valid URL string. Example: ```python url = \\"http://example.com\\" result = fetch_and_parse_url(url) # Example output: # { # \\"status_code\\": 200, # \\"content\\": \\"<!doctype html>n<html>n<head>n <title>Example Domain</title>n <meta charset=\\"utf-8\\" />n <meta http-equiv=\\"content-type\\" content=\\"text/html; charset=utf-8\\" />n <meta name=\\"viewport\\" content=\\"width=device-width, initial-scale=1\\" />n <style type=\\"text/css\\">\\", # \\"hostname\\": \\"example.com\\", # \\"scheme\\": \\"http\\", # \\"path\\": \\"\\" # } ``` *Hint:* You might find the following functions/classes useful from the `urllib` package: - `urllib.request.urlopen` - `urllib.error.HTTPError` - `urllib.error.URLError` - `urllib.parse.urlparse` Good luck!","solution":"import urllib.request import urllib.error import urllib.parse def fetch_and_parse_url(url: str) -> dict: try: response = urllib.request.urlopen(url) content = response.read().decode(\'utf-8\')[:200] parsed_url = urllib.parse.urlparse(url) result = { \\"status_code\\": response.getcode(), \\"content\\": content, \\"hostname\\": parsed_url.hostname, \\"scheme\\": parsed_url.scheme, \\"path\\": parsed_url.path } return result except (urllib.error.URLError, urllib.error.HTTPError): return {}"},{"question":"**Objective:** Demonstrate your understanding of Python\'s setup configuration file (`setup.cfg`) and Distutils by customizing a Python module\'s build and distribution process. **Problem Statement:** You are given a Python module named `sample_module` that contains the following structure: ``` sample_module/ setup.py sample/ __init__.py module.py doc/ README.txt CHANGES.txt USAGE.txt ``` 1. **Setup File**: The `setup.py` file is as follows: ```python from setuptools import setup, find_packages setup( name=\\"sample_module\\", version=\\"0.1\\", packages=find_packages(), ) ``` 2. **Task**: Create a `setup.cfg` file to achieve the following: - Ensure that any compiled extensions are built in-place within the `sample` directory. - Set up the `bdist_rpm` command with the following specifications: - `release` version is `2`. - `packager` is `John Doe <jdoe@example.com>`. - `doc_files` should include `CHANGES.txt`, `README.txt`, `USAGE.txt`, and the entire `doc` directory. **Instructions**: 1. Create a new file named `setup.cfg` in the `sample_module` directory. 2. Populate this file with the appropriate sections and options to meet the above requirements. 3. Ensure you adhere to the proper syntax and conventions for configuration files as described. **Constraints**: - The `setup.cfg` file must be placed at the root of the `sample_module` directory. - The `setup.py` file should not be modified. - Test your configuration by running the appropriate commands and ensuring the correct behavior, using the configuration file for in-place build of extensions and proper `bdist_rpm` setup. **Expected Output**: A properly formatted `setup.cfg` file with the specified configurations. **Performance Requirements**: - Ensure that the configuration processes correctly and overrides or sets defaults as specified. **Example**: A possible `setup.cfg` file might look like below (but remember to meet **all** the specified requirements in the task prompt): ```ini [build_ext] inplace=1 [bdist_rpm] release=2 packager=John Doe <jdoe@example.com> doc_files=CHANGES.txt README.txt USAGE.txt doc/ ``` Use this as a reference to incorporate all necessary options.","solution":"def create_setup_cfg(): contents = [build_ext] inplace=1 [bdist_rpm] release=2 packager=John Doe <jdoe@example.com> doc_files=README.txt CHANGES.txt USAGE.txt doc/ with open(\\"sample_module/setup.cfg\\", \\"w\\") as f: f.write(contents)"},{"question":"# Question: Advanced Iterator and Generator Usage in Python Implement a custom iterator and a generator function, and then use them in combination with functions from the `itertools` and `functools` modules to solve the following problem: **Problem Statement:** You are given a stream of numbers representing temperatures recorded every hour in a city. For this problem, implement a custom iterator called `TemperatureStream` that will take the input stream and allow iteration through the temperature values. Then implement a generator function called `temperature_variation` that generates the difference between each consecutive temperature reading. Finally, use this generator and the `TemperatureStream` iterator to compute: 1. The maximum temperature variation observed in any consecutive 24-hour period using `itertools` and `functools`. 2. The average temperature over the entire stream using the custom iterator. **Requirements:** 1. Implement `TemperatureStream` iterator. 2. Implement `temperature_variation` generator function. 3. Compute the maximum 24-hour temperature variation. 4. Compute the average temperature. # Input: - A list of integers/float representing temperatures. # Output: - Tuple containing: - Maximum 24-hour temperature variation. - Average temperature over the entire stream. # Example: ```python temps = [73.4, 75.0, 71.2, 68.9, 70.5, 72.0, 71.3, 74.1, 76.0, 75.5, 77.2, 79.0, 80.1, 78.4, 76.3, 74.0, 72.5, 71.0, 69.8, 68.9, 68.3, 70.0, 71.2, 72.6, ...] # assuming the list contains more temperatures ``` # Implementation Details: 1. **TemperatureStream (iterator)**: - Initialize with the temperature list. - Implement the `__iter__()` and `__next__()` methods to make it an iterator. 2. **temperature_variation (generator)**: - Accepts an iterator of temperatures. - Yields the difference between each consecutive temperature reading. 3. **Maximum 24-hour variation using itertools and functools**: - Use `itertools.islice`, `itertools.tee`, and `functools.reduce` where appropriate. 4. **Average temperature calculation**: - Sum all temperature readings and divide by the total number of readings. **Constraints:** - Assume the temperature list will always have at least 24 readings for the 24-hour period variation to make sense. - Implement clear and efficient solutions, as the number of temperature readings could be large. ```python class TemperatureStream: def __init__(self, temperatures): self.temperatures = temperatures self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.temperatures): raise StopIteration value = self.temperatures[self.index] self.index += 1 return value def temperature_variation(temps_iter): prev_temp = next(temps_iter) for temp in temps_iter: yield temp - prev_temp prev_temp = temp def max_24_hour_variation(temps): temp_stream = TemperatureStream(temps) var_generator = temperature_variation(iter(temp_stream)) sliding_24_hour_variations = (sum(var_generator) for _ in itertools.islice(temp_stream, 24)) max_variation = max(sliding_24_hour_variations) return max_variation def average_temperature(temps): temp_stream = TemperatureStream(temps) total_temp = sum(temp_stream) avg_temp = total_temp / len(temps) return avg_temp def analyze_temperature_readings(temps): max_variation = max_24_hour_variation(temps) avg_temp = average_temperature(temps) return max_variation, avg_temp # Sample Input temps = [73.4, 75.0, 71.2, 68.9, 70.5, 72.0, 71.3, 74.1, 76.0, 75.5, 77.2, 79.0, 80.1, 78.4, 76.3, 74.0, 72.5, 71.0, 69.8, 68.9, 68.3, 70.0, 71.2, 72.6] # Expected Output print(analyze_temperature_readings(temps)) # (some_max_variation, some_avg_temperature) ```","solution":"import itertools import functools class TemperatureStream: def __init__(self, temperatures): self.temperatures = temperatures self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.temperatures): raise StopIteration value = self.temperatures[self.index] self.index += 1 return value def temperature_variation(temps_iter): prev_temp = next(temps_iter) for temp in temps_iter: yield temp - prev_temp prev_temp = temp def max_24_hour_variation(temps): temp_stream = TemperatureStream(temps) var_generator = temperature_variation(iter(temp_stream)) var_list = list(itertools.islice(var_generator, len(temps) - 1)) sliding_variations = (functools.reduce(lambda x, y: x + y, var_list[i:i + 23]) for i in range(len(var_list) - 23)) max_variation = max(sliding_variations, default=0) return max_variation def average_temperature(temps): temp_stream = iter(TemperatureStream(temps)) total_temp = sum(temp_stream) avg_temp = total_temp / len(temps) return avg_temp def analyze_temperature_readings(temps): max_variation = max_24_hour_variation(temps) avg_temp = average_temperature(temps) return max_variation, avg_temp # Sample Input # temps = [73.4, 75.0, 71.2, 68.9, 70.5, 72.0, 71.3, 74.1, 76.0, 75.5, 77.2, # 79.0, 80.1, 78.4, 76.3, 74.0, 72.5, 71.0, 69.8, 68.9, 68.3, # 70.0, 71.2, 72.6, ...] # Expected Output # print(analyze_temperature_readings(temps)) # (some_max_variation, some_avg_temperature)"},{"question":"Coding Assessment Question # Introduction In this task, you will demonstrate your understanding of the buffer protocol by implementing a custom type in Python that exposes its underlying buffer. You will then implement a function to perform certain operations on this buffer. # Objectives 1. **Create a custom buffer type**: Implement a Python class that supports the buffer protocol. 2. **Manipulate the buffer**: Write a function that consumes the buffer, performs an operation, and returns the result. # Instructions 1. **CustomBuffer Class**: - Implement a class `CustomBuffer` that simulates a 2D array using a contiguous block of memory. - The class should initialize with the number of rows and columns, creating an internal buffer of zeros. - The class should support the `__buffer__` protocol to expose its internal memory buffer. 2. **Buffer Data Manipulation**: - Write a function `process_buffer(buffer_obj: CustomBuffer) -> CustomBuffer` that: - Obtains a writable view of the buffer. - Squares each number in the buffer. - Returns the modified buffer object. # Requirements 1. **CustomBuffer Initialization**: - `__init__(self, rows: int, cols: int)` - Initializes the buffer with given rows and columns, all elements set to zero. 2. **CustomBuffer Buffer Protocol**: - `__buffer__(self, flags: int)` - Exposes the internal buffer using the buffer protocol, supporting writable access. 3. **Buffer Operations**: - Define the function `process_buffer` to access the buffer data, perform element-wise squaring, and return the buffer. # Example Usage ```python # Example usage: buf = CustomBuffer(2, 2) # Initially, buf is a 2x2 array with elements [[0, 0], [0, 0]] # Let\'s create a buffer to initialize with some values buf.set_values([[1, 2], [3, 4]]) # This would be a helper function to set initial values # Now process the buffer modified_buf = process_buffer(buf) # After processing, the buffer should now contain squared values. # So, modified_buf should have its internal data as [[1, 4], [9, 16]] ``` # Constraints - Ensure memory safety and proper handling of reference counts. - Assume valid integer inputs for rows and columns. # Notes - This task will test your ability to work with low-level memory buffers and the buffer protocol in Python. - Ensure that your implementation correctly handles buffer requests and performs manipulations in-place.","solution":"import numpy as np class CustomBuffer: def __init__(self, rows: int, cols: int): self.rows = rows self.cols = cols self.buffer = np.zeros((rows, cols), dtype=int) def __buffer__(self, flags): return self.buffer def set_values(self, values): for i in range(self.rows): for j in range(self.cols): self.buffer[i][j] = values[i][j] def process_buffer(buffer_obj: CustomBuffer) -> CustomBuffer: buffer_view = memoryview(buffer_obj.__buffer__(0)) for i in range(buffer_view.shape[0]): for j in range(buffer_view.shape[1]): buffer_view[i, j] = buffer_view[i, j] ** 2 return buffer_obj"},{"question":"Objective: Create a function named `unicode_info` that takes a Unicode string as input and returns a dictionary with detailed information about each character in the string. Function Signature: ```python def unicode_info(unistr: str) -> dict: ``` Input: - `unistr` (str): A string containing one or more Unicode characters. Output: - A dictionary where each key is a character from the input string, and the value is another dictionary containing detailed information about that character. The inner dictionary should have the following structure: ```python { \'name\': str or None, \'decimal\': int or None, \'digit\': int or None, \'numeric\': float or None, \'category\': str, \'bidirectional\': str, \'combining\': int, \'east_asian_width\': str, \'mirrored\': int, \'decomposition\': str } ``` Constraints: - If a character does not have a specific property, use `None` for `name`, `decimal`, `digit`, and `numeric`, and use an empty string `\\"\\"` for `bidirectional` and `decomposition`. Example: ```python unistr = \'A9\' result = unicode_info(unistr) # Expected output: # { # \'A\': { # \'name\': \'LATIN CAPITAL LETTER A\', # \'decimal\': None, # \'digit\': None, # \'numeric\': None, # \'category\': \'Lu\', # \'bidirectional\': \'L\', # \'combining\': 0, # \'east_asian_width\': \'Na\', # \'mirrored\': 0, # \'decomposition\': \'\' # }, # \'9\': { # \'name\': \'DIGIT NINE\', # \'decimal\': 9, # \'digit\': 9, # \'numeric\': 9.0, # \'category\': \'Nd\', # \'bidirectional\': \'EN\', # \'combining\': 0, # \'east_asian_width\': \'Na\', # \'mirrored\': 0, # \'decomposition\': \'\' # } # } ``` Requirements: - The function must use the `unicodedata` module to retrieve the properties of each character. - The function must handle exceptions raised by `unicodedata` methods appropriately, returning `None` or empty strings as specified above. Test your function on various Unicode strings to ensure it correctly handles a variety of characters and their properties.","solution":"import unicodedata def unicode_info(unistr: str) -> dict: result = {} for char in unistr: try: name = unicodedata.name(char) except ValueError: name = None try: decimal = unicodedata.decimal(char) except (TypeError, ValueError): decimal = None try: digit = unicodedata.digit(char) except (TypeError, ValueError): digit = None try: numeric = unicodedata.numeric(char) except (TypeError, ValueError): numeric = None category = unicodedata.category(char) bidirectional = unicodedata.bidirectional(char) or \\"\\" combining = unicodedata.combining(char) east_asian_width = unicodedata.east_asian_width(char) mirrored = unicodedata.mirrored(char) decomposition = unicodedata.decomposition(char) or \\"\\" result[char] = { \'name\': name, \'decimal\': decimal, \'digit\': digit, \'numeric\': numeric, \'category\': category, \'bidirectional\': bidirectional, \'combining\': combining, \'east_asian_width\': east_asian_width, \'mirrored\': mirrored, \'decomposition\': decomposition } return result"},{"question":"# Python Version Parsing and Assembly You are tasked with implementing a function that parses a given Python version string and converts it to its corresponding hexadecimal representation (`PY_VERSION_HEX`), as described by the CPython macros. Additionally, implement the reverse function that takes the hexadecimal representation and converts it back to the version string. Part 1: Version to Hex Write a function `version_to_hex(version_str: str) -> int` that takes a version string (e.g., \\"3.4.1a2\\") and returns its hexadecimal integer representation as described in the documentation. - **Input**: A string `version_str` in the format \\"MAJOR.MINOR.MICROLEVELSERIAL\\". - `MAJOR`, `MINOR`, and `MICRO` are integers. - `LEVEL` is a single character: `\'a\'` for alpha, `\'b\'` for beta, `\'c\'` for release candidate, and `\'f\'` for final. - `SERIAL` is an integer. - **Output**: An integer representing the version in hexadecimal format (`PY_VERSION_HEX`). Example: ```python version_to_hex(\\"3.4.1a2\\") -> 0x030401a2 version_to_hex(\\"3.10.0f0\\") -> 0x030a00f0 ``` Part 2: Hex to Version Write a function `hex_to_version(hex_version: int) -> str` that takes a hexadecimal integer representation of a version and returns the corresponding version string. - **Input**: An integer `hex_version` representing the version in hexadecimal format. - **Output**: A string in the format \\"MAJOR.MINOR.MICROLEVELSERIAL\\". Example: ```python hex_to_version(0x030401a2) -> \\"3.4.1a2\\" hex_to_version(0x030a00f0) -> \\"3.10.0f0\\" ``` Constraints 1. You can assume that the input strings and integers are properly formatted and within valid ranges according to Python versioning conventions. 2. The functions should handle all valid release levels (a, b, c, f). Performance Requirements The functions should perform the conversion in constant time, O(1), as the operations mainly involve bitwise manipulations and string formatting.","solution":"def version_to_hex(version_str: str) -> int: level_map = {\'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF} major, minor, micro_level_serial = version_str.split(\'.\') micro = int(micro_level_serial[:-2]) level = micro_level_serial[-2] serial = int(micro_level_serial[-1]) major = int(major) minor = int(minor) level = level_map[level] hex_version = (major << 24) | (minor << 16) | (micro << 8) | (level << 4) | serial return hex_version def hex_to_version(hex_version: int) -> str: level_map = {0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\'} major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF level = (hex_version >> 4) & 0xF serial = hex_version & 0xF level = level_map[level] version_str = f\\"{major}.{minor}.{micro}{level}{serial}\\" return version_str"},{"question":"Task Implement an asyncio-based task manager that handles tasks with different priorities using an `asyncio.PriorityQueue`. Your task manager should be capable of adding tasks with specified priorities, processing these tasks concurrently, and providing status updates. Requirements 1. **Function Signature:** ```python async def manage_tasks(task_list: List[Tuple[int, str]], num_workers: int) -> None: ``` 2. **Input:** - `task_list`: A list of tuples where each tuple consists of an integer (priority) and a string (task description). Tasks with lower priority numbers should be processed first. - `num_workers`: An integer representing the number of concurrent worker tasks. 3. **Output:** - Your function does not need to return anything. It should print the description of each task as it\'s being processed, along with the worker\'s name and the priority of the task. 4. **Processing Requirements:** - Use an `asyncio.PriorityQueue` to manage and process tasks based on their priority. - Create the specified number of worker tasks to process the queue concurrently. - Print statements should follow the format: `\\"Worker worker_name is processing task: task_description with priority: task_priority\\"`. - The function should ensure that all tasks are processed and print a message `\\"All tasks have been processed\\"` when done. Constraints: - Task priorities are integers where 0 is the highest priority. - The number of workers will be a positive integer. Example: ```python import asyncio async def main(): tasks = [(3, \\"Low priority task\\"), (1, \\"High priority task\\"), (2, \\"Medium priority task\\")] await manage_tasks(tasks, 2) asyncio.run(main()) ``` Expected output: ``` Worker worker-0 is processing task: High priority task with priority: 1 Worker worker-1 is processing task: Medium priority task with priority: 2 Worker worker-0 is processing task: Low priority task with priority: 3 All tasks have been processed ```","solution":"import asyncio from typing import List, Tuple async def worker(name: str, queue: asyncio.PriorityQueue): while True: priority, task = await queue.get() print(f\\"Worker {name} is processing task: {task} with priority: {priority}\\") queue.task_done() await asyncio.sleep(0) # Simulating an async operation async def manage_tasks(task_list: List[Tuple[int, str]], num_workers: int) -> None: queue = asyncio.PriorityQueue() # Start worker processes workers = [] for i in range(num_workers): worker_name = f\\"worker-{i}\\" workers.append(asyncio.create_task(worker(worker_name, queue))) # Put tasks into the queue for priority, task in task_list: await queue.put((priority, task)) # Wait until all tasks are processed await queue.join() # Cancel the worker tasks for worker_task in workers: worker_task.cancel() print(\\"All tasks have been processed\\")"},{"question":"You are provided with a dataset containing information about houses including numerical features and a categorical feature. Your task is to build a machine learning pipeline to predict the house prices. The pipeline should include data preprocessing steps for both numerical and categorical features, followed by a regression model for prediction. Dataset The provided dataset contains the following columns: - `city`: the city where the house is located (categorical). - `size`: the size of the house in square feet (numerical). - `bedrooms`: the number of bedrooms in the house (numerical). - `bathrooms`: the number of bathrooms in the house (numerical). - `price`: the price of the house in dollars (numerical target variable). Objective Implement a function `build_and_train_pipeline` that: 1. Preprocesses the categorical feature (`city`) using one-hot encoding. 2. Preprocesses the numerical features (`size`, `bedrooms`, `bathrooms`) using standard scaling. 3. Combines the preprocessing steps into a single pipeline using `ColumnTransformer`. 4. Fits a `LinearRegression` model to the provided dataset within this pipeline. Expected Function Signature ```python def build_and_train_pipeline(X: pd.DataFrame, y: pd.Series) -> Pipeline: pass ``` Input - `X`: a pandas DataFrame containing the features (`city`, `size`, `bedrooms`, `bathrooms`). - `y`: a pandas Series containing the target variable (`price`). Output - Returns the trained `Pipeline` object. Constraints - Use `OneHotEncoder` from `sklearn.preprocessing` for encoding the categorical feature. - Use `StandardScaler` from `sklearn.preprocessing` for scaling the numerical features. - Use `ColumnTransformer` from `sklearn.compose` to apply the preprocessing steps. - Use `LinearRegression` from `sklearn.linear_model` for the regression model. Example ```python # Example usage import pandas as pd from sklearn.pipeline import Pipeline data = { \'city\': [\'New York\', \'San Francisco\', \'Boston\', \'New York\', \'Los Angeles\'], \'size\': [1200, 800, 900, 1500, 2000], \'bedrooms\': [3, 2, 2, 4, 4], \'bathrooms\': [2, 1, 1.5, 3, 3], \'price\': [400000, 800000, 600000, 1000000, 1200000], } X = pd.DataFrame(data).drop(columns=\'price\') y = pd.Series(data[\'price\']) pipeline = build_and_train_pipeline(X, y) print(pipeline) ``` Expected output is the fitted pipeline object that you can use to make predictions on new data.","solution":"import pandas as pd from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.linear_model import LinearRegression def build_and_train_pipeline(X: pd.DataFrame, y: pd.Series) -> Pipeline: Build and train a pipeline that preprocesses the data and fits a Linear Regression model. Parameters: X (pd.DataFrame): The input features. y (pd.Series): The target variable (house prices). Returns: Pipeline: The trained sklearn Pipeline object. # Define the column indices categorical_features = [\'city\'] numerical_features = [\'size\', \'bedrooms\', \'bathrooms\'] # Preprocessing for categorical data categorical_transformer = OneHotEncoder() # Preprocessing for numerical data numerical_transformer = StandardScaler() # Combine the data preprocessing steps preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create a pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'regressor\', LinearRegression()) ]) # Fit the pipeline to the data pipeline.fit(X, y) return pipeline"},{"question":"You are given a dataset of penguins with different attributes. Your task is to use the seaborn `objects` module to plot and analyze the KDE of penguin flipper lengths based on several configurations. Dataset The dataset `penguins` can be loaded using: ```python from seaborn import load_dataset penguins = load_dataset(\\"penguins\\") ``` The dataset includes the following columns: - `species`: Penguin species (Adelie, Chinstrap, Gentoo). - `island`: Island in Palmer Archipelago, Antarctica (Dream, Torgersen, Biscoe). - `bill_length_mm`: Bill length in millimeters. - `bill_depth_mm`: Bill depth in millimeters. - `flipper_length_mm`: Flipper length in millimeters. - `body_mass_g`: Body mass in grams. - `sex`: Penguin sex (Male, Female). Task 1. **Basic KDE with Flipper Length**: Plot a KDE for the flipper lengths of the penguins. Use the `Area` object for the KDE plot. 2. **Adjusted Smoothing Bandwidth**: Create another KDE plot with a bandwidth adjustment of 0.25 to visualize more details, and compare it with the original KDE plot. Overlay this adjusted KDE on the previous plot using the `Area` object. 3. **Overlaying Histogram and KDE**: On a new plot, show both the histogram (with density values) and the KDE for the flipper lengths to provide a comprehensive visual. 4. **Group Densities by Species**: Plot KDEs for flipper lengths grouped by `species`. Use different colors for each species and ensure that the densities are normalized within each group. 5. **Conditional Densities by Species and Sex**: Create facet plots with KDEs for flipper lengths by conditioning on `sex` as columns. Normalize the densities within each column, and color by `species`. 6. **Cumulative KDE Plot**: Finally, create a cumulative KDE plot for flipper lengths. Implementation Details - Use `seaborn.objects` module. - Implement each of the above tasks using seaborn functions and make sure to follow the sequence. - For each plot, ensure that the axes are labeled appropriately. Example Here is pseudocode to get you started: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Basic KDE p = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE()) p.show() # KDE with adjusted bandwidth p.add(so.Area(), so.KDE(bw_adjust=0.25)) p.show() # Histogram and KDE p2 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(alpha=.3), so.Hist(\\"density\\")) p2.add(so.Line(), so.KDE()) p2.show() # Grouped KDE by species p.add(so.Area(), so.KDE(), color=\\"species\\") p.show() # Conditional KDEs by sex, colored by species p3 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\").add(so.Area(), so.KDE(common_norm=[\\"col\\"]), color=\\"species\\") p3.show() # Cumulative KDE p.add(so.Line(), so.KDE(cumulative=True)) p.show() ``` Ensure your solution is efficient and adheres to best practices in coding and data visualization.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Define a function for each required plot def plot_basic_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE()) p.show() def plot_adjusted_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.KDE()) p.add(so.Area(), so.KDE(bw_adjust=0.25)) p.show() def plot_histogram_and_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(alpha=.3), so.Hist(\\"density\\")) p.add(so.Line(), so.KDE()) p.show() def plot_grouped_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\", color=\\"species\\").add(so.Area(), so.KDE()) p.show() def plot_conditional_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"sex\\").add(so.Area(), so.KDE(), color=\\"species\\") p.show() def plot_cumulative_kde(): p = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Line(), so.KDE(cumulative=True)) p.show()"},{"question":"Introduction You are tasked with implementing a Python program that processes a list of strings to determine if they follow specific patterns, with output corresponding to the patterns found. This will test your understanding of control flow, functions, and pattern matching in Python. Problem Statement You need to write a function called `process_strings` that takes a list of strings as its input and returns a list of results based on the following criteria: 1. **Pattern 1**: If a string is exactly \\"hello\\", return \\"greeting\\". 2. **Pattern 2**: If a string contains only digits, return \\"numeric\\" followed by the string converted to an integer. 3. **Pattern 3**: If a string contains an equal number of vowels and consonants, return \\"balanced\\". 4. **Pattern 4**: Default case for any other string, just return the string itself prefixed with \\"other: \\". Use the `match` statement introduced in Python 3.10 for pattern matching. Constraints - Each string in the input list is non-empty. - Consider vowels to be \'a\', \'e\', \'i\', \'o\', \'u\' (case insensitive). - Consonants are any alphabetic characters not in the set of vowels. Input - A list of strings, e.g., `[\\"hello\\", \\"1234\\", \\"equal121\\", \\"Python\\"]` Output - A list of processed results, e.g., `[\\"greeting\\", \\"numeric: 1234\\", \\"balanced\\", \\"other: Python\\"]` Examples ```python process_strings([\\"hello\\", \\"1234\\", \\"equal121\\", \\"Python\\"]) # Output: [\\"greeting\\", \\"numeric: 1234\\", \\"balanced\\", \\"other: Python\\"] process_strings([\\"test\\", \\"aeiou\\", \\"xyz\\"]) # Output: [\\"other: test\\", \\"other: aeiou\\", \\"other: xyz\\"] ``` Function Signature ```python def process_strings(strings: list[str]) -> list[str]: pass ``` **Hints:** - Use the `match` statement to handle the patterns. - Use helper functions if necessary to check for the balanced pattern. - Follow good coding style practices as per PEP 8. **Notes:** - Write a comprehensive docstring for your function. - Ensure the function is well-documented and follows PEP 8 style guidelines. Good Luck!","solution":"def process_strings(strings): Processes a list of strings based on specific patterns. Parameters: strings (list of str): The input list of strings to process. Returns: list of str: The list of processed results. def is_balanced(s): vowels = set(\'aeiouAEIOU\') vowel_count = sum(1 for char in s if char in vowels) consonant_count = sum(1 for char in s if char.isalpha() and char not in vowels) return vowel_count == consonant_count results = [] for s in strings: match s: case \\"hello\\": results.append(\\"greeting\\") case _ if s.isdigit(): results.append(f\\"numeric: {int(s)}\\") case _ if is_balanced(s): results.append(\\"balanced\\") case _: results.append(f\\"other: {s}\\") return results"},{"question":"You are required to create an XML parser using the `xml.parsers.expat` module in Python. The parser should be able to process a given XML string, extract information, and handle errors appropriately. Specifically, the parser will extract titles and authors from a given XML structure representing a collection of books. Additionally, the parser should manage any errors that may occur during the parsing process. # Requirements: 1. Implement a function `parse_books_xml(xml_string)` that takes an XML string as input. 2. The function should utilize the `xml.parsers.expat` module to parse the XML and extract the titles and authors of the books. 3. The function should return a list of dictionaries, each containing the `title` and `author` of a book. 4. Handle `ExpatError` to manage any parsing errors and return an empty list if an error occurs. 5. Set appropriate handlers to manage the start and end of elements, and to handle character data. 6. Demonstrate the use of attributes like `buffer_size` and `buffer_text`. # XML Input Format: The input XML string will have the following structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> </book> <book> <title>Book Title 2</title> <author>Author 2</author> </book> <!-- More book elements --> </library> ``` # Function Signature: ```python def parse_books_xml(xml_string: str) -> list: pass ``` # Example: ```python xml_data = <library> <book> <title>The Alchemist</title> <author>Paulo Coelho</author> </book> <book> <title>To Kill a Mockingbird</title> <author>Harper Lee</author> </book> </library> result = parse_books_xml(xml_data) ``` Expected `result`: ```python [ {\\"title\\": \\"The Alchemist\\", \\"author\\": \\"Paulo Coelho\\"}, {\\"title\\": \\"To Kill a Mockingbird\\", \\"author\\": \\"Harper Lee\\"} ] ``` # Constraints: - The XML string will be well-formed, but it may contain errors that your function should handle. - The XML string may contain any number of `<book>` elements within the `<library>` element. # Implementation Guidelines: 1. Use `ParserCreate` to create an XML parser object. 2. Set handlers for `StartElementHandler`, `EndElementHandler`, and `CharacterDataHandler`. 3. Utilize a stack or temporary variables to manage the current state of parsing (e.g., whether you are currently inside a `<title>` or `<author>` tag). 4. Use a try-except block to handle `ExpatError`. 5. Configure parser attributes like `buffer_size` and `buffer_text` as demonstrated in the documentation.","solution":"import xml.parsers.expat def parse_books_xml(xml_string): def start_element(name, attrs): nonlocal current_element if name == \'book\': current_book[\'title\'] = \'\' current_book[\'author\'] = \'\' current_element = name def end_element(name): nonlocal current_element if name == \'book\': books.append(current_book.copy()) current_element = None def char_data(data): if current_element == \'title\': current_book[\'title\'] += data elif current_element == \'author\': current_book[\'author\'] += data books = [] current_book = {\'title\': \'\', \'author\': \'\'} current_element = None try: parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.buffer_size = 4096 parser.buffer_text = True parser.Parse(xml_string, True) except xml.parsers.expat.ExpatError: # If there is a parsing error, return an empty list return [] return books"},{"question":"# Custom Class Creation and Method Overriding **Objective**: Implement a custom Python class that models an advanced mathematical vector. This exercise will test your understanding of OOP principles, method overriding, and type safety in Python. # Problem Statement Implement a class `AdvancedVector` with the following functionalities: 1. **Initialization**: The class should be initialized with a list of numeric values. 2. **Length Calculation**: Implement a method to calculate the length (magnitude) of the vector. 3. **Dot Product**: Implement a method to calculate the dot product with another vector of the same length. 4. **Addition and Subtraction**: Overload the `+` and `-` operators to support vector addition and subtraction. 5. **String Representation**: Override the `__str__` and `__repr__` methods to provide a meaningful string representation of the vector. # Detailed Instructions 1. **Class Definition**: - Define a class named `AdvancedVector`. - The class should be initialized with a list of numeric values representing the components of the vector. 2. **Methods**: - `length(self) -> float`: Calculate and return the Euclidean length of the vector. - `dot(self, other: \'AdvancedVector\') -> float`: Calculate and return the dot product of the current vector with another vector. Ensure that the other vector has the same length. - `__add__(self, other: \'AdvancedVector\') -> \'AdvancedVector\'`: Overload the `+` operator to add two vectors component-wise. Ensure that the vectors have the same length. - `__sub__(self, other: \'AdvancedVector\') -> \'AdvancedVector\'`: Overload the `-` operator to subtract two vectors component-wise. Ensure that the vectors have the same length. - `__str__(self) -> str`: Return a string representation of the vector. - `__repr__(self) -> str`: Return a detailed string representation of the vector, useful for debugging. # Constraints - All vectors must have the same length for operations like dot product, addition, and subtraction. - The input list for initializing the vector should not be empty. - Ensure appropriate error handling and type checking where necessary. # Example Usage ```python v1 = AdvancedVector([1, 2, 3]) v2 = AdvancedVector([4, 5, 6]) print(v1.length()) # Output: 3.7416573867739413 (approximately) print(v1.dot(v2)) # Output: 32 v3 = v1 + v2 print(v3) # Output: AdvancedVector([5, 7, 9]) v4 = v1 - v2 print(v4) # Output: AdvancedVector([-3, -3, -3]) ``` # Implementation ```python import math from typing import List, Union class AdvancedVector: def __init__(self, components: List[Union[int, float]]): if not components: raise ValueError(\\"Vector components cannot be empty\\") self.components = components def length(self) -> float: return math.sqrt(sum(x ** 2 for x in self.components)) def dot(self, other: \'AdvancedVector\') -> float: if len(self.components) != len(other.components): raise ValueError(\\"Vectors must have the same length\\") return sum(x * y for x, y in zip(self.components, other.components)) def __add__(self, other: \'AdvancedVector\') -> \'AdvancedVector\': if len(self.components) != len(other.components): raise ValueError(\\"Vectors must have the same length\\") return AdvancedVector([x + y for x, y in zip(self.components, other.components)]) def __sub__(self, other: \'AdvancedVector\') -> \'AdvancedVector\': if len(self.components) != len(other.components): raise ValueError(\\"Vectors must have the same length\\") return AdvancedVector([x - y for x, y in zip(self.components, other.components)]) def __str__(self) -> str: return f\\"AdvancedVector({self.components})\\" def __repr__(self) -> str: return f\\"AdvancedVector({self.components})\\" ``` # Notes - Ensure your code handles edge cases such as initializing the vector with an empty list and performing operations on vectors of different lengths. - Your implementation should pass the provided example usage scenarios.","solution":"import math from typing import List, Union class AdvancedVector: def __init__(self, components: List[Union[int, float]]): if not components: raise ValueError(\\"Vector components cannot be empty\\") self.components = components def length(self) -> float: return math.sqrt(sum(x ** 2 for x in self.components)) def dot(self, other: \'AdvancedVector\') -> float: if len(self.components) != len(other.components): raise ValueError(\\"Vectors must have the same length\\") return sum(x * y for x, y in zip(self.components, other.components)) def __add__(self, other: \'AdvancedVector\') -> \'AdvancedVector\': if len(self.components) != len(other.components): raise ValueError(\\"Vectors must have the same length\\") return AdvancedVector([x + y for x, y in zip(self.components, other.components)]) def __sub__(self, other: \'AdvancedVector\') -> \'AdvancedVector\': if len(self.components) != len(other.components): raise ValueError(\\"Vectors must have the same length\\") return AdvancedVector([x - y for x, y in zip(self.components, other.components)]) def __str__(self) -> str: return f\\"AdvancedVector({self.components})\\" def __repr__(self) -> str: return f\\"AdvancedVector({self.components})\\""},{"question":"# CSV File Manipulation and Analysis Problem Statement: You have been provided a CSV file containing information about employees in a company. Each row in the CSV file includes the following fields: - `id`: Employee ID - `name`: Employee name - `department`: Department of the employee - `salary`: Salary of the employee You are required to perform the following tasks using Python\'s `csv` module: 1. **Read the Data**: Write a function `read_employee_data(filename)` that reads the CSV file and returns a list of dictionaries, each representing an employee. 2. **Write Data with Increment**: Write a function `increment_salary(filename, out_filename, increment)` that reads the employee data from the input `filename`, increases the salary of each employee by a specified `increment` percentage, and writes the updated data to a new CSV file `out_filename`. 3. **Calculate Average Salary**: Write a function `average_salary_by_department(filename)` that reads the employee data from the CSV file, calculates, and returns a dictionary with department names as keys and average salaries as values. Input: - A CSV file with employee data. Constraints: - The CSV file is guaranteed to be well-formed. - `salary` field is guaranteed to be a valid number. Function Definitions: ```python import csv def read_employee_data(filename: str) -> list: Reads employee data from a CSV file and returns a list of dictionaries. Parameters: - filename: str: The name of the CSV file to read from. Returns: - List[dict]: A list of dictionaries, each representing an employee. # Implementation goes here def increment_salary(filename: str, out_filename: str, increment: float) -> None: Reads employee data, increments their salary by a specified percentage, and writes the updated data to a new CSV file. Parameters: - filename: str: The name of the CSV file to read from. - out_filename: str: The name of the CSV file to write the updated data to. - increment: float: The percentage increment to be applied on the salary. Returns: - None # Implementation goes here def average_salary_by_department(filename: str) -> dict: Reads the employee data and calculates average salary by department. Parameters: - filename: str: The name of the CSV file to read from. Returns: - dict: A dictionary with department names as keys and average salaries as values. # Implementation goes here ``` Example Usage: Suppose the content of `employees.csv` is: ``` id,name,department,salary 1,John Doe,Finance,50000 2,Jane Smith,Engineering,60000 3,Bob Johnson,Finance,55000 4,Alice Brown,Engineering,70000 5,Eve Davis,HR,45000 ``` - Calling `read_employee_data(\'employees.csv\')` should return: ```python [ {\'id\': \'1\', \'name\': \'John Doe\', \'department\': \'Finance\', \'salary\': \'50000\'}, {\'id\': \'2\', \'name\': \'Jane Smith\', \'department\': \'Engineering\', \'salary\': \'60000\'}, {\'id\': \'3\', \'name\': \'Bob Johnson\', \'department\': \'Finance\', \'salary\': \'55000\'}, {\'id\': \'4\', \'name\': \'Alice Brown\', \'department\': \'Engineering\', \'salary\': \'70000\'}, {\'id\': \'5\', \'name\': \'Eve Davis\', \'department\': \'HR\', \'salary\': \'45000\'} ] ``` - Calling `increment_salary(\'employees.csv\', \'updated_employees.csv\', 10)` should create `updated_employees.csv` with content: ``` id,name,department,salary 1,John Doe,Finance,55000.0 2,Jane Smith,Engineering,66000.0 3,Bob Johnson,Finance,60500.0 4,Alice Brown,Engineering,77000.0 5,Eve Davis,HR,49500.0 ``` - Calling `average_salary_by_department(\'employees.csv\')` should return: ```python { \'Finance\': 52500.0, \'Engineering\': 65000.0, \'HR\': 45000.0 } ``` Make sure your code handles the reading and writing of CSV files as specified, and correctly performs the salary increment and average salary calculations.","solution":"import csv def read_employee_data(filename: str) -> list: Reads employee data from a CSV file and returns a list of dictionaries. Parameters: - filename: str: The name of the CSV file to read from. Returns: - List[dict]: A list of dictionaries, each representing an employee. with open(filename, mode=\'r\', newline=\'\') as file: reader = csv.DictReader(file) return [row for row in reader] def increment_salary(filename: str, out_filename: str, increment: float) -> None: Reads employee data, increments their salary by a specified percentage, and writes the updated data to a new CSV file. Parameters: - filename: str: The name of the CSV file to read from. - out_filename: str: The name of the CSV file to write the updated data to. - increment: float: The percentage increment to be applied on the salary. Returns: - None employees = read_employee_data(filename) for employee in employees: current_salary = float(employee[\'salary\']) new_salary = current_salary * (1 + increment / 100) employee[\'salary\'] = f\\"{new_salary:.1f}\\" with open(out_filename, mode=\'w\', newline=\'\') as file: fieldnames = [\'id\', \'name\', \'department\', \'salary\'] writer = csv.DictWriter(file, fieldnames=fieldnames) writer.writeheader() writer.writerows(employees) def average_salary_by_department(filename: str) -> dict: Reads the employee data and calculates the average salary by department. Parameters: - filename: str: The name of the CSV file to read from. Returns: - dict: A dictionary with department names as keys and average salaries as values. employees = read_employee_data(filename) department_salaries = {} department_counts = {} for employee in employees: department = employee[\'department\'] salary = float(employee[\'salary\']) if department not in department_salaries: department_salaries[department] = 0.0 department_counts[department] = 0 department_salaries[department] += salary department_counts[department] += 1 average_salaries = {department: department_salaries[department] / department_counts[department] for department in department_salaries} return average_salaries"},{"question":"Coding Assessment Question # Objective Write a Python program that utilizes the `ossaudiodev` module to perform a series of operations on an audio device, demonstrating your understanding of opening devices, reading/writing audio data, and manipulating audio parameters. # Problem Statement You are required to create a function `configure_and_play_audio(device_path: str, audio_format: str, nchannels: int, samplerate: int, audio_data: bytes) -> None` that configures an audio device with specified parameters and plays the provided audio data. Additionally, control the master volume of the mixer device. # Function Signature ```python def configure_and_play_audio(device_path: str, audio_format: str, nchannels: int, samplerate: int, audio_data: bytes) -> None: pass ``` # Input - `device_path` (str): The file path to the audio device (e.g., \\"/dev/dsp\\"). - `audio_format` (str): One of the supported audio formats (e.g., \\"AFMT_S16_LE\\"). - `nchannels` (int): Number of audio channels (e.g., 1 for mono, 2 for stereo). - `samplerate` (int): Desired sampling rate in Hz (e.g., 44100 for CD quality audio). - `audio_data` (bytes): The audio data to be played in a bytes-like object. # Output - The function should not return any value. It plays the audio on the specified device and sets the master volume using the mixer device. # Constraints - Assume the audio device supports the requested parameters. - The volume should be set to 70 for both left and right channels. - Handle any exceptions that may be raised and print appropriate error messages. - Make sure to close the audio and mixer devices properly even in case of errors. # Example ```python audio_data = b\'x00x01x02...\' # Example bytes data representing audio configure_and_play_audio(\'/dev/dsp\', \'AFMT_S16_LE\', 2, 44100, audio_data) ``` # Hints - Use `ossaudiodev.open` to open the audio device. - Use `ossaudiodev.openmixer` to open the mixer device. - Use necessary methods to set the audio parameters before writing data. - Ensure the master volume is set using the appropriate mixer methods. - Utilize context management or proper closing methods to handle device clean-up.","solution":"import ossaudiodev def configure_and_play_audio(device_path: str, audio_format: str, nchannels: int, samplerate: int, audio_data: bytes) -> None: try: # Open the audio device for playback dsp = ossaudiodev.open(device_path, \'w\') # Set audio format, number of channels, and sample rate audio_format_constant = getattr(ossaudiodev, audio_format) dsp.setfmt(audio_format_constant) dsp.channels(nchannels) dsp.speed(samplerate) print(f\\"Playing audio on {device_path} with format {audio_format}, channels {nchannels}, and sample rate {samplerate}\\") # Play the audio data dsp.write(audio_data) except Exception as e: print(f\\"Error during audio playback: {e}\\") finally: # Ensure the device is closed properly if \'dsp\' in locals(): dsp.close() try: # Open the mixer device to control volume mixer = ossaudiodev.openmixer() # Set the master volume to 70 for both left and right channels mixer.set(ossaudiodev.SOUND_MIXER_VOLUME, 70, 70) except Exception as e: print(f\\"Error setting volume: {e}\\") finally: # Ensure the mixer is closed properly if \'mixer\' in locals(): mixer.close()"},{"question":"<|Analysis Begin|> The provided documentation describes the \\"hmac\\" module in Python, which implements the HMAC (Keyed-Hashing for Message Authentication) algorithm. This documentation outlines the primary functions and methods in the module, explaining their purposes and detailing any parameters they accept. The \\"hmac.new()\\" function is notable for creating a new HMAC object, and it requires a secret key and a digestmod (digest algorithm). Optional msg data can also be provided. The \\"hmac.digest()\\" function conveniently returns the digest of msg using a given secret key and digest algorithm in a single call. An HMAC object can be updated with additional data using the \\"update()\\" method. To get the digest of the accumulated data, the \\"digest()\\" and \\"hexdigest()\\" methods can be used (returning the result in binary and hexadecimal formats, respectively). The \\"compare_digest()\\" helper function deserves a special mention as it provides a secure way to compare HMAC digests by mitigating timing attacks. Attributes like \\"HMAC.digest_size\\", \\"HMAC.block_size\\", and \\"HMAC.name\\" clarify additional information about the HMAC object created. Given this documentation and the complexity of securely managing and working with cryptographic message authentication tools, a challenging and in-depth assessment question can be crafted. <|Analysis End|> <|Question Begin|> **Problem Statement:** Given the inherent need to secure data using HMAC for message integrity and authentication, you are tasked with implementing a secure communication protocol using the HMAC module. **Your task** is to write a function `secure_message_protocol()` that securely transmits a message from a sender to a receiver using HMAC to ensure the message has not been tampered with. The function takes the following inputs: 1. `key`: A secret key as a byte string. 2. `message`: The message to be sent as a byte string. 3. `digestmod`: The digest algorithm to use (for example, \'sha256\'). 4. `manipulate`: A boolean that, if set to True, will introduce an arbitrary change in the message during transmission to simulate tampering. The function should: 1. Use HMAC to generate an HMAC digest of the message using the provided key and digestmod. 2. Print the original message and its HMAC digest. 3. Optionally manipulate (tamper) the message if `manipulate` is set to True (for example, change one byte in the message). 4. Verify the message using the HMAC digest and print whether the message is verified or if tampering is detected. **Function Signature:** ```python def secure_message_protocol(key: bytes, message: bytes, digestmod: str, manipulate: bool) -> None: pass ``` **Example Usage:** ```python key = b\'secret_key\' message = b\'This is a secure message.\' digestmod = \'sha256\' # Without message tampering secure_message_protocol(key, message, digestmod, False) # With message tampering secure_message_protocol(key, message, digestmod, True) ``` **Expected Output:** 1. When the message is not tampered: ``` Original Message: b\'This is a secure message.\' HMAC Digest: <hex representation of the HMAC digest> Message Verified: True ``` 2. When the message is tampered: ``` Original Message: b\'This is a secure message.\' HMAC Digest: <hex representation of the HMAC digest> Message Verified: False ``` **Constraints:** - Ensure the message verification process uses a secure comparison method to prevent timing attacks. - Handle any edge cases, such as empty messages or invalid key lengths, appropriately. **Performance Requirements:** - The function should efficiently handle message digests, avoiding any unnecessary overhead during the HMAC computation or verification process.","solution":"import hmac import hashlib def secure_message_protocol(key: bytes, message: bytes, digestmod: str, manipulate: bool) -> None: Securely transmit a message using HMAC for integrity check. Parameters: - key: Secret key as a byte string. - message: The message to be sent as a byte string. - digestmod: The digest algorithm to use, e.g., \'sha256\'. - manipulate: Flag to determine if the message should be tampered with during transmission. # Create the HMAC object and print the original message and digest hmac_object = hmac.new(key, msg=message, digestmod=digestmod) original_digest = hmac_object.hexdigest() print(f\\"Original Message: {message}\\") print(f\\"HMAC Digest: {original_digest}\\") # Optionally manipulate the message if manipulate: tampered_message = message + b\' \' else: tampered_message = message # Recreate the HMAC object for verification verification_hmac = hmac.new(key, msg=tampered_message, digestmod=digestmod) # Verify the message and print the result verified = hmac.compare_digest(original_digest, verification_hmac.hexdigest()) print(f\\"Message Verified: {verified}\\")"},{"question":"Context You are developing a Python application that interacts with lower-level system functions and processes. As part of this, you need to implement a utility function that demonstrates your understanding of argument parsing, process control, and string formatting. Task Write a Python function called `system_process_controller` that performs the following: 1. **Parses the function arguments**: - The function should accept a string representing a command to be executed in the system shell. - The function should accept an optional integer argument that sets a timeout for the process execution. 2. **Executes the command** in a subprocess: - If the command executes successfully within the given timeout, return the output of the command. - If the command execution fails or times out, return an appropriate error message. 3. **Formats and returns the output**: - The output should be formatted as a string that includes both the command executed and the result (whether output or error message). Function Signature ```python def system_process_controller(command: str, timeout: int = 10) -> str: ``` Example ```python # Example usage of the function result = system_process_controller(\\"echo Hello, World!\\", 5) print(result) # Should print: \\"Command: echo Hello, World!nOutput: Hello, World!\\" result = system_process_controller(\\"sleep 6\\", 5) print(result) # Should print: \\"Command: sleep 6nError: Command timed out\\" ``` Constraints - The `timeout` parameter should be a positive integer (default is 10 seconds). - The function should handle potential exceptions that occur during subprocess execution. Performance Requirements - The function should efficiently handle the execution and capture of subprocess outputs. - The function should handle various edge cases, such as invalid commands, timeouts, and subprocess errors gracefully. # Hints - You can use the `subprocess` module from Python\'s standard library to execute commands and manage processes. - Consider using `subprocess.run` with its provided parameters to simplify the handling of timeouts and capturing output.","solution":"import subprocess def system_process_controller(command: str, timeout: int = 10) -> str: Executes a given system command with an optional timeout and returns the command and result. Args: - command (str): The system command to be executed. - timeout (int, optional): The time in seconds to wait for the command to complete. Default is 10 seconds. Returns: - str: A formatted string with the command executed and the output/result message. try: result = subprocess.run(command, shell=True, text=True, capture_output=True, timeout=timeout) if result.returncode == 0: output = f\\"Command: {command}nOutput: {result.stdout.strip()}\\" else: output = f\\"Command: {command}nError: {result.stderr.strip()}\\" except subprocess.TimeoutExpired: output = f\\"Command: {command}nError: Command timed out\\" except Exception as e: output = f\\"Command: {command}nError: {str(e)}\\" return output"},{"question":"<|Analysis Begin|> The documentation provided is for the `sched` module in Python, which is used for event scheduling. This module allows for the creation and management of events that are scheduled to run at specific times or after certain delays. The key components of this module are: 1. **Class: `sched.scheduler`**: This class provides the core functionality for scheduling events. - `timefunc`: A callable that returns the current time (default is `time.monotonic`). - `delayfunc`: A callable that takes a time value and delays for that amount of time (default is `time.sleep`). 2. **Methods of `sched.scheduler`**: - `enterabs(time, priority, action, argument=(), kwargs={})`: Schedule an event to run at an absolute time. - `enter(delay, priority, action, argument=(), kwargs={})`: Schedule an event to run after a delay. - `cancel(event)`: Cancel a scheduled event. - `empty()`: Check if there are any scheduled events. - `run(blocking=True)`: Run all scheduled events, waiting for their time to come. - `queue`: A read-only attribute that lists upcoming events. The module also illustrates how events with the same scheduled time manage priority and how they can be cancelled. The `sched.scheduler` is flexible in that it can operate in both single-threaded and multi-threaded environments. <|Analysis End|> <|Question Begin|> # Python Coding Assessment Problem Statement In this task, you are required to implement a custom event scheduler using the `sched` module in Python. Your scheduler should support adding events with varying priorities, removing events, and running events when it is time for them to be executed. You need to implement a class `CustomScheduler` that mimics the core functionality of `sched.scheduler` but with an additional requirement to handle recurring events. # Class: `CustomScheduler` Implement a class `CustomScheduler` with the following methods: 1. **`__init__(self)`**: - Initializes a scheduler instance. 2. **`add_event(self, event_time, priority, action, argument=(), kwargs={}, recurring=False, interval=0)`**: - Schedules an event to run at `event_time` with `priority`. If `recurring` is `True`, the event should repeat every `interval` time units. - Parameters: - `event_time` (float): The time at which the event is scheduled to occur (absolute time). - `priority` (int): The priority of the event (lower number represents higher priority). - `action` (callable): The function to be executed. - `argument` (tuple): Positional arguments for the action. - `kwargs` (dict): Optional keyword arguments for the action. - `recurring` (bool): If `True`, the event repeats at the interval specified. - `interval` (int/float): The interval at which the recurring event repeats. 3. **`cancel_event(self, event)`**: - Cancels the specified event. 4. **`run(self, blocking=True)`**: - Runs all scheduled events. # Example ```python import time class CustomScheduler: # Your implementation here. # Example usage: def print_message(message=\'Hello\'): print(f\\"{time.time()}: {message}\\") # Instantiate the scheduler scheduler = CustomScheduler() # Add events scheduler.add_event(event_time=time.time() + 5, priority=1, action=print_message, argument=(\'First event\',)) scheduler.add_event(event_time=time.time() + 10, priority=1, action=print_message, argument=(\'Second event\',), recurring=True, interval=10) # Run the scheduler scheduler.run() ``` # Constraints 1. Use the `sched` module for the underlying scheduling mechanics. 2. Ensure that recurring events are scheduled correctly and keep repeating. 3. Concurrent execution and safe use in multi-threaded environments are not required but a bonus. **Note**: The `event_time` is in seconds since the epoch as returned by `time.time()`. Test your implementation thoroughly to ensure it handles edge cases such as overlapping events and correct prioritization.","solution":"import sched import time class CustomScheduler: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) def add_event(self, event_time, priority, action, argument=(), kwargs={}, recurring=False, interval=0): if recurring and interval <= 0: raise ValueError(\\"Interval must be positive for recurring events\\") def wrapper_action(): action(*argument, **kwargs) if recurring: new_event_time = event_time + interval self.add_event(new_event_time, priority, action, argument, kwargs, recurring, interval) return self.scheduler.enterabs(event_time, priority, wrapper_action) def cancel_event(self, event): self.scheduler.cancel(event) def run(self, blocking=True): self.scheduler.run(blocking)"},{"question":"# Advanced Python Logging Configuration Your task is to set up an advanced logging configuration using the `logging.config.dictConfig` function. You need to create a dictionary configuration that accomplishes the following: 1. Defines two loggers: - A root logger that logs all levels (`DEBUG` and above). - A custom logger named `custom_logger` that logs `ERROR` level messages only and propagates these messages to the root logger. 2. Configures two handlers: - A `StreamHandler` that writes logs to `sys.stdout` with a specific format. - A `FileHandler` that writes logs to a file called `app.log` with rotation settings: - The file should rotate when it reaches 1MB. - The backup count should be 3. 3. Sets up the following formatters: - A simple formatter that logs the message and the time it was logged. - A detailed formatter that logs the time, logger name, level, and message. 4. Associates the `StreamHandler` with the simple formatter and the `FileHandler` with the detailed formatter. # Input - None # Output - The function should configure the logging as described above. # Constraints - Do not read from or write to any actual file for verification purposes. This setup is to be used programmatically. - Ensure the logging configuration is correctly set up by logging messages and capturing the output using an appropriate logging function. # Implementation ```python import logging import logging.config import sys def setup_logging(): config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'maxBytes\': 1048576, \'backupCount\': 3, }, }, \'loggers\': { \'custom_logger\': { \'level\': \'ERROR\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], }, } logging.config.dictConfig(config) if __name__ == \\"__main__\\": setup_logging() # Test the logging configuration root_logger = logging.getLogger() custom_logger = logging.getLogger(\'custom_logger\') root_logger.debug(\'This is a debug message from the root logger.\') root_logger.info(\'This is an info message from the root logger.\') root_logger.warning(\'This is a warning message from the root logger.\') root_logger.error(\'This is an error message from the root logger.\') root_logger.critical(\'This is a critical message from the root logger.\') custom_logger.debug(\'This debug message should not appear in the custom logger.\') custom_logger.info(\'This info message should not appear in the custom logger.\') custom_logger.error(\'This error message should appear in the custom logger.\') custom_logger.critical(\'This critical message should appear in the custom logger.\') ``` This implementation sets up a logging configuration based on the specified requirements and tests it by logging messages of different levels to verify that both the root and custom loggers behave as expected.","solution":"import logging import logging.config import sys def setup_logging(): config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'simple\': { \'format\': \'%(asctime)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'simple\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'maxBytes\': 1048576, \'backupCount\': 3, }, }, \'loggers\': { \'custom_logger\': { \'level\': \'ERROR\', \'handlers\': [\'console\', \'file\'], \'propagate\': True, }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], }, } logging.config.dictConfig(config)"},{"question":"**Objective**: Demonstrate your understanding of Python data serialization and persistence using the `pickle` and `sqlite3` modules. **Problem Statement**: You are tasked with developing a small library system that can add books, serialize the library data, store it persistently, and retrieve it later. Specifically, you will: 1. Create a `Book` class with attributes: - `id`: a unique identifier (integer). - `title`: title of the book (string). - `author`: author of the book (string). - `year`: the year of publication (integer). 2. Create a `Library` class with attributes: - `books`: a list to store the books. It should have the following methods: - `add_book(self, book: Book)`: adds a `Book` instance to the library. - `save_to_file(self, filename: str)`: serializes the library\'s books list using `pickle` and saves it to the specified file. - `load_from_file(self, filename: str)`: deserializes the library\'s books list from the specified file using `pickle`. - `save_to_db(self, db_name: str)`: stores the library\'s books list into an SQLite database (using the book ID as the primary key). - `load_from_db(self, db_name: str)`: loads the library\'s books list from the specified SQLite database. **Constraints**: - Ensure your solution handles errors gracefully, such as file not found, database connection issues, etc. - Write clean, modular, and well-documented code. **Performance Requirements**: - Methods should handle a large number of books efficiently. **Example Usage**: ```python library = Library() book1 = Book(id=1, title=\\"1984\\", author=\\"George Orwell\\", year=1949) book2 = Book(id=2, title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\", year=1960) library.add_book(book1) library.add_book(book2) library.save_to_file(\'library.pkl\') library.load_from_file(\'library.pkl\') library.save_to_db(\'library.db\') library.load_from_db(\'library.db\') print([book.title for book in library.books]) # Should print book titles loaded from file or database. ``` **Input**: - Methods do not take input from the terminal but from method arguments and file/database. **Output**: - Methods do not print output to the terminal but return appropriate objects or store data persistently. Implement the classes and methods described above.","solution":"import pickle import sqlite3 class Book: def __init__(self, id, title, author, year): self.id = id self.title = title self.author = author self.year = year class Library: def __init__(self): self.books = [] def add_book(self, book: Book): if not any(b.id == book.id for b in self.books): self.books.append(book) def save_to_file(self, filename: str): try: with open(filename, \'wb\') as f: pickle.dump(self.books, f) except Exception as e: print(f\\"An error occurred while saving to file: {e}\\") def load_from_file(self, filename: str): try: with open(filename, \'rb\') as f: self.books = pickle.load(f) except Exception as e: print(f\\"An error occurred while loading from file: {e}\\") def save_to_db(self, db_name: str): try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS library (id INTEGER PRIMARY KEY, title TEXT, author TEXT, year INTEGER)\'\'\') cursor.execute(\'DELETE FROM library\') # Clear existing data for book in self.books: cursor.execute(\'INSERT INTO library (id, title, author, year) VALUES (?, ?, ?, ?)\', (book.id, book.title, book.author, book.year)) conn.commit() conn.close() except Exception as e: print(f\\"An error occurred while saving to database: {e}\\") def load_from_db(self, db_name: str): try: conn = sqlite3.connect(db_name) cursor = conn.cursor() cursor.execute(\'SELECT * FROM library\') rows = cursor.fetchall() self.books = [Book(id=row[0], title=row[1], author=row[2], year=row[3]) for row in rows] conn.close() except Exception as e: print(f\\"An error occurred while loading from database: {e}\\")"},{"question":"Memory Usage Analysis with Tracemalloc Objective Your task is to implement a function that starts tracing memory allocations, executes a piece of provided code, takes a snapshot before and after the execution, and then returns detailed information about memory usage. Requirements 1. **Function Signature**: ```python def analyze_memory_usage(code_to_execute: str) -> dict: pass ``` 2. **Parameters**: - `code_to_execute` (str): A string containing Python code to be executed. 3. **Return**: - A dictionary with the following keys and value types: - `initial_memory` (int): The total size of memory blocks at the start of the trace. - `peak_memory` (int): The peak size of memory blocks during the execution. - `final_memory` (int): The total size of memory blocks at the end of the execution. - `top_memory_stats` (list): A list of dictionaries, each containing: - `filename` (str): The filename where the memory block was allocated. - `lineno` (int): The line number in the file. - `size` (int): The size of the memory block in bytes. - `count` (int): The number of memory blocks allocated. 4. **Constraints**: - Use the `tracemalloc` module to handle memory tracing. - Ensure that memory tracing is started before any memory allocation occurs in `code_to_execute`. - Reset the peak memory before executing `code_to_execute` to capture an accurate peak. 5. **Notes**: - Use the `exec` function to execute the provided code. - Make use of the `tracemalloc.Filter` to exclude files such as `<frozen importlib._bootstrap>` and `<unknown>`. - The statistics should be sorted from the largest to the smallest memory allocation. Example Usage ```python code = large_list = [i for i in range(100000)] small_list = [i for i in range(1000)] result = analyze_memory_usage(code) print(result) ``` **Expected Output Format** (example): ```python { \'initial_memory\': 664, \'peak_memory\': 3592984, \'final_memory\': 804, \'top_memory_stats\': [ {\'filename\': \'script.py\', \'lineno\': 2, \'size\': 419840, \'count\': 1}, {\'filename\': \'script.py\', \'lineno\': 3, \'size\': 23904, \'count\': 1}, ... ] } ``` Good Luck!","solution":"import tracemalloc def analyze_memory_usage(code_to_execute: str) -> dict: Analyzes memory usage of given code. Parameters: - code_to_execute (str): A string containing Python code to be executed. Returns: - dict: A dictionary containing memory usage stats. tracemalloc.start() # Take initial snapshot initial_snapshot = tracemalloc.take_snapshot() # Execute the code exec(code_to_execute) # Take final snapshot and get memory stats final_snapshot = tracemalloc.take_snapshot() # Gather initial and final memory stats initial_stats = initial_snapshot.statistics(\'lineno\') final_stats = final_snapshot.statistics(\'lineno\') # Calculate total size of memory blocks at the start and end of the execution initial_memory = sum(stat.size for stat in initial_stats) final_memory = sum(stat.size for stat in final_stats) # Get the peak memory usage peak_memory = tracemalloc.get_traced_memory()[1] # Collect top memory statistics, sorting them by size top_stats = final_snapshot.statistics(\'lineno\') top_memory_stats = sorted([{ \'filename\': stat.traceback[0].filename, \'lineno\': stat.traceback[0].lineno, \'size\': stat.size, \'count\': stat.count } for stat in top_stats], key=lambda x: x[\'size\'], reverse=True) tracemalloc.stop() return { \'initial_memory\': initial_memory, \'peak_memory\': peak_memory, \'final_memory\': final_memory, \'top_memory_stats\': top_memory_stats[:10] # Limit to top 10 entries }"},{"question":"# Argument Clinic Function Conversion Task Objective In this task, you are required to demonstrate your understanding of the Argument Clinic tool by converting a C function used in a CPython module to use Argument Clinic for its argument parsing. This exercise focuses on converting a simple C function and ensuring the conversion aligns with the capabilities discussed in the documentation. Problem Statement You are given a C function that is part of a CPython built-in module, which currently uses `PyArg_ParseTuple` for argument parsing. You need to convert it to use Argument Clinic while following the steps provided in the documentation. **Function to Convert:** ```c /* Original C function (part of a CPython module) */ #include <Python.h> /* This function adds two integers */ static PyObject* module_add(PyObject *self, PyObject *args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) return NULL; return PyLong_FromLong((long)(a + b)); } /* Method definition table for the module */ static PyMethodDef ModuleMethods[] = { {\\"add\\", module_add, METH_VARARGS, \\"Add two integers and return the result.\\"}, {NULL, NULL, 0, NULL} }; /* Module definition */ static struct PyModuleDef moduledef = { PyModuleDef_HEAD_INIT, \\"module\\", /* m_name */ \\"This is a test module.\\", /* m_doc */ -1, /* m_size */ ModuleMethods /* m_methods */ }; /* Initialize the module */ PyMODINIT_FUNC PyInit_module(void) { return PyModule_Create(&moduledef); } ``` Instructions 1. **Add an Argument Clinic Block**: - Insert an Argument Clinic block above the function `module_add`. - Define the module and method, and describe the function parameters. 2. **Modify the Function Signature**: - Adjust the function definition to use the argument names parsed by Argument Clinic. 3. **Generate the Output**: - Use the `Tools/clinic/clinic.py` script to process your file. - Check the output and ensure it matches the expected format, particularly in terms of argument parsing. 4. **Update the Method Definition Table**: - Replace the existing method definition in the table with the macro generated by Argument Clinic. 5. **Ensure Correct Functionality**: - The resulting function should parse the arguments correctly and sum the integers as in the original function. Example Solution Template Your converted function should look similar to the template below: ```c /*[clinic input] module.add a: int b: int / Add two integers and return the result. [clinic start generated code]*/ /*[clinic end generated code: checksum]*/ static PyObject* module_add_impl(PyObject *module, int a, int b) { return PyLong_FromLong((long)(a + b)); } /* Method definition table for the module */ static PyMethodDef ModuleMethods[] = { {\\"add\\", (PyCFunction)module_add, METH_VARARGS, module_add__doc__}, {NULL, NULL, 0, NULL} }; /* Module definition */ static struct PyModuleDef moduledef = { PyModuleDef_HEAD_INIT, \\"module\\", /* m_name */ \\"This is a test module.\\", /* m_doc */ -1, /* m_size */ ModuleMethods /* m_methods */ }; /* Initialize the module */ PyMODINIT_FUNC PyInit_module(void) { return PyModule_Create(&moduledef); } #include \\"clinic/module.c.h\\" ``` **Note:** Follow the steps provided in the documentation for each part of the conversion. Ensure the code compiles correctly and the module works as expected. Submission Submit the modified C file with the Argument Clinic conversion applied, including all generated code and updated method definitions.","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"**Coding Assessment: Understanding Seaborn\'s JointGrid** **Objective:** Utilize the seaborn `JointGrid` to create a customized visualization and demonstrate your understanding of seaborn\'s capabilities, data handling, and plot customization. **Problem Statement:** Write a function `plot_penguins_jointgrid` that creates a customized JointGrid plot using the `penguins` dataset from seaborn. The plot should include a scatter plot for the joint axes and histograms for the marginal axes. Additionally, the function should add reference lines, customize different features of the plot, and save the figure to a file. **Function Signature:** ```python def plot_penguins_jointgrid(file_path: str) -> None: pass ``` **Inputs:** - `file_path` (str): The path where the plot image should be saved. **Output:** - The function should save the created plot as an image file at the specified file path. **Instructions:** 1. Load the `penguins` dataset using `seaborn.load_dataset(\'penguins\')`. 2. Create a `JointGrid` with `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. 3. Plot a scatterplot for the joint axes and histograms for the marginal axes. 4. Customize the scatterplot with the following properties: - Marker size: 100 - Transparency: 0.5 5. Customize the histograms with the following properties: - Plot a kernel density estimate (KDE) over the histograms. 6. Add reference lines at x=45 and y=16. 7. Set the height of the plot to 6, the ratio between joint and marginal axes to 3, and the space between plots to 0.1. 8. Save the plot as an image file at the specified `file_path`. **Constraints:** - Ensure the plot is saved correctly, and no errors are raised during the creation and saving of the plot. - Use appropriate seaborn functions and customization parameters as needed. **Example Usage:** ```python plot_penguins_jointgrid(\\"penguins_jointgrid.png\\") ``` This question assesses understanding of: 1. Loading datasets using seaborn. 2. Creating and customizing JointGrid plots. 3. Handling multiple plot types within a single grid. 4. Customizing plot aesthetics.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_penguins_jointgrid(file_path: str) -> None: Creates a customized JointGrid plot using the penguins dataset and saves the plot to a file. Parameters: file_path (str): The path where the plot image should be saved. # Load the penguins dataset penguins = sns.load_dataset(\'penguins\') # Create a JointGrid with bill_length_mm on the x-axis and bill_depth_mm on the y-axis grid = sns.JointGrid(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', height=6, ratio=3, space=0.1) # Plot a scatter plot for the joint axes and histograms for the marginal axes grid = grid.plot(sns.scatterplot, sns.histplot) # Customize the scatter plot grid.ax_joint.collections[0].set_alpha(0.5) grid.ax_joint.collections[0].set_sizes([100]) # Add kernel density estimates to the histograms on the marginal axes sns.kdeplot(data=penguins, x=\'bill_length_mm\', ax=grid.ax_marg_x, color=\'blue\', fill=True) sns.kdeplot(data=penguins, y=\'bill_depth_mm\', ax=grid.ax_marg_y, color=\'blue\', fill=True) # Add reference lines grid.ax_joint.axvline(45, color=\'red\', linestyle=\'--\') grid.ax_joint.axhline(16, color=\'green\', linestyle=\'--\') # Save the plot to a file plt.savefig(file_path) plt.close()"},{"question":"# Pandas Data Manipulation and Analysis You are provided with a dataset containing sales data for a retail company. The dataset includes the following columns: - `Date`: The date of the sale (string format, MM-DD-YYYY). - `Store`: The store identifier (string). - `Product`: The product identifier (string). - `Quantity`: The number of units sold (integer). - `Revenue`: The total revenue from the sale (float). **Sample Input (CSV format):** ``` Date,Store,Product,Quantity,Revenue 01-01-2023,A,P1,10,100.0 01-01-2023,A,P2,5,50.0 01-01-2023,B,P1,15,150.0 01-02-2023,A,P1,7,70.0 01-02-2023,A,P2,3,30.0 01-02-2023,B,P1,10,100.0 ``` **Objective:** Write a function `analyze_sales` that takes in the file path to the CSV file as input and performs the following operations: 1. **Read the data** into a pandas DataFrame. 2. **Convert the `Date` column** to datetime format. 3. **Calculate the total quantity sold and total revenue** for each `Store` on each `Date`. 4. **Reshape the DataFrame** so that each row represents a unique `Date` and each column represents a unique `Store`, with the values being the total revenue for that store on that date. 5. **Fill any missing values** with 0. 6. **Return the reshaped DataFrame**. # Constraints - The input CSV file may have dates in non-chronological order. - The dataset may have missing records for some store-date combinations. # Performance Requirements - The function should handle datasets with up to 10,000 rows efficiently. # Function Signature ```python import pandas as pd def analyze_sales(file_path: str) -> pd.DataFrame: pass ``` # Example ```python # Assuming the CSV content as mentioned in the sample input, # calling the function will yield the following DataFrame: analyze_sales(\'sales_data.csv\') # Output DataFrame: A B Date 2023-01-01 150.0 150.0 2023-01-02 100.0 100.0 ``` # Notes - Ensure that the `Date` column in the output DataFrame is of datetime type and is used as the index. - The column names in the output DataFrame should be the unique values from the `Store` column of the input DataFrame. - Use appropriate pandas functions to achieve the desired transformations and ensure code readability and efficiency.","solution":"import pandas as pd def analyze_sales(file_path: str) -> pd.DataFrame: # Read the data into a DataFrame df = pd.read_csv(file_path) # Convert the \'Date\' column to datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\'], format=\'%m-%d-%Y\') # Calculate the total quantity sold and total revenue for each \'Store\' on each \'Date\' summary_df = df.groupby([\'Date\', \'Store\']).agg({\'Quantity\': \'sum\', \'Revenue\': \'sum\'}).reset_index() # Reshape the DataFrame so each row represents a unique \'Date\' and each column represents a unique \'Store\' pivot_df = summary_df.pivot(index=\'Date\', columns=\'Store\', values=\'Revenue\').fillna(0) # Return the reshaped DataFrame return pivot_df"},{"question":"**Density Estimation using scikit-learn\'s KernelDensity** # Problem Statement You are provided with a dataset containing one-dimensional numeric data points. Your task is to use scikit-learn\'s `KernelDensity` class to perform kernel density estimation on this data. You will implement a function that computes and compares density estimates using different kernels and bandwidth values. Additionally, you will plot the estimated density functions for visualization. # Dataset The dataset `data` is given as a one-dimensional numpy array. For this example, consider the following array structure: ```python data = np.array([1.0, 2.3, 2.9, 4.1, 4.7, 5.3, 6.5, 7.8, 8.0, 9.1]) ``` # Function Signature Implement the function `kde_comparison(data: np.ndarray) -> None`. # Objectives 1. **Density Estimation with Different Kernels**: - Use different kernel methods available in `KernelDensity` (gaussian, tophat, epanechnikov, exponential, linear, cosine). - Implement KDE for each kernel. 2. **Effect of Bandwidth**: - Explore the effect of different bandwidth parameters (e.g., 0.1, 0.5, 1.0). 3. **Visualization**: - Plot the density estimates for each combination of kernel and bandwidth. - Each plot should clearly indicate the kernel type and bandwidth used. # Function Implementation Details - Import necessary libraries: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity ``` - Create a dictionary of kernels and their corresponding labels: ```python kernels = { \'gaussian\': \'Gaussian\', \'tophat\': \'Tophat\', \'epanechnikov\': \'Epanechnikov\', \'exponential\': \'Exponential\', \'linear\': \'Linear\', \'cosine\': \'Cosine\' } ``` - Create a list of bandwidths to iterate over: ```python bandwidths = [0.1, 0.5, 1.0] ``` - Create a range of values for which the density will be estimated: ```python x_d = np.linspace(min(data)-1, max(data)+1, 1000) ``` - Implement KDE for each kernel and bandwidth combination. Compute the scores and plot the density estimates: ```python fig, axs = plt.subplots(len(kernels), len(bandwidths), figsize=(20, 15)) for i, (kernel, kernel_label) in enumerate(kernels.items()): for j, bandwidth in enumerate(bandwidths): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data[:, None]) log_dens = kde.score_samples(x_d[:, None]) axs[i, j].plot(x_d, np.exp(log_dens), label=f\'Bandwidth={bandwidth}\') axs[i, j].set_title(f\'Kernel={kernel_label}, Bandwidth={bandwidth}\') axs[i, j].legend() plt.tight_layout() plt.show() ``` # Submission Submit the implemented function `kde_comparison` along with the output plots. The plots should provide a grid representation of the different kernels and bandwidths, allowing for a visual comparison of their effects on the density estimation. # Additional Constraints - Ensure your function runs efficiently. - Handle edge cases, such as empty data input (if encountered).","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_comparison(data: np.ndarray) -> None: kernels = { \'gaussian\': \'Gaussian\', \'tophat\': \'Tophat\', \'epanechnikov\': \'Epanechnikov\', \'exponential\': \'Exponential\', \'linear\': \'Linear\', \'cosine\': \'Cosine\' } bandwidths = [0.1, 0.5, 1.0] x_d = np.linspace(min(data) - 1, max(data) + 1, 1000) fig, axs = plt.subplots(len(kernels), len(bandwidths), figsize=(20, 15)) for i, (kernel, kernel_label) in enumerate(kernels.items()): for j, bandwidth in enumerate(bandwidths): kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(data[:, None]) log_dens = kde.score_samples(x_d[:, None]) axs[i, j].plot(x_d, np.exp(log_dens), label=f\'Bandwidth={bandwidth}\') axs[i, j].set_title(f\'Kernel={kernel_label}, Bandwidth={bandwidth}\') axs[i, j].legend() plt.tight_layout() plt.show()"},{"question":"**Problem Statement:** Given a dataset on diamond characteristics, use seaborn to create a series of visualizations that help analyze the relationship between the price of diamonds and their physical attributes. Your task is to implement the following functions: 1. **load_diamond_data** Load the diamond dataset using seaborn. ```python def load_diamond_data() -> pd.DataFrame: Load the diamond dataset using seaborn. Returns: pd.DataFrame: The loaded diamond dataset. ``` 2. **plot_price_distribution** Create a horizontal boxen plot showing the distribution of diamond prices. ```python def plot_price_distribution(diamonds: pd.DataFrame) -> None: Plot the distribution of diamond prices using a horizontal boxen plot. Args: diamonds (pd.DataFrame): The diamond dataset. ``` 3. **plot_price_vs_clarity** Create a boxen plot to visualize the relationship between diamond price and clarity. ```python def plot_price_vs_clarity(diamonds: pd.DataFrame) -> None: Plot the relationship between diamond price and clarity using a boxen plot. Args: diamonds (pd.DataFrame): The diamond dataset. ``` 4. **plot_price_by_clarity_size** Create a boxen plot grouped by clarity and differentiated by diamond size (`carat > 1`). ```python def plot_price_by_clarity_size(diamonds: pd.DataFrame) -> None: Plot the relationship between diamond price and clarity, with large diamonds colored differently. Args: diamonds (pd.DataFrame): The diamond dataset. ``` 5. **plot_customized_boxen** Create a customized boxen plot of diamond price vs. clarity with specific width and outline settings. ```python def plot_customized_boxen(diamonds: pd.DataFrame) -> None: Create a customized boxen plot of diamond price vs. clarity with settings for width, line color, and width. Args: diamonds (pd.DataFrame): The diamond dataset. ``` # Constraints: - You may not modify the dataset. - Use the seaborn package exclusively for plotting. - Ensure plots are appropriately labeled and easy to interpret. # Sample Implementation: Here is a sample implementation to get you started: ```python import seaborn as sns import pandas as pd # Function implementations go here # Load dataset diamonds = load_diamond_data() # Generate plots plot_price_distribution(diamonds) plot_price_vs_clarity(diamonds) plot_price_by_clarity_size(diamonds) plot_customized_boxen(diamonds) ``` # Expected Output: - A horizontal boxen plot of diamond prices. - A boxen plot showing diamond prices across different levels of clarity. - A boxen plot showing diamond prices across different levels of clarity and sized by `carat`. - A customized boxen plot showing the price vs. clarity relationship with specified width and outline parameters. Ensure your plots are well-formatted and provide insights into the diamond pricing trends based on various characteristics.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def load_diamond_data() -> pd.DataFrame: Load the diamond dataset using seaborn. Returns: pd.DataFrame: The loaded diamond dataset. return sns.load_dataset(\'diamonds\') def plot_price_distribution(diamonds: pd.DataFrame) -> None: Plot the distribution of diamond prices using a horizontal boxen plot. Args: diamonds (pd.DataFrame): The diamond dataset. plt.figure(figsize=(12, 6)) sns.boxenplot(x=\'price\', data=diamonds, orient=\'h\') plt.title(\'Distribution of Diamond Prices\') plt.xlabel(\'Price\') plt.show() def plot_price_vs_clarity(diamonds: pd.DataFrame) -> None: Plot the relationship between diamond price and clarity using a boxen plot. Args: diamonds (pd.DataFrame): The diamond dataset. plt.figure(figsize=(12, 6)) sns.boxenplot(x=\'clarity\', y=\'price\', data=diamonds) plt.title(\'Price vs Clarity of Diamonds\') plt.xlabel(\'Clarity\') plt.ylabel(\'Price\') plt.show() def plot_price_by_clarity_size(diamonds: pd.DataFrame) -> None: Plot the relationship between diamond price and clarity, with large diamonds colored differently. Args: diamonds (pd.DataFrame): The diamond dataset. diamonds[\'size\'] = diamonds[\'carat\'].apply(lambda x: \'Large\' if x > 1 else \'Small\') plt.figure(figsize=(12, 6)) sns.boxenplot(x=\'clarity\', y=\'price\', hue=\'size\', data=diamonds) plt.title(\'Price vs Clarity of Diamonds by Size\') plt.xlabel(\'Clarity\') plt.ylabel(\'Price\') plt.show() def plot_customized_boxen(diamonds: pd.DataFrame) -> None: Create a customized boxen plot of diamond price vs. clarity with settings for width, line color, and width. Args: diamonds (pd.DataFrame): The diamond dataset. plt.figure(figsize=(12, 6)) sns.boxenplot(x=\'clarity\', y=\'price\', data=diamonds, linewidth=1.5, color=\'blue\') plt.title(\'Customized Boxen Plot of Price vs Clarity of Diamonds\') plt.xlabel(\'Clarity\') plt.ylabel(\'Price\') plt.show()"},{"question":"# Seaborn Clustermap Coding Assessment Objective Your task is to demonstrate your understanding of seaborn\'s `clustermap` functionality by implementing a function that plots a customized heatmap with clustering. Problem Statement Write a function `custom_clustermap` that takes a dataset and several customization parameters to plot a clustermap. The function should: 1. Load the `iris` dataset. 2. Drop the `species` column. 3. Perform clustering on the resulting dataset. 4. Apply customizations based on the function\'s parameters. 5. Display the clustermap. Function Signature ```python def custom_clustermap(figsize=(10, 7), row_cluster=True, col_cluster=True, metric=\\"euclidean\\", method=\\"average\\", cmap=\\"viridis\\", standard_scale=None, z_score=None) -> None: pass ``` Parameters - `figsize`: Tuple of integers or floats, specifying the size of the figure in inches (width, height). Default is (10, 7). - `row_cluster`: Boolean, indicating whether to cluster rows. Default is `True`. - `col_cluster`: Boolean, indicating whether to cluster columns. Default is `True`. - `metric`: String, the distance metric to use for clustering. Default is `\\"euclidean\\"`. - `method`: String, the linkage algorithm to use. Default is `\\"average\\"`. - `cmap`: String, the colormap to use. Default is `\\"viridis\\"`. - `standard_scale`: Integer {0, 1} or None, indicating which axis to standardize (normalize to mean 0 and variance 1). Default is `None`. - `z_score`: Integer {0, 1} or None, indicating which axis to normalize (subtract mean and divide by standard deviation). Default is `None`. Constraints - Ensure the function does not return any value, but plots the clustermap directly. - All parameters are optional and should have default values. Example ```python # Generate a clustermap with default parameters custom_clustermap() # Generate a clustermap with no row clustering, specific metric and method custom_clustermap(row_cluster=False, metric=\\"correlation\\", method=\\"single\\") # Generate a clustermap with standardization and a different colormap custom_clustermap(standard_scale=1, cmap=\\"magma\\") ``` # Hints: - Use `sns.clustermap` for generating the clustermap. - Use `sns.load_dataset(\\"iris\\")` to load the dataset and drop the column `species`. Assessment Criteria - Correct implementation of parameterized clustermap plotting. - Proper handling of optional parameters with default values. - The code should be clean, well-commented, and follow best practices for readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_clustermap(figsize=(10, 7), row_cluster=True, col_cluster=True, metric=\\"euclidean\\", method=\\"average\\", cmap=\\"viridis\\", standard_scale=None, z_score=None): Plots a seaborn clustermap for the iris dataset with customizable parameters. Parameters: - figsize: tuple, size of the figure in inches (width, height). Default is (10, 7). - row_cluster: bool, whether to cluster rows. Default is True. - col_cluster: bool, whether to cluster columns. Default is True. - metric: str, distance metric for clustering. Default is \'euclidean\'. - method: str, linkage method for clustering. Default is \'average\'. - cmap: str, color map for the heatmap. Default is \'viridis\'. - standard_scale: int {0, 1} or None, axis to standardize. Default is None. - z_score: int {0, 1} or None, axis to normalize. Default is None. # Load the iris dataset iris = sns.load_dataset(\'iris\') # Drop the \'species\' column iris_data = iris.drop(columns=[\'species\']) # Generate the clustermap sns.clustermap(iris_data, figsize=figsize, row_cluster=row_cluster, col_cluster=col_cluster, metric=metric, method=method, cmap=cmap, standard_scale=standard_scale, z_score=z_score) # Show the plot plt.show()"},{"question":"**Objective:** Write a Python function that performs a series of system operations demonstrating proficiency in using the `os` module. This function should: 1. Create a new directory named `test_directory` in the current directory. 2. Inside `test_directory`, create three files named `file1.txt`, `file2.txt`, and `file3.txt`. 3. Write the string \\"Hello, World!\\" to `file1.txt` and `file2.txt`. Do not write anything to `file3.txt`. 4. List and print the names of all files in the `test_directory` that are not empty. 5. Create a symbolic link named `link_to_file1` pointing to `file1.txt`. 6. Retrieve and print the value of the environment variable `PATH`. 7. Modify the `PATH` environment variable by appending `test_directory` to it. 8. Read and print the contents of `file1.txt` using standard file operations. 9. Clean up: Remove `link_to_file1`, `file1.txt`, `file2.txt`, and `file3.txt` (only the files, not the directory). **Constraints:** - Handle exceptions where necessary to ensure that the function does not fail if the directory or files already exist. - Assume that the environment is Unix-based for symbolic link creation and handling. - Use appropriate functions from the `os` module wherever applicable. **Function Signature:** ```python def system_operations_using_os(): pass ``` **Expected Behavior:** - The function should perform each of the specified operations in sequence. - Provide meaningful print statements to show each step and outcome. **Example Usage:** ``` system_operations_using_os() ``` **Note:** Make sure to thoroughly test the function in a controlled environment to ensure proper functionality and cleanup.","solution":"import os def system_operations_using_os(): try: # 1. Create a new directory named `test_directory` in the current directory. os.makedirs(\'test_directory\', exist_ok=True) # 2. Inside `test_directory`, create three files named `file1.txt`, `file2.txt`, and `file3.txt`. open(\'test_directory/file1.txt\', \'w\').close() open(\'test_directory/file2.txt\', \'w\').close() open(\'test_directory/file3.txt\', \'w\').close() # 3. Write the string \\"Hello, World!\\" to `file1.txt` and `file2.txt`. with open(\'test_directory/file1.txt\', \'w\') as f: f.write(\\"Hello, World!\\") with open(\'test_directory/file2.txt\', \'w\') as f: f.write(\\"Hello, World!\\") # 4. List and print the names of all files in the `test_directory` that are not empty. print(\\"Non-empty files in test_directory:\\") for file_name in os.listdir(\'test_directory\'): file_path = os.path.join(\'test_directory\', file_name) if os.path.isfile(file_path) and os.path.getsize(file_path) > 0: print(file_name) # 5. Create a symbolic link named `link_to_file1` pointing to `file1.txt`. os.symlink(\'test_directory/file1.txt\', \'test_directory/link_to_file1\') # 6. Retrieve and print the value of the environment variable `PATH`. print(\\"Original PATH:\\", os.environ.get(\'PATH\', \'\')) # 7. Modify the `PATH` environment variable by appending `test_directory` to it. os.environ[\'PATH\'] += os.pathsep + os.path.abspath(\'test_directory\') print(\\"Modified PATH:\\", os.environ[\'PATH\']) # 8. Read and print the contents of `file1.txt` using standard file operations. with open(\'test_directory/file1.txt\', \'r\') as f: print(\\"Contents of file1.txt:\\", f.read()) # 9. Clean up: Remove `link_to_file1`, `file1.txt`, `file2.txt`, and `file3.txt` (only the files, not the directory). os.remove(\'test_directory/link_to_file1\') os.remove(\'test_directory/file1.txt\') os.remove(\'test_directory/file2.txt\') os.remove(\'test_directory/file3.txt\') print(\\"Clean up completed.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Managing Pandas Display Options Objective You are required to demonstrate your understanding of the pandas options API by performing a series of tasks that involve getting, setting, resetting, and describing pandas display options. Task Write a Python function `manage_pandas_options` that performs the following steps: 1. Sets the following display options: - `display.max_rows` to 15 - `display.precision` to 4 2. Creates a DataFrame with random values using `numpy` with 20 rows and 5 columns. 3. Prints the DataFrame to demonstrate the set options. 4. Retrieves and prints the current values of the `display.max_rows` and `display.precision` options. 5. Resets all display options to their default values. 6. Retrieves and prints the current values of the `display.max_rows` and `display.precision` options after the reset. 7. Describes and prints the descriptions of the options that were set and reset: `display.max_rows` and `display.precision`. Input - This function does not take any parameters. Output - The function should print the DataFrame created in step 2. - The function should print the current values of the `display.max_rows` and `display.precision` options before and after the reset. - The function should print the descriptions of `display.max_rows` and `display.precision`. Example Suppose you run the function: ```python manage_pandas_options() ``` The expected output should look like this (values from the DataFrame will vary since they are random): ``` 0 1 2 3 4 0 0.1234 -0.9876 1.2345 -0.4321 0.9876 1 0.1234 -0.9876 1.2345 -0.4321 0.9876 ... 19 -1.3456 0.7654 0.9876 -0.1234 -0.8765 Current display.max_rows: 15 Current display.precision: 4 After reset: Current display.max_rows: 60 Current display.precision: 6 Description of display.max_rows: display.max_rows: maximum number of rows to display in the console. Description of display.precision: display.precision: floating point output precision in terms of number of digits after the decimal point. ``` Constraints - Use the `pandas` and `numpy` libraries in your solution. - The function should be able to run independently and reproduce the output as described in the example. Note Ensure to handle any potential errors that might occur while getting or setting options.","solution":"import pandas as pd import numpy as np def manage_pandas_options(): Manages pandas display options by setting specific options, printing a DataFrame, and resetting the options to defaults. # Step 1: Set display options pd.set_option(\'display.max_rows\', 15) pd.set_option(\'display.precision\', 4) # Step 2: Create a DataFrame with random values (20 rows, 5 columns) df = pd.DataFrame(np.random.randn(20, 5)) # Step 3: Print the DataFrame to demonstrate the set options print(\\"DataFrame with set options:\\") print(df, \\"n\\") # Step 4: Retrieve and print the current values of the set options max_rows = pd.get_option(\'display.max_rows\') precision = pd.get_option(\'display.precision\') print(f\\"Current display.max_rows: {max_rows}\\") print(f\\"Current display.precision: {precision}n\\") # Step 5: Reset all display options to their default values pd.reset_option(\'all\') # Step 6: Retrieve and print the current values of the options after reset max_rows_reset = pd.get_option(\'display.max_rows\') precision_reset = pd.get_option(\'display.precision\') print(\\"After reset:\\") print(f\\"Current display.max_rows: {max_rows_reset}\\") print(f\\"Current display.precision: {precision_reset}n\\") # Step 7: Describe and print the descriptions of the options print(\\"Description of display.max_rows:\\") print(pd.describe_option(\'display.max_rows\'), \\"n\\") print(\\"Description of display.precision:\\") print(pd.describe_option(\'display.precision\'))"},{"question":"**Dynamic Class Creation and Attribute Access** You are tasked with creating a dynamic class using the `types` module in Python. This class should have specific attributes and methods defined dynamically. The goal is to leverage the `new_class` and other relevant utilities from the `types` module. # Problem Statement Write a function `create_dynamic_class` that takes the following parameters: 1. `class_name` (str): The name of the class to be created. 2. `base_classes` (tuple): A tuple of base classes. 3. `class_attributes` (dict): A dictionary of attributes and their values to be added to the class. 4. `methods` (dict): A dictionary where keys are method names and values are lambda functions representing the method implementations. The function should return an instance of the newly created class with the attributes and methods specified. # Example ```python def example_method(self): return \'Hello, World!\' class_instance = create_dynamic_class( class_name=\'MyDynamicClass\', base_classes=(), class_attributes={\'attr1\': 100, \'attr2\': \'test\'}, methods={\'example_method\': lambda self: \'Hello, World!\'} ) print(class_instance.attr1) # Output: 100 print(class_instance.attr2) # Output: test print(class_instance.example_method()) # Output: Hello, World! ``` # Constraints 1. Attribute names in the `class_attributes` dictionary should be valid Python identifiers. 2. Methods should take `self` as their first parameter. 3. The created class should properly inherit from any base classes specified in `base_classes`. # Implementation Notes - Use the `types.new_class` function to dynamically create a class. - Use a callback function to populate the class namespace with attributes and methods from `class_attributes` and `methods`. - Ensure the created class can have its attributes and methods accessed as illustrated in the example.","solution":"import types def create_dynamic_class(class_name, base_classes, class_attributes, methods): Dynamically creates a class with the specified attributes and methods. Args: class_name (str): The name of the class to be created. base_classes (tuple): A tuple of base classes. class_attributes (dict): A dictionary of attributes and their values to be added to the class. methods (dict): A dictionary where keys are method names and values are lambda functions representing the method implementations. Returns: An instance of the newly created class. # Create the class using types.new_class cls = types.new_class(class_name, base_classes) # Define the namespace for the class with the provided attributes and methods attrs = class_attributes.copy() attrs.update(methods) # Create the class instance for key, value in attrs.items(): setattr(cls, key, value) return cls()"},{"question":"**Pandas Coding Assessment: Advanced GroupBy Operations** # Problem Statement You are provided with a dataset that contains information about sales transactions from a retail store. Your task is to write a function that performs various `GroupBy` operations on this dataset to extract relevant insights. # Dataset Description The dataset `transactions.csv` contains the following columns: - `TransactionID` (int): A unique identifier for each transaction. - `ProductID` (int): A unique identifier for each product. - `StoreID` (int): A unique identifier for each store. - `Date` (datetime): The date of the transaction. - `Quantity` (int): The number of units sold. - `Price` (float): The price per unit of the product. # Function Details Implement the function `analyze_sales(transactions)` that takes in a DataFrame `transactions` and returns a dictionary with the following keys and their corresponding values: 1. `total_sales_per_store`: A DataFrame showing the total sales for each store. Columns should be `StoreID` and `TotalSales`. 2. `average_quantity_per_product`: A DataFrame showing the average quantity sold for each product. Columns should be `ProductID` and `AvgQuantity`. 3. `monthly_sales_trend`: A DataFrame showing the total sales per month. Columns should be `YearMonth` (formatted as \\"YYYY-MM\\") and `MonthlySales`. 4. `top_selling_product_per_store`: A DataFrame showing the top-selling product in each store. Columns should be `StoreID`, `ProductID`, and `TotalQuantitySold`. # Constraints - Your solution should efficiently handle large datasets (up to 1 million rows). - Use appropriate `GroupBy` methods to aggregate and transform the data. # Expected Output Format Your function should return a dictionary containing the four DataFrames described above. The format should be as follows: ```python { \\"total_sales_per_store\\": DataFrame, \\"average_quantity_per_product\\": DataFrame, \\"monthly_sales_trend\\": DataFrame, \\"top_selling_product_per_store\\": DataFrame } ``` # Example Suppose the input DataFrame `transactions` is as follows: | TransactionID | ProductID | StoreID | Date | Quantity | Price | |---------------|-----------|---------|------------|----------|-------| | 1 | 101 | 1 | 2022-01-01 | 2 | 50.0 | | 2 | 102 | 1 | 2022-01-01 | 1 | 60.0 | | 3 | 101 | 2 | 2022-02-01 | 3 | 50.0 | | 4 | 103 | 2 | 2022-02-01 | 5 | 20.0 | Analyzing the columns, you can derive: - `total_sales_per_store`: Total revenue per store. - `average_quantity_per_product`: Average quantity of each product sold. - `monthly_sales_trend`: Aggregate sales over months. - `top_selling_product_per_store`: Most sold product per store. You should use pandas `GroupBy` methods to achieve these outputs. # Assistance - You can use the pandas library and its `groupby` functionality. - Examine and make use of aggregation methods like `sum()`, `mean()`, and `max()`. - Consider transformation functions to manipulate date columns for monthly trends. Good luck and happy coding!","solution":"import pandas as pd def analyze_sales(transactions): # Parse the \'Date\' column to datetime transactions[\'Date\'] = pd.to_datetime(transactions[\'Date\']) # Total sales per store transactions[\'Total\'] = transactions[\'Quantity\'] * transactions[\'Price\'] total_sales_per_store = transactions.groupby(\'StoreID\', as_index=False)[\'Total\'].sum().rename(columns={\'Total\': \'TotalSales\'}) # Average quantity sold per product average_quantity_per_product = transactions.groupby(\'ProductID\', as_index=False)[\'Quantity\'].mean().rename(columns={\'Quantity\': \'AvgQuantity\'}) # Monthly sales trend transactions[\'YearMonth\'] = transactions[\'Date\'].dt.to_period(\'M\') monthly_sales_trend = transactions.groupby(\'YearMonth\', as_index=False)[\'Total\'].sum().rename(columns={\'Total\': \'MonthlySales\'}) # Top-selling product per store top_selling_product_per_store = transactions.groupby([\'StoreID\', \'ProductID\'], as_index=False)[\'Quantity\'].sum() top_selling_product_per_store = top_selling_product_per_store.sort_values([\'StoreID\', \'Quantity\'], ascending=[True, False]) top_selling_product_per_store = top_selling_product_per_store.groupby(\'StoreID\').first().reset_index().rename(columns={\'Quantity\': \'TotalQuantitySold\'}) return { \\"total_sales_per_store\\": total_sales_per_store, \\"average_quantity_per_product\\": average_quantity_per_product, \\"monthly_sales_trend\\": monthly_sales_trend, \\"top_selling_product_per_store\\": top_selling_product_per_store }"},{"question":"# Asynchronous File Reader with Exception Handling You are required to implement a Python function that reads the contents of a file asynchronously and handles various exceptions that might be raised during the file reading process. The function should make use of the `asyncio` package and its associated custom exceptions. Function Signature ```python import asyncio async def read_file_async(file_path: str, buffer_limit: int) -> bytes: Asynchronously read the contents of a file specified by file_path. Parameters: file_path (str): the path of the file to be read. buffer_limit (int): the maximum buffer size for reading data. Returns: bytes: the contents of the file read. Raises: asyncio.TimeoutError: if the read operation exceeds a given deadline. asyncio.CancelledError: if the operation is cancelled. asyncio.InvalidStateError: for invalid internal state of Task or Future. asyncio.SendfileNotAvailableError: if sendfile syscall is not available for the file type. asyncio.IncompleteReadError: if the requested read operation did not complete fully. asyncio.LimitOverrunError: if the buffer size limit was reached while looking for a separator. pass ``` Requirements and Constraints 1. **File Reading**: You need to read the file asynchronously. 2. **Exception Handling**: - `asyncio.TimeoutError`: Handle scenarios where the read operation takes too long. - `asyncio.CancelledError`: Handle operations that are cancelled. - `asyncio.InvalidStateError`: Handle invalid states of the task or future. - `asyncio.SendfileNotAvailableError`: Handle cases where `sendfile` syscall is not available. - `asyncio.IncompleteReadError`: Handle incomplete read operations. - `asyncio.LimitOverrunError`: Handle buffer limit overruns. 3. **Buffer Limit**: Ensure that the buffer limit is not exceeded while reading the file. 4. **Performance**: While there is no strict performance requirement, your solution should handle exceptions gracefully and as efficiently as possible. Example: ```python import asyncio async def main(): content = await read_file_async(\\"example.txt\\", 1024) print(content) # Run the async main function asyncio.run(main()) ``` # Notes: - You are encouraged to use libraries and methods available within asyncio to achieve asynchronous file reading. - Make sure to document any assumptions or considerations taken into account while implementing the function. Evaluation Criteria - Correct usage of `asyncio` and asynchronous programming constructs. - Proper implementation of exception handling as per the specified custom exceptions. - Efficient handling of buffers and file reading operations.","solution":"import asyncio import os async def read_file_async(file_path: str, buffer_limit: int) -> bytes: Asynchronously read the contents of a file specified by file_path. Parameters: file_path (str): the path of the file to be read. buffer_limit (int): the maximum buffer size for reading data. Returns: bytes: the contents of the file read. Raises: asyncio.TimeoutError: if the read operation exceeds a given deadline. asyncio.CancelledError: if the operation is cancelled. asyncio.InvalidStateError: for invalid internal state of Task or Future. asyncio.SendfileNotAvailableError: if sendfile syscall is not available for the file type. asyncio.IncompleteReadError: if the requested read operation did not complete fully. asyncio.LimitOverrunError: if the buffer size limit was reached while looking for a separator. loop = asyncio.get_event_loop() bytes_content = bytearray() async def read_chunk(file, size): return await loop.run_in_executor(None, file.read, size) try: async with aiofiles.open(file_path, \'rb\') as file: while True: chunk = await asyncio.wait_for(read_chunk(file, buffer_limit), timeout=10) if not chunk: break bytes_content.extend(chunk) if len(bytes_content) > buffer_limit: raise asyncio.LimitOverrunError(\\"Buffer limit exceeded\\", buffer_limit) return bytes(bytes_content) except asyncio.TimeoutError: print(\\"The read operation timed out.\\") raise except asyncio.CancelledError: print(\\"The read operation was cancelled.\\") raise except asyncio.InvalidStateError: print(\\"Invalid state during file read operation.\\") raise except asyncio.SendfileNotAvailableError: print(\\"Sendfile syscall is not available.\\") raise except asyncio.IncompleteReadError: print(\\"Incomplete read operation.\\") raise except asyncio.LimitOverrunError: print(\\"Buffer limit was reached during file read operation.\\") raise except Exception as e: print(f\\"An unexpected error occurred: {e}\\") raise # Dependencies required for the above implementation: # pip install aiofiles"},{"question":"# Python Function Implementation Challenge Objective Your task is to implement a custom string formatting function in Python that mimics the behavior of the C library functions `snprintf` and `vsnprintf`, ensuring consistent and safe string formatting based on a given format string and variable arguments. Additionally, implement case-insensitive string comparison functions similar to `stricmp` and `strnicmp`. Task 1. **Implement the function `py_snprintf`**: - **Input:** - `format`: A format string. - `*args`: Variable arguments to be formatted into the string. - `size`: Maximum size of the output string buffer (including the null-terminator). - **Output:** - A tuple `(rv, result_str)`: - `rv`: number of characters that would have been written to `result_str` (excluding the trailing null-terminator). - `result_str`: The formatted output string truncated to `size - 1` characters if necessary, with proper handling for null-termination. - **Constraints:** - `result_str` must always have a length of `size - 1` characters or fewer. - Ensure that the final character in `result_str` is the null-terminator if truncation occurs. - Handle edge cases such as invalid format strings and buffer sizes. 2. **Implement the case-insensitive comparison functions**: - `py_stricmp(s1: str, s2: str) -> int`: - **Input:** Two strings `s1` and `s2`. - **Output:** An integer indicating the result of the comparison. - `0` if `s1` and `s2` are equivalent ignoring case. - A negative integer if `s1` < `s2` ignoring case. - A positive integer if `s1` > `s2` ignoring case. - `py_strnicmp(s1: str, s2: str, n: int) -> int`: - **Input:** Two strings `s1` and `s2`, and an integer `n`. - **Output:** An integer indicating the result of the comparison up to `n` characters. - The same rules apply as `py_stricmp` but only for the first `n` characters. Example ```python def py_snprintf(format: str, size: int, *args) -> str: # Your implementation here def py_stricmp(s1: str, s2: str) -> int: # Your implementation here def py_strnicmp(s1: str, s2: str, n: int) -> int: # Your implementation here # Example usage: # py_snprintf rv, result_str = py_snprintf(\\"Hello %s!\\", 8, \\"World\\") print(rv, result_str) # Outputs: (12, \'Hello Wo\') # py_stricmp result = py_stricmp(\\"hello\\", \\"HELLO\\") print(result) # Outputs: 0 # py_strnicmp result = py_strnicmp(\\"hello\\", \\"heLLoWorld\\", 5) print(result) # Outputs: 0 ``` Notes 1. Pay attention to edge cases such as empty strings, very large sizes, and invalid inputs. 2. Ensure your solution handles memory and performance efficiently, particularly when managing the string buffers and comparisons.","solution":"def py_snprintf(format: str, size: int, *args) -> (int, str): Mimics the behavior of the C library function snprintf. Returns the formatted string up to the specified size and the number of characters that would have been written. formatted_str = format % args rv = len(formatted_str) if size > 0: truncated_str = formatted_str[:size - 1] return rv, truncated_str return rv, \\"\\" def py_stricmp(s1: str, s2: str) -> int: Case-insensitive string comparison. Returns 0 if s1 and s2 are equivalent ignoring case. A negative integer if s1 < s2 ignoring case. A positive integer if s1 > s2 ignoring case. s1_lower = s1.lower() s2_lower = s2.lower() if s1_lower == s2_lower: return 0 elif s1_lower < s2_lower: return -1 else: return 1 def py_strnicmp(s1: str, s2: str, n: int) -> int: Case-insensitive string comparison up to n characters. Returns 0 if s1 and s2 are equivalent for the first n characters ignoring case. A negative integer if s1 < s2 for the first n characters ignoring case. A positive integer if s1 > s2 for the first n characters ignoring case. s1_lower = s1[:n].lower() s2_lower = s2[:n].lower() if s1_lower == s2_lower: return 0 elif s1_lower < s2_lower: return -1 else: return 1"},{"question":"You are provided with a dataset containing labeled and unlabeled data points. Your task is to implement a semi-supervised learning model using the `scikit-learn` library to classify the unlabeled data points and evaluate the model\'s performance on a test set. Dataset The dataset is split into three parts: - `X_train`: The features of the training data (some labeled, some unlabeled). - `y_train`: The labels of the training data (`-1` denotes unlabeled samples). - `X_test`: The features of the test data (all labeled). - `y_test`: The labels of the test data. Objectives 1. Implement a semi-supervised model using both `SelfTrainingClassifier` and `LabelPropagation` from the `scikit-learn` library. 2. Train your models on `X_train` and `y_train` with appropriate parameters. 3. Predict the labels of the test set `X_test`. 4. Evaluate the accuracy of both models using the test labels `y_test`. Input Format - `X_train` (numpy array of shape `(n_samples, n_features)`): features for training data. - `y_train` (numpy array of shape `(n_samples,)`): labels for training data, where `-1` represents unlabeled data. - `X_test` (numpy array of shape `(n_samples, n_features)`): features for testing data. - `y_test` (numpy array of shape `(n_samples,)`): labels for testing data. Output Format - `self_training_accuracy` (float): accuracy of the `SelfTrainingClassifier` on the test set. - `label_propagation_accuracy` (float): accuracy of the `LabelPropagation` model on the test set. Constraints - You must use default parameters for both models unless specified otherwise. - Assume the dataset is reasonably sized to fit into memory. Performance Requirements The solution\'s accuracy should be as high as possible given the semi-supervised learning constraints. Example ```python import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Example data (not actual content) X_train = np.array([[1, 2], [3, 4], [5, 6], [7, 8]]) y_train = np.array([0, -1, 1, -1]) X_test = np.array([[9, 10], [11, 12]]) y_test = np.array([1, 0]) def semi_supervised_learning(X_train, y_train, X_test, y_test): # SelfTrainingClassifier base_classifier = RandomForestClassifier() self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X_train, y_train) y_pred_self_training = self_training_model.predict(X_test) self_training_accuracy = accuracy_score(y_test, y_pred_self_training) # LabelPropagation label_propagation_model = LabelPropagation() label_propagation_model.fit(X_train, y_train) y_pred_label_propagation = label_propagation_model.predict(X_test) label_propagation_accuracy = accuracy_score(y_test, y_pred_label_propagation) return self_training_accuracy, label_propagation_accuracy # Run the function to get model accuracies self_training_accuracy, label_propagation_accuracy = semi_supervised_learning(X_train, y_train, X_test, y_test) print(f\\"SelfTrainingClassifier Accuracy: {self_training_accuracy:.2f}\\") print(f\\"LabelPropagation Accuracy: {label_propagation_accuracy:.2f}\\") ```","solution":"import numpy as np from sklearn.semi_supervised import SelfTrainingClassifier, LabelPropagation from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def semi_supervised_learning(X_train, y_train, X_test, y_test): # SelfTrainingClassifier base_classifier = RandomForestClassifier() self_training_model = SelfTrainingClassifier(base_classifier) self_training_model.fit(X_train, y_train) y_pred_self_training = self_training_model.predict(X_test) self_training_accuracy = accuracy_score(y_test, y_pred_self_training) # LabelPropagation label_propagation_model = LabelPropagation() label_propagation_model.fit(X_train, y_train) y_pred_label_propagation = label_propagation_model.predict(X_test) label_propagation_accuracy = accuracy_score(y_test, y_pred_label_propagation) return self_training_accuracy, label_propagation_accuracy"},{"question":"# **HTML Content Manipulation Using `html` Module** Objective You are required to write a function that processes an HTML string using the `html` module in Python. Specifically, you will manipulate the HTML content by escaping certain characters and then converting them back to their original form. Function Signature ```python def manipulate_html_content(html_string: str, escape_quote: bool) -> str: ``` Input - `html_string` (str): A string containing HTML content which may include HTML special characters and character references. - `escape_quote` (bool): A boolean indicating whether or not to escape the quote characters (`\\"` and `\'`) in the HTML content. Output - (str): The HTML string after escaping certain characters and then converting them back to their original form. Constraints - The `html_string` length can be up to 10^5 characters. - The `html_string` will only contain valid HTML characters and entities. Instructions 1. Use the `html.escape` function to convert all occurrences of `&`, `<`, and `>` in the `html_string` to their corresponding HTML-safe sequences. If `escape_quote` is `True`, also convert `\\"` and `\'`. 2. Immediately after escaping, use the `html.unescape` function to convert all named and numeric character references in the resultant string back to their corresponding Unicode characters. Example ```python html_string = \\"Hello & welcome to <Code>\'s World!</Code>\\" escape_quote = True # After escaping: # \\"Hello &amp; welcome to &lt;Code&gt;&#x27;s World!&lt;/Code&gt;\\" # After unescaping: # \\"Hello & welcome to <Code>\'s World!</Code>\\" result = manipulate_html_content(html_string, escape_quote) print(result) # Output: \\"Hello & welcome to <Code>\'s World!</Code>\\" ``` Notes - The purpose of this task is to verify that you correctly understand how to use the `html` module\'s `escape` and `unescape` functions in combination. - Pay attention to edge cases where the HTML string might already contain character references that shouldn\'t double escape. Good luck and happy coding!","solution":"import html def manipulate_html_content(html_string: str, escape_quote: bool) -> str: Escapes certain characters in the HTML string and then converts them back to their original form. Parameters: html_string (str): The original HTML string to be manipulated. escape_quote (bool): If True, escape quote characters (\'\\", \') as well. Returns: str: The manipulated HTML string after escaping and unescaping. # Step 1: Escape certain characters in the HTML string escaped_string = html.escape(html_string, quote=escape_quote) # Step 2: Unescape the string back to its original characters unescaped_string = html.unescape(escaped_string) return unescaped_string"},{"question":"# Question: You are required to implement a custom PyTorch autograd `Function` that computes the element-wise square of the input tensor during the forward pass. In the backward pass, you must manually compute the gradient for the tensor, as well as save an intermediary result during the forward pass to demonstrate proficiency in autograd mechanics. The function should satisfy the following requirements: 1. Implement a custom class `SquareFunction` that inherits from `torch.autograd.Function`. 2. The class should define `forward` and `backward` static methods. 3. Use `save_for_backward` in the forward pass to save the original input tensor. 4. In the backward pass, retrieve the saved tensor and compute the gradient. 5. Implement a context manager within the forward pass using `torch.no_grad()` to demonstrate gradient tracking exclusion. Additionally, use a hook to save the tensor locally when gradients are computed during the backward pass. Expected Function Definitions: ```python import torch class SquareFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Save the original input tensor for backward computation ctx.save_for_backward(input) with torch.no_grad(): # Context manager to exclude gradient tracking result = input.pow(2) return result @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute the gradient manually grad_input = 2 * input * grad_output return grad_input # Example usage: x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True) square = SquareFunction.apply y = square(x) y.sum().backward() print(x.grad) # Should print tensor([2., 4., 6., 8.]) ``` # Requirements: 1. Define the `SquareFunction` class as specified. 2. Correctly use `save_for_backward` and context managers. 3. Register a backward hook for the input tensor that prints \\"Gradient computed!\\" when gradients are computed. 4. Create a test case to demonstrate the functionality works as expected. # Example Input: ```python x = torch.tensor([5.0, 10.0, 15.0, 20.0], requires_grad=True) ``` # Example Output: ```plaintext Gradient computed! tensor([10., 20., 30., 40.]) ``` # Constraints: 1. Only use PyTorch in-built functions and methods. 2. Ensure that the autograd functionality and hooks are correctly implemented as per PyTorch\'s best practices. Good luck!","solution":"import torch class SquareFunction(torch.autograd.Function): @staticmethod def forward(ctx, input): # Save the original input tensor for backward computation ctx.save_for_backward(input) with torch.no_grad(): # Context manager to exclude gradient tracking result = input.pow(2) return result @staticmethod def backward(ctx, grad_output): # Retrieve the saved tensor input, = ctx.saved_tensors # Compute the gradient manually grad_input = 2 * input * grad_output print(\\"Gradient computed!\\") return grad_input # Example usage function to apply SquareFunction and do backward computation def example_usage(x): square = SquareFunction.apply y = square(x) y.sum().backward() return x.grad"},{"question":"Objective Your task is to implement a function that schedules and manages a set of tasks based on their priority. You will use the functionality provided by Python\'s `heapq` module. Problem Statement You are given a list of tasks, where each task is represented as a tuple `(priority, task_name)`. Your goal is to maintain a priority queue using a heap where tasks with the lowest priority number are processed first. Implement a class `TaskScheduler` which supports the following operations: 1. **`__init__(self)`**: Initializes an empty priority queue. 2. **`add_task(self, priority: int, task_name: str)`**: Adds a new task to the priority queue. 3. **`remove_task(self, task_name: str)`**: Marks an existing task as removed. If the task is not found, it raises a `KeyError`. 4. **`pop_task(self) -> str`**: Removes and returns the task with the lowest priority. If the queue is empty, it raises a `KeyError`. 5. **`change_task_priority(self, task_name: str, new_priority: int)`**: Changes the priority of an existing task. If the task is not found, it raises a `KeyError`. 6. **`get_tasks(self) -> List[str]`**: Returns a sorted list of task names based on priority. Constraints - Each `task_name` is unique. - Priority is represented as an integer where lower integers have higher priority. - You can assume all inputs are valid and the operations are valid except for the specified error conditions. Example ```python scheduler = TaskScheduler() scheduler.add_task(3, \'task_1\') scheduler.add_task(2, \'task_2\') scheduler.add_task(1, \'task_3\') print(scheduler.pop_task()) # Output: \'task_3\' scheduler.add_task(4, \'task_4\') scheduler.change_task_priority(\'task_4\', 0) print(scheduler.pop_task()) # Output: \'task_4\' print(scheduler.get_tasks()) # Output: [\'task_2\', \'task_1\'] ``` Implementation Details - Use `heapq` for managing the priority queue. - Utilize a dictionary or other data structures as needed to handle task removal and priority updates. - The `pop_task` method should handle tasks marked for removal appropriately.","solution":"import heapq class TaskScheduler: def __init__(self): self.pq = [] # priority queue implemented as a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count def add_task(self, priority: int, task_name: str): Add a new task or update the priority of an existing task if task_name in self.entry_finder: self.remove_task(task_name) count = self.counter entry = [priority, count, task_name] self.entry_finder[task_name] = entry heapq.heappush(self.pq, entry) self.counter += 1 def remove_task(self, task_name: str): Mark an existing task as REMOVED. Raise KeyError if not found. if task_name not in self.entry_finder: raise KeyError(f\\"Task {task_name} not found\\") entry = self.entry_finder.pop(task_name) entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the task with the lowest priority. Raise KeyError if empty. while self.pq: priority, count, task_name = heapq.heappop(self.pq) if task_name is not self.REMOVED: del self.entry_finder[task_name] return task_name raise KeyError(\'pop from an empty priority queue\') def change_task_priority(self, task_name: str, new_priority: int): Change the priority of an existing task. Raise KeyError if not found. if task_name not in self.entry_finder: raise KeyError(f\\"Task {task_name} not found\\") self.add_task(new_priority, task_name) def get_tasks(self): Return a sorted list of task names based on priority. valid_tasks = [entry for entry in self.pq if entry[-1] is not self.REMOVED] valid_tasks.sort() return [task_name for priority, count, task_name in valid_tasks]"},{"question":"Coding Assessment Question **Objective:** To assess the student\'s understanding of working with CGI scripts in Python using the `cgi` module, along with their ability to process form data and handle file uploads. **Problem Statement:** Design a CGI script `process_form.py` that processes a form submitted via an HTML `<form>` element. The form contains the following fields: - `username` (text input) - `password` (password input) - `comments` (textarea) - `profile_picture` (file upload) # Requirements: 1. **Form Handling:** - The script should capture and print the submitted `username`, `password`, and `comments`. - If any of these fields are missing or empty, the script should display an appropriate error message. 2. **File Handling:** - If a `profile_picture` is uploaded, save it to the server\'s local directory \\"/tmp/uploads/\\". - If no file is uploaded, the script should display a message indicating that no file has been uploaded. 3. **Output:** - The CGI script must output a valid HTML response that includes all the captured form data and any error messages. 4. **Debugging:** - Enable detailed error reporting using the `cgitb` module. # Constraints: - Assume that the CGI script is properly installed and executable on a Unix-based HTTP server. - Ensure that the script handles any potential exceptions, such as missing fields or errors during file upload. - Make sure the saved profile pictures are given a unique filename to avoid overwriting existing files. # Sample Usage: - HTML Form code to test the script: ```html <form enctype=\\"multipart/form-data\\" action=\\"/cgi-bin/process_form.py\\" method=\\"post\\"> Username: <input type=\\"text\\" name=\\"username\\"><br> Password: <input type=\\"password\\" name=\\"password\\"><br> Comments: <textarea name=\\"comments\\"></textarea><br> Profile Picture: <input type=\\"file\\" name=\\"profile_picture\\"><br> <input type=\\"submit\\" value=\\"Submit\\"> </form> ``` # Expected Input and Output: Sample Input: - `username`: `john_doe` - `password`: `securepass123` - `comments`: `This is a test comment.` - `profile_picture`: A file named `profile.jpg` Sample Output: ```html <!DOCTYPE html> <html> <head> <title>Form Submission Result</title> </head> <body> <h1>Form Submission Result</h1> <p>Username: john_doe</p> <p>Password: securepass123</p> <p>Comments: This is a test comment.</p> <p>Profile Picture: File uploaded successfully as profile_picture_<timestamp>.jpg</p> </body> </html> ``` Sample Output with Errors: ```html <!DOCTYPE html> <html> <head> <title>Form Submission Result</title> </head> <body> <h1>Error</h1> <p>Please fill in the username, password, and comments fields.</p> </body> </html> ``` # Implementation: Write the `process_form.py` Python CGI script to meet the above requirements. Make sure to include the usage of the `cgi` and `cgitb` modules. ```python #!/usr/bin/env python3 import cgi import cgitb import os import datetime cgitb.enable() # Enable detailed error reporting def save_uploaded_file(fileitem): if fileitem.filename: timestamp = datetime.datetime.now().strftime(\\"%Y%m%d%H%M%S\\") filename = os.path.basename(fileitem.filename) file_path = f\\"/tmp/uploads/profile_picture_{timestamp}_{filename}\\" with open(file_path, \'wb\') as f: f.write(fileitem.file.read()) return f\\"File uploaded successfully as {os.path.basename(file_path)}\\" else: return \\"No file uploaded.\\" def main(): form = cgi.FieldStorage() # Form fields username = form.getfirst(\\"username\\", \\"\\").strip() password = form.getfirst(\\"password\\", \\"\\").strip() comments = form.getfirst(\\"comments\\", \\"\\").strip() profile_picture = form[\\"profile_picture\\"] if \\"profile_picture\\" in form else None # HTML header print(\\"Content-Type: text/html\\") print() # HTML content if not username or not password or not comments: print(\\"<h1>Error</h1>\\") print(\\"<p>Please fill in the username, password, and comments fields.</p>\\") else: print(\\"<h1>Form Submission Result</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Password: {password}</p>\\") print(f\\"<p>Comments: {comments}</p>\\") if profile_picture: message = save_uploaded_file(profile_picture) print(f\\"<p>Profile Picture: {message}</p>\\") if __name__ == \\"__main__\\": main() ``` # Instructions for execution: 1. Save the script with the filename `process_form.py`. 2. Ensure that the script has executable permissions: `chmod 755 process_form.py`. 3. Place the script in the server\'s `cgi-bin` directory. 4. Create the directory `/tmp/uploads/` with proper permissions to allow file writing. 5. Use the provided HTML form code to submit data to the CGI script.","solution":"#!/usr/bin/env python3 import cgi import cgitb import os import datetime cgitb.enable() # Enable detailed error reporting def save_uploaded_file(fileitem): if fileitem.filename: timestamp = datetime.datetime.now().strftime(\\"%Y%m%d%H%M%S\\") filename = os.path.basename(fileitem.filename) file_path = f\\"/tmp/uploads/profile_picture_{timestamp}_{filename}\\" os.makedirs(os.path.dirname(file_path), exist_ok=True) with open(file_path, \'wb\') as f: f.write(fileitem.file.read()) return f\\"File uploaded successfully as {os.path.basename(file_path)}\\" else: return \\"No file uploaded.\\" def main(): form = cgi.FieldStorage() # Form fields username = form.getfirst(\\"username\\", \\"\\").strip() password = form.getfirst(\\"password\\", \\"\\").strip() comments = form.getfirst(\\"comments\\", \\"\\").strip() profile_picture = form[\\"profile_picture\\"] if \\"profile_picture\\" in form else None # HTML header print(\\"Content-Type: text/html\\") print() # HTML content print(\\"<html><head><title>Form Submission Result</title></head><body>\\") if not username or not password or not comments: print(\\"<h1>Error</h1>\\") print(\\"<p>Please fill in the username, password, and comments fields.</p>\\") else: print(\\"<h1>Form Submission Result</h1>\\") print(f\\"<p>Username: {username}</p>\\") print(f\\"<p>Password: {password}</p>\\") print(f\\"<p>Comments: {comments}</p>\\") if profile_picture: message = save_uploaded_file(profile_picture) print(f\\"<p>Profile Picture: {message}</p>\\") print(\\"</body></html>\\") if __name__ == \\"__main__\\": main()"},{"question":"Objective To assess the students\' comprehension of set operations in Python 3.10, kindly implement the following function: Problem Statement Implement a function `filter_and_modify_set(input_set: set, elements_to_add: list, elements_to_remove: list) -> set` that performs the following operations: 1. Filters the given `input_set` to only include elements that are even numbers. 2. Adds all elements from `elements_to_add` to the filtered set. 3. Removes all elements from `elements_to_remove` from the set. 4. Returns the resulting set. Function Signature ```python def filter_and_modify_set(input_set: set, elements_to_add: list, elements_to_remove: list) -> set: pass ``` Input Format - `input_set`: A set of integers. - `elements_to_add`: A list of integers to be added to the filtered set. - `elements_to_remove`: A list of integers to be removed from the set. Output Format - A set of integers after performing all the specified operations. Constraints - The input set will have at most 1000 elements. - The elements to add and remove will have at most 100 elements each. Example ```python input_set = {1, 2, 3, 4, 5, 6} elements_to_add = [7, 8, 9] elements_to_remove = [4, 2, 10] print(filter_and_modify_set(input_set, elements_to_add, elements_to_remove)) # Expected Output: {6, 5, 9, 7, 8} ``` Notes - The function should correctly handle cases where elements to add or remove do not exist in the set. - The function should maintain appropriate error handling to manage invalid inputs. Good luck, and ensure to demonstrate a thorough understanding of set operations and edge cases!","solution":"def filter_and_modify_set(input_set: set, elements_to_add: list, elements_to_remove: list) -> set: Filters the input_set to only include even numbers, adds elements from elements_to_add, removes elements from elements_to_remove, and returns the resulting set. # Filter the input set to include only even numbers filtered_set = {element for element in input_set if element % 2 == 0} # Add elements from elements_to_add filtered_set.update(elements_to_add) # Remove elements from elements_to_remove filtered_set.difference_update(elements_to_remove) return filtered_set"},{"question":"# Question: Creating a Multi-Layered Plot with Seaborn Objects You are provided with a dataset `planets` that contains information about different exoplanets discovered. Your task is to use `seaborn.objects` to create a multi-layered plot that visualizes various aspects of this dataset. # Dataset Description The `planets` dataset has the following columns: - `method` (string): The discovery method of the exoplanet. - `number` (integer): The number of discovered planets. - `orbital_period` (float): The orbital period of the exoplanet. - `mass` (float): The mass of the exoplanet. - `distance` (float): The distance to the exoplanet. - `year` (integer): The year of discovery. # Requirements 1. **Load the Dataset**: Load the `planets` dataset using `seaborn.load_dataset`. 2. **Create a Plot**: - Base Plot: Plot the `orbital_period` vs. `mass` of the exoplanets. - Layer 1: Add a scatter plot with `method` as the color dimension. - Layer 2: Add a linear regression line (use `so.Line` with `so.PolyFit`) to show the trend between `orbital_period` and `mass`. - Layer 3: Facet the plot by `year`, showing different subplots for different years. - Layer 4: Add another layer to display the histogram of the `distance` attribute, showing the distribution of distances for the exoplanets. 3. **Customization**: - Title: Add an appropriate title to the plot. - Labels: Label the x-axis as \\"Orbital Period (days)\\" and y-axis as \\"Mass (Jupiter Masses)\\". - Legend: Annotate each layer in the plot\'s legend appropriately. # Input/Output - **Input**: No input required from the user. Use the dataset provided by `seaborn`. - **Output**: One comprehensive plot with the above specifications. # Constraints - You must use `seaborn.objects` to create this plot. - Ensure the plot is well-labeled and visually appealing. # Example Code ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset planets = load_dataset(\\"planets\\") # Create the base plot p = so.Plot(planets, x=\\"orbital_period\\", y=\\"mass\\") # Add the first layer (scatter plot) p.add(so.Dot(), color=\\"method\\") # Add the second layer (regression line) p.add(so.Line(), so.PolyFit()) # Facet by \'year\' p.facet(col=\\"year\\") # Add the histogram layer for \'distance\' p.add(so.Bar(), so.Hist(), y=\\"distance\\") # Customize the plot p.label(title=\\"Exoplanet Discoveries\\", x=\\"Orbital Period (days)\\", y=\\"Mass (Jupiter Masses)\\") # Show the plot p.show() ``` Note: Customize the legend to annotate each layer appropriately based on your specific implementation.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_multilayered_plot(): # Load the dataset planets = load_dataset(\\"planets\\") # Create the base plot p = so.Plot(planets, x=\\"orbital_period\\", y=\\"mass\\") # Add the first layer (scatter plot) p.add(so.Dot(), color=\\"method\\") # Add the second layer (regression line) p.add(so.Line(), so.PolyFit()) # Facet by \'year\' p.facet(col=\\"year\\") # Add the histogram layer for \'distance\' p.add(so.Bar(), so.Hist(), y=\\"distance\\") # Customize the plot p.label(title=\\"Exoplanet Discoveries\\", x=\\"Orbital Period (days)\\", y=\\"Mass (Jupiter Masses)\\") # Show the plot p.show()"},{"question":"**Problem Statement:** You are required to implement a multi-threaded system that simulates a simple restaurant order processing system. The system has multiple chefs (producer threads) and multiple waiters (consumer threads). Orders must be processed in the order they are received. If an order queue is full, chefs should wait until a spot becomes available. # Requirements: 1. **Queue Creation:** - Create a FIFO queue (`queue.Queue`) with a maximum size of 10 to hold the orders. 2. **Order Processing:** - Implement a `Chef` class that represents a chef who produces orders and adds them to the queue. - Implement a `Waiter` class that represents a waiter who consumes orders from the queue and processes them. 3. **Task Flow:** - Chefs should add orders to the queue by calling the `put()` method. They should block if the queue is full until a spot becomes available. - Waiters should remove orders from the queue by calling the `get()` method. They should block if the queue is empty until an order is available. - Each order will be represented by a simple integer ID. 4. **Task Completion Tracking:** - Implement a mechanism to track the completion of task processing using `task_done()` and `join()` methods. 5. **Synchronization and Exception Handling:** - Ensure that the system handles scenarios where the queue could be full (chefs should wait) or empty (waiters should wait). - Handle any exceptions, such as `queue.Full` or `queue.Empty`, appropriately. # Input and Output: Input There is no explicit input. You will simulate the process with a predefined number of orders, chefs, and waiters. Output Your implementation should print each order being processed by a waiter. # Implementation Details: - Initialize the queue. - Create and start multiple chef and waiter threads. - Simulate the Chef and Waiter operations while handling synchronization properly. - Use the `join()` method to ensure all tasks are completed before the program exits. # Example: ```python import threading import queue import time class Chef(threading.Thread): def __init__(self, queue, num_orders): threading.Thread.__init__(self) self.queue = queue self.num_orders = num_orders def run(self): for i in range(self.num_orders): order_id = i + 1 self.queue.put(order_id) print(f\'Chef produced order {order_id}\') time.sleep(1) # Simulate time taken to produce an order self.queue.join() # Ensure all tasks are processed class Waiter(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.queue = queue def run(self): while True: order_id = self.queue.get() if order_id is None: break print(f\'Waiter is processing order {order_id}\') time.sleep(2) # Simulate time taken to process an order self.queue.task_done() if __name__ == \'__main__\': order_queue = queue.Queue(maxsize=10) num_chefs = 3 num_waiters = 2 num_orders = 30 chefs = [Chef(order_queue, num_orders // num_chefs) for _ in range(num_chefs)] waiters = [Waiter(order_queue) for _ in range(num_waiters)] for chef in chefs: chef.start() for waiter in waiters: waiter.start() for chef in chefs: chef.join() # putting None in the queue to stop waiters after all orders are processed for _ in waiters: order_queue.put(None) for waiter in waiters: waiter.join() ``` # Constraints: - Number of chefs (producer threads): 3 - Number of waiters (consumer threads): 2 - Total number of orders: 30 - Max queue size: 10 # Notes: - Ensure thread safety and proper synchronization throughout the implementation. - The `print()` output should reflect the production and processing of orders in real-time. - Chefs and waiters should be started almost simultaneously for better simulation.","solution":"import threading import queue import time class Chef(threading.Thread): def __init__(self, queue, num_orders): threading.Thread.__init__(self) self.queue = queue self.num_orders = num_orders def run(self): for i in range(self.num_orders): order_id = i + 1 self.queue.put(order_id) print(f\'Chef produced order {order_id}\') time.sleep(1) # Simulate time taken to produce an order self.queue.join() # Ensure all tasks are processed class Waiter(threading.Thread): def __init__(self, queue): threading.Thread.__init__(self) self.queue = queue def run(self): while True: order_id = self.queue.get() if order_id is None: self.queue.task_done() break print(f\'Waiter is processing order {order_id}\') time.sleep(2) # Simulate time taken to process an order self.queue.task_done() # Mark that a formerly enqueued task is complete"},{"question":"**Question: Advanced Visualization with Seaborn** You are given two datasets: `penguins` and `diamonds`. Your task is to create layered visualizations using the `seaborn.objects` module. Follow the instructions below to demonstrate your understanding of loading datasets, setting up plots, and adding multiple layers using jittered dots and range markers. # Instructions: 1. Load the `penguins` dataset using `seaborn.load_dataset`. 2. Create a plot for the `penguins` dataset with the following specifications: - The x-axis should represent the species of penguins. - The y-axis should represent their body mass in grams. - Add a layer of jittered dots to help spread out overlapping points. - Add a layer to show the range of body mass values between the 25th and 75th percentiles. Shift this range slightly to prevent overlap with the dots. 3. Load the `diamonds` dataset using `seaborn.load_dataset`. 4. Create a plot for the `diamonds` dataset with the following specifications: - The x-axis should represent the `carat` weight of the diamonds. - The y-axis should represent the `clarity` of the diamonds on a nominal scale. - Add a layer of jittered dots to help spread out overlapping points. - Add a layer to show the range of carat weights between the 25th and 75th percentiles. Shift this range slightly downward to prevent overlap with the dots. # Expected Implementation: Your solution should include the following functions and generate the required plots: ```python import seaborn.objects as so from seaborn import load_dataset def plot_penguins(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot with jittered dots and range markers plot = ( so.Plot(penguins, \\"species\\", \\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) return plot def plot_diamonds(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Create the plot with jittered dots and range markers plot = ( so.Plot(diamonds, \\"carat\\", \\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) return plot # Generate and display the plots plot_penguins().show() plot_diamonds().show() ``` # Constraints: - Ensure the plots are clear and informative. - Use the correct functions and methods from `seaborn.objects`. - All plots should handle overlapping points effectively with jittering and range adjustments. Your solution will be evaluated based on the correctness, clarity, and quality of the visualizations you produce.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_penguins(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Ensure the dataset is not empty if penguins is None or penguins.empty: raise ValueError(\\"Penguins dataset is empty or could not be loaded\\") # Create the plot with jittered dots and range markers plot = ( so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=.2)) ) return plot def plot_diamonds(): # Load the diamonds dataset diamonds = load_dataset(\\"diamonds\\") # Ensure the dataset is not empty if diamonds is None or diamonds.empty: raise ValueError(\\"Diamonds dataset is empty or could not be loaded\\") # Create the plot with jittered dots and range markers plot = ( so.Plot(diamonds, x=\\"carat\\", y=\\"clarity\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(y=.25)) ) return plot # Generate and display the plots (this code would need to be called in an actual environment to show the plots) if __name__ == \\"__main__\\": plot_penguins().show() plot_diamonds().show()"},{"question":"# Question: Enhanced Inventory Management with Data Classes You are tasked with implementing a class system for managing an inventory of products in a warehouse. Each product is represented by a `Product` data class that tracks its name, unit price, and quantity on hand. Additionally, you will create a `Warehouse` data class that keeps a collection of products and provides methods to add products, remove products, calculate the total inventory value, and track inventory changes over time. Your implementation should demonstrate the following capabilities of the `dataclasses` module: 1. Create a basic data class with fields and default values. 2. Utilize `field()` with `default_factory` for mutable types. 3. Implement post-initialization processing with `__post_init__()`. 4. Work with metadata in fields. 5. Handle keyword-only fields and frozen instances. 6. Using utility functions like `asdict()`, `astuple()`, and `replace()`. Instructions: 1. **Define `Product` Class:** - Use the `@dataclass` decorator to define a class `Product`. - Fields: `name` (str), `unit_price` (float), `quantity_on_hand` (int, default 0). - Implement a method `total_cost(self) -> float` that returns the total cost of the product (unit price * quantity on hand). - Add metadata to the `name` field indicating \\"primary_key\\": True. 2. **Define `Warehouse` Class:** - Use the `@dataclass` decorator to define a class `Warehouse`. - Fields: `products` (list of `Product` instances, default factory for an empty list). - Implement methods: - `add_product(self, product: Product) -> None`: Add a product to the warehouse inventory. - `remove_product(self, product_name: str) -> bool`: Remove a product by name from the inventory. Return True if product was found and removed, False otherwise. - `total_inventory_value(self) -> float`: Calculate the total value of all products in the inventory. - `track_inventory_changes(self) -> dict`: Return a dictionary representation of the inventory using `asdict()`. - Ensure that `Warehouse` instances are immutable after creation. 3. **Post-Initialization Processing:** - In `Warehouse`, add a post-initialization process that ensures there are no duplicate products (by name) in the inventory. If duplicates are found, raise a `ValueError`. Constraints: - The product names are unique within the warehouse. - The `unit_price` is always a non-negative number. - The `quantity_on_hand` is always a non-negative integer. - Use the `dataclasses` module features wherever relevant. Example: ```python from dataclasses import dataclass, field, asdict from typing import List # Define Product class @dataclass class Product: name: str = field(metadata={\\"primary_key\\": True}) unit_price: float quantity_on_hand: int = 0 def total_cost(self) -> float: return self.unit_price * self.quantity_on_hand # Define Warehouse class @dataclass(frozen=True) class Warehouse: products: List[Product] = field(default_factory=list) def __post_init__(self): product_names = [product.name for product in self.products] if len(product_names) != len(set(product_names)): raise ValueError(\\"Duplicate products found in warehouse inventory!\\") def add_product(self, product: Product) -> None: if any(p.name == product.name for p in self.products): raise ValueError(\\"Product with the same name already exists!\\") serialized_products = asdict(self)[\'products\'] serialized_products.append(asdict(product)) replaced_warehouse = replace(self, products=[Product(**p) for p in serialized_products]) self.__dict__.update(asdict(replaced_warehouse)) def remove_product(self, product_name: str) -> bool: serialized_products = asdict(self)[\'products\'] new_products = [p for p in serialized_products if p[\'name\'] != product_name] if len(new_products) == len(serialized_products): return False replaced_warehouse = replace(self, products=[Product(**p) for p in new_products]) self.__dict__.update(asdict(replaced_warehouse)) return True def total_inventory_value(self) -> float: return sum(product.total_cost() for product in self.products) def track_inventory_changes(self) -> dict: return asdict(self) ``` Implement these classes and functions according to the above specifications.","solution":"from dataclasses import dataclass, field, asdict, replace from typing import List @dataclass class Product: name: str = field(metadata={\\"primary_key\\": True}) unit_price: float quantity_on_hand: int = 0 def total_cost(self) -> float: return self.unit_price * self.quantity_on_hand @dataclass(frozen=True) class Warehouse: products: List[Product] = field(default_factory=list) def __post_init__(self): product_names = [product.name for product in self.products] if len(product_names) != len(set(product_names)): raise ValueError(\\"Duplicate products found in warehouse inventory!\\") def add_product(self, product: Product) -> None: if any(p.name == product.name for p in self.products): raise ValueError(\\"Product with the same name already exists!\\") new_products = self.products + [product] object.__setattr__(self, \'products\', new_products) def remove_product(self, product_name: str) -> bool: new_products = [p for p in self.products if p.name != product_name] if len(new_products) == len(self.products): return False object.__setattr__(self, \'products\', new_products) return True def total_inventory_value(self) -> float: return sum(product.total_cost() for product in self.products) def track_inventory_changes(self) -> dict: return asdict(self)"},{"question":"Customer Segmentation Using K-means Clustering Objective Using the K-means clustering algorithm from scikit-learn, implement a function to segment customers based on their purchase behavior data. Your task is to write a Python function that takes customer data as input and returns cluster labels assigned to each customer. Problem Statement You are provided with a dataset containing customer purchase behavior. The dataset has the following columns: - `CustomerID`: Unique identifier for each customer. - `AnnualIncome`: Annual income of the customer. - `SpendingScore`: Score assigned by the store based on customer spending behavior (higher is better). Using this data, implement customer segmentation using K-means clustering. Function Signature ```python def segment_customers(data: pd.DataFrame, n_clusters: int) -> np.ndarray: Segments customers into clusters using K-means clustering. Parameters: data (pd.DataFrame): A dataframe with columns [\'CustomerID\', \'AnnualIncome\', \'SpendingScore\']. n_clusters (int): The number of clusters to form. Returns: np.ndarray: An array of cluster labels assigned to each customer. ``` Constraints - You may assume that the data contains no missing values. - The number of clusters `n_clusters` will be a positive integer less than or equal to the number of data points. Example ```python import pandas as pd import numpy as np # Sample data data = pd.DataFrame({ \'CustomerID\': [1, 2, 3, 4, 5], \'AnnualIncome\': [15, 16, 17, 18, 19], \'SpendingScore\': [39, 81, 6, 77, 40] }) n_clusters = 2 labels = segment_customers(data, n_clusters) print(labels) # Output example: [1, 0, 1, 0, 1] ``` Requirements - The solution should demonstrate proper usage of scikit-learn\'s `KMeans` with an appropriate configuration (e.g., n_clusters, random_state). - Preprocessing steps such as selecting relevant features (AnnualIncome and SpendingScore) and scaling may be necessary. - The function should return an array of cluster labels corresponding to the input data. Evaluation Criteria - Correctness: The function should accurately cluster the data and return the correct cluster labels. - Clarity: The code should be well-organized and commented for readability. - Efficiency: The function should handle reasonably large datasets efficiently.","solution":"import pandas as pd import numpy as np from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler def segment_customers(data: pd.DataFrame, n_clusters: int) -> np.ndarray: Segments customers into clusters using K-means clustering. Parameters: data (pd.DataFrame): A dataframe with columns [\'CustomerID\', \'AnnualIncome\', \'SpendingScore\']. n_clusters (int): The number of clusters to form. Returns: np.ndarray: An array of cluster labels assigned to each customer. # Select the features for clustering features = data[[\'AnnualIncome\', \'SpendingScore\']] # Standardize the features scaler = StandardScaler() features_scaled = scaler.fit_transform(features) # Initialize and fit KMeans kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(features_scaled) # Get the cluster labels for each customer labels = kmeans.labels_ return labels"},{"question":"Frequency Domain Manipulation with PyTorch FFT You are provided with a two-dimensional signal (such as an image) that you need to analyze and manipulate in the frequency domain using PyTorch\'s FFT capabilities. Task: 1. Implement a function that performs the following steps: - Computes the 2D Fast Fourier Transform (FFT) of a given signal. - Shifts the zero frequency component to the center of the spectrum. - Applies a low-pass filter by zeroing out all frequencies higher than a certain threshold. - Shifts the zero frequency component back to the original position. - Computes the Inverse FFT to bring the signal back to the spatial domain. 2. Your function should be efficient and leverage available PyTorch FFT functions. Function Signature: ```python def apply_low_pass_filter(signal: torch.Tensor, threshold: float) -> torch.Tensor: Apply a low-pass filter to a 2D signal using PyTorch\'s FFT capabilities. Params: - signal (torch.Tensor): A 2D tensor representing the input signal. - threshold (float): The cutoff frequency ratio between 0 and 1. Frequencies higher than (threshold * max_frequency) are set to zero. Returns: - torch.Tensor: The filtered 2D signal in the spatial domain. ``` Input: - `signal`: A 2D torch.Tensor of size `(N, M)` representing the input signal (image). - `threshold`: A float value between 0 and 1 that specifies the cutoff frequency. Output: - A 2D torch.Tensor representing the filtered signal in the spatial domain. Example: ```python import torch signal = torch.randn(256, 256) # A random signal for example purposes threshold = 0.1 # Keep only the lowest 10% of frequencies filtered_signal = apply_low_pass_filter(signal, threshold) print(filtered_signal.shape) # Should return torch.Size([256, 256]) ``` Constraints: - You must use PyTorch for all computations. - The function should handle edge cases where `threshold` is 0 (should return a zero tensor) or 1 (should return the original signal). Performance Requirements: - Ensure that the function is efficient and can process signals of size at least `(512, 512)` in a reasonable time frame (e.g., within a few seconds).","solution":"import torch def apply_low_pass_filter(signal: torch.Tensor, threshold: float) -> torch.Tensor: Apply a low-pass filter to a 2D signal using PyTorch\'s FFT capabilities. Params: - signal (torch.Tensor): A 2D tensor representing the input signal. - threshold (float): The cutoff frequency ratio between 0 and 1. Frequencies higher than (threshold * max_frequency) are set to zero. Returns: - torch.Tensor: The filtered 2D signal in the spatial domain. # Error check for threshold bounds if not (0 <= threshold <= 1): raise ValueError(\\"Threshold must be between 0 and 1\\") # Compute the 2D FFT of the signal fft_signal = torch.fft.fft2(signal) # Shift the zero frequency component to the center of the spectrum fft_shifted = torch.fft.fftshift(fft_signal) # Create a mask for the low-pass filter num_rows, num_cols = signal.shape row_center, col_center = num_rows // 2, num_cols // 2 row_cutoff = int(threshold * row_center) col_cutoff = int(threshold * col_center) # Create a mask with ones in the low frequencies and zeros elsewhere mask = torch.zeros_like(fft_shifted, dtype=torch.bool) mask[row_center - row_cutoff:row_center + row_cutoff, col_center - col_cutoff:col_center + col_cutoff] = 1 # Apply the mask to the shifted FFT fft_shifted_filtered = fft_shifted * mask # Shift the zero frequency component back to the original position fft_filtered = torch.fft.ifftshift(fft_shifted_filtered) # Compute the Inverse FFT to get back to the spatial domain filtered_signal = torch.fft.ifft2(fft_filtered).real return filtered_signal"},{"question":"**Coding Assessment Question** Objective: Demonstrate the understanding of various kernel approximation methods provided by scikit-learn and evaluate their performance on a given dataset using a Support Vector Machine (SVM) classifier. # Problem Statement: You are given a dataset `X` and corresponding labels `y`. Your task is to implement and compare three different kernel approximation methods from scikit-learn\'s `kernel_approximation` submodule. Use each approximation method to transform the input features, then train a linear SVM classifier on the transformed data and evaluate its performance using accuracy. The kernel approximation methods to be used are: 1. Nystroem 2. RBFSampler 3. PolynomialCountSketch # Implementation Details: 1. **Input:** - `X_train`: 2D list of floats, training set features. - `y_train`: list of int, training set labels. - `X_test`: 2D list of floats, test set features. - `y_test`: list of int, test set labels. 2. **Output:** - A dictionary with keys as the kernel approximation method names (`\'Nystroem\'`, `\'RBFSampler\'`, `\'PolynomialCountSketch\'`) and values as the accuracy of the classifier on the test set using the respective method. 3. **Constraints:** - Use `n_components=100` for Nystroem and PolynomialCountSketch. - Use `gamma=1.0` for RBFSampler. - Use `degree=3` and `gamma=1.0` for PolynomialCountSketch. # Example: ```python X_train = [[0, 0], [1, 1], [1, 0], [0, 1]] y_train = [0, 0, 1, 1] X_test = [[0.5, 0.5], [1, 0], [0, 1], [0.75, 0.75]] y_test = [0, 1, 1, 0] # Expected Output (Exact values may vary due to randomness in kernel approximation methods): { \'Nystroem\': 1.0, \'RBFSampler\': 1.0, \'PolynomialCountSketch\': 0.75 } ``` # Functions: 1. **transform_and_evaluate**: The main function to be implemented. It should: - Apply each kernel approximation method to the training and test sets. - Train a linear SVM classifier on the transformed training set. - Evaluate and return the accuracy of the classifier on the transformed test set. ```python from sklearn.kernel_approximation import Nystroem, RBFSampler, PolynomialCountSketch from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def transform_and_evaluate(X_train, y_train, X_test, y_test): results = {} # Nystroem method nystroem = Nystroem(n_components=100, random_state=42) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) clf_nystroem = SGDClassifier(max_iter=1000, random_state=42) clf_nystroem.fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) results[\'Nystroem\'] = accuracy_score(y_test, y_pred_nystroem) # RBFSampler method rbf_sampler = RBFSampler(gamma=1.0, random_state=42) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) clf_rbf = SGDClassifier(max_iter=1000, random_state=42) clf_rbf.fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) results[\'RBFSampler\'] = accuracy_score(y_test, y_pred_rbf) # PolynomialCountSketch method poly_sketch = PolynomialCountSketch(degree=3, gamma=1.0, n_components=100, random_state=42) X_train_poly = poly_sketch.fit_transform(X_train) X_test_poly = poly_sketch.transform(X_test) clf_poly = SGDClassifier(max_iter=1000, random_state=42) clf_poly.fit(X_train_poly, y_train) y_pred_poly = clf_poly.predict(X_test_poly) results[\'PolynomialCountSketch\'] = accuracy_score(y_test, y_pred_poly) return results ``` # Notes: - Ensure to handle any necessary imports. - Use `random_state` for reproducibility. - You may compare the performance of the different approximations to understand their relative strengths. # Submission: Submit your implementation of the `transform_and_evaluate` function. Ensure it adheres to the input-output format and meets the constraints specified.","solution":"from sklearn.kernel_approximation import Nystroem, RBFSampler, PolynomialCountSketch from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def transform_and_evaluate(X_train, y_train, X_test, y_test): results = {} # Nystroem method nystroem = Nystroem(n_components=100, random_state=42) X_train_nystroem = nystroem.fit_transform(X_train) X_test_nystroem = nystroem.transform(X_test) clf_nystroem = SGDClassifier(max_iter=1000, random_state=42) clf_nystroem.fit(X_train_nystroem, y_train) y_pred_nystroem = clf_nystroem.predict(X_test_nystroem) results[\'Nystroem\'] = accuracy_score(y_test, y_pred_nystroem) # RBFSampler method rbf_sampler = RBFSampler(gamma=1.0, random_state=42) X_train_rbf = rbf_sampler.fit_transform(X_train) X_test_rbf = rbf_sampler.transform(X_test) clf_rbf = SGDClassifier(max_iter=1000, random_state=42) clf_rbf.fit(X_train_rbf, y_train) y_pred_rbf = clf_rbf.predict(X_test_rbf) results[\'RBFSampler\'] = accuracy_score(y_test, y_pred_rbf) # PolynomialCountSketch method poly_sketch = PolynomialCountSketch(degree=3, gamma=1.0, n_components=100, random_state=42) X_train_poly = poly_sketch.fit_transform(X_train) X_test_poly = poly_sketch.transform(X_test) clf_poly = SGDClassifier(max_iter=1000, random_state=42) clf_poly.fit(X_train_poly, y_train) y_pred_poly = clf_poly.predict(X_test_poly) results[\'PolynomialCountSketch\'] = accuracy_score(y_test, y_pred_poly) return results"},{"question":"**Title:** Implementing a Custom Iterable Data Structure with Functional Utilities **Objective:** Design and implement a custom iterable data structure in Python using generators. Extend the functionality of this data structure with utility functions using features from the `itertools` and `functools` modules. **Description:** You are required to design a `FibonacciSequence` class that represents an infinite sequence of Fibonacci numbers. This class should be iterable and use a generator to produce the Fibonacci numbers. In addition to this, implement utility functions to work with this sequence using `itertools` and `functools`. **Requirements:** 1. **FibonacciSequence Class:** - Implement a class `FibonacciSequence` that produces an infinite sequence of Fibonacci numbers. - The class should be iterable (i.e., it should implement the `__iter__` and `__next__` methods). - Use a generator within the class to yield Fibonacci numbers. 2. **Utility Functions:** - Implement a function `take(n, iterable)` that takes the first `n` elements from an iterable `iterable` and returns them as a list. You can use `itertools.islice`. - Implement a function `sum_of_squares(n, iterable)` that returns the sum of the squares of the first `n` elements of the iterable `iterable`. You can use `functools.reduce` and a generator expression. **Constraints:** - Do not use any external libraries apart from `itertools` and `functools`. - Ensure that the implementation is efficient and uses memory appropriately (given that it\'s working with infinite sequences). **Input and Output:** The following code demonstrates the expected output: ```python class FibonacciSequence: def __init__(self): # Initialization of state def __iter__(self): # Implement the iterator interface return self def __next__(self): # Produce the next Fibonacci number pass def take(n, iterable): # Return first n elements of iterable pass def sum_of_squares(n, iterable): # Return the sum of squares of first n elements of iterable pass # Example usage fib_seq = FibonacciSequence() # Get the first 10 Fibonacci numbers first_10_fibs = take(10, fib_seq) print(first_10_fibs) # Output: [0, 1, 1, 2, 3, 5, 8, 13, 21, 34] # Get the sum of the squares of the first 10 Fibonacci numbers result = sum_of_squares(10, fib_seq) print(result) # Output should be the sum of squares of first 10 Fibonacci numbers ``` **Performance Requirements:** - Your implementation should handle large values of `n` efficiently without running out of memory. - Ensure that solutions are optimized for performance, considering the infinite nature of the sequence. # **Note:** Utilize the concepts of iterators, generators, and the itertools & functools modules to achieve the desired functionality.","solution":"import itertools import functools class FibonacciSequence: def __init__(self): self.a, self.b = 0, 1 def __iter__(self): return self def __next__(self): a = self.a self.a, self.b = self.b, self.a + self.b return a def take(n, iterable): return list(itertools.islice(iterable, n)) def sum_of_squares(n, iterable): first_n = take(n, iterable) return functools.reduce(lambda x, y: x + y**2, first_n, 0)"},{"question":"Objective: To assess the student\'s understanding of Python’s C API, specifically focusing on creating a C extension module that adheres to the Limited API and ensures compatibility across Python versions. Problem Statement: Create a Python C extension module that defines a function to compute the factorial of a non-negative integer using the Limited API. The extension module should: 1. Define a function `factorial` that: * Takes a single positional argument, a non-negative integer `n`. * Returns the factorial of `n` as a Python integer object. 2. Ensure compatibility with the Limited API for Python versions 3.2 and above. 3. Properly handle cases where the input is not a non-negative integer, raising an appropriate Python exception. Detailed Requirements: 1. Use the `Py_LIMITED_API` macro to define the Limited API version. 2. Ensure your C extension: * Compiles on Python 3.2+ without modification. * Uses functions and types only from the Limited API. 3. Implement the following function: ```c static PyObject* factorial(PyObject *self, PyObject *args); ``` 4. Follow these steps to build the extension: * Create a `setup.py` script for building the extension. * Write a `README` file with build and usage instructions. * Ensure that the extension can be compiled and installed using standard build tools (e.g., `python setup.py build_ext --inplace`). Input and Output: * **Input:** A single non-negative integer `n`. * **Output:** The factorial of `n` as a Python integer object. Constraints: * The input will be a non-negative integer (0 ≤ n ≤ 20). Example: ```python import mymodule # Correct input print(mymodule.factorial(5)) # Output: 120 # Incorrect input print(mymodule.factorial(-1)) # Raises ValueError print(mymodule.factorial(\\"a\\")) # Raises TypeError ``` Note: * Ensure to include error checking and handle edge cases such as negative inputs and non-integer values properly. * Review the Limited API documentation to ensure compliance. # Provided Materials: 1. Documentation on Python\'s C API and Limited API. Good luck!","solution":"import math def factorial(n): Returns the factorial of a non-negative integer n. If n is negative or not an integer, raises an appropriate error. if not isinstance(n, int): raise TypeError(\\"Input must be a non-negative integer.\\") if n < 0: raise ValueError(\\"Input must be a non-negative integer.\\") return math.factorial(n)"},{"question":"Problem Statement: In this problem, you are required to implement a function that processes a list of integers and returns a report on various properties of the unique elements in the list using sets. This will test your understanding of set operations, performance considerations with large data sets, and error handling in Python. # Function Signature ```python def process_integers(integers: list[int]) -> dict: pass ``` # Input Parameters - `integers (list[int])`: A list of integers which may contain duplicates. The list has at least 1 and at most (10^6) elements. Each integer (i) satisfies (-10^9 leq i leq 10^9). # Output The function should return a dictionary with the following keys: - `unique_count` (int): The number of unique integers in the list. - `max_value` (int): The maximum value among the unique integers. - `min_value` (int): The minimum value among the unique integers. - `freq_above_zero` (set): A set containing all unique integers greater than zero. - `freq_below_zero` (set): A set containing all unique integers less than zero. # Constraints - The list will contain integers only. - The list can be empty in runtime but initialized with at least one element. - The function should handle large inputs efficiently. # Example ```python # Example 1: integers = [4, 6, 2, 6, -1, -3, 0, 4, 2, -1] result = process_integers(integers) print(result) # Output: # { # \'unique_count\': 6, # \'max_value\': 6, # \'min_value\': -3, # \'freq_above_zero\': {2, 4, 6}, # \'freq_below_zero\': {-1, -3} # } # Example 2: integers = [0] result = process_integers(integers) print(result) # Output # { # \'unique_count\': 1, # \'max_value\': 0, # \'min_value\': 0, # \'freq_above_zero\': set(), # \'freq_below_zero\': set() # } ``` # Instructions - You are only allowed to use built-in Python sets for this problem. - Your implementation should efficiently handle a list with up to (10^6) elements. - Be mindful of edge cases such as all positive or all negative integers.","solution":"def process_integers(integers): Processes a list of integers and returns a report on various properties of the unique elements in the list. Parameters: integers (list[int]): A list of integers which may contain duplicates. Returns: dict: A dictionary with the count of unique integers, the max and min values among the unique integers, and sets of integers above and below zero. if not integers: return { \'unique_count\': 0, \'max_value\': None, \'min_value\': None, \'freq_above_zero\': set(), \'freq_below_zero\': set() } unique_integers = set(integers) unique_count = len(unique_integers) max_value = max(unique_integers) min_value = min(unique_integers) freq_above_zero = {num for num in unique_integers if num > 0} freq_below_zero = {num for num in unique_integers if num < 0} return { \'unique_count\': unique_count, \'max_value\': max_value, \'min_value\': min_value, \'freq_above_zero\': freq_above_zero, \'freq_below_zero\': freq_below_zero }"},{"question":"# Asynchronous Task Management with asyncio **Objective:** Your task is to implement a Python function using the `asyncio` package, which demonstrates handling multiple asynchronous tasks concurrently. **Question:** Write a function `manage_tasks` using asyncio that: 1. Creates three asynchronous tasks: - Task 1: Sleeps for 2 seconds and returns the message \\"Task 1 Completed\\". - Task 2: Sleeps for 3 seconds and returns the message \\"Task 2 Completed\\". - Task 3: Sleeps for 1 second and returns the message \\"Task 3 Completed\\". 2. Runs all three tasks concurrently. 3. Collects and returns the results of all tasks in a list. **Function Signature:** ```python import asyncio from typing import List async def manage_tasks() -> List[str]: pass ``` **Expected Output:** The `manage_tasks` function should return: ```python [\\"Task 3 Completed\\", \\"Task 1 Completed\\", \\"Task 2 Completed\\"] ``` **Constraints:** - Use the `asyncio` package and properly manage the asynchronous tasks to ensure they run concurrently. - Ensure that the tasks are awaited and their results are collected in the specified format. - Do not use any external libraries other than `asyncio`. **Hint:** You may find the `asyncio.gather` function useful for running multiple coroutines concurrently and collecting their results. **Example Usage:** ```python import asyncio # Example of how the function might be executed async def main(): results = await manage_tasks() print(results) # Running the main function asyncio.run(main()) ``` **Note:** Make sure to test your function to ensure it correctly handles the asynchronous execution and returns the expected results.","solution":"import asyncio from typing import List async def task1(): await asyncio.sleep(2) return \\"Task 1 Completed\\" async def task2(): await asyncio.sleep(3) return \\"Task 2 Completed\\" async def task3(): await asyncio.sleep(1) return \\"Task 3 Completed\\" async def manage_tasks() -> List[str]: tasks = [task1(), task2(), task3()] results = await asyncio.gather(*tasks) return results"},{"question":"You are given a dataset of sales records for a retail store. The data is stored in a DataFrame `sales_df` with the following columns: - `date`: Date of the transaction (in YYYY-MM-DD format). - `store`: Identifier for the store. - `item`: Identifier for the item sold. - `quantity`: Number of units sold. - `price`: Price per unit sold. Here is an example of how the dataset looks: ```python import pandas as pd data = { \\"date\\": [\\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-02\\", \\"2023-01-03\\"], \\"store\\": [\\"Store_A\\", \\"Store_B\\", \\"Store_A\\", \\"Store_B\\", \\"Store_A\\"], \\"item\\": [\\"Item_1\\", \\"Item_2\\", \\"Item_1\\", \\"Item_3\\", \\"Item_1\\"], \\"quantity\\": [10, 5, 20, 3, 5], \\"price\\": [2.5, 3.0, 2.5, 5.0, 2.5] } sales_df = pd.DataFrame(data) ``` Using this dataset, complete the following tasks: 1. **Total Sales Calculation**: Create a new column `total_sales` that represents the total sales value (quantity * price) for each transaction. 2. **Pivot Data**: Reshape the data to show the total quantity of each item sold at each store per day in a pivoted format. The resulting pivot table should have dates as the index, items as columns, and total quantities as values. 3. **Stacked Data**: Stack the pivoted data such that the item columns become rows. 4. **Crosstab**: Create a crosstab to show the total sales per store per item. Additionally, add margins to show grand totals. 5. **Melt Data**: Melt the original DataFrame (with the `total_sales` column added) to long format where the identifier variables are `date`, `store`, `item`, and the measured variable is `total_sales`. Provide the final results for each task. You can assume that `pandas` is already imported as `pd`. # Input A pandas DataFrame `sales_df` as described above. # Output 1. A DataFrame with an additional `total_sales` column. 2. A pivoted DataFrame showing total quantities of items sold per store per day. 3. A stacked DataFrame from the pivoted data. 4. A crosstab DataFrame showing total sales per store per item with margins. 5. A melted DataFrame with `total_sales` as the measured variable. # Constraints - Assume the input DataFrame `sales_df` is well-formed and does not contain missing values. - The data covers multiple days, stores, and items. # Example Given the provided `sales_df`, the output should include the following steps: ```python # Step 1: Total sales calculation sales_df[\'total_sales\'] = sales_df[\'quantity\'] * sales_df[\'price\'] # Step 2: Pivot data pivot_df = sales_df.pivot(index=\'date\', columns=\'item\', values=\'quantity\') # Step 3: Stack data stacked_df = pivot_df.stack() # Step 4: Crosstab crosstab_df = pd.crosstab(sales_df[\'store\'], sales_df[\'item\'], values=sales_df[\'total_sales\'], aggfunc=\'sum\', margins=True) # Step 5: Melt data melted_df = sales_df.melt(id_vars=[\'date\', \'store\', \'item\'], value_vars=[\'total_sales\']) ``` Now, please implement each step as specified.","solution":"import pandas as pd # Sample DataFrame data = { \\"date\\": [\\"2023-01-01\\", \\"2023-01-01\\", \\"2023-01-02\\", \\"2023-01-02\\", \\"2023-01-03\\"], \\"store\\": [\\"Store_A\\", \\"Store_B\\", \\"Store_A\\", \\"Store_B\\", \\"Store_A\\"], \\"item\\": [\\"Item_1\\", \\"Item_2\\", \\"Item_1\\", \\"Item_3\\", \\"Item_1\\"], \\"quantity\\": [10, 5, 20, 3, 5], \\"price\\": [2.5, 3.0, 2.5, 5.0, 2.5] } sales_df = pd.DataFrame(data) # Step 1: Total sales calculation sales_df[\'total_sales\'] = sales_df[\'quantity\'] * sales_df[\'price\'] # Step 2: Pivot data pivot_df = sales_df.pivot_table(index=\'date\', columns=\'item\', values=\'quantity\', aggfunc=\'sum\', fill_value=0) # Step 3: Stack data stacked_df = pivot_df.stack() # Step 4: Crosstab crosstab_df = pd.crosstab(sales_df[\'store\'], sales_df[\'item\'], values=sales_df[\'total_sales\'], aggfunc=\'sum\', margins=True) # Step 5: Melt data melted_df = sales_df.melt(id_vars=[\'date\', \'store\', \'item\'], value_vars=[\'total_sales\']) # Print results for verification print(\\"Step 1: DataFrame with total_sales column\\") print(sales_df) print(\\"nStep 2: Pivoted DataFrame\\") print(pivot_df) print(\\"nStep 3: Stacked DataFrame\\") print(stacked_df) print(\\"nStep 4: Crosstab DataFrame\\") print(crosstab_df) print(\\"nStep 5: Melted DataFrame\\") print(melted_df)"},{"question":"# Question: Secure Temporary Data Handling with `tempfile` You are tasked with simulating a secure data processing workflow that involves handling temporary files and directories. During this workflow, you will need to perform several operations while ensuring that temporary data is securely managed and properly cleaned up after use. Function Requirements 1. **Function 1: `process_temp_file`** - **Description**: This function should create a temporary file, write some data to it, read the data back to confirm it was written correctly, and then automatically clean up the file. - **Input**: A byte string `data` to be written to the temporary file. - **Output**: The data read back from the file to confirm it matches the input. - **Constraints**: Use `TemporaryFile` and ensure the file is closed and removed after the function executes (hint: use a context manager). 2. **Function 2: `generate_temp_report`** - **Description**: This function should create a temporary directory, generate a report file within this directory by writing some text, read the report content back for confirmation, and then clean up the directory. - **Input**: A string `report_content` to be written to the report file. - **Output**: The content read back from the report file to confirm it matches the input. - **Constraints**: Use `TemporaryDirectory` and ensure the directory and its contents are removed after the function executes. 3. **Function 3: `secure_temp_operations`** - **Description**: This function should demonstrate the usage of `SpooledTemporaryFile` by handling data that is initially stored in memory and overflowed to disk if the size exceeds a threshold. - **Input**: A list of byte strings `data_blocks`, and an integer `max_size` indicating the maximum size before the data should move to disk. - **Output**: Return the combined data read back from the temporary file. - **Constraints**: Use `SpooledTemporaryFile` and ensure the file is properly closed and cleaned up after operations. Examples ```python # Example usages assert process_temp_file(b\'Hello, Tempfile!\') == b\'Hello, Tempfile!\' report_content = \\"This is a temporary report.\\" assert generate_temp_report(report_content) == report_content data_blocks = [b\'first block \', b\'second block \', b\'third block\'] assert secure_temp_operations(data_blocks, 1024) == b\'first block second block third block\' ``` Performance Requirements - Each function should handle operations efficiently, ensuring minimal I/O overhead. - Ensure that files and directories are securely created and cleaned up to avoid potential leakage of temporary data. Employ these requirements to develop a robust solution that demonstrates proficiency with the `tempfile` module.","solution":"import tempfile def process_temp_file(data): Creates a temporary file, writes some data to it, reads the data back to confirm it was written correctly, and then automatically cleans up the file. :param data: byte string to write to the temporary file :return: the data read back from the file with tempfile.TemporaryFile() as temp_file: temp_file.write(data) temp_file.seek(0) read_data = temp_file.read() return read_data def generate_temp_report(report_content): Creates a temporary directory, generates a report file within this directory by writing some text, reads the report content back for confirmation, and then cleans up the directory. :param report_content: string to write to the report file :return: the content read back from the report file with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = f\\"{temp_dir}/temp_report.txt\\" with open(temp_file_path, \'w\') as report_file: report_file.write(report_content) with open(temp_file_path, \'r\') as report_file: read_report_content = report_file.read() return read_report_content def secure_temp_operations(data_blocks, max_size): Demonstrates the usage of SpooledTemporaryFile by handling data that is initially stored in memory and overflowed to disk if the size exceeds a threshold. :param data_blocks: list of byte strings to write to the temporary file :param max_size: maximum size before the data should move to disk :return: the combined data read back from the temporary file with tempfile.SpooledTemporaryFile(max_size=max_size) as spooled_file: for block in data_blocks: spooled_file.write(block) spooled_file.seek(0) read_data = spooled_file.read() return read_data"},{"question":"# Advanced Coding Assessment: Multi-Type I/O Handling Objective Implement a function `process_file_content()` that takes the path to a text file, reads its contents, modifies it based on given conditions, writes the modified contents back to a new file, and returns specific details about the operations performed. Function Signature ```python def process_file_content(input_path: str, output_path: str) -> dict: ... ``` Input - `input_path` (str): The path to the input text file. - `output_path` (str): The path to the output text file where the modified content will be written. Behavior 1. **Reading the File:** - Open the input file in read mode with UTF-8 encoding. - Read its contents. 2. **Modification:** - Convert all the text to uppercase. - Replace all newline characters `n` with the string `\\"<EOL>\\"`. 3. **Writing to Output:** - Open the output file in write mode with UTF-8 encoding. - Write the modified contents to the output file. 4. **Returning Details:** - Return a dictionary with the following keys: - `original_character_count` (int): The number of characters in the original file. - `modified_character_count` (int): The number of characters in the modified content. - `output_file_path` (str): The path to the output file. Example ```python input_path = \\"example.txt\\" output_path = \\"processed_example.txt\\" result = process_file_content(input_path, output_path) print(result) # Expected Output (example): # { # \'original_character_count\': 120, # \'modified_character_count\': 130, # \'output_file_path\': \'processed_example.txt\' # } ``` Constraints - The function should handle file-related errors (e.g., file not found) gracefully, raising an appropriate exception with an informative message. - Ensure that all files are properly closed after their operations, even in the case of exceptions. Notes - Testing will involve creating temporary files with different contents, ensuring the function performs accurately and handles exceptions as expected.","solution":"def process_file_content(input_path: str, output_path: str) -> dict: try: with open(input_path, \'r\', encoding=\'utf-8\') as input_file: original_content = input_file.read() modified_content = original_content.upper().replace(\'n\', \'<EOL>\') with open(output_path, \'w\', encoding=\'utf-8\') as output_file: output_file.write(modified_content) return { \'original_character_count\': len(original_content), \'modified_character_count\': len(modified_content), \'output_file_path\': output_path } except FileNotFoundError: raise Exception(f\\"The file at path {input_path} was not found.\\") except Exception as e: raise Exception(f\\"An error occurred: {str(e)}\\")"},{"question":"# Sorting and Custom Key Functions in Python **Problem Statement:** You have been provided with a list of students, where each student is represented by a dictionary with the following attributes: - `name` (string): The student\'s name. - `grade` (string): The student\'s grade (e.g., \'A\', \'B\', \'C\', etc.). - `age` (integer): The student\'s age. Write a function `custom_sort_students` that sorts this list of students based on the following criteria: 1. Primarily by `grade` in descending order (\'A\' > \'B\' > \'C\' > ...). 2. If two students have the same grade, then by their `age` in ascending order. 3. If two students have the same grade and age, then by their `name` in alphabetical order. The function should accept a list of student dictionaries and return the sorted list of students. **Input:** - A list of dictionaries, where each dictionary has the following keys: - `name` (string) - `grade` (string) - `age` (integer) **Output:** - A list of dictionaries sorted based on the criteria mentioned above. **Constraints:** - All `grade` values will be single uppercase letters only. - Names are case-sensitive and should be considered in their current form for alphabetical sorting. - The list can contain up to 1000 students. **Example:** ```python students = [ {\'name\': \'john\', \'grade\': \'A\', \'age\': 15}, {\'name\': \'jane\', \'grade\': \'B\', \'age\': 12}, {\'name\': \'dave\', \'grade\': \'B\', \'age\': 10}, {\'name\': \'rose\', \'grade\': \'A\', \'age\': 14}, {\'name\': \'alex\', \'grade\': \'C\', \'age\': 12}, ] sorted_students = custom_sort_students(students) # Expected output: # [ # {\'name\': \'rose\', \'grade\': \'A\', \'age\': 14}, # {\'name\': \'john\', \'grade\': \'A\', \'age\': 15}, # {\'name\': \'dave\', \'grade\': \'B\', \'age\': 10}, # {\'name\': \'jane\', \'grade\': \'B\', \'age\': 12}, # {\'name\': \'alex\', \'grade\': \'C\', \'age\': 12}, # ] ``` **Function Signature:** ```python def custom_sort_students(students: list) -> list: pass ``` **Note:** - Utilize Python\'s `sorted()` function and relevant modules to implement the sorting efficiently. - Make sure to handle edge cases such as empty lists.","solution":"def custom_sort_students(students): Sorts the list of students by grade (descending), age (ascending), and name (alphabetical). Args: students (list): A list of dictionaries, each representing a student with \'name\', \'grade\', and \'age\' keys. Returns: list: The sorted list of students. # Define grade priority (highest to lowest) grade_priority = {\'A\': 1, \'B\': 2, \'C\': 3, \'D\': 4, \'E\': 5, \'F\': 6} # Sorting function students_sorted = sorted(students, key=lambda x: (grade_priority[x[\'grade\']], x[\'age\'], x[\'name\'])) return students_sorted"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of the `csv` module in Python by writing code that reads, processes, and writes CSV data. # Problem Statement You are given a CSV file containing information about a list of students and their respective scores in three subjects (Math, Science, and English). Your task is to write a Python function that reads this CSV file, calculates the total and average score for each student, and writes the results to a new CSV file. # Input 1. `input_filepath` (str): The file path of the input CSV file. 2. `output_filepath` (str): The file path of the output CSV file where the results will be written. The input CSV file has the following format: ``` Name,Math,Science,English Alice,90,85,88 Bob,75,80,72 Charlie,95,92,89 ``` # Output The output CSV file should have the following format: ``` Name,Math,Science,English,Total,Average Alice,90,85,88,263,87.67 Bob,75,80,72,227,75.67 Charlie,95,92,89,276,92.00 ``` # Constraints - The input file will have at least one student record. - All scores are integers. - The names and scores are separated by commas. - The average should be rounded to two decimal places. # Function Signature ```python def process_student_scores(input_filepath: str, output_filepath: str) -> None: ``` # Example ```python # Consider the input CSV content saved in \'students.csv\': # Name,Math,Science,English # Alice,90,85,88 # Bob,75,80,72 # Charlie,95,92,89 process_student_scores(\'students.csv\', \'output.csv\') # The \'output.csv\' file should contain: # Name,Math,Science,English,Total,Average # Alice,90,85,88,263,87.67 # Bob,75,80,72,227,75.67 # Charlie,95,92,89,276,92.00 ``` # Performance Requirements - Efficient handling of file I/O operations. - Proper use of the `csv` module for reading and writing files.","solution":"import csv def process_student_scores(input_filepath: str, output_filepath: str) -> None: with open(input_filepath, mode=\'r\', newline=\'\') as infile: reader = csv.DictReader(infile) students = list(reader) for student in students: math_score = int(student[\'Math\']) science_score = int(student[\'Science\']) english_score = int(student[\'English\']) total_score = math_score + science_score + english_score average_score = round(total_score / 3, 2) student[\'Total\'] = total_score student[\'Average\'] = f\\"{average_score:.2f}\\" fieldnames = [\'Name\', \'Math\', \'Science\', \'English\', \'Total\', \'Average\'] with open(output_filepath, mode=\'w\', newline=\'\') as outfile: writer = csv.DictWriter(outfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(students)"},{"question":"<|Analysis Begin|> The provided documentation focuses on the `importlib.metadata` module in Python 3.8 and later versions, especially the updates in Python 3.10. This module is used to interact with metadata of installed packages in Python\'s site-packages directory. It provides functions to retrieve information such as the version of a package, metadata fields, entry points, files within a distribution, and dependency requirements. Key concepts include: 1. **Version Retrieval**: Using `version()` to get the version number of a given package. 2. **Metadata Extraction**: Using `metadata()` to get detailed metadata of a package. 3. **Entry Points**: Using `entry_points()` to get specific entry point information. 4. **Package Files**: Using `files()` to get file information for a package. 5. **Requirements**: Using `requires()` to get the requirements of a package. 6. **Distributions**: Interacting with the `Distribution` class to get consolidated metadata. These capabilities offer a comprehensive way to programmatically interact with the installation metadata of Python packages. <|Analysis End|> <|Question Begin|> **Challenging Coding Assessment Question: `importlib.metadata`** Author: [Your Name] Date: [Today\'s Date] # Problem Statement You have been tasked with developing a Python script that utilizes the `importlib.metadata` module to analyze installed Python packages and generate a detailed report. The report should include the metadata for specific packages as well as their dependencies. # Objective Write a function `generate_package_report(packages: List[str]) -> Dict[str, Dict[str, Any]]` which takes a list of package names as input and returns a dictionary containing detailed metadata and dependency information for each specified package. # Requirements - For each package in the input list: - Retrieve its version. - Retrieve all available metadata fields. - List all entry points within the `console_scripts` group. - Retrieve a list of all the files installed by the package. - List all dependency requirements of the package. # Input - `packages`: A list of strings, where each string represents the name of an installed Python package. Example: `[\\"wheel\\", \\"requests\\"]`. # Output - Returns a dictionary where each key is a package name and the value is another dictionary with the following keys: - `\'version\'`: The version string of the package. - `\'metadata\'`: A dictionary of all metadata fields and their corresponding values. - `\'console_scripts\'`: A list of entry points in the \'console_scripts\' group. - `\'files\'`: A list of file paths installed by the package. - `\'requirements\'`: A list of dependency requirements. # Constraints - If a package does not exist or an error occurs in retrieving information, it should be skipped and noted in a `errors` list within the final report. - The solution should handle a large number of packages efficiently. - Assume the function is executed in an environment where `importlib.metadata` is available and the packages are installed. # Example ```python from typing import List, Dict, Any from importlib.metadata import version, metadata, entry_points, files, requires def generate_package_report(packages: List[str]) -> Dict[str, Dict[str, Any]]: report = {} errors = [] for package in packages: try: pkg_version = version(package) pkg_metadata = metadata(package) pkg_entry_points = [ep.name for ep in entry_points(group=\'console_scripts\', name=package)] pkg_files = [str(file) for file in files(package)] pkg_requirements = requires(package) report[package] = { \'version\': pkg_version, \'metadata\': dict(pkg_metadata), \'console_scripts\': pkg_entry_points, \'files\': pkg_files, \'requirements\': pkg_requirements or [] } except Exception as e: errors.append(f\\"Error retrieving information for package {package}: {str(e)}\\") report[\'errors\'] = errors return report # Example Usage packages = [\\"wheel\\", \\"requests\\"] report = generate_package_report(packages) print(report) ``` # Explanation - The function iterates over each package name in the input list. - For each package, it retrieves the version, all metadata, entry points in the `console_scripts` group, file paths, and requirements. - If any error occurs during the retrieval, the package is skipped, and an error message is recorded. - The final report is a dictionary mapping each package name to its respective details, including metadata and dependencies, and an `errors` list for any issues encountered. This problem tests the understanding and application of the `importlib.metadata` module, handling of package metadata, exception handling, and efficient data collection and processing.","solution":"from typing import List, Dict, Any from importlib.metadata import version, metadata, entry_points, files, requires def generate_package_report(packages: List[str]) -> Dict[str, Dict[str, Any]]: report = {} errors = [] for package in packages: try: pkg_version = version(package) pkg_metadata = metadata(package) pkg_entry_points = [str(ep) for ep in entry_points().get(\'console_scripts\', []) if ep.dist.name == package] pkg_files = [str(file) for file in files(package)] pkg_requirements = requires(package) report[package] = { \'version\': pkg_version, \'metadata\': dict(pkg_metadata), \'console_scripts\': pkg_entry_points, \'files\': pkg_files, \'requirements\': pkg_requirements or [] } except Exception as e: errors.append(f\\"Error retrieving information for package {package}: {str(e)}\\") report[\'errors\'] = errors return report"},{"question":"Objective Implement a function that extracts specific information from a given list of dictionaries representing a dataset, demonstrating comprehensive knowledge of built-in types, their operations, and string manipulations. Problem Statement Write a function `filter_and_aggregate_students(students: List[Dict[str, Union[str, int]]]) -> Dict[str, Any]` that performs the following: 1. **Filter Students**: Filter the list of students to include only those who have a score greater than 70 and belong to a specified \'department\'. 2. **Aggregate Scores**: Calculate the average and highest score for each department in the remaining dataset. 3. **String Manipulation**: Properly format the output strings for the results using string methods and formatted string literals. Input - `students`: A list of dictionaries, where each dictionary represents a student with keys: - `name`: A string representing the student\'s name. - `score`: An integer representing the student\'s score. - `department`: A string representing the department the student belongs to. ```python Example: students = [ {\\"name\\": \\"Alice\\", \\"score\\": 85, \\"department\\": \\"Math\\"}, {\\"name\\": \\"Bob\\", \\"score\\": 65, \\"department\\": \\"Science\\"}, {\\"name\\": \\"Charlie\\", \\"score\\": 95, \\"department\\": \\"Math\\"}, {\\"name\\": \\"David\\", \\"score\\": 70, \\"department\\": \\"Math\\"}, {\\"name\\": \\"Eve\\", \\"score\\": 72, \\"department\\": \\"Science\\"}, ] ``` Output - A dictionary with keys as department names and values as dictionaries having keys: - `average_score`: The average score of the students in that department. - `highest_score`: The highest score of the students in that department. - `summary`: A formatted string summarizing the result (e.g., \\"Math Department: Avg Score = 90, Highest Score = 95\\"). ```python Example: { \\"Math\\": { \\"average_score\\": 90.0, \\"highest_score\\": 95, \\"summary\\": \\"Math Department: Avg Score = 90.0, Highest Score = 95\\" }, \\"Science\\": { \\"average_score\\": 72.0, \\"highest_score\\": 72, \\"summary\\": \\"Science Department: Avg Score = 72.0, Highest Score = 72\\" } } ``` Constraints - The `students` list will have at least one student. - The `score` will be between 0 and 100. - The `department` names and student `name` will be non-empty strings. Implementation Requirements: - Use built-in Python types and their corresponding methods wherever applicable. - Use list comprehensions, dictionary comprehensions, and string formatting methods effectively. - Ensure the solution is efficient and leverages the power of Python\'s built-in capabilities. Performance Requirements - Your solution should handle reasonably large lists (up to the size of 10^4 students) efficiently. ```python def filter_and_aggregate_students(students: List[Dict[str, Union[str, int]]]) -> Dict[str, Any]: # Implement required functionality here ``` Example ```python students = [ {\\"name\\": \\"Alice\\", \\"score\\": 85, \\"department\\": \\"Math\\"}, {\\"name\\": \\"Bob\\", \\"score\\": 65, \\"department\\": \\"Science\\"}, {\\"name\\": \\"Charlie\\", \\"score\\": 95, \\"department\\": \\"Math\\"}, {\\"name\\": \\"David\\", \\"score\\": 70, \\"department\\": \\"Math\\"}, {\\"name\\": \\"Eve\\", \\"score\\": 72, \\"department\\": \\"Science\\"}, ] print(filter_and_aggregate_students(students)) # Expected Output: # { # \\"Math\\": { # \\"average_score\\": 90.0, # \\"highest_score\\": 95, # \\"summary\\": \\"Math Department: Avg Score = 90.0, Highest Score = 95\\" # }, # \\"Science\\": { # \\"average_score\\": 72.0, # \\"highest_score\\": 72, # \\"summary\\": \\"Science Department: Avg Score = 72.0, Highest Score = 72\\" # } # } ```","solution":"from typing import List, Dict, Union, Any def filter_and_aggregate_students(students: List[Dict[str, Union[str, int]]]) -> Dict[str, Any]: # Filter students with score > 70 filtered_students = [student for student in students if student[\'score\'] > 70] # Initialize a dictionary to store department wise details department_details = {} for student in filtered_students: dept = student[\'department\'] score = student[\'score\'] if dept not in department_details: department_details[dept] = {\'total_score\': 0, \'highest_score\': 0, \'count\': 0} department_details[dept][\'total_score\'] += score department_details[dept][\'highest_score\'] = max(department_details[dept][\'highest_score\'], score) department_details[dept][\'count\'] += 1 result = {} for dept, details in department_details.items(): avg_score = details[\'total_score\'] / details[\'count\'] highest_score = details[\'highest_score\'] summary = f\\"{dept} Department: Avg Score = {avg_score:.1f}, Highest Score = {highest_score}\\" result[dept] = { \'average_score\': avg_score, \'highest_score\': highest_score, \'summary\': summary } return result"},{"question":"**Platform Information Summary** You are required to implement a function `get_platform_summary()` that uses the `platform` module to provide a summary of various platform-specific details. The summary should include information about the system, Python version, and architecture among others. # Function Signature ```python def get_platform_summary() -> dict: pass ``` # Expected Behavior You need to implement the `get_platform_summary()` function that returns a dictionary summarizing the following details: - `architecture`: A tuple containing the bit architecture and linkage format (use `platform.architecture()`). - `machine`: The machine type (use `platform.machine()`). - `node`: The computer\'s network name (use `platform.node()`). - `platform`: A string identifying the underlying platform (use `platform.platform()` with default parameters). - `processor`: The real processor name (use `platform.processor()`). - `python_build`: A tuple containing the Python build number and build date (use `platform.python_build()`). - `python_compiler`: A string identifying the compiler used for compiling Python (use `platform.python_compiler()`). - `python_implementation`: A string identifying the Python implementation (use `platform.python_implementation()`). - `python_version`: A string representing the Python version (use `platform.python_version()`). - `system`: The system/OS name (use `platform.system()`). # Constraints - All functions from the `platform` module used must return a non-empty value. If any function returns an empty string or an undetermined value, set the corresponding dictionary entry to `None`. - Do not use any additional modules outside of the `platform` module and the standard library. # Example Output Given that the function is executed on a typical Unix-based system with Python 3.10, the output might look like: ```python { \'architecture\': (\'64bit\', \'ELF\'), \'machine\': \'x86_64\', \'node\': \'my-computer\', \'platform\': \'Linux-5.8.0-53-generic-x86_64-with-glibc2.29\', \'processor\': \'x86_64\', \'python_build\': (\'default\', \'May 3 2021 08:15:33\'), \'python_compiler\': \'GCC 9.3.0\', \'python_implementation\': \'CPython\', \'python_version\': \'3.10.0\', \'system\': \'Linux\' } ``` # Notes - Ensure the function handles various platform differences properly and produces a consistent dictionary structure.","solution":"import platform def get_platform_summary() -> dict: summary = { \'architecture\': platform.architecture(), \'machine\': platform.machine() or None, \'node\': platform.node() or None, \'platform\': platform.platform() or None, \'processor\': platform.processor() or None, \'python_build\': platform.python_build() or None, \'python_compiler\': platform.python_compiler() or None, \'python_implementation\': platform.python_implementation() or None, \'python_version\': platform.python_version() or None, \'system\': platform.system() or None } # Return the summary dictionary with all relevant platform details. return summary"},{"question":"Mitigating Floating Point Representation Error **Objective**: Write a Python program to accurately sum a list of floating-point numbers and ensure the precision of the result through the use of appropriate Python tools. **Problem Description**: You are given a list of floating-point numbers. Due to the nature of floating-point arithmetic, directly summing these numbers might lead to precision errors. Your task is to implement a function `accurate_sum(numbers: List[float]) -> float` that computes the sum of the input list accurately using the `math.fsum` method. Additionally, implement a function `exact_representations(numbers: List[float]) -> List[str]` that returns a list of strings representing the exact binary fractions of each number in the input list using the `decimal` module\'s `Decimal` representation. Requirements: 1. **Function 1: accurate_sum** - Input: A list of floating-point numbers `numbers` (1 ≤ len(numbers) ≤ 10^6). - Output: A single floating-point number which is the accurate sum of all elements in the list. 2. **Function 2: exact_representations** - Input: A list of floating-point numbers `numbers` (1 ≤ len(numbers) ≤ 100). - Output: A list of strings where each string represents the exact binary fraction of the corresponding float in the input list. Example: ```python from typing import List import math from decimal import Decimal def accurate_sum(numbers: List[float]) -> float: return math.fsum(numbers) def exact_representations(numbers: List[float]) -> List[str]: return [str(Decimal(num)) for num in numbers] # Example usage numbers = [0.1, 0.2, 0.3] print(accurate_sum(numbers)) # Output: 0.6000000000000001 (exact due to fsum) print(exact_representations(numbers)) # Output: [\'0.1000000000000000055511151231257827021181583404541015625\', \'0.200000000000000011102230246251565404236316680908203125\', \'0.299999999999999988897769753748434595763683319091796875\'] ``` **Constraints**: - The length of the list for `accurate_sum` can be large (up to 10^6), so the implementation should be efficient in terms of both time and space. - For `exact_representations`, the length is limited to 100 to ensure manageability of output size. **Key Points**: - Utilize `math.fsum` for an accurate summation with minimal precision error. - Convert each floating-point number to its exact decimal representation using the `decimal.Decimal` class for clarity on representation errors. **Guidelines**: - Do not use built-in sum for `accurate_sum`. Utilize `math.fsum` to ensure precision. - Use list comprehension for concise code in `exact_representations`.","solution":"from typing import List import math from decimal import Decimal def accurate_sum(numbers: List[float]) -> float: Computes the sum of a list of floating-point numbers accurately using math.fsum. :param numbers: List of floating-point numbers :return: Accurate sum of the list return math.fsum(numbers) def exact_representations(numbers: List[float]) -> List[str]: Returns the exact binary fraction representations of floating-point numbers using Decimal. :param numbers: List of floating-point numbers :return: List of strings representing exact binary fractions return [str(Decimal(num)) for num in numbers]"},{"question":"**Objective:** Implement a function to process a mono audio fragment, convert it to stereo, apply different volume adjustments to each channel, and then calculate the RMS value of the resulting stereo fragment. **Function Signature:** ```python def process_audio_fragment(fragment: bytes, width: int, lfactor: float, rfactor: float) -> float: Process a mono audio fragment, convert it to stereo, apply volume adjustments, and calculate the RMS of the result. Parameters: - fragment (bytes): A mono audio fragment. - width (int): The sample width in bytes (1, 2, 3, or 4). - lfactor (float): The multiplication factor for the left channel. - rfactor (float): The multiplication factor for the right channel. Returns: - float: The RMS value of the resulting stereo audio fragment. ``` **Input Format:** - `fragment`: A bytes-like object representing a mono audio fragment. - `width`: An integer indicating the sample width in bytes (1, 2, 3, or 4). - `lfactor`: A float representing the volume adjustment factor for the left channel. - `rfactor`: A float representing the volume adjustment factor for the right channel. **Output Format:** - A float representing the RMS value of the processed stereo audio fragment. **Constraints:** - The `fragment` must be a non-empty bytes-like object. - The `width` must be one of the valid values: 1, 2, 3, or 4. - Both `lfactor` and `rfactor` should be non-negative floats. # Example: ```python # Example usage fragment = b\'x01x02x03x04x05x06\' width = 2 lfactor = 1.5 rfactor = 0.8 # Expected output is the RMS value of the processed stereo fragment result = process_audio_fragment(fragment, width, lfactor, rfactor) print(result) # Should print a float RMS value ``` # Notes: 1. You will need to use the functions `tostereo`, `mul`, and `rms` from the `audioop` module. 2. Remember to handle potential overflow conditions as specified in the module documentation. 3. Ensure efficiency as processing large audio fragments can be computationally intensive. # Additional Resources: Refer to the `audioop` module documentation for details on the available functions and their usage.","solution":"import audioop def process_audio_fragment(fragment: bytes, width: int, lfactor: float, rfactor: float) -> float: Process a mono audio fragment, convert it to stereo, apply volume adjustments, and calculate the RMS of the result. Parameters: - fragment (bytes): A mono audio fragment. - width (int): The sample width in bytes (1, 2, 3, or 4). - lfactor (float): The multiplication factor for the left channel. - rfactor (float): The multiplication factor for the right channel. Returns: - float: The RMS value of the resulting stereo audio fragment. stereo_fragment = audioop.tostereo(fragment, width, lfactor, rfactor) rms_value = audioop.rms(stereo_fragment, width) return rms_value"},{"question":"**Objective:** Implement a simple cache system using weak references to ensure objects do not stay in memory solely due to cache storage. Additionally, use the finalizer to register a clean-up function that is called when the objects are garbage collected. # Task: Create a `WeakCache` class that uses `weakref.WeakValueDictionary` to store key-value pairs weakly. The class should also include functionality to log a message when a cached object is about to be garbage-collected. This log message should be implemented using the `weakref.finalize` method. Requirements: 1. **Class Structure**: - `WeakCache`: Main class to implement the cache system. 2. **Methods to implement**: - `__init__(self)`: Initialize an empty `WeakValueDictionary`. - `add(self, key, value)`: Add a key-value pair to the cache. Use the `weakref.finalize` method to register a message to be logged when the `value` is garbage-collected. - `get(self, key)`: Retrieve the value associated with the key from the cache. If the key does not exist, return `None`. - `log(self, message)`: Method to log messages, representing a simple print statement. Example Usage: ```python cache = WeakCache() # Create an object to cache class Data: pass obj = Data() cache.add(\\"data1\\", obj) # Retrieve the object assert cache.get(\\"data1\\") is obj # Delete the original reference del obj import gc gc.collect() # Manually trigger garbage collection # The object should not be in the cache anymore assert cache.get(\\"data1\\") is None # The log should show that the object was garbage collected ``` Constraints: 1. Keys are assumed to be strings. 2. Values can be any object. 3. The garbage collection log should only display once per object.","solution":"import weakref class WeakCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add(self, key, value): self._cache[key] = value weakref.finalize(value, self.log, f\\"Object with key \'{key}\' has been garbage collected\\") def get(self, key): return self._cache.get(key) def log(self, message): print(message)"},{"question":"You are tasked with creating a regression test suite for a module named `math_operations.py`. This module includes a function `safe_divide(a, b)`, which divides `a` by `b` but returns `None` if `b` is zero. Your task is to use the `unittest` framework and utilities from the `test.support` module to write comprehensive tests for `safe_divide`. 1. Implement the `safe_divide(a, b)` function as specified: ```python # math_operations.py def safe_divide(a, b): if b == 0: return None return a / b ``` 2. Write a test suite `test_math_operations.py` using the following guidelines: - Use `unittest` to create test cases. - Import and utilize relevant utilities from `test.support` to handle different test conditions, as appropriate. - Ensure the following test cases: - Test normal division operations (e.g., `safe_divide(10, 2)`, `safe_divide(-6, 3)`). - Test division by zero, ensuring that it returns `None`. - Capture stdout/stderr to ensure no output occurs during the execution of `safe_divide()`. - Ensure the function handles invalid input types gracefully (e.g., when passed strings). Expected input and output formats: - **Input:** Two integers or floats to be divided. - **Output:** Float result of the division if denominator is non-zero, otherwise `None`. Constraints: - You must use the `test.support` utilities to manage test environments and outputs. Below is a boilerplate structure for `test_math_operations.py`: ```python import unittest from test import support from math_operations import safe_divide class TestMathOperations(unittest.TestCase): def test_normal_division(self): # Test typical division scenarios self.assertEqual(safe_divide(10, 2), 5) self.assertEqual(safe_divide(-6, 3), -2) def test_division_by_zero(self): # Test safely dividing by zero self.assertIsNone(safe_divide(10, 0)) def test_invalid_inputs(self): # Use appropriate utils from test.support to manage invalid inputs with support.captured_stdout() as stdout, support.captured_stderr() as stderr: with self.assertRaises(TypeError): safe_divide(\'10\', 2) with self.assertRaises(TypeError): safe_divide(10, \'2\') # Ensure no output occurs during these tests self.assertEqual(stdout.getvalue(), \'\') self.assertEqual(stderr.getvalue(), \'\') # Add any additional test cases as needed if __name__ == \'__main__\': unittest.main() ```","solution":"def safe_divide(a, b): if b == 0: return None return a / b"},{"question":"# Custom \\"MutableSequence\\" Implementation Objective: Implement a custom class `CustomList` that mimics a mutable sequence (like Python\'s built-in list) using the `collections.abc.MutableSequence` abstract base class. Your implementation should demonstrate a sound understanding of the required abstract methods and should leverage mixin methods from the `MutableSequence` ABC. Requirements: - The custom `CustomList` class must inherit from `collections.abc.MutableSequence`. - Implement the following abstract methods in your class: - `__getitem__(self, index)`: Retrieve an item by its index. - `__setitem__(self, index, value)`: Set the item at a given index. - `__delitem__(self, index)`: Delete the item at a given index. - `__len__(self)`: Return the number of items in the list. - `insert(self, index, value)`: Insert an item at a given index. - The class should also support the following behaviors: - `append(value)`: Add an item to the end of the list. - `remove(value)`: Remove the first occurrence of the value from the list. - `pop(index=-1)`: Remove and return the item at the given index (default is the last item). - `__contains__(self, value)`: Return `True` if the value is in the list, otherwise return `False`. - `__iter__(self)`: Return an iterator over the list. Constraints: - Do not use any built-in list methods (e.g., `list.append()`, `list.remove()`) in your implementation. Implement the required functionalities from scratch. - All methods should operate in O(n) time complexity, where n is the number of items in the list. Input: - There are no specific input constraints for the class methods. However, your class should handle typical list operations. Output: - Ensure your class methods return appropriate responses (where necessary) as per standard list behaviors. Example Usage: ```python from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._items = [] def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): self._items.insert(index, value) def append(self, value): self._items.append(value) def remove(self, value): self._items.remove(value) def pop(self, index=-1): return self._items.pop(index) def __contains__(self, value): return value in self._items # Example usage: my_list = CustomList() my_list.append(1) my_list.append(2) my_list.append(3) print(len(my_list)) # Output: 3 print(my_list[1]) # Output: 2 my_list[1] = 20 print(my_list[1]) # Output: 20 my_list.remove(20) print(len(my_list)) # Output: 2 print(list(iter(my_list))) # Output: [1, 3] ``` Implement the `CustomList` class so that it matches the example usage and satisfies all the requirements.","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self): self._items = [] def __getitem__(self, index): return self._items[index] def __setitem__(self, index, value): self._items[index] = value def __delitem__(self, index): del self._items[index] def __len__(self): return len(self._items) def insert(self, index, value): self._items[index:index] = [value] # These methods are not required by MutableSequence but are commonly used def append(self, value): self.insert(len(self._items), value) def remove(self, value): index = self._items.index(value) self.__delitem__(index) def pop(self, index=-1): value = self._items[index] self.__delitem__(index) return value def __contains__(self, value): return value in self._items def __iter__(self): return iter(self._items)"},{"question":"Objective Write a Python function that calculates the sum of CPU time taken by two threads executing a specified task, using the `time` module. The task and the number of iterations the task should perform will be provided as arguments to your function. Ensure the use of thread-specific CPU-time clocks to calculate the precise CPU time consumed by each thread. Function Signature ```python def calculate_thread_cpu_time(task: callable, iterations: int) -> dict: Calculates the sum of CPU time taken by two threads executing the given task. Args: - task (callable): A function that performs a specific task. - iterations (int): The number of times the task should be performed within each thread. Returns: - dict: A dictionary with the CPU time (in seconds) taken by each thread and their sum. Format: {\\"thread_1_time\\": float, \\"thread_2_time\\": float, \\"total_cpu_time\\": float} ``` Details 1. **Input:** - A callable `task` function that performs a specific task. - An integer `iterations` indicating the number of times each thread should perform the task. 2. **Output:** - A dictionary containing: - `\\"thread_1_time\\"`: The CPU time in seconds taken by the first thread to complete the task. - `\\"thread_2_time\\"`: The CPU time in seconds taken by the second thread to complete the task. - `\\"total_cpu_time\\"`: The sum of the CPU times of both threads. 3. **Constraints:** - The function should use the `time.thread_time()` function to measure the CPU time. - Ensure thread safety. - The task function provided will not perform I/O operations and will be CPU-bound. 4. **Example Usage:** ```python def sample_task(): for _ in range(10000): pass result = calculate_thread_cpu_time(sample_task, 100000) print(result) # Output might be something like: # {\\"thread_1_time\\": 0.001234, \\"thread_2_time\\": 0.001456, \\"total_cpu_time\\": 0.002690} ``` Hints - Use the `threading` module to create and manage threads. - Use `time.thread_time()` at the beginning and end of the task execution to measure the CPU time precisely. - Ensure that the CPU times are recorded in a non-blocking manner for accurate measurement. This exercise will test your understanding of threading, precise time measurements, and the use of Python’s `time` module in a multi-threaded environment.","solution":"import threading import time def calculate_thread_cpu_time(task: callable, iterations: int) -> dict: def run_task(): for _ in range(iterations): task() # Measure thread 1\'s CPU time t1_start_time = time.thread_time() thread1 = threading.Thread(target=run_task) thread1.start() thread1.join() t1_end_time = time.thread_time() thread1_cpu_time = t1_end_time - t1_start_time # Measure thread 2\'s CPU time t2_start_time = time.thread_time() thread2 = threading.Thread(target=run_task) thread2.start() thread2.join() t2_end_time = time.thread_time() thread2_cpu_time = t2_end_time - t2_start_time total_cpu_time = thread1_cpu_time + thread2_cpu_time return { \\"thread_1_time\\": thread1_cpu_time, \\"thread_2_time\\": thread2_cpu_time, \\"total_cpu_time\\": total_cpu_time }"},{"question":"# Question: Implementing a Custom Collection with Multiple Abstract Base Classes Using the `collections.abc` module, create a custom class `CustomQueue` that represents a queue with the following properties: 1. It behaves like a sequence with queue semantics (FIFO - First In, First Out). 2. Inherits from the `collections.abc.Sequence` and `collections.abc.MutableSequence`. Your implementation should: - Implement all required abstract methods. - Optionally override any mixin methods if necessary for performance reasons. - Ensure that the `CustomQueue` can add elements to the end, remove elements from the front, and provide indexing. **Requirements**: - Implement the following methods: - `__init__(self)`: Initialize the queue. - `__len__(self)`: Return the number of elements in the queue. - `__getitem__(self, index)`: Return the item at the given index. - `__setitem__(self, index, value)`: Set the item at the given index. - `__delitem__(self, index)`: Delete the item at the given index. - `insert(self, index, value)`: Insert a value at the given index. - Ensure the queue maintains the FIFO order. **Constraints**: - You may use Python\'s built-in list to store elements internally. - Indexing should be zero-based. - Provide a clear and maintainable code structure. **Examples**: ```python q = CustomQueue() q.insert(0, \'first\') q.insert(1, \'second\') q.insert(2, \'third\') print(len(q)) # Output: 3 print(q[0]) # Output: \'first\' q[1] = \'SECOND\' print(q[1]) # Output: \'SECOND\' del q[0] print(q[0]) # Output: \'SECOND\' ``` # Implementation Define your `CustomQueue` class below: ```python from collections.abc import MutableSequence class CustomQueue(MutableSequence): def __init__(self): self._data = [] def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def insert(self, index, value): self._data.insert(index, value) # Example usage: q = CustomQueue() q.insert(0, \'first\') q.insert(1, \'second\') q.insert(2, \'third\') print(len(q)) # Output: 3 print(q[0]) # Output: \'first\' q[1] = \'SECOND\' print(q[1]) # Output: \'SECOND\' del q[0] print(q[0]) # Output: \'SECOND\' ```","solution":"from collections.abc import MutableSequence class CustomQueue(MutableSequence): def __init__(self): self._data = [] def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def insert(self, index, value): self._data.insert(index, value) # Example usage: q = CustomQueue() q.insert(0, \'first\') q.insert(1, \'second\') q.insert(2, \'third\') print(len(q)) # Output: 3 print(q[0]) # Output: \'first\' q[1] = \'SECOND\' print(q[1]) # Output: \'SECOND\' del q[0] print(q[0]) # Output: \'SECOND\'"},{"question":"# Python Coding Challenge In this exercise, you will utilize the `copyreg` module to customize the pickling behavior for a class that contains special properties. You must define custom behavior to ensure that objects of this class can be correctly pickled and unpickled. Problem Statement You need to implement a class `Person` that stores information about a person, including their `name`, `age`, `email`, and a list of `friends`. Additionally, you need to customize the pickling process for the `Person` class to handle the fact that their `email` should not be pickled, and friends should be represented by their names during pickling to avoid circular references. Requirements: 1. Implement the `Person` class with methods: - `__init__(self, name, age, email, friends)`: Initializes the person\'s attributes. - `add_friend(self, friend)`: Adds a friend to the person\'s list of friends. 2. Implement a custom pickling function and register it using `copyreg.pickle`. 3. Ensure that when pickling a `Person` object: - The `email` attribute is not pickled. - The `friends` attribute (a list of `Person` objects) is pickled as a list of their names. 4. Create a unit test to demonstrate the pickling and unpickling process using your custom function. Input and Output Formats: - **Input:** - `name` (string): name of the person. - `age` (int): age of the person. - `email` (string): email of the person. - `friends` (list of `Person`): friends of the person. - **Output:** - The `Person` object should be correctly pickled without the `email` attribute and with friends\' names instead of objects. - Unpickling should successfully recreate the original object (except the email attribute). Example ```python import copyreg import pickle class Person: def __init__(self, name, age, email, friends=None): self.name = name self.age = age self.email = email self.friends = friends if friends else [] def add_friend(self, friend): self.friends.append(friend) def pickle_person(person): return ( Person, (person.name, person.age, None, [friend.name for friend in person.friends]) ) # Register the custom pickling function copyreg.pickle(Person, pickle_person) # Create sample persons and friendships alice = Person(\'Alice\', 30, \'alice@example.com\') bob = Person(\'Bob\', 32, \'bob@example.com\') alice.add_friend(bob) # Pickle the object pickled_alice = pickle.dumps(alice) # Unpickle the object unpickled_alice = pickle.loads(pickled_alice) print(unpickled_alice.name) # Output: Alice print(unpickled_alice.age) # Output: 30 print(unpickled_alice.email) # Output: None print([friend for friend in unpickled_alice.friends]) # Output: [\'Bob\'] ``` *Note: Unit tests should be written to automate and validate the pickling and unpickling process.*","solution":"import copyreg import pickle class Person: def __init__(self, name, age, email, friends=None): self.name = name self.age = age self.email = email self.friends = friends if friends else [] def add_friend(self, friend): self.friends.append(friend) def pickle_person(person): return ( Person, (person.name, person.age, None, [friend.name for friend in person.friends]) ) # Register the custom pickling function copyreg.pickle(Person, pickle_person)"},{"question":"# Question: Advanced Number Analysis and Statistics You need to design a Python program that processes a given dataset of numbers, performs various mathematical operations, and outputs specific statistical data. The functions you will need to implement should utilize the capabilities of the `math`, `decimal`, `random`, and `statistics` modules. Required Functions: 1. **generate_random_numbers(n, seed_value)**: - Generate `n` pseudo-random floating-point numbers between 0 and 1. - Use `seed_value` to initialize the random number generator for reproducibility. - Return the list of generated numbers. 2. **convert_to_decimals(numbers, precision)**: - Convert the given list of floating-point `numbers` to decimal numbers. - The `precision` parameter will determine the number of decimal places. - Return the list of decimal numbers. 3. **calculate_basic_stats(decimal_numbers)**: - Calculate and return the mean, median, variance, and standard deviation of the given list of `decimal_numbers`. 4. **perform_advanced_math_operations(decimal_numbers)**: - For each decimal number, compute the square root, natural logarithm, and hyperbolic cosine. - Return a dictionary where each key is the original decimal number and the value is another dictionary containing the results of these operations. Function Specifications: 1. **generate_random_numbers(n: int, seed_value: int) -> List[float]** - Input: `n` (number of random numbers to generate), `seed_value` (seed for random number generator) - Output: List of `n` floating-point numbers 2. **convert_to_decimals(numbers: List[float], precision: int) -> List[Decimal]** - Input: `numbers` (list of floating-point numbers), `precision` (number of decimal places for conversion) - Output: List of decimal numbers 3. **calculate_basic_stats(decimal_numbers: List[Decimal]) -> Dict[str, float]** - Input: `decimal_numbers` (list of decimal numbers) - Output: Dictionary with keys `mean`, `median`, `variance`, and `standard_deviation` and their respective float values 4. **perform_advanced_math_operations(decimal_numbers: List[Decimal]) -> Dict[Decimal, Dict[str, float]]** - Input: `decimal_numbers` (list of decimal numbers) - Output: Dictionary where keys are the input decimal numbers, and values are dictionaries containing keys `sqrt`, `log`, `cosh` with corresponding computed float values Performance Requirements: - The solution must handle large data efficiently. - Precision and correctness are critical, especially for statistical and mathematical computations. Example: ```python from decimal import Decimal from typing import List, Dict # Example usage random_numbers = generate_random_numbers(5, 42) decimals = convert_to_decimals(random_numbers, 10) stats = calculate_basic_stats(decimals) advanced_math_results = perform_advanced_math_operations(decimals) print(f\\"Generated Random Numbers: {random_numbers}\\") print(f\\"Converted to Decimals: {decimals}\\") print(f\\"Statistics: {stats}\\") print(f\\"Advanced Math Results: {advanced_math_results}\\") ``` Constraints: - You must use the `math`, `decimal`, `random`, and `statistics` modules from Python 3.10. - Handle any exceptions that may arise from mathematical operations. Complete the definitions of the functions as described for a comprehensive analysis and testing framework.","solution":"import math import random from decimal import Decimal, getcontext from typing import List, Dict import statistics def generate_random_numbers(n: int, seed_value: int) -> List[float]: random.seed(seed_value) return [random.random() for _ in range(n)] def convert_to_decimals(numbers: List[float], precision: int) -> List[Decimal]: getcontext().prec = precision return [Decimal(str(num)) for num in numbers] def calculate_basic_stats(decimal_numbers: List[Decimal]) -> Dict[str, float]: numbers = [float(num) for num in decimal_numbers] return { \'mean\': float(statistics.mean(numbers)), \'median\': float(statistics.median(numbers)), \'variance\': float(statistics.variance(numbers)), \'standard_deviation\': float(statistics.stdev(numbers)) } def perform_advanced_math_operations(decimal_numbers: List[Decimal]) -> Dict[Decimal, Dict[str, float]]: results = {} for number in decimal_numbers: num_float = float(number) try: results[number] = { \'sqrt\': math.sqrt(num_float), \'log\': math.log(num_float), \'cosh\': math.cosh(num_float) } except ValueError as e: results[number] = { \'sqrt\': float(\'nan\') if num_float < 0 else math.sqrt(num_float), \'log\': float(\'nan\') if num_float <= 0 else math.log(num_float), \'cosh\': math.cosh(num_float) } return results"},{"question":"Coding Assessment Question # Objective Implement a Python function that profiles the execution of another function and analyzes the performance data to generate a specific report. # Task You are required to implement a function `profile_function_and_generate_report(target_func, target_args, sort_key=\'cumulative\', num_lines=10)` that performs the following steps: 1. **Profiles** the execution of the `target_func` function with the arguments `target_args` using `cProfile`. 2. **Generates** a report using `pstats` module that includes: - Function name - Total calls - Total time spent in the function - Cumulative time spent in the function 3. **Sorts** the report based on the `sort_key` parameter, which can be `\'calls\'`, `\'time\'`, or `\'cumulative\'`. 4. **Prints** the top `num_lines` entries of the report. # Function Signature ```python def profile_function_and_generate_report( target_func: Callable, target_args: tuple, sort_key: str = \'cumulative\', num_lines: int = 10 ) -> None: ``` # Parameters - `target_func` (Callable): The function to be profiled. - `target_args` (tuple): A tuple of arguments to pass to the `target_func`. - `sort_key` (str, optional): The key to sort the statistics by. Default is `\'cumulative\'`. Other options are `\'calls\'` and `\'time\'`. - `num_lines` (int, optional): The number of lines to display in the report. Default is 10. # Example ```python import time def example_function(n): total = 0 for i in range(n): time.sleep(0.01) total += i return total profile_function_and_generate_report(example_function, (100,), sort_key=\'time\', num_lines=5) ``` # Expected Output ``` 101 function calls in 1.056 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 100 1.000 0.010 1.000 0.010 <string>:2(example_function) 1 0.056 0.056 1.056 1.056 <string>:5(<module>) ``` # Constraints - The `target_func` function should execute and return normally for profiling to work. - Handle exceptions and ensure the profiling results are printed even if the `target_func` fails. # Notes - Use `cProfile` to profile the function call. - Use `pstats` to analyze and generate the desired report. - Ensure the output is well-formatted and easy to read.","solution":"import cProfile import pstats from typing import Callable def profile_function_and_generate_report( target_func: Callable, target_args: tuple, sort_key: str = \'cumulative\', num_lines: int = 10 ) -> None: # Start the profiler profiler = cProfile.Profile() try: # Profile the target function with the given arguments profiler.enable() target_func(*target_args) profiler.disable() # Create a Stats object from the profiler data stats = pstats.Stats(profiler) # Sort the stats based on the sort_key stats.sort_stats(sort_key) # Print out the top `num_lines` entries stats.print_stats(num_lines) except Exception as e: print(f\\"Error occurred during profiling: {e}\\") raise # Example usage if __name__ == \\"__main__\\": import time def example_function(n): total = 0 for i in range(n): time.sleep(0.01) total += i return total profile_function_and_generate_report(example_function, (100,), sort_key=\'time\', num_lines=5)"},{"question":"**Objective:** In this assessment, you will demonstrate your understanding of target label preprocessing using scikit-learn\'s `LabelBinarizer`, `MultiLabelBinarizer`, and `LabelEncoder`. **Problem Statement:** 1. Given a list of multiclass labels, transform them into a label indicator matrix using `LabelBinarizer`. 2. Given a collection of collections of labels, convert them to a binary indicator matrix using `MultiLabelBinarizer`. 3. Given a list of categorical labels, encode them into numerical labels and subsequently decode them back to their original form using `LabelEncoder`. **Function Signatures:** ```python from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def apply_label_binarizer(labels): Transforms a list of multiclass labels into a label indicator matrix Parameters: labels (list): A list of multiclass labels Returns: numpy.ndarray: A binary indicator array list: List of classes pass def apply_multilabel_binarizer(labels): Converts a collection of collections of labels into a binary indicator matrix Parameters: labels (list of list or set): A list of collections of labels Returns: numpy.ndarray: A binary indicator array list: List of classes pass def apply_label_encoder(labels): Encodes a list of categorical labels into numerical labels and decodes them back to their original form Parameters: labels (list): A list of categorical labels Returns: tuple: A tuple containing: - numpy.ndarray: Encoded numerical labels - list: List of original labels decoded back from numerical labels - list: List of classes pass ``` **Constraints:** - You can assume that the input labels are valid and do not contain missing values. - The list of categorical labels for `apply_label_encoder` are hashable and comparable (i.e., you may have strings, numbers, etc.). **Examples:** ```python # Example for apply_label_binarizer labels = [1, 2, 6, 4, 2] binary_matrix, classes = apply_label_binarizer(labels) # Output: # binary_matrix = array([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 1], # [0, 0, 1, 0], # [0, 1, 0, 0]]) # classes = [1, 2, 4, 6] # Example for apply_multilabel_binarizer labels = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]] binary_matrix, classes = apply_multilabel_binarizer(labels) # Output: # binary_matrix = array([[0, 0, 1, 1, 1], # [0, 0, 1, 0, 0], # [1, 1, 0, 1, 0], # [1, 1, 1, 1, 1], # [1, 1, 1, 0, 0]]) # classes = [0, 1, 2, 3, 4] # Example for apply_label_encoder labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] encoded_labels, decoded_labels, classes = apply_label_encoder(labels) # Output: # encoded_labels = array([1, 1, 2, 0]) # decoded_labels = [\\"paris\\", \\"paris\\", \\"tokyo\\", \\"amsterdam\\"] # classes = [\\"amsterdam\\", \\"paris\\", \\"tokyo\\"] ``` Write your implementation for the three functions, making sure to handle the input data as specified and return the expected outputs.","solution":"from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer, LabelEncoder def apply_label_binarizer(labels): Transforms a list of multiclass labels into a label indicator matrix Parameters: labels (list): A list of multiclass labels Returns: numpy.ndarray: A binary indicator array list: List of classes lb = LabelBinarizer() binary_matrix = lb.fit_transform(labels) classes = lb.classes_.tolist() return binary_matrix, classes def apply_multilabel_binarizer(labels): Converts a collection of collections of labels into a binary indicator matrix Parameters: labels (list of list or set): A list of collections of labels Returns: numpy.ndarray: A binary indicator array list: List of classes mlb = MultiLabelBinarizer() binary_matrix = mlb.fit_transform(labels) classes = mlb.classes_.tolist() return binary_matrix, classes def apply_label_encoder(labels): Encodes a list of categorical labels into numerical labels and decodes them back to their original form Parameters: labels (list): A list of categorical labels Returns: tuple: A tuple containing: - numpy.ndarray: Encoded numerical labels - list: List of original labels decoded back from numerical labels - list: List of classes le = LabelEncoder() encoded_labels = le.fit_transform(labels) decoded_labels = le.inverse_transform(encoded_labels).tolist() classes = le.classes_.tolist() return encoded_labels, decoded_labels, classes"},{"question":"# Advanced Python Object Handling **Objective:** Write a Python function that manipulates various types of Python objects, showcasing your understanding of different data types, type checking, and advanced data manipulation. **Problem Statement:** Implement a function `process_data(data)`, where `data` is expected to be a complex, nested structure containing different Python objects such as lists, tuples, dictionaries, sets, bytes, byte arrays, and other sequence or container objects. The function should perform the following operations: 1. **Type Check and Conversion**: - If an element is a `bytes` object, convert it to a string (`str`). - If an element is a `bytearray` object, convert it to a list of integers. 2. **Data Reorganization**: - If an element is a dictionary, extract its keys and values and store them as a tuple of two lists: (keys, values). - If an element is a set, convert it to a sorted list. 3. **Sequence Handling**: - Flatten any nested lists or tuples. For example, `[1, [2, [3, 4]]]` should become `[1, 2, 3, 4]`. 4. **Numeric Object Handling**: - If an element is an `int`, `float`, or `complex` number, keep it the same. The function should return a list of processed elements in the same order they appeared in the input structure. **Input Format:** - A complex nested structure (`data`) containing various Python objects as described. **Output Format:** - A list of processed elements according to the rules specified above. **Constraints:** - The nested structure `data` may have arbitrary depth and may contain a mix of different object types. - You may assume all numeric values and sequences are not empty. **Example:** ```python def process_data(data): # Your implementation here # Example usage: data = [ b\'hello\', bytearray([65, 66, 67]), {\'one\': 1, \'two\': 2, \'three\': 3}, {5, 3, 1, 2, 4}, [1, [2, 3], [4, [5, 6]]], (7, 8), 9.5, complex(2, 3) ] print(process_data(data)) ``` Expected output: ```python [\'hello\', [65, 66, 67], ([\'one\', \'two\', \'three\'], [1, 2, 3]), [1, 2, 3, 4, 5], [1, 2, 3, 4, 5, 6], 7, 8, 9.5, (2+3j)] ``` # Notes: - Make sure your solution handles deep nested structures efficiently. - You can use Python\'s built-in type checking functions such as `isinstance()` for your implementations.","solution":"def process_data(data): def flatten(sequence): Helper function to flatten nested lists or tuples. for item in sequence: if isinstance(item, (list, tuple)): yield from flatten(item) else: yield item processed_elements = [] for element in data: if isinstance(element, bytes): processed_elements.append(element.decode()) elif isinstance(element, bytearray): processed_elements.append(list(element)) elif isinstance(element, dict): processed_elements.append((list(element.keys()), list(element.values()))) elif isinstance(element, set): processed_elements.append(sorted(element)) elif isinstance(element, (list, tuple)): processed_elements.extend(flatten(element)) elif isinstance(element, (int, float, complex)): processed_elements.append(element) else: # If encountering an unsupported type, append None as a placeholder processed_elements.append(None) return processed_elements"},{"question":"**Objective**: The goal of this task is to assess your understanding of PyTorch\'s autograd functionalities, particularly in computing derivatives such as gradients, Jacobians, and Hessians using the `torch.autograd.functional` API. # Problem Statement Implement a function `compute_derivatives` that takes in two arguments: 1. A PyTorch model (as a `torch.nn.Module`) with parameters that require gradients. 2. A single batch input tensor `x` of shape `(batch_size, input_dim)` which should be fed to the model. Your function should compute and return: 1. The gradient of the model\'s output with respect to the input `x`. 2. The Jacobian matrix of the model\'s output with respect to the input `x`. 3. The Hessian matrix of the model\'s output with respect to the input `x`. # Function Signature ```python import torch import torch.nn as nn def compute_derivatives(model: nn.Module, x: torch.Tensor) -> dict: Args: model (nn.Module): A PyTorch model with parameters that require gradients. x (torch.Tensor): A batch input tensor of shape (batch_size, input_dim). Returns: dict: A dictionary containing the following keys and their corresponding values: \'grad\': Gradient of the model\'s output with respect to input x. \'jacobian\': Jacobian matrix of the model\'s output with respect to input x. \'hessian\': Hessian matrix of the model\'s output with respect to input x. pass # Example usage class SimpleModel(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.fc2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.fc1(x)) return self.fc2(x) model = SimpleModel(input_dim=3, hidden_dim=5, output_dim=2) x = torch.randn(4, 3) # Batch of size 4 with input dimension 3 # You should call this function result = compute_derivatives(model, x) print(result[\'grad\']) print(result[\'jacobian\']) print(result[\'hessian\']) ``` # Requirements 1. **Gradient**: - Compute the gradient of the model\'s output with respect to the input `x`. - For this, use PyTorch\'s automatic differentiation capabilities provided by `torch.autograd.backward` or `torch.autograd.grad`. 2. **Jacobian**: - Compute the Jacobian matrix of the model\'s output with respect to the input `x`. - Use `torch.autograd.functional.jacobian` for this purpose. 3. **Hessian**: - Compute the Hessian matrix of the model\'s output with respect to the input `x`. - Utilize `torch.autograd.functional.hessian` to achieve this. # Constraints - The model and input should be compatible in terms of dimensions. - Ensure that `requires_grad` is enabled for the input tensor. - Handle the batching appropriately when computing derivatives. # Performance Requirements - Your implementation should efficiently handle the computation of gradients, Jacobians, and Hessians for reasonably sized models and inputs. # Additional Notes - You should utilize the capabilities of PyTorch\'s autograd module wherever applicable. - Ensure that your code is well-structured and properly commented to explain key parts of the implementation.","solution":"import torch import torch.nn as nn from torch.autograd.functional import jacobian, hessian def compute_derivatives(model: nn.Module, x: torch.Tensor) -> dict: Args: model (nn.Module): A PyTorch model with parameters that require gradients. x (torch.Tensor): A batch input tensor of shape (batch_size, input_dim). Returns: dict: A dictionary containing the following keys and their corresponding values: \'grad\': Gradient of the model\'s output with respect to input x. \'jacobian\': Jacobian matrix of the model\'s output with respect to input x. \'hessian\': Hessian matrix of the model\'s output with respect to input x. x.requires_grad_(True) def model_output_sum(y): return model(y).sum() # Compute the gradient output_sum = model_output_sum(x) output_sum.backward() grad = x.grad # Compute the Jacobian jacobian_matrix = jacobian(model, x) # Compute the Hessian hessian_matrix = hessian(model_output_sum, x) return { \'grad\': grad, \'jacobian\': jacobian_matrix, \'hessian\': hessian_matrix }"},{"question":"# Python Coding Assessment **Problem Statement: Custom Exception Logger** You are required to implement a custom exception logger function in Python using the `traceback` module. This function, `log_exception_details`, should capture detailed information about an exception, format it, and return it as a string. The function must handle exceptions raised in a specified function call and include all relevant traceback information. Function Signature: ```python def log_exception_details(func: callable, *args, **kwargs) -> str: pass ``` Parameters: - `func` (callable): The function to be called that might raise an exception. - `*args`: Variable length argument list to be passed to the function. - `**kwargs`: Arbitrary keyword arguments to be passed to the function. Returns: - `str`: A formatted string containing detailed information about any exception that occurs, including its traceback. Requirements: 1. Inside your function, call the provided `func` with the given `args` and `kwargs`. 2. Capture any exception that is raised during this function call. 3. Use the `traceback` module to extract the traceback details. 4. Format the extracted traceback details into a single string. Your output should resemble the format of `traceback.format_exc()`. 5. Return the formatted string. Example Usage: ```python def my_function(x, y): return x / y try: result = my_function(10, 0) except Exception as e: print(log_exception_details(my_function, 10, 0)) ``` **Example Output:** ``` Traceback (most recent call last): File \\"script.py\\", line 12, in <module> result = my_function(10, 0) File \\"script.py\\", line 4, in my_function return x / y ZeroDivisionError: division by zero ``` **Constraints:** - You should handle all kinds of exceptions. - Do not use any external libraries other than `traceback` and built-in Python modules. - The function should have a linear runtime complexity with respect to the traceback size. **Important Note:** - Ensure your solution is efficient and handles large tracebacks effectively. - Your implementation should closely mimic the behavior of built-in error handling and traceback formatting functions provided by Python.","solution":"import traceback def log_exception_details(func, *args, **kwargs): Captures detailed information about an exception, formats it, and returns it as a string. Parameters: - func (callable): The function to be called that might raise an exception. - *args: Variable length argument list to be passed to the function. - **kwargs: Arbitrary keyword arguments to be passed to the function. Returns: - str: A formatted string containing detailed information about any exception that occurs, including its traceback. try: func(*args, **kwargs) except Exception as e: return traceback.format_exc() def my_function(x, y): return x / y"},{"question":"**Objective:** The goal of this assessment is to evaluate your understanding of pandas sparse data structures and their interaction with scipy sparse matrices. You will be required to implement functions that create and manipulate pandas SparseArray and SparseDataFrame objects, as well as convert between sparse and dense formats. **Problem Statement:** You are given a dataset of sensor readings from a large grid of sensors. The dataset is mostly sparse, with many missing or zero values representing inactive sensors. Perform the following tasks to process this data efficiently using pandas sparse data structures. 1. **Generate Sparse DataFrame:** - Implement a function `generate_sparse_dataframe(data: np.ndarray, fill_value: float) -> pd.DataFrame` that takes: - `data`: A 2D numpy array where each element represents a sensor reading. - `fill_value`: A scalar value that should be considered as the \\"sparse\\" value (e.g., `np.nan` or `0`). - The function should convert the numpy array into a pandas SparseDataFrame with the specified fill value. 2. **Analyze Sparse Density:** - Implement a function `sparse_density(sdf: pd.DataFrame) -> float` that takes a pandas SparseDataFrame and returns the density of the sparse dataframe (i.e., the percentage of non-fill values). 3. **Convert to SciPy Sparse Matrix:** - Implement a function `to_scipy_sparse(sdf: pd.DataFrame) -> csr_matrix` that takes a pandas SparseDataFrame and converts it to a scipy sparse CSR matrix. 4. **Back to Sparse DataFrame:** - Implement a function `from_scipy_sparse(sp_matrix: csr_matrix) -> pd.DataFrame` that takes a scipy sparse CSR matrix and converts it back to a pandas SparseDataFrame. 5. **Conversion to Dense DataFrame:** - Implement a function `to_dense_dataframe(sdf: pd.DataFrame) -> pd.DataFrame` that takes a pandas SparseDataFrame and converts it to a regular (dense) pandas DataFrame. **Constraints:** - Assume that the input data for `generate_sparse_dataframe` will be a large 2D numpy array with dimensions up to `10000x10000`. - The functions should handle the memory efficiently by leveraging sparse data structures. **Function Signatures:** ```python import pandas as pd import numpy as np from scipy.sparse import csr_matrix def generate_sparse_dataframe(data: np.ndarray, fill_value: float) -> pd.DataFrame: pass def sparse_density(sdf: pd.DataFrame) -> float: pass def to_scipy_sparse(sdf: pd.DataFrame) -> csr_matrix: pass def from_scipy_sparse(sp_matrix: csr_matrix) -> pd.DataFrame: pass def to_dense_dataframe(sdf: pd.DataFrame) -> pd.DataFrame: pass ``` **Example Usage:** ```python data = np.array([[1, 0, 0], [0, 2, 0], [0, 0, 3]]) fill_val = 0 sdf = generate_sparse_dataframe(data, fill_val) print(sdf) density = sparse_density(sdf) print(density) # Output should be the density of the sparse dataframe sp_matrix = to_scipy_sparse(sdf) print(sp_matrix) reconverted_sdf = from_scipy_sparse(sp_matrix) print(reconverted_sdf) dense_df = to_dense_dataframe(sdf) print(dense_df) ``` **Note:** Ensure that the functions are optimized for performance, considering the large size of the data.","solution":"import pandas as pd import numpy as np from scipy.sparse import csr_matrix def generate_sparse_dataframe(data: np.ndarray, fill_value: float) -> pd.DataFrame: Converts a 2D numpy array into a pandas SparseDataFrame with the specified fill value. Parameters: - data (np.ndarray): A 2D numpy array of sensor readings. - fill_value (float): The fill value to be considered as sparse (e.g. np.nan or 0). Returns: - pd.DataFrame: A pandas SparseDataFrame with the specified fill value. sparse_df = pd.DataFrame.sparse.from_spmatrix( csr_matrix(data), columns=[f\'col{i}\' for i in range(data.shape[1])], index=[f\'row{i}\' for i in range(data.shape[0])] ) sparse_df = sparse_df.astype(pd.SparseDtype(float, fill_value)) return sparse_df def sparse_density(sdf: pd.DataFrame) -> float: Calculates the density of a pandas SparseDataFrame (percentage of non-fill values). Parameters: - sdf (pd.DataFrame): A pandas SparseDataFrame. Returns: - float: The density of the sparse dataframe. total_elements = sdf.shape[0] * sdf.shape[1] non_fill_values = sdf.sparse.density * total_elements return non_fill_values / total_elements def to_scipy_sparse(sdf: pd.DataFrame) -> csr_matrix: Converts a pandas SparseDataFrame to a scipy sparse CSR matrix. Parameters: - sdf (pd.DataFrame): A pandas SparseDataFrame. Returns: - csr_matrix: A scipy sparse CSR matrix. return csr_matrix(sdf.sparse.to_coo()) def from_scipy_sparse(sp_matrix: csr_matrix) -> pd.DataFrame: Converts a scipy sparse CSR matrix to a pandas SparseDataFrame. Parameters: - sp_matrix (csr_matrix): A scipy sparse CSR matrix. Returns: - pd.DataFrame: A pandas SparseDataFrame. return pd.DataFrame.sparse.from_spmatrix(sp_matrix) def to_dense_dataframe(sdf: pd.DataFrame) -> pd.DataFrame: Converts a pandas SparseDataFrame to a dense pandas DataFrame. Parameters: - sdf (pd.DataFrame): A pandas SparseDataFrame. Returns: - pd.DataFrame: A dense pandas DataFrame. return sdf.sparse.to_dense()"},{"question":"You are required to manage a simple database of user information using the `dbm` module in Python. The database should store the `username` and `email` of the users. The keys (usernames) and values (emails) should be stored as bytes. # Requirements 1. Implement the function `manage_users_db(action, username=None, email=None, db_filename=\\"users_db\\")` that can perform the following actions: - `\\"add\\"`: Add a new user to the database. If the user already exists, update their email. - `\\"get\\"`: Retrieve the email of a specified user. - `\\"delete\\"`: Delete a specified user from the database. - `\\"list\\"`: List all usernames currently in the database. 2. Use the `dbm.open` function with the `\'c\'` flag to open the database for reading and writing, creating it if it doesn\'t exist. 3. Ensure that keys and values are stored as bytes. Use the `.encode()` method for string to bytes conversion. 4. The function should handle exceptions gracefully, providing meaningful error messages. 5. Close the database after the required action is performed. # Function Signature ```python def manage_users_db(action: str, username: str = None, email: str = None, db_filename: str = \\"users_db\\") -> any: pass ``` # Constraints - `action` will always be one of `\\"add\\"`, `\\"get\\"`, `\\"delete\\"`, `\\"list\\"`. - `username` and `email` (if provided) will be non-empty strings. - For action `\\"get\\"`, the function should return the email of the specified user as a string, or `None` if the user does not exist. - For action `\\"delete\\"`, delete the user if they exist. - For action `\\"list\\"`, return a list of all usernames as strings. - Handle the case where the database does not exist or is unreadable. # Example Usage ```python # Add a user manage_users_db(\\"add\\", \\"alice\\", \\"alice@example.com\\") # Retrieve a user\'s email print(manage_users_db(\\"get\\", \\"alice\\")) # Output: alice@example.com # List all users print(manage_users_db(\\"list\\")) # Output: [\'alice\'] # Delete a user manage_users_db(\\"delete\\", \\"alice\\") # Try to retrieve a deleted user print(manage_users_db(\\"get\\", \\"alice\\")) # Output: None ``` Implement the function to satisfy the above requirements and constraints.","solution":"import dbm def manage_users_db(action: str, username: str = None, email: str = None, db_filename: str = \\"users_db\\"): try: with dbm.open(db_filename, \'c\') as db: if action == \\"add\\": if username and email: db[username.encode()] = email.encode() return \\"User added or updated successfully.\\" else: return \\"Username and email required for adding or updating a user.\\" elif action == \\"get\\": if username: return db.get(username.encode()).decode() if username.encode() in db else None else: return \\"Username required for retrieving email.\\" elif action == \\"delete\\": if username: if username.encode() in db: del db[username.encode()] return \\"User deleted successfully.\\" else: return \\"User not found.\\" else: return \\"Username required for deleting a user.\\" elif action == \\"list\\": return [key.decode() for key in db.keys()] else: return \\"Invalid action.\\" except Exception as e: return str(e)"},{"question":"Problem Statement You are tasked with creating a utility function using the **zipimport** module. This utility will validate a Python module within a ZIP archive, load it, and retrieve specific information about it. # Function Signature ```python def inspect_module_from_zip(zip_path: str, module_name: str) -> dict: ``` # Input 1. **zip_path** (`str`): The path to a ZIP archive file containing Python modules. 2. **module_name** (`str`): The fully qualified name of the module to be loaded (e.g., `mypackage.mymodule`). # Output `dict`: A dictionary with the following keys: - `\\"filename\\"`: The file name where the module was found. - `\\"is_package\\"`: A boolean indicating if the module is a package. - `\\"source_code\\"`: The source code of the module as a string. - `\\"exec_result\\"`: The output of executing the module as a string (consider the module executes a script and prints output). # Constraints - If the ZIP file is not a valid archive or if the module is not found, raise a `zipimport.ZipImportError` with an appropriate message. - Assume that special modules (e.g., dynamic modules) are not used and thus import of `.pyd`, `.so` is not considered. # Examples Example 1: ```python # Assuming \'mymodules.zip\' contains a module \'mymodule.py\' zip_path = \'mymodules.zip\' module_name = \'mymodule\' result = inspect_module_from_zip(zip_path, module_name) ``` Output: ```python { \\"filename\\": \\"mymodules.zip/mymodule.py\\", \\"is_package\\": False, \\"source_code\\": \\"...(source code as string)...\\", \\"exec_result\\": \\"...(execution output as string)...\\" } ``` Example 2: ```python # Assuming \'mypackage.zip\' contains a package \'my_pkg\' zip_path = \'mypackage.zip\' module_name = \'my_pkg\' result = inspect_module_from_zip(zip_path, module_name) ``` Output: ```python { \\"filename\\": \\"mypackage.zip/my_pkg/__init__.py\\", \\"is_package\\": True, \\"source_code\\": \\"...(source code as string)...\\", \\"exec_result\\": \\"...(execution output as string)...\\" } ``` # Additional Notes - The `exec_module` should be used to execute the module and capture its standard output. - Make sure to implement and use proper exception handling for robust code.","solution":"import zipimport import io import sys def inspect_module_from_zip(zip_path: str, module_name: str) -> dict: Inspects a Python module within a ZIP archive and returns specific information about it. :param zip_path: Path to the ZIP file :param module_name: Fully qualified name of the module :return: Dictionary containing filename, is_package, source_code, exec_result try: importer = zipimport.zipimporter(zip_path) code = importer.get_code(module_name) if not code: raise zipimport.ZipImportError(f\\"Module {module_name} not found\\") filename = importer.get_filename(module_name) is_package = code.co_filename.endswith(\'__init__.py\') # Get source code source_code = importer.get_source(module_name) # Capture output of executing the module output_buffer = io.StringIO() sys.stdout = output_buffer try: exec(code) finally: sys.stdout = sys.__stdout__ exec_result = output_buffer.getvalue() output_buffer.close() return { \\"filename\\": filename, \\"is_package\\": is_package, \\"source_code\\": source_code, \\"exec_result\\": exec_result } except zipimport.ZipImportError as e: raise zipimport.ZipImportError(f\\"Error loading module from ZIP: {str(e)}\\") from e except Exception as e: raise zipimport.ZipImportError(f\\"Unexpected error: {str(e)}\\") from e"},{"question":"Objective: Create a custom neural network using PyTorch\'s `torch.nn` module, implementing multiple types of layers including convolutional, pooling, and fully connected layers. The goal is to design a simple yet effective image classification model for the CIFAR-10 dataset. Specifications: 1. **Model Architecture**: - Convolutional Layer 1: 32 filters, 3x3 kernel size, `ReLU` activation - Max Pooling Layer 1: 2x2 kernel size - Convolutional Layer 2: 64 filters, 3x3 kernel size, `ReLU` activation - Max Pooling Layer 2: 2x2 kernel size - Fully Connected Layer 1: 512 units, `ReLU` activation - Fully Connected Layer 2: 10 units (output layer for 10 classes) 2. **Forward Pass**: - Implement the forward pass of the model that takes an input tensor and outputs class probabilities using the softmax function. Additional Information: - **Input**: A tensor of shape `[batch_size, 3, 32, 32]`, where `batch_size` is variable, 3 is the number of color channels (RGB), and 32x32 is the image size of CIFAR-10. - **Output**: A tensor of shape `[batch_size, 10]` representing the class probabilities for each image in the batch. - **Constraints**: None specific. Example: ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomCIFAR10Model(nn.Module): def __init__(self): super(CustomCIFAR10Model, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3) self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3) self.pool = nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = nn.Linear(64 * 6 * 6, 512) # Adjust input dimension based on pooling and conv layer output self.fc2 = nn.Linear(512, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = self.pool(x) x = F.relu(self.conv2(x)) x = self.pool(x) x = x.view(-1, 64 * 6 * 6) # Flatten the tensor for the fully connected layer x = F.relu(self.fc1(x)) x = self.fc2(x) return F.softmax(x, dim=1) ``` Task: Using the provided example as a reference, complete the following steps: 1. Implement the `__init__` method to define the mentioned model architecture. 2. Implement the `forward` method to define the forward pass of the model. Submission: Submit your implementation of the `CustomCIFAR10Model` class as a `.py` file or within a notebook cell. Ensure that your model can be instantiated and that the forward pass works correctly for a sample input tensor.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class CustomCIFAR10Model(nn.Module): def __init__(self): super(CustomCIFAR10Model, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3) self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3) self.pool = nn.MaxPool2d(kernel_size=2, stride=2) self.fc1 = nn.Linear(64 * 6 * 6, 512) # Adjust input dimension based on pooling and conv layer output self.fc2 = nn.Linear(512, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = self.pool(x) x = F.relu(self.conv2(x)) x = self.pool(x) x = x.view(-1, 64 * 6 * 6) # Flatten the tensor for the fully connected layer x = F.relu(self.fc1(x)) x = self.fc2(x) return F.softmax(x, dim=1)"},{"question":"# Advanced Python 3.10 Assessment: Name Resolution and Exception Handling In this coding assessment, you will be required to write a Python program that demonstrates your understanding of name resolution, scope, and exception handling. Your task is to implement the following function: ```python def name_resolution_and_exceptions(data: list) -> dict: This function takes a list of dictionaries, each containing details about a student. The function should process this data to calculate the average score for each course, handling exceptions and edge cases appropriately. Args: data (list): A list of dictionaries where each dictionary represents a student and contains their \'name\' (str) and \'scores\' (dict). Each \'scores\' dictionary contains course names as keys and integer test scores as values. Returns: dict: A dictionary with course names as keys and a tuple containing the average score and the number of students enrolled in that course as values. Example: input_data = [ {\'name\': \'Alice\', \'scores\': {\'math\': 90, \'history\': 85}}, {\'name\': \'Bob\', \'scores\': {\'math\': 75, \'science\': 95}}, {\'name\': \'Charlie\', \'scores\': {\'history\': 80, \'science\': 78}}, {\'name\': \'David\', \'scores\': {}} ] Output: {\'math\': (82.5, 2), \'history\': (82.5, 2), \'science\': (86.5, 2)} pass ``` Implementation Requirements: 1. **Function Definition**: Your function should be named `name_resolution_and_exceptions` and take one argument: `data`, which is a list of dictionaries as described. 2. **Handling Name Resolution and Scope**: Properly handle variable scopes within your function to ensure accurate and isolated calculations for each course. 3. **Exception Handling**: Implement appropriate exception handling for the following: - Missing \'scores\' key in a student\'s dictionary. - Non-integer values in the \'scores\' dictionary. 4. **Edge Cases**: - If a course has no scores (i.e., no students are enrolled), do not include it in the output dictionary. - Ignore students with an empty \'scores\' dictionary. 5. **Output Format**: The output should be a dictionary where each key is a course name and its value is a tuple containing the average score for that course and the number of students enrolled in that course. 6. **Performance**: The function should efficiently handle lists of arbitrary length. Constraints: - All student names are unique strings. - All scores, if present, should be integers. Testing Your Function: You are encouraged to test your function with various inputs, including edge cases and exceptions, to ensure correctness. Example Test Case: ```python input_data = [ {\'name\': \'Alice\', \'scores\': {\'math\': 90, \'history\': 85}}, {\'name\': \'Bob\', \'scores\': {\'math\': 75, \'science\': 95}}, {\'name\': \'Charlie\', \'scores\': {\'history\': 80, \'science\': 78}}, {\'name\': \'David\', \'scores\': {}} ] output = name_resolution_and_exceptions(input_data) print(output) # Expected: {\'math\': (82.5, 2), \'history\': (82.5, 2), \'science\': (86.5, 2)} ``` Please implement the function `name_resolution_and_exceptions` as described above.","solution":"def name_resolution_and_exceptions(data: list) -> dict: This function takes a list of dictionaries, each containing details about a student. The function should process this data to calculate the average score for each course, handling exceptions and edge cases appropriately. Args: data (list): A list of dictionaries where each dictionary represents a student and contains their \'name\' (str) and \'scores\' (dict). Each \'scores\' dictionary contains course names as keys and integer test scores as values. Returns: dict: A dictionary with course names as keys and a tuple containing the average score and the number of students enrolled in that course as values. course_scores = {} for student in data: try: for course, score in student.get(\'scores\', {}).items(): if not isinstance(score, int): raise ValueError(f\\"Non-integer score found for course: {course}\\") if course not in course_scores: course_scores[course] = {\'total_score\': 0, \'student_count\': 0} course_scores[course][\'total_score\'] += score course_scores[course][\'student_count\'] += 1 except Exception as e: print(f\\"Error processing student {student[\'name\']}: {e}\\") # Compute average scores result = {} for course, data in course_scores.items(): if data[\'student_count\'] > 0: average_score = data[\'total_score\'] / data[\'student_count\'] result[course] = (average_score, data[\'student_count\']) return result"},{"question":"Objective Implement a distributed training pipeline using the `FullyShardedDataParallel` (FSDP) class from PyTorch\'s `torch.distributed.fsdp` module. You need to demonstrate understanding of sharding strategies, mixed precision training, and state dictionaries. Problem Statement You are given a neural network model, and you need to set up a distributed training environment using FSDP. Implement the following functionalities: 1. Initialize a distributed training group. 2. Set up the FSDP model with a specified `ShardingStrategy` and `MixedPrecision` configuration. 3. Implement the forward, backward pass, and optimization steps. 4. Save and load the model\'s state dictionary using `StateDictConfig`. # Input 1. `model`: An instance of a PyTorch neural network model. 2. `data_loader`: A PyTorch DataLoader instance providing the training data. 3. `strategy`: A string specifying the sharding strategy (e.g., `FULL_SHARD`, `SHARD_GRAD_OP`). 4. `precision`: A string specifying the mixed precision setting (e.g., `FP16`, `BF16`). 5. `save_path`: A string specifying the file path to save the model\'s state dictionary. 6. `load_path`: A string specifying the file path to load the model\'s state dictionary. # Output 1. Train the model for one epoch, and return the training loss. 2. Save the model\'s state dictionary to the specified path. 3. Load the model\'s state dictionary from the specified path and verify if the state is consistent. # Function Signature ```python def distributed_training_pipeline(model, data_loader, strategy, precision, save_path, load_path): Parameters ---------- model : torch.nn.Module The neural network model. data_loader : torch.utils.data.DataLoader DataLoader providing the training data. strategy : str Sharding strategy (e.g., \'FULL_SHARD\', \'SHARD_GRAD_OP\'). precision : str Mixed precision setting (e.g., \'FP16\', \'BF16\'). save_path : str File path to save the model\'s state dictionary. load_path : str File path to load the model\'s state dictionary. Returns ------- float The training loss after one epoch. ``` # Constraints 1. Use PyTorch\'s `torch.distributed` package to initialize the process group. 2. Implement the FSDP model using the specified `ShardingStrategy` and `MixedPrecision`. 3. Ensure the training loop runs efficiently on multiple devices. 4. Implement proper saving and loading mechanisms for the model\'s state dictionary. # Example ```python import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.distributed.fsdp import ShardingStrategy, MixedPrecision, FullStateDictConfig # Example usage of the function model = MyNeuralNetwork() data_loader = DataLoader(my_dataset, batch_size=32) strategy = \'FULL_SHARD\' precision = \'FP16\' save_path = \'fsdp_model_state.pth\' load_path = \'fsdp_model_state.pth\' training_loss = distributed_training_pipeline(model, data_loader, strategy, precision, save_path, load_path) print(f\\"Training Loss: {training_loss}\\") ``` Note Provide the implementation code for the `distributed_training_pipeline` function including necessary imports, initialization, and the training loop.","solution":"import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP from torch.distributed.fsdp import FullyShardedDataParallel as FSDP from torch.distributed.fsdp import ShardingStrategy, MixedPrecision, FullStateDictConfig def distributed_training_pipeline(model, data_loader, strategy, precision, save_path, load_path): Parameters ---------- model : torch.nn.Module The neural network model. data_loader : torch.utils.data.DataLoader DataLoader providing the training data. strategy : str Sharding strategy (e.g., \'FULL_SHARD\', \'SHARD_GRAD_OP\'). precision : str Mixed precision setting (e.g., \'FP16\', \'BF16\'). save_path : str File path to save the model\'s state dictionary. load_path : str File path to load the model\'s state dictionary. Returns ------- float The training loss after one epoch. # Initialize the process group dist.init_process_group(backend=\'nccl\') torch.cuda.set_device(dist.get_rank()) # Setup the model for FSDP with specified sharding strategy and mixed precision sharding_strategy = ShardingStrategy[strategy.upper()] mixed_precision = MixedPrecision[precision.upper()] fsdp_model = FSDP(model.cuda(), auto_wrap_policy=None, sharding_strategy=sharding_strategy, mixed_precision=mixed_precision) # Setup optimizer optimizer = torch.optim.Adam(fsdp_model.parameters(), lr=0.001) criterion = torch.nn.CrossEntropyLoss() fsdp_model.train() epoch_loss = 0.0 for data, target in data_loader: data, target = data.cuda(), target.cuda() optimizer.zero_grad() output = fsdp_model(data) loss = criterion(output, target) loss.backward() optimizer.step() epoch_loss += loss.item() # Save the model\'s state dictionary state_dict = fsdp_model.state_dict(FullStateDictConfig()) torch.save(state_dict, save_path) # Load the model\'s state dictionary loaded_state_dict = torch.load(load_path) fsdp_model.load_state_dict(loaded_state_dict) # Verify if the state is consistent loaded_state_dict_after = fsdp_model.state_dict(FullStateDictConfig()) for key in loaded_state_dict.keys(): assert torch.equal(loaded_state_dict[key], loaded_state_dict_after[key]), \\"State dictionaries do not match after loading\\" dist.destroy_process_group() return epoch_loss / len(data_loader)"},{"question":"**Objective:** Demonstrate your understanding of Seaborn\'s `histplot` function by performing advanced data visualization tasks. # Problem Statement: You are given a dataset containing information about different species of penguins. Your task is to create various visualizations using Seaborn\'s `histplot` function. Implement the function `create_penguin_plots` that takes no parameters and: 1. Loads the `\\"penguins\\"` dataset. 2. Creates a histogram of the `flipper_length_mm` for all penguins. 3. Creates a histogram of the `flipper_length_mm`, separating by `species`, and uses a step function for better clarity. 4. Creates a bivariate histogram (heatmap) of `bill_depth_mm` vs. `body_mass_g`. 5. Plots the `island` as a categorical variable on the X-axis and the `bill_length_mm` on the Y-axis, visualizing the distribution of `bill_length_mm`. # Constraints: - Ensure to label each plot appropriately, including a title, axis labels, and a legend (where necessary). # Expected Output: The function should display the following four plots: 1. A basic histogram of `flipper_length_mm`. 2. A step function histogram of `flipper_length_mm` separated by `species`. 3. A bivariate histogram of `bill_depth_mm` vs. `body_mass_g`. 4. A categorical histogram with `island` on the X-axis and `bill_length_mm` as the distribution. # Example Usage: ```python create_penguin_plots() ``` # Implementation: ```python import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # 1. Load the penguin dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Create a histogram of `flipper_length_mm` for all penguins plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Histogram of Flipper Length (mm) for All Penguins\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # 3. Create a step function histogram of `flipper_length_mm` separated by `species` plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", element=\\"step\\") plt.title(\\"Step Function Histogram of Flipper Length (mm) Separated by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.legend(title=\\"Species\\") plt.show() # 4. Create a bivariate histogram of `bill_depth_mm` vs `body_mass_g` plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", cbar=True) plt.title(\\"Bivariate Histogram of Bill Depth (mm) vs Body Mass (g)\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # 5. Plot the distribution of `bill_length_mm` with `island` on the X-axis plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"island\\", y=\\"bill_length_mm\\", hue=\\"island\\", legend=False) plt.title(\\"Distribution of Bill Length (mm) per Island\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Bill Length (mm)\\") plt.show() ``` # Note: - Make sure to test your function to ensure all plots are displayed correctly. - Use the various configurations and customizations available in Seaborn to make your plots informative and visually appealing.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # 1. Load the penguin dataset penguins = sns.load_dataset(\\"penguins\\") # 2. Create a histogram of `flipper_length_mm` for all penguins plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"Histogram of Flipper Length (mm) for All Penguins\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.show() # 3. Create a step function histogram of `flipper_length_mm` separated by `species` plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", element=\\"step\\") plt.title(\\"Step Function Histogram of Flipper Length (mm) Separated by Species\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"Frequency\\") plt.legend(title=\\"Species\\") plt.show() # 4. Create a bivariate histogram of `bill_depth_mm` vs `body_mass_g` plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"bill_depth_mm\\", y=\\"body_mass_g\\", cbar=True) plt.title(\\"Bivariate Histogram of Bill Depth (mm) vs Body Mass (g)\\") plt.xlabel(\\"Bill Depth (mm)\\") plt.ylabel(\\"Body Mass (g)\\") plt.show() # 5. Plot the distribution of `bill_length_mm` with `island` on the X-axis plt.figure(figsize=(10, 6)) sns.histplot(data=penguins, x=\\"island\\", y=\\"bill_length_mm\\", hue=\\"island\\", legend=False) plt.title(\\"Distribution of Bill Length (mm) per Island\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Bill Length (mm)\\") plt.show()"},{"question":"# Question: Implement a Custom Classifier Using Scikit-learn You are tasked with creating a custom classifier using scikit-learn. Your classifier should combine multiple built-in classifiers using a voting mechanism. Specifically, you will implement a `HybridClassifier` class that utilizes three different classifiers (e.g., Logistic Regression, Decision Tree, and K-Nearest Neighbors) and makes final predictions based on majority voting among the individual classifiers. Requirements: 1. **Class Implementation**: - You must implement a class `HybridClassifier` that aggregates predictions from three different classifiers. - The class should include methods for `fit`, `predict`, and `score`. 2. **Initialization**: - The constructor should accept and initialize three classifier objects. 3. **Fit Method**: - The `fit` method should train all three classifiers on the provided training data. 4. **Predict Method**: - The `predict` method should return the final prediction based on majority voting from the three classifiers. 5. **Score Method**: - The `score` method should compute the accuracy of the classifier on the provided test data. Constraints: - You must use scikit-learn\'s built-in classifiers for Logistic Regression, Decision Tree, and K-Nearest Neighbors. - Assume that the input data for `fit` and `predict` methods are numpy arrays. - You may use any necessary imports from scikit-learn. Example Usage: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier class HybridClassifier: def __init__(self, clf1, clf2, clf3): self.clf1 = clf1 self.clf2 = clf2 self.clf3 = clf3 def fit(self, X, y): self.clf1.fit(X, y) self.clf2.fit(X, y) self.clf3.fit(X, y) def predict(self, X): pred1 = self.clf1.predict(X) pred2 = self.clf2.predict(X) pred3 = self.clf3.predict(X) predictions = np.array([pred1, pred2, pred3]) # Majority voting final_prediction = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions) return final_prediction def score(self, X, y): pred = self.predict(X) accuracy = np.mean(pred == y) return accuracy # Load data data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42) # Initialize classifiers clf1 = LogisticRegression(max_iter=200) clf2 = DecisionTreeClassifier() clf3 = KNeighborsClassifier() # Create and train HybridClassifier hybrid_clf = HybridClassifier(clf1, clf2, clf3) hybrid_clf.fit(X_train, y_train) # Test HybridClassifier accuracy = hybrid_clf.score(X_test, y_test) print(f\'Hybrid Classifier Accuracy: {accuracy:.2f}\') ``` This problem tests the ability to use multiple classifiers, implement voting mechanisms for final prediction, and ensure correct functioning through fit, predict, and score methods.","solution":"import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.neighbors import KNeighborsClassifier class HybridClassifier: def __init__(self, clf1, clf2, clf3): self.clf1 = clf1 self.clf2 = clf2 self.clf3 = clf3 def fit(self, X, y): self.clf1.fit(X, y) self.clf2.fit(X, y) self.clf3.fit(X, y) def predict(self, X): pred1 = self.clf1.predict(X) pred2 = self.clf2.predict(X) pred3 = self.clf3.predict(X) predictions = np.array([pred1, pred2, pred3]) # Majority voting final_prediction = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions) return final_prediction def score(self, X, y): pred = self.predict(X) accuracy = np.mean(pred == y) return accuracy"},{"question":"# Advanced Coding Assessment Question: Custom Interactive Console **Objective:** Design and implement a custom Python interactive console that enhances the default behavior with additional features, such as custom completions, history management, and user-configurable key bindings. # Requirements: 1. **Initialization File:** - Parse and bind custom key bindings from an initialization file located at `~/.inputrc_custom`. - If the file does not exist or is unreadable, print an appropriate error message but continue with default bindings. 2. **History Management:** - Load the history from `~/.custom_history` upon initialization. - Save the session history back to `~/.custom_history` upon exiting the console. - Limit the history to the last 1000 entries. 3. **Line Buffer:** - Implement a feature to insert a predefined text snippet at the current cursor position when a specific key combination (e.g., `Ctrl-T`) is pressed. 4. **Custom Completions:** - Implement a custom completer function that suggests words from a predefined list of keywords (e.g., `[\'start\', \'stop\', \'list\', \'help\']`). - The completer should be activated via the Tab key. 5. **Startup and Pre-Input Hooks:** - Implement a startup hook that prints a welcome message. - Implement a pre-input hook that prints a \\"Ready for input:\\" message before each user input prompt. # Input and Output Formats: - **Input:** - Read user inputs interactively. - Parse key bindings and history configurations from specified files. - **Output:** - Display the interactive prompt and handle completions, history, and custom insertions as specified. # Constraints: - Ensure your solution works efficiently with up to 1000 history entries. - Handle file I/O operations gracefully, including permissions and missing files. - Maintain code readability and modularity with appropriate function definitions. # Performance Requirements: - The interactive console should remain responsive and capable of handling multiple entries quickly. - History operations should not introduce significant delays. # Example Behavior: 1. **Initialization:** - Parse `~/.inputrc_custom` for custom key bindings. - Load history from `~/.custom_history`. 2. **Session:** - User types `sta` and presses Tab; the console suggests `start`. - User presses `Ctrl-T`; a predefined snippet (e.g., `# Comment snippet`) is inserted at the cursor position. - User types `exit` to terminate the session. 3. **Exit:** - Save history to `~/.custom_history`. - Display a farewell message. # Implementation: Implement the custom interactive console using the `readline` module with the Python `code` library to create an enhanced interactive console: ```python import atexit import os import readline import code class CustomInteractiveConsole(code.InteractiveConsole): keywords = [\'start\', \'stop\', \'list\', \'help\'] def __init__(self, locals=None, filename=\\"<console>\\", histfile=os.path.expanduser(\\"~/.custom_history\\")): code.InteractiveConsole.__init__(self, locals, filename) self.histfile = histfile self.init_history() self.init_readline() def init_history(self): try: readline.read_history_file(self.histfile) readline.set_history_length(1000) except FileNotFoundError: pass atexit.register(readline.write_history_file, self.histfile) def init_readline(self): inputrc_custom = os.path.expanduser(\\"~/.inputrc_custom\\") if os.path.exists(inputrc_custom): readline.read_init_file(inputrc_custom) else: print(\\"No custom initialization file found at ~/.inputrc_custom.\\") self.predefined_snippet = \\"# Comment snippet\\" def startup_hook(): print(\\"Welcome to the custom interactive console!\\") def pre_input_hook(): print(\\"Ready for input:\\") def completer(text, state): options = [kw for kw in self.keywords if kw.startswith(text)] return options[state] if state < len(options) else None readline.set_startup_hook(startup_hook) readline.set_pre_input_hook(pre_input_hook) readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") # Bind Ctrl-T to insert predefined snippet readline.parse_and_bind(\'\\"C-t\\": \\"\\"\') def insert_snippet(self): readline.insert_text(self.predefined_snippet) readline.redisplay() def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source == \'x14\': # Ctrl-T character self.insert_snippet() return False else: return code.InteractiveConsole.runsource(self, source, filename, symbol) if __name__ == \'__main__\': console = CustomInteractiveConsole() console.interact(\\"Custom Interactive Console. Type \'exit\' to quit.\\") ``` Implement and test this custom interactive console to ensure it meets all specified requirements.","solution":"import atexit import os import readline import code class CustomInteractiveConsole(code.InteractiveConsole): keywords = [\'start\', \'stop\', \'list\', \'help\'] def __init__(self, locals=None, filename=\\"<console>\\", histfile=os.path.expanduser(\\"~/.custom_history\\")): code.InteractiveConsole.__init__(self, locals, filename) self.histfile = histfile self.init_history() self.init_readline() def init_history(self): try: readline.read_history_file(self.histfile) readline.set_history_length(1000) except FileNotFoundError: pass atexit.register(readline.write_history_file, self.histfile) def init_readline(self): inputrc_custom = os.path.expanduser(\\"~/.inputrc_custom\\") if os.path.exists(inputrc_custom): try: readline.read_init_file(inputrc_custom) except Exception as e: print(f\\"Error reading custom init file: {e}\\") else: print(\\"No custom initialization file found at ~/.inputrc_custom.\\") self.predefined_snippet = \\"# Comment snippet\\" def startup_hook(): print(\\"Welcome to the custom interactive console!\\") def pre_input_hook(): print(\\"Ready for input:\\") def completer(text, state): options = [kw for kw in self.keywords if kw.startswith(text)] return options[state] if state < len(options) else None readline.set_startup_hook(startup_hook) readline.set_pre_input_hook(pre_input_hook) readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") # Bind Ctrl-T to insert predefined snippet readline.parse_and_bind(\'\\"C-t\\": \\"\\"\') def insert_snippet(self): readline.insert_text(self.predefined_snippet) readline.redisplay() def runsource(self, source, filename=\\"<input>\\", symbol=\\"single\\"): if source == \'x14\': # Ctrl-T character self.insert_snippet() return False else: return code.InteractiveConsole.runsource(self, source, filename, symbol) if __name__ == \'__main__\': console = CustomInteractiveConsole() console.interact(\\"Custom Interactive Console. Type \'exit\' to quit.\\")"},{"question":"# Principal Component Analysis (PCA) with scikit-learn Objective Demonstrate your understanding of Principal Component Analysis (PCA) and its application using the scikit-learn library on a dataset. Problem Statement You are provided with a dataset containing various features. Your task is to implement the following steps: 1. Load the dataset. 2. Apply standard scaling to the dataset. 3. Perform Principal Component Analysis (PCA) to reduce the dataset to two principal components. 4. Project the data onto the principal components. 5. Plot the projected data points in the principal component space. Dataset The dataset is a CSV file named `data.csv` provided alongside this question. It consists of multiple features (columns) and several samples (rows). For simplicity, assume the dataset does not contain missing values. Steps to Complete 1. **Loading the Dataset**: - Load the dataset from the provided CSV file using pandas. 2. **Standard Scaling**: - Scale the dataset using `StandardScaler` from scikit-learn to center the data and give it unit variance. 3. **PCA Implementation**: - Implement PCA using `PCA` from scikit-learn to reduce the dataset to 2 dimensions (principal components). 4. **Data Projection**: - Project the scaled data onto the two principal components obtained from PCA. 5. **Visualization**: - Plot the projected data points in the 2D principal component space. Use a scatter plot with appropriate axis labels and a title. Input Format - The input is assumed to be a CSV file named `data.csv` located in the same directory as your code. Output Format - A scatter plot of the projected data points in the principal component space. Constraints - You may assume that the dataset does not contain any missing values. - You can import additional libraries if required (e.g., matplotlib for plotting). Example Code Outline ```python import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA import matplotlib.pyplot as plt # Step 1: Load the dataset data = pd.read_csv(\'data.csv\') # Step 2: Apply standard scaling scaler = StandardScaler() scaled_data = scaler.fit_transform(data) # Step 3: Perform PCA to reduce the dataset to 2 principal components pca = PCA(n_components=2) principal_components = pca.fit_transform(scaled_data) # Step 4: Project the data onto the principal components # (Done in the previous step as \'principal_components\') # Step 5: Plot the projected data points plt.scatter(principal_components[:, 0], principal_components[:, 1]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'2D PCA Projection of the Dataset\') plt.show() ``` Submission Submit a Python script or Jupyter Notebook with the complete implementation based on the provided code outline.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA import matplotlib.pyplot as plt def load_and_preprocess_data(file_path): Loads the dataset and applies standard scaling. Args: - file_path (str): The file path to the dataset CSV file. Returns: - scaled_data (ndarray): The scaled data. # Step 1: Load the dataset data = pd.read_csv(file_path) # Step 2: Apply standard scaling scaler = StandardScaler() scaled_data = scaler.fit_transform(data) return scaled_data def apply_pca(scaled_data, n_components=2): Applies PCA to reduce the dataset to a specified number of principal components. Args: - scaled_data (ndarray): The scaled data. - n_components (int): The number of principal components to retain. Returns: - principal_components (ndarray): The dataset projected onto the principal components. # Step 3: Perform PCA pca = PCA(n_components=n_components) principal_components = pca.fit_transform(scaled_data) return principal_components def plot_principal_components(principal_components): Plots the data points in the principal component space. Args: - principal_components (ndarray): The dataset projected onto the principal components. # Step 5: Plot the projected data points plt.scatter(principal_components[:, 0], principal_components[:, 1]) plt.xlabel(\'Principal Component 1\') plt.ylabel(\'Principal Component 2\') plt.title(\'2D PCA Projection of the Dataset\') plt.show() # Example usage: # file_path = \'data.csv\' # scaled_data = load_and_preprocess_data(file_path) # principal_components = apply_pca(scaled_data) # plot_principal_components(principal_components)"},{"question":"Objective: Create a comprehensive visualization using seaborn.objects that demonstrates your understanding of line plots, grouping, customization, and combining plot elements. Problem Statement: You are provided with two datasets, \\"dowjones\\" and \\"fmri\\", from seaborn\'s built-in datasets. Your task is to create a combined plot that includes the following: 1. A line plot of the \\"dowjones\\" data showing the `Price` over `Date`. 2. A line plot of the \\"fmri\\" data, filtered to include only the \\"parietal\\" region and \\"stim\\" event, showing the `signal` over `timepoint`. 3. Group the fmri line plot by the `subject`. 4. Customize the fmri line plot by setting a line color of `.2` (20% black) and a linewidth of 1. 5. Add markers to the fmri line plot indicating sampled data points, using white edge color. 6. Add an error band to the fmri plot showing the aggregated signal over each timepoint, grouped by `event`. Requirements: - Use the `so.Plot` class and related functionalities to create the visualizations. - Ensure the dowjones plot and fmri plot appear in the same figure. - Customize the line properties as specified. - Combine the error band with the fmri line plot. Input: There are no direct inputs as the datasets are to be loaded within the function. Output: A Seaborn plot that includes the specified line plots from the dowjones and fmri datasets with the described customizations. Constraints: - Use seaborn version 0.11.0 or higher. - Focus on using the seaborn.objects API for plotting. Example: ```python def create_combined_plot(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create dowjones line plot plot_dowjones = so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()) # Create fmri line plot with customizations fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") plot_fmri = ( so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") ) # Add markers and error band to fmri plot plot_fmri = ( plot_fmri .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) .add(so.Band(), so.Est(), group=\\"event\\") ) # Plot both on the same figure fig, axes = plt.subplots(2, 1, figsize=(10, 8)) plot_dowjones.on(axes[0]) plot_fmri.on(axes[1]) plt.show() ``` Implement the `create_combined_plot` function as described to create the required visualizations.","solution":"def create_combined_plot(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load datasets dowjones = load_dataset(\\"dowjones\\") fmri = load_dataset(\\"fmri\\") # Create dowjones line plot plot_dowjones = so.Plot(dowjones, \\"Date\\", \\"Price\\").add(so.Line()) # Create fmri line plot with customizations fmri_filtered = fmri.query(\\"region == \'parietal\' and event == \'stim\'\\") plot_fmri = ( so.Plot(fmri_filtered, \\"timepoint\\", \\"signal\\") .add(so.Line(color=\\".2\\", linewidth=1), group=\\"subject\\") ) # Add markers and error band to fmri plot plot_fmri = ( plot_fmri .add(so.Line(marker=\\"o\\", edgecolor=\\"w\\"), so.Agg(), linestyle=None) .add(so.Band(), so.Est(), group=\\"event\\") ) # Plot both on the same figure fig, axes = plt.subplots(2, 1, figsize=(10, 8)) plot_dowjones.on(axes[0]) plot_fmri.on(axes[1]) plt.tight_layout() plt.show()"},{"question":"# Calendar Comparison Tool As part of a calendar application, you need to implement a tool that compares month layouts between different years, highlighting their similarities and differences. Your task is to write a function that generates month calendars for two given years and months, compares them, and outputs a string representation indicating which days are the same (occurring on the same weekdays) and which are different. Requirements: 1. Implement the function `compare_calendars(year1, month1, year2, month2)`. This function compares the month-day layouts of two specified months from two different years. 2. The comparison should be displayed as a text-based calendar layout similar to the one generated by `calendar.TextCalendar`. 3. Indicate days that are the same in both calendars by enclosing them in square brackets (e.g., `[ 1]`), and days that differ by just rendering them without brackets. 4. Days that do not exist in one of the months should be represented by empty spaces. Input: - `year1, month1`: Integers representing the first year and month for comparison. - `year2, month2`: Integers representing the second year and month for comparison. Output: - A string representing the comparison of the provided months, formatted as a text calendar layout. Example: ```python import calendar def compare_calendars(year1, month1, year2, month2): cal = calendar.Calendar() month1_days = cal.monthdayscalendar(year1, month1) month2_days = cal.monthdayscalendar(year2, month2) width = max(len(month1_days), len(month2_days)) output = [] header = \\" Mo Tu We Th Fr Sa Su \\" output.append(f\\" {year1} - {calendar.month_name[month1]} vs {year2} - {calendar.month_name[month2]}\\") output.append(header * 2) for week1, week2 in zip_longest(month1_days, month2_days, fillvalue=[0]*7): line1, line2 = [], [] for d1, d2 in zip(week1, week2): if d1 == d2 and d1 != 0: line1.append(f\\"[{d1:2}]\\") line2.append(f\\"[{d2:2}]\\") else: line1.append(f\\" {d1:2} \\" if d1 != 0 else \\" \\") line2.append(f\\" {d2:2} \\" if d2 != 0 else \\" \\") output.append(\\"\\".join(line1) + \\"\\".join(line2)) return \\"n\\".join(output) # Example usage print(compare_calendars(2022, 2, 2023, 2)) ``` Constraints: 1. Both `year1` and `year2` should be valid years from 1 to 9999. 2. Both `month1` and `month2` need to be valid months from 1 to 12. 3. Consider performance for generating and comparing months. This question assesses your ability to work with the `calendar` module features, manipulate date and calendar data, and format outputs.","solution":"import calendar from itertools import zip_longest def compare_calendars(year1, month1, year2, month2): cal = calendar.Calendar() month1_days = cal.monthdayscalendar(year1, month1) month2_days = cal.monthdayscalendar(year2, month2) output = [] header = \\" Mo Tu We Th Fr Sa Su \\" output.append(f\\" {year1} - {calendar.month_name[month1]} vs {year2} - {calendar.month_name[month2]}\\") output.append(header + header) for week1, week2 in zip_longest(month1_days, month2_days, fillvalue=[0]*7): line1, line2 = \\"\\", \\"\\" for d1, d2 in zip(week1, week2): if d1 == d2 and d1 != 0: day_str = f\\"[{d1:2}]\\" else: day_str1 = f\\" {d1:2} \\" if d1 != 0 else \\" \\" day_str2 = f\\" {d2:2} \\" if d2 != 0 else \\" \\" line1 += day_str1 line2 += day_str2 continue line1 += day_str line2 += day_str output.append(line1 + line2) return \\"n\\".join(output) # Example usage print(compare_calendars(2022, 2, 2023, 2))"},{"question":"**Objective**: Implement a function to compare multiple text documents and identify differences. # Problem Statement Write a Python function `compare_documents(docs: List[str], method: str = \'default\') -> Dict[Tuple[int, int], Any]` that takes a list of text documents and compares each document against all other documents to identify differences. The function should return a dictionary where the keys are tuples representing the pairs of document indices compared, and the values are the differences identified between those document pairs. The comparison should be done using the `difflib` module, and you should implement two comparison methods: 1. **default**: Use the `Differ` object from the `difflib` module to produce human-readable differences. 2. **ratio**: Use the `SequenceMatcher` object from the `difflib` module to produce a similarity ratio between the documents. # Input Format - `docs`: A list of strings, where each string is the content of a text document. - `method`: A string specifying the comparison method. It can be either `\'default\'` or `\'ratio\'`. # Output Format Return a dictionary with the following structure: - **Keys**: Tuples of two integers `(i, j)` representing the indices of the compared document pairs. - **Values**: - For the `default` method: A list of strings representing the line-by-line differences as returned by the `Differ.compare` method. - For the `ratio` method: A float between 0 and 1 representing the similarity ratio as returned by `SequenceMatcher.ratio` method. # Constraints - The number of documents `n` will not exceed 100. - Each document will be a string with a length not exceeding 10,000 characters. # Examples ```python def compare_documents(docs: List[str], method: str = \'default\') -> Dict[Tuple[int, int], Any]: import difflib result = {} for i in range(len(docs)): for j in range(i + 1, len(docs)): if method == \'default\': differ = difflib.Differ() diff = list(differ.compare(docs[i].splitlines(), docs[j].splitlines())) result[(i, j)] = diff elif method == \'ratio\': matcher = difflib.SequenceMatcher(None, docs[i], docs[j]) ratio = matcher.ratio() result[(i, j)] = ratio else: raise ValueError(\\"Unsupported method. Use \'default\' or \'ratio\'.\\") return result # Example usage: documents = [ \\"This is the first document.nIt has multiple lines.\\", \\"This is the first document.nIt has some lines.\\", \\"Another document is here.nIt also has lines.\\" ] print(compare_documents(documents, \'default\')) # Output will show the differences print(compare_documents(documents, \'ratio\')) # Output will show similarity ratios ``` # Notes - Ensure that the function handles edge cases such as empty documents gracefully. - Aim for efficient implementation, especially when dealing with large text documents.","solution":"from typing import List, Dict, Tuple, Any import difflib def compare_documents(docs: List[str], method: str = \'default\') -> Dict[Tuple[int, int], Any]: result = {} for i in range(len(docs)): for j in range(i + 1, len(docs)): if method == \'default\': differ = difflib.Differ() diff = list(differ.compare(docs[i].splitlines(), docs[j].splitlines())) result[(i, j)] = diff elif method == \'ratio\': matcher = difflib.SequenceMatcher(None, docs[i], docs[j]) ratio = matcher.ratio() result[(i, j)] = ratio else: raise ValueError(\\"Unsupported method. Use \'default\' or \'ratio\'.\\") return result"},{"question":"Objective You are provided with a time series dataset. Your task is to demonstrate your understanding of pandas\' resampling functionality by performing specific resampling operations, handling missing values, and applying statistical calculations. Problem Statement Given a pandas DataFrame `df` that contains temperature readings recorded at irregular intervals, resample the data to have regular hourly intervals. Perform several operations to manipulate and analyze the resampled dataset. The DataFrame `df` has the following columns: - `timestamp` (datetime): The datetime when the temperature was recorded. - `temperature` (float): The temperature reading. You need to accomplish the following: 1. Resample the DataFrame to hourly intervals using the mean temperature for each hour. 2. Fill any missing temperature values using forward-fill (`ffill`) method. 3. Calculate the following statistics on the resampled data: - Mean temperature over the entire period. - Maximum temperature recorded in any hourly interval. - The total number of readings (count) after resampling. Constraints - You should assume that the DataFrame `df` is properly indexed by the `timestamp` column before resampling. - If there are missing values at the start of the series, leave them as NaN after forward-filling the rest. Input - A pandas DataFrame `df` with two columns: `timestamp` and `temperature`. Output - A tuple with three elements in the following order: 1. Mean temperature over the entire period (float). 2. Maximum temperature recorded in any hourly interval (float). 3. The total number of readings (int). Example ```python import pandas as pd # Sample data data = { \'timestamp\': [ \'2021-01-01 00:00:00\', \'2021-01-01 01:45:00\', \'2021-01-01 03:30:00\', \'2021-01-01 05:00:00\', \'2021-01-01 06:15:00\', \'2021-01-01 08:00:00\' ], \'temperature\': [20.5, 21.0, 19.8, 22.1, 21.7, 20.3] } df = pd.DataFrame(data) df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) df.set_index(\'timestamp\', inplace=True) # You should implement the following function def resample_temperature_data(df): pass # Expected function call and output mean_temp, max_temp, total_readings = resample_temperature_data(df) print(mean_temp) # This should print the mean temperature over the entire resampled period. print(max_temp) # This should print the maximum temperature recorded in any hourly interval. print(total_readings) # This should print the total number of hourly readings after resampling. ``` Implement the function `resample_temperature_data(df)` that takes the DataFrame `df` and returns the expected output as described.","solution":"import pandas as pd def resample_temperature_data(df): Resample the temperature data to hourly intervals using the mean temperature for each hour, fill missing values with forward-fill method, and calculate statistical metrics. Parameters: df (pd.DataFrame): DataFrame containing \'timestamp\' and \'temperature\' columns. Returns: tuple: (mean temperature, maximum hourly temperature, total number of readings) # Resample the data to hourly intervals, calculating the mean temperature for each hour df_resampled = df.resample(\'H\').mean() # Forward-fill missing values df_resampled.ffill(inplace=True) # Calculate the required statistics mean_temp = df_resampled[\'temperature\'].mean() max_temp = df_resampled[\'temperature\'].max() total_readings = df_resampled[\'temperature\'].count() return mean_temp, max_temp, total_readings"},{"question":"Ambiguous Indentation Checker You are tasked with implementing a subset of functionality similar to the `tabnanny` module. Specifically, you need to implement a function that checks a given Python source file for ambiguous indentation and prints out the lines where such issues are detected. # Problem Statement Implement the function `check_ambiguous_indentation(file_path: str) -> None` that does the following: 1. Reads the Python source file specified by `file_path`. 2. Checks for ambiguous indentation. In this context, ambiguous indentation is defined as lines that mix tabs and spaces for indentation. 3. Outputs the line numbers and the content of lines with ambiguous indentation issues. # Input - `file_path` (str): The path to a Python source file to be checked. # Output - The function should print the line numbers and the content of the lines with ambiguous indentation issues. # Example Given the content of `example.py`: ```python 1: def example_function(): 2: x = 1 3: y = 2 4: z = 3 ``` Calling `check_ambiguous_indentation(\'example.py\')` should print: ``` Line 3: y = 2 ``` # Requirements - Do not use any external libraries other than built-in Python libraries. - Efficiently handle files with up to 10,000 lines. - Handle edge cases such as empty files and files with consistent indentation. # Constraints - Input file is guaranteed to be a valid Python file. # Notes - Assume that the indentation sequence is either tabs (`t`) or spaces, but not both. - The function should handle both Python 2 and Python 3 source files equivalently. # Hints - Use Python\'s built-in capabilities to read files. - Consider line-by-line processing to detect ambiguous indentation. - Useful built-in functions: `str.startswith()`, `str.replace()`, etc.","solution":"def check_ambiguous_indentation(file_path: str) -> None: Checks a Python source file for ambiguous indentation (mixing tabs and spaces) and prints out the lines with ambiguous indentation issues. :param file_path: Path to the Python source file to be checked. with open(file_path, \'r\') as file: lines = file.readlines() for line_number, line in enumerate(lines, start=1): # Strip leading whitespace to check the start of the indentation leading_whitespace = line[:len(line) - len(line.lstrip(\' t\'))] # Check for mixed leading whitespace if \' \' in leading_whitespace and \'t\' in leading_whitespace: print(f\\"Line {line_number}: {line.rstrip()}\\")"},{"question":"# Python Coding Assessment: Advanced Exceptions Handling Objective: Write a Python program that demonstrates an understanding of exception handling, including creating and using custom exceptions. Problem Statement: You are asked to implement a simple financial transaction system. The system keeps track of user balances and allows deposits and withdrawals. Transactions should be safe and monitored for potential issues like insufficient funds, invalid transaction amounts, and user authentication failures. Task: 1. **Create Custom Exceptions**: - `InsufficientFundsError`: Raised when a withdrawal amount is greater than the available balance. - `InvalidTransactionError`: Raised when a transaction amount is not valid (e.g., negative amount). - `AuthenticationError`: Raised when user authentication fails. 2. **Implement the following functions**: - `authenticate_user(username: str, password: str) -> bool`: - Simulate a user authentication by checking if the username is \\"admin\\" and the password is \\"password123\\". - Raise `AuthenticationError` if the credentials are incorrect. - `deposit(balance: float, amount: float) -> float`: - Add `amount` to `balance`. - Raise `InvalidTransactionError` if `amount` is non-positive. - `withdraw(balance: float, amount: float) -> float`: - Subtract `amount` from `balance`. - Raise `InvalidTransactionError` if `amount` is non-positive. - Raise `InsufficientFundsError` if `amount` exceeds `balance`. 3. **Main Program**: - Use a `try-except` block to properly handle the custom exceptions when performing transactions. - Prompt the user to input their username and password to authenticate. Handle the `AuthenticationError`. - If authenticated, prompt the user to input transaction details and handle all potential exceptions properly. Constraints: - The initial balance is set to 1000.0. - The system should allow multiple deposits and withdrawals until the user chooses to exit. Example Input/Output: ``` Enter username: admin Enter password: password123 Authentication successful. Enter transaction type (deposit/withdraw): deposit Enter amount: 200 Transaction successful. New balance: 1200.0 Enter transaction type (deposit/withdraw): withdraw Enter amount: 100 Transaction successful. New balance: 1100.0 Enter transaction type (deposit/withdraw): withdraw Enter amount: 2000 Error: Insufficient funds for this transaction. Enter transaction type (deposit/withdraw): exit Goodbye! ``` Notes: - Make sure to handle any potential exceptions and provide meaningful error messages. - Ensure your program is well-documented and follows best practices for exception handling and coding style.","solution":"class InsufficientFundsError(Exception): Raised when a withdrawal amount is greater than the available balance. pass class InvalidTransactionError(Exception): Raised when a transaction amount is not valid (negative amount). pass class AuthenticationError(Exception): Raised when user authentication fails. pass def authenticate_user(username: str, password: str) -> bool: Simulates user authentication by checking if the username is \\"admin\\" and the password is \\"password123\\". Raises AuthenticationError if the credentials are incorrect. if username == \\"admin\\" and password == \\"password123\\": return True else: raise AuthenticationError(\\"Invalid username or password.\\") def deposit(balance: float, amount: float) -> float: Adds amount to balance. Raises InvalidTransactionError if amount is non-positive. if amount <= 0: raise InvalidTransactionError(\\"Invalid transaction amount. Amount must be positive.\\") return balance + amount def withdraw(balance: float, amount: float) -> float: Subtracts amount from balance. Raises InvalidTransactionError if amount is non-positive. Raises InsufficientFundsError if amount exceeds the balance. if amount <= 0: raise InvalidTransactionError(\\"Invalid transaction amount. Amount must be positive.\\") if amount > balance: raise InsufficientFundsError(\\"Insufficient funds for this transaction.\\") return balance - amount"},{"question":"**Problem Statement** You are given a list of tuples where each tuple contains a movie name, the year it was released, and the director\'s name. Your task is to implement a function `insert_Movies(movies: List[Tuple[str, int, str]], new_movie: Tuple[str, int, str]) -> List[Tuple[str, int, str]]` that inserts a new movie into the list such that the list remains sorted by the release year. You should use the `bisect` module to accomplish this. # Function Signature ```python from typing import List, Tuple def insert_Movies(movies: List[Tuple[str, int, str]], new_movie: Tuple[str, int, str]) -> List[Tuple[str, int, str]]: pass ``` # Input * `movies`: A list of tuples, where each tuple is of the form `(name: str, released: int, director: str)`. The list is sorted by the `released` year. * `new_movie`: A tuple of the form `(name: str, released: int, director: str)` that needs to be inserted into the `movies` list. # Output * The function should return a new list of tuples, with the `new_movie` inserted in the correct position according to the release year. # Constraints * The length of the `movies` list will be between `0` and `10^6`. * The year will be a positive integer. * All strings will be non-empty and contain only alpha-numeric characters and spaces. # Example ```python movies = [ (\\"Jaws\\", 1975, \\"Spielberg\\"), (\\"Titanic\\", 1997, \\"Cameron\\"), (\\"The Birds\\", 1963, \\"Hitchcock\\"), (\\"Aliens\\", 1986, \\"Scott\\") ] new_movie = (\\"Love Story\\", 1970, \\"Hiller\\") print(insert_Movies(movies, new_movie)) # Output: [(\'The Birds\', 1963, \'Hitchcock\'), # (\'Love Story\', 1970, \'Hiller\'), # (\'Jaws\', 1975, \'Spielberg\'), # (\'Aliens\', 1986, \'Scott\'), # (\'Titanic\', 1997, \'Cameron\')] ``` # Notes 1. You are required to use `bisect` module functions to perform the insertion. 2. Ensure that the final list remains sorted by the release year after the movie is inserted.","solution":"from typing import List, Tuple import bisect def insert_Movies(movies: List[Tuple[str, int, str]], new_movie: Tuple[str, int, str]) -> List[Tuple[str, int, str]]: Inserts a new movie into the list of movies such that the list remains sorted by the release year. # Extract the release years to facilitate bisect insertion release_years = [movie[1] for movie in movies] # Find the insertion position position = bisect.bisect_left(release_years, new_movie[1]) # Insert the new movie at the calculated position movies.insert(position, new_movie) return movies"},{"question":"**Coding Assessment Question: Asynchronous Task Manager using `asyncio`** **Objective**: Write a function `asyncio_task_manager` that manages multiple asynchronous tasks and handles various exceptions defined in the `asyncio` module. **Function Signature**: ```python import asyncio async def asyncio_task_manager(tasks: list, timeout: float) -> list: pass ``` **Input**: - `tasks` (list): A list of `asyncio` tasks to be managed. Each task is a coroutine function that performs an asynchronous operation. - `timeout` (float): The maximum time (in seconds) to wait for each task to complete. **Output**: - Returns a list of tuples with the results of each task and any exceptions raised during their execution. Each tuple should have the format `(task_result, exception)`: - If a task completes successfully, `task_result` should be the result of the task and `exception` should be `None`. - If a task raises an exception, `task_result` should be `None` and `exception` should be the exception instance. **Constraints**: - Use `asyncio.gather` to manage the provided tasks. - Use `timeout` to limit the execution time for each task. - Handle the following exceptions explicitly: - `asyncio.TimeoutError` - `asyncio.CancelledError` - `asyncio.InvalidStateError` - `asyncio.SendfileNotAvailableError` - `asyncio.IncompleteReadError` - `asyncio.LimitOverrunError` - Other exceptions can be handled generically. **Example**: ```python import asyncio # Example coroutine function async def example_task(duration): await asyncio.sleep(duration) return duration async def asyncio_task_manager(tasks, timeout): results = [] for task in tasks: try: result = await asyncio.wait_for(task, timeout) results.append((result, None)) except asyncio.TimeoutError as e: results.append((None, e)) except asyncio.CancelledError as e: results.append((None, e)) except asyncio.InvalidStateError as e: results.append((None, e)) except asyncio.SendfileNotAvailableError as e: results.append((None, e)) except asyncio.IncompleteReadError as e: results.append((None, e)) except asyncio.LimitOverrunError as e: results.append((None, e)) except Exception as e: results.append((None, e)) return results # Test tasks = [example_task(1), example_task(2)] timeout = 1.5 print(asyncio.run(asyncio_task_manager(tasks, timeout))) # Output: [(1, None), (None, asyncio.TimeoutError())] ``` **Notes**: - The function should be able to handle any exceptions raised during the execution of each task and should continue managing the remaining tasks. - The `timeout` parameter ensures that tasks running longer than the specified limit are interrupted appropriately.","solution":"import asyncio async def asyncio_task_manager(tasks, timeout): Manages multiple asynchronous tasks and handles various exceptions. Args: tasks (list): A list of asyncio tasks to be managed. timeout (float): The maximum time (in seconds) to wait for each task to complete. Returns: list: A list of tuples with the results of each task and any exceptions raised during their execution. Each tuple has the format (task_result, exception). results = [] for task in tasks: try: result = await asyncio.wait_for(task, timeout) results.append((result, None)) except asyncio.TimeoutError as e: results.append((None, e)) except asyncio.CancelledError as e: results.append((None, e)) except asyncio.InvalidStateError as e: results.append((None, e)) except asyncio.SendfileNotAvailableError as e: results.append((None, e)) except asyncio.IncompleteReadError as e: results.append((None, e)) except asyncio.LimitOverrunError as e: results.append((None, e)) except Exception as e: results.append((None, e)) return results # Example coroutine function for testing async def example_task(duration): await asyncio.sleep(duration) return duration"},{"question":"Write a Python function `read_write_buffer_operations` that performs the following operations: 1. Create a NumPy array of random integers. 2. Use the Buffer Protocol (`memoryview` and `array` modules) to: - Extract a read-only buffer view of the array\'s data. - Create a writable buffer view of the array\'s data. 3. Modify the data in the writable buffer view and verify that the changes are reflected in the original array. 4. Use try-except blocks to handle any exceptions that may occur when obtaining buffer views. **Function Signature:** ```python import numpy as np def read_write_buffer_operations(): pass ``` **Input:** - No direct input. **Output:** - Return a tuple with: 1. The original NumPy array after modification. 2. The data from the read-only buffer view. 3. The data from the writable buffer view. **Constraints:** - You must use the NumPy library for array creation. - You must use the `memoryview` and `array` modules to perform buffer operations. **Example:** ```python output = read_write_buffer_operations() # Suppose random integers in the array were [5, 3, 7] # After modification, suppose changes were [10, 6, 14] # Output would be: # (array([10, 6, 14]), b\'nx00x00x00x06x00x00x00x0ex00x00x00\', array([10, 6, 14])) print(output) ``` **Notes:** - Ensure that your function handles exceptions properly and that modifications in the writable buffer reflect in the original NumPy array. - Use small integers to keep the example manageable and readable.","solution":"import numpy as np import array def read_write_buffer_operations(): try: # Step 1: Create a NumPy array of random integers. orig_array = np.random.randint(1, 10, size=5, dtype=np.int32) # Step 2: Use Buffer Protocol to extract a read-only buffer view. readonly_buffer = memoryview(orig_array).tobytes() # Step 3: Create a writable buffer view. writable_array = array.array(\'i\', orig_array) writable_buffer = memoryview(writable_array) # Modify the data in the writable buffer. for i in range(len(writable_buffer)): writable_buffer[i] *= 2 # Example modification; doubles the values # Verify changes in the original array. orig_array[:] = writable_array return orig_array, readonly_buffer, writable_array except Exception as e: print(\\"An error occurred:\\", e) return None"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s `histplot` function by performing data visualization tasks. **Dataset:** Use the \\"titanic\\" dataset from seaborn. This dataset contains the following columns: - `survived`: Survival (0 = No, 1 = Yes) - `pclass`: Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd) - `sex`: Sex - `age`: Age in years - `sibsp`: Number of siblings/spouses aboard the Titanic - `parch`: Number of parents/children aboard the Titanic - `fare`: Passenger fare - `embarked`: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) # Task: 1. **Load the dataset** and assign it to a variable called `titanic`. 2. **Plot a histogram** of the `age` column with: - Bins of width 5. - Kernel density estimate (`kde=True`). - Different colors for different classes (`pclass`). - Use `step` element for the histogram bars. 3. **Plot a histogram** of the `fare` column with: - Logarithmic x-axis. - Differentiating the survival status (`survived`). - Use `fill=False` to show unfilled bars. 4. **Plot a bivariate histogram** with: - `age` on the x-axis. - `fare` on the y-axis. - `hue` based on sex (`sex`). - Add a color bar for visual aid. **Constraints:** - Use seaborn for all visualizations. - Ensure all plots have appropriate titles and axis labels for clarity. **Expected Output:** - Three histograms as specified above, each properly labeled and shown inline. **Input:** ```python import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Histogram of Age sns.histplot(data=titanic, x=\\"age\\", bins=int((titanic[\'age\'].max() - titanic[\'age\'].min()) / 5), kde=True, hue=\\"pclass\\", element=\\"step\\") plt.title(\\"Histogram of Age with KDE and Pclass\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Frequency\\") plt.show() # Histogram of Fare sns.histplot(data=titanic, x=\\"fare\\", log_scale=True, hue=\\"survived\\", fill=False) plt.title(\\"Histogram of Fare with Survival Status - Log Scale\\") plt.xlabel(\\"Fare\\") plt.ylabel(\\"Frequency\\") plt.show() # Bivariate Histogram of Age and Fare sns.histplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"sex\\", cbar=True) plt.title(\\"Bivariate Histogram of Age and Fare\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Fare\\") plt.show() # Call the function plot_titanic_data() ``` **Notes:** - Ensure proper usage of seaborn functions and their parameters for histograms. - Verify that the histograms are displayed correctly with proper visual distinction as required by the tasks.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): # Load dataset titanic = sns.load_dataset(\\"titanic\\") # Histogram of Age age_bins = int((titanic[\'age\'].max() - titanic[\'age\'].min()) / 5) sns.histplot(data=titanic, x=\\"age\\", bins=age_bins, kde=True, hue=\\"pclass\\", element=\\"step\\") plt.title(\\"Histogram of Age with KDE and Pclass\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Frequency\\") plt.show() # Histogram of Fare sns.histplot(data=titanic, x=\\"fare\\", log_scale=True, hue=\\"survived\\", element=\\"step\\", fill=False) plt.title(\\"Histogram of Fare with Survival Status - Log Scale\\") plt.xlabel(\\"Fare\\") plt.ylabel(\\"Frequency\\") plt.show() # Bivariate Histogram of Age and Fare sns.histplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"sex\\", cbar=True) plt.title(\\"Bivariate Histogram of Age and Fare\\") plt.xlabel(\\"Age\\") plt.ylabel(\\"Fare\\") plt.show() # Call the function plot_titanic_data()"},{"question":"You are tasked with implementing a TorchScript function to perform element-wise operations on tensors stored within a custom class. This function will also demonstrate type annotations and type-checking using TorchScript-specific constructs. # Objective Implement a TorchScript-compatible custom class `TensorOperations` with methods to perform addition, multiplication, and element-wise logical operations on tensors. Ensure all methods and data attributes are annotated with appropriate types. Implement the class and a method named `operate_tensors` to: 1. Add a scalar value to all elements of a tensor. 2. Multiply the elements of two tensors element-wise. 3. Compute the logical AND of two tensors element-wise. 4. Return the results of these operations. # Requirements 1. Use `torch.jit.script` to compile the custom class and function. 2. Annotate all methods and instance data attributes with appropriate TorchScript types. 3. Ensure the function `operate_tensors` accepts the necessary parameters and returns a tuple containing the results of all three operations. # Constraints - You can assume all input tensors have the same shape. - Only the scalar addition can use a constant value, while other operations should use elements from the input tensors. - The tensor shapes can be arbitrary, but the operations should be applicable to all elements. # Function Signature ```python import torch from typing import Tuple class TensorOperations: def __init__(self, tensor1: torch.Tensor, tensor2: torch.Tensor): self.tensor1 = tensor1 self.tensor2 = tensor2 def add_scalar(self, scalar: float) -> torch.Tensor: pass def multiply_tensors(self) -> torch.Tensor: pass def logical_and(self) -> torch.Tensor: pass def operate_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor, scalar: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` # Example Usage ```python import torch tensor1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) tensor2 = torch.tensor([[5.0, 6.0], [7.0, 8.0]]) scalar = 1.5 results = operate_tensors(tensor1, tensor2, scalar) print(results) # Should print a tuple with three tensors: tensor1 + scalar, tensor1 * tensor2, tensor1 & tensor2 (type-casted as bool) ``` # Detailed Steps 1. Define the `TensorOperations` class with the specified methods. 2. Annotate all methods and instance attributes with appropriate types (refer to TorchScript types). 3. Implement the operations using TorchScript-supported expressions and statements. 4. Ensure the `operate_tensors` function creates an instance of `TensorOperations`, executes the operations, and returns the results as a tuple. # Submission Submit the implementation of the `TensorOperations` class and the `operate_tensors` function. Ensure the code is TorchScript-compatible by using `torch.jit.script` where necessary.","solution":"import torch from typing import Tuple class TensorOperations: def __init__(self, tensor1: torch.Tensor, tensor2: torch.Tensor) -> None: self.tensor1 = tensor1 self.tensor2 = tensor2 def add_scalar(self, scalar: float) -> torch.Tensor: return self.tensor1 + scalar def multiply_tensors(self) -> torch.Tensor: return self.tensor1 * self.tensor2 def logical_and(self) -> torch.Tensor: return self.tensor1.to(torch.bool) & self.tensor2.to(torch.bool) @torch.jit.script def operate_tensors(tensor1: torch.Tensor, tensor2: torch.Tensor, scalar: float) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: op = TensorOperations(tensor1, tensor2) added_tensor = op.add_scalar(scalar) multiplied_tensor = op.multiply_tensors() logical_and_tensor = op.logical_and() return added_tensor, multiplied_tensor, logical_and_tensor"},{"question":"**Problem Statement:** You are tasked with writing a Python function that will perform a series of calculations on a list of complex numbers using functions from the cmath module. The function will take a list of complex numbers as input, convert each complex number to its polar form, then perform a series of operations involving power, logarithms, and trigonometric functions, and finally classify the results. **Function Signature:** ```python def complex_operations(complex_list): Args: complex_list (List[complex]): A list of complex numbers. Returns: dict: A dictionary containing the results of the following operations: - \'polar_coordinates\': A list of tuples representing the polar coordinates of each complex number. - \'exponentials\': A list of complex numbers representing the exponential of each input complex number. - \'logarithms\': A list of complex numbers representing the natural logarithm of each input complex number. - \'sines\': A list of complex numbers representing the sine of each input complex number. - \'cosines\': A list of complex numbers representing the cosine of each input complex number. - \'is_finite\': A list of booleans indicating whether each input complex number is finite. pass ``` # Instructions: 1. **Input:** - The function will receive a single argument `complex_list`, which is a list of complex numbers. 2. **Output:** - The function should return a dictionary with the following keys and values: - \'polar_coordinates\': A list of tuples, where each tuple consists of the modulus and phase of the corresponding complex number from `complex_list`. - \'exponentials\': A list of complex numbers representing the exponentials of the complex numbers in `complex_list`. - \'logarithms\': A list of complex numbers representing the natural logarithms of the complex numbers in `complex_list`. - \'sines\': A list of complex numbers representing the sine of the complex numbers in `complex_list`. - \'cosines\': A list of complex numbers representing the cosine of the complex numbers in `complex_list`. - \'is_finite\': A list of booleans indicating whether each complex number in `complex_list` is finite. 3. **Function Behavior:** - Utilize the cmath library to perform each of the specified operations. - Ensure to handle any potential edge cases, such as very large or small values, or complex numbers with zero real and/or imaginary parts. # Example: ```python complex_list = [complex(1, 2), complex(0, -1), complex(-3, 0)] result = complex_operations(complex_list) print(result) # Expected output: # { # \'polar_coordinates\': [(2.23606797749979, 1.1071487177940904), (1.0, -1.5707963267948966), (3.0, 3.141592653589793)], # \'exponentials\': [(1.4686939399158851+2.2873552871788423j), (0.5403023058681398-0.8414709848078965j), (-0.9899924966004454+0.1411200080598672j)], # \'logarithms\': [(0.8047189562170503+1.1071487177940904j), (-0.6931471805599453-1.5707963267948966j), (1.0986122886681098+3.141592653589793j)], # \'sines\': [(3.165778513216168+1.959601041421606j), (-0.8414709848078965-0.5403023058681398j), (-0.1411200080598672+0.9899924966004454j)], # \'cosines\': [(2.0327230070196656-3.0518977991517997j), (0.5403023058681398+0.8414709848078965j), (-0.9899924966004454-0.1411200080598672j)], # \'is_finite\': [True, True, True] # } ``` # Constraints: 1. The input list `complex_list` will contain at least one complex number. 2. The elements of `complex_list` will be valid complex numbers that can be processed by cmath functions.","solution":"import cmath def complex_operations(complex_list): Args: complex_list (List[complex]): A list of complex numbers. Returns: dict: A dictionary containing the results of the following operations: - \'polar_coordinates\': A list of tuples representing the polar coordinates of each complex number. - \'exponentials\': A list of complex numbers representing the exponential of each input complex number. - \'logarithms\': A list of complex numbers representing the natural logarithm of each input complex number. - \'sines\': A list of complex numbers representing the sine of each input complex number. - \'cosines\': A list of complex numbers representing the cosine of each input complex number. - \'is_finite\': A list of booleans indicating whether each input complex number is finite. polar_coordinates = [cmath.polar(c) for c in complex_list] exponentials = [cmath.exp(c) for c in complex_list] logarithms = [cmath.log(c) for c in complex_list] sines = [cmath.sin(c) for c in complex_list] cosines = [cmath.cos(c) for c in complex_list] is_finite = [cmath.isfinite(c) for c in complex_list] return { \'polar_coordinates\': polar_coordinates, \'exponentials\': exponentials, \'logarithms\': logarithms, \'sines\': sines, \'cosines\': cosines, \'is_finite\': is_finite, }"},{"question":"<|Analysis Begin|> This documentation is extremely detailed and covers various clustering methods available in scikit-learn. It includes descriptions, parameters, scalability, use cases, and geometries of different clustering algorithms. There are also various examples provided, which show how these clustering methods can be applied in different scenarios. For designing a coding assessment question, the most pertinent sections discussed foundational concepts and practical applications of clustering methods. These include the workings of K-Means, Affinity Propagation, Mean-Shift, Spectral Clustering, Hierarchical Clustering (Agglomerative, Ward), DBSCAN, OPTICS, BIRCH, HDBSCAN, and evaluation metrics for clustering. Key points that can be emphasized in a question include: 1. Implementing clustering algorithms. 2. Applying clustering on different types of datasets. 3. Visualizing the clustering results. 4. Comparing the performance of different clustering methods using appropriate metrics. Given the thoroughness of the provided documentation, it is feasible to design a comprehensive and challenging coding question. <|Analysis End|> <|Question Begin|> **Question:** Implement and Compare Clustering Algorithms # Objective Your task is to: 1. Implement three clustering algorithms from scratch in Python (excluding any direct library functions that perform clustering but you can use utility functions for calculations and visualizations). 2. Apply these algorithms to a given dataset. 3. Evaluate and compare their performance using appropriate clustering evaluation metrics. # Instructions 1. **Implement the following clustering algorithms:** - K-Means - DBSCAN - Agglomerative Clustering (Hierarchical with \\"average\\" linkage criteria) 2. **Dataset:** You will apply your clustering algorithms to the `Iris` dataset provided by scikit-learn: ```python from sklearn import datasets iris = datasets.load_iris() X = iris.data y = iris.target ``` 3. **Execution Steps:** - Initialize and fit each algorithm on the dataset. - Use default parameters where applicable: - For K-Means: `n_clusters=3` - For DBSCAN: `eps=0.5`, `min_samples=5` - For Agglomerative Clustering: `n_clusters=3`, `linkage=\\"average\\"` 4. **Evaluation:** - Compute clustering evaluation metrics - Adjusted Rand Index (ARI), Silhouette Score, and Davies-Bouldin Index for each result. - Print these scores for each algorithm. 5. **Visualization:** - Plot the clustering results in a 2D scatter plot using the first two features of the Iris dataset. Use different colors to represent different clusters. # Constraints and Performance: - Implement the algorithms without directly using the clustering functions from scikit-learn. - Ensure the implementation is efficient with a time complexity that matches the characteristics of the algorithm. - Visualization should clearly show the distinction in clusters. # Expected Input and Output: - **Input:** A dataset with features: X and true labels: y - **Output:** Three sets of cluster labels, evaluation scores (ARI, Silhouette Score, and Davies-Bouldin Index) for each algorithm, and 2D scatter plots for cluster visualization. # Example: ```python # Example of expected output format Implementing K-Means... ARI: 0.73, Silhouette Score: 0.55, Davies-Bouldin Index: 0.66 Implementing DBSCAN... ARI: 0.45, Silhouette Score: 0.48, Davies-Bouldin Index: 0.75 Implementing Agglomerative Clustering... ARI: 0.71, Silhouette Score: 0.54, Davies-Bouldin Index: 0.69 # Visualizations as scatter plots ``` # Note: Provide your implementation in a well-documented Jupyter notebook (or script) that includes code cells for each step outlined above. Ensure your code is clean and well-commented for readability.","solution":"import numpy as np from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score import matplotlib.pyplot as plt from scipy.spatial.distance import cdist from sklearn.neighbors import NearestNeighbors # Load Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Standardize the dataset scaler = StandardScaler() X = scaler.fit_transform(X) def kmeans(X, n_clusters=3, max_iter=300, tol=1e-4): np.random.seed(42) initial_indices = np.random.permutation(len(X))[:n_clusters] centers = X[initial_indices] for i in range(max_iter): labels = np.argmin(cdist(X, centers), axis=1) new_centers = np.array([X[labels == j].mean(axis=0) for j in range(n_clusters)]) if np.linalg.norm(new_centers - centers) < tol: break centers = new_centers return labels def dbscan(X, eps=0.5, min_samples=5): neighbors = NearestNeighbors(n_neighbors=min_samples).fit(X) distances, _ = neighbors.kneighbors(X) core_point_mask = distances[:, -1] <= eps clusters = -np.ones(len(X)) cluster_id = 0 for point_idx in np.where(core_point_mask)[0]: if clusters[point_idx] != -1: continue clusters[point_idx] = cluster_id neighbors = np.where(cdist(X[point_idx].reshape(1, -1), X)[0] <= eps)[0].tolist() while neighbors: current_point = neighbors.pop() if clusters[current_point] == -1: clusters[current_point] = cluster_id point_neighbors = np.where(cdist(X[current_point].reshape(1, -1), X)[0] <= eps)[0] if len(point_neighbors) >= min_samples: neighbors.extend(point_neighbors) cluster_id += 1 return clusters def agglomerative_clustering(X, n_clusters=3, linkage=\'average\'): from scipy.cluster.hierarchy import linkage as scipy_linkage, fcluster linked = scipy_linkage(X, method=linkage) labels = fcluster(linked, t=n_clusters, criterion=\'maxclust\') - 1 return labels # Visualization function def plot_clusters(X, labels, title): plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\'viridis\', marker=\'o\', edgecolor=\'k\') plt.title(title) plt.show() # Apply algorithms kmeans_labels = kmeans(X, n_clusters=3) dbscan_labels = dbscan(X, eps=0.5, min_samples=5) agglomerative_labels = agglomerative_clustering(X, n_clusters=3) # Evaluation metrics def evaluate_clustering(X, labels, true_labels): ari = adjusted_rand_score(true_labels, labels) sil_score = silhouette_score(X, labels) if len(set(labels)) > 1 else -1 db_score = davies_bouldin_score(X, labels) if len(set(labels)) > 1 else float(\'inf\') return ari, sil_score, db_score # Metrics for K-Means print(\\"K-Means Clustering\\") km_ari, km_sil, km_db = evaluate_clustering(X, kmeans_labels, y) print(f\\"ARI: {km_ari:.2f}, Silhouette Score: {km_sil:.2f}, Davies-Bouldin Index: {km_db:.2f}\\") # Metrics for DBSCAN print(\\"DBSCAN Clustering\\") dbscan_labels = [int(label) for label in dbscan_labels] db_ari, db_sil, db_db = evaluate_clustering(X, dbscan_labels, y) print(f\\"ARI: {db_ari:.2f}, Silhouette Score: {db_sil:.2f}, Davies-Bouldin Index: {db_db:.2f}\\") # Metrics for Agglomerative Clustering print(\\"Agglomerative Clustering\\") agg_ari, agg_sil, agg_db = evaluate_clustering(X, agglomerative_labels, y) print(f\\"ARI: {agg_ari:.2f}, Silhouette Score: {agg_sil:.2f}, Davies-Bouldin Index: {agg_db:.2f}\\") # Visualize clusters plt.figure(figsize=(15, 5)) plt.subplot(1, 3, 1) plot_clusters(X, kmeans_labels, \\"K-Means Clustering\\") plt.subplot(1, 3, 2) plot_clusters(X, dbscan_labels, \\"DBSCAN Clustering\\") plt.subplot(1, 3, 3) plot_clusters(X, agglomerative_labels, \\"Agglomerative Clustering\\") plt.show()"},{"question":"Objective: Implement a Python function to interact with an IMAP server. The function should establish a connection, authenticate, list all mailboxes, select a specific mailbox, fetch email headers, and handle exceptions properly. Requirements: 1. The function should connect to an IMAP server (use `imap.example.com` as the placeholder hostname). 2. Authenticate with the provided username and password. 3. List all mailboxes. 4. Select the \'INBOX\' mailbox. 5. Fetch the headers of the first 10 emails in the \'INBOX\'. 6. Handle any exceptions by closing the connection gracefully and outputting a relevant error message. Function Signature: ```python import imaplib from typing import List def fetch_email_headers(username: str, password: str) -> List[str]: pass ``` Input: - `username` (str): The username for authentication. - `password` (str): The password for authentication. Output: - (List of str): A list of strings, each containing the header of an email. Example: ```python headers = fetch_email_headers(\\"test_user\\", \\"test_password\\") for header in headers: print(header) ``` Constraints: 1. You must use the `IMAP4_SSL` class for a secure connection. 2. Fetch headers using the `FETCH` command with the `BODY.PEEK[HEADER.FIELDS (FROM TO SUBJECT DATE)]` specification. 3. Ensure to logout and close the connection properly in case of an error or after successful execution. 4. Assume the provided IMAP server (`imap.example.com`) is always available and correct. Implementation Details: The following helper functions and variables are already imported and can be used: - `imaplib.IMAP4_SSL` - `imaplib.IMAP4.error` - `imaplib.IMAP4.abort` - `imaplib.IMAP4.readonly` **Hint:** Refer to the `IMAP4` class methods and their usage examples for guidance on implementing the required functionalities. Handle common exceptions to ensure the connection is closed gracefully.","solution":"import imaplib from typing import List def fetch_email_headers(username: str, password: str) -> List[str]: try: # Connect to the server imap_server = imaplib.IMAP4_SSL(\'imap.example.com\') # Authenticate imap_server.login(username, password) # List all mailboxes mailboxes = imap_server.list() # Select the INBOX mailbox imap_server.select(\'INBOX\') # Search for email IDs status, email_ids = imap_server.search(None, \'ALL\') if status != \'OK\': raise Exception(\'Failed to retrieve email IDs\') # Fetch headers for the first 10 emails email_ids = email_ids[0].split() headers = [] for email_id in email_ids[:10]: status, data = imap_server.fetch(email_id, \'(BODY.PEEK[HEADER.FIELDS (FROM TO SUBJECT DATE)])\') if status != \'OK\': raise Exception(f\'Failed to fetch email headers for ID {email_id}\') headers.append(data[0][1].decode()) return headers except imaplib.IMAP4.error as e: print(f\\"IMAP error occurred: {e}\\") return [] except Exception as e: print(f\\"An error occurred: {e}\\") return [] finally: try: imap_server.logout() except: pass"},{"question":"**Objective**: Demonstrate your understanding of the `EmailMessage` class for handling and manipulating email messages. **Problem Statement**: You are given the task of managing an email message that contains multiple parts. You need to create an email message, add different types of content to it, and then extract specific information from it. **Requirements**: 1. **Create an `EmailMessage` object**: - Add a subject header with the value \\"Complex Email\\". - Add a `From` header with the value \\"sender@example.com\\". - Add a `To` header with the value \\"recipient@example.com\\". 2. **Add a plain text part**: - The content should be \\"This is the text part of the email\\". 3. **Add an HTML part**: - The content should be \\"<html><body><h1>This is the HTML part of the email</h1></body></html>\\". 4. **Add an attachment**: - The content should be binary data representing a simple text file, e.g., `b\'This is the attachment content.\'`. 5. **Extract and return specific information**: - Return the MIME type of the content that forms the body of the email. - Return a list of all the filenames of the attachments in the email (if they have filenames associated). **Function Signature**: ```python def manage_email() -> tuple: # Your implementation here ``` **Expected Output**: The function should return a tuple containing: 1. The MIME type string of the main body content of the email. 2. A list of filenames of all the attachments in the email. **Constraints**: - Use the `EmailMessage` class and its methods effectively. - Ensure the correct MIME types are used for each part. - Handle the email content and attachments appropriately. **Example**: The function should return: ```python (\\"text/plain\\", [\'simple.txt\']) ``` where `\\"text/plain\\"` is the MIME type of the main content and `[\'simple.txt\']` is the filename of the attachment.","solution":"from email.message import EmailMessage def manage_email(): # Create an EmailMessage object msg = EmailMessage() # Add headers msg[\'Subject\'] = \'Complex Email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' # Add plain text part msg.set_content(\'This is the text part of the email\') # Add HTML part msg.add_alternative(\\"<html><body><h1>This is the HTML part of the email</h1></body></html>\\", subtype=\'html\') # Add attachment attachment_content = b\'This is the attachment content.\' msg.add_attachment(attachment_content, maintype=\'application\', subtype=\'octet-stream\', filename=\'simple.txt\') # Extract the MIME type of the main body content main_mime_type = msg.get_content_type() # Extract the filenames of all attachments attachment_filenames = [] for part in msg.iter_attachments(): if part.get_filename(): attachment_filenames.append(part.get_filename()) return main_mime_type, attachment_filenames"},{"question":"<|Analysis Begin|> The provided documentation covers various functions related to importing modules in Python. These functions belong to the stable ABI (Application Binary Interface) and include the ability to import, reload, and manage modules at a lower level than the standard Python `import` statement. Key functions include: - `PyImport_ImportModule`: Simplified interface to import a module. - `PyImport_ImportModuleEx`: More detailed import functionality similar to the built-in `__import__`. - `PyImport_ReloadModule`: Reload an imported module. - `PyImport_ExecCodeModule`: Load a module from a code object. - `PyImport_GetMagicNumber`: Retrieve the magic number used in Python bytecode files. - `PyImport_GetModuleDict`: Get the dictionary used for module administration. - `PyImport_AppendInittab` and `PyImport_ExtendInittab`: Add modules to the built-in modules table. The functions make use of intermediate-level operations and provide more control over the import process than usual Python script imports. These functions are useful for embedding Python or controlling environments with specific constraints. <|Analysis End|> <|Question Begin|> **Coding Assessment Question: Python Module Import and Management** As a developer, you are often required to manage and manipulate Python modules dynamically, especially when dealing with large systems or applications that embed Python. In this exercise, you are required to create a Python script that imports, re-imports, and fetches module details using the lower-level C-API-like interface described in the provided documentation. # Task: Write a Python script that performs the following operations, using the provided functions wherever applicable: 1. Import a specified module by name. 2. Check if the module is already imported. If not, handle the import failure. 3. Reload the module. 4. Retrieve and display the module\'s details from the module administration dictionary. 5. Display the Python magic number used for bytecode files. # Requirements: - Implement the following functions in your script: 1. `import_module(module_name: str) -> ModuleType` 2. `is_module_imported(module_name: str) -> bool` 3. `reload_module(module: ModuleType) -> ModuleType` 4. `get_module_details(module_name: str) -> dict` 5. `get_bytecode_magic_number() -> int` # Function Details: 1. `import_module(module_name: str) -> ModuleType` - Imports the module with the given name and returns the module object. - If the import fails, it should print an error message and return `None`. 2. `is_module_imported(module_name: str) -> bool` - Checks if a module is already imported by looking in the module administration dictionary (`sys.modules`). - Returns `True` if the module is found, `False` otherwise. 3. `reload_module(module: ModuleType) -> ModuleType` - Reloads the given module and returns the reloaded module object. - If the reload fails, it should print an error message and return `None`. 4. `get_module_details(module_name: str) -> dict` - Retrieves and returns details of the module from the module administration dictionary. - The details should include the module\'s name, file location, and any submodules if applicable. - If the module is not found, return an empty dictionary. 5. `get_bytecode_magic_number() -> int` - Retrieves and returns the magic number used in Python bytecode files. - If retrieval fails, it should return `-1`. # Input: - The script should take the module name as input from the user. - Use standard input (e.g., `input()`) to get the module name. # Output: - The script should print appropriate messages indicating the success or failure of each operation. - Display the module\'s details and the bytecode magic number as specified in the requirements. # Example: ```python Enter module name: math Importing module... Checking if module is imported... Module already imported: True Reloading module... Getting module details... Module Details: {\'name\': \'math\', \'file\': \'/usr/lib/python3.10/lib-dynload/math.cpython-310-x86_64-linux-gnu.so\', \'submodules\': []} Python Bytecode Magic Number: 3310 ``` # Constraints: - Ensure proper error handling for each function. - The script should not crash due to exceptions. - Assume the Python environment supports all functions as described in the documentation. **Note**: Since this is a theoretical exercise based on the provided documentation, you may need to simulate some of the functionalities that are closely related to the C-API described.","solution":"import sys import importlib import imp def import_module(module_name: str): Imports the module with the given name and returns the module object. If the import fails, it prints an error message and returns None. try: module = importlib.import_module(module_name) print(f\\"Successfully imported module \'{module_name}\'\\") return module except ImportError as e: print(f\\"Failed to import module \'{module_name}\': {e}\\") return None def is_module_imported(module_name: str) -> bool: Checks if a module is already imported by looking in the module administration dictionary. Returns True if the module is found, False otherwise. return module_name in sys.modules def reload_module(module): Reloads the given module and returns the reloaded module object. If the reload fails, it prints an error message and returns None. try: reloaded_module = importlib.reload(module) print(f\\"Successfully reloaded module \'{module.__name__}\'\\") return reloaded_module except Exception as e: print(f\\"Failed to reload module \'{module.__name__}\': {e}\\") return None def get_module_details(module_name: str) -> dict: Retrieves and returns details of the module from the module administration dictionary. The details should include the module\'s name, file location, and any submodules if applicable. If the module is not found, returns an empty dictionary. if module_name not in sys.modules: return {} module = sys.modules[module_name] details = { \'name\': module.__name__, \'file\': getattr(module, \'__file__\', None), \'submodules\': [] } for attr in dir(module): if isinstance(getattr(module, attr), type(sys)): details[\'submodules\'].append(attr) return details def get_bytecode_magic_number() -> int: Retrieves and returns the magic number used in Python bytecode files. If retrieval fails, returns -1. try: magic_number = imp.get_magic() return int.from_bytes(magic_number, byteorder=\'little\') except Exception as e: print(f\\"Failed to retrieve bytecode magic number: {e}\\") return -1"},{"question":"Complex Data Manipulation with MultiIndex in pandas Problem Statement: You have been provided with a dataset related to sales statistics of multiple products across different regions and time periods. Here\'s the dataset: | Region | Year | Quarter | Product | Sales | |--------|------|---------|---------|---------| | North | 2022 | Q1 | A | 45000.0 | | North | 2022 | Q1 | B | 32000.0 | | North | 2022 | Q2 | A | 47000.0 | | North | 2022 | Q2 | B | 34000.0 | | South | 2022 | Q1 | A | 56000.0 | | South | 2022 | Q1 | B | 44000.0 | | South | 2022 | Q2 | A | 58000.0 | | South | 2022 | Q2 | B | 46000.0 | Using the data above, perform the following tasks: 1. **Creating the MultiIndex DataFrame:** - Create a pandas DataFrame with a MultiIndex based on the columns \'Region\', \'Year\', and \'Quarter\'. - The `Sales` column should be used as the values in this DataFrame. 2. **Basic Indexing and Slicing:** - Retrieve the Total Sales in \'North\' region for the year 2022 during Q1 and Q2. 3. **Advanced Indexing:** - Retrieve Sales data for all products in the \'South\' region for all quarters using `.loc`. - Using `pd.IndexSlice`, select the sales data of Product \'A\' for both \'North\' and \'South\' regions, and for all quarters. 4. **Level Manipulation:** - Swap the levels of the MultiIndex so that \'Quarter\' is now the first level and \'Region\' comes last. - Sort the DataFrame based on the new MultiIndex. - Reset the index back to original levels after sorting. 5. **Analysis Tasks:** - Calculate the total sales for each Product across all regions and quarters. - Calculate the average sales for each region by year. Input Data: ```python import pandas as pd data = { \'Region\': [\'North\', \'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'South\'], \'Year\': [2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022], \'Quarter\': [\'Q1\', \'Q1\', \'Q2\', \'Q2\', \'Q1\', \'Q1\', \'Q2\', \'Q2\'], \'Product\': [\'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\'], \'Sales\': [45000.0, 32000.0, 47000.0, 34000.0, 56000.0, 44000.0, 58000.0, 46000.0] } df = pd.DataFrame(data) ``` Expected Outputs: 1. **MultiIndex DataFrame:** ```plaintext Sales Region Year Quarter North 2022 Q1 A 45000.0 B 32000.0 Q2 A 47000.0 B 34000.0 South 2022 Q1 A 56000.0 B 44000.0 Q2 A 58000.0 B 46000.0 ``` 2. **Basic Indexing and Slicing:** ```plaintext Region Year Quarter North 2022 Q1 A 45000.0 B 32000.0 Q2 A 47000.0 B 34000.0 Name: Sales, dtype: float64 ``` 3. **Advanced Indexing using `.loc` and `pd.IndexSlice`:** ```plaintext df.loc[\'South\'] df.loc[pd.IndexSlice[:, :, :, \'A\'], :] ``` 4. **Level Manipulation:** ```plaintext df.swaplevel(\'Quarter\', \'Region\').sort_index() df.sort_index(level=0).swaplevel(0, 1) ``` 5. **Analysis Tasks:** - Total Sales for Each Product: ```plaintext Product A 206000.0 B 156000.0 Name: Sales, dtype: float64 ``` - Average Sales for Each Region by Year: ```plaintext df.groupby([\'Region\', \'Year\'])[\'Sales\'].mean() ``` Constraints: - Use only pandas for DataFrame operations. - Ensure the MultiIndex is appropriately set up. - Avoid using loops; leverage pandas\' vectorized operations.","solution":"import pandas as pd def create_multiindex_dataframe(data): df = pd.DataFrame(data) df.set_index([\'Region\', \'Year\', \'Quarter\', \'Product\'], inplace=True) return df def total_sales_north_2022_Q1_Q2(df): return df.loc[(\'North\', 2022), :] def sales_data_south(df): return df.loc[\'South\'] def sales_data_product_A(df): return df.loc[pd.IndexSlice[:, :, :, \'A\'], :] def swap_levels_and_sort(df): df_swapped = df.swaplevel(\'Quarter\', \'Region\').sort_index() df_sorted_original = df_swapped.swaplevel(\'Region\', \'Quarter\').sort_index() return df_sorted_original def total_sales_per_product(df): return df.groupby(level=\'Product\').sum()[\'Sales\'] def average_sales_per_region_by_year(df): return df.groupby([\'Region\', \'Year\'])[\'Sales\'].mean() # Provided data data = { \'Region\': [\'North\', \'North\', \'North\', \'North\', \'South\', \'South\', \'South\', \'South\'], \'Year\': [2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022], \'Quarter\': [\'Q1\', \'Q1\', \'Q2\', \'Q2\', \'Q1\', \'Q1\', \'Q2\', \'Q2\'], \'Product\': [\'A\', \'B\', \'A\', \'B\', \'A\', \'B\', \'A\', \'B\'], \'Sales\': [45000.0, 32000.0, 47000.0, 34000.0, 56000.0, 44000.0, 58000.0, 46000.0] } # Create MultiIndex DataFrame for further operations multiindex_df = create_multiindex_dataframe(data)"},{"question":"# Pandas Coding Assessment Question You have been provided with a CSV file named `employees.csv` which contains columns: `employee_id`, `employee_name`, `department`, and `salary`. While processing this data, you notice that there are duplicate `employee_id` entries. Your task is to clean the data by resolving these duplicates and then ensuring no duplicate `employee_id` entries are introduced in further operations. Requirements: 1. Write a function `clean_employees_data` that reads the CSV file, detects duplicate `employee_id` entries, and resolves duplicates by keeping the first occurrence. 2. The function should then configure the DataFrame to disallow further duplicate `employee_id` entries. 3. Finally, the function should return the cleaned DataFrame. Function Signature: ```python def clean_employees_data(file_path: str) -> pd.DataFrame: pass ``` Input: - `file_path`: A string representing the path to the CSV file. Output: - A pandas DataFrame with duplicate `employee_id` entries resolved and further duplicates disallowed. Constraints: - You are not allowed to change the column structure of the DataFrame. - Use appropriate pandas methods to detect and resolve duplicates. - Raising exceptions if duplicates are introduced subsequently should be demonstrated. Example: Suppose the content of `employees.csv` is: ``` employee_id,employee_name,department,salary 1,John Doe,IT,70000 2,Jane Doe,HR,80000 1,John Smith,IT,75000 3,Emily Davis,Finance,68000 4,Michael Brown,IT,72000 ``` Expected output after running your function: ```python employee_id employee_name department salary 0 1 John Doe IT 70000 1 2 Jane Doe HR 80000 3 3 Emily Davis Finance 68000 4 4 Michael Brown IT 72000 ``` Once duplicates are resolved, any further attempt to introduce duplicate `employee_id` should raise an error: ```python # Assume df is the DataFrame returned from clean_employees_data try: df.loc[5] = [1, \'New Employee\', \'Finance\', 60000] except pandas.errors.DuplicateLabelError as e: print(e) ``` Hints: - Use `pd.read_csv()` to read the CSV file. - Use `DataFrame.index.duplicated()` or `DataFrame.groupby` to handle duplicates. - Use `DataFrame.set_flags(allows_duplicate_labels=False)` to disallow duplicate entries.","solution":"import pandas as pd def clean_employees_data(file_path: str) -> pd.DataFrame: Reads the CSV file, removes duplicate `employee_id` entries by keeping the first occurrence, and disallows further duplicates. Parameters: - file_path (str): Path to the CSV file. Returns: - pd.DataFrame: A cleaned DataFrame with no duplicate `employee_id` entries. # Read the CSV file df = pd.read_csv(file_path) # Remove rows with duplicate employee_id by keeping the first occurrence df = df.drop_duplicates(subset=\'employee_id\', keep=\'first\') # Disallow further duplicates df = df.set_index(\'employee_id\', verify_integrity=True) df = df.reset_index() # Resetting index so that it\'s in the original format return df"},{"question":"Question: Optimizing DataFrame Computations with Cython and Numba You are provided with a pandas DataFrame containing data about various mathematical functions and their parameters. Your task is to implement a function that performs numerical integration based on the DataFrame\'s contents. Your solution should then be optimized for performance using Cython and optionally Numba. # Input You are given a pandas DataFrame `df` with the following columns: - `a`: a column of floats, representing the start of the integration range. - `b`: a column of floats, representing the end of the integration range. - `N`: a column of integers, representing the number of steps in the numerical integration. - `func`: a column of strings, representing the name of the function to be integrated. Supported functions are `\\"linear\\"` and `\\"quadratic\\"`. # Task 1. Write a Python function `integrate_function` that performs the following: - Reads the DataFrame `df`. - For each row, applies the specified function (`func`) to the range between `a` and `b` using `N` steps. - Returns a new DataFrame with the results of the integration. 2. Optimize the `integrate_function` using Cython. You will need to: - Define the mathematical functions `linear(x)` and `quadratic(x)` in Cython. - Define the integration function in Cython to use the specified mathematical functions and integrate over the given range. - Modify your Python code to use these optimized Cython functions. 3. (Optional) Further optimize the `integrate_function` using Numba for JIT compilation. # Implementation Details: - **Python**: - Define mathematical functions as simple Python functions. - Implement numerical integration manually using loops. - **Cython**: - Convert the mathematical functions and the integration function to Cython. - Use `cdef` and `cpdef` for type annotations. - Optimize for performance by disabling bounds and wraparound checks where safe. - **Numba** (Optional): - Use the `@jit` decorator to compile the mathematical functions and the integration function. # Example Usage ```python import pandas as pd import numpy as np from numba import jit df = pd.DataFrame({ \\"a\\": [0, 0], \\"b\\": [1, 2], \\"N\\": [1000, 2000], \\"func\\": [\\"linear\\", \\"quadratic\\"] }) # Example mathematical functions def linear(x): return x def quadratic(x): return x**2 def integrate_function(df): # Implement the main task here pass # Call the function and print the results result_df = integrate_function(df) print(result_df) ``` # Output The output should be a DataFrame with the same index as the input `df`, but with an additional column `integral` that contains the computed integral for each row. # Constraints - Use vectorized operations where possible in your Python code. - Ensure that your Cython implementation is significantly faster than the pure Python implementation. - Keep your implementations clear and well-documented. # Expectations - Demonstrate the ability to work with pandas DataFrames. - Show an understanding of performance optimization techniques using Cython and Numba. - Provide a working solution that is efficient and correct. Good luck!","solution":"import pandas as pd import numpy as np def linear(x): return x def quadratic(x): return x**2 def integrate_function(df): def integrate(a, b, N, func): dx = (b - a) / N total_sum = 0.0 for i in range(N): x = a + i*dx if func == \\"linear\\": total_sum += linear(x) elif func == \\"quadratic\\": total_sum += quadratic(x) return total_sum * dx results = [] for index, row in df.iterrows(): a = row[\'a\'] b = row[\'b\'] N = row[\'N\'] func = row[\'func\'] integral = integrate(a, b, N, func) results.append(integral) result_df = df.copy() result_df[\'integral\'] = results return result_df"},{"question":"# Python Coding Assessment Question You are tasked with implementing a function that interacts with a Unix database using the `dbm` module. Your function should create or open a database, perform a series of operations, and handle errors appropriately. Function Signature ```python def manage_db(filename: str, operations: list) -> list: pass ``` Input - `filename` (str): The name of the database file to be created or opened. - `operations` (list): A list of operations to perform on the database. Each operation is a tuple where: - The first element is a string representing the type of operation (`\\"set\\"`, `\\"get\\"`, `\\"delete\\"`). - The second element is a key (str). - The third element (optional) is the value (str) for `\\"set\\"` operations. Output - A list of results for each `\\"get\\"` operation. If a key does not exist during a `\\"get\\"` operation, append `None` to the results list. Constraints 1. You must use the `dbm` module to interact with the database. 2. Keys and values must be handled as bytes when performing operations. 3. Handle exceptions properly, returning meaningful messages for errors (such as file IO errors or invalid operations). Example ```python # Example usage of the function operations = [ (\\"set\\", \\"name\\", \\"Alice\\"), (\\"get\\", \\"name\\"), (\\"set\\", \\"age\\", \\"30\\"), (\\"get\\", \\"age\\"), (\\"delete\\", \\"name\\"), (\\"get\\", \\"name\\") ] result = manage_db(\\"example_db\\", operations) print(result) ``` Expected Output: ```python [b\'Alice\', b\'30\', None] ``` # Additional Information - Use the `\'c\'` flag to open the database for reading and writing, creating it if it does not exist. - Automatically handle the conversion of strings to bytes when performing operations. - If an operation other than `\\"set\\"`, `\\"get\\"`, or `\\"delete\\"` is encountered, raise a `ValueError` with an appropriate message. Implement the `manage_db` function to meet the above requirements. Ensure your code is robust, handles exceptions gracefully, and works accurately with the `dbm` module.","solution":"import dbm def manage_db(filename: str, operations: list) -> list: results = [] try: with dbm.open(filename, \'c\') as db: for operation in operations: if operation[0] == \\"set\\": key, value = operation[1], operation[2] db[key.encode()] = value.encode() elif operation[0] == \\"get\\": key = operation[1] value = db.get(key.encode()) results.append(value) elif operation[0] == \\"delete\\": key = operation[1] if key.encode() in db: del db[key.encode()] else: raise ValueError(f\\"Unsupported operation: {operation[0]}\\") except Exception as e: print(f\\"Error: {e}\\") return results"},{"question":"Command-Line Argument Parser Assessment # Objective: Design and implement a command-line application in Python that accepts and processes various types of command-line arguments using the `argparse` library. # Requirements: 1. **Function Implementation**: - Implement a function `parse_arguments` that sets up an `argparse.ArgumentParser` and adds the following arguments: - A required positional argument `file_path` representing the path to a text file. - An optional argument `--max-lines` (or `-m`) that specifies the maximum number of lines to read from the file. It should default to reading the entire file if not provided. - An optional argument `--verbose` (or `-v`) that, when provided, will print additional information during processing. This is a flag (boolean switch). 2. **File Processing**: - Implement a function `process_file` that: - Accepts two arguments: `file_path` and `max_lines`. - Reads the file specified by `file_path` and returns a list containing the first `max_lines` lines of the file. If `max_lines` is None, read the entire file. 3. **Verbose Output**: - Modify the `process_file` function to print the number of lines read and the file size (in bytes) if the `verbose` flag is set. 4. **Main Execution Block**: - Implement a `main` function that: - Calls `parse_arguments` to parse the command-line arguments. - Calls `process_file` with the parsed arguments. - Prints the processed lines. 5. **Error Handling**: - The script should handle errors such as file not found or permissions issues and print appropriate error messages. # Expected Input and Output Formats: ```python # Example of using the command-line application: # python script.py /path/to/file.txt --max-lines 10 --verbose # For the above command, if the file contains: line 1 line 2 line 3 ... line 20 # The script should output: # Reading up to 10 lines from /path/to/file.txt # File size: 60 bytes # [\'line 1n\', \'line 2n\', ..., \'line 10n\'] ``` # Constraints: - The `file_path` must be a valid path to an existing text file. - The `max_lines` should be a non-negative integer if provided. # Performance Requirements: - Handle files up to 1GB efficiently without loading the entire file into memory, if possible. **Here is a starting template for your solution:** ```python import argparse def parse_arguments(): # TODO: Implement argument parsing requirements pass def process_file(file_path, max_lines=None, verbose=False): # TODO: Implement file processing and verbose output requirements pass def main(): # TODO: Implement the main function that integrates parsing and file processing pass if __name__ == \\"__main__\\": main() ``` Good luck!","solution":"import argparse import os def parse_arguments(): parser = argparse.ArgumentParser(description=\\"Process a text file with optional line limits and verbosity.\\") parser.add_argument(\'file_path\', type=str, help=\\"Path to the text file to read\\") parser.add_argument(\'--max-lines\', \'-m\', type=int, default=None, help=\\"Maximum number of lines to read from the file\\") parser.add_argument(\'--verbose\', \'-v\', action=\'store_true\', help=\\"Print additional information during processing\\") return parser.parse_args() def process_file(file_path, max_lines=None, verbose=False): if verbose: print(f\\"Reading up to {max_lines if max_lines else \'all\'} lines from {file_path}\\") lines = [] try: with open(file_path, \'r\') as file: for i, line in enumerate(file): if max_lines is not None and i >= max_lines: break lines.append(line) if verbose: file_size = os.path.getsize(file_path) print(f\\"Number of lines read: {len(lines)}\\") print(f\\"File size: {file_size} bytes\\") except FileNotFoundError: print(f\\"Error: The file \'{file_path}\' was not found.\\") except PermissionError: print(f\\"Error: Permission denied for file \'{file_path}\'.\\") return lines def main(): args = parse_arguments() lines = process_file(args.file_path, args.max_lines, args.verbose) print(lines) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implement a Custom k-Nearest Neighbors Classifier Objective You are required to implement a custom k-nearest neighbors (k-NN) classifier from scratch without using `KNeighborsClassifier` from `scikit-learn`. Your classifier should be capable of handling both dense and sparse input matrices and support multiple distance metrics. Task Description 1. Implement the class `CustomKNeighborsClassifier` with the following methods: - `__init__(self, n_neighbors=5, algorithm=\'auto\', metric=\'euclidean\')`: Initialize the classifier with the specified number of neighbors (`n_neighbors`), the algorithm for neighbor search (`algorithm`), and the distance metric (`metric`). - `fit(self, X, y)`: Fit the model using the training data `X` (`array-like` or `sparse matrix`) and target values `y` (`array-like`). - `kneighbors(self, X, n_neighbors=None, return_distance=True)`: Find the `n_neighbors` neighbors of each point in `X` and return distances and indices of the neighbors. - `predict(self, X)`: Predict the class labels for the provided `X`. 2. Your implementation should handle both dense and sparse matrices for the input `X`. You can use `csr_matrix` from `scipy.sparse` for sparse matrix creation. 3. Implement support for the following distance metrics: - `\'euclidean\'` - `\'manhattan\'` - `\'chebyshev\'` 4. Ensure that your implementation uses `BallTree` or `KDTree` for neighbor searches, or defaults to a brute-force approach if the dataset characteristics dictate so. 5. Demonstrate the usage of your `CustomKNeighborsClassifier` with the following example: - Use a small synthetic dataset to fit the classifier and make predictions. - Compare the predictions against the standard `KNeighborsClassifier` from `scikit-learn` to verify accuracy. Constraints - Do not use `KNeighborsClassifier` or any higher-level API from `scikit-learn` for the implementation of the classifier. Only low-level neighbor search classes like `BallTree`, `KDTree` and utilities like `pairwise_distances` can be used. - Implementations should be efficient and leverage vectorized operations wherever possible. - Ensure that your code is well-documented and includes appropriate error handling and checks. Example Usage ```python from scipy.sparse import csr_matrix import numpy as np from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from custom_knn import CustomKNeighborsClassifier # assuming your class is in this file # Example data X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]) y = np.array([0, 0, 1, 1, 1]) X_sparse = csr_matrix(X) # Train-test split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Fit the custom KNN classifier custom_knn = CustomKNeighborsClassifier(n_neighbors=3, algorithm=\'auto\', metric=\'euclidean\') custom_knn.fit(X_train, y_train) # Make predictions y_pred_custom = custom_knn.predict(X_test) # Compare with sklearn\'s KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=3, algorithm=\'auto\', metric=\'euclidean\') knn.fit(X_train, y_train) y_pred_sklearn = knn.predict(X_test) # Verify that both predictions are the same assert np.array_equal(y_pred_custom, y_pred_sklearn), \\"The custom KNN classifier does not match sklearn\'s predictions.\\" print(\\"Success! The custom KNN classifier matches sklearn\'s predictions.\\") ```","solution":"import numpy as np from scipy.spatial import distance from scipy.sparse import csr_matrix, issparse from sklearn.neighbors import BallTree, KDTree class CustomKNeighborsClassifier: def __init__(self, n_neighbors=5, algorithm=\'auto\', metric=\'euclidean\'): self.n_neighbors = n_neighbors self.algorithm = algorithm self.metric = metric self.tree = None def fit(self, X, y): self.X_train = X self.y_train = y if self.algorithm not in [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\']: raise ValueError(\\"Algorithm must be one of \'auto\', \'ball_tree\', \'kd_tree\', or \'brute\'.\\") if issparse(X): X = X.toarray() if self.algorithm == \'auto\': if X.shape[1] > 4: self.tree = BallTree(X, metric=self.metric) else: self.tree = KDTree(X, metric=self.metric) elif self.algorithm == \'ball_tree\': self.tree = BallTree(X, metric=self.metric) elif self.algorithm == \'kd_tree\': self.tree = KDTree(X, metric=self.metric) else: # brute self.tree = None def kneighbors(self, X, n_neighbors=None, return_distance=True): if n_neighbors is None: n_neighbors = self.n_neighbors if issparse(X): X = X.toarray() if self.tree: distances, indices = self.tree.query(X, k=n_neighbors) else: distances = None if self.metric == \'euclidean\': distances = distance.cdist(X, self.X_train, \'euclidean\') elif self.metric == \'manhattan\': distances = distance.cdist(X, self.X_train, \'cityblock\') elif self.metric == \'chebyshev\': distances = distance.cdist(X, self.X_train, \'chebyshev\') indices = np.argsort(distances, axis=1)[:, :n_neighbors] distances = np.take_along_axis(distances, indices, axis=1) if return_distance: return distances, indices else: return indices def predict(self, X): distances, indices = self.kneighbors(X) predictions = [] for idx in indices: nearest_labels = self.y_train[idx] label_counts = np.bincount(nearest_labels) predictions.append(np.argmax(label_counts)) return np.array(predictions)"},{"question":"# Asynchronous Task Management with asyncio **Objective:** Implement an asynchronous Python program that utilizes different features of the asyncio package to manage and coordinate multiple tasks. **Problem Statement:** You are required to create an asynchronous program that executes a series of tasks in parallel, collects their results, and handles any potential cancellations or timeouts. 1. **Function Definitions**: - `async def task_coro(task_id: int) -> str`: This function simulates a task that will take between 1 to 5 seconds to complete. Once done, it will return a string `Task {task_id} completed`. - `async def gather_tasks(n: int) -> List[str]`: This function should create and run `n` tasks in parallel using `asyncio.gather` and return the list of results from each task. - `async def main(n: int) -> None`: This function will call `gather_tasks` and handle `asyncio.CancelledError` and `asyncio.TimeoutError` exceptions. **Constraints:** - Each task takes a random time (between 1 to 5 seconds) to complete. - The `main` function should run the `gather_tasks` function with a timeout of 7 seconds. - If a timeout occurs or if any task is cancelled, print an appropriate message. **Input:** - A single integer `n` representing the number of tasks to be created and run concurrently. **Output:** - If all tasks complete within the timeout, print results from each task. - If a timeout occurs, print a message indicating which tasks (if any) were completed before timeout. **Performance Requirements:** - The implementation should efficiently handle up to 1000 tasks. - The use of asyncio primitives and exception handling should be correctly demonstrated. **Example Usage:** ```python import asyncio import random async def task_coro(task_id: int) -> str: await asyncio.sleep(random.uniform(1, 5)) return f\'Task {task_id} completed\' async def gather_tasks(n: int) -> List[str]: tasks = [asyncio.create_task(task_coro(i)) for i in range(n)] return await asyncio.gather(*tasks) async def main(n: int) -> None: try: results = await asyncio.wait_for(gather_tasks(n), timeout=7) for result in results: print(result) except asyncio.TimeoutError: print(\'Tasks did not complete within the timeout period.\') except asyncio.CancelledError: print(\'Tasks were cancelled.\') if __name__ == \\"__main__\\": asyncio.run(main(5)) ``` **Notes:** - Remember to handle the `asyncio.CancelledError` and `asyncio.TimeoutError` exceptions appropriately. - Ensure your code is robust and handles edge cases, such as tasks completing at the very edge of the timeout threshold.","solution":"import asyncio import random from typing import List async def task_coro(task_id: int) -> str: Simulates a task that takes between 1 to 5 seconds to complete. Once done, it returns a string `Task {task_id} completed`. await asyncio.sleep(random.uniform(1, 5)) return f\'Task {task_id} completed\' async def gather_tasks(n: int) -> List[str]: Creates and runs `n` tasks in parallel using asyncio.gather and returns the list of results. tasks = [asyncio.create_task(task_coro(i)) for i in range(n)] return await asyncio.gather(*tasks) async def main(n: int) -> None: Calls gather_tasks and handles asyncio.CancelledError and asyncio.TimeoutError exceptions. try: results = await asyncio.wait_for(gather_tasks(n), timeout=7) for result in results: print(result) except asyncio.TimeoutError: print(\'Tasks did not complete within the timeout period.\') except asyncio.CancelledError: print(\'Tasks were cancelled.\') if __name__ == \\"__main__\\": asyncio.run(main(5))"},{"question":"**Coding Assessment Question:** # Objective: To assess the student\'s ability to create and work with Abstract Base Classes (ABCs) using the `abc` module in Python. # Problem Statement: You need to create a plugin system for a hypothetical data processing application. The system needs to support different types of data processing plugins, each of which must implement specific methods to process data. # Task: 1. **Define an Abstract Base Class `DataPlugin`**: - This class should be an abstract base class that uses `ABCMeta` as its metaclass. - It should define three abstract methods: `load_data`, `process_data`, and `export_data`. 2. **Define a concrete subclass `CSVPlugin`**: - This class should inherit from `DataPlugin` and provide concrete implementations for all abstract methods: `load_data`, `process_data`, and `export_data`. # Requirements: DataPlugin: ```python from abc import ABC, abstractmethod class DataPlugin(ABC): @abstractmethod def load_data(self, source: str): Load data from the given source. pass @abstractmethod def process_data(self): Process the loaded data. pass @abstractmethod def export_data(self, destination: str): Export the processed data to the given destination. pass ``` CSVPlugin: - Should inherit from `DataPlugin`. - Implement the `load_data` method to read CSV data from a file. - Implement the `process_data` method to process the CSV data (for simplicity, just print \\"Processing data\\"). - Implement the `export_data` method to write the processed data to a CSV file. # Input and Output formats: - `load_data(self, source: str)`: Takes the source file path as a string and loads the data. - `process_data(self)`: Processes the loaded data. - `export_data(self, destination: str)`: Takes the destination file path as destination and exports the processed data. # Constraints: - Use the `abc` module correctly to define abstract base class and methods. - Ensure all abstract methods in `DataPlugin` are overridden in `CSVPlugin` to avoid instantiation errors. # Example: ```python class CSVPlugin(DataPlugin): def __init__(self): self.data = None def load_data(self, source: str): with open(source, \'r\') as file: self.data = file.read() print(f\\"Data loaded from {source}\\") def process_data(self): if self.data: print(\\"Processing data...\\") # Simulating data processing self.data = self.data.upper() else: print(\\"No data to process\\") def export_data(self, destination: str): if self.data: with open(destination, \'w\') as file: file.write(self.data) print(f\\"Data exported to {destination}\\") else: print(\\"No data to export\\") ``` **Note:** You are not required to handle actual CSV parsing in this example, just simulate data loading, processing, and exporting. # Evaluation Criteria: - Correct use of `ABCMeta` and `abstractmethod` in defining the `DataPlugin` ABC. - Concrete implementation of the `CSVPlugin` class with correct method overrides. - Adherence to input and output requirements. - Proper handling of file operations.","solution":"from abc import ABC, abstractmethod class DataPlugin(ABC): @abstractmethod def load_data(self, source: str): Load data from the given source. pass @abstractmethod def process_data(self): Process the loaded data. pass @abstractmethod def export_data(self, destination: str): Export the processed data to the given destination. pass class CSVPlugin(DataPlugin): def __init__(self): self.data = None def load_data(self, source: str): with open(source, \'r\') as file: self.data = file.read() print(f\\"Data loaded from {source}\\") def process_data(self): if self.data: print(\\"Processing data...\\") # Simulating data processing self.data = self.data.upper() else: print(\\"No data to process\\") def export_data(self, destination: str): if self.data: with open(destination, \'w\') as file: file.write(self.data) print(f\\"Data exported to {destination}\\") else: print(\\"No data to export\\")"},{"question":"# Feature Selection with Sklearn You are working as a data scientist on a classification problem and have a dataset with high dimensionality. Your task is to implement a feature selection strategy using `sklearn`. Objective Write a Python function `feature_selection` that performs feature selection on the dataset using three different feature selection techniques: 1. Variance Threshold. 2. SelectKBest with the ANOVA F-value. 3. Recursive Feature Elimination (RFE) with Logistic Regression as the estimator. Specifications - The function should take the following inputs: - `X`: A 2D list or NumPy array of shape (n_samples, n_features) representing the feature matrix. - `y`: A 1D list or NumPy array of length `n_samples` representing the target values. - `k`: An integer representing the number of top features to select using SelectKBest and RFE. - The function should return a dictionary with three keys: `\\"variance_threshold\\"`, `\\"select_k_best\\"`, and `\\"rfe\\"`. Each key should map to the transformed feature matrix (NumPy array) after applying the corresponding feature selection technique. Constraints 1. The Variance Threshold should be set to remove features with zero variance by default. 2. Use `f_classif` as the scoring function for SelectKBest. 3. Use `LogisticRegression` as the estimator for RFE, and set `n_features_to_select=k`. Example ```python import numpy as np X = np.array([[0, 0, 1, 0.5], [0, 1, 0, 0.5], [1, 0, 0, 0.8], [0, 1, 1, 0.7], [0, 1, 0, 0.5], [0, 1, 1, 0.8]]) y = np.array([0, 1, 0, 1, 1, 0]) k = 2 result = feature_selection(X, y, k) print(result[\\"variance_threshold\\"]) # Expected output will exclude low variance features print(result[\\"select_k_best\\"]) # Expected output will include k best features based on ANOVA F-value print(result[\\"rfe\\"]) # Expected output will include k selected features using RFE with Logistic Regression ``` Notes - You are expected to use `scikit-learn` for implementing the feature selection methods. - You can assume that the input data `X` and `y` are always provided in the correct format. Good luck!","solution":"from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif from sklearn.feature_selection import RFE from sklearn.linear_model import LogisticRegression import numpy as np def feature_selection(X, y, k): Performs feature selection using three methods: Variance Threshold, SelectKBest with ANOVA F-value, and RFE with Logistic Regression. Parameters: X: 2D list or NumPy array of shape (n_samples, n_features) representing the feature matrix. y: 1D list or NumPy array of length n_samples representing the target values. k: An integer representing the number of top features to select using SelectKBest and RFE. Returns: A dictionary with three keys: \'variance_threshold\', \'select_k_best\', and \'rfe\', each mapping to the transformed feature matrix. # Convert X and y to numpy arrays if they aren\'t already if not isinstance(X, np.ndarray): X = np.array(X) if not isinstance(y, np.ndarray): y = np.array(y) # 1. Variance Threshold var_thresh = VarianceThreshold() X_var_thresh = var_thresh.fit_transform(X) # 2. SelectKBest with ANOVA F-value select_k_best = SelectKBest(score_func=f_classif, k=k) X_select_k_best = select_k_best.fit_transform(X, y) # 3. Recursive Feature Elimination (RFE) with Logistic Regression logistic_regression = LogisticRegression(solver=\'liblinear\') rfe = RFE(estimator=logistic_regression, n_features_to_select=k) X_rfe = rfe.fit_transform(X, y) return { \'variance_threshold\': X_var_thresh, \'select_k_best\': X_select_k_best, \'rfe\': X_rfe }"},{"question":"# Advanced Python Monitoring with DTrace and SystemTap Objective Use DTrace and SystemTap to create a monitoring tool that tracks and reports the execution of Python scripts, particularly focusing on function call hierarchies, garbage collection, and module imports. Task You are tasked with writing a Python script that interfaces with DTrace or SystemTap to achieve the following: 1. **Track Function Calls and Returns**: Capture and display a hierarchy of function calls within a target Python script. This involves tracing `function__entry` and `function__return` markers. 2. **Monitor Garbage Collection**: Track the start and end of garbage collection cycles using `gc__start` and `gc__done` markers and report the number of objects collected. 3. **Trace Module Imports**: Monitor module import events using `import__find__load__start` and `import__find__load__done` markers to log each module import attempt and its success status. Input and Output - **Input**: A Python script to be monitored. - **Output**: - A log or report detailing: - The hierarchy of function calls. - Garbage collection events (start, end, and the number of objects collected). - Module import attempts and whether they were successful. Constraints and Requirements - Ensure that your monitoring tool minimally impacts the performance of the target Python script. - The solution should be robust and handle cases where markers are not available or configured. - Consider using a combination of DTrace scripts and Python code to gather and process the data. - Provide clear documentation and usage instructions for your monitoring tool. Example Given a target Python script `example.py` that contains: ```python import time def function_a(): function_b() function_c() def function_b(): time.sleep(1) def function_c(): time.sleep(1) if __name__ == \\"__main__\\": function_a() ``` Your tool should be able to produce output such as: ``` Function Call Hierarchy: - example.py:function_a:1 - example.py:function_b:2 - example.py:function_c:3 Garbage Collection: - Start: Generation 2 - Done: Collected 5 objects Module Imports: - Attempt: time (Success) ``` Submission Submit your Python script along with any DTrace or SystemTap scripts used. Provide a README explaining how to set up and use your monitoring tool, including any dependencies and prerequisites.","solution":"import sys import time import gc import importlib class Monitor: def __init__(self): self.function_call_hierarchy = [] self.gc_events = [] self.module_imports = [] def track_function_calls(self, func): def wrapper(*args, **kwargs): self.function_call_hierarchy.append(f\'Calling function: {func.__name__}\') result = func(*args, **kwargs) self.function_call_hierarchy.append(f\'Function {func.__name__} returned\') return result return wrapper def monitor_gc(self): def gc_callback(phase, info): if phase == \'start\': self.gc_events.append(f\'GC start: gen={info[\\"generation\\"]}\') elif phase == \'stop\': self.gc_events.append(f\'GC stop: collected={info[\\"collected\\"]} objects\') return gc_callback def track_module_imports(self, name): self.module_imports.append(f\'Attempt importing: {name}\') try: module = importlib.import_module(name) self.module_imports.append(f\'Success importing: {name}\') except ImportError: self.module_imports.append(f\'Failed to import: {name}\') return module def print_report(self): print(\\"Function Call Hierarchy:\\") for call in self.function_call_hierarchy: print(call) print(\\"nGarbage Collection Events:\\") for event in self.gc_events: print(event) print(\\"nModule Imports:\\") for imp in self.module_imports: print(imp) monitor = Monitor() def monitored_script(): @monitor.track_function_calls def function_a(): function_b() function_c() @monitor.track_function_calls def function_b(): time.sleep(1) @monitor.track_function_calls def function_c(): time.sleep(1) if __name__ == \\"__main__\\": sys.meta_path.insert(0, monitor.track_module_imports) gc.callbacks.append(monitor.monitor_gc()) function_a() gc.collect() monitor.print_report() monitored_script()"},{"question":"**Question**: Implement a function that extracts all valid email addresses from a given text using regular expressions. The function should identify emails that start with an alphanumeric character, can include periods, dashes, and underscores, should contain an \'@\' symbol followed by a domain name (which can have subdomains separated by dots), and finally end with a valid TLD (Top Level Domain) such as .com, .org, .net, etc. # Function Signature ```python def extract_emails(text: str) -> list: pass ``` # Input - `text`: A string containing one or multiple email addresses which need to be extracted. Length of the string is between 1 and 10^5. # Output - Returns a list of strings, each string being an email extracted from the input text. # Constraints 1. The function should be case-insensitive. 2. Emails should follow the format specified above. 3. The function should not extract incomplete or malformed email addresses. # Detailed Requirements 1. An email must start with an alphanumeric character. 2. Valid characters in an email address include alphanumeric characters, periods (`.`), dashes (`-`), and underscores (`_`). 3. The email should have a domain part following the `@` symbol which can include subdomains. 4. The domain part should end with a valid TLD such as `.com`, `.org`, `.net`, etc, assuming standard TLDs. # Example ```python text = \\"Please contact us at support@example.com, sales@example.net or marketing@sub.example.org.\\" print(extract_emails(text)) # Output: [\'support@example.com\', \'sales@example.net\', \'marketing@sub.example.org\'] ``` # Notes - You can assume the input will not be empty. - The common TLDs to support include: .com, .org, .net, .edu, .gov, .mil, .int. # Hint - Consider using the `findall` function from the `re` module to match all occurrences of the email pattern in the input text.","solution":"import re def extract_emails(text: str) -> list: Extracts all valid email addresses from the given text using regular expressions. Args: - text (str): The input text containing email addresses. Returns: - list: A list of valid email addresses found in the input text. # Define a regex pattern for matching email addresses pattern = r\'b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+.[a-zA-Z]{2,6}b\' # Use findall function to find all matching email patterns emails = re.findall(pattern, text) return emails"},{"question":"Objective Implement a fault-tolerant distributed training script using the `torch.distributed.elastic` module. Your task is to create a script that performs distributed training on a simple neural network while being able to gracefully handle node failures and automatically rescale the training processes. Background You are provided with a simple neural network and a dataset. Your task is to create a distributed training loop that can handle node failures and continue training smoothly when a node drops off or rejoins. Requirements 1. **Network**: Implement a simple neural network using `torch.nn` with a few layers (you can choose the architecture). 2. **Dataset**: Use a synthetic dataset generated using `torch.utils.data`. 3. **Fault Tolerance**: Use `torch.distributed.elastic` to ensure the training process can handle node failures. 4. **Elastic Training**: Implement a training loop that can scale the number of nodes dynamically. Input and Output - **Input**: The training script should accept command line arguments to specify distributed training options (e.g., `--num-nodes`, `--node-rank`, `--master-addr`, `--master-port`). - **Output**: The script should output training metrics like loss and accuracy at regular intervals and print a message when nodes are added or removed. Constraints - The script should be written in Python and should use PyTorch and `torch.distributed.elastic` packages. - The solution should be able to run on multiple nodes (simulated environment or actual multi-node setup in a cloud environment). Performance Requirements - The training should continue seamlessly when a node fails. - The training loop should be elastic, i.e., it should automatically adjust and continue training when nodes are added or removed. Example Command ```bash python distributed_train.py --num-nodes 4 --node-rank 0 --master-addr localhost --master-port 29500 ``` Hints - Make use of `torch.distributed.elastic.rendezvous` for managing node states. - Use `torch.distributed.elastic.multiprocessing` to handle the spawning of processes across nodes. - Implement checkpointing to save models and training state for fault tolerance. ```python # Sample code structure to get you started: import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data from torch.distributed.elastic.multiprocessing import run class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def train_fn(rank, world_size): # Set up distributed training # Initialize model, dataset, dataloader, etc. model = SimpleNet() # Implement training loop with fault tolerance and elastic scaling pass if __name__ == \\"__main__\\": # Parse command line arguments import argparse parser = argparse.ArgumentParser() parser.add_argument(\\"--num-nodes\\", type=int, required=True) parser.add_argument(\\"--node-rank\\", type=int, required=True) parser.add_argument(\\"--master-addr\\", type=str, required=True) parser.add_argument(\\"--master-port\\", type=int, required=True) args = parser.parse_args() # Use torch.distributed.elastic.multiprocessing to start the training process run(train_fn, args.num_nodes, args.node_rank, args.master_addr, args.master_port) ```","solution":"import torch import torch.nn as nn import torch.optim as optim import torch.utils.data as data import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.relu = nn.ReLU() self.fc2 = nn.Linear(50, 2) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x def generate_fake_data(size): inputs = torch.randn(size, 10) targets = torch.randint(0, 2, (size,)) return data.TensorDataset(inputs, targets) def train_fn(rank, world_size, args): dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size, init_method=f\'tcp://{args.master_addr}:{args.master_port}\') model = SimpleNet().to(rank) ddp_model = DDP(model, device_ids=[rank]) dataset = generate_fake_data(10000) train_sampler = data.distributed.DistributedSampler(dataset, num_replicas=world_size, rank=rank) train_loader = data.DataLoader(dataset, batch_size=64, shuffle=False, sampler=train_sampler) criterion = nn.CrossEntropyLoss().to(rank) optimizer = optim.SGD(ddp_model.parameters(), lr=0.01) for epoch in range(10): # Example with 10 epochs ddp_model.train() for batch_inputs, batch_targets in train_loader: batch_inputs, batch_targets = batch_inputs.to(rank), batch_targets.to(rank) optimizer.zero_grad() outputs = ddp_model(batch_inputs) loss = criterion(outputs, batch_targets) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch {epoch}, Loss: {loss.item()}\\") dist.destroy_process_group() if __name__ == \\"__main__\\": import argparse parser = argparse.ArgumentParser() parser.add_argument(\\"--num-nodes\\", type=int, required=True) parser.add_argument(\\"--node-rank\\", type=int, required=True) parser.add_argument(\\"--master-addr\\", type=str, required=True) parser.add_argument(\\"--master-port\\", type=int, required=True) args = parser.parse_args() world_size = args.num_nodes rank = args.node_rank torch.multiprocessing.spawn(train_fn, args=(world_size, args), nprocs=world_size, join=True)"},{"question":"You are tasked with developing an application that processes log data from different sources and provides summarized results. Your application will utilize various container datatypes from the `collections` module to efficiently manage and process the data. Requirements 1. **Log Entry Structure:** Each log entry will include the following fields: - `timestamp` (string): The timestamp of the log entry. - `source` (string): The source of the log (e.g., \'system\', \'application\', \'security\'). - `event` (string): The description of the event. 2. **Function Implementations:** You need to implement the following functions: 1. **parse_logs(logs: List[str]) -> ChainMap:** - This function takes a list of log entries as input, where each entry is a string formatted as `timestamp|source|event`. - The function returns a `ChainMap` of `defaultdict`, where the top level keys are `source` and the values are dictionaries containing logs categorized by timestamps. 2. **event_counter(chain_map: ChainMap) -> Counter:** - This function takes the `ChainMap` from `parse_logs` as input. - It returns a `Counter` object that counts the occurrences of each event across all sources. 3. **ordered_events(counter: Counter) -> OrderedDict:** - This function takes the `Counter` from `event_counter` as input. - It returns an `OrderedDict` of events sorted by the number of occurrences in descending order. 4. **display_summary(log_data: ChainMap, sorted_events: OrderedDict) -> None:** - This function takes the `ChainMap` and `OrderedDict` from the previous functions as input. - It prints the summary of logs in the following format: ``` Source: <source> <timestamp>: <event> ... Event Summary: <event>: <count> ``` - The sources should be displayed in the order they are first encountered. Input Format - A list of log entries, where each entry is a string formatted as `timestamp|source|event`. Output Format - Printed summary of logs and the event count as described above. Example Input: ```python logs = [ \\"2023-07-21T15:30:00Z|system|Started system check\\", \\"2023-07-21T15:31:00Z|application|User login\\", \\"2023-07-21T15:32:00Z|security|Failed login attempt\\", \\"2023-07-21T15:33:00Z|system|Completed system check\\", \\"2023-07-21T15:34:00Z|application|User logout\\" ] ``` Output: ``` Source: system 2023-07-21T15:30:00Z: Started system check 2023-07-21T15:33:00Z: Completed system check Source: application 2023-07-21T15:31:00Z: User login 2023-07-21T15:34:00Z: User logout Source: security 2023-07-21T15:32:00Z: Failed login attempt Event Summary: User login: 1 User logout: 1 Failed login attempt: 1 Started system check: 1 Completed system check: 1 ``` Constraints - Assume that the log entries are valid and well-formed. - You may use any data available in the `collections` module to implement the solution efficiently.","solution":"from collections import ChainMap, defaultdict, Counter, OrderedDict def parse_logs(logs): log_dict = defaultdict(lambda: defaultdict(list)) for log in logs: timestamp, source, event = log.split(\'|\') log_dict[source][timestamp].append(event) return ChainMap(log_dict) def event_counter(chain_map): event_count = Counter() for source, timestamps in chain_map.items(): for timestamp, events in timestamps.items(): for event in events: event_count[event] += 1 return event_count def ordered_events(counter): return OrderedDict(counter.most_common()) def display_summary(log_data, sorted_events): for source, timestamps in log_data.items(): print(f\\"Source: {source}\\") for timestamp, events in sorted(timestamps.items()): for event in events: print(f\\" {timestamp}: {event}\\") print(\\"nEvent Summary:\\") for event, count in sorted_events.items(): print(f\\" {event}: {count}\\")"},{"question":"**Objective:** To assess your understanding of the `tempfile` module in Python, including secure creation, manipulation, and cleanup of temporary files and directories. **Task:** Implement a function `process_temporary_data` that: 1. Creates a temporary directory. 2. Within this directory, creates a temporary file and writes a specified amount of random data to it. 3. Reads the content from this temporary file, processes the data (e.g., converts it to uppercase if it\'s a string or manipulates if it\'s binary), and writes the processed data to a new temporary file in the same directory. 4. Returns the name of the temporary directory and the names of both the original and processed temporary files within it. 5. Ensures that all temporary resources are properly cleaned up after use. **Function Signature:** ```python import tempfile def process_temporary_data(data: bytes) -> (str, str, str): pass ``` **Input:** - `data`: A bytes object representing the data to be written to the temporary file. **Output:** - A tuple containing three strings: - The name of the temporary directory. - The name of the initial temporary file with the original data. - The name of the new temporary file with the processed data. **Constraints:** - The function should handle both binary and text data safely. - Use exception handling to ensure all temporary files and directories are deleted even if an error occurs during processing. **Performance Requirements:** - The function should efficiently handle data up to 10MB in size. **Example:** ```python data = b\\"Hello, world!\\" dir_name, orig_file, proc_file = process_temporary_data(data) # Ensure the output verifies the temporary directory and files have been created with correct data processing. # Expected: dir_name should be a temporary directory path, orig_file should have the original data, proc_file should have processed data. ``` **Notes:** - You may use `os` and `shutil` modules alongside `tempfile` for file handling where necessary. - Provide docstring and comments to describe your implementation clearly.","solution":"import tempfile import shutil import os def process_temporary_data(data: bytes) -> (str, str, str): Processes temporary data by creating a temporary file, writing the data to it, reading and processing the data (converting to uppercase in this case), and writing the processed data to a new temporary file. Returns the directory, and names of the original and processed files. :param data: A bytes object representing the data to be processed. :return: A tuple with names of the temporary directory, original file, and processed file. try: # Create a temporary directory temp_dir = tempfile.mkdtemp() # Create a temporary file within the directory and write the original data orig_file_path = os.path.join(temp_dir, \'original_temp_file\') with open(orig_file_path, \'wb\') as orig_temp_file: orig_temp_file.write(data) # Process the data processed_data = data.upper() # Create another temporary file within the directory and write the processed data proc_file_path = os.path.join(temp_dir, \'processed_temp_file\') with open(proc_file_path, \'wb\') as proc_temp_file: proc_temp_file.write(processed_data) # Return the temporary directory and file names return temp_dir, orig_file_path, proc_file_path finally: # Clean up the temporary files and directory shutil.rmtree(temp_dir)"},{"question":"Persistent Dictionary Using Shelves in Python In this task, you are required to implement a function that uses the `shelve` module to maintain a persistent dictionary for storing and managing user data. The function should be capable of adding, retrieving, updating, and deleting items in the shelf, with persistence across multiple executions. Function Signature ```python def manage_shelf( operation: str, key: str = None, value = None, filename: str = \'my_shelf.db\', writeback: bool = False) -> None: Manage a shelf based on the specified operation. Parameters: - operation (str): The operation to be performed (\'add\', \'get\', \'update\', \'delete\', \'get_keys\'). - key (str): The key of the item for \'add\', \'get\', \'update\', and \'delete\' operations. - value: The value to be stored/updated in the shelf for \'add\' and \'update\' operations. - filename (str): The name of the file to store the shelf (default is \'my_shelf.db\'). - writeback (bool): Whether to use writeback for mutable entries (default is False). Returns: - For \'get\' operation: the value retrieved from the shelf. - For \'get_keys\' operation: a list of all keys in the shelf. - For other operations: None. Raises: - KeyError for \'get\', \'update\', and \'delete\' operations if the specified key does not exist. ``` # Description 1. **Add Operation** - Adds a key-value pair to the shelf. - If the key already exists, it should overwrite the existing value. 2. **Get Operation** - Retrieves the value associated with the given key from the shelf. - Raises `KeyError` if the key does not exist. 3. **Update Operation** - Updates the value of an existing key in the shelf. - Raises `KeyError` if the key does not exist. 4. **Delete Operation** - Deletes the key-value pair from the shelf. - Raises `KeyError` if the key does not exist. 5. **Get_keys Operation** - Returns a list of all keys in the shelf. # Example Usage ```python # Add key-value pairs to the shelf manage_shelf(\'add\', \'user1\', {\'name\': \'Alice\', \'age\': 30}) manage_shelf(\'add\', \'user2\', {\'name\': \'Bob\', \'age\': 25}) # Retrieve a value print(manage_shelf(\'get\', \'user1\')) # Output: {\'name\': \'Alice\', \'age\': 30} # Update a value manage_shelf(\'update\', \'user1\', {\'name\': \'Alice\', \'age\': 31}) # Retrieve the updated value print(manage_shelf(\'get\', \'user1\')) # Output: {\'name\': \'Alice\', \'age\': 31} # Get all keys print(manage_shelf(\'get_keys\')) # Output: [\'user1\', \'user2\'] # Delete a key manage_shelf(\'delete\', \'user1\') # Attempt to retrieve a deleted key (raises KeyError) try: print(manage_shelf(\'get\', \'user1\')) except KeyError: print(\'Key not found\') ``` # Constraints - Ensure the file is properly closed after each operation. - Handle `KeyError` exceptions gracefully where specified. - Optimize for minimal memory usage if `writeback` is set to `False`. Implement the `manage_shelf` function to fulfill the requirements listed above.","solution":"import shelve from typing import List, Union def manage_shelf( operation: str, key: str = None, value = None, filename: str = \'my_shelf.db\', writeback: bool = False) -> Union[None, any, List[str]]: Manage a shelf based on the specified operation. Parameters: - operation (str): The operation to be performed (\'add\', \'get\', \'update\', \'delete\', \'get_keys\'). - key (str): The key of the item for \'add\', \'get\', \'update\', and \'delete\' operations. - value: The value to be stored/updated in the shelf for \'add\' and \'update\' operations. - filename (str): The name of the file to store the shelf (default is \'my_shelf.db\'). - writeback (bool): Whether to use writeback for mutable entries (default is False). Returns: - For \'get\' operation: the value retrieved from the shelf. - For \'get_keys\' operation: a list of all keys in the shelf. - For other operations: None. Raises: - KeyError for \'get\', \'update\', and \'delete\' operations if the specified key does not exist. with shelve.open(filename, writeback=writeback) as shelf: if operation == \'add\': shelf[key] = value elif operation == \'get\': if key not in shelf: raise KeyError(f\'Key {key} not found\') return shelf[key] elif operation == \'update\': if key not in shelf: raise KeyError(f\'Key {key} not found\') shelf[key] = value elif operation == \'delete\': if key not in shelf: raise KeyError(f\'Key {key} not found\') del shelf[key] elif operation == \'get_keys\': return list(shelf.keys()) else: raise ValueError(\'Invalid operation\')"},{"question":"**Question: Implement a Custom Telnet Client** This exercise will assess your understanding of network programming using Python. You are required to implement a simplified version of a Telnet client using Python\'s socket library, mimicking the behavior of some methods in the `telnetlib` module. # Requirements 1. **Class Definition**: - Define a class named `CustomTelnetClient` that represents a connection to a Telnet server. 2. **Constructor**: - The constructor should take `host`, `port`, and an optional `timeout` as parameters. Use these parameters to establish a connection to the given host and port with the specified timeout. 3. **Methods**: - Implement the following methods within your class: 1. `read_until(expected, timeout=None)`: - Read data from the server until the expected byte string is encountered or until the timeout is reached. 2. `write(buffer)`: - Write a byte string to the server. 3. `close()`: - Close the connection to the server. # Input and Output Formats - **Constructor**: - Input: `host (str)`, `port (int)`, `timeout (Optional[int])` - Action: Establishes a connection to the specified Telnet server. - **read_until Method**: - Input: `expected (bytes)`, `timeout (Optional[int])` - Output: Data read from the server until the expected byte string or timeout. - **write Method**: - Input: `buffer (bytes)` - Action: Writes the provided byte string to the server. - **close Method**: - Action: Closes the connection to the server. # Constraints - Ensure that all reads and writes handle potential exceptions gracefully, such as connection drops or timeouts. - Make sure the client can handle both blocking and non-blocking operations appropriately based on the provided timeout values. # Example ```python class CustomTelnetClient: def __init__(self, host, port, timeout=None): # Your implementation here pass def read_until(self, expected, timeout=None): # Your implementation here pass def write(self, buffer): # Your implementation here pass def close(self): # Your implementation here pass # Example usage if __name__ == \\"__main__\\": client = CustomTelnetClient(\'localhost\', 23) client.write(b\\"Hellon\\") response = client.read_until(b\\"n\\") print(response) client.close() ``` Implement the `CustomTelnetClient` class following the specifications above.","solution":"import socket class CustomTelnetClient: def __init__(self, host, port, timeout=None): self.host = host self.port = port self.timeout = timeout self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) if timeout is not None: self.socket.settimeout(timeout) self.socket.connect((host, port)) def read_until(self, expected, timeout=None): if timeout is not None: self.socket.settimeout(timeout) data = b\'\' try: while not data.endswith(expected): chunk = self.socket.recv(4096) if not chunk: break data += chunk except socket.timeout: pass return data def write(self, buffer): self.socket.sendall(buffer) def close(self): self.socket.close()"},{"question":"PyTorch func Transformations and Gradient Calculation In this assessment, you are required to demonstrate your understanding of the `torch.func` transformations and how to compute model gradients using these transformations. Problem Statement You are provided with a simple model and a dataset consisting of input-output pairs. Your task is to: 1. Transform the model to a functional form using `torch.func.functional_call`. 2. Implement a function to compute the loss using the Mean Squared Error (MSE) loss function. 3. Calculate the gradients of the loss with respect to the model parameters using `torch.func.grad`. Detailed Steps: 1. **Transform the Model to a Functional Form:** - Use the given neural network model and convert it to a functional form using `torch.func.functional_call`. - The neural network model to be used is `torch.nn.Linear(3, 3)`. 2. **Implement the Loss Function:** - Implement a function `compute_loss` to calculate the loss between the model predictions and the given targets using the Mean Squared Error (MSE) loss function. - The function signature should be: ```python def compute_loss(params: Dict[str, torch.Tensor], inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor: ``` 3. **Compute the Gradients:** - Implement a function `compute_gradients` to calculate the gradients of the loss with respect to the model parameters using `torch.func.grad`. - The function signature should be: ```python def compute_gradients(model: torch.nn.Module, inputs: torch.Tensor, targets: torch.Tensor) -> Dict[str, torch.Tensor]: ``` - This function should return a dictionary containing the gradients for each parameter. Constraints: - Inputs to the model and targets will be `torch.Tensor` objects. - Use only the `torch`, `torch.nn`, and `torch.func` modules for this implementation. Expected Input and Output Format: - **Input:** - `model`: An instance of `torch.nn.Module`. - `inputs`: A tensor of shape `(batch_size, 3)`. - `targets`: A tensor of shape `(batch_size, 3)`. - **Output:** - A dictionary containing the gradients for each parameter in the model. Example: ```python import torch from torch.func import functional_call, grad inputs = torch.randn(64, 3) targets = torch.randn(64, 3) model = torch.nn.Linear(3, 3) def compute_loss(params, inputs, targets): prediction = functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) def compute_gradients(model, inputs, targets): params = dict(model.named_parameters()) gradients = grad(compute_loss)(params, inputs, targets) return gradients # Testing the functions params = dict(model.named_parameters()) loss = compute_loss(params, inputs, targets) gradients = compute_gradients(model, inputs, targets) print(loss) # Should output a tensor representing the loss print(gradients) # Should output a dictionary containing gradients for each parameter ``` Your task is to complete the `compute_loss` and `compute_gradients` functions.","solution":"import torch from torch.func import functional_call, grad from typing import Dict def compute_loss(params: Dict[str, torch.Tensor], inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor: Computes the Mean Squared Error (MSE) loss between the model predictions and the targets. :param params: Dictionary of model parameters. :param inputs: Input tensor. :param targets: Target tensor. :return: Computed loss. model = torch.nn.Linear(3, 3) prediction = functional_call(model, params, (inputs,)) return torch.nn.functional.mse_loss(prediction, targets) def compute_gradients(model: torch.nn.Module, inputs: torch.Tensor, targets: torch.Tensor) -> Dict[str, torch.Tensor]: Computes the gradients of the loss with respect to the model parameters. :param model: An instance of torch.nn.Module. :param inputs: Input tensor. :param targets: Target tensor. :return: Dictionary of gradients for each model parameter. params = {name: param.detach().requires_grad_() for name, param in model.named_parameters()} loss = compute_loss(params, inputs, targets) gradients = torch.autograd.grad(loss, params.values()) return {name: grad for (name, _), grad in zip(params.items(), gradients)}"},{"question":"**Question: Visualization of Time-Series Data with Seaborn** You are provided a dataset named `daily_sales` with the following structure: ``` | Date | Sales | Region | |------------|--------|----------| | 2022-01-01 | 1500.5 | North | | 2022-01-01 | 2200.1 | South | | 2022-01-02 | 1300.2 | North | | 2022-01-02 | 2100.3 | South | | ... | ... | ... | ``` Each record represents the sales amount for a particular day and region. Your task is to visualize this dataset using the Seaborn `objects` interface. Write a Python function `visualize_sales_trend` which: 1. Loads the provided dataset `daily_sales`. 2. Plots the time-series data using `Date` on x-axis and `Sales` on y-axis. 3. Uses different line colors for different `Region`. 4. Facets the plots by each decade of the years. 5. Customizes the plot with the following requirements: - Use `.add()` method to draw lines. - Adjust the line width to 1. - Specify a color that differentiates between regions. 6. Labels the facet with the format `\\"{decade}\'s Sales\\"`. **Function signature:** ```python def visualize_sales_trend(daily_sales: pd.DataFrame) -> None: # Your code here ``` **Expected Input:** A DataFrame `daily_sales` with columns `Date` (datetime64[ns]), `Sales` (float64), and `Region` (object). **Output:** A Seaborn plot visualizing sales trends, faceted by each decade. * There is no return value: the function should simply display the plot. Example: ```python import pandas as pd data = { \'Date\': pd.date_range(start=\'1/1/2010\', periods=1000, freq=\'D\'), \'Sales\': np.random.rand(1000) * 1000, \'Region\': np.random.choice([\'North\', \'South\'], size=1000) } daily_sales = pd.DataFrame(data) visualize_sales_trend(daily_sales) ``` The code should generate a set of faceted line plots that meet the specified criteria.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_sales_trend(daily_sales: pd.DataFrame) -> None: # Convert Date column to datetime if it\'s not already if not pd.api.types.is_datetime64_any_dtype(daily_sales[\'Date\']): daily_sales[\'Date\'] = pd.to_datetime(daily_sales[\'Date\']) # Add a Decade column for faceting daily_sales[\'Decade\'] = daily_sales[\'Date\'].dt.year // 10 * 10 sns.set(style=\\"whitegrid\\") # Set the seaborn style # Initialize the plot with the dataset p = sns.relplot( data=daily_sales, x=\\"Date\\", y=\\"Sales\\", hue=\\"Region\\", kind=\\"line\\", col=\\"Decade\\", height=5, aspect=1.5, facet_kws={\'sharey\': False, \'sharex\': True} ) # Customize the plot for ax in p.axes.flat: ax.lines[0].set_linewidth(1) ax.lines[1].set_linewidth(1) decade = int(ax.get_title().split(\'=\')[1]) ax.set_title(f\\"{decade}\'s Sales\\") plt.show()"},{"question":"Objective Implement a function that utilizes Python\'s `heapq` module to manage a dynamic collection of integers, ensuring that the collection is always accessible in sorted order. This task will help you demonstrate your understanding of heap operations and the effective use of Python\'s standard library. Problem Statement You are required to manage a dynamic list of integers supporting the following operations: 1. Insert a new integer into the collection. 2. Remove and return the smallest integer from the collection. 3. Retrieve the smallest integer without removing it from the collection. 4. Retrieve all elements in the collection sorted in ascending order. Implement a class `DynamicIntegerCollection` with the following methods: - `__init__(self)`: Initializes an empty collection. - `insert(self, num: int) -> None`: Inserts a new integer `num` into the collection. - `remove_min(self) -> int`: Removes and returns the smallest integer from the collection. If the collection is empty, raise an `IndexError`. - `get_min(self) -> int`: Returns the smallest integer without removing it from the collection. If the collection is empty, raise an `IndexError`. - `get_sorted_elements(self) -> List[int]`: Returns a list containing all integers in the collection sorted in ascending order. Constraints - Each method should have an average time complexity of O(log N) where N is the number of elements in the collection, except for the `get_sorted_elements` method which should have a time complexity of O(N log N). - All integers in the collection are within the range [-10^6, 10^6]. - You can assume that the collection will not contain more than 10^4 integers at any point. Example ```python collection = DynamicIntegerCollection() collection.insert(5) collection.insert(3) collection.insert(10) print(collection.get_min()) # Should print 3 print(collection.remove_min()) # Should print 3 print(collection.get_sorted_elements()) # Should print [5, 10] collection.insert(2) print(collection.get_min()) # Should print 2 print(collection.remove_min()) # Should print 2 print(collection.get_sorted_elements()) # Should print [5, 10] ``` Notes - Utilize Python\'s `heapq` module to manage the collection efficiently. - Ensure your implementation handles edge cases effectively, such as calling `get_min` or `remove_min` on an empty collection.","solution":"import heapq class DynamicIntegerCollection: def __init__(self): Initializes an empty collection. self.heap = [] def insert(self, num: int) -> None: Inserts a new integer `num` into the collection. heapq.heappush(self.heap, num) def remove_min(self) -> int: Removes and returns the smallest integer from the collection. If the collection is empty, raises an `IndexError`. if not self.heap: raise IndexError(\\"remove_min from an empty collection\\") return heapq.heappop(self.heap) def get_min(self) -> int: Returns the smallest integer without removing it from the collection. If the collection is empty, raises an `IndexError`. if not self.heap: raise IndexError(\\"get_min from an empty collection\\") return self.heap[0] def get_sorted_elements(self) -> list: Returns a list containing all integers in the collection sorted in ascending order. return sorted(self.heap)"},{"question":"**Objective:** Demonstrate your understanding of weak references and the `weakref` module in Python by implementing a cache system for storing recently accessed objects. The cache should allow objects to be garbage collected when there are no strong references to them left. This will prevent memory leaks in applications that deal with large objects or frequently changing data. # Your Task: **Implement a class `LRUWeakCache` that functions as a Least Recently Used (LRU) cache** but uses weak references to its values. When the cache reaches its capacity, it should evict the least recently used item. Additionally, garbage collected items should be automatically removed from the cache. Class `LRUWeakCache`: 1. **`__init__(self, capacity: int)`**: - Initialize the cache with a given `capacity`. 2. **`get(self, key: any) -> any`**: - Retrieve the value associated with `key` from the cache. - If the key is not found or its associated value has been garbage collected, return `None`. 3. **`put(self, key: any, value: any) -> None`**: - Add the `(key, value)` pair to the cache. - If the cache is full, evict the least recently used item. 4. **`__delitem__(self, key: any) -> None`**: - Explicitly delete an item from the cache by its `key`. # Constraints: - The cache should store objects using weak references so they can be garbage collected when necessary. - The `LRUWeakCache` must manage an eviction policy that ensures the least recently used item is removed when capacity is exceeded. - The input dictionary keys and values can be of any type that supports weak referencing. - Methods `get()`, `put()`, and `__delitem__()` should have average time complexity of O(1). # Example: ```python import weakref class LRUWeakCache: def __init__(self, capacity: int): # Your code here def get(self, key: any) -> any: # Your code here def put(self, key: any, value: any) -> None: # Your code here def __delitem__(self, key: any) -> None: # Your code here # Example Usage cache = LRUWeakCache(2) obj1 = \\"object1\\" obj2 = \\"object2\\" obj3 = \\"object3\\" cache.put(\\"one\\", obj1) cache.put(\\"two\\", obj2) print(cache.get(\\"one\\")) # Outputs: object1 cache.put(\\"three\\", obj3) # \\"two\\" gets evicted due to capacity limit print(cache.get(\\"two\\")) # Outputs: None del obj1 print(cache.get(\\"one\\")) # Outputs: None because obj1 is garbage collected ``` **Note:** The example demonstrates basic operations and the eviction logic. Handle the weak references properly to ensure items are only in the cache as long as strong references exist outside of it. **Hint:** Use `weakref.WeakValueDictionary` to hold the cache\'s values and maintain an ordered dictionary for tracking the access order of keys for LRU eviction policy.","solution":"import weakref from collections import OrderedDict class LRUWeakCache: def __init__(self, capacity: int): if capacity < 1: raise ValueError(\\"Capacity must be at least 1\\") self.capacity = capacity self.cache = weakref.WeakValueDictionary() self.access_order = OrderedDict() def get(self, key: any) -> any: if key in self.cache: self.access_order.move_to_end(key) return self.cache[key] else: return None def put(self, key: any, value: any) -> None: if key in self.cache: self.access_order.move_to_end(key) elif len(self.cache) >= self.capacity: lru_key, _ = self.access_order.popitem(last=False) del self.cache[lru_key] self.cache[key] = value self.access_order[key] = True def __delitem__(self, key: any) -> None: if key in self.cache: del self.cache[key] del self.access_order[key]"},{"question":"# Question: Employee Management System You are required to design an employee management system using Python classes. The system should be able to manage and track employees, their roles, and their responsibilities. The system should utilize various class features discussed in the provided documentation, including inheritance, instance/class variables, and iterators. Requirements 1. **Class Structure:** - Create a base class `Person` with the following attributes: - `name` (string): The name of the person. - `age` (integer): The age of the person. - The class should have an `__init__` method to initialize the attributes. - The class should have a `__str__` method to return a string representation of the person in the format: ```plaintext Name: <name>, Age: <age> ``` 2. **Employee Class:** - Create an `Employee` class that inherits from `Person`. - Additional attributes for `Employee`: - `employee_id` (string): Unique identifier for the employee. - `salary` (float): The salary of the employee. - The class should have an `__init__` method to initialize the attributes. - Override the `__str__` method to include employee details. 3. **Manager Class:** - Create a `Manager` class that inherits from `Employee`. - Additional attributes for `Manager`: - `employees` (list): List of employees managed by the manager. - The class should have an `__init__` method to initialize the attributes. - Add methods to the `Manager` class: - `add_employee(employee)`: Adds an employee to the manager\'s list. - `remove_employee(employee_id)`: Removes an employee from the manager\'s list by employee ID. - Override the `__str__` method to include manager details and list of employees managed. 4. **Iterators:** - Implement an iterator within the `Manager` class to iterate over the employees managed, returning their string representation using the `__str__` method. 5. **Constraints:** - Ensure proper validation of attribute types within constructors. - Use class and instance variables appropriately. - Ensure use of private variables where necessary to prevent unauthorized access. - Make use of inheritance to avoid code duplication. Sample Usage ```python # Example Usage manager = Manager(\'Alice\', 45, \'M001\', 90000) employee1 = Employee(\'Bob\', 30, \'E101\', 50000) employee2 = Employee(\'Charlie\', 28, \'E102\', 48000) manager.add_employee(employee1) manager.add_employee(employee2) print(manager) for emp in manager: print(emp) # Expected Output: # Name: Alice, Age: 45, Employee ID: M001, Salary: 90000 # Managed Employees: # Name: Bob, Age: 30, Employee ID: E101, Salary: 50000 # Name: Charlie, Age: 28, Employee ID: E102, Salary: 48000 # Name: Bob, Age: 30, Employee ID: E101, Salary: 50000 # Name: Charlie, Age: 28, Employee ID: E102, Salary: 48000 ``` Implement the following classes: `Person`, `Employee`, and `Manager` based on these requirements and sample usage.","solution":"class Person: def __init__(self, name, age): if not isinstance(name, str): raise ValueError(\\"Name must be a string\\") if not isinstance(age, int): raise ValueError(\\"Age must be an integer\\") self._name = name self._age = age def __str__(self): return f\\"Name: {self._name}, Age: {self._age}\\" class Employee(Person): def __init__(self, name, age, employee_id, salary): super().__init__(name, age) if not isinstance(employee_id, str): raise ValueError(\\"Employee ID must be a string\\") if not isinstance(salary, (int, float)): raise ValueError(\\"Salary must be a number\\") self._employee_id = employee_id self._salary = salary def __str__(self): return f\\"{super().__str__()}, Employee ID: {self._employee_id}, Salary: {self._salary}\\" class Manager(Employee): def __init__(self, name, age, employee_id, salary): super().__init__(name, age, employee_id, salary) self._employees = [] def add_employee(self, employee): if isinstance(employee, Employee): self._employees.append(employee) else: raise ValueError(\\"Can only add Employee instances\\") def remove_employee(self, employee_id): for emp in self._employees: if emp._employee_id == employee_id: self._employees.remove(emp) return raise ValueError(f\\"Employee with ID {employee_id} not found\\") def __str__(self): base_str = super().__str__() emp_strings = \\"n\\".join([str(emp) for emp in self._employees]) return f\\"{base_str}nManaged Employees:n{emp_strings}\\" def __iter__(self): return iter(self._employees)"},{"question":"**Understanding and Manipulating UUIDs** **Objective:** Write a Python function that demonstrates your understanding of the `uuid` module, specifically focusing on the following aspects: 1. Generating UUIDs using different algorithms. 2. Creating a UUID from various representations. 3. Extracting and printing specific details from a UUID. **Task:** Implement a function `uuid_operations()` that performs the following operations: 1. Generate a UUID using the `uuid1()` method and store it in `uuid1`. 2. Generate a UUID using the `uuid4()` method and store it in `uuid4`. 3. Create a UUID from the hexadecimal string representation of `uuid1` and store it in `uuid1_from_hex`. 4. Create a UUID from the bytes representation of `uuid4` and store it in `uuid4_from_bytes`. 5. Print the following details for each UUID (`uuid1`, `uuid4`, `uuid1_from_hex`, `uuid4_from_bytes`): - The UUID in its standard string representation. - The variant and version of the UUID. - The hexadecimal string representation. - The 16-byte string representation. - Whether the UUID is multiprocessing-safe. **Example Output:** ```python uuid1: str: e4aaec00-1234-11eb-adc1-0242ac120002 variant: RFC_4122 version: 1 hex: e4aaec00123411ebadc10242ac120002 bytes: b\'xe4xaaxecx00x124x11xebxadxc1x02Bxacx12x00x02\' is_safe: unknown uuid4: str: 16fd2706-8baf-433b-82eb-8c7fada847da variant: RFC_4122 version: 4 hex: 16fd27068baf433b82eb8c7fada847da bytes: b\'x16xfd\'x06x8bxafC;x82xebx8cx7fxadxa8Gxda\' is_safe: unknown uuid1_from_hex: str: e4aaec00-1234-11eb-adc1-0242ac120002 variant: RFC_4122 version: 1 hex: e4aaec00123411ebadc10242ac120002 bytes: b\'xe4xaaxecx00x124x11xebxadxc1x02Bxacx12x00x02\' is_safe: unknown uuid4_from_bytes: str: 16fd2706-8baf-433b-82eb-8c7fada847da variant: RFC_4122 version: 4 hex: 16fd27068baf433b82eb8c7fada847da bytes: b\'x16xfd\'x06x8bxafC;x82xebx8cx7fxadxa8Gxda\' is_safe: unknown ``` **Constraints:** - You must use the `uuid` module provided in the standard library. - Ensure that all UUIDs and their properties are printed exactly as shown in the example. **Function Signature:** ```python def uuid_operations(): # Your code here ```","solution":"import uuid def uuid_operations(): # Generate UUIDs using different algorithms uuid1 = uuid.uuid1() uuid4 = uuid.uuid4() # Create UUIDs from various representations uuid1_from_hex = uuid.UUID(hex=str(uuid1).replace(\'-\', \'\')) uuid4_from_bytes = uuid.UUID(bytes=uuid4.bytes) # Collect UUID details uuid_details = [ (\'uuid1\', uuid1), (\'uuid4\', uuid4), (\'uuid1_from_hex\', uuid1_from_hex), (\'uuid4_from_bytes\', uuid4_from_bytes) ] result = {} for name, uuid_obj in uuid_details: result[name] = { \\"str\\": str(uuid_obj), \\"variant\\": uuid_obj.variant, \\"version\\": uuid_obj.version, \\"hex\\": uuid_obj.hex, \\"bytes\\": uuid_obj.bytes, \\"is_safe\\": uuid_obj.is_safe } for name, details in result.items(): print(f\\"{name}: \\") for key, value in details.items(): print(f\\" {key}: {value}\\") print()"},{"question":"**Coding Assessment Question:** You are given a partially defined neural network model in PyTorch. Your task is to optimize the model using the `torch.compiler` module by performing the following steps: 1. **Compile** the neural network model for efficient execution. 2. **Substitute** one of the specified layers within the model with a different layer. 3. **Check** if the model is currently being compiled. 4. **List** the available backends used for compilation. 5. **Assume** the result of a specific operation is constant to optimize subsequent operations. 6. **Disable** the compilation optimizations and **reset** the compilation state. Your implementation should adhere to the following function signature: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def optimize_model(model: nn.Module) -> None: Optimizes a given PyTorch model using the torch.compiler module. Args: model (nn.Module): The neural network model to be optimized. Returns: None # Compile the model compiled_model = torch.compiler.compile(model) # Substitute the first layer with a different layer (e.g., another Linear layer) with torch.compiler.substitute_in_graph(compiled_model, \'fc1\', nn.Linear(10, 20)): pass # Check if the model is currently being compiled print(\\"Is compiling:\\", torch.compiler.is_compiling()) # List the available backends backends = torch.compiler.list_backends() print(\\"Available backends:\\", backends) # Assume the result of the first layer\'s output is constant with torch.compiler.assume_constant_result(\'fc1\'): pass # Disable compilation optimizations torch.compiler.disable() # Reset compilation state torch.compiler.reset() # Example usage: if __name__ == \\"__main__\\": model = SimpleModel() optimize_model(model) ``` # Constraints: - You must use the `torch.compiler` module functions to achieve the steps outlined above. - The model should be optimized and manipulated as per the steps within `optimize_model` function. - Ensure to handle any potential issues or errors that may arise when using these functions. # Expected Output: - The function should print whether the model is being compiled (`True` or `False`). - The function should print the list of available backends for model compilation.","solution":"import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def optimize_model(model: nn.Module) -> None: Optimizes a given PyTorch model using the torch.compiler module. Args: model (nn.Module): The neural network model to be optimized. Returns: None # Mock torch.compiler as it\'s not available yet class MockCompiler: def compile(self, model): return model class substitute_in_graph: def __init__(self, model, layer_name, new_layer): pass def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): pass def is_compiling(self): return False def list_backends(self): return [\\"backend1\\", \\"backend2\\"] class assume_constant_result: def __init__(self, layer_name): pass def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): pass def disable(self): pass def reset(self): pass torch.compiler = MockCompiler() # Compile the model compiled_model = torch.compiler.compile(model) # Substitute the first layer with a different layer (e.g., another Linear layer) with torch.compiler.substitute_in_graph(compiled_model, \'fc1\', nn.Linear(10, 20)): pass # Check if the model is currently being compiled compiling_status = torch.compiler.is_compiling() print(\\"Is compiling:\\", compiling_status) # List the available backends backends = torch.compiler.list_backends() print(\\"Available backends:\\", backends) # Assume the result of the first layer\'s output is constant with torch.compiler.assume_constant_result(\'fc1\'): pass # Disable compilation optimizations torch.compiler.disable() # Reset compilation state torch.compiler.reset() # Example usage (uncomment in real environment where torch.compiler is available): # if __name__ == \\"__main__\\": # model = SimpleModel() # optimize_model(model)"},{"question":"You are provided with a partially implemented Python class that is supposed to handle a custom iterator. The iterator has both synchronous and asynchronous capabilities. **Task:** Complete the implementation of the `CustomIterator` class to make it compatible with Python\'s iterator protocol. Specifically, implement the following methods: 1. `__iter__()`: This method should make the class instances iterable. 2. `__next__()`: This method should return the next item from the iterator, or raise `StopIteration` when the iterator is exhausted. 3. `asend(value)`: This method should handle sending a value to the asynchronous iterator and processing the result appropriately as per `PyIter_Send`. **Input:** The class will be initialized with a list of elements. ```python elements = [1, 2, 3, 4, 5] it = CustomIterator(elements) ``` **Output:** The `__next__()` method should return the next element in the list. If the list is exhausted, it should raise a `StopIteration` exception. The `asend(value)` method will handle asynchronous sending of a value and should return a result based on the simulated behavior of `PyIter_Send`. Constraints: - Ensure proper handling of exceptions and edge cases. - Demonstrate both synchronous and asynchronous iteration capabilities. ```python class CustomIterator: def __init__(self, elements): self.elements = elements self.index = 0 def __iter__(self): # Implement this method pass def __next__(self): # Implement this method pass async def asend(self, value): # Implement this method pass # Example Usage elements = [1, 2, 3, 4, 5] it = CustomIterator(elements) for element in it: print(element) await it.asend(10) ``` Implement the class as described and ensure it adheres to the specifications provided.","solution":"class CustomIterator: def __init__(self, elements): self.elements = elements self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.elements): element = self.elements[self.index] self.index += 1 return element else: raise StopIteration async def asend(self, value): # Simulate asynchronous handling of the value # Here we simply return the value for demonstration purposes. return f\\"Received: {value}\\" # Example Usage elements = [1, 2, 3, 4, 5] it = CustomIterator(elements) for element in it: print(element) # Since async cannot be directly run in a regular script, this part is commented. # This shows how it would be used in an async context. # async def main(): # result = await it.asend(10) # print(result) # import asyncio # asyncio.run(main())"},{"question":"Coding Assessment Question: # Objective: Implement an incremental encoder and decoder for a simple custom text codec that reverses the input text during encoding and restores it during decoding. # Problem Statement: Your task is to create a custom codec named `reverse_codec` that reverses the input string when encoding and restores it (reverses it again) during decoding. Implement incremental encoding and decoding methods to handle encoding and decoding in multiple steps. # Requirements: 1. Implement a class `ReverseIncrementalEncoder` that: - Inherits from `codecs.IncrementalEncoder`. - Implements the `encode` method to reverse the input string. - Implements the `reset` and `getstate`/`setstate` methods as required. 2. Implement a class `ReverseIncrementalDecoder` that: - Inherits from `codecs.IncrementalDecoder`. - Implements the `decode` method to reverse the input string back to its original form. - Implements the `reset` and `getstate`/`setstate` methods as required. 3. Implement a function `register_reverse_codec` that registers your custom codec using `codecs.register`. # Expected Input and Output Formats: Input: - The input will be a string to be encoded or decoded. Output: - The output after encoding will be the reversed string. - The output after decoding will be the string restored to its original form. # Constraints: - Ensure your incremental encoder and decoder handle inputs piece-by-piece and maintain state across multiple calls. # Example: ```python import codecs # Your code will go here # Register the custom codec register_reverse_codec() # Fetch the incremental encoder and decoder encoder = codecs.getincrementalencoder(\'reverse_codec\')() decoder = codecs.getincrementaldecoder(\'reverse_codec\')() # Encoding in steps encoded_part1 = encoder.encode(\\"Hello, \\", final=False) encoded_part2 = encoder.encode(\\"World!\\", final=True) encoded_result = encoded_part1 + encoded_part2 print(encoded_result) # Output should be \\"!dlroW ,olleH\\" # Decoding in steps decoded_part1 = decoder.decode(encoded_result[:6], final=False) decoded_part2 = decoder.decode(encoded_result[6:], final=True) decoded_result = decoded_part1 + decoded_part2 print(decoded_result) # Output should be \\"Hello, World!\\" ``` # Your Task: Implement the classes and function for the `reverse_codec` based on the specifications provided above.","solution":"import codecs class ReverseIncrementalEncoder(codecs.IncrementalEncoder): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.buffer = \\"\\" def encode(self, input, final=False): self.buffer += input if final: result = self.buffer[::-1] self.buffer = \\"\\" return result return \\"\\" def reset(self): self.buffer = \\"\\" def getstate(self): return self.buffer, 0 def setstate(self, state): self.buffer, _ = state class ReverseIncrementalDecoder(codecs.IncrementalDecoder): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self.buffer = \\"\\" def decode(self, input, final=False): self.buffer += input if final: result = self.buffer[::-1] self.buffer = \\"\\" return result return \\"\\" def reset(self): self.buffer = \\"\\" def getstate(self): return self.buffer, 0 def setstate(self, state): self.buffer, _ = state def reverse_codec_search(name): if name == \'reverse_codec\': return codecs.CodecInfo( name=\'reverse_codec\', encode=lambda input: (input[::-1], len(input)), decode=lambda input: (input[::-1], len(input)), incrementalencoder=ReverseIncrementalEncoder, incrementaldecoder=ReverseIncrementalDecoder ) return None def register_reverse_codec(): codecs.register(reverse_codec_search)"},{"question":"Multithreaded Application with Synchronization Primitives Objective: In this assessment, you are required to create a multithreaded application that simulates a simple library system. The system must allow multiple users to borrow and return books concurrently. You will use the `threading` module to manage threads and synchronization primitives to ensure thread safety while handling shared resources. Requirements: 1. **Book Management**: - Implement a class `Library` with methods to borrow and return books. - Use a `dictionary` to store the availability of books, where keys are book titles and values are quantities. 2. **User Interaction**: - Implement a class `User` that represents a user trying to borrow or return a book. Each `User` instance will run in a separate thread. 3. **Synchronization**: - Use `Lock` or `RLock` to ensure that checking out and returning books are thread-safe operations. - Avoid deadlocks and race conditions. Input: 1. List of books with quantities (`library_inventory`). 2. List of operations for users, where each operation is a tuple (`operation_type`, `book_title`). Output: Print logs of each operation performed by users, indicating whether the operation was successful or not. # Implementation Details: 1. **Class `Library`**: - **Attributes**: - `inventory`: A dictionary where keys are book titles and values are quantities. - `lock`: A `Lock` or `RLock` to ensure thread-safe operations. - **Methods**: - `borrow_book(self, book_title: str) -> bool`: If the book is available, decrement its quantity and return `True`. Otherwise, return `False`. - `return_book(self, book_title: str) -> None`: Increment the quantity of the returned book. 2. **Class `User`**: - **Attributes**: - `library`: An instance of the `Library` class. - `operations`: A list of operations to perform. - **Methods**: - `run(self) -> None`: Perform operations to borrow or return books based on the `operations` list. This method will be executed in a separate thread. 3. **Main Function**: - Initialize the `Library` with the given inventory. - Create multiple `User` instances with their respective operations. - Start all threads and wait for them to complete. - Print the final inventory of books. # Example: ```python import threading from typing import List, Dict, Tuple class Library: def __init__(self, inventory: Dict[str, int]): self.inventory = inventory self.lock = threading.RLock() def borrow_book(self, book_title: str) -> bool: with self.lock: if self.inventory.get(book_title, 0) > 0: self.inventory[book_title] -= 1 print(f\\"Book borrowed: {book_title}\\") return True else: print(f\\"Failed to borrow book: {book_title}\\") return False def return_book(self, book_title: str) -> None: with self.lock: if book_title in self.inventory: self.inventory[book_title] += 1 else: self.inventory[book_title] = 1 print(f\\"Book returned: {book_title}\\") class User(threading.Thread): def __init__(self, library: Library, operations: List[Tuple[str, str]]): super().__init__() self.library = library self.operations = operations def run(self) -> None: for operation_type, book_title in self.operations: if operation_type == \'borrow\': self.library.borrow_book(book_title) elif operation_type == \'return\': self.library.return_book(book_title) def main(library_inventory: Dict[str, int], user_operations: List[List[Tuple[str, str]]]): library = Library(library_inventory) users = [User(library, operations) for operations in user_operations] for user in users: user.start() for user in users: user.join() print(\\"Final Library Inventory:\\", library.inventory) # Test example if __name__ == \\"__main__\\": library_inventory = { \'Book A\': 2, \'Book B\': 1, \'Book C\': 0 } user_operations = [ [(\'borrow\', \'Book A\'), (\'return\', \'Book A\')], [(\'borrow\', \'Book B\'), (\'borrow\', \'Book B\')], [(\'return\', \'Book C\'), (\'borrow\', \'Book C\')] ] main(library_inventory, user_operations) ``` # Constraints: - The number of books and users is relatively small (up to 100). - Ensure your implementation handles typical concurrency issues such as race conditions and deadlocks. - The library should not allow negative quantities of books. Notes: 1. Make sure to use the `threading` module\'s synchronization primitives appropriately. 2. The example provided (in the main function) demonstrates how the classes should be used. 3. You may assume that the input is well-formed and does not require additional validation.","solution":"import threading from typing import Dict, List, Tuple class Library: def __init__(self, inventory: Dict[str, int]): self.inventory = inventory self.lock = threading.RLock() def borrow_book(self, book_title: str) -> bool: with self.lock: if self.inventory.get(book_title, 0) > 0: self.inventory[book_title] -= 1 print(f\\"Book borrowed: {book_title}\\") return True else: print(f\\"Failed to borrow book: {book_title}\\") return False def return_book(self, book_title: str) -> None: with self.lock: if book_title in self.inventory: self.inventory[book_title] += 1 else: self.inventory[book_title] = 1 print(f\\"Book returned: {book_title}\\") class User(threading.Thread): def __init__(self, library: Library, operations: List[Tuple[str, str]]): super().__init__() self.library = library self.operations = operations def run(self) -> None: for operation_type, book_title in self.operations: if operation_type == \'borrow\': self.library.borrow_book(book_title) elif operation_type == \'return\': self.library.return_book(book_title) def main(library_inventory: Dict[str, int], user_operations: List[List[Tuple[str, str]]]): library = Library(library_inventory) users = [User(library, operations) for operations in user_operations] for user in users: user.start() for user in users: user.join() print(\\"Final Library Inventory:\\", library.inventory) # Test example if __name__ == \\"__main__\\": library_inventory = { \'Book A\': 2, \'Book B\': 1, \'Book C\': 0 } user_operations = [ [(\'borrow\', \'Book A\'), (\'return\', \'Book A\')], [(\'borrow\', \'Book B\'), (\'borrow\', \'Book B\')], [(\'return\', \'Book C\'), (\'borrow\', \'Book C\')] ] main(library_inventory, user_operations)"},{"question":"You are given a dataset containing information about movies, including columns for movie title, genre, release year, rating, and revenue. Using pandas, you are required to perform a series of operations to analyze this dataset. # Problem Statement 1. Load the dataset into a pandas DataFrame. 2. Perform initial data exploration by displaying the first few records and summarizing the dataset. 3. Clean the dataset by: - Handling missing values: Remove rows where the rating and revenue are missing. - Standardize the genre column: Ensure all genre values are in lowercase. 4. Perform the following analyses: - Compute the average rating for each genre. - Determine the top 5 highest-grossing movies. 5. Visualize the distribution of movie revenues using a histogram. 6. Export the transformed dataset to a new CSV file. # Expected Functions 1. Function to Load Dataset: ```python def load_dataset(file_path: str) -> pd.DataFrame: Args: - file_path (str): Path to the CSV file containing the dataset. Returns: - pd.DataFrame: Loaded DataFrame. ``` 2. Function to Perform Initial Exploration: ```python def explore_dataset(df: pd.DataFrame): Args: - df (pd.DataFrame): DataFrame to explore. Prints: - First few rows of the DataFrame. - Summary of the DataFrame. ``` 3. Function to Clean Dataset: ```python def clean_dataset(df: pd.DataFrame) -> pd.DataFrame: Args: - df (pd.DataFrame): DataFrame to clean. Returns: - pd.DataFrame: Cleaned DataFrame. ``` 4. Function to Compute Average Rating by Genre: ```python def average_rating_by_genre(df: pd.DataFrame) -> pd.Series: Args: - df (pd.DataFrame): DataFrame containing movie data. Returns: - pd.Series: Average rating for each genre. ``` 5. Function to Get Top 5 Highest-grossing Movies: ```python def top_5_highest_grossing(df: pd.DataFrame) -> pd.DataFrame: Args: - df (pd.DataFrame): DataFrame containing movie data. Returns: - pd.DataFrame: DataFrame of top 5 highest-grossing movies. ``` 6. Function to Visualize Revenue Distribution: ```python def visualize_revenue_distribution(df: pd.DataFrame): Args: - df (pd.DataFrame): DataFrame containing movie data. Creates: - Histogram of movie revenues. ``` 7. Function to Export Dataset to CSV: ```python def export_dataset(df: pd.DataFrame, file_path: str): Args: - df (pd.DataFrame): DataFrame to export. - file_path (str): Path to save the new CSV file. Exports the DataFrame to a CSV file. ``` # Constraints - The dataset file provided will be in CSV format. - Ensure proper handling of missing values and data types. - Leverage pandas\' in-built methods for efficient computations. # Example Dataset ```plaintext title,genre,year,rating,revenue Inception,Sci-Fi,2010,8.8,829.89 The Dark Knight,Action,2008,9.0,1004.45 Interstellar,Sci-Fi,2014,8.6,677.47 Prestige,Drama,2006,8.5,109.46 Batman Begins,Action,2005,8.2,373.71 ... ``` Implement the series of functions as specified to complete the tasks. You are expected to demonstrate your understanding of fundamental and advanced pandas concepts through this exercise.","solution":"import pandas as pd import matplotlib.pyplot as plt def load_dataset(file_path: str) -> pd.DataFrame: Load the dataset from a CSV file into a pandas DataFrame. Args: - file_path (str): Path to the CSV file containing the dataset. Returns: - pd.DataFrame: Loaded DataFrame. return pd.read_csv(file_path) def explore_dataset(df: pd.DataFrame): Perform initial exploration of the dataset. Args: - df (pd.DataFrame): DataFrame to explore. Prints: - First few rows of the DataFrame. - Summary of the DataFrame. print(\\"First few records:\\") print(df.head()) print(\\"nSummary of the dataset:\\") print(df.info()) print(df.describe()) def clean_dataset(df: pd.DataFrame) -> pd.DataFrame: Clean the dataset by handling missing values and standardizing the genre column. Args: - df (pd.DataFrame): DataFrame to clean. Returns: - pd.DataFrame: Cleaned DataFrame. # Remove rows where the rating and revenue are missing df = df.dropna(subset=[\'rating\', \'revenue\']) # Standardize the genre column to lowercase df[\'genre\'] = df[\'genre\'].str.lower() return df def average_rating_by_genre(df: pd.DataFrame) -> pd.Series: Compute the average rating for each genre. Args: - df (pd.DataFrame): DataFrame containing movie data. Returns: - pd.Series: Average rating for each genre. return df.groupby(\'genre\')[\'rating\'].mean() def top_5_highest_grossing(df: pd.DataFrame) -> pd.DataFrame: Get the top 5 highest-grossing movies. Args: - df (pd.DataFrame): DataFrame containing movie data. Returns: - pd.DataFrame: DataFrame of top 5 highest-grossing movies. return df.nlargest(5, \'revenue\') def visualize_revenue_distribution(df: pd.DataFrame): Visualize the distribution of movie revenues using a histogram. Args: - df (pd.DataFrame): DataFrame containing movie data. Creates: - Histogram of movie revenues. plt.hist(df[\'revenue\'], bins=30, edgecolor=\'k\') plt.xlabel(\'Revenue (in millions)\') plt.ylabel(\'Frequency\') plt.title(\'Distribution of Movie Revenues\') plt.show() def export_dataset(df: pd.DataFrame, file_path: str): Export the DataFrame to a CSV file. Args: - df (pd.DataFrame): DataFrame to export. - file_path (str): Path to save the new CSV file. Exports the DataFrame to a CSV file. df.to_csv(file_path, index=False)"},{"question":"**Question: Custom Set Utility Function** # Objective: Develop a Python function `custom_set_utility` that performs a series of operations on a given list of commands. The commands will include creation of sets and frozensets, addition and removal of elements from sets, and membership checks. Your function should correctly handle each command and return the expected results. # Requirements: 1. **Input:** - A list of commands where each command is represented as a tuple. Each tuple can be: - `(\'create_set\', iterable)`: Creates a set from the given iterable and returns its size. - `(\'create_frozenset\', iterable)`: Creates a frozenset from the given iterable and returns its size. - `(\'add\', set_obj, element)`: Adds the element to the given set object and returns the new size of the set. - `(\'discard\', set_obj, element)`: Discards the element from the given set object and returns whether the element was found and removed (True) or not (False). - `(\'contains\', set_obj, element)`: Checks if the element is in the given set object and returns the result (True or False). - `(\'pop\', set_obj)`: Removes and returns an arbitrary element from the given set. Raises a `KeyError` if the set is empty. - `(\'clear\', set_obj)`: Clears all elements from the given set and returns the new size (which should be 0). 2. **Output:** - A list of results where each result corresponds to the return value of the respective command. # Constraints: - The input is guaranteed to contain valid commands. - For the purpose of this exercise, you can assume that no set exceeds 100 elements. # Example: ```python def custom_set_utility(commands): results = [] sets_map = {} for command in commands: if command[0] == \'create_set\': new_set = set(command[1]) set_id = id(new_set) sets_map[set_id] = new_set results.append((set_id, len(new_set))) elif command[0] == \'create_frozenset\': new_frozenset = frozenset(command[1]) frozenset_id = id(new_frozenset) sets_map[frozenset_id] = new_frozenset results.append((frozenset_id, len(new_frozenset))) elif command[0] == \'add\': set_obj = sets_map[command[1]] set_obj.add(command[2]) results.append(len(set_obj)) elif command[0] == \'discard\': set_obj = sets_map[command[1]] result = set_obj.discard(command[2]) results.append(result) elif command[0] == \'contains\': set_obj = sets_map[command[1]] result = command[2] in set_obj results.append(result) elif command[0] == \'pop\': set_obj = sets_map[command[1]] try: result = set_obj.pop() results.append(result) except KeyError: results.append(KeyError) elif command[0] == \'clear\': set_obj = sets_map[command[1]] set_obj.clear() results.append(len(set_obj)) return results # Example usage: commands = [ (\'create_set\', [1, 2, 3]), (\'add\', id([1, 2, 3]), 4), (\'discard\', id([1, 2, 3]), 2), (\'contains\', id([1, 2, 3]), 3), (\'pop\', id([1, 2, 3])), (\'clear\', id([1, 2, 3])) ] print(custom_set_utility(commands)) ``` # Note: Your function does not need to handle invalid input types or values for this exercise. Focus on correctly implementing the operations as specified.","solution":"def custom_set_utility(commands): results = [] sets_map = {} for command in commands: if command[0] == \'create_set\': new_set = set(command[1]) set_id = id(new_set) sets_map[set_id] = new_set results.append((set_id, len(new_set))) elif command[0] == \'create_frozenset\': new_frozenset = frozenset(command[1]) frozenset_id = id(new_frozenset) sets_map[frozenset_id] = new_frozenset results.append((frozenset_id, len(new_frozenset))) elif command[0] == \'add\': set_obj = sets_map[command[1]] set_obj.add(command[2]) results.append(len(set_obj)) elif command[0] == \'discard\': set_obj = sets_map[command[1]] result = command[2] in set_obj set_obj.discard(command[2]) results.append(result) elif command[0] == \'contains\': set_obj = sets_map[command[1]] result = command[2] in set_obj results.append(result) elif command[0] == \'pop\': set_obj = sets_map[command[1]] try: result = set_obj.pop() results.append(result) except KeyError: results.append(KeyError) elif command[0] == \'clear\': set_obj = sets_map[command[1]] set_obj.clear() results.append(len(set_obj)) return results"},{"question":"# Web Scraper with Proxy and Custom Headers Problem Statement You are tasked with creating a Python function `fetch_webpage_content(url: str, headers: dict, proxy: dict = None) -> str` which fetches the HTML content of a given `url`. The function needs to handle custom headers and optionally support proxy settings. Requirements: 1. **Custom Headers:** Allow the user to specify custom headers. 2. **Proxy Support:** Provide optional proxy configuration. 3. **Error Handling:** Handle and report HTTP and URL errors gracefully. 4. **HTTP Methods:** Support both GET and POST methods. Function Signature: ```python def fetch_webpage_content(url: str, headers: dict, proxy: dict = None) -> str: pass ``` Input: - `url` (str): The URL of the webpage to fetch. - `headers` (dict): A dictionary of HTTP headers to include in the request. - `proxy` (dict): (Optional) A dictionary containing proxy settings. Example: `{\'http\': \'http://proxy.example.com:8080\', \'https\': \'https://proxy.example.com:8080\'}`. Output: - Returns the HTML content of the webpage as a string. Constraints: - Make sure to handle common HTTP errors such as 404 (Not Found) and 403 (Forbidden). - The function should throw an appropriate exception message for network-related errors. Example: ```python url = \\"http://example.com\\" headers = { \'User-Agent\': \'Mozilla/5.0 (Windows NT 6.1; Win64; x64)\', \'Accept-Language\': \'en-US,en;q=0.5\' } proxy = { \'http\': \'http://proxy.example.com:8080\', \'https\': \'https://proxy.example.com:8080\' } try: content = fetch_webpage_content(url, headers, proxy) print(content) except Exception as e: print(f\\"An error occurred: {e}\\") ``` Instructions: 1. **Create a function `fetch_webpage_content` that takes the specified parameters**. 2. **Use the `urllib` package to open and read the URL**. 3. **Handle the provided headers and proxy settings**. 4. **Implement appropriate error handling**. 5. **Ensure the function returns the webpage content**. 6. **You may use additional helper functions if necessary**. Performance: - Ensure efficient handling of URL open operations and error management.","solution":"import urllib.request import urllib.error def fetch_webpage_content(url: str, headers: dict, proxy: dict = None) -> str: Fetches the HTML content of a given URL with support for custom headers and optional proxy settings. Args: - url (str): The URL of the webpage to fetch. - headers (dict): A dictionary of HTTP headers to include in the request. - proxy (dict, optional): A dictionary containing proxy settings. Returns: - str: The HTML content of the webpage. # Setup proxy handler if proxy is provided if proxy: proxy_handler = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) # Create request with headers request = urllib.request.Request(url, headers=headers) try: response = urllib.request.urlopen(request) content = response.read().decode(\'utf-8\') return content except urllib.error.HTTPError as e: # Handle HTTP errors (e.g., 404, 403) raise Exception(f\\"HTTPError: {e.code}, Reason: {e.reason}\\") except urllib.error.URLError as e: # Handle URL errors (e.g., failed to reach a server) raise Exception(f\\"URLError: {e.reason}\\") except Exception as e: # Handle other possible exceptions raise Exception(f\\"An unexpected error occurred: {e}\\")"},{"question":"**Problem Statement:** You are provided with a dataset containing 2D points drawn from a mixture of two Gaussian distributions. Your task is to implement a kernel density estimation (KDE) to estimate the probability density function of the data and visualize it using different kernels and bandwidths. You will also be required to draw new samples from the estimated distribution and visualize them. **Input:** - A 2D numpy array `data` of shape (N, 2) containing the dataset. - A list `kernels` containing different kernel names (e.g., \'gaussian\', \'tophat\', \'epanechnikov\'). - A list `bandwidths` containing different bandwidth values (e.g., [0.1, 0.5, 1.0]). **Output:** - For each combination of kernel and bandwidth: 1. Plot the KDE estimate as a contour plot. 2. Plot the newly generated samples. - Return a dictionary containing the KDE score samples for each kernel and bandwidth combination. **Constraints:** - The dataset `data` contains between 100 and 1000 points. - The bandwidth values are positive and within the range [0.01, 2.0]. **Performance Requirements:** - Your solution should efficiently handle the input size and parameters within reasonable runtime limits. **Function Signature:** ```python import numpy as np def kde_density_estimation_and_sampling(data: np.ndarray, kernels: list, bandwidths: list) -> dict: Estimate the probability density function of a given dataset using Kernel Density Estimation (KDE) with different kernels and bandwidths. Visualize the KDE and generated samples. Parameters: data (np.ndarray): 2D array of shape (N, 2) containing the dataset. kernels (list): List of kernel names. bandwidths (list): List of bandwidth values. Returns: dict: A dictionary containing the KDE score samples for each kernel and bandwidth combination. pass ``` **Example:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_blobs # Generating sample data data, _ = make_blobs(n_samples=300, centers=2, cluster_std=0.60, random_state=0) kernels = [\'gaussian\', \'tophat\', \'epanechnikov\'] bandwidths = [0.1, 0.5, 1.0] # Define the function signature results = kde_density_estimation_and_sampling(data, kernels, bandwidths) # Example output structure # results = { # \'gaussian_0.1\': array([ ... ]), # \'gaussian_0.5\': array([ ... ]), # \'gaussian_1.0\': array([ ... ]), # \'tophat_0.1\': array([ ... ]), # ... # } ``` **Note:** - Make sure to use `sklearn.neighbors.KernelDensity` for KDE. - Use appropriate visualization techniques (e.g., contour plots) to show the KDE estimate. - Use the `.sample()` method of the fitted KDE model to generate new samples. *Hints:* - Read the documentation of `KernelDensity` class to understand how to fit the model and get score samples. - Use `matplotlib` for plotting the KDE estimates and the newly generated samples.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def kde_density_estimation_and_sampling(data: np.ndarray, kernels: list, bandwidths: list) -> dict: results = {} for kernel in kernels: for bw in bandwidths: # Initialize and fit the KDE model kde = KernelDensity(kernel=kernel, bandwidth=bw) kde.fit(data) # Create a grid for plotting x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1 y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1 x_grid, y_grid = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)) xy_grid = np.vstack([x_grid.ravel(), y_grid.ravel()]).T # Evaluate the KDE on the grid z_grid = kde.score_samples(xy_grid).reshape(x_grid.shape) # Plot KDE estimate as contour plot plt.figure() plt.contourf(x_grid, y_grid, np.exp(z_grid), cmap=\'Blues\') plt.scatter(data[:, 0], data[:, 1], s=5, facecolor=\'white\') plt.title(f\\"KDE with {kernel} kernel and bandwidth {bw}\\") plt.xlabel(\\"X\\") plt.ylabel(\\"Y\\") plt.colorbar(label=\'Density\') plt.show() # Generate new samples new_samples = kde.sample(100) # Plot newly generated samples plt.figure() plt.scatter(new_samples[:, 0], new_samples[:, 1], s=5, facecolor=\'red\') plt.title(f\\"Generated samples with {kernel} kernel and bandwidth {bw}\\") plt.xlabel(\\"X\\") plt.ylabel(\\"Y\\") plt.show() # Save the results key = f\'{kernel}_{bw}\' results[key] = new_samples return results"},{"question":"# XML Data Transformation You are provided with an XML document containing information about different countries. The task is to write a Python function that reads this XML data, modifies it based on specific criteria, and outputs the transformed XML. **Input**: - An XML file named `country_data.xml` with the structure as given below: ```xml <?xml version=\\"1.0\\"?> <data> <country name=\\"Liechtenstein\\"> <rank>1</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank>4</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> <country name=\\"Panama\\"> <rank>68</rank> <year>2011</year> <gdppc>13600</gdppc> <neighbor name=\\"Costa Rica\\" direction=\\"W\\"/> <neighbor name=\\"Colombia\\" direction=\\"E\\"/> </country> </data> ``` **Output**: - A transformed XML document saved as `transformed_country_data.xml` with the following modifications: 1. Increase the rank of each country by 1. 2. Add an attribute `updated` with value `yes` to each `<rank>` element. 3. Remove countries that have a `rank` greater than 50. # Function Signature ```python def transform_country_data(input_file: str, output_file: str): pass ``` # Constraints: - You must ensure the output XML maintains proper formatting and indentation for readability. - Use the `xml.etree.ElementTree` module for all XML manipulations. # Example Given the input XML structure shown above, the output XML saved in `transformed_country_data.xml` should look like this: ```xml <?xml version=\\"1.0\\"?> <data> <country name=\\"Liechtenstein\\"> <rank updated=\\"yes\\">2</rank> <year>2008</year> <gdppc>141100</gdppc> <neighbor name=\\"Austria\\" direction=\\"E\\"/> <neighbor name=\\"Switzerland\\" direction=\\"W\\"/> </country> <country name=\\"Singapore\\"> <rank updated=\\"yes\\">5</rank> <year>2011</year> <gdppc>59900</gdppc> <neighbor name=\\"Malaysia\\" direction=\\"N\\"/> </country> </data> ``` # Notes - Assume the input XML file is always well-formed and valid. - You can assume the input and output file paths are correct.","solution":"import xml.etree.ElementTree as ET def transform_country_data(input_file: str, output_file: str): tree = ET.parse(input_file) root = tree.getroot() for country in root.findall(\'country\'): rank_element = country.find(\'rank\') rank = int(rank_element.text) if rank <= 50: rank_element.text = str(rank + 1) rank_element.set(\'updated\', \'yes\') else: root.remove(country) tree.write(output_file, xml_declaration=True, encoding=\'utf-8\', method=\\"xml\\")"},{"question":"# runpy Module Coding Assessment **Objective:** Create and execute a simple Python module dynamically using the `runpy` module. **Problem Statement:** You have been given the task of dynamically creating and running a Python module using the `runpy` module. Write a function `create_and_run_module` that performs the following tasks: 1. Create a new Python module named `dynamic_module.py` within a specified directory. The module should contain a function `greet(name)` that takes a string `name` as input and returns a greeting message in the format `\\"Hello, <name>!\\"`. 2. Use the `runpy.run_path` function from the `runpy` module to execute the code of the `dynamic_module.py` file and capture the resulting module globals dictionary. 3. From the globals dictionary, retrieve the `greet` function and call it with an input name of your choice. 4. Return the greeting message produced by the `greet` function. **Function Signature:** ```python def create_and_run_module(directory: str) -> str: pass ``` **Input:** - `directory` (str): The path to the directory where the new Python module `dynamic_module.py` should be created. **Output:** - A string representing the greeting message produced by the `greet` function within the created module. **Constraints:** - The provided `directory` path is valid and writable. **Example:** ```python # Assume the module is created in the temporary directory \'/tmp\' result = create_and_run_module(\'/tmp\') print(result) # Output: \\"Hello, World!\\" ``` **Explanation:** In the example provided, if the function `create_and_run_module` is correctly implemented, it will create a module named `dynamic_module.py` in the `/tmp` directory. It will then execute this module, call the `greet` function with a predetermined name (e.g., \\"World\\"), and return the resulting greeting message. **Note:** - You should clean up and remove the created `dynamic_module.py` file after executing it to avoid clutter. **Tips:** - Make use of file operations in Python to create and write to the module file. - Use the `runpy.run_path` function to dynamically execute the created module. - Handle any potential errors gracefully to ensure the module executes correctly.","solution":"import os import runpy def create_and_run_module(directory: str) -> str: Creates a Python module `dynamic_module.py` in the specified directory, then executes it using runpy.run_path, and returns the result of calling the `greet` function defined in the created module. module_code = def greet(name): return f\\"Hello, {name}!\\" module_path = os.path.join(directory, \'dynamic_module.py\') # Create the module file with open(module_path, \'w\') as f: f.write(module_code) try: # Execute the module and capture its global variables module_globals = runpy.run_path(module_path) # Retrieve the greet function greet_func = module_globals[\'greet\'] # Call the greet function with a sample name greeting = greet_func(\\"World\\") return greeting finally: # Clean up and remove the created module file os.remove(module_path)"},{"question":"# Description You are tasked with implementing a custom CUDA tensor operation pipeline using PyTorch, incorporating various CUDA features, such as device management, memory management, and CUDA streams. The pipeline will process data stored on multiple CUDA devices, perform operations asynchronously, and utilize CUDA graphs for efficient execution. # Requirements 1. **Device Management**: Implement a function that selects a CUDA device based on user input and verifies its availability. 2. **Tensor Operations**: Create tensors on specified CUDA devices and perform addition and multiplication on these tensors. 3. **Memory Management**: Monitor memory usage before and after operations and ensure efficient memory allocation and deallocation. 4. **Asynchronous Execution**: Execute tensor operations asynchronously using CUDA streams. 5. **CUDA Graphs**: Capture a sequence of tensor operations using CUDA graphs and replay it for multiple data. # Function Specifications Function 1: `select_cuda_device(device_index)` - **Input**: `device_index` (int) - The index of the CUDA device to select. - **Output**: None - **Behavior**: Sets the selected CUDA device and prints the device name. Raises an error if the device is not available. Function 2: `perform_tensor_operations(device_index, size)` - **Input**: - `device_index` (int) - The index of the CUDA device where tensors will be created and processed. - `size` (tuple of int) - The size of the tensors to be created. - **Output**: A dictionary containing: - `result_add` (Tensor) - Result of the tensor addition. - `result_mul` (Tensor) - Result of the tensor multiplication. - `memory_before` (int) - Memory usage before operations. - `memory_after` (int) - Memory usage after operations. Function 3: `async_tensor_operations(device_index, size)` - **Input**: - `device_index` (int) - The index of the CUDA device where tensors will be created and processed. - `size` (tuple of int) - The size of the tensors to be created. - **Output**: None - **Behavior**: Performs tensor operations asynchronously using CUDA streams and measures the elapsed time. Function 4: `cuda_graph_operations(device_index, size, iterations)` - **Input**: - `device_index` (int) - The index of the CUDA device where tensors will be created and processed. - `size` (tuple of int) - The size of the tensors to be created. - `iterations` (int) - The number of times to replay the graph. - **Output**: None - **Behavior**: Captures a sequence of tensor operations (addition and multiplication) using CUDA graphs and replays it for the specified number of iterations. Print the result tensor after each replay. # Constraints - Ensure that the scripts support CUDA 11 or later. - Validate that CUDA is available and handle exceptions if not. - Ensure memory operations are efficient and avoid memory leaks. - Demonstrate proper synchronization when using non-default CUDA streams. # Sample Usage ```python # Example usage device_index = 0 # Assuming CUDA device 0 is available tensor_size = (1000, 1000) # Size of tensors iterations = 5 # Number of graph replays select_cuda_device(device_index) results = perform_tensor_operations(device_index, tensor_size) print(f\\"Memory before: {results[\'memory_before\']} bytes\\") print(f\\"Memory after: {results[\'memory_after\']} bytes\\") async_tensor_operations(device_index, tensor_size) cuda_graph_operations(device_index, tensor_size, iterations) ``` Ensure the implementation adheres to the requirements and constraints specified, and demonstrate proficiency in PyTorch\'s CUDA functionality.","solution":"import torch import time def select_cuda_device(device_index): Sets the selected CUDA device and prints the device name. Raises an error if the device is not available. if not torch.cuda.is_available(): raise RuntimeError(\\"CUDA is not available.\\") if device_index < 0 or device_index >= torch.cuda.device_count(): raise ValueError(f\\"Invalid CUDA device index {device_index}.\\") torch.cuda.set_device(device_index) print(f\\"Selected CUDA device: {torch.cuda.get_device_name(device_index)}\\") def perform_tensor_operations(device_index, size): Creates tensors on the specified CUDA device and performs addition and multiplication on these tensors, monitoring memory usage before and after the operations. select_cuda_device(device_index) # Memory usage before operations memory_before = torch.cuda.memory_allocated(device_index) # Create tensors a = torch.ones(size, device=f\'cuda:{device_index}\') b = torch.ones(size, device=f\'cuda:{device_index}\') # Perform tensor operations result_add = a + b result_mul = a * b # Memory usage after operations memory_after = torch.cuda.memory_allocated(device_index) return { \'result_add\': result_add, \'result_mul\': result_mul, \'memory_before\': memory_before, \'memory_after\': memory_after } def async_tensor_operations(device_index, size): Performs tensor operations asynchronously using CUDA streams and measures the elapsed time. select_cuda_device(device_index) # Create tensors a = torch.ones(size, device=f\'cuda:{device_index}\') b = torch.ones(size, device=f\'cuda:{device_index}\') # Create CUDA stream stream = torch.cuda.Stream(device=device_index) # Asynchronous operations start_event = torch.cuda.Event(enable_timing=True) end_event = torch.cuda.Event(enable_timing=True) start_event.record(stream) with torch.cuda.stream(stream): result_add = a + b result_mul = a * b end_event.record(stream) stream.synchronize() # Wait for the stream to finish all tasks elapsed_time = start_event.elapsed_time(end_event) # Time in milliseconds print(f\\"Asynchronous operations took {elapsed_time} ms.\\") def cuda_graph_operations(device_index, size, iterations): Captures a sequence of tensor operations (addition and multiplication) using CUDA graphs and replays it for the specified number of iterations. select_cuda_device(device_index) # Create tensors a = torch.ones(size, device=f\'cuda:{device_index}\') b = torch.ones(size, device=f\'cuda:{device_index}\') # Capture the sequence of operations in a CUDA graph graph = torch.cuda.CUDAGraph() stream = torch.cuda.Stream(device=device_index) with torch.cuda.stream(stream): torch.cuda.synchronize(device_index) stream.wait_stream(torch.cuda.default_stream(device=device_index)) with torch.cuda.graph(graph): result_add = a + b result_mul = a * b # Record results to the output for i in range(iterations): graph.replay() print(f\\"Iteration {i + 1}, Result add sum: {result_add.sum().item()}\\") print(f\\"Iteration {i + 1}, Result mul sum: {result_mul.sum().item()}\\")"},{"question":"**Advanced Python Type Creation and Manipulation** **Problem Statement:** You are required to create a new heap-allocated type in Python and demonstrate its interaction with other types, along with certain checks and manipulations. You should follow the steps below to implement the solution in Python using the C API functions provided in the documentation. **Requirements:** 1. **Create a New Type:** - Use the `PyType_FromModuleAndSpec` function to create a new heap-allocated type called `CustomType`. - The new type should have a custom deallocation method and a custom method that returns a string \'Hello from CustomType!\' 2. **Define Type Slots:** - Define the necessary type slots for `CustomType`, including the deallocation and custom method slots. 3. **Check Type and Subtype:** - Implement a function to check if an object is of type `CustomType` using `PyType_Check` and `PyType_CheckExact`. - Implement another function to check if a given type is a subtype of `CustomType` using `PyType_IsSubtype`. 4. **Garbage Collection Support:** - Ensure the new type supports garbage collection by setting the appropriate type flags and implementing the necessary GC methods. 5. **Module and State Access:** - Associate the created type with a module and demonstrate how to retrieve the module and its state information using `PyType_GetModule` and `PyType_GetModuleState`. **Input:** - Define the input as appropriate for testing you implemented above functionalities. **Output:** - Print results demonstrating that the custom type and associated functionalities are working correctly. **Constraints:** - Implement the solution in a manner that is compatible with Python 3.10 and later. - Ensure proper memory management to avoid leaks. **Performance Requirements:** - The solution should handle typical use cases efficiently without significant performance bottlenecks. **Code Template:** ```python # Include necessary headers for Python C API # Define CustomType deallocation and custom method functions # Define type slots and type spec for CustomType # Implement functions to check type and subtype using PyType_Check, PyType_CheckExact, and PyType_IsSubtype # Ensure CustomType supports garbage collection and implements necessary GC methods # Associate CustomType with a module and demonstrate module/state retrieval using PyType_GetModule and PyType_GetModuleState # Test and demonstrate the implemented functionalities ``` Provide the complete implementation of the above requirements.","solution":"import gc class CustomType: def __del__(self): print(\\"CustomType instance deallocated\\") def custom_method(self): return \\"Hello from CustomType!\\" def check_type(obj): return isinstance(obj, CustomType) def check_exact_type(obj): return type(obj) is CustomType def check_subtype(obj): return issubclass(type(obj), CustomType) # A simple module to demonstrate access to the type and state class CustomModule: def __init__(self): self.custom_type = CustomType def get_type(self): return self.custom_type # Create a module instance custom_module = CustomModule() # Functions to access module and state def get_module(): return custom_module def get_module_state(): return custom_module print(\\"Testing CustomType functionality:\\") custom_instance = CustomType() print(custom_instance.custom_method()) print(f\\"custom_instance is CustomType: {check_type(custom_instance)}\\") print(f\\"Type of custom_instance is exactly CustomType: {check_exact_type(custom_instance)}\\") print(f\\"Is CustomType a subclass of itself: {check_subtype(custom_instance)}\\") print(\\"nTesting module and state access:\\") module_ref = get_module() print(f\\"Module reference: {module_ref}\\") module_state = get_module_state() print(f\\"Module state, custom_type: {module_state.get_type()}\\")"},{"question":"# Python Bytecode Analysis Task Objective Utilize the \\"dis\\" module to analyze Python bytecode for a given function. The task will focus on identifying specific bytecode instructions related to certain operations within the function. Problem Statement You are given a Python function that performs various operations. Your task is to write a Python program that: 1. Analyzes the bytecode of the given function. 2. Identifies and prints all occurrences of a specific bytecode instruction, along with the line number and the index position within the bytecode sequence where the instruction occurs. Function Signature ```python def analyze_bytecode(func: callable, instruction_name: str) -> None: ``` Parameters - `func`: A Python function whose bytecode will be analyzed. - `instruction_name`: A string representing the name of the bytecode instruction to be identified. Output For each occurrence of the specified bytecode instruction, the function should print a tuple in the following format: ``` (line number, instruction_index) ``` Where: - `line number` is the line number in the source code where the instruction is located. - `instruction_index` is the index position within the bytecode sequence. Example ```python import dis def example_function(x, y): z = x + y return z analyze_bytecode(example_function, \'BINARY_ADD\') ``` Expected output: ``` (3, 6) ``` Explanation: Line number `3` corresponds to the line `z = x + y` and the index position `6` is the bytecode index where the `BINARY_ADD` instruction occurs. Notes - You may use the `dis.Bytecode` class and its methods to analyze the bytecode. - Ensure your solution can handle functions with multiple occurrences of the specified instruction. - The provided function should be clear and self-contained, with all necessary validation handled correctly.","solution":"import dis def analyze_bytecode(func: callable, instruction_name: str) -> None: Analyzes the bytecode of the given function and identifies all occurrences of a specific bytecode instruction. Parameters: func (callable): The Python function whose bytecode will be analyzed. instruction_name (str): The name of the bytecode instruction to be identified. Output: Prints a tuple (line number, instruction_index) for each occurrence of the specified bytecode instruction. bytecode = dis.Bytecode(func) for instr in bytecode: if instr.opname == instruction_name: print((instr.starts_line, instr.offset))"},{"question":"# Question Overview You are provided with a dataset containing information about health expenditures over time for various countries. Using the seaborn library, your task is to create a line plot that visualizes the spending relative to each country\'s maximum spending for the given period and additionally highlight the percentage change in spending from a specific baseline year. Input The input dataset is named `healthexp` and consists of the following columns: - `Year` (int): The year of the recorded health expenditure. - `Spending_USD` (float): The health expenditure amount in USD. - `Country` (str): The name of the country. Requirements 1. Create a line plot with the following specifications: - `Year` on the x-axis. - `Spending_USD` on the y-axis, normalized relative to each country\'s maximum spending. - Different colors for each `Country`. 2. Add a second version of the plot showing the percentage change in spending from the year 1970. Output - A line plot showing spending relative to the maximum amount for each country. - A line plot showing the percentage change in spending from the year 1970. Constraints - Use the seaborn library and its `seaborn.objects` interface. - Ensure that each plot is appropriately labeled. Example Code You can start with a basic framework like this: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset healthexp = load_dataset(\\"healthexp\\") # Assume this loads the necessary dataset # Create the plot for spending relative to the maximum amount plot1 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm()) .label(y=\\"Spending relative to maximum amount\\") ) plot1.show() # Create the plot for percentage change in spending from 1970 baseline plot2 = ( so.Plot(healthexp, x=\\"Year\\", y=\\"Spending_USD\\", color=\\"Country\\") .add(so.Lines(), so.Norm(where=\\"x == x.min()\\", percent=True)) .label(y=\\"Percent change in spending from 1970 baseline\\") ) plot2.show() ``` Note: Ensure that the dataset is correctly loaded and that the normalization and labeling is performed as specified.","solution":"import seaborn.objects as so import pandas as pd def create_plots(dataset): Create two plots: 1. Spending relative to the maximum amount for each country. 2. Percentage change in spending from the year 1970. Parameters: dataset (pandas.DataFrame): The health expenditure dataset. Returns: plot1, plot2: The created seaborn plots. # Normalize Spending_USD relative to each country\'s maximum spending dataset[\'Max_Spending_USD\'] = dataset.groupby(\'Country\')[\'Spending_USD\'].transform(\'max\') dataset[\'Normalized_Spending\'] = dataset[\'Spending_USD\'] / dataset[\'Max_Spending_USD\'] # Calculate percentage change from the year 1970 baseline_1970 = dataset[dataset[\'Year\'] == 1970][[\'Country\', \'Spending_USD\']].set_index(\'Country\') dataset = dataset.join(baseline_1970, on=\'Country\', rsuffix=\'_1970\') dataset[\'Percent_Change\'] = ((dataset[\'Spending_USD\'] - dataset[\'Spending_USD_1970\']) / dataset[\'Spending_USD_1970\']) * 100 # Create the plot for spending relative to the maximum amount plot1 = ( so.Plot(dataset, x=\\"Year\\", y=\\"Normalized_Spending\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Spending relative to maximum amount\\") ) # Create the plot for percentage change in spending from 1970 baseline plot2 = ( so.Plot(dataset, x=\\"Year\\", y=\\"Percent_Change\\", color=\\"Country\\") .add(so.Line()) .label(y=\\"Percent change in spending from 1970 baseline\\") ) return plot1, plot2"},{"question":"# Rational Number Operations with `fractions.Fraction` In this exercise, you will implement a function that performs a series of operations using the `fractions.Fraction` class. This function will take a list of operations, execute them, then return a result based on those operations. Function Signature ```python def rational_operations(operations: list) -> float: ``` Input - `operations`: A list of tuples where each tuple contains: 1. A string indicating the operation. Possible values include: `\\"add\\"`, `\\"subtract\\"`, `\\"multiply\\"`, `\\"divide\\"`, `\\"limit_denominator\\"`, `\\"as_ratio\\"`, `\\"from_float\\"`, `\\"from_decimal\\"`. 2. Additional arguments required for the operation, which can be `Fraction` objects, integers, floats, or decimal strings based on the operation. Output - The function returns the result of the operations. If the final operation is `\\"as_ratio\\"`, return a tuple of integers. If the final result is a fraction, return its float representation. Example Usage ```python from fractions import Fraction from decimal import Decimal # Example 1: Basic arithmetic operations operations = [ (\\"add\\", Fraction(1, 2), Fraction(1, 3)), (\\"subtract\\", Fraction(1, 4)), (\\"multiply\\", Fraction(3)), (\\"divide\\", Fraction(2)), (\\"as_ratio\\",) ] print(rational_operations(operations)) # Output: (3, 8) # Example 2: Limiting denominator operations = [ (\\"from_float\\", 1.4), (\\"limit_denominator\\", 10), (\\"as_ratio\\",) ] print(rational_operations(operations)) # Output: (7, 5) # Example 3: Creating from decimal operations = [ (\\"from_decimal\\", Decimal(\'2.5\')), (\\"as_ratio\\",) ] print(rational_operations(operations)) # Output: (5, 2) ``` Constraints - Ensure the series of operations maintains type consistency (e.g., don\'t mix fractions with non-fraction types unless converting). - Utilize proper error handling for division by zero or invalid operations. - Operations should be performed in the order given in the input list. Hints - Use the `__add__`, `__sub__`, `__mul__`, and `__truediv__` methods for arithmetic operations on `Fraction` objects. - Use the class methods `from_float` and `from_decimal` to handle conversions from float and decimal. - The `limit_denominator` method will help you approximate fractions. - Handle unexpected input gracefully and ensure type consistency across operations.","solution":"from fractions import Fraction from decimal import Decimal def rational_operations(operations: list) -> float: Performs a series of operations using the fractions.Fraction class. Args: operations (list): A list of tuples indicating operations and their arguments. Returns: float: Result of the operations as a float or tuple if \'as_ratio\' is the final operation. result = None for operation in operations: op = operation[0] if op == \\"add\\": if result is None: result = operation[1] else: result += operation[1] elif op == \\"subtract\\": if result is None: result = operation[1] else: result -= operation[1] elif op == \\"multiply\\": if result is None: result = operation[1] else: result *= operation[1] elif op == \\"divide\\": if result is None: result = operation[1] else: result /= operation[1] elif op == \\"limit_denominator\\": max_denominator = operation[1] if len(operation) > 1 else 1000000 result = result.limit_denominator(max_denominator) elif op == \\"as_ratio\\": return result.numerator, result.denominator elif op == \\"from_float\\": result = Fraction.from_float(operation[1]) elif op == \\"from_decimal\\": result = Fraction(operation[1]) return float(result)"},{"question":"**Coding Assessment Question** # Objective Write a Python program using the `fnmatch` module to organize files based on their extensions into directories named after the extension. # Problem Statement You are provided with a list of filenames in a directory. Your task is to write a function `organize_files_by_extension(files: List[str]) -> Dict[str, List[str]]` that groups these files by their extensions. The function should return a dictionary where the keys are the file extensions (e.g., `.txt`, `.jpg`) and the values are lists of filenames with that extension. # Input - `files`: A list of strings representing filenames. # Output - A dictionary where each key is a file extension and the corresponding value is a list of filenames with that extension. # Example Example 1: ```python files = [\'file1.txt\', \'file2.jpg\', \'file3.txt\', \'file4.png\', \'file5.jpg\', \'file6.jpeg\'] output = organize_files_by_extension(files) print(output) ``` **Expected Output:** ```python { \'.txt\': [\'file1.txt\', \'file3.txt\'], \'.jpg\': [\'file2.jpg\', \'file5.jpg\'], \'.png\': [\'file4.png\'], \'.jpeg\': [\'file6.jpeg\'] } ``` # Constraints 1. The input list will contain minimum 1 and maximum 1000 filenames. 2. Each filename will be a non-empty string with a valid extension (e.g., \\"filename.ext\\"). 3. Filenames and extensions can be case-sensitive. # Requirements You need to use the `fnmatch.filter` function to filter files based on their extensions. # Solution Template ```python from typing import List, Dict import fnmatch def organize_files_by_extension(files: List[str]) -> Dict[str, List[str]]: # Your code goes here pass # Example usage files = [\'file1.txt\', \'file2.jpg\', \'file3.txt\', \'file4.png\', \'file5.jpg\', \'file6.jpeg\'] output = organize_files_by_extension(files) print(output) ``` # Notes - You are required to handle the grouping of files case-sensitively. - Ensure your function works efficiently within the provided constraints.","solution":"from typing import List, Dict import fnmatch def organize_files_by_extension(files: List[str]) -> Dict[str, List[str]]: organized_files = {} for file in files: extension = \\".\\" + file.split(\'.\')[-1] if extension not in organized_files: organized_files[extension] = [] organized_files[extension].append(file) return organized_files # Example usage files = [\'file1.txt\', \'file2.jpg\', \'file3.txt\', \'file4.png\', \'file5.jpg\', \'file6.jpeg\'] output = organize_files_by_extension(files) print(output)"},{"question":"# Task You are tasked with creating a custom cookie policy and utilizing it to manage cookies retrieved from a specific website. Your objective involves creating a program that: 1. Uses an instance of `CookieJar` to store cookies. 2. Loads and saves cookies in a Mozilla-compatible file format using `MozillaCookieJar`. 3. Implements a custom `CookiePolicy` by subclassing `DefaultCookiePolicy` to: - Block cookies from certain domains. - Allow cookies only from certain secure domains. 4. Utilizes `HTTPCookieProcessor` to handle cookies during web requests. 5. Demonstrates the functionality using the specified URL. # Detailed Instructions: Step 1: Define the Custom Cookie Policy Create a class `MyCookiePolicy` inheriting from `DefaultCookiePolicy` that: - Blocks cookies from the domains `blockeddomain.com` and `otherdomain.net`. - Only allows cookies from secure (HTTPS) domains, e.g., `example.com`. Step 2: Managing Cookies with `MozillaCookieJar` - Initialize a `MozillaCookieJar` object to load cookies from `cookies.txt`. - Save any new cookies back into `cookies.txt`. Step 3: Making HTTP requests - Use `urllib.request` along with the cookie handler to open the URL `https://www.example.com`. - Ensure the cookies are set correctly as per your custom cookie policy. # Constraints - Assume the cookie file `cookies.txt` exists in the current working directory. - Your program should handle potential exceptions, such as file not found or permission errors. # Input - The program does not take any direct input but reads from cookies.txt if it exists. # Output - The program should print the cookies stored in the `MozillaCookieJar` before and after making the HTTP request. # Example ```python # Your code here ``` # Your Implementation: 1. Implement the `MyCookiePolicy` class. 2. Create the `CookieJar` setup using the `MozillaCookieJar` for handling cookies. 3. Configure `HTTPCookieProcessor` with the cookie jar and custom policy. 4. Make a sample HTTP request to `https://www.example.com`. 5. Load the initial cookies, print them, make the request, and then print the cookies again to show changes.","solution":"import http.cookiejar import urllib.request from urllib.request import HTTPCookieProcessor from http.cookiejar import DefaultCookiePolicy, MozillaCookieJar class MyCookiePolicy(DefaultCookiePolicy): def set_ok(self, cookie, request): blocked_domains = [\'blockeddomain.com\', \'otherdomain.net\'] if any(cookie.domain.endswith(domain) for domain in blocked_domains): return False if not request.full_url.startswith(\'https://\'): return False return super(MyCookiePolicy, self).set_ok(cookie, request) def load_and_print_cookies(cookie_jar): try: cookie_jar.load(ignore_discard=True, ignore_expires=True) except FileNotFoundError: pass print(\\"Cookies before request:\\") for cookie in cookie_jar: print(cookie) def save_and_print_cookies(cookie_jar): cookie_jar.save(ignore_discard=True, ignore_expires=True) print(\\"Cookies after request:\\") for cookie in cookie_jar: print(cookie) def main(): cookie_jar = MozillaCookieJar(\'cookies.txt\') cookie_policy = MyCookiePolicy() load_and_print_cookies(cookie_jar) opener = urllib.request.build_opener(HTTPCookieProcessor(cookie_jar)) urllib.request.install_opener(opener) # Make a sample HTTP request to a secure URL url = \'https://www.example.com\' try: response = opener.open(url) response.read() # we should read the response to ensure the request completes except Exception as e: print(f\\"Failed to open the URL: {e}\\") save_and_print_cookies(cookie_jar) if __name__ == \\"__main__\\": main()"},{"question":"# Python Grammar Parsing Task Objective You are tasked with implementing a simple parser for a subset of Python statements based on a given grammar. Specifically, you need to parse `if` statements with optional `elif` and `else` branches, and represent them using a nested dictionary structure. Requirements 1. Implement a function `parse_if_statement(s: str) -> dict` that takes a string representing a Python `if` statement and returns a nested dictionary representing the parsed structure. 2. Your parser should handle: - `if` statements with conditional expressions and corresponding blocks. - Optional `elif` branches with their own conditions and blocks. - An optional `else` branch with its block. 3. Assume blocks are single line statements without nested compound statements for simplicity. 4. Raise a `SyntaxError` for invalid statements. Input - `s`: A string representing a Python `if` statement. Output - A nested dictionary representing the parsed `if` statement structure. Constraints - The string will only contain `if`, `elif`, `else`, and corresponding blocks. - Indentation is not required as the input will be a single string. Example ```python s = \\"if x > 0: print(\'positive\') elif x < 0: print(\'negative\') else: print(\'zero\')\\" expected_output = { \'type\': \'if\', \'condition\': \'x > 0\', \'block\': \\"print(\'positive\')\\", \'elif\': [ { \'condition\': \'x < 0\', \'block\': \\"print(\'negative\')\\" } ], \'else\': \\"print(\'zero\')\\" } assert parse_if_statement(s) == expected_output ``` Notes - The conditions and blocks should be represented as strings. - The `elif` branches, if present, should be represented as a list of dictionaries, each with its condition and block. - Use recursion or iterative parsing as appropriate to handle nested structures. Required Function Signature ```python def parse_if_statement(s: str) -> dict: pass ```","solution":"def parse_if_statement(s: str) -> dict: import re if_pattern = re.compile(r\'ifs+(.*?):s*(.*?)s*(elif.*|else.*)?\', re.DOTALL) elif_pattern = re.compile(r\'elifs+(.*?):s*(.*?)s*(elif.*|else.*)?\', re.DOTALL) else_pattern = re.compile(r\'else:s*(.*)\', re.DOTALL) def parse_statement(s): if_match = if_pattern.match(s) if if_match: condition = if_match.group(1).strip() block = if_match.group(2).strip() rest = if_match.group(3) result = { \'type\': \'if\', \'condition\': condition, \'block\': block, \'elif\': [], \'else\': None } while rest: elif_match = elif_pattern.match(rest) if elif_match: condition = elif_match.group(1).strip() block = elif_match.group(2).strip() result[\'elif\'].append({ \'condition\': condition, \'block\': block }) rest = elif_match.group(3) else: else_match = else_pattern.match(rest) if else_match: result[\'else\'] = else_match.group(1).strip() break return result raise SyntaxError(\\"Invalid if statement\\") return parse_statement(s)"},{"question":"# Decision Tree Classification and Pruning in scikit-learn You are provided a dataset that contains information about various features of flowers along with their species. The dataset is similar to the Iris dataset and contains four features: 1. Sepal Length 2. Sepal Width 3. Petal Length 4. Petal Width Your task is to write a Python function using scikit-learn that performs the following: 1. Load the dataset from a CSV file. 2. Train a decision tree classifier on the dataset. 3. Predict the species of the flowers based on their features. 4. Visualize the trained decision tree. 5. Implement cost-complexity pruning to avoid overfitting. 6. Evaluate and print the performance of the pruned tree on a test set. The function signature should be: ```python import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def decision_tree_classifier_with_pruning(csv_file: str, ccp_alpha: float) -> float: Trains a Decision Tree Classifier on the dataset, visualizes it, applies cost-complexity pruning, and evaluates its performance. Args: - csv_file (str): The path to the CSV file containing the dataset. - ccp_alpha (float): The complexity parameter used for pruning. Returns: - float: Accuracy of the pruned decision tree classifier on the test set. pass ``` # Instructions: 1. **Loading the Data:** - Read the dataset from the CSV file using pandas. - Assume the CSV file includes a header row with column names: `SepalLength`, `SepalWidth`, `PetalLength`, `PetalWidth`, and `Species`. 2. **Splitting the Data:** - Split the data into training and testing sets using an 80-20 ratio. 3. **Training the Model:** - Utilize the `DecisionTreeClassifier` from scikit-learn to train on the training set. 4. **Visualizing the Tree:** - Visualize the trained decision tree using the `plot_tree` function and display it using `matplotlib.pyplot`. 5. **Pruning the Tree:** - Use the `ccp_alpha` parameter to apply cost-complexity pruning and obtain a pruned decision tree. 6. **Evaluating Performance:** - Calculate and return the accuracy of the pruned tree on the test set. - Print the accuracy. # Constraints: - You should import the necessary libraries for data handling, model building, visualization, and evaluation. - Use a fixed random state (e.g., `random_state=42`) for reproducibility. - Consider edge cases like missing or inconsistent values in the dataset by handling or cleaning the data appropriately. # Example Usage: ```python csv_file = \'path_to_your_dataset.csv\' ccp_alpha = 0.01 accuracy = decision_tree_classifier_with_pruning(csv_file, ccp_alpha) print(f\\"Pruned tree accuracy: {accuracy}\\") ``` In your solution, ensure that the function: - Adequately responds to the objectives. - Creates visual outputs where required. - Utilizes pruning techniques to potentially improve model generalization.","solution":"import pandas as pd from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt def decision_tree_classifier_with_pruning(csv_file: str, ccp_alpha: float) -> float: Trains a Decision Tree Classifier on the dataset, visualizes it, applies cost-complexity pruning, and evaluates its performance. Args: - csv_file (str): The path to the CSV file containing the dataset. - ccp_alpha (float): The complexity parameter used for pruning. Returns: - float: Accuracy of the pruned decision tree classifier on the test set. # Load dataset df = pd.read_csv(csv_file) # Check for missing values and handle them if necessary (e.g., fill with mean or remove rows) df.dropna(inplace=True) # Split dataset into features and target X = df[[\'SepalLength\', \'SepalWidth\', \'PetalLength\', \'PetalWidth\']] y = df[\'Species\'] # Split dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train the Decision Tree Classifier clf = DecisionTreeClassifier(random_state=42) clf.fit(X_train, y_train) # Visualize the trained decision tree plt.figure(figsize=(20, 10)) plot_tree(clf, filled=True, feature_names=[\'SepalLength\', \'SepalWidth\', \'PetalLength\', \'PetalWidth\'], class_names=clf.classes_) plt.title(\\"Decision Tree before Pruning\\") plt.show() # Apply cost-complexity pruning to avoid overfitting clf_pruned = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha) clf_pruned.fit(X_train, y_train) # Predict on test set using the pruned tree y_pred = clf_pruned.predict(X_test) # Evaluate and print performance accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"Objective: Implement a function to perform a series of arithmetic and transformation operations on a list of numbers. Your function will take a list of numbers and a list of operations to perform on these numbers. The operations will be performed sequentially on the list. Function Signature: ```python def perform_operations(numbers: list, operations: list) -> list: Perform a series of arithmetic and transformation operations on a list of numbers. ``` Input: 1. `numbers` (list): A list of integers or floats. 2. `operations` (list): A list of operations. Each operation is represented as a tuple: - The first element of the tuple is a string indicating the type of operation. The operation can be one of the following: - `\\"add\\"`: Addition (requires two numbers) - `\\"subtract\\"`: Subtraction (requires two numbers) - `\\"multiply\\"`: Multiplication (requires two numbers) - `\\"divide\\"`: True division (requires two numbers) - `\\"floordivide\\"`: Floor division (requires two numbers) - `\\"mod\\"`: Modulus (requires two numbers) - `\\"power\\"`: Power (requires two numbers) - `\\"negate\\"`: Negation (requires one number) - `\\"absolute\\"`: Absolute value (requires one number) - `\\"invert\\"`: Bitwise NOT (requires one number) - The subsequent elements of the tuple are the operands for the respective operations. Output: Return the list of numbers with the operations applied sequentially. Constraints: - You may assume that the list `operations` will contain valid operations for the given list `numbers`. - Avoid using Python\'s built-in operators directly for arithmetic operations to demonstrate your understanding of how to work with the `python310` package. Example: ```python # Example 1 numbers = [4, 2, -3, 8] operations = [(\\"add\\", 0, 1), (\\"negate\\", 2), (\\"absolute\\", 2), (\\"invert\\", 1)] result = perform_operations(numbers, operations) print(result) # Output: [6, -3, 3, -3, 8] # Example 2 numbers = [5, 3, 7] operations = [(\\"multiply\\", 0, 1), (\\"subtract\\", 2, 1), (\\"power\\", 0, 2)] result = perform_operations(numbers, operations) print(result) # Output: [15, 4, 225] ``` # Note: - The first operation `(\\"add\\", 0, 1)` adds the numbers at index 0 and index 1, the result (6) replaces the number at index 0. - The second operation `(\\"negate\\", 2)` negates the number at index 2. - The third operation `(\\"absolute\\", 2)` takes the absolute value of the number at index 2. - The fourth operation `(\\"invert\\", 1)` performs bitwise inversion of the number at index 1. You will need to make use of the functions available in the `python310` package to implement these operations.","solution":"def perform_operations(numbers: list, operations: list) -> list: Perform a series of arithmetic and transformation operations on a list of numbers. import operator for operation in operations: op_type = operation[0] if op_type == \\"add\\": index1, index2 = operation[1], operation[2] numbers[index1] = operator.add(numbers[index1], numbers[index2]) elif op_type == \\"subtract\\": index1, index2 = operation[1], operation[2] numbers[index1] = operator.sub(numbers[index1], numbers[index2]) elif op_type == \\"multiply\\": index1, index2 = operation[1], operation[2] numbers[index1] = operator.mul(numbers[index1], numbers[index2]) elif op_type == \\"divide\\": index1, index2 = operation[1], operation[2] numbers[index1] = operator.truediv(numbers[index1], numbers[index2]) elif op_type == \\"floordivide\\": index1, index2 = operation[1], operation[2] numbers[index1] = operator.floordiv(numbers[index1], numbers[index2]) elif op_type == \\"mod\\": index1, index2 = operation[1], operation[2] numbers[index1] = operator.mod(numbers[index1], numbers[index2]) elif op_type == \\"power\\": index1, index2 = operation[1], operation[2] numbers[index1] = operator.pow(numbers[index1], numbers[index2]) elif op_type == \\"negate\\": index = operation[1] numbers[index] = operator.neg(numbers[index]) elif op_type == \\"absolute\\": index = operation[1] numbers[index] = operator.abs(numbers[index]) elif op_type == \\"invert\\": index = operation[1] numbers[index] = operator.inv(numbers[index]) return numbers"},{"question":"You are given two datasets: `fmri` and `seaice` from the seaborn library. Your task is to create visualizations that make use of `seaborn.objects.Plot`, demonstrating your understanding of preprocessing, filtering, and plotting with bands and lines. # Part 1: fmri Dataset 1. Load the `fmri` dataset using the `load_dataset` function from the `seaborn` library. 2. Filter the dataset to include only the data where `region` is `\'parietal\'`. 3. Using the `seaborn.objects.Plot` method, create a plot with: - `timepoint` on the x-axis. - `signal` on the y-axis. - Lines grouped by `subject`. - A band to represent the range of the signal for each `event` type over time. # Part 2: Seaice Dataset 1. Load the `seaice` dataset using the `load_dataset` function from the `seaborn` library. 2. Preprocess the dataset to: - Extract the `Day` of the year from the `Date`. - Filter data for years 1980 and 2019. - Restructure the dataset to have `Day` as rows and `Extent` for years 1980 and 2019 as columns. - Drop any rows with missing values and reset the index. 3. Using the `seaborn.objects.Pplot` method, create a plot with: - `Day` on the x-axis. - A band showing the range between the extent of sea ice for the years 1980 and 2019. - Customize the appearance of the band to have an `alpha` of 0.5 and an `edgewidth` of 2. # Input and Output - **Input:** You do not need to worry about taking input as this task requires you to write code that generates the visualizations. - **Output:** The output should be visualizations displayed using seaborn. # Example Code ```python import seaborn.objects as so from seaborn import load_dataset # Part 1: fmri Dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Line(linewidth=.5), group=\\"subject\\") .add(so.Band()) ) # Part 2: Seaice Dataset seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) p = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") p.add(so.Band(alpha=.5, edgewidth=2)) ``` Make sure your solution is efficient and handles any potential data issues appropriately.","solution":"import seaborn.objects as so from seaborn import load_dataset # Part 1: fmri Dataset def plot_fmri_parietal(): fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") return ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Line(linewidth=.5), group=\\"subject\\") .add(so.Band()) ) # Part 2: Seaice Dataset def plot_seaice_1980_2019(): seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year ) .query(\\"Year in [1980, 2019]\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .dropna() .reset_index() ) p = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") return p.add(so.Band(alpha=.5, edgewidth=2))"},{"question":"You are required to write a Python function using the `memoryview` object to efficiently manipulate large data buffers. The goal is to process an input string such that all occurrences of a specific substring are replaced with another substring. Function Signature ```python def replace_substring(buffer: str, target: str, replacement: str) -> memoryview: ``` Inputs - `buffer` (str): A large input string that we want to process. - `target` (str): The substring that we want to find and replace in the buffer. - `replacement` (str): The substring that will replace each occurrence of the target. Outputs - The function should return a `memoryview` object containing the modified string. Constraints - The function should avoid creating multiple copies of the data to ensure efficient memory usage. - The length of the buffer will be at least 1 and can be very large (up to several Gigabytes). - The target and replacement strings will be non-empty, and the target string will be shorter than the buffer. - Consider performance, especially concerning memory usage and processing time. Example ```python buffer = \\"This is a sample buffer. Buffer processing is efficient.\\" target = \\"buffer\\" replacement = \\"data stream\\" # Expected to replace \'buffer\' with \'data stream\' resulting in: # \\"This is a sample data stream. Buffer processing is efficient.\\" result = replace_substring(buffer, target, replacement) print(result.tobytes().decode(\'utf-8\')) # Output should be a memoryview object that, when converted to bytes and then string, matches: # \\"This is a sample data stream. Buffer processing is efficient.\\" ``` Additional Notes - You can use the `PyMemoryView_FromObject` function to create the memoryview. - Ensure your implementation avoids unnecessary memory allocations to handle large data efficiently. - Document any assumptions or design choices you make in your implementation.","solution":"def replace_substring(buffer: str, target: str, replacement: str) -> memoryview: Replaces all occurrences of target substring in the buffer with the replacement substring using memoryview. :param buffer: The input string to be processed. :param target: The substring to be replaced. :param replacement: The substring to replace the target with. :return: A memoryview object containing the modified string. # Perform the replacement in the string modified_buffer = buffer.replace(target, replacement) # Convert the modified string to bytes and create a memoryview from it buffer_bytes = modified_buffer.encode(\'utf-8\') return memoryview(buffer_bytes)"},{"question":"**Coding Assessment Question** You are provided with the `penguins` dataset from the `seaborn` library, containing various measurements of penguins. Your task is to create different types of empirical cumulative distribution function (ECDF) plots using `seaborn` and customize them according to the instructions below. This will demonstrate your ability to use `seaborn` to visualize data effectively. # Instructions: 1. **Load Data:** - Load the `penguins` dataset from the `seaborn` library. 2. **Plot ECDF for `flipper_length_mm`:** - Create a basic ECDF plot for the `flipper_length_mm` variable along the x-axis. 3. **Flip Plot Axis:** - Create an ECDF plot for the `flipper_length_mm` variable along the y-axis. 4. **Wide-form Dataset Plot:** - Create an ECDF plot for the variables `bill_length_mm` and `bill_depth_mm` without specifying `x` or `y`. 5. **Plot with Subgroups:** - Create an ECDF plot for `bill_length_mm` with subgroups differentiated by the `species` of the penguins. Use a `hue` parameter. 6. **Different Statistic:** - Create an ECDF plot for `bill_length_mm`, differentiating by `species`, that shows the count instead of the proportion. 7. **Complementary CDF:** - Create an ECDF plot for `bill_length_mm`, differentiating by `species`, and plot the empirical complementary CDF. # Requirements: - Your code should be well-organized and include comments explaining each step. - Ensure the ECDF plots are visually clear and properly labeled. - Use appropriate styling and customization in Seaborn to enhance the readability of the plots. # Submission: Submit a Python file or Jupyter Notebook containing your code and the generated plots. # Example Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # Plot ECDF for flipper_length_mm (x-axis) sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.show() # Plot ECDF for flipper_length_mm (y-axis) sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\") plt.show() # Wide-form dataset plot for bill measurements sns.ecdfplot(data=penguins.filter(like=\\"bill_\\", axis=\\"columns\\")) plt.show() # Plot with subgroups based on species sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.show() # Different statistic (count) for bill_length_mm by species sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.show() # Complementary CDF for bill_length_mm by species sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.show() ``` Ensure your plots are clear and well-labeled. Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_dataset(): Load the penguins dataset from seaborn. return sns.load_dataset(\\"penguins\\") def plot_ecdf_flipper_length_xaxis(penguins): Plot ECDF for flipper_length_mm on the x-axis. sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Length (mm)\\") plt.xlabel(\\"Flipper Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() def plot_ecdf_flipper_length_yaxis(penguins): Plot ECDF for flipper_length_mm on the y-axis. sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\") plt.title(\\"ECDF of Flipper Length (mm) on Y-axis\\") plt.ylabel(\\"Flipper Length (mm)\\") plt.xlabel(\\"ECDF\\") plt.show() def plot_ecdf_wide_form(penguins): Plot ECDF for bill_length_mm and bill_depth_mm as wide-form dataset. sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\") sns.ecdfplot(data=penguins, x=\\"bill_depth_mm\\") plt.title(\\"ECDF of Bill Length and Bill Depth\\") plt.xlabel(\\"Measurement (mm)\\") plt.ylabel(\\"ECDF\\") plt.legend([\\"Bill Length\\", \\"Bill Depth\\"]) plt.show() def plot_ecdf_subgroups_by_species(penguins): Plot ECDF for bill_length_mm differentiated by species. sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\") plt.title(\\"ECDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"ECDF\\") plt.show() def plot_ecdf_count_statistic_by_species(penguins): Plot ECDF for bill_length_mm by species with count statistic. sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\") plt.title(\\"ECDF of Bill Length (mm) by Species (Count)\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Count\\") plt.show() def plot_ecdf_complementary_cdf(penguins): Plot complementary ECDF for bill_length_mm by species. sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True) plt.title(\\"Complementary ECDF of Bill Length (mm) by Species\\") plt.xlabel(\\"Bill Length (mm)\\") plt.ylabel(\\"Complementary ECDF\\") plt.show()"},{"question":"# Custom Python Interpreter Objective Create a custom interactive Python interpreter using the `code` module. The interpreter should have the following features: 1. **Command History:** The ability to keep track of past commands and allow users to navigate through their command history. 2. **Custom Commands:** Implement at least two custom commands: - `!exit` - Exits the interpreter. - `!history` - Prints the list of previously executed commands. Instructions You need to implement a class `CustomInterpreter` that extends the `InteractiveConsole` class from the `code` module. The class should include the ability to execute Python commands interactively along with the custom features mentioned above. Requirements 1. **Initialization:** Initialize the interactive console with an empty command history. 2. **Command Execution:** Override the standard execution method to capture and store each command in the history, except for the custom commands. 3. **Custom Commands:** - Implement the `!exit` command to gracefully terminate the interpreter loop. - Implement the `!history` command to print a list of previously entered commands. Input and Output - **Input:** Commands entered by the user in the console. - **Output:** Execution results of the Python commands or results of the custom commands. Example Here is an example session with the custom interpreter: ``` >>> 2 + 2 4 >>> !history 2 + 2 >>> print(\\"Hello, World!\\") Hello, World! >>> !history 2 + 2 print(\\"Hello, World!\\") >>> !exit ``` Constraints - You should not use any external libraries except those mentioned in the documentation. - The custom interpreter should allow execution of valid Python commands along with the custom commands defined. Implementation Skeleton Here is a starting point for your implementation: ```python from code import InteractiveConsole class CustomInterpreter(InteractiveConsole): def __init__(self): super().__init__() self.history = [] def push(self, line): if line == \\"!exit\\": print(\\"Exiting the interpreter...\\") return True elif line == \\"!history\\": for command in self.history: print(command) return False else: self.history.append(line) return super().push(line) # Example usage: # interpreter = CustomInterpreter() # interpreter.interact() ``` Complete the implementation to fully support the requirements outlined above.","solution":"from code import InteractiveConsole class CustomInterpreter(InteractiveConsole): def __init__(self): super().__init__() self.history = [] def push(self, line): if line == \\"!exit\\": print(\\"Exiting the interpreter...\\") return True # Indicate that the interactive session should end elif line == \\"!history\\": for command in self.history: print(command) return False # Don\'t end the interactive session else: self.history.append(line) return super().push(line)"},{"question":"**Custom Codec Implementation and Usage** You are required to implement a custom codec to encode and decode a specific format. This format includes a prefix and suffix around the original message. # Specifications 1. **Prefix**: `\\"[ENC]\\"`. 2. **Suffix**: `\\"[END]\\"`. Example: - Original Message: `\\"Hello, World!\\"` - Encoded Message: `\\"[ENC]Hello, World![END]\\"` # Requirements 1. **Implement a custom codec class** with `encode` and `decode` methods: - `encode(input, errors=\'strict\')`: Adds the prefix and suffix to the input text. - `decode(input, errors=\'strict\')`: Removes the prefix and suffix from the input text. 2. **Handle Errors** appropriately: If the decode method receives an input that does not contain the expected prefix and suffix, it should raise a `ValueError` with the message `\\"Decoding error: Invalid format\\"`. 3. **Register the custom codec** in the `codecs` module with the name `\\"custom_enc\\"`. 4. **Provide functions** `register_custom_codec()` to register the codec and `unregister_custom_codec()` to unregister it. # Function Signatures - `class CustomCodecInfo: def encode(input: str, errors: str = \'strict\') -> tuple: pass def decode(input: str, errors: str = \'strict\') -> tuple: pass ` - `def register_custom_codec() -> None: pass` - `def unregister_custom_codec() -> None: pass` # Example Usage ```python # Register the custom codec register_custom_codec() # Encode a message encoded_message = codecs.encode(\\"Hello, World!\\", \\"custom_enc\\") print(encoded_message) # Output: \\"[ENC]Hello, World![END]\\" # Decode a message decoded_message = codecs.decode(\\"[ENC]Hello, World![END]\\", \\"custom_enc\\") print(decoded_message) # Output: \\"Hello, World!\\" # Unregister the custom codec unregister_custom_codec() ``` # Constraints - You must not use any third-party libraries. - You must handle errors as described in the requirements. - Your implementation must be compatible with Python 3.10. Implement the required functions and class to complete this task.","solution":"import codecs class CustomCodecInfo: prefix = \\"[ENC]\\" suffix = \\"[END]\\" @staticmethod def encode(input: str, errors: str = \'strict\') -> tuple: encoded = CustomCodecInfo.prefix + input + CustomCodecInfo.suffix return encoded, len(encoded) @staticmethod def decode(input: str, errors: str = \'strict\') -> tuple: if input.startswith(CustomCodecInfo.prefix) and input.endswith(CustomCodecInfo.suffix): decoded = input[len(CustomCodecInfo.prefix):-len(CustomCodecInfo.suffix)] return decoded, len(decoded) raise ValueError(\\"Decoding error: Invalid format\\") def search_function(encoding_name): if encoding_name == \'custom_enc\': return codecs.CodecInfo( name=\'custom_enc\', encode=CustomCodecInfo.encode, decode=CustomCodecInfo.decode ) return None def register_custom_codec() -> None: codecs.register(search_function) def unregister_custom_codec() -> None: codecs.unregister(search_function)"},{"question":"You are provided with a PyTorch module that performs a simple mathematical operation and your task is to design a transformation function using `torch.fx` to modify this module. Specifically, your transformation should replace all instances of `torch.add` operations within the module with `torch.mul` operations. Task 1. Define a PyTorch `nn.Module` named `SimpleModule` with a `forward` method that computes `torch.add` on two inputs. 2. Implement a function `transform_add_to_mul` using `torch.fx` that takes an instance of `SimpleModule`, traces its graph, and replaces all `torch.add` operations with `torch.mul`. 3. Ensure that the transformed module performs `torch.mul` on the same inputs. 4. Verify the correctness of the transformed module by comparing its outputs with manual multiplication of inputs. Requirements 1. The `SimpleModule` should: - Take two tensors as inputs and return their sum using `torch.add`. 2. The `transform_add_to_mul` function should: - Use symbolic tracing to acquire the graph. - Replace `torch.add` nodes with `torch.mul` nodes. - Return a new `torch.fx.GraphModule` with the updated graph. 3. The transformed module should: - Perform element-wise multiplication on the inputs instead of addition. Input Format - The `SimpleModule` should work with any input dimensions provided they are valid for `torch.add` and `torch.mul`. Output Format - The function `transform_add_to_mul` should return a new `GraphModule` where all `torch.add` operations are replaced by `torch.mul`. - You should verify that the transformed module\'s output is equivalent to multiplying the inputs directly. Example ```python import torch import torch.nn as nn import torch.fx as fx class SimpleModule(nn.Module): def forward(self, x, y): return torch.add(x, y) def transform_add_to_mul(module): # Step 1: Acquire the graph tracer = fx.Tracer() graph = tracer.trace(module) # Step 2: Modify the Graph to replace add with mul for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul # Step 3: Create a new GraphModule new_module = fx.GraphModule(module, graph) return new_module # Testing the transformation module = SimpleModule() transformed_module = transform_add_to_mul(module) # Test tensors x = torch.tensor([1.0, 2.0, 3.0]) y = torch.tensor([4.0, 5.0, 6.0]) # Verifying correctness assert torch.allclose(transformed_module(x, y), x * y) ``` In this task, students will demonstrate their understanding of tracing, graph manipulation, and validating the transformation using `torch.fx`.","solution":"import torch import torch.nn as nn import torch.fx as fx class SimpleModule(nn.Module): def forward(self, x, y): return torch.add(x, y) def transform_add_to_mul(module): # Step 1: Acquire the graph tracer = fx.Tracer() graph = tracer.trace(module) # Step 2: Modify the Graph to replace add with mul for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.add: node.target = torch.mul # Step 3: Create a new GraphModule new_module = fx.GraphModule(module, graph) return new_module"},{"question":"**Objective:** Evaluate the student\'s understanding of using the seaborn `objects` module to create bar plots with different aggregation methods and transformations. **Question:** You are given a dataset named `diamonds` which contains various attributes of diamonds, such as `cut`, `clarity`, and `carat`. Your task is to create a function `plot_diamond_statistics` that takes no arguments and performs the following: 1. Load the `diamonds` dataset using `seaborn.load_dataset(\\"diamonds\\")`. 2. Create a bar plot showing the average `carat` weight for each `clarity` category using `seaborn.objects`. 3. Create another bar plot showing the median `carat` weight for each `clarity` category. 4. Create a custom bar plot that calculates the interquartile range (IQR) of the `carat` weight for each `clarity` category. 5. Create a bar plot that shows the average `carat` weight for each `cut` category, color-coded by `clarity`. Display all plots inline. **Function Signature:** ```python def plot_diamond_statistics(): pass ``` **Constraints and Notes:** - Use the `seaborn.objects` module to create the plots. - For the first three plots, the x-axis should represent the `clarity` categories, and the y-axis should represent the `carat` weight. - For the fourth plot, use the `color` parameter to differentiate between different `clarity` categories. - Ensure that all plots are clearly labeled with appropriate titles for better readability. **Example:** ```python # Example usage of the function: plot_diamond_statistics() # This should display four plots as specified in the requirements. ``` **Expected Output:** The function will display four different bar plots as described and will help in visualizing the statistics of diamond carat weights across different categories of clarity and cut.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd import numpy as np def plot_diamond_statistics(): # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create the average carat weight bar plot for each clarity category avg_plot = sns.catplot( x=\\"clarity\\", y=\\"carat\\", data=diamonds, kind=\\"bar\\", ci=None, estimator=np.mean ) avg_plot.fig.suptitle(\'Average Carat Weight by Clarity\') # Create the median carat weight bar plot for each clarity category median_plot = sns.catplot( x=\\"clarity\\", y=\\"carat\\", data=diamonds, kind=\\"bar\\", ci=None, estimator=np.median ) median_plot.fig.suptitle(\'Median Carat Weight by Clarity\') # Create the IQR plot for each clarity category def iqr(arr): return np.percentile(arr, 75) - np.percentile(arr, 25) iqr_plot = sns.catplot( x=\\"clarity\\", y=\\"carat\\", data=diamonds, kind=\\"bar\\", ci=None, estimator=iqr ) iqr_plot.fig.suptitle(\'IQR of Carat Weight by Clarity\') # Create the average carat weight bar plot for each cut category, color-coded by clarity avg_cut_plot = sns.catplot( x=\\"cut\\", y=\\"carat\\", hue=\\"clarity\\", data=diamonds, kind=\\"bar\\", ci=None, estimator=np.mean ) avg_cut_plot.fig.suptitle(\'Average Carat Weight by Cut and Clarity\') plt.show()"},{"question":"Objective The objective of this assessment is to test your understanding and ability to work with the `poplib` module to interact with a POP3 email server, retrieve messages, and process them according to specified criteria. Task You need to implement a class `EmailClient` using the `poplib` module with the following methods: 1. **`__init__(self, host, port, use_ssl=True, timeout=10)`** - **Input**: Initialize the class with server details. - `host` (str): The POP3 server address. - `port` (int): The server port. - `use_ssl` (bool): Whether to use SSL for the connection. By default, it is `True`. - `timeout` (int): Timeout for the connection in seconds. By default, it is `10`. - The method should establish a connection to the server using the provided details. 2. **`login(self, username, password)`** - **Input**: Login to the server. - `username` (str): Username for the POP3 server. - `password` (str): Password for the POP3 server. - **Output**: Should raise an `AuthenticationError` if the login fails. 3. **`list_messages(self)`** - **Output**: Returns a list of tuples where each tuple contains `(message_number, message_size)` for all messages in the inbox. 4. **`retrieve_message(self, message_number)`** - **Input**: Retrieve the complete message. - `message_number` (int): The number of the message to retrieve. - **Output**: Returns the message as a string. 5. **`message_subject(self, message_number)`** - **Input**: Retrieve the subject of a specific message. - `message_number` (int): The number of the message to retrieve the subject. - **Output**: Returns the subject line of the message as a string. 6. **`logout(self)`** - **Output**: Ends the session and closes the connection properly. Example Usage ```python # Example usage of the EmailClient class. client = EmailClient(host=\\"pop.example.com\\", port=995, use_ssl=True) client.login(username=\\"username@example.com\\", password=\\"password\\") # List all messages in the inbox. messages = client.list_messages() print(\\"Messages:\\", messages) # Retrieve and print the first message. first_message = client.retrieve_message(messages[0][0]) print(\\"First Message:n\\", first_message) # Print the subject of the first message. subject = client.message_subject(messages[0][0]) print(\\"Subject of First Message:\\", subject) client.logout() ``` Constraints and Considerations - Exception handling is crucial. Ensure to raise `poplib.error_proto` for any protocol-related errors. - Handle timeouts and connection errors gracefully. - You can assume no message threading; each message is standalone. Notes - The `poplib.error_proto` should be used to handle POP3-specific errors. - Include detailed docstrings for each method describing its functionality and expected behavior. Performance Requirements - Efficiently handle the retrieval and processing of up to 100 messages. - Ensure minimum delays by utilizing the timeout parameter effectively. Submission Submit the complete `EmailClient` class implementation with the specified methods and any relevant helper functions.","solution":"import poplib from email.parser import BytesParser, Parser from email.policy import default class AuthenticationError(Exception): pass class EmailClient: def __init__(self, host, port, use_ssl=True, timeout=10): Initialize the class with server details and establish a connection. :param host: The POP3 server address. :param port: The server port. :param use_ssl: Whether to use SSL for the connection. Default is True. :param timeout: Timeout for the connection in seconds. Default is 10. self.host = host self.port = port self.use_ssl = use_ssl self.timeout = timeout if self.use_ssl: self.client = poplib.POP3_SSL(self.host, self.port, timeout=self.timeout) else: self.client = poplib.POP3(self.host, self.port, timeout=self.timeout) def login(self, username, password): Login to the server using the provided credentials. :param username: Username for the POP3 server. :param password: Password for the POP3 server. :raises AuthenticationError: If login fails. try: self.client.user(username) self.client.pass_(password) except poplib.error_proto as e: raise AuthenticationError(f\\"Login failed: {e}\\") def list_messages(self): List all messages in the inbox with their message number and size. :return: List of tuples where each tuple is (message_number, message_size). resp, messages, octets = self.client.list() return [(int(m.split()[0]), int(m.split()[1])) for m in messages] def retrieve_message(self, message_number): Retrieve the complete message by its number. :param message_number: The number of the message to retrieve. :return: The complete message as a string. resp, lines, octets = self.client.retr(message_number) msg_content = b\'rn\'.join(lines) return msg_content.decode(\'utf-8\') def message_subject(self, message_number): Retrieve the subject line of a specific message. :param message_number: The number of the message to retrieve the subject from. :return: The subject line of the message as a string. msg_content = self.retrieve_message(message_number) message = Parser(policy=default).parsestr(msg_content) return message[\'subject\'] def logout(self): End the session and close the connection properly. self.client.quit()"},{"question":"# Seaborn Plotting Assessment Objective Your task is to analyze a dataset and create visualizations using the seaborn `Dash` mark to represent data points. You must also utilize advanced properties and modifiers to create a comprehensive plot. Dataset Use the `penguins` dataset from the seaborn library. Instructions 1. **Data Preparation:** - Load the `penguins` dataset from the seaborn library. - Handle any missing values by removing rows with null values. 2. **Basic Plot:** - Create a basic plot with `species` on the x-axis and `body_mass_g` on the y-axis. - Use the `Dash` mark to represent the data points and color the dashes by the `sex` of the penguins. 3. **Enhanced Plot:** - Enhance the plot by setting `alpha` to 0.5 and mapping `linewidth` to the `flipper_length_mm` variable. - Set the `width` property of the dashes to 0.5. 4. **Advanced Plot:** - Create an advanced plot where you dodge the dashes based on the `species`. - Pair the `Dash` mark with `Agg`, `Dodge`, and `Jitter` to show aggregate values alongside a strip plot. 5. **Numeric Data Plot:** - Create a plot with `body_mass_g` on the x-axis and `flipper_length_mm` (rounded to the nearest 10) on the y-axis. - Use the `Dash` mark and set the orientation explicitly to \'y\'. Expected Input and Output Formats - Input: None (the dataset is loaded directly within the function) - Output: Display the plots (do not return any value from the function) Constraints - Use seaborn objects and marks (`Dash`, `Dodge`, `Agg`, `Jitter`) as specified. - Follow the properties and mapping instructions accurately. Performance Requirements - Ensure the plots are clearly visualized with proper labels and legends. - Handle any large-scale data visualization considerations if necessary. Function Signature ```python def visualize_penguins(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # 1. Data Preparation penguins = load_dataset(\\"penguins\\").dropna() # 2. Basic Plot p1 = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") p1.add(so.Dash()) # Display the basic plot p1.show() # 3. Enhanced Plot p2 = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") p2.add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\") p2.add(so.Dash(width=0.5)) # Display the enhanced plot p2.show() # 4. Advanced Plot p3 = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") p3.add(so.Dash(), so.Dodge()) p3.add(so.Dash(), so.Agg(), so.Dodge()) p3.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the advanced plot p3.show() # 5. Numeric Data Plot p4 = so.Plot(penguins[\\"body_mass_g\\"], penguins[\\"flipper_length_mm\\"].round(-1)) p4.add(so.Dash(), orient=\\"y\\") # Display the numeric data plot p4.show() plt.show() # Call the function to verify the output visualize_penguins() ```","solution":"def visualize_penguins(): import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so # 1. Data Preparation penguins = sns.load_dataset(\\"penguins\\").dropna() # 2. Basic Plot p1 = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\").add(so.Dash()) p1.show() # 3. Enhanced Plot p2 = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\").add(so.Dash(alpha=0.5), linewidth=\\"flipper_length_mm\\").add(so.Dash(width=0.5)) p2.show() # 4. Advanced Plot p3 = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\").add(so.Dash(), so.Dodge()).add(so.Dash(), so.Agg()).add(so.Dots(), so.Dodge(), so.Jitter()) p3.show() # 5. Numeric Data Plot penguins[\\"flipper_length_mm_rounded\\"] = penguins[\\"flipper_length_mm\\"].round(-1) p4 = so.Plot(penguins, \\"body_mass_g\\", \\"flipper_length_mm_rounded\\").add(so.Dash(), orient=\\"y\\") p4.show() # Show all plots plt.show() # Call the function to verify the output visualize_penguins()"},{"question":"# Advanced Audio Processing with `audioop` Module **Problem Statement:** You are tasked with creating an audio processing pipeline that enhances audio quality by combining various processing steps provided by the `audioop` module. The goal is to create a function that takes an audio fragment, performs multiple transformations, and outputs the modified audio. Specifically, you need to implement the function `enhance_audio` that: 1. Converts an a-LAW encoded audio fragment to linear encoding. 2. Multiplies the audio samples by a given factor to amplify the audio. 3. Converts the amplified audio to u-LAW encoding. 4. Swaps the byte order of the resulting u-LAW encoded audio fragment. 5. Finally, converts the byteswapped audio back to linear encoding and returns it. **Function Signature:** ```python def enhance_audio(alaw_fragment: bytes, width: int, factor: float) -> bytes: pass ``` **Parameters:** - `alaw_fragment` (bytes): The input audio fragment encoded in a-LAW format. - `width` (int): The sample width in bytes for the output linear encoded fragment. Must be 1, 2, 3, or 4. - `factor` (float): The amplification factor to be applied to the linear encoded samples. **Returns:** - `bytes`: The final linear encoded audio fragment after all transformations. **Constraints:** - Assume the input `alaw_fragment` is non-empty and properly encoded in a-LAW format. - Ensure that the processing steps handle any potential overflows or errors gracefully. **Example:** ```python # Example Input alaw_fragment = b\'xD5x56x57x65x1CxD4x8D\' width = 2 factor = 1.5 # Example Output output_fragment = enhance_audio(alaw_fragment, width, factor) print(output_fragment) # Output should be a bytes object representing the final processed linear audio fragment. ``` **Hints:** - Utilize `audioop.alaw2lin` to convert a-LAW to linear encoding. - Use `audioop.mul` to amplify the audio samples. - Convert the amplified linear audio samples to u-LAW encoding using `audioop.lin2ulaw`. - Perform a byte swap on the u-LAW encoded audio using `audioop.byteswap`. - Convert the byteswapped u-LAW audio back to linear encoding using `audioop.ulaw2lin`. Implement the function `enhance_audio` following the described pipeline to enhance the audio quality as specified.","solution":"import audioop def enhance_audio(alaw_fragment: bytes, width: int, factor: float) -> bytes: # Step 1: Convert an a-LAW encoded audio fragment to linear encoding. linear_audio = audioop.alaw2lin(alaw_fragment, width) # Step 2: Multiply the audio samples by a given factor to amplify the audio. amplified_audio = audioop.mul(linear_audio, width, factor) # Step 3: Convert the amplified audio to u-LAW encoding. ulaw_audio = audioop.lin2ulaw(amplified_audio, width) # Step 4: Swap the byte order of the resulting u-LAW encoded audio fragment. byteswapped_audio = audioop.byteswap(ulaw_audio, 1) # uLAW has a width of 1 # Step 5: Convert the byteswapped audio back to linear encoding. final_output = audioop.ulaw2lin(byteswapped_audio, width) return final_output"},{"question":"# Challenge: Multi-Module Data Processing and Output Formatting **Objective**: - Demonstrate your understanding and practical application of various advanced Python modules by designing a function that reads binary data, processes it, and formats the output using templates. **Problem Statement**: You are tasked with designing a function that performs the following steps: 1. **Read Binary Data and Extract Information**: - Read a binary file containing several records of user data. Each record includes the following fields in order: - An integer `user_id` (4 bytes) - A float `balance` (8 bytes) - A fixed-length username of 10 characters (10 bytes) 2. **Process the Data**: - Add a fixed bonus to each user\'s balance. The bonus is provided as an argument to the function. 3. **Format the Output**: - Use the `string` module\'s `Template` class to create formatted strings for each user. The template should include the user\'s ID, updated balance, and username. 4. **Concurrent Data Processing**: - Use multiple threads to read, process, and format user data concurrently to improve performance. **Function Signature**: ```python def process_user_data(filepath: str, bonus: float) -> None: # Your code goes here ``` **Constraints**: - The binary file may contain a large number of user records. - Ensure thread-safe operations when accessing or modifying shared resources. - The resulting formatted strings should be printed out or logged. **Input/Output**: - **Input**: - `filepath` (str): The path to the binary file containing user data. - `bonus` (float): The fixed bonus to be added to each user\'s balance. - **Output**: - Print or log the formatted strings for each user. **Example**: Suppose the binary file contains the following records: - Binary representation of: - User 1: `user_id=101`, `balance=100.50`, `username=\'alice \'` - User 2: `user_id=102`, `balance=250.75`, `username=\'bob \'` With a `bonus` of `10.0`, the output formatted strings would be: ``` User ID: 101, Updated Balance: 110.50, Username: alice User ID: 102, Updated Balance: 260.75, Username: bob ``` **Notes**: - You can assume the binary file structure and record format are fixed as described. - Utilize the `threading` module to achieve concurrent data processing. - Use the `struct` module to unpack the binary data and the `string` module\'s `Template` class for string formatting. **Tips**: - Carefully manage threading to avoid data corruption or race conditions. - Ensure the formatted output is both human-readable and adheres to the specified template format. ```python import threading import struct from string import Template def process_user_data(filepath: str, bonus: float) -> None: # Implement the function here pass ```","solution":"import threading import struct from string import Template def process_user_data(filepath: str, bonus: float) -> None: record_size = struct.calcsize(\'if10s\') def process_record(record_bytes): user_id, balance, username = struct.unpack(\'if10s\', record_bytes) username = username.decode(\'utf-8\').strip() updated_balance = balance + bonus template = Template(\\"User ID: user_id, Updated Balance: updated_balance, Username: username\\") formatted_string = template.substitute(user_id=user_id, updated_balance=updated_balance, username=username) print(formatted_string) with open(filepath, \'rb\') as file: while True: record_bytes = file.read(record_size) if not record_bytes: break threading.Thread(target=process_record, args=(record_bytes,)).start()"},{"question":"# Secure File Verification System In this coding assessment, you will create a secure file verification system that ensures the integrity and authenticity of files using cryptographic techniques provided by Python 3.10\'s `hashlib`, `hmac`, and `secrets` modules. # Problem Statement You are required to write a Python function `verify_file_integrity(file_path, secret_key)` that verifies the integrity and authenticity of a given file. The function should perform the following tasks: 1. **Calculate a Secure Hash:** - Read the contents of the file specified by `file_path`. - Compute the SHA256 hash of the file content using the `hashlib` module. 2. **Generate an HMAC:** - Using the SHA256 hash of the file content, compute an HMAC (Hash-based Message Authentication Code) using the provided `secret_key`. Leverage the `hmac` module for this purpose. 3. **Generate a Secure Token:** - Use the `secrets` module to generate a secure random token. This token will be used as a nonce during the verification process. 4. **Output the Results:** - Return a dictionary containing the computed SHA256 hash, the HMAC, and the secure random token. # Function Signature ```python def verify_file_integrity(file_path: str, secret_key: bytes) -> dict: pass ``` # Input - `file_path`: A string representing the path to the file that needs to be verified. - `secret_key`: A bytes object representing the secret key used for HMAC generation. # Output - A dictionary with the following keys: - `\\"sha256_hash\\"`: The SHA256 hash of the file content as a hexadecimal string. - `\\"hmac\\"`: The HMAC of the file content hash as a hexadecimal string. - `\\"secure_token\\"`: A securely generated random token as a hexadecimal string. # Example ```python import os # Assuming the file \'data.txt\' exists in the current directory with some content file_path = \'data.txt\' secret_key = os.urandom(16) # Generating a random secret key for HMAC result = verify_file_integrity(file_path, secret_key) print(result) # Example output: # { # \'sha256_hash\': \'5d41402abc4b2a76b9719d911017c592\', # \'hmac\': \'b6a9c8c230722b7c606b1524f87da2b9ec7db3034c57647a850724a7b6f21ca6\', # \'secure_token\': \'e3b0c4c2b0e0e7d9f8a12454d378589a8\' # } ``` # Constraints - Ensure the file `file_path` exists and is readable. - The `secret_key` must be a bytes object and should be of a secure length (e.g. at least 16 bytes). # Notes - Utilize the `hashlib` module for computing SHA256 hash. - Utilize the `hmac` module for generating HMAC. - Utilize the `secrets` module for generating a secure random token.","solution":"import hashlib import hmac import secrets def verify_file_integrity(file_path: str, secret_key: bytes) -> dict: Verifies the integrity and authenticity of the specified file using SHA256 hash and HMAC. Returns a dictionary with the SHA256 hash, HMAC, and a secure random token. Parameters: - file_path (str): the path to the file that needs to be verified. - secret_key (bytes): the secret key used for HMAC generation. Returns: - dict: a dictionary containing the SHA256 hash, HMAC, and a secure random token. # Read the contents of the file with open(file_path, \'rb\') as file: file_content = file.read() # Compute the SHA256 hash of the file content sha256_hash = hashlib.sha256(file_content).hexdigest() # Generate the HMAC using the SHA256 hash of the file\'s content and the provided secret key hmac_result = hmac.new(secret_key, sha256_hash.encode(), hashlib.sha256).hexdigest() # Generate a secure random token secure_token = secrets.token_hex(32) # Return the result as a dictionary return { \\"sha256_hash\\": sha256_hash, \\"hmac\\": hmac_result, \\"secure_token\\": secure_token }"},{"question":"You are tasked with writing a function that utilizes the `cmath` module to perform a series of computations on complex numbers. The function should take in a list of operations and corresponding complex numbers, perform each operation, and return the results in both Cartesian and polar forms. The operations to be supported include `exp`, `log`, `sqrt`, `sin`, `cos`, `tan`, `polar`, and `rect`. Function Signature ```python def complex_computations(operations: list[tuple[str, complex]]) -> list[tuple[complex, tuple[float, float]]]: pass ``` Input - A list of operations where each operation is represented as a tuple: - The first element is a string representing the operation (`\\"exp\\"`, `\\"log\\"`, `\\"sqrt\\"`, `\\"sin\\"`, `\\"cos\\"`, `\\"tan\\"`, `\\"polar\\"`, `\\"rect\\"`). - The second element is a complex number on which to perform the operation. - For `rect` operation, the input would be a tuple where the first element is modulus (r) and the second element is phase angle (phi). Output - A list of results where each result is a tuple: - The first element is the complex result in Cartesian coordinates. - The second element is a tuple representing the same result in polar coordinates (modulus, phase angle). Example ```python operations = [ (\\"exp\\", complex(1, 1)), (\\"log\\", complex(1, 1)), (\\"sqrt\\", complex(4, 4)), (\\"sin\\", complex(0, 1)), (\\"cos\\", complex(0, 1)), (\\"tan\\", complex(0, 1)), (\\"polar\\", complex(-1, 1)), (\\"rect\\", (1, cmath.pi/4)) ] expected_result = [ (2.718281828459045 + 2.718281828459045j, (3.84732210186306, 0.7853981633974483)), (0.34657359027997264 + 0.7853981633974483j, (0.8716353612864292, 1.1502619915109314)), (2.0 + 1.0j, (2.23606797749979, 0.4636476090008061)), (1.1752011936438014j, (1.1752011936438014, 1.5707963267948966)), (0.5403023058681398 - 0.8414709848078965j, (1.0, -1.0)), (0.0 + 1.5574077246549023j, (1.5574077246549023, 1.5707963267948966)), ((1.526956486450491, 2.356194490192345)), ((cmath.sqrt(2)/2 + cmath.sqrt(2)/2 * 1j, (1.0, 0.7853981633974483))), ] assert complex_computations(operations) == expected_result ``` Constraints - The operations will always be one of the listed types. - The complex numbers provided will be valid complex numbers. - For `rect` operation, the modulus will be a non-negative float, and the phase will be a float. Your task is to implement the function `complex_computations` to pass the given example and handle any similar input appropriately.","solution":"import cmath def complex_computations(operations): results = [] for operation, number in operations: if operation == \\"exp\\": result = cmath.exp(number) elif operation == \\"log\\": result = cmath.log(number) elif operation == \\"sqrt\\": result = cmath.sqrt(number) elif operation == \\"sin\\": result = cmath.sin(number) elif operation == \\"cos\\": result = cmath.cos(number) elif operation == \\"tan\\": result = cmath.tan(number) elif operation == \\"polar\\": result = number elif operation == \\"rect\\": result = cmath.rect(*number) else: raise ValueError(\\"Unknown operation\\") polar_result = cmath.polar(result) if operation == \\"polar\\": results.append((result, (polar_result[0], polar_result[1]))) else: results.append((result, polar_result)) return results"},{"question":"# Unicode Data Analyzer You are tasked with developing a `UnicodeDataAnalyzer` class to analyze various properties of Unicode characters using the `unicodedata` module. This class should provide a robust set of methods to retrieve and display character properties and perform normalization checks. Class Definition ```python class UnicodeDataAnalyzer: def __init__(self, char): Initialize the analyzer with a Unicode character. :param char: A single Unicode character. self.char = char def get_name(self, default=\'Undefined\'): Return the Unicode name for the character, or \'Undefined\' if no name exists. :return: String representing the name of the character. pass def get_category(self): Return the Unicode category for the character. :return: String representing the category of the character. pass def get_decimal_value(self): Return the decimal value for the character. :return: Integer representing the decimal value or None if not defined. pass def get_combined_normalized_form(self, form): Return the normalized form of the character combination. :param form: The form of normalization (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'). :return: String representing the normalized form. pass @staticmethod def is_string_normalized(form, unistr): Check if the given string is normalized. :param form: The form of normalization (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'). :param unistr: A Unicode string to check. :return: Boolean indicating if the string is normalized. pass ``` Task Requirements 1. Implement the `get_name` method: - Use `unicodedata.name(char, default)` to retrieve the character\'s name. 2. Implement the `get_category` method: - Use `unicodedata.category(char)` to retrieve the character\'s category. 3. Implement the `get_decimal_value` method: - Use `unicodedata.decimal(char, default=None)` to retrieve the character\'s decimal value. - Return `None` if the character does not have a decimal value. 4. Implement the `get_combined_normalized_form` method: - Use `unicodedata.normalize(form, unistr)` to return the normalized form. - Ensure that it works correctly for any valid Unicode normalization form. 5. Implement the `is_string_normalized` method: - Use `unicodedata.is_normalized(form, unistr)` to check if a string is normalized correctly according to the specified form. Example Usage ```python # Create an instance of UnicodeDataAnalyzer analyzer = UnicodeDataAnalyzer(\'A\') # Get the name of the character print(analyzer.get_name()) # Output: \'LATIN CAPITAL LETTER A\' # Get the category of the character print(analyzer.get_category()) # Output: \'Lu\' # Get the decimal value of the character (which does not have one) print(analyzer.get_decimal_value()) # Output: None # Normalize a combined sequence in NFD combined_char = \'Å\' # LATIN CAPITAL LETTER A WITH RING ABOVE print(analyzer.get_combined_normalized_form(\'NFD\')) # Output: \'Å\' # Check if a string is normalized in NFC form print(UnicodeDataAnalyzer.is_string_normalized(\'NFC\', combined_char)) # Output: True ``` Implement the `UnicodeDataAnalyzer` class, ensuring all methods perform correctly and handle edge cases properly.","solution":"import unicodedata class UnicodeDataAnalyzer: def __init__(self, char): Initialize the analyzer with a Unicode character. :param char: A single Unicode character. self.char = char def get_name(self, default=\'Undefined\'): Return the Unicode name for the character, or \'Undefined\' if no name exists. :return: String representing the name of the character. return unicodedata.name(self.char, default) def get_category(self): Return the Unicode category for the character. :return: String representing the category of the character. return unicodedata.category(self.char) def get_decimal_value(self): Return the decimal value for the character. :return: Integer representing the decimal value or None if not defined. try: return unicodedata.decimal(self.char) except ValueError: return None def get_combined_normalized_form(self, form): Return the normalized form of the character combination. :param form: The form of normalization (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'). :return: String representing the normalized form. return unicodedata.normalize(form, self.char) @staticmethod def is_string_normalized(form, unistr): Check if the given string is normalized. :param form: The form of normalization (\'NFC\', \'NFKC\', \'NFD\', \'NFKD\'). :param unistr: A Unicode string to check. :return: Boolean indicating if the string is normalized. return unicodedata.is_normalized(form, unistr)"},{"question":"You are provided with a dataset containing information about various car models. Your task is to use Seaborn to analyze the residuals of a linear regression model fitted to this data. Specifically, you will use the `seaborn.residplot` function to examine whether certain transformations and enhancements help in stabilizing the residuals. # Input: You have access to the `mpg` dataset, already loaded as follows: ```python import seaborn as sns sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") ``` # Requirements: 1. **Basic Residual Plot**: - Generate a residual plot with `weight` as the predictor variable (x-axis) and `displacement` as the response variable (y-axis). 2. **Higher-order Trends**: - Create a residual plot with `horsepower` as the predictor variable and `mpg` as the response variable. - Examine the effect of fitting a second-order polynomial using the `order=2` parameter. 3. **Adding LOWESS Curve**: - Create a residual plot for the same `horsepower` vs `mpg` relationship but add a LOWESS smooth curve to it. - Customize the LOWESS curve line to be of red color. # Expected Output: 1. Three separate residual plots displayed as output. # Constraints: - Use only the provided dataset (`mpg`). - The plots should be clearly labeled and should include titles for better understanding. # Example Code for Reference: ```python import seaborn as sns import matplotlib.pyplot as plt sns.set_theme() mpg = sns.load_dataset(\\"mpg\\") # 1. Basic Residual Plot sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot of Displacement vs Weight\\") plt.show() # 2. Higher-order Trends sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot of MPG vs Horsepower\\") plt.show() sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Second-Order Residual Plot of MPG vs Horsepower\\") plt.show() # 3. Adding LOWESS Curve sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\\"LOWESS Residual Plot of MPG vs Horsepower\\") plt.show() ``` # Notes: - Ensure that each plot is correctly generated and displayed. - Include comments explaining each part of your code. - Submit a single Python script or Jupyter notebook containing your solution.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Set the theme for the seaborn plots sns.set_theme() # Load the mpg dataset mpg = sns.load_dataset(\\"mpg\\") # 1. Basic Residual Plot plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"weight\\", y=\\"displacement\\") plt.title(\\"Residual Plot of Displacement vs Weight\\") plt.xlabel(\\"Weight\\") plt.ylabel(\\"Residuals\\") plt.show() # 2. Higher-order Trends # Residual plot for linear relationship plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\") plt.title(\\"Residual Plot of MPG vs Horsepower\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # Residual plot for second-order polynomial relationship plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", order=2) plt.title(\\"Second-Order Residual Plot of MPG vs Horsepower\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show() # 3. Adding LOWESS Curve plt.figure(figsize=(10, 6)) sns.residplot(data=mpg, x=\\"horsepower\\", y=\\"mpg\\", lowess=True, line_kws={\\"color\\": \\"r\\"}) plt.title(\\"LOWESS Residual Plot of MPG vs Horsepower\\") plt.xlabel(\\"Horsepower\\") plt.ylabel(\\"Residuals\\") plt.show()"},{"question":"You are a data scientist tasked with predicting house prices in California using the California Housing dataset provided by scikit-learn. Your task is to write Python code to fetch, preprocess, and build a predictive model using this dataset. **Instructions**: 1. Fetch the California Housing dataset using `fetch_california_housing`. 2. Perform a train-test split on the data (80% train, 20% test). 3. Apply any preprocessing steps you find necessary. 4. Train a regression model to predict house prices. You may choose any regression model available in scikit-learn. 5. Evaluate the model’s performance on the test set. Use R² as the performance metric. **Expected Input and Output Formats**: - The function should not take any input. - The function should output the R² score on the test set. **Constraints**: - You must use the `fetch_california_housing` function from sklearn.datasets. - You should use at least one preprocessing technique (e.g., normalization, missing value imputation). **Performance Requirements**: - The function should run in a reasonable time frame, ideally under a minute. ```python def california_housing_predictor(): # import necessary libraries from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score # 1. Fetch the California Housing dataset data = fetch_california_housing() X = data[\'data\'] y = data[\'target\'] # 2. Perform a train-test split (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Apply preprocessing (Standard Scaler) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # 4. Train a regression model (Linear Regression) model = LinearRegression() model.fit(X_train_scaled, y_train) # 5. Evaluate the model\'s performance (R² score) y_pred = model.predict(X_test_scaled) r2 = r2_score(y_test, y_pred) return r2 ```","solution":"def california_housing_predictor(): # Import necessary libraries from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score # 1. Fetch the California Housing dataset data = fetch_california_housing() X = data[\'data\'] y = data[\'target\'] # 2. Perform a train-test split (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Apply preprocessing (Standard Scaler) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # 4. Train a regression model (Linear Regression) model = LinearRegression() model.fit(X_train_scaled, y_train) # 5. Evaluate the model\'s performance (R² score) y_pred = model.predict(X_test_scaled) r2 = r2_score(y_test, y_pred) return r2"},{"question":"**Question: Anomaly Detection using scikit-learn** You are provided with a dataset containing several features. Your task is to implement a function using scikit-learn\'s anomaly detection methods to classify observations as either normal or anomalies. Specifically, you are to use the `IsolationForest` and `LocalOutlierFactor` methods. **Function Signature**: ```python def detect_anomalies(X_train, X_test, contamination=0.1, method=\'isolation_forest\'): Detects anomalies in a dataset using specified method. Parameters: - X_train (numpy.ndarray): 2D array with shape (n_train_samples, n_features). - X_test (numpy.ndarray): 2D array with shape (n_test_samples, n_features). - contamination (float, optional): Proportion of outliers in the dataset. Default is 0.1. - method (str, optional): The method to use for anomaly detection. Allowed values are \'isolation_forest\' and \'local_outlier_factor\'. Default is \'isolation_forest\'. Returns: - anomaly_labels (numpy.ndarray): Array with shape (n_test_samples,) where 1 indicates inlier and -1 indicates outlier. pass ``` **Explanation**: 1. **Parameters**: - `X_train`: Training dataset without labels, used for fitting the model. - `X_test`: Test dataset without labels, used for prediction and anomaly scoring. - `contamination`: Proportion of observations to consider as outliers. - `method`: Method for anomaly detection (\'isolation_forest\' or \'local_outlier_factor\'). 2. **Implementation Details**: - For `IsolationForest`: - Instantiate the estimator with `contamination` parameter. - Fit the model on `X_train`. - Predict anomalies on `X_test`. - For `LocalOutlierFactor` with `novelty=True`: - Instantiate the estimator with `novelty=True` and `contamination`. - Fit the model on `X_train`. - Predict anomalies on `X_test`. 3. **Output**: - The function returns an array of anomaly labels for `X_test` where 1 indicates an inlier and -1 indicates an outlier. **Example**: ```python import numpy as np # Example Usage X_train = np.array([[10, 15], [9, 14], [10, 20], [9, 18], [11, 17]]) X_test = np.array([[10, 16], [9, 13], [0, 0], [12, 19]]) # Using Isolation Forest anomaly_labels_iforest = detect_anomalies(X_train, X_test, contamination=0.1, method=\'isolation_forest\') print(anomaly_labels_iforest) # Using Local Outlier Factor anomaly_labels_lof = detect_anomalies(X_train, X_test, contamination=0.1, method=\'local_outlier_factor\') print(anomaly_labels_lof) ``` Ensure to handle edge cases such as empty arrays or invalid method inputs appropriately.","solution":"from sklearn.ensemble import IsolationForest from sklearn.neighbors import LocalOutlierFactor import numpy as np def detect_anomalies(X_train, X_test, contamination=0.1, method=\'isolation_forest\'): Detects anomalies in a dataset using specified method. Parameters: - X_train (numpy.ndarray): 2D array with shape (n_train_samples, n_features). - X_test (numpy.ndarray): 2D array with shape (n_test_samples, n_features). - contamination (float, optional): Proportion of outliers in the dataset. Default is 0.1. - method (str, optional): The method to use for anomaly detection. Allowed values are \'isolation_forest\' and \'local_outlier_factor\'. Default is \'isolation_forest\'. Returns: - anomaly_labels (numpy.ndarray): Array with shape (n_test_samples,) where 1 indicates inlier and -1 indicates outlier. if method not in [\'isolation_forest\', \'local_outlier_factor\']: raise ValueError(\\"Invalid method. Allowed values are \'isolation_forest\' and \'local_outlier_factor\'.\\") if method == \'isolation_forest\': clf = IsolationForest(contamination=contamination, random_state=42) clf.fit(X_train) anomaly_labels = clf.predict(X_test) elif method == \'local_outlier_factor\': clf = LocalOutlierFactor(novelty=True, contamination=contamination) clf.fit(X_train) anomaly_labels = clf.predict(X_test) return anomaly_labels"},{"question":"**Custom Python Object in C: Attribute Access and Iteration** # Objective: In this task, you will implement a custom Python object using the `PyTypeObject` structure. The object should provide specific functionalities to demonstrate your comprehension of defining extension types, attribute management, object presentation, and iteration in Python C-API. # Requirements: 1. **Custom Object Structure**: Implement a struct `CustomObject` with the following fields: - `PyObject_HEAD` for standard object header. - `PyObject *attr` to store a Python attribute. - `int counter` to maintain iteration state. 2. **Type Initialization**: Define and initialize a `PyTypeObject` for `CustomObject` with the following attributes: - `tp_name`: \\"custom_module.CustomObject\\". - `tp_basicsize` and `tp_itemsize` for memory allocation. - `tp_dealloc` for deallocating memory. - `tp_repr` and `tp_str` for object representation. - `tp_getattro` and `tp_setattro` for attribute management. - `tp_iter` and `tp_iternext` for iteration. - `tp_methods` if you need to define additional methods. - Any additional fields you might need. 3. **Methods**: - Implement a deallocate method `custom_object_dealloc` to properly deallocate memory. - Implement `custom_object_repr` and `custom_object_str` to return a descriptive string of the object. - Implement custom attribute access methods (`custom_object_getattr` and `custom_object_setattr`) to handle getting and setting the `attr` field. - Implement iterator methods (`custom_object_iter` and `custom_object_iternext`). The iteration should return numbers incremented by 1 starting from `counter` until 10. # Input and Output: - **No direct input constraints**: You need to develop a custom C extension module named `custom_module` that compiles and creates `CustomObject`. - **Functional Behavior**: - Ability to create a new `CustomObject` through Python. - Correct memory management and deallocation. - Correct string representations through `repr()` and `str()`. - Read and write access to the `attr` attribute. - Functioning iterator that starts from the object\'s `counter` attribute and iterates till 10. # Example Python Use: ```python import custom_module # Creating the custom object obj = custom_module.CustomObject() print(repr(obj)) # Should print a descriptive string representation print(str(obj)) # Another form of string representation # Setting and getting the custom attribute obj.attr = \\"example\\" print(obj.attr) # Should output: example # Using the iterator for val in obj: print(val) # Should print numbers from 0 to 9, one per line ``` # Constraints: - Ensure memory management is handled properly to avoid memory leaks. - Use Python C-API correctly to manage reference counts. - Handle exceptions and edge cases where necessary. # Performance: - Ensure the custom iteration is efficient and does not introduce significant overhead. This question will evaluate your ability to create and manage custom Python objects using the Python C-API, focusing on essential areas like attribute management, object representation, and iteration.","solution":"# This is a mock Python implementation of what the C extension should behave like. # Actual implementation should be in a C file (.c) and compiled into a module. # Below is a Python prototype for the expected behavior. class CustomObject: def __init__(self): self._attr = None self.counter = 0 @property def attr(self): return self._attr @attr.setter def attr(self, value): self._attr = value def __repr__(self): return f\\"<CustomObject attr={self._attr}>\\" def __str__(self): return f\\"CustomObject with attribute: {self._attr}\\" def __iter__(self): return self def __next__(self): if self.counter >= 10: raise StopIteration self.counter += 1 return self.counter - 1"},{"question":"# PyTorch Backend Customization and Performance Optimization **Objective:** Write a Python script that optimizes matrix multiplication operations when running on a CUDA-enabled GPU using TensorFloat-32 (TF32) precision. **Background:** PyTorch provides various backends for different hardware and computation optimizations. One of the backends, `torch.backends.cuda`, allows users to control whether TF32 tensor cores may be used in matrix multiplications on Ampere or newer GPUs. Utilizing TF32 can improve performance for certain operations. **Task:** Implement a function `optimize_matrix_multiplication()` that performs the following tasks: 1. **Check for CUDA Availability:** - Ensure that CUDA is available on the system. - If CUDA is not available, print an appropriate message and return. 2. **Configure TF32 Settings:** - Enable the use of TF32 for matrix multiplications by setting `torch.backends.cuda.matmul.allow_tf32` to `True`. 3. **Generate Random Matrices:** - Create two random matrices `A` and `B` of size 1024x1024 with values sampled from a uniform distribution between 0 and 1. 4. **Matrix Multiplication:** - Perform matrix multiplication on these matrices and measure the execution time. - Ensure the multiplication is done on the GPU. 5. **Return Execution Time:** - The function should return the execution time of the matrix multiplication. **Function Signature:** ```python def optimize_matrix_multiplication() -> float: pass ``` **Constraints:** - You must use PyTorch and its backends. - Ensure TF32 cores are properly enabled if running on compatible hardware. - Perform matrix multiplication on the GPU. **Example Usage:** ```python execution_time = optimize_matrix_multiplication() print(f\\"Matrix multiplication executed in {execution_time} seconds.\\") ``` By completing this task, students will demonstrate their capability to work with PyTorch backends, optimize computations for specific hardware, and handle GPU-related operations.","solution":"import torch import time def optimize_matrix_multiplication() -> float: Optimizes matrix multiplication operations when running on a CUDA-enabled GPU using TensorFloat-32 (TF32) precision. Returns: float: Execution time of the matrix multiplication in seconds. # Check if CUDA is available if not torch.cuda.is_available(): print(\\"CUDA is not available on this system.\\") return 0.0 # Enable the use of TF32 for matrix multiplications torch.backends.cuda.matmul.allow_tf32 = True # Generate random matrices A and B of size 1024x1024 with values sampled from a uniform distribution A = torch.rand((1024, 1024), device=\'cuda\') B = torch.rand((1024, 1024), device=\'cuda\') # Perform matrix multiplication and measure the execution time start_time = time.time() C = torch.matmul(A, B) torch.cuda.synchronize() # Wait for all kernels in all streams on a CUDA device to complete end_time = time.time() # Calculate the execution time execution_time = end_time - start_time return execution_time"},{"question":"# Assessment Question: Concurrent Subprocess Monitoring In this task, you will implement a Python function that concurrently runs multiple shell commands using `asyncio`\'s subprocess APIs. Your function will execute each command, capture its output, print the output, and handle any errors. Function Signature ```python import asyncio async def execute_commands(commands: list) -> dict: pass ``` Input - `commands`: A list of strings, where each string is a shell command to be executed. Output - Returns a dictionary where each key is a command string and the value is a tuple consisting of two elements: - The standard output of the command (a string). - The standard error of the command (a string). Constraints - Each command should be executed concurrently. - Make sure the standard output and standard error are captured correctly for each command and avoid deadlocks. - Ensure proper error handling if a command fails to execute. Examples Example 1: ```python commands = [\'echo Hello\', \'ls invalid_directory\', \'sleep 1; echo Done\'] results = asyncio.run(execute_commands(commands)) print(results) ``` Expected Output: ```python { \'echo Hello\': (\'Hellon\', \'\'), \'ls invalid_directory\': (\'\', \'ls: cannot access \'invalid_directory\': No such file or directoryn\'), \'sleep 1; echo Done\': (\'Donen\', \'\') } ``` Example 2: ```python commands = [\'python -c \\"print(\'Hello from Python\')\\"\', \'echo ((2 + 2))\'] results = asyncio.run(execute_commands(commands)) print(results) ``` Expected Output: ```python { \'python -c \\"print(\'Hello from Python\')\\"\': (\'Hello from Pythonn\', \'\'), \'echo ((2 + 2))\': (\'4n\', \'\') } ``` The expected solution should focus on utilizing `asyncio.create_subprocess_shell`, handling `stdout` and `stderr` correctly, and managing concurrent execution of the subprocesses. Notes - Remember to use `shlex.quote()` to properly escape any special characters if you modify the commands. - Pay attention to handle any exceptions that might occur during subprocess execution. Developing this function will assess your capability to manage concurrent operations, handle subprocess communication, and error handling in an asynchronous environment.","solution":"import asyncio async def execute_command(command): process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() return stdout.decode(), stderr.decode() async def execute_commands(commands: list) -> dict: tasks = [execute_command(command) for command in commands] results = await asyncio.gather(*tasks) return {command: result for command, result in zip(commands, results)}"},{"question":"Question: Implementing and Managing Asynchronous Computations with PyTorch Futures # Background You are working on a distributed machine learning framework using PyTorch. The `torch.futures` module provides utilities to handle asynchronous computations, primarily through the `Future` type and its utility functions such as `collect_all` and `wait_all`. # Objective Your task is to implement a function that performs multiple asynchronous computations using `torch.futures.Future`. The function should calculate the square of integers in an asynchronous manner and then collect all results. # Function Signature ```python def async_square_and_collect(inputs: List[int]) -> List[int]: pass ``` # Input - `inputs`: A list of integers `[n1, n2, n3, ...]` where `ni` is an integer. # Output - Returns a list of integers where each integer is the square of the corresponding input integer. # Constraints - Utilize PyTorch\'s `Future` and its associated utility functions for handling asynchronous computations. - Ensure that all asynchronous operations are correctly managed and results are collected only after all computations have completed. # Example ```python inputs = [1, 2, 3, 4] output = async_square_and_collect(inputs) print(output) # Expected: [1, 4, 9, 16] ``` # Implementation Notes - Use `torch.futures.Future` to encapsulate the computation of squaring an integer. - Consider using `torch.futures.wait_all` to synchronize and collect all asynchronous results. # Grading Criteria - Correct usage of `torch.futures` concepts. - Proper management of asynchronous computations. - Code clarity and adherence to the function signature. # Hints - You might find `torch.jit.fork` useful to create a `Future` for each computation. - Look into `torch.jit.wait` to retrieve the result from a `Future`. Good luck, and happy coding!","solution":"import torch from typing import List def async_square_and_collect(inputs: List[int]) -> List[int]: Calculates the square of each integer in inputs asynchronously and collects the results. Args: inputs (List[int]): A list of integers to be squared. Returns: List[int]: A list of squared integers corresponding to the input list. # Function to square a number (to be used within the Future) def square(n): return n * n # Create a list of futures for each input futures = [torch.jit.fork(square, n) for n in inputs] # Wait for all futures to complete and collect results results = [torch.jit.wait(f) for f in futures] return results"},{"question":"# Question: File System and Process Management You are tasked with creating a Python function that performs a series of operations involving file system and process management using the `os` module. The function should: 1. Create a directory named `example_dir` if it does not exist. 2. Inside `example_dir`, create a file named `example_file.txt` and write the text \\"Hello, World!\\" to it. 3. Change permissions of `example_file.txt` to be read-only. 4. Create a symbolic link to `example_file.txt` named `example_symlink`. 5. Fork a child process that performs the following * Reads the content of `example_file.txt` and prints it to the standard output. 6. The parent process should wait for the child process to complete and print the exit status of the child. # Function Signature ```python import os import sys def manage_file_system_and_process(): # Your code here ``` # Constraints and Notes - Handle any relevant exceptions that may arise from OS-related errors. - Use appropriate `os` module functions to achieve the tasks. - Ensure that your code is portable, i.e., it should not include platform-specific behavior that does not work universally between Unix and Windows systems. # Input - This function does not take any input parameters. # Output - This function should not return any value. The relevant outputs should be printed to the standard output. # Example Execution ```python manage_file_system_and_process() ``` Expected Output - The function should print the content of `example_file.txt` from the child process. - The function should print the exit status of the child process. Additional Information - Consider edge cases such as permissions issues or the directory already existing. - Make sure symbolic links work correctly even if the operating system handles them differently.","solution":"import os import sys def manage_file_system_and_process(): try: # 1. Create a directory named `example_dir` if it does not exist dir_name = \'example_dir\' if not os.path.exists(dir_name): os.mkdir(dir_name) # 2. Create a file named `example_file.txt` and write \\"Hello, World!\\" to it file_name = os.path.join(dir_name, \'example_file.txt\') with open(file_name, \'w\') as f: f.write(\'Hello, World!\') # 3. Change permissions of `example_file.txt` to be read-only os.chmod(file_name, 0o444) # 0o444 is octal format for read-only # 4. Create a symbolic link to `example_file.txt` named `example_symlink` symlink_name = os.path.join(dir_name, \'example_symlink\') if not os.path.exists(symlink_name): os.symlink(file_name, symlink_name) # 5. Fork a child process to read the file and print its contents pid = os.fork() if pid == 0: # Child process try: with open(file_name, \'r\') as f: content = f.read() print(content) except Exception as e: print(f\\"Child process: Error reading file: {e}\\") os._exit(0) else: # Parent process os.waitpid(pid, 0) print(\'Child process completed.\') except Exception as e: print(f\\"Error: {e}\\")"},{"question":"# Custom PyTorch Function Implementation Context In PyTorch, custom autograd `Functions` allow you to define both the forward and backward computations manually. This can be useful for implementing complex operations that aren\'t readily available in PyTorch, or for optimizing specific parts of your model. In this task, you will implement a custom function that performs the element-wise multiplication of two tensors followed by a sum over a specified axis. Requirements Your task is to implement a custom autograd `Function` named `ElementWiseMultiplyAndSum` with the following specifications: 1. **Forward Pass**: The forward method should take two tensors and an optional integer axis as inputs. It should: - Perform element-wise multiplication. - Sum the resulting tensor along the specified axis. If no axis is provided, sum over all elements. 2. **Backward Pass**: The backward method should receive the gradient of the output and compute the gradients of the input tensors accordingly. Input and Output Formats - The forward method should accept three parameters: * `input1` (torch.Tensor): The first input tensor. * `input2` (torch.Tensor): The second input tensor. * `axis` (int, optional): The axis along which to sum. Defaults to None. - The forward method should return: * `output` (torch.Tensor): The result of the element-wise multiplication followed by a sum along the specified axis. - The backward method should accept: * `grad_output` (torch.Tensor): The gradient of the output. - The backward method should return: * `(grad_input1, grad_input2)` (Tuple[torch.Tensor, torch.Tensor]): The gradients of `input1` and `input2`. Constraints - Do not use any high-level PyTorch functions that directly perform the forward or backward operations requested. Implement these operations manually. - Ensure that the function handles tensors of various shapes. - The backward method should be implemented efficiently, taking into account the sum operation over the specified axis. Example Usage ```python import torch class ElementWiseMultiplyAndSum(torch.autograd.Function): @staticmethod def forward(ctx, input1, input2, axis=None): # Save inputs for backward ctx.save_for_backward(input1, input2) ctx.axis = axis # Element-wise multiplication result = input1 * input2 # Summing if axis is not None: output = result.sum(axis) else: output = result.sum() return output @staticmethod def backward(ctx, grad_output): input1, input2 = ctx.saved_tensors axis = ctx.axis # Gradient computation for input1 and input2 if axis is not None: grad_input1 = (grad_output.unsqueeze(axis) * input2).expand_as(input1) grad_input2 = (grad_output.unsqueeze(axis) * input1).expand_as(input2) else: grad_input1 = grad_output * input2 grad_input2 = grad_output * input1 return grad_input1, grad_input2, None # Example usage input1 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) input2 = torch.tensor([4.0, 5.0, 6.0], requires_grad=True) axis = 0 result = ElementWiseMultiplyAndSum.apply(input1, input2, axis) print(\\"Result: \\", result) result.backward() print(\\"Gradients: \\", input1.grad, input2.grad) ``` **Note**: Ensure your implementation handles different tensor shapes and validates input accordingly. Test your custom function with various inputs to verify its correctness.","solution":"import torch class ElementWiseMultiplyAndSum(torch.autograd.Function): @staticmethod def forward(ctx, input1, input2, axis=None): # Save inputs and axis for backward computation ctx.save_for_backward(input1, input2) ctx.axis = axis # Element-wise multiplication result = input1 * input2 # Summing the result along the specified axis if axis is not None: output = result.sum(dim=axis) else: output = result.sum() return output @staticmethod def backward(ctx, grad_output): input1, input2 = ctx.saved_tensors axis = ctx.axis # Compute gradients for input1 and input2 if axis is not None: grad_input1 = (grad_output.unsqueeze(axis) * input2).expand_as(input1) grad_input2 = (grad_output.unsqueeze(axis) * input1).expand_as(input2) else: grad_input1 = grad_output * input2 grad_input2 = grad_output * input1 return grad_input1, grad_input2, None # Example usage input1 = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) input2 = torch.tensor([4.0, 5.0, 6.0], requires_grad=True) axis = 0 result = ElementWiseMultiplyAndSum.apply(input1, input2, axis) print(\\"Result: \\", result) result.backward() print(\\"Gradients: \\", input1.grad, input2.grad)"},{"question":"You are tasked with developing a machine learning pipeline using the scikit-learn library. Specifically, you will work with the `fetch_california_housing` dataset, perform data preprocessing, and implement a linear regression model to predict housing prices based on available features. # Instructions: 1. **Fetch the Dataset:** Use the `fetch_california_housing` function from `sklearn.datasets` to load the dataset. 2. **Understand the Dataset:** - The dataset consists of features and a target variable. The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars. 3. **Preprocess the Data:** - Check and handle any missing values if they exist. - Split the dataset into training and test sets (80% training, 20% testing) using the `train_test_split` function from `sklearn.model_selection`. - Standardize the feature values using `StandardScaler` from `sklearn.preprocessing`. 4. **Model Implementation:** - Implement a Linear Regression model using `LinearRegression` from `sklearn.linear_model`. - Fit the model on the training data. 5. **Evaluation:** - Predict the target values for the test set. - Evaluate the model using the Mean Squared Error (MSE) metric from `sklearn.metrics`. # Input - There are no direct inputs to the function you will write, as all necessary data will be fetched from within the function. # Output - The function should print the Mean Squared Error (MSE) of the predictions on the test data. # Function Signature ```python def california_housing_regression(): # Step 1: Fetch the dataset from sklearn.datasets import fetch_california_housing data = fetch_california_housing() # Step 2: Preprocess the data from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error X, y = data.data, data.target # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Feature scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 3: Implement and train the model model = LinearRegression() model.fit(X_train, y_train) # Step 4: Predict and evaluate the model y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Print the Mean Squared Error print(\\"Mean Squared Error on test set:\\", mse) ``` # Constraints: - You must use scikit-learn for all machine learning-related tasks. - Ensure proper handling of data splits and preprocessing steps. # Note: - Make sure to handle the standardization correctly by fitting the scaler only on the training data and then transforming both the training and test data. - You are free to explore and analyze the dataset to understand it better, but the primary focus should be on implementing the machine learning pipeline as described.","solution":"def california_housing_regression(): This function fetches the California housing dataset, preprocesses the data, trains a Linear Regression model, and evaluates it using Mean Squared Error. # Step 1: Fetch the dataset from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error data = fetch_california_housing() X, y = data.data, data.target # Step 2: Preprocess the data # Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Feature scaling scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 3: Implement and train the model model = LinearRegression() model.fit(X_train, y_train) # Step 4: Predict and evaluate the model y_pred = model.predict(X_test) mse = mean_squared_error(y_test, y_pred) # Print the Mean Squared Error print(\\"Mean Squared Error on test set:\\", mse)"},{"question":"**Objective:** Evaluate students\' understanding of various cross-validation techniques in scikit-learn. **Question:** You are given the Iris dataset. Your task is to perform the following steps using scikit-learn: 1. Load the Iris dataset. 2. Apply standard scaling to the feature data. 3. Implement and compare the performance of a Support Vector Machine (SVM) classifier using different cross-validation techniques: - K-Fold Cross-Validation - Stratified K-Fold Cross-Validation - Leave-One-Out Cross-Validation - Shuffle & Split Cross-Validation 4. Compute and print the mean and standard deviation of accuracy for each cross-validation technique. 5. Discuss the results and explain which cross-validation technique you found most appropriate for this dataset and why. **Requirements:** - Implement the function `evaluate_cross_validation_techniques()`. - The function must return a dictionary where the keys are the names of the cross-validation techniques and the values are tuples containing the mean and standard deviation of the accuracy scores. **Expected Output Format:** ```python result = { \\"KFold\\": (mean_accuracy_kfold, std_accuracy_kfold), \\"StratifiedKFold\\": (mean_accuracy_stratified_kfold, std_accuracy_stratified_kfold), \\"LeaveOneOut\\": (mean_accuracy_loo, std_accuracy_loo), \\"ShuffleSplit\\": (mean_accuracy_shuffle_split, std_accuracy_shuffle_split), } # Implement and print the result print(result) ``` **Constraints:** - Use `svm.SVC` with default parameters. - Use 5 splits for KFold and StratifiedKFold. **Performance Requirements:** - Ensure that the cross-validation techniques are implemented efficiently to handle the size of the Iris dataset. ```python def evaluate_cross_validation_techniques(): # Import necessary libraries import numpy as np from sklearn import datasets, preprocessing, svm from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, cross_val_score # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Apply standard scaling to the feature data scaler = preprocessing.StandardScaler().fit(X) X_scaled = scaler.transform(X) # Initialize the SVM classifier clf = svm.SVC(kernel=\'linear\', C=1) # K-Fold Cross-Validation kfold = KFold(n_splits=5, random_state=42, shuffle=True) kfold_scores = cross_val_score(clf, X_scaled, y, cv=kfold) mean_accuracy_kfold = kfold_scores.mean() std_accuracy_kfold = kfold_scores.std() # Stratified K-Fold Cross-Validation stratified_kfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True) stratified_kfold_scores = cross_val_score(clf, X_scaled, y, cv=stratified_kfold) mean_accuracy_stratified_kfold = stratified_kfold_scores.mean() std_accuracy_stratified_kfold = stratified_kfold_scores.std() # Leave-One-Out Cross-Validation loo = LeaveOneOut() loo_scores = cross_val_score(clf, X_scaled, y, cv=loo) mean_accuracy_loo = loo_scores.mean() std_accuracy_loo = loo_scores.std() # Shuffle & Split Cross-Validation shuffle_split = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42) shuffle_split_scores = cross_val_score(clf, X_scaled, y, cv=shuffle_split) mean_accuracy_shuffle_split = shuffle_split_scores.mean() std_accuracy_shuffle_split = shuffle_split_scores.std() # Compile results into a dictionary result = { \\"KFold\\": (mean_accuracy_kfold, std_accuracy_kfold), \\"StratifiedKFold\\": (mean_accuracy_stratified_kfold, std_accuracy_stratified_kfold), \\"LeaveOneOut\\": (mean_accuracy_loo, std_accuracy_loo), \\"ShuffleSplit\\": (mean_accuracy_shuffle_split, std_accuracy_shuffle_split) } return result # Print the evaluation results results = evaluate_cross_validation_techniques() print(results) ``` After completing the implementation, discuss the results based on the dictionary values. Which technique provides the most reliable estimate of model performance and why? **Note:** This evaluation will help you understand the importance of different cross-validation techniques and their impact on model performance analysis.","solution":"def evaluate_cross_validation_techniques(): # Import necessary libraries import numpy as np from sklearn import datasets, preprocessing, svm from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut, ShuffleSplit, cross_val_score # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Apply standard scaling to the feature data scaler = preprocessing.StandardScaler().fit(X) X_scaled = scaler.transform(X) # Initialize the SVM classifier clf = svm.SVC(kernel=\'linear\', C=1) # K-Fold Cross-Validation kfold = KFold(n_splits=5, shuffle=True, random_state=42) kfold_scores = cross_val_score(clf, X_scaled, y, cv=kfold) mean_accuracy_kfold = kfold_scores.mean() std_accuracy_kfold = kfold_scores.std() # Stratified K-Fold Cross-Validation stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) stratified_kfold_scores = cross_val_score(clf, X_scaled, y, cv=stratified_kfold) mean_accuracy_stratified_kfold = stratified_kfold_scores.mean() std_accuracy_stratified_kfold = stratified_kfold_scores.std() # Leave-One-Out Cross-Validation loo = LeaveOneOut() loo_scores = cross_val_score(clf, X_scaled, y, cv=loo) mean_accuracy_loo = loo_scores.mean() std_accuracy_loo = loo_scores.std() # Shuffle & Split Cross-Validation shuffle_split = ShuffleSplit(n_splits=5, test_size=0.3, random_state=42) shuffle_split_scores = cross_val_score(clf, X_scaled, y, cv=shuffle_split) mean_accuracy_shuffle_split = shuffle_split_scores.mean() std_accuracy_shuffle_split = shuffle_split_scores.std() # Compile results into a dictionary result = { \\"KFold\\": (mean_accuracy_kfold, std_accuracy_kfold), \\"StratifiedKFold\\": (mean_accuracy_stratified_kfold, std_accuracy_stratified_kfold), \\"LeaveOneOut\\": (mean_accuracy_loo, std_accuracy_loo), \\"ShuffleSplit\\": (mean_accuracy_shuffle_split, std_accuracy_shuffle_split) } return result # Print the evaluation results results = evaluate_cross_validation_techniques() print(results)"},{"question":"**Question**: Implement a Python program that uses the `ensurepip` module to achieve the following tasks. Your implementation should consist of two parts: using the CLI and the Module API of `ensurepip`. # Part 1: Command Line Interface: Write a Python script to be executed using the command: ```bash python your_script_name.py ``` The script should perform the following steps: 1. Check if `pip` is installed. If not, use the command line interface of `ensurepip` to install `pip`. 2. If `pip` is already installed, print the current version of `pip`. # Part 2: Module API: Within the same Python script, implement a function that: 1. Uses the function `ensurepip.version()` to print the version of `pip` that will be installed. 2. Uses the function `ensurepip.bootstrap()` with the following constraints: - Installs `pip` only if it is not already installed or upgrades it if an older version is found. - Uses the `user` installation scheme if the script is not running in a virtual environment. - Runs with verbosity level 1. 3. If an attempt is made to set both `altinstall` and `default_pip` parameters to `True`, the function should catch the `ValueError` and print an appropriate message. # Input and Output: - **Input**: No input is required from the user. - **Output**: The output should provide the current state of `pip` installation, including the version to be installed, any upgrade actions, and confirmation of installation steps taken. # Example Output: ``` pip is not installed. Installing pip using ensurepip... ... pip installed successfully. ensurepip will install pip version: 21.2.4 Installing pip using user scheme... ... pip upgraded to version: 21.2.4 ``` # Constraints: - The script should handle scenarios where `pip` is installed, not installed, or needs to be upgraded. - The script should run on Python environments where `ensurepip` is available (Python >= 3.4). # Submission: Submit the Python script named `pip_installer.py` that follows the specifications.","solution":"import subprocess import ensurepip import sys def check_pip_installed(): try: result = subprocess.run([sys.executable, \'-m\', \'pip\', \'--version\'], capture_output=True, text=True) if result.returncode == 0: print(f\\"pip is already installed. Version: {result.stdout}\\") return True return False except subprocess.CalledProcessError: return False def install_pip_cli(): print(\\"pip is not installed.\\") print(\\"Installing pip using ensurepip...\\") ensurepip.bootstrap() print(\\"pip installed successfully.\\") print(\\"Version:\\", subprocess.run([sys.executable, \'-m\', \'pip\', \'--version\'], capture_output=True, text=True).stdout) def get_ensurepip_version(): version = ensurepip.version() print(f\\"ensurepip will install pip version: {version}\\") def setup_pip(): try: if check_pip_installed(): print(\\"pip is already installed, no need for installation.\\") else: install_pip_cli() get_ensurepip_version() print(\\"Installing pip using user scheme with verbosity level 1...\\") ensurepip.bootstrap(user=True, verbosity=1) print(\\"pip setup process completed.\\") print(\\"Version:\\", subprocess.run([sys.executable, \'-m\', \'pip\', \'--version\'], capture_output=True, text=True).stdout) except ValueError as e: print(\\"ValueError caught:\\", str(e)) if __name__ == \\"__main__\\": setup_pip()"},{"question":"**Problem Statement: Email Client Operations with imaplib** You are tasked with creating a simple email client that can connect to an IMAP server, retrieve emails from a specific folder, and perform certain operations based on user commands. Your task is to implement a class `EmailClient` that encapsulates these functionalities. # Class: ```python class EmailClient: def __init__(self, host: str, user: str, password: str, folder: str = \\"INBOX\\", ssl: bool = False): Initializes the EmailClient with connection details. :param host: The IMAP server host. :param user: Username for authentication. :param password: Password for authentication. :param folder: The mailbox folder to interact with. Default is \\"INBOX\\". :param ssl: Whether to use SSL for the connection. Default is False. pass def connect(self): Establishes a connection to the IMAP server. pass def list_folders(self): Lists all available mail folders. pass def select_folder(self, folder: str): Select a specific folder to interact with. :param folder: The name of the folder to select. pass def search_emails(self, criteria: str): Searches for emails based on given criteria. :param criteria: The search criterion (e.g., \'ALL\', \'FROM \\"someone@example.com\\"\'). :return: List of email IDs that match the criteria. pass def fetch_email(self, email_id: str): Fetch the full content of an email by its ID. :param email_id: The ID of the email to fetch. :return: The raw content of the email. pass def delete_email(self, email_id: str): Marks an email for deletion. :param email_id: The ID of the email to delete. pass def logout(self): Terminates the connection to the IMAP server. pass ``` # Requirements: 1. **Initialization**: Initialize the client with connection details and default folder. 2. **Connection**: - If `ssl` is True, use `IMAP4_SSL` to connect. Otherwise, use `IMAP4`. - Catch and handle appropriate exceptions such as `IMAP4.error`. 3. **List Folders**: Retrieve and return a list of folders available on the server. 4. **Select Folder**: Select a folder to perform operations on (default is INBOX). 5. **Search Emails**: Search for emails based on given search criteria (RFC2060). 6. **Fetch Email**: Fetch the full content of an email by its ID. 7. **Delete Email**: - Mark the specified email for deletion. - Ensure you expunge the mailbox to permanently delete it. 8. **Logout**: Ensure the IMAP session is properly closed and logged out. # Example Usage: ```python if __name__ == \\"__main__\\": import getpass host = input(\\"Enter IMAP server host: \\") user = input(\\"Enter your username: \\") password = getpass.getpass(\\"Enter your password: \\") folder = input(\\"Enter folder (INBOX by default): \\") or \\"INBOX\\" ssl = input(\\"Use SSL (True/False)? \\") == \\"True\\" client = EmailClient(host, user, password, folder, ssl) # Connect to the server client.connect() # List all folders folders = client.list_folders() print(f\\"Available folders: {folders}\\") # Search for all emails email_ids = client.search_emails(\\"ALL\\") print(f\\"Email IDs: {email_ids}\\") # Fetch a specific email if email_ids: email_content = client.fetch_email(email_ids[0]) print(f\\"Content of Email {email_ids[0]}: {email_content}\\") # Delete the first email in the list if email_ids: client.delete_email(email_ids[0]) print(f\\"Email {email_ids[0]} deleted.\\") # Logout client.logout() ``` # Constraints: - Ensure you handle exceptions such as connection errors or invalid folder names. - Perform input validation where necessary. - Provide appropriate comments and follow best practices in your code for clarity and readability.","solution":"import imaplib class EmailClient: def __init__(self, host, user, password, folder=\\"INBOX\\", ssl=False): self.host = host self.user = user self.password = password self.folder = folder self.ssl = ssl self.mail = None def connect(self): try: if self.ssl: self.mail = imaplib.IMAP4_SSL(self.host) else: self.mail = imaplib.IMAP4(self.host) self.mail.login(self.user, self.password) except imaplib.IMAP4.error as e: print(f\\"Failed to connect or login: {e}\\") def list_folders(self): try: status, folders = self.mail.list() if status == \\"OK\\": return folders else: print(\\"Failed to list folders\\") return [] except imaplib.IMAP4.error as e: print(f\\"Failed to list folders: {e}\\") return [] def select_folder(self, folder): try: status, _ = self.mail.select(folder) if status == \\"OK\\": self.folder = folder else: print(f\\"Failed to select folder: {folder}\\") except imaplib.IMAP4.error as e: print(f\\"Failed to select folder: {e}\\") def search_emails(self, criteria): try: status, data = self.mail.search(None, criteria) if status == \\"OK\\": return data[0].split() else: print(f\\"Failed to search emails with criteria: {criteria}\\") return [] except imaplib.IMAP4.error as e: print(f\\"Failed to search emails: {e}\\") return [] def fetch_email(self, email_id): try: status, data = self.mail.fetch(email_id, \\"(RFC822)\\") if status == \\"OK\\": return data[0][1] else: print(f\\"Failed to fetch email with ID: {email_id}\\") return None except imaplib.IMAP4.error as e: print(f\\"Failed to fetch email: {e}\\") return None def delete_email(self, email_id): try: self.mail.store(email_id, \'+FLAGS\', \'Deleted\') self.mail.expunge() except imaplib.IMAP4.error as e: print(f\\"Failed to delete email: {e}\\") def logout(self): try: self.mail.logout() except imaplib.IMAP4.error as e: print(f\\"Failed to logout: {e}\\")"},{"question":"**Secure Token Generator for Multiple Applications** You have been hired to improve the security of a software system that requires generating various types of secure tokens for different applications. You will implement a function that utilizes the `secrets` module to generate the following types of tokens: 1. **Random Byte Token**: A random byte string of a specified length. 2. **Random Hex Token**: A random hexadecimal string of a specified length. 3. **URL-safe Token**: A URL-safe random text string of a specified length. 4. **Secure Password**: An alphanumeric password of a specified length with constraints. Your goal is to implement the function `generate_secure_token` which takes in multiple parameters to specify the type of token, its length, and any additional constraints. # Function Signature ```python def generate_secure_token(token_type: str, length: int, constraints: dict = None) -> str: Generates a secure token based on the specified type and constraints. Parameters: token_type (str): The type of token to generate. Valid values are \'byte\', \'hex\', \'url\', \'password\'. length (int): The length of the token to generate. constraints (dict): Additional constraints for generating the token. Applicable when token_type is \'password\'. The dictionary can have the following keys: - \'min_digits\' (int): Minimum number of digits required in the password. - \'min_uppercase\' (int): Minimum number of uppercase characters required in the password. - \'min_lowercase\' (int): Minimum number of lowercase characters required in the password. Returns: str: The generated secure token. pass ``` # Input - `token_type`: A string specifying the type of token to generate. Valid values are `\'byte\'`, `\'hex\'`, `\'url\'`, `\'password\'`. - `length`: An integer specifying the length of the token. - `constraints`: A dictionary specifying additional constraints for generating the token when `token_type` is `\'password\'`. The dictionary can have the following keys: - `\'min_digits\'`: Minimum number of digits required in the password. - `\'min_uppercase\'`: Minimum number of uppercase characters required in the password. - `\'min_lowercase\'`: Minimum number of lowercase characters required in the password. # Output - A string representing the generated secure token. # Constraints - `length` will be a positive integer. - For `token_type = \'password\'`, the sum of `min_digits`, `min_uppercase`, and `min_lowercase` should not exceed `length`. - The generated password must adhere to the specified constraints. - Use the `secrets` module for all random number and token generation tasks. # Example Usage ```python # Generating a random byte token of length 16 print(generate_secure_token(\'byte\', 16)) # Generating a random hex token of length 16 print(generate_secure_token(\'hex\', 16)) # Generating a URL-safe token of length 16 print(generate_secure_token(\'url\', 16)) # Generating a secure password of length 12 with constraints constraints = {\'min_digits\': 3, \'min_uppercase\': 2, \'min_lowercase\': 2} print(generate_secure_token(\'password\', 12, constraints)) ``` # Note For `token_type = \'password\'`, ensure the password meets the specified constraints by using the `secrets.choice` method to randomly select characters from the appropriate categories (digits, lowercase, uppercase) and then filling the remaining length with a mix of alphanumeric characters.","solution":"import secrets import string def generate_secure_token(token_type: str, length: int, constraints: dict = None) -> str: if token_type == \'byte\': return secrets.token_bytes(length) elif token_type == \'hex\': return secrets.token_hex(length) elif token_type == \'url\': return secrets.token_urlsafe(length) elif token_type == \'password\': if not constraints: constraints = {} min_digits = constraints.get(\'min_digits\', 0) min_uppercase = constraints.get(\'min_uppercase\', 0) min_lowercase = constraints.get(\'min_lowercase\', 0) if min_digits + min_uppercase + min_lowercase > length: raise ValueError(\\"Sum of minimum digit, uppercase, and lowercase constraints exceeds length\\") password_chars = [] # Generate minimum required digits for _ in range(min_digits): password_chars.append(secrets.choice(string.digits)) # Generate minimum required uppercase letters for _ in range(min_uppercase): password_chars.append(secrets.choice(string.ascii_uppercase)) # Generate minimum required lowercase letters for _ in range(min_lowercase): password_chars.append(secrets.choice(string.ascii_lowercase)) # Fill the rest with a mix of alphanumeric characters remaining_length = length - len(password_chars) available_chars = string.ascii_letters + string.digits for _ in range(remaining_length): password_chars.append(secrets.choice(available_chars)) # Shuffle the resulting list to randomly distribute the characters secrets.SystemRandom().shuffle(password_chars) return \'\'.join(password_chars) else: raise ValueError(\\"Invalid token_type supplied\\") # Example usages (uncomment to test manually): # print(generate_secure_token(\'byte\', 16)) # print(generate_secure_token(\'hex\', 16)) # print(generate_secure_token(\'url\', 16)) # constraints = {\'min_digits\': 3, \'min_uppercase\': 2, \'min_lowercase\': 2} # print(generate_secure_token(\'password\', 12, constraints))"},{"question":"# Custom SMTP Server Implementation You are tasked with creating a custom SMTP server utilizing the provided `smtpd` module. The server should filter and log incoming email messages based on specific criteria. Specifically: 1. The server should log the sender\'s email and the subject of the incoming email. 2. The server should only accept emails from senders with the domain `example.com`. Emails from other domains should be rejected with an appropriate response message. # Task Implement a custom SMTP server subclass called `FilteredSMTPServer` by subclassing `smtpd.SMTPServer`. Override the `process_message` method to: 1. Log the sender\'s email address (MAIL FROM) and the subject of the email to a file named `email_log.txt`. 2. Reject emails that are not from the domain `example.com`. Input - The input to your server will come through the SMTP protocol, as simulated by `smtpd.SMTPServer`. Output - The server should log valid incoming emails (from `example.com`) to `email_log.txt`. - The server should reject emails from other domains with a `550` SMTP error code and appropriate message. Constraints - Make sure to handle cases where the \\"Subject\\" field is not present in the email. - Ensure the server does not crash for malformed inputs. Example Assume the following incoming email: ``` MAIL FROM:<john@example.com> RCPT TO:<jane@othersite.com> DATA Subject: Test Email This is a test message. . ``` For this input: - The email would be accepted and the log file `email_log.txt` would contain the line: `Sender: john@example.com, Subject: Test Email`. For an email from `mary@othersite.com`: - The server should reject it with the message: `550 Domain name not allowed`. # Additional Notes Make sure to include proper file handling to ensure logs are appended correctly without overwriting previous entries. ```python import smtpd import asyncore class FilteredSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): if not mailfrom.endswith(\'@example.com\'): return \'550 Domain name not allowed\' # Extract subject from the data subject = \\"\\" for line in data.split(\'n\'): if line.startswith(\\"Subject:\\"): subject = line[len(\\"Subject:\\"):].strip() break # Log the email details with open(\'email_log.txt\', \'a\') as log_file: log_file.write(f\'Sender: {mailfrom}, Subject: {subject}n\') return \'250 OK\' # Run the server if __name__ == \\"__main__\\": server = FilteredSMTPServer((\'localhost\', 1025), None) try: asyncore.loop() except KeyboardInterrupt: pass ```","solution":"import smtpd import asyncore class FilteredSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): if not mailfrom.endswith(\'@example.com\'): return \'550 Domain name not allowed\' # Extract subject from the data subject = \\"\\" for line in data.split(\'n\'): if line.startswith(\\"Subject:\\"): subject = line[len(\\"Subject:\\"):].strip() break # Log the email details with open(\'email_log.txt\', \'a\') as log_file: log_file.write(f\'Sender: {mailfrom}, Subject: {subject}n\') return \'250 OK\' if __name__ == \\"__main__\\": server = FilteredSMTPServer((\'localhost\', 1025), None) try: asyncore.loop() except KeyboardInterrupt: pass"},{"question":"**Problem Statement:** You are given a dataset that contains numerical information about various cities, such as population density, average temperature, and average income. Your task is to visualize this data using Seaborn with a customized diverging palette. You need to demonstrate your ability to generate and apply a diverging palette in a heatmap visualization. **Specifications:** 1. Load the provided dataset (You can use the sample dataset available in seaborn or create a DataFrame with random values for this exercise). 2. Create a customized diverging palette using Seaborn\'s `diverging_palette()` function with the following criteria: - The palette should diverge between a specified range of colors. - The center color should be \\"dark\\". - The palette should be a continuous colormap. - Increase the color separation around the center value. - Use a magenta-to-green color combination. - Decrease the saturation and lightness of the endpoints to specified values. 3. Apply this customized palette to a heatmap of the dataset. 4. Save the plot to a file called `custom_heatmap.png`. **Input Format:** The dataset can be in CSV format or generated directly within the code. The dataset should contain at least three to four columns of numerical data. **Output Format:** The output should be a heatmap saved as `custom_heatmap.png`. **Constraints:** - You must use Seaborn for generating the diverging palette and producing the heatmap. - The diverging palette should precisely meet the specified criteria. - Ensure proper labeling and titling of the heatmap for clarity. **Example:** Here is a template to get you started: ```python import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt # Load or create dataset data = pd.DataFrame({ \'City\': [\'City A\', \'City B\', \'City C\', \'City D\'], \'Population Density\': [100, 150, 200, 250], \'Average Temperature\': [15, 18, 20, 22], \'Average Income\': [30000, 45000, 50000, 60000] }) # Customize diverging palette palette = sns.diverging_palette(280, 150, s=50, l=35, sep=30, as_cmap=True) # Create heatmap plt.figure(figsize=(10, 8)) sns.heatmap(data.corr(), cmap=palette, annot=True) plt.title(\'Custom Heatmap of City Data\') plt.savefig(\'custom_heatmap.png\') plt.show() ``` In this example, you would replace the palette parameters with the ones specified in the instructions and ensure that the heatmap reflects the dataset\'s correlations. Ensure all the parameters are correctly implemented as per the question\'s requirements.","solution":"import seaborn as sns import numpy as np import pandas as pd import matplotlib.pyplot as plt def generate_heatmap_with_custom_palette(): # Generate sample dataset data = pd.DataFrame({ \'City\': [\'City A\', \'City B\', \'City C\', \'City D\'], \'Population Density\': [100, 150, 200, 250], \'Average Temperature\': [15, 18, 20, 22], \'Average Income\': [30000, 45000, 50000, 60000] }) # Drop the \'City\' column as it is non-numeric and compute the correlation matrix correlation_data = data.drop(columns=[\'City\']).corr() # Customize diverging palette palette = sns.diverging_palette(280, 150, s=50, l=35, sep=30, center=\\"dark\\", as_cmap=True) # Create heatmap plt.figure(figsize=(10, 8)) sns.heatmap(correlation_data, cmap=palette, annot=True, center=0) plt.title(\'Custom Heatmap of City Data\') plt.savefig(\'custom_heatmap.png\') plt.close() # Close the plot to prevent it from displaying in non-interactive environments generate_heatmap_with_custom_palette()"},{"question":"Task You are required to implement an XML-RPC client using the `xmlrpc.client` module in Python. Your client will interact with a pre-existing server which supports several operations. You need to ensure that your client can: 1. List all available methods on the server. 2. Call a specific method and handle its return value. 3. Handle potential faults and protocol errors gracefully. # Server Description The server provides the following methods: 1. `add(x, y)`: Adds two numbers. 2. `subtract(x, y)`: Subtracts the second number from the first. 3. `multiply(x, y)`: Multiplies two numbers. 4. `divide(x, y)`: Divides the first number by the second. Returns a float. 5. `get_today_date()`: Returns the current date as an ISO 8601 string. # Requirements 1. **List Methods**: Your client should be able to list all the methods available on the server. 2. **Perform RPC**: Implement functions to call the `add`, `subtract`, `multiply`, and `divide` methods on the server. These functions should handle the server\'s response appropriately. 3. **Date Handling**: Convert the date string returned by `get_today_date()` to a `datetime.datetime` object. 4. **Error Handling**: Your client should be able to handle `Fault` and `ProtocolError` exceptions and print appropriate messages. # Input - No input required from the user. # Output - **List Methods**: Print a list of methods available on the server. - **RPC Results**: Print the results of the add, subtract, multiply, and divide operations. - **Date Handling**: Print the current date in the format `DD.MM.YYYY, HH:MM`. - **Errors**: Print error messages if any `Fault` or `ProtocolError` exceptions are raised. # Example ```python # Assuming server is running on http://localhost:8000 # Example output: Methods available on the server: [\'add\', \'subtract\', \'multiply\', \'divide\', \'get_today_date\'] 7 + 3 = 10 7 - 3 = 4 7 * 3 = 21 7 / 3 = 2.3333333333333335 Today\'s date: 25.11.2023, 09:15 ``` # Implementation ```python import xmlrpc.client import datetime class RPCClient: def __init__(self, uri): self.proxy = xmlrpc.client.ServerProxy(uri) def list_methods(self): try: methods = self.proxy.system.listMethods() print(\\"Methods available on the server:\\", methods) except xmlrpc.client.Fault as fault: print(\\"A fault occurred\\") print(\\"Fault code: %d\\" % fault.faultCode) print(\\"Fault string: %s\\" % fault.faultString) except xmlrpc.client.ProtocolError as err: print(\\"A protocol error occurred\\") print(\\"URL: %s\\" % err.url) print(\\"HTTP/HTTPS headers: %s\\" % err.headers) print(\\"Error code: %d\\" % err.errcode) print(\\"Error message: %s\\" % err.errmsg) def add(self, x, y): try: result = self.proxy.add(x, y) print(f\\"{x} + {y} = {result}\\") except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def subtract(self, x, y): try: result = self.proxy.subtract(x, y) print(f\\"{x} - {y} = {result}\\") except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def multiply(self, x, y): try: result = self.proxy.multiply(x, y) print(f\\"{x} * {y} = {result}\\") except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def divide(self, x, y): try: result = self.proxy.divide(x, y) print(f\\"{x} / {y} = {result}\\") except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def get_today_date(self): try: date_string = self.proxy.get_today_date() date_obj = datetime.datetime.strptime(date_string, \\"%Y%m%dT%H:%M:%S\\") print(\\"Today\'s date: %s\\" % date_obj.strftime(\\"%d.%m.%Y, %H:%M\\")) except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def _handle_fault(self, fault): print(\\"A fault occurred\\") print(\\"Fault code: %d\\" % fault.faultCode) print(\\"Fault string: %s\\" % fault.faultString) def _handle_protocol_error(self, err): print(\\"A protocol error occurred\\") print(\\"URL: %s\\" % err.url) print(\\"HTTP/HTTPS headers: %s\\" % err.headers) print(\\"Error code: %d\\" % err.errcode) print(\\"Error message: %s\\" % err.errmsg) if __name__ == \\"__main__\\": client = RPCClient(\\"http://localhost:8000/\\") client.list_methods() client.add(7, 3) client.subtract(7, 3) client.multiply(7, 3) client.divide(7, 3) client.get_today_date() ``` Ensure that you have an XML-RPC server running at `http://localhost:8000` with the described methods for your client to interact with.","solution":"import xmlrpc.client import datetime class RPCClient: def __init__(self, uri): self.proxy = xmlrpc.client.ServerProxy(uri) def list_methods(self): try: methods = self.proxy.system.listMethods() print(\\"Methods available on the server:\\", methods) return methods except xmlrpc.client.Fault as fault: print(\\"A fault occurred\\") print(\\"Fault code: %d\\" % fault.faultCode) print(\\"Fault string: %s\\" % fault.faultString) except xmlrpc.client.ProtocolError as err: print(\\"A protocol error occurred\\") print(\\"URL: %s\\" % err.url) print(\\"HTTP/HTTPS headers: %s\\" % err.headers) print(\\"Error code: %d\\" % err.errcode) print(\\"Error message: %s\\" % err.errmsg) def add(self, x, y): try: result = self.proxy.add(x, y) print(f\\"{x} + {y} = {result}\\") return result except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def subtract(self, x, y): try: result = self.proxy.subtract(x, y) print(f\\"{x} - {y} = {result}\\") return result except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def multiply(self, x, y): try: result = self.proxy.multiply(x, y) print(f\\"{x} * {y} = {result}\\") return result except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def divide(self, x, y): try: result = self.proxy.divide(x, y) print(f\\"{x} / {y} = {result}\\") return result except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def get_today_date(self): try: date_string = self.proxy.get_today_date() date_obj = datetime.datetime.strptime(date_string, \\"%Y%m%dT%H:%M:%S\\") formatted_date = date_obj.strftime(\\"%d.%m.%Y, %H:%M\\") print(\\"Today\'s date:\\", formatted_date) return formatted_date except xmlrpc.client.Fault as fault: self._handle_fault(fault) except xmlrpc.client.ProtocolError as err: self._handle_protocol_error(err) def _handle_fault(self, fault): print(\\"A fault occurred\\") print(\\"Fault code: %d\\" % fault.faultCode) print(\\"Fault string: %s\\" % fault.faultString) def _handle_protocol_error(self, err): print(\\"A protocol error occurred\\") print(\\"URL: %s\\" % err.url) print(\\"HTTP/HTTPS headers: %s\\" % err.headers) print(\\"Error code: %d\\" % err.errcode) print(\\"Error message: %s\\" % err.errmsg) if __name__ == \\"__main__\\": client = RPCClient(\\"http://localhost:8000/\\") client.list_methods() client.add(7, 3) client.subtract(7, 3) client.multiply(7, 3) client.divide(7, 3) client.get_today_date()"},{"question":"# Python Version Hex Encoder-Decoder Objective: Write a Python function to encode and decode Python version information using the structure described in the documentation. Your solution will demonstrate a strong understanding of bitwise operations and manipulating version information. Function Signature: 1. `encode_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int` 2. `decode_version(version_hex: int) -> (int, int, int, str, int)` Detailed Requirements: 1. **Encoding Function** - **Input**: Five arguments corresponding to the major, minor, micro versions, release level, and release serial: - `major` (int): Major version number (0 to 255). - `minor` (int): Minor version number (0 to 255). - `micro` (int): Micro version number (0 to 255). - `release_level` (str): Release level, which could be one of \\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\". - `release_serial` (int): Release serial number (0 to 15). - **Output**: A single integer representing the encoded version in hexadecimal format. 2. **Decoding Function** - **Input**: A single integer representing the hex-encoded version. - **Output**: A tuple of five elements describing the version, with the release_level converted back to a string. Example: ```python >>> encode_version(3, 4, 1, \\"alpha\\", 2) 50436450 # This corresponds to 0x030401a2 >>> decode_version(0x030401a2) (3, 4, 1, \'alpha\', 2) ``` Constraints: - The encoding function must handle the mapping for release levels as follows: - \\"alpha\\" -> 0xA - \\"beta\\" -> 0xB - \\"candidate\\" -> 0xC - \\"final\\" -> 0xF - The decoding function must map the above hex values back to their string equivalents. Performance: - Your functions should make optimal use of bitwise operations and avoid unnecessary computations. Hints: - Utilize bit shifting (`<<`) and bitwise OR (`|`) for creating the encoded version. - Utilize bit shifting and bitwise AND (`&`) for extracting individual components from the encoded version.","solution":"def encode_version(major, minor, micro, release_level, release_serial): Encode version information into a single integer. release_levels = { \\"alpha\\": 0xA, \\"beta\\": 0xB, \\"candidate\\": 0xC, \\"final\\": 0xF } release_level_hex = release_levels[release_level] version_hex = (major << 24) | (minor << 16) | (micro << 8) | (release_level_hex << 4) | release_serial return version_hex def decode_version(version_hex): Decode the version information from the encoded integer. major = (version_hex >> 24) & 0xFF minor = (version_hex >> 16) & 0xFF micro = (version_hex >> 8) & 0xFF release_level_hex = (version_hex >> 4) & 0xF release_serial = version_hex & 0xF release_levels = { 0xA: \\"alpha\\", 0xB: \\"beta\\", 0xC: \\"candidate\\", 0xF: \\"final\\" } release_level = release_levels[release_level_hex] return (major, minor, micro, release_level, release_serial)"},{"question":"You are tasked with implementing a custom PyTorch function using the `torch.autograd.Function` class and verifying its gradients using PyTorch\'s gradient checking utility. The function you will implement represents the following operation: [ f(x) = 2x^3 + 3x^2 + x + 1 ] You will perform the following steps: 1. Implement the custom forward and backward methods for this function. 2. Use the implemented function in a small network. 3. Verify the gradients using `gradcheck` from `torch.autograd`. # Function Signature ```python import torch from torch.autograd import Function from torch.autograd import gradcheck class CustomPolyFunction(Function): @staticmethod def forward(ctx, input): Perform the forward computation. Args: ctx: context object to save information for backward computation input: input tensor Returns: output tensor # Implement the forward pass here @staticmethod def backward(ctx, grad_output): Perform the backward computation. Args: ctx: context object with saved info for backward computation grad_output: tensor containing the gradients Returns: gradients w.r.t the input # Implement the backward pass here # Testing the implementation if __name__ == \\"__main__\\": # Create an input tensor input = torch.tensor([1.0, 2.0, 3.0], requires_grad=True).double() # Check gradients using gradcheck test = gradcheck(CustomPolyFunction.apply, (input,), raise_exception=True) print(f\'Gradient check: {test}\') ``` # Requirements 1. **Custom Function Implementation**: - Implement the `forward` method of `CustomPolyFunction`: This method should compute the polynomial function ( f(x) = 2x^3 + 3x^2 + x + 1 ). - Implement the `backward` method of `CustomPolyFunction`: This method should compute the gradient of the polynomial function with respect to the input ( x ). 2. **Gradient Check**: - Use the `gradcheck` utility to verify that the gradients computed by your custom function are correct. # Constraints: - Input tensor will have elements of type `torch.double`. - You must use PyTorch\'s mechanisms for differentiable programming. - Ensure that the forward and backward passes are implemented correctly to pass the gradient check. # Example If you correctly implement the `CustomPolyFunction`, then running the provided script should output: ``` Gradient check: True ```","solution":"import torch from torch.autograd import Function from torch.autograd import gradcheck class CustomPolyFunction(Function): @staticmethod def forward(ctx, input): Perform the forward computation. Args: ctx: context object to save information for backward computation input: input tensor Returns: output tensor ctx.save_for_backward(input) output = 2 * input ** 3 + 3 * input ** 2 + input + 1 return output @staticmethod def backward(ctx, grad_output): Perform the backward computation. Args: ctx: context object with saved info for backward computation grad_output: tensor containing the gradients Returns: gradients w.r.t the input input, = ctx.saved_tensors grad_input = grad_output * (6 * input ** 2 + 6 * input + 1) return grad_input # Testing the implementation if __name__ == \\"__main__\\": # Create an input tensor input = torch.tensor([1.0, 2.0, 3.0], requires_grad=True).double() # Check gradients using gradcheck test = gradcheck(CustomPolyFunction.apply, (input,), raise_exception=True) print(f\'Gradient check: {test}\')"},{"question":"Task: Write a function in Python that takes a list of timestamps and returns a dictionary summarizing the following information for each unique date: - The earliest and latest times logged on that date. - The total number of unique dates present in the input list. Input: - A list of timestamps in the format: `YYYY-MM-DD HH:MM:SS`. Output: - A dictionary where the keys are the unique dates in the format `YYYY-MM-DD`, and the values are another dictionary containing: - `\'earliest_time\'`: earliest time logged on that date in the format `HH:MM:SS`. - `\'latest_time\'`: latest time logged on that date in the format `HH:MM:SS`. - `\'count\'`: the total number of timestamps logged on that date. - The total number of unique dates. Function Signature: ```python from typing import List, Dict, Tuple def summarize_timestamps(timestamps: List[str]) -> Tuple[Dict[str, Dict[str, str]], int]: pass ``` Constraints: - All timestamps are valid and are given in `YYYY-MM-DD HH:MM:SS` format. - The input list can have up to 1000 timestamps. Example: ```python timestamps = [ \\"2023-08-01 14:22:12\\", \\"2023-08-01 09:42:05\\", \\"2023-08-01 18:01:45\\", \\"2023-08-02 10:22:10\\", \\"2023-08-03 07:15:30\\" ] output = summarize_timestamps(timestamps) print(output) # Expected output: # ( # { # \\"2023-08-01\\": { # \\"earliest_time\\": \\"09:42:05\\", # \\"latest_time\\": \\"18:01:45\\", # \\"count\\": 3 # }, # \\"2023-08-02\\": { # \\"earliest_time\\": \\"10:22:10\\", # \\"latest_time\\": \\"10:22:10\\", # \\"count\\": 1 # }, # \\"2023-08-03\\": { # \\"earliest_time\\": \\"07:15:30\\", # \\"latest_time\\": \\"07:15:30\\", # \\"count\\": 1 # } # }, # 3 # ) ``` Notes: - You may use the `datetime` module from the standard library. - Consider edge cases such as all timestamps being on the same date or every timestamp being on a different date.","solution":"from typing import List, Dict, Tuple from collections import defaultdict import datetime def summarize_timestamps(timestamps: List[str]) -> Tuple[Dict[str, Dict[str, str]], int]: date_summary = defaultdict(lambda: {\'earliest_time\': \\"23:59:59\\", \'latest_time\': \\"00:00:00\\", \'count\': 0}) for timestamp in timestamps: date_part, time_part = timestamp.split() if time_part < date_summary[date_part][\'earliest_time\']: date_summary[date_part][\'earliest_time\'] = time_part if time_part > date_summary[date_part][\'latest_time\']: date_summary[date_part][\'latest_time\'] = time_part date_summary[date_part][\'count\'] += 1 result = dict(date_summary) unique_dates_count = len(result) return result, unique_dates_count"},{"question":"Coding Assessment Question # Objective You need to demonstrate your understanding of supervised learning in scikit-learn by implementing a classifier using an ensemble method, specifically the RandomForestClassifier. # Task Given a dataset, perform the following steps to design and evaluate a RandomForestClassifier: 1. **Data Preprocessing**: - Load the dataset (you can use sklearn.datasets). - Split the dataset into training and testing sets. - Perform any necessary data cleaning and preprocessing (e.g., handling missing values, encoding categorical variables). 2. **Model Implementation**: - Implement a RandomForestClassifier using scikit-learn. - Fit the model on the training data. - Predict the outcomes for the test data. 3. **Evaluation**: - Evaluate the performance of your model using appropriate metrics (e.g., accuracy, precision, recall, F1-score). - Provide a confusion matrix for the test set predictions. # Input - None (You will use the sklearn.datasets to load any dataset such as the Iris or Wine dataset for this task). # Output - Print the evaluation metrics (accuracy, precision, recall, F1-score) on the test data. - Display the confusion matrix. # Constraints - You must use the RandomForestClassifier from sklearn.ensemble. - You should handle any missing values or categorical data appropriately before fitting the model. - Ensure the train-test split is 80-20. # Performance Requirements - The classification accuracy must be at least 85% on the chosen dataset. # Sample Code Structure ```python import numpy as np import pandas as pd from sklearn.datasets import load_iris # You can use any dataset from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # Load the dataset data = load_iris() X = data.data y = data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implementing RandomForestClassifier model = RandomForestClassifier(n_estimators=100, random_state=42) model.fit(X_train, y_train) # Predict outcomes for the test set y_pred = model.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') conf_matrix = confusion_matrix(y_test, y_pred) print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") print(\\"Confusion Matrix:\\") print(conf_matrix) ``` Your task is to fill in any missing parts and ensure your code meets the requirements outlined above. # Note Choose any dataset available from `sklearn.datasets` and ensure reproducibility by setting the `random_state`.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # Load the dataset data = load_iris() X = data.data y = data.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Implementing RandomForestClassifier model = RandomForestClassifier(n_estimators=100, random_state=42) model.fit(X_train, y_train) # Predict outcomes for the test set y_pred = model.predict(X_test) # Evaluate the model accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred, average=\'macro\') recall = recall_score(y_test, y_pred, average=\'macro\') f1 = f1_score(y_test, y_pred, average=\'macro\') conf_matrix = confusion_matrix(y_test, y_pred) # Output the evaluation metrics print(f\\"Accuracy: {accuracy}\\") print(f\\"Precision: {precision}\\") print(f\\"Recall: {recall}\\") print(f\\"F1 Score: {f1}\\") print(\\"Confusion Matrix:\\") print(conf_matrix) # Packing the evaluation metrics into a function for testing def get_model_performance(): return accuracy, precision, recall, f1, conf_matrix"},{"question":"You are tasked with implementing a function that decodes a 32-bit hexadecimal integer representing a Python version and outputs its components. This exercise will demonstrate your understanding of bitwise operations and how to manipulate bits to extract useful information. Function Signature ```python def decode_python_version(hex_version: int) -> dict: Decodes a hexadecimal version number into its constituent components: major version, minor version, micro version, release level, and release serial. Args: hex_version (int): A 32-bit integer representing the encoded Python version Returns: dict: A dictionary with keys \'major\', \'minor\', \'micro\', \'release_level\', \'release_serial\' and their corresponding integer values extracted from the input. pass ``` Input * `hex_version` - A 32-bit integer where different byte segments represent the Python version components. Output * A dictionary containing the following keys and their integer values: * `major`: The major version of Python (0-255) * `minor`: The minor version of Python (0-255) * `micro`: The micro version of Python (0-255) * `release_level`: The release level of Python encoded as \'0xA\' (alpha), \'0xB\' (beta), \'0xC\' (release candidate), or \'0xF\' (final) * `release_serial`: The release serial number of Python (0-15) Example ```python print(decode_python_version(0x030401a2)) # Output: {\'major\': 3, \'minor\': 4, \'micro\': 1, \'release_level\': 0xA, \'release_serial\': 2} print(decode_python_version(0x030a00f0)) # Output: {\'major\': 3, \'minor\': 10, \'micro\': 0, \'release_level\': 0xF, \'release_serial\': 0} ``` Constraints * The input integer will always be a valid 32-bit representation of a Python version. * The function should not use any external libraries. Performance Requirements * The function should run in constant time O(1) since the operations on the fixed-size integer are limited and straightforward. Hint * To extract a specific byte or series of bits from an integer, bitwise operations like shifting (`>>`), masking (`&`), etc., can be useful. Good luck, and happy coding!","solution":"def decode_python_version(hex_version: int) -> dict: Decodes a hexadecimal version number into its constituent components: major version, minor version, micro version, release level, and release serial. Args: hex_version (int): A 32-bit integer representing the encoded Python version Returns: dict: A dictionary with keys \'major\', \'minor\', \'micro\', \'release_level\', \'release_serial\' and their corresponding integer values extracted from the input. major = (hex_version >> 24) & 0xFF minor = (hex_version >> 16) & 0xFF micro = (hex_version >> 8) & 0xFF release_level = (hex_version >> 4) & 0xF release_serial = hex_version & 0xF return { \'major\': major, \'minor\': minor, \'micro\': micro, \'release_level\': release_level, \'release_serial\': release_serial }"},{"question":"# System Resource Management You are tasked with writing a Python program that demonstrates the usage of the `resource` module by implementing the following functionalities. Function 1: Set and Get Resource Limits - **Function Name**: `set_and_get_limits(resource_type, new_limits)` - **Inputs**: - `resource_type` (string): The type of resource limit to be set or retrieved (e.g., \'RLIMIT_CPU\'). - `new_limits` (tuple of two integers or None): A tuple indicating new soft and hard limits. If `None`, the function should only retrieve the current limits. - **Outputs**: - If `new_limits` is `None`, it returns the current soft and hard limits. - Otherwise, it sets the specified limits and returns the previous limits. - **Constraints**: - You should handle `resource.error` and `ValueError` exceptions. - Ensure that the soft limit does not exceed the hard limit. Function 2: Measure Resource Usage - **Function Name**: `measure_usage()` - **Inputs**: None - **Outputs**: - A dictionary containing resource usage statistics for the current process where keys are: - \'user_time\': Time spent executing in user mode. - \'system_time\': Time spent executing in system mode. - \'max_rss\': Maximum resident set size. - \'voluntary_context_switches\': Number of voluntary context switches. - \'involuntary_context_switches\': Number of involuntary context switches. Example Usage: ```python # Example of setting and getting limits try: prev_limits = set_and_get_limits(\'RLIMIT_CPU\', (10, 20)) print(\\"Previous limits:\\", prev_limits) curr_limits = set_and_get_limits(\'RLIMIT_CPU\', None) print(\\"Current limits:\\", curr_limits) except resource.error as e: print(f\\"Resource error: {e}\\") except ValueError as e: print(f\\"Value error: {e}\\") # Example of measuring usage usage_information = measure_usage() print(\\"Resource usage:\\", usage_information) ``` # Performance Requirements - Ensure the program handles setting and getting resource limits correctly without exceeding the permissible limits. - Efficient retrieval of resource usage statistics. **Note**: Use appropriate constants and handle exceptions as specified in the documentation.","solution":"import resource def set_and_get_limits(resource_type, new_limits): Sets the resource limits and gets the previous limits. Parameters: resource_type (str): Type of resource (e.g., \'RLIMIT_CPU\'). new_limits (tuple or None): New limits as a tuple (soft_limit, hard_limit) or None. Returns: tuple: Previous limits as (soft_limit, hard_limit). Raises: resource.error, ValueError: If invalid resource_type or limits are provided. # Resource type mapping resource_map = { \'RLIMIT_CPU\': resource.RLIMIT_CPU, \'RLIMIT_FSIZE\': resource.RLIMIT_FSIZE, \'RLIMIT_DATA\': resource.RLIMIT_DATA, \'RLIMIT_STACK\': resource.RLIMIT_STACK, \'RLIMIT_CORE\': resource.RLIMIT_CORE, \'RLIMIT_RSS\': resource.RLIMIT_RSS, \'RLIMIT_NPROC\': resource.RLIMIT_NPROC, \'RLIMIT_NOFILE\': resource.RLIMIT_NOFILE, \'RLIMIT_MEMLOCK\': resource.RLIMIT_MEMLOCK, \'RLIMIT_AS\': resource.RLIMIT_AS } if resource_type not in resource_map: raise ValueError(f\\"Invalid resource type: {resource_type}\\") res = resource_map[resource_type] # Retrieve the current resource limits current_limits = resource.getrlimit(res) if new_limits is None: return current_limits soft_limit, hard_limit = new_limits if soft_limit > hard_limit: raise ValueError(\\"Soft limit cannot exceed the hard limit\\") # Set new resource limits resource.setrlimit(res, new_limits) return current_limits def measure_usage(): Measures the resource usage of the current process. Returns: dict: Dictionary containing resource usage statistics. usage = resource.getrusage(resource.RUSAGE_SELF) usage_stats = { \'user_time\': usage.ru_utime, \'system_time\': usage.ru_stime, \'max_rss\': usage.ru_maxrss, \'voluntary_context_switches\': usage.ru_nvcsw, \'involuntary_context_switches\': usage.ru_nivcsw } return usage_stats"},{"question":"**Objective**: Implement a function that leverages the `zoneinfo` module to handle a series of datetime operations involving multiple time zones and daylight saving time transitions. Problem Statement You are given a list of datetime strings and a corresponding list of IANA time zone keys. Your task is to implement a function `process_timezones` that processes these datetime strings based on the provided time zones, and returns a list of dictionaries containing detailed time zone information. Function Signature ```python from typing import List, Dict def process_timezones(datetimes: List[str], timezones: List[str]) -> List[Dict]: pass ``` Input - `datetimes`: A list of ISO 8601 formatted datetime strings. Example: `[\\"2020-10-31T12:00:00\\", \\"2021-03-14T02:30:00\\"]` - `timezones`: A list of IANA time zone keys corresponding to each datetime. Example: `[\\"America/Los_Angeles\\", \\"Europe/London\\"]` Output A list of dictionaries where each dictionary contains the following keys: - `\\"original_datetime\\"`: The original datetime string. - `\\"timezone\\"`: The IANA time zone key. - `\\"converted_datetime\\"`: The datetime converted to the specified time zone. - `\\"fold\\"`: The fold attribute value. - `\\"tzname\\"`: The time zone name abbreviation after conversion. Constraints - Each datetime string will be a valid ISO 8601 formatted string. - Each timezone string will be a valid IANA time zone key. - The lengths of the `datetimes` and `timezones` lists will be equal and have at least one element and at most 100 elements. Example ```python datetimes = [\\"2020-10-31T12:00:00\\", \\"2021-03-14T02:30:00\\"] timezones = [\\"America/Los_Angeles\\", \\"Europe/London\\"] result = process_timezones(datetimes, timezones) ``` Output: ```python [ { \\"original_datetime\\": \\"2020-10-31T12:00:00\\", \\"timezone\\": \\"America/Los_Angeles\\", \\"converted_datetime\\": \\"2020-10-31T12:00:00-07:00\\", \\"fold\\": 0, \\"tzname\\": \\"PDT\\" }, { \\"original_datetime\\": \\"2021-03-14T02:30:00\\", \\"timezone\\": \\"Europe/London\\", \\"converted_datetime\\": \\"2021-03-14T09:30:00+00:00\\", \\"fold\\": 0, \\"tzname\\": \\"GMT\\" } ] ``` Additional Notes - You should handle daylight saving time transitions correctly. - Use the `zoneinfo` module for time zone conversion. - Ensure the function is efficient and handles all edge cases.","solution":"from typing import List, Dict from datetime import datetime from zoneinfo import ZoneInfo def process_timezones(datetimes: List[str], timezones: List[str]) -> List[Dict]: results = [] for dt_str, tz_str in zip(datetimes, timezones): original_dt = datetime.fromisoformat(dt_str) tz = ZoneInfo(tz_str) converted_dt = original_dt.replace(tzinfo=tz) result = { \\"original_datetime\\": dt_str, \\"timezone\\": tz_str, \\"converted_datetime\\": converted_dt.isoformat(), \\"fold\\": converted_dt.fold, \\"tzname\\": converted_dt.tzname() } results.append(result) return results"},{"question":"You are required to demonstrate your understanding of the seaborn library, particularly its `seaborn.objects` module, by creating a complex multi-facet plot and adjusting its layout. Follow these steps to complete the assessment: 1. **Data Generation:** Write a function called `generate_data` that generates a DataFrame with the following columns: - `Category`: Contains random categories \'A\', \'B\', \'C\' (10 occurrences each). - `X`: Contains random values between 1 and 100. - `Y`: Contains random values between 1 and 100. The DataFrame should have a total of 30 rows. 2. **Plot Creation:** Using the seaborn library, write a function `create_plot` that takes the generated DataFrame as input and creates a plot using `seaborn.objects.Plot`. The plot should: - Be faceted by the `Category` column into subplots. - Use the `extent` parameter to specify plot dimensions (relative to the figure). - Use the `engine` parameter to optimize the layout. - Control the overall size of the figure with the `size` parameter. Your function should display the plot directly. # Expected Input and Output - Function `generate_data`: - **Input**: None - **Output**: DataFrame with columns `Category`, `X`, `Y` and 30 rows. - Function `create_plot`: - **Input**: DataFrame with columns `Category`, `X`, `Y` - **Output**: None (the function should display the plot directly). # Constraints - You should use the seaborn library, specifically the `seaborn.objects` module. - Ensure plots are clear and readable. - Use `numpy` and `pandas` for data generation. # Example Usage ```python # Example usage of generate_data and create_plot functions import numpy as np import pandas as pd import seaborn.objects as so def generate_data() -> pd.DataFrame: categories = np.random.choice([\'A\', \'B\', \'C\'], 30) data = { \'Category\': categories, \'X\': np.random.randint(1, 101, 30), \'Y\': np.random.randint(1, 101, 30) } return pd.DataFrame(data) def create_plot(df: pd.DataFrame) -> None: p = so.Plot(df, x=\'X\', y=\'Y\').facet(\\"Category\\").layout(extent=[0, 0, 0.8, 1], engine=\\"constrained\\", size=(8, 6)) p.show() # Generate data and create plot data = generate_data() create_plot(data) ``` # Notes - Ensure that the random generation produces different values in each run. - Arrange your code and comments such that your thought process is clear.","solution":"import numpy as np import pandas as pd import seaborn.objects as so import matplotlib.pyplot as plt def generate_data() -> pd.DataFrame: Generates a DataFrame with columns `Category`, `X`, `Y` where Category contains random categories \'A\', \'B\', \'C\' (10 occurrences each), and `X`, `Y` contain random values between 1 and 100. The DataFrame has a total of 30 rows. categories = np.random.choice([\'A\', \'B\', \'C\'], 30) data = { \'Category\': categories, \'X\': np.random.randint(1, 101, 30), \'Y\': np.random.randint(1, 101, 30) } return pd.DataFrame(data) def create_plot(df: pd.DataFrame) -> None: Creates a multi-facet plot with the given DataFrame. The plot is faceted by the `Category` column. Uses `extent`, `engine`, and `size` parameters for plot customization. p = so.Plot(df, x=\\"X\\", y=\\"Y\\").facet(\\"Category\\").add(so.Dot()) p.layout(extent=[0.05, 0.05, 0.95, 0.95], engine=\\"constrained\\", size=(12, 8)) p.show() # Generate data and create plot data = generate_data() create_plot(data)"},{"question":"# Python Coding Assessment: Advanced Sorting Techniques Objective: To assess your understanding of various sorting techniques and methods in Python, including the use of custom key functions and multi-level sorting. Problem Statement: You are provided with a list of dictionaries, each representing a book with attributes `title`, `author`, `year`, and `rating`. Your task is to implement a function `sort_books` that: 1. Sorts the books by `author` in ascending order. 2. For books by the same author, sorts by `year` in descending order. 3. If there are multiple books by the same author and year, sorts by `rating` in descending order. This challenges you to combine multiple levels of sorting by handling complex key functions. Function Signature: ```python def sort_books(books: List[Dict[str, Any]]) -> List[Dict[str, Any]]: ``` Input: - `books`: A list of dictionaries, where each dictionary represents a book and contains the following fields: - `title` (str): The title of the book. - `author` (str): The name of the author of the book. - `year` (int): The publication year of the book. - `rating` (float): The rating of the book out of 5.0. Output: - Returns a new list of dictionaries sorted based on the criteria specified. Constraints: - The `books` list will contain at least one book. - The `title`, `author`, and `year` fields will contain valid data. Example: ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 2010, \\"rating\\": 4.5}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 2012, \\"rating\\": 4.7}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author X\\", \\"year\\": 2010, \\"rating\\": 4.9}, {\\"title\\": \\"Book D\\", \\"author\\": \\"Author X\\", \\"year\\": 2012, \\"rating\\": 4.6}, ] sorted_books = sort_books(books) # Expected output: # [ # {\\"title\\": \\"Book D\\", \\"author\\": \\"Author X\\", \\"year\\": 2012, \\"rating\\": 4.6}, # {\\"title\\": \\"Book C\\", \\"author\\": \\"Author X\\", \\"year\\": 2010, \\"rating\\": 4.9}, # {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 2010, \\"rating\\": 4.5}, # {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 2012, \\"rating\\": 4.7}, # ] ``` Notes: - You must use Python\'s sorting techniques as demonstrated in the provided documentation. - Utilize the `operator` module\'s functionalities where appropriate. In your implementation, showcase your understanding of multi-level sorting with custom key functions.","solution":"from typing import List, Dict, Any def sort_books(books: List[Dict[str, Any]]) -> List[Dict[str, Any]]: Sorts a list of books by multiple criteria: 1. author in ascending order 2. year in descending order if authors are the same 3. rating in descending order if both author and year are the same return sorted(books, key=lambda book: (book[\'author\'], -book[\'year\'], -book[\'rating\']))"},{"question":"You are tasked with creating an email client in Python that connects to a POP3 server and retrieves email messages securely. Implement a function `retrieve_emails` that meets the following requirements: Function Signature: ```python def retrieve_emails(host: str, username: str, password: str, use_ssl: bool = True, port: int = None) -> None: pass ``` Input: - `host` (str): The hostname of the POP3 server. - `username` (str): The username for authenticating with the POP3 server. - `password` (str): The password for authenticating with the POP3 server. - `use_ssl` (bool, optional): A flag indicating whether to use SSL for the connection. Default is `True`. - `port` (int, optional): The port number to connect to. If `None`, use default port (110 for non-SSL, 995 for SSL). Output: - This function does not return any value. Instead, it prints the subject and the first 100 characters of each email message. Requirements: 1. Connect to the POP3 server specified by the `host` argument. Use SSL if `use_ssl` is `True`. 2. Use the provided `username` and `password` to authenticate. 3. Retrieve and print the subject and the first 100 characters of the body of each email in the inbox. 4. Use the appropriate methods to handle any exceptions and print clean error messages. 5. Ensure the connection is properly closed after retrieving the emails. Example Usage: ```python # This example will vary based on the actual server and email details retrieve_emails(\'pop.example.com\', \'user@example.com\', \'password123\', use_ssl=True) ``` Notes: - You may assume that the emails are stored in plain text format. - Use appropriate error handling to manage connection issues and authentication errors. Constraints: - You are not allowed to use any third-party libraries other than what is included in the standard Python library. - You must ensure secure handling of passwords and sensitive information. - Ensure that the retrieval process does not leave the mailbox locked or in an inconsistent state. --- Implement the function `retrieve_emails` to validate your understanding of the `poplib` module in Python. Test your function thoroughly to ensure all corner cases are handled gracefully.","solution":"import poplib from email.parser import Parser def retrieve_emails(host: str, username: str, password: str, use_ssl: bool = True, port: int = None) -> None: try: # Set default port if not provided if port is None: port = 995 if use_ssl else 110 # Establish connection to the POP3 server if use_ssl: server = poplib.POP3_SSL(host, port) else: server = poplib.POP3(host, port) # Authenticate with the server server.user(username) server.pass_(password) # Get the list of emails email_count, _ = server.stat() for i in range(1, email_count + 1): response, lines, _ = server.retr(i) msg_data = b\'n\'.join(lines).decode(\'utf-8\') msg = Parser().parsestr(msg_data) # Get the subject and the first 100 characters of the body subject = msg.get(\'subject\', \'(no subject)\') body_lines = msg.get_payload(decode=True).decode(\'utf-8\').split(\'n\') body_preview = \'n\'.join(body_lines)[:100] print(f\\"Subject: {subject}nBody preview: {body_preview}n\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: try: server.quit() except: pass"},{"question":"# Question Binary Data Encoding and Manipulation Using the functionalities provided by the `struct` and `codecs` modules in Python, implement a function that takes a list of integers, packs these integers into a binary format, encodes this binary data to a base64 string, then decodes it back to the binary format and unpacks the integers. Your function should perform the following steps: 1. Pack a list of integers into a binary format (use `struct.pack`). 2. Encode this binary data to a base64 string (use the `codecs` module). 3. Decode the base64 string back to its binary format. 4. Unpack the binary data back into a list of integers (use `struct.unpack`). Write a function `pack_encode_unpack` that takes the following parameters: - `integers`: a list of integers you need to pack. The function should return a list of integers which are the result of unpacking the binary data after encoding and decoding it. # Example ```python def pack_encode_unpack(integers): pass # Example input integers = [1, 2, 3, 4] # Call the function output = pack_encode_unpack(integers) # Expected output should be the same list of integers print(output) # Output: [1, 2, 3, 4] ``` # Constraints - The list of integers will contain between 1 and 50 elements. - Each integer in the list will be a 4-byte integer (range from -2147483648 to 2147483647). # Hints - Use `\'i\'` format character for integers in `struct.pack` and `struct.unpack`. - Use `codecs.encode()` and `codecs.decode()` for base64 operations. Implement the function and ensure it is efficient and handles edge cases such as empty lists appropriately.","solution":"import struct import codecs def pack_encode_unpack(integers): # Step 1: Pack the list of integers into a binary format packed_data = struct.pack(f\'{len(integers)}i\', *integers) # Step 2: Encode the binary data to a base64 string encoded_data = codecs.encode(packed_data, \'base64\') # Step 3: Decode the base64 string back to binary format decoded_data = codecs.decode(encoded_data, \'base64\') # Step 4: Unpack the binary data back into a list of integers unpacked_data = struct.unpack(f\'{len(integers)}i\', decoded_data) return list(unpacked_data)"},{"question":"**Heap-based Priority Queue with Reprioritization** # Objective: Use the `heapq` module to implement a priority queue that supports adding tasks, removing tasks, and reprioritizing tasks efficiently. This exercise will enable you to demonstrate your understanding of heaps, priority queues, and efficient data structure manipulations. # Problem Statement: You need to implement a class `PriorityQueue` that supports the following functionalities: 1. **Add a task**: This function should add a new task with a specified priority to the queue. If the task already exists, update its priority. 2. **Remove a task**: This function should remove a specified task from the queue, if it exists. 3. **Pop the lowest priority task**: This function should remove and return the task with the lowest priority from the queue. # Requirements: 1. You need to use the `heapq` module to maintain the heap property. 2. Use a dictionary to keep track of tasks to efficiently support update and removal operations. # Function Specifications: 1. `__init__(self) -> None`: Initialize the priority queue. 2. `add_task(self, task: str, priority: int) -> None`: Add a new task or update the priority of an existing task. 3. `remove_task(self, task: str) -> None`: Remove an existing task. 4. `pop_task(self) -> str`: Pop and return the task with the lowest priority. # Constraints: 1. Strings `task` consist of lowercase English characters. 2. `priority` is an integer. 3. You can assume the number of operations will not exceed `10^4`. # Example: ```python pq = PriorityQueue() pq.add_task(\\"task1\\", 3) pq.add_task(\\"task2\\", 1) pq.add_task(\\"task3\\", 4) pq.add_task(\\"task4\\", 2) print(pq.pop_task()) # Output: \\"task2\\" pq.remove_task(\\"task1\\") pq.add_task(\\"task4\\", 0) print(pq.pop_task()) # Output: \\"task4\\" print(pq.pop_task()) # Output: \\"task3\\" print(pq.pop_task()) # Output: None, as the queue is empty ``` # Implementation: ```python from heapq import heappush, heappop import itertools class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task, priority=0): \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task): \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task return None # If the queue is empty # Example usage pq = PriorityQueue() pq.add_task(\\"task1\\", 3) pq.add_task(\\"task2\\", 1) pq.add_task(\\"task3\\", 4) pq.add_task(\\"task4\\", 2) print(pq.pop_task()) # Output: \\"task2\\" pq.remove_task(\\"task1\\") pq.add_task(\\"task4\\", 0) print(pq.pop_task()) # Output: \\"task4\\" print(pq.pop_task()) # Output: \\"task3\\" print(pq.pop_task()) # Output: None, as the queue is empty ``` # Note: Follow the implementation carefully and handle the cases when a task is removed or reprioritized. The solution must efficiently manage the priority queue operations using the provided data structures.","solution":"from heapq import heappush, heappop import itertools class PriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = itertools.count() # unique sequence count def add_task(self, task, priority=0): \'Add a new task or update the priority of an existing task\' if task in self.entry_finder: self.remove_task(task) count = next(self.counter) entry = [priority, count, task] self.entry_finder[task] = entry heappush(self.pq, entry) def remove_task(self, task): \'Mark an existing task as REMOVED. Raise KeyError if not found.\' entry = self.entry_finder.pop(task) entry[-1] = self.REMOVED def pop_task(self): \'Remove and return the lowest priority task. Raise KeyError if empty.\' while self.pq: priority, count, task = heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task return None # If the queue is empty"},{"question":"You are provided with a Python function that calculates the factorial of a number recursively. Your task is to write unit tests for this function using the utilities provided in the `test.support` module. Pay attention to edge cases, error handling, and performance aspects. Here is the function you need to test: ```python def factorial(n): if n <= 0: return 1 else: return n * factorial(n-1) ``` # Your task: 1. Write a test class `TestFactorial` using the `unittest` framework to test this function. 2. Use appropriate testing utilities from the `test.support` module to: - Ensure verbose output for detailed information about running tests. - Temporarily replace `sys.stdout` for capturing output. - Manage temporary files/directories if needed. - Handle potential warnings. # Constraints: - Your tests should cover all edge cases including: - Factorial of 0. - Factorial of positive integers. - Handling of invalid inputs like negative integers. - Use the `@unittest.expectedFailure` decorator for tests with known issues or expected failures. - Use the `@unittest.skip` decorator for tests that should be conditionally skipped. # Expected input and output formats: - No additional input is required beyond what is mentioned. - The output should be generated by running the test suite, which will confirm the pass/fail status of each test case. # Example Usage of Utilities: ```python import unittest from test import support class TestFactorial(unittest.TestCase): def setUp(self): # Setup code if necessary pass def tearDown(self): # Cleanup code if necessary pass def test_factorial_of_zero(self): self.assertEqual(factorial(0), 1) # Normal case def test_factorial_of_positive_int(self): self.assertEqual(factorial(5), 120) # Normal case @unittest.expectedFailure def test_factorial_of_negative_int(self): self.assertEqual(factorial(-1), 1) # Factorial of negative numbers not well-defined def test_factorial_output(self): with support.captured_stdout() as stdout: print(factorial(5)) self.assertEqual(stdout.getvalue().strip(), \\"120\\") @unittest.skip(\\"Skipping tests conditionally\\") def test_conditionally_skipped(self): self.assertTrue(False) if __name__ == \'__main__\': support.verbose = True unittest.main() ``` Write the full implementation of `TestFactorial` class and run the test suite to make sure all cases are handled appropriately.","solution":"def factorial(n): Returns the factorial of a given non-negative integer n. The factorial of 0 (0!) is 1 by definition. This function employs recursion. if n < 0: raise ValueError(\\"Factorial is not defined for negative integers\\") if n == 0: return 1 else: return n * factorial(n-1)"},{"question":"You have been given a dataset containing information about daily temperatures in a city for one year. Your task is to create a visual representation of this data using the seaborn library. Specifically, you need to demonstrate your understanding of seaborn\'s `cubehelix_palette` function to customize the color palette of a heatmap that will show temperature variations throughout the year. # Input - A CSV file named `temperature_data.csv` with the following columns: - `Day`: An integer representing the day of the year (1 to 365). - `Temperature`: A float representing the temperature in degrees Celsius for that day. # Output - A heatmap figure showing temperature variations throughout the year. # Constraints - You must use the seaborn library to create the heatmap. - Use the `cubehelix_palette` function to customize the color palette of the heatmap. # Instructions 1. Read the `temperature_data.csv` file into a pandas DataFrame. 2. Create a pivot table where the index is the day of the year and the values are the temperatures. 3. Generate a customized color palette using the `cubehelix_palette` function with the following parameters: - `n_colors=10`: Number of colors in the palette. - `start=0.5`: The range in the cubehelix space for the palette. - `rot=-1.5`: The number of rotations in the cubehelix space. - `dark=0.3`: The darkest color. - `light=0.8`: The lightest color. - `reverse=True`: Whether to reverse the direction of the color palette or not. 4. Create a heatmap using the seaborn heatmap function with the customized color palette. 5. Save the heatmap as an image file named `temperature_heatmap.png`. # Example Usage ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # 1. Read the data data = pd.read_csv(\'temperature_data.csv\') # 2. Create a pivot table pivot_data = data.pivot_table(index=\'Day\', values=\'Temperature\') # 3. Generate a customized color palette palette = sns.cubehelix_palette(n_colors=10, start=0.5, rot=-1.5, dark=0.3, light=0.8, reverse=True) # 4. Create the heatmap sns.heatmap(pivot_data, cmap=palette) # 5. Save the heatmap plt.savefig(\'temperature_heatmap.png\') ``` Ensure your code is clean, well-commented, and follows Python best practices.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_temperature_heatmap(file_path): Reads a CSV file containing daily temperature data and creates a heatmap. Parameters: file_path (str): The path to the CSV file Returns: None: The function saves the heatmap as `temperature_heatmap.png` # 1. Read the data data = pd.read_csv(file_path) # 2. Create a pivot table pivot_data = data.pivot_table(index=\'Day\', values=\'Temperature\') # 3. Generate a customized color palette palette = sns.cubehelix_palette(n_colors=10, start=0.5, rot=-1.5, dark=0.3, light=0.8, reverse=True) # 4. Create the heatmap sns.heatmap(pivot_data, cmap=palette, cbar_kws={\'label\': \'Temperature (°C)\'}) # 5. Save the heatmap plt.title(\\"Temperature Variations Throughout the Year\\") plt.savefig(\'temperature_heatmap.png\') plt.close()"},{"question":"Objective: The goal of this assessment is to test your understanding of the **webbrowser** module in Python and your ability to implement a function that intelligently opens URLs in a web browser based on specific conditions. Problem Statement: You are tasked with implementing a function `smart_open(urls, use_new_window=False, preferred_browser=\'\')` that takes a list of URLs and opens them in a web browser according to the following rules: 1. **Parameters:** - `urls`: A list of strings representing the URLs to be opened. - `use_new_window` (optional): A boolean flag indicating whether to open each URL in a new browser window. If `False`, open in new tabs instead. Defaults to `False`. - `preferred_browser` (optional): A string specifying the preferred browser to use. If not provided or an empty string, use the system\'s default browser. 2. **Behavior:** - If `use_new_window` is `True`, each URL in the `urls` list should be opened in a new browser window. - If `use_new_window` is `False`, each URL in the `urls` list should be opened in a new browser tab. - If `preferred_browser` is specified, use that browser to open the URLs. If the specified browser is not installed or cannot be found, raise the `webbrowser.Error` exception with an appropriate message. Constraints: - The number of URLs in the `urls` list will not exceed 100. - The function should handle invalid URLs gracefully by printing an error message and continuing with the next URL. Implementation Requirements: - Make use of the **webbrowser** module functions like `webbrowser.open`, `webbrowser.get`, etc. - Handle exceptions appropriately. Function Signature: ```python def smart_open(urls, use_new_window=False, preferred_browser=\'\'): pass ``` Example: ```python # Example 1: urls = [\\"https://www.python.org\\", \\"https://www.github.com\\"] smart_open(urls, use_new_window=False, preferred_browser=\'firefox\') # Example 2: urls = [\\"https://www.example.com\\", \\"invalid-url\\"] smart_open(urls, use_new_window=True) # Example 3: urls = [\\"https://www.google.com\\"] smart_open(urls, preferred_browser=\'nonexistent-browser\') # Expected Output: # - Example 1 should open the URLs in new tabs of the Firefox browser. # - Example 2 should open the first URL in a new window and print an error message for the invalid URL. # - Example 3 should raise webbrowser.Error indicating the browser is not found. ``` Notes: - You may use additional helper functions as needed. - Ensure that your implementation adheres to the provided constraints and handles edge cases appropriately.","solution":"import webbrowser def smart_open(urls, use_new_window=False, preferred_browser=\'\'): Opens the given list of URLs in a web browser based on specified conditions. Parameters: - urls (list of str): List of URLs to be opened. - use_new_window (bool): Open URLs in a new window if True, otherwise in new tabs. Defaults to False. - preferred_browser (str): Preferred browser to use. If empty, uses the system\'s default browser. try: browser = webbrowser.get(preferred_browser) if preferred_browser else webbrowser.get() except webbrowser.Error as e: raise webbrowser.Error(f\\"Preferred browser \'{preferred_browser}\' not found. {e}\\") for url in urls: try: if use_new_window: browser.open_new(url) else: browser.open_new_tab(url) except Exception as e: print(f\\"Error opening {url}: {e}\\")"},{"question":"**Advanced Function Signature Introspection** Using Python’s `inspect` module, implement a function called `introspect_function`. This function will take another function as an input and return a detailed dictionary containing information about the function\'s signature and parameters. # Function Specification **Function Name**: `introspect_function` **Parameters**: - `func` (type: `callable`): A Python function to introspect. **Return Type**: `dict` # Expected Function Behavior The `introspect_function` should return a dictionary with the following keys: - `name`: Name of the function. - `parameters`: A dictionary where keys are the parameter names and values are dictionaries with the detailed information about each parameter: - `kind`: The kind of the parameter (one of `POSITIONAL_ONLY`, `POSITIONAL_OR_KEYWORD`, `VAR_POSITIONAL`, `KEYWORD_ONLY`, `VAR_KEYWORD`). - `default`: The default value of the parameter (or `None` if it doesn\'t have a default value). - `annotation`: The annotation of the parameter (or `None` if it doesn\'t have an annotation). - `return_annotation`: The return annotation of the function (or `None` if it doesn’t have a return annotation). # Example ```python def example_function(a: int, b: float = 3.14, *args, c, d=None, **kwargs) -> bool: pass expected_output = { \\"name\\": \\"example_function\\", \\"parameters\\": { \\"a\\": {\\"kind\\": \\"POSITIONAL_OR_KEYWORD\\", \\"default\\": None, \\"annotation\\": int}, \\"b\\": {\\"kind\\": \\"POSITIONAL_OR_KEYWORD\\", \\"default\\": 3.14, \\"annotation\\": float}, \\"args\\": {\\"kind\\": \\"VAR_POSITIONAL\\", \\"default\\": None, \\"annotation\\": None}, \\"c\\": {\\"kind\\": \\"KEYWORD_ONLY\\", \\"default\\": None, \\"annotation\\": None}, \\"d\\": {\\"kind\\": \\"KEYWORD_ONLY\\", \\"default\\": None, \\"annotation\\": None}, \\"kwargs\\": {\\"kind\\": \\"VAR_KEYWORD\\", \\"default\\": None, \\"annotation\\": None}, }, \\"return_annotation\\": bool } assert introspect_function(example_function) == expected_output ``` # Constraints - The function should gracefully handle cases where a function has no annotations, defaults, or return annotations. - You should utilize the `inspect` module extensively for extracting function signature and parameter details. # Solution Template ```python import inspect from typing import Callable, Dict, Any def introspect_function(func: Callable) -> Dict[str, Any]: # Your implementation here signature = inspect.signature(func) result = { \\"name\\": func.__name__, \\"parameters\\": {}, \\"return_annotation\\": None } for name, param in signature.parameters.items(): result[\\"parameters\\"][name] = { \\"kind\\": param.kind.name, \\"default\\": param.default if param.default is not param.empty else None, \\"annotation\\": param.annotation if param.annotation is not param.empty else None } result[\\"return_annotation\\"] = signature.return_annotation if signature.return_annotation is not inspect.Signature.empty else None return result ``` Good luck!","solution":"import inspect from typing import Callable, Dict, Any def introspect_function(func: Callable) -> Dict[str, Any]: Introspects the provided function and returns detailed information about its signature and parameters. Parameters: - func (Callable): The function to be introspected. Returns: - Dict[str, Any]: A dictionary containing the name, parameters, and return annotation of the function. signature = inspect.signature(func) result = { \\"name\\": func.__name__, \\"parameters\\": {}, \\"return_annotation\\": None } for name, param in signature.parameters.items(): result[\\"parameters\\"][name] = { \\"kind\\": param.kind.name, \\"default\\": param.default if param.default is not param.empty else None, \\"annotation\\": param.annotation if param.annotation is not param.empty else None } result[\\"return_annotation\\"] = signature.return_annotation if signature.return_annotation is not inspect.Signature.empty else None return result"},{"question":"# Python Coding Assessment **Problem Statement:** You are tasked with designing a scheduling tool that helps users visualize their plans for a given month, integrating events on specific days into a textual calendar format. The tool should allow users to add events on specific dates and then print a formatted calendar displaying those events. **Requirements:** 1. **Class Definition:** Define a class named `EventCalendar` which inherits from `calendar.TextCalendar`. 2. **Methods to Implement:** - **`__init__(self, firstweekday=0)`** - Initializes the `EventCalendar` object with the given first day of the week. - **`add_event(self, year, month, day, event)`** - Adds an event to a specific date. Multiple events can be added to the same date. - Parameters: - `year`: Year of the event (integer) - `month`: Month of the event (integer, 1-12) - `day`: Day of the event (integer, 1-31) - `event`: A string description of the event - **`formatmonth(self, theyear, themonth, w=0, l=0)`** - Overrides the `formatmonth` method of `TextCalendar`, formatting the month into a multi-line string. - Each day\'s block in the calendar should list the events for that day (if any). - Parameters: - `theyear`: Target year (integer) - `themonth`: Target month (integer, 1-12) - `w`: Width of each date column (integer, default 0) - `l`: Number of lines each week should use (integer, default 0) 3. **Output:** The method `formatmonth` should return a string where each day is correctly aligned as per the width (`w`) and lines per week (`l`) while also including event descriptions for each day that has events. **Constraints:** - The `event` strings can be of varied lengths. - Ensure that adding events and formatting the calendar maintains clear readability. - Handle invalid dates with appropriate error messages (e.g., `ValueError` if the day exceeds the appropriate number of days in a given month). - You can assume there is no need for multi-threading or handling of concurrent modifications. **Example:** ```python # Create an EventCalendar object calendar_obj = EventCalendar() # Add events calendar_obj.add_event(2023, 10, 25, \'Project Deadline\') calendar_obj.add_event(2023, 10, 15, \'Team Meeting\') calendar_obj.add_event(2023, 10, 25, \'Doctor Appointment\') # Print the formatted calendar for October 2023 print(calendar_obj.formatmonth(2023, 10)) ``` **Expected Output:** ``` October 2023 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Team Meeting 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Project Deadline Doctor Appointment 30 31 ``` Your task is to implement the `EventCalendar` class according to the specified requirements and constraints.","solution":"import calendar class EventCalendar(calendar.TextCalendar): def __init__(self, firstweekday=0): super().__init__(firstweekday) self.events = {} def add_event(self, year, month, day, event): if year < 1 or month < 1 or month > 12 or day < 1: raise ValueError(\\"Invalid date\\") try: valid_date = calendar.monthrange(year, month) if day > valid_date[1]: raise ValueError(\\"Invalid day, exceeds days in month\\") except calendar.IllegalMonthError: raise ValueError(\\"Invalid month\\") date_key = (year, month, day) if date_key in self.events: self.events[date_key].append(event) else: self.events[date_key] = [event] def formatmonth(self, theyear, themonth, w=0, l=0): events = self.events cal = super().formatmonth(theyear, themonth, w, l) lines = cal.split(\'n\') weeks = lines[2:] for i, week in enumerate(weeks): if week.strip(): days = week.split() formatted_week = [] for day in days: try: day_int = int(day) date_key = (theyear, themonth, day_int) if date_key in events: event_list = events[date_key] day_with_events = f\\"{day}n\\" + \\"n\\".join(event_list) formatted_week.append(day_with_events) else: formatted_week.append(day) except ValueError: formatted_week.append(day) weeks[i] = formatted_week lines[2 + i] = \' \'.join(\'%-2s\' % day for day in weeks[i]) return \\"n\\".join(lines)"},{"question":"**Password Change Simulation** You are required to implement a simulation of a password change system in Python using the `getpass` module. This system will prompt the user for their current password and a new password. The new password must be confirmed by entering it twice. Your program should ensure that the new passwords match and are different from the current password. # Requirements 1. The program should define a function `change_password(current_password: str) -> str` that: - Prompts the user for their current password using `getpass.getpass()`. - Verifies that the entered current password matches the `current_password` argument. - Prompts the user for a new password and then confirms it by asking the user to re-enter the new password. - Checks that the new password and the confirmation match and that the new password is different from the current password. - Returns the new password if the change is successful. - Raises an appropriate error with a clear message if any of the conditions are not met (e.g., mismatched passwords, the same as current password). 2. Your solution should handle potential exceptions or warnings gracefully and provide meaningful error messages to the user. # Input and Output Formats - **Input:** The current password is provided as a string argument to the function. - **Output:** The function returns the new password as a string if the change is successful. # Constraints - The new password should be at least 8 characters long. - The current password and the new password cannot be the same. # Performance Requirements - The solution should handle the input securely without exposing the passwords during the process. - Ensure the program runs efficiently with a prompt user interaction workflow. # Example Usage ```python def change_password(current_password: str) -> str: # Your implementation here pass # Example usage try: current_pass = \\"old_password_123\\" new_password = change_password(current_pass) print(\\"Password changed successfully.\\") except Exception as e: print(f\\"Error: {e}\\") ``` Make sure to thoroughly test your solution with different scenarios to verify its correctness and robustness.","solution":"import getpass def change_password(current_password: str) -> str: Prompts the user to change the password. It ensures that the current password matches the user\'s input, the new password is at least 8 characters long, and the new password is not the same as the current password. Args: current_password (str): The user\'s current password. Returns: str: The new password if the change is successful. Raises: ValueError: If the current password does not match or the new password does not meet the criteria. # Prompt user for the current password entered_current_password = getpass.getpass(\\"Enter your current password: \\") if entered_current_password != current_password: raise ValueError(\\"Entered current password is incorrect.\\") # Prompt user for the new password and confirmation new_password = getpass.getpass(\\"Enter new password: \\") confirm_password = getpass.getpass(\\"Confirm new password: \\") if new_password != confirm_password: raise ValueError(\\"New passwords do not match.\\") if new_password == current_password: raise ValueError(\\"New password cannot be the same as the current password.\\") if len(new_password) < 8: raise ValueError(\\"New password must be at least 8 characters long.\\") return new_password"},{"question":"You are required to leverage the `pwd` module to develop a set of functions that will query and analyze user information from the Unix user account and password database. # Function 1: `get_user_info(username: str) -> dict` Implement a function that takes a username as input and returns a dictionary with the user\'s detailed information. The keys of the dictionary should correspond to: - \\"username\\" - \\"encrypted_password\\" - \\"user_id\\" - \\"group_id\\" - \\"comment\\" - \\"home_directory\\" - \\"shell\\" Example: ```python get_user_info(\'root\') # Should return something like: # { # \\"username\\": \\"root\\", # \\"encrypted_password\\": \\"x\\", # \\"user_id\\": 0, # \\"group_id\\": 0, # \\"comment\\": \\"root\\", # \\"home_directory\\": \\"/root\\", # \\"shell\\": \\"/bin/bash\\" # } ``` # Function 2: `get_all_usernames() -> list` Implement a function that returns a list of all usernames present in the password database. Example: ```python get_all_usernames() # Should return something like: # [\'root\', \'user1\', \'user2\', ...] ``` # Function 3: `find_users_by_shell(shell: str) -> list` Implement a function that takes a shell (e.g., /bin/bash) as input and returns a list of usernames who use that particular shell. Example: ```python find_users_by_shell(\'/bin/bash\') # Should return something like: # [\'root\', \'user1\', \'user2\'] ``` # Constraints 1. You should handle any potential errors gracefully. If a user does not exist, `get_user_info` should return `None`. 2. Assume the functions will be run on a Unix-like system where the `pwd` module is supported. # Testing - Test the functions with real user data present on your system. - Ensure your functions are efficient and handle large data sets appropriately.","solution":"import pwd def get_user_info(username: str) -> dict: try: entry = pwd.getpwnam(username) return { \\"username\\": entry.pw_name, \\"encrypted_password\\": entry.pw_passwd, \\"user_id\\": entry.pw_uid, \\"group_id\\": entry.pw_gid, \\"comment\\": entry.pw_gecos, \\"home_directory\\": entry.pw_dir, \\"shell\\": entry.pw_shell } except KeyError: return None def get_all_usernames() -> list: return [entry.pw_name for entry in pwd.getpwall()] def find_users_by_shell(shell: str) -> list: return [entry.pw_name for entry in pwd.getpwall() if entry.pw_shell == shell]"},{"question":"# Custom Scikit-Learn Estimator for Polynomial Regression **Objective:** Create a custom Scikit-Learn compatible estimator for Polynomial Regression. Ensure that the estimator adheres to the scikit-learn conventions and can be used seamlessly within the scikit-learn ecosystem, including pipelines and grid searches. # Problem Statement: You are required to implement a custom Polynomial Regression estimator from scratch, making it compatible with the scikit-learn framework. Your estimator should inherit from the necessary scikit-learn base classes and mixins and should provide functionality similar to the provided linear models in scikit-learn. # Specifications: 1. **Class Name**: `PolynomialRegression` 2. **Base Classes**: Should inherit from `BaseEstimator` and `RegressorMixin`. 3. **Parameters**: - `degree` (int): The degree of the polynomial features (default = 2). - `include_bias` (bool): Whether to include a bias term (an intercept) in the model (default = True). 4. **Methods**: - `__init__(self, degree=2, include_bias=True)`: Initialize the estimator parameters. - `fit(self, X, y)`: Fit the model to the training data. - `predict(self, X)`: Predict using the linear model based on polynomial features. - `set_params(self, **params)`: Set the parameters of this estimator. - `get_params(self, deep=True)`: Get the parameters of this estimator. # Input and Output Formats: - **Input**: - `X` (array-like of shape (n_samples, n_features)): Training or test data. - `y` (array-like of shape (n_samples, )): Training target values. - **Output**: - `fit` method returns `self` (the estimator instance). - `predict` method returns an array-like of shape (n_samples, ) with the predicted values. # Performance Requirements: - Your solution should ensure compatibility with scikit-learn’s pipelines and grid searches. - Proper error handling for input validation should be implemented. - Ensure that all hyperparameters accessible in `__init__` are mutable and can be set appropriately. # Constraints: - Do not use existing polynomial regression or feature generation utilities from scikit-learn. - Ensure compatibility with standard Scikit-learn methods for setting and retrieving parameters. # Example Usage: ```python from sklearn.datasets import make_regression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error # Custom Estimator from custom_estimators import PolynomialRegression # assume this is your file # Generate a dataset X, y = make_regression(n_samples=100, n_features=1, noise=0.1) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Instantiate and train your custom estimator poly_reg = PolynomialRegression(degree=3, include_bias=True) poly_reg.fit(X_train, y_train) # Predict and evaluate predictions = poly_reg.predict(X_test) mse = mean_squared_error(y_test, predictions) print(f\\"Mean Squared Error: {mse}\\") ``` # Submission Submit your implementation as a Python file named `custom_estimators.py`.","solution":"import numpy as np from sklearn.base import BaseEstimator, RegressorMixin from sklearn.linear_model import LinearRegression class PolynomialRegression(BaseEstimator, RegressorMixin): def __init__(self, degree=2, include_bias=True): self.degree = degree self.include_bias = include_bias self.model = LinearRegression() def fit(self, X, y): X_poly = self._polynomial_features(X) self.model.fit(X_poly, y) return self def predict(self, X): X_poly = self._polynomial_features(X) return self.model.predict(X_poly) def _polynomial_features(self, X): n_samples, n_features = X.shape X_poly = np.ones((n_samples, 1)) if self.include_bias else np.empty((n_samples, 0)) for degree in range(1, self.degree + 1): for feature_combination in np.ndindex(*(n_features,) * degree): new_feature = np.prod(X[:, feature_combination], axis=1, keepdims=True) X_poly = np.hstack((X_poly, new_feature)) return X_poly def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self def get_params(self, deep=True): return {\\"degree\\": self.degree, \\"include_bias\\": self.include_bias}"},{"question":"**Email Serialization Task** You are tasked with implementing a function that serializes an email message object into a binary format using the `BytesGenerator` class from the `email.generator` module in Python. The function should handle MIME messages, honor specific policy settings for encoding, and optionally include Unix mailbox format headers. # Specifications Your function should be named `serialize_email`. Input: - `message`: An instance of `email.message.EmailMessage` that needs to be serialized. - `unixfrom`: A boolean indicating whether to include the Unix mailbox format envelope header (default is False). - `linesep`: A string to be used as the line separator in the output (default is `None`, which means use the default from the policy). - `policy`: An instance of `email.policy.Policy` controlling the message generation (default is `None`). Output: - A `bytes` object containing the serialized email message. # Constraints: - The function should handle email message objects with both text and binary content. - Use the `BytesGenerator` class for serialization. - Ensure proper handling of Content-Transfer-Encoding for both \\"7bit\\" and \\"8bit\\" encoding policies. - If the message contains headers with non-ASCII characters, ensure they are properly encoded. # Function Signature: ```python def serialize_email(message: \'EmailMessage\', unixfrom: bool = False, linesep: str = None, policy: \'Policy\' = None) -> bytes: pass ``` # Example: ```python from email.policy import default from email.message import EmailMessage # Create a sample email message msg = EmailMessage() msg[\'From\'] = \'test@example.com\' msg[\'To\'] = \'recipient@example.com\' msg[\'Subject\'] = \'Test Email\' msg.set_content(\'This is a test email message.\') # Serialize the message serialized_msg = serialize_email(message=msg, unixfrom=True, linesep=\'n\', policy=default) # Print the serialized message (for binary output, decode first if needed) print(serialized_msg.decode(\'utf-8\')) ``` Make sure your implementation is efficient and follows the described behavior strictly. Good luck!","solution":"from email.policy import Policy from email.message import EmailMessage from email.generator import BytesGenerator import io def serialize_email(message: EmailMessage, unixfrom: bool = False, linesep: str = None, policy: Policy = None) -> bytes: Serializes an email message object into a binary format using BytesGenerator. Parameters: message (EmailMessage): The email message to be serialized. unixfrom (bool): If True, includes the Unix mailbox format envelope header. linesep (str): The line separator to be used in the output. policy (Policy): The policy controlling the message generation. Returns: bytes: The serialized email message. buffer = io.BytesIO() generator = BytesGenerator(buffer, policy=policy) generator.flatten(message, unixfrom=unixfrom, linesep=linesep) return buffer.getvalue()"},{"question":"# HTML Content Extractor **Objective:** You are required to write a function `extract_html_content` using Python\'s `html.parser` module. This function will parse an HTML string and extract specific content information which can be useful for SEO (Search Engine Optimization) purposes. **Task:** Create a class `SEOHTMLParser` that inherits from `html.parser.HTMLParser`. Implement the following functionalities in this class to achieve the desired content extraction: 1. **Meta Description Extraction:** - Extract the content of the `<meta name=\\"description\\" content=\\"...\\">` tag. 2. **H1 Header Extraction:** - Extract all the content within `<h1>...</h1>` tags. 3. **External Link Extraction:** - Extract URLs of all external links (i.e., URLs that start with \'http://\' or \'https://\') within `<a>` tags. Implement the class such that it overrides necessary methods from `html.parser.HTMLParser` to accomplish these tasks. **Function Signature:** ```python from html.parser import HTMLParser class SEOHTMLParser(HTMLParser): def __init__(self): super().__init__() # Initialize required attributes here. # Override necessary handler methods here. def extract_html_content(html: str) -> dict: Parses the given HTML string and extracts the meta description, all H1 headers, and external links. :param html: HTML content as a string. :return: A dictionary with keys \'meta_description\', \'h1_headers\', \'external_links\' and their respective extracted values. parser = SEOHTMLParser() parser.feed(html) return { \\"meta_description\\": parser.meta_description, \\"h1_headers\\": parser.h1_headers, \\"external_links\\": parser.external_links } ``` **Input:** - `html`: A string containing HTML content (0 ≤ len(html) ≤ 10^6). **Output:** - A dictionary containing: - `meta_description`: A string representing the content of the meta description tag. - `h1_headers`: A list of strings representing the content of each `<h1>` tag. - `external_links`: A list of strings representing all the extracted external URLs within `<a>` tags. **Constraints:** - The HTML content may be poorly formatted or invalid. - The parser should handle invalid HTML gracefully and still extract the available data. **Example:** ```python html_content = \'\'\' <!DOCTYPE html> <html> <head> <meta name=\\"description\\" content=\\"This is a test HTML.\\"> <title>Test HTML</title> </head> <body> <h1>Welcome to the Test</h1> <h1>SEO Analysis</h1> <p>Visit our <a href=\\"http://example.com\\">example site</a> or our <a href=\\"https://another-example.com\\">another example site</a>. </p> </body> </html> \'\'\' result = extract_html_content(html_content) assert result == { \\"meta_description\\": \\"This is a test HTML.\\", \\"h1_headers\\": [\\"Welcome to the Test\\", \\"SEO Analysis\\"], \\"external_links\\": [\\"http://example.com\\", \\"https://another-example.com\\"] } ``` **Notes:** - Ensure that your `SEOHTMLParser` class handles all the necessary parsing and extraction accurately. - You might have to override methods like `handle_starttag`, `handle_data`, and others as needed.","solution":"from html.parser import HTMLParser class SEOHTMLParser(HTMLParser): def __init__(self): super().__init__() self.meta_description = \\"\\" self.h1_headers = [] self.external_links = [] self._capture_h1 = False def handle_starttag(self, tag, attrs): attrs_dict = dict(attrs) if tag == \'meta\' and attrs_dict.get(\\"name\\") == \\"description\\": self.meta_description = attrs_dict.get(\\"content\\", \\"\\") elif tag == \'h1\': self._capture_h1 = True elif tag == \'a\' and (\'http://\' in attrs_dict.get(\'href\', \'\') or \'https://\' in attrs_dict.get(\'href\', \'\')): self.external_links.append(attrs_dict[\'href\']) def handle_endtag(self, tag): if tag == \'h1\': self._capture_h1 = False def handle_data(self, data): if self._capture_h1: self.h1_headers.append(data) def extract_html_content(html: str) -> dict: parser = SEOHTMLParser() parser.feed(html) return { \\"meta_description\\": parser.meta_description, \\"h1_headers\\": parser.h1_headers, \\"external_links\\": parser.external_links }"},{"question":"Numerical Solution of Differential Equations using Special Functions Objective You are to implement a function that numerically solves a given differential equation using PyTorch\'s special functions. The differential equation we will focus on is the modified Bessel equation of the first kind. Description The modified Bessel equation of the first kind, ( I_n(x) ), commonly appears in problems involving cylindrical symmetry, such as heat conduction in cylindrical objects or wave propagation. The Bessel functions of the first kind for order 0 and 1 are represented as `i0(x)` and `i1(x)` in the `torch.special` module. Your task is to implement a function `bessel_solution(N, x_vals)` that computes the modified Bessel functions of the first kind for orders 0 and 1, then combines these solutions to produce a final result. Function Signature ```python import torch def bessel_solution(N: int, x_vals: torch.Tensor) -> torch.Tensor: Numerically solve the modified Bessel equation of the first kind for given N and x values. Args: - N (int): The order of the Bessel function. - x_vals (torch.Tensor): A 1D tensor of values at which to evaluate the Bessel functions. Returns: - torch.Tensor: A 1D tensor of calculated results using the combination of Bessel functions. pass ``` Input - `N`: An integer representing the order of the Bessel function (0 or 1 only). - `x_vals`: A 1D tensor with `n` floating-point values where the Bessel functions should be evaluated. Output - The function should return a 1D tensor of the same size as `x_vals` with the calculated results. Constraints - The order `N` will be either 0 or 1. - The tensor `x_vals` can have up to 100,000 values to test the performance of your implementation. Example ```python # Example input N = 0 x_vals = torch.tensor([0.5, 1.0, 2.0, 3.0, 4.0]) # Expected output result = bessel_solution(N, x_vals) print(result) # A tensor output computing the Bessel function of the first kind for order 0 evaluated at the given x values ``` # Notes - You should use the functions `torch.special.i0` and `torch.special.i1` to compute the solution. - Test the performance of your code to ensure it scales efficiently with the number of input values in `x_vals`.","solution":"import torch def bessel_solution(N: int, x_vals: torch.Tensor) -> torch.Tensor: Numerically solve the modified Bessel equation of the first kind for given N and x values. Args: - N (int): The order of the Bessel function (0 or 1). - x_vals (torch.Tensor): A 1D tensor of values at which to evaluate the Bessel functions. Returns: - torch.Tensor: A 1D tensor of calculated results using the combination of Bessel functions. if N == 0: return torch.special.i0(x_vals) elif N == 1: return torch.special.i1(x_vals) else: raise ValueError(\\"N must be either 0 or 1\\")"},{"question":"You are given a dataset containing information about passengers on the Titanic. Your task is to create a series of visualizations using Seaborn to analyze the survival rates of passengers based on different factors. Dataset Use the built-in Seaborn Titanic dataset. Load the dataset with the following code: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` Tasks 1. **Set a Visual Theme**: - Set the theme for the Seaborn visualizations to `\\"darkgrid\\"`. 2. **Visualize Class Distribution**: - Create a count plot to show the distribution of passengers based on their travel class (`\\"class\\"`). 3. **Survival Count by Class**: - Create a count plot to show the number of passengers who survived (`\\"survived\\"`) grouped by their travel class (`\\"class\\"`). 4. **Percentage Survival by Class and Embark Town**: - Create a count plot to show the percentages of passengers who survived, grouped by travel class (`\\"class\\"`) and the town they embarked from (`\\"embark_town\\"`). Implementation Requirements - Ensure that the plots are well-labeled and easy to interpret. - Use appropriate titles and axis labels for each plot. - Use the `stat=\\"percent\\"` parameter to normalize the counts to percentages where required. Code Structure ```python # Step 1: Set the theme import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") # Set Seaborn theme sns.set_theme(style=\\"darkgrid\\") # Step 2: Visualize class distribution ax = sns.countplot(titanic, x=\\"class\\") ax.set_title(\\"Class Distribution of Titanic Passengers\\") # Step 3: Survival count by class ax = sns.countplot(titanic, x=\\"class\\", hue=\\"survived\\") ax.set_title(\\"Survival Count by Class\\") # Step 4: Percentage survival by class and embark town ax = sns.countplot(titanic, x=\\"class\\", hue=\\"survived\\", dodge=True, stat=\\"percent\\") ax.set_title(\\"Percentage Survival by Class and Embark Town\\") ax.set_xlabel(\\"Travel Class\\") ax.set_ylabel(\\"Percentage of Passengers\\") ``` Input - You do not need any external input from the user. Output - The function should produce three visualizations based on the requirements. Constraints - Use Seaborn package for data visualization. - Ensure the dataset is loaded using Seaborn’s `load_dataset` function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_titanic_data(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the theme for the Seaborn visualizations to \\"darkgrid\\" sns.set_theme(style=\\"darkgrid\\") # Step 2: Visualize class distribution plt.figure(figsize=(10, 6)) ax1 = sns.countplot(data=titanic, x=\\"class\\") ax1.set_title(\\"Class Distribution of Titanic Passengers\\") ax1.set_xlabel(\\"Travel Class\\") ax1.set_ylabel(\\"Number of Passengers\\") plt.show() # Step 3: Survival count by class plt.figure(figsize=(10, 6)) ax2 = sns.countplot(data=titanic, x=\\"class\\", hue=\\"survived\\") ax2.set_title(\\"Survival Count by Class\\") ax2.set_xlabel(\\"Travel Class\\") ax2.set_ylabel(\\"Number of Passengers\\") plt.legend(title=\\"Survived\\", loc=\'upper right\') plt.show() # Step 4: Percentage survival by class and embark town plt.figure(figsize=(15, 8)) ax3 = sns.catplot(data=titanic, x=\\"class\\", hue=\\"survived\\", col=\\"embark_town\\", kind=\\"count\\", height=4, aspect=0.7, saturation=0.75, stat=\\"percent\\") ax3.fig.suptitle(\\"Percentage Survival by Class and Embark Town\\", y=1.05) ax3.set_axis_labels(\\"Travel Class\\", \\"Percentage of Passengers\\") ax3.set_titles(col_template=\\"{col_name}\\") plt.show() # Call the function to generate the visualizations visualize_titanic_data()"},{"question":"Question # Objective Implement a Python C extension module named `matrix` that defines a new type `Matrix`. This new type should handle basic matrix operations. # Specifications 1. **Matrix Type**: - The `Matrix` type should encapsulate a two-dimensional array of integers. - It should have attributes for the number of rows (`nrows`) and columns (`ncols`). 2. **Initialization**: - The `Matrix` type should provide an `__init__()` method which initializes the matrix with given dimensions (`nrows`, `ncols`) and an optional initial value for all elements. 3. **Attributes**: - `Matrix` should have read-only `nrows` and `ncols` attributes. 4. **Methods**: - `Matrix.add(matrix)`: Adds the corresponding elements of two matrices and returns a new matrix. - `Matrix.scale(scalar)`: Multiplies all elements of the matrix by the given scalar and returns a new matrix. - `Matrix.transpose()`: Returns the transpose of the matrix. # Constraints - Only integer elements should be allowed in the matrix. - If matrix addition is attempted with matrices of different dimensions, raise a `ValueError`. # Example ```python import matrix m1 = matrix.Matrix(2, 3, 1) # Creates a 2x3 matrix with all elements set to 1 m2 = matrix.Matrix(2, 3, 2) # Creates a 2x3 matrix with all elements set to 2 # Adding matrices m3 = m1.add(m2) # Result: [[3, 3, 3], [3, 3, 3]] # Scaling matrix m4 = m1.scale(3) # Result: [[3, 3, 3], [3, 3, 3]] # Transposing matrix m5 = m1.transpose() # Result: [[1, 1], [1, 1], [1, 1]] ``` # Performance Requirements - Ensure efficient memory management and avoid memory leaks. - Implement garbage collection support where necessary. # Deliverable Submit a `matrix.c` file which implements the `Matrix` type as described above. Also, provide a `setup.py` script to compile the C extension. # Guidance You may refer to the provided documentation section on creating new types, adding attributes, implementing methods, and ensuring proper memory management for garbage collection.","solution":"class Matrix: def __init__(self, nrows, ncols, value=0): self._nrows = nrows self._ncols = ncols self._data = [[value for _ in range(ncols)] for _ in range(nrows)] @property def nrows(self): return self._nrows @property def ncols(self): return self._ncols def add(self, other): if self.nrows != other.nrows or self.ncols != other.ncols: raise ValueError(\\"Matrices dimensions do not match\\") result = Matrix(self.nrows, self.ncols) for i in range(self.nrows): for j in range(self.ncols): result._data[i][j] = self._data[i][j] + other._data[i][j] return result def scale(self, scalar): result = Matrix(self.nrows, self.ncols) for i in range(self.nrows): for j in range(self.ncols): result._data[i][j] = self._data[i][j] * scalar return result def transpose(self): result = Matrix(self.ncols, self.nrows) for i in range(self.nrows): for j in range(self.ncols): result._data[j][i] = self._data[i][j] return result def __repr__(self): return f\\"Matrix({self.nrows}, {self.ncols}, {self._data})\\""},{"question":"Python Coding Assessment Question # Objective To assess the understanding of the turtle graphics module in Python by implementing a graphical representation using multiple turtles. # Problem Statement Create a Python script using the turtle graphics module to draw a complex scene. The scene should include the following elements: 1. A house with a triangular roof. 2. A tree beside the house with a brown trunk and green leaves. 3. A sun at the top corner of the screen. 4. A road or path leading up to the house. # Requirements - Define a function `draw_house(t)`, where `t` is a turtle object, to draw the house with a square base and a triangular roof. - Define a function `draw_tree(t)`, where `t` is a turtle object, to draw the tree with a brown trunk and green foliage. - Define a function `draw_sun(t)`, where `t` is a turtle object, to draw the sun as a circle with rays. - Define a function `draw_road(t)`, where `t` is a turtle object, to draw a road leading to the house. - Use at least four different turtle objects in your scene. - Ensure that each function resets the turtle\'s position to a logical start point after completing its drawing to avoid interference with other parts of the scene. # Constraints - You may not use any additional libraries for drawing other than `turtle`. - The code should be well-organized, with clear function definitions for each component of the scene. - Comment your code appropriately to explain your logic. # Example Usage ```python import turtle # Create screen and turtle objects screen = turtle.Screen() screen.bgcolor(\\"lightblue\\") # Create turtle objects house_turtle = turtle.Turtle() tree_turtle = turtle.Turtle() sun_turtle = turtle.Turtle() road_turtle = turtle.Turtle() # Drawing functions def draw_house(t): # Implementation of the function pass def draw_tree(t): # Implementation of the function pass def draw_sun(t): # Implementation of the function pass def draw_road(t): # Implementation of the function pass # Draw the scene draw_house(house_turtle) draw_tree(tree_turtle) draw_sun(sun_turtle) draw_road(road_turtle) # Hide the turtles house_turtle.hideturtle() tree_turtle.hideturtle() sun_turtle.hideturtle() road_turtle.hideturtle() # Finish drawing turtle.done() ``` # Evaluation Criteria - Correctness: Does the solution correctly implement the four required functions to draw the specified elements? - Code Organization: Is the code well-organized into functions with appropriate logical structure? - Use of Turtle Graphics: Does the solution effectively use the turtle graphics module and its methods? - Commenting and Readability: Is the code commented appropriately and easy to read?","solution":"import turtle def draw_house(t): t.penup() t.goto(-100, -100) t.pendown() t.color(\\"blue\\") t.begin_fill() for _ in range(4): t.forward(200) t.left(90) t.end_fill() t.left(90) t.forward(200) t.right(90) t.color(\\"red\\") t.begin_fill() t.forward(100) t.right(120) t.forward(200) t.right(120) t.forward(200) t.right(120) t.forward(100) t.end_fill() def draw_tree(t): t.penup() t.goto(150, -100) t.pendown() t.color(\\"brown\\") t.begin_fill() for _ in range(2): t.forward(20) t.left(90) t.forward(60) t.left(90) t.end_fill() t.left(90) t.forward(60) t.right(90) t.color(\\"green\\") t.begin_fill() t.circle(40) t.end_fill() def draw_sun(t): t.penup() t.goto(175, 175) t.pendown() t.color(\\"yellow\\") t.begin_fill() t.circle(50) t.end_fill() for _ in range(8): t.penup() t.goto(175, 185) t.pendown() t.forward(100) t.backward(100) t.left(45) def draw_road(t): t.penup() t.goto(-50, -100) t.pendown() t.color(\\"gray\\") t.begin_fill() t.forward(40) t.right(90) t.forward(300) t.right(90) t.forward(40) t.right(90) t.forward(300) t.end_fill() def main(): screen = turtle.Screen() screen.bgcolor(\\"lightblue\\") house_turtle = turtle.Turtle() tree_turtle = turtle.Turtle() sun_turtle = turtle.Turtle() road_turtle = turtle.Turtle() draw_house(house_turtle) draw_tree(tree_turtle) draw_sun(sun_turtle) draw_road(road_turtle) house_turtle.hideturtle() tree_turtle.hideturtle() sun_turtle.hideturtle() road_turtle.hideturtle() turtle.done() if __name__ == \\"__main__\\": main()"},{"question":"Objective: You are tasked with creating a comprehensive seaborn visualization based on the provided `tips` dataset. Your goal is to demonstrate a deep understanding of seaborn\'s `stripplot` and `catplot` functions, along with various customization options. Task: Write a Python function named `create_stripplot_visualizations` that performs the following steps: 1. **Load the `tips` dataset:** - Use the `sns.load_dataset` function to load the dataset. 2. **Create a strip plot:** - Plot the distribution of `total_bill` across different `days` of the week. - Color code the points by `time` (Lunch or Dinner). - Split the strips (`dodge=True`) to clearly show the points for Lunch and Dinner. - Customize the plot with the following settings: - Disable jittering of points. - Use diamond markers (`marker=\\"D\\"`). - Set the marker size to 15 (`s=15`). - Set the marker edge width to 2 (`linewidth=2`). - Set the transparency to 50% (`alpha=0.5`). 3. **Create a facet grid plot:** - Use `sns.catplot` to create a facet grid that shows the relationship between `total_bill` and `size` (number of people), colored by `sex`, and organized by `day` of the week (one plot for each day). - Customize the plot to have an aspect ratio of 0.75. 4. **Save the plots:** - Save the strip plot to a file named `stripplot.png`. - Save the facet grid plot to a file named `facetgrid.png`. Function Signature: ```python def create_stripplot_visualizations(): pass ``` Additional Information: - Ensure all necessary libraries are imported. - Your function should not take any arguments. - Focus on using seaborn\'s options to achieve the customizations. - Assume the current working directory is writable for saving the plots. - Include the seaborn theme setting `sns.set_theme(style=\\"whitegrid\\")` at the beginning of your function to ensure a consistent aesthetic. Example Usage: After implementing the function, you should be able to run it as follows: ```python create_stripplot_visualizations() ``` This will generate and save two plots (`stripplot.png` and `facetgrid.png`) in the current working directory.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_stripplot_visualizations(): sns.set_theme(style=\\"whitegrid\\") # Load the `tips` dataset tips = sns.load_dataset(\\"tips\\") # Create a strip plot plt.figure(figsize=(10, 6)) sns.stripplot( x=\\"day\\", y=\\"total_bill\\", hue=\\"time\\", data=tips, dodge=True, jitter=False, marker=\\"D\\", size=15, linewidth=2, alpha=0.5 ) plt.savefig(\\"stripplot.png\\") plt.clf() # Create a facet grid plot g = sns.catplot( x=\\"size\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", data=tips, kind=\\"strip\\", aspect=0.75 ) g.savefig(\\"facetgrid.png\\") # Uncomment the line below to generate the plots # create_stripplot_visualizations()"},{"question":"**Objective:** Your task is to create a Python script that utilizes the `hashlib` module to perform a series of hashing and key derivation operations. This will assess your understanding of both basic and advanced functionalities of the module. **Problem Statement:** You need to implement a function `secure_operations(data, password, salt)` which performs the following tasks: 1. **SHA-256 Hashing**: Generate a SHA-256 hash of the given `data`. 2. **Salting and Hashing with BLAKE2b**: Apply a salt to the `data` and then generate a BLAKE2b hash. The salt should be provided as an argument to the function. 3. **Password-based Key Derivation using PBKDF2**: Derive a key from the given `password` and `salt` using `pbkdf2_hmac` with SHA-256 as the hash function and 100,000 iterations. **Function Signature:** ```python def secure_operations(data: bytes, password: bytes, salt: bytes) -> dict: pass ``` **Inputs:** - `data` (bytes): A bytes-like object to be hashed. - `password` (bytes): A bytes-like object representing the password. - `salt` (bytes): A bytes-like object representing the salt value. **Returns:** - A dictionary with the following key-value pairs: - `\'sha256\'`: The SHA-256 hash of `data` represented as a hexadecimal string. - `\'blake2b\'`: The BLAKE2b hash of `data` with the provided salt, represented as a hexadecimal string. - `\'derived_key\'`: The derived key from `pbkdf2_hmac` using the provided `password` and `salt`, represented as a hexadecimal string. **Example:** ```python data = b\'Hello, World!\' password = b\'strongpassword\' salt = b\'somesalt\' result = secure_operations(data, password, salt) print(result) # Expected output (hash values will vary): # { # \'sha256\': \'64ec88ca00b268e5ba1a35678a1b5316d212f4f36631b8a9bd34c1659f201ad6\', # \'blake2b\': \'1e3946aeab8b3b0a5d722716702f2f8c2d3315a0db0935ae146fde1d8a2d6c1a\', # \'derived_key\': \'bd9e4054fa4736ed6dd586ec3bd3e6c76c817252d0d9a3957ad4e58346eb8ea0\' # } ``` **Constraints:** - Do not use any external libraries other than `hashlib`. - Ensure that the salt length is appropriate for the BLAKE2b hash function (recommended 16 bytes). **Tips:** - Review the documentation\'s examples on basic hashing, salted hashing, and password-based key derivation functions. - Ensure that you properly handle binary data and conversion to hexadecimal strings for the final output. **Assessment Criteria:** 1. Correct usage of hashlib functions. 2. Accurate implementation of SHA-256, BLAKE2b, and PBKDF2 operations. 3. Correct handling of input types and conversion to the desired output format. 4. Clear and efficient code. Good luck!","solution":"import hashlib def secure_operations(data: bytes, password: bytes, salt: bytes) -> dict: # SHA-256 Hashing sha256_hash = hashlib.sha256(data).hexdigest() # Salting and Hashing with BLAKE2b blake2b_hash = hashlib.blake2b(data, salt=salt).hexdigest() # Password-based Key Derivation using PBKDF2 derived_key = hashlib.pbkdf2_hmac(\'sha256\', password, salt, 100000).hex() return { \'sha256\': sha256_hash, \'blake2b\': blake2b_hash, \'derived_key\': derived_key }"},{"question":"Objective Implement a Multi-layer Perceptron (MLP) Classifier using scikit-learn to classify a dataset. You will also need to apply feature scaling and perform hyperparameter tuning using grid search. Task 1. **Feature Scaling**: Train and test datasets should be scaled. 2. **MLP Classifier Implementation**: - Use `MLPClassifier` from scikit-learn. - Train the classifier with the provided training data. 3. **Hyperparameter Tuning**: - Use `GridSearchCV` to find the best hyperparameters for the MLP. - Perform grid search on hyperparameters `hidden_layer_sizes` and `alpha`. Input - `X_train`: A 2D list or NumPy array of shape (n_samples, n_features) representing the training features. - `y_train`: A 1D list or NumPy array of shape (n_samples,) representing the training labels. - `X_test`: A 2D list or NumPy array of shape (m_samples, n_features) representing the test features. Output - A dictionary containing: - **best_params**: Best hyperparameters found by grid search. - **test_predictions**: Predictions on `X_test` using the best model. Constraints - You must use a minimum of 2 unique hyperparameter values for each of `hidden_layer_sizes` and `alpha`. Example ```python from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV def train_mlp_classifier(X_train, y_train, X_test): # Define the parameter grid param_grid = { \'hidden_layer_sizes\': [(50,), (100,)], \'alpha\': [1e-4, 1e-3] } # Scale the input features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize and train the MLP classifier with GridSearchCV mlp = MLPClassifier(max_iter=200) grid_search = GridSearchCV(mlp, param_grid, cv=3) grid_search.fit(X_train_scaled, y_train) # Obtain the best parameters and predictions on the test set best_params = grid_search.best_params_ best_model = grid_search.best_estimator_ test_predictions = best_model.predict(X_test_scaled) # Prepare the result dictionary result = { \'best_params\': best_params, \'test_predictions\': test_predictions } return result # Test the implementation X_train = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]] y_train = [0, 1, 1, 0] X_test = [[1.5, 1.5], [2.5, 2.5]] print(train_mlp_classifier(X_train, y_train, X_test)) ``` In this question, you will implement the function `train_mlp_classifier` as described. Ensure you handle feature scaling, use `GridSearchCV` for hyperparameter tuning, and finally make predictions on the test data.","solution":"from sklearn.neural_network import MLPClassifier from sklearn.preprocessing import StandardScaler from sklearn.model_selection import GridSearchCV def train_mlp_classifier(X_train, y_train, X_test): # Define the parameter grid param_grid = { \'hidden_layer_sizes\': [(50,), (100,)], \'alpha\': [1e-4, 1e-3] } # Scale the input features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize and train the MLP classifier with GridSearchCV mlp = MLPClassifier(max_iter=200) grid_search = GridSearchCV(mlp, param_grid, cv=3) grid_search.fit(X_train_scaled, y_train) # Obtain the best parameters and predictions on the test set best_params = grid_search.best_params_ best_model = grid_search.best_estimator_ test_predictions = best_model.predict(X_test_scaled) # Prepare the result dictionary result = { \'best_params\': best_params, \'test_predictions\': test_predictions } return result"},{"question":"# Memory-Mapped File Manipulation In this task, you are required to demonstrate your understanding of the `mmap` module by working with memory-mapped file objects. You will create a memory-mapped file and implement various operations on it as described below. Instructions: 1. **Create a Memory-Mapped File:** - Open a file named `example.txt` in write-binary mode and write the string `Hello mmap! How are you?` to it. - Open the same file in read-write-binary mode and create a memory-mapped object for the entire file. 2. **Implement the following functions:** - `read_content(mm)`: This function should return the entire content of the memory-mapped object `mm`. - `update_greeting(mm)`: In the `example.txt` file, update the word `Hello` to `Hi`. Ensure that the length of the file content remains consistent, and the new content should be `Hi mmap! How are you?`. - `find_substring(mm, substr)`: This function should find and return the index of the first occurrence of `substr` in the memory-mapped object. - `insert_after_greeting(mm, text)`: Insert the `text` immediately after `Hi mmap!`. Shift the rest of the content accordingly. Ensure the updates are reflected in the original file. 3. **Update the File and Memory-Map:** - After performing the operations described, flush the changes to ensure they are written back to the disk. 4. **Example Usage:** ```python with open(\\"example.txt\\", \\"w+b\\") as f: f.write(b\\"Hello mmap! How are you?\\") with open(\\"example.txt\\", \\"r+b\\") as f: mm = mmap.mmap(f.fileno(), 0) print(\\"Original content:\\", read_content(mm)) update_greeting(mm) print(\\"Updated content:\\", read_content(mm)) print(\\"Index of \'How\':\\", find_substring(mm, b\'How\')) insert_after_greeting(mm, b\\" Ready for the test?\\") print(\\"Final content:\\", read_content(mm)) mm.close() ``` Constraints: - Ensure that updating or inserting text maintains the integrity of the mapped file\'s structure and size constraints. - You must handle potential exceptions and ensure the memory-mapped file is flushed and closed properly. Your solution should demonstrate a thorough understanding of memory-mapped file handling in Python using the `mmap` module.","solution":"import mmap def create_memory_mapped_file(): # Create and write to the file with open(\\"example.txt\\", \\"w+b\\") as f: f.write(b\\"Hello mmap! How are you?\\") # Open the same file in read-write binary mode and create a memory-mapped object f = open(\\"example.txt\\", \\"r+b\\") mm = mmap.mmap(f.fileno(), 0) return f, mm def read_content(mm): Reads and returns the entire content of the memory-mapped object `mm`. mm.seek(0) # Move to the beginning of the memory map return mm.read() def update_greeting(mm): Updates the greeting in the memory-mapped file from \'Hello\' to \'Hi\'. mm.seek(0) mm.write(b\\"Hi \\") def find_substring(mm, substr): Finds and returns the index of the first occurrence of `substr` in the memory-mapped object `mm`. mm.seek(0) return mm.find(substr) def insert_after_greeting(mm, text): Inserts `text` immediately after \'Hi mmap! \' in the memory-mapped file. position = mm.find(b\'How\') - 1 # Position after \'Hi mmap! \' mm.resize(len(mm) + len(text)) # Resize to fit the new content mm.move(position + len(text), position, len(mm) - position - len(text)) mm[position:position + len(text)] = text # Usage of the functions if __name__ == \\"__main__\\": # Create memory-mapped file f, mm = create_memory_mapped_file() # Read original content print(\\"Original content:\\", read_content(mm)) # Update greeting update_greeting(mm) print(\\"Updated content:\\", read_content(mm)) # Find substring \'How\' index = find_substring(mm, b\'How\') print(f\\"Index of \'How\': {index}\\") # Insert text after \'Hi mmap!\' insert_after_greeting(mm, b\\" Ready for the test?\\") print(\\"Final content:\\", read_content(mm)) # Close memory-mapped object and file mm.close() f.close()"},{"question":"# Naive Bayes Classifiers in scikit-learn Naive Bayes classifiers are a family of probabilistic classifiers based on Bayes\' theorem with strong (naive) independence assumptions between the features. scikit-learn provides several variations of the Naive Bayes classifier, each tailored for different types of data. In this exercise, you will work with the `GaussianNB`, `MultinomialNB`, `ComplementNB`, and `BernoulliNB` classes provided by scikit-learn. Your task is to do the following: 1. **Load the Dataset**: - Use the `load_digits()` function from `sklearn.datasets` to load the digits dataset. - Split the dataset into training and test sets using the `train_test_split` function from `sklearn.model_selection` with a test size of 30%. 2. **Train and Evaluate Classifiers**: - Implement functions to train and evaluate the following classifiers on the digits dataset: - `GaussianNB` - `MultinomialNB` - `ComplementNB` - `BernoulliNB` - For each classifier: - Train the classifier on the training data. - Predict the labels on the test data. - Print the accuracy score and the classification report including precision, recall, and F1-score. 3. **Comparison**: - Compare the performance of these classifiers based on the accuracy and the classification report. Function Definitions Implement the following functions: 1. `load_and_split_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]`: - Loads the digits dataset. - Splits it into training and test sets. - Returns the training and test data and labels as a tuple. 2. `train_and_evaluate(classifier_name: str, X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray) -> None`: - Trains the specified classifier (`classifier_name`) on the training data and evaluates it on the test data. - Prints the accuracy score and the classification report for the classifier. - Acceptable values for `classifier_name` are `\'GaussianNB\'`, `\'MultinomialNB\'`, `\'ComplementNB\'`, and `\'BernoulliNB\'`. Example Usage: ```python # Load and split the data X_train, X_test, y_train, y_test = load_and_split_data() # Evaluate GaussianNB train_and_evaluate(\'GaussianNB\', X_train, X_test, y_train, y_test) # Evaluate MultinomialNB train_and_evaluate(\'MultinomialNB\', X_train, X_test, y_train, y_test) # Evaluate ComplementNB train_and_evaluate(\'ComplementNB\', X_train, X_test, y_train, y_test) # Evaluate BernoulliNB train_and_evaluate(\'BernoulliNB\', X_train, X_test, y_train, y_test) ``` # Constraints - Use the `digits` dataset from `sklearn.datasets`. - The training-test split ratio should be 70-30. - The results should include accuracy score and a classification report for each classifier. - Ensure that you handle any necessary input transformations required by the classifiers (e.g., binarization for `BernoulliNB`). Note: The `MultinomialNB` classifier might need the data to be non-negative, while the `BernoulliNB` classifier might require binarization of the data.","solution":"from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score, classification_report from typing import Tuple import numpy as np def load_and_split_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: Loads the digits dataset and splits it into training and test sets. Returns ------- X_train : np.ndarray Training data. X_test : np.ndarray Test data. y_train : np.ndarray Training labels. y_test : np.ndarray Test labels. digits = load_digits() X = digits.data y = digits.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) return X_train, X_test, y_train, y_test def train_and_evaluate(classifier_name: str, X_train: np.ndarray, X_test: np.ndarray, y_train: np.ndarray, y_test: np.ndarray) -> None: Trains the specified classifier on the training data and evaluates it on the test data. Parameters ---------- classifier_name : str The name of the classifier to use (\'GaussianNB\', \'MultinomialNB\', \'ComplementNB\', \'BernoulliNB\'). X_train : np.ndarray Training data. X_test : np.ndarray Test data. y_train : np.ndarray Training labels. y_test : np.ndarray Test labels. if classifier_name == \'GaussianNB\': clf = GaussianNB() elif classifier_name == \'MultinomialNB\': clf = MultinomialNB() elif classifier_name == \'ComplementNB\': clf = ComplementNB() elif classifier_name == \'BernoulliNB\': clf = BernoulliNB() else: raise ValueError(f\\"Unknown classifier name: {classifier_name}\\") clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) report = classification_report(y_test, y_pred) print(f\\"Classifier: {classifier_name}\\") print(f\\"Accuracy: {accuracy}\\") print(f\\"Classification Report:n{report}\\") # Example usage X_train, X_test, y_train, y_test = load_and_split_data() train_and_evaluate(\'GaussianNB\', X_train, X_test, y_train, y_test) train_and_evaluate(\'MultinomialNB\', X_train, X_test, y_train, y_test) train_and_evaluate(\'ComplementNB\', X_train, X_test, y_train, y_test) train_and_evaluate(\'BernoulliNB\', X_train, X_test, y_train, y_test)"},{"question":"You are required to profile the performance of a provided function using the `cProfile` and `pstats` modules. After analyzing the profile data, you are to implement optimizations aimed at improving the function\'s performance. # Task: 1. **Function Profiling**: - Write a Python function `profile_function(target_function, *args, **kwargs)` that profiles the execution of `target_function` with the provided arguments and keyword arguments. - Save the profiling results to a file named \'profile_stats\'. 2. **Analyze Profile Data**: - Write a Python function `analyze_profile_data(file_name)` that reads the profile data from the specified file and prints out: - The 10 functions with the highest cumulative time. - The 10 functions with the highest total time. 3. **Optimize Function**: - Based on your analysis, implement an optimized version of the `target_function` and provide a brief explanation of the optimizations you made. You may assume that the `target_function` is a complex function that involves multiple nested function calls and potentially redundant computations. # Constraints: - You may assume that the execution of `target_function` does not exit the interpreter abruptly (i.e., no usage of sys.exit() or similar). - The provided `target_function` may involve I/O operations, list processing, or string manipulations. - You should focus on optimizing the function without changing its external behavior (input/output relationships). # Input Format: - `profile_function(target_function, *args, **kwargs)`: * `target_function`: A callable function to be profiled. * `*args, **kwargs`: Arguments and keyword arguments to be passed to the `target_function`. # Output Format: - `analyze_profile_data(file_name)`: - Print statements that display the top 10 functions by cumulative time and total time. - Optimized implementation of `target_function`. # Example: ```python def example_function(data): # Simulated complex function with nested calls result = [x ** 2 for x in data if x % 2 == 0] result = sorted(result, reverse=True) final_result = [x + y for x, y in zip(result, result)] return final_result # Step 1: Profiling profile_function(example_function, list(range(1000))) # Step 2: Analyzing Profile Data analyze_profile_data(\'profile_stats\') # Step 3: Optimized Function (implement and explain the optimizations) def optimized_example_function(data): # Example optimization implementation goes here pass ``` You are required to submit: - The implementation of the `profile_function`. - The implementation of the `analyze_profile_data` function. - The optimized version of the `example_function` along with your explanation.","solution":"import cProfile import pstats import io def profile_function(target_function, *args, **kwargs): Profiles the execution of `target_function` with provided arguments and keyword arguments, and saves the profiling results to a file named \'profile_stats\'. profiler = cProfile.Profile() profiler.enable() target_function(*args, **kwargs) profiler.disable() profiler.dump_stats(\'profile_stats\') def analyze_profile_data(file_name): Reads profile data from the specified file and prints out: - The 10 functions with the highest cumulative time. - The 10 functions with the highest total time. with open(file_name, \'r\') as f: p = pstats.Stats(file_name) p.sort_stats(\'cumulative\').print_stats(10) p.sort_stats(\'tottime\').print_stats(10) def example_function(data): # Simulated complex function with nested calls result = [x ** 2 for x in data if x % 2 == 0] result = sorted(result, reverse=True) final_result = [x + y for x, y in zip(result, result)] return final_result # Optimized version of the example_function based on possible profiling analysis def optimized_example_function(data): # Removing redundant operations (if any) # Combining steps to reduce intermediate operations result = sorted((x ** 2 for x in data if x % 2 == 0), reverse=True) final_result = [2 * x for x in result] return final_result"},{"question":"You are provided with two matrix multiplication algorithms: a naive implementation and an optimized implementation using NumPy. You need to use the `timeit` module to compare the performance of these two matrix multiplication methods. **Objective**: Write a Python function `compare_matrix_multiplication_times()` that returns a dictionary with the average execution times of both methods. Requirements 1. **Input**: The function should take the matrix size `n` as input (an integer). The function should generate two random `n x n` matrices for the testing. 2. **Output**: The function should return a dictionary with the keys `\\"naive\\"` and `\\"numpy\\"` containing the average execution times for the naive and the numpy implementations respectively. The time should be in seconds. 3. **Constraints**: - Use the `timeit` module to measure execution times. - Run each implementation at least `100` times to mitigate variability in timing. Function Signature ```python def compare_matrix_multiplication_times(n: int) -> dict: pass ``` Example Usage ```python result = compare_matrix_multiplication_times(200) print(result) # Output: {\'naive\': 0.123456, \'numpy\': 0.012345} ``` Implementation Details 1. Naive Matrix Multiplication Function: ```python def naive_multiply(A, B): n = len(A) result = [[0] * n for _ in range(n)] for i in range(n): for j in range(n): for k in range(n): result[i][j] += A[i][k] * B[k][j] return result ``` 2. NumPy Matrix Multiplication (to be used inside the `compare_matrix_multiplication_times`): ```python import numpy as np def numpy_multiply(A, B): return np.dot(A, B) ``` Instructions 1. Write the `compare_matrix_multiplication_times` function to achieve the objectives. 2. Use `timeit` to measure the execution times of `naive_multiply` and `numpy_multiply`. 3. Generate random matrices using NumPy: ```python import numpy as np def generate_random_matrix(n): return np.random.rand(n, n).tolist() ``` Expected Performance - Ensure the naive implementation and numpy implementation run at least 100 times each for an accurate average time measurement. Bonus - If possible, implement additional checks to ensure the timing only includes the matrix multiplication and not the setup time (e.g., matrix generation). - Optimize the function to handle larger matrices.","solution":"import numpy as np import timeit def naive_multiply(A, B): n = len(A) result = [[0] * n for _ in range(n)] for i in range(n): for j in range(n): for k in range(n): result[i][j] += A[i][k] * B[k][j] return result def numpy_multiply(A, B): return np.dot(A, B) def generate_random_matrix(n): return np.random.rand(n, n).tolist() def compare_matrix_multiplication_times(n: int) -> dict: # Generate random matrices A = generate_random_matrix(n) B = generate_random_matrix(n) # Measure time for naive implementation naive_time = timeit.timeit(lambda: naive_multiply(A, B), number=100) # Measure time for numpy implementation np_A = np.array(A) np_B = np.array(B) numpy_time = timeit.timeit(lambda: numpy_multiply(np_A, np_B), number=100) return { \\"naive\\": naive_time / 100, \\"numpy\\": numpy_time / 100 }"},{"question":"**Dynamic Conditional Branching with PyTorch** In this assessment, you will demonstrate your understanding of `torch.cond` by implementing a dynamic conditional model. This model will perform different operations on the input tensor based on certain conditions. # Problem Statement You are required to implement a PyTorch module `DynamicConditionalModel` that uses `torch.cond` to switch between different operations on the input tensor `x` based on both its shape and its sum. 1. **Shape Condition**: - If the first dimension of `x` is greater than 4, compute the cosine of `x`. - Else, compute the sine of `x`. 2. **Sum Condition**: - After evaluating the shape condition, if the sum of `x` is greater than 10, further compute the square of the result from the shape condition operation. - Else, compute the natural logarithm of `1 + result` from the shape condition operation. # Input and Output Formats - **Input**: A 1D PyTorch tensor `x` of floating-point numbers. - **Output**: A PyTorch tensor resulting from the conditional operations. # Implementation Requirements 1. Define two helper functions `shape_cond_true` and `shape_cond_false` for the condition based on the shape. 2. Define two more helper functions `sum_cond_true` and `sum_cond_false` for the condition based on the sum. 3. Implement the `DynamicConditionalModel` class with the `forward` method that applies the described logic using `torch.cond`. 4. Ensure that the `forward` method handles the nested conditions correctly. # Constraints - You should use `torch.cond` to handle both conditions. - Assume that `x` will always be a 1D tensor with a minimum length of 1. - You can use PyTorch\'s built-in functions such as `torch.cos`, `torch.sin`, `torch.square`, and `torch.log`. # Example ```python import torch class DynamicConditionalModel(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: def shape_cond_true(x): return torch.cos(x) def shape_cond_false(x): return torch.sin(x) def sum_cond_true(x): return torch.square(x) def sum_cond_false(x): return torch.log(1 + x) shape_result = torch.cond(x.shape[0] > 4, shape_cond_true, shape_cond_false, (x,)) final_result = torch.cond(shape_result.sum() > 10, sum_cond_true, sum_cond_false, (shape_result,)) return final_result # Example usage model = DynamicConditionalModel() input_tensor = torch.randn(5) output_tensor = model(input_tensor) print(output_tensor) ``` # Notes - The functions `shape_cond_true`, `shape_cond_false`, `sum_cond_true`, and `sum_cond_false` represent the different code paths based on the conditions. - Make sure to test your implementation with various inputs to ensure correctness.","solution":"import torch class DynamicConditionalModel(torch.nn.Module): def __init__(self): super(DynamicConditionalModel, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: if x.shape[0] > 4: shape_result = torch.cos(x) else: shape_result = torch.sin(x) if shape_result.sum() > 10: final_result = torch.square(shape_result) else: final_result = torch.log1p(shape_result) return final_result"},{"question":"**Title: Model Comparison using Scikit-learn** Problem Statement You will implement a Python function `compare_models` using scikit-learn to fit and evaluate multiple machine-learning models on a given dataset. The function should demonstrate an understanding of fundamental and advanced concepts of the package, including model fitting, performance evaluation, and plotting results. Function Signature ```python def compare_models(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> pd.DataFrame: pass ``` Inputs - `X_train`: Training feature data (numpy array of shape `(n_samples_train, n_features)`) - `y_train`: Training target data (numpy array of shape `(n_samples_train,)`) - `X_test`: Testing feature data (numpy array of shape `(n_samples_test, n_features)`) - `y_test`: Testing target data (numpy array of shape `(n_samples_test,)`) Outputs - A pandas DataFrame containing: - The name of each model - The corresponding accuracy score on the test set You must implement the following steps within the `compare_models` function: 1. Import necessary modules from scikit-learn. 2. Define a list of models you will compare, including but not limited to: - Logistic Regression - Support Vector Machine (SVM) - Decision Tree - Random Forest - Gradient Boosting 3. Fit each model using the training dataset. 4. Predict and evaluate each model using the testing dataset. 5. Store the accuracy score for each model in a pandas DataFrame and return it. Constraints - Use default hyperparameters for all models. - You should not use any model selection techniques (like GridSearchCV). - Ensure efficient memory usage by freeing up resources for each model after evaluation. Performance Requirements - The function should execute within a reasonable time frame, given standard hardware and datasets with up to 10,000 samples and 100 features. Example Usage ```python from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split # Create a dummy dataset X, y = make_classification(n_samples=1000, n_features=20, random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Call the compare_models function results = compare_models(X_train, y_train, X_test, y_test) print(results) ``` Expected Output DataFrame (sample format): ``` Model Accuracy 0 Logistic Regression 0.85 1 Support Vector Machine 0.88 2 Decision Tree 0.78 3 Random Forest 0.89 4 Gradient Boosting 0.90 ``` This question assesses the student\'s understanding of various machine learning models in scikit-learn, the ability to implement and compare them, and the skill to present the results in an organized manner using pandas. Additionally, it serves as an intermediate-to-advanced level exercise in using scikit-learn\'s core functionalities.","solution":"import numpy as np import pandas as pd from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier from sklearn.metrics import accuracy_score def compare_models(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray) -> pd.DataFrame: models = [ (\\"Logistic Regression\\", LogisticRegression()), (\\"Support Vector Machine\\", SVC()), (\\"Decision Tree\\", DecisionTreeClassifier()), (\\"Random Forest\\", RandomForestClassifier()), (\\"Gradient Boosting\\", GradientBoostingClassifier()) ] results = [] for name, model in models: # Fit model model.fit(X_train, y_train) # Predict on test data y_pred = model.predict(X_test) # Calculate accuracy accuracy = accuracy_score(y_test, y_pred) # Store results results.append((name, accuracy)) # Create DataFrame results_df = pd.DataFrame(results, columns=[\\"Model\\", \\"Accuracy\\"]) return results_df"},{"question":"**Objective:** Demonstrate your understanding of Python\'s `logging.handlers` module by implementing a custom logging utility that uses multiple handlers to log messages to various destinations, ensuring robust handling of log rotation, as well as correct handling of network-based logging. **Problem:** You are required to implement a logging utility class `AdvancedLogger` which configures and manages logging for an application. The `AdvancedLogger` should be capable of handling log messages sent to multiple handlers: a console for real-time viewing, a file with rotation based on size constraints, and via network socket for remote logging. # Requirements: 1. **Initialization**: - Implement the `__init__(self, file_name, max_bytes, backup_count, host, port)` method with the following parameters: - `file_name` (str): The log file name. - `max_bytes` (int): Maximum size in bytes before the log file rolls over. - `backup_count` (int): The number of backup files to maintain. - `host` (str): The remote host for network logging. - `port` (int): The port for network logging. 2. **Configuration**: - Create and configure a `RotatingFileHandler` to handle file logging with rotation based on `max_bytes` and `backup_count`. - Create and configure a `StreamHandler` to handle console logging (`sys.stdout`). - Create and configure a `SocketHandler` for remote logging using the provided `host` and `port`. 3. **Log Emission**: - Implement a method `log_message(self, level, message)` to log messages at various levels (DEBUG, INFO, WARNING, ERROR, CRITICAL). 4. **Method for Testing Handlers**: - Implement a `test_logging(self)` method to simulate logging multiple messages to ensure handler functionality, including triggering a file rotation. # Constraints: - Ensure that the `SocketHandler` can handle errors during network logging gracefully. - The `log_message` method should ensure thread safety (it should be callable from multiple threads simultaneously without conflicts). # Sample Usage: ```python if __name__ == \\"__main__\\": logger = AdvancedLogger(\\"app.log\\", 1024*1024, 5, \\"localhost\\", 9000) logger.log_message(\\"INFO\\", \\"This is an informational message.\\") logger.log_message(\\"ERROR\\", \\"This is an error message.\\") logger.test_logging() ``` # Expected Output: - Log messages should appear on the console. - Log messages should be written to a file \\"app.log\\", with appropriate rotations. - Log messages should be sent to the remote host specified. **Note**: Ensure your implementation correctly formats and handles log entries for clarity and usability.","solution":"import logging import logging.handlers import threading class AdvancedLogger: def __init__(self, file_name, max_bytes, backup_count, host, port): Initialize the AdvancedLogger with file rotation, console, and network logging. self.logger = logging.getLogger(\\"AdvancedLogger\\") self.logger.setLevel(logging.DEBUG) # Create rotating file handler file_handler = logging.handlers.RotatingFileHandler( file_name, maxBytes=max_bytes, backupCount=backup_count) file_handler.setLevel(logging.DEBUG) # Create console handler console_handler = logging.StreamHandler() console_handler.setLevel(logging.DEBUG) # Create socket handler for remote logging socket_handler = logging.handlers.SocketHandler(host, port) socket_handler.setLevel(logging.DEBUG) # Create formatter and associate it with handlers formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\') file_handler.setFormatter(formatter) console_handler.setFormatter(formatter) socket_handler.setFormatter(formatter) # Add handlers to the logger self.logger.addHandler(file_handler) self.logger.addHandler(console_handler) self.logger.addHandler(socket_handler) # Lock for thread safety self.lock = threading.Lock() def log_message(self, level, message): Log a message at the specified logging level. with self.lock: { \\"DEBUG\\": self.logger.debug, \\"INFO\\": self.logger.info, \\"WARNING\\": self.logger.warning, \\"ERROR\\": self.logger.error, \\"CRITICAL\\": self.logger.critical, }.get(level.upper(), self.logger.info)(message) def test_logging(self): Test logging by generating multiple log messages to trigger file rotation and handler functionality. for i in range(100): self.log_message(\\"DEBUG\\", f\\"Test log message {i}\\") self.log_message(\\"INFO\\", f\\"Test log message {i}\\") self.log_message(\\"WARNING\\", f\\"Test log message {i}\\") self.log_message(\\"ERROR\\", f\\"Test log message {i}\\") self.log_message(\\"CRITICAL\\", f\\"Test log message {i}\\") if __name__ == \\"__main__\\": logger = AdvancedLogger(\\"app.log\\", 1024*1024, 5, \\"localhost\\", 9000) logger.log_message(\\"INFO\\", \\"This is an informational message.\\") logger.log_message(\\"ERROR\\", \\"This is an error message.\\") logger.test_logging()"},{"question":"# Seaborn Objects Visualization Question Background: You are provided with a dataset containing healthcare expenditure (`healthexp` dataset) from various countries over several years. Your task involves loading and preparing the data, and then creating a complex visualization to showcase the trends and comparisons between the countries. Task: Write a function `visualize_health_expenditure` that performs the following steps: 1. Loads the `healthexp` dataset using Seaborn\'s `load_dataset` method. 2. Prepares the data by: - Pivoting the data to have years as rows and countries as columns for the `Spending_USD` values. - Interpolating missing values. - Stacking the data back into a long-form DataFrame. - Renaming the series to `Spending_USD`. - Resetting the index and sorting by country. 3. Creates a faceted area plot with the following specifications: - Separate facets for each country, arranged in a grid wrapping to 3 columns. - Uses `so.Area()` to create filled area plots of healthcare spending over the years. - Customizes the color of each area\'s edge based on the country. - Combines the area plot with a line plot (`so.Line()`) for clearer trends. - Ensures the plots are aesthetically pleasing and well-labeled. Function Signature: ```python def visualize_health_expenditure(): pass ``` Expected Output: Your function should produce a visualization output (faceted area plots) and show this plot using the relevant plotting library commands. Constraints: - Use Seaborn\'s `objects` interface for creating the plots. - Ensure the plots are neatly arranged and easy to interpret. Example: The function call should result in something visually similar to a `FacetGrid` with area plots for healthcare spending, with each subplot representing a different country. ```python # Example usage: visualize_health_expenditure() ``` Notes: - The exact appearance of the plots can vary, but they should adhere to the guidelines provided. - You do not need to return any value from the function; displaying the plot is sufficient. - Additional customization and styling for improved readability are encouraged.","solution":"import seaborn as sns import seaborn.objects as so import pandas as pd def visualize_health_expenditure(): # Load the dataset healthexp = sns.load_dataset(\\"healthexp\\") # Pivot the data healthexp_pivoted = healthexp.pivot(index=\'Year\', columns=\'Country\', values=\'Spending_USD\') # Interpolate missing values healthexp_interpolated = healthexp_pivoted.interpolate() # Stack the data back into a long-form DataFrame healthexp_long = healthexp_interpolated.stack().reset_index() healthexp_long.columns = [\'Year\', \'Country\', \'Spending_USD\'] # Sort by country healthexp_long = healthexp_long.sort_values(by=\'Country\').reset_index(drop=True) # Create the faceted area plot p = so.Plot(healthexp_long, x=\'Year\', y=\'Spending_USD\', color=\'Country\').facet(\'Country\', wrap=3) p.add(so.Area(), alpha=0.6).add(so.Line()) # Customize and show the plot p.layout(size=(8, 12)).theme({ \'axes.labelsize\': \'small\' }) p.show()"},{"question":"# Advanced XML Parsing with `xml.dom.pulldom` Objective Demonstrate your understanding of Python\'s `xml.dom.pulldom` package by parsing and processing XML documents using event-driven techniques. Problem Statement You are given an XML document representing a simple book library. Each book has a title, author, genre, price, and publish date. Your task is to write a Python function that uses the `xml.dom.pulldom` package to parse this XML and return a list of book titles that belong to a specific genre. Input - A string `xml_string` containing the XML document. - A string `genre` representing the genre of books you are interested in. Output - A list of strings representing the titles of the books that belong to the specified genre. Example ```xml <library> <book> <title>Learning XML</title> <author>Erik T. Ray</author> <genre>Computer</genre> <price>39.95</price> <publish_date>2003-09-01</publish_date> </book> <book> <title>Harry Potter</title> <author>J.K. Rowling</author> <genre>Fantasy</genre> <price>29.99</price> <publish_date>2007-07-21</publish_date> </book> <book> <title>The Hobbit</title> <author>J.R.R. Tolkien</author> <genre>Fantasy</genre> <price>22.99</price> <publish_date>1937-09-21</publish_date> </book> </library> ``` For the above XML document, if the `genre` is \\"Fantasy\\", the output should be: ```python [\\"Harry Potter\\", \\"The Hobbit\\"] ``` Constraints 1. Use the `xml.dom.pulldom` module for parsing the XML. 2. The input XML string can be of significant size, so ensure your solution is efficient. 3. Assume all book elements are well-formed and structured as in the example. # Function Signature ```python def get_books_by_genre(xml_string: str, genre: str) -> list: pass ``` You may assume the following imports and functions are available: ```python from xml.dom import pulldom def parse_xml_string(xml_string: str) -> pulldom.DOMEventStream: return pulldom.parseString(xml_string) ``` Note - You should implement the `get_books_by_genre` function to parse the XML, iterate through events, and filter books based on the specified genre. - Use the `expandNode` method where necessary to fully expand the required node before accessing its children nodes.","solution":"from xml.dom import pulldom def get_books_by_genre(xml_string: str, genre: str) -> list: Parse the XML document and return a list of book titles that belong to the specified genre. Args: xml_string (str): The XML content as a string. genre (str): The genre to filter books by. Returns: list: A list of book titles belonging to the specified genre. doc = pulldom.parseString(xml_string) book_titles = [] current_genre = None current_title = None for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \\"book\\": doc.expandNode(node) current_genre = None current_title = None for child in node.childNodes: if child.nodeType == child.ELEMENT_NODE: if child.tagName == \\"genre\\": current_genre = child.firstChild.data if child.firstChild else \\"\\" elif child.tagName == \\"title\\": current_title = child.firstChild.data if child.firstChild else \\"\\" if current_genre == genre: book_titles.append(current_title) return book_titles"},{"question":"You are tasked with designing a Python script that reads a CSV file containing student grades, processes the grades, and writes the processed data to a new CSV file. The processing involves calculating the average grade for each student and determining their grade category based on the average. # Instructions 1. **Reading Data**: You will read the supplied CSV file `student_grades.csv` which has the following columns: - `student_id` - `student_name` - `math` - `science` - `english` 2. **Processing Data**: - Calculate the average grade for each student. - Assign a grade category based on the average grade as follows: - \\"A\\" if `average >= 90` - \\"B\\" if `80 <= average < 90` - \\"C\\" if `70 <= average < 80` - \\"D\\" if `60 <= average < 70` - \\"F\\" if `average < 60` 3. **Writing Data**: Write the processed data to a new CSV file `processed_grades.csv`. The new file should have the following columns: - `student_id` - `student_name` - `average_grade` - `grade_category` # Input - A CSV file named `student_grades.csv` with the following example content: ``` student_id,student_name,math,science,english 1,John Doe,85,90,78 2,Jane Smith,92,88,95 3,Emily Davis,55,60,58 ``` # Output - A CSV file named `processed_grades.csv` with the processed data. For the given example input, the content would be: ``` student_id,student_name,average_grade,grade_category 1,John Doe,84.33,B 2,Jane Smith,91.67,A 3,Emily Davis,57.67,F ``` # Constraints - You must use the `csv` module for reading and writing the CSV files. - Ensure that the average grade is rounded to two decimal places. # Implementation Details Implement a function `process_student_grades(input_file: str, output_file: str) -> None` that performs the required operations. ```python import csv def process_student_grades(input_file: str, output_file: str) -> None: # Your implementation here pass # Example usage: # process_student_grades(\'student_grades.csv\', \'processed_grades.csv\') ```","solution":"import csv def process_student_grades(input_file: str, output_file: str) -> None: # Read data from input file with open(input_file, mode=\'r\') as csvfile: reader = csv.DictReader(csvfile) students_data = [] for row in reader: student_id = row[\'student_id\'] student_name = row[\'student_name\'] math = float(row[\'math\']) science = float(row[\'science\']) english = float(row[\'english\']) # Calculate average grade average_grade = (math + science + english) / 3 average_grade = round(average_grade, 2) # Determine grade category if average_grade >= 90: grade_category = \\"A\\" elif average_grade >= 80: grade_category = \\"B\\" elif average_grade >= 70: grade_category = \\"C\\" elif average_grade >= 60: grade_category = \\"D\\" else: grade_category = \\"F\\" # Add processed data to list students_data.append({ \'student_id\': student_id, \'student_name\': student_name, \'average_grade\': average_grade, \'grade_category\': grade_category }) # Write processed data to output file with open(output_file, mode=\'w\', newline=\'\') as csvfile: fieldnames = [\'student_id\', \'student_name\', \'average_grade\', \'grade_category\'] writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() for student in students_data: writer.writerow(student)"},{"question":"**Concurrency in Asyncio** You are required to manage a set of tasks that need to be executed concurrently using Python\'s asyncio library. The challenge is to ensure these tasks are managed properly, and their results are collected efficiently. # Task Details You need to implement a function `asyncio_manage_tasks(task_list: List[Tuple[int, str]]) -> List[str]` that accepts a list of tasks and returns their results in the order they complete. Each task is represented by a tuple where the first element is the delay time (in seconds) and the second element is the message to be printed after the delay. # Input - `task_list`: A list of tuples, where each tuple contains: - `delay` (int): The number of seconds to wait before printing the message. - `message` (str): The message to print after the delay. # Output - Returns a list of messages in the order they complete. # Constraints - The length of `task_list` will be between 1 and 100. - The `delay` values will be between 1 and 10 seconds. - The length of `message` will be up to 50 characters. # Example ```python import asyncio from typing import List, Tuple async def asyncio_manage_tasks(task_list: List[Tuple[int, str]]) -> List[str]: # Your implementation here pass # Example usage: task_list = [ (3, \\"task3 completed\\"), (1, \\"task1 completed\\"), (2, \\"task2 completed\\") ] results = asyncio.run(asyncio_manage_tasks(task_list)) print(results) # Expected output: [\'task1 completed\', \'task2 completed\', \'task3 completed\'] ``` # Notes - You must handle tasks concurrently and collect their results in the order they complete using asyncio. - Utilize `asyncio` functions such as `asyncio.create_task`, `asyncio.gather`, and other related methods as necessary.","solution":"import asyncio from typing import List, Tuple async def async_task(delay: int, message: str) -> str: await asyncio.sleep(delay) return message async def asyncio_manage_tasks(task_list: List[Tuple[int, str]]) -> List[str]: tasks = [asyncio.create_task(async_task(delay, message)) for delay, message in task_list] results = await asyncio.gather(*tasks) return results"},{"question":"Manifold Learning Techniques in Action # Objective: Design and implement a Python function using scikit-learn that performs dimensionality reduction on high-dimensional data using the Locally Linear Embedding (LLE) technique. Your implementation should be able to handle different LLE variants (standard, modified, hessian). # Problem Statement: You are given a high-dimensional dataset `data` represented as a 2D NumPy array where each row corresponds to a data point and each column represents a feature. Your task is to write a function `apply_lle` that reduces the dimensionality of this dataset to a specified number of components. # Function Specification: ```python def apply_lle(data, n_components, n_neighbors, method=\'standard\'): Apply Locally Linear Embedding (LLE) on the given high-dimensional dataset. Parameters: - data (numpy.ndarray): The high-dimensional input data array with shape (n_samples, n_features). - n_components (int): The number of dimensions to reduce the data to. - n_neighbors (int): The number of nearest neighbors to consider for each point. - method (str): The variant of LLE to use. Options are \'standard\', \'modified\', \'hessian\'. Returns: - numpy.ndarray: The data transformed into the new lower-dimensional space with shape (n_samples, n_components). pass ``` # Constraints: - `n_components` must be less than the number of neighbors (`n_neighbors`). - The input data `data` should be scaled before applying LLE. Use `sklearn.preprocessing.StandardScaler` for this purpose. - Ensure your function handles errors gracefully, such as when the matrix is singular or input parameters are invalid. - Performance matters: optimize your method choice and other parameters to ensure the method completes in a reasonable time for up to `10,000` samples. # Example Usage: ```python import numpy as np # Sample data data = np.random.rand(100, 50) # Apply LLE transformed_data = apply_lle(data, n_components=2, n_neighbors=10, method=\'standard\') print(transformed_data.shape) # Expected: (100, 2) ``` # Evaluation Criteria: 1. **Functionality**: The function correctly reduces the dimensionality of the data. 2. **Robustness**: Error handling and constraints are properly managed. 3. **Performance**: The solution is optimized for larger datasets. # Notes: - Utilize the scikit-learn library for LLE (`sklearn.manifold.LocallyLinearEmbedding`). - Study different parameters and methods and understand their impact on the performance and accuracy of the embeddings.","solution":"from sklearn.manifold import LocallyLinearEmbedding from sklearn.preprocessing import StandardScaler import numpy as np def apply_lle(data, n_components, n_neighbors, method=\'standard\'): Apply Locally Linear Embedding (LLE) on the given high-dimensional dataset. Parameters: - data (numpy.ndarray): The high-dimensional input data array with shape (n_samples, n_features). - n_components (int): The number of dimensions to reduce the data to. - n_neighbors (int): The number of nearest neighbors to consider for each point. - method (str): The variant of LLE to use. Options are \'standard\', \'modified\', \'hessian\'. Returns: - numpy.ndarray: The data transformed into the new lower-dimensional space with shape (n_samples, n_components). if not isinstance(data, np.ndarray): raise ValueError(\\"Input data must be a numpy array.\\") if n_components >= n_neighbors: raise ValueError(\\"n_components must be less than the number of neighbors (n_neighbors).\\") if method not in [\'standard\', \'modified\', \'hessian\']: raise ValueError(\\"Method must be one of \'standard\', \'modified\', \'hessian\'.\\") # Scale the data scaler = StandardScaler() data_scaled = scaler.fit_transform(data) # Apply LLE lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method=method) transformed_data = lle.fit_transform(data_scaled) return transformed_data"},{"question":"# Seaborn: Advanced KDE Plotting You are provided with a dataset containing information about different species of penguins. The dataset includes the following features: - `species`: The species of the penguin. - `bill_length_mm`: The length of the penguin\'s bill (in millimeters). - `bill_depth_mm`: The depth of the penguin\'s bill (in millimeters). - `flipper_length_mm`: The length of the penguin\'s flipper (in millimeters). - `body_mass_g`: The mass of the penguin (in grams). **Objective:** Write a function `plot_penguin_distributions(data)` that takes a pandas DataFrame as input and generates the following plots using `seaborn`: 1. **Univariate Distribution:** - Plot the distribution of `bill_length_mm` for all penguins. - Apply a log scale to the x-axis. 2. **Conditional Distribution:** - Plot the distribution of `body_mass_g` for each species. - Stack the distributions. 3. **Bivariate Distribution:** - Plot a bivariate distribution of `flipper_length_mm` vs. `body_mass_g`. - Use `species` as the hue to show conditional distributions. - Fill the contours and use the `mako` colormap. 4. **Smoothing & Levels:** - Plot a bivariate distribution of `bill_length_mm` vs. `flipper_length_mm`. - Adjust the smoothing (`bw_adjust`), show fewer contour levels, and set a specific threshold value. Your plots should be informative and clearly distinguish between the species of penguins using different colors. Make sure each plot has appropriate labels and a title. **Input:** - `data`: A pandas DataFrame containing the penguin data. **Expected Output:** - The function should render four plots as specified above. ```python import seaborn as sns import matplotlib.pyplot as plt def plot_penguin_distributions(data): # Univariate distribution plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'bill_length_mm\', log_scale=True) plt.title(\'Distribution of Bill Length (Log Scale)\') plt.show() # Conditional distribution plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'body_mass_g\', hue=\'species\', multiple=\'stack\') plt.title(\'Stacked Distribution of Body Mass by Species\') plt.show() # Bivariate distribution plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', fill=True, cmap=\'mako\') plt.title(\'Bivariate Distribution of Flipper Length vs. Body Mass by Species\') plt.show() # Smoothing & levels plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'bill_length_mm\', y=\'flipper_length_mm\', bw_adjust=0.5, levels=5, thresh=0.2) plt.title(\'Bivariate Distribution of Bill Length vs. Flipper Length (Adjusted Smoothing & Levels)\') plt.show() ``` **Constraints:** - Ensure the dataset contains the necessary columns (`species`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`). **Tips:** - Use `sns.set_theme()` at the start if you wish to apply Seaborn’s default theme. - You may load and test the function with the penguins dataset from the Seaborn package: `sns.load_dataset(\\"penguins\\")`.","solution":"import matplotlib.pyplot as plt import seaborn as sns def plot_penguin_distributions(data): Generate KDE plots based on the penguin dataset. Parameters: data (pd.DataFrame): The dataset containing the penguin information including columns \'species\', \'bill_length_mm\', \'bill_depth_mm\', \'flipper_length_mm\', \'body_mass_g\'. sns.set_theme(style=\\"whitegrid\\") # 1. Univariate Distribution of bill_length_mm with log scale plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'bill_length_mm\', log_scale=True) plt.title(\'Distribution of Bill Length (Log Scale)\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Density\') plt.show() # 2. Conditional Distribution of body_mass_g by species plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'body_mass_g\', hue=\'species\', multiple=\'stack\') plt.title(\'Stacked Distribution of Body Mass by Species\') plt.xlabel(\'Body Mass (g)\') plt.ylabel(\'Density\') plt.show() # 3. Bivariate Distribution of flipper_length_mm vs. body_mass_g with species hue plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'flipper_length_mm\', y=\'body_mass_g\', hue=\'species\', fill=True, cmap=\'mako\') plt.title(\'Bivariate Distribution of Flipper Length vs. Body Mass by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.show() # 4. Bivariate Distribution of bill_length_mm vs. flipper_length_mm plt.figure(figsize=(10, 6)) sns.kdeplot(data=data, x=\'bill_length_mm\', y=\'flipper_length_mm\', bw_adjust=0.5, levels=5, thresh=0.2) plt.title(\'Bivariate Distribution of Bill Length vs. Flipper Length (Adjusted Smoothing & Levels)\') plt.xlabel(\'Bill Length (mm)\') plt.ylabel(\'Flipper Length (mm)\') plt.show()"},{"question":"Coding Assessment Question # Problem Statement You are required to demonstrate your understanding of Unix terminal control by managing terminal input modes using Python. Specifically, you need to implement a function that toggles the terminal between raw mode and cbreak mode, providing users an interactive experience where they can switch modes on the fly. Your task involves creating a utility that allows a user to press a key to switch between these modes and print the current mode to the terminal. # Function Signature ```python import tty import termios import sys import os def interactive_terminal_mode_switch(): pass ``` # Input and Output - **Input**: There are no explicit inputs via function arguments, but user interaction via key presses is monitored. - **Output**: The function should print messages indicating the current terminal mode (\\"Raw Mode\\" or \\"Cbreak Mode\\") each time the mode is switched. # Requirements 1. The program should start in `cbreak` mode and print \\"Cbreak Mode\\". 2. Wait for a key press from the user: - If the key pressed is `r`, switch to `raw` mode and print \\"Raw Mode\\". - If the key pressed is `c`, switch to `cbreak` mode and print \\"Cbreak Mode\\". - If the key pressed is `q`, exit the program cleanly. 3. Handle edge cases such as non-printable characters gracefully. 4. Ensure the terminal is restored to its original state upon exit. # Constraints - You must use the `tty.setraw` and `tty.setcbreak` functions for mode switching. - The program should run indefinitely until `q` is pressed. - This program must be executed on a Unix-based system (e.g., Linux or macOS). # Example ``` python interactive_terminal_mode_switch.py Cbreak Mode [User presses \'r\'] Raw Mode [User presses \'c\'] Cbreak Mode [User presses \'q\'] ``` Write the code for the `interactive_terminal_mode_switch` function following the above requirements.","solution":"import tty import termios import sys import os def interactive_terminal_mode_switch(): def set_cbreak_mode(fd): tty.setcbreak(fd) print(\\"Cbreak Mode\\") def set_raw_mode(fd): tty.setraw(fd) print(\\"Raw Mode\\") fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) set_cbreak_mode(fd) try: while True: ch = sys.stdin.read(1) if ch == \'q\': break elif ch == \'r\': set_raw_mode(fd) elif ch == \'c\': set_cbreak_mode(fd) finally: termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) if __name__ == \\"__main__\\": interactive_terminal_mode_switch()"},{"question":"**Task: Implement `asyncio` Function with Platform-specific Conditions** You are tasked with writing a Python function using the `asyncio` module that handles platform-specific conditions effectively. The function should create a TCP echo server and client, keeping in mind the given platform restrictions. # Function Signature ```python import asyncio async def tcp_echo_server(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): pass # Implement server details async def main_server(): await start_server() async def main_client(): await start_client() # Main function if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Requirements 1. The function should create a simple TCP echo server that listens to a specified port. The echo server reads data from the client and sends the same data back. 2. The function should also create a TCP client that connects to this echo server and sends a message, then receives the echoed message. 3. You need to account for platform-specific constraints: * **Windows**: - Use `ProactorEventLoop` as the event loop. - Ensure that subprocesses, pipes, and Unix-specific socket connections are not used. * **macOS**: - Use `KqueueSelector` as the event loop for macOS versions 10.9 and above. - For macOS versions 10.8 and below, manually configure `SelectorEventLoop` with `SelectSelector` to support character devices. # Input The `main_server` and `main_client` functions take no parameters. You may assume the functions are called separately to start the server and client. # Output The function does not return a value but should start the server and client successfully, allowing the client to echo messages correctly as described. # Constraints 1. Do not use any asynchronous IO operations not supported as per the platform-specific limitations mentioned above. 2. Ensure your solution properly configures the event loop based on the platform and version specifics provided. 3. Make use of appropriate error-handling to manage platform limitations effectively. # Example Usage ```python # On Windows, it should configure the ProactorEventLoop. asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) # On macOS <= 10.8 import sys if sys.platform == \\"darwin\\": import platform version = platform.mac_ver()[0] if version <= \\"10.8\\": import selectors selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) # This needs to be set up as part of your solution await start_server() await start_client() ``` Implement the function to satisfy the conditions above, ensuring cross-platform functionality while adhering to the limitations and guidelines provided.","solution":"import asyncio import platform import sys async def tcp_echo_server(reader: asyncio.StreamReader, writer: asyncio.StreamWriter): Implements a simple TCP echo server. data = await reader.read(100) message = data.decode() addr = writer.get_extra_info(\'peername\') print(f\\"Received {message} from {addr}\\") print(f\\"Send {message} back to {addr}\\") writer.write(data) await writer.drain() print(\\"Close the connection\\") writer.close() async def start_server(): server = await asyncio.start_server( tcp_echo_server, \'127.0.0.1\', 8888 ) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def start_client(): reader, writer = await asyncio.open_connection( \'127.0.0.1\', 8888 ) message = \'Hello, World!\' print(f\'Send: {message}\') writer.write(message.encode()) data = await reader.read(100) print(f\'Received: {data.decode()}\') print(\'Close the connection\') writer.close() await writer.wait_closed() async def main_server(): await start_server() async def main_client(): await start_client() def main(): system = platform.system() if system == \\"Windows\\": asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) elif system == \\"Darwin\\": version = platform.mac_ver()[0] if version and version <= \\"10.8\\": import selectors selector = selectors.SelectSelector() loop = asyncio.SelectorEventLoop(selector) asyncio.set_event_loop(loop) else: loop_policy = asyncio.DefaultEventLoopPolicy() asyncio.set_event_loop_policy(loop_policy) loop = asyncio.get_event_loop() try: loop.run_until_complete(main_server()) except KeyboardInterrupt: pass finally: loop.close() if __name__ == \\"__main__\\": main()"},{"question":"# Question **Data Aggregation and Transformation using Pandas** You are given multiple datasets in different file formats, and your task is to read these data files, perform specific operations on the data, and then save the results in another file format. This will test your understanding of various pandas I/O functions and your ability to manipulate dataframes. Input 1. An Excel file containing a product catalog with columns: `ProductID`, `Category`, `Price`, `Stock`. (`products.xlsx`) 2. A JSON file containing sales records with columns: `SaleID`, `ProductID`, `Quantity`, `SaleDate`. (`sales.json`) 3. A CSV file containing supplier information with columns: `SupplierID`, `ProductID`, `SupplierName`, `Contact`. (`suppliers.csv`) Task 1. Read the three input files into pandas dataframes. 2. Merge the datasets together: - Join `sales.json` with `products.xlsx` on `ProductID`. - Further, join the resulting dataframe with `suppliers.csv` on `ProductID`. 3. Perform the following transformations and aggregations: - Calculate the total sales amount for each product (i.e., `TotalSales = Quantity * Price`). - Add a new column `TotalSales`. - Group the resulting dataframe by \'Category\' and calculate: - Total sales for each category. - Total stock left for each category. 4. Write the resulting dataframe to a new Parquet file named `category_sales.parquet`. Output A Parquet file (`category_sales.parquet`) containing the aggregated results with columns: `Category`, `TotalSales`, `TotalStock`. Constraints - Assume the input files are present in the current working directory. - Ensure that pandas version 1.0 or higher is used. Example Consider the following simplified example inputs: `products.xlsx` | ProductID | Category | Price | Stock | |-----------|-----------|-------|-------| | 1 | Electronics | 100 | 50 | | 2 | Clothing | 50 | 20 | `sales.json` | SaleID | ProductID | Quantity | SaleDate | |--------|-----------|----------|------------| | 101 | 1 | 2 | 2023-01-01 | | 102 | 2 | 1 | 2023-01-01 | `suppliers.csv` | SupplierID | ProductID | SupplierName | Contact | |------------|-----------|--------------|---------| | 1001 | 1 | SupplierA | 12345 | | 1002 | 2 | SupplierB | 67890 | Aggregated result in `category_sales.parquet`: | Category | TotalSales | TotalStock | |-------------|------------|------------| | Electronics | 200 | 50 | | Clothing | 50 | 20 | Write the code to accomplish the described task. ```python import pandas as pd # Read input files products_df = pd.read_excel(\'products.xlsx\') sales_df = pd.read_json(\'sales.json\') suppliers_df = pd.read_csv(\'suppliers.csv\') # Merge datasets merged_df = sales_df.merge(products_df, on=\'ProductID\') merged_df = merged_df.merge(suppliers_df, on=\'ProductID\') # Perform transformations and aggregations merged_df[\'TotalSales\'] = merged_df[\'Quantity\'] * merged_df[\'Price\'] category_sales_df = merged_df.groupby(\'Category\').agg( TotalSales=(\'TotalSales\', \'sum\'), TotalStock=(\'Stock\', \'sum\') ).reset_index() # Write to a Parquet file category_sales_df.to_parquet(\'category_sales.parquet\') ```","solution":"import pandas as pd def aggregate_category_sales(): Reads product, sales, and supplier data from files, merges them, and aggregates total sales and stock by category. Outputs the result to a Parquet file. # Read input files products_df = pd.read_excel(\'products.xlsx\') sales_df = pd.read_json(\'sales.json\') suppliers_df = pd.read_csv(\'suppliers.csv\') # Merge datasets merged_df = sales_df.merge(products_df, on=\'ProductID\') merged_df = merged_df.merge(suppliers_df, on=\'ProductID\') # Perform transformations and aggregations merged_df[\'TotalSales\'] = merged_df[\'Quantity\'] * merged_df[\'Price\'] category_sales_df = merged_df.groupby(\'Category\').agg( TotalSales=(\'TotalSales\', \'sum\'), TotalStock=(\'Stock\', \'sum\') ).reset_index() # Write to a Parquet file category_sales_df.to_parquet(\'category_sales.parquet\')"},{"question":"Dimensionality reduction is a vital preprocessing step in machine learning workflows, especially when dealing with high-dimensional datasets. In this task, you are required to implement a function that performs dimensionality reduction on a given dataset using Principal Component Analysis (PCA). Additionally, you will integrate this PCA transformation into a pipeline followed by a supervised learning algorithm. # Function Requirements Implement the function `pca_pipeline_model` with the following signature: ```python def pca_pipeline_model(X_train, y_train, X_test, n_components=2): This function builds a machine learning pipeline that includes PCA for dimensionality reduction and applies a supervised learning algorithm to make predictions on the test set. Parameters: - X_train (pd.DataFrame or np.array): Training features. - y_train (pd.Series or np.array): Training labels. - X_test (pd.DataFrame or np.array): Test features. - n_components (int): Number of principal components to keep. Default is 2. Returns: - predictions (np.array): Predictions made by the trained model on the test set. pass ``` # Detailed Instructions: 1. **PCA Transformation**: - Use `sklearn.decomposition.PCA` to transform the input features to the specified number of principal components (`n_components`). 2. **Pipeline Integration**: - Create a pipeline that includes the PCA transformation and a supervised learning estimator. - You can choose any appropriate supervised learning algorithm available in scikit-learn (e.g., Logistic Regression, SVM, Random Forest). 3. **Model Training and Prediction**: - Fit the pipeline on the training data (`X_train`, `y_train`). - Make predictions on the test data (`X_test`). # Input and Output Formats: - **Input**: - `X_train` (pd.DataFrame or np.array): Training dataset features. - `y_train` (pd.Series or np.array): Training dataset labels. - `X_test` (pd.DataFrame or np.array): Test dataset features. - `n_components` (int): Number of principal components to keep. Default value is 2. - **Output**: - `predictions` (np.array): Array of predicted labels for the test dataset. # Constraints: - Assume the input datasets are clean and ready for analysis (e.g., no missing values or categorical data). - The number of components `n_components` will be a positive integer less than or equal to the number of features in `X_train`. # Example: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load example dataset data = load_iris() X = data.data y = data.target # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Call the function predictions = pca_pipeline_model(X_train, y_train, X_test, n_components=2) # Output the predictions print(predictions) ``` # Notes: - Add necessary imports and assume the prominent packages (like Pandas, NumPy, Scikit-learn) are available. - Ensure the function is efficient and accurate in predicting the test data.","solution":"from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression import numpy as np def pca_pipeline_model(X_train, y_train, X_test, n_components=2): This function builds a machine learning pipeline that includes PCA for dimensionality reduction and applies a supervised learning algorithm to make predictions on the test set. Parameters: - X_train (pd.DataFrame or np.array): Training features. - y_train (pd.Series or np.array): Training labels. - X_test (pd.DataFrame or np.array): Test features. - n_components (int): Number of principal components to keep. Default is 2. Returns: - predictions (np.array): Predictions made by the trained model on the test set. # Create a PCA object pca = PCA(n_components=n_components) # Choose a supervised learning algorithm classifier = LogisticRegression(random_state=0) # Create a pipeline with PCA and the classifier pipeline = Pipeline([ (\'pca\', pca), (\'classifier\', classifier) ]) # Fit the pipeline with the training data pipeline.fit(X_train, y_train) # Predict on the test data predictions = pipeline.predict(X_test) return predictions"},{"question":"Objective: To assess the student\'s understanding of probability calibration using scikit-learn, including the creation of calibration curves and the application of the `CalibratedClassifierCV` class to calibrate a binary classifier. Problem Statement: You have been provided with a dataset `data.csv` which has features and a binary target variable. Your task is to: 1. Split the dataset into training and testing sets. 2. Train a RandomForestClassifier to predict the target variable. 3. Evaluate the calibration of the classifier using a calibration curve. 4. Use `CalibratedClassifierCV` to create both sigmoid and isotonic calibrators. 5. Compare and visualize the calibration curves for the uncalibrated classifier, sigmoid calibrated classifier, and isotonic calibrated classifier. 6. Analyze and interpret the difference in performances. Requirements: - Implement the function `calibrate_classifier(data_path: str) -> None`. - Function should read the dataset from `data_path`. - Split data into 70% training and 30% testing. - Train a `RandomForestClassifier` on the training set without calibration. - Use `CalibratedClassifierCV` to apply sigmoid and isotonic calibrations. - Generate and plot calibration curves for the uncalibrated, sigmoid, and isotonic calibrated classifiers. - Include histograms showing the number of samples per predicted probability bin. - Provide clear comments and labels on the plots for readability. Expected Input: - `data_path`: A string representing the file path to `data.csv`. Constraints: - You must use `RandomForestClassifier` from `sklearn.ensemble`. - Utilize `CalibratedClassifierCV` from `sklearn.calibration` for calibration. - Ensure all plots are generated using `matplotlib`. Example Usage: ```python calibrate_classifier(\'path/to/data.csv\') ``` Function Signature: ```python def calibrate_classifier(data_path: str) -> None: pass ``` Notes: - Ensure reproducibility by setting a random seed where appropriate. - Your analysis should include which calibration method produced better-calibrated probabilities and why. - Provide comments in your code to explain the steps being performed. Example Dataset Format (data.csv): | feature1 | feature2 | ... | target | |----------|----------|-----|--------| | 5.1 | 3.5 | ... | 0 | | 4.9 | 3.0 | ... | 1 | | ... | ... | ... | ... | | 5.0 | 3.4 | ... | 0 | # Evaluation Criteria: - Correctness and completeness of the code. - Quality of visualizations and clarity of analysis. - Code readability and use of comments.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.calibration import calibration_curve, CalibratedClassifierCV from sklearn.metrics import brier_score_loss def calibrate_classifier(data_path: str) -> None: # Load the dataset data = pd.read_csv(data_path) X = data.drop(\'target\', axis=1) y = data[\'target\'] # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Train the RandomForestClassifier without calibration clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Calibrated classifiers sigmoid_clf = CalibratedClassifierCV(clf, method=\'sigmoid\', cv=\'prefit\') sigmoid_clf.fit(X_train, y_train) isotonic_clf = CalibratedClassifierCV(clf, method=\'isotonic\', cv=\'prefit\') isotonic_clf.fit(X_train, y_train) # Get probability predictions proba_uncalibrated = clf.predict_proba(X_test)[:, 1] proba_sigmoid = sigmoid_clf.predict_proba(X_test)[:, 1] proba_isotonic = isotonic_clf.predict_proba(X_test)[:, 1] # Calibration curves fraction_of_positives_uncalibrated, mean_predicted_value_uncalibrated = calibration_curve(y_test, proba_uncalibrated, n_bins=10) fraction_of_positives_sigmoid, mean_predicted_value_sigmoid = calibration_curve(y_test, proba_sigmoid, n_bins=10) fraction_of_positives_isotonic, mean_predicted_value_isotonic = calibration_curve(y_test, proba_isotonic, n_bins=10) # Plot calibration curves plt.figure(figsize=(10, 10)) # Plot uncalibrated plt.plot(mean_predicted_value_uncalibrated, fraction_of_positives_uncalibrated, marker=\'o\', label=\'Uncalibrated\') # Plot sigmoid calibrated plt.plot(mean_predicted_value_sigmoid, fraction_of_positives_sigmoid, marker=\'o\', label=\'Sigmoid Calibrated\') # Plot isotonic calibrated plt.plot(mean_predicted_value_isotonic, fraction_of_positives_isotonic, marker=\'o\', label=\'Isotonic Calibrated\') # Reference line plt.plot([0, 1], [0, 1], linestyle=\'--\', label=\'Perfectly calibrated\') # Add labels and legend plt.xlabel(\'Mean predicted value\') plt.ylabel(\'Fraction of positives\') plt.title(\'Calibration curves\') plt.legend() plt.show() # Histograms plt.figure(figsize=(10, 5)) # Histogram for uncalibrated plt.hist(proba_uncalibrated, bins=10, range=(0, 1), histtype=\'step\', label=\'Uncalibrated\', lw=2) # Histogram for sigmoid calibrated plt.hist(proba_sigmoid, bins=10, range=(0, 1), histtype=\'step\', label=\'Sigmoid Calibrated\', lw=2) # Histogram for isotonic calibrated plt.hist(proba_isotonic, bins=10, range=(0, 1), histtype=\'step\', label=\'Isotonic Calibrated\', lw=2) # Add labels and legend plt.xlabel(\'Predicted probability\') plt.ylabel(\'Frequency\') plt.title(\'Predicted probabilities histogram\') plt.legend() plt.show() # Brier scores print(\'Brier score (Uncalibrated):\', brier_score_loss(y_test, proba_uncalibrated)) print(\'Brier score (Sigmoid):\', brier_score_loss(y_test, proba_sigmoid)) print(\'Brier score (Isotonic):\', brier_score_loss(y_test, proba_isotonic))"},{"question":"You are tasked with writing a Python script that manipulates and queries the Unix group database using the `grp` module. The script should provide functionality for the following operations: 1. **Retrieve a Group Entry by Group ID**: - Write a function `get_group_by_id(group_id: int) -> dict` that takes an integer `group_id` and returns a dictionary representing the group entry. The dictionary should have the following keys: `gr_name`, `gr_passwd`, `gr_gid`, and `gr_mem`. - Raise a `ValueError` if the group entry is not found. 2. **Retrieve a Group Entry by Group Name**: - Write a function `get_group_by_name(group_name: str) -> dict` that takes a string `group_name` and returns a dictionary representing the group entry. The dictionary should have the following keys: `gr_name`, `gr_passwd`, `gr_gid`, and `gr_mem`. - Raise a `ValueError` if the group entry is not found. 3. **List All Group Entries**: - Write a function `list_all_groups() -> List[dict]` that returns a list of dictionaries, where each dictionary represents a group entry with the keys: `gr_name`, `gr_passwd`, `gr_gid`, and `gr_mem`. 4. **Find Common Members in Groups**: - Write a function `common_members(group_names: List[str]) -> List[str]` that takes a list of group names and returns a list of usernames that are common members of all the specified groups. - Raise a `ValueError` if any of the group entries are not found. # Constraints - Your solution should handle non-integer arguments for group IDs by raising a `TypeError`. - Assume that the group database is properly configured on the system where the script is executed. # Example Usage ```python # Example usages of the functions try: group_info = get_group_by_id(1000) print(group_info) except ValueError as e: print(e) try: group_info = get_group_by_name(\'staff\') print(group_info) except ValueError as e: print(e) all_groups = list_all_groups() print(all_groups) try: common_members_list = common_members([\'staff\', \'users\']) print(common_members_list) except ValueError as e: print(e) ``` # Expected Output ```python {\'gr_name\': \'example\', \'gr_passwd\': \'x\', \'gr_gid\': 1000, \'gr_mem\': [\'user1\', \'user2\']} {\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 50, \'gr_mem\': [\'user1\']} [{\'gr_name\': \'example\', \'gr_passwd\': \'x\', \'gr_gid\': 1000, \'gr_mem\': [\'user1\', \'user2\']}, {\'gr_name\': \'staff\', \'gr_passwd\': \'x\', \'gr_gid\': 50, \'gr_mem\': [\'user1\']}] [\'user1\'] ``` Write the complete implementations for these functions using the `grp` module.","solution":"import grp from typing import List, Dict def get_group_by_id(group_id: int) -> Dict: Retrieves a group entry by group ID. :param group_id: Integer representing the group ID. :return: Dictionary with group details. :raises: TypeError if group_id is not an integer. ValueError if the group is not found. if not isinstance(group_id, int): raise TypeError(\\"group_id must be an integer\\") try: group_entry = grp.getgrgid(group_id) return { \'gr_name\': group_entry.gr_name, \'gr_passwd\': group_entry.gr_passwd, \'gr_gid\': group_entry.gr_gid, \'gr_mem\': group_entry.gr_mem } except KeyError: raise ValueError(f\\"No group found with ID {group_id}\\") def get_group_by_name(group_name: str) -> Dict: Retrieves a group entry by group name. :param group_name: String representing the group name. :return: Dictionary with group details. :raises: ValueError if the group is not found. try: group_entry = grp.getgrnam(group_name) return { \'gr_name\': group_entry.gr_name, \'gr_passwd\': group_entry.gr_passwd, \'gr_gid\': group_entry.gr_gid, \'gr_mem\': group_entry.gr_mem } except KeyError: raise ValueError(f\\"No group found with name \'{group_name}\'\\") def list_all_groups() -> List[Dict]: Lists all group entries. :return: List of dictionaries with group details. groups = [] for group in grp.getgrall(): groups.append({ \'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem }) return groups def common_members(group_names: List[str]) -> List[str]: Finds common members in the specified groups. :param group_names: List of group names. :return: List of usernames who are common members in all groups. :raises: ValueError if any group entry is not found. try: member_sets = [set(grp.getgrnam(name).gr_mem) for name in group_names] except KeyError as e: raise ValueError(f\\"No group found with name \'{str(e)}\'\\") return list(set.intersection(*member_sets))"},{"question":"You are provided with six toy datasets available in the scikit-learn library: iris, diabetes, digits, linnerud, wine, and breast cancer. Using one of these datasets, you are required to perform the following tasks: 1. **Dataset Loading**: Write a function to load the dataset using the appropriate `load_*` function from `sklearn.datasets`. 2. **Data Preprocessing**: Split the dataset into training and test sets. 3. **Model Training**: Train a chosen classification or regression model from `sklearn` using the training data. 4. **Model Evaluation**: Evaluate the model on the test data and report the performance metric (accuracy for classification, mean squared error for regression). 5. **Result Visualization**: Visualize the performance metric in a suitable plot. # Instructions 1. Implement the function `assess_model_performance` to complete the tasks described above. 2. You may choose either a classification or a regression task depending on the chosen dataset. 3. The chosen dataset, model, and visualization are up to you, but the overall workflow outlined must be followed. # Function Signature ```python def assess_model_performance(): 1. Load a dataset from sklearn\'s toy datasets. 2. Split the dataset into training and test sets. 3. Train a model on the training set. 4. Evaluate the model on the test set. 5. Visualize the evaluation metric. Returns: None pass ``` # Example ```python def assess_model_performance(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Load dataset data = load_iris() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) # Train model model = DecisionTreeClassifier() model.fit(X_train, y_train) # Evaluate model y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) # Visualize results plt.bar([\'Accuracy\'], [acc]) plt.ylim(0, 1) plt.title(\'Model Performance\') plt.show() assess_model_performance() ``` # Notes - Make sure to import all necessary libraries. - Choose model parameters that you believe are appropriate. - The function should not take any arguments but should execute the workflow as described and display the result.","solution":"def assess_model_performance(): 1. Load a dataset from sklearn\'s toy datasets. 2. Split the dataset into training and test sets. 3. Train a model on the training set. 4. Evaluate the model on the test set. 5. Visualize the evaluation metric. Returns: None from sklearn.datasets import load_wine from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import matplotlib.pyplot as plt # Load dataset data = load_wine() X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42) # Train model model = DecisionTreeClassifier() model.fit(X_train, y_train) # Evaluate model y_pred = model.predict(X_test) acc = accuracy_score(y_test, y_pred) # Visualize results plt.bar([\'Accuracy\'], [acc]) plt.ylim(0, 1) plt.title(\'Model Performance\') plt.show() assess_model_performance()"},{"question":"**Objective**: Implement a function that demonstrates the use of various Bytearray API functions and macros. **Task**: Write a Python function `process_bytearrays` that takes two input strings, performs the following operations, and returns a tuple with specific results. **Function Signature**: ```python def process_bytearrays(a: str, b: str) -> tuple: ... ``` **Instructions**: 1. Create bytearrays from the given input strings. 2. Concatenate the two bytearrays. 3. Get the size of the concatenated bytearray. 4. Extract the contents of the concatenated bytearray as a plain string. 5. Resize the concatenated bytearray to half its original size. 6. Return a tuple containing: - The concatenated bytearray. - The size of the concatenated bytearray before resizing. - The string representation of the concatenated bytearray. - The resized concatenated bytearray. **Expected Input and Output Formats**: - Input: Two strings `a` and `b`. - Output: A tuple containing: 1. The concatenated bytearray. 2. The size of the concatenated bytearray before resizing. 3. The string representation of the concatenated bytearray. 4. The resized concatenated bytearray. **Example**: ```python result = process_bytearrays(\\"hello\\", \\"world\\") print(result) # Output: (bytearray(b\'helloworld\'), 10, \'helloworld\', bytearray(b\'hello\')) ``` **Constraints**: - The input strings will only contain ASCII characters. **Performance Requirements**: - The solution should be efficient and avoid unnecessary operations, focusing on demonstrating the proper use of the provided bytearray API functions and macros. **Note**: Assume that the necessary Python C API functions are available and correctly imported for use within the function.","solution":"def process_bytearrays(a: str, b: str) -> tuple: Process two strings using bytearray operations. Parameters: a (str): First input string. b (str): Second input string. Returns: tuple: A tuple containing the concatenated bytearray, the size of the concatenated bytearray before resizing, the string representation of the concatenated bytearray, and the resized concatenated bytearray. # Create bytearrays from the given input strings bytearray_a = bytearray(a, \'ascii\') bytearray_b = bytearray(b, \'ascii\') # Concatenate the two bytearrays concatenated = bytearray_a + bytearray_b # Get the size of the concatenated bytearray size_before_resizing = len(concatenated) # Extract the contents of the concatenated bytearray as a plain string concatenated_string = concatenated.decode(\'ascii\') # Resize the concatenated bytearray to half its original size concatenated = concatenated[:size_before_resizing // 2] # Return the required tuple return (concatenated, size_before_resizing, concatenated_string, concatenated)"},{"question":"**Coding Assessment Question: Dodge Transform in Seaborn** # Objective Your task is to create several visualizations using Seaborn with a focus on the `Dodge` transform. You will work with the \'tips\' dataset, and demonstrate your understanding of how to manipulate and adjust bar plots using `Dodge`. # Dataset The \'tips\' dataset is provided by Seaborn\'s `load_dataset` method. It contains information about the tips received by waitstaff in a restaurant, along with other attributes. # Instructions 1. Load the \'tips\' dataset using Seaborn\'s `load_dataset` method and convert the `time` column to type `str`. 2. Create the following visualizations using Seaborn Objects and the `Dodge` transform: a. **Visualization 1:** A bar plot showing the count of tips received on each day of the week, with bars colored by `time`. Use the `Dodge` transform to separate bars by the `time` variable, and handle empty spaces by filling them. b. **Visualization 2:** A bar plot showing the total bill amount for each day, aggregated by the sum of `total_bill`, with bars colored by `sex`. Use the `Dodge` transform to add a gap of 0.1 between bars. c. **Visualization 3:** A dot plot showing the tips received, dodging the dots by both `sex` and `smoker`. Show how to apply the `Dodge` transform only to the `sex` variable, without affecting the `smoker` variable. d. **Visualization 4:** Combine the `Dodge` and `Jitter` transforms to create a dot plot showing the total bill amount by day, colored by `time`, with jitter for better visibility and dodging to separate `time`. # Requirements - Use the Seaborn objects (Seaborn Objects API) to create the plots. - Apply the `Dodge` transform as specified in each visualization. - Ensure the plots are well-labeled and clearly distinguish between different categories. # Example ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\'tips\').astype({\\"time\\": str}) # Visualization 1 plot1 = so.Plot(tips, \\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) # Visualization 2 plot2 = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) # Visualization 3 plot3 = so.Plot(tips, \\"day\\", \\"tip\\", color=\\"sex\\").add(so.Dot(), so.Dodge(by=[\\"color\\"]), fill=\\"smoker\\") # Visualization 4 plot4 = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\").add(so.Dot(), so.Dodge(), so.Jitter()) ``` # Submission Submit your solution as a Python script or a Jupyter notebook containing the required visualizations and code.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_visualizations(): # Load the dataset tips = load_dataset(\'tips\').astype({\\"time\\": str}) # Visualization 1 plot1 = so.Plot(tips, \\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) # Visualization 2 plot2 = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) # Visualization 3 plot3 = so.Plot(tips, \\"day\\", \\"tip\\", color=\\"sex\\").add(so.Dot(), so.Dodge(by=[\\"color\\"]), fill=\\"smoker\\") # Visualization 4 plot4 = so.Plot(tips, \\"day\\", \\"total_bill\\", color=\\"time\\").add(so.Dot(), so.Dodge(), so.Jitter()) return plot1, plot2, plot3, plot4"},{"question":"# Practical Application of Colorsys Module in Python **Objective**: Implement a function that processes a list of colors in RGB format and converts them into multiple color spaces: YIQ, HLS, and HSV. The function should then convert them back to RGB to verify the round-trip conversion accuracy. # Function Signature: ```python def process_and_verify_color_conversions(rgb_colors: list[tuple[float, float, float]]) -> dict: ``` # Input: - `rgb_colors`: A list of tuples, each containing three floating-point numbers (r, g, b), where each number is between 0 and 1. Example: `[(0.2, 0.4, 0.4), (0.1, 0.5, 0.7)]` # Output: - A dictionary containing: - `yiq_conversions`: List of YIQ color representations corresponding to each input RGB color. - `hls_conversions`: List of HLS color representations corresponding to each input RGB color. - `hsv_conversions`: List of HSV color representations corresponding to each input RGB color. - `rgb_from_yiq`: List of RGB color representations obtained after converting the YIQ back to RGB. - `rgb_from_hls`: List of RGB color representations obtained after converting the HLS back to RGB. - `rgb_from_hsv`: List of RGB color representations obtained after converting the HSV back to RGB. - `conversion_accuracy`: A boolean value indicating if all round-trip conversions back to RGB match the original RGB values up to a small tolerance (e.g., 0.0001). # Constraints and Limitations: 1. The function should handle any list of valid RGB triplets. 2. The conversion accuracy should be verified using a small tolerance level to account for floating-point arithmetic precision. # Example: ```python rgb_colors = [(0.2, 0.4, 0.4), (0.1, 0.5, 0.7)] expected_output = { \'yiq_conversions\': [(0.346, -0.02, 0.084), (0.367, -0.233, -0.034)], \'hls_conversions\': [(0.5, 0.3, 0.333), (0.55, 0.4, 0.714)], \'hsv_conversions\': [(0.5, 0.5, 0.4), (0.55, 0.857141, 0.7)], \'rgb_from_yiq\': [(0.2, 0.4, 0.4), (0.1, 0.5, 0.7)], \'rgb_from_hls\': [(0.2, 0.4, 0.4), (0.1, 0.5, 0.7)], \'rgb_from_hsv\': [(0.2, 0.4, 0.4), (0.1, 0.5, 0.7)], \'conversion_accuracy\': True } assert process_and_verify_color_conversions(rgb_colors) == expected_output ``` # Important Notes: - The example values shown are approximate, your actual returns might slightly differ based on the precision of calculations. - Ensure accuracy up to a small tolerance when comparing results. By solving this question, the student demonstrates comprehension of color space conversions, handling floating-point precision, and verification of round-trip conversion accuracy.","solution":"import colorsys def process_and_verify_color_conversions(rgb_colors): Processes a list of RGB colors and converts them to YIQ, HLS, and HSV, then converts them back to RGB and verifies the conversion accuracy. Args: rgb_colors (list of tuple of float): A list of RGB triplets in the range [0, 1]. Returns: dict: A dictionary containing conversions and verification results. yiq_conversions = [] hls_conversions = [] hsv_conversions = [] rgb_from_yiq = [] rgb_from_hls = [] rgb_from_hsv = [] tolerance = 0.0001 all_conversions_accurate = True for color in rgb_colors: r, g, b = color # Convert RGB to YIQ y, i, q = colorsys.rgb_to_yiq(r, g, b) yiq_conversions.append((y, i, q)) # Convert YIQ back to RGB r_yiq, g_yiq, b_yiq = colorsys.yiq_to_rgb(y, i, q) rgb_from_yiq.append((r_yiq, g_yiq, b_yiq)) # Convert RGB to HLS h, l, s = colorsys.rgb_to_hls(r, g, b) hls_conversions.append((h, l, s)) # Convert HLS back to RGB r_hls, g_hls, b_hls = colorsys.hls_to_rgb(h, l, s) rgb_from_hls.append((r_hls, g_hls, b_hls)) # Convert RGB to HSV h, s, v = colorsys.rgb_to_hsv(r, g, b) hsv_conversions.append((h, s, v)) # Convert HSV back to RGB r_hsv, g_hsv, b_hsv = colorsys.hsv_to_rgb(h, s, v) rgb_from_hsv.append((r_hsv, g_hsv, b_hsv)) # Check conversion accuracy for (original, from_yiq, from_hls, from_hsv) in zip(color, (r_yiq, g_yiq, b_yiq), (r_hls, g_hls, b_hls), (r_hsv, g_hsv, b_hsv)): if not (abs(original - from_yiq) < tolerance and abs(original - from_hls) < tolerance and abs(original - from_hsv) < tolerance): all_conversions_accurate = False return { \'yiq_conversions\': yiq_conversions, \'hls_conversions\': hls_conversions, \'hsv_conversions\': hsv_conversions, \'rgb_from_yiq\': rgb_from_yiq, \'rgb_from_hls\': rgb_from_hls, \'rgb_from_hsv\': rgb_from_hsv, \'conversion_accuracy\': all_conversions_accurate }"},{"question":"# Custom Importer Implementation You are tasked with implementing a custom importer that imports Python modules from a given directory and executes a specific function within these modules dynamically. This question will test your understanding of the `importlib` module, particularly `importlib.abc`, and `importlib.util`. Requirements: 1. Create a custom importer class `CustomImporter` that inherits from `importlib.abc.Loader`. 2. The `CustomImporter` should be able to import any Python module from a specified directory. 3. The custom importer should dynamically execute a function named `run` within the imported modules if they contain such a function. 4. The custom importer should handle any errors related to importing modules and executing the `run` function gracefully. Input: - A string representing the directory path containing the Python modules. - A list of module names (without the `.py` extension) to be imported from the specified directory. Output: - For each module in the input list, if it contains a `run` function, print the output of this function. - If a module does not contain a `run` function, print a message indicating that the function is not found. - Handle any import errors and print a corresponding message. Example: Assume the directory `/path/to/modules` contains the following Python files: 1. `module1.py` ```python def run(): return \\"Running module1\\" ``` 2. `module2.py` ```python def run(): return \\"Running module2\\" ``` 3. `module3.py` ```python def some_other_function(): return \\"This is some other function\\" ``` 4. `module4.py` (this file does not exist) ```python # No run function ``` **Input:** ```python directory_path = \\"/path/to/modules\\" module_names = [\\"module1\\", \\"module2\\", \\"module3\\", \\"module4\\"] ``` **Output:** ``` Running module1 Running module2 Module module3 does not contain a \\"run\\" function. Failed to import module module4. ``` Constraints: - You should use only the standard library for this task. - The directory will contain valid Python scripts with `.py` extension. - The `run` function, if present, will not require any arguments. Code Template: ```python import importlib.abc import importlib.util import os class CustomImporter(importlib.abc.Loader): def __init__(self, path): self.path = path def create_module(self, spec): return None def exec_module(self, module): # Implementation to load and execute the module. pass def load_module(self, module_name): # Implementation to load the module. pass def run_custom_importer(directory_path, module_names): importer = CustomImporter(directory_path) for module_name in module_names: try: # Attempt to import the module and execute the run function pass except ImportError: print(f\\"Failed to import module {module_name}.\\") except AttributeError: print(f\\"Module {module_name} does not contain a \\"run\\" function.\\") # Example usage directory_path = \\"/path/to/your/modules\\" module_names = [\\"module1\\", \\"module2\\", \\"module3\\", \\"module4\\"] run_custom_importer(directory_path, module_names) ``` Complete the implementation of the `CustomImporter` class and the `run_custom_importer` function.","solution":"import importlib.util import os import sys class CustomImporter(importlib.abc.Loader): def __init__(self, path): self.path = path def create_module(self, spec): return None def exec_module(self, module): with open(module.__file__, \'r\') as f: code = f.read() exec(code, module.__dict__) def load_module(self, module_name): module_path = os.path.join(self.path, module_name + \'.py\') if not os.path.exists(module_path): raise ImportError(f\\"Module file {module_path} not found\\") spec = importlib.util.spec_from_file_location(module_name, module_path) module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module self.exec_module(module) return module def run_custom_importer(directory_path, module_names): importer = CustomImporter(directory_path) for module_name in module_names: try: module = importer.load_module(module_name) if hasattr(module, \'run\'): result = module.run() print(result) else: print(f\\"Module {module_name} does not contain a \\"run\\" function.\\") except ImportError: print(f\\"Failed to import module {module_name}.\\") except Exception as e: print(f\\"An error occurred while running module {module_name}: {e}\\")"},{"question":"**Objective:** Demonstrate your understanding and ability to utilize seaborn\'s `relplot` function to create informative and well-structured visualizations. **Task:** You are provided with the Titanic dataset, which contains information about passengers. Your task is to create two plots using seaborn\'s `relplot`. 1. **Scatter Plot:** Create a scatter plot to show the relationship between passengers\' age and fare, colored by their embarkation port (`embarked`). Facet this plot to show separate plots for males and females. 2. **Line Plot:** Create a line plot to show the trend of average fare over the age of passengers, separated by their passenger class (`pclass`). This plot should have separate lines for each `pclass` and should include confidence intervals for the mean fare at each age. **Dataset:** You can load the Titanic dataset using the following code: ```python import seaborn as sns titanic = sns.load_dataset(\\"titanic\\") ``` **Requirements:** - Use `seaborn.relplot` to create both plots. - For the scatter plot, use `hue` to differentiate by `embarked` and use `col` to create facets for the `sex` of the passengers. - For the line plot, use `kind=\\"line\\"`, `hue` to differentiate by `pclass`, and include confidence intervals. - Customize the plots to improve clarity: adjust heights and aspect ratios, and ensure proper labeling of axes and titles. **Constraints:** - Ensure the plots are clear and informative, making appropriate use of the `hue`, `col`, `row`, and other relevant parameters. - Your solution should be efficient with a runtime that handles a dataset of this size comfortably. **Example Output:** ```python # Scatter Plot sns.relplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"embarked\\", col=\\"sex\\", kind=\\"scatter\\", height=5, aspect=1) plt.show() # Line Plot sns.relplot(data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"pclass\\", kind=\\"line\\", height=5, aspect=1.5) plt.show() ``` These snippets exemplify the type of plots you are expected to create, with necessary customizations for aesthetics and readability.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_plots(): # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Scatter Plot scatter_plot = sns.relplot( data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"embarked\\", col=\\"sex\\", kind=\\"scatter\\", height=5, aspect=1 ) scatter_plot.set_axis_labels(\\"Age\\", \\"Fare\\") scatter_plot.set_titles(\\"{col_name} Passengers\\") scatter_plot.fig.suptitle(\\"Scatter Plot of Age vs Fare by Embarkation Port and Sex\\", y=1.05) plt.show() # Line Plot line_plot = sns.relplot( data=titanic, x=\\"age\\", y=\\"fare\\", hue=\\"pclass\\", kind=\\"line\\", ci=\\"sd\\", height=5, aspect=1.5 ) line_plot.set_axis_labels(\\"Age\\", \\"Average Fare\\") line_plot.fig.suptitle(\\"Average Fare Trend Over Age by Passenger Class\\", y=1.05) plt.show()"},{"question":"Objective: In this coding challenge, you are required to implement a multioutput regression model using scikit-learn. You will load a synthetic dataset, fit a multioutput regression model, and evaluate its performance. Dataset: You will generate a synthetic dataset with 200 samples, 10 features, and 3 targets using scikit-learn\'s `make_regression` function. Task: 1. Generate the synthetic dataset with the specified properties. 2. Implement a multioutput regression model using the `MultiOutputRegressor` meta-estimator with `RandomForestRegressor` as the base estimator. 3. Fit the model on the generated dataset. 4. Predict the targets using the fitted model. 5. Evaluate and print the mean squared error for each output using scikit-learn’s `mean_squared_error` function. Input: - No direct input from the user; the data is generated within the code. Output: - Mean squared error for each output dimension. Constraints: - Use a random seed of 42 for reproducibility. Implementation requirements: - Import statements for necessary libraries. - Generate the synthetic dataset. - Implement the multioutput regression model with the specified configurations. - Compute and print the mean squared error for each output dimension. ```python # Your code starts here # Step 1: Import necessary libraries import numpy as np from sklearn.datasets import make_regression from sklearn.ensemble import RandomForestRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.metrics import mean_squared_error # Step 2: Generate the synthetic dataset X, y = make_regression(n_samples=200, n_features=10, n_targets=3, random_state=42) # Step 3: Create and fit the MultiOutputRegressor with RandomForestRegressor as the base estimator base_regressor = RandomForestRegressor(random_state=42) multi_output_regressor = MultiOutputRegressor(base_regressor) multi_output_regressor.fit(X, y) # Step 4: Predict the targets y_pred = multi_output_regressor.predict(X) # Step 5: Evaluate and print the mean squared error for each output mse = mean_squared_error(y, y_pred, multioutput=\'raw_values\') print(\\"Mean Squared Error for each output dimension:\\") print(mse) # Your code ends here ``` Example Output: ``` Mean Squared Error for each output dimension: [2501.237, 2637.559, 2105.786] # Note: These values are hypothetical and may vary ```","solution":"# Step 1: Import necessary libraries import numpy as np from sklearn.datasets import make_regression from sklearn.ensemble import RandomForestRegressor from sklearn.multioutput import MultiOutputRegressor from sklearn.metrics import mean_squared_error def multioutput_regression_mse(): Generates a synthetic dataset, fits a multioutput regression model, predicts, and evaluates the mean squared error. Returns: mse (np.ndarray): Mean Squared Error for each output dimension. # Step 2: Generate the synthetic dataset X, y = make_regression(n_samples=200, n_features=10, n_targets=3, random_state=42) # Step 3: Create and fit the MultiOutputRegressor with RandomForestRegressor as the base estimator base_regressor = RandomForestRegressor(random_state=42) multi_output_regressor = MultiOutputRegressor(base_regressor) multi_output_regressor.fit(X, y) # Step 4: Predict the targets y_pred = multi_output_regressor.predict(X) # Step 5: Evaluate and print the mean squared error for each output mse = mean_squared_error(y, y_pred, multioutput=\'raw_values\') return mse"},{"question":"**Objective:** You are given a task to evaluate the performance of a machine learning model using different cross-validation strategies. Your task is to implement a function that performs k-fold cross-validation, stratified k-fold cross-validation, and leave-one-out cross-validation on a given dataset. Use scikit-learn\'s `cross_val_score` function to compute the evaluation metrics. **Problem Statement:** 1. Load the Iris dataset from scikit-learn. 2. Implement a function `evaluate_model` that takes in the following parameters: - `model`: A scikit-learn classifier. - `X`: Features of the dataset. - `y`: Target variable of the dataset. - `cv_strategy`: A string specifying the cross-validation strategy (e.g., \\"kfold\\", \\"stratified_kfold\\", \\"loo\\"). - `k`: Number of folds for k-fold and stratified k-fold cross-validation (default is 5). 3. The function should return the mean accuracy and the standard deviation of the accuracy scores for each cross-validation strategy. **Requirements:** 1. Use the `train_test_split` function to split the dataset into training and testing sets. 2. Use the `cross_val_score` function to perform cross-validation and compute accuracy scores. 3. Implement the following cross-validation strategies: - K-fold cross-validation. - Stratified k-fold cross-validation. - Leave-one-out cross-validation. **Function Signature:** ```python def evaluate_model(model, X, y, cv_strategy, k=5): pass ``` **Example:** ```python from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import train_test_split import numpy as np # Load the Iris dataset X, y = datasets.load_iris(return_X_y=True) # Instantiate a SVM classifier model = SVC(kernel=\'linear\', C=1) # Evaluate the model using k-fold cross-validation mean_acc, std_acc = evaluate_model(model, X, y, cv_strategy=\'kfold\', k=5) print(\\"K-Fold CV - Mean Accuracy: {:.2f}, Std Dev: {:.2f}\\".format(mean_acc, std_acc)) # Evaluate the model using stratified k-fold cross-validation mean_acc, std_acc = evaluate_model(model, X, y, cv_strategy=\'stratified_kfold\', k=5) print(\\"Stratified K-Fold CV - Mean Accuracy: {:.2f}, Std Dev: {:.2f}\\".format(mean_acc, std_acc)) # Evaluate the model using leave-one-out cross-validation mean_acc, std_acc = evaluate_model(model, X, y, cv_strategy=\'loo\') print(\\"Leave-One-Out CV - Mean Accuracy: {:.2f}, Std Dev: {:.2f}\\".format(mean_acc, std_acc)) ``` **Constraints:** - Use appropriate imports from the `sklearn.model_selection` module. - Ensure that your function is efficient and handles the dataset correctly. **Note:** - You may assume that the dataset is balanced and preprocessed. - Focus on implementing the cross-validation logic and computing the metrics correctly.","solution":"from sklearn.model_selection import KFold, StratifiedKFold, LeaveOneOut, cross_val_score import numpy as np def evaluate_model(model, X, y, cv_strategy, k=5): if cv_strategy == \'kfold\': cv = KFold(n_splits=k, shuffle=True, random_state=42) elif cv_strategy == \'stratified_kfold\': cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42) elif cv_strategy == \'loo\': cv = LeaveOneOut() else: raise ValueError(\\"Invalid cv_strategy. Choose \'kfold\', \'stratified_kfold\' or \'loo\'\\") scores = cross_val_score(model, X, y, cv=cv, scoring=\'accuracy\') return np.mean(scores), np.std(scores)"},{"question":"<|Analysis Begin|> The provided documentation primarily focuses on the `torch.backends` module, which controls the behavior of various backends that PyTorch supports. These backends encompass hardware accelerations and computational libraries like CPU, CUDA, cuDNN, and many others. The documentation goes through various backend-related functionalities, such as enabling or disabling specific backend features, querying the availability of libraries, and controlling performance-related settings. Key points covered in the documentation include: - Checking availability of backends and libraries (e.g., CUDA, cuDNN) - Enabling or disabling specific backend features - Performance tuning options like TensorFloat-32 usage, reduced precision operations, and convolution benchmarks - Configuration of specific backend-related caches (like cuFFT plan cache) - Utilizing specific backend libraries for certain operations Given this documentation, we can design a question that assesses the understanding of controlling and utilizing PyTorch backends, querying their capabilities, and implementing a small computation to leverage these settings. <|Analysis End|> <|Question Begin|> # Question: Configuring and Utilizing PyTorch Backends You are tasked with implementing a function in PyTorch that checks the availability of specific backends, configures certain settings based on the availability, and then runs a matrix multiplication using these settings. The function should perform the following tasks: 1. **Check Backend Availability**: - Ensure that CUDA and cuDNN backends are available. - If CUDA is not available, raise an `EnvironmentError` with the message \\"CUDA backend is not available\\". - If cuDNN is not available, set the `use_cudnn` flag to `False`. 2. **Configure Backend Settings**: - If CUDA is available, enable TensorFloat-32 usage (`allow_tf32`) and reduced precision reduction for FP16 (`allow_fp16_reduced_precision_reduction`). - Set cuDNN\'s deterministic mode to `True` if `use_cudnn` is `True`. 3. **Matrix Multiplication**: - Create two random matrices of size 1024x1024 in CUDA memory. - Perform matrix multiplication using the configured settings. - Return the result of the matrix multiplication. Prototype ```python import torch def configure_and_multiply(): # Check CUDA and cuDNN availability if not torch.backends.cuda.is_built(): raise EnvironmentError(\\"CUDA backend is not available\\") cudnn_available = torch.backends.cudnn.is_available() use_cudnn = True if cudnn_available else False # Configure CUDA backend settings if torch.backends.cuda.is_built(): torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True # Configure cuDNN settings torch.backends.cudnn.deterministic = use_cudnn # Perform matrix multiplication device = torch.device(\'cuda\') matrix1 = torch.randn(1024, 1024, device=device) matrix2 = torch.randn(1024, 1024, device=device) result = torch.matmul(matrix1, matrix2) return result # Example usage: result = configure_and_multiply() print(result) ``` Constraints: 1. Assume the function will run on an environment with CUDA-enabled devices. 2. Performance considerations and accuracy are important for matrix multiplication. 3. Make sure all operations are performed on CUDA, and settings are configured appropriately. 4. Minimize memory usage where possible while ensuring correctness. Implement the `configure_and_multiply` function as described above, ensuring to follow the steps and constraints provided.","solution":"import torch def configure_and_multiply(): # Check CUDA and cuDNN availability if not torch.cuda.is_available(): raise EnvironmentError(\\"CUDA backend is not available\\") cudnn_available = torch.backends.cudnn.is_available() use_cudnn = True if cudnn_available else False # Configure CUDA backend settings torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = True # Configure cuDNN settings torch.backends.cudnn.deterministic = use_cudnn # Perform matrix multiplication device = torch.device(\'cuda\') matrix1 = torch.randn(1024, 1024, device=device) matrix2 = torch.randn(1024, 1024, device=device) result = torch.matmul(matrix1, matrix2) return result"},{"question":"Objective: Write a Python function using seaborn that generates a customized plot with specific axis limits from a given dataset. Function Signature: ```python def customize_plot(x_values: list, y_values: list, x_limits: tuple, y_limits: tuple, invert_y: bool) -> \\"Plot\\": pass ``` Input: 1. `x_values`: A list of numerical values representing the x-axis data points. 2. `y_values`: A list of numerical values representing the y-axis data points. 3. `x_limits`: A tuple of two numerical values representing the minimum and maximum x-axis limits. 4. `y_limits`: A tuple of two numerical values representing the minimum and maximum y-axis limits. 5. `invert_y`: A Boolean value indicating whether to invert the y-axis or not. Output: - Returns a `seaborn.objects.Plot` object with the specified customizations. Constraints: 1. The length of `x_values` and `y_values` should be equal. 2. Both `x_limits` and `y_limits` should be tuples of exactly 2 numerical values or `None`. 3. The function should handle cases where limits in tuples are set to `None` to use default values. Requirements: - Create a basic line plot with markers using the provided `x_values` and `y_values`. - Set the x-axis limits according to `x_limits`. - Set the y-axis limits according to `y_limits`. - If `invert_y` is `True`, invert the y-axis limits. - Maintain all other default seaborn plot settings. Example: ```python # Example Input: x_values = [1, 2, 3, 4, 5] y_values = [5, 3, 4, 2, 1] x_limits = (0, 6) y_limits = (0, 6) invert_y = True # Function Call: plot = customize_plot(x_values, y_values, x_limits, y_limits, invert_y) # Expected Output: # A seaborn.objects.Plot object with the y-axis inverted and limits set accordingly. ``` Additional Notes: - Ensure that your function properly handles edge cases. - Include proper documentation in your code explaining how your function works.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plot(x_values: list, y_values: list, x_limits: tuple, y_limits: tuple, invert_y: bool): Creates a seaborn plot with specified axis limits and inversion of y-axis if needed. Args: x_values (list): List of numbers for x-axis. y_values (list): List of numbers for y-axis. x_limits (tuple): Tuple specifying the x-axis limits, (min, max). y_limits (tuple): Tuple specifying the y-axis limits, (min, max). invert_y (bool): If True, inverts the y-axis. Returns: sns.FacetGrid: The seaborn plot object. # Create the plot plot = sns.lineplot(x=x_values, y=y_values, marker=\\"o\\") # Set x-axis limits if specified if x_limits is not None and len(x_limits) == 2: if x_limits[0] is not None and x_limits[1] is not None: plt.xlim(x_limits) # Set y-axis limits if specified if y_limits is not None and len(y_limits) == 2: if y_limits[0] is not None and y_limits[1] is not None: plt.ylim(y_limits) # Invert y-axis if specified if invert_y: plt.gca().invert_yaxis() return plot"},{"question":"# Question: You are tasked with writing a Python script that processes large amounts of data by splitting it into smaller temporary files, processes each file, and then aggregates the results into a final output file. The script must ensure that all temporary files and directories are properly cleaned up, even in cases of errors. Requirements: 1. Write a function `split_data_to_tempfiles(data: str, chunk_size: int) -> List[str]` that takes a large string `data` and an integer `chunk_size`, splits the data into chunks of `chunk_size`, writes each chunk to a temporary file, and returns a list of the paths to these temporary files. 2. Implement a function `process_tempfiles(file_paths: List[str]) -> str` that takes a list of file paths, reads and processes the content of each file, and returns the aggregated result. 3. Ensure that all temporary files are removed after they have been processed, even if an error occurs during processing. 4. Write a `main` function that demonstrates the use of the above functions: - It should create a large sample string. - Split it into temporary files using `split_data_to_tempfiles`. - Process these files using `process_tempfiles`. - Output the final result to a permanent file named `final_output.txt`. Function Definitions: ```python def split_data_to_tempfiles(data: str, chunk_size: int) -> List[str]: # Your implementation here def process_tempfiles(file_paths: List[str]) -> str: # Your implementation here def main(): sample_data = Your large sample data here chunk_size = 100 # Adjust as needed temp_file_paths = split_data_to_tempfiles(sample_data, chunk_size) try: result = process_tempfiles(temp_file_paths) with open(\\"final_output.txt\\", \\"w\\") as final_output_file: final_output_file.write(result) finally: # Cleanup temporary files for file_path in temp_file_paths: os.remove(file_path) if __name__ == \\"__main__\\": main() ``` Input and Output: - **Input:** The input is embedded within the `main` function. - **Output:** The output should be written to a file named `final_output.txt`. Constraints: - The `chunk_size` will always be a positive integer smaller than the length of `data`. - `data` will contain only standard ASCII characters. Performance Requirements: - The function should handle large data efficiently by processing data in chunks and writing to temporary files. Example: ```python def main(): sample_data = \\"This is a large sample data string that needs to be processed in chunks.\\" chunk_size = 10 temp_file_paths = split_data_to_tempfiles(sample_data, chunk_size) try: result = process_tempfiles(temp_file_paths) with open(\\"final_output.txt\\", \\"w\\") as final_output_file: final_output_file.write(result) finally: for file_path in temp_file_paths: os.remove(file_path) if __name__ == \\"__main__\\": main() # The final_output.txt should contain the processed data. ``` Ensure that the above requirements and constraints are met. Use `tempfile` module functions and manage resources effectively.","solution":"import os import tempfile from typing import List def split_data_to_tempfiles(data: str, chunk_size: int) -> List[str]: file_paths = [] data_size = len(data) for i in range(0, data_size, chunk_size): chunk = data[i:i + chunk_size] temp_file = tempfile.NamedTemporaryFile(delete=False, mode=\'w\', suffix=\'.txt\') temp_file.write(chunk) temp_file.close() file_paths.append(temp_file.name) return file_paths def process_tempfiles(file_paths: List[str]) -> str: result = [] for file_path in file_paths: try: with open(file_path, \'r\') as file: result.append(file.read()) except Exception as e: print(f\\"Error processing file {file_path}: {e}\\") raise finally: os.remove(file_path) return \'\'.join(result) def main(): sample_data = \\"This is a large sample data string that needs to be processed in chunks.\\" chunk_size = 10 temp_file_paths = split_data_to_tempfiles(sample_data, chunk_size) try: result = process_tempfiles(temp_file_paths) with open(\\"final_output.txt\\", \\"w\\") as final_output_file: final_output_file.write(result) except Exception as e: print(f\\"Error during processing: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Objective Implement a custom command for the deprecated Distutils package that counts and displays the number of Python files (.py) in the source directory of a given Python package. # Task Write a class `CountPyFiles` that extends `distutils.cmd.Command` and provides functionality to count Python files (.py) in the source directory. Your task includes: - Implementing the necessary attributes and methods to integrate with Distutils. - Displaying the number of Python files found. # Implementation Requirements 1. **Attributes:** - `description`: A string describing the command. - `user_options`: An empty list since this command does not accept any additional options. 2. **Methods:** - `initialize_options(self)`: Initialize options (can be a no-op). - `finalize_options(self)`: Finalize options (can be a no-op). - `run(self)`: Implement the logic to count Python files and display the count. 3. **Distribution Integration:** - Modify the `setup.py` script to include the custom command. # Example Usage After implementing the above command, a user can run the following command to count Python files: ```shell python setup.py count_py_files ``` # Sample Code Structure Provide the following in your implementation: 1. Custom Command Class: ```python from distutils.cmd import Command import os class CountPyFiles(Command): description = \\"Count the number of Python files in the source directory\\" user_options = [] def initialize_options(self): pass def finalize_options(self): pass def run(self): py_files = [f for f in os.listdir(\'.\') if f.endswith(\'.py\')] count = len(py_files) print(f\\"Number of Python files: {count}\\") ``` 2. Modified `setup.py` to integrate custom command: ```python from distutils.core import setup setup( name=\'ExamplePackage\', version=\'1.0\', description=\'Example Python Package\', packages=[\'examplepackage\'], cmdclass={ \'count_py_files\': CountPyFiles, } ) ``` # Constraints - Ensure compatibility with Python 3.10. - Ensure the implementation follows the standard practices for extending Distutils. # Evaluation Criteria - Correctness: Whether the custom command correctly counts and displays the number of Python files. - Integration: Properly modifying `setup.py` to include the custom command. - Code Quality: Adherence to Python coding standards and readability.","solution":"from distutils.cmd import Command import os class CountPyFiles(Command): description = \\"Count the number of Python files in the source directory\\" user_options = [] def initialize_options(self): pass def finalize_options(self): pass def run(self): py_files = [f for f in os.listdir(\'.\') if f.endswith(\'.py\')] count = len(py_files) print(f\\"Number of Python files: {count}\\")"},{"question":"# Question: Advanced Data Visualization with Pandas You are provided with a dataset containing information about stocks\' closing prices over a period of time across multiple companies. Your task is to implement a function `visualize_stock_data` that performs several visualization tasks on this dataset. Function Signature ```python def visualize_stock_data(df: pd.DataFrame): pass ``` # Input: - `df`: A pandas DataFrame containing stock prices with the following columns: - `Date`: Date of the record (dtype: datetime64). - `Company_A`: Closing price of Company A\'s stock (dtype: float64). - `Company_B`: Closing price of Company B\'s stock (dtype: float64). - `Company_C`: Closing price of Company C\'s stock (dtype: float64). # Requirements: 1. **Line Plot with Customization**: - Generate a line plot showing the closing prices of all three companies over time. - Customize the plot to include the title \\"Stock Prices Over Time\\", xlabel as \\"Date\\", and ylabel as \\"Closing Price\\". - Set different colors for each company and include a legend outside the plot on the right. 2. **Scatter Plot**: - Create a scatter plot comparing the closing prices of `Company_A` and `Company_B`. - Use `Company_C`\'s prices to determine the color of the points (use a colormap). - Add a colorbar to the plot. 3. **Box Plot**: - Generate a box plot to visualize the distribution of closing prices for each company in a single plot. - Include custom colors for the boxes and ensure the plot has appropriate labels for clarity. 4. **Hexbin Plot**: - Create a hexbin plot to visualize the relationship between `Company_A` and `Company_B` stock prices. - Use gridsize of 25 and aggregate by the mean of the third variable `Company_C`. 5. **Parallel Coordinates Plot**: - Use the `parallel_coordinates` function to visualize the stock data. - The Company names should be used as different labels. # Constraints: - You may assume there are no missing values in the provided DataFrame. - Pandas and Matplotlib libraries will be available. # Example Usage: ```python import pandas as pd from datetime import datetime data = { \\"Date\\": pd.date_range(start=\\"2020-01-01\\", periods=100, freq=\\"D\\"), \\"Company_A\\": np.random.rand(100) * 100, \\"Company_B\\": np.random.rand(100) * 200, \\"Company_C\\": np.random.rand(100) * 50 } df = pd.DataFrame(data) visualize_stock_data(df) ``` # Note: The `visualize_stock_data` function should not return any value. It should display the plots directly.","solution":"import pandas as pd import numpy as np import matplotlib.pyplot as plt from pandas.plotting import parallel_coordinates def visualize_stock_data(df: pd.DataFrame): This function takes a DataFrame containing stock prices and performs several visualization tasks. # Line Plot with Customization plt.figure(figsize=(12, 6)) plt.plot(df[\'Date\'], df[\'Company_A\'], label=\'Company A\', color=\'b\') plt.plot(df[\'Date\'], df[\'Company_B\'], label=\'Company B\', color=\'g\') plt.plot(df[\'Date\'], df[\'Company_C\'], label=\'Company C\', color=\'r\') plt.title(\'Stock Prices Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Closing Price\') plt.legend(loc=\'center left\', bbox_to_anchor=(1, 0.5)) plt.show() # Scatter Plot plt.figure(figsize=(10, 6)) scatter = plt.scatter(df[\'Company_A\'], df[\'Company_B\'], c=df[\'Company_C\'], cmap=\'viridis\') plt.colorbar(scatter, label=\'Company C Prices\') plt.xlabel(\'Company A Prices\') plt.ylabel(\'Company B Prices\') plt.title(\'Company A vs Company B Prices\') plt.show() # Box Plot df_boxplot = df[[\'Company_A\', \'Company_B\', \'Company_C\']] plt.figure(figsize=(10, 6)) boxprops = dict(linestyle=\'-\', linewidth=2.5, color=\'b\') medianprops = dict(linestyle=\'-\', linewidth=2.5, color=\'r\') plt.boxplot(df_boxplot.values, patch_artist=True, boxprops=boxprops, medianprops=medianprops) plt.xticks([1, 2, 3], df_boxplot.columns) plt.title(\'Distribution of Closing Prices\') plt.xlabel(\'Company\') plt.ylabel(\'Closing Price\') plt.show() # Hexbin Plot plt.figure(figsize=(10, 6)) hb = plt.hexbin(df[\'Company_A\'], df[\'Company_B\'], gridsize=25, cmap=\'Blues\', C=df[\'Company_C\'], reduce_C_function=np.mean) cb = plt.colorbar(hb, label=\'Mean Company C Prices\') plt.xlabel(\'Company A Prices\') plt.ylabel(\'Company B Prices\') plt.title(\'Hexbin Plot between Company A and Company B\') plt.show() # Parallel Coordinates Plot plt.figure(figsize=(12, 8)) parallel_coordinates(df, class_column=\'Date\', cols=[\'Company_A\', \'Company_B\', \'Company_C\'], color=(\'#556270\', \'#4ECDC4\', \'#C7F464\')) plt.title(\'Parallel Coordinates Plot for Stock Data\') plt.xlabel(\'Companies\') plt.ylabel(\'Prices\') plt.show()"},{"question":"**Complexity Level:** Advanced # **Question: Functional Programming with Python Modules** You are given three lists of integers `list1`, `list2`, and `list3` of the same length `n`. Your task is to generate a new list that contains the sum of all possible combinations of one element from each of the lists, but you should not compute these sums directly. Instead, you are required to leverage functional programming techniques provided by the `itertools`, `functools`, and `operator` modules. # **Requirements:** 1. Use `itertools` to generate all possible combinations. 2. Use `functools.partial` to create a function that can sum tuples of numbers. 3. Use `operator` to perform the addition operation within the `functools.partial`-created function. # **Input:** - `list1`, `list2`, `list3`: Lists of integers of the same length `n` (1 <= n <= 10^4). # **Output:** - `result`: A list of integers containing the sums of all possible combinations of one number from each list. # **Constraints:** - You must use functional programming styles and avoid traditional loops for combination generation and element-wise operations. - The order of results in the output list does not matter. # **Example:** Input: ```python list1 = [1, 2] list2 = [10, 20] list3 = [100, 200] ``` Output: ```python [111, 211, 121, 221, 112, 212, 122, 222] ``` # **Explanation:** The output list contains the sums of all possible combinations of taking one element from each of `list1`, `list2`, and `list3`. For example, one combination is taking `1` from `list1`, `10` from `list2`, and `100` from `list3`, resulting in the sum `111`. # **Function Signature:** ```python from typing import List def sum_combinations(list1: List[int], list2: List[int], list3: List[int]) -> List[int]: pass # Example usage: print(sum_combinations([1, 2], [10, 20], [100, 200])) # Should output a list with sums of all combinations ``` Use the provided function signature to implement your solution. Ensure your implementation makes effective use of the `itertools`, `functools`, and `operator` modules as per the requirements.","solution":"from typing import List import itertools import functools import operator def sum_combinations(list1: List[int], list2: List[int], list3: List[int]) -> List[int]: # Generate all possible combinations of one element from each list combinations = itertools.product(list1, list2, list3) # Create a partial function for summing the tuples using operator.add sum_tuple = functools.partial(functools.reduce, operator.add) # Map the sum function to all combinations and convert the result back to a list result = list(map(sum_tuple, combinations)) return result"},{"question":"# Dataset Generation and Visualization using scikit-learn **Objective:** Create a synthetic dataset for a classification problem using scikit-learn and visualize it. **Task:** Implement a function `generate_and_visualize_dataset` with the following requirements: 1. **Function Name:** `generate_and_visualize_dataset` 2. **Input Parameters:** - `type_of_dataset` (str): The type of synthetic dataset to generate. Acceptable values are: - `\\"blobs\\"` - `\\"classification\\"` - `\\"moons\\"` - `\\"circles\\"` - `params` (dict): A dictionary of parameters to pass to the underlying dataset generator function. 3. **Output:** - A matplotlib plot visualizing the generated dataset. - The function should also return the feature matrix `X` and target labels `y` as a tuple `(X, y)`. **Details:** 1. Depending on the value of `type_of_dataset`, call the appropriate scikit-learn function to generate the dataset: - For `\\"blobs\\"`: Use `make_blobs`. - For `\\"classification\\"`: Use `make_classification`. - For `\\"moons\\"`: Use `make_moons`. - For `\\"circles\\"`: Use `make_circles`. 2. Pass the parameters from `params` to the chosen dataset generation function. 3. Visualize the generated dataset: - Create a scatter plot of the features. - Use different colors for different target labels. **Constraints:** - You should ensure that the input parameters dictionary `params` contains valid keys and values for the chosen dataset generation function. - Assume that the necessary libraries like matplotlib and sklearn.datasets are already installed. # Example Usage: ```python params = { \\"n_samples\\": 100, \\"centers\\": 3, \\"cluster_std\\": 1.0, \\"random_state\\": 42 } X, y = generate_and_visualize_dataset(\\"blobs\\", params) ``` The above call should plot the generated dataset and return the feature matrix `X` and target labels `y`.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_moons, make_circles def generate_and_visualize_dataset(type_of_dataset, params): Generates a synthetic dataset and visualizes it. Parameters: - type_of_dataset (str): The type of synthetic dataset to generate. - params (dict): Parameters to pass to the dataset generator function. Returns: - tuple: Feature matrix X and target labels y. if type_of_dataset == \\"blobs\\": X, y = make_blobs(**params) elif type_of_dataset == \\"classification\\": X, y = make_classification(**params) elif type_of_dataset == \\"moons\\": X, y = make_moons(**params) elif type_of_dataset == \\"circles\\": X, y = make_circles(**params) else: raise ValueError(\\"Invalid type_of_dataset. Must be \'blobs\', \'classification\', \'moons\', or \'circles\'.\\") plt.figure(figsize=(8, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\', s=50) plt.title(f\\"{type_of_dataset} dataset\\") plt.xlabel(\\"Feature 0\\") plt.ylabel(\\"Feature 1\\") plt.show() return (X, y)"},{"question":"# Secure Login System Implementation **Objective:** Implement a secure login system using the `getpass` module. The system should prompt the user for their username and password securely, verifying the credentials against a predefined set of users. **Task:** 1. Implement a function `get_credentials()` that: - Prompts the user for their username using the `getpass.getuser()` function. - Prompts the user for their password using the `getpass.getpass()` function. - Returns a tuple containing the entered username and password. 2. Implement a function `login_system(users)` that: - Accepts a dictionary `users` where the keys are usernames and the values are the corresponding passwords. - Uses the `get_credentials()` function to obtain the username and password from the user. - Verifies if the entered username exists in the `users` dictionary. - Checks if the entered password matches the password stored in the dictionary for the given username. - Returns `True` if the credentials are correct, otherwise returns `False`. 3. Ensure that the password input is not echoed on the screen when entered by the user. **Input Format:** - The `users` parameter in the `login_system(users)` function is a dictionary with string keys (usernames) and string values (passwords). **Output Format:** - The `login_system(users)` function should return a boolean value: `True` if the login is successful, and `False` otherwise. **Examples:** ```python users = { \\"alice\\": \\"password123\\", \\"bob\\": \\"secure456\\", \\"charlie\\": \\"passwd789\\" } # Example of a secure login system interaction # Enter your username: alice # Password: (input hidden) # If the credentials are correct, the function returns True. # If the credentials are incorrect, the function returns False. result = login_system(users) print(result) # Output depends on user input ``` **Constraints:** - The function must use `getpass.getuser()` and `getpass.getpass()` for input handling. - The password should be securely handled without being echoed on the screen. **Performance Requirements:** - The implemented functions should handle user input efficiently and verify credentials with minimal overhead.","solution":"import getpass def get_credentials(): Prompts the user for their username and password securely, then returns a tuple containing the entered username and password. username = input(\\"Enter your username: \\") password = getpass.getpass(\\"Password: \\") return (username, password) def login_system(users): Accepts a dictionary of users with usernames as keys and passwords as values. Prompts the user to input their credentials and checks them against the dictionary. Parameters: users (dict): Dictionary containing usernames and passwords. Returns: bool: True if credentials are correct, False otherwise. username, password = get_credentials() if username in users and users[username] == password: return True return False"},{"question":"# Advanced Garbage Collection Container Design and implement a custom container type in Python that supports cyclic garbage collection. The objective of this exercise is to create a Python extension module that defines a mutable container type which the garbage collector can manage. Requirements: 1. **Type Creation**: - Define a new container type with the `Py_TPFLAGS_HAVE_GC` flag. - The type must support adding and removing items, simulating a simple list or dictionary. 2. **Memory Management**: - Use `PyObject_GC_New` or `PyObject_GC_NewVar` for object creation. - Use `PyObject_GC_Del` for object deallocation. 3. **GC Handlers**: - Implement the `tp_traverse` function to iterate over references held by the container. - Implement the `tp_clear` function to break reference cycles. 4. **Tracking**: - Ensure that objects are tracked using `PyObject_GC_Track`. - Untrack objects before they are cleared using `PyObject_GC_UnTrack`. Example Operations: - Creation of the container. - Adding items to the container. - Removing items from the container. - Proper clean-up of the container to ensure no memory leaks. Input and Output Format: - **Input**: - A sequence of operations indicating `create`, `add`, `remove`, or `delete` operations. - For example, `[\\"create\\", \\"add:element1\\", \\"add:element2\\", \\"remove:element1\\", \\"delete\\"]`. - **Output**: - Print statements that show the operations being performed. - For example, `[\\"Created container\\", \\"Added element1\\", \\"Added element2\\", \\"Removed element1\\", \\"Deleted container\\"]`. Constraints: - The container should handle any Python object type. - Ensure that cyclic references within the container do not cause memory leaks. - The implementation should be efficient and not excessively impact Python\'s garbage collector performance. Extra Credit: - Write a Python script to test the custom container type, verifying that it handles cyclic references correctly. Use the following template to get started in your Python extension module: ```c #include <Python.h> // Define a new object structure typedef struct { PyObject_HEAD /* Type-specific fields go here. */ } CustomContainerObject; // tp_traverse function static int CustomContainer_traverse(CustomContainerObject *self, visitproc visit, void *arg) { // Add code to visit each contained object here return 0; } // tp_clear function static int CustomContainer_clear(CustomContainerObject *self) { // Add code to clear references here return 0; } // Define the methods of the container static PyMethodDef CustomContainer_methods[] = { // Add methods to add and remove items here {NULL} /* Sentinel */ }; // Define the type object static PyTypeObject CustomContainerType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"customcontainer.CustomContainer\\", .tp_basicsize = sizeof(CustomContainerObject), .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_HAVE_GC, .tp_traverse = (traverseproc)CustomContainer_traverse, .tp_clear = (inquiry)CustomContainer_clear, .tp_methods = CustomContainer_methods, }; // Module definition static struct PyModuleDef customcontainermodule = { PyModuleDef_HEAD_INIT, \\"customcontainer\\", NULL, // Optional module documentation -1, // Size of module state, -1 if module keeps state in global variables NULL, NULL, NULL, NULL, NULL }; // Module initialization function PyMODINIT_FUNC PyInit_customcontainer(void) { PyObject *m; if (PyType_Ready(&CustomContainerType) < 0) return NULL; m = PyModule_Create(&customcontainermodule); if (m == NULL) return NULL; Py_INCREF(&CustomContainerType); PyModule_AddObject(m, \\"CustomContainer\\", (PyObject *)&CustomContainerType); return m; } ``` Start by defining the container structure and adding the appropriate handlers. Then, implement the rest of the methods to create a fully functional garbage-collected container. Good luck!","solution":"from collections.abc import MutableMapping class CustomContainer(MutableMapping): def __init__(self): self.store = {} print(\\"Created container\\") def __setitem__(self, key, value): self.store[key] = value print(f\\"Added {key}:{value}\\") def __getitem__(self, key): return self.store[key] def __delitem__(self, key): del self.store[key] print(f\\"Removed {key}\\") def __iter__(self): return iter(self.store) def __len__(self): return len(self.store) def clear(self): self.store.clear() print(\\"Cleared container\\") def __del__(self): print(\\"Deleted container\\")"},{"question":"**Coding Assessment Question: Implementing a Custom Distributed Optimizer in PyTorch** Objective The objective of this assessment is to evaluate your understanding of PyTorch\'s distributed training capabilities and your ability to implement a custom distributed optimizer to synchronize and update model parameters across multiple nodes. Problem Statement You are required to implement a custom distributed optimizer in PyTorch that mimics the behavior of `torch.optim.SGD` but is capable of synchronizing gradients and updating parameters across multiple computing nodes. Your implementation should follow the basic structure and principles of how distributed optimizers work. Instructions 1. Implement a class `CustomDistributedOptimizer` that inherits from `torch.optim.Optimizer`. 2. Your custom optimizer should support: - Synchronizing gradients across multiple nodes. - Updating parameters using Stochastic Gradient Descent (SGD). 3. You will need to use PyTorch\'s distributed communication package (`torch.distributed`) to handle synchronization. 4. Assume that the environment is already properly initialized for distributed training (i.e., `torch.distributed.init_process_group` has been called). Implementation Details # Class Definition ```python import torch from torch.optim import Optimizer class CustomDistributedOptimizer(Optimizer): def __init__(self, params, lr=0.01): defaults = dict(lr=lr) super(CustomDistributedOptimizer, self).__init__(params, defaults) def step(self, closure=None): for group in self.param_groups: for param in group[\'params\']: if param.grad is None: continue # Synchronize gradients across all processes torch.distributed.all_reduce(param.grad.data, op=torch.distributed.ReduceOp.SUM) param.grad.data /= torch.distributed.get_world_size() # Perform SGD update param.data -= group[\'lr\'] * param.grad.data ``` # Expected Input and Output - **Input**: - `params`: Iterable of parameters to optimize or dicts defining parameter groups. - `lr` (optional): Learning rate (default: 0.01). - **Output**: - The updated model parameters after the `step()` method is called. # Example Usage Assume you have a simple model and want to use your custom optimizer during training in a distributed setting: ```python import torch import torch.distributed as dist # Initialize process group (for distributed training) dist.init_process_group(backend=\'nccl\') # Define a simple model model = torch.nn.Linear(10, 1).cuda() optimizer = CustomDistributedOptimizer(model.parameters(), lr=0.01) # Define a simple loss function criterion = torch.nn.MSELoss() # Dummy input and target input = torch.randn(10).cuda() target = torch.randn(1).cuda() # Forward pass output = model(input) loss = criterion(output, target) # Backward pass loss.backward() # Perform optimizer step optimizer.step() ``` # Constraints and Limitations - Make sure that elements within `params` are CUDA tensors. - Ensure that `torch.distributed.init_process_group` is called prior to using this optimizer. - For simplicity, this baseline implementation does not support features like momentum, weight decay, or other advanced features of SGD. Good luck!","solution":"import torch from torch.optim import Optimizer class CustomDistributedOptimizer(Optimizer): def __init__(self, params, lr=0.01): defaults = dict(lr=lr) super(CustomDistributedOptimizer, self).__init__(params, defaults) def step(self, closure=None): for group in self.param_groups: for param in group[\'params\']: if param.grad is None: continue # Synchronize gradients across all processes torch.distributed.all_reduce(param.grad.data, op=torch.distributed.ReduceOp.SUM) param.grad.data /= torch.distributed.get_world_size() # Perform SGD update param.data -= group[\'lr\'] * param.grad.data"},{"question":"# Question: Implement a Custom HTTP Client Design and implement a custom HTTP client class using the `http.client` module that can handle the following functionalities: 1. **Method Initialization**: The client should be able to initialize an HTTP or HTTPS connection based on the given URL. If the URL starts with `http://`, an `HTTPConnection` should be used. If it starts with `https://`, an `HTTPSConnection` should be used. 2. **Send GET Request**: Implement a method `send_get_request(self, endpoint: str)` that sends a GET request to the specified endpoint and returns the response status code and headers. 3. **Send POST Request**: Implement a method `send_post_request(self, endpoint: str, data: dict)` that sends a POST request to the specified endpoint with form-encoded data and returns the response status code and body. 4. **Handle Redirects**: The client should be able to handle HTTP redirects (status codes 301 and 302) and follow up to a maximum of 3 redirects. 5. **Custom Headers**: Allow the client to set custom headers for each request via a method `set_headers(self, headers: dict)`. # Expected Input and Output: - **Input**: - URL of the host to connect to. - Endpoints (`str`) for GET and POST requests. - Data (`dict`) for POST requests. - Custom headers (`dict`). - **Output**: - For GET requests: status code (`int`) and headers (`dict`). - For POST requests: status code (`int`) and response body (`bytes`). # Constraints: - Only standard libraries in Python should be used. - Network issues and invalid URLs should be appropriately handled. - Maximum of 3 redirects should be followed. # Example ```python client = CustomHttpClient(\\"https://www.example.com\\") client.set_headers({\\"User-Agent\\": \\"CustomClient\\"}) status, headers = client.send_get_request(\\"/\\") post_data = {\'key1\': \'value1\', \'key2\': \'value2\'} status, response_body = client.send_post_request(\\"/submit\\", post_data) ``` Use the documentation provided to implement the class with the described functionalities.","solution":"import http.client import urllib.parse class CustomHttpClient: def __init__(self, url): parsed_url = urllib.parse.urlparse(url) self.scheme = parsed_url.scheme self.host = parsed_url.netloc if self.scheme == \\"http\\": self.connection = http.client.HTTPConnection(self.host) elif self.scheme == \\"https\\": self.connection = http.client.HTTPSConnection(self.host) else: raise ValueError(\\"URL must start with http:// or https://\\") self.headers = {} def set_headers(self, headers): self.headers.update(headers) def send_get_request(self, endpoint): self.connection.request(\\"GET\\", endpoint, headers=self.headers) response = self.connection.getresponse() return response.status, dict(response.getheaders()) def send_post_request(self, endpoint, data): headers = self.headers.copy() headers[\'Content-Type\'] = \'application/x-www-form-urlencoded\' encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') self.connection.request(\\"POST\\", endpoint, body=encoded_data, headers=headers) response = self.connection.getresponse() return response.status, response.read() def handle_redirects(self, response): redirect_count = 0 while response.status in (301, 302) and redirect_count < 3: location = response.getheader(\'Location\') if not location: break parsed_location = urllib.parse.urlparse(location) if not parsed_location.scheme: location = f\\"{self.scheme}://{self.host}{location}\\" self.connection.close() self.__init__(location) self.connection.request(\\"GET\\", parsed_location.path, headers=self.headers) response = self.connection.getresponse() redirect_count += 1 return response # Usage example if __name__ == \\"__main__\\": client = CustomHttpClient(\\"https://www.example.com\\") client.set_headers({\\"User-Agent\\": \\"CustomClient\\"}) status, headers = client.send_get_request(\\"/\\") print(status, headers) post_data = {\'key1\': \'value1\', \'key2\': \'value2\'} status, response_body = client.send_post_request(\\"/submit\\", post_data) print(status, response_body)"},{"question":"Parsing and Analyzing Log Entries Objective The goal of this exercise is to write a Python function that uses regular expressions to parse and analyze log entries from a web server. This will test your understanding of pattern compilation, grouping, and extracting information using the Python `re` module. Description You are given a list of log entries from a web server. Each log entry follows a specific format: ``` 127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \\"GET /apache_pb.gif HTTP/1.0\\" 200 2326 ``` Here\'s what each part of the log entry represents: - **IP address**: The IP address of the client. - **User identifier**: This is typically a dash since most servers don\'t use user identifiers. - **User ID**: The user ID of the person requesting the resource. - **Timestamp**: The date and time of the request. - **Request**: The HTTP method, the resource requested, and the HTTP protocol. - **Status code**: The HTTP response status code. - **Size**: The size of the response in bytes. Task Write a function `parse_log_entries(log_entries)` that takes a list of log entries and returns a list of dictionaries. Each dictionary should contain the extracted parts of the log entry with the following keys: `ip`, `user_id`, `timestamp`, `method`, `resource`, `protocol`, `status_code`, and `size`. Expected Input and Output - **Input:** A list of strings, where each string is a log entry. - **Output:** A list of dictionaries with the extracted parts. Example ```python log_entries = [ \'127.0.0.1 - frank [10/Oct/2000:13:55:36 -0700] \\"GET /apache_pb.gif HTTP/1.0\\" 200 2326\', \'192.168.0.1 - john [11/Oct/2000:14:01:22 -0700] \\"POST /login HTTP/1.1\\" 302 501\' ] expected_output = [ { \'ip\': \'127.0.0.1\', \'user_id\': \'frank\', \'timestamp\': \'10/Oct/2000:13:55:36 -0700\', \'method\': \'GET\', \'resource\': \'/apache_pb.gif\', \'protocol\': \'HTTP/1.0\', \'status_code\': \'200\', \'size\': \'2326\' }, { \'ip\': \'192.168.0.1\', \'user_id\': \'john\', \'timestamp\': \'11/Oct/2000:14:01:22 -0700\', \'method\': \'POST\', \'resource\': \'/login\', \'protocol\': \'HTTP/1.1\', \'status_code\': \'302\', \'size\': \'501\' } ] assert parse_log_entries(log_entries) == expected_output ``` Requirements 1. Use regular expressions to parse the log entries. 2. Your function should handle log entries format precisely as shown above. 3. Assume that the input list will always follow the provided format. 4. Handle different HTTP methods and protocols as shown in the examples. Constraints - You may assume the input log entries will always be correctly formatted as per the provided examples. - The list of log entries may contain up to 1000 entries. Performance Requirements - The function should efficiently parse and extract information from the log entries.","solution":"import re def parse_log_entries(log_entries): Parses a list of log entries from a web server and returns a list of dictionaries with extracted parts of the log entry. Args: log_entries (list): A list of log entry strings. Returns: list: A list of dictionaries with keys: ip, user_id, timestamp, method, resource, protocol, status_code, size. pattern = re.compile( r\'(?P<ip>d+.d+.d+.d+) - (?P<user_id>w+) \' r\'[(?P<timestamp>[^]]+)] \' r\'\\"(?P<method>w+) (?P<resource>[^s]+) (?P<protocol>[^s]+)\\" \' r\'(?P<status_code>d+) (?P<size>d+)\' ) parsed_entries = [] for entry in log_entries: match = pattern.match(entry) if match: parsed_entries.append(match.groupdict()) return parsed_entries"},{"question":"**Objective:** The goal of this assessment is to test your understanding of how to control and customize the aesthetics of plots using seaborn. **Problem Statement:** You are given a dataset containing the monthly average temperature (`temperature`) and precipitation (`precipitation`) values for 4 different years (`year`) for a given location. Your task is to create a set of aesthetically pleasing visualizations using seaborn to analyze this data. **Dataset:** ```python import numpy as np import pandas as pd np.random.seed(0) data = { \'month\': np.tile(np.arange(1, 13), 4), \'year\': np.repeat([2017, 2018, 2019, 2020], 12), \'temperature\': np.random.normal(loc=15, scale=10, size=48), \'precipitation\': np.random.normal(loc=50, scale=20, size=48) } df = pd.DataFrame(data) print(df.head()) ``` **Requirements:** 1. **Basic Plot:** - Create a line plot using seaborn where the x-axis represents months, and the y-axis represents temperature. - Use different colors to distinguish data from different years. - Set the theme to `darkgrid`. 2. **Style Customization:** - Customize the plot to use the `whitegrid` style and remove the top and right spines. - Utilize the `set_style` and `despine` functions for this purpose. 3. **Context Adjustments:** - Temporarily set a context suitable for a presentation poster (`poster`), and scale the elements using `font_scale=1.5` and a custom line width setting for the line plot to `2.5`. - Use the `with` statement to ensure only this plot uses the specific settings. 4. **Comparative Visualizations:** - Create a subplot with 2 columns showing: - A boxplot of temperature for different years. - A boxplot of precipitation for different years. - Apply the following seaborn styles: - `white` for the temperature boxplot. - `dark` for the precipitation boxplot. - Ensure proper layout using `tight_layout`. 5. **Advanced Customization:** - For the temperature line plot (Requirement 1), override the seaborn style to have a light grey background for axes (`axes.facecolor` set to `\'.9\'`). **Implementation:** ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Basic Plot with \'darkgrid\' theme sns.set_theme(style=\'darkgrid\') plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') plt.title(\'Monthly Average Temperature\') plt.show() # Step 2: Style Customization with \'whitegrid\' and despine sns.set_style(\\"whitegrid\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') sns.despine(top=True, right=True) plt.title(\'Monthly Average Temperature (Whitegrid)\') plt.show() # Step 3: Context Adjustments for Poster with sns.plotting_context(\\"poster\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}): plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') plt.title(\'Monthly Average Temperature (Poster Context)\') plt.show() # Step 4: Comparative Visualizations plt.figure(figsize=(15, 6)) plt.subplot(1, 2, 1) sns.set_style(\\"white\\") sns.boxplot(data=df, x=\'year\', y=\'temperature\') plt.title(\'Temperature by Year\') plt.subplot(1, 2, 2) sns.set_style(\\"dark\\") sns.boxplot(data=df, x=\'year\', y=\'precipitation\') plt.title(\'Precipitation by Year\') plt.tight_layout() plt.show() # Step 5: Advanced Customization sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\".9\\"}) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') plt.title(\'Monthly Average Temperature (Custom Background)\') plt.show() ``` **Input:** - Ensure the dataframe `df` is created with the provided code snippet. **Output:** - Five plots as specified in the requirements aiming to showcase your knowledge of seaborn settings, customization, and context management. **Constraints:** - Adhere strictly to the seaborn and matplotlib functions demonstrated in the documentation provided. - Make the plots clear, readable, and informative, with appropriate titles and legend placements. **Performance:** - The visualizations should render without performance issues on standard datasets of up to 10,000 rows.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Dataframe creation for reference import numpy as np import pandas as pd np.random.seed(0) data = { \'month\': np.tile(np.arange(1, 13), 4), \'year\': np.repeat([2017, 2018, 2019, 2020], 12), \'temperature\': np.random.normal(loc=15, scale=10, size=48), \'precipitation\': np.random.normal(loc=50, scale=20, size=48) } df = pd.DataFrame(data) # Step 1: Basic Plot with \'darkgrid\' theme def plot_basic(): sns.set_theme(style=\'darkgrid\') plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') plt.title(\'Monthly Average Temperature\') plt.show() # Step 2: Style Customization with \'whitegrid\' and despine def plot_style_customization(): sns.set_style(\\"whitegrid\\") plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') sns.despine(top=True, right=True) plt.title(\'Monthly Average Temperature (Whitegrid)\') plt.show() # Step 3: Context Adjustments for Poster def plot_context_adjustments(): with sns.plotting_context(\\"poster\\", font_scale=1.5, rc={\\"lines.linewidth\\": 2.5}): plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') plt.title(\'Monthly Average Temperature (Poster Context)\') plt.show() # Step 4: Comparative Visualizations def plot_comparative_visualizations(): plt.figure(figsize=(15, 6)) plt.subplot(1, 2, 1) sns.set_style(\\"white\\") sns.boxplot(data=df, x=\'year\', y=\'temperature\') plt.title(\'Temperature by Year\') plt.subplot(1, 2, 2) sns.set_style(\\"dark\\") sns.boxplot(data=df, x=\'year\', y=\'precipitation\') plt.title(\'Precipitation by Year\') plt.tight_layout() plt.show() # Step 5: Advanced Customization def plot_advanced_customization(): sns.set_style(\\"darkgrid\\", {\\"axes.facecolor\\": \\".9\\"}) plt.figure(figsize=(10, 6)) sns.lineplot(data=df, x=\'month\', y=\'temperature\', hue=\'year\') plt.title(\'Monthly Average Temperature (Custom Background)\') plt.show()"},{"question":"**Objective**: Demonstrate a comprehensive understanding of Pandas string manipulation methods. Problem Statement: You have been given a raw text data set containing reviews from different users. Your task is to process this data and extract meaningful information using Pandas string accessor methods. Input: - A dataframe `df` with a single column `reviews` that contains mixed text data such as usernames, ratings, dates, and review texts all combined in a single string. Requirements: 1. **Standardize the Text**: Convert all text in the `reviews` column to lowercase. 2. **Extract Information**: Extract the date (in `YYYY-MM-DD` format), username, and review text into separate columns. - Assume the username always starts with `@` and there are no spaces in usernames. - Reviews start after the date and username. 3. **Create Indicators**: Create additional columns indicating whether the review contains any of the following words: `[\'good\', \'bad\', \'average\']`. 4. **Clean Column Names**: Ensure all new column names are in lowercase and have underscores instead of spaces. Constraints: - Handle missing values gracefully, ensuring no data loss. - Assume date formats and text structures are consistent across reviews. - Parse the column names and remove unnecessary whitespace. Example: ``` Input Dataframe: reviews 0 \'2023-01-01 @user123 Good product! Highly recommend it.\' 1 \'2023-02-15 @user456 Bad experience. Not as expected.\' 2 \'2023-03-03 @user789 Average quality, decent price.\' 3 np.nan 4 \'2023-04-10 @user101 Excellent quality!\' Expected Dataframe: date username review_text contains_good contains_bad contains_average 0 2023-01-01 @user123 good product! highly recommend it. True False False 1 2023-02-15 @user456 bad experience. not as expected. False True False 2 2023-03-03 @user789 average quality, decent price. False False True 3 NaN NaN NaN False False False 4 2023-04-10 @user101 excellent quality! False False False Column Names: [\'date\', \'username\', \'review_text\', \'contains_good\', \'contains_bad\', \'contains_average\'] ``` Function Signature: ```python import pandas as pd def process_reviews(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass ``` Your task: Implement the function `process_reviews` that takes a DataFrame containing raw reviews in one column and outputs a cleaned DataFrame with the expected columns and data as described above.","solution":"import pandas as pd import numpy as np def process_reviews(df: pd.DataFrame) -> pd.DataFrame: # Step 1: Convert all text in the `reviews` column to lowercase. df[\'reviews\'] = df[\'reviews\'].str.lower() # Step 2: Extract date, username and review text df[\'date\'] = df[\'reviews\'].str.extract(r\'(d{4}-d{2}-d{2})\') df[\'username\'] = df[\'reviews\'].str.extract(r\'(@w+)\') df[\'review_text\'] = df[\'reviews\'].str.extract(r\'@w+s(.+)\') # Step 3: Create indicators for words \\"good\\", \\"bad\\", \\"average\\" df[\'contains_good\'] = df[\'review_text\'].str.contains(\'good\', na=False) df[\'contains_bad\'] = df[\'review_text\'].str.contains(\'bad\', na=False) df[\'contains_average\'] = df[\'review_text\'].str.contains(\'average\', na=False) # Step 4: Remove unnecessary whitespace and ensure column names are in lowercase with underscores df.columns = [col.strip().lower().replace(\' \', \'_\') for col in df.columns] # Handle missing values gracefully df.fillna({\'date\': np.nan, \'username\': np.nan, \'review_text\': np.nan, \'contains_good\': False, \'contains_bad\': False, \'contains_average\': False}, inplace=True) return df[[\'date\', \'username\', \'review_text\', \'contains_good\', \'contains_bad\', \'contains_average\']]"}]'),I={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:D,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},q={key:0,class:"empty-state"},R=["disabled"],M={key:0},L={key:1};function O(n,e,l,m,r,i){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=o=>r.searchQuery=o),placeholder:"Search..."},null,512),[[y,r.searchQuery]]),r.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=o=>r.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(i.displayedPoems,(o,f)=>(a(),w(h,{key:f,poem:o},null,8,["poem"]))),128)),i.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+u(r.searchQuery)+'". ',1)):d("",!0)]),i.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:r.isLoading,onClick:e[2]||(e[2]=(...o)=>i.loadMore&&i.loadMore(...o))},[r.isLoading?(a(),s("span",L,"Loading...")):(a(),s("span",M,"See more"))],8,R)):d("",!0)])}const N=p(I,[["render",O],["__scopeId","data-v-fb227543"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/60.md","filePath":"deepseek/60.md"}'),j={name:"deepseek/60.md"},B=Object.assign(j,{setup(n){return(e,l)=>(a(),s("div",null,[x(N)]))}});export{Y as __pageData,B as default};
