import{_ as p,o as a,c as s,a as t,m as u,t as c,C as g,M as _,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const I=p(k,[["render",S],["__scopeId","data-v-d602132a"]]),A=JSON.parse('[{"question":"Objective Design and implement an asynchronous network echo server using asyncio\'s event loop that handles multiple simultaneous client connections. Description You are required to implement an echo server that listens on a specified host and port, handles multiple concurrent client connections, and echoes back any data it receives from a client. The server should use asyncio\'s event loop for handling connections and data in a non-blocking manner. Requirements 1. Implement a function `start_echo_server(host, port)` that: - Listens on the specified host and port. - Accepts multiple simultaneous client connections. - Reads data from each connected client and echoes it back to the sender. - Handles connections and data asynchronously using asyncio\'s event loop. 2. Implement appropriate error handling and cleanup: - Ensure connections are properly closed when done. - Handle any exceptions that occur during data handling without crashing the server. 3. The server should keep running indefinitely, accepting and handling new client connections. 4. You should implement the logic using asyncio\'s low-level event loop API rather than the higher-level `asyncio.run()` function. Example Usage ```python import asyncio async def handle_client(reader, writer): data = await reader.read(100) message = data.decode() writer.write(data) await writer.drain() writer.close() async def start_echo_server(host, port): srv = await asyncio.start_server(handle_client, host, port) async with srv: await srv.serve_forever() if __name__ == \'__main__\': host = \'localhost\' port = 8888 loop = asyncio.get_event_loop() loop.run_until_complete(start_echo_server(host, port)) loop.run_forever() ``` Constraints - Do not use `asyncio.run()` to run your event loop. - Ensure that the server can handle multiple clients concurrently. - Implement proper logging to record any errors or exceptions. Additional Notes - You can test the server using various network tools (e.g., `telnet`, `nc`) or by writing client code to connect to the server and send messages. - Focus on handling data asynchronously and ensuring robust error handling. Submission Submit your implementation of the `start_echo_server` function along with a brief explanation of how your implementation works and any considerations taken into account for error handling and resource cleanup.","solution":"import asyncio async def handle_client(reader, writer): try: while True: data = await reader.read(100) if not data: break writer.write(data) await writer.drain() except Exception as e: print(f\'Error: {e}\') finally: writer.close() await writer.wait_closed() async def start_echo_server(host, port): server = await asyncio.start_server(handle_client, host, port) async with server: await server.serve_forever() if __name__ == \'__main__\': host = \'localhost\' port = 8888 loop = asyncio.get_event_loop() loop.run_until_complete(start_echo_server(host, port)) loop.run_forever()"},{"question":"# Advanced ASCII Character Manipulation Problem Statement: You have been provided with a text input where you need to perform a series of operations on ASCII characters. 1. Validate if all characters in the string belong to the ASCII set. 2. Identify and separate the control characters from the printable characters. 3. Convert all control characters to their caret notation using the `unctrl` function. 4. For printable characters not being alphanumeric, replace them with their corresponding ASCII values in hexadecimal notation. You are required to implement the following function: ```python def process_ascii_text(text: str) -> str: Process the given text according to the specified rules: 1. Validate if all characters in the string belong to the ASCII set; raise a ValueError if not. 2. Identify and separate control characters from printable characters. 3. Convert all control characters to their caret notation using the `unctrl` function. 4. For printable characters that are not alphanumeric, replace them with their corresponding ASCII values in hexadecimal notation. Args: text (str): The input text to be processed. Returns: str: The processed text as per the above rules. Control characters (with caret notation) will be followed by printable characters (alphanumeric and non-alphanumeric characters represented in hexadecimal). pass ``` Constraints: - Raise a `ValueError` if any character in the string is non-ASCII. - The input string `text` will always contain at least one character. Example: ```python text = \\"HellonWorld!\\" # \'Hello\' -> remains as \'Hello\' # \'n\' -> careted to \\"^J\\" # \'!\' -> replaced with its hexadecimal value \\"x21\\" result = process_ascii_text(text) print(result) # Output should be \\"Hello^JWorldx21\\" ``` Notes: - You can use the `curses.ascii` module in your implementation. - Consider using the `ord()` and `hex()` built-in Python functions where needed. - Ensure your implementation handles both single-character strings and the entire text correctly.","solution":"import curses.ascii def process_ascii_text(text: str) -> str: Process the given text according to the specified rules: 1. Validate if all characters in the string belong to the ASCII set; raise a ValueError if not. 2. Identify and separate control characters from printable characters. 3. Convert all control characters to their caret notation using the `unctrl` function. 4. For printable characters that are not alphanumeric, replace them with their corresponding ASCII values in hexadecimal notation. Args: text (str): The input text to be processed. Returns: str: The processed text as per the above rules. result = [] for char in text: # Check for ASCII if not curses.ascii.isascii(char): raise ValueError(\\"All characters must be ASCII.\\") if curses.ascii.iscntrl(char): result.append(curses.ascii.unctrl(char)) elif not char.isalnum(): result.append(f\\"x{ord(char):02x}\\") else: result.append(char) return \'\'.join(result)"},{"question":"# Coding Assessment: Advanced Seaborn Bar Plots **Objective:** Write a function that generates a complex seaborn bar plot based on a given dataset and set of parameters. Your function should demonstrate an understanding of seaborn\'s `barplot` capabilities, including handling different data formats, error bar configurations, and customization options. **Function Signature:** ```python import seaborn as sns import pandas as pd def advanced_bar_plot(data: pd.DataFrame, x: str, y: str, hue: str = None, orient: str = \'v\', errorbar: str = \'ci\', estimator: str = \'mean\') -> sns.axisgrid.FacetGrid: Generates a customized seaborn bar plot. Parameters: - data (pd.DataFrame): The input dataset. - x (str): Column name to be plotted on the x-axis. - y (str): Column name to be plotted on the y-axis. - hue (str, optional): Column name for color encoding. Defaults to None. - orient (str, optional): \'v\' for vertical bars, \'h\' for horizontal bars. Defaults to \'v\'. - errorbar (str, optional): Error bar configuration. \'ci\' for confidence interval, \'sd\' for standard deviation, or None. Defaults to \'ci\'. - estimator (str, optional): Statistical function for aggregation. Defaults to \'mean\'. Returns: - sns.axisgrid.FacetGrid: The generated seaborn FacetGrid object with the bar plot. ``` **Expected Input and Output Formats:** - *Input:* - `data`: A pandas DataFrame containing the dataset. - `x`: A string representing the column name to be plotted on the x-axis. - `y`: A string representing the column name to be plotted on the y-axis. - `hue`: (Optional) A string representing the column name for color encoding. - `orient`: (Optional) A string, either \'v\' for vertical bars or \'h\' for horizontal bars. - `errorbar`: (Optional) A string, either \'ci\' for confidence interval, \'sd\' for standard deviation, or None for no error bars. - `estimator`: (Optional) A string representing the statistical function for aggregation (e.g., \'mean\', \'sum\'). - *Output:* - A seaborn `FacetGrid` object containing the generated bar plot. **Constraints:** - Ensure the function handles missing data gracefully and doesn\'t crash if columns specified are not found in the dataset. - The function should include relevant error handling for incorrect parameter types and values. - Consider performance optimization for large datasets. **Example:** ```python import seaborn as sns import pandas as pd # Sample Data data = sns.load_dataset(\\"penguins\\") # Generate the plot plot = advanced_bar_plot(data, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", orient=\'v\', errorbar=\'sd\', estimator=\'mean\') # Display the plot plot.fig.show() ``` **Notes:** - Utilize `sns.barplot`, `sns.catplot`, and other relevant seaborn functions as needed. - Customize the appearance using matplotlib keyword arguments if necessary. - Ensure the plot contains appropriate labels, titles, and legends. Test similar datasets, such as the `flights` dataset, to validate your function.","solution":"import seaborn as sns import pandas as pd def advanced_bar_plot(data: pd.DataFrame, x: str, y: str, hue: str = None, orient: str = \'v\', errorbar: str = \'ci\', estimator: str = \'mean\'): Generates a customized seaborn bar plot. Parameters: - data (pd.DataFrame): The input dataset. - x (str): Column name to be plotted on the x-axis. - y (str): Column name to be plotted on the y-axis. - hue (str, optional): Column name for color encoding. Defaults to None. - orient (str, optional): \'v\' for vertical bars, \'h\' for horizontal bars. Defaults to \'v\'. - errorbar (str, optional): Error bar configuration. \'ci\' for confidence interval, \'sd\' for standard deviation, or None. Defaults to \'ci\'. - estimator (str, optional): Statistical function for aggregation. Defaults to \'mean\'. Returns: - sns.axisgrid.FacetGrid: The generated seaborn FacetGrid object with the bar plot. # Check if provided columns x and y exist in the DataFrame if x not in data.columns or y not in data.columns: raise ValueError(f\\"Column \'{x}\' or \'{y}\' not found in the dataset.\\") if hue is not None and hue not in data.columns: raise ValueError(f\\"Column \'{hue}\' not found in the dataset.\\") # Set estimator function estimator_func = None if estimator == \'mean\': estimator_func = \'mean\' elif estimator == \'sum\': estimator_func = \'sum\' else: raise ValueError(\\"Estimator not recognized. Use \'mean\' or \'sum\'.\\") # Generate the plot plot = sns.catplot( data=data, kind=\\"bar\\", x=x if orient == \'v\' else y, y=y if orient == \'v\' else x, hue=hue, ci=None if errorbar is None else errorbar, estimator=estimator_func, orient=orient ) # Customize the plot plot.set_axis_labels(x, y) if hue: plot.add_legend(title=hue) return plot"},{"question":"Advanced Coding Assessment: Unittest Framework # Objective Design a testing module using Python’s unittest framework that demonstrates various capabilities such as setting up class-level fixtures, individual test cases, handling exceptions, and using different assertion methods. # Problem Statement You are tasked with writing a test script for a library management system. The system includes a `Library` class with methods to add and remove books, check for the presence of a book, and list all books. Your task is to create unit tests for this `Library` class using the `unittest` framework. # Implementation 1. Implement a `Library` class as outlined below. 2. Write a `TestLibrary` class that tests the `Library` class using the `unittest` framework. 3. Use setup and teardown methods appropriately to manage resources. # `Library` Class Implement the following methods in the `Library` class: 1. `add_book(title)`: Adds a book to the library. 2. `remove_book(title)`: Removes a book from the library. 3. `has_book(title)`: Returns `True` if the book is in the library, `False` otherwise. 4. `list_books()`: Returns a list of all book titles in the library. # Constraints - The books should be stored in a set to ensure each book title is unique. - Removing a book that doesn’t exist in the library should raise a `ValueError`. # Example ```python class Library: def __init__(self): self.books = set() def add_book(self, title): self.books.add(title) def remove_book(self, title): if title not in self.books: raise ValueError(\\"The book is not available in the library.\\") self.books.remove(title) def has_book(self, title): return title in self.books def list_books(self): return list(self.books) ``` # Testing Requirements Using the `unittest` framework: 1. Write a `TestLibrary` class that inherits from `unittest.TestCase`. 2. Use the `setUpClass` and `tearDownClass` class methods to prepare resources before all tests and clean up after all tests. 3. Use the `setUp` and `tearDown` methods for setup and cleanup required for each test. 4. Implement the following test cases: - `test_add_book`: Verify that books are added correctly. - `test_remove_book`: Verify that books are removed correctly and handle exceptions for non-existing books. - `test_has_book`: Check the existence of books in the library. - `test_list_books`: Verify the listing of all books. # Example Usage ```python import unittest class TestLibrary(unittest.TestCase): @classmethod def setUpClass(cls): print(\\"Setting up the Library test environment\\") @classmethod def tearDownClass(cls): print(\\"Cleaning up the Library test environment\\") def setUp(self): self.library = Library() def tearDown(self): pass # No specific tearDown needed for each test in this case def test_add_book(self): self.library.add_book(\\"1984\\") self.assertTrue(self.library.has_book(\\"1984\\")) def test_remove_book(self): self.library.add_book(\\"1984\\") self.library.remove_book(\\"1984\\") self.assertFalse(self.library.has_book(\\"1984\\")) def test_remove_book_non_existing(self): with self.assertRaises(ValueError): self.library.remove_book(\\"Non-existing Book\\") def test_has_book(self): self.library.add_book(\\"1984\\") self.assertTrue(self.library.has_book(\\"1984\\")) self.assertFalse(self.library.has_book(\\"Brave New World\\")) def test_list_books(self): self.library.add_book(\\"1984\\") self.library.add_book(\\"Brave New World\\") books = self.library.list_books() self.assertIn(\\"1984\\", books) self.assertIn(\\"Brave New World\\", books) if __name__ == \'__main__\': unittest.main() ``` - Ensure your implementation includes the `Library` class and the `TestLibrary` class. - Your solution will be assessed based on correctness, completeness, and the use of unittest framework functionalities. # Submission - Provide your implementation in a single `.py` file.","solution":"class Library: def __init__(self): self.books = set() def add_book(self, title): self.books.add(title) def remove_book(self, title): if title not in self.books: raise ValueError(\\"The book is not available in the library.\\") self.books.remove(title) def has_book(self, title): return title in self.books def list_books(self): return list(self.books)"},{"question":"# Custom Scikit-Learn Estimator In this question, you will create a custom scikit-learn compatible estimator. Your task is to implement a new classifier that predicts based on the nearest neighbor\'s class. Specifically, you will create a class `CustomNearestNeighborClassifier` that follows the scikit-learn API conventions. Requirements 1. **Initialization**: Implement the `__init__` method to accept a single parameter `metric` which determines the distance metric to use for finding the nearest neighbor. It should default to `\'euclidean\'`. 2. **Fitting**: Implement the `fit` method to learn from the training data. Store the training data and their respective class labels. 3. **Prediction**: Implement the `predict` method to predict the class of each test sample based on the class of the nearest neighbor from the training data. 4. **Parameter Handling**: Implement the `get_params` and `set_params` methods to allow compatibility with scikit-learn\'s model selection tools. 5. **Validation**: Use `sklearn.utils.validation` methods to ensure the inputs are consistent and correctly formatted. 6. **Attributes**: Use the proper conventions for attributes, storing learned information with trailing underscores. # Detailed Instructions - **Class**: `CustomNearestNeighborClassifier` - **Methods to Implement**: - `__init__(self, metric=\'euclidean\')` - `fit(self, X, y)` - `predict(self, X)` - `get_params(self, deep=True)` - `set_params(self, **params)` # Example Usage ```python import numpy as np from sklearn.utils.estimator_checks import check_estimator # Define the custom classifier class CustomNearestNeighborClassifier(ClassifierMixin, BaseEstimator): def __init__(self, metric=\'euclidean\'): self.metric = metric def fit(self, X, y): X, y = check_X_y(X, y) self.X_ = X self.y_ = y return self def predict(self, X): check_is_fitted(self) X = check_array(X) closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest] def get_params(self, deep=True): return {\'metric\': self.metric} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self # Example usage X_train = np.array([[0, 0], [1, 1], [2, 2]]) y_train = np.array([0, 1, 1]) X_test = np.array([[1.5, 1.5]]) clf = CustomNearestNeighborClassifier(metric=\'euclidean\') clf.fit(X_train, y_train) predictions = clf.predict(X_test) print(predictions) # Expected output: array([1]) # Check estimator validity check_estimator(CustomNearestNeighborClassifier()) ``` Input & Output Format - **Input** - `X`: A 2D numpy array of shape `(n_samples, n_features)` representing the training data. - `y`: A 1D numpy array of shape `(n_samples,)` representing the class labels - `X_test`: A 2D numpy array of shape `(n_samples, n_features)` representing the data to be predicted. - **Output** - Returns a 1D numpy array of shape `(n_samples,)` representing the predicted class labels. # Constraints - The `metric` parameter can accept only `\'euclidean\'` for the scope of this task. - Assume the training and test data do not contain missing values. **Good luck!**","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.metrics import euclidean_distances from sklearn.utils.validation import check_X_y, check_array, check_is_fitted class CustomNearestNeighborClassifier(BaseEstimator, ClassifierMixin): def __init__(self, metric=\'euclidean\'): self.metric = metric def fit(self, X, y): X, y = check_X_y(X, y) self.X_ = X self.y_ = y return self def predict(self, X): check_is_fitted(self) X = check_array(X) closest = np.argmin(euclidean_distances(X, self.X_), axis=1) return self.y_[closest] def get_params(self, deep=True): return {\'metric\': self.metric} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self"},{"question":"Objective: Implement a class in Python that mimics a basic buffer handling system with methods to read from and write to the buffer. This exercise will test your ability to work with buffers and string manipulations in Python. Description: 1. Create a class called `SimpleBuffer` with the following methods: - `__init__(self, initial_data: str)`: Initializes the buffer with the given string. Store the data in a private variable. - `read_buffer(self) -> str`: Returns the current contents of the buffer as a string. - `write_buffer(self, data: str) -> None`: Writes the given string to the buffer, replacing any existing contents. - `append_to_buffer(self, data: str) -> None`: Appends the given string to the current buffer contents. 2. Ensure that the buffer enforces that it can only contain characters (i.e., strings). If the provided data is not a string, the method should raise a `TypeError`. 3. Ensure that the buffer efficiently handles large strings. This means your solution should not create excessive intermediate copies of the buffer contents. Constraints: - You should not use external libraries (e.g., NumPy) that provide buffer support. Use only standard Python constructs. - The length of any string (initial data or data to be written/appended) will not exceed 1,000,000 characters. Example Usage: ```python buffer = SimpleBuffer(\\"Hello\\") print(buffer.read_buffer()) # Output: \\"Hello\\" buffer.write_buffer(\\"World\\") print(buffer.read_buffer()) # Output: \\"World\\" buffer.append_to_buffer(\\"!!!\\") print(buffer.read_buffer()) # Output: \\"World!!!\\" ``` Performance Requirements: - The methods should handle strings efficiently even when they reach the constraint limit of 1,000,000 characters. Happy coding!","solution":"class SimpleBuffer: def __init__(self, initial_data: str): if not isinstance(initial_data, str): raise TypeError(\'Initial data must be a string\') self._buffer = initial_data def read_buffer(self) -> str: return self._buffer def write_buffer(self, data: str) -> None: if not isinstance(data, str): raise TypeError(\'Data must be a string\') self._buffer = data def append_to_buffer(self, data: str) -> None: if not isinstance(data, str): raise TypeError(\'Data must be a string\') self._buffer += data"},{"question":"Coding Assessment Question # Objective: Design a function that retrieves and processes Unix user account data, demonstrating your understanding of the `pwd` module\'s functionalities. # Problem: You are required to implement a function `get_user_info(usernames)`, which takes a list of Unix usernames and returns their corresponding user information. The function should retrieve detailed user account data for the provided usernames and return the results in a structured format. # Function Signature: ```python def get_user_info(usernames: List[str]) -> List[Dict[str, Union[str, int]]]: ``` # Input: - `usernames`: A list of Unix usernames (type: List[str]). # Output: - A list of dictionaries, each containing user information for a given username. The dictionary should have the following keys: - `\\"login_name\\"`: The login name of the user (type: str). - `\\"user_id\\"`: The numerical user ID (type: int). - `\\"group_id\\"`: The numerical group ID (type: int). - `\\"user_name\\"`: The user name or comment field (type: str). - `\\"home_directory\\"`: The user home directory (type: str). - `\\"shell\\"`: The user command interpreter (type: str). # Example: ```python usernames = [\\"alice\\", \\"bob\\"] result = get_user_info(usernames) ``` If \\"alice\\" corresponds to: ``` login_name = \\"alice\\" user_id = 1001 group_id = 1001 user_name = \\"Alice Doe\\" home_directory = \\"/home/alice\\" shell = \\"/bin/bash\\" ``` And \\"bob\\" corresponds to: ``` login_name = \\"bob\\" user_id = 1002 group_id = 1002 user_name = \\"Bob Smith\\" home_directory = \\"/home/bob\\" shell = \\"/bin/zsh\\" ``` Then the result should be: ```python [ { \\"login_name\\": \\"alice\\", \\"user_id\\": 1001, \\"group_id\\": 1001, \\"user_name\\": \\"Alice Doe\\", \\"home_directory\\": \\"/home/alice\\", \\"shell\\": \\"/bin/bash\\" }, { \\"login_name\\": \\"bob\\", \\"user_id\\": 1002, \\"group_id\\": 1002, \\"user_name\\": \\"Bob Smith\\", \\"home_directory\\": \\"/home/bob\\", \\"shell\\": \\"/bin/zsh\\" } ] ``` # Constraints: - Assume that all provided usernames exist in the password database. - Retrieve the information using the `pwd.getpwnam` function. - Handle potential `KeyError` exceptions gracefully by skipping inaccessible usernames and returning information for the rest. # Notes: - It\'s important to ensure that the function performs efficiently and handles multiple user information retrievals in a single run. - Use the `pwd` module only, as accessing the `/etc/passwd` file directly is not allowed in this task.","solution":"import pwd from typing import List, Dict, Union def get_user_info(usernames: List[str]) -> List[Dict[str, Union[str, int]]]: user_info_list = [] for username in usernames: try: user_data = pwd.getpwnam(username) user_info = { \\"login_name\\": user_data.pw_name, \\"user_id\\": user_data.pw_uid, \\"group_id\\": user_data.pw_gid, \\"user_name\\": user_data.pw_gecos, \\"home_directory\\": user_data.pw_dir, \\"shell\\": user_data.pw_shell } user_info_list.append(user_info) except KeyError: # Skip users that are not found continue return user_info_list"},{"question":"# Coding Assessment: Advanced Color Palette Customization in Seaborn Objective: Design a function using the Seaborn library to create visualizations with specific color palettes that are suitable for both categorical and numerical data representation. Function Requirements: 1. **Function Signature**: ```python def create_custom_palette_visualization(data: pd.DataFrame, save_path: str): pass ``` 2. **Input**: - `data`: - Type: `pandas.DataFrame` - Contains columns: \'category\', \'value1\', \'value2\' - \'category\': Categorical data - \'value1\' and \'value2\': Numerical data - `save_path`: - Type: string - The file path to save the resulting plot image. 3. **Output**: - None, but the function should save a multi-panel plot as an image file to the provided `save_path`. 4. **Constraints**: - The function should create two subplots in a single figure: 1. A scatter plot using color to represent the \'category\' column, employing a qualitative color palette. 2. A heatmap representing the correlation between \'value1\' and \'value2\', using a customized sequential color palette. 5. **Specific Implementation Details**: - Use a qualitative color palette for the scatter plot appropriate for categorical data. - For the heatmap, use a custom sequential color palette created with the `cubehelix_palette` function. Customize the palette with at least two parameters of your choice (e.g., start, rot, dark, light, reverse). - Ensure that the figures are appropriately labeled, and a color bar is included in the heatmap subplot. Example Data: ```python import pandas as pd data = pd.DataFrame({ \'category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\'], \'value1\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'value2\': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] }) ``` Example Function Call: ```python create_custom_palette_visualization(data, \'output_plot.png\') ``` Evaluation Criteria: - Correct usage of Seaborn to create and apply color palettes. - Proper selection and application of qualitative and sequential palettes based on data types. - Visualization clarity and aesthetic quality. - Correct file output as an image.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_visualization(data: pd.DataFrame, save_path: str): # Check if required columns are present in the data if not all(column in data.columns for column in [\'category\', \'value1\', \'value2\']): raise ValueError(\\"Data must contain \'category\', \'value1\', and \'value2\' columns.\\") # Initialize the figure with 2 subplots fig, axs = plt.subplots(1, 2, figsize=(14, 6)) # Create a scatter plot with a qualitative palette scatter_palette = sns.color_palette(\'Set2\') sns.scatterplot(x=\'value1\', y=\'value2\', hue=\'category\', palette=scatter_palette, data=data, ax=axs[0]) axs[0].set_title(\'Scatter Plot with Categorical Data\') axs[0].set_xlabel(\'Value 1\') axs[0].set_ylabel(\'Value 2\') # Create a correlation heatmap with a customized sequential color palette correlation_matrix = data[[\'value1\', \'value2\']].corr() heatmap_palette = sns.cubehelix_palette(start=.5, rot=-.75, light=0.85, dark=0.15, as_cmap=True) sns.heatmap(correlation_matrix, annot=True, cmap=heatmap_palette, ax=axs[1]) axs[1].set_title(\'Correlation Heatmap\') # Adjust layout to prevent overlap and save the plot to the specified path plt.tight_layout() plt.savefig(save_path) plt.close()"},{"question":"# Advanced PyTorch Coding Assessment Objective Implement a PyTorch function that demonstrates the use of both Core Aten IR and Prims IR. This function will take a tensor input, perform a series of operations using Core Aten IR, and then further process the result with Prims IR. Task Write a function `advanced_tensor_processing` to perform the following steps: 1. Accept an input tensor. 2. Use Core Aten IR operators to: - Compute the mean and standard deviation of the tensor. - Normalize the tensor using the computed mean and standard deviation. 3. Use Prims IR operators to: - Change the element type of the normalized tensor to `float32`. - Broadcast the tensor along a new dimension. 4. Return the processed tensor. Constraints - The input tensor will always be of type `torch.Tensor`. - Ensure the solution handles tensors with varying shapes and sizes. Function Signature ```python import torch def advanced_tensor_processing(input_tensor: torch.Tensor) -> torch.Tensor: # Your code here pass ``` Example Usage ```python import torch # Example Tensor input_tensor = torch.tensor([[1.0, 2.0], [3.0, 4.0]]) # Processed Tensor output_tensor = advanced_tensor_processing(input_tensor) # expected output: tensor(...) with appropriate transformations applied print(output_tensor) ``` Assessment Criteria - Correct usage of both Core Aten IR and Prims IR operations. - Proper handling of type conversion and broadcasting in the prims stage. - Effective normalization computations using Aten IR. - Code readability and documentation. Good luck, and happy coding!","solution":"import torch def advanced_tensor_processing(input_tensor: torch.Tensor) -> torch.Tensor: Perform a series of operations on the input tensor using Core Aten IR and Prims IR. Parameters: input_tensor (torch.Tensor): The input tensor. Returns: torch.Tensor: The processed tensor. # Core Aten IR operations mean = torch.mean(input_tensor) std = torch.std(input_tensor) normalized_tensor = (input_tensor - mean) / std # Prims IR operations # Convert the normalized tensor to float32 (if it\'s not already) normalized_tensor_float = normalized_tensor.to(torch.float32) # Broadcast the tensor along a new dimension # Adding an extra dimension at the beginning (axis 0) broadcasted_tensor = normalized_tensor_float.unsqueeze(0) return broadcasted_tensor"},{"question":"You are provided with two datasets: `penguins` and `flights`. Your task is to create a series of bar plots using the seaborn `so` module that accurately visualize the given data with specific customization requirements. # Datasets - `penguins`: Contains information on different species of penguins, including body mass and sex. - `flights`: Contains information on the number of passengers on flights over different months in the year 1960. # Tasks 1. **Bar Plot of Monthly Passengers in 1960:** - Create a bar plot showing the number of passengers for each month in 1960. - The x-axis should represent the months in the correct order (January to December). - Customize the bars to be blue with an opacity of 0.6. 2. **Bar Plot of Penguin Species Counts:** - Create a bar plot showing the count of each penguin species. - Customize the bars to have different colors based on the sex of the penguins. - Use a dodge transform to separate the bars by sex. 3. **Body Mass Distribution by Species with Error Bars:** - Create a bar plot showing the mean body mass of each penguin species. - Customize the bars to be semi-transparent. - Add error bars to represent the standard deviation of the body mass for each species. - Use different colors to differentiate between male and female penguins. - Add a legend to indicate which colors represent which sex. # Input - The datasets `penguins` and `flights` are already loaded using `load_dataset(\\"penguins\\")` and `load_dataset(\\"flights\\").query(\\"year == 1960\\")`, respectively. # Output - Display three plots according to the specifications above. # Constraints - You must use the `so.Plot` and related seaborn objects (`so.Bar`, `so.Hist`, `so.Dodge`, `so.Range`, `so.Est`) for creating the visualizations. # Example (Using Pseudocode) - Bar Plot of Monthly Passengers: ```python so.Plot(flights[\\"month\\"], flights[\\"passengers\\"]) .add(so.Bar(color=\\"blue\\", alpha=0.6)) .show() ``` - Bar Plot of Penguin Species Counts: ```python so.Plot(penguins, x=\\"species\\", color=\\"sex\\") .add(so.Bar(), so.Hist(), so.Dodge()) .show() ``` - Body Mass Distribution by Species with Error Bars: ```python so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") .add(so.Bar(alpha=0.5), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .show() ``` # Requirements Implement the required plots in a single Python script or Jupyter notebook and ensure it runs without errors. The output should directly display the visualizations.","solution":"import seaborn.objects as so import seaborn as sns import matplotlib.pyplot as plt # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\").query(\\"year == 1960\\") # Bar Plot of Monthly Passengers in 1960 def plot_monthly_passengers(flights): plt.figure(figsize=(10, 6)) so.Plot(flights, x=\\"month\\", y=\\"passengers\\").add(so.Bar(color=\\"blue\\", alpha=0.6)).show() # Bar Plot of Penguin Species Counts def plot_penguin_species_counts(penguins): plt.figure(figsize=(10, 6)) so.Plot(penguins, x=\\"species\\", color=\\"sex\\").add(so.Bar(), so.Hist(), so.Dodge()).show() # Body Mass Distribution by Species with Error Bars def plot_body_mass_distribution(penguins): plt.figure(figsize=(10, 6)) p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") p.add(so.Bar(alpha=0.5), so.Agg(), so.Dodge()) p.add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) p.show() # Executing the plotting functions plot_monthly_passengers(flights) plot_penguin_species_counts(penguins) plot_body_mass_distribution(penguins)"},{"question":"**Question: Time Zone Aware Meeting Scheduler** You are tasked with creating a utility function for scheduling meetings between teams in different time zones. The function should receive a list of meeting times in UTC and convert them to the local time zones of the participants, adjusting for daylight saving time if applicable. Additionally, the function should format the local meeting times into readable strings. **Function Signature:** ```python from datetime import datetime, timedelta, timezone, tzinfo from typing import List, Tuple def schedule_meetings(meeting_times: List[str], timezones: List[str]) -> List[str]: ``` **Parameters:** - `meeting_times`: A list of strings representing meeting times in ISO 8601 format (e.g., `\'2023-09-15T13:00:00Z\'`). All times are initially in UTC. - `timezones`: A list of strings representing the IANA time zone names corresponding to the locations of the participants (e.g., `\'America/New_York\'`, `\'Europe/London\'`, `\'Asia/Tokyo\'`). **Returns:** - A list of strings where each string represents the meeting time converted to the corresponding local time zone, formatted as `\'YYYY-MM-DD HH:MM:SS TZ\'`, where `TZ` is the time zone abbreviation. **Constraints:** - Assume that all inputs are valid. - The number of `meeting_times` and `timezones` will be equal. - You may assume that the `zoneinfo` module is available. **Example:** ```python meeting_times = [ \'2023-09-15T13:00:00Z\', \'2023-09-15T18:00:00Z\', \'2023-09-15T23:00:00Z\' ] timezones = [ \'America/New_York\', \'Europe/London\', \'Asia/Tokyo\' ] result = schedule_meetings(meeting_times, timezones) # The expected output format should look like this (actual values may vary based on DST): # [ # \'2023-09-15 09:00:00 EDT\', # \'2023-09-15 19:00:00 BST\', # \'2023-09-16 08:00:00 JST\' # ] ``` **Task:** Implement the `schedule_meetings` function, ensuring that it handles time zone conversions accurately, including adjustments for daylight saving time where applicable.","solution":"from datetime import datetime from zoneinfo import ZoneInfo from typing import List def schedule_meetings(meeting_times: List[str], timezones: List[str]) -> List[str]: local_times = [] for utc_time, timezone in zip(meeting_times, timezones): utc_datetime = datetime.fromisoformat(utc_time.replace(\'Z\', \'+00:00\')) local_datetime = utc_datetime.astimezone(ZoneInfo(timezone)) formatted_local_time = local_datetime.strftime(\'%Y-%m-%d %H:%M:%S %Z\') local_times.append(formatted_local_time) return local_times"},{"question":"# Pandas Coding Assessment Objective: To assess your comprehension of pandas\' advanced data types and their manipulation, including datetime, categorical, and interval data types. Problem Statement: You are provided with a dataset that includes various data types. Your task is to implement a function that processes this dataset using pandas to achieve specific transformations and analyses. Function Signature: ```python def process_dataset(df: pd.DataFrame) -> dict: Process the input DataFrame and return a dictionary of results based on specified transformations and analyses. Parameters: df (pd.DataFrame): Input DataFrame with at least the following columns: - \'timestamp\' (str): Timestamps in the format \\"YYYY-MM-DD HH:MM:SS\\". - \'category\' (str): Categorical data. - \'amount\' (float): An amount that might contain missing values (NaNs). - \'interval_start\' (str): Start of an interval in the format \\"YYYY-MM-DD\\" - \'interval_end\' (str): End of an interval in the format \\"YYYY-MM-DD\\" Returns: dict: A dictionary with the following keys and their respective results: - \'hours_sum\': Total sum of hours from \'timestamp\'. - \'categories_count\': A pandas Series with counts of each category. - \'mean_amount\': Mean of the \'amount\' column, excluding NaNs. - \'interval_lengths\': A pandas Series with the lengths of each interval in days. ``` Constraints: - You must utilize the respective pandas data types for each transformation. - Performance: The function should handle dataframes with up to 1 million rows efficiently. Example: Suppose you have the following DataFrame: ```python import pandas as pd data = { \'timestamp\': [\'2022-01-01 08:30:00\', \'2022-01-02 10:00:00\', \'2022-01-03 12:15:00\'], \'category\': [\'A\', \'B\', \'A\'], \'amount\': [100.0, None, 150.0], \'interval_start\': [\'2022-01-01\', \'2022-01-02\', \'2022-01-03\'], \'interval_end\': [\'2022-01-02\', \'2022-01-03\', \'2022-01-04\'] } df = pd.DataFrame(data) result = process_dataset(df) ``` The expected output would be: ```python { \'hours_sum\': 30, \'categories_count\': pd.Series([2, 1], index=[\'A\', \'B\']), \'mean_amount\': 125.0, \'interval_lengths\': pd.Series([1, 1, 1]) } ``` # Notes: 1. You may assume all input data is correctly formatted. 2. Be sure to handle missing values appropriately. 3. Efficiently transform and analyze the data based on the requirements outlined. # Implementation Guidance: - Convert \'timestamp\' to pandas DatetimeArray, extract hours, and sum them. - Convert \'category\' to a pandas Categorical type and count occurrences. - Use pandas\' nullable float support to calculate the mean of \'amount\' excluding NaNs. - Construct IntervalArray from \'interval_start\' and \'interval_end\' to compute interval lengths in days.","solution":"import pandas as pd def process_dataset(df: pd.DataFrame) -> dict: Process the input DataFrame and return a dictionary of results based on specified transformations and analyses. Parameters: df (pd.DataFrame): Input DataFrame with at least the following columns: - \'timestamp\' (str): Timestamps in the format \\"YYYY-MM-DD HH:MM:SS\\". - \'category\' (str): Categorical data. - \'amount\' (float): An amount that might contain missing values (NaNs). - \'interval_start\' (str): Start of an interval in the format \\"YYYY-MM-DD\\" - \'interval_end\' (str): End of an interval in the format \\"YYYY-MM-DD\\" Returns: dict: A dictionary with the following keys and their respective results: - \'hours_sum\': Total sum of hours from \'timestamp\'. - \'categories_count\': A pandas Series with counts of each category. - \'mean_amount\': Mean of the \'amount\' column, excluding NaNs. - \'interval_lengths\': A pandas Series with the lengths of each interval in days. # Convert \'timestamp\' to datetime and extract hours df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) hours_sum = df[\'timestamp\'].dt.hour.sum() # Convert \'category\' to categorical and count occurrences df[\'category\'] = df[\'category\'].astype(\'category\') categories_count = df[\'category\'].value_counts() # Calculate mean of \'amount\', excluding NaNs mean_amount = df[\'amount\'].mean() # Convert \'interval_start\' and \'interval_end\' to datetime df[\'interval_start\'] = pd.to_datetime(df[\'interval_start\']) df[\'interval_end\'] = pd.to_datetime(df[\'interval_end\']) # Calculate the lengths of each interval in days interval_lengths = (df[\'interval_end\'] - df[\'interval_start\']).dt.days return { \'hours_sum\': hours_sum, \'categories_count\': categories_count, \'mean_amount\': mean_amount, \'interval_lengths\': interval_lengths }"},{"question":"# XML Parsing with `xml.sax` **Objective**: Write a Python program to parse an XML string and print out specified tags and their attributes. **Task**: You are required to implement a SAX parser using the `xml.sax` package that extracts and prints the names of tags and their attributes from an XML string. **Specifications**: - Implement a custom `ContentHandler` class called `MyContentHandler` which extends `xml.sax.ContentHandler`. - Override the `startElement` method to print the start tag name and its attributes each time an element starts. - Override the `endElement` method to print the end tag name each time an element ends. - Override the `characters` method to handle the text within an element. - Parse the given XML string using the SAX parser and your custom handler. **Input**: - An XML string. **Output**: - For each start tag: print the tag name followed by its attributes in the format `attribute_name=\\"attribute_value\\"`. - For each end tag: print the tag name. **Example**: Given the following XML string: ```xml <note> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> ``` Your program should output: ```plaintext Start tag: note Start tag: to End tag: to Start tag: from End tag: from Start tag: heading End tag: heading Start tag: body End tag: body End tag: note ``` **Constraints**: - You should use the `xml.sax.parseString` method to parse the XML string. - You must handle standard SAX parsing exceptions. - Assume the XML string is well-formed and does not require validation against a DTD or schema. **Notes**: - Pay attention to the event-driven nature of SAX parsing and ensure thread-safety if required. **Starter Code**: ```python import xml.sax class MyContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Start tag: {name}\\") for attrName in attrs.getNames(): print(f\' {attrName}=\\"{attrs.getValue(attrName)}\\"\') def endElement(self, name): print(f\\"End tag: {name}\\") def characters(self, content): # Handle character data if needed pass def parse_xml_string(xml_string): parser = xml.sax.make_parser() handler = MyContentHandler() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler) # Example XML string xml_string = <note> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> parse_xml_string(xml_string) ``` Make sure your solution adheres to the specifications and handles both tags and attributes appropriately.","solution":"import xml.sax class MyContentHandler(xml.sax.ContentHandler): def startElement(self, name, attrs): print(f\\"Start tag: {name}\\") for attrName in attrs.getNames(): print(f\' {attrName}=\\"{attrs.getValue(attrName)}\\"\') def endElement(self, name): print(f\\"End tag: {name}\\") def characters(self, content): # Handle character data if needed pass def parse_xml_string(xml_string): parser = xml.sax.make_parser() handler = MyContentHandler() parser.setContentHandler(handler) xml.sax.parseString(xml_string, handler)"},{"question":"Objective The primary objective of this question is to evaluate the student\'s understanding of density estimation using Kernel Density Estimation (KDE) in scikit-learn, including choosing appropriate kernels and bandwidth parameters. The task will also test their ability to interpret the results and apply KDE to a practical dataset. Question You are given a dataset containing geographical coordinates (latitude and longitude) of sightings of two different species. Your task is to estimate and visualize the density of sightings of each species using Kernel Density Estimation with appropriate kernels and bandwidth parameters. Dataset The dataset is given in a CSV file named `species_sightings.csv` with the following columns: - `species`: The species identifier (either \\"species_1\\" or \\"species_2\\"). - `latitude`: Latitude coordinate of the sighting. - `longitude`: Longitude coordinate of the sighting. Requirements 1. **Data Loading and Preprocessing**: - Load the dataset from the provided CSV file. - Separate the data for the two species. 2. **Kernel Density Estimation**: - Implement KDE for both species data using the `KernelDensity` class from scikit-learn. - Use two different kernels for each species. For instance, you may choose `gaussian` for `species_1` and `tophat` for `species_2`. - Choose an appropriate bandwidth parameter for each KDE to achieve a smooth density estimate. 3. **Visualization**: - Visualize the density estimates of sightings for both species on a map of South America. - Use contour plots or heatmaps to represent the density of sightings. 4. **Code Implementation**: - Your function should take the following input: - `file_path`: Path to the CSV file (`species_sightings.csv`). - `bandwidth_species_1`: Bandwidth parameter for KDE of species_1. - `bandwidth_species_2`: Bandwidth parameter for KDE of species_2. - The output should be the density plots for both species. ```python import pandas as pd from sklearn.neighbors import KernelDensity import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap def estimate_and_visualize_density(file_path, bandwidth_species_1, bandwidth_species_2): # Load the data data = pd.read_csv(file_path) # Separate the data for the two species species_1_data = data[data[\'species\'] == \'species_1\'][[\'latitude\', \'longitude\']].values species_2_data = data[data[\'species\'] == \'species_2\'][[\'latitude\', \'longitude\']].values # Kernel Density Estimation for species 1 kde_species_1 = KernelDensity(kernel=\'gaussian\', bandwidth=bandwidth_species_1).fit(species_1_data) x_grid = np.linspace(-60, -30, 100) # Longitude range y_grid = np.linspace(-30, 10, 100) # Latitude range X, Y = np.meshgrid(x_grid, y_grid) xy_sample = np.vstack([Y.ravel(), X.ravel()]).T z_species_1 = np.exp(kde_species_1.score_samples(xy_sample)) Z_species_1 = z_species_1.reshape(X.shape) # Kernel Density Estimation for species 2 kde_species_2 = KernelDensity(kernel=\'tophat\', bandwidth=bandwidth_species_2).fit(species_2_data) z_species_2 = np.exp(kde_species_2.score_samples(xy_sample)) Z_species_2 = z_species_2.reshape(X.shape) # Plotting the results fig, ax = plt.subplots(1, 2, figsize=(14, 6)) # Plot for species 1 m = Basemap(projection=\'merc\', llcrnrlat=-30, urcrnrlat=10, llcrnrlon=-60, urcrnrlon=-30, ax=ax[0]) m.drawcoastlines() m.contourf(X, Y, Z_species_1, latlon=True, cmap=\'Reds\') ax[0].set_title(\'Density Estimation for Species 1\') # Plot for species 2 m = Basemap(projection=\'merc\', llcrnrlat=-30, urcrnrlat=10, llcrnrlon=-60, urcrnrlon=-30, ax=ax[1]) m.drawcoastlines() m.contourf(X, Y, Z_species_2, latlon=True, cmap=\'Blues\') ax[1].set_title(\'Density Estimation for Species 2\') plt.show() # Example usage: # estimate_and_visualize_density(\\"species_sightings.csv\\", 1.0, 1.0) ``` Constraints - Use the `KernelDensity` class from scikit-learn for density estimation. - Choose suitable bandwidth parameters to achieve a meaningful density estimate. - The visualization should provide a clear representation of the density distribution for each species. Performance Requirements - Ensure the function executes efficiently even for larger datasets. - The estimation and visualization should handle real-world latitude and longitude ranges appropriately.","solution":"import pandas as pd from sklearn.neighbors import KernelDensity import numpy as np import matplotlib.pyplot as plt def estimate_and_visualize_density(file_path, bandwidth_species_1, bandwidth_species_2): # Load the data data = pd.read_csv(file_path) # Separate the data for the two species species_1_data = data[data[\'species\'] == \'species_1\'][[\'latitude\', \'longitude\']].values species_2_data = data[data[\'species\'] == \'species_2\'][[\'latitude\', \'longitude\']].values # Kernel Density Estimation for species 1 with a Gaussian kernel kde_species_1 = KernelDensity(kernel=\'gaussian\', bandwidth=bandwidth_species_1).fit(species_1_data) # Kernel Density Estimation for species 2 with a Tophat kernel kde_species_2 = KernelDensity(kernel=\'tophat\', bandwidth=bandwidth_species_2).fit(species_2_data) # Create a grid over the area of interest x_grid = np.linspace(data[\'longitude\'].min(), data[\'longitude\'].max(), 100) y_grid = np.linspace(data[\'latitude\'].min(), data[\'latitude\'].max(), 100) X, Y = np.meshgrid(x_grid, y_grid) xy_sample = np.vstack([Y.ravel(), X.ravel()]).T # Evaluate the KDE on the grid z_species_1 = np.exp(kde_species_1.score_samples(xy_sample)) Z_species_1 = z_species_1.reshape(X.shape) z_species_2 = np.exp(kde_species_2.score_samples(xy_sample)) Z_species_2 = z_species_2.reshape(X.shape) # Plotting the results fig, ax = plt.subplots(1, 2, figsize=(14, 6)) # Plot for species 1 ax[0].contourf(X, Y, Z_species_1, levels=20, cmap=\'Reds\') ax[0].set_title(\'Density Estimation for Species 1\') ax[0].set_xlabel(\'Longitude\') ax[0].set_ylabel(\'Latitude\') # Plot for species 2 ax[1].contourf(X, Y, Z_species_2, levels=20, cmap=\'Blues\') ax[1].set_title(\'Density Estimation for Species 2\') ax[1].set_xlabel(\'Longitude\') ax[1].set_ylabel(\'Latitude\') plt.tight_layout() plt.show()"},{"question":"**Objective:** Demonstrate an understanding of Python\'s type system and meta-programming by creating, managing, and manipulating custom types using Python\'s `type` and metaclasses. **Question:** You are required to implement a custom type system in Python where you define new types dynamically and ensure that they conform to certain constraints. You will implement a metaclass that enforces these constraints and provides additional functionality. # Part 1: Define the Metaclass Create a metaclass `CustomMeta` that: 1. Ensures each class has a class attribute `type_id` which must be an integer. 2. Provides a class method `get_type_id()` that returns the `type_id`. # Part 2: Create the Custom Type Use the metaclass `CustomMeta` to define a class `CustomType`. Ensure: 1. The class attribute `type_id` is set to a unique integer. 2. The class method `get_type_id()` returns the `type_id`. # Part 3: Dynamic Creation Write a function `create_custom_type(name, type_id)` that: 1. Dynamically creates a new class with the given `name` and `type_id`. 2. The new class should use `CustomMeta` as its metaclass. # Input Format - `create_custom_type(name, type_id)`: - `name`: A string which is the name of the new class. - `type_id`: An integer which will be the class attribute `type_id`. # Output Format - The function should return the dynamically created class. # Example: ```python # Part 1: Define the Metaclass class CustomMeta(type): # Implementation details here # Part 2: Create the Custom Type class CustomType(metaclass=CustomMeta): type_id = 1 # Implementation details here # Part 3: Dynamic Creation def create_custom_type(name, type_id): # Implementation details here # Example Usage MyType = create_custom_type(\'MyType\', 100) print(MyType.type_id) # Output: 100 print(MyType.get_type_id()) # Output: 100 ``` # Constraints 1. The `type_id` must be unique for each type. 2. The created types must have the `get_type_id` method. # Notes - Ensure to handle any edge cases such as invalid `type_id` values. - Write error handling to cover cases where `type_id` is not provided or is not an integer.","solution":"class CustomMeta(type): def __new__(mcs, name, bases, dct): if \'type_id\' not in dct or not isinstance(dct[\'type_id\'], int): raise ValueError(\\"Classes must have an integer `type_id`.\\") cls = super().__new__(mcs, name, bases, dct) return cls def get_type_id(cls): return cls.type_id class CustomType(metaclass=CustomMeta): type_id = 1 def create_custom_type(name, type_id): if not isinstance(type_id, int): raise ValueError(\\"`type_id` must be an integer.\\") return CustomMeta(name, (CustomType,), {\'type_id\': type_id})"},{"question":"You are provided with a data processing task and need to efficiently handle large byte arrays without making unnecessary copies. Your task is to implement two functions: `create_memoryview` and `process_memoryview`. 1. `create_memoryview(data: bytes, writeable: bool) -> memoryview` - This function takes a bytes object `data` and a boolean `writeable`. It should return a memoryview of the given data, which should be writable if the `writeable` flag is set to `True`. 2. `process_memoryview(mview: memoryview) -> bytes` - This function takes a memoryview `mview`. If the data within the memoryview is contiguous, it should return the data as a bytes object without creating a copy. If the data is not contiguous, it should return a new bytes object that is a contiguous copy of the data in the memoryview. # Constraints - The size of `data` can be up to 10^6 bytes. - You may assume the memoryview passed to `process_memoryview` originates from `create_memoryview`. # Implementation Details - In `create_memoryview`, use either `PyMemoryView_FromObject` or `PyMemoryView_FromMemory` functions based on the writeability requirement. - In `process_memoryview`, ensure you use `PyMemoryView_GetContiguous` to check for contiguity and create copies accordingly. # Function Signatures ```python def create_memoryview(data: bytes, writeable: bool) -> memoryview: pass def process_memoryview(mview: memoryview) -> bytes: pass ``` # Example Usage ```python # Example data = b\\"example data\\" mview = create_memoryview(data, writeable=False) result = process_memoryview(mview) assert result == data ``` Make sure your implementation handles both contiguous and non-contiguous memory efficiently.","solution":"def create_memoryview(data: bytes, writeable: bool) -> memoryview: Creates and returns a memoryview of the given data. If writeable is True, the memoryview is writable. if writeable: return memoryview(bytearray(data)) else: return memoryview(data) def process_memoryview(mview: memoryview) -> bytes: Processes the given memoryview. Returns the data as a bytes object without creating a copy if the memoryview is contiguous. Otherwise, returns a new bytes object that is a contiguous copy of the data in the memoryview. if mview.contiguous: return mview.tobytes() else: return mview.tobytes()"},{"question":"You are tasked with creating an imaginary messaging platform using asyncio provided transports and protocols. Specifically, you need to implement a custom protocol for a TCP-based chat server that can handle multiple clients. This will involve creating a server that accepts connections, receives messages from clients, and broadcasts those messages to all connected clients. # Requirements 1. **ChatServerProtocol**: This class should inherit from `asyncio.Protocol` and handle the connection, data reception, and connection loss. - It should maintain a list of connected clients. - When a message is received from a client, it should broadcast that message to all other connected clients. - Ensure proper cleanup when a client disconnects. 2. **ChatClientProtocol**: This class should inherit from `asyncio.Protocol` and be used by clients to connect to the server and send/receive messages. - It should initially send a greeting message to the server upon connection. - It should manage the connection lifecycle and handle incoming messages from the server. 3. **Server setup**: Use asyncio\'s `loop.create_server()` to set up the server. 4. **Client setup**: Use asyncio\'s `loop.create_connection()` to connect clients to the server. # Constraints - Ensure the solution is non-blocking and uses asyncio effectively. - Handle exceptions and ensure robust error handling in both server and client implementations. # Example - You may use the provided examples in the documentation as a reference for the basic setup, but the main logic for handling multiple clients and broadcasting messages needs to be implemented. # Input No specific input is provided. # Output Your program should print logs for connections, received messages, and disconnections on the server side, and messages received on the client side. # Performance Requirements The server should be able to handle at least 100 concurrent connections without significant performance degradation. # Detailed Steps 1. Implement the `ChatServerProtocol` class. 2. Implement the `ChatClientProtocol` class. 3. Write an async function to set up the server. 4. Write an async function to set up the clients and initiate connection. 5. Test the server and client interaction by running the server and multiple clients in separate asyncio tasks. Here is a non-complete template to guide you: ```python import asyncio class ChatServerProtocol(asyncio.Protocol): clients = [] def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') self.clients.append(self) print(f\\"Connection from {self.peername}\\") def data_received(self, data): message = data.decode() print(f\\"Message from {self.peername}: {message}\\") for client in self.clients: if client != self: client.transport.write(data) def connection_lost(self, exc): print(f\\"Connection lost from {self.peername}\\") self.clients.remove(self) class ChatClientProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport message = \\"Hello, server!\\" self.transport.write(message.encode()) print(f\\"Connected to server: {message}\\") def data_received(self, data): print(f\\"Received from server: {data.decode()}\\") def connection_lost(self, exc): print(f\\"Disconnected from the server\\") async def run_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() async def run_client(): loop = asyncio.get_running_loop() transport, protocol = await loop.create_connection( lambda: ChatClientProtocol(), \'127.0.0.1\', 8888 ) if __name__ == \\"__main__\\": # Adjust depending on whether you want to run the server or the client asyncio.run(run_server()) # asyncio.run(run_client()) ``` You need to structure the above template and flesh out the details for the full implementation and testing of both server and client protocols.","solution":"import asyncio class ChatServerProtocol(asyncio.Protocol): clients = [] def connection_made(self, transport): self.transport = transport self.peername = transport.get_extra_info(\'peername\') self.clients.append(self) print(f\\"Connection from {self.peername}\\") def data_received(self, data): message = data.decode() print(f\\"Message from {self.peername}: {message}\\") for client in self.clients: if client != self: client.transport.write(data) def connection_lost(self, exc): print(f\\"Connection lost from {self.peername}\\") self.clients.remove(self) class ChatClientProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport message = \\"Hello, server!\\" self.transport.write(message.encode()) print(f\\"Connected to server: {message}\\") def data_received(self, data): print(f\\"Received from server: {data.decode()}\\") def connection_lost(self, exc): print(f\\"Disconnected from the server\\") async def run_server(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: ChatServerProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() async def run_client(): loop = asyncio.get_running_loop() transport, protocol = await loop.create_connection( lambda: ChatClientProtocol(), \'127.0.0.1\', 8888 ) # Keep the connection open while True: await asyncio.sleep(1) if __name__ == \\"__main__\\": # Adjust depending on whether you want to run the server or the client asyncio.run(run_server()) # asyncio.run(run_client())"},{"question":"**Problem Statement: Custom Formatter for Inventory Report** You are working with a logistic system that tracks inventory data. Your task is to implement a custom formatter that formats inventory reports. Implement a class `InventoryFormatter` that inherits from `string.Formatter` and adds custom formatting capabilities. # Requirements: 1. **Custom Formatting Fields**: - `{item_name.upper}`: This should convert the item name to upper case. - `{quantity:comma}`: This should format the quantity with commas as thousands separators. - `{price:currency}`: This should format the price as a currency, prefixed by `` and rounded to two decimal places. 2. **Class Implementation**: - You need to implement methods to support these custom formatting operations. # Class Definition: ```python import string class InventoryFormatter(string.Formatter): def format_field(self, value, format_spec): if format_spec == \\"upper\\": return str(value).upper() elif format_spec == \\"comma\\": return f\\"{value:,}\\" elif format_spec == \\"currency\\": return f\\"{value:.2f}\\" else: return super().format_field(value, format_spec) ``` # Example Usage: ```python data = { \'item_name\': \'laptop\', \'quantity\': 1500, \'price\': 1234.5678 } formatter = InventoryFormatter() # Format with the custom fields report = formatter.format( \\"Item: {item_name:upper}, Quantity: {quantity:comma}, Price: {price:currency}\\", **data ) print(report) ``` Expected Output: ``` Item: LAPTOP, Quantity: 1,500, Price: 1234.57 ``` # Constraints: - You should not alter the original data. - Only the specified custom format specifiers need to be supported. - Ensure the solution works efficiently even for larger datasets (up to 10,000 records). Implement the `InventoryFormatter` class and ensure the correctness of custom format operations as specified.","solution":"import string class InventoryFormatter(string.Formatter): def format_field(self, value, format_spec): if format_spec == \\"upper\\": return str(value).upper() elif format_spec == \\"comma\\": return f\\"{value:,}\\" elif format_spec == \\"currency\\": return f\\"{value:.2f}\\" else: return super().format_field(value, format_spec)"},{"question":"**Objective:** Demonstrate understanding of loading datasets using scikit-learn, preprocessing data, and applying machine learning models. **Problem Statement:** Load the \'California Housing\' dataset using the `fetch_california_housing` function from the `sklearn.datasets` module. Using this dataset, you are required to implement a Regression model to predict the median house value. 1. Load the dataset and perform a brief Exploratory Data Analysis (EDA) to understand the structure of the data. 2. Preprocess the data by handling missing values, splitting the dataset into features and target variable, and standardizing the features. 3. Split the dataset into training and testing sets (80% training and 20% testing). 4. Implement a Linear Regression model using scikit-learn. 5. Train the model on the training data and evaluate its performance on the testing data using appropriate metrics (e.g., Mean Squared Error). **Constraints:** - You must use the `fetch_california_housing` function to load the dataset. - You should handle any missing values appropriately (if any). - The regression model should be implemented using scikit-learn\'s `LinearRegression` class. **Input/Output:** - **Input:** - The dataset is fetched using `fetch_california_housing`. - **Output:** - Print the first 5 rows of the dataset after loading it. - Print the Mean Squared Error on the testing set after training the model. **Performance Requirements:** - The solution should efficiently load and preprocess the data. - Training and evaluation of the model should be done within reasonable time limits. # Example ```python # importing required libraries from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error # Step 1: Load the dataset data = fetch_california_housing() X, y = data[\'data\'], data[\'target\'] # Print the first 5 rows of the dataset print(X[:5]) # Step 2: Preprocess the data (handling missing values, if any, is not shown here as the dataset is clean) # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Step 4: Implement a Linear Regression model model = LinearRegression() # Step 5: Train the model and evaluate its performance model.fit(X_train, y_train) predictions = model.predict(X_test) # Print Mean Squared Error on the testing set mse = mean_squared_error(y_test, predictions) print(f\\"Mean Squared Error: {mse}\\") ```","solution":"from sklearn.datasets import fetch_california_housing from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error def load_and_preprocess_data(): # Step 1: Load the dataset data = fetch_california_housing() X, y = data[\'data\'], data[\'target\'] # Print the first 5 rows of the dataset print(X[:5]) # Step 3: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_and_evaluate_model(X_train, X_test, y_train, y_test): # Step 4: Implement a Linear Regression model model = LinearRegression() # Step 5: Train the model on the training data model.fit(X_train, y_train) # Predict on the testing set predictions = model.predict(X_test) # Evaluate its performance on the testing data using Mean Squared Error mse = mean_squared_error(y_test, predictions) return mse # Load and preprocess the data X_train, X_test, y_train, y_test = load_and_preprocess_data() # Train the model and evaluate its performance mse = train_and_evaluate_model(X_train, X_test, y_train, y_test) # Print Mean Squared Error print(f\\"Mean Squared Error: {mse}\\")"},{"question":"Objective You are required to implement a custom scope management system in Python utilizing concepts similar to Python\'s internal management of variable scopes using cell objects. Your task is to write a class `ScopeManager` that manages the storage and retrieval of variables across different scopes. Requirements 1. **Class Name**: `ScopeManager` 2. **Methods**: - `set_variable(name: str, value: Any) -> None`: Store the variable with the given name and value in the current scope. - `get_variable(name: str) -> Any`: Retrieve the value of the variable with the given name from the nearest enclosing scope. - `enter_scope() -> None`: Enter a new scope. - `exit_scope() -> None`: Exit the current scope. Constraints 1. The `ScopeManager` should support nested scopes and correctly handle variable shadowing. 2. You should not use existing Python modules that provide similar functionality. You must manage scopes and variables manually. 3. You can assume scope management methods (`enter_scope`, `exit_scope`) will be called in a valid sequence (i.e., there will be an unmatched `enter_scope` for every scope). Example Usage ```python scope_manager = ScopeManager() scope_manager.enter_scope() scope_manager.set_variable(\'x\', 10) scope_manager.enter_scope() scope_manager.set_variable(\'y\', 20) print(scope_manager.get_variable(\'x\')) # Output: 10 print(scope_manager.get_variable(\'y\')) # Output: 20 scope_manager.exit_scope() print(scope_manager.get_variable(\'y\')) # Raises KeyError scope_manager.exit_scope() ``` Additional Notes - You may simulate cell objects using dictionaries or other data structures if desired. - If a variable is not found in any scope, you should raise a `KeyError`. Write your solution in the space provided below. Ensure your code is clear, well-documented, and properly tested.","solution":"class ScopeManager: def __init__(self): self.scopes = [{}] def set_variable(self, name: str, value: any) -> None: self.scopes[-1][name] = value def get_variable(self, name: str) -> any: for scope in reversed(self.scopes): if name in scope: return scope[name] raise KeyError(f\\"Variable \'{name}\' not found in any scope\\") def enter_scope(self) -> None: self.scopes.append({}) def exit_scope(self) -> None: if len(self.scopes) > 1: self.scopes.pop() else: raise IndexError(\\"No scope to exit\\")"},{"question":"# Question: Using urllib to Fetch and Process Web Page Data You are tasked with creating a Python script that fetches and processes data from a given URL. Specifically, you will use the `urllib` package to obtain web page content, handle potential errors and responses, and validate the fetched data against supplied criteria. Requirements: 1. **Input**: - A string representing the URL to fetch (e.g., `\\"http://example.com\\"`). - A dictionary containing headers to be included in the request (e.g., `{\\"User-Agent\\": \\"CustomAgent/1.0\\"}`). - An integer timeout value for the socket connection (in seconds). - A string containing a substring to search for in the web page content (e.g., `\\"Python\\"`). 2. **Output**: - The raw HTML content of the fetched web page if successful. - If the substring is not found in the HTML content, raise a custom exception `SubstringNotFoundException`. 3. **Constraints**: - You must handle potential errors, such as `URLError` and `HTTPError`. - Implement basic authentication if the URL requires it. - The socket timeout should be set using the provided timeout value. 4. **Performance**: - Optimize the script to handle possible large responses efficiently. Function Signature: ```python def fetch_and_process_url(url: str, headers: dict, timeout: int, substring: str) -> str: pass ``` Instructions: 1. Import necessary modules (e.g., `urllib.request`, `urllib.error`, `socket`, etc.). 2. Create a custom exception `SubstringNotFoundException` for handling cases where the required substring is not found. 3. Implement a function `fetch_and_process_url` that: - Sets the socket timeout. - Fetches the web page content using the given URL and headers. - Handles potential errors during the fetching process. - Searches for the specified substring within the HTML content. - Returns the raw HTML content if the substring is found. - Raises the `SubstringNotFoundException` if the substring is not found. Example Usage: ```python url = \\"http://example.com\\" headers = {\\"User-Agent\\": \\"CustomAgent/1.0\\"} timeout = 10 substring = \\"Example Domain\\" try: html_content = fetch_and_process_url(url, headers, timeout, substring) print(\\"Fetched content:\\", html_content) except SubstringNotFoundException: print(\\"Substring not found in the fetched content.\\") except URLError as e: print(\\"Failed to reach a server. Reason:\\", e.reason) except HTTPError as e: print(f\\"The server couldn\'t fulfill the request. Error code: {e.code}\\") ``` **Note**: Ensure that your script can handle different types of URLs such as HTTP and HTTPS, and can also manage redirects effectively.","solution":"import urllib.request import urllib.error import socket class SubstringNotFoundException(Exception): pass def fetch_and_process_url(url: str, headers: dict, timeout: int, substring: str) -> str: # Setting the socket timeout socket.setdefaulttimeout(timeout) # Creating the request object with headers request = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(request) as response: html_content = response.read().decode(\'utf-8\') if substring not in html_content: raise SubstringNotFoundException(f\\"The substring \'{substring}\' was not found in the content.\\") return html_content except urllib.error.HTTPError as e: print(f\\"HTTP Error: {e.code} - {e.reason}\\") raise except urllib.error.URLError as e: print(f\\"URL Error: {e.reason}\\") raise except socket.timeout: print(\\"Request timed out\\") raise # Example Usage: # url = \\"http://example.com\\" # headers = {\\"User-Agent\\": \\"CustomAgent/1.0\\"} # timeout = 10 # substring = \\"Example Domain\\" # try: # html_content = fetch_and_process_url(url, headers, timeout, substring) # print(\\"Fetched content:\\", html_content) # except SubstringNotFoundException: # print(\\"Substring not found in the fetched content.\\") # except urllib.error.URLError as e: # print(\\"Failed to reach a server. Reason:\\", e.reason) # except urllib.error.HTTPError as e: # print(f\\"The server couldn\'t fulfill the request. Error code: {e.code}\\")"},{"question":"**Objective:** Demonstrate your understanding of Python\'s built-in constants by implementing a function that interacts with objects and uses constants appropriately. Problem Statement Implement a function `compare_objects(obj1, obj2)` that compares two objects using the `__eq__`, `__lt__`, and other related special methods. Your function should adhere to the following requirements: 1. **Return `True` or `False` if the comparison results are definitive**: - For equality, use the `==` operator. - For less than, use the `<` operator. 2. **Return `NotImplemented` if the comparison is not implemented**: - If the comparison cannot be performed because either object returns `NotImplemented`, your function should also return `NotImplemented`. 3. **Handle the following edge cases correctly**: - If one of the objects is `None`, your function should return `False` for both equality and less-than checks. - If both objects are `None`, return `True` for equality and `False` for less-than checks. - If either object is of `EllipsisType`, return `False` for equality and `NotImplemented` for less-than checks. 4. **Raise `TypeError` if invalid types are encountered**: - If types other than those specifically handled (including `None` and `Ellipsis`) are compared and result in a TypeError, re-raise the TypeError. ```python def compare_objects(obj1, obj2): Compare two objects using custom rules involving Python built-in constants. Parameters: - obj1: Any object - obj2: Any object Returns: - bool: Result of comparison if definitive - NotImplemented: If comparison cannot be performed Raises: - TypeError: If invalid types are encountered during comparison # Your implementation here ``` Input - `obj1`: Any object type - `obj2`: Any object type Output - `True`, `False` or `NotImplemented` Constraints 1. You must not use any external libraries. 2. The function should handle basic Python types and the specific constants listed (`None`, `Ellipsis`). Example Usages ```python compare_objects(5, 10) # Expected: False (5 is not equal to 10) compare_objects(\'hello\', \'hello\') # Expected: True (\'hello\' is equal to \'hello\') compare_objects(None, 10) # Expected: False (None does not equal 10) compare_objects(None, None) # Expected: True (None is equal to None) compare_objects(..., 5) # Expected: NotImplemented (Ellipsis with int is not implemented) ``` *Note*: Ensure to manage various edge cases effectively and appropriately use the mentioned constants.","solution":"def compare_objects(obj1, obj2): Compare two objects using custom rules involving Python built-in constants. Parameters: - obj1: Any object - obj2: Any object Returns: - bool: Result of comparison if definitive - NotImplemented: If comparison cannot be performed Raises: - TypeError: If invalid types are encountered during comparison # Handle None comparisons first if obj1 is None and obj2 is None: return True if obj1 is None or obj2 is None: return False # Handle EllipsisType (which is ...) if obj1 is Ellipsis or obj2 is Ellipsis: if obj1 is Ellipsis and obj2 is Ellipsis: return True return NotImplemented try: if obj1 == obj2: return True if obj1 < obj2: return False except TypeError: raise TypeError(\\"Comparison threw TypeError, which means invalid types\\") return False"},{"question":"**Question: Advanced Data Visualization with Seaborn** **Objective:** You are tasked with creating an advanced data visualization using the seaborn library to explore the relationships and distributions within the \\"penguins\\" dataset. This exercise will assess your ability to use various seaborn functionalities to create a meaningful and informative composite plot. **Dataset:** Use the \\"penguins\\" dataset that can be loaded directly from seaborn. **Task:** 1. Load the \\"penguins\\" dataset. 2. Create a composite plot consisting of the following elements: - A scatter plot showing the relationship between flipper length and bill length, with different species highlighted using color. This plot should be faceted by the \\"sex\\" of the penguins. - Add a marginal histogram (using `histplot`) to show the univariate distribution of flipper lengths for each species on the x-axis of the scatter plot. - Below the scatter plot, add a bar plot showing the count of each species categorized by the \\"island\\" variable. Each bar should be displayed as a separate facet based on the \\"sex\\" variable. **Requirements:** - Use the relevant figure-level functions to create the faceted scatter plot and bar plot. - Customize the plots to ensure clear labeling (e.g., axes labels, plot titles) and an informative legend. - Ensure that all subplots within the composite plot are aligned and do not overlap. **Expected Input:** - No specific input. The task is defined to use the seaborn \\"penguins\\" dataset. **Expected Output:** - A single matplotlib figure with the composite plot described. **Constraints:** - The solution should use seaborn\'s figure-level functions (`displot`, `relplot`, `catplot`, etc.) to manage the faceting and overall layout. Feel free to utilize the seaborn documentation to reference any additional customizations needed for your plots. ```python # Your implementation goes here import seaborn as sns import matplotlib.pyplot as plt # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Composite plot setup # 1. Scatter plot between \'flipper_length_mm\' and \'bill_length_mm\' faceted by \'sex\' g = sns.relplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"scatter\\", facet_kws={\'sharex\': True, \'sharey\': True} ) # Adding marginal histograms for ax in g.axes.flat: sns.histplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", ax=ax, legend=False, element=\\"bars\\" ) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Bill Length (mm)\\") g.set_titles(col_template=\\"{col_name} Penguins\\") # 2. Bar plot of species count by island, faceted by \'sex\' f, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True) sns.countplot(data=penguins, x=\\"island\\", hue=\\"species\\", ax=axs[0]) axs[0].set_title(\\"Species Count by Island (Male)\\") sns.countplot(data=penguins[penguins[\'sex\'] == \'Female\'], x=\\"island\\", hue=\\"species\\", ax=axs[1]) axs[1].set_title(\\"Species Count by Island (Female)\\") # Adjust layout and show the plot plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_composite_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Composite plot setup # 1. Scatter plot between \'flipper_length_mm\' and \'bill_length_mm\' faceted by \'sex\' g = sns.relplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"bill_length_mm\\", hue=\\"species\\", col=\\"sex\\", kind=\\"scatter\\" ) # Adding marginal histograms for ax in g.axes.flat: sns.histplot( data=penguins, x=\\"flipper_length_mm\\", hue=\\"species\\", multiple=\\"stack\\", ax=ax, legend=False, element=\\"bars\\" ) g.set_axis_labels(\\"Flipper Length (mm)\\", \\"Bill Length (mm)\\") g.set_titles(col_template=\\"{col_name}\\") # 2. Bar plot of species count by island, faceted by \'sex\' f, axs = plt.subplots(nrows=2, figsize=(12, 8), sharex=True) sns.countplot(data=penguins[penguins[\'sex\'] == \'Male\'], x=\\"island\\", hue=\\"species\\", ax=axs[0]) axs[0].set_title(\\"Species Count by Island (Male)\\") sns.countplot(data=penguins[penguins[\'sex\'] == \'Female\'], x=\\"island\\", hue=\\"species\\", ax=axs[1]) axs[1].set_title(\\"Species Count by Island (Female)\\") # Adjust layout plt.tight_layout() sns.move_legend(g, \\"upper left\\", bbox_to_anchor=(1, 1)) # Show the plot plt.show() # Call the function to create the plot create_penguins_composite_plot()"},{"question":"You are tasked to write a Python program that processes strings and performs various operations based on the ASCII character class checks provided by the `curses.ascii` module. # Task: Implement a function `process_ascii_string(s: str) -> Tuple[int, int, int, int, int, str, str]` that takes a single string input `s` and returns a tuple containing the following information: 1. Count of alphanumeric characters. 2. Count of alphabetic characters. 3. Count of control characters. 4. Count of whitespace characters. 5. Count of printable characters. 6. All control characters transformed into their string representations using `curses.ascii.unctrl`. 7. The string with all ASCII control characters replaced by their mnemonic names. # Function Signature: ```python from typing import Tuple def process_ascii_string(s: str) -> Tuple[int, int, int, int, int, str, str]: pass ``` # Input: - `s`: A string consisting of ASCII characters. (0 <= len(s) <= 1000) # Output: A tuple with the following: 1. An integer representing the number of alphanumeric characters in the string. 2. An integer representing the number of alphabetic characters in the string. 3. An integer representing the number of control characters in the string. 4. An integer representing the number of whitespace characters in the string. 5. An integer representing the number of printable characters in the string. 6. A string where control characters are replaced by their corresponding representations using `curses.ascii.unctrl`. 7. A string where all ASCII control characters (0x00-0x1f and 0x7f) are replaced by their mnemonic names based on `curses.ascii.controlnames`. # Constraints: - The function must utilize the capabilities of the `curses.ascii` module effectively. - The function should handle strings with a length up to 1000 characters efficiently. # Example: ```python from curses.ascii import controlnames def process_ascii_string(s: str) -> Tuple[int, int, int, int, int, str, str]: import curses.ascii as ca alnum_count = sum(ca.isalnum(c) for c in s) alpha_count = sum(ca.isalpha(c) for c in s) ctrl_count = sum(ca.isctrl(c) for c in s) space_count = sum(ca.isspace(c) for c in s) printable_count = sum(ca.isprint(c) for c in s) unctrl_chars = \'\'.join(ca.unctrl(c) for c in s) mnemonic_string = \'\'.join( controlnames[ord(c)] if ca.isctrl(c) else c for c in s ) return (alnum_count, alpha_count, ctrl_count, space_count, printable_count, unctrl_chars, mnemonic_string) # Example usage: print(process_ascii_string(\\"Hello,nWorld!x01x02\\")) # Expected output: (10, 10, 2, 1, 11, \'Hello,^JWorld!^A^B\', \'Hello,NLETLWorld!^A^B\') ```","solution":"from typing import Tuple import curses.ascii as ca def process_ascii_string(s: str) -> Tuple[int, int, int, int, int, str, str]: alnum_count = sum(ca.isalnum(c) for c in s) alpha_count = sum(ca.isalpha(c) for c in s) ctrl_count = sum(ca.isctrl(c) for c in s) space_count = sum(ca.isspace(c) for c in s) printable_count = sum(ca.isprint(c) for c in s) unctrl_chars = \'\'.join(ca.unctrl(c) for c in s) mnemonic_string = \'\'.join( ca.controlnames[ord(c)] if ca.isctrl(c) else c for c in s ) return (alnum_count, alpha_count, ctrl_count, space_count, printable_count, unctrl_chars, mnemonic_string)"},{"question":"# Question: You are required to create a Python script that performs the following tasks using the `urllib.request` module: 1. **Fetch Content**: - Create a function `fetch_content(url: str) -> str` that takes a URL as input and returns the first 500 characters of its content as a string. - Handle any potential errors (like network issues or HTTP errors) gracefully by returning an appropriate error message. 2. **POST Request**: - Create a function `post_request(url: str, data: dict) -> str` that takes a URL and a dictionary of data as input, sends a POST request with the data to the URL, and returns the server\'s response as a string. 3. **Handle Redirection**: - Create a function `handle_redirection(url: str) -> str` that takes a URL as input, follows any redirections to fetch the final URL\'s content, and returns the first 300 characters of the content. 4. **Basic Authentication**: - Create a function `fetch_with_basic_auth(url: str, username: str, password: str) -> str` that takes a URL, a username, and a password as input, performs HTTP Basic Authentication, and returns the content of the URL as a string. # Constraints: - You must use the `urllib.request` module exclusively for HTTP operations. - Handle exceptions and errors appropriately, ensuring that the script does not crash unexpectedly. - Include docstrings for all functions and ensure that the code is clean and well-documented. # Example Usage: ```python # Fetch content from a URL print(fetch_content(\'http://www.example.com\')) # Send a POST request data = {\'key1\': \'value1\', \'key2\': \'value2\'} print(post_request(\'http://www.example.com/post\', data)) # Handle redirection print(handle_redirection(\'http://www.example.com/redirect\')) # Fetch content with basic authentication print(fetch_with_basic_auth(\'http://www.example.com/protected\', \'user\', \'pass\')) ``` Implement the four functions as specified, ensuring to handle various edge cases and potential errors.","solution":"import urllib.request import urllib.parse import urllib.error import base64 def fetch_content(url: str) -> str: Fetch the first 500 characters of content from the given URL. :param url: URL to fetch content from :return: First 500 characters of the content or an error message try: with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') return content[:500] except urllib.error.URLError as e: return f\\"Error fetching content: {e.reason}\\" def post_request(url: str, data: dict) -> str: Send a POST request to the given URL with the provided data and return the response. :param url: URL to send the POST request to :param data: Dictionary containing data to send in the POST request :return: Server response or an error message try: data_encoded = urllib.parse.urlencode(data).encode(\'utf-8\') req = urllib.request.Request(url, data=data_encoded, method=\'POST\') with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: return f\\"Error in POST request: {e.reason}\\" def handle_redirection(url: str) -> str: Handle redirection and fetch the first 300 characters of content from the final URL. :param url: Initial URL to fetch content from :return: First 300 characters of the final content or an error message try: with urllib.request.urlopen(url) as response: content = response.read().decode(\'utf-8\') return content[:300] except urllib.error.URLError as e: return f\\"Error handling redirection: {e.reason}\\" def fetch_with_basic_auth(url: str, username: str, password: str) -> str: Perform HTTP Basic Authentication and fetch the content from the given URL. :param url: URL to fetch content from :param username: Username for basic authentication :param password: Password for basic authentication :return: Content of the URL or an error message try: credentials = f\\"{username}:{password}\\" encoded_credentials = base64.b64encode(credentials.encode(\'ascii\')).decode(\'ascii\') headers = {\'Authorization\': \'Basic \' + encoded_credentials} req = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.URLError as e: return f\\"Error fetching with basic auth: {e.reason}\\""},{"question":"Given the documentation on the `sklearn.cross_decomposition` module and its emphasis on Partial Least Squares (PLS) methods, implement a simplified version of the `PLSSVD` algorithm without using any high-level scikit-learn utilities. # Question Implement a Python function `simplified_plssvd` that takes two centered matrices `X` and `Y`, and an integer `n_components`. The function should: 1. Compute the cross-covariance matrix `C = X^T Y`. 2. Perform Singular Value Decomposition (SVD) on `C`. 3. Select the `n_components` largest singular values and their corresponding singular vectors. 4. Transform `X` and `Y` using the selected singular vectors. # Function Signature ```python def simplified_plssvd(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: pass ``` # Input - `X`: A centered numpy array of shape (n_samples, n_features). - `Y`: A centered numpy array of shape (n_samples, n_targets). - `n_components`: An integer specifying the number of components to keep. # Output - A tuple `(X_transformed, Y_transformed)`: - `X_transformed`: A numpy array of shape (n_samples, n_components) representing the transformed `X` matrix. - `Y_transformed`: A numpy array of shape (n_samples, n_components) representing the transformed `Y` matrix. # Constraints - Ensure the input matrices `X` and `Y` are centered (i.e., have zero mean). # Example ```python import numpy as np # Example centered data X = np.array([[1, 0], [-1, 0], [0, 1], [0, -1]]) Y = np.array([[0.1, 0.2], [-0.1, -0.2], [0.05, 0.1], [-0.05, -0.1]]) # Simplified PLSSVD with 1 component X_transformed, Y_transformed = simplified_plssvd(X, Y, n_components=1) print(X_transformed) print(Y_transformed) ``` This question tests the student\'s ability to apply fundamental concepts of SVD and matrix transformations directly, demonstrating a deep understanding of the underlying mechanics of the `PLSSVD` algorithm.","solution":"import numpy as np from typing import Tuple def simplified_plssvd(X: np.ndarray, Y: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]: Applies the Simplified Partial Least Squares Singular Value Decomposition (PLSSVD) to the centered matrices X and Y. Parameters: - X: A centered numpy array of shape (n_samples, n_features). - Y: A centered numpy array of shape (n_samples, n_targets). - n_components: Number of components to keep. Returns: - Tuple of transformed X and Y matrices, each of shape (n_samples, n_components). # Compute the cross-covariance matrix C = np.dot(X.T, Y) # Perform Singular Value Decomposition U, S, VT = np.linalg.svd(C, full_matrices=False) # Select the top n_components singular values and their corresponding singular vectors U_selected = U[:, :n_components] VT_selected = VT[:n_components, :] # Transform X and Y using the selected singular vectors X_transformed = np.dot(X, U_selected) Y_transformed = np.dot(Y, VT_selected.T) return X_transformed, Y_transformed"},{"question":"<|Analysis Begin|> The `select` module provides interfaces to monitor multiple file descriptors to see if I/O operations such as reading or writing will not block. It offers several methods for I/O multiplexing, including `select.select()`, `select.poll()`, `select.epoll()`, and several others depending on the operating system. The module is beneficial for network servers that handle many connections simultaneously, as it allows for monitoring multiple file descriptors efficiently. Key Points: 1. **select.select()**: Used for the most straightforward interface to the Unix `select()` system call. 2. **select.poll()**: Provides better scalability than `select.select()` since it only requires listing the file descriptors of interest. 3. **select.epoll()**: Available on Linux for edge or level-triggered interfaces for I/O events. 4. **select.kqueue()** and **select.kevent()**: Available on BSD systems for kernel event notifications. 5. **select.devpoll()**: Available on Solaris and derivatives, offering efficient polling of active file descriptors. The question we formulate needs to focus on implementing a function that utilizes one or more of these I/O multiplexing mechanisms. The student should implement functionality to monitor several file descriptors for readiness for reading and writing, making use of these high-level I/O primitives. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Background The `select` module is a powerful tool that allows for monitoring multiple file descriptors to optimize I/O operations. It is especially useful in network programming, where a server may need to handle multiple client connections simultaneously. Objective Implement a simple chat server using Python\'s `select` module to handle multiple clients simultaneously. Your server should be able to: 1. Accept new connections. 2. Read messages from clients. 3. Broadcast received messages to all connected clients. 4. Handle client disconnections gracefully. Requirements 1. **Function Name**: `run_chat_server` 2. **Inputs**: - `host (str)`: The hostname or IP address to bind the server to. - `port (int)`: The port number to bind the server to. 3. **Behavior**: - The server should use the `select.select()` method to monitor sockets for readability and writability. - The server should be able to handle multiple client connections. - When a message is received from a client, it should be broadcast to all other connected clients. - If a client disconnects, the server should remove them from the monitored sockets and broadcast a message informing other clients about the disconnection. 4. **Constraints**: - The server should be able to handle up to 10 clients simultaneously. - The server should handle potential errors, such as clients disconnecting abruptly. - Use non-blocking sockets to ensure the server does not get stuck on any single client. 5. **Libraries allowed**: `select`, `socket`, `sys` 6. **Performance**: - The server should efficiently handle multiple clients with minimal delay in message broadcasting. Example Usage ```python # Start the chat server run_chat_server(\'127.0.0.1\', 12345) ``` Implementation Skeleton ```python import select import socket import sys def run_chat_server(host: str, port: int): # Create a TCP/IP socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # Bind the socket to the port server_socket.bind((host, port)) # Listen for incoming connections server_socket.listen(10) server_socket.setblocking(False) # List of sockets to monitor for incoming connections sockets_list = [server_socket] # Mapping of socket to client data clients = {} print(f\\"Chat server started on {host}:{port}\\") while True: # Wait for any of the sockets to be ready for processing read_sockets, write_sockets, error_sockets = select.select(sockets_list, [], sockets_list) for sock in read_sockets: if sock == server_socket: # The server socket is ready to accept a new connection client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"New connection from {client_address}\\") else: try: # A client socket has sent a message message = sock.recv(1024) if message: # Broadcast the message to all other clients for s in clients: if s != sock: s.send(message) else: # Handle disconnection print(f\\"Client {clients[sock]} disconnected\\") sockets_list.remove(sock) del clients[sock] sock.close() except Exception as e: print(f\\"Exception: {e}\\") sockets_list.remove(sock) del clients[sock] sock.close() for sock in error_sockets: sockets_list.remove(sock) if sock in clients: del clients[sock] sock.close() ``` Good luck!","solution":"import select import socket import sys def run_chat_server(host: str, port: int): # Create a TCP/IP socket server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) # Bind the socket to the port server_socket.bind((host, port)) # Listen for incoming connections server_socket.listen(10) server_socket.setblocking(False) # List of sockets to monitor for incoming connections sockets_list = [server_socket] # Mapping of socket to client address clients = {} print(f\\"Chat server started on {host}:{port}\\") while True: # Wait for any of the sockets to be ready for processing read_sockets, _, exception_sockets = select.select(sockets_list, [], sockets_list) for sock in read_sockets: if sock == server_socket: # The server socket is ready to accept a new connection client_socket, client_address = server_socket.accept() client_socket.setblocking(False) sockets_list.append(client_socket) clients[client_socket] = client_address print(f\\"New connection from {client_address}\\") else: try: # A client socket has sent a message message = sock.recv(1024) if message: # Broadcast the message to all other clients for client_socket in clients: if client_socket != sock: client_socket.send(message) else: # Handle client disconnection print(f\\"Client {clients[sock]} disconnected\\") sockets_list.remove(sock) del clients[sock] sock.close() except Exception as e: print(f\\"Exception: {e}\\") sockets_list.remove(sock) del clients[sock] sock.close() for sock in exception_sockets: sockets_list.remove(sock) if sock in clients: del clients[sock] sock.close()"},{"question":"Objective You are tasked with writing a Python script that interacts with the NIS (Network Information Service) to perform various operations using the `nis` module. Your script should demonstrate an understanding of key lookups, retrieving map contents, and handling errors effectively. Problem Statement You are to implement the following function: ```python def query_nis_service(key: str, mapname: str, domain: str = None) -> dict: Perform various NIS operations and return their results in a structured dictionary. Parameters: - key: A string representing the key to be looked up in the specified NIS map. - mapname: A string representing the name of the NIS map to query. - domain: An optional string representing the NIS domain to use. If not provided, the default system domain should be used. Returns: - A dictionary with the following structure: { \\"default_domain\\": <default_domain>, \\"all_maps\\": <list_of_all_maps>, \\"key_match\\": <matching_value_for_key>, \\"map_contents\\": <contents_of_map>, } If any operation raises a \'nis.error\', include \'NIS_ERROR\' as the value in place of the result for the corresponding key. pass ``` Input - `key` (str): A string that you want to look up in the specified NIS map. - `mapname` (str): A string representing the name of the NIS map where you want to look up the key. - `domain` (str, optional): A string representing the NIS domain for the lookup. This is optional. If not specified, use the default NIS domain. Output A dictionary with the following keys: - `default_domain` (str): The default NIS domain of the system. - `all_maps` (list of str): A list of all valid NIS maps in the specified or default domain. - `key_match` (bytes or str): The value matching the provided key in the specified map. If an error occurs, the value should be `\'NIS_ERROR\'`. - `map_contents` (dict): A dictionary containing all key-value pairs in the specified map. If an error occurs, the value should be `\'NIS_ERROR\'`. Constraints - Assume the module is run on a Unix-based system where NIS is properly configured and accessible. Here\'s an example usage: ```python key = \\"some_key\\" mapname = \\"some_map\\" domain = \\"some_domain\\" result = query_nis_service(key, mapname, domain) print(result) ``` **Note:** The actual values in the example usage may vary based on the NIS configuration and data. Implementation Notes - You must handle the `nis.error` exception appropriately to ensure that the script completes without crashing even if some operations fail. - The returned values for key lookups and map contents can contain arbitrary binary data. - Optimize for readability and robustness by ensuring clean and organized exception handling.","solution":"import nis def query_nis_service(key: str, mapname: str, domain: str = None) -> dict: Perform various NIS operations and return their results in a structured dictionary. Parameters: - key: A string representing the key to be looked up in the specified NIS map. - mapname: A string representing the name of the NIS map to query. - domain: An optional string representing the NIS domain to use. If not provided, the default system domain should be used. Returns: - A dictionary with the following structure: { \\"default_domain\\": <default_domain>, \\"all_maps\\": <list_of_all_maps>, \\"key_match\\": <matching_value_for_key>, \\"map_contents\\": <contents_of_map>, } If any operation raises a \'nis.error\', include \'NIS_ERROR\' as the value in place of the result for the corresponding key. result = {} try: default_domain = nis.get_default_domain() result[\'default_domain\'] = default_domain except nis.error: result[\'default_domain\'] = \'NIS_ERROR\' try: all_maps = nis.maps(domain) result[\'all_maps\'] = all_maps except nis.error: result[\'all_maps\'] = \'NIS_ERROR\' try: key_match = nis.match(key, mapname, domain) result[\'key_match\'] = key_match except nis.error: result[\'key_match\'] = \'NIS_ERROR\' try: map_contents = nis.cat(mapname, domain) result[\'map_contents\'] = map_contents except nis.error: result[\'map_contents\'] = \'NIS_ERROR\' return result"},{"question":"# **I/O Stream Operations in Python** **Objective:** Demonstrate understanding of Python\'s `io` module by performing file operations in text and binary modes, utilizing both buffered and unbuffered I/O streams. **Task:** 1. Create a function `process_file(input_file_path: str, output_file_path: str) -> None` that: - Reads text from a file specified by `input_file_path`. - Writes the text to another file specified by `output_file_path` in binary mode. - While reading text, ensure that an explicit encoding (`utf-8`) is specified. - Convert the read text to uppercase before writing it to the binary file. - The function should handle any necessary buffering appropriately for efficient I/O performance. - Ensure that any intermediate I/O operations make use of buffering to enhance performance. **Function Signature:** ```python def process_file(input_file_path: str, output_file_path: str) -> None: pass ``` **Example:** Given an `input_file.txt` contains: ``` Hello, World! This is a test file. ``` The resultant `output_file.bin` should contain the text in uppercase: ``` HELLO, WORLD! THIS IS A TEST FILE. ``` **Constraints:** - The input file will contain text data. - The output file should be written in binary mode. - The total size of the input file will be less than 10MB. **Additional Requirements:** - Handle exceptions appropriately. - Ensure the files are closed properly after operations. - Write the function in a way that enhances performance through the use of buffering. **Evaluation:** Your solution will be evaluated based on: - Correctness and completeness of the function implementation. - Use of correct file modes and encodings. - Proper management of file resources. - Efficient handling of I/O operations, especially with regard to buffering.","solution":"import io def process_file(input_file_path: str, output_file_path: str) -> None: try: # Open the input file in text mode with explicit utf-8 encoding with io.open(input_file_path, \'r\', encoding=\'utf-8\', buffering=4096) as input_file: # Read the entire content as a text string text_content = input_file.read() # Convert the text to uppercase uppercase_text = text_content.upper() # Open the output file in binary mode with io.open(output_file_path, \'wb\', buffering=4096) as output_file: # Write the uppercase text as binary data output_file.write(uppercase_text.encode(\'utf-8\')) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Objective Demonstrate comprehensive understanding of Python\'s codec registry, encoding/decoding operations, and Unicode error handling mechanisms. # Problem Statement You are tasked with writing a Python script to encode and decode strings using specified encodings, and to handle encoding errors using custom error handlers registered in the codec registry. # Instructions 1. **Register a custom Unicode error handler:** - Implement and register an error handler named `my_replace_errors` that replaces problematic sequences with `\'<invalid>\'`. 2. **Encoding and Decoding Functions:** - Implement a function `encode_string(input_str: str, encoding: str) -> bytes` that encodes `input_str` using the specified `encoding`. If encoding fails due to an error, use the `my_replace_errors` handler. - Implement a function `decode_string(input_bytes: bytes, encoding: str) -> str` that decodes `input_bytes` using the specified `encoding`. If decoding fails due to an error, raise the appropriate exception. 3. **Codec Existence Check:** - Implement a function `codec_exists(encoding: str) -> bool` that returns `True` if there is a registered codec for the specified `encoding`, otherwise returns `False`. # Constraints - You may only use standard Python libraries. - Assume the input string and encoding names are always valid strings. - The functions should handle common encodings like \'utf-8\', \'ascii\', \'latin-1\', etc. # Example ```python # Example usage: def my_replace_errors(exception): return (\'<invalid>\', exception.end) PyCodec_RegisterError(\\"my_replace_errors\\", my_replace_errors) input_str = \\"Hello, World! 😊\\" encoding = \\"ascii\\" if codec_exists(encoding): encoded = encode_string(input_str, encoding) decoded = decode_string(encoded, encoding) print(f\\"Encoded: {encoded}\\") print(f\\"Decoded: {decoded}\\") else: print(f\\"Codec for encoding \'{encoding}\' does not exist.\\") ``` # Expected Output ```plaintext Encoded: b\'Hello, World! <invalid>\' Decoded: Hello, World! <invalid> ``` Note: The example usage demonstrates registration of the custom error handler, encoding with a problematic character using \'ascii\' encoding, and decoding it back while handling errors.","solution":"import codecs def my_replace_errors(exception): Custom error handler that replaces problematic sequences with \'<invalid>\'. return (\'<invalid>\', exception.end) # Register the custom error handler codecs.register_error(\\"my_replace_errors\\", my_replace_errors) def encode_string(input_str: str, encoding: str) -> bytes: Encode the input string using the specified encoding. If the encoding fails, use the \'my_replace_errors\' handler. return input_str.encode(encoding, errors=\'my_replace_errors\') def decode_string(input_bytes: bytes, encoding: str) -> str: Decode the input bytes using the specified encoding. If decoding fails, raise the appropriate exception. return input_bytes.decode(encoding) def codec_exists(encoding: str) -> bool: Check if there is a registered codec for the specified encoding. try: codecs.lookup(encoding) return True except LookupError: return False"},{"question":"**Assessing Time Series Data with pandas Resampler** # Objective: Demonstrate your ability to handle time series data using pandas\' Resampler class by resampling, aggregating, and computing statistics on a data set. # Question: You are provided with a DataFrame containing time series data for hourly temperature readings over a year. Your task is to perform the following operations: 1. **Resample the Data**: Aggregate the data by `day` and compute the daily mean temperature. 2. **Identify Extremes**: Find and return the date with the highest daily mean temperature and its corresponding value. 3. **Missing Data Handling**: For any days with missing hourly data, use forward fill (`ffill`) to fill in those gaps before performing the resampling operations. # Input: A DataFrame `df` with the following structure: - `index`: DatetimeIndex representing hourly timestamps. - `columns`: A single column named `temperature` representing temperature readings. # Output: - A tuple containing: 1. A DataFrame with daily mean temperatures. 2. A tuple with the date of the highest daily mean temperature and the temperature value. # Constraints: - Ensure your solution can handle large DataFrames efficiently. - Assume there may be missing hourly data, but not more than 24 consecutive hours. # Example: ```python import pandas as pd import numpy as np # Example DataFrame date_rng = pd.date_range(start=\'1/1/2022\', end=\'31/12/2022\', freq=\'H\') df = pd.DataFrame(date_rng, columns=[\'date\']) df[\'temperature\'] = np.random.uniform(low=-10, high=30, size=(len(date_rng))) df = df.set_index(\'date\') # Function to implement def analyze_temperature(df): # Ensure the data is sorted by date df = df.sort_index() # Fill missing values df = df.resample(\'H\').mean() df = df.ffill() # Resample to daily frequency and compute mean temperature daily_mean = df.resample(\'D\').mean() # Identify the date with the highest mean temperature max_temp_date = daily_mean[\'temperature\'].idxmax() max_temp_value = daily_mean[\'temperature\'].max() # Return the result return daily_mean, (max_temp_date, max_temp_value) # Example Execution daily_mean, max_temp = analyze_temperature(df) print(daily_mean.head()) print(f\\"Highest daily mean temperature: {max_temp}\\") ``` Output of example execution may vary due to random temperature values. ```plaintext temperature date 2022-01-01 11.123456 2022-01-02 12.234567 2022-01-03 10.345678 2022-01-04 13.456789 ... Highest daily mean temperature: (Timestamp(\'2022-07-15 00:00:00\'), 25.678912) ```","solution":"import pandas as pd def analyze_temperature(df): Analyze temperature data to compute daily mean temperatures, handle missing values, and identify the day with the highest daily mean temperature. Parameters: df (DataFrame): DataFrame with DatetimeIndex and a \'temperature\' column. Returns: tuple: (DataFrame with daily mean temperatures, tuple with the date of the highest daily mean temperature and its value) # Ensure the data is sorted by date df = df.sort_index() # Fill missing values using forward fill df = df.resample(\'H\').mean() df = df.ffill() # Resample to daily frequency and compute mean temperature daily_mean = df.resample(\'D\').mean() # Identify the date with the highest mean temperature max_temp_date = daily_mean[\'temperature\'].idxmax() max_temp_value = daily_mean[\'temperature\'].max() # Return the result return daily_mean, (max_temp_date, max_temp_value)"},{"question":"Coding Assessment Question # Objective You are required to create and register custom reduction functions for a complex object serialization process using the `copyreg` module and pickling methods. # Question You are given a class named `Person` which contains personal information and a nested class `Address`, which encapsulates the address details of the person. Your task is to implement custom reduction (pickling) functions for these classes, register them using `copyreg`, and demonstrate their usage through the pickling and unpickling processes. # Class Definitions ```python class Address: def __init__(self, street, city, zip_code): self.street = street self.city = city self.zip_code = zip_code class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address ``` # Requirements 1. Write reduction functions for both `Person` and `Address` classes. - The reduction function for `Address` should return a tuple of three elements representing the `street`, `city`, and `zip_code`. - The reduction function for `Person` should return a tuple consisting of the `name`, `age`, and the `Address` instance. 2. Register these reduction functions using `copyreg.pickle`. 3. Write a demonstration function `serialize_and_deserialize` that: - Creates instances of `Person` and `Address`. - Serializes and deserializes the `Person` instance using the `pickle` module. - Verifies that the deserialized instance retains the original data. # Constraints - You may assume that the input values for `name`, `age`, `street`, `city`, and `zip_code` will always be valid strings and integers. - You should use the `copyreg` and `pickle` modules for serialization and deserialization processes. # Function Signature ```python def serialize_and_deserialize(): - Create instances: address = Address(\\"123 Main St\\", \\"Anytown\\", \\"12345\\") person = Person(\\"John Doe\\", 30, address) - Serialize and deserialize `person` instance. - Verify the deserialized instance retains the original attributes. - Print the verify (use assert statements to check equality). pass ``` # Example ```python import copyreg import pickle # Define the class definitions as provided # Implement the reduction functions for Address and Person # Register the reduction functions with copyreg # Implement the serialize_and_deserialize function serialize_and_deserialize() # Expected Output: # No AssertionError should be raised indicating successful serialization and deserialization ``` Complete the function implementation as per the requirements and ensure all code is correctly working by testing it.","solution":"import copyreg import pickle class Address: def __init__(self, street, city, zip_code): self.street = street self.city = city self.zip_code = zip_code class Person: def __init__(self, name, age, address): self.name = name self.age = age self.address = address def reduce_address(address): return (Address, (address.street, address.city, address.zip_code)) def reduce_person(person): return (Person, (person.name, person.age, person.address)) copyreg.pickle(Address, reduce_address) copyreg.pickle(Person, reduce_person) def serialize_and_deserialize(): address = Address(\\"123 Main St\\", \\"Anytown\\", \\"12345\\") person = Person(\\"John Doe\\", 30, address) # Serialize the person instance serialized_person = pickle.dumps(person) # Deserialize the person instance deserialized_person = pickle.loads(serialized_person) assert deserialized_person.name == person.name assert deserialized_person.age == person.age assert deserialized_person.address.street == person.address.street assert deserialized_person.address.city == person.address.city assert deserialized_person.address.zip_code == person.address.zip_code print(\\"Serialization and Deserialization successful!\\")"},{"question":"**Title: Create and Manipulate Dynamic Function Objects in Python using the C API** Objective: Write a Python extension module (using Cython or a similar tool) that exposes a function `create_dynamic_function` to Python. This function should demonstrate the use of the internal C API functions for managing Python function objects. Description: 1. **Function Name:** `create_dynamic_function` 2. **Input:** - `code_string`: A string containing Python code defining a function. The code should have a single function which takes two arguments and returns their sum. - `function_name`: A string representing the name of the function to be created. - `docstring`: A string to be set as the function’s docstring. 3. **Output:** A Python function object that: - Executes the provided `code_string` to create a function. - Has the name specified by `function_name`. - Contains the specified `docstring`. - Returns the sum of two parameters when called. 4. **Requirements:** - Utilize the functions provided in the PyFunctionObject C API to create, modify, and retrieve function attributes. - Ensure that attributes like `globals`, `defaults`, `closure`, and `Annotations` are appropriately set and retrieved. - Raise appropriate Python exceptions for invalid inputs or on failure to create the function. 5. **Performance:** The function creation should be efficient and must use the least number of API calls necessary to achieve the desired functionality. Constraints: - The `code_string` will always contain valid Python function code. - The `function_name` will be a valid Python identifier. - The `docstring` will be a simple string without special characters. Example: Here is an example of how the `create_dynamic_function` might be used: ```python # Assume we have the module `dynamic_func` compiled from C extension from dynamic_func import create_dynamic_function code_string = \\"def temp_func(a, b):n return a + b\\" function_name = \\"add\\" docstring = \\"This function adds two numbers.\\" # Create a dynamic function dynamic_func = create_dynamic_function(code_string, function_name, docstring) # Test the created function assert dynamic_func.__name__ == \\"add\\" assert dynamic_func.__doc__ == \\"This function adds two numbers.\\" assert dynamic_func(3, 4) == 7 ``` Deliverables: - Submit the Python extension module source code. - Provide a Python script demonstrating the use of the `create_dynamic_function` with various inputs. Note: Ensure that your code follows best practices for writing Python C extensions and includes necessary error handling.","solution":"def create_dynamic_function(code_string, function_name, docstring): Creates a dynamic Python function from the provided code string, function name, and docstring. Parameters: code_string (str): A string containing Python code defining a function. function_name (str): A string representing the name of the function to be created. docstring (str): A string to be set as the function\'s docstring. Returns: function: A Python function object with the specified attributes. # Compile the code string into a code object code_object = compile(code_string, \'<string>\', \'exec\') # Create a new namespace dictionary to execute the compiled code namespace = {} # Execute the compiled code within the namespace exec(code_object, namespace) # Retrieve the function from the namespace function = namespace[function_name] # Set the function\'s docstring function.__doc__ = docstring return function"},{"question":"Objective: You are required to implement a machine learning pipeline using scikit-learn to predict the type of iris flower from the well-known Iris dataset. This task will assess your ability to apply and understand various concepts of scikit-learn, including model selection, hyperparameter tuning, and feature scaling. Dataset: The Iris dataset consists of 150 samples with 4 features each: `sepal length`, `sepal width`, `petal length`, and `petal width`. Each sample belongs to one of three classes: `Iris-setosa`, `Iris-versicolor`, or `Iris-virginica`. Task: 1. Load the Iris dataset using scikit-learn. 2. Split the dataset into a training set (70%) and a test set (30%). 3. Standardize the features using `StandardScaler`. 4. Implement an SVM classifier with RBF kernel. 5. Use GridSearchCV for hyperparameter tuning to find the best values for `C` and `gamma`. Perform a 5-fold cross-validation. 6. Evaluate the performance of the tuned model on the test set. 7. Return the accuracy score for the test set. Input: Your function will not take any input parameters. Output: Your function should return a float value representing the accuracy score of the classifier on the test set. Function Signature: ```python def iris_classification() -> float: pass ``` Constraints and Requirements: 1. Use `StandardScaler` for feature scaling. 2. Use `SVC` with an RBF kernel for classification. 3. Use `GridSearchCV` to tune the parameters `C` and `gamma`. 4. Use `accuracy_score` from `sklearn.metrics` to evaluate your model. Example: ```python accuracy = iris_classification() print(f\'Accuracy: {accuracy:.2f}\') ``` This question will require a well-thought-out coding solution that demonstrates the student’s understanding of fundamental and advanced concepts of scikit-learn.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def iris_classification() -> float: # Load the dataset iris = datasets.load_iris() X, y = iris.data, iris.target # Split the dataset into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Set up the SVC model with RBF kernel svm = SVC(kernel=\'rbf\') # Hyperparameter tuning using GridSearchCV param_grid = {\'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001]} grid_search = GridSearchCV(svm, param_grid, cv=5) grid_search.fit(X_train, y_train) # Evaluate the performance on the test set best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"You are provided with a list of tasks each represented by a tuple `(priority, task_name)` where `priority` is an integer and `task_name` is a string. Your initial task is to implement a priority queue with the following capabilities: 1. **Add a new task**: This should add a new task to the priority queue maintaining the heap property. 2. **Pop the highest priority task**: This should return the task with the highest priority (smallest value) and remove it from the queue. 3. **Change a task\'s priority**: This should update the priority of a specific task. 4. **Remove a specific task**: This should remove a specified task from the queue. # Function Definitions 1. `add_task(task_list, task_name, priority)`: - **Input**: `task_list` (list of tuples), `task_name` (string), `priority` (int) - **Output**: None - **Behavior**: Adds the task `(priority, task_name)` to the heap `task_list`. 2. `pop_top_task(task_list)`: - **Input**: `task_list` (list of tuples) - **Output**: tuple (int, string) - **Behavior**: Pops and returns the task with the highest priority from the heap `task_list`. 3. `change_task_priority(task_list, task_name, new_priority)`: - **Input**: `task_list` (list of tuples), `task_name` (string), `new_priority` (int) - **Output**: None - **Behavior**: Changes the priority of `task_name` to `new_priority`. 4. `remove_task(task_list, task_name)`: - **Input**: `task_list` (list of tuples), `task_name` (string) - **Output**: None - **Behavior**: Remove the specified task from the list (raise error if not found). # Constraints: - Assume task names are unique. - The list can be empty initially or pre-populated with tasks. - Tasks will have unique names, and the same task won\'t be added twice. # Example: ```python # Create an empty task list tasks = [] # Add tasks add_task(tasks, \'task1\', 3) add_task(tasks, \'task2\', 1) add_task(tasks, \'task3\', 2) # Pop the highest priority task assert pop_top_task(tasks) == (1, \'task2\') # Change the priority of a task change_task_priority(tasks, \'task1\', 0) # Remove a task remove_task(tasks, \'task3\') # Final state of the task queue assert tasks == [(0, \'task1\')] ``` Implement the above functions ensuring they perform efficiently using the `heapq` module.","solution":"import heapq def add_task(task_list, task_name, priority): Adds the task `(priority, task_name)` to the heap `task_list`. heapq.heappush(task_list, (priority, task_name)) def pop_top_task(task_list): Pops and returns the task with the highest priority from the heap `task_list`. return heapq.heappop(task_list) def change_task_priority(task_list, task_name, new_priority): Changes the priority of `task_name` to `new_priority`. # Find and remove the existing task found = False for i, (priority, task) in enumerate(task_list): if task == task_name: task_list.pop(i) found = True break if found: # Re-heapify after removing the item heapq.heapify(task_list) # Add the task with the new priority heapq.heappush(task_list, (new_priority, task_name)) else: raise ValueError(f\\"Task \'{task_name}\' not found\\") def remove_task(task_list, task_name): Remove the specified task from the list (raises error if not found). found = False for i, item in enumerate(task_list): if item[1] == task_name: task_list.pop(i) found = True break if found: heapq.heapify(task_list) else: raise ValueError(f\\"Task \'{task_name}\' not found\\")"},{"question":"**Objective:** Implement a functional cache system for storing large binary image objects using weak references to ensure memory efficiency. Problem Statement You are tasked with implementing a simple image cache system using weak references. The cache should store images associated with unique names. When an image is no longer referenced elsewhere in the program, it should be automatically removed from the cache. Implement a class `ImageCache` with the following requirements: 1. **Methods**: - `add_image(name: str, image: object) -> None`: Store a named image in the cache. - `get_image(name: str) -> object`: Retrieve the image by its name. Returns `None` if the image is not in the cache or has been garbage collected. - `remove_image(name: str) -> None`: Remove the image from the cache if it exists. 2. **Constraints**: - Use `weakref.WeakValueDictionary` to implement the cache. - Assume `image` is an object of any type that supports weak references. 3. **Input and Output**: - `name`: A string representing the unique name for the image. - `image`: An object representing the image. - `get_image` will return the corresponding image object, or `None` if the image has been collected. Example Usage: ```python cache = ImageCache() img1 = SomeLargeImageObject() img2 = SomeLargeImageObject() cache.add_image(\\"image1\\", img1) cache.add_image(\\"image2\\", img2) assert cache.get_image(\\"image1\\") is img1 assert cache.get_image(\\"image2\\") is img2 # Removing references to image objects del img1 del img2 # Force garbage collection import gc gc.collect() assert cache.get_image(\\"image1\\") is None assert cache.get_image(\\"image2\\") is None ``` Implementation: Implement the `ImageCache` class to meet the above requirements: ```python import weakref class ImageCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_image(self, name, image): Stores a named image in the cache. self._cache[name] = image def get_image(self, name): Retrieves the image by its name, if it still exists in the cache. return self._cache.get(name) def remove_image(self, name): Removes the image from the cache if it exists. if name in self._cache: del self._cache[name] ``` This question assesses the following skills: - Understanding and practical use of weak references and weak reference containers. - Proper usage and implementation of cache mechanisms. - Memory management in Python.","solution":"import weakref class ImageCache: def __init__(self): self._cache = weakref.WeakValueDictionary() def add_image(self, name, image): Stores a named image in the cache. self._cache[name] = image def get_image(self, name): Retrieves the image by its name, if it still exists in the cache. return self._cache.get(name) def remove_image(self, name): Removes the image from the cache if it exists. if name in self._cache: del self._cache[name]"},{"question":"Coding Assessment Question # Objective Implement a function in Python that creates a slice object with specified `start`, `stop`, and `step` values. Then, extract these values and adjust them according to a given sequence length, finally using these values to return the sliced portion of the sequence. Use the concepts outlined in the provided documentation related to slice objects. # Function Signature ```python def custom_slice(sequence, start, stop, step): Slices the given sequence using the provided start, stop, and step values. Parameters: sequence (list or str): The sequence to slice. start (int or None): The starting index of the slice. stop (int or None): The stopping index of the slice. step (int or None): The step value for the slice. Returns: list or str: The sliced portion of the sequence. ``` # Input 1. `sequence`: A list or string from which the slice is to be extracted. 2. `start`: An integer specifying the start index, or `None`. 3. `stop`: An integer specifying the stop index, or `None`. 4. `step`: An integer specifying the step value, or `None`. # Output The function should return a list or string representing the sliced portion of the input `sequence`. # Constraints 1. The slice operation should be done considering the functionalities provided in the documentation. 2. The function should handle out-of-bound indices and step values appropriately. 3. If `start`, `stop`, or `step` is `None`, it should be treated as `None` in the slice object. # Example ```python sequence = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] start = 2 stop = 8 step = 2 assert custom_slice(sequence, start, stop, step) == [2, 4, 6] sequence = \\"Hello, World!\\" start = 7 stop = None step = -1 assert custom_slice(sequence, start, stop, step) == \\"W ,olleH\\" ``` # Note * You are not required to implement the low-level C API functionalities directly, as Python natively supports slice operations in a straightforward manner. However, your implementation should showcase an understanding of how slice indices are managed and handled internally. * Consider edge cases such as negative indices and steps, start or stop indices beyond the sequence length, and step values of zero (invalid).","solution":"def custom_slice(sequence, start, stop, step): Slices the given sequence using the provided start, stop, and step values. Parameters: sequence (list or str): The sequence to slice. start (int or None): The starting index of the slice. stop (int or None): The stopping index of the slice. step (int or None): The step value for the slice. Returns: list or str: The sliced portion of the sequence. # Return an index error on invalid step value if step == 0: raise ValueError(\\"step value cannot be 0\\") # Create a slice object slice_obj = slice(start, stop, step) # Return the sliced sequence return sequence[slice_obj]"},{"question":"# Task Implement a Python function that mimics the file selection mechanism of the `sdist` command using a manifest template. Your function will read a manifest template file (`MANIFEST.in`), process the commands, and generate a list of files to be included in a source distribution. # Function Signature ```python def generate_file_list(manifest_path: str, root_dir: str) -> List[str]: pass ``` # Input 1. `manifest_path` (str): The path to the `MANIFEST.in` file containing the manifest template. 2. `root_dir` (str): The root directory of the source code where the file search starts. # Output - Returns a List of strings, each representing the relative path of a file to be included in the source distribution. # Constraints 1. Only commands supported in the `MANIFEST.in` file are `include`, `recursive-include`, and `prune`. 2. The `MANIFEST.in` file may contain comments (lines starting with `#`) or blank lines, which should be ignored. 3. File and directory names in the `MANIFEST.in` file should be interpreted with slash-separated paths. # Example `MANIFEST.in` ``` # Include text files include *.txt # Include all Python files in examples directory and subdirectories recursive-include examples *.py # Exclude build directories prune examples/sample?/build ``` # Example Directory Structure ``` root_dir/ README.txt setup.py examples/ example1.py example2.txt sample1/ build/ build.log script.py sample2/ build/ build.log script.py ``` # Expected Output ```python >>> generate_file_list(\'path/to/MANIFEST.in\', \'root_dir\') [\'README.txt\', \'examples/example1.py\', \'examples/sample1/script.py\', \'examples/sample2/script.py\'] ``` # Notes - Use the `os` and `glob` modules to aid file and directory manipulation. - Ensure your function handles large directory structures efficiently.","solution":"import os import glob from typing import List def generate_file_list(manifest_path: str, root_dir: str) -> List[str]: Process the MANIFEST.in file and generate the list of files to include in the source distribution. include_patterns = [] prune_patterns = [] files_to_include = set() # Read the MANIFEST.in file with open(manifest_path, \'r\') as file: for line in file: line = line.strip() if line and not line.startswith(\'#\'): parts = line.split() command = parts[0] if command == \'include\': patterns = parts[1:] include_patterns.extend(patterns) elif command == \'recursive-include\': directory = parts[1] patterns = parts[2:] for pattern in patterns: include_patterns.append(os.path.join(directory, \'**\', pattern)) elif command == \'prune\': directory = parts[1] prune_patterns.append(os.path.join(directory, \'**\')) for pattern in include_patterns: for filepath in glob.glob(os.path.join(root_dir, pattern), recursive=True): if os.path.isfile(filepath): relative_path = os.path.relpath(filepath, root_dir) files_to_include.add(relative_path) for pattern in prune_patterns: for filepath in glob.glob(os.path.join(root_dir, pattern), recursive=True): if os.path.isfile(filepath): relative_path = os.path.relpath(filepath, root_dir) if relative_path in files_to_include: files_to_include.remove(relative_path) return sorted(files_to_include)"},{"question":"Replace BatchNorm with GroupNorm Objective Implement a function to replace all BatchNorm layers in a given PyTorch model with GroupNorm layers. Instructions 1. Write a function `replace_batchnorm_with_groupnorm(model: nn.Module, num_groups: int) -> nn.Module` that takes in a PyTorch model and replaces all BatchNorm layers with GroupNorm layers. 2. Your implementation should handle models of arbitrary depth and complexity, replacing every instance of BatchNorm1d, BatchNorm2d, and BatchNorm3d with GroupNorm. 3. Demonstrate your function using the following model architecture with BatchNorm layers: ```python import torch from torch import nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layers = nn.Sequential( nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU() ) def forward(self, x): return self.layers(x) ``` 4. After converting the BatchNorm layers to GroupNorm layers, print the model architecture to verify the replacement. Function Signature ```python def replace_batchnorm_with_groupnorm(model: nn.Module, num_groups: int) -> nn.Module: # Your code here ``` Example Usage ```python model = SimpleModel() print(\\"Original model:\\") print(model) # Replace BatchNorm layers model = replace_batchnorm_with_groupnorm(model, num_groups=4) print(\\"nModified model:\\") print(model) ``` Expected Output ``` Original model: SimpleModel( (layers): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() ) ) Modified model: SimpleModel( (layers): Sequential( (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): GroupNorm(4, 16) (2): ReLU() (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (4): GroupNorm(4, 32) (5): ReLU() ) ) ``` Constraints - Only `BatchNorm1d`, `BatchNorm2d`, and `BatchNorm3d` should be replaced. - GroupNorm should be instantiated with the provided number of groups. - Ensure `num_channels % num_groups == 0` when creating GroupNorm layers. Notes - Utilizing recursion or a model traversal technique can be beneficial for depth-independent modifications. - Consider handling both `nn.Sequential` and custom-defined layers within models.","solution":"import torch from torch import nn def replace_batchnorm_with_groupnorm(model: nn.Module, num_groups: int) -> nn.Module: Replace all instances of BatchNorm1d, BatchNorm2d, and BatchNorm3d in a model with GroupNorm layers having the specified number of groups. for name, module in model.named_children(): if isinstance(module, nn.Sequential): model._modules[name] = replace_batchnorm_with_groupnorm(module, num_groups) elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)): if isinstance(module, nn.BatchNorm1d): num_channels = module.num_features gn_layer = nn.GroupNorm(num_groups, num_channels) elif isinstance(module, nn.BatchNorm2d): num_channels = module.num_features gn_layer = nn.GroupNorm(num_groups, num_channels) elif isinstance(module, nn.BatchNorm3d): num_channels = module.num_features gn_layer = nn.GroupNorm(num_groups, num_channels) model._modules[name] = gn_layer else: replace_batchnorm_with_groupnorm(module, num_groups) # recursive call for submodules return model import torch from torch import nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.layers = nn.Sequential( nn.Conv2d(3, 16, kernel_size=3, padding=1), nn.BatchNorm2d(16), nn.ReLU(), nn.Conv2d(16, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU() ) def forward(self, x): return self.layers(x) # Example usage model = SimpleModel() print(\\"Original model:\\") print(model) # Replace BatchNorm layers model = replace_batchnorm_with_groupnorm(model, num_groups=4) print(\\"nModified model:\\") print(model)"},{"question":"**Objective:** Using the seaborn package, you will demonstrate your ability to create and customize swarm plots for data visualization, showcasing both fundamental and advanced concepts. **Dataset:** You will use the built-in \\"tips\\" dataset from seaborn for this exercise. **Requirements:** 1. **Load Dataset:** - Load the \\"tips\\" dataset into a pandas DataFrame. 2. **Swarm Plot 1:** - Create a swarm plot to visualize the distribution of total bills (`total_bill`) across different days (`day`). 3. **Swarm Plot 2:** - Create a swarm plot with the `day` variable on the y-axis and `total_bill` on the x-axis. Use the `hue` parameter to differentiate between smokers (`smoker`). Ensure the plot does not display the legend. 4. **Swarm Plot 3:** - Create a horizontal swarm plot to visualize the distribution of total bills (`total_bill`) against the size of the dining party (`size`). Adjust the plot so that the points are smaller and the plot uses the native scale for `size`. 5. **Swarm Plot 4:** - Create a multifaceted swarm plot using `catplot` to show the relationship between time of day (`time`), total bill (`total_bill`), and gender (`sex`) across different days (`day`). Set the aspect ratio of each subplot to 0.5. 6. **Customization:** - In Plot 4 (multifaceted plot), customize the plot by using \'o\' markers and setting the linewidth to 1. **Constraints:** - Use seaborn\'s capabilities to load the dataset. - Each plot should be displayed separately, and customization should be visible in the output. ```python import seaborn as sns import matplotlib.pyplot as plt # Task 1: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Task 2: Swarm plot to visualize total bills across different days plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill Distribution by Day\\") plt.show() # Task 3: Swarm plot with \'day\' on y-axis, \'total_bill\' on x-axis, colored by \'smoker\'; no legend plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, y=\\"day\\", x=\\"total_bill\\", hue=\\"smoker\\", legend=False) plt.title(\\"Total Bill by Day and Smoker Status\\") plt.show() # Task 4: Horizontal swarm plot for total bills by size of dining party, smaller points, native scale plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"size\\", orient=\\"h\\", size=3, native_scale=True) plt.title(\\"Total Bill Distribution by Party Size\\") plt.show() # Task 5 and 6: Multifaceted swarm plot for time of day, total bill, and gender across days, customized g = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5, marker=\'o\', linewidth=1 ) g.fig.suptitle(\\"Total Bill by Time, Gender, and Day\\") plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_tips_dataset(): Loads the tips dataset from seaborn. return sns.load_dataset(\\"tips\\") def plot_swarm_total_bill_by_day(tips): Creates a swarm plot to visualize the distribution of total bills across different days. plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Total Bill Distribution by Day\\") plt.show() def plot_swarm_total_bill_by_day_and_smoker(tips): Creates a swarm plot with \'day\' on y-axis, \'total_bill\' on x-axis, differentiated by smoker status; no legend. plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, y=\\"day\\", x=\\"total_bill\\", hue=\\"smoker\\", dodge=True) plt.legend([],[], frameon=False) plt.title(\\"Total Bill by Day and Smoker Status\\") plt.show() def plot_horizontal_swarm_total_bill_by_size(tips): Creates a horizontal swarm plot to visualize the distribution of total bills against the size of the dining party. Adjusted plot uses smaller points and native scale. plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"size\\", orient=\\"h\\", size=3, native_scale=True) plt.title(\\"Total Bill Distribution by Party Size\\") plt.show() def plot_multifaceted_swarm_total_bill_by_time_gender_day(tips): Creates a multifaceted swarm plot to show the relationship between time of day, total bill, and gender across different days. Sets the aspect ratio of each subplot to 0.5 and customizes the plot with \'o\' markers and linewidth of 1. g = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5, marker=\'o\', linewidth=1 ) g.fig.suptitle(\\"Total Bill by Time, Gender, and Day\\") plt.show()"},{"question":"Objective Your task is to create a Python script that utilizes PyTorch\'s `torch.__config__` package to demonstrate understanding and manipulation of PyTorch\'s configuration and parallel computation settings. Requirements 1. **Implement a function `config_and_parallel_info()`**: - Function should use `torch.__config__.show()` to print the current PyTorch configuration. - Function should use `torch.__config__.parallel_info()` to print the information regarding parallel computation capabilities and settings. 2. **Implement a function `analyze_parallel_info()`**: - This function should call `torch.__config__.parallel_info()` and return a structured dictionary containing key pieces of information about the parallel computation settings. 3. **Implement a custom configuration display function `display_custom_config(format=\'JSON\')`**: - This function should call `torch.__config__.show()` and then display this configuration in a specified format (\'JSON\' or \'TEXT\'). For JSON formatting, read the configuration into a dictionary and then print it in JSON format using Python\'s `json` module. For TEXT, simply print out the configuration as is. Example ```python def config_and_parallel_info(): import torch.__config__ torch.__config__.show() torch.__config__.parallel_info() def analyze_parallel_info(): import torch.__config__ parallel_info_output = torch.__config__.parallel_info() # Assuming parallel_info_output is a string, extract key information info_dict = {} # Properly parse the string to extract relevant data, store it in the dictionary return info_dict def display_custom_config(format=\'JSON\'): import json import torch.__config__ from io import StringIO import sys old_stdout = sys.stdout sys.stdout = mystdout = StringIO() torch.__config__.show() sys.stdout = old_stdout config_str = mystdout.getvalue() if format == \'JSON\': # Convert the configuration string to a dictionary config_dict = {} # Parse the configuration into a dictionary print(json.dumps(config_dict, indent=4)) elif format == \'TEXT\': print(config_str) else: raise ValueError(\\"Unsupported format specified\\") ``` Constraints - Do not make any assumptions about the content returned by `torch.__config__.show()` and `torch.__config__.parallel_info()`. Your code should be flexible enough to handle different outputs gracefully. - Ensure the `analyze_parallel_info()` function can handle and parse whatever relevant information is provided by `parallel_info`. Hints - Use Python\'s `json` module for handling JSON formatting. - Redirect and capture print output using `sys.stdout` for processing the raw configuration data.","solution":"import torch.__config__ import json from io import StringIO import sys def config_and_parallel_info(): Prints the current PyTorch configuration and parallel computation information. torch.__config__.show() torch.__config__.parallel_info() def analyze_parallel_info(): Extracts key pieces of information about the parallel computation settings and returns it as a dictionary. parallel_info_output = torch.__config__.parallel_info() # Parse the output and extract important information info_dict = {} for line in parallel_info_output.split(\'n\'): if \\":\\" in line: key, value = line.split(\\":\\", 1) info_dict[key.strip()] = value.strip() return info_dict def display_custom_config(format=\'JSON\'): Displays current PyTorch configuration in a specified format (\'JSON\' or \'TEXT\'). old_stdout = sys.stdout sys.stdout = mystdout = StringIO() torch.__config__.show() sys.stdout = old_stdout config_str = mystdout.getvalue() if format == \'JSON\': # Convert the configuration string to a dictionary config_dict = {} for line in config_str.split(\'n\'): if \\":\\" in line: key, value = line.split(\\":\\", 1) config_dict[key.strip()] = value.strip() print(json.dumps(config_dict, indent=4)) elif format == \'TEXT\': print(config_str) else: raise ValueError(\\"Unsupported format specified\\")"},{"question":"**Coding Question: Data Compression and Archiving** # Objective: Implement a Python function that can compress a given set of files using multiple compression algorithms and then archive all compressed files into a single ZIP file. # Function Signature: ```python def compress_and_archive(files: list[str], archive_name: str) -> None: pass ``` # Input: - `files` (list of str): A list of file paths (strings) to be compressed. - `archive_name` (str): The name of the final ZIP archive file (example: \\"archive.zip\\"). # Output: - The function should not return anything. It should create a ZIP file named `archive_name` containing all the compressed files. The compressed files should be in the same directory as the original files. # Constraints: - Use at least three different compression algorithms from the provided documentation: `gzip`, `bz2`, and `lzma`. - Ensure that compressed files are named appropriately (original name with an added extension indicating the compression algorithm, e.g., `example.txt.gz`, `example.txt.bz2`, `example.txt.xz`). - Handle any exceptions that may occur during the compression or archiving process. # Example: ```python files = [\'example1.txt\', \'example2.txt\', \'example3.txt\'] archive_name = \'compressed_files.zip\' compress_and_archive(files, archive_name) ``` After execution, a single ZIP file `compressed_files.zip` should be created, containing: - `example1.txt.gz` - `example2.txt.bz2` - `example3.txt.xz` # Guidance: 1. **File Compression:** - Use the `gzip` module to compress files with `.gz` extension. - Use the `bz2` module to compress files with `.bz2` extension. - Use the `lzma` module to compress files with `.xz` extension. 2. **Archiving:** - Use the `zipfile` module to create the ZIP archive and add the compressed files to it. # Important Notes: - Ensure that the original files are not altered during the process. - Manage the temporary compressed files properly - they should not remain in the same directory once the ZIP archive is created.","solution":"import gzip import bz2 import lzma import os import shutil from zipfile import ZipFile def compress_file(file_path: str, algorithm: str) -> str: Compress a file using the specified algorithm and return the path to the compressed file. extension = { \'gzip\': \'.gz\', \'bz2\': \'.bz2\', \'lzma\': \'.xz\' }[algorithm] compressed_file_path = file_path + extension with open(file_path, \'rb\') as f_in: if algorithm == \'gzip\': with gzip.open(compressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algorithm == \'bz2\': with bz2.open(compressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algorithm == \'lzma\': with lzma.open(compressed_file_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) return compressed_file_path def compress_and_archive(files: list[str], archive_name: str) -> None: compressed_files = [] try: for file_path in files: for algorithm in [\'gzip\', \'bz2\', \'lzma\']: compressed_file = compress_file(file_path, algorithm) compressed_files.append(compressed_file) with ZipFile(archive_name, \'w\') as zipf: for compressed_file in compressed_files: zipf.write(compressed_file, os.path.basename(compressed_file)) finally: for compressed_file in compressed_files: if os.path.exists(compressed_file): os.remove(compressed_file)"},{"question":"# Coding Assessment: Ensuring Deterministic Results with Memory Initialization in PyTorch Objective: Implement a function that performs matrix multiplication, ensuring deterministic results by properly handling uninitialized memory using the `torch.utils.deterministic.fill_uninitialized_memory` attribute. Function Signature: ```python import torch def deterministic_matrix_multiplication(A: torch.Tensor, B: torch.Tensor, fill_memory: bool = True) -> torch.Tensor: Perform matrix multiplication ensuring deterministic results and handling uninitialized memory. Args: - A (torch.Tensor): The first matrix for multiplication. - B (torch.Tensor): The second matrix for multiplication. - fill_memory (bool): A flag to determine whether to fill uninitialized memory. Default is True. Returns: - torch.Tensor: The result of matrix multiplication of A and B. # Your implementation here. ``` Input: - `A` and `B` are 2D tensors of appropriate dimensions for matrix multiplication. - `fill_memory` is an optional boolean flag (default `True`) that determines whether to fill uninitialized memory with known values. Output: - Returns the resultant 2D tensor after multiplying A and B. Constraints: - The function must ensure that all operations performed are deterministic. - If `fill_memory` is `True`, any uninitialized memory should be filled with proper values for deterministic results. - If `fill_memory` is `False`, do not fill uninitialized memory and aim for optimal performance. Example: ```python A = torch.randn((3, 3)) B = torch.randn((3, 3)) result = deterministic_matrix_multiplication(A, B, fill_memory=True) print(result) ``` In this assessment, you need to make sure your matrix multiplication handles the `fill_uninitialized_memory` attribute properly while ensuring deterministic results. The implementation of this would test students\' understanding of PyTorch\'s deterministic settings and their ability to manage performance trade-offs.","solution":"import torch def deterministic_matrix_multiplication(A: torch.Tensor, B: torch.Tensor, fill_memory: bool = True) -> torch.Tensor: Perform matrix multiplication ensuring deterministic results and handling uninitialized memory. Args: - A (torch.Tensor): The first matrix for multiplication. - B (torch.Tensor): The second matrix for multiplication. - fill_memory (bool): A flag to determine whether to fill uninitialized memory. Default is True. Returns: - torch.Tensor: The result of matrix multiplication of A and B. if fill_memory: # Ensure the filling of uninitialized memory is enabled for deterministic results torch.use_deterministic_algorithms(True) torch.manual_seed(0) # Set the random seed for reproducibility else: torch.use_deterministic_algorithms(False) result = torch.matmul(A, B) return result"},{"question":"# Sparse Data Manipulation in Pandas **Objective:** You are provided with a dataset that contains a large proportion of missing values. You need to transform this dataset into a sparse representation for better memory efficiency, perform some operations on it, and convert it back to a dense format. Instructions: 1. **Load the Dataset:** - You are provided with a NumPy array `data` of shape (10000, 5) where 99% of the values are `np.nan`. ```python import numpy as np # Creating the dataset data = np.random.randn(10000, 5) data[data < 0.95] = np.nan ``` 2. **Convert to Sparse DataFrame:** - Convert the given dataset into a pandas `DataFrame`. - Convert this dense DataFrame into a sparse DataFrame with `SparseDtype`. 3. **Sparse Operations:** - Calculate the density of the sparse DataFrame, which indicates the percentage of non-missing values. 4. **Element-wise Operation:** - Apply the `numpy.abs` (absolute value) function to the sparse DataFrame. - Ensure that the result remains a sparse DataFrame. 5. **Conversion Between Formats:** - Convert the modified sparse DataFrame back to a dense DataFrame. - Convert this dense DataFrame to a `scipy.sparse.coo_matrix`. Expected Functions: 1. `convert_to_sparse(data: np.ndarray) -> pd.DataFrame` - **Input:** A NumPy array `data` of shape (10000, 5). - **Output:** A pandas sparse DataFrame. 2. `calculate_density(sparse_df: pd.DataFrame) -> float` - **Input:** A pandas sparse DataFrame. - **Output:** Density of the sparse DataFrame. 3. `apply_abs(sparse_df: pd.DataFrame) -> pd.DataFrame` - **Input:** A pandas sparse DataFrame. - **Output:** A pandas sparse DataFrame after applying the `numpy.abs` function. 4. `convert_to_dense(sparse_df: pd.DataFrame) -> pd.DataFrame` - **Input:** A pandas sparse DataFrame. - **Output:** A dense pandas DataFrame. 5. `convert_to_coo(dense_df: pd.DataFrame) -> scipy.sparse.coo_matrix` - **Input:** A dense pandas DataFrame. - **Output:** A `scipy.sparse.coo_matrix`. Example Workflow: ```python import pandas as pd import numpy as np # Step 1: Convert to sparse DataFrame sparse_df = convert_to_sparse(data) # Step 2: Calculate density density = calculate_density(sparse_df) print(f\\"Density: {density}\\") # Step 3: Apply absolute value function abs_sparse_df = apply_abs(sparse_df) # Step 4: Convert to dense DataFrame dense_df = convert_to_dense(abs_sparse_df) # Step 5: Convert to COO matrix coo_matrix = convert_to_coo(dense_df) print(coo_matrix) ``` You are required to implement the functions as specified. Constraints: - Ensure that the large dataset is handled efficiently in terms of memory usage by leveraging sparse data structures. - The conversion functions should handle large data gracefully without running into memory issues. **Notes:** - Use pandas and scipy libraries for this task. - Ensure proper handling of `np.nan` and other missing values during conversion and calculation.","solution":"import pandas as pd import numpy as np from scipy.sparse import coo_matrix def convert_to_sparse(data: np.ndarray) -> pd.DataFrame: Converts a dense NumPy array into a sparse Pandas DataFrame. Parameters: data (np.ndarray): Dense NumPy array. Returns: pd.DataFrame: Sparse DataFrame. df = pd.DataFrame(data) sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) return sdf def calculate_density(sparse_df: pd.DataFrame) -> float: Calculates the density of a sparse DataFrame. Parameters: sparse_df (pd.DataFrame): Sparse DataFrame. Returns: float: Density of the sparse DataFrame. total_elements = sparse_df.size non_missing_elements = sparse_df.sparse.density * total_elements density = non_missing_elements / total_elements return density def apply_abs(sparse_df: pd.DataFrame) -> pd.DataFrame: Applies the numpy absolute value function to a sparse DataFrame. Parameters: sparse_df (pd.DataFrame): Sparse DataFrame. Returns: pd.DataFrame: Sparse DataFrame after applying abs. abs_sparse_df = np.abs(sparse_df) return abs_sparse_df def convert_to_dense(sparse_df: pd.DataFrame) -> pd.DataFrame: Converts a sparse DataFrame into a dense DataFrame. Parameters: sparse_df (pd.DataFrame): Sparse DataFrame. Returns: pd.DataFrame: Dense DataFrame. dense_df = sparse_df.sparse.to_dense() return dense_df def convert_to_coo(dense_df: pd.DataFrame) -> coo_matrix: Converts a dense DataFrame into a scipy sparse COO matrix. Parameters: dense_df (pd.DataFrame): Dense DataFrame. Returns: coo_matrix: Scipy sparse COO matrix. return coo_matrix(dense_df.values)"},{"question":"Implementing a Signal-based Timeout Mechanism Objective Design a Python function that performs a long-running computation but can be interrupted and stopped using a signal alarm if it takes too long. This will help you understand and demonstrate the use of Python\'s `signal` module to handle asynchronous events. Problem Statement Implement a function called `timed_computation` that performs a factorial computation of a given number but ensures that the computation does not exceed a specified time limit. If the computation exceeds the limit, it should raise a `TimeoutError`. Specifications 1. Implement a function `timed_computation(n: int, timeout: int) -> int`: - `n` (int): The parameter for which to compute the factorial. - `timeout` (int): The maximum number of seconds allowed for the computation to run. 2. If the computation completes within the timeout, return the factorial of `n`. 3. If the computation exceeds the timeout, raise a `TimeoutError`. Example ```python try: result = timed_computation(50000, 2) print(\\"Computation completed:\\", result) except TimeoutError: print(\\"Computation timed out!\\") ``` Constraints and Notes - Use the `signal` module to set the timeout. - The function should handle cleaning up any alarms or signal handlers after the computation is done or interrupted. - Assume `n` is always a positive integer. - Import necessary modules within the function instead of global imports. - Factorial of large numbers can be time-consuming, hence the need for effective signal management. Implementation Details - You will need to define a custom handler using `signal.signal()` to handle `SIGALRM` and raise a `TimeoutError`. - Use `signal.alarm()` to set the alarm timer. - Ensure that upon completion of the task or timeout, the alarm is disabled using `signal.alarm(0)`. Here is the function signature for reference: ```python import signal def timed_computation(n: int, timeout: int) -> int: # Your code here pass ``` This assessment will test your ability to work with Python signals, handle timeouts correctly, and manage long-running computations effectively.","solution":"import signal def timed_computation(n: int, timeout: int) -> int: Computes the factorial of n but ensures the computation does not exceed a specified timeout. If the computation takes too long, raises a TimeoutError. :param n: The number to compute the factorial of :param timeout: The maximum number of seconds allowed for the computation to run :return: The factorial of n if it completes within the timeout def handler(signum, frame): raise TimeoutError(\\"Computation timed out!\\") # Set the signal handler and a timeout alarm signal.signal(signal.SIGALRM, handler) signal.alarm(timeout) try: # Perform the factorial computation result = 1 for i in range(2, n + 1): result *= i return result finally: # Disable the alarm signal.alarm(0)"},{"question":"You are given the `penguins` dataset, which contains several measurements of penguins such as bill length, bill depth, species, etc. Your task is to create a Python function that visualizes the relationship between two specified measurements for this dataset using Seaborn\'s `JointGrid`. The function should create a combined plot consisting of a scatter plot for the joint distribution and histograms for the marginal distributions. Additionally, you should provide customization ability, such as specifying reference lines, plot size, layout, and limits for the axes. # Function Signature: ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_penguin_measurements(x: str, y: str, reference_lines: [(str, float)], height: float = 6, ratio: int = 5, space: float = 0.2, xlim: (float, float) = None, ylim: (float, float) = None): pass ``` # Input: - `x`: A string representing the column name for the x-axis measurement. - `y`: A string representing the column name for the y-axis measurement. - `reference_lines`: A list of tuples containing the type of reference line (\'x\' or \'y\') and the value for the reference line. - `height`: A float representing the height of the plot (default is 6). - `ratio`: An integer representing the ratio between the joint and marginal axes (default is 5). - `space`: A float representing the space between the plots (default is 0.2). - `xlim`: A tuple of floats representing the limits for the x-axis (default is None). - `ylim`: A tuple of floats representing the limits for the y-axis (default is None). # Output: - The function does not return anything but displays the combined plot. # Constraints: - The column names provided for `x` and `y` must exist in the `penguins` dataset. - The values for reference lines must be numeric. - Ensure the plots are correctly labeled and styled for clarity. # Example Usage: ```python visualize_penguin_measurements(\'bill_length_mm\', \'bill_depth_mm\', reference_lines=[(\'x\', 45), (\'y\', 16)], height=6, ratio=5, space=0.2, xlim=(30, 60), ylim=(10, 25)) ``` This would generate a combined plot where the joint distribution of bill length and bill depth is shown as a scatter plot, along with marginal histograms. It would also draw vertical and horizontal reference lines at the specified values and customize the plot according to the given parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn as sns from seaborn import * import pandas as pd # If required: from seaborn.datasets import load_dataset # Load the penguins dataset from seaborn\'s built-in function penguins = sns.load_dataset(\'penguins\') def visualize_penguin_measurements(x, y, reference_lines=[], height=6, ratio=5, space=0.2, xlim=None, ylim=None): Visualizes the relationship between two specified measurements for the penguins dataset using a combined plot of scatter plot and marginal histograms. Parameters: - x (str): Column name for the x-axis measurement. - y (str): Column name for the y-axis measurement. - reference_lines (list of tuples): A list of tuples with the type of reference line (\'x\' or \'y\') and the value. - height (float): The height of the plot. - ratio (int): The ratio between the joint and marginal axes. - space (float): The space between the plots. - xlim (tuple): The limits for the x-axis. - ylim (tuple): The limits for the y-axis. Returns: - None (displays the combined plot). # Create the JointGrid object g = sns.JointGrid(data=penguins, x=x, y=y, height=height, ratio=ratio, space=space, xlim=xlim, ylim=ylim) # Create scatter plot and marginal histograms g.plot(sns.scatterplot, sns.histplot) # Add reference lines for line in reference_lines: if line[0] == \'x\': # Vertical reference line g.ax_joint.axvline(line[1], color=\'r\', linestyle=\'--\') elif line[0] == \'y\': # Horizontal reference line g.ax_joint.axhline(line[1], color=\'r\', linestyle=\'--\') # Set x and y axis labels g.set_axis_labels(x, y) # Show the plot plt.show()"},{"question":"# **Asyncio Queues Assessment** **Objective:** You are required to implement an application using `asyncio.Queue` to manage tasks with different priorities. We will simulate a task processing system where tasks are processed based on their priority (lower priority number indicates higher urgency). **Tasks:** 1. Implement a `TaskProcessor` class that will manage tasks using `asyncio.PriorityQueue`. The class should have the following methods: - `__init__(self, maxsize=0)`: Initialize the priority queue with an optional `maxsize`. - `add_task(self, priority, task_detail)`: Add a task with a specific priority to the queue. This method should be non-blocking and should raise an exception if the queue is full. - `remove_task(self)`: Retrieve and remove the highest priority task from the queue. This method should be blocking. - `complete_task(self)`: Indicate that a retrieved task has been completed. - `process_tasks(self, num_workers)`: Create `num_workers` worker tasks to process items in the queue concurrently. Each worker should print a message indicating the task detail and the priority of the task it processed. This method should block until all tasks are processed. 2. Implement a function `main()` which: - Creates an instance of `TaskProcessor`. - Adds tasks with random priorities to the queue. - Processes the tasks with a specified number of worker tasks. **Input:** - Number of tasks to add. - Number of worker tasks. **Output:** - Printed statements indicating the processing of each task and its priority. - Total time taken to process all tasks. **Example:** ```python import asyncio import random class TaskProcessor: def __init__(self, maxsize=0): self.queue = asyncio.PriorityQueue(maxsize=maxsize) def add_task(self, priority, task_detail): try: self.queue.put_nowait((priority, task_detail)) except asyncio.QueueFull: print(f\\"Queue is full! Cannot add task: {task_detail}\\") async def remove_task(self): task = await self.queue.get() return task async def complete_task(self): self.queue.task_done() async def worker(self, name): while True: try: priority, task_detail = await self.remove_task() print(f\\"{name} is processing task: \'{task_detail}\' with priority {priority}\\") await asyncio.sleep(random.uniform(0.1, 0.3)) # Simulate task processing time await self.complete_task() except asyncio.CancelledError: break async def process_tasks(self, num_workers): tasks = [asyncio.create_task(self.worker(f\\"Worker-{i}\\")) for i in range(num_workers)] await self.queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) async def main(): num_tasks = 10 num_workers = 4 processor = TaskProcessor() for i in range(num_tasks): priority = random.randint(1, 10) task_detail = f\\"Task-{i}\\" processor.add_task(priority, task_detail) await processor.process_tasks(num_workers) asyncio.run(main()) ``` In this example: - We create a `TaskProcessor` instance. - Add tasks with random priorities to the queue. - Process the tasks using four workers. - Print statements indicate which worker is processing which task. **Constraints:** - You cannot change the signature of the provided methods. - Use `async/await` properly to manage async operations. **Performance Requirements:** - Ensure that all tasks are processed efficiently, utilizing all worker tasks concurrently.","solution":"import asyncio import random class TaskProcessor: def __init__(self, maxsize=0): self.queue = asyncio.PriorityQueue(maxsize=maxsize) def add_task(self, priority, task_detail): Add a task with a specific priority to the queue. try: self.queue.put_nowait((priority, task_detail)) except asyncio.QueueFull: print(f\\"Queue is full! Cannot add task: {task_detail}\\") async def remove_task(self): Retrieve and remove the highest priority task from the queue. task = await self.queue.get() return task async def complete_task(self): Indicate that a retrieved task has been completed. self.queue.task_done() async def worker(self, name): Worker that processes tasks from the queue. while True: try: priority, task_detail = await self.remove_task() print(f\\"{name} is processing task: \'{task_detail}\' with priority {priority}\\") await asyncio.sleep(random.uniform(0.1, 0.3)) # Simulate task processing time await self.complete_task() except asyncio.CancelledError: break async def process_tasks(self, num_workers): Process items in the queue concurrently with num_workers number of workers. tasks = [asyncio.create_task(self.worker(f\\"Worker-{i}\\")) for i in range(num_workers)] await self.queue.join() for task in tasks: task.cancel() await asyncio.gather(*tasks, return_exceptions=True) async def main(): num_tasks = 10 num_workers = 4 processor = TaskProcessor() for i in range(num_tasks): priority = random.randint(1, 10) task_detail = f\\"Task-{i}\\" processor.add_task(priority, task_detail) await processor.process_tasks(num_workers) # Run the main function if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"Signal Windowing in PyTorch Implement a function in PyTorch that applies a chosen window function from the `torch.signal.windows` module to an input signal and calculates the Root Mean Square Error (RMSE) between the original and windowed signals. Your function should be able to handle various types of window functions provided in the `torch.signal.windows` module. Function Signature ```python def apply_window_and_evaluate(signal: torch.Tensor, window_type: str, window_params: dict) -> float: Apply a window function from torch.signal.windows to an input signal and calculate the RMSE between the original and windowed signals. Args: - signal (torch.Tensor): 1-D Input signal of shape (n,). - window_type (str): Type of window function to apply (e.g., \'hann\', \'hamming\', \'blackman\'). - window_params (dict): Additional parameters required for the specific window function. Returns: - float: The Root Mean Square Error (RMSE) between the original and windowed signals. ``` Requirements 1. **Input Signal (signal)**: - A 1-dimensional PyTorch tensor representing the input signal. - Example: `signal = torch.tensor([0.1, 0.2, 0.3, ..., 1.0])` 2. **Window Type (window_type)**: - A string specifying which window function to use from the `torch.signal.windows` module. - Valid window types include: \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\'. 3. **Window Parameters (window_params)**: - A dictionary containing any additional parameters required for the chosen window function. For example, the \'gaussian\' window requires the parameter \'std\' (standard deviation). 4. **Output**: - The function should return a float representing the RMSE between the original signal and the windowed signal. Constraints - You should handle exceptions where the window type is not valid or where required window parameters are not provided. Performance Requirements - The function should be efficient and capable of handling signals of length up to 100,000. Example Usage ```python import torch from torch.signal import windows # Example signal signal = torch.linspace(0, 1, steps=1000) # Apply hann window and evaluate RMSE rmse = apply_window_and_evaluate(signal, \'hann\', {}) print(f\\"RMSE with hann window: {rmse}\\") # Apply gaussian window with std = 0.5 and evaluate RMSE rmse = apply_window_and_evaluate(signal, \'gaussian\', {\'std\': 0.5}) print(f\\"RMSE with gaussian window (std=0.5): {rmse}\\") ``` Detailed Explanation 1. **Signal Windowing**: Your function should generate the window using the specified window function and apply it to the input signal. 2. **RMSE Calculation**: The RMSE is calculated as: [ text{RMSE} = sqrt{frac{1}{n} sum_{i=1}^{n} (signal[i] - windowed_signal[i])^2} ] 3. **Exception Handling**: Your function should gracefully handle cases where the window type is invalid or the required parameters are missing. Good luck! Make sure to test your function with various window functions and parameters to ensure its correctness.","solution":"import torch import torch.nn.functional as F def apply_window_and_evaluate(signal: torch.Tensor, window_type: str, window_params: dict) -> float: Apply a window function from torch.signal.windows to an input signal and calculate the RMSE between the original and windowed signals. Args: - signal (torch.Tensor): 1-D Input signal of shape (n,). - window_type (str): Type of window function to apply (e.g., \'hann\', \'hamming\', \'blackman\'). - window_params (dict): Additional parameters required for the specific window function. Returns: - float: The Root Mean Square Error (RMSE) between the original and windowed signals. # Validate the input signal if len(signal.shape) != 1: raise ValueError(\\"Input signal must be a 1-D tensor.\\") # Get the window function if window_type == \'hann\': window = torch.hann_window(len(signal), **window_params) elif window_type == \'hamming\': window = torch.hamming_window(len(signal), **window_params) elif window_type == \'blackman\': window = torch.blackman_window(len(signal), **window_params) else: raise ValueError(f\\"Unsupported window type: {window_type}\\") # Apply the window function windowed_signal = signal * window # Calculate RMSE mse = F.mse_loss(signal, windowed_signal) rmse = torch.sqrt(mse).item() return rmse"},{"question":"Coding Assessment Question # Objective To assess your understanding of the `torch.cond` control flow operator in PyTorch, you are required to write a PyTorch module that changes its behavior based on the sum of tensor values. # Problem Statement Implement a PyTorch module called `SumBasedDynamicModule` that utilizes `torch.cond` to decide between two different functions based on the sum of the input tensor elements. Specifically: 1. If the sum of the elements of the input tensor is greater than 10, apply the function `greater_than_ten`: - This function will compute the element-wise product of the tensor with itself (i.e., `x * x`). 2. Otherwise, apply the function `less_than_or_equal_to_ten`: - This function will compute the element-wise natural logarithm of the tensor plus one (i.e., `torch.log(x + 1)`). 3. Ensure your module can handle tensors of any shape. # Input - A single input tensor `x`. # Output - A tensor that is the result of applying either `greater_than_ten` or `less_than_or_equal_to_ten` based on the sum of input tensor elements. # Constraints - The input tensor will only contain positive elements. # Example ```python import torch def greater_than_ten(x: torch.Tensor): return x * x def less_than_or_equal_to_ten(x: torch.Tensor): return torch.log(x + 1) class SumBasedDynamicModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.cond(x.sum() > 10, greater_than_ten, less_than_or_equal_to_ten, (x,)) # Example Usage: dynamic_module = SumBasedDynamicModule() # Input tensor with sum = 6 (less than 10) input_tensor1 = torch.tensor([2.0, 1.0, 3.0]) output1 = dynamic_module(input_tensor1) print(output1) # Expected to apply less_than_or_equal_to_ten # Input tensor with sum = 20 (greater than 10) input_tensor2 = torch.tensor([5.0, 5.0, 10.0]) output2 = dynamic_module(input_tensor2) print(output2) # Expected to apply greater_than_ten ``` # Evaluation Criteria - Correctness: The module should correctly apply the `greater_than_ten` or `less_than_or_equal_to_ten` function based on the sum of the input tensor. - Code Quality: Follow best practices for code style and readability. - Efficiency: Ensure that the solution is efficient and handles varying input tensor shapes effectively.","solution":"import torch def greater_than_ten(x: torch.Tensor): return x * x def less_than_or_equal_to_ten(x: torch.Tensor): return torch.log(x + 1) class SumBasedDynamicModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: return torch.where(x.sum() > 10, greater_than_ten(x), less_than_or_equal_to_ten(x))"},{"question":"# Encoding and Decoding Binary Data with `binascii` **Background:** You are given a file that contains binary data. Your task is to read the binary data, encode it using a specified encoding method from the `binascii` module, and then decode it back to the original binary format to verify correctness. The possible encoding methods are `uu`, `base64`, `hex`, and `quoted-printable`. **Task:** Implement a function `encode_decode_binary(file_path: str, encoding: str) -> bool` that takes two arguments: 1. `file_path` (str): The path to a binary file. 2. `encoding` (str): The encoding method to use. It can be either `uu`, `base64`, `hex`, or `quoted-printable`. Your function should perform the following steps: 1. Read the binary data from the file specified by `file_path`. 2. Encode the binary data using the specified `encoding` method. 3. Decode the encoded data back to the original binary format. 4. Verify if the decoded binary data matches the original binary data and return `True` if they match; otherwise, return `False`. **Constraints:** - The file size should not exceed 1 MB. - The encoding method will always be one of `uu`, `base64`, `hex`, or `quoted-printable`. **Example:** Assume `example.bin` is a binary file in your working directory. ```python result = encode_decode_binary(\'example.bin\', \'base64\') print(result) # Output: True (if encoding and decoding is correct) ``` **Notes:** - You may find the following functions from the `binascii` module useful: `b2a_uu`, `a2b_uu`, `b2a_base64`, `a2b_base64`, `hexlify`, `unhexlify`, `b2a_qp`, and `a2b_qp`. - Handle any required parameters for these functions as described in the `binascii` documentation provided.","solution":"import binascii def encode_decode_binary(file_path: str, encoding: str) -> bool: Reads binary data from a file, encodes it using the specified encoding method, decodes it back, and checks if the decoded data matches the original data. Parameters: file_path (str): The path to the binary file. encoding (str): The encoding method (\'uu\', \'base64\', \'hex\', \'quoted-printable\'). Returns: bool: True if the decoded data matches the original data, False otherwise. # Read the binary data from the file with open(file_path, \'rb\') as file: original_data = file.read() # Encode the data if encoding == \'uu\': encoded_data = binascii.b2a_uu(original_data) decoded_data = binascii.a2b_uu(encoded_data) elif encoding == \'base64\': encoded_data = binascii.b2a_base64(original_data) decoded_data = binascii.a2b_base64(encoded_data) elif encoding == \'hex\': encoded_data = binascii.hexlify(original_data) decoded_data = binascii.unhexlify(encoded_data) elif encoding == \'quoted-printable\': encoded_data = binascii.b2a_qp(original_data) decoded_data = binascii.a2b_qp(encoded_data) else: raise ValueError(\\"Unsupported encoding type\\") # Check if the decoded data matches the original data return original_data == decoded_data"},{"question":"# Seaborn Coding Assessment: Advanced Pairwise Plotting and Customization Objective: Demonstrate your understanding and proficiency in using the seaborn `so.Plot` class to create complex visualizations that incorporate multiple paired relationships and customizations. Task: Using the `mpg` dataset from seaborn, write a function `plot_pairwise_relationships` that creates a detailed pairwise plot. The plot should include: 1. Pairwise relationships between the following sets of variables: - `x`: \\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\" - `y`: \\"acceleration\\", \\"mpg\\" 2. The plot should: - Not cross-pair (i.e., pair each position in the `x` list with the corresponding position in the `y` list). - Include customized axis labels. - Utilize facets to differentiate between the `origin` of the cars. 3. Organize subplots in a 2x2 grid. Input: - None Output: - A multi-faceted pairwise plot displayed using seaborn. Constraints: - Assume the `mpg` dataset is readily available through seaborn. - The function should handle any potential errors gracefully and provide meaningful error messages. Example Function Definition: ```python def plot_pairwise_relationships(): # Import necessary libraries import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Implement the plotting code ( so.Plot(mpg) .pair( x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], y=[\\"acceleration\\", \\"mpg\\"], cross=False ) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", x2=\\"Horsepower\\", x3=\\"Cylinders\\", y0=\\"Acceleration\\", y1=\\"Miles per Gallon (MPG)\\") .facet(col=\\"origin\\") .wrap(2) .add(so.Dots()) ) ``` You should ensure that your function correctly implements these specifications and visually verify the output. Additional Tip: Refer to the seaborn documentation on `so.Plot` for more information about available methods and their usage.","solution":"def plot_pairwise_relationships(): # Import necessary libraries import seaborn.objects as so from seaborn import load_dataset # Load the dataset mpg = load_dataset(\\"mpg\\") # Implement the plotting code try: ( so.Plot(mpg) .pair( x=[\\"displacement\\", \\"weight\\", \\"horsepower\\", \\"cylinders\\"], y=[\\"acceleration\\", \\"mpg\\"], cross=False ) .label(x0=\\"Displacement (cu in)\\", x1=\\"Weight (lb)\\", x2=\\"Horsepower\\", x3=\\"Cylinders\\", y0=\\"Acceleration\\", y1=\\"Miles per Gallon (MPG)\\") .facet(col=\\"origin\\") .wrap(2) .add(so.Dots()) ).show() except Exception as e: print(f\\"An error occurred while plotting: {e}\\")"},{"question":"**Advanced Cross-Validation and Model Evaluation Task** Welcome to this coding task! Your understanding of scikit-learn\'s cross-validation techniques will be crucial for solving this problem. # Problem Statement A retail company is analyzing its sales data to predict future sales. To achieve better predictive performance and ensure robust evaluation of the model, they need to correctly implement cross-validation given the complexities of the data. Your task is to: 1. **Preprocess the dataset** to handle any missing values or data transformations required. 2. **Implement a stratified k-fold cross-validation** (with `k=5`) to evaluate a Support Vector Machine (SVM) classifier on the sales data. 3. **Implement a custom cross-validation function** that splits the data based on a provided group identifier (e.g., grouping by store location). # Dataset The dataset you will use (`sales_data.csv`) contains the following columns: - `store_id`: Identifier for the store (grouping identifier) - `date`: The date of the sales entry - `sales`: Sales amount for that day - `promotion`: Whether there was a promotion that day (binary: 0 or 1) - `holiday`: Whether the day was a holiday (binary: 0 or 1) - `y`: Target variable, indicating whether sales exceeded a certain threshold (binary: 0 or 1) # Input * The dataset should be loaded from a file named `sales_data.csv`. * You may assume the file is in the same directory as your script. # Output * Print the mean accuracy and the standard deviation of the accuracy across the 5 folds. * Print the accuracy scores for each fold using the custom cross-validation with group-based splitting. # Function Signature ```python def evaluate_sales_model(file_path: str): pass ``` # Implementation Details 1. **Data Preprocessing**: - Handle any missing values by replacing them with the median of the respective column. - Any necessary data transformation should be explicitly done in this step. 2. **Stratified K-Fold Cross-Validation**: - Use the `StratifiedKFold` class to create 5 splits. - Implement and evaluate an SVM classifier (`svm.SVC`) with linear kernel and `C=1`. 3. **Custom Cross-Validation**: - Implement a custom cross-validation function that performs cross-validation based on `store_id`. - Ensure that the same store is not represented in both training and testing sets for any given split. - Use the splits to evaluate the accuracy of the SVM classifier. # Example Usage ```python evaluate_sales_model(\\"sales_data.csv\\") ``` # Constraints - Do not use any additional libraries apart from `numpy`, `pandas`, and `scikit-learn`. # Performance Requirements - Ensure your solution runs efficiently given the dataset size (approximately 10,000 rows). Good luck!","solution":"import pandas as pd import numpy as np from sklearn.model_selection import StratifiedKFold, train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score def evaluate_sales_model(file_path: str): # Load the dataset df = pd.read_csv(file_path) # Data Preprocessing df[\'sales\'] = df[\'sales\'].fillna(df[\'sales\'].median()) df[\'promotion\'] = df[\'promotion\'].fillna(df[\'promotion\'].median()) df[\'holiday\'] = df[\'holiday\'].fillna(df[\'holiday\'].median()) # Extract features and target X = df[[\'sales\', \'promotion\', \'holiday\']].values y = df[\'y\'].values store_id = df[\'store_id\'].values # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) stratified_accuracies = [] for train_index, test_index in skf.split(X, y): X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] clf = SVC(kernel=\'linear\', C=1) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) stratified_accuracies.append(accuracy) print(f\\"Stratified K-Fold Mean Accuracy: {np.mean(stratified_accuracies):.4f}\\") print(f\\"Stratified K-Fold Standard Deviation: {np.std(stratified_accuracies):.4f}\\") print(\\"Stratified K-Fold Accuracies:\\", stratified_accuracies) # Custom Cross-Validation with Grouping by Store ID unique_stores = np.unique(store_id) np.random.shuffle(unique_stores) n_splits = 5 split_size = len(unique_stores) // n_splits custom_accuracies = [] for i in range(n_splits): test_stores = unique_stores[i * split_size:(i + 1) * split_size] train_stores = np.setdiff1d(unique_stores, test_stores) train_index = np.isin(store_id, train_stores) test_index = np.isin(store_id, test_stores) X_train, X_test = X[train_index], X[test_index] y_train, y_test = y[train_index], y[test_index] clf = SVC(kernel=\'linear\', C=1) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) custom_accuracies.append(accuracy) print(f\\"Custom Cross-Validation Mean Accuracy: {np.mean(custom_accuracies):.4f}\\") print(f\\"Custom Cross-Validation Standard Deviation: {np.std(custom_accuracies):.4f}\\") print(\\"Custom Cross-Validation Accuracies:\\", custom_accuracies)"},{"question":"**Objective**: Your task is to create a Python script that uses the `timeit` module to compare the performance of different implementations of computing the square of numbers in a list. # Problem Statement You are given a list of integers from `1` to `n`. Implement three different methods to compute the square of each number in the list and use the `timeit` module to measure and compare their performance. The methods are: 1. Using a list comprehension. 2. Using a `for` loop. 3. Using the `map` function with a lambda. # Function Signature ```python def compare_square_methods(n: int) -> str: pass ``` # Input - `n` (int): The largest number in the list of consecutive integers starting from 1 (1 ≤ n ≤ 10000). # Output - Return a string that contains the timing results of each method. The result should be formatted as follows: ``` List comprehension: X.XXXXXXX seconds For loop: Y.YYYYYYY seconds Map function: Z.ZZZZZZZ seconds ``` # Constraints - Use `timeit` to measure the execution time of each method with `1000` repetitions. - Include the setup code required for each method, ensuring that the code runs independently without requiring further imports or external variables outside the function. - Performance is critical, so the methods should be written to minimize overhead. # Example ```python print(compare_square_methods(1000)) ``` Expected output (example): ``` List comprehension: 0.0500000 seconds For loop: 0.0600000 seconds Map function: 0.0550000 seconds ``` **Note**: The exact timing values will vary depending on the system and environment where the code is executed. # Guidelines 1. Make sure to follow the function signature provided. 2. Include necessary setup code in the `timeit` calls. 3. Ensure the formatted output is correct to pass the automated test validations.","solution":"import timeit def compare_square_methods(n: int) -> str: setup_code = f n = {n} nums = list(range(1, n+1)) list_comprehension_code = squares = [x * x for x in nums] for_loop_code = squares = [] for x in nums: squares.append(x * x) map_function_code = squares = list(map(lambda x: x * x, nums)) # Measure the time taken by each method list_comprehension_time = timeit.timeit(list_comprehension_code, setup=setup_code, number=1000) for_loop_time = timeit.timeit(for_loop_code, setup=setup_code, number=1000) map_function_time = timeit.timeit(map_function_code, setup=setup_code, number=1000) # Format the results result = (f\\"List comprehension: {list_comprehension_time:.7f} secondsn\\" f\\"For loop: {for_loop_time:.7f} secondsn\\" f\\"Map function: {map_function_time:.7f} seconds\\") return result"},{"question":"# Advanced Seaborn Plotting **Objective**: Create a comprehensive layered plot using Seaborn\'s `objects` interface to assess your understanding of plotting, applying transformations, variable assignments, and annotations. **Problem Statement**: You are provided with a dataset `tips` from Seaborn\'s sample datasets, which contains the following fields: - `total_bill`: The total bill amount. - `tip`: The tip amount. - `day`: The day of the week. - `time`: The time of the day (Lunch or Dinner). - `sex`: Gender of the person paying. - `smoker`: Whether the person is a smoker. - `size`: The size of the party. **Task**: 1. Load the `tips` dataset. 2. Create a layered plot that includes the following layers: - A scatter plot (dots) showing `total_bill` vs. `tip`. - A regression line (using polynomial fit) for `total_bill` vs. `tip`. - A bar plot showing the total bill for each day, separated by gender and taking into account the size of the party. 3. The scatter plot should have different colors for different days. 4. The bar plot should be faceted by day. 5. Annotate the layers appropriately in the legend. **Function Signature**: `def create_advanced_seaborn_plot() -> None:` **Requirements**: - You must use the `seaborn.objects` interface. - The plot should be clear and well-labeled. - The creation of the plot should follow good coding practices, including commenting where necessary. - Ensure the layers are distinguished both by color and by shape/mark. **Expected Output**: - A visual plot displayed as a result of running the function `create_advanced_seaborn_plot()`. **Example**: The final plot should be similar to the examples provided in the documentation but incorporates multiple facets and layers. ```python import seaborn.objects as so from seaborn import load_dataset def create_advanced_seaborn_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot p = (so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"day\\") .add(so.Dot(), pointsize=\\"size\\") .add(so.Line(color=\\".3\\", linewidth=3), so.PolyFit()) .facet(col=\\"day\\") .add(so.Bar(), so.Hist(), so.Dodge(), orient=\\"v\\") .scale(pointsize=(2, 10)) .label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Tips vs. Total Bill by Day\\") ) # Draw the plot p.show() # Call the function to draw the plot create_advanced_seaborn_plot() ``` Ensure that your function `create_advanced_seaborn_plot` aligns with the above specifications and produces a similar output.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_advanced_seaborn_plot(): # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot object with facets by day and layer additions p = (so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"day\\") .add(so.Dot(), pointsize=\\"size\\") .add(so.Line(color=\\".3\\", linewidth=3), so.PolyFit()) .facet(col=\\"day\\") .add(so.Bar(), col=\\"day\\", row=\\"sex\\", height=\\"size\\", orient=\\"v\\") .scale(pointsize=(2, 10)) .label(x=\\"Total Bill\\", y=\\"Tip\\", title=\\"Tips vs. Total Bill by Day and Gender\\") ) # Draw the plot p.show() # Call the function to draw the plot create_advanced_seaborn_plot()"},{"question":"Objective As a Python developer, you are tasked with creating a function to interact with Python floating point values. You are required to implement a function that uses various functionalities of the `float.h` interface to perform the following operations: 1. **Check if the input is a float**. 2. **Convert a string to a float**, if it is valid. 3. **Convert a float to a double**. 4. **Retrieve the maximum and minimum representable float values**. Your task is to implement the following function: ```python def float_operations(data): This function takes a dictionary `data` with the following keys: - \'check_float\': object to check if it is a float - \'convert_string\': string to convert to float - \'float_value\': float to convert to double It should return a dictionary with the results: - \'is_float\': whether \'check_float\' is a float - \'float_from_string\': the float value from \'convert_string\', or raise ValueError if the string is not a valid float - \'double_value\': the double representation of \'float_value\' - \'max_float\': the maximum representable float value - \'min_float\': the minimum representable float value pass ``` Input and Output Format **Input**: `data` (dictionary) - `check_float`: Can be any Python object. - `convert_string`: A string potentially representing a float. - `float_value`: A float. Example input: ```python data = { \\"check_float\\": 3.14, \\"convert_string\\": \\"123.456\\", \\"float_value\\": 78.9 } ``` **Output**: A dictionary with the following keys: - `is_float`: Boolean, whether `check_float` is a float. - `float_from_string`: The float value converted from the string, or raises a ValueError if the string is not convertable. - `double_value`: The double representation of the given float. - `max_float`: Maximum representable float. - `min_float`: Minimum representable float. Expected output for the example input: ```python { \\"is_float\\": True, \\"float_from_string\\": 123.456, \\"double_value\\": 78.9, \\"max_float\\": 1.7976931348623157e+308, \\"min_float\\": 2.2250738585072014e-308 } ``` Constraints - You may assume all input strings are well-formed and valid. - Float values will be within the range of standard double precision. Requirements - You must implement the function specified. - You should handle possible exceptions for string to float conversion, while other operations can assume no error. - Use Python\'s built-in capabilities to handle these checks and conversions.","solution":"import sys def float_operations(data): This function takes a dictionary `data` with the following keys: - \'check_float\': object to check if it is a float - \'convert_string\': string to convert to float - \'float_value\': float to convert to double It should return a dictionary with the results: - \'is_float\': whether \'check_float\' is a float - \'float_from_string\': the float value from \'convert_string\', or raise ValueError if the string is not a valid float - \'double_value\': the double representation of \'float_value\' - \'max_float\': the maximum representable float value - \'min_float\': the minimum representable float value results = {} # Check if an object is a float results[\'is_float\'] = isinstance(data[\'check_float\'], float) # Convert string to float try: results[\'float_from_string\'] = float(data[\'convert_string\']) except ValueError: results[\'float_from_string\'] = None # Double representation in Python is the same as float results[\'double_value\'] = float(data[\'float_value\']) # Getting max and min float values in Python results[\'max_float\'] = sys.float_info.max results[\'min_float\'] = sys.float_info.min return results"},{"question":"# Question: Dynamic Class Creation and Type Resolution In this assignment, you will need to demonstrate your understanding of dynamic class creation and type resolution using the `types` module in Python 3.10. Task 1: Dynamic Class Creation Implement a function `create_dynamic_class` that creates a new class dynamically. The function should take the following parameters: - `name` (string): The name of the new class. - `bases` (tuple): A tuple of base classes. - `attributes` (dict): A dictionary of attributes (name-value pairs) to add to the class. Your function should return an instance of the newly created class with the provided attributes. Task 2: Type Resolver Implement a function `resolve_class_bases` that resolves the bases of a class dynamically. The function should take a tuple of base classes and return a tuple where each non-type base is replaced with the unpacked result of its `__mro_entries__` method (if it has one). # Constraints - You should not use any in-built or third-party libraries for dynamic class creation other than the `types` module. - Focus on handling edge cases, such as empty base classes or attributes. - Assume Python 3.10 environment. # Input and Output Format **create_dynamic_class(name: str, bases: tuple, attributes: dict) -> object** - Example: ```python class Base: def greet(self): return \\"Hello\\" new_class_instance = create_dynamic_class(\\"DynamicClass\\", (Base,), {\\"x\\": 10}) print(new_class_instance.greet()) # Output: \\"Hello\\" print(new_class_instance.x) # Output: 10 ``` **resolve_class_bases(bases: tuple) -> tuple** - Example: ```python class Meta: def __mro_entries__(self, bases): return (int,) resolved_bases = resolve_class_bases((Meta(),)) print(resolved_bases) # Output: (<class \'int\'>,) ``` # Implementation Implement both functions within a Python module. Ensure you write test cases to validate the functionality of these utility functions. ```python import types def create_dynamic_class(name: str, bases: tuple, attributes: dict) -> object: def exec_body(ns): ns.update(attributes) new_class = types.new_class(name, bases=bases, exec_body=exec_body) return new_class() def resolve_class_bases(bases: tuple) -> tuple: return types.resolve_bases(bases) ``` Using these functions, test the creation and resolution of dynamically generated classes.","solution":"import types def create_dynamic_class(name: str, bases: tuple, attributes: dict) -> object: Create a new class dynamically with the given name, bases, and attributes. def exec_body(ns): ns.update(attributes) new_class = types.new_class(name, bases=bases, exec_body=exec_body) return new_class() def resolve_class_bases(bases: tuple) -> tuple: Resolve the bases of a class dynamically, using __mro_entries__ if applicable. resolved_bases = [] for base in bases: if hasattr(base, \'__mro_entries__\'): resolved_bases.extend(base.__mro_entries__(bases)) else: resolved_bases.append(base) return tuple(resolved_bases)"},{"question":"# Question: Creating and Visualizing Customized Diverging Color Palettes You are given a dataset and asked to generate and visualize different configurations of diverging color palettes using seaborn. You are to create a heatmap for better analytical interpretation of the given dataset. **Objective**: Write a Python function `plot_custom_heatmaps` that reads a dataset, generates multiple configurations of diverging color palettes, and visualizes these palettes on a heatmap. **Function Signature**: ```python def plot_custom_heatmaps(dataset_path: str) -> None: pass ``` # Input: - `dataset_path`: A string representing the path to a CSV file containing the dataset to be visualized. Assume the dataset is numerical. # Output: - The function will not return anything but should display four heatmaps on the same figure, each using a different configuration of diverging palettes. # Constraints: - Use seaborn\'s `sns.heatmap()` function for visualization. - The heatmap should use diverging palettes created using seaborn\'s `sns.diverging_palette()` function. - The diverging palettes should exhibit the following characteristics: 1. Blue to Red with center as white. 2. Blue to Red with center as dark. 3. Magenta to Green with saturation of endpoints reduced to 50. 4. Magenta to Green with lightness of endpoints reduced to 35. # Example: ```python dataset_path = \\"path/to/your/numerical_dataset.csv\\" plot_custom_heatmaps(dataset_path) ``` * This will visualize the heatmap of the dataset using the four specified diverging color palettes with the specified characteristics. # Additional Notes: - Ensure that seaborn and matplotlib libraries are properly imported. - Use appropriate subplotting techniques to display the four heatmaps on a single figure. - Handle all import statements within the function as well as data loading procedures. **Tips**: - Refer to the seaborn documentation for any additional information necessary to understand the functionalities of diverging_palette and heatmap.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_custom_heatmaps(dataset_path: str) -> None: Reads a dataset, generates multiple configurations of diverging color palettes, and visualizes these palettes on a heatmap. # Load the dataset data = pd.read_csv(dataset_path) # Create figure and axes fig, axes = plt.subplots(2, 2, figsize=(15, 12)) # First diverging palette: Blue to Red with center as white palette1 = sns.diverging_palette(240, 10, as_cmap=True) sns.heatmap(data, cmap=palette1, ax=axes[0, 0], cbar=True) axes[0, 0].set_title(\'Palette 1: Blue to Red with center as white\') # Second diverging palette: Blue to Red with center as dark palette2 = sns.diverging_palette(240, 10, center=\\"dark\\", as_cmap=True) sns.heatmap(data, cmap=palette2, ax=axes[0, 1], cbar=True) axes[0, 1].set_title(\'Palette 2: Blue to Red with center as dark\') # Third diverging palette: Magenta to Green with saturation of endpoints reduced to 50 palette3 = sns.diverging_palette(276, 150, s=50, as_cmap=True) sns.heatmap(data, cmap=palette3, ax=axes[1, 0], cbar=True) axes[1, 0].set_title(\'Palette 3: Magenta to Green with s=50\') # Fourth diverging palette: Magenta to Green with lightness of endpoints reduced to 35 palette4 = sns.diverging_palette(276, 150, l=35, as_cmap=True) sns.heatmap(data, cmap=palette4, ax=axes[1, 1], cbar=True) axes[1, 1].set_title(\'Palette 4: Magenta to Green with l=35\') # Adjust layout plt.tight_layout() plt.show()"},{"question":"**Problem Statement: Custom File Reader with Transformations** In this exercise, you will generate a custom implementation of a file reader that can perform different transformations on the content of a file before returning it. You will use the `builtins` module to access the built-in `open` function and create a flexible class that can perform different types of text transformations. # Task 1. Create a custom function `custom_open(path, mode=\'r\', transform=None)` that: - Uses the built-in `open()` function by importing the `builtins` module to open the file. - Takes an additional `transform` parameter that specifies the type of transformation to apply to the file content. If `transform` is `None`, the file content is returned as-is. 2. Implement a class `Transformer` that: - Has an `__init__` method that accepts a file object. - Defines a method `read(self, count=-1)` that reads content from the file and applies the specified transformation. - Supports at least three types of text transformations: - `\'uppercase\'` - Converts all text to uppercase. - `\'lowercase\'` - Converts all text to lowercase. - `\'reverse\'` - Reverses the entire text. # Implementation Details - **Input:** - `path` (str): The path to the file to be read. - `mode` (str): The mode in which the file should be opened (default is `\'r\'`). - `transform` (str): The type of transformation to apply (default is `None`). - **Output:** - A string containing the possibly transformed content of the file. - **Constraints:** - The function should handle any potential file I/O errors gracefully. - The implementation should not use any external libraries for transformations. - Valid values for `transform` are `\'uppercase\'`, `\'lowercase\'`, and `\'reverse\'`. If an invalid value is provided, raise a `ValueError`. # Example Usage ```python # Assume \'example.txt\' contains the text \\"Hello, World!\\" # No transformation print(custom_open(\'example.txt\')) # Output: \\"Hello, World!\\" # Convert text to uppercase print(custom_open(\'example.txt\', transform=\'uppercase\')) # Output: \\"HELLO, WORLD!\\" # Convert text to lowercase print(custom_open(\'example.txt\', transform=\'lowercase\')) # Output: \\"hello, world!\\" # Reverse the text print(custom_open(\'example.txt\', transform=\'reverse\')) # Output: \\"!dlroW ,olleH\\" ``` # Notes - You should import the `builtins` module to use the built-in `open()` function. - Implement error handling to manage file not found or access errors. - Your code should be efficient and avoid reading the entire file into memory unnecessarily for large files.","solution":"import builtins class Transformer: def __init__(self, file, transform=None): self.file = file self.transform = transform def read(self, count=-1): content = self.file.read(count) if self.transform == \'uppercase\': return content.upper() elif self.transform == \'lowercase\': return content.lower() elif self.transform == \'reverse\': return content[::-1] elif self.transform is None: return content else: raise ValueError(f\\"Invalid transform value: {self.transform}\\") def custom_open(path, mode=\'r\', transform=None): try: with builtins.open(path, mode) as file: transformer = Transformer(file, transform) return transformer.read() except FileNotFoundError: raise FileNotFoundError(f\\"The file at path \'{path}\' was not found.\\") except IOError as e: raise IOError(f\\"An IOError occurred: {e}\\")"},{"question":"# Exercise: Working with `pickletools` for Pickle Data Analysis **Objective:** This exercise aims to evaluate your understanding and ability to work with the `pickletools` module to analyze and manipulate pickle data in Python. **Problem Statement:** You are provided with two pickle files containing serialized Python objects: 1. `data1.pickle` 2. `data2.pickle` Your task is to write a Python function `compare_pickle_files(file1: str, file2: str) -> dict` that performs the following: 1. Disassembles both pickle files using `pickletools.dis` and gathers the opcodes used in each file. 2. Compares the sets of opcodes in both files and identifies any opcodes that are present in one file but not the other. 3. Returns a dictionary with two keys: - `\'file1_unique_opcodes\'` containing the set of opcodes unique to the first file. - `\'file2_unique_opcodes\'` containing the set of opcodes unique to the second file. You should make use of the `pickletools.genops` function to iterate over the pickled opcodes for each file efficiently. **Input:** - `file1` (str): The path to the first pickle file. - `file2` (str): The path to the second pickle file. **Output:** - A dictionary with the specified keys and values. Each value is a set of unique opcodes. **Constraints:** - You may assume that both pickle files are valid and can be successfully disassembled using `pickletools`. **Example Usage:** ```python result = compare_pickle_files(\'data1.pickle\', \'data2.pickle\') print(result) ``` **Expected Result Format:** The output dictionary might look something like this (the actual opcodes will vary based on the data in the pickle files): ```python { \'file1_unique_opcodes\': {\'PROTOCOL\', \'BININT1\', \'TUPLE2\'}, \'file2_unique_opcodes\': {\'LONG_BINPUT\', \'EMPTY_DICT\'} } ``` **Note:** It is not necessary to annotate the opcodes with descriptions in this task, only to identify and compare the opcodes used. **Starter Code:** ```python import pickletools def compare_pickle_files(file1: str, file2: str) -> dict: def get_opcodes(filename): with open(filename, \'rb\') as f: pickle_data = f.read() opcodes = {opcode.name for opcode, arg, pos in pickletools.genops(pickle_data)} return opcodes opcodes_file1 = get_opcodes(file1) opcodes_file2 = get_opcodes(file2) unique_file1 = opcodes_file1 - opcodes_file2 unique_file2 = opcodes_file2 - opcodes_file1 return { \'file1_unique_opcodes\': unique_file1, \'file2_unique_opcodes\': unique_file2 } # Example usage (assuming \'data1.pickle\' and \'data2.pickle\' exist): # result = compare_pickle_files(\'data1.pickle\', \'data2.pickle\') # print(result) ``` **Explanation:** - The `compare_pickle_files` function takes two file paths as input. - It reads and disassembles each pickle file to gather the opcodes used. - It then compares the sets of opcodes and identifies unique opcodes for each file. - Finally, it returns a dictionary containing the unique opcode sets for each file.","solution":"import pickletools def compare_pickle_files(file1: str, file2: str) -> dict: def get_opcodes(filename): with open(filename, \'rb\') as f: pickle_data = f.read() opcodes = {opcode.name for opcode, arg, pos in pickletools.genops(pickle_data)} return opcodes opcodes_file1 = get_opcodes(file1) opcodes_file2 = get_opcodes(file2) unique_file1 = opcodes_file1 - opcodes_file2 unique_file2 = opcodes_file2 - opcodes_file1 return { \'file1_unique_opcodes\': unique_file1, \'file2_unique_opcodes\': unique_file2 } # Example usage (assuming \'data1.pickle\' and \'data2.pickle\' exist): # result = compare_pickle_files(\'data1.pickle\', \'data2.pickle\') # print(result)"},{"question":"You are tasked with analyzing the `diamonds` dataset provided by seaborn using different visualization techniques. Specifically, your goal is to explore the relationships between different quality metrics (`\'cut\'`, `\'clarity\'`, and `\'color\'`) and the carat size of the diamonds. **Requirements:** 1. **Load the dataset**: Load the `diamonds` dataset from seaborn. 2. **Mean Carat by Clarity**: Create a bar plot that shows the mean carat size for each `clarity` category. 3. **Median Carat by Cut**: Create a bar plot that displays the median carat size for each `cut` category. 4. **Custom Aggregation (Interquartile Range of Carat by Color)**: Create a bar plot that shows the interquartile range (IQR) of the carat size for each `color` category. 5. **Combined Visual (Mean Carat by Clarity, Dodged by Cut)**: Create a single bar plot that shows the mean carat size for each `clarity` category, with bars dodged/colored by `cut`. # Input and Output Formats: - **Input**: The question does not require any external input from the user. - **Output**: The output should be four plots as described above. - **Constraints and Limitations**: - Use seaborn\'s high-level `objects` interface for creating all plots. - Ensure proper labeling for clarity. # Example: Here’s a hypothetical visual representation guide for Mean Carat by Clarity: ```python import seaborn.objects as so from seaborn import load_dataset # Step 1: Load the dataset diamonds = load_dataset(\\"diamonds\\") # Step 2: Mean Carat by Clarity p1 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add(so.Bar(), so.Agg()) p1.show() # Step 3: Median Carat by Cut p2 = so.Plot(diamonds, \\"cut\\", \\"carat\\").add(so.Bar(), so.Agg(\\"median\\")) p2.show() # Step 4: Custom Aggregation (IQR of Carat by Color) p3 = so.Plot(diamonds, \\"color\\", \\"carat\\").add( so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25))) p3.show() # Step 5: Combined Visual (Mean Carat by Clarity, Dodged by Cut) p4 = so.Plot(diamonds, \\"clarity\\", \\"carat\\").add( so.Bar(), so.Agg(), so.Dodge(), color=\\"cut\\") p4.show() ``` Implement these steps and produce the corresponding plots.","solution":"import seaborn as sns import seaborn.objects as so # Load the dataset diamonds = sns.load_dataset(\\"diamonds\\") # Plot 1: Mean Carat by Clarity p1 = so.Plot(diamonds, x=\'clarity\', y=\'carat\').add(so.Bar(), so.Agg()) p1.show() # Plot 2: Median Carat by Cut p2 = so.Plot(diamonds, x=\'cut\', y=\'carat\').add(so.Bar(), so.Agg(func=\'median\')) p2.show() # Plot 3: Custom Aggregation (Interquartile Range of Carat by Color) p3 = so.Plot(diamonds, x=\'color\', y=\'carat\').add(so.Bar(), so.Agg(lambda x: x.quantile(0.75) - x.quantile(0.25))) p3.show() # Plot 4: Combined Visual (Mean Carat by Clarity, Dodged by Cut) p4 = so.Plot(diamonds, x=\'clarity\', y=\'carat\').add(so.Bar(), so.Agg(), so.Dodge(), color=\'cut\') p4.show()"},{"question":"You are required to write a Python function named `analyze_robots_txt(url, user_agents_and_urls)` that evaluates the contents of a `robots.txt` file at a given URL for multiple user agents and URLs. The function should return a summary including whether each URL is fetchable by each user agent, the crawl delay for each user agent, and the request rate for each user agent. **Function Signature:** ```python def analyze_robots_txt(url: str, user_agents_and_urls: dict) -> dict: pass ``` **Input:** - `url` (str): The URL of the `robots.txt` file on the site you want to analyze. - `user_agents_and_urls` (dict): A dictionary where keys are user agent strings and values are lists of URLs to check for fetchability. Example: ```python { \'Googlebot\': [\'http://example.com/page1\', \'http://example.com/page2\'], \'Bingbot\': [\'http://example.com/page1\', \'http://example.com/login\'] } ``` **Output:** - Returns a dictionary summarizing the analyzation of the `robots.txt` file. The structure of the returned dictionary should be as follows: ```python { \'user_agents\': { \'Googlebot\': { \'fetchable\': { \'http://example.com/page1\': True, \'http://example.com/page2\': False }, \'crawl_delay\': 10, \'request_rate\': {\'requests\': 5, \'seconds\': 60} }, \'Bingbot\': { \'fetchable\': { \'http://example.com/page1\': True, \'http://example.com/login\': False }, \'crawl_delay\': None, \'request_rate\': None } } } ``` **Task:** 1. Use the `urllib.robotparser.RobotFileParser` class. 2. Set and read the `robots.txt` file from the given URL. 3. For each user agent and URL, determine if the URL can be fetched. 4. Check and include the crawl delay and request rate for each user agent in the output. **Constraints:** 1. Handle invalid or missing `robots.txt` files gracefully by assuming fetchability and no delay or request rate. 2. Assume that the URLs in `user_agents_and_urls` can belong to the same domain as the `robots.txt` file or to different subdomains under the same primary domain. **Example Usage:** ```python url = \'http://example.com/robots.txt\' user_agents_and_urls = { \'Googlebot\': [\'http://example.com/page1\', \'http://example.com/page2\'], \'Bingbot\': [\'http://example.com/page1\', \'http://example.com/login\'] } result = analyze_robots_txt(url, user_agents_and_urls) print(result) # Expected output format: # { # \'user_agents\': { # \'Googlebot\': { # \'fetchable\': { # \'http://example.com/page1\': True, # \'http://example.com/page2\': False # }, # \'crawl_delay\': 10, # \'request_rate\': {\'requests\': 5, \'seconds\': 60} # }, # \'Bingbot\': { # \'fetchable\': { # \'http://example.com/page1\': True, # \'http://example.com/login\': False # }, # \'crawl_delay\': None, # \'request_rate\': None # } # } # } ``` **Assessment Criteria:** - Correct usage of the `RobotFileParser` class to parse and analyze the `robots.txt` file. - Accurate determination of fetch capabilities, crawl delays, and request rates for each user agent. - Proper dictionary construction and output formatting. - Handling edge cases, such as invalid or non-existent `robots.txt` files.","solution":"import urllib.robotparser import urllib.request def analyze_robots_txt(url: str, user_agents_and_urls: dict) -> dict: rp = urllib.robotparser.RobotFileParser() try: rp.set_url(url) rp.read() except: # Handle the case where robots.txt cannot be read return { \'user_agents\': { user_agent: { \'fetchable\': {u: True for u in urls}, \'crawl_delay\': None, \'request_rate\': None } for user_agent, urls in user_agents_and_urls.items() } } result = {\'user_agents\': {}} for user_agent, urls in user_agents_and_urls.items(): fetchable = {u: rp.can_fetch(user_agent, u) for u in urls} crawl_delay = rp.crawl_delay(user_agent) request_rate = rp.request_rate(user_agent) if request_rate: request_rate = {\'requests\': request_rate.requests, \'seconds\': request_rate.seconds} result[\'user_agents\'][user_agent] = { \'fetchable\': fetchable, \'crawl_delay\': crawl_delay, \'request_rate\': request_rate } return result"},{"question":"**Objective**: Demonstrate your understanding of PyTorch nested tensors by constructing, manipulating, and converting nested tensors. You\'ll implement a series of functions to perform and verify these operations. # Problem Statement You are required to implement three functions: 1. **create_nested_tensor**: Constructs a nested tensor from a list of variable-length 1D tensors. 2. **convert_to_padded**: Converts a nested tensor to a padded tensor with a specified padding value. 3. **sum_nested_tensors**: Takes two compatible nested tensors and returns a nested tensor corresponding to their element-wise sum. # Detailed Specifications 1. **Function**: `create_nested_tensor(tensor_list)` - **Input**: - `tensor_list` (list): A list of `torch.Tensor`, where each tensor is 1D and can have different lengths. - **Output**: - Returns a `torch.jagged` nested tensor constructed from the input list. - **Constraints**: - Ensure that each tensor in the list is 1D. 2. **Function**: `convert_to_padded(nested_tensor, padding_value)` - **Input**: - `nested_tensor` (`torch.jagged` nested tensor): A nested tensor with a single ragged dimension. - `padding_value` (float): The value to use for padding. - **Output**: - Returns a PyTorch `Tensor` padded with the specified value so that all sequences match the length of the longest sequence in the original nested tensor. - **Constraints**: - Handle the conversion efficiently without losing the structure. 3. **Function**: `sum_nested_tensors(nested_tensor1, nested_tensor2)` - **Input**: - `nested_tensor1` (`torch.jagged` nested tensor): The first nested tensor. - `nested_tensor2` (`torch.jagged` nested tensor): The second nested tensor. - **Output**: - Returns a `torch.jagged` nested tensor representing the element-wise sum of the two nested tensors. - **Constraints**: - Assume the input nested tensors have compatible shapes and dimensions. # Example Usage ```python import torch # 1. Create a nested tensor a = torch.tensor([1, 2, 3]) b = torch.tensor([4, 5]) c = torch.tensor([6]) nested_tensor = create_nested_tensor([a, b, c]) print(nested_tensor) # Expected nested tensor with jagged layout # 2. Convert nested tensor to padded tensor padded_tensor = convert_to_padded(nested_tensor, padding_value=-1) print(padded_tensor) # Expected output: # tensor([[1, 2, 3], # [4, 5, -1], # [6, -1, -1]]) # 3. Sum of nested tensors nested_tensor1 = create_nested_tensor([a, b, c]) nested_tensor2 = create_nested_tensor([a, b, c]) sum_nested = sum_nested_tensors(nested_tensor1, nested_tensor2) print(sum_nested) # Expected output similar structure as nested_tensor with summed elements. ``` **Note**: Feel free to use auxiliary functions if needed. # Additional Constraints 1. Ensure that the nested tensors used in `sum_nested_tensors` share the same offset structure; otherwise, handle exceptions where shapes are incompatible. 2. Aim for optimized implementations making use of PyTorch\'s efficient operations where possible.","solution":"import torch import torch.nn.functional as F def create_nested_tensor(tensor_list): Creates a nested tensor from a list of 1D tensors. assert all(tensor.dim() == 1 for tensor in tensor_list), \\"All tensors must be 1D\\" return tensor_list def convert_to_padded(nested_tensor, padding_value): Converts a nested tensor to a padded tensor with a specified padding value. max_length = max(len(tensor) for tensor in nested_tensor) padded_tensors = [F.pad(tensor, (0, max_length - len(tensor)), value=padding_value) for tensor in nested_tensor] return torch.stack(padded_tensors) def sum_nested_tensors(nested_tensor1, nested_tensor2): Returns a nested tensor corresponding to the element-wise sum of the input nested tensors. assert len(nested_tensor1) == len(nested_tensor2), \\"Nested tensors must have the same number of elements\\" sum_tensors = [tensor1 + tensor2 for tensor1, tensor2 in zip(nested_tensor1, nested_tensor2)] return sum_tensors"},{"question":"**Python Coding Challenge: Implementing a Module Loader using `importlib`** # Context The `imp` module, which was used in the past for interfacing with the import system in Python, has been deprecated since Python 3.4. Modern Python code should use the `importlib` module instead, which offers a more powerful and flexible way to manage imports. # Task Your task is to create a custom function to load a specified module given its name and path using the `importlib` module. # Function Signature ```python def custom_import(module_name: str, module_path: str) -> object: pass ``` # Inputs 1. `module_name` (string): The name of the module you want to import. 2. `module_path` (string): The path to the directory containing the module. # Output - Returns the imported module as an object. # Constraints - You can assume that the provided module path is valid and contains the module file (`.py`). - Handle exceptions gracefully, such as when the module file does not exist or there is an import error. - You should not use the deprecated `imp` module functions. # Example ```python # Example directory structure: # /path/to/modules # ├── module1.py # └── module2.py # Content of /path/to/modules/module1.py # def hello(): # return \\"Hello from module1\\" # Example usage: module_path = \\"/path/to/modules\\" module_name = \\"module1\\" mod = custom_import(module_name, module_path) print(mod.hello()) # Output: \\"Hello from module1\\" ``` # Requirements - Implement the `custom_import` function using the `importlib` module. - Ensure that the function dynamically loads the module from the specified path. # Hints - Explore the `importlib.util` functions, such as `spec_from_file_location` and `module_from_spec`, for implementing the solution. - Ensure that the dynamically imported module is added to `sys.modules`. Happy coding!","solution":"import importlib.util import sys import os def custom_import(module_name: str, module_path: str) -> object: Dynamically imports a module given its name and path. :param module_name: Name of the module to be imported. :param module_path: Path to the directory containing the module file. :return: The imported module object. try: module_file_path = os.path.join(module_path, module_name + \'.py\') spec = importlib.util.spec_from_file_location(module_name, module_file_path) if spec and spec.loader: module = importlib.util.module_from_spec(spec) sys.modules[module_name] = module spec.loader.exec_module(module) return module else: raise ImportError(f\\"Cannot find the module {module_name} at {module_path}\\") except Exception as e: print(f\\"Error importing module {module_name}: {e}\\") return None"},{"question":"**Question: Implement a WAV File Converter** You are tasked with designing a function that reads a WAV file and writes a new WAV file with modified audio parameters. This exercise evaluates your ability to interact with file handling, data manipulation, and the use of the `wave` module. # Function Specification ```python def convert_wav(input_file, output_file, new_channels, new_framerate, new_sample_width): Converts a WAV file to new specifications and saves it as a new file. Parameters: input_file (str): The path to the input WAV file. output_file (str): The path where the output WAV file will be saved. new_channels (int): The number of channels for the output WAV file (e.g., 1 for mono, 2 for stereo). new_framerate (int): The frame rate (sampling frequency) for the output WAV file. new_sample_width (int): The sample width in bytes for the output WAV file. Raises: wave.Error: If there is an issue with the input WAV file or the conversion process. pass ``` # Input - `input_file`: A string representing the path to the input WAV file. - `output_file`: A string representing the path where the output WAV file will be saved. - `new_channels`: An integer representing the number of channels in the output WAV file. - `new_framerate`: An integer representing the frame rate (sampling frequency) of the output WAV file. - `new_sample_width`: An integer representing the sample width in bytes for the output WAV file. # Output - The function should write a new WAV file to the specified `output_file` path with the modified parameters. # Constraints - The input file will always be a valid WAV file (PCM format). - The output file parameters (`new_channels`, `new_framerate`, `new_sample_width`) will have reasonable values. # Example Usage ```python convert_wav(\'input.wav\', \'output.wav\', 1, 22050, 2) ``` This should: - Read `input.wav`. - Convert it to mono (`new_channels` = 1), with a frame rate of 22050 Hz (`new_framerate` = 22050) and a sample width of 2 bytes (`new_sample_width` = 2). - Write the modified audio to `output.wav`. # Notes - Ensure proper error handling by raising `wave.Error` if the conversion process encounters any issues. - Make use of the methods provided by `Wave_read` and `Wave_write` objects efficiently. - Remember to close any files you open to avoid resource leaks.","solution":"import wave def convert_wav(input_file, output_file, new_channels, new_framerate, new_sample_width): try: with wave.open(input_file, \'rb\') as infile: params = infile.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params frames = infile.readframes(n_frames) with wave.open(output_file, \'wb\') as outfile: outfile.setnchannels(new_channels) outfile.setsampwidth(new_sample_width) outfile.setframerate(new_framerate) if new_channels == 1 and n_channels != 1: # Convert stereo to mono by averaging the channels frames = [sum(frames[i:i + sampwidth * n_channels:sampwidth]) // n_channels for i in range(0, len(frames), sampwidth * n_channels)] frames = bytearray(frames) elif new_channels == 2 and n_channels == 1: # Convert mono to stereo by duplicating the single channel frames = bytearray(frames + frames) outfile.writeframes(frames) except wave.Error as e: raise wave.Error(f\\"Error processing WAV file: {e}\\") except Exception as e: raise wave.Error(f\\"Unexpected error: {e}\\")"},{"question":"**Coding Question: Pandas Windowing Operations** # Objective Demonstrate your understanding of pandas windowing operations by manipulating time-series data using rolling, expanding, and exponentially weighted windows. # Problem Statement You are provided with a DataFrame of daily temperature readings over one month. Your task is to implement a series of windowing operations to analyze the data. # Input - A pandas DataFrame `df` with columns: - `date`: Date of the recording (including weekends). - `temperature`: Temperature recorded on the given date. ```python import pandas as pd data = { \'date\': pd.date_range(\'2022-01-01\', periods=31, freq=\'D\'), \'temperature\': [0.6, 2.3, np.nan, 2.1, 3.5, 5.7, 2.6, 1.9, 3.2, 4.0, np.nan, 3.7, 2.6, 1.4, 3.5, 6.1, 5.4, 4.2, 3.8, 2.9, 2.4, 3.7, 4.8, np.nan, 5.2, 3.1, 2.7, 3.9, 5.6, 4.3, 3.1] } df = pd.DataFrame(data) ``` # Output - A dictionary with the following keys and their respective values: - `\'7-day Rolling Mean\'`: Series containing 7-day rolling averages. - `\'Expanding Max\'`: Series containing the expanding maximum temperature. - `\'Weighted Moving Average\'`: Series containing the exponentially weighted moving average (`span=5`). - `\'Max Temp Custom Window\'`: Using a custom window where the window size is 3 days for weekends and expanding window for weekdays. # Constraints 1. The rolling mean and expanding max should handle `NaN` values by ignoring them. 2. The rolling window and custom window calculations should set the label at the center of the window. 3. Use the `adjust=True` option for the exponentially weighted moving average. # Function Signature ```python def analyze_temperature_data(df: pd.DataFrame) -> dict: pass ``` # Implementation Requirements - Use pandas windowing operations as described. - Handle possible `NaN` values in calculations. - Correctly apply custom logic for weekends and weekdays in custom window. # Example ```python def analyze_temperature_data(df: pd.DataFrame) -> dict: df[\'date\'] = pd.to_datetime(df[\'date\']) df.set_index(\'date\', inplace=True) rolling_mean = df[\'temperature\'].rolling(window=7, center=True, min_periods=1).mean() expanding_max = df[\'temperature\'].expanding(min_periods=1).max() weighted_avg = df[\'temperature\'].ewm(span=5, adjust=True).mean() def custom_window_func(idx_type): if idx_type.weekday() >= 5: # For weekends return 3 else: # For weekdays return idx + 1 from pandas.api.indexers import BaseIndexer class CustomBaseIndexer(BaseIndexer): def get_window_bounds(self, num_values, min_periods, center, closed, step): start = np.empty(num_values, dtype=np.int64) end = np.empty(num_values, dtype=np.int64) for i in range(num_values): window_size = custom_window_func(df.index[i]) start[i] = max(0, i + 1 - window_size) end[i] = i + 1 return start, end custom_indexer = CustomBaseIndexer(window_size=1) custom_window_max = df[\'temperature\'].rolling(window=custom_indexer, center=True, min_periods=1).max() return { \'7-day Rolling Mean\': rolling_mean, \'Expanding Max\': expanding_max, \'Weighted Moving Average\': weighted_avg, \'Max Temp Custom Window\': custom_window_max } ``` # Note - Ensure that the DataFrame handling and conversions are correct. - Your solution should be robust to handle missing values (`NaN`). - Performance considerations should be factored for handling larger data sets efficiently.","solution":"import pandas as pd import numpy as np def analyze_temperature_data(df: pd.DataFrame) -> dict: df[\'date\'] = pd.to_datetime(df[\'date\']) df.set_index(\'date\', inplace=True) rolling_mean = df[\'temperature\'].rolling(window=7, center=True, min_periods=1).mean() expanding_max = df[\'temperature\'].expanding(min_periods=1).max() weighted_avg = df[\'temperature\'].ewm(span=5, adjust=True).mean() def custom_window_func(date): # For weekends, use a rolling window of 3 days if date.weekday() >= 5: return 3 else: # For weekdays, use expanding window (Essentially growing window) return pd.Series(np.arange(1, df.index.get_loc(date) + 2)) custom_window_max = pd.Series(index=df.index) for i in range(len(df)): window_size = custom_window_func(df.index[i]) if isinstance(window_size, pd.Series): # Expanding window custom_window_max.iloc[i] = df[\'temperature\'].iloc[:i+1].max() else: # Rolling window of 3 days window_start = max(0, i - 1) window_end = i + 2 custom_window_max.iloc[i] = df[\'temperature\'].iloc[window_start:window_end].max() return { \'7-day Rolling Mean\': rolling_mean, \'Expanding Max\': expanding_max, \'Weighted Moving Average\': weighted_avg, \'Max Temp Custom Window\': custom_window_max }"},{"question":"You are required to write a Python function that performs a classification task using the scikit-learn package. Specifically, you will use the breast cancer dataset (`load_breast_cancer`) provided by scikit-learn. Your task is to load the dataset, preprocess it if necessary, split it into training and testing sets, and train a classifier. Finally, you will evaluate the performance of your model using appropriate metrics. # Function Signature ```python def classify_breast_cancer(random_seed: int) -> float: This function loads the breast cancer dataset, splits it into train and test sets, trains a classification model, and evaluates its performance. Parameters: - random_seed (int): a seed value for reproducibility of the splits. Returns: - float: The accuracy of the classifier on the test set. ``` # Input - `random_seed` (int): A seed value to ensure reproducibility when splitting the dataset into training and testing sets. # Output - A float representing the accuracy of the classifier on the test set. # Requirements - You must use the `load_breast_cancer` function from `sklearn.datasets` to load the dataset. - You must split the dataset into training and testing sets using `train_test_split` from `sklearn.model_selection`, with 70% of the data for training and 30% for testing. - You must use a classifier from `sklearn.neighbors` (e.g., `KNeighborsClassifier`). - Evaluate the model using accuracy as the metric using `sklearn.metrics.accuracy_score`. - Ensure to use the `random_seed` parameter for all operations requiring random state (e.g., `train_test_split` and classifier). # Example Usage ```python accuracy = classify_breast_cancer(42) print(f\\"Accuracy of the classifier: {accuracy:.4f}\\") ``` # Constraints - You must use only the scikit-learn package for this task and the standard Python libraries. # Notes - Consider any necessary data preprocessing steps such as scaling before training the classifier. - Make sure to handle any exceptions that might occur during data loading or model fitting processes.","solution":"from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score def classify_breast_cancer(random_seed: int) -> float: This function loads the breast cancer dataset, splits it into train and test sets, trains a classification model, and evaluates its performance. Parameters: - random_seed (int): a seed value for reproducibility of the splits. Returns: - float: The accuracy of the classifier on the test set. # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_seed) # Standardize the data scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Initialize and train the K-Nearest Neighbors classifier classifier = KNeighborsClassifier() classifier.fit(X_train, y_train) # Make predictions on the test set y_pred = classifier.predict(X_test) # Evaluate the accuracy of the classifier accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"# PyTorch Practical Assessment: TorchScript Module Implementation Objective Your task is to implement a PyTorch `nn.Module` that performs specific tensor operations and incorporates TorchScript functionalities - such as type annotations, scripting, and using built-in and custom TorchScript types. Instructions 1. Implement a custom PyTorch module named `TensorOperationsModule` which inherits from `torch.nn.Module`. 2. The module should have the following attributes: - `self.weight`: a torch tensor initialized with random values. - `self.bias`: a scalar float value. 3. Implement the `forward` method with the following signature: ```python def forward(self, x: torch.Tensor, multiply: bool) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: ``` - If `multiply` is `True`, the method should return the tensor `x` multiplied by `self.weight` and `self.bias` added as a new dimension. - If `multiply` is `False`, the method should return a tuple of the tensor `x` added to `self.weight` and `x` subtracted by `self.weight`. 4. Script the `TensorOperationsModule` using TorchScript. 5. Write a helper function `run_model` that accepts the following parameters: - A `TensorOperationsModule` instance. - An input tensor `x`. - A Boolean `multiply`. The function should return the output of the scripted module. 6. Include TorchScript type annotations for all necessary types and ensure compatibility of your implementation with TorchScript\'s type-checking rules. Requirements - You should use TorchScript\'s type annotations and features as necessary. - Ensure that all TorchScript rules and type-checking constraints are respected. - Implement error handling for scenarios where tensor dimensions are invalid for the given operations. - The solution should be efficient and handle various edge cases for tensor sizes and operations. Example Below is an example usage of the `TensorOperationsModule` and the `run_model` function: ```python import torch # Example implementation class TensorOperationsModule(torch.nn.Module): def __init__(self, weight_size: int): super().__init__() self.weight = torch.rand(weight_size) self.bias = 0.5 @torch.jit.export def forward(self, x: torch.Tensor, multiply: bool) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: if multiply: return x * self.weight + self.bias else: return x + self.weight, x - self.weight # Script the module scripted_module = torch.jit.script(TensorOperationsModule(3)) # Run the model def run_model(module: TensorOperationsModule, x: torch.Tensor, multiply: bool) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: return module(x, multiply) # Example execution x = torch.tensor([1.0, 2.0, 3.0]) output = run_model(scripted_module, x, True) print(output) ``` Constraints - Expected input tensor `x` should have compatible dimensions with the weight tensor in `TensorOperationsModule`. - Handle tensor operations carefully to avoid dimension mismatches. **Note**: Sharing the complete implementation and execution outputs is crucial for full evaluation.","solution":"import torch from typing import Union, Tuple class TensorOperationsModule(torch.nn.Module): def __init__(self, weight_size: int): super().__init__() self.weight = torch.rand(weight_size) self.bias = 0.5 @torch.jit.export def forward(self, x: torch.Tensor, multiply: bool) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: if multiply: return x * self.weight + self.bias else: return x + self.weight, x - self.weight # Script the module scripted_module = torch.jit.script(TensorOperationsModule(3)) # Run the model def run_model(module: TensorOperationsModule, x: torch.Tensor, multiply: bool) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]: return module(x, multiply)"},{"question":"# C Extension for Python: List Manipulation As part of this coding assessment, you are required to write a Python C extension that manipulates Python list objects using the functions provided by the Python C API. This question will test your understanding of managing Python objects and interfacing with Python from C. Task: 1. **Create a new Python list of specified length**: - Define a function `create_list` that takes an integer `n` and returns a new list of length `n` with all elements initialized to `None`. 2. **Append an item to a Python list**: - Define a function `append_to_list` that takes a list object and an item, appends the item to the list, and returns the modified list. 3. **Get an item from a Python list**: - Define a function `get_item` that takes a list object and an index, and returns the item at the specified index. 4. **Set an item in a Python list**: - Define a function `set_item` that takes a list object, an index, and an item, sets the item at the specified index, and returns the modified list. 5. **Reverse a Python list**: - Define a function `reverse_list` that takes a list object, reverses the list in place, and returns the modified list. Input and Output Format: 1. `create_list(n)` - **Input**: An integer `n`. - **Output**: A new Python list of length `n` with all elements set to `None`. 2. `append_to_list(lst, item)` - **Input**: A Python list `lst` and an item `item`. - **Output**: The Python list `lst` with `item` appended to the end. 3. `get_item(lst, index)` - **Input**: A Python list `lst` and an index `index`. - **Output**: The item at position `index` in the list `lst`. 4. `set_item(lst, index, item)` - **Input**: A Python list `lst`, an index `index`, and an item `item`. - **Output**: The Python list `lst` with `item` set at position `index`. 5. `reverse_list(lst)` - **Input**: A Python list `lst`. - **Output**: The Python list `lst` reversed in place. Constraints: - You can assume the list and indices are valid. - You must handle all memory management and reference counting as per Python C API requirements. - All functions should return `NULL` on failure and set the appropriate Python exception. Example: ```python # Python pseudo-code illustrating the expected behavior # create_list lst = create_list(3) # Expected output: [None, None, None] # append_to_list lst = append_to_list(lst, 4) # Expected output: [None, None, None, 4] # get_item item = get_item(lst, 2) # Expected output: None # set_item lst = set_item(lst, 2, 7) # Expected output: [None, None, 7, 4] # reverse_list lst = reverse_list(lst) # Expected output: [4, 7, None, None] ``` Implement the functions in a Python C extension module and provide the implementation details.","solution":"from typing import List def create_list(n: int) -> List[None]: Returns a new list of length n with all elements initialized to None. return [None] * n def append_to_list(lst: List, item) -> List: Appends an item to the end of the list and returns the modified list. lst.append(item) return lst def get_item(lst: List, index: int): Returns the item at the specified index in the list. return lst[index] def set_item(lst: List, index: int, item) -> List: Sets the item at the specified index in the list and returns the modified list. lst[index] = item return lst def reverse_list(lst: List) -> List: Reverses the list in place and returns the modified list. lst.reverse() return lst"},{"question":"# Advanced Logging Configuration in Python Objective: Design a logging configuration in Python that handles logs from multiple modules and threads. Your configuration should enable: 1. Different log levels being logged to different destinations (e.g., DEBUG logs to file, ERROR logs to console). 2. Custom formatting for different logging destinations. 3. Handling contextual information in log records. 4. Support for rotating file handlers to manage log file sizes. Task: Implement a Python script that sets up a logging configuration meeting the following specifications: 1. Log messages with level `DEBUG` and higher to a file named `app.log`, ensuring that the log file rotates every 1 MB, keeping 3 backups. 2. Log messages with level `ERROR` and higher to the console. 3. Use different formatting styles for console and file logging: - Console Format: `[LEVEL] - message` - File Format: `YYYY-MM-DD HH:MM:SS,ms - LEVEL - ThreadName - ModuleName - message` 4. Include threading information in log records. 5. Demonstrate logging from multiple modules with one of the modules logging from a separate thread. Expected Input and Output: - Input: - No direct input is required. The behavior should be demonstrated within the script using logging statements and thread creation. - Output: - Log messages should be correctly written to `app.log` with the specified rotation and formatting. - ERROR and higher level messages should be printed to the console with the appropriate format. Constraints: - You must define the logging configuration programmatically using Python\'s logging module. - Use threading to demonstrate the multi-threaded logging capability. Performance: - Ensure that logging operations do not degrade the application\'s performance, especially under multi-threaded conditions. Example of Usage in Main Script: ```python import logging import logging.config import threading import time # Configuring Logging as per the specifications def setup_logging(): # Your logging configuration code here # Sample function to demonstrate logging in a different module def sample_logging_function(): logger = logging.getLogger(\'sample_module\') logger.debug(\'This is a DEBUG message from a sample module.\') logger.error(\'This is an ERROR message from a sample module.\') # Sample function to demonstrate logging from a separate thread def thread_logging_function(): logger = logging.getLogger(\'threaded_module\') logger.info(\'This is an INFO message from a threaded function.\') logger.critical(\'This is a CRITICAL message from a threaded function.\') if __name__ == \'__main__\': setup_logging() # Logging in main module main_logger = logging.getLogger(\'main_module\') main_logger.info(\'This is an INFO message from the main module.\') # Logging from another module sample_logging_function() # Create and start a thread thread = threading.Thread(target=thread_logging_function) thread.start() thread.join() ``` Submission: Submit the Python script (`logging_config.py`) with the logging configuration and demonstration as specified above.","solution":"import logging import logging.config from logging.handlers import RotatingFileHandler import threading # Configuring Logging as per the specifications def setup_logging(): logging_config = { \'version\': 1, \'disable_existing_loggers\': False, \'formatters\': { \'console\': { \'format\': \'[%(levelname)s] - %(message)s\', }, \'file\': { \'format\': \'%(asctime)s - %(levelname)s - %(threadName)s - %(module)s - %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S,%f\', }, }, \'handlers\': { \'file_handler\': { \'class\': \'logging.handlers.RotatingFileHandler\', \'level\': \'DEBUG\', \'formatter\': \'file\', \'filename\': \'app.log\', \'maxBytes\': 1_000_000, \'backupCount\': 3, }, \'console_handler\': { \'class\': \'logging.StreamHandler\', \'level\': \'ERROR\', \'formatter\': \'console\', }, }, \'root\': { \'level\': \'DEBUG\', \'handlers\': [\'file_handler\', \'console_handler\'], }, } logging.config.dictConfig(logging_config) # Sample function to demonstrate logging in a different module def sample_logging_function(): logger = logging.getLogger(\'sample_module\') logger.debug(\'This is a DEBUG message from a sample module.\') logger.error(\'This is an ERROR message from a sample module.\') # Sample function to demonstrate logging from a separate thread def thread_logging_function(): logger = logging.getLogger(\'threaded_module\') logger.info(\'This is an INFO message from a threaded function.\') logger.critical(\'This is a CRITICAL message from a threaded function.\') if __name__ == \'__main__\': setup_logging() # Logging in main module main_logger = logging.getLogger(\'main_module\') main_logger.info(\'This is an INFO message from the main module.\') # Logging from another module sample_logging_function() # Create and start a thread thread = threading.Thread(target=thread_logging_function) thread.start() thread.join()"},{"question":"**Advanced Problem: Comprehensive Data Encoding and Decoding** # Problem Statement You are tasked with implementing a set of utility functions to perform encoding and decoding between binary data and ASCII encodings including uuencoding, base64, and quoted-printable. Additionally, you will handle conversions between binary data and hexadecimal representations. Your task is to implement the following functions: 1. `uu_encode(data: bytes, backtick: bool=False) -> str`: - Input: `data` - bytes object of length at most 45. - Input: `backtick` - boolean to control if zeros are represented by backtik. - Output: A uuencoded string representation of the input bytes. 2. `uu_decode(data: str) -> bytes`: - Input: `data` - a single line of uuencoded ASCII string. - Output: Bytes object decoded from the input string. 3. `base64_encode(data: bytes, newline: bool=True) -> str`: - Input: `data` - bytes object. - Input: `newline` - boolean to control if newline char is appended. - Output: A base64 encoded ASCII string. 4. `base64_decode(data: str) -> bytes`: - Input: `data` - base64 encoded ASCII string. - Output: Bytes object decoded from the input string. 5. `qp_encode(data: bytes, quotetabs: bool=False, istext: bool=True, header: bool=False) -> str`: - Input: `data` - bytes object. - Input: `quotetabs` - boolean to encode tabs and spaces. - Input: `istext` - boolean to control if newlines are encoded. - Input: `header` - boolean for RFC 1522 header compliance. - Output: A quoted-printable encoded ASCII string. 6. `qp_decode(data: str, header: bool=False) -> bytes`: - Input: `data` - quoted-printable ASCII string. - Output: Bytes object decoded from the input string. 7. `hex_encode(data: bytes, sep: Optional[Union[str, bytes]]=None, bytes_per_sep: int=1) -> str`: - Input: `data` - bytes object. - Input: `sep` - separator character inserted every few bytes (optional). - Input: `bytes_per_sep` - number of bytes between separators. - Output: Hexadecimal string representation of the input bytes. 8. `hex_decode(hexstr: str) -> bytes`: - Input: `hexstr` - hexadecimal string. - Output: Bytes object decoded from the input string. **Constraints:** - The `data` for `uu_encode` must be at most 45 bytes in length. - The `hexstr` must contain only hexadecimal digits. - Handle any errors gracefully by raising appropriate exceptions. # Example Usage ```python # Binary Data data = b\'Hello World!\' # UUEncode uu_encoded = uu_encode(data) print(uu_encoded) # Output should be a uuencoded ASCII string print(uu_decode(uu_encoded)) # Output should be b\'Hello World!\' # Base64 Encode base64_encoded = base64_encode(data) print(base64_encoded) # Output should be a base64 encoded ASCII string print(base64_decode(base64_encoded)) # Output should be b\'Hello World!\' # Quoted-Printable Encode qp_encoded = qp_encode(data) print(qp_encoded) # Output should be a quoted-printable encoded ASCII string print(qp_decode(qp_encoded)) # Output should be b\'Hello World!\' # Hexadecimal Encode hex_encoded = hex_encode(data) print(hex_encoded) # Output should be a hexadecimal string print(hex_decode(hex_encoded)) # Output should be b\'Hello World!\' ``` # Evaluation Your implementation will be evaluated on the correctness, efficiency, and handling of corner cases, ensuring the functions yield correct results for a wide range of inputs and handle errors as specified.","solution":"import base64 import binascii import quopri def uu_encode(data: bytes, backtick: bool=False) -> str: Encodes data using uuencode. encoded = binascii.b2a_uu(data).decode() if backtick: encoded = encoded.replace(\' \', \'`\') return encoded.strip() def uu_decode(data: str) -> bytes: Decodes uuencoded data. return binascii.a2b_uu(data) def base64_encode(data: bytes, newline: bool=True) -> str: Encodes data using base64. encoded = base64.b64encode(data).decode(\'ascii\') if newline: encoded += \'n\' return encoded def base64_decode(data: str) -> bytes: Decodes base64 encoded data. return base64.b64decode(data) def qp_encode(data: bytes, quotetabs: bool=False, istext: bool=True, header: bool=False) -> str: Encodes data using quoted-printable. return quopri.encodestring(data, quotetabs=quotetabs, header=header).decode(\'ascii\') def qp_decode(data: str, header: bool=False) -> bytes: Decodes quoted-printable encoded data. return quopri.decodestring(data, header=header) def hex_encode(data: bytes, sep: str=None, bytes_per_sep: int=1) -> str: Encodes data to hexadecimal format. hexstr = data.hex() if sep: hexstr = sep.join([hexstr[i:i+2*bytes_per_sep] for i in range(0, len(hexstr), 2*bytes_per_sep)]) return hexstr def hex_decode(hexstr: str) -> bytes: Decodes a hexadecimal string to bytes. return bytes.fromhex(hexstr)"},{"question":"Task You are required to implement a function that takes a JSON string representing email components and constructs an email message using Python\'s `email` package. The email message should then be encoded using Base64 encoding. The function should return the encoded email message. Input - A JSON-formatted string called `email_json` with the following structure: ```json { \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient@example.com\\", \\"subject\\": \\"Test Subject\\", \\"body\\": \\"This is the email body.\\", \\"attachments\\": [ { \\"filename\\": \\"test.txt\\", \\"content\\": \\"dGVzdCBjb250ZW50IGZvciBlbmNvZGluZw==\\", // base64 encoded content \\"type\\": \\"text/plain\\" } ] } ``` Output - A Base64 encoded string representing the entire email message. Constraints - The email must include the specified attachments and headers. - You may assume the JSON string and Base64 content are always valid. - Use appropriate MIME types for attachments. Example ```python import base64 import json from email.message import EmailMessage from email.policy import default def create_and_encode_email(email_json: str) -> str: # Your implementation here # Example input email_json = \'\'\' { \\"from\\": \\"sender@example.com\\", \\"to\\": \\"recipient@example.com\\", \\"subject\\": \\"Test Subject\\", \\"body\\": \\"This is the email body.\\", \\"attachments\\": [ { \\"filename\\": \\"test.txt\\", \\"content\\": \\"dGVzdCBjb250ZW50IGZvciBlbmNvZGluZw==\\", \\"type\\": \\"text/plain\\" } ] } \'\'\' # Example output (for reference purposes only) encoded_email = create_and_encode_email(email_json) print(encoded_email) ``` Note - This task will test your understanding of JSON handling, email message creation and manipulation using the `email` package, and Base64 encoding in Python.","solution":"import base64 import json from email.message import EmailMessage from email.policy import default def create_and_encode_email(email_json: str) -> str: # Parse the JSON string to a dictionary email_data = json.loads(email_json) # Create the email message msg = EmailMessage() msg[\'From\'] = email_data[\'from\'] msg[\'To\'] = email_data[\'to\'] msg[\'Subject\'] = email_data[\'subject\'] msg.set_content(email_data[\'body\']) # Process attachments for attachment in email_data.get(\'attachments\', []): # Decode the base64 content content = base64.b64decode(attachment[\'content\']) # Add the attachment to the email message msg.add_attachment(content, maintype=attachment[\'type\'].split(\'/\')[0], subtype=attachment[\'type\'].split(\'/\')[1], filename=attachment[\'filename\']) # Get the whole email as bytes email_bytes = msg.as_bytes(policy=default) # Encode the email bytes to base64 encoded_email = base64.b64encode(email_bytes).decode(\'utf-8\') return encoded_email"},{"question":"You are given a dataset containing information about tips received by waiters in a restaurant. Your task is to perform a thorough exploratory analysis using Seaborn\'s regression plotting functionalities. Dataset Description The dataset contains the following columns: - `total_bill`: Total bill amount in dollars. - `tip`: Tip amount given in dollars. - `sex`: Gender of the person paying the bill (Male/Female). - `smoker`: Whether the person was a smoker (Yes/No). - `day`: Day of the week (Thur/Fri/Sat/Sun). - `time`: Time of day (Lunch/Dinner). - `size`: Size of the group. Tasks 1. **Basic Regression Plot**: Create a basic regression plot using `lmplot` to show the relationship between `total_bill` and `tip`. 2. **Polynomial Regression**: Create a polynomial regression plot of order 2 for the relationship between `total_bill` and `tip`. 3. **Condition on Categorical Variable**: Create a regression plot conditioned on the `sex` variable using `hue`. 4. **Faceting with Multiple Variables**: Create a regression plot with facets based on `smoker` status and `time` of day. Use a grid layout with `col` for `smoker` and `row` for `time`. 5. **Residual Plot**: Create a residual plot to check if a simple linear regression is appropriate for the relationship between `total_bill` and `tip`. 6. **Advanced Plot (Joint or Pair Plot)**: Choose either a `jointplot` or `pairplot` to explore the relationships between `total_bill`, `tip`, and `size`. Include the `smoker` variable as a conditioning variable. 7. **Logistic Regression**: Create a new binary column `big_tip` that is `True` if the tip is greater than 15% of `total_bill` and `False` otherwise. Create a logistic regression plot to show the probability of giving a big tip based on the `total_bill`. Expected Input and Output - **Input**: A Pandas DataFrame named `tips` containing the columns described above. - **Output**: A series of plots as described in the tasks. Constraints and Considerations - Use Seaborn for all plotting. - Ensure the plots are correctly labeled and visually informative. - Use appropriate parameters to enhance plot interpretability where necessary. ```python import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 1: Basic Regression Plot sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.show() # Task 2: Polynomial Regression sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, order=2) plt.show() # Task 3: Condition on Categorical Variable sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", data=tips) plt.show() # Task 4: Faceting with Multiple Variables sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"smoker\\", row=\\"time\\", data=tips) plt.show() # Task 5: Residual Plot sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.show() # Task 6: Advanced Plot (Joint or Pair Plot) sns.jointplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, kind=\\"reg\\") plt.show() # Optionally, you can use pairplot for multiple relationships sns.pairplot(tips, x_vars=[\\"total_bill\\", \\"size\\"], y_vars=[\\"tip\\"], hue=\\"smoker\\", kind=\\"reg\\") plt.show() # Task 7: Logistic Regression tips[\\"big_tip\\"] = (tips[\\"tip\\"] / tips[\\"total_bill\\"]) > 0.15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True, y_jitter=0.03) plt.show() ```","solution":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def load_tips_dataset(): Load the tips dataset. return sns.load_dataset(\\"tips\\") def basic_regression_plot(tips): Create a basic regression plot using lmplot. sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.show() def polynomial_regression_plot(tips): Create a polynomial regression plot of order 2. sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, order=2) plt.show() def regression_plot_conditioned_on_sex(tips): Create a regression plot conditioned on the sex variable using hue. sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"sex\\", data=tips) plt.show() def faceting_regression_plot(tips): Create a regression plot with facets based on smoker status and time of day. sns.lmplot(x=\\"total_bill\\", y=\\"tip\\", hue=\\"smoker\\", col=\\"smoker\\", row=\\"time\\", data=tips) plt.show() def residual_plot(tips): Create a residual plot to check if a simple linear regression is appropriate. sns.residplot(x=\\"total_bill\\", y=\\"tip\\", data=tips) plt.show() def advanced_joint_plot(tips): Create a jointplot to explore relationships between total_bill, tip, and size. sns.jointplot(x=\\"total_bill\\", y=\\"tip\\", data=tips, kind=\\"reg\\") plt.show() def pair_plot(tips): Create a pairplot to explore relationships between total_bill, tip, and size. sns.pairplot(tips, x_vars=[\\"total_bill\\", \\"size\\"], y_vars=[\\"tip\\"], hue=\\"smoker\\", kind=\\"reg\\") plt.show() def logistic_regression_plot(tips): Create a logistic regression plot to show the probability of giving a big tip based on the total_bill. tips[\\"big_tip\\"] = (tips[\\"tip\\"] / tips[\\"total_bill\\"]) > 0.15 sns.lmplot(x=\\"total_bill\\", y=\\"big_tip\\", data=tips, logistic=True, y_jitter=0.03) plt.show()"},{"question":"# Object Protocol Coding Challenge **Objective:** Implement a custom Python class that simulates a simplified version of Python\'s dictionary using the Python C API attribute functions. This class should handle setting, getting, and deleting attributes, as well as producing a string representation of the dictionary contents. **Question:** You are tasked with writing a Python class `CustomDict` that closely mimics the behavior of a dictionary by leveraging Python\'s C-API attribute functions. The class should be able to: 1. Set key-value pairs using attribute assignment. 2. Retrieve values using attribute access. 3. Delete keys using the `del` statement. 4. Provide a string representation of the dictionary. 5. Handle non-existent attribute access appropriately. 6. Determine if it contains a certain key. 7. Compare two `CustomDict` objects based on their contents. **Function Details:** 1. **Setting attributes** - Use `PyObject_SetAttrString` to set values for the CustomDict. 2. **Getting attributes** - Use `PyObject_GetAttrString` to retrieve values. 3. **Deleting attributes** - Use `PyObject_DelAttrString` to delete keys. 4. **String representation** - Use `PyObject_Str` or `PyObject_Repr` to produce a string representation. **Implementation Tasks:** - Define the class `CustomDict`. - Implement the required methods: `__setattr__`, `__getattr__`, `__delattr__`, and `__repr__`. - Use appropriate error handling mechanisms provided in the documentation for handling attribute access and modification errors. **Constraints:** 1. Assume that all keys are strings. 2. Values can be of any type. 3. Operations must not throw errors and must handle non-existent keys gracefully. **Expected Function Signatures:** ```python class CustomDict: def __setattr__(self, key: str, value: Any) -> None: # implementation def __getattr__(self, key: str) -> Any: # implementation def __delattr__(self, key: str) -> None: # implementation def __repr__(self) -> str: # implementation ``` **Example Usage:** ```python cdict = CustomDict() cdict.name = \\"Alice\\" print(cdict.name) # Output: Alice del cdict.name try: print(cdict.name) except AttributeError: print(\\"Attribute not found.\\") # Expected behavior cdict.age = 30 cdict.language = \\"Python\\" print(repr(cdict)) # Output could be something like \\"{\'age\': 30, \'language\': \'Python\'}\\" ``` **Note:** You need to handle the NotImplemented scenario where keys or operations are not supported. It should return an appropriate placeholder.","solution":"class CustomDict: def __init__(self): self._dict = {} def __setattr__(self, key: str, value: any) -> None: if key == \\"_dict\\": super().__setattr__(key, value) else: self._dict[key] = value def __getattr__(self, key: str) -> any: try: return self._dict[key] except KeyError: raise AttributeError(f\\"\'CustomDict\' object has no attribute \'{key}\'\\") def __delattr__(self, key: str) -> None: try: del self._dict[key] except KeyError: raise AttributeError(f\\"\'CustomDict\' object has no attribute \'{key}\'\\") def __repr__(self) -> str: return repr(self._dict) def __contains__(self, key: str) -> bool: return key in self._dict def __eq__(self, other) -> bool: if isinstance(other, CustomDict): return self._dict == other._dict return NotImplemented"},{"question":"# Advanced Mathematical Computation Problem Statement Design a function called `compute_expression` that takes an integer `n` as input and returns the result of the following expression: [ text{result} = sum_{k=1}^{n} left( frac{binom{n}{k}}{sqrt{k}} cdot cosleft(frac{k cdot pi}{n}right) right) - lambda cdot sin(log(text{gcd}(n, k) + 1)) ] Where: - (binom{n}{k}) is the binomial coefficient, computed using `math.comb`. - (sqrt{k}) is the square root of (k), computed using `math.sqrt`. - (cosleft(frac{k cdot pi}{n}right)) is the cosine of (frac{k cdot pi}{n}), computed using `math.cos` and `math.pi`. - (sin(log(text{gcd}(n, k) + 1))) is the sine of the logarithm of the greatest common divisor of (n) and (k) plus one, computed using `math.sin`, `math.log` and `math.gcd`. - (lambda) is a constant equal to `math.tau`. You should use the relevant functions from the `math` module to accomplish this task. Function Signature ```python import math def compute_expression(n: int) -> float: # Your code here ``` Input - An integer `n` where (1 leq n leq 100). Output - Returns a float that is the computed result of the given expression. Example ```python assert math.isclose(compute_expression(5), some_expected_value, rel_tol=1e-09) ``` Additional Information - Make sure to handle both the mathematical operations and the translation into the correct functions from the `math` module. - Use the summation notation (sum) and loop through `k` from 1 to `n` accordingly.","solution":"import math def compute_expression(n: int) -> float: lambda_val = math.tau # Using math.tau which is equivalent to 2 * math.pi result = 0.0 for k in range(1, n + 1): binom_coeff = math.comb(n, k) sqrt_k = math.sqrt(k) cos_term = math.cos(k * math.pi / n) gcd_term = math.gcd(n, k) sin_log_gcd_term = math.sin(math.log(gcd_term + 1)) term = (binom_coeff / sqrt_k) * cos_term - (lambda_val * sin_log_gcd_term) result += term return result"},{"question":"**Objective**: Demonstrate your understanding of the `pipes` module by constructing and manipulating a pipeline to process text data. **Task**: You are required to create a function `process_text(data: str) -> str` that processes a given input string through a series of pipeline commands using the `pipes` module. The pipeline should perform the following operations: 1. Convert all alphabetical characters to uppercase. 2. Replace all spaces with hyphens (`-`). **Input**: - `data` (str): A string consisting of ASCII characters. **Output**: - (str): The processed string after passing through the pipeline. **Constraints**: - You must use at least two pipelines with the `pipes` module. - Assume that input string length will not exceed 10000 characters. - Performance should take into account the constraints on input size. **Implementation**: - Implement the `process_text` function using the `pipes` module as specified. - Use `append()` method to add commands to the pipeline. - Use `open()` method to read from or write to a file-like object through the pipeline. ```python import pipes def process_text(data: str) -> str: t = pipes.Template() # Append commands to convert lowercase to uppercase t.append(\'tr a-z A-Z\', \'--\') # Append commands to replace spaces with hyphens t.append(\'tr \\" \\" \\"-\\"\', \'--\') # Write the data to a file-like object through the pipeline f = t.open(\'pipefile\', \'w\') f.write(data) f.close() # Read and return the processed string with open(\'pipefile\', \'r\') as file: processed_data = file.read() return processed_data # Example usage: data = \\"hello world\\" print(process_text(data)) # Output should be: \\"HELLO-WORLD\\" ``` **Note**: - Although the `pipes` module is deprecated, ensure to use it for this exercise to demonstrate proficiency in handling legacy code.","solution":"import pipes def process_text(data: str) -> str: # Create a new pipes.Template object to construct our pipeline t = pipes.Template() # Append command to convert all lower-case letters to upper-case t.append(\'tr a-z A-Z\', \'--\') # Append command to replace spaces with hyphens t.append(\'tr \\" \\" \\"-\\"\', \'--\') # Open a file-like object for writing using the pipeline with t.open(\'pipefile\', \'w\') as f: f.write(data) # Read the processed data from the file-like object with open(\'pipefile\', \'r\') as file: processed_data = file.read() return processed_data"},{"question":"**Question:** Using the seaborn package, you are required to create a visual analysis of a dataset to explore multi-dimensional relationships. Here is what you need to do: 1. Load the `tips` dataset provided by seaborn. 2. Create a `FacetGrid` plot using seaborn where: - The rows represent the days of the week (`\\"day\\"` column). - The columns differentiate between `Lunch` and `Dinner` (`\\"time\\"` column). - Use different colors to distinguish between smokers and non-smokers (`\\"smoker\\"` column). 3. Inside each grid, plot a regression plot (`sns.regplot`) showing the relationship between the total bill (`\\"total_bill\\"`) and tip amount (`\\"tip\\"`). - Use the color palette: `{\\"Yes\\": \\"orange\\", \\"No\\": \\"blue\\"}`. - Ensure each plot has a color and a label for smokers and non-smokers. - Adjust the size of each grid plot to have a height of 3 and an aspect ratio of 1.5. 4. Customize the resulting plot such that: - Axis labels are set to `Total bill (USD)` for x-axis and `Tip (USD)` for y-axis. - Titles are margin titles indicating the day and time respectively. - Ticks are set on x-axis for values [10, 30, 50] and y-axis for values [0, 10]. - Legends are automatically added to the plot explaining the colors corresponding to smokers and non-smokers. 5. Finally, show the plot using `plt.show()` for display. Write your solution in a function named `analyze_tips_data()` which will perform all the steps mentioned above. **Function Signature:** ```python def analyze_tips_data(): pass ``` **Expected Output:** A multi-dimensional grid plot showcasing the analysis as specified. Ensure your code is well-commented to explain each step and customization done.","solution":"import seaborn as sns import matplotlib.pyplot as plt def analyze_tips_data(): This function creates a visual analysis of the tips dataset to explore multi-dimensional relationships using seaborn\'s FacetGrid and regression plots. # Load the tips dataset tips = sns.load_dataset(\'tips\') # Define a color palette for smokers palette = {\\"Yes\\": \\"orange\\", \\"No\\": \\"blue\\"} # Create a FacetGrid g = sns.FacetGrid(tips, row=\\"day\\", col=\\"time\\", hue=\\"smoker\\", palette=palette, height=3, aspect=1.5, margin_titles=True) # Map the regression plot onto the grid g.map(sns.regplot, \\"total_bill\\", \\"tip\\", scatter=True, fit_reg=True) # Add axis labels and ticks g.set_axis_labels(\\"Total bill (USD)\\", \\"Tip (USD)\\") # Adjust ticks for ax in g.axes.flat: ax.set_xticks([10, 30, 50]) ax.set_yticks([0, 10]) # Add legends g.add_legend() # Show the plot plt.show()"},{"question":"**Objective:** Implement a Python function that uses the `syslog` module to log system messages. The function should configure specific logging options, log messages with appropriate priority levels, and manage multiple facilities. **Function Signature:** ```python def log_system_messages(ident: str, log_option: int, facility: int, messages: list) -> None: Logs system messages using the syslog module. Parameters: ident (str): A string prepended to every message. log_option (int): Logging options (combined bit field). facility (int): Default facility for messages without an explicitly encoded facility. messages (list): A list of tuples where each tuple contains a priority level and a message. e.g., [(syslog.LOG_ERR, \\"Error message\\"), (syslog.LOG_INFO, \\"Information message\\")] Returns: None pass ``` # Requirements: 1. Use `syslog.openlog()` with the provided `ident`, `log_option`, and `facility` to set up logging options. 2. Iterate over the `messages` list, and for each message, use `syslog.syslog()` to log the message with the given priority level. 3. After logging all messages, ensure to call `syslog.closelog()` to close the logging session and reset settings. 4. Handle any exceptions that may occur during logging and print an appropriate error message. # Constraints: 1. The `ident` parameter must be a non-empty string. 2. The `messages` list must contain at least one tuple (priority level, message). 3. Use the appropriate constants from the `syslog` module for setting log options, facilities, and priority levels. # Example: ```python log_system_messages( ident=\\"my_app\\", log_option=syslog.LOG_PID | syslog.LOG_CONS, facility=syslog.LOG_USER, messages=[ (syslog.LOG_INFO, \\"Application started\\"), (syslog.LOG_ERR, \\"An error occurred\\"), (syslog.LOG_WARNING, \\"This is a warning\\") ] ) ``` **Expected Behavior:** - The function sets up logging with the `ident` \\"my_app\\". - Logs the three messages provided with their respective priority levels and the default facility `LOG_USER`. - Closes the logging session after completing the logging of all messages. # Performance: The function should efficiently log messages even if the `messages` list is large. Ensure that each `syslog.syslog()` call is properly formatted and minimal time is spent in setting up and closing the logging session.","solution":"import syslog def log_system_messages(ident: str, log_option: int, facility: int, messages: list) -> None: Logs system messages using the syslog module. Parameters: ident (str): A string prepended to every message. log_option (int): Logging options (combined bit field). facility (int): Default facility for messages without an explicitly encoded facility. messages (list): A list of tuples where each tuple contains a priority level and a message. e.g., [(syslog.LOG_ERR, \\"Error message\\"), (syslog.LOG_INFO, \\"Information message\\")] Returns: None try: # Open the syslog with given ident, option, and facility syslog.openlog(ident, log_option, facility) # Iterate through each message and log it with the given priority for priority, message in messages: syslog.syslog(priority, message) except Exception as e: print(f\\"An error occurred while logging messages: {e}\\") finally: # Close the syslog syslog.closelog()"},{"question":"**Objective:** To assess your understanding of Python generator objects, their creation, and their practical use in Python. **Question:** Write a Python generator function called `chunked_readlines` which takes two parameters: 1. `filename`: A string representing the path to a text file. 2. `chunk_size`: An integer representing the number of lines to return in each iteration. The generator should read the file line by line and yield a list containing `chunk_size` lines at a time. If the number of remaining lines is less than `chunk_size`, yield the remaining lines. You are required to handle exceptions that might occur during file operations, such as the file not existing or being unreadable, and yield an appropriate message as a single-element list in such cases. **Function Signature:** ```python def chunked_readlines(filename: str, chunk_size: int) -> Iterator[List[str]]: pass ``` **Input:** - `filename`: A string representing the path to a text file. - `chunk_size`: An integer greater than 0. **Output:** - An iterator generating lists, each containing up to `chunk_size` lines from the file. **Constraints:** - `chunk_size` will always be a positive integer. - The `filename` will be a valid string but may refer to a non-existent or inaccessible file. **Example Usage:** ```python # Example text file content: # line1 # line2 # line3 # line4 # line5 for chunk in chunked_readlines(\'example.txt\', 2): print(chunk) # Expected Output: # [\'line1\', \'line2\'] # [\'line3\', \'line4\'] # [\'line5\'] for chunk in chunked_readlines(\'nonexistent.txt\', 2): print(chunk) # Expected Output: # [\'Error: File not found or unreadable\'] ``` **Performance Requirements:** - The function should efficiently handle large files. - Minimize memory usage by reading lines in chunks rather than loading the entire file into memory. Good luck!","solution":"from typing import Iterator, List def chunked_readlines(filename: str, chunk_size: int) -> Iterator[List[str]]: A generator function that reads a file and yields lists of lines, each with up to `chunk_size` lines. If an exception occurs while reading the file, it yields an error message. try: with open(filename, \'r\') as file: chunk = [] for line in file: chunk.append(line.strip()) if len(chunk) == chunk_size: yield chunk chunk = [] if chunk: yield chunk except (FileNotFoundError, IOError): yield [\\"Error: File not found or unreadable\\"]"},{"question":"Implement a Python function to manage and analyze student grades. Your function should be able to compute the average grade, find the highest and lowest grades, and categorize students based on predefined grade thresholds. The function should also handle exceptions gracefully. # Detailed Instructions: 1. **Function Definitions**: - Create a function `manage_grades(grades: List[Tuple[str, int]]) -> Dict[str, Any]` which accepts a list of tuples where each tuple consists of a student name (string) and their grade (integer). - The function should return a dictionary with the following keys: - `average`: A float representing the average grade. - `highest`: A tuple with the name(s) of the student(s) and the highest grade. - `lowest`: A tuple with the name(s) of the student(s) and the lowest grade. - `categories`: A dictionary categorizing students into groups: - `A`: 90 <= grade <= 100 - `B`: 80 <= grade < 90 - `C`: 70 <= grade < 80 - `D`: 60 <= grade < 70 - `F`: grade < 60 2. **Input Constraints**: - Grades should be within the range 0 to 100. - Names should be unique and non-empty strings. - Handle cases where the input list is empty by raising a custom exception `EmptyGradesList`. 3. **Exception Handling**: - Define a custom exception class `EmptyGradesList(Exception)` that should be raised when the list of grades is empty. - Use exception handling to manage input errors (e.g., incorrect data types). 4. **Performance Requirements**: - Ensure the function handles up to 10,000 entries efficiently. 5. **Example**: ```python grades = [ (\\"Alice\\", 85), (\\"Bob\\", 95), (\\"Charlie\\", 77), (\\"David\\", 62), (\\"Eve\\", 59) ] result = manage_grades(grades) # Expected output: # { # \'average\': 75.6, # \'highest\': [(\\"Bob\\", 95)], # \'lowest\': [(\\"Eve\\", 59)], # \'categories\': { # \'A\': [(\\"Bob\\", 95)], # \'B\': [(\\"Alice\\", 85)], # \'C\': [(\\"Charlie\\", 77)], # \'D\': [(\\"David\\", 62)], # \'F\': [(\\"Eve\\", 59)] # } # } ``` # Note: - Use Python\'s built-in functions and standard libraries where applicable. - Ensure your code follows Python\'s coding standards and style guidelines.","solution":"from typing import List, Tuple, Dict, Any class EmptyGradesList(Exception): Custom exception raised when the list of grades is empty. pass def manage_grades(grades: List[Tuple[str, int]]) -> Dict[str, Any]: if not grades: raise EmptyGradesList(\\"The grade list is empty.\\") # Extract grades for computations grades_only = [grade for _, grade in grades] average = sum(grades_only) / len(grades_only) if grades_only else 0.0 highest_grade = max(grades_only) lowest_grade = min(grades_only) highest_students = [(name, grade) for name, grade in grades if grade == highest_grade] lowest_students = [(name, grade) for name, grade in grades if grade == lowest_grade] categories = {\'A\': [], \'B\': [], \'C\': [], \'D\': [], \'F\': []} for name, grade in grades: if 90 <= grade <= 100: categories[\'A\'].append((name, grade)) elif 80 <= grade < 90: categories[\'B\'].append((name, grade)) elif 70 <= grade < 80: categories[\'C\'].append((name, grade)) elif 60 <= grade < 70: categories[\'D\'].append((name, grade)) else: categories[\'F\'].append((name, grade)) return { \'average\': average, \'highest\': highest_students, \'lowest\': lowest_students, \'categories\': categories }"},{"question":"# Question: You are provided with a dataset named `flights` containing 10 years of monthly airline passenger data. Your task is to write a code that demonstrates your ability to visualize this data using seaborn and highlights the advanced features offered by seaborn for customizing these visualizations. **Dataset Description:** - The dataset `flights` contains the following columns: - `year`: The year of the observation. - `month`: The month of the observation. - `passengers`: The number of passengers. # Task Requirements: 1. **Data Preparation:** - Load the `flights` dataset using seaborn. - Pivot the dataset to wide-form so that each month\'s data becomes a separate column. 2. **Visualization:** - Create a line plot to show the total number of passengers each year. - Customize the line plot by setting the theme, adding markers, and displaying confidence intervals. 3. **Advanced Visualization:** - Create a multi-line plot to show the number of passengers for each month over the years. - Use different colors for each month and add a legend. 4. **Statistical Measures:** - Display error bars instead of bands and extend them to two standard error widths. - Use `units` to show lines for individual years without applying a semantic mapping. 5. **Customization:** - Adjust line width and marker size to distinguish between various months clearly. - Use a specific color palette for the plots. # Implementation Details: - **Input**: The function does not require any input arguments. You will directly work within the function to load the dataset, manipulate it, and create the visualizations. - **Output**: The function should display the plots described above. # Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt import matplotlib as mpl def plot_flight_data(): # Step 1: Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Step 2: Pivot the dataset to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Step 3: Create a line plot for total passengers each year plt.figure(figsize=(10, 6)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", markers=True, ci=\\"sd\\") plt.title(\\"Total Passengers Each Year\\") plt.show() # Step 4: Multi-line plot for each month over the years plt.figure(figsize=(14, 8)) sns.lineplot(data=flights_wide) plt.title(\\"Monthly Passengers Over Years\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Step 5: Show error bars and use \'units\' for individual year lines plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", err_style=\\"bars\\", errorbar=(\\"se\\", 2)) plt.title(\\"Total Passengers Each Year with Error Bars\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Step 6: Customize line and marker appearance palette = sns.color_palette(\\"husl\\", 12) plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", units=\\"year\\", estimator=None, palette=palette, lw=1.2, markers=True, markersize=10) plt.title(\\"Monthly Passengers Over Years with Custom Appearance\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Call the function plot_flight_data() ``` # Constraints: - Ensure that the visualizations are informative and aesthetically pleasing. - Use seaborn\'s capabilities to enhance the visual understanding of the data.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_flight_data(): # Step 1: Load the flights dataset flights = sns.load_dataset(\\"flights\\") # Step 2: Pivot the dataset to wide-form flights_wide = flights.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") # Step 3: Create a line plot for total passengers each year plt.figure(figsize=(10, 6)) sns.set_theme(style=\\"whitegrid\\") sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", marker=\\"o\\", ci=\\"sd\\") plt.title(\\"Total Passengers Each Year\\") plt.show() # Step 4: Multi-line plot for each month over the years plt.figure(figsize=(14, 8)) sns.lineplot(data=flights_wide, palette=\\"tab10\\") plt.title(\\"Monthly Passengers Over Years\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Step 5: Show error bars and use \'units\' for individual year lines plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", err_style=\\"bars\\", errorbar=(\\"se\\", 2), markers=True) plt.title(\\"Total Passengers Each Year with Error Bars\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Step 6: Customize line and marker appearance palette = sns.color_palette(\\"husl\\", 12) plt.figure(figsize=(14, 8)) sns.lineplot(data=flights, x=\\"year\\", y=\\"passengers\\", hue=\\"month\\", units=\\"year\\", estimator=None, palette=palette, lw=1.2, markers=True, markersize=8) plt.title(\\"Monthly Passengers Over Years with Custom Appearance\\") plt.legend(title=\'Month\', bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() # Call the function plot_flight_data()"},{"question":"# PyTorch Custom Operator Implementation and Testing In this problem, you are required to implement a custom PyTorch operator and test its functionality, including gradient checking. We\'ll guide you through the steps to ensure that your implementation covers the required aspects. Steps: 1. **Define a New Custom Operator:** - Implement a new custom operator using `torch.library.custom_op`. - The custom operation we want to create is `custom_square`, which simply computes the square of an input tensor. 2. **Register the Operator:** - Register the operator using `torch.library.register_kernel`. 3. **Gradient Check:** - Use `torch.autograd.gradcheck` to verify that the gradients for the custom operator are correctly computed. Implementation Details: 1. **Custom Operator Function**: - Name: `custom_square` - Input: A tensor of any shape. - Output: A tensor of the same shape where each element is the square of the corresponding input tensor element. 2. **Custom Operator Registration**: - Register the operator using `torch.library.register_kernel`. 3. **Gradient Check**: - Use `torch.autograd.gradcheck` to ensure that the gradient computation is correct for the `custom_square` operator. Example: Here\'s an outline of the expected implementation steps in the form of a code outline: ```python import torch from torch import autograd # Step 1: Define the custom operator def custom_square(tensor): return tensor * tensor # Step 2: Register the custom operator def register_custom_op(): my_lib = torch.library.Library(\\"my_custom_lib\\", \\"DEF\\") # Register our custom_square operator my_lib.define(\\"custom_square(Tensor self) -> Tensor\\") my_lib.impl(\\"custom_square\\", custom_square) # Step 3: Use opcheck to test the operator implementation def test_custom_op(): # Creating a simple tensor tensor = torch.tensor([2.0, 3.0, 4.0], requires_grad=True) # Perform the operation result = custom_square(tensor) # Check gradients grad_check_passed = autograd.gradcheck(custom_square, tensor) return result, grad_check_passed # Register the custom operator register_custom_op() # Test the custom operator result, grad_check_passed = test_custom_op() print(\\"Result of custom_square:\\", result) print(\\"Gradient check passed:\\", grad_check_passed) ``` # Requirements: 1. **Function Definitions**: - `custom_square(tensor: torch.Tensor) -> torch.Tensor`: Implements the custom square operation. - `register_custom_op()`: Registers the custom operator with PyTorch. - `test_custom_op() -> (torch.Tensor, bool)`: Tests the custom operator and performs gradient checking. 2. **Constraints**: - Ensure that the operator works for tensors of any shape. - The gradient check must pass, demonstrating correct autograd support. 3. **Performance**: - The custom operator should be tested on small test cases (e.g., tensors with a few elements) to validate its correctness. Implement the above functions and verify their functionality.","solution":"import torch from torch import autograd # Step 1: Define the custom operator def custom_square(tensor): return tensor * tensor # Step 2: Register the custom operator def register_custom_op(): my_lib = torch.library.Library(\\"my_custom_lib\\", \\"DEF\\") # Register our custom_square operator my_lib.define(\\"custom_square(Tensor self) -> Tensor\\") my_lib.impl(\\"custom_square\\", custom_square) # Step 3: Use opcheck to test the operator implementation def test_custom_op(): # Creating a simple tensor tensor = torch.tensor([2.0, 3.0, 4.0], dtype=torch.double, requires_grad=True) # Perform the operation result = custom_square(tensor) # Check gradients grad_check_passed = autograd.gradcheck(custom_square, (tensor,)) return result, grad_check_passed # Register the custom operator register_custom_op() # Test the custom operator result, grad_check_passed = test_custom_op() print(\\"Result of custom_square:\\", result) print(\\"Gradient check passed:\\", grad_check_passed)"},{"question":"You are provided with the `seaborn` library to visualize the given `tips` dataset. Your task is to create a complex plot that demonstrates your understanding of the `seaborn.objects` module. Follow the steps below to complete the task: # Task: 1. Load the `tips` dataset from `seaborn`. 2. Create a `seaborn.objects.Plot` object with `total_bill` on the x-axis and `tip` on the y-axis. 3. Add a `Dot` mark to show individual data points. 4. Add a `Line` mark with a `PolyFit` transformation to show a polynomial regression fit. 5. Add a `Bar` mark with a `Hist` and `Dodge` transformation to show the histogram of tips, categorized by `sex`. 6. Ensure that all three layers are included in your plot. 7. Customize the plot\'s aesthetics as follows: - Set `color` for dots based on `time` of the meal. - Set the `linewidth` of the line to 2 and color to \'blue\'. - Use `Dodge` to separate bars by `sex` in the histogram. - Include all relevant labels in the legend and label the y-axis as \\"Tips\\". # Constraints: - You must use the `seaborn.objects` module to create the plot. - Ensure different layers and transforms are applied correctly. # Expected Output: Your plot should include: - `Dot` representation of each data point. - A polynomial regression fit line. - A histogram of tips, separated by `sex`. - Appropriate axis labels and legend annotations for all layers. # Example Code Structure: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add Dot layer .add(so.Dot(color=\\"time\\")) # Add Line layer with PolyFit transformation .add(so.Line(color=\\"blue\\", linewidth=2), so.PolyFit()) # Add Bar layer with Hist and Dodge transformation .add(so.Bar(color=\\"sex\\"), so.Hist(), so.Dodge()) # Additional plot customization here .label(y=\\"Tips\\") ) # Display the plot plot ``` Ensure your code adheres to the guidelines and produces the expected result.","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") # Create the plot plot = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\") # Add Dot layer .add(so.Dot(color=\\"time\\")) # Add Line layer with PolyFit transformation .add(so.Line(color=\\"blue\\", linewidth=2), so.PolyFit()) # Add Bar layer with Hist and Dodge transformation .add(so.Bar(color=\\"sex\\"), so.Hist(), so.Dodge()) # Additional plot customization here .label(y=\\"Tips\\") ) # Display the plot plot"},{"question":"# Question: **Understanding and Manipulating Date and Time with Scheduled Events** You are part of a team developing a calendaring application that needs to: 1. Track scheduled events on specific dates. 2. Determine overlaps between events. 3. Adjust scheduled events based on time zone changes. Implement the following functions based on the requirements provided: Function 1: `add_event(event_list, event)` - **Input**: - `event_list` (List of tuples): Each tuple contains two elements, a `datetime` object representing the start time and a `timedelta` object representing the duration. - `event` (tuple): A tuple containing a `datetime` object representing the start time and a `timedelta` object representing the duration of the new event. - **Output**: - Returns the updated `event_list` with the new event added. Function 2: `find_overlapping_events(event_list)` - **Input**: - `event_list` (List of tuples): Each tuple contains two elements, a `datetime` object representing the start time and a `timedelta` object representing the duration. - **Output**: - Returns a list of tuples where each tuple contains two events that overlap in `event_list`. Function 3: `adjust_events_for_timezone(event_list, target_timezone)` - **Input**: - `event_list` (List of tuples): Each tuple contains two elements, a `datetime` object representing the start time (timezone aware) and a `timedelta` object representing the duration. - `target_timezone` (\\"timezone\\"): A `timezone` object representing the target time zone to which the events should be adjusted. - **Output**: - Returns the adjusted `event_list` with times changed to the `target_timezone`. Constraints: 1. You can assume all datetime objects used are timezone-aware. 2. Time complexity should be considered for large lists of events. # Example: ```python from datetime import datetime, timedelta, timezone events = [ (datetime(2023, 10, 1, 12, 0, tzinfo=timezone.utc), timedelta(hours=2)), (datetime(2023, 10, 1, 14, 0, tzinfo=timezone.utc), timedelta(hours=1)), ] new_event = (datetime(2023, 10, 1, 13, 0, tzinfo=timezone.utc), timedelta(hours=1)) # Function 1 updated_events = add_event(events, new_event) print(updated_events) # Expected to include new_event # Function 2 overlapping_events = find_overlapping_events(updated_events) print(overlapping_events) # Expected to find the overlapping events # Function 3 adjusted_events = adjust_events_for_timezone(updated_events, timezone(timedelta(hours=1))) print(adjusted_events) # Expected times adjusted to new timezone ```","solution":"from datetime import datetime, timedelta, timezone from typing import List, Tuple def add_event(event_list: List[Tuple[datetime, timedelta]], event: Tuple[datetime, timedelta]) -> List[Tuple[datetime, timedelta]]: Adds a new event to the event list. Parameters: event_list (List[Tuple[datetime, timedelta]]): The current list of events. event (Tuple[datetime, timedelta]): The new event to add. Returns: List[Tuple[datetime, timedelta]]: The updated list of events. event_list.append(event) return event_list def find_overlapping_events(event_list: List[Tuple[datetime, timedelta]]) -> List[Tuple[Tuple[datetime, timedelta], Tuple[datetime, timedelta]]]: Finds all pairs of overlapping events in the event list. Parameters: event_list (List[Tuple[datetime, timedelta]]): The list of events. Returns: List[Tuple[Tuple[datetime, timedelta], Tuple[datetime, timedelta]]]: List of tuples containing overlapping events. overlaps = [] for i in range(len(event_list)): for j in range(i + 1, len(event_list)): start1, duration1 = event_list[i] end1 = start1 + duration1 start2, duration2 = event_list[j] end2 = start2 + duration2 if start1 < end2 and start2 < end1: overlaps.append((event_list[i], event_list[j])) return overlaps def adjust_events_for_timezone(event_list: List[Tuple[datetime, timedelta]], target_timezone: timezone) -> List[Tuple[datetime, timedelta]]: Adjusts all events to the specified target time zone. Parameters: event_list (List[Tuple[datetime, timedelta]]): The list of events to adjust. target_timezone (timezone): The target time zone. Returns: List[Tuple[datetime, timedelta]]: The list of events adjusted to the target time zone. adjusted_list = [] for start, duration in event_list: new_start = start.astimezone(target_timezone) adjusted_list.append((new_start, duration)) return adjusted_list"},{"question":"You are required to parse an XML document to extract specific information using the `xml.sax` package. Implement a SAX-based XML parser to achieve the following: **Objective**: Write a Python function `parse_employee_data(xml_string: str) -> List[Dict[str, str]]` that parses an XML string containing employee data and extracts specific details from it. **Input**: - `xml_string` (str): A string containing XML data representing multiple employees. Each employee has the structure: ```xml <employee> <id>001</id> <name>John Doe</name> <position>Software Engineer</position> <salary>60000</salary> </employee> ``` **Output**: - A list of dictionaries, where each dictionary represents an employee and contains the following keys: `id`, `name`, `position`, and `salary`. **Constraints**: - The XML string should be well-formed. - Handle any potential parsing errors that may occur. **Function Signature**: ```python def parse_employee_data(xml_string: str) -> List[Dict[str, str]]: pass ``` **Example**: Input: ```python xml_data = \'\'\' <employees> <employee> <id>001</id> <name>John Doe</name> <position>Software Engineer</position> <salary>60000</salary> </employee> <employee> <id>002</id> <name>Jane Smith</name> <position>Data Analyst</position> <salary>55000</salary> </employee> </employees> \'\'\' ``` Output: ```python [ { \'id\': \'001\', \'name\': \'John Doe\', \'position\': \'Software Engineer\', \'salary\': \'60000\' }, { \'id\': \'002\', \'name\': \'Jane Smith\', \'position\': \'Data Analyst\', \'salary\': \'55000\' } ] ``` **Instructions**: 1. Define a custom `ContentHandler` class to handle the parsing events. 2. Use the `xml.sax.parseString` function to parse the provided XML string. 3. Handle parsing errors gracefully by using the appropriate exception handling mechanisms provided by the `xml.sax` package. Implement the function `parse_employee_data` by following the outlined steps: ```python import xml.sax def parse_employee_data(xml_string: str) -> List[Dict[str, str]]: class EmployeeHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.employee = {} self.employees = [] def startElement(self, name, attrs): self.current_data = name if name == \\"employee\\": self.employee = {} def endElement(self, name): if name == \\"employee\\": self.employees.append(self.employee) self.current_data = \\"\\" def characters(self, content): if self.current_data in {\\"id\\", \\"name\\", \\"position\\", \\"salary\\"}: self.employee[self.current_data] = content handler = EmployeeHandler() try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXException as e: raise e return handler.employees # Example usage xml_data = \'\'\' <employees> <employee> <id>001</id> <name>John Doe</name> <position>Software Engineer</position> <salary>60000</salary> </employee> <employee> <id>002</id> <name>Jane Smith</name> <position>Data Analyst</position> <salary>55000</salary> </employee> </employees> \'\'\' output = parse_employee_data(xml_data) print(output) ``` Ensure your function accurately parses the XML data and returns the desired output format. Handle any potential errors that might occur during parsing.","solution":"import xml.sax from typing import List, Dict def parse_employee_data(xml_string: str) -> List[Dict[str, str]]: class EmployeeHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.employee = {} self.employees = [] def startElement(self, name, attrs): self.current_data = name if name == \\"employee\\": self.employee = {} def endElement(self, name): if self.current_data in {\\"id\\", \\"name\\", \\"position\\", \\"salary\\"}: self.employee[self.current_data] = self.content.strip() if name == \\"employee\\": self.employees.append(self.employee) self.current_data = \\"\\" def characters(self, content): self.content = content handler = EmployeeHandler() try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXException as e: raise e return handler.employees"},{"question":"# Advanced Seaborn Aesthetics Customization Problem Statement: You are provided with a dataset comprising temperature recordings for different cities across various months. Your task is to create a visual representation of this data using Seaborn, incorporating advanced customization features. Write a function `plot_temperature_data` that takes the dataset and styling parameters as input and returns a customized line plot. Function Signature: ```python def plot_temperature_data(data: pd.DataFrame, theme: str, style: str, context: str, despine: bool, remove_spines: list, font_scale: float, linewidth: float) -> None: pass ``` Input: - `data` (pd.DataFrame): A DataFrame containing the temperature data with columns `City`, `Month`, and `Temperature`. - `theme` (str): The theme to apply using `sns.set_theme()`. - `style` (str): The style to set using `sns.set_style()`. Choices are `darkgrid`, `whitegrid`, `dark`, `white`, `ticks`. - `context` (str): The plotting context to set using `sns.set_context()`. Choices are `paper`, `notebook`, `talk`, `poster`. - `despine` (bool): Whether to remove the spines using `sns.despine()`. - `remove_spines` (list): List of spines to remove if `despine` is True. Choices are `top`, `right`, `bottom`, `left`. - `font_scale` (float): Scale for the font size. - `linewidth` (float): Width of the lines in the plot. Output: - The function should display a customized line plot based on the provided parameters. Constraints: - Ensure proper import of necessary libraries (`numpy`, `pandas`, `seaborn`, `matplotlib.pyplot`). - Handle cases where potentially invalid style, context, or spines to remove are provided. - The function should be robust and handle unexpected input gracefully. Example Usage: ```python import pandas as pd # Sample data data = pd.DataFrame({ \'City\': [\'New York\', \'New York\', \'New York\', \'Los Angeles\', \'Los Angeles\', \'Los Angeles\'], \'Month\': [\'January\', \'February\', \'March\', \'January\', \'February\', \'March\'], \'Temperature\': [30, 32, 45, 60, 62, 70] }) # Plot temperature data plot_temperature_data( data=data, theme=\'darkgrid\', style=\'whitegrid\', context=\'talk\', despine=True, remove_spines=[\'top\', \'right\'], font_scale=1.2, linewidth=2.0 ) ```","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_temperature_data(data: pd.DataFrame, theme: str, style: str, context: str, despine: bool, remove_spines: list, font_scale: float, linewidth: float) -> None: This function creates a customized line plot for temperature data using Seaborn. Parameters: - data (pd.DataFrame): DataFrame containing temperature data with columns \'City\', \'Month\', and \'Temperature\'. - theme (str): Theme to apply using sns.set_theme(). - style (str): Style to set using sns.set_style(). - context (str): Plotting context to set using sns.set_context(). - despine (bool): Whether to remove spines using sns.despine(). - remove_spines (list): List of spines to remove if despine is True. - font_scale (float): Scale for the font size. - linewidth (float): Width of the lines in the plot. # Validate inputs valid_styles = [\'darkgrid\', \'whitegrid\', \'dark\', \'white\', \'ticks\'] valid_contexts = [\'paper\', \'notebook\', \'talk\', \'poster\'] valid_spines = [\'top\', \'right\', \'bottom\', \'left\'] if style not in valid_styles: raise ValueError(f\'Invalid style: {style}. Valid options are {valid_styles}\') if context not in valid_contexts: raise ValueError(f\'Invalid context: {context}. Valid options are {valid_contexts}\') if not all(spine in valid_spines for spine in remove_spines): raise ValueError(f\'Invalid spine in remove_spines. Valid options are {valid_spines}\') # Apply theme and style sns.set_theme(style=theme) sns.set_style(style) sns.set_context(context, font_scale=font_scale) # Create the plot line_plot = sns.lineplot(data=data, x=\'Month\', y=\'Temperature\', hue=\'City\', linewidth=linewidth) # Remove spines if necessary if despine: sns.despine() for spine in remove_spines: line_plot.spines[spine].set_visible(False) plt.show()"},{"question":"# Question: Python Keyword Validator In this task, you\'ll implement a function that validates a list of strings by checking whether each string is a Python keyword or a soft keyword. You will use the `keyword` module to perform this validation. Function Signature ```python def validate_python_keywords(words: list) -> dict: pass ``` Input - `words`: A list of strings, where each string represents a word to be checked. (1 ≤ len(words) ≤ 1000) Output - A dictionary with two keys: - `\'keywords\'`: A list of words that are recognized as Python keywords. - `\'soft_keywords\'`: A list of words that are recognized as Python soft keywords. Constraints - The function should efficiently handle the provided list of words and avoid redundant checks. - The order of keywords and soft keywords in the output lists should match their order in the input list. Example ```python words = [\\"if\\", \\"while\\", \\"match\\", \\"async\\", \\"await\\", \\"fake\\"] output = { \\"keywords\\": [\\"if\\", \\"while\\", \\"async\\", \\"await\\"], \\"soft_keywords\\": [\\"match\\"] } ``` Note - In the example, \\"if\\", \\"while\\", \\"async\\", and \\"await\\" are standard Python keywords. - \\"match\\" is considered a soft keyword. - \\"fake\\" is neither a keyword nor a soft keyword and thus does not appear in the output. Explanation - You will use `keyword.iskeyword(s)` to check if a word is a keyword. - You will use `keyword.issoftkeyword(s)` to check if a word is a soft keyword. Implement the function `validate_python_keywords` to achieve the functionality described.","solution":"import keyword def validate_python_keywords(words: list) -> dict: This function takes a list of words and validates each word to see if it is a Python keyword or a soft keyword. It returns a dictionary with two keys: - \'keywords\': A list of words that are recognized as Python keywords. - \'soft_keywords\': A list of words that are recognized as Python soft keywords. result = { \'keywords\': [], \'soft_keywords\': [] } for word in words: if keyword.iskeyword(word): result[\'keywords\'].append(word) elif keyword.issoftkeyword(word): result[\'soft_keywords\'].append(word) return result"},{"question":"**Objective:** Evaluate understanding of Python\'s tuple and struct sequence C-API functions, error handling, and memory management. **Problem:** Implement a Python function `create_custom_struct_sequence` that: 1. Creates a new tuple with a specified size and initializes its elements with provided values. 2. Defines a new struct sequence type with named fields. 3. Creates an instance of this struct sequence and sets its fields using values from the previously created tuple. 4. Returns the struct sequence instance. **Function Signature:** ```python def create_custom_struct_sequence(size: int, values: List[Any], field_names: List[str], struct_name: str) -> PyObject: :param size: Size of the tuple to create. :param values: Values to initialize the tuple. :param field_names: Names of the fields in the struct sequence. :param struct_name: Name of the struct sequence type. :return: Instance of the created struct sequence with fields set to the corresponding values. pass ``` **Requirements:** 1. Create a tuple using `PyTuple_New(size)` and initialize it with values from the `values` list. 2. Define a struct sequence type using `PyStructSequence_NewType()` with fields specified by `field_names` and the type name `struct_name`. 3. Create an instance of the struct sequence using `PyStructSequence_New()` and set its fields to match the tuple\'s elements. 4. Ensure proper error handling, including checking for NULL returns from API functions and managing references correctly. 5. The function should raise appropriate Python exceptions if any step fails. **Example:** ```python size = 3 values = [1, \\"two\\", 3.0] field_names = [\\"first\\", \\"second\\", \\"third\\"] struct_name = \\"CustomStruct\\" custom_struct = create_custom_struct_sequence(size, values, field_names, struct_name) print(custom_struct.first) # Output: 1 print(custom_struct.second) # Output: \\"two\\" print(custom_struct.third) # Output: 3.0 ``` **Notes:** - Use Python\'s `ctypes` or C extension to wrap the necessary C-API functions for this task. - Ensure proper reference counting and memory management to prevent memory leaks or segmentation faults. - Provide comments explaining the steps and reasoning, focusing on handling edge cases and errors.","solution":"import ctypes from typing import List, Any def create_custom_struct_sequence(size: int, values: List[Any], field_names: List[str], struct_name: str): # Define and create necessary CTypes structures for simulation. class CustomStruct(ctypes.Structure): _fields_ = [(field_names[i], ctypes.py_object) for i in range(size)] CustomStruct.__name__ = struct_name # Set the name of the struct if size != len(values) or size != len(field_names): raise ValueError(\\"Size of tuple, values list, and field names list must be the same.\\") # Instantiate the struct and set the field values from the tuple. instance = CustomStruct() for i in range(size): setattr(instance, field_names[i], values[i]) return instance"},{"question":"# Python Keyword and Soft Keyword Analyzer **Objective:** Write a Python function `analyze_strings(strings: List[str]) -> Dict[str, Dict[str, bool]]` that takes a list of strings as input and returns a dictionary where the keys are the strings from the input list. The values are dictionaries themselves, with two keys: `\'is_keyword\'` and `\'is_softkeyword\'`. The corresponding values should be boolean, indicating whether the string is a Python keyword or a soft keyword, respectively. **Function Signature:** ```python from typing import List, Dict def analyze_strings(strings: List[str]) -> Dict[str, Dict[str, bool]]: ``` **Input:** - `strings`: A list of strings (1 ≤ len(strings) ≤ 1000). **Output:** - A dictionary where: - Each key is a string from the input list. - Each value is a dictionary with two boolean keys: `\'is_keyword\'` and `\'is_softkeyword\'`. **Example:** ```python strings = [\\"if\\", \\"class\\", \\"myVar\\", \\"match\\", \\"case\\"] result = analyze_strings(strings) ``` **Expected output:** ```python { \\"if\\": {\\"is_keyword\\": True, \\"is_softkeyword\\": False}, \\"class\\": {\\"is_keyword\\": True, \\"is_softkeyword\\": False}, \\"myVar\\": {\\"is_keyword\\": False, \\"is_softkeyword\\": False}, \\"match\\": {\\"is_keyword\\": False, \\"is_softkeyword\\": True}, \\"case\\": {\\"is_keyword\\": False, \\"is_softkeyword\\": True} } ``` **Constraints:** - You may assume that all strings in the input list contain only alphabetic characters and are non-empty. - You should make use of the `keyword` module to determine whether a string is a keyword or a soft keyword. **Notes:** - Consider edge cases where there might be no keywords or soft keywords in the input list. - Your solution should handle the input efficiently. **Documentation for Reference:** The `keyword` module provides the following: - `keyword.iskeyword(s)`: Return \\"True\\" if `s` is a Python keyword. - `keyword.kwlist`: A list of all keywords defined for the interpreter. - `keyword.issoftkeyword(s)`: Return \\"True\\" if `s` is a Python soft keyword. - `keyword.softkwlist`: A list of all soft keywords defined for the interpreter.","solution":"from typing import List, Dict import keyword def analyze_strings(strings: List[str]) -> Dict[str, Dict[str, bool]]: Analyzes each string in the input list to determine if it is a Python keyword or a softkeyword. Parameters: strings (List[str]): A list of strings to be analyzed. Returns: Dict[str, Dict[str, bool]]: A dictionary where each key is an input string and the value is another dictionary with two keys \'is_keyword\' and \'is_softkeyword\', indicating if the string is a keyword or softkeyword, respectively. result = {} for s in strings: result[s] = { \'is_keyword\': keyword.iskeyword(s), \'is_softkeyword\': keyword.issoftkeyword(s) } return result"},{"question":"You are given a dataset that includes information about various products sold in different store branches across different days. You are required to perform data manipulation tasks using pandas to analyze this dataset effectively. Input A CSV file named `sales_data.csv` with the following columns: - `Date`: Date of sale in `YYYY-MM-DD` format. - `Branch`: Store branch identifier (string). - `Product`: Product category (string). - `UnitsSold`: Number of units sold (integer). - `UnitPrice`: Price per unit (float). Requirements 1. **Data Cleaning and Preprocessing**: - Read the CSV file into a pandas DataFrame. - Ensure the `Date` column is in `datetime` format. - Identify and handle any missing values in the dataset. For this task, assume the missing values should be filled with `0` for numeric columns and `\'Unknown\'` for categorical columns. 2. **Data Manipulation**: - Create a new column `TotalSales` which is computed as the product of `UnitsSold` and `UnitPrice`. - Pivot the dataframe to create a summary table with `Date` as rows, `Branch` as columns, and values as the sum of `TotalSales`. 3. **Analysis**: - Find the date with the highest total sales across all branches. - Calculate the total units sold for each product category and identify the product category with the highest total units sold. Expected Functions Implement the following functions: 1. ```python def preprocess_data(filename: str) -> pd.DataFrame: Reads a CSV file and preprocesses the dataset by converting \'Date\' to datetime format and handling missing values. Parameters: filename (str): The path to the CSV file to read. Returns: pd.DataFrame: The cleaned and preprocessed dataframe. pass ``` 2. ```python def create_summary_table(df: pd.DataFrame) -> pd.DataFrame: Creates a pivot table from the dataframe with \'Date\' as rows, \'Branch\' as columns, and values as the sum of \'TotalSales\'. Parameters: df (pd.DataFrame): The input dataframe. Returns: pd.DataFrame: The pivot table dataframe. pass ``` 3. ```python def highest_sales_date(pivot_df: pd.DataFrame) -> pd.Timestamp: Finds the date with the highest total sales across all branches in the pivot table. Parameters: pivot_df (pd.DataFrame): The pivot table dataframe. Returns: pd.Timestamp: The date with the highest total sales. pass ``` 4. ```python def highest_units_sold_product_category(df: pd.DataFrame) -> str: Identifies the product category with the highest total units sold. Parameters: df (pd.DataFrame): The input dataframe. Returns: str: The product category with the highest total units sold. pass ``` Example Usage ```python # Assume \'sales_data.csv\' is the provided dataset file df = preprocess_data(\'sales_data.csv\') pivot_df = create_summary_table(df) max_sales_date = highest_sales_date(pivot_df) top_product_category = highest_units_sold_product_category(df) print(f\\"The date with the highest total sales is: {max_sales_date}\\") print(f\\"The product category with the highest total units sold is: {top_product_category}\\") ``` Note: Ensure your solution handles edge cases, such as handling when the dataset is empty or contains only missing values.","solution":"import pandas as pd def preprocess_data(filename: str) -> pd.DataFrame: Reads a CSV file and preprocesses the dataset by converting \'Date\' to datetime format and handling missing values. Parameters: filename (str): The path to the CSV file to read. Returns: pd.DataFrame: The cleaned and preprocessed dataframe. df = pd.read_csv(filename) df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Handling missing values df[\'UnitsSold\'] = df[\'UnitsSold\'].fillna(0) df[\'UnitPrice\'] = df[\'UnitPrice\'].fillna(0) df[\'Branch\'] = df[\'Branch\'].fillna(\'Unknown\') df[\'Product\'] = df[\'Product\'].fillna(\'Unknown\') return df def create_summary_table(df: pd.DataFrame) -> pd.DataFrame: Creates a pivot table from the dataframe with \'Date\' as rows, \'Branch\' as columns, and values as the sum of \'TotalSales\'. Parameters: df (pd.DataFrame): The input dataframe. Returns: pd.DataFrame: The pivot table dataframe. df[\'TotalSales\'] = df[\'UnitsSold\'] * df[\'UnitPrice\'] pivot_df = df.pivot_table(index=\'Date\', columns=\'Branch\', values=\'TotalSales\', aggfunc=\'sum\') pivot_df = pivot_df.fillna(0) # Fill NaNs with 0 for the pivot table return pivot_df def highest_sales_date(pivot_df: pd.DataFrame) -> pd.Timestamp: Finds the date with the highest total sales across all branches in the pivot table. Parameters: pivot_df (pd.DataFrame): The pivot table dataframe. Returns: pd.Timestamp: The date with the highest total sales. total_sales_per_day = pivot_df.sum(axis=1) highest_sales_date = total_sales_per_day.idxmax() return highest_sales_date def highest_units_sold_product_category(df: pd.DataFrame) -> str: Identifies the product category with the highest total units sold. Parameters: df (pd.DataFrame): The input dataframe. Returns: str: The product category with the highest total units sold. total_units_per_product = df.groupby(\'Product\')[\'UnitsSold\'].sum() highest_units_product_category = total_units_per_product.idxmax() return highest_units_product_category"},{"question":"**Objective:** Your goal is to design a simple convolutional neural network (CNN) using PyTorch\'s `torch.nn.functional` module. You will implement the CNN class with the appropriate layers and forward method, selectively applying different types of convolutions, pooling, activation functions, and loss functions. **Task:** 1. **Define a CNN class**: Implement a class `SimpleCNN` that inherits from `torch.nn.Module`. 2. **Constructor (`__init__` method)**: * Initialize the convolutional layers * Initialize the pooling layers * Initialize any activation functions * Define at least one fully connected (linear) layer * Define loss function 3. **Forward pass (`forward` method)**: * Implement the forward method to define the forward propagation through the network * Include convolutions, pooling, activations, and the final fully connected layer * Implement at least one type of pooling * Apply a non-linear activation function between layers 4. **Loss calculation method**: * Create a separate method to compute the loss based on model predictions and targets # Expected Input and Output - **Input**: The input will be a batch of images with the shape `(batch_size, channels, height, width)`. - **Output**: The output should be the predictions from the network and the computed loss value. # Constraints: * Use only the functions provided in the `torch.nn.functional` module. * At least one convolutional layer should use `conv2d`. * Use ReLU (`torch.nn.functional.relu`) as the non-linear activation function. * Use `max_pool2d` for pooling. * Use `cross_entropy` for the loss function. Below is a scaffold to help you structure your class. ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() # Define the convolutional, pooling, activation, and fully connected layers def forward(self, x): # Define the forward pass using the layers pass def compute_loss(self, predictions, targets): # Compute and return the loss pass ``` **Your task**: Complete the SimpleCNN class based on the guidelines above.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() # Convolutional layer 1 self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1) # Convolutional layer 2 self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) # Max pooling layer self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0) # Fully connected layer self.fc1 = nn.Linear(32 * 8 * 8, 10) # Assuming input image size is 32x32 pixels def forward(self, x): # Convolutional layer 1 x = self.conv1(x) x = F.relu(x) x = self.pool(x) # Convolutional layer 2 x = self.conv2(x) x = F.relu(x) x = self.pool(x) # Flatten the tensor x = x.view(-1, 32 * 8 * 8) # Fully connected layer x = self.fc1(x) return x def compute_loss(self, predictions, targets): # Compute and return the cross entropy loss loss = F.cross_entropy(predictions, targets) return loss"},{"question":"# Task: Create a Backup and Log System You have been tasked with developing a Python script that will help automate the backup and logging of important files in a directory. Your script should perform the following tasks: 1. **Find Files:** Use the `glob` module to find all files in a specified directory that match a given pattern (e.g., `*.txt` for text files). 2. **Log Files:** Use the `datetime` module to log the names of the found files along with the current date and time. This log should be written to a file named `backup_log.txt`. 3. **Backup Files:** Use the `zlib` module to compress each found file and save the compressed data in a new file with the same name appended with `.gz` (e.g., `example.txt` becomes `example.txt.gz`). 4. **Command Line Arguments:** Use the `argparse` module to accept the following command line arguments: - `--directory`: The directory to search for files (default is the current working directory). - `--pattern`: The file pattern to search for (default is `*.txt`). - `--logfile`: The name of the log file (default is `backup_log.txt`). # Example Usage ```sh python backup_and_log.py --directory=/path/to/dir --pattern=*.py --logfile=my_log.txt ``` # Constraints - You must use the specified modules (`glob`, `datetime`, `zlib`, `argparse`) to achieve the desired functionality. - Ensure that your code handles exceptions appropriately (e.g., file not found, permission errors). # Input - Command line arguments as described above. # Output - A log file (`backup_log.txt` by default) with entries for each file found and the date and time it was found. - Compressed files with a `.gz` extension in the same directory as the original files. # Requirements - Your solution should be efficient and handle large directories with many files. # Example Suppose you have a directory `/example/dir` containing the files `test1.txt`, `test2.txt`, and `script.py`. Running the script with the following arguments: ```sh python backup_and_log.py --directory=/example/dir --pattern=*.txt --logfile=backup_log.txt ``` # The resulting `backup_log.txt` might contain: ``` Found test1.txt at 2023-10-10 15:30:00 Found test2.txt at 2023-10-10 15:30:02 ``` # And the directory might then contain: ``` /example/dir/test1.txt.gz /example/dir/test2.txt.gz /example/dir/script.py /example/dir/backup_log.txt ``` Implement the function as described.","solution":"import argparse import glob import datetime import zlib import os def find_files(directory, pattern): Find all files in the specified directory that match the given pattern. return glob.glob(os.path.join(directory, pattern)) def log_files(files, logfile): Log the names of the found files along with the current date and time. with open(logfile, \'a\') as log: for file in files: current_time = datetime.datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\") log.write(f\\"Found {file} at {current_time}n\\") def backup_files(files): Compress each found file and save the compressed data in a new file with the same name appended with .gz. for file in files: with open(file, \'rb\') as f: data = f.read() compressed_data = zlib.compress(data, level=9) with open(file + \'.gz\', \'wb\') as f: f.write(compressed_data) def main(): parser = argparse.ArgumentParser(description=\'Backup and Log System\') parser.add_argument(\'--directory\', type=str, default=\'.\', help=\'The directory to search for files\') parser.add_argument(\'--pattern\', type=str, default=\'*.txt\', help=\'The file pattern to search for\') parser.add_argument(\'--logfile\', type=str, default=\'backup_log.txt\', help=\'The name of the log file\') args = parser.parse_args() files = find_files(args.directory, args.pattern) log_files(files, args.logfile) backup_files(files) if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> The **Logging Cookbook** document provides a comprehensive guide to various aspects of logging in Python. It covers basic to advanced topics, including usage in multiple modules, logging from multiple threads, custom handlers, filters, logging configuration through dictionaries, logging to multiple destinations, working with different logging levels, and adding contextual information. The document includes several examples to illustrate these concepts. Key points include: 1. **Basic and Advanced Logging Setup**: Sample codes for setting up basic logging to file and console, and configuring multiple handlers and formatters. 2. **Logging in Complex Scenarios**: Handling logging in multi-threaded applications, using different logging levels, sending logs over the network, and asynchronous logging using queues. 3. **Customization**: Creating custom handlers, formatters, filters, and log record factories to meet specific needs. 4. **Logging Structure and Contextual Information**: Using structured logging, adding contextual data to log records, and adapting logs to different formats like JSON and RFC 5424. 5. **Practical Applications and Examples**: Examples of how to use logging in real-world applications, such as web applications, CLI tools, and GUI applications. 6. **Best Practices**: Guidelines on how to properly configure logging in libraries and applications, the pitfalls to avoid, and ensuring compatibility. Overall, this extensive documentation serves well to design a challenging and comprehensive assessment question. <|Analysis End|> <|Question Begin|> # Python310 Coding Assessment Question Context: Your task is to demonstrate proficiency in Python\'s logging module by implementing a custom logging configuration for a multi-threaded application, which logs messages with both contextual and structured information. Requirements: 1. **Multi-threaded Logging**: - Create a main thread and two worker threads. - Each worker thread should perform tasks and log messages that include thread-specific contextual information. 2. **Custom Logger Configuration**: - Configure a logger that: - Logs messages to the console at `INFO` level and above. - Logs messages with level `DEBUG` and above to a rolling file named `app.log` with a maximum file size of 5000 bytes and a backup count of 3. - Uses a custom formatter that includes the timestamp, thread name, log level, and message. - Adds structured information (JSON format) to certain log messages. 3. **Contextual Information**: - Add thread-specific contextual information such as `threadName` and `taskId` to log messages. 4. **Structured Logging**: - Implement a feature to log structured data (like task details) in JSON format. Function Specifications: 1. **configure_logger()**: - Initializes and configures the logger as per the requirements. 2. **worker_thread(task_id)**: - Simulates work, logs both normal and structured messages, and runs in a separate thread. 3. **main()**: - Starts the main thread, creates and starts worker threads, and simulates logging events from the main thread. Expected Input and Output: - **Input**: No direct input. - **Output**: Logging output as configured, visible in the console and in the `app.log` file. Constraints: - Use the `logging` module. - Ensure thread safety. - Use JSON for structured logging. Performance Requirements: - The solution should efficiently handle multiple threads. - Logging should not significantly impact the performance of the application. # Example Logging Output: Console: ``` 2023-10-20 12:00:00,123 Thread-1 INFO Task-1: Starting task 2023-10-20 12:00:02,456 Thread-2 INFO Task-2: Starting task ``` `app.log`: ``` 2023-10-20 12:00:00,123 Thread-1 DEBUG Task-1: Task details: {\\"step\\": 1, \\"status\\": \\"started\\"} 2023-10-20 12:00:01,789 MainThread DEBUG Main task processing 2023-10-20 12:00:02,456 Thread-2 DEBUG Task-2: Task details: {\\"step\\": 1, \\"status\\": \\"started\\"} ``` # Implementation: ```python import logging import logging.handlers import json import threading import time def configure_logger(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Console handler for INFO level and above console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # File handler for DEBUG level and logs to a rolling file file_handler = logging.handlers.RotatingFileHandler(\'app.log\', maxBytes=5000, backupCount=3) file_handler.setLevel(logging.DEBUG) formatter = logging.Formatter(\'%(asctime)s %(threadName)-12s %(levelname)-8s %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) logger.addHandler(console_handler) logger.addHandler(file_handler) def worker_thread(task_id): logger = logging.getLogger() structured_data = { \\"taskId\\": task_id, \\"step\\": 1, \\"status\\": \\"started\\" } logger.info(f\\"Task-{task_id}: Starting task\\") logger.debug(f\\"Task-{task_id}: Task details: {json.dumps(structured_data)}\\") # Simulate work time.sleep(2) structured_data[\\"step\\"] = 2 structured_data[\\"status\\"] = \\"completed\\" logger.info(f\\"Task-{task_id}: Task completed\\") logger.debug(f\\"Task-{task_id}: Task details: {json.dumps(structured_data)}\\") def main(): configure_logger() logger = logging.getLogger() logger.info(\\"Main thread: Starting application\\") threads = [] for i in range(2): t = threading.Thread(target=worker_thread, name=f\\"Worker-{i+1}\\", args=(i+1,)) threads.append(t) t.start() for t in threads: t.join() logger.info(\\"Main thread: Application finished\\") if __name__ == \'__main__\': main() ``` # Explanation: 1. **Logger Configuration**: - Configures a console handler for logging INFO level messages. - Configures a rotating file handler for DEBUG level messages. - Uses a custom formatter. 2. **Worker Threads**: - Simulates task processing and logs both normal and structured messages. 3. **Main Function**: - Initializes logging, starts worker threads, and logs from the main thread. This design assesses the understanding of advanced logging features such as custom formatters, rotating file handlers, and contextual/structured logging in a multi-threaded environment.","solution":"import logging import logging.handlers import json import threading import time def configure_logger(): logger = logging.getLogger() logger.setLevel(logging.DEBUG) # Console handler for INFO level and above console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) # File handler for DEBUG level and logs to a rolling file file_handler = logging.handlers.RotatingFileHandler(\'app.log\', maxBytes=5000, backupCount=3) file_handler.setLevel(logging.DEBUG) formatter = logging.Formatter(\'%(asctime)s %(threadName)-12s %(levelname)-8s %(message)s\') console_handler.setFormatter(formatter) file_handler.setFormatter(formatter) logger.addHandler(console_handler) logger.addHandler(file_handler) def worker_thread(task_id): logger = logging.getLogger() structured_data = { \\"taskId\\": task_id, \\"step\\": 1, \\"status\\": \\"started\\" } logger.info(f\\"Task-{task_id}: Starting task\\") logger.debug(f\\"Task-{task_id}: Task details: {json.dumps(structured_data)}\\") # Simulate work time.sleep(2) structured_data[\\"step\\"] = 2 structured_data[\\"status\\"] = \\"completed\\" logger.info(f\\"Task-{task_id}: Task completed\\") logger.debug(f\\"Task-{task_id}: Task details: {json.dumps(structured_data)}\\") def main(): configure_logger() logger = logging.getLogger() logger.info(\\"Main thread: Starting application\\") threads = [] for i in range(2): t = threading.Thread(target=worker_thread, name=f\\"Worker-{i+1}\\", args=(i+1,)) threads.append(t) t.start() for t in threads: t.join() logger.info(\\"Main thread: Application finished\\") if __name__ == \'__main__\': main()"},{"question":"**Objective**: Implement and utilize a PyTorch DataLoader for a custom dataset. **Background**: In many machine learning tasks, data handling and preprocessing are as crucial as the model itself. PyTorch provides a powerful utility called DataLoader to iterate over datasets seamlessly and efficiently. For this assessment, you will create a custom dataset and utilize DataLoader to iterate over it. **Task**: 1. **Create a Custom Dataset**: Implement a custom dataset class `CustomDataset` by extending `torch.utils.data.Dataset`. This dataset should: - Load a dataset of your choice (e.g., CIFAR-10, MNIST, or a custom data file of random values). - Implement `__len__` method to return the size of the dataset. - Implement `__getitem__` method to return a data sample and its corresponding label. 2. **Implement DataLoader**: Create a DataLoader for the `CustomDataset`. The DataLoader should: - Shuffle the data at the beginning of each epoch. - Allow for batching of data samples. - Utilize multiple worker threads for loading. 3. **Iterate through DataLoader**: Write a function that takes the DataLoader and an epoch count as input, then iterates through the data batches for the given number of epochs, printing the shape of each batch (both data and labels). **Specifications**: - Use PyTorch\'s `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` modules. - Ensure your dataset class handles any necessary data preprocessing steps (e.g., normalization). - Your DataLoader should handle both training and validation splits if applicable. **Input**: - No direct input from the user. Load the data within your code. - You may use torchvision datasets or create a NumPy array of random values as your dataset. **Output**: - Print the shape of each batch\'s data and labels for each epoch. **Sample Structure**: ```python import torch from torch.utils.data import Dataset, DataLoader import numpy as np class CustomDataset(Dataset): def __init__(self, data, labels): # Initialize your data and labels here self.data = ... self.labels = ... def __len__(self): # Return the size of the dataset return len(self.data) def __getitem__(self, idx): # Return a data sample and its corresponding label return self.data[idx], self.labels[idx] def create_data_loader(dataset, batch_size=32, shuffle=True, num_workers=2): return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers) def iterate_through_dataloader(dataloader, num_epochs=1): for epoch in range(num_epochs): for i, (data, labels) in enumerate(dataloader): # Print the shape of each batch\'s data and labels print(f\'Epoch: {epoch}, Batch: {i}, Data shape: {data.shape}, Labels shape: {labels.shape}\') # Example usage: if __name__ == \\"__main__\\": # Create dummy data data = np.random.rand(1000, 3, 32, 32) # Assuming image data: 1000 samples of 3x32x32 labels = np.random.randint(0, 10, size=(1000,)) # 1000 labels for classification into 10 classes # Initialize dataset and dataloader dataset = CustomDataset(data, labels) dataloader = create_data_loader(dataset) # Iterate through dataloader iterate_through_dataloader(dataloader, num_epochs=2) ``` Ensure your code is well-commented and handles edge cases. You are encouraged to test with different dataset sizes and batch configurations.","solution":"import torch from torch.utils.data import Dataset, DataLoader import numpy as np class CustomDataset(Dataset): def __init__(self, data, labels): self.data = torch.from_numpy(data).float() self.labels = torch.from_numpy(labels).long() def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx], self.labels[idx] def create_data_loader(dataset, batch_size=32, shuffle=True, num_workers=2): return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers) def iterate_through_dataloader(dataloader, num_epochs=1): for epoch in range(num_epochs): print(f\'Starting epoch {epoch}\') for i, (data, labels) in enumerate(dataloader): print(f\'Epoch: {epoch}, Batch: {i}, Data shape: {data.shape}, Labels shape: {labels.shape}\')"},{"question":"# Question: Implementing a Custom MemoryView Utility in Python Objective Write a Python function `create_memoryview(obj)` that takes an object supporting the buffer interface and returns a memoryview object. Further, implement a utility function `memoryview_to_bytes(mview)` that converts a given memoryview object back into a bytes object. Requirements 1. The function `create_memoryview(obj)` should: - Accept an object that provides the buffer interface (e.g., bytes, bytearray). - Return a memoryview of the object. 2. The function `memoryview_to_bytes(mview)` should: - Accept a memoryview object. - Convert and return the memoryview content as a bytes object. 3. Write appropriate error handling to ensure the input object supports the buffer interface and that provided memoryview objects are valid. Example ```python def create_memoryview(obj): # Your implementation here def memoryview_to_bytes(mview): # Your implementation here # Example usage: # Creating a memoryview from a bytearray data = bytearray(b\\"Hello, World!\\") mview = create_memoryview(data) print(mview) # Expected: <memory at some_address> # Converting memoryview back to bytes bytes_data = memoryview_to_bytes(mview) print(bytes_data) # Expected: b\\"Hello, World!\\" ``` Constraints - The input object to `create_memoryview(obj)` must support the buffer interface (`bytes`, `bytearray`, and other similar objects). - Use of Python C API or low-level system functions is not allowed; utilize Python\'s built-in capabilities. - Ensure that your code handles exceptions and invalid inputs gracefully. Notes - This question requires a deep understanding of Python\'s memory management and buffer protocols. - Consider edge cases such as empty buffers or invalid memoryview objects.","solution":"def create_memoryview(obj): Creates and returns a memoryview object from the given object that supports the buffer interface. if not isinstance(obj, (bytes, bytearray)): raise TypeError(\\"Object must be of type bytes or bytearray\\") return memoryview(obj) def memoryview_to_bytes(mview): Converts and returns a bytes object from the given memoryview object. if not isinstance(mview, memoryview): raise TypeError(\\"Input must be a memoryview object\\") return bytes(mview)"},{"question":"# LZMA Compression and Decompression Challenge The LZMA algorithm is known for its excellent compression ratio. Your task is to implement two functions using the `lzma` module to compress and decompress data in memory. You will implement the following two functions: 1. `compress_data(data: bytes) -> bytes`: This function takes a `bytes` object and returns the compressed data as a `bytes` object. 2. `decompress_data(compressed_data: bytes) -> bytes`: This function takes a `bytes` object of compressed data and returns the decompressed data as a `bytes` object. Requirements: 1. Your `compress_data` function should use the default `.xz` container format for compression. 2. The `decompress_data` function should be able to handle the compressed data created by `compress_data` (i.e., using the `.xz` format). 3. The functions should handle any potential errors that may arise during compression or decompression (e.g., when data cannot be decompressed). Input and Output Format: - Both `compress_data` and `decompress_data` should take a single argument which is a `bytes` object. - Both functions should return a `bytes` object as described above. Constraints: - The input data for `compress_data` will be at most 10^6 bytes. - The input data for `decompress_data` will be the output of `compress_data`. - The functions should handle edge cases such as empty input data. Example Usage: ```python # Example usage of compress_data and decompress_data functions original_data = b\\"This is some data to be compressed\\" compressed_data = compress_data(original_data) print(type(compressed_data)) # output should be <class \'bytes\'> decompressed_data = decompress_data(compressed_data) print(decompressed_data) # output should be b\\"This is some data to be compressed\\" # Ensure decompressed data is the same as the original data assert decompressed_data == original_data, \\"Decompression did not yield the original data\\" ``` # Implementation ```python import lzma def compress_data(data: bytes) -> bytes: try: compressed = lzma.compress(data) return compressed except lzma.LZMAError as e: print(f\\"Compression failed: {e}\\") return b\\"\\" def decompress_data(compressed_data: bytes) -> bytes: try: decompressed = lzma.decompress(compressed_data) return decompressed except lzma.LZMAError as e: print(f\\"Decompression failed: {e}\\") return b\\"\\" ``` Your task is to complete the implementation of the `compress_data` and `decompress_data` functions to meet the requirements outlined above. Make sure to handle any exceptions that might occur during the compression and decompression processes.","solution":"import lzma def compress_data(data: bytes) -> bytes: Compresses the input bytes using LZMA algorithm and returns the compressed data. try: compressed = lzma.compress(data) return compressed except lzma.LZMAError as e: print(f\\"Compression failed: {e}\\") return b\\"\\" def decompress_data(compressed_data: bytes) -> bytes: Decompresses the input bytes using the LZMA algorithm and returns the decompressed data. try: decompressed = lzma.decompress(compressed_data) return decompressed except lzma.LZMAError as e: print(f\\"Decompression failed: {e}\\") return b\\"\\""},{"question":"# PyTorch Custom DataLoader Implementation **Objective:** Your task is to create a custom DataLoader that takes a list of file paths containing numerical data, reads the files, preprocesses the data (normalizing it), and then batches the data for training a neural network. # Instructions: 1. **Reading Data:** - Implement a method to read numerical data from each file. Assume each file contains a 2D array of floats stored as plain text with space-separated values. 2. **Data Normalization:** - Normalize the data from each file such that the values are scaled to have a mean of 0 and standard deviation of 1. 3. **Batching Data:** - Create a DataLoader that batches the data into specified sizes. - Ensure that shuffling of data is possible, providing an option to enable or disable shuffling. # Implementation Details: - **Function Signature:** ```python import torch from torch.utils.data import Dataset, DataLoader class CustomDataset(Dataset): def __init__(self, file_paths): # Initialize with a list of file paths pass def __len__(self): # Return the total number of samples pass def __getitem__(self, idx): # Return one sample of data (normalized) pass class CustomDataLoader: def __init__(self, dataset, batch_size, shuffle=False): # Initialize the DataLoader with dataset, batch_size, and shuffle option pass def __iter__(self): # Return an iterator for the DataLoader pass def __next__(self): # Return the next batch of data pass ``` - **Input:** - `file_paths`: List of file paths containing numerical data. - `batch_size`: Integer specifying the size of each batch. - `shuffle`: Boolean indicating whether to shuffle the data or not (default is `False`). - **Output:** - Batches of normalized data tensors. # Example Usage: ```python file_paths = [\\"data/file1.txt\\", \\"data/file2.txt\\", \\"data/file3.txt\\"] batch_size = 32 shuffle = True dataset = CustomDataset(file_paths) data_loader = CustomDataLoader(dataset, batch_size, shuffle) for batch in data_loader: print(batch) ``` # Constraints: - Assume all input files are well-formed and contain numerical data only. - Ensure memory efficiency when reading large files. - The performance of data loading and preprocessing should be optimized. # Evaluation Criteria: - Correctness of data reading and normalization. - Proper implementation of DataLoader with batching and shuffling. - Efficient handling of the data processing pipeline. Happy Coding!","solution":"import torch from torch.utils.data import Dataset, DataLoader import numpy as np import os class CustomDataset(Dataset): def __init__(self, file_paths): Initialize the dataset with a list of file paths. Read all files and normalize the data. self.data = [] for path in file_paths: if os.path.isfile(path): data = np.loadtxt(path) # Normalize data (mean 0, std 1) mean = np.mean(data) std = np.std(data) normalized_data = (data - mean) / std self.data.extend(normalized_data) self.data = np.array(self.data) def __len__(self): # Return the total number of samples return len(self.data) def __getitem__(self, idx): # Return one sample of data (normalized) return self.data[idx] class CustomDataLoader: def __init__(self, dataset, batch_size, shuffle=False): Initialize the DataLoader with dataset, batch size, and shuffle option. self.dataset = dataset self.batch_size = batch_size self.shuffle = shuffle self.indices = list(range(len(dataset))) self.current_idx = 0 if self.shuffle: np.random.shuffle(self.indices) def __iter__(self): # Return an iterator for the DataLoader self.current_idx = 0 if self.shuffle: np.random.shuffle(self.indices) return self def __next__(self): # Return the next batch of data if self.current_idx >= len(self.dataset): raise StopIteration batch_indices = self.indices[self.current_idx:self.current_idx+self.batch_size] batch = [self.dataset[i] for i in batch_indices] self.current_idx += self.batch_size return torch.tensor(batch) # Example usage if __name__ == \\"__main__\\": file_paths = [\\"data/file1.txt\\", \\"data/file2.txt\\", \\"data/file3.txt\\"] batch_size = 32 shuffle = True dataset = CustomDataset(file_paths) data_loader = CustomDataLoader(dataset, batch_size, shuffle) for batch in data_loader: print(batch)"},{"question":"Assume you are working on an application where you need to process an XML document containing information about books. Each book has details such as the title, author, price, and a flag indicating whether it is a bestseller. Write a function, `extract_bestseller_titles`, that takes in the filename of an XML document and returns a list of titles of bestseller books. You will utilize the `xml.dom.pulldom` module to parse the XML document efficiently. The input XML file is well-formed and follows the structure shown below: ```xml <library> <book> <title>The Art of Code</title> <author>Jane Doe</author> <price>59.99</price> <bestseller>true</bestseller> </book> <book> <title>Learning Python</title> <author>John Smith</author> <price>45.00</price> <bestseller>false</bestseller> </book> <!-- More book elements --> </library> ``` Function Signature ```python def extract_bestseller_titles(filename: str) -> list: pass ``` Input - `filename` (str): The path to the XML file containing book information. Output - (list): A list of strings representing the titles of bestseller books. Constraints - Only valid XML files will be provided. - The XML file will contain a `library` element as the root, with multiple `book` elements. - Each `book` element will contain exactly one `title`, `author`, `price`, and `bestseller` element. Example Given an XML file `books.xml` with the contents: ```xml <library> <book> <title>The Art of Code</title> <author>Jane Doe</author> <price>59.99</price> <bestseller>true</bestseller> </book> <book> <title>Learning Python</title> <author>John Smith</author> <price>45.00</price> <bestseller>false</bestseller> </book> <book> <title>Deep Learning 101</title> <author>Susan Green</author> <price>70.50</price> <bestseller>true</bestseller> </book> </library> ``` Calling `extract_bestseller_titles(\'books.xml\')` should return: ```python [\'The Art of Code\', \'Deep Learning 101\'] ``` Requirements - You must use the `xml.dom.pulldom` module for parsing. - Ensure that your parsing efficiently handles the size of the XML document. - Implement error handling for common XML parsing errors such as invalid file paths. Hints - Parse the XML file using the `pulldom.parse` function. - Loop through the events and nodes using the DOM event stream. - Use the `expandNode` method when a `bestseller` element is encountered to extract the book title if it is a bestseller.","solution":"from xml.dom import pulldom def extract_bestseller_titles(filename: str) -> list: Extracts and returns a list of titles of bestseller books from the given XML file. Args: filename (str): The path to the XML file containing book information. Returns: list: A list of titles of bestseller books. titles = [] try: # Open the XML document doc = pulldom.parse(filename) for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \'book\': # Expand the <book> node doc.expandNode(node) # Extract relevant data from the book node title = None bestseller = False for child in node.childNodes: if child.nodeType == child.ELEMENT_NODE: if child.tagName == \'title\': title = child.firstChild.data if child.tagName == \'bestseller\' and child.firstChild.data.lower() == \'true\': bestseller = True if bestseller and title: titles.append(title) except Exception as e: print(f\\"An error occurred: {e}\\") return titles"},{"question":"**Title**: Command-Line Tool Implementation using `getopt` **Problem Statement**: You are required to implement a Python script that processes command-line arguments using the `getopt` module. Your script should be able to parse both short and long options and handle various command-line arguments appropriately. The script will simulate a simple file processing operation based on the options provided. **Specifications**: 1. Your script should support the following command-line options: - `-i <filename>` or `--input=<filename>`: Specifies the input file. - `-o <filename>` or `--output=<filename>`: Specifies the output file. - `-v` or `--verbose`: Enables verbose mode. - `-h` or `--help`: Displays usage information and exits. 2. The script should handle and display a proper error message if: - An unrecognized option is provided. - An option requiring an argument is not provided with one. 3. After parsing the command-line options, the script should: - Print the names of the input and output files if provided. - Print a message indicating verbose mode is enabled if the `-v` or `--verbose` option is set. - Display the usage information and exit if the `-h` or `--help` option is provided. - Display an appropriate error message and exit if any error occurs during option parsing. **Input**: The input to the script should be similar to what would be passed as command-line arguments. For example: ``` python3 your_script.py -i input.txt -o output.txt -v ``` **Output**: Based on the options provided, the script should output appropriate messages. If the above example input is used, the output should be: ``` Input file: input.txt Output file: output.txt Verbose mode enabled ``` **Function Implementation**: You need to implement the function `main()` that will handle the argument parsing and perform the relevant actions based on the options provided. **Example Usage**: ```python import getopt import sys def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"hi:o:v\\", [\\"help\\", \\"input=\\", \\"output=\\", \\"verbose\\"]) except getopt.GetoptError as err: print(f\\"Error: {err}\\") print_usage() sys.exit(2) input_file = None output_file = None verbose = False for o, a in opts: if o in (\\"-h\\", \\"--help\\"): print_usage() sys.exit() elif o in (\\"-i\\", \\"--input\\"): input_file = a elif o in (\\"-o\\", \\"--output\\"): output_file = a elif o in (\\"-v\\", \\"--verbose\\"): verbose = True else: assert False, \\"Unhandled option\\" if input_file: print(f\\"Input file: {input_file}\\") if output_file: print(f\\"Output file: {output_file}\\") if verbose: print(\\"Verbose mode enabled\\") def print_usage(): print(\\"Usage: script.py [-i <filename>] [-o <filename>] [-v]\\") if __name__ == \\"__main__\\": main() ``` **Notes**: - Make sure to handle all edge cases and errors as described. - You can assume that the filenames provided as arguments are valid and accessible.","solution":"import getopt import sys def print_usage(): Prints the usage information for the script. print(\\"Usage: script.py [-i <filename>] [-o <filename>] [-v] [-h]\\") print(\\"-i <filename> or --input=<filename>: Specifies the input file.\\") print(\\"-o <filename> or --output=<filename>: Specifies the output file.\\") print(\\"-v or --verbose: Enables verbose mode.\\") print(\\"-h or --help: Displays this message and exits.\\") def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"hi:o:v\\", [\\"help\\", \\"input=\\", \\"output=\\", \\"verbose\\"]) except getopt.GetoptError as err: print(f\\"Error: {err}\\") print_usage() sys.exit(2) input_file = None output_file = None verbose = False for o, a in opts: if o in (\\"-h\\", \\"--help\\"): print_usage() sys.exit() elif o in (\\"-i\\", \\"--input\\"): input_file = a elif o in (\\"-o\\", \\"--output\\"): output_file = a elif o in (\\"-v\\", \\"--verbose\\"): verbose = True else: assert False, \\"Unhandled option\\" if input_file: print(f\\"Input file: {input_file}\\") if output_file: print(f\\"Output file: {output_file}\\") if verbose: print(\\"Verbose mode enabled\\") if __name__ == \\"__main__\\": main()"},{"question":"# Objective Implement a Python class that demonstrates the behaviors of descriptors as described in the documentation. This question assesses your understanding of Python descriptors, including data descriptors and methods. **Requirements:** 1. Implement a class `Descriptor` which will act as a base class for various types of descriptors. 2. Implement a `GetSetDescriptor` class that inherits from `Descriptor` and mimics the behavior of a get-set descriptor. 3. Implement a `MemberDescriptor` class that inherits from `Descriptor` and mimics the behavior of a member descriptor. 4. Implement a `MethodDescriptor` class that inherits from `Descriptor` and mimics the behavior of a method descriptor. 5. Implement a `ClassMethodDescriptor` class that inherits from `Descriptor` and mimics the behavior of a class method descriptor. 6. Implement functionality to differentiate between data descriptors and method descriptors. # Input and Output - The `Descriptor` class and its subclasses should define the following methods: - `__init__(self, name)`: Initialize the descriptor with a name. - `__get__(self, instance, owner)`: Retrieve the value or method associated with the descriptor. - `__set__(self, instance, value)`: Set the value for data descriptors (not applicable for method descriptors). - `__delete__(self, instance)`: Optional - delete the value associated with the descriptor (for data descriptors). - The `Descriptor` class should also implement a method: - `is_data_descriptor() -> bool`: Returns `True` if the descriptor is a data descriptor, otherwise `False`. # Constraints - Do not use the built-in `property` or `classmethod` decorators. - Your solution should work for Python 3.10 and above. # Example Here is an example of how your implementation could be used: ```python class MyClass: x = GetSetDescriptor(\'x\') y = MemberDescriptor(\'y\') z = MethodDescriptor(\'z\') w = ClassMethodDescriptor(\'w\') # Example usage: obj = MyClass() obj.x = 10 print(obj.x) # Should output 10 MyClass.y = 20 print(obj.y) # Should output 20 def method(self): return \\"method called\\" MyClass.z = method print(obj.z()) # Should output \\"method called\\" def class_method(cls): return \\"class method called\\" MyClass.w = class_method print(MyClass.w()) # Should output \\"class method called\\" print(MyClass.x.is_data_descriptor()) # Should output True print(MyClass.z.is_data_descriptor()) # Should output False ``` # Performance Requirements - The solution should handle setting and getting attributes for multiple instances of a class efficiently. You are allowed to use `type` to create dynamic classes if necessary. Your implementation should correctly differentiate between data and method descriptors. Good luck!","solution":"class Descriptor: def __init__(self, name): self.name = name def __get__(self, instance, owner): raise NotImplementedError def __set__(self, instance, value): raise NotImplementedError def __delete__(self, instance): raise NotImplementedError def is_data_descriptor(self): return hasattr(self, \'__set__\') class GetSetDescriptor(Descriptor): def __init__(self, name): super().__init__(name) self._values = {} def __get__(self, instance, owner): if instance is None: return self return self._values.get(instance, None) def __set__(self, instance, value): self._values[instance] = value def __delete__(self, instance): if instance in self._values: del self._values[instance] class MemberDescriptor(Descriptor): def __init__(self, name): super().__init__(name) def __get__(self, instance, owner): if instance is None: return self return getattr(instance, \\"__\\" + self.name, None) def __set__(self, instance, value): setattr(instance, \\"__\\" + self.name, value) class MethodDescriptor(Descriptor): def __init__(self, name): super().__init__(name) def __get__(self, instance, owner): if instance is None: return self return getattr(instance, self.name).__get__(instance, owner) def __set__(self, instance, value): raise AttributeError(\\"Cannot set value to a method descriptor\\") class ClassMethodDescriptor(Descriptor): def __init__(self, name): super().__init__(name) def __get__(self, instance, owner): if owner is None: owner = type(instance) method = getattr(owner, self.name) return method.__get__(None, owner) def __set__(self, instance, value): raise AttributeError(\\"Cannot set value to a class method descriptor\\")"},{"question":"Implementing a Secure File Transfer Utility with `uu` Module You are tasked with creating a secure file transfer utility that uses the `uu` module to encode and decode files. The utility should be robust, handling file operations securely, and provide informative error messages in case of issues. Function 1: `secure_transfer_encode` Write a function `secure_transfer_encode` which: 1. Takes the following parameters: - `input_file_path` (str): Path to the input file that needs to be encoded. - `output_file_path` (str): Path where the encoded file will be saved. - `backtick` (bool, optional): Specifies whether zeros should be represented by \'`\' instead of spaces. Defaults to `False`. 2. Reads the binary content of the input file, encodes it using `uu`, and writes the encoded content to the output file. 3. Handles exceptions and errors gracefully: - If the input file does not exist, raise a `FileNotFoundError` with an appropriate message. - If the output file cannot be written to, raise an `IOError` with an appropriate message. Function 2: `secure_transfer_decode` Write a function `secure_transfer_decode` which: 1. Takes the following parameters: - `input_file_path` (str): Path to the input uuencoded file. - `output_file_path` (str, optional): Path where the decoded file will be saved. If None, use the name from the uuencode header. - `quiet` (bool, optional): Silences warning messages if true. Defaults to `False`. 2. Reads the uuencoded content from the input file, decodes it using `uu`, and writes the decoded content to the output file. 3. Ensures that the file operations are handled securely: - If the input file does not exist, raise a `FileNotFoundError` with an appropriate message. - If the output file already exists, raise a `uu.Error` with an appropriate message. # Constraints 1. Ensure that the input and output file paths are valid strings. 2. Handle all possible edge cases, such as missing files, permission errors, and invalid data. 3. Write clean, readable, and well-documented code. # Example ```python # Example usage of secure_transfer_encode and secure_transfer_decode try: secure_transfer_encode(\'example.txt\', \'encoded_example.txt\', backtick=True) secure_transfer_decode(\'encoded_example.txt\', \'decoded_example.txt\', quiet=True) except Exception as e: print(f\\"An error occurred: {e}\\") ``` In this example: - `secure_transfer_encode` reads the content of \'example.txt\', encodes it, and writes the result to \'encoded_example.txt\'. - `secure_transfer_decode` reads the content of \'encoded_example.txt\', decodes it, and writes the result to \'decoded_example.txt\'.","solution":"import uu import os def secure_transfer_encode(input_file_path, output_file_path, backtick=False): Encodes the input file and saves the result in the output file. Parameters: input_file_path (str): Path to the input file that needs to be encoded. output_file_path (str): Path where the encoded file will be saved. backtick (bool, optional): Specifies whether zeros should be represented by \'`\' instead of spaces. Defaults to False. Raises: FileNotFoundError: If the input file does not exist. IOError: If the output file cannot be written to. try: with open(input_file_path, \'rb\') as in_file, open(output_file_path, \'wb\') as out_file: uu.encode(in_file, out_file, name=os.path.basename(input_file_path), mode=os.stat(input_file_path).st_mode, backtick=backtick) except FileNotFoundError: raise FileNotFoundError(f\\"The input file: {input_file_path} does not exist.\\") except IOError: raise IOError(f\\"Could not write to the output file: {output_file_path}.\\") def secure_transfer_decode(input_file_path, output_file_path=None, quiet=False): Decodes the uuencoded input file and saves the result in the output file. Parameters: input_file_path (str): Path to the input uuencoded file. output_file_path (str, optional): Path where the decoded file will be saved. If None, use the name from the uuencode header. quiet (bool, optional): Silences warning messages if true. Defaults to False. Raises: FileNotFoundError: If the input file does not exist. uu.Error: If the output file already exists. IOError: If decoding fails or file operations encounter errors. try: with open(input_file_path, \'rb\') as in_file: if output_file_path and os.path.exists(output_file_path): raise uu.Error(f\\"The output file: {output_file_path} already exists.\\") uu.decode(in_file, out_file=output_file_path, quiet=quiet) except FileNotFoundError: raise FileNotFoundError(f\\"The input file: {input_file_path} does not exist.\\") except IOError as e: raise IOError(f\\"Decoding failed: {e}\\")"},{"question":"**Problem Statement:** You are tasked with developing a utility script that compresses and decompresses text files using the `bz2` module in Python. Your script should be capable of handling large files efficiently by employing incremental compression and decompression. **Requirements:** 1. **Function Implementations:** - Implement a function `compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None` which: - Reads text data from `input_filename`. - Compresses the data incrementally using the specified `compresslevel` (defaulting to 9). - Writes the compressed data to `output_filename`. - Implement a function `decompress_file(input_filename: str, output_filename: str) -> None` which: - Reads compressed data from `input_filename`. - Decompresses the data incrementally. - Writes the decompressed data to `output_filename`. 2. **Input and Output:** - All filenames provided will be valid text files with appropriate read/write permissions. - The `compress_file` function should create a compressed file at the `output_filename` path. - The `decompress_file` function should create a decompressed text file at the `output_filename` path, ensuring the content matches the original text data. 3. **Constraints:** - You should read and write data in chunks of size 4096 bytes to handle large files efficiently. - The functions should handle edge cases gracefully, such as when incomplete data is provided or when files are empty. 4. **Error Handling:** - Ensure your functions raise appropriate exceptions for I/O errors and invalid compression levels. - The exceptions should provide clear error messages indicating the nature of the problem. **Example Usage:** ```python # Example text file content: \\"example.txt\\" # \\"The quick brown fox jumps over the lazy dog.\\" compress_file(\\"example.txt\\", \\"example.bz2\\") # The file \\"example.bz2\\" is now a bzip2-compressed file of \\"example.txt\\" decompress_file(\\"example.bz2\\", \\"example_decompressed.txt\\") # The file \\"example_decompressed.txt\\" should now have the same content as \\"example.txt\\" # Verify the content matches the original text file content: with open(\\"example.txt\\", \\"r\\") as original, open(\\"example_decompressed.txt\\", \\"r\\") as decompressed: assert original.read() == decompressed.read() ``` **Additional Notes:** - Consider adding logging to give feedback on the progress of compression and decompression, especially for large files. - Thoroughly test your implementations with various file sizes and content patterns to ensure reliability and efficiency.","solution":"import bz2 def compress_file(input_filename: str, output_filename: str, compresslevel: int = 9) -> None: if compresslevel < 1 or compresslevel > 9: raise ValueError(\\"compresslevel must be between 1 and 9\\") try: with open(input_filename, \'rb\') as input_file, bz2.BZ2File(output_filename, \'wb\', compresslevel=compresslevel) as output_file: while True: chunk = input_file.read(4096) if not chunk: break output_file.write(chunk) except FileNotFoundError: raise FileNotFoundError(f\\"File {input_filename} not found\\") except IOError as e: raise IOError(f\\"I/O error occurred: {e}\\") def decompress_file(input_filename: str, output_filename: str) -> None: try: with bz2.BZ2File(input_filename, \'rb\') as input_file, open(output_filename, \'wb\') as output_file: while True: chunk = input_file.read(4096) if not chunk: break output_file.write(chunk) except FileNotFoundError: raise FileNotFoundError(f\\"File {input_filename} not found\\") except IOError as e: raise IOError(f\\"I/O error occurred: {e}\\") except bz2.BZ2File as e: raise IOError(f\\"Failed to decompress: {e}\\")"},{"question":"Advanced Usage of PyTorch Autograd **Objective:** Assess the ability to implement and use advanced features of PyTorch\'s autograd module, including custom functions and functional APIs. --- You are required to implement a custom neural network layer in PyTorch and evaluate its performance using the autograd profiler. Your implementation will cover the following aspects: 1. Create a custom autograd function for a novel activation function. 2. Implement a simple neural network utilizing this custom function. 3. Compute the Jacobian of the network\'s output with respect to its inputs using functional APIs. 4. Use PyTorch\'s autograd profiler to measure and report the execution time for forward and backward passes of your network. # Task Breakdown 1. **Custom Autograd Function:** - Implement a custom autograd function named `MyActivation` which applies the following activation: `y = x^3 + 2*x`. - Ensure that your function correctly defines the `forward` and `backward` methods. 2. **Neural Network Implementation:** - Create a simple neural network class `MyNetwork` that contains one fully connected layer followed by the `MyActivation` function. - The network should take an input tensor of shape `(batch_size, input_dim)` and output a tensor of shape `(batch_size, output_dim)`. 3. **Jacobian Computation:** - Write a function `compute_jacobian` that computes the Jacobian of the network’s output with respect to its input using `torch.autograd.functional.jacobian`. 4. **Profiling:** - Use the autograd profiler to measure the execution time of the forward and backward passes of your network. - Print a summary of the profiling results. # Implementation Requirements - **Custom Autograd Function:** ```python import torch from torch.autograd import Function class MyActivation(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input ** 3 + 2 * input @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (3 * input ** 2 + 2) return grad_input ``` - **Neural Network Class:** ```python import torch.nn as nn class MyNetwork(nn.Module): def __init__(self, input_dim, output_dim): super(MyNetwork, self).__init__() self.fc = nn.Linear(input_dim, output_dim) def forward(self, x): x = self.fc(x) x = MyActivation.apply(x) return x ``` - **Jacobian Computation Function:** ```python def compute_jacobian(network, inputs): return torch.autograd.functional.jacobian(network, inputs) ``` - **Profiling Code:** ```python def profile_network(network, inputs): with torch.autograd.profiler.profile() as prof: outputs = network(inputs) loss = outputs.sum() loss.backward() print(prof.key_averages().table(sort_by=\\"self_cpu_time_total\\")) ``` # Expected Input and Output Formats - **Inputs:** - `network`: Instance of `MyNetwork`. - `inputs`: Input tensor of shape `(batch_size, input_dim)`. - **Outputs:** - Jacobian tensor of appropriate shape. - Printout of profiling results showing execution times. # Constraints and Specifications - The input tensor will have `requires_grad` set to `True`. - `batch_size`, `input_dim`, and `output_dim` will be small enough to fit in memory for testing. # Performance Requirements - Ensure the implementation is efficient in terms of memory and computation, but the main focus is correctness. **Note:** Use the provided code snippets as starting points for your implementation, ensuring all parts integrate well together. Good luck!","solution":"import torch from torch.autograd import Function # Custom autograd Function class MyActivation(Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) return input ** 3 + 2 * input @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = grad_output * (3 * input ** 2 + 2) return grad_input import torch.nn as nn # Simple Neural Network utilizing the custom MyActivation function class MyNetwork(nn.Module): def __init__(self, input_dim, output_dim): super(MyNetwork, self).__init__() self.fc = nn.Linear(input_dim, output_dim) def forward(self, x): x = self.fc(x) x = MyActivation.apply(x) return x # Function to compute the Jacobian of the network’s output with respect to its input def compute_jacobian(network, inputs): return torch.autograd.functional.jacobian(network, inputs) # Function to use PyTorch\'s autograd profiler def profile_network(network, inputs): with torch.autograd.profiler.profile() as prof: outputs = network(inputs) loss = outputs.sum() loss.backward() print(prof.key_averages().table(sort_by=\\"self_cpu_time_total\\"))"},{"question":"# Python Coding Assessment Question Custom Calculator Shell You will be creating a custom command-line calculator shell using Python\'s `cmd` module. The shell should support basic arithmetic operations, command history, and the ability to evaluate complex expressions. Follow the specifications below: Requirements: 1. **Subclass `cmd.Cmd`:** - Create a class `CalculatorShell` that inherits from `cmd.Cmd`. - Set the `prompt` attribute to `\\"(calc) \\"`. 2. **Implement Command Methods:** - Implement command methods for addition (`do_add`), subtraction (`do_sub`), multiplication (`do_mul`), and division (`do_div`). - Each method should take two arguments and print the result. - The commands should be invoked as `add 3 5`, `sub 10 7`, etc. - Implement the method `do_eval` to evaluate complex expressions. - The command should be invoked as `eval 2 + 3 * 5`. 3. **Command History:** - Enable command history so that users can navigate through the command history using the up and down arrow keys. 4. **Pre- and Post-Command Hooks:** - Override the `precmd` method to print the command to be executed. - Override the `postcmd` method to print \\"Command executed!\\" after a command finishes executing. 5. **Command Aliases and Help:** - Add an alias for the addition command, such that `sum 3 5` also invokes the `do_add` method. - Provide meaningful docstrings for each command to be displayed with the `help` command. 6. **Exit Command:** - Implement a `do_exit` method to exit the shell. - The command should be invoked as `exit`. - The method should return a true value to exit the command loop. Input: The inputs will be the commands entered in the calculator shell as described above. Output: The output should be the results of the commands executed, printed to the console. Example: ```python (calc) help Documented commands (type help <topic>): ======================================== add div eval exit mul sum sub (calc) add 3 5 Executing command: add 3 5 8 Command executed! ``` Implementation: ```python import cmd class CalculatorShell(cmd.Cmd): prompt = \\"(calc) \\" def do_add(self, args): Add two numbers: add 3 5 try: x, y = map(float, args.split()) print(x + y) except ValueError: print(\\"Invalid arguments\\") def do_sub(self, args): Subtract two numbers: sub 10 7 try: x, y = map(float, args.split()) print(x - y) except ValueError: print(\\"Invalid arguments\\") def do_mul(self, args): Multiply two numbers: mul 2 3 try: x, y = map(float, args.split()) print(x * y) except ValueError: print(\\"Invalid arguments\\") def do_div(self, args): Divide two numbers: div 10 2 try: x, y = map(float, args.split()) if y == 0: print(\\"Cannot divide by zero\\") else: print(x / y) except ValueError: print(\\"Invalid arguments\\") def do_eval(self, args): Evaluate an expression: eval 2 + 3 * 5 try: print(eval(args)) except Exception as e: print(f\\"Error evaluating expression: {e}\\") def do_exit(self, args): Exit the calculator: exit print(\\"Exiting calculator shell.\\") return True def precmd(self, line): print(f\\"Executing command: {line}\\") return line def postcmd(self, stop, line): print(\\"Command executed!\\") return stop def do_sum(self, args): Alias for add command return self.do_add(args) if __name__ == \'__main__\': CalculatorShell().cmdloop() ```","solution":"import cmd class CalculatorShell(cmd.Cmd): prompt = \\"(calc) \\" def do_add(self, args): Add two numbers: add 3 5 try: x, y = map(float, args.split()) print(x + y) except ValueError: print(\\"Invalid arguments\\") def do_sub(self, args): Subtract two numbers: sub 10 7 try: x, y = map(float, args.split()) print(x - y) except ValueError: print(\\"Invalid arguments\\") def do_mul(self, args): Multiply two numbers: mul 2 3 try: x, y = map(float, args.split()) print(x * y) except ValueError: print(\\"Invalid arguments\\") def do_div(self, args): Divide two numbers: div 10 2 try: x, y = map(float, args.split()) if y == 0: print(\\"Cannot divide by zero\\") else: print(x / y) except ValueError: print(\\"Invalid arguments\\") def do_eval(self, args): Evaluate an expression: eval 2 + 3 * 5 try: print(eval(args)) except Exception as e: print(f\\"Error evaluating expression: {e}\\") def do_exit(self, args): Exit the calculator: exit print(\\"Exiting calculator shell.\\") return True def precmd(self, line): print(f\\"Executing command: {line}\\") return line def postcmd(self, stop, line): print(\\"Command executed!\\") return stop def do_sum(self, args): Alias for add command return self.do_add(args) if __name__== \'__main__\': CalculatorShell().cmdloop()"},{"question":"**Question:** You are tasked with demonstrating your knowledge of the `telnetlib` module by creating a Telnet client that connects to a given Telnet server, executes a series of commands, and processes the output. Specifically, your task is to implement a `TelnetClient` class with the following specifications: 1. **Constructor (__init__)**: - Takes `host`, `port`, and `timeout` as parameters. - Initializes a `Telnet` object and connects to the specified host and port. 2. **Methods**: - `execute_commands(commands: list) -> str`: Takes a list of command strings, sends each command to the connected Telnet server, and collects the responses. Returns the combined responses as a single string. - `close_connection()`: Closes the Telnet connection. - `set_debug(level: int)`: Sets the debug level of the Telnet connection. - `login(user: str, password: str)`: Handles login to the Telnet server. Assumes the server will prompt with `b\\"login: \\"` for the username and `b\\"Password: \\"` for the password. **Constraints**: - Handle EOFError gracefully and ensure connections are closed properly in case of exceptions. - Raise appropriate errors if methods are called before establishing a connection. **Example Usage**: ```python from telnetlib import Telnet class TelnetClient: def __init__(self, host: str, port: int = 23, timeout: int = 10): self.host = host self.port = port self.timeout = timeout # Initialize and open the Telnet connection here def execute_commands(self, commands: list) -> str: # Send each command to the server and collect responses pass def close_connection(self): # Close the Telnet connection pass def set_debug(self, level: int): # Set the debug level for the Telnet connection pass def login(self, user: str, password: str): # Handle login to the Telnet server pass # Example usage: client = TelnetClient(\'localhost\') client.login(\'username\', \'password\') response = client.execute_commands([\'ls\', \'echo Hello\']) client.close_connection() print(response) ``` **Notes**: - The execution of commands should ensure the connection remains stable and data integrity is maintained. - Debug information should be adjustable through the `set_debug` method. - The client should demonstrate error handling, especially for typical Telnet errors. Document any assumptions you make and ensure your code is well-commented to explain your logic and decisions.","solution":"import telnetlib import time class TelnetClient: def __init__(self, host: str, port: int = 23, timeout: int = 10): self.host = host self.port = port self.timeout = timeout self.connection = None try: self.connection = telnetlib.Telnet(self.host, self.port, self.timeout) except Exception as e: raise ConnectionError(f\\"Failed to connect to {self.host}:{self.port} - {str(e)}\\") def execute_commands(self, commands: list) -> str: if not self.connection: raise ConnectionError(\\"No active connection to a Telnet server.\\") responses = [] try: for command in commands: self.connection.write(command.encode(\'ascii\') + b\\"n\\") time.sleep(1) # wait to get the response response = self.connection.read_very_eager().decode(\'ascii\') responses.append(response) except EOFError: raise ConnectionError(\\"Connection closed unexpectedly by the Telnet server.\\") return \\"n\\".join(responses) def close_connection(self): if self.connection: self.connection.close() self.connection = None def set_debug(self, level: int): if not self.connection: raise ConnectionError(\\"No active connection to a Telnet server.\\") self.connection.set_debuglevel(level) def login(self, user: str, password: str): if not self.connection: raise ConnectionError(\\"No active connection to a Telnet server.\\") try: self.connection.read_until(b\\"login: \\") self.connection.write(user.encode(\'ascii\') + b\\"n\\") self.connection.read_until(b\\"Password: \\") self.connection.write(password.encode(\'ascii\') + b\\"n\\") except EOFError: raise ConnectionError(\\"Connection closed unexpectedly during login by the Telnet server.\\")"},{"question":"**Objective:** Ensure that the input data in a PyTorch operation meets certain performance optimization conditions and apply a persistent algorithm if so. **Problem Statement:** Implement a function `optimize_for_gpu(input_tensor)` to check if the given `input_tensor` satisfies the conditions for selecting a persistent algorithm to optimize performance. If the conditions are met, the function should apply a mock persistent algorithm (for the sake of this exercise, feel free to simulate this with a placeholder operation). If the conditions are not met, the function should return an informative message indicating which condition(s) failed. **Function Signature:** ```python def optimize_for_gpu(input_tensor: torch.Tensor) -> torch.Tensor: pass ``` **Input:** - `input_tensor (torch.Tensor)`: A tensor to be checked and potentially optimized. The tensor could be of any size. **Output:** - `torch.Tensor`: The optimized tensor if conditions are met; otherwise, an informative message in the form of a string. **Constraints and Requirements:** 1. cuDNN should be enabled in your PyTorch environment. Ensure to check with `torch.backends.cudnn.enabled`. 2. The input tensor should be on a GPU. 3. The input tensor should have data type `torch.float16`. 4. The machine should have a V100 GPU. 5. The input tensor should not be in `PackedSequence` format. **Steps to follow in the function:** 1. Check if cuDNN is enabled (use `torch.backends.cudnn.enabled`). 2. Check if the tensor is on a GPU (use `input_tensor.is_cuda`). 3. Check if the tensor data type is `torch.float16` (use `input_tensor.dtype`). 4. Simulate checking for a V100 GPU. (This can be simplified for this exercise by assuming that if the tensor is on a GPU, it\'s on a V100). 5. Ensure the tensor is not in `PackedSequence` format (for simplicity, assume it isn’t). Provide appropriate messages for unmet conditions for easier debugging. **Example:** ```python import torch # Example Input tensor = torch.randn((10, 10), dtype=torch.float16).cuda() # Example Usage of the optimize_for_gpu function result = optimize_for_gpu(tensor) print(result) # Should output the tensor optimized or provide a message indicating which condition was not met. ``` **Notes:** - You might simulate the presence of a V100 GPU and `PackedSequence` format for the purpose of this question, as checking these programmatically without specific libraries or devices might be complex. - The function might include print statements for debugging purposes.","solution":"import torch def optimize_for_gpu(input_tensor: torch.Tensor) -> torch.Tensor: # Check if cuDNN is enabled if not torch.backends.cudnn.enabled: return \\"cuDNN is not enabled.\\" # Check if the tensor is on a GPU if not input_tensor.is_cuda: return \\"Input tensor is not on a GPU.\\" # Check if the tensor data type is float16 if input_tensor.dtype != torch.float16: return \\"Input tensor is not of type torch.float16.\\" # For simplicity, assume if tensor is on GPU, it\'s on a V100 # In a real-world scenario, device type should be verified. # Check if the tensor is in PackedSequence format # Assuming tensor is not in PackedSequence format for this exercise # Placeholder for optimization using a persistent algorithm # For the purpose of this exercise, we simulate this with a no-op optimized_tensor = input_tensor return optimized_tensor"},{"question":"# Directory Synchronization Tool Implement a Python function `sync_directories(src_dir, dest_dir)` that synchronizes the contents of two directories `src_dir` and `dest_dir`. The function should ensure that: 1. All files and subdirectories present in `src_dir` are replicated in `dest_dir` with the same contents. 2. Any files or subdirectories that are present in `dest_dir` but not in `src_dir` get deleted from `dest_dir`. 3. The function should utilize the `filecmp` module\'s capabilities to detect differences and `shutil` module for file operations. **Function Signature:** ```python def sync_directories(src_dir: str, dest_dir: str) -> None: ``` **Constraints:** - `src_dir` and `dest_dir` are valid directory paths. - Both directories can have nested subdirectories and files. - The function should handle file permissions and path issues gracefully. - Ensure minimal file operations by utilizing the caching mechanism in `filecmp`. **Input:** - `src_dir` (str): Path to the source directory. - `dest_dir` (str): Path to the destination directory. **Output:** - The function does not return anything. **Example:** ```python # Example directory structure for src and dest: # src_dir # ├── folder1 # │ └── file1.txt # └── folder2 # └── file2.txt # dest_dir before synchronization # dest_dir # ├── folder1 # │ └── old_file.txt # └── folder3 # └── file3.txt # After sync_directories(src_dir, dest_dir): # dest_dir # ├── folder1 # │ └── file1.txt # └── folder2 # └── file2.txt sync_directories(\'src_dir\', \'dest_dir\') ``` **Hints:** - Use `filecmp.dircmp` to get the comparison object for the directories. - Use attributes of `dircmp` such as `left_only`, `right_only`, `common_dirs`, `common_files`, `diff_files` to determine actions. - Remember to handle subdirectories recursively.","solution":"import os import shutil import filecmp def sync_directories(src_dir: str, dest_dir: str) -> None: Synchronizes the contents of src_dir with dest_dir. Replicates all files and subdirectories in src_dir to dest_dir and deletes any files or subdirectories in dest_dir that are not present in src_dir. # Ensure dest_dir exists if not os.path.exists(dest_dir): os.makedirs(dest_dir) def sync(src, dest): cmp = filecmp.dircmp(src, dest) # Copy files and directories from src to dest for item in cmp.left_only: src_path = os.path.join(src, item) dest_path = os.path.join(dest, item) if os.path.isdir(src_path): shutil.copytree(src_path, dest_path) else: shutil.copy2(src_path, dest_path) # Remove files and directories in dest that are not present in src for item in cmp.right_only: dest_path = os.path.join(dest, item) if os.path.isdir(dest_path): shutil.rmtree(dest_path) else: os.remove(dest_path) # Update files that are present in both but different for item in cmp.diff_files: src_path = os.path.join(src, item) dest_path = os.path.join(dest, item) shutil.copy2(src_path, dest_path) # Recursively sync common subdirectories for common_dir in cmp.common_dirs: sync(os.path.join(src, common_dir), os.path.join(dest, common_dir)) sync(src_dir, dest_dir)"},{"question":"**Question: Implementing Custom Exception Handling in Python** **Objective**: You are required to design a custom exception handling system that mirrors some of the functionalities described in the Python/C API documentation provided. **Task**: 1. Implement a custom exception class hierarchy in Python. 2. Implement functions to raise, catch, and manage these exceptions. **Details**: 1. **Custom Exception Classes**: - Implement a base exception class called `MyBaseException`. - Implement three derived exception classes: - `MyInputException` (inherits from `MyBaseException`) - `MyProcessingException` (inherits from `MyBaseException`) - `MyOutputException` (inherits from `MyBaseException`) 2. **Exception Handling Functions**: - Implement a function `raise_exception(exception_type, message)` that raises a specified exception from the custom hierarchy with a provided message. - Implement a function `process_exceptions(func)` that: - Takes a function `func` as an argument. - Executes the function within a try...except block. - Catches and categorizes exceptions based on the custom exception classes. - Logs the exception details to a list `exception_log` in the following format: `(<ExceptionClassName>, <ExceptionMessage>)`. - Returns the result of the function if no exception occurs. **Example**: ```python # Define custom exceptions class MyBaseException(Exception): pass class MyInputException(MyBaseException): pass class MyProcessingException(MyBaseException): pass class MyOutputException(MyBaseException): pass # Implement raising function def raise_exception(exception_type, message): # Your code here # Implement processing function exception_log = [] def process_exceptions(func): # Your code here # Test the functionality def test_function(): raise_exception(MyInputException, \\"Invalid input\\") result = process_exceptions(test_function) print(exception_log) # Output: [(\'MyInputException\', \'Invalid input\')] ``` **Constraints**: - You must use the given class names and function signatures. - The function `raise_exception` should raise an appropriate exception based on the provided type and message. **Hint**: - Use the `isinstance` function to check for exceptions in the `process_exceptions` function. **Evaluation**: - Correct implementation of the custom exception hierarchy. - Correct handling and raising of exceptions. - Proper logging of exception details. - The correctness of the output for different test cases.","solution":"# Define custom exceptions class MyBaseException(Exception): pass class MyInputException(MyBaseException): pass class MyProcessingException(MyBaseException): pass class MyOutputException(MyBaseException): pass # Implement raising function def raise_exception(exception_type, message): if not issubclass(exception_type, MyBaseException): raise TypeError(\\"Invalid exception type\\") raise exception_type(message) # Implement processing function exception_log = [] def process_exceptions(func): try: result = func() return result except MyBaseException as e: exception_log.append((type(e).__name__, str(e))) return None"},{"question":"# Advanced Logging with Filters and Contextual Information **Problem Statement:** You are required to implement a logging system for a hypothetical data processing application. This system has to support the following functionality: 1. Log messages to both console and a file. 2. Customize the console and file logs to have different message formats. 3. Filter out certain messages from being logged based on specific criteria. 4. Add contextual information (like username, IP address) dynamically to the log records. **Requirements:** 1. **Logger Configuration:** - Create a logger named `data_processor`. - Configure it to log DEBUG and higher-level messages. 2. **Handlers:** - Create a `FileHandler` named `file_handler` that logs messages of level DEBUG and higher to a file named `app.log`. Format the log messages with timestamps. - Create a `StreamHandler` named `console_handler` that logs messages of level INFO and higher to the console, without timestamps. 3. **Filters:** - Create a filter that will exclude any log messages containing the word \\"exclude\\". 4. **Contextual Information:** - Add context-specific information, such as `username` and `client_ip`, to log records dynamically. The `username` and `client_ip` values should be passed as dictionary parameters when logging. 5. **Main Functionality:** - Implement a function `process_data(context: dict)`: - This function should log some example messages illustrating different levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). - It should demonstrate the use of contextual information and filters. **Input:** The function `process_data(context: dict)` will receive a dictionary containing: - `username` (str): Username of the client. - `client_ip` (str): IP address of the client. **Output:** The output will be the log messages produced during the running of the `process_data` function. **Constraints:** - Use Python logging module. - Ensure the context information appears in the log records. Example Code Usage: ```python def setup_logging(): pass # Your logging setup code here def process_data(context): pass # Your processing and logging code here # Example context dictionary context = { \\"username\\": \\"test_user\\", \\"client_ip\\": \\"192.168.1.1\\" } # Setting up logging setup_logging() # Processing data with logs process_data(context) ``` **Performance Requirements:** - Efficient filtering and dynamic contextual logging are essential. Use the provided `context` dictionary in the `process_data` function for testing and demonstrating dynamic contextual logging.","solution":"import logging class ContextFilter(logging.Filter): def __init__(self, context): super().__init__() self.context = context def filter(self, record): record.username = self.context.get(\'username\') record.client_ip = self.context.get(\'client_ip\') return True def setup_logging(context): logger = logging.getLogger(\'data_processor\') logger.setLevel(logging.DEBUG) # Create file handler for logging to a file file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.DEBUG) file_formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(username)s - %(client_ip)s - %(message)s\') file_handler.setFormatter(file_formatter) # Create console handler for logging to the console console_handler = logging.StreamHandler() console_handler.setLevel(logging.INFO) console_formatter = logging.Formatter(\'%(levelname)s - %(username)s - %(client_ip)s - %(message)s\') console_handler.setFormatter(console_formatter) # Filter to exclude messages containing the word \\"exclude\\" exclude_filter = logging.Filter() exclude_filter.filter = lambda record: \'exclude\' not in record.getMessage() # Adding filters and handlers logger.addFilter(ContextFilter(context)) file_handler.addFilter(exclude_filter) console_handler.addFilter(exclude_filter) logger.addHandler(file_handler) logger.addHandler(console_handler) def process_data(context): logger = logging.getLogger(\'data_processor\') # Example logging with different levels logger.debug(\'Debug message\') logger.info(\'Info message\') logger.warning(\'Warning message\') logger.error(\'Error message\') logger.critical(\'Critical message\') logger.info(\'This is a message to exclude from logs\') # Example usage of the logging setup context = { \'username\': \'test_user\', \'client_ip\': \'192.168.1.1\' } setup_logging(context) process_data(context)"},{"question":"**Title**: Memory Usage Tracker **Description:** You need to create a Python function that takes a list of variables (of any type) and returns a detailed report of the memory usage of these variables. Additionally, your function should handle potential exceptions that may arise during size calculation gracefully and provide useful debugging information. **Function Signature:** ```python def memory_usage_report(variables: list) -> str: ``` **Input:** - `variables` (list): A list containing variables of any type (e.g., integers, strings, lists, dictionaries). **Output:** - A formatted string report containing the size in bytes of each variable and a summary of total memory usage. If any exception happens during the size calculation, the exception type, value, and traceback should be included in the output report instead of the size. **Requirements:** 1. Use `sys.getsizeof()` to calculate the size of each variable. 2. Use `sys.exc_info()` to catch and report exceptions if they occur. 3. Format the output to include the variable index, size (if retrievable), and exception details (if any). **Example:** ```python x = 10 y = \'hello\' z = [1, 2, 3] variables = [x, y, z] print(memory_usage_report(variables)) # Expected Output: Variable 0: 28 bytes Variable 1: 54 bytes Variable 2: 80 bytes Total memory usage: 162 bytes ``` # Constraints: - You may not use any external libraries except for `sys`. - You must gracefully handle exceptions that occur during size calculation. # Hints: - Use `sys.getsizeof()` to get the size of objects in bytes. - Use a try-except block to handle exceptions and `sys.exc_info()` to capture exception details. - Iterate through the list of variables and build the report string dynamically.","solution":"import sys import traceback def memory_usage_report(variables: list) -> str: report_lines = [] total_memory = 0 for index, var in enumerate(variables): try: size = sys.getsizeof(var) report_lines.append(f\\"Variable {index}: {size} bytes\\") total_memory += size except Exception as e: exc_type, exc_value, exc_traceback = sys.exc_info() tb_lines = traceback.format_exception(exc_type, exc_value, exc_traceback) tb_text = \'\'.join(tb_lines).strip() report_lines.append(f\\"Variable {index}: Exception - {exc_type.__name__}, {exc_value}nTraceback: {tb_text}\\") report_lines.append(f\\"Total memory usage: {total_memory} bytes\\") return \\"n\\".join(report_lines)"},{"question":"**File System and Process Management in Python** **Objective**: Implement a Python script that performs the following tasks leveraging the `os` module. 1. **File Creation and Writing**: - Create a new directory named `test_dir`. - Inside `test_dir`, create a file named `ProcessInfo.txt`. - Write information about the current process ID, parent process ID, and the environment variables (key-value pairs) to `ProcessInfo.txt`. 2. **Process Creation and Environment Variable Manipulation**: - Create a new process that runs a simple Python script. This new process should inherit the environment variables of the parent process but should add a new environment variable named `NEW_VAR` with a value of your choice. - The new Python script (child process) should read `NEW_VAR` and append its value to `ProcessInfo.txt`. 3. **Deleting Files and Directories**: - Once the child process has completed, delete the `ProcessInfo.txt` file and the `test_dir` directory. **Constraints**: - Use the `os` module functions such as `os.mkdir()`, `os.open()`, `os.fork()`, `os.execvp()`, `os.environ`, and `os.remove()`. - Ensure the usage of appropriate error handling for file and process operations. **Input and Output** - **Input**: No explicit input from the user. Use hardcoded values for file names and environmental variable. - You may assume all operations are running on a Unix-like system that supports `os.fork()`. **Detailed Steps**: 1. **Step 1**: Create the directory `test_dir`. 2. **Step 2**: Create the file `ProcessInfo.txt` within `test_dir`. - Add current and parent process info and environment variables to the file. 3. **Step 3**: Use `os.fork()` to create a new process. - In the child process: - Add a new environment variable. - Run a simple Python command (`os.execvp()` recommended) to write the new environment variable to the file. - Ensure the parent process waits for the child process to complete. 4. **Step 4**: After the child process completes, - Remove the file `ProcessInfo.txt`. - Remove the directory `test_dir`. **Submission Requirements** - Python script implementing the above tasks. - Ensure that the script is well-documented and includes necessary error handling. Example Output *Directory and file creation are done silently, no output* After running the script, the `test_dir` should be created temporarily, `ProcessInfo.txt` should have the logged process information which will be deleted by the end of the script. **Note**: Even though there is no explicit input or output, you can add print statements to demonstrate the process flow and result.","solution":"import os import sys def create_directory(directory_name): try: os.mkdir(directory_name) except FileExistsError: pass def write_process_info(file_path): with open(file_path, \'w\') as file: pid = os.getpid() ppid = os.getppid() env_vars = os.environ file.write(f\\"Process ID: {pid}n\\") file.write(f\\"Parent Process ID: {ppid}n\\") file.write(\\"Environment Variables:n\\") for key, value in env_vars.items(): file.write(f\\"{key}: {value}n\\") def append_new_var_to_file(file_path, new_var_value): with open(file_path, \'a\') as file: file.write(f\\"nNEW_VAR: {new_var_value}n\\") def child_process(file_path, new_var_value): os.environ[\'NEW_VAR\'] = new_var_value append_new_var_to_file(file_path, os.environ[\'NEW_VAR\']) sys.exit(0) def main(): directory_name = \'test_dir\' file_name = \'ProcessInfo.txt\' file_path = os.path.join(directory_name, file_name) new_var_value = \'HelloWorld\' # Step 1: Create the directory create_directory(directory_name) # Step 2: Write process info to file write_process_info(file_path) # Step 3: Fork the process pid = os.fork() if pid == 0: # Child process child_process(file_path, new_var_value) else: # Parent process os.wait() # Step 4: Clean up os.remove(file_path) os.rmdir(directory_name) if __name__ == \'__main__\': main()"},{"question":"# Python Version Encoding and Decoding **Objective**: Implement a Python class `PythonVersion` that can encode and decode Python version numbers based on the provided macros. Class `PythonVersion` **Methods:** 1. **__init__(self, version_hex: int)** - Initializes the `PythonVersion` instance with a version encoded as a 32-bit hexadecimal integer. - Parses and sets the attributes `major`, `minor`, `micro`, `release_level`, and `release_serial` based on the input integer. 2. **__str__(self) -> str** - Returns a string representation of the Python version in the format `major.minor.micro` followed by the release level and serial (e.g., `3.4.1a2`). 3. **to_hex(self) -> int** - Returns the encoded 32-bit hexadecimal integer based on the instance\'s attributes. **Input:** - Version number encoded as a 32-bit hexadecimal integer. **Output:** - String representation of the version number. - Encoded 32-bit hexadecimal integer for the given version. **Constraints:** - `version_hex` should be a valid 32-bit integer. - Release levels are: \'0xA\' for alpha, \'0xB\' for beta, \'0xC\' for release candidate, and \'0xF\' for the final release. **Example:** ```python v = PythonVersion(0x030401a2) print(str(v)) # Output: \\"3.4.1a2\\" print(v.to_hex()) # Output: \\"50648258\\" or \\"0x030401a2\\" (This is the hexadecimal representation) ``` Hints: - Use bit manipulation to extract and encode the different parts of the version. - You may find the Python built-in function `hex()` and `int()` useful for this task. Write a Python class `PythonVersion` with the described methods to pass the provided examples.","solution":"class PythonVersion: def __init__(self, version_hex: int): self.version_hex = version_hex self.major = (version_hex >> 24) & 0xFF self.minor = (version_hex >> 16) & 0xFF self.micro = (version_hex >> 8) & 0xFF self.release_level_code = (version_hex >> 4) & 0xF self.release_serial = version_hex & 0xF release_levels = {0xA: \'a\', 0xB: \'b\', 0xC: \'rc\', 0xF: \'\'} self.release_level = release_levels[self.release_level_code] def __str__(self): if self.release_level: return f\\"{self.major}.{self.minor}.{self.micro}{self.release_level}{self.release_serial}\\" else: return f\\"{self.major}.{self.minor}.{self.micro}\\" def to_hex(self) -> int: return (self.major << 24) | (self.minor << 16) | (self.micro << 8) | (self.release_level_code << 4) | self.release_serial"},{"question":"# **Coding Assessment Question** Web Navigation Utility You are required to create a Python utility that helps users manage and open multiple URLs using the `webbrowser` module. This utility should be able to perform the following functions: 1. **Open a single URL** - The URL should open in a new tab. 2. **Open multiple URLs in separate tabs or windows** - The user should specify whether to open in tabs or windows. 3. **Search and open a list of URLs** - Open URLs containing a specific keyword in their full address. Requirements 1. **Function: `open_url(url: str) -> None`** - **Input:** A string `url` representing the web address. - **Output:** None. - **Functionality:** Opens the given `url` in a new browser tab. 2. **Function: `open_multiple_urls(urls: list[str], mode: str) -> None`** - **Input:** - A list `urls` of strings, each string being a web address. - A string `mode` which can be `\\"tab\\"` or `\\"window\\"`. - **Output:** None. - **Functionality:** Opens each URL in the list in a new tab if `mode` is `\\"tab\\"`, or in new windows if `mode` is `\\"window\\"`. 3. **Function: `search_and_open_urls(urls: list[str], keyword: str) -> None`** - **Input:** - A list `urls` of strings, each string being a web address. - A string `keyword` representing a search term. - **Output:** None. - **Functionality:** Opens URLs that contain the `keyword` in their address in new tabs. Constraints 1. Assume all URLs are valid and properly formatted. 2. Use the default web browser of the operating system. 3. Keywords are case-insensitive. 4. The `mode` parameter in `open_multiple_urls` function can only be `\\"tab\\"` or `\\"window\\"`. Example ```python # Function implementations def open_url(url: str) -> None: webbrowser.open_new_tab(url) def open_multiple_urls(urls: list[str], mode: str) -> None: for url in urls: if mode == \\"tab\\": webbrowser.open_new_tab(url) elif mode == \\"window\\": webbrowser.open_new(url) def search_and_open_urls(urls: list[str], keyword: str) -> None: keyword = keyword.lower() for url in urls: if keyword in url.lower(): webbrowser.open_new_tab(url) # Usage Example: urls = [\\"https://www.python.org\\", \\"https://docs.python.org/3/\\", \\"https://www.github.com\\"] open_url(\\"https://www.python.org\\") open_multiple_urls(urls, \\"tab\\") search_and_open_urls(urls, \\"python\\") ``` Ensure your solution adheres to the constraints and demonstrates usage of the `webbrowser` module effectively.","solution":"import webbrowser def open_url(url: str) -> None: Opens the given URL in a new browser tab. :param url: The URL to open. webbrowser.open_new_tab(url) def open_multiple_urls(urls: list[str], mode: str) -> None: Opens multiple URLs in tabs or windows. :param urls: List of URLs to open. :param mode: Mode to open URLs - \\"tab\\" or \\"window\\". for url in urls: if mode == \\"tab\\": webbrowser.open_new_tab(url) elif mode == \\"window\\": webbrowser.open_new(url) def search_and_open_urls(urls: list[str], keyword: str) -> None: Opens URLs containing a specific keyword in their address in new tabs. :param urls: List of URLs to search. :param keyword: The keyword to search for in the URLs. keyword = keyword.lower() for url in urls: if keyword in url.lower(): webbrowser.open_new_tab(url)"},{"question":"**Question: Dining Philosophers Problem Using asyncio Synchronization Primitives** The Dining Philosophers Problem is a classic synchronization problem. There are five philosophers sitting around a circular dining table. Each philosopher alternates between thinking and eating. There are five chopsticks placed on the table, one between each pair of philosophers. A philosopher needs both left and right chopsticks to eat, but can only pick one chopstick at a time. The problem is to find a way to ensure that no philosopher will be starved or deadlocked. Your task is to implement a solution to the Dining Philosophers Problem using asyncio synchronization primitives (Lock, Event, Condition, Semaphore, BoundedSemaphore) to guarantee that no philosopher will starve or deadlock. # Requirements 1. **Initialize**: * Five philosophers (represented as asyncio tasks). * Five chopsticks (represented as asyncio Locks). 2. **Define the Philosopher Task**: * Each philosopher should repeatedly think, pick up their chopsticks, eat, and put down their chopsticks. * Picking up chopsticks should be done by acquiring the locks associated with the chopsticks. 3. **Ensure No Deadlocks**: * Implement the solution in such a way that it avoids deadlocks, ensuring no philosopher is left waiting indefinitely. # Function Signature You need to implement the following function: ```python import asyncio async def dining_philosophers(): pass # Your implementation here # Example usage within an asyncio event loop if __name__ == \\"__main__\\": asyncio.run(dining_philosophers()) ``` # Constraints - Each philosopher should alternate between thinking (simulated using asyncio.sleep) for a random time (1-3 seconds) and eating (simulated using asyncio.sleep) for a random time (1-2 seconds). - Use asyncio Locks to represent chopsticks. - Use additional asyncio synchronization primitives (such as Semaphore or Condition) if necessary to prevent deadlocks. # Input and Output There is no input to the function other than implicitly configured within the function itself. The function `dining_philosophers` should run indefinitely, simulating the philosophers\' thinking and eating cycles while ensuring no deadlocks occur. Implement appropriate logging or print statements to observe the philosophers\' state changes (thinking, eating). Example log entries could be: ``` Philosopher 1 is thinking. Philosopher 1 is hungry. Philosopher 1 picked up left chopstick. Philosopher 1 picked up right chopstick. Philosopher 1 is eating. Philosopher 1 put down right chopstick. Philosopher 1 put down left chopstick. ``` This will help in understanding the philosophers\' states and ensuring they do not starve or deadlock. # Performance Requirements Ensure that the implementation is efficient and avoids unnecessary blocking, enabling all philosophers to eat regularly.","solution":"import asyncio import random class Philosopher: def __init__(self, index, left_chopstick, right_chopstick, waiter): self.index = index self.left_chopstick = left_chopstick self.right_chopstick = right_chopstick self.waiter = waiter async def dine(self): while True: await self.think() await self.eat() async def think(self): print(f\'Philosopher {self.index} is thinking.\') await asyncio.sleep(random.uniform(1, 3)) async def eat(self): print(f\'Philosopher {self.index} is hungry.\') async with self.waiter: # Ensure mutual exclusivity at the waiter level async with self.left_chopstick: print(f\'Philosopher {self.index} picked up left chopstick.\') async with self.right_chopstick: print(f\'Philosopher {self.index} picked up right chopstick.\') print(f\'Philosopher {self.index} is eating.\') await asyncio.sleep(random.uniform(1, 2)) print(f\'Philosopher {self.index} put down right chopstick.\') print(f\'Philosopher {self.index} put down left chopstick.\') async def dining_philosophers(): num_philosophers = 5 chopsticks = [asyncio.Lock() for _ in range(num_philosophers)] waiter = asyncio.Semaphore(num_philosophers - 1) # Allows at most (n-1) philosophers to try to eat simultaneously philosophers = [Philosopher(i, chopsticks[i], chopsticks[(i + 1) % num_philosophers], waiter) for i in range(num_philosophers)] await asyncio.gather(*(philosopher.dine() for philosopher in philosophers)) if __name__ == \\"__main__\\": asyncio.run(dining_philosophers())"},{"question":"# Question: Implementing a Custom Matrix Class You are required to implement a custom matrix class in Python, which must support the following functionalities: 1. **Initialization**: The matrix should be initialized using a list of lists, where each sub-list represents a row of the matrix. 2. **Element-wise Addition**: Implement the `__add__` method to support element-wise addition of two matrices of the same dimensions. 3. **Element-wise Multiplication**: Implement the `__mul__` method to support element-wise multiplication of two matrices of the same dimensions. 4. **Matrix Transposition**: Implement a method `transpose` to return a new matrix which is the transpose of the current matrix. 5. **Row and Column Slicing**: Implement the `__getitem__` method to allow slicing to retrieve specific rows or columns of the matrix. 6. **String Representation**: Implement the `__str__` method to return a string representation of the matrix for easy printing. **Requirements:** - Use list comprehensions where applicable. - Implement generator expressions to handle large data efficiently during the transposition. - Ensure that the methods handle edge cases, such as empty matrices and incompatible dimensions for operations. - Use proper error handling to raise `ValueError` where operations cannot be completed. # Function Descriptions - **__init__(self, data)** - **Input**: `data` (List of lists) - A 2D list where each sublist represents a row. - **__add__(self, other)** - **Input**: `other` (Another Matrix instance) - **Output**: A new Matrix instance containing the element-wise sum. - **__mul__(self, other)** - **Input**: `other` (Another Matrix instance) - **Output**: A new Matrix instance containing the element-wise product. - **transpose(self)** - **Output**: A new Matrix instance which is the transpose of the current matrix. - **__getitem__(self, key)** - **Input**: `key` (Integer index or a slice) - To retrieve rows or columns. - **Output**: List representing the requested row or column. - **__str__(self)** - **Output**: String representation of the matrix. **Constraints:** - Matrices involved in operations should have compatible dimensions for those operations. If not, raise a `ValueError`. - The `data` provided during initialization should be a well-formed 2D list. If not, raise a `ValueError`. # Example ```python # Create two matrices matrix1 = Matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) matrix2 = Matrix([[9, 8, 7], [6, 5, 4], [3, 2, 1]]) # Element-wise addition matrix_sum = matrix1 + matrix2 print(matrix_sum) # Element-wise multiplication matrix_product = matrix1 * matrix2 print(matrix_product) # Transpose matrix_transpose = matrix1.transpose() print(matrix_transpose) # Row and column slicing row = matrix1[1] # Should return [4, 5, 6] column = matrix1[:,1] # Should return [2, 5, 8] print(row, column) ``` # Solution Outline Here\'s a brief outline of how you can structure the `Matrix` class. ```python class Matrix: def __init__(self, data): # Initialize the matrix with a 2D list. pass def __add__(self, other): # Implement element-wise addition. pass def __mul__(self, other): # Implement element-wise multiplication. pass def transpose(self): # Implement transpose using generator expression. pass def __getitem__(self, key): # Implement row and column slicing. pass def __str__(self): # Implement string representation for easy printing. pass ``` # Your Task Implement the full `Matrix` class as described above. Ensure to use list comprehensions and generator expressions appropriately and handle edge cases with proper error messages.","solution":"class Matrix: def __init__(self, data): if not all(isinstance(row, list) for row in data): raise ValueError(\\"Data should be a list of lists.\\") self.data = data self.rows = len(data) self.cols = len(data[0]) if data else 0 if not all(len(row) == self.cols for row in data): raise ValueError(\\"All rows must have the same number of columns.\\") def __add__(self, other): if not isinstance(other, Matrix) or self.rows != other.rows or self.cols != other.cols: raise ValueError(\\"Matrices should have the same dimensions for addition.\\") return Matrix([[self.data[i][j] + other.data[i][j] for j in range(self.cols)] for i in range(self.rows)]) def __mul__(self, other): if not isinstance(other, Matrix) or self.rows != other.rows or self.cols != other.cols: raise ValueError(\\"Matrices should have the same dimensions for multiplication.\\") return Matrix([[self.data[i][j] * other.data[i][j] for j in range(self.cols)] for i in range(self.rows)]) def transpose(self): return Matrix([[self.data[j][i] for j in range(self.rows)] for i in range(self.cols)]) def __getitem__(self, key): if isinstance(key, int): return self.data[key] elif isinstance(key, tuple) and key[0] == slice(None): col_idx = key[1] return [self.data[i][col_idx] for i in range(self.rows)] else: raise ValueError(\\"Invalid indexing method.\\") def __str__(self): return \\"n\\".join([\\" \\".join(map(str, row)) for row in self.data])"},{"question":"Consider a scenario where you are tasked with processing raw email messages obtained from different sources. You need to write a Python function to parse these email messages and extract specific information from them. # Objective Implement a function `extract_email_info(data: Union[bytes, str], source_type: str, subpart_mime_type: str) -> Dict[str, str]` that parses the email message and extracts the following information: - Sender email (`From` header) - Recipient email (`To` header) - Email subject (`Subject` header) - Body of the email or specific subpart content if it\'s a multipart message # Constraints - The `data` parameter represents the raw email message, and its type may vary based on the source: - If `source_type` is `\\"bytes\\"`, `data` will be a bytes-like object. - If `source_type` is `\\"string\\"`, `data` will be a string. - The `subpart_mime_type` parameter is a string representing the MIME type of the subpart whose content should be extracted (e.g., `\\"text/plain\\"` for plain text parts). # Function Signature ```python from typing import Union, Dict def extract_email_info(data: Union[bytes, str], source_type: str, subpart_mime_type: str) -> Dict[str, str]: pass ``` # Expected Output The function should return a dictionary with the following keys and their corresponding values: - `\\"From\\"` - `\\"To\\"` - `\\"Subject\\"` - `\\"Body\\"` or the content of the specified MIME type subpart # Example Usage ```python # Example Email Data email_data_bytes = b\\"From: sender@example.comnTo: recipient@example.comnSubject: Test EmailnnThis is the body of the email.\\" email_data_string = \\"From: sender@example.comnTo: recipient@example.comnSubject: Test EmailnnThis is the body of the email.\\" # Example Calls print(extract_email_info(email_data_bytes, \\"bytes\\", \\"text/plain\\")) # Output: {\'From\': \'sender@example.com\', \'To\': \'recipient@example.com\', \'Subject\': \'Test Email\', \'Body\': \'This is the body of the email.\'} print(extract_email_info(email_data_string, \\"string\\", \\"text/plain\\")) # Output: {\'From\': \'sender@example.com\', \'To\': \'recipient@example.com\', \'Subject\': \'Test Email\', \'Body\': \'This is the body of the email.\'} ``` # Notes - When dealing with multipart messages, you need to check if the message is multipart and retrieve the subpart with the specified MIME type. - If the `Body` or specified MIME type subpart content is not found, return an appropriate message indicating the absence of the content.","solution":"from typing import Union, Dict import email from email import policy from email.parser import BytesParser, Parser def extract_email_info(data: Union[bytes, str], source_type: str, subpart_mime_type: str) -> Dict[str, str]: if source_type == \\"bytes\\": msg = BytesParser(policy=policy.default).parsebytes(data) elif source_type == \\"string\\": msg = Parser(policy=policy.default).parsestr(data) else: raise ValueError(\\"source_type must be either \'bytes\' or \'string\'\\") email_info = { \\"From\\": msg.get(\\"From\\", \\"\\"), \\"To\\": msg.get(\\"To\\", \\"\\"), \\"Subject\\": msg.get(\\"Subject\\", \\"\\"), \\"Body\\": \\"\\", } # If the message is multipart, we need to find the correct subpart if msg.is_multipart(): for part in msg.walk(): if part.get_content_type() == subpart_mime_type: email_info[\\"Body\\"] = part.get_payload(decode=True).decode(part.get_content_charset(failobj=\'utf-8\')) break else: if msg.get_content_type() == subpart_mime_type: email_info[\\"Body\\"] = msg.get_payload(decode=True).decode(msg.get_content_charset(failobj=\'utf-8\')) else: email_info[\\"Body\\"] = \\"Content of the specified MIME type not found.\\" return email_info"},{"question":"Background In machine learning, correctly encoding labels is crucial for algorithm training and evaluation. `scikit-learn` provides several transformer classes to help preprocess labels, including `LabelEncoder`, `LabelBinarizer`, and `MultiLabelBinarizer`. Understanding how and when to use these tools is key to creating effective machine learning models. Problem Statement You are tasked with creating a function to preprocess labels for a multiclass classification problem. Your function should accept a list of labels, decide whether to binarize or encode them based on the nature of the labels, and return the transformed labels along with the classes. Specifically: - If the labels contain only numeric values, the function should use `LabelBinarizer`. - If the labels are non-numeric, the function should use `LabelEncoder`. Function Signature ```python from typing import List, Tuple, Union import numpy as np def preprocess_labels(labels: List[Union[int, str]]) -> Tuple[np.ndarray, np.ndarray]: pass ``` Input - `labels`: A list of labels which can contain either integers or strings. Each label represents a class in a multiclass classification problem. Output - A tuple containing: - A numpy array representing the transformed labels. - A numpy array of containing the classes after transformation. Constraints - The list of labels will always have at least one label. - There will be no mix of numeric and non-numeric labels in the same list. Example ```python labels_numeric = [1, 2, 2, 6] transformed_labels_numeric, classes_numeric = preprocess_labels(labels_numeric) print(transformed_labels_numeric) # Output: [[1 0 0] # [0 1 0] # [0 1 0] # [0 0 1]] print(classes_numeric) # Output: [1 2 6] labels_non_numeric = [\\"cat\\", \\"dog\\", \\"dog\\", \\"fish\\"] transformed_labels_non_numeric, classes_non_numeric = preprocess_labels(labels_non_numeric) print(transformed_labels_non_numeric) # Output: [0 1 1 2] print(classes_non_numeric) # Output: [\'cat\' \'dog\' \'fish\'] ``` Evaluation Your implementation will be evaluated based on: - Correctness: The function should return the correct transformed labels and classes. - Efficiency: The function should handle label lists efficiently. - Readability: The code should be well-organized and easy to understand. Notes To complete this task, you may find these references from the `scikit-learn` documentation useful: - `sklearn.preprocessing.LabelBinarizer` - `sklearn.preprocessing.LabelEncoder` Good luck!","solution":"from typing import List, Tuple, Union import numpy as np from sklearn.preprocessing import LabelEncoder, LabelBinarizer def preprocess_labels(labels: List[Union[int, str]]) -> Tuple[np.ndarray, np.ndarray]: if all(isinstance(label, int) for label in labels): lb = LabelBinarizer() transformed_labels = lb.fit_transform(labels) classes = lb.classes_ else: le = LabelEncoder() transformed_labels = le.fit_transform(labels) classes = le.classes_ return transformed_labels, classes"},{"question":"**Objective:** Implement a custom iterator using `itertools` to solve a specific problem. **Problem Statement:** Consider the following task: You are given a large dataset of transactions in the form of a list of amounts. Your goal is to: 1. **Filter out** transactions that are below a minimum value threshold. 2. **Accumulate** the remaining transactions to get a running total of the amount over time. 3. Generate a **cumulative report** every `n` transactions, displaying the transaction amounts and their running totals up to that point. **Function Signature:** ```python def transaction_report(transactions: list, min_value: float, report_freq: int) -> list: pass ``` **Input:** - `transactions`: List of floats representing transaction amounts. - `min_value`: Float representing the minimum value threshold to filter transactions. - `report_freq`: Integer representing the number of transactions to include in each report section. **Output:** - Return a list of tuples, where each tuple contains: - A list of transactions for that report segment. - The running totals of those transactions. **Constraints:** - Only transactions with values greater than or equal to `min_value` should be considered. - The output list should include sections with up to `report_freq` transactions. - If there are fewer than `report_freq` transactions remaining at the end, include them in the final segment. **Example:** ```python transactions = [100, 50, 20, 200, 500, 10, 300, 40] min_value = 50 report_freq = 3 expected_output = [ ([100, 50, 200], [100, 150, 350]), ([500, 300], [850, 1150]) ] assert transaction_report(transactions, min_value, report_freq) == expected_output ``` # Implementation Requirements - Use appropriate `itertools` functions to achieve the solution. - Ensure the solution is efficient and handles large datasets. - Include error handling for edge cases such as empty transaction lists, and invalid input values for `min_value` and `report_freq`. # Hints - Use `itertools.filterfalse` or `itertools.filter` to filter transactions. - Use `itertools.accumulate` to calculate running totals. - Use `itertools.islice` and other necessary functions to segment the transactions efficiently. Your task is to implement the `transaction_report` function that meets the requirements outlined above.","solution":"import itertools def transaction_report(transactions, min_value, report_freq): Generates a transaction report filtering transactions below a certain value and reporting the running total every `report_freq` transactions. Parameters: transactions (list): List of transaction amounts (floats). min_value (float): Minimum value threshold to filter transactions. report_freq (int): Number of transactions to include in each report section. Returns: list: A list of tuples, where each tuple contains a list of transactions for that report segment and their running totals. # Filter transactions based on min_value filtered_transactions = filter(lambda x: x >= min_value, transactions) # Generate cumulative totals cumulative_totals = list(itertools.accumulate(filtered_transactions)) # Re-filter for the same filtered transactions filtered_transactions = list(filter(lambda x: x >= min_value, transactions)) # Create the report report = [] for i in range(0, len(filtered_transactions), report_freq): segment_transactions = filtered_transactions[i:i + report_freq] segment_totals = cumulative_totals[i:i + report_freq] report.append((segment_transactions, segment_totals)) return report"},{"question":"You are tasked with creating a visualization utility function using the Seaborn library. The function should demonstrate the use of different methods to create dark palettes and apply them to simple visualizations. # Function Implementation Implement a function `create_dark_palette_visualizations` that takes no input and generates a series of visualizations using different dark palettes created through Seaborn. Requirements: 1. Create and display a dark palette using a named color (e.g., \\"seagreen\\"). 2. Create and display a dark palette using a hex code (e.g., \\"#79C\\"). 3. Create and display a dark palette using the husl system (e.g., `(20, 60, 50)`). 4. Create and display a dark palette with an increased number of colors (e.g., using \\"xkcd:golden\\", with 8 colors). 5. Create and display a continuous colormap from a dark palette (e.g., using the color \\"#b285bc\\"). The function should display each palette as a horizontal bar plot to visualize the colors in the palette. Output: The function should produce a series of horizontal bar plots, each labeled appropriately to indicate which palette creation method was used. Example output: ``` Dark Palette using named color \\"seagreen\\": [plot] Dark Palette using hex code \\"#79C\\": [plot] Dark Palette using husl system (20, 60, 50): [plot] Dark Palette with increased number of colors \\"xkcd:golden\\", 8 colors: [plot] Continuous colormap from dark palette \\"#b285bc\\": [plot] ``` Constraints: - Ensure the plots are clear and the colors are easily distinguishable. - Use appropriate labels for each plot to indicate the palette used. - The function should leverage Seaborn for palette creation and Matplotlib for plotting. # Note: Make sure you have Seaborn and Matplotlib installed in your Python environment to run this function. ```python import seaborn as sns import matplotlib.pyplot as plt def create_dark_palette_visualizations(): # Implement your function here pass ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_dark_palette_visualizations(): fig, axes = plt.subplots(5, 1, figsize=(8, 12), constrained_layout=True) # Dark palette using named color \\"seagreen\\" palette = sns.dark_palette(\\"seagreen\\", as_cmap=False) sns.barplot(x=list(range(len(palette))), y=[1]*len(palette), palette=palette, ax=axes[0]) axes[0].set_title(\'Dark Palette using named color \\"seagreen\\"\') axes[0].set_yticks([]) axes[0].set_xticks([]) # Dark palette using hex code \\"#79C\\" palette = sns.dark_palette(\\"#79C\\", as_cmap=False) sns.barplot(x=list(range(len(palette))), y=[1]*len(palette), palette=palette, ax=axes[1]) axes[1].set_title(\'Dark Palette using hex code \\"#79C\\"\') axes[1].set_yticks([]) axes[1].set_xticks([]) # Dark palette using the husl system (20, 60, 50) palette = sns.dark_palette((20, 60, 50), input=\\"husl\\", as_cmap=False) sns.barplot(x=list(range(len(palette))), y=[1]*len(palette), palette=palette, ax=axes[2]) axes[2].set_title(\'Dark Palette using husl system (20, 60, 50)\') axes[2].set_yticks([]) axes[2].set_xticks([]) # Dark palette with an increased number of colors \\"xkcd:golden\\", 8 colors palette = sns.dark_palette(\\"xkcd:golden\\", n_colors=8, as_cmap=False) sns.barplot(x=list(range(len(palette))), y=[1]*len(palette), palette=palette, ax=axes[3]) axes[3].set_title(\'Dark Palette with increased number of colors \\"xkcd:golden\\", 8 colors\') axes[3].set_yticks([]) axes[3].set_xticks([]) # Continuous colormap from dark palette \\"#b285bc\\" cmap = sns.dark_palette(\\"#b285bc\\", as_cmap=True) sns.heatmap([list(range(100))], cmap=cmap, ax=axes[4], cbar=False) axes[4].set_title(\'Continuous colormap from dark palette \\"#b285bc\\"\') axes[4].set_yticks([]) axes[4].set_xticks([]) plt.show()"},{"question":"# Advanced Coding Assessment: Custom Iterator and Generator Objective: Design and implement custom iterators and generators using Python\'s functional programming features. Task: You are provided with a list of tuples where each tuple represents a range (inclusive). For example: ```python ranges = [(1, 4), (10, 13), (20, 22)] ``` Your task is to: 1. **Create a custom iterator** `RangeIterator` that iterates over each number in all ranges sequentially. 2. **Implement a generator function** `even_numbers` that iterates through the numbers provided by `RangeIterator` and yields only the even numbers. 3. Use the `itertools` module to create a function that combines the even numbers from the `RangeIterator` output and returns them as a list. Requirements: 1. Implement the `RangeIterator` class. 2. Implement the `even_numbers` generator. 3. Implement the `combine_even_numbers` function using `itertools`. Specifications: - **RangeIterator class:** - **Input:** List of tuples representing ranges. - **Output:** Produces each number in the ranges sequentially. - **Example:** ```python ranges = [(1, 4), (10, 13)] iterator = RangeIterator(ranges) output = list(iterator) # Output should be [1, 2, 3, 4, 10, 11, 12, 13] ``` - **even_numbers generator:** - **Input:** An instance of `RangeIterator`. - **Output:** Yields only even numbers from the ranges. - **Example:** ```python ranges = [(1, 4), (10, 13)] iterator = RangeIterator(ranges) evens = list(even_numbers(iterator)) # Output should be [2, 4, 10, 12] ``` - **combine_even_numbers function:** - **Input:** An instance of `RangeIterator`. - **Output:** Returns a list of even numbers from the ranges. - **Example:** ```python ranges = [(1, 4), (10, 13)] iterator = RangeIterator(ranges) combined = combine_even_numbers(iterator) # Output should be [2, 4, 10, 12] ``` Constraints: - You must use Python 3.10. - Utilize the features of `itertools` and `functools` wherever appropriate. - Your function should be efficient and handle large ranges smoothly. Code Template: ```python import itertools from typing import List, Tuple class RangeIterator: def __init__(self, ranges: List[Tuple[int, int]]): self.ranges = ranges self.current_range = None self.current = None self.index = 0 def __iter__(self): return self def __next__(self): if self.current_range is None and self.index < len(self.ranges): self.current_range = self.ranges[self.index] self.current = self.current_range[0] self.index += 1 elif self.current > self.current_range[1]: if self.index >= len(self.ranges): raise StopIteration self.current_range = self.ranges[self.index] self.current = self.current_range[0] self.index += 1 if self.current is not None: result = self.current self.current += 1 return result else: raise StopIteration def even_numbers(iterator: RangeIterator): for number in iterator: if number % 2 == 0: yield number def combine_even_numbers(iterator: RangeIterator) -> List[int]: return list(itertools.compress(iterator, (num % 2 == 0 for num in iterator))) # Example Usage ranges = [(1, 4), (10, 13), (20, 22)] iterator = RangeIterator(ranges) evens_iterator = even_numbers(iterator) combined_evens = combine_even_numbers(iterator) print(list(evens_iterator)) # Expected output: [2, 4, 10, 12, 20, 22] print(combined_evens) # Expected output: [2, 4, 10, 12, 20, 22] ```","solution":"import itertools from typing import List, Tuple, Iterator class RangeIterator: def __init__(self, ranges: List[Tuple[int, int]]): self.ranges = ranges self.current_range = None self.current = None self.index = 0 self._prepare_next_range() def _prepare_next_range(self): if self.index < len(self.ranges): self.current_range = self.ranges[self.index] self.current = self.current_range[0] self.index += 1 else: self.current_range = None self.current = None def __iter__(self) -> Iterator[int]: return self def __next__(self) -> int: if self.current is None: raise StopIteration result = self.current self.current += 1 if self.current > self.current_range[1]: self._prepare_next_range() return result def even_numbers(iterator: RangeIterator) -> Iterator[int]: for number in iterator: if number % 2 == 0: yield number def combine_even_numbers(iterator: RangeIterator) -> List[int]: even_iter = even_numbers(iterator) return list(even_iter)"},{"question":"- Python `stat` Module Objective In this exercise, you will create a function that analyzes files in a directory tree and categorizes them based on their types and permissions. Problem Statement Write a Python function `analyze_directory(directory_path)` that takes a single argument: - `directory_path` (str): The path to the directory you would like to analyze. The function should: 1. Recursively traverse the directory and its subdirectories. 2. For each file, determine its type (regular file, directory, symbolic link, character device, block device, FIFO, socket, door, port, whiteout). 3. For each file, determine its permissions (read, write, execute) for the owner, group, and others. 4. Collect and return a dictionary where: - The keys are the relative paths of the files (from the given directory). - The values are strings indicating the file type and permissions in human-readable form similar to `-rwxrwxrwx`. Constraints - Do not use any external libraries except `os` and `stat`. - Handle any exceptions that may arise (e.g., permissions issues). Example ```python import os import stat def analyze_directory(directory_path): result = {} def walktree(top): for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode except (FileNotFoundError, PermissionError): continue if stat.S_ISDIR(mode): # It\'s a directory, recurse into it walktree(pathname) file_type = \\"directory\\" elif stat.S_ISREG(mode): file_type = \\"regular file\\" elif stat.S_ISLNK(mode): file_type = \\"symbolic link\\" elif stat.S_ISCHR(mode): file_type = \\"character device\\" elif stat.S_ISBLK(mode): file_type = \\"block device\\" elif stat.S_ISFIFO(mode): file_type = \\"FIFO\\" elif stat.S_ISSOCK(mode): file_type = \\"socket\\" elif \'S_ISDOOR\' in dir(stat) and stat.S_ISDOOR(mode): # Door might not be available on all systems file_type = \\"door\\" elif \'S_ISPORT\' in dir(stat) and stat.S_ISPORT(mode): # Port might not be available on all systems file_type = \\"event port\\" elif \'S_ISWHT\' in dir(stat) and stat.S_ISWHT(mode): # Whiteout might not be available on all systems file_type = \\"whiteout\\" else: file_type = \\"unknown\\" permissions = stat.filemode(mode) result[os.path.relpath(pathname, directory_path)] = f\\"{file_type}, {permissions}\\" walktree(directory_path) return result # Usage directory_path = \\"/path/to/your/directory\\" report = analyze_directory(directory_path) for path, details in report.items(): print(f\\"{path}: {details}\\") ``` Notes - You have to ensure the whole directory tree is analyzed. - Your function should work across different operating systems so handle file types accordingly. - The human-readable form for file permissions should be similar to what is typically shown by tools like `ls -l`. Performance Requirements - The function should be efficient enough to handle large directories with thousands of files without significant performance degradation.","solution":"import os import stat def analyze_directory(directory_path): result = {} def walktree(top): for f in os.listdir(top): pathname = os.path.join(top, f) try: mode = os.lstat(pathname).st_mode except (FileNotFoundError, PermissionError): continue # Determine file type if stat.S_ISDIR(mode): file_type = \\"directory\\" walktree(pathname) elif stat.S_ISREG(mode): file_type = \\"regular file\\" elif stat.S_ISLNK(mode): file_type = \\"symbolic link\\" elif stat.S_ISCHR(mode): file_type = \\"character device\\" elif stat.S_ISBLK(mode): file_type = \\"block device\\" elif stat.S_ISFIFO(mode): file_type = \\"FIFO\\" elif stat.S_ISSOCK(mode): file_type = \\"socket\\" elif \'S_ISDOOR\' in dir(stat) and stat.S_ISDOOR(mode): # Door might not be available on all systems file_type = \\"door\\" elif \'S_ISPORT\' in dir(stat) and stat.S_ISPORT(mode): # Port might not be available on all systems file_type = \\"event port\\" elif \'S_ISWHT\' in dir(stat) and stat.S_ISWHT(mode): # Whiteout might not be available on all systems file_type = \\"whiteout\\" else: file_type = \\"unknown\\" # Determine file permissions permissions = stat.filemode(mode) result[os.path.relpath(pathname, directory_path)] = f\\"{file_type}, {permissions}\\" walktree(directory_path) return result"},{"question":"You are tasked with creating a utility that reads an AIFF or AIFF-C audio file, manipulates its audio data, and writes the processed audio data to a new file while preserving the original file\'s parameters. Task 1. Implement a function `process_audio(input_filename, output_filename, gain)` that: - Opens the `input_filename` for reading using the `aifc` module. - Retrieves and prints the following parameters of the input audio file: number of channels, sample width, frame rate, and number of frames. - Reads all the frames of the input file and applies a volume gain (a linear multiplier) to the audio samples. The gain should be an argument to the function. - Writes the processed frames to the `output_filename`, preserving all the original parameters of the input audio file. - Closes both the input and output files properly. Constraints - The function should handle both AIFF and AIFF-C files. - The sample width will always be two bytes (16 bits). - The gain is a positive float value representing the linear gain to apply (e.g., a gain of 2.0 doubles the volume, a gain of 0.5 halves it). - Assume the input and output files are properly formatted and exist in the filesystem. Function Signature ```python def process_audio(input_filename: str, output_filename: str, gain: float) -> None: pass ``` Example Usage ```python input_filename = \\"example.aiff\\" output_filename = \\"output.aifc\\" gain = 1.5 process_audio(input_filename, output_filename, gain) ``` Expected Output You should monitor and print the following on the console during the process: - Number of channels: 2 - Sample width: 2 bytes - Frame rate: 44100 frames/second - Number of frames: 100000 (Example values for illustration) The task will validate your understanding of: - Opening files for reading/writing with `aifc` module. - Retrieving and setting audio file parameters. - Reading and manipulating audio frame data. - Preserving file parameters in the output file.","solution":"import aifc import struct def process_audio(input_filename: str, output_filename: str, gain: float) -> None: with aifc.open(input_filename, \'rb\') as in_file: # Retrieve input file parameters num_channels = in_file.getnchannels() sample_width = in_file.getsampwidth() frame_rate = in_file.getframerate() num_frames = in_file.getnframes() # Print the parameters print(f\\"Number of channels: {num_channels}\\") print(f\\"Sample width: {sample_width} bytes\\") print(f\\"Frame rate: {frame_rate} frames/second\\") print(f\\"Number of frames: {num_frames}\\") # Read the audio frames audio_frames = in_file.readframes(num_frames) # Process the audio frames by applying the gain format = f\\"<{num_frames * num_channels}h\\" audio_samples = struct.unpack(format, audio_frames) processed_samples = [int(sample * gain) for sample in audio_samples] # Clip the samples to stay within the range of int16 processed_samples = [ max(min(sample, 32767), -32768) for sample in processed_samples ] # Pack the processed samples back into bytes processed_frames = struct.pack(format, *processed_samples) with aifc.open(output_filename, \'wb\') as out_file: # Set the output file parameters out_file.setnchannels(num_channels) out_file.setsampwidth(sample_width) out_file.setframerate(frame_rate) out_file.setnframes(num_frames) # Write the processed frames to the output file out_file.writeframes(processed_frames)"},{"question":"# Python Iterator Implementation Problem Objective You are required to implement a custom iterator class in Python that models a specific functionality involving iteration and possibly asynchronous iteration. Problem Description Design and implement a Python class `SquaredIterator` which takes an integer `n` as input and iterates over the squares of non-negative integers starting from 0 up to `n-1`. Requirements: 1. Implement the `SquaredIterator` class with the following methods: - `__init__(self, n)`: Initializes the iterator with the maximum number `n`. - `__iter__(self)`: Returns the iterator object itself. - `__next__(self)`: Returns the next square value in the sequence. 2. Implement an asynchronous version of the iterator `AsyncSquaredIterator` that yields the squares using async/await operations. 3. Ensure that once the iteration exceeds the limit `n`, a `StopIteration` is raised for the synchronous iterator and a `StopAsyncIteration` is raised for the asynchronous iterator. Example Usage: ```python # Synchronous iterator usage squared_iter = SquaredIterator(5) for value in squared_iter: print(value) # Output: # 0 # 1 # 4 # 9 # 16 # Asynchronous iterator usage import asyncio async def main(): async for value in AsyncSquaredIterator(5): print(value) asyncio.run(main()) # Output: # 0 # 1 # 4 # 9 # 16 ``` Input Constraints: - The input `n` will be a non-negative integer less than or equal to 1000. Additional Notes: - Ensure your code adheres to Python\'s iterator protocol. - The asynchronous implementation should use `async for` and `await` keywords appropriately to handle asynchronous operations. Submission: Submit your implementation of `SquaredIterator` and `AsyncSquaredIterator` classes.","solution":"class SquaredIterator: def __init__(self, n): self.n = n self.current = 0 def __iter__(self): return self def __next__(self): if self.current >= self.n: raise StopIteration result = self.current ** 2 self.current += 1 return result import asyncio class AsyncSquaredIterator: def __init__(self, n): self.n = n self.current = 0 def __aiter__(self): return self async def __anext__(self): if self.current >= self.n: raise StopAsyncIteration result = self.current ** 2 self.current += 1 await asyncio.sleep(0) # Simulate async operation return result"},{"question":"Understanding PDP and ICE Plots with sklearn.inspection Objective: You are tasked with analyzing the relationship between input features and the target response of a model using Partial Dependence (PDP) and Individual Conditional Expectation (ICE) plots in the sklearn.inspection module. Problem Statement: Given a dataset `X` of features and a target variable `y`, perform the following steps: 1. Fit a GradientBoostingClassifier to the data `X` and target `y`. 2. Create PDP plots for the first two features individually and a two-way PDP plot for these two features combined. 3. Create ICE plots for the first two features. 4. Extract the raw partial dependence values for the first two features and return them as part of the solution. Function Signature: ```python from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay from sklearn.inspection import partial_dependence def analyze_partial_dependence_and_ice(X, y): Analyze the partial dependence and individual conditional expectation plots. Args: X: np.ndarray or pandas.DataFrame, shape (n_samples, n_features) The input samples. y: np.ndarray or pandas.Series, shape (n_samples) The target values. Returns: dict: A dictionary containing raw partial dependence values in the format: {\\"feature_0\\": {\\"average\\": np.ndarray, \\"grid_values\\": np.ndarray}, \\"feature_1\\": {\\"average\\": np.ndarray, \\"grid_values\\": np.ndarray}} # Your code here to fit the model and create the plots # Return the raw partial dependence values ``` Input: - `X`: A numerical 2D array-like (n_samples, n_features) representing the input features. - `y`: A 1D array-like (n_samples) representing the target variable. Output: - A dictionary containing raw partial dependence values for the first two features. The dictionary should be in the following format: ```python { \\"feature_0\\": { \\"average\\": np.ndarray, \\"grid_values\\": np.ndarray }, \\"feature_1\\": { \\"average\\": np.ndarray, \\"grid_values\\": np.ndarray } } ``` Constraints: - Assume that `X` contains at least two features. - You may use any necessary libraries such as numpy and matplotlib for handling arrays and plotting. Example Usage: ```python import numpy as np # Example dataset X = np.random.rand(100, 5) y = np.random.randint(0, 2, 100) result = analyze_partial_dependence_and_ice(X, y) print(result) ``` # Implementation Details: 1. Use `GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)` to fit the model. 2. Use `PartialDependenceDisplay.from_estimator` for creating PDP and ICE plots. 3. Use `partial_dependence` function to extract and return the raw partial dependence values for the first two features. Your task is to complete the `analyze_partial_dependence_and_ice` function by following the provided steps and adhering to the input/output specifications.","solution":"from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay, partial_dependence import matplotlib.pyplot as plt def analyze_partial_dependence_and_ice(X, y): Analyze the partial dependence and individual conditional expectation plots. Args: X: np.ndarray or pandas.DataFrame, shape (n_samples, n_features) The input samples. y: np.ndarray or pandas.Series, shape (n_samples) The target values. Returns: dict: A dictionary containing raw partial dependence values in the format: {\\"feature_0\\": {\\"average\\": np.ndarray, \\"grid_values\\": np.ndarray}, \\"feature_1\\": {\\"average\\": np.ndarray, \\"grid_values\\": np.ndarray}} # Fit the GradientBoostingClassifier model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Create PDP plots for the first two features individually and a two-way PDP plot fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5)) features = [0, 1, (0, 1)] PartialDependenceDisplay.from_estimator(model, X, features=features, ax=ax) plt.tight_layout() plt.show() # Create ICE plots for the first two features fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) PartialDependenceDisplay.from_estimator(model, X, features=[0], kind=\\"individual\\", ax=ax[0]) PartialDependenceDisplay.from_estimator(model, X, features=[1], kind=\\"individual\\", ax=ax[1]) plt.tight_layout() plt.show() # Extract raw partial dependence values for the first two features pd_results_0 = partial_dependence(model, X, features=[0]) pd_results_1 = partial_dependence(model, X, features=[1]) result = { \\"feature_0\\": { \\"average\\": pd_results_0.average[0], \\"grid_values\\": pd_results_0.grid_values[0] }, \\"feature_1\\": { \\"average\\": pd_results_1.average[0], \\"grid_values\\": pd_results_1.grid_values[0] } } return result"},{"question":"**Objective**: Write a unit test for a given Python function using the `unittest` framework and utilities from the `test.support` module. The function to be tested simulates a basic file operation, but you need to ensure the tests handle various edge cases and conditions. **Function to Test**: ```python def read_file(file_path): Reads the content of the file at the given file_path. Returns the content as a string. Raises FileNotFoundError if the file does not exist. with open(file_path, \'r\') as file: return file.read() ``` **Requirements**: 1. **Basic Functionality**: Write a test that verifies the function returns correct content for a valid file. 2. **File Not Found**: Write a test that ensures the function raises `FileNotFoundError` for a non-existent file. 3. **Temporary Directories**: Use `test.support.os_helper` to create temporary directories and files for testing. 4. **Environment Variables**: Use `test.support.os_helper` to temporarily set/unset environment variables during testing. 5. **Resource Availability**: Use decorators from `test.support` to skip tests if certain resources are not available (e.g., the file system does not support non-ASCII filenames). 6. **Stream Capture**: Use utilities from `test.support` to capture and verify output streams if needed. **Implementation**: Create a test suite in a new module named `test_read_file.py`. Follow the guidelines for using `unittest` and `test.support` as specified in the provided documentation. **Example Boilerplate**: ```python import unittest from test import support from test.support import os_helper import os class TestReadFile(unittest.TestCase): def setUp(self): # Setup code to run before each test self.temp_dir = os_helper.temp_dir() def tearDown(self): # Cleanup code to run after each test os_helper.rmtree(self.temp_dir) def test_read_file_valid(self): # Test that read_file returns correct content for a valid file with self.temp_dir as dir: file_path = os.path.join(dir, \\"test.txt\\") content = \\"Hello, World!\\" with open(file_path, \'w\') as file: file.write(content) self.assertEqual(read_file(file_path), content) def test_read_file_not_found(self): # Test that read_file raises FileNotFoundError for a non-existent file with self.assertRaises(FileNotFoundError): read_file(\\"non_existent_file.txt\\") def test_with_environment_variables(self): # Temporarily set an environment variable with os_helper.EnvironmentVarGuard() as env: env.set(\\"MY_ENV_VAR\\", \\"42\\") self.assertEqual(os.getenv(\\"MY_ENV_VAR\\"), \\"42\\") @support.requires(test.support.os_helper.TESTFN_NONASCII) def test_with_nonascii_filename(self): # Test with non-ASCII filename support with self.temp_dir as dir: non_ascii_file = os.path.join(dir, \\"测试文件.txt\\") content = \\"Some content\\" with open(non_ascii_file, \'w\') as file: file.write(content) self.assertEqual(read_file(non_ascii_file), content) if __name__ == \'__main__\': unittest.main() ``` **Submission**: - Submit your `test_read_file.py` script containing the complete test suite that adheres to the above requirements. - Ensure your code is well-commented, following best practices for test clarity and maintainability.","solution":"def read_file(file_path): Reads the content of the file at the given file_path. Returns the content as a string. Raises FileNotFoundError if the file does not exist. with open(file_path, \'r\') as file: return file.read()"},{"question":"**Problem Statement:** You are tasked with implementing a Python class that simulates the behavior described in the provided documentation for manipulating Python integer objects (`PyLongObject`) through a set of methods. This will involve creating integers from various types of data and converting integers back into those types, mimicking the API described. This class should particularly emulate the error handling behavior as closely as possible in Python, without actually using the C-API. **Class Requirements:** 1. **Class Name**: `PythonIntegerEmulator` 2. **Methods**: - `from_long(v: int) -> int`: Takes a C `long` integer value and returns it as a Python integer. Python integers are already arbitrary precision, so this method will largely be a pass-through. - `from_unsigned_long(v: int) -> int`: Takes a C `unsigned long` integer value and returns it. Raises `ValueError` if the input is negative. - `as_long(value: int) -> int`: Converts a Python integer to a C `long`. Raises `OverflowError` if the value is out of range for `long`. - `as_unsigned_long(value: int) -> int`: Converts a Python integer to an `unsigned long`. Raises `OverflowError` if the value is out of range or negative. - `from_double(v: float) -> int`: Takes a double and returns its integer part. **Constraints**: - Assume that the values passed for the integer methods are within typical ranges handled by Python natively. You do not need to cover every edge case handled by the low-level C API. - Python\'s integer type and float type should be used directly without any third-party libraries. **Expected Behavior**: - You should handle conversion between types manually without relying on Python\'s internal error messages (i.e., you should raise `ValueError` or `OverflowError` as needed). - Follow the principle of precision and raise an appropriate error if the range is exceeded like the documented C functions. **Example Usage**: ```python class PythonIntegerEmulator: LONG_MAX = 2**31 - 1 LONG_MIN = -(2**31) UNSIGNED_LONG_MAX = 2**32 - 1 @staticmethod def from_long(v: int) -> int: return int(v) @staticmethod def from_unsigned_long(v: int) -> int: if v < 0: raise ValueError(\\"Cannot convert negative value to unsigned long\\") return int(v) @staticmethod def as_long(value: int) -> int: if value > PythonIntegerEmulator.LONG_MAX or value < PythonIntegerEmulator.LONG_MIN: raise OverflowError(\\"Value out of range for long\\") return value @staticmethod def as_unsigned_long(value: int) -> int: if value < 0 or value > PythonIntegerEmulator.UNSIGNED_LONG_MAX: raise OverflowError(\\"Value out of range for unsigned long\\") return value @staticmethod def from_double(v: float) -> int: return int(v) # Example usage emulator = PythonIntegerEmulator() print(emulator.from_long(1234)) # 1234 print(emulator.from_unsigned_long(1234)) # 1234 print(emulator.as_long(2147483647)) # 2147483647 print(emulator.as_unsigned_long(4294967295)) # 4294967295 print(emulator.from_double(1234.56)) # 1234 ``` **Assessment Criteria**: - Correct implementation of the methods. - Proper handling of errors and edge cases. - Clean and readable code.","solution":"class PythonIntegerEmulator: LONG_MAX = 2**31 - 1 LONG_MIN = -(2**31) UNSIGNED_LONG_MAX = 2**32 - 1 @staticmethod def from_long(v: int) -> int: Converts a C `long` to a Python integer. Python integers are arbitrary precision. return int(v) @staticmethod def from_unsigned_long(v: int) -> int: Converts a C `unsigned long` to a Python integer. Raises `ValueError` if the input is negative. if v < 0: raise ValueError(\\"Cannot convert negative value to unsigned long\\") return int(v) @staticmethod def as_long(value: int) -> int: Converts a Python integer to a C `long`. Raises `OverflowError` if the value is out of range for `long`. if value > PythonIntegerEmulator.LONG_MAX or value < PythonIntegerEmulator.LONG_MIN: raise OverflowError(\\"Value out of range for long\\") return value @staticmethod def as_unsigned_long(value: int) -> int: Converts a Python integer to an `unsigned long`. Raises `OverflowError` if the value is out of range or negative. if value < 0 or value > PythonIntegerEmulator.UNSIGNED_LONG_MAX: raise OverflowError(\\"Value out of range for unsigned long\\") return value @staticmethod def from_double(v: float) -> int: Converts a double to a Python integer. return int(v)"},{"question":"# K-Nearest Neighbors and Dimensionality Reduction with NCA In this assessment, you are required to demonstrate your knowledge of the `sklearn.neighbors` module by performing the following tasks: 1. Implement a function to find the k-nearest neighbors for a given dataset. 2. Use the `NeighborhoodComponentsAnalysis` (NCA) algorithm to perform dimensionality reduction on the same dataset and illustrate how this impacts the k-nearest neighbors search. Instructions: 1. **Function to Find Nearest Neighbors:** Create a function `find_nearest_neighbors(X, n_neighbors, algorithm=\'auto\')` which: - Takes the input `X` as a numpy array of shape (n_samples, n_features) representing the dataset. - Takes an integer `n_neighbors` representing the number of neighbors to find. - Takes a string `algorithm` that specifies the nearest neighbor search algorithm to use (`auto`, `ball_tree`, `kd_tree` or `brute`). Default should be `auto`. - Returns the indices of the nearest neighbors and the distances from each point to its neighbors. ```python def find_nearest_neighbors(X, n_neighbors, algorithm=\'auto\'): pass ``` 2. **Dimensionality Reduction with NCA:** Create a function `dimensionality_reduction_with_nca(X, y, n_components)` which: - Takes the input `X` as a numpy array of shape (n_samples, n_features) representing the dataset. - Takes the input `y` as the labels (targets) associated with the dataset. - Takes an integer `n_components` specifying the number of dimensions for the reduced dataset. - Uses the `NeighborhoodComponentsAnalysis` to project the data into a new feature space with the specified number of components. - Returns the transformed dataset after applying NCA. ```python def dimensionality_reduction_with_nca(X, y, n_components): pass ``` 3. **Impact Analysis:** Write a script that: - Loads a sample dataset (e.g., Iris dataset from scikit-learn). - Finds the nearest neighbors on the original dataset using the `find_nearest_neighbors` function. - Applies NCA to reduce the dimensionality of the data to 2 dimensions. - Finds the nearest neighbors on the reduced dataset. - Compares and discusses the nearest neighbor results before and after dimensionality reduction. Example Usage: ```python # Sample Usage from sklearn.datasets import load_iris # Load dataset data = load_iris() X, y = data.data, data.target # Find nearest neighbors in original space indices_orig, distances_orig = find_nearest_neighbors(X, n_neighbors=3) # Apply dimensionality reduction using NCA X_nca = dimensionality_reduction_with_nca(X, y, n_components=2) # Find nearest neighbors in reduced space indices_nca, distances_nca = find_nearest_neighbors(X_nca, n_neighbors=3) # Analyze and compare results print(\'Original Neighbors:\', indices_orig) print(\'Reduced Neighbors:\', indices_nca) ``` Constraints and Requirements: - You are required to use `scikit-learn` for the implementation. Ensure that all imports are properly handled. - Numpy should be used for any array manipulation. - The solutions must be efficient in terms of both time and space complexity. - Proper commenting and code readability will be considered in the assessment. Performance Constraints: - The functions should handle datasets with at least 1000 samples and 50 features efficiently. - The dimensionality reduction must be meaningful, preserving the distinction among clusters in the reduced space.","solution":"import numpy as np from sklearn.neighbors import NearestNeighbors from sklearn.neighbors import NeighborhoodComponentsAnalysis def find_nearest_neighbors(X, n_neighbors, algorithm=\'auto\'): Finds the k-nearest neighbors for a given dataset. Parameters: - X (numpy array): The dataset of shape (n_samples, n_features). - n_neighbors (int): The number of neighbors to find. - algorithm (str): The nearest neighbor search algorithm to use (\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'). Returns: - indices (numpy array): The indices of the nearest neighbors. - distances (numpy array): The distances to the nearest neighbors. nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm).fit(X) distances, indices = nbrs.kneighbors(X) return indices, distances def dimensionality_reduction_with_nca(X, y, n_components): Uses NeighborhoodComponentsAnalysis to perform dimensionality reduction. Parameters: - X (numpy array): The dataset of shape (n_samples, n_features). - y (array-like): The labels associated with the dataset. - n_components (int): The number of dimensions to reduce the dataset to. Returns: - X_nca (numpy array): The transformed dataset after applying NCA. nca = NeighborhoodComponentsAnalysis(n_components=n_components, random_state=42) X_nca = nca.fit_transform(X, y) return X_nca # Script to load dataset, find nearest neighbors, perform NCA, and compare results from sklearn.datasets import load_iris # Load dataset data = load_iris() X, y = data.data, data.target # Find nearest neighbors in original space indices_orig, distances_orig = find_nearest_neighbors(X, n_neighbors=3) # Apply dimensionality reduction using NCA X_nca = dimensionality_reduction_with_nca(X, y, n_components=2) # Find nearest neighbors in reduced space indices_nca, distances_nca = find_nearest_neighbors(X_nca, n_neighbors=3) # Analyze and compare results print(\'Original Neighbors:\', indices_orig) print(\'Reduced Neighbors:\', indices_nca)"},{"question":"# **Question: Implement a Custom Business Timestamp Series Generator** Objective Design and implement a function called `generate_custom_business_series` that generates a pandas `Series` object consisting of custom business timestamps based on a given set of rules and constraints. Function Signature ```python def generate_custom_business_series(start_date: str, end_date: str, offset: str, holidays: list) -> pd.Series: pass ``` Parameters - `start_date` (str): The starting date of the time series in the format \'YYYY-MM-DD\'. - `end_date` (str): The ending date of the time series in the format \'YYYY-MM-DD\'. - `offset` (str): The custom business offset frequency string (e.g., \'C\', \'CBMonthBegin\', \'CBMonthEnd\', \'B\'). - `holidays` (list): A list of dates (strings in \'YYYY-MM-DD\' format) representing holidays, which should be excluded from the business series. Returns - `pd.Series`: A pandas Series object containing the custom business timestamps. Constraints and Requirements 1. The `start_date` should be before or equal to the `end_date`. 2. The function should utilize the appropriate pandas offset class, indicated by the `offset` parameter, such as `CustomBusinessDay`, `CustomBusinessHour`, `BusinessDay`, etc. 3. The timestamps should exclude weekends and any dates listed in the `holidays` parameter. 4. The series should normalize the timestamps to midnight of each day (i.e., `normalize=True`). 5. Ensure that all properties and methods related to the chosen offset class are correctly utilized. Example ```python import pandas as pd holidays = [\'2023-12-25\', \'2024-01-01\', \'2024-04-01\'] start_date = \'2023-12-01\' end_date = \'2024-01-31\' offset = \'C\' series = generate_custom_business_series(start_date, end_date, offset, holidays) print(series) ``` Output (assuming a daily custom business day offset with specified holidays): ``` 2023-12-01 2023-12-04 2023-12-05 ... 2024-01-30 2024-01-31 dtype: datetime64[ns] ``` In this example, weekends and holiday dates should be excluded. Ensure that the function efficiently handles large date ranges and a significant number of holidays. Test Cases 1. Test with no holidays and a business day offset. 2. Test with multiple holidays and a custom business day offset. 3. Test with a small date range to verify date series generation logic. Notes - Utilize the `CustomBusinessDay` class and handle specific properties available for customizing the business days, like `holidays`, `weekmask`, etc. - Consider the performance and make sure the function is efficient for larger date ranges.","solution":"import pandas as pd from pandas.tseries.offsets import CustomBusinessDay def generate_custom_business_series(start_date: str, end_date: str, offset: str, holidays: list) -> pd.Series: Generates a pandas Series object consisting of custom business timestamps. Parameters: - start_date: The starting date of the time series in the format \'YYYY-MM-DD\'. - end_date: The ending date of the time series in the format \'YYYY-MM-DD\'. - offset: The custom business offset frequency string (e.g., \'C\', \'CBMonthBegin\', \'CBMonthEnd\', \'B\'). - holidays: A list of dates (strings in \'YYYY-MM-DD\' format) representing holidays to be excluded. Returns: - pd.Series: A pandas Series object containing the custom business timestamps. # Define the custom business day offset custom_bday = CustomBusinessDay(holidays=holidays) # Generate date range with custom business day offset date_range = pd.date_range(start=start_date, end=end_date, freq=custom_bday, normalize=True) # Convert the date range to a pandas Series return pd.Series(date_range)"},{"question":"# Question: Feature Selection and ML Pipeline Integration Feature selection is a crucial step in building a robust machine learning pipeline. Your task is to implement a pipeline that utilizes a combination of feature selection methods provided by the `scikit-learn` library and a machine learning classifier to achieve the best possible performance. Task 1. **Step 1**: Load the `digits` dataset from `sklearn.datasets`. 2. **Step 2**: Apply `VarianceThreshold` to remove features with low variance. Set the threshold to `0.16`. 3. **Step 3**: Use `SelectKBest` with `chi2` as the scoring function to select the top 20 features. 4. **Step 4**: Apply `RFE` with `LogisticRegression` as the estimator to select the top 10 features. 5. **Step 5**: Integrate the above feature selection steps into a `Pipeline`, and append a `RandomForestClassifier` as the final estimator. 6. **Step 6**: Split the dataset into training and testing sets using `train_test_split`. 7. **Step 7**: Train the pipeline on the training set and evaluate its accuracy on the testing set. Constraints - You must use the specific feature selection methods and parameters mentioned in the task. - Use `random_state=42` for any random number generation (e.g., in `train_test_split` and the classifier) for consistency. Input and Output - You do not need to take any specific input. The dataset and parameter values are pre-defined. - Output the accuracy of the trained pipeline on the test set. Code Template ```python import numpy as np from sklearn.datasets import load_digits from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, RFE from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier # Step 1: Load the dataset digits = load_digits() X, y = digits.data, digits.target # Step 2: Apply VarianceThreshold var_thresh = VarianceThreshold(threshold=0.16) # Step 3: Apply SelectKBest select_k_best = SelectKBest(chi2, k=20) # Step 4: Apply RFE log_reg = LogisticRegression(solver=\'liblinear\', random_state=42) rfe = RFE(estimator=log_reg, n_features_to_select=10, step=1) # Step 5: Create the pipeline pipeline = Pipeline([ (\'variance_threshold\', var_thresh), (\'select_k_best\', select_k_best), (\'rfe\', rfe), (\'classifier\', RandomForestClassifier(random_state=42)) ]) # Step 6: Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 7: Train the pipeline and evaluate pipeline.fit(X_train, y_train) accuracy = pipeline.score(X_test, y_test) print(f\\"Accuracy: {accuracy:.4f}\\") ``` In this task, students are expected to demonstrate their understanding of various feature selection methods and their integration within a machine learning pipeline.","solution":"import numpy as np from sklearn.datasets import load_digits from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, RFE from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier def build_and_evaluate_pipeline(): # Step 1: Load the dataset digits = load_digits() X, y = digits.data, digits.target # Step 2: Apply VarianceThreshold var_thresh = VarianceThreshold(threshold=0.16) # Step 3: Apply SelectKBest select_k_best = SelectKBest(chi2, k=20) # Step 4: Apply RFE log_reg = LogisticRegression(solver=\'liblinear\', random_state=42) rfe = RFE(estimator=log_reg, n_features_to_select=10, step=1) # Step 5: Create the pipeline pipeline = Pipeline([ (\'variance_threshold\', var_thresh), (\'select_k_best\', select_k_best), (\'rfe\', rfe), (\'classifier\', RandomForestClassifier(random_state=42)) ]) # Step 6: Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 7: Train the pipeline and evaluate pipeline.fit(X_train, y_train) accuracy = pipeline.score(X_test, y_test) return accuracy"},{"question":"# Custom SAX ContentHandler **Objective**: Implement a custom SAX ContentHandler to parse and transform a specific XML-like document format. You are given an XML-like document structured as follows: ```xml <root> <item id=\\"1\\" value=\\"10\\">Item 1</item> <item id=\\"2\\" value=\\"20\\">Item 2</item> <item id=\\"3\\" value=\\"30\\">Item 3</item> </root> ``` Your task is to implement a class `MyContentHandler` that extends `xml.sax.handler.ContentHandler`. This handler should process the XML document and store the items in a list of dictionaries. Each dictionary should have the keys \'id\', \'value\', and \'text\' corresponding to the attributes and text of each `<item>` element. **Specifications**: 1. Define the class `MyContentHandler` extending `xml.sax.handler.ContentHandler`. 2. Implement the following methods in `MyContentHandler`: - `startElement(self, name, attrs)`: Extracts item elements and their attributes. - `characters(self, content)`: Collects character data within item elements. - `endElement(self, name)`: Finalizes the collection of item elements. 3. The handler should store each item as a dictionary in a list attribute named `items`. **Input**: A string representing the XML content: ```python xml_data = <root> <item id=\\"1\\" value=\\"10\\">Item 1</item> <item id=\\"2\\" value=\\"20\\">Item 2</item> <item id=\\"3\\" value=\\"30\\">Item 3</item> </root> ``` **Output**: The list of dictionaries representing the items: ```python [{\'id\': \'1\', \'value\': \'10\', \'text\': \'Item 1\'}, {\'id\': \'2\', \'value\': \'20\', \'text\': \'Item 2\'}, {\'id\': \'3\', \'value\': \'30\', \'text\': \'Item 3\'}] ``` **Constraints**: - Assume the input XML is well-formed. - The data within each `<item>` element is simple text without nested elements or additional attributes. **Performance requirements**: - The solution should correctly handle typical, small-sized XML files within reasonable memory and time constraints. # Example Usage ```python import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.items = [] self.current_item = {} self.current_content = [] def startElement(self, name, attrs): if name == \'item\': self.current_item = { \'id\': attrs.get(\'id\', \'\'), \'value\': attrs.get(\'value\', \'\') } self.current_content = [] def characters(self, content): self.current_content.append(content) def endElement(self, name): if name == \'item\': self.current_item[\'text\'] = \'\'.join(self.current_content).strip() self.items.append(self.current_item) xml_data = <root> <item id=\\"1\\" value=\\"10\\">Item 1</item> <item id=\\"2\\" value=\\"20\\">Item 2</item> <item id=\\"3\\" value=\\"30\\">Item 3</item> </root> def parse_xml(data): handler = MyContentHandler() xml.sax.parseString(data, handler) return handler.items print(parse_xml(xml_data)) ```","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.items = [] self.current_item = {} self.current_content = [] def startElement(self, name, attrs): if name == \'item\': self.current_item = { \'id\': attrs.get(\'id\', \'\'), \'value\': attrs.get(\'value\', \'\') } self.current_content = [] def characters(self, content): self.current_content.append(content) def endElement(self, name): if name == \'item\': self.current_item[\'text\'] = \'\'.join(self.current_content).strip() self.items.append(self.current_item) def parse_xml(data): handler = MyContentHandler() xml.sax.parseString(data, handler) return handler.items"},{"question":"# Coding Assessment: Utilizing the MPS Backend in PyTorch Objective: To assess your understanding of PyTorch\'s MPS backend for accelerating computations on macOS devices with GPU support. Problem Statement: You are provided with a simple neural network architecture and some input data. Your task is to: 1. Verify the availability of the MPS backend. 2. If available, move the model and input data to the MPS device. 3. Perform forward propagation and output the model\'s predictions. Requirements: 1. **Check MPS Availability:** Implement the code to check if the MPS backend is available on the current system. 2. **Move to Device:** If MPS is available, move the neural network model and input tensor to the MPS device. 3. **Prediction:** Perform a forward pass with the given input tensor and return the predictions. Implementation: 1. **Input Format:** - A neural network class `SimpleNN`, which defines a simple two-layer neural network. - An input tensor `input_tensor` of shape `(batch_size, input_features)`. 2. **Output Format:** - A tensor containing the model predictions. Function Signature: ```python def run_on_mps(input_tensor: torch.Tensor) -> torch.Tensor: # Define the neural network class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(in_features=3, out_features=5) self.fc2 = torch.nn.Linear(in_features=5, out_features=2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleNN() # Check if MPS is available if torch.backends.mps.is_available(): mps_device = torch.device(\\"mps\\") input_tensor = input_tensor.to(mps_device) model.to(mps_device) else: raise RuntimeError(\\"MPS backend is not available\\") # Perform forward pass and return predictions with torch.no_grad(): predictions = model(input_tensor) return predictions ``` Example: ```python import torch # Define the input_tensor input_tensor = torch.randn(10, 3) # Call the function predictions = run_on_mps(input_tensor) print(predictions) ``` Constraints: - Assume that the input tensor will always have a shape compatible with the neural network (i.e., the second dimension should be 3). - Handle cases where the MPS backend is not available by raising an appropriate error message. **Note:** Ensure you have the necessary setup to execute the code involving the MPS backend. This means you need to be running on macOS 12.3+ with an MPS-enabled device, and PyTorch built with MPS support.","solution":"import torch def run_on_mps(input_tensor: torch.Tensor) -> torch.Tensor: # Define the neural network class SimpleNN(torch.nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = torch.nn.Linear(in_features=3, out_features=5) self.fc2 = torch.nn.Linear(in_features=5, out_features=2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleNN() # Check if MPS is available if torch.backends.mps.is_available(): mps_device = torch.device(\\"mps\\") input_tensor = input_tensor.to(mps_device) model.to(mps_device) else: raise RuntimeError(\\"MPS backend is not available\\") # Perform forward pass and return predictions with torch.no_grad(): predictions = model(input_tensor) return predictions"},{"question":"Objective The objective of this exercise is to test your understanding of the seaborn `hls_palette` function and its various customization capabilities. You will also need to demonstrate how to apply these palettes within a plot to ensure effective data visualization. Problem Statement You are given a dataset containing information about different species of flowers. You need to perform the following tasks: 1. **Generate Color Palettes:** Create three different color palettes using the `hls_palette` function in Seaborn: - Palette A: 5 colors with default settings. - Palette B: 8 colors with a lightness of 0.4. - Palette C: 6 colors with a saturation of 0.7 and hue angles starting at 0.3. 2. **Plot Using Palettes:** Using the `iris` dataset from Seaborn, create three different scatter plots using these palettes. Each plot should represent: - `sepal_length` on the x-axis. - `sepal_width` on the y-axis. - Different species of flowers should be colored using one of the palettes above. 3. **Customize and Label:** Customize each plot to include appropriate labels for the axes, a title, and a legend indicating the species. Ensure that the color palette used in each scatter plot is easily distinguishable and aesthetically pleasing. Input No input is required as you will use the internal `iris` dataset from Seaborn. Output Three scatter plots saved as images: `plot_palette_a.png`, `plot_palette_b.png`, and `plot_palette_c.png`. Function Signature ```python def generate_and_plot_palettes(): pass ``` # Constraints 1. Use only the `seaborn` and `matplotlib` libraries for visualization. 2. Save each scatter plot as a separate image file. # Example ```python def generate_and_plot_palettes(): import seaborn as sns import matplotlib.pyplot as plt iris = sns.load_dataset(\'iris\') # Generate palettes palette_a = sns.hls_palette(5) palette_b = sns.hls_palette(8, l=0.4) palette_c = sns.hls_palette(6, s=0.7, h=0.3) # Plot 1: Palette A plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_a, data=iris) plt.title(\'Scatter Plot using Palette A\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.savefig(\'plot_palette_a.png\') plt.clf() # Plot 2: Palette B plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_b, data=iris) plt.title(\'Scatter Plot using Palette B\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.savefig(\'plot_palette_b.png\') plt.clf() # Plot 3: Palette C plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_c, data=iris) plt.title(\'Scatter Plot using Palette C\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.savefig(\'plot_palette_c.png\') plt.clf() ```","solution":"def generate_and_plot_palettes(): import seaborn as sns import matplotlib.pyplot as plt # Load iris dataset iris = sns.load_dataset(\'iris\') # Generate palettes palette_a = sns.hls_palette(5) palette_b = sns.hls_palette(8, l=0.4) palette_c = sns.hls_palette(6, s=0.7, h=0.3) # Plot 1: Palette A plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_a, data=iris) plt.title(\'Scatter Plot using Palette A\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.savefig(\'plot_palette_a.png\') plt.clf() # Plot 2: Palette B plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_b, data=iris) plt.title(\'Scatter Plot using Palette B\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.savefig(\'plot_palette_b.png\') plt.clf() # Plot 3: Palette C plt.figure(figsize=(8, 6)) sns.scatterplot(x=\'sepal_length\', y=\'sepal_width\', hue=\'species\', palette=palette_c, data=iris) plt.title(\'Scatter Plot using Palette C\') plt.xlabel(\'Sepal Length\') plt.ylabel(\'Sepal Width\') plt.legend(title=\'Species\') plt.savefig(\'plot_palette_c.png\') plt.clf()"},{"question":"# Comprehensive Coding Assessment Objective: To assess your understanding of Python’s typing hints and your ability to write unit tests using the `unittest` module. Problem Statement: You are required to implement a function and write unit tests for it using Python\'s `unittest` framework. 1. **Function Implementation**: Implement a function `find_primes_in_range(start: int, end: int) -> list[int]`, which returns a list of all prime numbers in the specified range (inclusive of `start` and exclusive of `end`). - **Input**: - `start` (int): The starting integer of the range. - `end` (int): The ending integer of the range. - **Output**: - list[int]: A list of prime numbers within the range [`start`, `end`). - **Constraints**: - `start` should be greater than or equal to 0. - `end` should be greater than `start`. - The function should handle large ranges efficiently. - **Examples**: ```python assert find_primes_in_range(10, 20) == [11, 13, 17, 19] assert find_primes_in_range(2, 10) == [2, 3, 5, 7] assert find_primes_in_range(15, 15) == [] ``` 2. **Unit Tests**: Write a test class `TestFindPrimesInRange` to test the `find_primes_in_range` function. Use Python\'s `unittest` framework. - **Requirements**: - At least five test cases covering different scenarios: 1. Normal cases with a mix of prime and non-prime numbers. 2. Edge cases where the range has no prime numbers. 3. Very small ranges like `start = end - 1`. 4. Large ranges to test performance. 5. Cases where `start` is 0. - Use type hints within your tests where appropriate. - **Example Test Cases**: ```python import unittest from your_module import find_primes_in_range class TestFindPrimesInRange(unittest.TestCase): def test_normal_case(self): self.assertEqual(find_primes_in_range(10, 20), [11, 13, 17, 19]) def test_no_primes(self): self.assertEqual(find_primes_in_range(14, 16), []) def test_small_range(self): self.assertEqual(find_primes_in_range(17, 18), [17]) def test_large_range(self): result = find_primes_in_range(1000, 1100) # You can precompute the expected primes to check against expected = [1009, 1013, 1019, 1021, 1031, 1033, 1039, 1049, 1051, 1061, 1063, 1069, 1087, 1091, 1093, 1097] self.assertEqual(result, expected) def test_start_is_zero(self): self.assertEqual(find_primes_in_range(0, 10), [2, 3, 5, 7]) if __name__ == \'__main__\': unittest.main() ``` Submission: Submit your `find_primes_in_range` function implementation and the test class `TestFindPrimesInRange`. Ensure your code is well-documented and follows Python conventions.","solution":"def find_primes_in_range(start: int, end: int) -> list[int]: Returns a list of all prime numbers in the specified range [start, end). Parameters: start (int): The starting integer of the range (inclusive). end (int): The ending integer of the range (exclusive). Returns: list[int]: A list of prime numbers within the range [start, end). if start >= end: return [] def is_prime(n: int) -> bool: if n < 2: return False for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True return [num for num in range(start, end) if is_prime(num)]"},{"question":"# Question: Multi-Dimensional Fourier Transform and Image Filtering You are tasked with implementing a function that performs a frequency-domain filtering operation on a grayscale image using PyTorch. The function should: 1. Compute the 2-dimensional discrete Fourier Transform (DFT) of the input image. 2. Apply a low-pass filter in the frequency domain. 3. Compute the 2-dimensional inverse discrete Fourier Transform (IDFT) to obtain the filtered image. The low-pass filter should set all frequency components outside a specified radius to zero. Function Signature ```python def frequency_domain_filter(image: torch.Tensor, radius: int) -> torch.Tensor: Applies a low-pass filter in the frequency domain to a grayscale image. Parameters: - image (torch.Tensor): 2D tensor representing the grayscale image (shape: HxW). - radius (int): Radius of the low-pass filter in the frequency domain. Returns: - torch.Tensor: 2D tensor representing the filtered grayscale image (shape: HxW). ``` Input - `image`: A 2D tensor of shape (H, W) where H is the height and W is the width of the image. * The pixel values are in the range [0, 1]. - `radius`: An integer specifying the radius of the low-pass filter. Output - The function should return a 2D tensor of shape (H, W) containing the filtered image. Constraints - The input `image` tensor will have shape (H, W) with H and W being powers of 2 (to optimize FFT operations). - The radius `r` will be a positive integer less than min(H/2, W/2). Example ```python import torch # Example grayscale image image = torch.rand(256, 256) # Apply frequency domain filter with radius 30 filtered_image = frequency_domain_filter(image, 30) ``` Requirements 1. The implementation should efficiently compute the 2-dimensional DFT and IDFT using `torch.fft`. 2. Use `torch.fft.fftshift` and `torch.fft.ifftshift` to properly center the zero-frequency component. 3. Ensure the low-pass filter is applied correctly in the frequency domain. Evaluation Your solution will be evaluated based on: - Correctness: Properly applying the DFT, filter, and IDFT. - Efficiency: Leveraging PyTorch’s efficient FFT functions. - Code clarity: Well-documented and readable code.","solution":"import torch import torch.fft def frequency_domain_filter(image: torch.Tensor, radius: int) -> torch.Tensor: Applies a low-pass filter in the frequency domain to a grayscale image. Parameters: - image (torch.Tensor): 2D tensor representing the grayscale image (shape: HxW). - radius (int): Radius of the low-pass filter in the frequency domain. Returns: - torch.Tensor: 2D tensor representing the filtered grayscale image (shape: HxW). # Compute the FFT of the image fft_image = torch.fft.fft2(image) fft_image = torch.fft.fftshift(fft_image) # Create the low-pass filter mask H, W = image.shape Y, X = torch.meshgrid(torch.arange(H), torch.arange(W), indexing=\'ij\') center_y, center_x = H // 2, W // 2 distance = torch.sqrt((Y - center_y)**2 + (X - center_x)**2) mask = distance <= radius # Apply the low-pass filter filtered_fft_image = fft_image * mask # Compute the inverse FFT to get the filtered image filtered_fft_image = torch.fft.ifftshift(filtered_fft_image) filtered_image = torch.fft.ifft2(filtered_fft_image) # Return the real part of the filtered image return filtered_image.real"},{"question":"Question: Mocking and Testing with `unittest.mock` You are tasked with developing a small Python package for a library management system. The system must interact with an external payment service to process book rental payments. However, as this payment service is not available during development, you need to use the `unittest.mock` package to simulate its behavior and verify interactions with it. # 1. Design a Class for Payment Processing: Create a class `LibraryPaymentProcessor` with the following functions: - `__init__(self, payment_service)`: Initialize with an external `payment_service`. - `process_payment(self, amount)`: Calls the `payment_service` to process a payment of the specified `amount`. Returns `True` if the payment is successful, else `False`. # 2. Write a Test Class: Create a test class `TestLibraryPaymentProcessor` using `unittest`. Within this class: - Mock the `payment_service`. - Verify the interactions between `LibraryPaymentProcessor` and the mocked `payment_service`. - Check specific scenarios where the payment is successful, where the payment fails, and where the service raises an exception. # Constraints: 1. **Initialize**: You can define the `payment_service` mocked object as necessary during the tests. 2. **Process Method**: Ensure that `process_payment` correctly interfaces with `payment_service`. # Example Usage: ```python payment_service_mock = Mock() library_payment_processor = LibraryPaymentProcessor(payment_service_mock) ``` # Expected Input/Output: Input: - `process_payment(100)` Output: - Call to `payment_service.process` with the amount `100`. Performance Requirements: 1. The `process_payment` method should call the external service with expected arguments. 2. Proper assertions should be made to confirm that methods are called the expected number of times with the correct parameters. # Your Task: 1. Implement the `LibraryPaymentProcessor` class. 2. Write tests in `TestLibraryPaymentProcessor` class that: - Test a successful payment. - Test a failed payment. - Test the case where the payment service raises an exception. ```python import unittest from unittest.mock import Mock, patch # Write your LibraryPaymentProcessor class implementation here. class LibraryPaymentProcessor: def __init__(self, payment_service): self.payment_service = payment_service def process_payment(self, amount): try: return self.payment_service.process(amount) except Exception as e: return False # Write your TestLibraryPaymentProcessor class implementation here. class TestLibraryPaymentProcessor(unittest.TestCase): def setUp(self): self.payment_service_mock = Mock() self.payment_processor = LibraryPaymentProcessor(self.payment_service_mock) def test_successful_payment(self): self.payment_service_mock.process.return_value = True result = self.payment_processor.process_payment(100) self.payment_service_mock.process.assert_called_once_with(100) self.assertTrue(result) def test_failed_payment(self): self.payment_service_mock.process.return_value = False result = self.payment_processor.process_payment(100) self.payment_service_mock.process.assert_called_once_with(100) self.assertFalse(result) def test_payment_service_exception(self): self.payment_service_mock.process.side_effect = Exception(\\"Service Error\\") result = self.payment_processor.process_payment(100) self.payment_service_mock.process.assert_called_once_with(100) self.assertFalse(result) # Run the tests if __name__ == \'__main__\': unittest.main() ``` Provide the implementation for the `LibraryPaymentProcessor` and ensure the test cases in `TestLibraryPaymentProcessor` verify all the expected scenarios.","solution":"import unittest class LibraryPaymentProcessor: def __init__(self, payment_service): Initialize with an external payment_service. self.payment_service = payment_service def process_payment(self, amount): Calls the payment_service to process a payment of the specified amount. Returns True if the payment is successful, else False. try: return self.payment_service.process(amount) except Exception: return False"},{"question":"# Email Attachment Organizer You are required to implement a function that processes a given directory containing email messages and organizes the attachments into corresponding subdirectories based on email senders. Each email message in the directory is a file with a `.eml` extension. Your function should read each `.eml` file, process the email contents to extract attachments, and then place these attachments into subdirectories named after the senders\' email addresses. Expected Function Signature ```python def organize_attachments(directory: str): pass ``` Input - `directory`: A string representing the path to the directory containing the `.eml` email files. Output The function does not return anything. It creates subdirectories within the given directory based on senders\' email addresses and places the extracted attachments in these subdirectories. # Details 1. Each subdirectory should be named after the email address of the sender of the corresponding email. 2. Attachments must be placed in the subdirectories corresponding to their senders. 3. If an email does not have any attachments, no subdirectory should be created for that email. 4. Handle cases where multiple emails are from the same sender by placing each sender\'s attachments in the same directory. 5. Assume that the provided directory path is valid and contains only `.eml` email files. Example Suppose the directory contains three email files: - `email1.eml` from `alice@example.com` with attachments `image1.jpg` and `doc1.pdf` - `email2.eml` from `bob@example.com` with no attachments - `email3.eml` from `alice@example.com` with attachment `image2.jpg` After running the function, the directory structure should look like this: ``` /path/to/directory /alice@example.com image1.jpg doc1.pdf image2.jpg /bob@example.com (This directory should not exist as bob@example.com has no attachments) ``` You need to use the `email` package to parse the emails and extract attachments. Constraints - The directory contains a maximum of 100 `.eml` files. - Each `.eml` file is less than or equal to 10MB in size. Performance Requirements - The solution should efficiently handle extracting attachments from a large number of emails with minimal performance overhead. **Note:** You are encouraged to use the `os`, `email`, and `mimetypes` packages to complete this task effectively.","solution":"import os from email import policy from email.parser import BytesParser def organize_attachments(directory: str): for filename in os.listdir(directory): if filename.endswith(\'.eml\'): filepath = os.path.join(directory, filename) with open(filepath, \'rb\') as f: email_message = BytesParser(policy=policy.default).parse(f) sender = email_message.get(\'From\') if sender: for part in email_message.iter_attachments(): if part.get_filename(): sender_dir = os.path.join(directory, sender) os.makedirs(sender_dir, exist_ok=True) attachment_path = os.path.join(sender_dir, part.get_filename()) with open(attachment_path, \'wb\') as attachment_file: attachment_file.write(part.get_payload(decode=True))"},{"question":"**Problem Statement:** You are given a Python script `analyze_imports.py` that is intended to determine and report all the modules imported by another Python script. Your task is to implement a function that utilizes the `modulefinder` module to do this. The function should handle potential errors gracefully and give a detailed report of loaded and missing modules. **Function Signature:** ```python def analyze_script_imports(script_path: str, exclude_modules: list = [], replace_paths: list = []) -> str: pass ``` **Input:** - `script_path` (str): The path to the Python script file to be analyzed. - `exclude_modules` (list): A list of module names to exclude from the analysis. Default is an empty list. - `replace_paths` (list): A list of `(oldpath, newpath)` tuples for path replacement in module names. Default is an empty list. **Output:** - (str): A string containing the report of loaded and missing modules. **Example:** ```python # Contents of \'example_script.py\': # import os # import sys # try: # import non_existent_module # except ImportError: # pass report = analyze_script_imports(\'example_script.py\') print(report) # Expected Output: # Loaded modules: # os: list of global names in os module # sys: list of global names in sys module # __main__: os,sys,non_existent_module # --------------------------------------------------- # Modules not imported: # non_existent_module ``` **Constraints:** - You can assume the input script file exists and is readable. - The function should handle exceptions where modules cannot be imported. - The output should adhere to the format shown in the example. Implement the `analyze_script_imports` function that will perform the described functionality using the `modulefinder` module.","solution":"import modulefinder def analyze_script_imports(script_path: str, exclude_modules: list = [], replace_paths: list = []) -> str: Analyzes the import statements of the given Python script and reports loaded and missing modules. Args: script_path (str): The path to the Python script file to be analyzed. exclude_modules (list): A list of module names to exclude from the analysis. Default is an empty list. replace_paths (list): A list of (oldpath, newpath) tuples for path replacement in module names. Default is an empty list. Returns: str: A string containing the report of loaded and missing modules. # Initialize ModuleFinder finder = modulefinder.ModuleFinder(excludes=exclude_modules) # Run the modulefinder on the given script finder.run_script(script_path) # Create output output = [\\"Loaded modules:\\"] for name, mod in finder.modules.items(): if name == \\"__main__\\": continue output.append(f\\"{name}: {\', \'.join(mod.globalnames.keys())}\\") output.append(\\"-\\" * 50) output.append(\\"Modules not imported:\\") for name in finder.badmodules.keys(): output.append(name) return \\"n\\".join(output)"},{"question":"# Exception Handling and Custom Exception Design Objective: You are tasked with implementing a robust exception handling mechanism in a Python application. This involves creating custom exceptions, raising them under specific conditions, and constructing a nested exception handling mechanism that provides informative feedback with a complete context of the errors encountered. Requirements: 1. **Custom Exceptions:** - Create two custom exceptions: `InvalidDataError` (subclass of `ValueError`) and `ComputationError` (subclass of `ArithmeticError`). - `InvalidDataError` should take an additional attribute called `invalid_value` that stores the value which caused the exception. 2. **Function Implementation:** - Implement a function `process_data(data)`, where `data` is expected to be a list of integers. - If any element in `data` is not an integer, raise `InvalidDataError` with the `invalid_value`. - Implement a function `compute_mean(numbers)` which takes a list of integers and returns the mean value. - If the list is empty, raise `ComputationError` with an appropriate error message. 3. **Exception Handling and Chaining:** - Implement a main function `main()` that: 1. Calls `process_data(data)` with user-provided data. 2. Calls `compute_mean(processed_data)` to compute the mean. 3. Uses nested `try-except` blocks to handle `InvalidDataError` and `ComputationError`. - When handling `InvalidDataError`, log the `invalid_value` and raise a new `ComputationError` from it with a custom message. - Ensure that the original context of the error is preserved and can be introspected. Input/Output: - **Input:** A list of any Python objects (not necessarily integers). - **Output:** The mean of the processed list if all inputs are valid integers. If not, appropriate error handling where the context shows the chain of exceptions raised. Constraints: - The input list may contain elements of mixed types. - The custom exceptions should include all necessary information in their string representation. Example: ```python # Example input list data = [1, \'x\', 3] # Expected behavior: # - process_data should raise InvalidDataError(\'x\') # - main should handle InvalidDataError, log the invalid value, and raise ComputationError with context. try: main() except Exception as e: print(e) # This should print the complete chain of exceptions. ``` Solution Skeleton: ```python class InvalidDataError(ValueError): def __init__(self, invalid_value): self.invalid_value = invalid_value super().__init__(f\\"Invalid data encountered: {invalid_value}\\") class ComputationError(ArithmeticError): pass def process_data(data): processed_data = [] for item in data: if not isinstance(item, int): raise InvalidDataError(item) processed_data.append(item) return processed_data def compute_mean(numbers): if not numbers: raise ComputationError(\\"Cannot compute mean of an empty list\\") return sum(numbers) / len(numbers) def main(): data = [1, \'x\', 3] try: processed_data = process_data(data) mean_value = compute_mean(processed_data) print(f\\"The mean value is: {mean_value}\\") except InvalidDataError as ide: print(f\\"Logging error: {ide.invalid_value}\\") raise ComputationError(\\"Error in data processing, computation failed.\\") from ide except ComputationError as ce: # This could catch errors directly from compute_mean if any print(f\\"Computation error occurred: {ce}\\") if __name__ == \\"__main__\\": main() ``` Complete the implementation, ensuring the correct exception handling and chaining with proper logging at each step.","solution":"class InvalidDataError(ValueError): def __init__(self, invalid_value): self.invalid_value = invalid_value super().__init__(f\\"Invalid data encountered: {invalid_value}\\") class ComputationError(ArithmeticError): def __init__(self, message): super().__init__(message) def process_data(data): processed_data = [] for item in data: if not isinstance(item, int): raise InvalidDataError(item) processed_data.append(item) return processed_data def compute_mean(numbers): if not numbers: raise ComputationError(\\"Cannot compute mean of an empty list\\") return sum(numbers) / len(numbers) def main(data): try: processed_data = process_data(data) mean_value = compute_mean(processed_data) return mean_value except InvalidDataError as ide: print(f\\"Logging error: {ide.invalid_value}\\") raise ComputationError(\\"Error in data processing, computation failed.\\") from ide except ComputationError as ce: print(f\\"Computation error occurred: {ce}\\") raise if __name__ == \\"__main__\\": data = [1, \'x\', 3] try: mean = main(data) print(f\\"The mean is: {mean}\\") except Exception as e: print(f\\"Exception occurred: {e}\\")"},{"question":"# Coding Assessment Task Objective: You are tasked with creating a utility function in Python that uses the `compileall` module to compile all Python source files within a given directory (including its subdirectories) into byte-code files. The function should support several options to customize the compilation process. Task: Implement the function `compile_python_sources(directory, force_recompile=False, max_depth=-1, report_only_errors=False, optimization_level=-1)` that will: 1. Recursively compile all Python source files in the specified `directory` into byte-code files. 2. Have the option to force recompilation of all files using the `force_recompile` flag. 3. Limit the recursion depth to the specified `max_depth` (default is unlimited recursion as per `sys.getrecursionlimit`). 4. Control verbosity of the output using the `report_only_errors` flag; if True, only errors should be reported; if False, filenames and other information should also be included in the output. 5. Support setting the optimization level for the compilation through the `optimization_level` parameter. Input: - `directory` (str): Path of the directory to compile. - `force_recompile` (bool, optional): If `True`, forces recompilation of all files. Default is `False`. - `max_depth` (int, optional): Maximum depth of directory recursion. Default is `-1` (no limit). - `report_only_errors` (bool, optional): If `True`, only error messages are printed. Default is `False`. - `optimization_level` (int or list of int, optional): Specifies the optimization level for the compiler. Default is `-1` (no optimization). Output: The function should return `True` if all files compiled successfully, otherwise `False`. Constraints: - You must use the `compile_dir` function from the `compileall` module. - You should handle invalid input such as non-existent directories appropriately by raising a `ValueError` with a relevant message. Example Usage: ```python result = compile_python_sources(\'/path/to/directory\', force_recompile=True, max_depth=2, report_only_errors=True, optimization_level=[1, 2]) print(result) # Expected Output: True (if successful) or False ``` Additional Information: Refer to the provided documentation excerpt for the `compileall` module functions and their parameters to correctly implement the utility function.","solution":"import compileall import os def compile_python_sources(directory, force_recompile=False, max_depth=-1, report_only_errors=False, optimization_level=-1): Recursively compile all Python source files in the specified directory into byte-code files. Parameters: directory (str): Path of the directory to compile. force_recompile (bool, optional): If True, forces recompilation of all files. Default is False. max_depth (int, optional): Maximum depth of directory recursion. Default is -1 (no limit). report_only_errors (bool, optional): If True, only error messages are printed. Default is False. optimization_level (int or list of int, optional): Specifies the optimization level for the compiler. Default is -1 (no optimization). Returns: bool: True if all files compiled successfully, otherwise False. if not os.path.isdir(directory): raise ValueError(f\\"Invalid directory: {directory}\\") # Compile the directory and capture the result compile_result = compileall.compile_dir( dir=directory, force=force_recompile, maxlevels=max_depth, quiet=report_only_errors, optimize=optimization_level ) return compile_result"},{"question":"**Unix Group Membership Using Python** You are tasked with writing a Python program that utilizes the `grp` module to perform various operations on Unix group data. Implement the following functions based on the specified requirements: 1. **Function: get_group_by_id** - **Input**: `gid` (integer) - The numeric group ID. - **Output**: A dictionary containing the group\'s attributes (if found) or `None` (if not found). - **Constraints**: - Raise a `TypeError` if `gid` is not an integer. - Return `None` instead of raising a `KeyError` if the group ID is not found. 2. **Function: get_group_by_name** - **Input**: `name` (string) - The name of the group. - **Output**: A dictionary containing the group\'s attributes (if found) or `None` (if not found). - **Constraints**: - Raise a `TypeError` if `name` is not a string. - Return `None` instead of raising a `KeyError` if the group name is not found. 3. **Function: get_all_groups** - **Output**: A list of dictionaries, each containing the attributes of all available group entries. - **Constraints**: None. # Example ```python def get_group_by_id(gid): # Your implementation here def get_group_by_name(name): # Your implementation here def get_all_groups(): # Your implementation here # Example usage: print(get_group_by_id(0)) # Example output: {\'gr_name\': \'groupname\', \'gr_passwd\': \'\', \'gr_gid\': 0, \'gr_mem\': [\'user1\', \'user2\']} print(get_group_by_name(\'groupname\')) # Example output: {\'gr_name\': \'groupname\', \'gr_passwd\': \'\', \'gr_gid\': 0, \'gr_mem\': [\'user1\', \'user2\']} print(get_all_groups()) # Example output: [{\'gr_name\': \'groupname1\', \'gr_passwd\': \'\', \'gr_gid\': 0, \'gr_mem\': [\'user1\', \'user2\']}, ...] ``` # Notes - Ensure proper exception handling and input validation in your implementation. - The returned dictionary should contain the keys: \'gr_name\', \'gr_passwd\', \'gr_gid\', and \'gr_mem\'.","solution":"import grp def get_group_by_id(gid): Returns the group attributes given a group ID. if not isinstance(gid, int): raise TypeError(\\"gid must be an integer\\") try: group = grp.getgrgid(gid) return { \'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem } except KeyError: return None def get_group_by_name(name): Returns the group attributes given a group name. if not isinstance(name, str): raise TypeError(\\"name must be a string\\") try: group = grp.getgrnam(name) return { \'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem } except KeyError: return None def get_all_groups(): Returns a list of all group attributes. groups = grp.getgrall() return [{ \'gr_name\': group.gr_name, \'gr_passwd\': group.gr_passwd, \'gr_gid\': group.gr_gid, \'gr_mem\': group.gr_mem } for group in groups]"},{"question":"Coding Assessment Question # Objective Develop a Python script that automates the creation of a manifest file, `MANIFEST.in`, which will contain a specified list of files and patterns. This script will help in automating the inclusion and exclusion of files for a source distribution using the `sdist` command in Python. # Problem Statement Write a Python function `generate_manifest` that creates a `MANIFEST.in` file based on the given criteria. # Function Signature ```python def generate_manifest(include_patterns: list, exclude_patterns: list, default_files: bool, prune_directories: bool) -> None: ``` # Input - `include_patterns` (list): A list of patterns to include in the `MANIFEST.in` file. - `exclude_patterns` (list): A list of patterns to exclude from the `MANIFEST.in` file. - `default_files` (bool): A flag indicating whether to include default files (`README`, `setup.py`, etc.). - `prune_directories` (bool): A flag indicating whether to prune typical version control directories (like `CVS`, `.git`, etc.). # Output - The function should create and write to a `MANIFEST.in` file in the current directory with the specified inclusions and exclusions. # Constraints - Do not include any redundant file paths in the `MANIFEST.in`. - Do follow the standard format and order of the manifest file rules as described in the documentation. # Example Given the following input: ```python include_patterns = [\\"*.md\\", \\"src/*.py\\"] exclude_patterns = [\\"*.pyc\\", \\"tests/*\\"] default_files = True prune_directories = True ``` The `MANIFEST.in` created should look like: ``` include *.md recursive-include src *.py exclude *.pyc exclude tests/* include README include setup.py prune CVS prune .git prune .hg prune .svn prune build/ ``` # Additional Information - This script should generate the `MANIFEST.in` file based on the order: includes, excludes, default files, and pruned directories. - Remember that the paths in `MANIFEST.in` should use forward slashes for directory separators, ensuring compatibility across different operating systems. Good Luck!","solution":"def generate_manifest(include_patterns: list, exclude_patterns: list, default_files: bool, prune_directories: bool) -> None: with open(\\"MANIFEST.in\\", \\"w\\") as file: for pattern in include_patterns: file.write(f\\"include {pattern}n\\") for pattern in exclude_patterns: file.write(f\\"exclude {pattern}n\\") if default_files: default_files_list = [\\"README\\", \\"setup.py\\"] for default_file in default_files_list: file.write(f\\"include {default_file}n\\") if prune_directories: prune_dirs = [\\"CVS\\", \\".git\\", \\".hg\\", \\".svn\\", \\"build/\\"] for directory in prune_dirs: file.write(f\\"prune {directory}n\\")"},{"question":"# Question: Implementing Internationalization using gettext in Python You are given the task of localizing a Python application to support multiple languages. The application should be able to change its message language based on user preference dynamically. Your task is to leverage the functions and classes from the \\"gettext\\" module to implement this functionality. # Objectives 1. **Setup Translation**: Bind the translation domain to find `.mo` files in a specified directory. 2. **Translate Messages**: Create functions to fetch translated messages, including those with plural forms. 3. **Switch Languages**: Implement a function to switch languages dynamically based on user input. # Requirements 1. **Set up the Environment**: - Write a function `setup_translation(domain, localedir)` that binds the domain to the locale directory. - This function should prepare the environment to find the `.mo` files for the specified domain. 2. **Translation Functions**: - Implement a function `get_translated_message(message)` that returns the translated version of the message. - Implement a function `get_translated_plural_message(singular, plural, n)` that returns the correctly pluralized translated message. 3. **Language Switching**: - Implement a function `switch_language(domain, languages)` that switches the language based on user preference. - Once the language is switched, the translation functions should use the new language settings. # Input and Output Formats - **Inputs**: - `setup_translation(domain: str, localedir: str)`: Strings representing the translation domain and the locale directory path. - `get_translated_message(message: str)`: A string message to be translated. - `get_translated_plural_message(singular: str, plural: str, n: int)`: Strings for singular and plural forms and an integer `n` to determine the pluralization. - `switch_language(domain: str, languages: List[str])`: Strings for the translation domain and a list of language codes. - **Outputs**: - `setup_translation` and `switch_language` should set the necessary environment but return nothing. - `get_translated_message` and `get_translated_plural_message` should return the respective translated messages. - **Constraints**: - Assume `.mo` files for different languages are available in the specified `localedir`. - The `switch_language` function is expected to handle only valid language codes. # Example Usage Here is an example usage of the functions you are to implement: ```python # Set up the environment setup_translation(\'myapp\', \'/path/to/locale\') # Get translated messages print(get_translated_message(\'Hello, World!\')) # Should print translated message based on current language print(get_translated_plural_message(\'There is one file\', \'There are many files\', 5)) # Correct plural translation # Switch to a different language switch_language(\'myapp\', [\'de\']) # Switch to German print(get_translated_message(\'Hello, World!\')) # Should print the message in German ``` # Notes - Ensure that the environment is clean and does not produce unnecessary side effects. - Handle any exceptions where the `.mo` file might not be found or the language code is invalid gracefully. Good luck!","solution":"import gettext current_translation = None def setup_translation(domain, localedir): global current_translation current_translation = gettext.translation(domain, localedir, fallback=True) gettext.install(domain, localedir) def get_translated_message(message): if current_translation: return current_translation.gettext(message) return message def get_translated_plural_message(singular, plural, n): if current_translation: return current_translation.ngettext(singular, plural, n) return singular if n == 1 else plural def switch_language(domain, languages): global current_translation current_translation = gettext.translation(domain, localedir=None, languages=languages, fallback=True) gettext.install(domain, localedir=None, names=None, codeset=None)"},{"question":"You are tasked with creating a file organization script using the `shutil` module. This script has multiple responsibilities such as copying files, moving directories, and creating compressed archives. # Task Implement a function `organize_files(base_directory, dest_directory, archive_path)` that performs the following actions: 1. **Copy files**: - Copy all `.txt` files from `base_directory` to `dest_directory`. - Use `shutil.copy2()` to ensure that file metadata is preserved. - Ensure that if a file with the same name exists in the destination, it is overwritten. 2. **Move directories**: - Move any directory within `base_directory` that starts with `data_` to `dest_directory`. - Use `shutil.move()` to perform the operation. - Ensure this operation does not overwrite any existing directories in the destination. 3. **Create an archive**: - Create a `.tar.gz` compressed archive of the `dest_directory` and save it to the location specified by `archive_path`. - Use `shutil.make_archive()` for this task. # Input - `base_directory`: A string representing the path to the source directory. - `dest_directory`: A string representing the path to the destination directory. - `archive_path`: A string representing the file path for the .tar.gz archive (excluding the extension). # Output - The function does not return anything. - After execution, `dest_directory` should contain all `.txt` files copied from `base_directory`, and all directories that start with `data_` moved from `base_directory`. - The specified `archive_path` should point to a valid .tar.gz file that contains the contents of `dest_directory`. # Constraints - You can assume that `base_directory` and `dest_directory` are valid existing directories. - You can assume `archive_path` is a valid path where the archive can be created. - Handle any potential exceptions and ensure your function is robust against common filesystem issues. # Example ```python import os from shutil import copy2, move, make_archive def organize_files(base_directory, dest_directory, archive_path): # Your code goes here pass # Example usage: base_directory = \\"/path/to/base\\" dest_directory = \\"/path/to/dest\\" archive_path = \\"/path/to/archive/my_archive\\" organize_files(base_directory, dest_directory, archive_path) # After execution, all .txt files from base_directory should be in dest_directory, # data_* directories should also be moved to dest_directory, # and the archive my_archive.tar.gz should be created. ``` # Notes - Ensure that your solution comprehensively uses the `shutil` module\'s functions. - You can use `os.path` functions to help with path manipulations.","solution":"import os import shutil def organize_files(base_directory, dest_directory, archive_path): # Ensure the destination directory exists if not os.path.exists(dest_directory): os.makedirs(dest_directory) # Copy .txt files from base_directory to dest_directory for item in os.listdir(base_directory): item_path = os.path.join(base_directory, item) if os.path.isfile(item_path) and item.endswith(\'.txt\'): shutil.copy2(item_path, dest_directory) # Move directories starting with \'data_\' from base_directory to dest_directory for item in os.listdir(base_directory): item_path = os.path.join(base_directory, item) if os.path.isdir(item_path) and item.startswith(\'data_\'): dest_path = os.path.join(dest_directory, item) if not os.path.exists(dest_path): # Only move if it doesn\'t already exist shutil.move(item_path, dest_directory) # Create a .tar.gz archive of the dest_directory shutil.make_archive(archive_path, \'gztar\', dest_directory)"},{"question":"# Question: File System Organizer You are tasked with writing a Python script to organize files within a specified directory. The script should: 1. Traverse the directory and all its subdirectories. 2. Move all files into a new directory structure where files are sorted into subdirectories based on their file extensions. For example, all `.txt` files should move to a `/txt` subdirectory, all `.jpg` files to a `/jpg` subdirectory, and so on. 3. Generate a log file named `operation_log.txt` in the root of the organization directory. This log file should list all the files that have been moved, along with their old and new paths. 4. Ignore hidden files and directories. # Specifications - **Input**: A single input, `root_dir` (a string), which is the path to the root directory to be organized. - **Output**: No direct output, but the side effects must include: 1. Files organized into subdirectories by extension. 2. An `operation_log.txt` file in the root of `root_dir`. # Constraints - You can assume the `root_dir` path provided always exists. - The script should handle a large number of files efficiently. - The script should handle file name conflicts by appending a number to the file name. # Example Assume the directory structure is as follows before running the script: ``` root_dir/ │ ├── file1.txt ├── image1.jpg ├── docs/ │ ├── file2.txt │ ├── report.pdf │ └── images/ ├── image2.jpg └── image3.jpg ``` After running the script, the directory structure should be transformed to: ``` root_dir/ │ ├── log.txt ├── txt/ │ ├── file1.txt │ ├── file2.txt │ ├── jpg/ │ ├── image1.jpg │ ├── image2.jpg │ └── image3.jpg │ └── pdf/ └── report.pdf ``` And the `operation_log.txt` file should contain something similar to: ``` Moved file1.txt to txt/file1.txt Moved docs/file2.txt to txt/file2.txt Moved image1.jpg to jpg/image1.jpg Moved images/image2.jpg to jpg/image2.jpg Moved images/image3.jpg to jpg/image3.jpg Moved docs/report.pdf to pdf/report.pdf ``` # Implementation Implement this functionality in the `organize_filesystem` function. Ensure to import necessary modules and handle potential exceptions. ```python import os import shutil import logging def organize_filesystem(root_dir: str): # Your implementation here ```","solution":"import os import shutil from collections import defaultdict def organize_filesystem(root_dir: str): files_moved = [] ext_dict = defaultdict(list) # Traversing the directory for dirpath, dirnames, filenames in os.walk(root_dir): # Ignore hidden directories dirnames[:] = [d for d in dirnames if not d.startswith(\'.\')] for filename in filenames: if filename.startswith(\'.\'): continue # Ignore hidden files full_path = os.path.join(dirpath, filename) ext = filename.split(\'.\')[-1] if len(filename.split(\'.\')) == 1: # No extension ext = \'no_ext\' ext_dict[ext].append(full_path) # Processing and organizing files for ext, files in ext_dict.items(): new_dir = os.path.join(root_dir, ext) os.makedirs(new_dir, exist_ok=True) for file_path in files: new_path = os.path.join(new_dir, os.path.basename(file_path)) if os.path.exists(new_path): file_root, file_ext = os.path.splitext(new_path) counter = 1 while os.path.exists(new_path): new_path = f\'{file_root}_{counter}{file_ext}\' counter += 1 shutil.move(file_path, new_path) files_moved.append(f\'Moved {file_path} to {new_path}\') # Generating the log file log_path = os.path.join(root_dir, \'operation_log.txt\') with open(log_path, \'w\') as log_file: log_file.write(\'n\'.join(files_moved))"},{"question":"# Compression and Decompression with Custom Filters **Objective:** Your task is to implement a Python function that compresses a given input string using custom compression filters and then decompresses it to verify the integrity of the operation. You will use the `lzma` module and follow these steps: 1. Compress the input string with a specific custom filter chain. 2. Decompress the compressed data. 3. Verify that the decompressed data matches the original input string. **Requirements:** - Your solution should define functions `custom_compress` and `custom_decompress`. - `custom_compress` should take a `string` and return a compressed `bytes` object. - `custom_decompress` should take a `bytes` object of compressed data and return the decompressed `string`. **Custom Filter Chain:** Use the following custom filter chain for compression: ```python filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6 | lzma.PRESET_EXTREME}, ] ``` **Input:** - A `string` that needs to be compressed. **Output:** - `custom_compress` should return `bytes` object containing the compressed data. - `custom_decompress` should return the decompressed `string`. **Constraints:** - The functions should handle any valid string input. - Ensure that all necessary imports and error handling are in place. **Example:** ```python import lzma def custom_compress(string): # Implement compression using the specified custom filter chain. ... def custom_decompress(compressed_data): # Implement decompression to retrieve the original data. ... # Example usage: original_string = \\"Data that needs to be compressed and decompressed.\\" compressed_data = custom_compress(original_string) assert original_string == custom_decompress(compressed_data) ``` Implement the functions `custom_compress` and `custom_decompress` to solve the problem.","solution":"import lzma def custom_compress(string): Compresses the given string using a specific custom filter chain. Args: string (str): The input string to be compressed. Returns: bytes: The compressed data. filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6 | lzma.PRESET_EXTREME}, ] compressor = lzma.LZMACompressor(format=lzma.FORMAT_RAW, filters=filters) compressed_data = compressor.compress(string.encode(\'utf-8\')) compressed_data += compressor.flush() return compressed_data def custom_decompress(compressed_data): Decompresses the given compressed data using the same custom filter chain. Args: compressed_data (bytes): The compressed data. Returns: str: The decompressed string. filters = [ {\\"id\\": lzma.FILTER_DELTA, \\"dist\\": 4}, {\\"id\\": lzma.FILTER_LZMA2, \\"preset\\": 6 | lzma.PRESET_EXTREME}, ] decompressor = lzma.LZMADecompressor(format=lzma.FORMAT_RAW, filters=filters) decompressed_data = decompressor.decompress(compressed_data) return decompressed_data.decode(\'utf-8\')"},{"question":"**Objective**: Implement a Python class that demonstrates the understanding of iterators and async iterators based on the Python C API functions described in the documentation. **Problem Statement**: You are to implement a class called `CustomIterator` that behaves both as a synchronous iterator and an asynchronous iterator. Your implementation should simulate the behavior of the `PyIter_Check`, `PyIter_Next`, and `PyIter_Send` functions provided in the Python C API documentation. # Specifications: - **Class Name**: `CustomIterator` - **Methods**: 1. `__init__(self, data)`: Initializes the iterator with a list of data. 2. `__iter__(self)`: Returns the iterator object itself. 3. `__next__(self)`: Retrieves the next value from the iterator. Raises `StopIteration` when no more items are available. 4. `__aiter__(self)`: Returns the async iterator object itself. 5. `__anext__(self)`: Returns the next item asynchronously. Raises `StopAsyncIteration` when no more items are available. 6. `send(self, value)`: Sends a value into the iterator. Simulates the behavior of `PyIter_Send`. # Input and Output Formats: - **Input**: - Initialization with a list, e.g., `data = [1, 2, 3, 4, 5]`. - **Output**: - The iterator should yield items until it is exhausted, upon which it should raise `StopIteration` for synchronous iteration and `StopAsyncIteration` for asynchronous iteration. - The `send` method should simulate sending a value into the iterator, reflecting it in the output as described. # Constraints and Considerations: - You should manage internal state to handle the iteration correctly. - Ensure proper handling of errors and termination conditions. - Implement the `send` method to change the state or behavior of the iterator based on the sent value. # Example Usage: ```python # Synchronous iteration data = [1, 2, 3, 4, 5] iterator = CustomIterator(data) for item in iterator: print(item) # Output: 1 2 3 4 5 # Asynchronous iteration import asyncio async def async_iter_test(): async for item in CustomIterator(data): print(item) # Output: 1 2 3 4 5 asyncio.run(async_iter_test()) # Using send method it = CustomIterator([1, 2, 3]) print(it.send(99)) # Custom behavior simulation ``` Your task is to implement the `CustomIterator` class based on the above specifications.","solution":"class CustomIterator: def __init__(self, data): self.data = data self.index = 0 def __iter__(self): return self def __next__(self): if self.index < len(self.data): value = self.data[self.index] self.index += 1 return value else: raise StopIteration def __aiter__(self): return self async def __anext__(self): if self.index < len(self.data): value = self.data[self.index] self.index += 1 return value else: raise StopAsyncIteration def send(self, value): if 0 <= self.index < len(self.data): self.data[self.index] = value return True else: return False"},{"question":"Description You are to implement a Python function `execute_python_code` that simulates different modes of how the Python interpreter reads and executes code based on the type of input provided. The function should handle three input types: 1. **File Input**: Where the input is a list of statements or newlines. 2. **Interactive Input**: Where the input is a list of statements optionally followed by a compound statement. 3. **Expression Input**: Where the input is a single expression to be evaluated using `eval()`. Function Signature ```python def execute_python_code(input_type: str, code: str) -> str: pass ``` Input - `input_type` (str): A string that indicates the type of input. It can be \\"file\\", \\"interactive\\", or \\"expression\\". - `code` (str): A string containing the Python code to be executed based on the `input_type`. Output - Returns a string representing the result of the code execution. For \\"file\\" and \\"interactive\\" inputs, this should be the output of the code (if any) as printed to the console. For \\"expression\\" input, this should be the result of the evaluated expression. Constraints - The code must be a valid Python code. - The `code` can include multiple lines for \\"file\\" and \\"interactive\\" types. - For \\"expression\\" type, the `code` should be a single valid Python expression. Example ```python result = execute_python_code(\\"file\\", \\"a = 5nb = 10nprint(a + b)\\") print(result) # Output should be: \\"15n\\" result = execute_python_code(\\"interactive\\", \\"a = 5nif True:n b = 10nprint(a + b)\\") print(result) # Output should be: \\"15n\\" result = execute_python_code(\\"expression\\", \\"5 + 10\\") print(result) # Output should be: \\"15\\" ``` Note - You will need to handle the setup and capturing of standard output correctly to simulate the behavior of a Python interpreter. - Think about the different sceneries and how each input type should behave.","solution":"import io import contextlib def execute_python_code(input_type: str, code: str) -> str: output = io.StringIO() if input_type == \\"file\\" or input_type == \\"interactive\\": code_lines = code.split(\'n\') full_code = \\"n\\".join(code_lines) with contextlib.redirect_stdout(output): exec(full_code) elif input_type == \\"expression\\": result = eval(code) output.write(str(result) + \'n\') else: raise ValueError(\\"Invalid input type\\") return output.getvalue()"},{"question":"Objective: Demonstrate your comprehension of the `bisect` module functions `bisect_left`, `bisect_right`, `insort_left`, and `insort_right` by solving the following tasks. This will test your ability to maintain a sorted list efficiently and locate specific values. Problem Statement: You are given a list of tuples where each tuple contains a `name` and a `score`. You need to maintain this list in sorted order based on the scores using the `bisect` module\'s `insort_left` function. Along with maintaining the list, you need to implement functionalities to: 1. Find the first student with a score greater than a given `x`. 2. Find the first student with a score less than or equal to a given `y`. Function Signatures: You need to implement the following functions: 1. `def add_student(students: List[Tuple[str, int]], student: Tuple[str, int]) -> None:` - **Input:** - `students` (List[Tuple[str, int]]): A list of tuples where each tuple contains a `name` (str) and a `score` (int). The list is initially empty. - `student` (Tuple[str, int]): A tuple containing a `name` (str) and a `score` (int). - **Output:** - None. The function should update the `students` list in place by adding the `student` while maintaining the sorted order based on scores using `insort_left`. 2. `def first_student_gt(students: List[Tuple[str, int]], x: int) -> Optional[Tuple[str, int]]:` - **Input:** - `students` (List[Tuple[str, int]]): A list of tuples sorted based on scores. - `x` (int): A given score threshold. - **Output:** - A tuple containing the `name` and `score` of the first student with a score greater than `x`. If no such student exists, return `None`. 3. `def first_student_le(students: List[Tuple[str, int]], y: int) -> Optional[Tuple[str, int]]:` - **Input:** - `students` (List[Tuple[str, int]]): A list of tuples sorted based on scores. - `y` (int): A given score threshold. - **Output:** - A tuple containing the `name` and `score` of the first student with a score less than or equal to `y`. If no such student exists, return `None`. Constraints: - The list `students` should always remain sorted based on scores. - Use the `insort_left` function from the `bisect` module for inserting a student. - `1 <= len(name) <= 100` where `name` is the student\'s name. - `0 <= score <= 100`. Example: ```python students = [] add_student(students, (\\"Alice\\", 85)) add_student(students, (\\"Bob\\", 90)) add_student(students, (\\"Charlie\\", 75)) assert first_student_gt(students, 80) == (\\"Alice\\", 85) assert first_student_gt(students, 90) == None assert first_student_le(students, 85) == (\\"Charlie\\", 75) assert first_student_le(students, 70) == None ```","solution":"import bisect from typing import List, Tuple, Optional def add_student(students: List[Tuple[str, int]], student: Tuple[str, int]) -> None: Insert a student into the sorted list of students using insort_left to maintain the order. bisect.insort_left(students, student, key=lambda x: x[1]) def first_student_gt(students: List[Tuple[str, int]], x: int) -> Optional[Tuple[str, int]]: Find the first student with a score greater than x. idx = bisect.bisect_right([score for name, score in students], x) return students[idx] if idx < len(students) else None def first_student_le(students: List[Tuple[str, int]], y: int) -> Optional[Tuple[str, int]]: Find the first student with a score less than or equal to y. idx = bisect.bisect_right([score for name, score in students], y) return students[idx - 1] if idx > 0 else None"},{"question":"**Question: Performance Comparison Using `timeit`** To assess your understanding of the `timeit` module and your ability to measure and compare execution times of different implementations, you are required to: 1. Implement three different functions that each find the factorial of a number `n`. 2. Measure and compare the performance of these functions using the `timeit` module. # Task 1. Implement the following three functions to calculate the factorial of a number `n`: - `factorial_recursive(n)`: Uses a recursive approach. - `factorial_iterative(n)`: Uses an iterative approach. - `factorial_reduce(n)`: Uses the `reduce` function from the `functools` module. 2. Measure the execution time of each function for `n = 100` using the `timeit` module. Perform each measurement with `number=10000` to get a reliable average time. 3. Print the execution time for each function, and identify which method is the most efficient based on the measured times. # Implementation Details 1. **Input and Output**: - There are no inputs or outputs other than the measurements to be printed. 2. **Constraints**: - The number `n` for factorial calculation is fixed at 100 for this task. - Use `number=10000` in the `timeit` function calls for measuring execution time. 3. **Performance Requirements**: - Perform the measurements accurately using the `timeit` module. - Ensure the functions are implemented in a way that correctly calculates factorials. # Example Output ```python Execution time of factorial_recursive: X.XXXXXX s Execution time of factorial_iterative: Y.YYYYYY s Execution time of factorial_reduce: Z.ZZZZZZ s The most efficient method is: <method name> ``` # Hint Here is an example of how to use the `timeit` module for measuring the execution time of a function: ```python import timeit def example_function(): pass execution_time = timeit.timeit(\\"example_function()\\", setup=\\"from __main__ import example_function\\", number=10000) print(f\\"Execution time: {execution_time}\\") ``` # Submission Submit a Python script (`.py` file) containing your implementations of the three functions and the code to measure and print their execution times as described.","solution":"import timeit from functools import reduce def factorial_recursive(n): Calculates factorial of n using recursion. if n == 0 or n == 1: return 1 else: return n * factorial_recursive(n - 1) def factorial_iterative(n): Calculates factorial of n using an iterative approach. result = 1 for i in range(2, n+1): result *= i return result def factorial_reduce(n): Calculates factorial of n using reduce from functools. if n == 0 or n == 1: return 1 return reduce(lambda x, y: x * y, range(1, n + 1)) if __name__ == \\"__main__\\": n = 100 number = 10000 time_recursive = timeit.timeit(\\"factorial_recursive(n)\\", setup=\\"from __main__ import factorial_recursive, n\\", number=number) time_iterative = timeit.timeit(\\"factorial_iterative(n)\\", setup=\\"from __main__ import factorial_iterative, n\\", number=number) time_reduce = timeit.timeit(\\"factorial_reduce(n)\\", setup=\\"from __main__ import factorial_reduce, n\\", number=number) print(f\\"Execution time of factorial_recursive: {time_recursive:.6f} s\\") print(f\\"Execution time of factorial_iterative: {time_iterative:.6f} s\\") print(f\\"Execution time of factorial_reduce: {time_reduce:.6f} s\\") if time_recursive < time_iterative and time_recursive < time_reduce: most_efficient = \\"factorial_recursive\\" elif time_iterative < time_recursive and time_iterative < time_reduce: most_efficient = \\"factorial_iterative\\" else: most_efficient = \\"factorial_reduce\\" print(f\\"The most efficient method is: {most_efficient}\\")"},{"question":"Create a function `safe_file_operation` that safely performs reading from or writing to a file based on given parameters, ensuring that the file is properly closed after the operation, even if an error occurs. To implement this function, you must utilize the `contextlib` module\'s `contextmanager` decorator. # Requirements 1. The function `safe_file_operation` should take three parameters: - `file_path`: A string representing the path to the file. - `operation`: A string that can either be `\\"read\\"` or `\\"write\\"`. - `content`: A string to be written to the file when the operation is `\\"write\\"` (optional, default is an empty string). 2. Depending on the operation specified: - If `operation` is `\\"read\\"`, the function should open the file in read mode and return its contents as a string. - If `operation` is `\\"write\\"`, the function should open the file in write mode and write the provided `content` to the file. 3. Use the `contextlib` module\'s `contextmanager` decorator to ensure that the file is properly closed after the operation, even if an error occurs during file access. # Input - `file_path`: (str) Path to the file to read from or write to. - `operation`: (str) Operation to be performed, should be either `\\"read\\"` or `\\"write\\"`. - `content`: (str) Content to write to the file when the operation is `\\"write\\"`; default is \\"\\". # Output - For the `\\"read\\"` operation: Return the content of the file as a string. - For the `\\"write\\"` operation: Return a success message `f\\"Content written to {file_path}\\"`. # Constraints - The `file_path` must be a valid path to a file. - The `operation` parameter must be either `\\"read\\"` or `\\"write\\"`. - If `operation` is `\\"write\\"`, the `content` should be written into the file. # Example Example 1 ```python result = safe_file_operation(\'example.txt\', \'write\', \'Hello, world!\') print(result) # Output: Content written to example.txt ``` Example 2 ```python # Assuming \'example.txt\' contains \'Hello, world!\' result = safe_file_operation(\'example.txt\', \'read\') print(result) # Output: Hello, world! ``` # Implementation Notes - Use the `contextlib.contextmanager` decorator to manage the file opening and closing. - Ensure proper handling of exceptions and resource release on errors.","solution":"import contextlib @contextlib.contextmanager def open_file(file_path, mode): Context manager to safely open and close a file. Args: - file_path: Path to the file. - mode: Mode in which to open the file. Yields: - file object file = None try: file = open(file_path, mode) yield file finally: if file is not None: file.close() def safe_file_operation(file_path, operation, content=\\"\\"): Safely performs reading from or writing to a file. Args: - file_path: Path to the file. - operation: Operation to perform, either \'read\' or \'write\'. - content: Content to write to the file (only used if operation is \'write\'). Returns: - When operation is \'read\': Content of the file as a string. - When operation is \'write\': Success message. if operation == \\"read\\": with open_file(file_path, \\"r\\") as file: return file.read() elif operation == \\"write\\": with open_file(file_path, \\"w\\") as file: file.write(content) return f\\"Content written to {file_path}\\" else: raise ValueError(\\"Operation must be either \'read\' or \'write\'\\")"},{"question":"Using the provided \\"uu\\" module, implement a Python function that takes an input file, encodes it using uuencode, then decodes it back and checks if the original file content is the same as the decoded content. Your function should handle potential errors and provide meaningful messages. You are required to implement the function as described below: # Function Specification: **Function Name**: `encode_and_verify(filepath: str) -> bool` **Input**: - `filepath`: A string representing the path to the input file. **Output**: - Returns `True` if the decoded content matches the original file content, `False` otherwise. **Constraints**: - You must use the `uu.encode` and `uu.decode` functions to perform encoding and decoding. - Handle exceptions raised during encoding/decoding and return `False` if any exceptions occur. - Assume the `filepath` is valid and the file exists. # Example: ```python # Assume \'example.txt\' contains the text \\"Hello, World!\\" result = encode_and_verify(\'example.txt\') print(result) # Should print: True if encoding and decoding are correct ``` # Implementation Details: 1. **Reading the Original File**: - Read the content of `filepath`. 2. **Encoding the File**: - Encode the original file using the `uu.encode` function and write the encoded content to a temporary file. 3. **Decoding the File**: - Decode the temporary encoded file using the `uu.decode` function and write the decoded content to another temporary file. 4. **Comparing the Content**: - Compare the content of the original file and the decoded file. - If the contents match, return `True`. - Handle any `uu.Error` exceptions during this process and return `False` if any errors occur. # Note: Make sure to clean up any temporary files created during the encoding/decoding process.","solution":"import uu import os import tempfile def encode_and_verify(filepath: str) -> bool: try: # Read the original file content with open(filepath, \'rb\') as original_file: original_content = original_file.read() # Create a temporary file for encoding with tempfile.NamedTemporaryFile(delete=False) as encoded_file: encoded_filepath = encoded_file.name uu.encode(filepath, encoded_filepath) # Create a temporary file for decoding with tempfile.NamedTemporaryFile(delete=False) as decoded_file: decoded_filepath = decoded_file.name uu.decode(encoded_filepath, decoded_filepath) # Read the decoded file content with open(decoded_filepath, \'rb\') as decoded_file: decoded_content = decoded_file.read() # Clean up the temporary files os.remove(encoded_filepath) os.remove(decoded_filepath) # Compare original and decoded content return original_content == decoded_content except (uu.Error, IOError) as e: # In case of any error, print the error and return False print(f\\"Error during encoding/decoding: {e}\\") return False"},{"question":"# Tkinter Message Box Utility You are tasked with creating a utility class using the tkinter.messagebox module. This class, `MessageBoxUtility`, should provide methods to display different types of message boxes and handle their responses. Requirements: 1. Create a class `MessageBoxUtility` with the following methods: - `show_information(self, title: str, message: str) -> None`: Displays an information message box with the given title and message. - `show_warning(self, title: str, message: str) -> None`: Displays a warning message box with the given title and message. - `show_error(self, title: str, message: str) -> None`: Displays an error message box with the given title and message. - `ask_yes_no(self, title: str, message: str) -> bool`: Displays a yes/no question dialog with the given title and message. Returns `True` if \'Yes\' is clicked, and `False` if \'No\' is clicked. - `ask_ok_cancel(self, title: str, message: str) -> bool`: Displays an OK/Cancel question dialog with the given title and message. Returns `True` if \'OK\' is clicked, and `False` if \'Cancel\' is clicked. - `ask_retry_cancel(self, title: str, message: str) -> bool`: Displays a Retry/Cancel question dialog with the given title and message. Returns `True` if \'Retry\' is clicked, and `False` if \'Cancel\' is clicked. - `ask_yes_no_cancel(self, title: str, message: str) -> Union[bool, None]`: Displays a yes/no/cancel question dialog with the given title and message. Returns `True` if \'Yes\' is clicked, `False` if \'No\' is clicked, and `None` if \'Cancel\' is clicked. 2. Ensure that the message boxes are modal and capture the input focus until they are dismissed. Constraints: - You must use the `tkinter.messagebox` module. - Each method should handle the display of the appropriate message box and return or process the correct value based on the user\'s interaction. Example Usage: ```python def main(): utility = MessageBoxUtility() utility.show_information(\\"Info\\", \\"This is an information message.\\") utility.show_warning(\\"Warning\\", \\"This is a warning message.\\") utility.show_error(\\"Error\\", \\"This is an error message.\\") response = utility.ask_yes_no(\\"Question\\", \\"Do you want to proceed?\\") print(f\\"User response: {response}\\") response = utility.ask_ok_cancel(\\"Question\\", \\"Would you like to confirm?\\") print(f\\"User response: {response}\\") response = utility.ask_retry_cancel(\\"Retry\\", \\"Would you like to retry?\\") print(f\\"User response: {response}\\") response = utility.ask_yes_no_cancel(\\"Complex Question\\", \\"Choose an option.\\") print(f\\"User response: {response}\\") if __name__ == \\"__main__\\": main() ``` Develop the `MessageBoxUtility` class and its methods as specified to meet the requirements.","solution":"import tkinter.messagebox as mb class MessageBoxUtility: def show_information(self, title: str, message: str) -> None: Displays an information message box with the given title and message. mb.showinfo(title, message) def show_warning(self, title: str, message: str) -> None: Displays a warning message box with the given title and message. mb.showwarning(title, message) def show_error(self, title: str, message: str) -> None: Displays an error message box with the given title and message. mb.showerror(title, message) def ask_yes_no(self, title: str, message: str) -> bool: Displays a yes/no question dialog with the given title and message. Returns True if \'Yes\' is clicked, and False if \'No\' is clicked. return mb.askyesno(title, message) def ask_ok_cancel(self, title: str, message: str) -> bool: Displays an OK/Cancel question dialog with the given title and message. Returns True if \'OK\' is clicked, and False if \'Cancel\' is clicked. return mb.askokcancel(title, message) def ask_retry_cancel(self, title: str, message: str) -> bool: Displays a Retry/Cancel question dialog with the given title and message. Returns True if \'Retry\' is clicked, and False if \'Cancel\' is clicked. return mb.askretrycancel(title, message) def ask_yes_no_cancel(self, title: str, message: str) -> bool: Displays a yes/no/cancel question dialog with the given title and message. Returns True if \'Yes\' is clicked, False if \'No\' is clicked, and None if \'Cancel\' is clicked. return mb.askyesnocancel(title, message)"},{"question":"**Coding Assessment Question** **Objective**: To assess students\' understanding of fundamental and advanced concepts of the `curses` module in Python. # Problem Statement Create a terminal-based text editor application using the `curses` module. The text editor should function similarly to a basic note-taking tool with the following features: 1. **Initialization and Termination**: - Initialize the `curses` application. - Handle proper termination to restore terminal state even if an error occurs. 2. **Text Window**: - Create a window that covers the entire screen and acts as the main text area. - Allow the text window to scroll when the text exceeds the window\'s height. 3. **Text Input and Display**: - Capture user input and display it in the text window. - Support basic text attributes (bold and underline) that can be toggled on and off using specific key combinations. - Ctrl+B to toggle bold text. - Ctrl+U to toggle underline text. 4. **Simple Commands**: - Save the current text to a file. Use `Ctrl+S` to save the file. - Clear the current text window using `Ctrl+C`. - Exit the editor using `Ctrl+Q`. # Input and Output Specifications - **Input**: User\'s keystrokes to input text and execute commands. - **Output**: Displayed text in the terminal window with appropriate attributes (bold or underline). # Constraints - Only ASCII characters need to be supported. - The application should be responsive and handle large amounts of text efficiently. - Ensure the proper restoration of terminal settings upon application exit or crash. # Performance Requirements - Efficiently handle text input and screen updates without noticeable lag for large text inputs. ```python import curses def main(stdscr): # Clear screen stdscr.clear() # Set up window max_y, max_x = stdscr.getmaxyx() textwin = curses.newwin(max_y - 1, max_x - 1, 0, 0) # Enable keyboard input curses.cbreak() curses.noecho() stdscr.keypad(True) try: # Application main loop bold = False underline = False content = \'\' cursor_y, cursor_x = 0, 0 while True: key = textwin.getch() # Handling special key combinations if key == curses.KEY_CTRL_Q: break elif key == curses.KEY_CTRL_S: # Save file functionality with open(\\"output.txt\\", \\"w\\") as f: f.write(content) elif key == curses.KEY_CTRL_C: # Clear screen content = \'\' cursor_y, cursor_x = 0, 0 textwin.clear() elif key == curses.KEY_CTRL_B: bold = not bold elif key == curses.KEY_CTRL_U: underline = not underline else: # Add character to content if key == curses.KEY_BACKSPACE: if len(content) > 0: content = content[:-1] textwin.delch(cursor_y, cursor_x - 1) cursor_x -= 1 else: attr = (curses.A_BOLD if bold else 0) | (curses.A_UNDERLINE if underline else 0) textwin.addch(cursor_y, cursor_x, key, attr) content += chr(key) cursor_x += 1 # Refresh the window to update display textwin.refresh() except Exception as e: stdscr.addstr(0, 0, f\\"An error occurred: {str(e)}\\") stdscr.refresh() stdscr.getkey() finally: curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main) ``` # Additional Information - The `Ctrl` key combinations are simulated by `curses.KEY_CTRL_Q`, `curses.KEY_CTRL_S`, `curses.KEY_CTRL_C`, `curses.KEY_CTRL_B`, and `curses.KEY_CTRL_U`. You will need to implement these and ensure proper handling of special key inputs. - Make sure to handle corner cases such as attempting to move the cursor out of bounds, modifying the window size, and saving large texts efficiently.","solution":"import curses def main(stdscr): # Initialize the curses application curses.curs_set(1) stdscr.clear() # Create text window covering the entire screen max_y, max_x = stdscr.getmaxyx() textwin = curses.newwin(max_y, max_x, 0, 0) # Enable keyboard input in the right mode curses.cbreak() curses.noecho() stdscr.keypad(True) bold = False underline = False content = [] cursor_y, cursor_x = 0, 0 while True: key = textwin.getch() if key == curses.KEY_CTRL_Q: break elif key == ord(\'x13\'): # Ctrl+S with open(\\"output.txt\\", \\"w\\") as f: for line in content: f.write(line + \'n\') elif key == ord(\'x03\'): # Ctrl+C content = [] cursor_y, cursor_x = 0, 0 textwin.clear() elif key == ord(\'x02\'): # Ctrl+B bold = not bold elif key == ord(\'x15\'): # Ctrl+U underline = not underline elif key in [curses.KEY_ENTER, ord(\'n\')]: content.append(\'\') cursor_y += 1 cursor_x = 0 else: if key == curses.KEY_BACKSPACE: if cursor_x > 0: content[cursor_y] = content[cursor_y][:-1] cursor_x -= 1 textwin.delch(cursor_y, cursor_x) else: try: if bold: textwin.attron(curses.A_BOLD) if underline: textwin.attron(curses.A_UNDERLINE) if len(content) <= cursor_y: content.append(chr(key)) else: if len(content[cursor_y]) > cursor_x: content[cursor_y] = content[cursor_y][:cursor_x] + chr(key) + content[cursor_y][cursor_x:] else: content[cursor_y] += chr(key) textwin.addch(cursor_y, cursor_x, key) if bold: textwin.attroff(curses.A_BOLD) if underline: textwin.attroff(curses.A_UNDERLINE) cursor_x += 1 except curses.error: pass textwin.refresh() # Restore terminal settings curses.nocbreak() stdscr.keypad(False) curses.echo() curses.endwin() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Python Coding Assessment: File Descriptor Control with `fcntl` Objective Develop a function to manage file descriptor properties and file locks using the `fcntl` module. Problem Statement You are required to implement two functions using the `fcntl` module in Python. The first function should modify and retrieve the file status flags of a file, and the second should apply and check file locks on the file descriptor. Function 1: `modify_file_flags(fd: int) -> int` 1. **Input**: - `fd` (int): A file descriptor obtained from opening a file. 2. **Output**: - Returns the file status flags after setting the `O_NONBLOCK` flag on the file descriptor. 3. **Implementation Details**: - Use `fcntl.fcntl` to get the current file status flags. - Modify the flags to include `os.O_NONBLOCK`. - Use `fcntl.fcntl` to set the modified flags back to the file descriptor. - Return the modified flags. Example: ```python fd = os.open(\'example.txt\', os.O_RDONLY) flags = modify_file_flags(fd) print(flags) # Should include O_NONBLOCK flag os.close(fd) ``` Function 2: `apply_and_check_lock(fd: int) -> bool` 1. **Input**: - `fd` (int): A file descriptor obtained from opening a file. 2. **Output**: - Returns `True` if the lock was successfully acquired and verified, `False` otherwise. 3. **Implementation Details**: - Use `fcntl.lockf` to apply an exclusive lock (`LOCK_EX`) to the entire file. - Verify if the lock was applied correctly by checking the absence of any exceptions. - Release the lock using `LOCK_UN`. - Return `True` if the locking and unlocking operations were successful, otherwise return `False`. Example: ```python fd = os.open(\'example.txt\', os.O_RDWR) lock_acquired = apply_and_check_lock(fd) print(lock_acquired) # Should print True if lock was acquired and released successfully os.close(fd) ``` Constraints - Assume the file descriptor passed to functions is valid and corresponds to a regular file. - Handle potential `OSError` exceptions appropriately. - Perform operations in a way that minimizes the risk of deadlocks. Notes - You may need to import additional modules such as `os` and `fcntl`. - Please ensure proper opening and closing of file descriptors in your testing, though your functions will only deal with already opened file descriptors.","solution":"import os import fcntl def modify_file_flags(fd: int) -> int: Modify the file descriptor to add the O_NONBLOCK flag and return the updated status flags. current_flags = fcntl.fcntl(fd, fcntl.F_GETFL) new_flags = current_flags | os.O_NONBLOCK fcntl.fcntl(fd, fcntl.F_SETFL, new_flags) return new_flags def apply_and_check_lock(fd: int) -> bool: Apply an exclusive lock to the file descriptor and returns True if successful, False otherwise. try: fcntl.lockf(fd, fcntl.LOCK_EX) fcntl.lockf(fd, fcntl.LOCK_UN) return True except OSError: return False"},{"question":"You are given a list of colors in RGB format, and you need to convert them to YIQ, HLS, and HSV formats using the `colorsys` module. Your task is to write a function that accepts this list of RGB colors and returns a dictionary with three keys: `\'yiq\'`, `\'hls\'`, and `\'hsv\'`. Each key should map to a list of the converted colors in the respective format. # Function Signature ```python def convert_colors(rgb_colors: List[Tuple[float, float, float]]) -> Dict[str, List[Tuple[float, float, float]]]: pass ``` # Input - `rgb_colors`: A list of tuples, where each tuple contains three floating-point numbers (ranging from 0 to 1) representing an RGB color. # Output - Returns a dictionary with three keys: - `\'yiq\'`: A list of tuples, each containing the YIQ representation of the original colors. - `\'hls\'`: A list of tuples, each containing the HLS representation of the original colors. - `\'hsv\'`: A list of tuples, each containing the HSV representation of the original colors. # Example ```python rgb_colors = [(0.2, 0.4, 0.4), (0.8, 0.1, 0.1)] output = convert_colors(rgb_colors) # Expected output (values may vary slightly due to floating-point precision): # { # \'yiq\': [(0.346, 0.0704, 0.0704), (0.412, 0.574, -0.1264)], # \'hls\': [(0.5, 0.3, 0.3333), (0.0, 0.45, 0.8)], # \'hsv\': [(0.5, 0.5, 0.4), (0.0, 0.875, 0.8)] # } ``` # Constraints - All input RGB values are within the range [0, 1]. # Notes - Use the `colorsys` module for the conversions. - Make sure to handle floating-point precision appropriately.","solution":"import colorsys from typing import List, Tuple, Dict def convert_colors(rgb_colors: List[Tuple[float, float, float]]) -> Dict[str, List[Tuple[float, float, float]]]: yiq_colors = [colorsys.rgb_to_yiq(r, g, b) for r, g, b in rgb_colors] hls_colors = [colorsys.rgb_to_hls(r, g, b) for r, g, b in rgb_colors] hsv_colors = [colorsys.rgb_to_hsv(r, g, b) for r, g, b in rgb_colors] return { \'yiq\': yiq_colors, \'hls\': hls_colors, \'hsv\': hsv_colors }"},{"question":"# Accessing and Managing Annotations in Python Objective Your task is to implement a function that safely retrieves and evaluates the annotations dictionary for given objects in Python. The function should support different types of objects (functions, classes, and modules) and handle the stringized annotations properly by evaluating them. Function Signature ```python def get_and_evaluate_annotations(obj: object) -> dict: This function retrieves and evaluates the annotations dictionary for a given object. Parameters: obj (object): The object whose annotations are to be retrieved. This can be a function, class, or module. Returns: dict: A dictionary representation of evaluated annotations. ``` Inputs - `obj`: The object whose `__annotations__` need to be retrieved and evaluated. This can be any callable object, class, or module. Outputs - A dictionary containing the evaluated annotations of the given object. Constraints - You should support Python versions 3.9 and newer. - If the object does not have annotations, return an empty dictionary. - If the annotations are stringized, evaluate them to their appropriate Python values. Example ```python from typing import List class MyClass: a: int b: List[str] def my_function(x: \'int\', y: \'List[str]\') -> \'None\': pass annotations_class = get_and_evaluate_annotations(MyClass) assert annotations_class == {\'a\': int, \'b\': List[str]} annotations_function = get_and_evaluate_annotations(my_function) assert annotations_function == {\'x\': int, \'y\': List[str], \'return\': None} ``` Notes - Use the directives provided in the documentation to handle different Python versions correctly. - Ensure that your function does not modify the original `__annotations__` attribute of the input object. - Properly handle cases where annotations might not be present. Hints - You may use the `eval` function to evaluate stringized annotations. - The `inspect` module can be helpful for accessing annotations. - Remember to handle the global and local namespaces correctly when evaluating stringized annotations.","solution":"import typing import types import sys def get_and_evaluate_annotations(obj: object) -> dict: Retrieves and evaluates the annotations dictionary for a given object. Parameters: obj (object): The object whose annotations are to be retrieved. This can be a function, class, or module. Returns: dict: A dictionary representation of evaluated annotations. annotations = {} if hasattr(obj, \'__annotations__\'): annotations = obj.__annotations__ elif isinstance(obj, types.ModuleType) and hasattr(obj, \'__dict__\'): annotations = {key: value.__annotations__ for key, value in obj.__dict__.items() if hasattr(value, \'__annotations__\')} else: return {} evaluated_annotations = {} for name, annotation in annotations.items(): if isinstance(annotation, str): try: evaluated_annotations[name] = eval(annotation, obj.__globals__) except NameError: evaluated_annotations[name] = eval(annotation, sys.modules[obj.__module__].__dict__) else: evaluated_annotations[name] = annotation return evaluated_annotations"},{"question":"# Seaborn Theme and Display Configuration You are provided with the following dataset, which contains information about different species of the `Iris` flower: ```python data = { \'sepal_length\': [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6], \'sepal_width\': [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4], \'petal_length\': [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4], \'petal_width\': [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\'] } ``` Your task is to create a Seaborn Scatterplot using this dataset and apply specific theme and display configurations as described below: 1. Set the background color of the axes to `white`. 2. Use the `whitegrid` style for the plot. 3. Sync the plot theme with Matplotlib\'s global state using `mpl.rcParams`. 4. Render the plot as an SVG image. 5. Disable HiDPI scaling for the plot. 6. Adjust the embedded image scaling factor to `0.7`. The expected output is an SVG image of a scatter plot with the specified configurations. # Implementation 1. **Input**: Use the provided `data` dictionary. 2. **Output**: An SVG image of the scatter plot, displayed in the Jupyter Notebook. 3. **Constraints**: Use the Seaborn\'s `objects` API and the `so.Plot` class. 4. **Notes**: Ensure that all theme and display configurations are applied to the plot. Please write the code to achieve this task. Make sure to follow the guidelines provided.","solution":"import matplotlib.pyplot as plt import seaborn.objects as so import seaborn as sns import pandas as pd def create_scatterplot(data): # Create a DataFrame from the provided data df = pd.DataFrame(data) # Set Seaborn theme sns.set_theme(style=\\"whitegrid\\") # Sync with Matplotlib\'s global state plt.rcParams[\\"axes.facecolor\\"] = \\"white\\" plt.rcParams[\\"figure.dpi\\"] = 100 # Disable HiDPI scaling plt.rcParams[\\"savefig.dpi\\"] = 100 plt.rcParams[\\"image.interpolation\\"] = \\"none\\" plt.rcParams[\\"image.resample\\"] = False # Create a scatter plot using seaborn.objects API plot = so.Plot(df, x=\\"sepal_length\\", y=\\"sepal_width\\", color=\\"species\\").add(so.Dots()) # Convert the plot to SVG and display it plt.figtext(0.5, 0.01, \'Iris Setosa Scatter Plot\', ha=\'center\', fontsize=12) plot.save(\\"plot.svg\\", format=\\"svg\\", dpi=100, bbox_inches=\'tight\') return plot # Example usage data = { \'sepal_length\': [5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6], \'sepal_width\': [3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4], \'petal_length\': [1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4], \'petal_width\': [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3], \'species\': [\'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\', \'setosa\'] } create_scatterplot(data)"},{"question":"You are given a dataset containing information about passengers on the Titanic. Using seaborn, you need to generate a series of violin plots to explore the distribution of some key features across various categories. # Tasks: 1. **Load the Titanic dataset**: - Use the seaborn library to load the \\"titanic\\" dataset into a DataFrame. 2. **Basic Violin Plot**: - Create a violin plot to display the distribution of passengers\' age. 3. **Bivariate Violin Plot**: - Create a bivariate violin plot to display the distribution of age across different classes (First, Second, Third). 4. **Violin Plot with Hue**: - Create a violin plot to display the age distribution for different classes, with an additional hue parameter to differentiate between passengers who survived and those who did not. 5. **Customized Violin Plot**: - Create a violin plot with the following customizations: - Split the violins to show both survived and not survived within the same class. - Use `inner=\\"quart\\"` to only show the data quartiles. - Adjust the bandwidth using `bw_adjust=0.7`. - Normalize the width of each violin to represent the number of observations using `density_norm=\\"count\\"`. - Add a title \\"Age Distribution by Class and Survival\\". # Input and Output: - **Input**: The function should not take any input from the user. - **Output**: The function should display the specified violin plots directly using seaborn. # Constraints and Requirements: - You are only allowed to use seaborn and other standard Python libraries (like pandas) for data manipulation and plotting. - Ensure your plots have clear labels and titles to make them understandable. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt def generate_violin_plots(): # Load the dataset df = sns.load_dataset(\\"titanic\\") # Task 2: Basic Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"age\\"]) plt.title(\\"Age Distribution of Titanic Passengers\\") plt.show() # Task 3: Bivariate Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\") plt.title(\\"Age Distribution by Class\\") plt.show() # Task 4: Violin Plot with Hue plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True) plt.title(\\"Age Distribution by Class and Survival\\") plt.show() # Task 5: Customized Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\", bw_adjust=0.7, density_norm=\\"count\\") plt.title(\\"Age Distribution by Class and Survival\\") plt.show() generate_violin_plots() ``` This problem assess a student\'s comprehension of: - Loading and handling data with seaborn. - Basic and advanced plotting of violin plots. - Implementation of multiple seaborn `violinplot` parameters.","solution":"import seaborn as sns import matplotlib.pyplot as plt def generate_violin_plots(): # Task 1: Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Task 2: Basic Violin Plot for Age Distribution plt.figure(figsize=(10, 6)) sns.violinplot(x=df[\\"age\\"]) plt.title(\\"Age Distribution of Titanic Passengers\\") plt.xlabel(\\"Age\\") plt.show() # Task 3: Bivariate Violin Plot for Age Distribution by Class plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\") plt.title(\\"Age Distribution by Class\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Task 4: Violin Plot with Hue for Age Distribution by Class and Survival plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True) plt.title(\\"Age Distribution by Class and Survival\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show() # Task 5: Customized Violin Plot plt.figure(figsize=(10, 6)) sns.violinplot(data=df, x=\\"class\\", y=\\"age\\", hue=\\"alive\\", split=True, inner=\\"quart\\", bw_adjust=0.7, cut=0) plt.title(\\"Age Distribution by Class and Survival\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.show()"},{"question":"# Advanced DataFrame Manipulation and Analysis with Pandas Objective The objective of this question is to assess your ability to work with Pandas DataFrame operations, including merging, concatenating, and comparing datasets. This will demonstrate your understanding of fundamental and advanced concepts of the Pandas package. Problem Statement You are provided with sales data from two different regions for the first quarter of the year, stored in two separate CSV files. Each file contains information about the date, product, and the number of units sold. Your task is to perform the following operations and report the results: 1. **Load the Data**: Load the two CSV files into two separate DataFrame objects (`df1` for region 1 and `df2` for region 2). 2. **Concatenate the DataFrames**: Concatenate the two DataFrames to create a single DataFrame while preserving the original index. 3. **Add a Region Column**: Add a new column to indicate the region from which each row of data originated (either \\"Region 1\\" or \\"Region 2\\"). 4. **Merge with Product Information**: You are provided with another CSV file containing information about products (like `product_id`, `product_name`, and `category`). Merge this product information with your concatenated DataFrame on `product_id`. 5. **Identify Missing Data**: Identify any missing data in your merged DataFrame and update these missing values using relevant information from the more comprehensive CSV file, if available. 6. **Comparison**: Before and after filling in the missing values, compare the two DataFrames and summarize the differences. 7. **Save Results**: Save your final DataFrame result to a new CSV file named `merged_sales_data.csv`. Input Format 1. `region1_sales.csv` - Contains sales data from Region 1 (columns: date, product_id, units_sold). 2. `region2_sales.csv` - Contains sales data from Region 2 (columns: date, product_id, units_sold). 3. `product_info.csv` - Contains product information (columns: product_id, product_name, category). Output Format 1. `merged_sales_data.csv` - The final merged DataFrame saved as a CSV file. Constraints - Assume no duplicate entries within the individual region datasets. - Ensure the date column is converted to datetime format during the loading process. - Handle missing data appropriately using methods demonstrated in the documentation. Example Assume the following data: `region1_sales.csv`: ``` date,product_id,units_sold 2023-01-01,1,10 2023-02-01,2,20 2023-03-01,3,30 ``` `region2_sales.csv`: ``` date,product_id,units_sold 2023-01-01,2,15 2023-02-01,3,25 2023-03-01,4,35 ``` `product_info.csv`: ``` product_id,product_name,category 1,Gadget,A 2,Widget,B 3,Thingamajig,C 4,Doohickey,D ``` Template for Solution ```python import pandas as pd # Step 1: Load the Data df1 = pd.read_csv(\'region1_sales.csv\', parse_dates=[\'date\']) df2 = pd.read_csv(\'region2_sales.csv\', parse_dates=[\'date\']) product_info = pd.read_csv(\'product_info.csv\') # Step 2: Concatenate the DataFrames concatenated_df = pd.concat([df1, df2], ignore_index=False) # Step 3: Add a Region Column concatenated_df[\'region\'] = \'Region 1\' concatenated_df.loc[df2.index, \'region\'] = \'Region 2\' # Step 4: Merge with Product Information merged_df = pd.merge(concatenated_df, product_info, on=\'product_id\', how=\'left\') # Step 5: Identify and Update Missing Data # Assuming we might update from some additional source # Step 6: Comparison # making a copy of the merged DF for comparison: merged_df_before_fill = merged_df.copy() # Update missing values here... # For example: merged_df[\'units_sold\'].fillna(merged_df[\'units_sold\'].mean(), inplace=True) comparison_result = merged_df_before_fill.compare(merged_df, keep_shape=True) # Step 7: Save the Results merged_df.to_csv(\'merged_sales_data.csv\', index=False) # Optionally print comparison_result to stdout for review print(comparison_result) ``` Note: Make sure to provide the path to the CSV files if they are not in the same directory as your script.","solution":"import pandas as pd def process_sales_data(region1_path, region2_path, product_info_path, output_path): This function processes sales data from two different regions by loading, concatenating, merging with product information, handling missing data, and saving the final results. # Step 1: Load the Data df1 = pd.read_csv(region1_path, parse_dates=[\'date\']) df2 = pd.read_csv(region2_path, parse_dates=[\'date\']) product_info = pd.read_csv(product_info_path) # Step 2: Concatenate the DataFrames concatenated_df = pd.concat([df1, df2], ignore_index=False) # Step 3: Add a Region Column concatenated_df[\'region\'] = \'Region 1\' concatenated_df.loc[df2.index, \'region\'] = \'Region 2\' # Step 4: Merge with Product Information merged_df = pd.merge(concatenated_df, product_info, on=\'product_id\', how=\'left\') # Step 5: Identify and Update Missing Data missing_data = merged_df.isnull().sum() # Fill missing product_name and category if any if missing_data.any(): if \'product_name\' in missing_data: merged_df[\'product_name\'].fillna(\'Unknown Product\', inplace=True) if \'category\' in missing_data: merged_df[\'category\'].fillna(\'Unknown Category\', inplace=True) # Step 6: Comparison # Make a copy of the merged DataFrame before filling missing values merged_df_before_fill = merged_df.copy() # Comparison result (detect changes) comparison_result = merged_df_before_fill.compare(merged_df, keep_shape=True).dropna(how=\'all\') # Step 7: Save the Results merged_df.to_csv(output_path, index=False) return concatenated_df, merged_df, comparison_result # Example usage: # concatenated_df, merged_df, comparison_result = process_sales_data(\'region1_sales.csv\', \'region2_sales.csv\', \'product_info.csv\', \'merged_sales_data.csv\')"},{"question":"Objective: You are required to use the `unicodedata` module to implement a function that processes a given Unicode string. The function should verify the normalization of the string, provide detailed information about each character in the string, and optionally transform it based on specific character properties. Function Name: `process_unicode_string` Input: - `unistr` (str): A Unicode string to be processed. - `form` (str): A normalization form, one of `\'NFC\'`, `\'NFKC\'`, `\'NFD\'`, or `\'NFKD\'` (default is `\'NFC\'`). Output: A dictionary with the following structure: ```python { \\"is_normalized\\": bool, # Whether the input string is in the specified normalization form \\"normalized_string\\": str, # The normalized form of the input string \\"character_details\\": [ # List of dictionaries containing details about each character in the string { \\"character\\": str, \\"name\\": str, \\"decimal\\": int or None, \\"digit\\": int or None, \\"numeric\\": float or None, \\"category\\": str, \\"bidirectional\\": str, \\"combining\\": int, \\"east_asian_width\\": str, \\"mirrored\\": int, \\"decomposition\\": str }, ... ] } ``` Constraints: - If a character does not have a decimal, digit, or numeric property, return `None` for that property in the `character_details` output. - The input string will have at most a length of 1000 characters. Example: ```python import unicodedata def process_unicode_string(unistr: str, form: str = \'NFC\') -> dict: # Your implementation here # Example usage unistr = \\"Café 𝔘𝔫𝔦𝔠𝔬𝔡𝔢\\" form = \'NFD\' result = process_unicode_string(unistr, form) print(result) ``` Explanation: 1. **Normalization Check:** First, check if the input string `unistr` is in the specified normalization form `form` using `is_normalized`. 2. **Normalization:** Normalize the string using `normalize` and store the result. 3. **Character Details:** For each character in the normalized string, extract its various properties like name, decimal value, digit value, numeric value, general category, bidirectional class, combining class, East Asian width, mirrored property, and decomposition mapping, and store these details in a structured format. The provided implementation should handle the unicode processing, character property extraction, and normalization effectively with proper error handling for characters without certain properties.","solution":"import unicodedata def process_unicode_string(unistr: str, form: str = \'NFC\') -> dict: is_normalized = unicodedata.is_normalized(form, unistr) normalized_string = unicodedata.normalize(form, unistr) character_details = [] for c in normalized_string: char_info = { \\"character\\": c, \\"name\\": unicodedata.name(c, \\"UNKNOWN\\"), \\"decimal\\": unicodedata.decimal(c, None), \\"digit\\": unicodedata.digit(c, None), \\"numeric\\": unicodedata.numeric(c, None), \\"category\\": unicodedata.category(c), \\"bidirectional\\": unicodedata.bidirectional(c), \\"combining\\": unicodedata.combining(c), \\"east_asian_width\\": unicodedata.east_asian_width(c), \\"mirrored\\": unicodedata.mirrored(c), \\"decomposition\\": unicodedata.decomposition(c) } character_details.append(char_info) return { \\"is_normalized\\": is_normalized, \\"normalized_string\\": normalized_string, \\"character_details\\": character_details }"},{"question":"**Coding Assessment Question:** Seaborn is a powerful visualization library in Python that allows for easy creation of informative and attractive statistical graphics. One of its features is the ability to generate color palettes to customize the appearance of plots. # Task: Write a function `create_custom_palette_plot` that generates a custom seaborn color palette using `sns.husl_palette()` and applies it to a bar plot of a given dataset. The function should allow the user to specify the number of colors, lightness, saturation, hue start-point, and whether the palette should be continuous. # Function Signature: ```python def create_custom_palette_plot(data, num_colors=6, lightness=0.5, saturation=0.9, hue_start=0.0, as_cmap=False): Generates a custom seaborn color palette and applies it to a bar plot. Parameters: - data (pandas.DataFrame): The input data for the bar plot. - num_colors (int): Number of colors in the palette. Default is 6. - lightness (float): Lightness of the colors, ranging from 0 to 1. Default is 0.5. - saturation (float): Saturation of the colors, ranging from 0 to 1. Default is 0.9. - hue_start (float): Start-point for hue sampling, ranging from 0 to 1. Default is 0.0. - as_cmap (bool): Whether to return a continuous colormap. Default is False. Returns: None Behavior: - The function should create a seaborn bar plot using the input data and custom color palette. - The plot should be displayed inline. pass ``` # Constraints: 1. The `data` parameter should be a pandas DataFrame containing numeric data suitable for a bar plot. 2. Ensure appropriate error handling for incorrect types or values of parameters. 3. Utilize seaborn and matplotlib libraries for plot creation and display. # Example Usage: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Sample data data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'D\'], \'Values\': [10, 20, 15, 25] }) # Create a custom palette plot create_custom_palette_plot(data, num_colors=4, lightness=0.6, saturation=0.7, hue_start=0.2, as_cmap=False) ``` This example should produce a bar plot with categories \'A\', \'B\', \'C\', and \'D\' on the x-axis and their respective values on the y-axis, using a custom color palette with the specified attributes. # Additional Notes: - Students should make use of `sns.barplot()` and ensure that the plot\'s color is set using the palette created by `sns.husl_palette()`. - Encourage the use of `sns.set_theme()` or other seaborn settings to improve the plot aesthetics.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_custom_palette_plot(data, num_colors=6, lightness=0.5, saturation=0.9, hue_start=0.0, as_cmap=False): Generates a custom seaborn color palette and applies it to a bar plot. Parameters: - data (pandas.DataFrame): The input data for the bar plot. DataFrame must contain \'Category\' and \'Values\' columns. - num_colors (int): Number of colors in the palette. Default is 6. - lightness (float): Lightness of the colors, ranging from 0 to 1. Default is 0.5. - saturation (float): Saturation of the colors, ranging from 0 to 1. Default is 0.9. - hue_start (float): Start-point for hue sampling, ranging from 0 to 1. Default is 0.0. - as_cmap (bool): Whether to return a continuous colormap. Default is False. Returns: None Behavior: - The function should create a seaborn bar plot using the input data and custom color palette. - The plot should be displayed inline. # Input validation if not isinstance(data, pd.DataFrame): raise TypeError(\\"data must be a pandas DataFrame\\") if \'Category\' not in data.columns or \'Values\' not in data.columns: raise ValueError(\\"data must contain \'Category\' and \'Values\' columns\\") if not isinstance(num_colors, int) or num_colors <= 0: raise ValueError(\\"num_colors must be a positive integer\\") if not (0 <= lightness <= 1): raise ValueError(\\"lightness must be between 0 and 1\\") if not (0 <= saturation <= 1): raise ValueError(\\"saturation must be between 0 and 1\\") if not (0 <= hue_start <= 1): raise ValueError(\\"hue_start must be between 0 and 1\\") # Create custom color palette palette = sns.husl_palette(n_colors=num_colors, l=lightness, s=saturation, h=hue_start) # Create the bar plot sns.set_theme(style=\\"whitegrid\\") plt.figure(figsize=(10, 6)) bar_plot = sns.barplot(x=\'Category\', y=\'Values\', data=data, palette=palette if not as_cmap else sns.color_palette(palette, as_cmap=True)) plt.show()"},{"question":"Advanced Python Memory Buffer Management # Objective Your task is to implement several Python functions to interact with memory buffers in a similar manner to the old buffer protocol described below. Your functions should handle both read and write operations, ensuring proper error handling and compatibility with single-segment buffers. # Python Functions to Implement: 1. **`as_char_buffer(obj)`**: - **Input**: A Python object that should support a buffer interface (e.g., a bytes object). - **Output**: A tuple containing a reference to a read-only character buffer and the length of the buffer. On error, it should raise a `TypeError`. 2. **`as_read_buffer(obj)`**: - **Input**: A Python object that should support a buffer interface. - **Output**: A tuple containing a reference to a read-only buffer with arbitrary data and the length of the buffer. On error, it should raise a `TypeError`. 3. **`check_read_buffer(o)`**: - **Input**: A Python object. - **Output**: Returns `True` if the object supports a readable buffer interface, else `False`. 4. **`as_write_buffer(obj)`**: - **Input**: A Python object that should support a writable buffer interface. - **Output**: A tuple containing a reference to a writable memory buffer and the length of the buffer. On error, it should raise a `TypeError`. # Constraints - Implementations must use Python\'s buffer protocol. - You must use appropriate exception handling and raise `TypeError` when a buffer interface is not supported. - The input object for write operations should be mutable (e.g., a `bytearray` object). # Example Usage ```python # Example of as_char_buffer usage buffer, length = as_char_buffer(b\'hello\') print(buffer) # Should print a bytes-like object print(length) # Should print 5 # Example of as_read_buffer usage buffer, length = as_read_buffer(b\'hello\') print(buffer) # Should print a bytes-like object print(length) # Should print 5 # Example of check_read_buffer usage supports_buffer = check_read_buffer(b\'hello\') print(supports_buffer) # Should print True # Example of as_write_buffer usage buffer, length = as_write_buffer(bytearray(b\'hello\')) print(buffer) # Should print a writable bytes-like object print(length) # Should print 5 ``` # Implementation Note You may find the built-in libraries `memoryview` or `bytearray` helpful in interacting with the buffer protocol.","solution":"def as_char_buffer(obj): Returns a reference to a read-only character buffer and its length. Raises TypeError if the object does not support the buffer interface. try: buffer = memoryview(obj).tobytes() length = len(buffer) return buffer, length except TypeError: raise TypeError(\\"Object does not support buffer interface\\") def as_read_buffer(obj): Returns a reference to a read-only buffer with arbitrary data and its length. Raises TypeError if the object does not support the buffer interface. try: buffer = memoryview(obj) length = len(buffer) return buffer, length except TypeError: raise TypeError(\\"Object does not support buffer interface\\") def check_read_buffer(o): Returns True if the object supports a readable buffer interface, else False. try: memoryview(o) return True except TypeError: return False def as_write_buffer(obj): Returns a reference to a writable memory buffer and its length. Raises TypeError if the object does not support a writable buffer interface. try: buffer = memoryview(obj) if not buffer.readonly: length = len(buffer) return buffer, length else: raise TypeError(\\"Object does not support writable buffer interface\\") except TypeError: raise TypeError(\\"Object does not support buffer interface\\")"},{"question":"You are given a 2D grayscale image represented as a PyTorch tensor. Your task is to implement a Python function using PyTorch that performs the following operations: 1. Compute the 2D Fast Fourier Transform (FFT) of the image. 2. Shift the zero frequency component to the center of the spectrum. 3. Apply a circular low-pass filter to the frequency domain representation of the image. 4. Shift the zero frequency component back to the original position. 5. Compute the inverse 2D FFT to obtain the filtered image. The circular low-pass filter should have a cutoff frequency such that all frequencies outside a circle of radius `cutoff_radius` (measured from the center of the frequency domain) are set to zero. # Function Signature ```python import torch def low_pass_filter(image: torch.Tensor, cutoff_radius: float) -> torch.Tensor: Applies a circular low-pass filter to a 2D grayscale image. Parameters: image (torch.Tensor): A 2D tensor representing the grayscale image. Shape: (H, W) cutoff_radius (float): The cutoff radius for the low-pass filter. Returns: torch.Tensor: The filtered image as a 2D tensor of the same shape (H, W). ``` # Input - `image`: A 2D tensor with shape `(H, W)`, where `H` and `W` are the height and width of the grayscale image. - `cutoff_radius`: A float representing the cutoff radius for the low-pass filter. # Output - A 2D tensor with the same shape as the input image `(H, W)` representing the filtered image. # Constraints 1. You must use PyTorch\'s `torch.fft` module for FFT computations. 2. The implementation should handle any image size. 3. The `cutoff_radius` must be a valid float value between 0 and min(H, W) / 2. # Example ```python import torch image = torch.rand((256, 256)) # Example input image cutoff_radius = 50.0 # Example cutoff radius filtered_image = low_pass_filter(image, cutoff_radius) print(filtered_image.shape) # Should output: torch.Size([256, 256]) ``` # Explanation 1. Compute the 2D FFT of the image using `torch.fft.fft2`. 2. Shift the zero-frequency component to the center using `torch.fft.fftshift`. 3. Create a circular mask for the low-pass filter. 4. Apply the mask to the frequency domain representation. 5. Shift the zero-frequency component back using `torch.fft.ifftshift`. 6. Compute the inverse 2D FFT using `torch.fft.ifft2`. 7. Return the real part of the inverse FFT as the filtered image. Implementing the function will test students\' understanding of PyTorch\'s FFT operations and their ability to manipulate frequency domain data effectively.","solution":"import torch def low_pass_filter(image: torch.Tensor, cutoff_radius: float) -> torch.Tensor: Applies a circular low-pass filter to a 2D grayscale image. Parameters: image (torch.Tensor): A 2D tensor representing the grayscale image. Shape: (H, W) cutoff_radius (float): The cutoff radius for the low-pass filter. Returns: torch.Tensor: The filtered image as a 2D tensor of the same shape (H, W). H, W = image.shape assert 0 < cutoff_radius <= min(H, W) / 2, \\"Invalid cutoff radius.\\" # Compute the 2D FFT of the image fft_image = torch.fft.fft2(image) # Shift the zero-frequency component to the center fft_shifted = torch.fft.fftshift(fft_image) # Create a circular mask for the low-pass filter Y, X = torch.meshgrid(torch.arange(H), torch.arange(W), indexing=\'ij\') center_y, center_x = H // 2, W // 2 distance = ((Y - center_y) ** 2 + (X - center_x) ** 2).sqrt() mask = distance <= cutoff_radius # Apply the mask to the frequency domain representation fft_filtered = fft_shifted * mask # Shift the zero-frequency component back to the original position fft_filtered_shifted_back = torch.fft.ifftshift(fft_filtered) # Compute the inverse 2D FFT to obtain the filtered image filtered_image = torch.fft.ifft2(fft_filtered_shifted_back) # Return the real part of the inverse FFT return filtered_image.real"},{"question":"# Question Using the `seaborn` library, visualize the `tips` dataset to uncover interesting relationships and patterns. Task 1. **Load the dataset**: - Load the `tips` dataset from `seaborn`. 2. **Scatter Plot with Size Customization**: - Create a scatter plot using the `scatterplot` function. - Map the `total_bill` to the x-axis, `tip` to the y-axis. - Use `size` to represent the `size` column. - Set the sizes range from 20 to 200 for the markers and ensure the legend shows all unique size values. 3. **Color Encoding**: - Map the variable `day` to color (`hue`). - Ensure you use a different palette for the `day` variable, for example, \'deep\' or \'bright\'. 4. **Multiple Subplots**: - Use `relplot` to show separate scatter plots for `Lunch` and `Dinner`. - In the facets, include `total_bill` on the x-axis and `tip` on the y-axis. - Use `hue` to differentiate `smoker` status and `style` to differentiate `sex` of the customers. # Constraints - You should use `seaborn` functions and the `matplotlib.pyplot` interface for this task. - Ensure the plots are legible and correctly labeled with appropriate titles and legends. - Handle the task in a Jupyter Notebook environment. # Expected Output - A single plot showing a scatter plot visualizing `total_bill` against `tip` with the points sized by `size` and colors differentiated by `day`. - Multiple subplots (faceting) based on `time` (Lunch vs Dinner) with custom `hue` and `style`. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Task 2: Scatter plot with size customization plt.figure(figsize=(10, 6)) scatter = sns.scatterplot(data=tips, x=\\"total_bill\\", y=\\"tip\\", size=\\"size\\", sizes=(20, 200), legend=\\"full\\", hue=\\"day\\", palette=\\"deep\\") plt.title(\\"Scatter plot of Total Bill vs Tip with Size based on `size` Variable\\") plt.legend(title=\'size\') plt.show() # Task 4: Multiple subplots rel_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"smoker\\", style=\\"sex\\", kind=\\"scatter\\" ) rel_plot.set_titles(\\"{col_name} time\\") rel_plot.fig.suptitle(\\"Facet plots for Lunch and Dinner Time\\") plt.show() ``` # Implementation Complete the tasks described using the provided code blocks as a starting point.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Scatter plot with size customization plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", size=\\"size\\", sizes=(20, 200), legend=\\"full\\", hue=\\"day\\", palette=\\"deep\\" ) plt.title(\\"Scatter plot of Total Bill vs Tip with Size based on `size` Variable\\") plt.legend(title=\'size\') plt.show() # Multiple subplots rel_plot = sns.relplot( data=tips, x=\\"total_bill\\", y=\\"tip\\", col=\\"time\\", hue=\\"smoker\\", style=\\"sex\\", kind=\\"scatter\\" ) rel_plot.set_titles(\\"{col_name} time\\") rel_plot.fig.suptitle(\\"Facet plots for Lunch and Dinner Time\\") plt.show()"},{"question":"# Asynchronous Task Scheduling and Handling You are required to write a Python program to demonstrate your understanding of advanced asynchronous programming using the `asyncio` module. Specifically, you need to implement a system that performs the following tasks simultaneously: 1. Fetch data from a remote server which simulates a delay. 2. Perform a CPU-bound computation on the fetched data. 3. Output the result after both tasks have been completed. Your solution should handle scenarios in which: * The remote server sometimes fails to respond. * The computation task takes a significant amount of time, potentially blocking other tasks from executing if not handled correctly. Requirements: 1. Implement a function `fetch_data` that simulates fetching data from a remote server. * This function should take an integer `delay` as input and sleep for that many seconds before returning a string `\\"data\\"`. * Introduce a 50% chance of raising an exception simulating a failed remote server response. 2. Implement a function `compute` that simulates a CPU-bound task. * This function should take the fetched data and an integer `compute_time` (in seconds) as input and perform a computation for that duration before returning the result `\\"computed_data\\"`. 3. Implement a main function to run these tasks concurrently using `asyncio`, ensuring: * Fetching data and computing run as concurrent tasks. * Any exceptions raised in the data fetching task are handled gracefully. * The program should print \\"data fetch failed\\" if the fetch_data task fails. * Print the time taken to complete the entire process. * Ensure the CPU-bound task does not block other tasks. Input & Output: * There are no direct inputs to the program apart from parameters within your functions. * The expected print output should be: * `\\"data fetch failed\\"` if `fetch_data` fails. * `\\"computed_data\\"` if the entire process completes successfully. * The time taken to complete the process (printed as a floating point number of seconds). Constraints: * Your program should handle delays and CPU operations without blocking the event loop. * Ensure proper exception handling and task cancellation where needed. * Use `asyncio` features to manage concurrency effectively. Example ```python import asyncio import random async def fetch_data(delay): await asyncio.sleep(delay) if random.choice([True, False]): raise Exception(\\"Server error\\") return \\"data\\" async def compute(data, compute_time): # Simulate CPU-bound task start = asyncio.get_running_loop().time() while True: if asyncio.get_running_loop().time() - start >= compute_time: break return \\"computed_data\\" async def main(): try: data = await fetch_data(2) except Exception as e: print(\\"data fetch failed\\") return result = await compute(data, 3) print(result) return # Use asyncio.run to execute the main function if __name__ == \'__main__\': import time start_time = time.time() asyncio.run(main()) print(\\"Time taken:\\", time.time() - start_time) ``` Note: 1. Make sure your function `compute` is designed to simulate CPU-bound computations without blocking the entire event loop during its execution. 2. Ensure to handle potential exceptions gracefully during data fetching. 3. Utilize `asyncio` best practices to keep your tasks non-blocking wherever possible.","solution":"import asyncio import random async def fetch_data(delay): await asyncio.sleep(delay) if random.choice([True, False]): raise Exception(\\"Server error\\") return \\"data\\" async def compute(data, compute_time): start = asyncio.get_running_loop().time() while True: if asyncio.get_running_loop().time() - start >= compute_time: break return \\"computed_data\\" async def main(): start_time = asyncio.get_running_loop().time() try: data = await fetch_data(2) except Exception as e: print(\\"data fetch failed\\") print(\\"Time taken:\\", asyncio.get_running_loop().time() - start_time) return result = await compute(data, 3) print(result) print(\\"Time taken:\\", asyncio.get_running_loop().time() - start_time) # Use asyncio.run to execute the main function if __name__ == \'__main__\': asyncio.run(main())"},{"question":"**PyTorch Tensor Attribute Manipulation** **Problem Statement:** In this task, you will implement a function in PyTorch that manipulates the attributes of Tensors. Specifically, you need to write a function `tensor_attribute_manipulation` that does the following: * Takes in three parameters: 1. A Tensor `x` which should have its attributes manipulated. 2. A string `dtype_str` representing the target data type for the Tensor. The string can be any of the following: `\\"float32\\"`, `\\"float64\\"`, `\\"int32\\"`, `\\"int64\\"`, `\\"bool\\"`. 3. An optional string parameter `device_str` representing the target device for the Tensor. The string can be either `\\"cpu\\"` or `\\"cuda:0\\"`. If the parameter is not provided, assume it to be `\\"cpu\\"`. * Converts the input Tensor to the target data type specified by `dtype_str`. * Converts the input Tensor to the target device specified by `device_str`. If `cuda:0` is provided but GPU is not available, default to `cpu`. * Returns a tuple of the following form: (`dtype`, `device`, `is_contiguous_format`, `stride`) where: - `dtype`: the data type of the Tensor after conversion. - `device`: the device of the Tensor after conversion. - `is_contiguous_format`: a boolean indicating whether the Tensor\'s memory is in contiguous format. - `stride`: a tuple representing the stride of the Tensor. **Function Signature:** ```python import torch def tensor_attribute_manipulation(x: torch.Tensor, dtype_str: str, device_str: str = \\"cpu\\") -> tuple: #Implementation goes here pass ``` **Example:** ```python # Example input x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32) dtype_str = \\"int64\\" device_str = \\"cpu\\" # Expected Output tensor_attribute_manipulation(x, dtype_str, device_str) # (torch.int64, device(type=\'cpu\'), True, (3, 1)) ``` **Explanation:** The function takes a 2x3 Tensor with `dtype=torch.float32` on CPU and converts it to `dtype=torch.int64` on CPU. The returned tuple contains: 1. The data type `torch.int64`. 2. The device as `cpu`. 3. A boolean indicating that the Tensor is in contiguous format. 4. The stride of the Tensor. **Constraints:** 1. The function should handle cases where the provided `dtype_str` is invalid by raising a `ValueError`. 2. The function should handle conversion to `cuda:0` only if GPU is available; otherwise, it should default to `cpu`. **Notes:** - Use `torch.device` for device management. - Use `torch.dtype` to verify and convert data types. - Ensure proper error handling and validation for the string inputs.","solution":"import torch def tensor_attribute_manipulation(x: torch.Tensor, dtype_str: str, device_str: str = \\"cpu\\") -> tuple: # Dictionary to map string to torch dtype dtype_map = { \\"float32\\": torch.float32, \\"float64\\": torch.float64, \\"int32\\": torch.int32, \\"int64\\": torch.int64, \\"bool\\": torch.bool } # Handling invalid dtype_str if dtype_str not in dtype_map: raise ValueError(f\\"Invalid dtype_str: {dtype_str}. Valid options are: {\', \'.join(dtype_map.keys())}\\") # Getting the target dtype target_dtype = dtype_map[dtype_str] # Handling the device strings target_device = torch.device(device_str if (device_str == \\"cpu\\" or (device_str == \\"cuda:0\\" and torch.cuda.is_available())) else \\"cpu\\") # Converting the tensor to the desired dtype and device x_converted = x.to(dtype=target_dtype, device=target_device) # Creating the return tuple result = ( x_converted.dtype, x_converted.device, x_converted.is_contiguous(), x_converted.stride() ) return result"},{"question":"# Question: Implementing Custom Operations and Modules with PyTorch In this exercise, you are required to demonstrate your understanding of extending PyTorch by creating a custom linear operation and integrating it within a custom neural network module. You are to implement the custom operation using `torch.autograd.Function` and build a corresponding PyTorch module that utilizes this custom operation. Requirements: 1. **Custom Linear Function**: - Implement a `MyLinearFunction` class by subclassing `torch.autograd.Function`. - Define the `forward` and `backward` static methods. - The `forward` method should perform a linear transformation: ( y = xW^T + b ) - The `backward` method should compute the gradients for the input, weights, and bias. 2. **Custom Linear Module**: - Implement a `MyLinear` class by subclassing `torch.nn.Module`. - The module should initialize the weights and bias. - The `forward` method of the module should use the `MyLinearFunction` to perform the forward pass. 3. **Testing**: - Create an instance of the custom module and test it with a simple input to verify the forward and backward pass. - Use `torch.autograd.gradcheck` to validate the gradients of the custom function. Specifications 1. **`MyLinearFunction` Class Implementation**: ```python # Custom Linear Function class MyLinearFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weight, bias): # Save tensors for backward pass output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) ctx.save_for_backward(input, weight, bias) return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias ``` 2. **`MyLinear` Class Implementation**: ```python # Custom Linear Module class MyLinear(torch.nn.Module): def __init__(self, input_features, output_features, bias=True): super(MyLinear, self).__init__() self.input_features = input_features self.output_features = output_features self.weight = torch.nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = torch.nn.Parameter(torch.Tensor(output_features)) else: self.register_parameter(\'bias\', None) # Initialize parameters torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) torch.nn.init.uniform_(self.bias, -bound, bound) def forward(self, input): return MyLinearFunction.apply(input, self.weight, self.bias) ``` 3. **Testing your Implementation**: ```python def test_custom_linear(): # Input features and output features input_features, output_features = 5, 3 # Sample input tensor input = torch.randn(2, input_features, requires_grad=True) # Custom linear module linear = MyLinear(input_features, output_features) # Forward pass output = linear(input) print(\\"Output:n\\", output) # Backward pass output.sum().backward() print(\\"Gradients:n\\", input.grad) # Gradient check input = (torch.randn(2, input_features, dtype=torch.double, requires_grad=True), torch.randn(output_features, input_features, dtype=torch.double, requires_grad=True), torch.randn(output_features, dtype=torch.double, requires_grad=True)) assert torch.autograd.gradcheck(MyLinearFunction.apply, input), \\"Gradcheck failed!\\" test_custom_linear() ``` Constraints: - Ensure that your code adheres to the guidelines of efficient memory use and computation. - Your `MyLinearFunction` should handle cases with and without bias. - Use `torch.nn.init` for parameter initialization. Given these requirements and specifications, implement and test your custom classes as outlined above.","solution":"import torch import math # Custom Linear Function class MyLinearFunction(torch.autograd.Function): @staticmethod def forward(ctx, input, weight, bias): # Save tensors for backward pass output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) ctx.save_for_backward(input, weight, bias) return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_weight = grad_bias = None if ctx.needs_input_grad[0]: grad_input = grad_output.mm(weight) if ctx.needs_input_grad[1]: grad_weight = grad_output.t().mm(input) if bias is not None and ctx.needs_input_grad[2]: grad_bias = grad_output.sum(0) return grad_input, grad_weight, grad_bias # Custom Linear Module class MyLinear(torch.nn.Module): def __init__(self, input_features, output_features, bias=True): super(MyLinear, self).__init__() self.input_features = input_features self.output_features = output_features self.weight = torch.nn.Parameter(torch.Tensor(output_features, input_features)) if bias: self.bias = torch.nn.Parameter(torch.Tensor(output_features)) else: self.register_parameter(\'bias\', None) # Initialize parameters torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) torch.nn.init.uniform_(self.bias, -bound, bound) def forward(self, input): return MyLinearFunction.apply(input, self.weight, self.bias)"},{"question":"# Advanced Python Coding Assessment Objective Demonstrate comprehension and proficiency in handling dictionary operations, including merging and iteration using Python\'s `dict` type, as detailed in the Python/C API documentation. Question Write a Python function `merge_dictionaries(dict1, dict2, override=False)` that merges two dictionaries, `dict1` and `dict2`, according to the following rules: - If `override` is `True`, the values from `dict2` will overwrite the existing values in `dict1` for matching keys. - If `override` is `False`, the values from `dict1` will be preserved, and the values from `dict2` will only be added if the key is not present in `dict1`. - Do not use Python’s built-in dictionary `.update()` method or dictionary unpacking. - You must iterate through `dict2` and manually merge the key-value pairs into `dict1`. Function Signature ```python def merge_dictionaries(dict1: dict, dict2: dict, override: bool = False) -> dict: ``` Input - `dict1`: A dictionary containing key-value pairs. (example: `{\'a\': 1, \'b\': 2}`) - `dict2`: A dictionary containing key-value pairs. (example: `{\'b\': 3, \'c\': 4}`) - `override`: A boolean indicating whether to overwrite existing key-value pairs in `dict1` with those from `dict2`. Output - Returns `dict1` after merging the key-value pairs from `dict2` according to the specified rules. Constraints - The keys in both dictionaries will be hashable. - The dictionaries can contain nested dictionaries as values. Example ```python dict1 = {\'a\': 1, \'b\': 2} dict2 = {\'b\': 3, \'c\': 4} # Example 1 result = merge_dictionaries(dict1, dict2, override=True) print(result) # Expected output: {\'a\': 1, \'b\': 3, \'c\': 4} # Example 2 result = merge_dictionaries(dict1, dict2, override=False) print(result) # Expected output: {\'a\': 1, \'b\': 2, \'c\': 4} ``` Good luck!","solution":"def merge_dictionaries(dict1: dict, dict2: dict, override: bool = False) -> dict: Merges two dictionaries dict1 and dict2 based on the override flag. Args: dict1 (dict): The first dictionary. dict2 (dict): The second dictionary. override (bool): If True, values from dict2 overwrite those in dict1 for matching keys. Returns: dict: The merged dictionary. for key, value in dict2.items(): if override or key not in dict1: dict1[key] = value return dict1"},{"question":"**Assessment Question:** You are tasked with visualizing a dataset containing information about different flowers. The dataset includes the following columns: - `sepal_length` - `sepal_width` - `petal_length` - `petal_width` - `species` Using seaborn, create a visualization that demonstrates the following: 1. A scatter plot of petal length vs petal width for different species. 2. Add a rug plot along the x-axis (petal length) and y-axis (petal width) to the scatter plot. 3. Use the `hue` parameter to differentiate between species. 4. Customize the rug plot: - Set the height of the rug lines to 0.1. - Place the rug outside the axes (negative height value) and ensure they are not clipped. - Adjust the rug lines to be thinner and semi-transparent. **Input Format:** - The dataset can be loaded using seaborn\'s `load_dataset` function, via `seaborn.load_dataset(\\"iris\\")`. **Expected Output:** - A scatter plot with rug plots that meet the customized configuration. **Constraints:** 1. Ensure that the plot is properly labeled with appropriate axis titles and a legend. 2. The solution must use seaborn\'s functionalities. Sample Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset iris = sns.load_dataset(\\"iris\\") # Set the seaborn theme sns.set_theme() # Create the scatter plot with rug plots # (complete this part with the required customizations) plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_flower_data(): # Load the dataset iris = sns.load_dataset(\\"iris\\") # Set the seaborn theme sns.set_theme() # Create the scatter plot with rug plots scatter = sns.scatterplot(x=\'petal_length\', y=\'petal_width\', hue=\'species\', data=iris) # Add rug plots with customized settings sns.rugplot(x=\'petal_length\', y=\'petal_width\', hue=\'species\', data=iris, height=0.1, clip_on=False, linewidth=0.5, alpha=0.5) # Label the axes scatter.set_xlabel(\'Petal Length\') scatter.set_ylabel(\'Petal Width\') # Display the legend scatter.legend() # Show the plot plt.show()"},{"question":"# Seaborn Plotting Context Customizer You are required to write a function `customize_plotting_context` that demonstrates your understanding of the `seaborn` library\'s `plotting_context` feature. The function should: 1. Accept a dataset and a context style. 2. Plot a scatter plot of the dataset using the given context style. 3. After displaying this plot, temporarily change to another context style within a `with` block and display a line plot of the same dataset. Input: - A dictionary `data` with two keys \\"x\\" and \\"y\\", each of which maps to a list of numerical values representing coordinates. - A string `context_style` representing the context to be temporarily applied to the plotting environment outside the `with` block. - A string `temporary_context_style` representing the context to be temporarily applied inside the `with` block. Output: - The function should not return anything but should display two plots (a scatter plot and a line plot) according to the specified contexts. Example: ```python data = { \\"x\\": [1, 2, 3, 4, 5], \\"y\\": [5, 4, 3, 2, 1] } customize_plotting_context(data, \\"talk\\", \\"paper\\") ``` This function should: 1. Display a scatter plot of the data using the \\"talk\\" context style. 2. Within a `with` block, change the context to \\"paper\\" and display a line plot of the same data. Constraints: - Do not use any external libraries other than `seaborn` and `matplotlib`. - Ensure that both plots are displayed in the same figure by using the appropriate context and plt.show() commands. Note: Make sure to use different markers or colors for the scatter plot and the line plot to distinguish them visually.","solution":"import seaborn as sns import matplotlib.pyplot as plt def customize_plotting_context(data, context_style, temporary_context_style): Plots a scatter plot and a line plot of the data using the seaborn library with different context styles. Parameters: data (dict): A dictionary containing \\"x\\" and \\"y\\" coordinates. context_style (str): The context style to be used for scatter plot. temporary_context_style (str): The context style to be used within the block for line plot. # Apply the outer context style sns.set_context(context_style) # First plot: Scatter plot plt.figure() sns.scatterplot(x=data[\'x\'], y=data[\'y\'], color=\'b\', marker=\'o\') plt.title(\'Scatter Plot\') plt.show() # Second plot within a temporary context with sns.plotting_context(temporary_context_style): plt.figure() sns.lineplot(x=data[\'x\'], y=data[\'y\'], color=\'r\', marker=\'o\') plt.title(\'Line Plot\') plt.show()"},{"question":"# Question: XML to HTML Table Conversion You are provided with an XML document string representing a table of records. Each record contains fields such as `name`, `age`, `email`, and `address`. You are required to write a Python function using the `xml.dom.minidom` module to convert this XML document into an HTML table. # Function Signature ```python def xml_to_html_table(xml_string: str) -> str: pass ``` # Input - `xml_string`: A string containing the XML document representing the table records. # Output - A string containing the HTML representation of the table. # XML Input Format ```xml <records> <record> <name>John Doe</name> <age>29</age> <email>john.doe@example.com</email> <address>123 Elm Street</address> </record> <record> <name>Jane Smith</name> <age>34</age> <email>jane.smith@example.com</email> <address>456 Oak Avenue</address> </record> <!-- More records can be added here --> </records> ``` # HTML Output Format ```html <table> <tr> <th>Name</th> <th>Age</th> <th>Email</th> <th>Address</th> </tr> <tr> <td>John Doe</td> <td>29</td> <td>john.doe@example.com</td> <td>123 Elm Street</td> </tr> <tr> <td>Jane Smith</td> <td>34</td> <td>jane.smith@example.com</td> <td>456 Oak Avenue</td> </tr> <!-- More rows as per the input records --> </table> ``` # Constraints 1. You must use the `xml.dom.minidom` module for parsing and manipulating the XML data. 2. The order of records in the output HTML table should be the same as they appear in the XML input. 3. Ensure that the HTML output is valid and properly formatted. # Example ```python xml_input = <records> <record> <name>John Doe</name> <age>29</age> <email>john.doe@example.com</email> <address>123 Elm Street</address> </record> <record> <name>Jane Smith</name> <age>34</age> <email>jane.smith@example.com</email> <address>456 Oak Avenue</address> </record> </records> print(xml_to_html_table(xml_input)) ``` Output ```html <table> <tr> <th>Name</th> <th>Age</th> <th>Email</th> <th>Address</th> </tr> <tr> <td>John Doe</td> <td>29</td> <td>john.doe@example.com</td> <td>123 Elm Street</td> </tr> <tr> <td>Jane Smith</td> <td>34</td> <td>jane.smith@example.com</td> <td>456 Oak Avenue</td> </tr> </table> ``` # Notes - You should handle cases where the input XML might not have any records and return an empty table structure. - Utilize the `writexml()` or `toprettyxml()` method for producing the HTML output to ensure proper formatting.","solution":"from xml.dom.minidom import parseString def xml_to_html_table(xml_string: str) -> str: Convert an XML document representing table records into an HTML table. dom = parseString(xml_string) records = dom.getElementsByTagName(\\"record\\") html = \'<table>n\' html += \' <tr>n\' html += \' <th>Name</th>n\' html += \' <th>Age</th>n\' html += \' <th>Email</th>n\' html += \' <th>Address</th>n\' html += \' </tr>n\' for record in records: name = record.getElementsByTagName(\\"name\\")[0].childNodes[0].data age = record.getElementsByTagName(\\"age\\")[0].childNodes[0].data email = record.getElementsByTagName(\\"email\\")[0].childNodes[0].data address = record.getElementsByTagName(\\"address\\")[0].childNodes[0].data html += \' <tr>n\' html += f\' <td>{name}</td>n\' html += f\' <td>{age}</td>n\' html += f\' <td>{email}</td>n\' html += f\' <td>{address}</td>n\' html += \' </tr>n\' html += \'</table>\' return html"},{"question":"Implement a Custom Python Object with Specific Methods Objective: Create a custom Python object `CustomObject` using Python\'s C API (as if you are extending Python with C), implementing the following methods: 1. `tp_repr` for a string representation of the object. 2. `tp_getattro` and `tp_setattro` for managing attributes. 3. `tp_richcompare` for comparison operations. 4. `tp_iter` and `tp_iternext` for iteration. Task: 1. **Define the `CustomObject` Structure**: - Include a Python object header. - Add at least one additional field for demonstration purposes (e.g., an integer or a string). 2. **Implement the `tp_repr` Method**: - This method should return a string that includes the type name of the object and any relevant data (e.g., the value of the integer field). 3. **Implement the `tp_getattro` and `tp_setattro` Methods**: - `tp_getattro` should return the attribute value if it exists, or raise an `AttributeError` if it doesn\'t. - `tp_setattro` should set the attribute value or delete it if the value passed is `NULL`. 4. **Implement the `tp_richcompare` Method**: - This method should compare the `CustomObject` instances based on their fields. - Implement at least equality (`==`) and inequality (`!=`) comparisons. 5. **Implement Iteration Support**: - Define `tp_iter` to return an iterator object (`self`). - Define `tp_iternext` to iterate over an internal list or range. Expected Input and Output: - **Input**: Creation and manipulation of `CustomObject` instances. - **Output**: Proper representation, attribute management, comparison results, and iteration behavior. Constraints: 1. Follow Python\'s C API conventions. 2. Handle errors appropriately, especially in deallocating and finalizing objects. 3. Ensure efficient attribute retrieval and modification. Example: ```python # This is a conceptual example in Python, mimic its behavior using Python\'s C API obj = CustomObject(value=10) print(repr(obj)) # Should print something like: \\"CustomObject(value: 10)\\" print(obj.value) # Should print: 10 obj.value = 20 print(obj.value) # Should print: 20 del obj.value obj1 = CustomObject(value=10) obj2 = CustomObject(value=20) print(obj1 == obj2) # Should print: False iterator = iter(obj1) for item in iterator: print(item) ``` # Note: For this task, it\'s not required to write the C code directly but to design the structure and explain how you would implement each method in the C language.","solution":"# As we are supposed to provide a conceptual Python equivalent and not actual C API code, here is a Python implementation. # Please note this is a conceptual demonstration to mimic behavior as requested. class CustomObject: def __init__(self, value=0): self.value = value self.attributes = {} def __repr__(self): return f\\"<CustomObject(value: {self.value})>\\" def __getattr__(self, name): if name in self.attributes: return self.attributes[name] raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{name}\'\\") def __setattr__(self, name, value): if name in [\'value\', \'attributes\']: super().__setattr__(name, value) else: self.attributes[name] = value def __delattr__(self, name): if name in self.attributes: del self.attributes[name] else: raise AttributeError(f\\"\'{type(self).__name__}\' object has no attribute \'{name}\'\\") def __eq__(self, other): if isinstance(other, CustomObject): return self.value == other.value return NotImplemented def __ne__(self, other): return not self == other def __iter__(self): self._iter_idx = 0 self._iter_list = list(range(self.value)) # example for demonstration return self def __next__(self): if self._iter_idx < len(self._iter_list): result = self._iter_list[self._iter_idx] self._iter_idx += 1 return result else: raise StopIteration # Example Usage if __name__ == \\"__main__\\": obj = CustomObject(value=10) print(repr(obj)) # Should print: <CustomObject(value: 10)> obj.some_attribute = \\"test\\" print(obj.some_attribute) # Should print: test obj.value = 20 print(repr(obj)) # Should print: <CustomObject(value: 20)> del obj.some_attribute try: print(obj.some_attribute) # Should raise AttributeError except AttributeError as e: print(e) # Should print error message obj1 = CustomObject(value=10) obj2 = CustomObject(value=20) print(obj1 == obj2) # Should print: False iterator = iter(obj1) for item in iterator: print(item) # Should print numbers from 0 to 9"},{"question":"**Coding Challenge: Advanced PList Manipulation** **Problem Statement:** You have been assigned the task of creating a tool that takes an XML formatted plist file (as a bytes object) containing a list of users, updates specific entries, and writes the updated data back to a new binary formatted plist file. Each user entry is a dictionary containing the keys \\"username\\", \\"email\\", and \\"login_count\\". **Task:** 1. Write a function `update_user_logins(plist_data: bytes, updates: dict) -> bytes` that performs the following operations: - Parses the given XML formatted plist data. - Updates the `login_count` for users whose usernames are provided in the `updates` dictionary. - Adds any new users provided in the `updates` dictionary to the plist data. - Returns the updated data as a binary formatted plist. **Function Signature:** ```python def update_user_logins(plist_data: bytes, updates: dict) -> bytes: pass ``` **Parameters:** - `plist_data` (bytes): The XML formatted plist data as a bytes object. - `updates` (dict): A dictionary where the keys are usernames and the values are dictionaries with \\"email\\" and \\"login_count\\" keys. **Returns:** - The updated plist data as a binary formatted bytes object. **Constraints:** - Each user dictionary must contain the keys \\"username\\", \\"email\\", and \\"login_count\\". - New users provided in the `updates` dict should be added to the plist data. - The function should handle any potential exceptions that arise from parsing or writing the plist files. **Example:** ```python import plistlib # Example XML formatted plist data plist_data = b<plist version=\\"1.0\\"> <array> <dict> <key>username</key> <string>alice</string> <key>email</key> <string>alice@example.com</string> <key>login_count</key> <integer>5</integer> </dict> <dict> <key>username</key> <string>bob</string> <key>email</key> <string>bob@example.com</string> <key>login_count</key> <integer>3</integer> </dict> </array> </plist> # Example updates dictionary updates = { \\"alice\\": {\\"email\\": \\"alice_new@example.com\\", \\"login_count\\": 10}, \\"charlie\\": {\\"email\\": \\"charlie@example.com\\", \\"login_count\\": 1} } # Example function call updated_plist_data = update_user_logins(plist_data, updates) # Expected output: the updated plist data in binary format print(plistlib.loads(updated_plist_data)) ``` **Note:** This example shows the expected structure, but the actual output is a binary formatted plist file.","solution":"import plistlib import io def update_user_logins(plist_data: bytes, updates: dict) -> bytes: Updates the login count for users in the plist data and adds new users from the updates dictionary. :param plist_data: The XML formatted plist data. :param updates: A dictionary of updates containing usernames and their new login count and email. :return: The updated binary formatted plist data. # Parse the XML plist plist = plistlib.loads(plist_data) # Convert plist to a dictionary with usernames as keys for easy lookup user_dict = {user[\\"username\\"]: user for user in plist} # Apply updates and add new users for username, update in updates.items(): if username in user_dict: # Update existing user user_dict[username][\\"email\\"] = update[\\"email\\"] user_dict[username][\\"login_count\\"] = update[\\"login_count\\"] else: # Add new user user_dict[username] = { \\"username\\": username, \\"email\\": update[\\"email\\"], \\"login_count\\": update[\\"login_count\\"] } # Convert the updated dictionary back to a list updated_plist = list(user_dict.values()) # Write the updated plist to binary format output = io.BytesIO() plistlib.dump(updated_plist, output, fmt=plistlib.FMT_BINARY) return output.getvalue()"},{"question":"# Advanced Python Tracing with SystemTap Objective Create a Python script that can be monitored using SystemTap. You will utilize the pre-defined static markers available in CPython to trace function calls and garbage collection cycles in your script. Task 1. Write a Python script based on the following requirements: - Define at least three functions: `function_1`, `function_2`, and `function_3`, where: - `function_1` calls `function_2` and `function_3`. - `function_2` performs a basic arithmetic operation and includes a block print statement. - `function_3` raises and catches an exception. - Include at least one garbage collection action using the `gc` module. - Use DTrace/SystemTap compatible markers to monitor function entries, function returns, and garbage collection cycles. 2. Write a SystemTap script to trace: - All function calls and returns with the function name, file name, and line number. - Garbage collection start and end events. Requirements - Your Python script should be named `trace_example.py`. - Your SystemTap script should be named `trace_example.stp`. Input No input is required from the user. Output The output should demonstrate the trace of the function calls and garbage collection events, similar to: ``` Function Entry traced: function_1 in trace_example.py:5 Function Entry traced: function_2 in trace_example.py:10 Function Return traced: function_2 in trace_example.py:15 Function Entry traced: function_3 in trace_example.py:20 Exception traced: SomeError in trace_example.py:25 Function Return traced: function_3 in trace_example.py:28 Function Return traced: function_1 in trace_example.py:8 GC cycle started GC cycle finished ``` Constraints - Ensure compatibility with Python 3.10 CPython configuration. - Use appropriate SystemTap static markers in your SystemTap script. - Your Python script should use the `gc` module correctly. Performance The solution should efficiently trace the required events without significant overhead. # Submission 1. `trace_example.py` - Python script meeting the above requirements. 2. `trace_example.stp` - SystemTap script for monitoring the Python script.","solution":"import gc def function_1(): function_2() function_3() def function_2(): result = 1 + 1 print(f\\"function_2 result: {result}\\") def function_3(): try: raise ValueError(\\"An error occurred in function_3\\") except ValueError as e: print(e) if __name__ == \\"__main__\\": function_1() gc.collect()"},{"question":"You are tasked with creating a utility script to gather and display various details about the system it\'s run on. Your script should use the functions provided by the `platform` module to collect this information. Specifically, your function should collect and return the following details in a dictionary format: 1. **Architecture**: This should include both bit architecture and linkage information. 2. **Machine Type**: The type of machine, e.g., \'AMD64\'. 3. **Network Name**: The network name of the computer. 4. **Platform**: The human-readable identifier of the underlying platform. 5. **Processor**: The real processor name. 6. **Python Build**: The build number and build date of Python. 7. **Python Compiler**: The compiler used to compile Python. 8. **System Release**: The system\'s release. 9. **System Name**: The system/OS name. 10. **System Version**: The system\'s release version. 11. **uname**: A named tuple with system, node, release, version, machine, and processor details. # Function Signature ```python def gather_system_details() -> dict: pass ``` # Expected Output The function should return a dictionary with the keys mapped to the corresponding details. Example Output: ```python { \\"architecture\\": (\\"64bit\\", \\"ELF\\"), \\"machine_type\\": \\"x86_64\\", \\"network_name\\": \\"my-computer\\", \\"platform\\": \\"Linux-5.4.0-42-generic-x86_64-with-Ubuntu-18.04-bionic\\", \\"processor\\": \\"x86_64\\", \\"python_build\\": (\\"default\\", \\"Jul 20 2020 20:14:34\\"), \\"python_compiler\\": \\"GCC 7.5.0\\", \\"system_release\\": \\"5.4.0-42-generic\\", \\"system_name\\": \\"Linux\\", \\"system_version\\": \\"#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\\", \\"uname\\": { \\"system\\": \\"Linux\\", \\"node\\": \\"my-computer\\", \\"release\\": \\"5.4.0-42-generic\\", \\"version\\": \\"#46-Ubuntu SMP Fri Jul 10 00:24:02 UTC 2020\\", \\"machine\\": \\"x86_64\\", \\"processor\\": \\"x86_64\\" } } ``` # Constraints - Ensure your function handles cases where some information might not be available by returning an empty string (`\'\'`) for such cases. - You are expected to handle exceptions that might arise from function calls and ensure that the program continues to gather other available information. # Hints - Use try-except blocks to handle potential issues with retrieving information. - Refer to the `platform` module documentation for the exact usage and behavior of each function.","solution":"import platform def gather_system_details() -> dict: try: architecture = platform.architecture() except Exception: architecture = (\'\', \'\') try: machine_type = platform.machine() except Exception: machine_type = \'\' try: network_name = platform.node() except Exception: network_name = \'\' try: sys_platform = platform.platform() except Exception: sys_platform = \'\' try: processor = platform.processor() except Exception: processor = \'\' try: python_build = platform.python_build() except Exception: python_build = (\'\', \'\') try: python_compiler = platform.python_compiler() except Exception: python_compiler = \'\' try: system_release = platform.release() except Exception: system_release = \'\' try: system_name = platform.system() except Exception: system_name = \'\' try: system_version = platform.version() except Exception: system_version = \'\' try: uname = platform.uname() uname_dict = uname._asdict() except Exception: uname_dict = {\\"system\\": \\"\\", \\"node\\": \\"\\", \\"release\\": \\"\\", \\"version\\": \\"\\", \\"machine\\": \\"\\", \\"processor\\": \\"\\"} return { \\"architecture\\": architecture, \\"machine_type\\": machine_type, \\"network_name\\": network_name, \\"platform\\": sys_platform, \\"processor\\": processor, \\"python_build\\": python_build, \\"python_compiler\\": python_compiler, \\"system_release\\": system_release, \\"system_name\\": system_name, \\"system_version\\": system_version, \\"uname\\": uname_dict }"},{"question":"**Complex Linear Regression and Cross-Validation using Scikit-learn** # Objective: Your objective is to implement a Ridge Regression model that uses cross-validation to optimize the alpha parameter. You will use the `diabetes` dataset provided by `scikit-learn`. # Problem Statement: 1. Load the diabetes dataset using `sklearn.datasets.load_diabetes`. 2. Split the dataset into `X` (features) and `y` (target variable). 3. Implement a Ridge Regression model with built-in cross-validation to find the optimal value of the regularization parameter `alpha`. 4. Report the optimal `alpha` value, the coefficients of the model, and the mean squared error on the test set. # Instructions: 1. Load necessary libraries. 2. Load the diabetes dataset. 3. Split the dataset into training and test sets using an 80-20 split. 4. Implement the `RidgeCV` model to identify the optimal `alpha` from the following list: `[0.1, 1.0, 10.0, 100.0]`. 5. Fit the model on the training data. 6. Use the fitted model to predict the target values on the test set. 7. Calculate the Mean Squared Error (MSE) of your predictions on the test set. 8. Output the optimal `alpha` value, the coefficients of the model, and the MSE. # Requirements: 1. Use the `RidgeCV` class from `sklearn.linear_model`. 2. Use the `mean_squared_error` function from `sklearn.metrics`. 3. Ensure reproducibility by setting a random seed. # Constraints: - You may use any helper functions to aid in your implementation. - The solution should be efficient and clear. # Example Output: Here is an example of the expected output format: ``` Optimal alpha: 1.0 Coefficients: [ 29.18, -246.61, 539.84, 324.39, 123.02, -573.14, 151.46, 82.14, 375.74, 84.34] Mean Squared Error: 2000.43 ``` # Implementation: ```python import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error # Load the diabetes dataset diabetes = load_diabetes() X = diabetes.data y = diabetes.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # List of alphas to tune alphas = [0.1, 1.0, 10.0, 100.0] # Build and fit the RidgeCV model with the given alphas model = RidgeCV(alphas=alphas) model.fit(X_train, y_train) # Retrieve the optimal alpha value optimal_alpha = model.alpha_ # Retrieve the coefficients coefficients = model.coef_ # Make predictions on the test set y_pred = model.predict(X_test) # Calculate the mean squared error mse = mean_squared_error(y_test, y_pred) # Output the results print(f\\"Optimal alpha: {optimal_alpha}\\") print(f\\"Coefficients: {coefficients}\\") print(f\\"Mean Squared Error: {mse}\\") ```","solution":"import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import RidgeCV from sklearn.metrics import mean_squared_error def ridge_regression_with_cv(): Perform Ridge Regression with cross-validation on the diabetes dataset to find the optimal alpha and calculate the mean squared error on the test set. Returns the optimal alpha, coefficients of the model, and mean squared error. # Load the diabetes dataset diabetes = load_diabetes() X, y = diabetes.data, diabetes.target # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # List of alphas to tune alphas = [0.1, 1.0, 10.0, 100.0] # Build and fit the RidgeCV model with the given alphas model = RidgeCV(alphas=alphas) model.fit(X_train, y_train) # Retrieve the optimal alpha value optimal_alpha = model.alpha_ # Retrieve the coefficients coefficients = model.coef_ # Make predictions on the test set y_pred = model.predict(X_test) # Calculate the mean squared error mse = mean_squared_error(y_test, y_pred) return optimal_alpha, coefficients, mse"},{"question":"Coding Assessment Question # Question You are tasked with implementing a graph transformation using the `torch.fx` module in PyTorch. Your transformation should achieve the following: 1. **Symbolically trace** a provided `torch.nn.Module`. 2. **Replace** all ReLU (Rectified Linear Unit) activations in the graph with Leaky ReLU (Leaky version of a Rectified Linear Unit) activations. Assume a negative slope of 0.01 for the Leaky ReLU. 3. **Return** a new `torch.nn.Module` that uses the transformed graph. Your implementation should meet the following requirements: - The input to your transformation will be an instance of a custom `torch.nn.Module`. - The output should be an instance of `torch.nn.Module` with the modified graph. # Input and Output Formats - **Input**: An instance of a custom `torch.nn.Module` which makes use of ReLU activations. - **Output**: An instance of `torch.nn.Module` with all ReLU activations replaced by Leaky ReLU activations. # Constraints - Ensure that your transformation handles any nested submodules within the input module. - The replacement must preserve the structure and the functionality of the original model except for the activation change. # Example ```python import torch import torch.nn as nn import torch.fx class ExampleModule(nn.Module): def __init__(self): super(ExampleModule, self).__init__() self.linear1 = nn.Linear(10, 10) self.linear2 = nn.Linear(10, 10) def forward(self, x): x = self.linear1(x) x = torch.relu(x) x = self.linear2(x) x = torch.relu(x) return x def replace_relu_with_leaky_relu(module: nn.Module) -> nn.Module: class ReplaceReLUTracer(torch.fx.Tracer): def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: # Consider all submodules as leaves except nn.ReLU if isinstance(m, torch.nn.ReLU): return False return super().is_leaf_module(m, module_qualified_name) # Step 1: Symbolically trace the module to get a graph graph = ReplaceReLUTracer().trace(module) # Step 2: Iterate over the nodes and replace torch.relu with F.leaky_relu for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.relu: node.target = torch.nn.functional.leaky_relu node.kwargs[\'negative_slope\'] = 0.01 # Step 3: Return a new GraphModule with the modified graph return torch.fx.GraphModule(module, graph) # Creating an instance of the ExampleModule example_module = ExampleModule() # Transforming the module transformed_module = replace_relu_with_leaky_relu(example_module) # Testing the transformed module input_tensor = torch.randn(1, 10) output = transformed_module(input_tensor) print(output) ``` In this example, the `replace_relu_with_leaky_relu` function takes an instance of `ExampleModule`, traces its graph, replaces all ReLU activations with Leaky ReLU activations, and returns the transformed module. # Note You should not modify the existing structure of the module except for changing the activation functions. Ensure that the transformation correctly handles modules with multiple ReLU activations and preserves the overall behavior of the network.","solution":"import torch import torch.nn as nn import torch.fx def replace_relu_with_leaky_relu(module: nn.Module) -> nn.Module: class ReplaceReLUTracer(torch.fx.Tracer): def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool: # Consider all submodules as leaves except nn.ReLU if isinstance(m, torch.nn.ReLU): return False return super().is_leaf_module(m, module_qualified_name) # Step 1: Symbolically trace the module to get a graph graph = ReplaceReLUTracer().trace(module) # Step 2: Create a new model from the traced graph with modified activation functions graph_module = torch.fx.GraphModule(module, graph) for node in graph_module.graph.nodes: if node.op == \'call_module\' and isinstance(getattr(graph_module, node.target), nn.ReLU): with graph_module.graph.inserting_after(node): new_node = graph_module.graph.call_module(\'leaky_relu\', args=node.args, kwargs={\'negative_slope\': 0.01}) node.replace_all_uses_with(new_node) graph_module.graph.erase_node(node) # Rebuild the forward method with the new graph graph_module.recompile() # Update the remaining relu nodes to leaky relu if \'leaky_relu\' not in graph_module._modules: graph_module.add_module(\'leaky_relu\', nn.LeakyReLU(negative_slope=0.01)) return graph_module # Example custom module with ReLU activations class ExampleModule(nn.Module): def __init__(self): super(ExampleModule, self).__init__() self.linear1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.linear2 = nn.Linear(10, 10) def forward(self, x): x = self.linear1(x) x = self.relu(x) x = self.linear2(x) x = self.relu(x) return x"},{"question":"Flask Test Suite Development Objective Your task is to set up a test suite for a Flask application and write unit tests to assess its functionality. Task A Flask application has been partially implemented with user authentication and blog functionality. Your task is to: 1. Set up a testing environment for the Flask application. 2. Write tests for the authentication and blog functionality. Description 1. **Setting up the test environment**: - Create the `conftest.py` file to set up fixtures for the test client, the CLI runner, and a temporary database. - Populate the database with initial test data from a given SQL script. 2. **Writing tests for authentication**: - Write tests for the `register` and `login` views to ensure they work correctly with valid and invalid data. 3. **Writing tests for blog functionality**: - Ensure that blog posts can be created, updated, and deleted only by logged-in users. - Test that posts are correctly displayed on the index page and can be edited by their authors. Expected Input and Output Formats - **Input**: No input files; the initial setup includes an SQL script for populating the database. - **Output**: Test files implementing the required fixtures and tests. The tests should be runnable via the `pytest` command. Constraints and Requirements - Use `pytest` for writing tests. - Use the `coverage` package to measure the test coverage. - Ensure all tests pass and aim for 100% code coverage. Performance Requirements - Tests should run efficiently within a reasonable time frame (~1 second per test). Example Structure ``` tests/ |-- conftest.py |-- test_auth.py |-- test_blog.py |-- data.sql ``` Example Code and SQL Data - **conftest.py**: ```python import os import tempfile import pytest from flaskr import create_app from flaskr.db import get_db, init_db with open(os.path.join(os.path.dirname(__file__), \'data.sql\'), \'rb\') as f: _data_sql = f.read().decode(\'utf8\') @pytest.fixture def app(): db_fd, db_path = tempfile.mkstemp() app = create_app({ \'TESTING\': True, \'DATABASE\': db_path, }) with app.app_context(): init_db() get_db().executescript(_data_sql) yield app os.close(db_fd) os.unlink(db_path) @pytest.fixture def client(app): return app.test_client() @pytest.fixture def runner(app): return app.test_cli_runner() ``` - **data.sql**: ```sql INSERT INTO user (username, password) VALUES (\'test\', \'testpasswordhash\'), (\'other\', \'otherpasswordhash\'); INSERT INTO post (title, body, author_id, created) VALUES (\'test title\', \'test body\', 1, \'2022-01-01 00:00:00\'); ``` - **test_auth.py**: ```python import pytest from flask import g, session from flaskr.db import get_db def test_register(client, app): assert client.get(\'/auth/register\').status_code == 200 response = client.post( \'/auth/register\', data={\'username\': \'a\', \'password\': \'a\'} ) assert response.headers[\\"Location\\"] == \\"/auth/login\\" with app.app_context(): assert get_db().execute( \\"SELECT * FROM user WHERE username = \'a\'\\", ).fetchone() is not None @pytest.mark.parametrize((\'username\', \'password\', \'message\'), ( (\'\', \'\', b\'Username is required.\'), (\'a\', \'\', b\'Password is required.\'), (\'test\', \'test\', b\'already registered\'), )) def test_register_validate_input(client, username, password, message): response = client.post( \'/auth/register\', data={\'username\': username, \'password\': password} ) assert message in response.data ``` - **test_blog.py**: ```python import pytest from flaskr.db import get_db def test_index(client, auth): response = client.get(\'/\') assert b\\"Log In\\" in response.data assert b\\"Register\\" in response.data auth.login() response = client.get(\'/\') assert b\'Log Out\' in response.data assert b\'test title\' in response.data assert b\'by test on 2022-01-01\' in response.data assert b\'test body\' in response.data assert b\'href=\\"/1/update\\"\' in response.data @pytest.mark.parametrize(\'path\', ( \'/create\', \'/1/update\', \'/1/delete\', )) def test_login_required(client, path): response = client.post(path) assert response.headers[\\"Location\\"] == \\"/auth/login\\" def test_create(client, auth, app): auth.login() assert client.get(\'/create\').status_code == 200 client.post(\'/create\', data={\'title\': \'created\', \'body\': \'\'}) with app.app_context(): db = get_db() count = db.execute(\'SELECT COUNT(id) FROM post\').fetchone()[0] assert count == 2 def test_update(client, auth, app): auth.login() assert client.get(\'/1/update\').status_code == 200 client.post(\'/1/update\', data={\'title\': \'updated\', \'body\': \'\'}) with app.app_context(): db = get_db() post = db.execute(\'SELECT * FROM post WHERE id = 1\').fetchone() assert post[\'title\'] == \'updated\' ``` # Submission Submit your `conftest.py`, `test_auth.py`, `test_blog.py`, and `data.sql` files in a zip file named `<your_name>_flask_tests.zip`.","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"Objective: This question assesses your understanding of using the \\"meta\\" device in PyTorch to handle models and tensors without using actual data, and then properly initializing the data when needed. Problem Statement: You are tasked with writing a function to load a neural network model on a meta device, analyze its structure, and then move it to the CPU with initialized parameters. Specifically, you will follow these steps: 1. Load a model from a file onto a meta device. 2. Print out the parameter shapes of the model while on the meta device. 3. Convert the model back to the CPU, initializing the parameters as random normal values with a mean of 0 and a standard deviation of 1. Function Signature: ```python def load_and_initialize_model(file_path: str) -> torch.nn.Module: pass ``` Input: - `file_path` (str): The path to the file containing the saved model. Output: - Returns a `torch.nn.Module` which is the model moved to the CPU with parameters initialized. Constraints: - Assume the model structure can fit in memory once it is fully loaded with actual data. Example Usage: ```python # Assume the model saved in \'model.pth\' is a simple linear model # with an input feature size of 5 and output feature size of 2 model = load_and_initialize_model(\'model.pth\') print(model) # Expected output: # Linear(in_features=5, out_features=2, bias=True) # with parameters initialized to random normal values with mean=0 and std=1 ``` Notes: - Use `torch.load` with `map_location=\'meta\'` to load the model on the meta device. - Use the `torch.device(\'meta\')` context manager if needed for constructing tensors on the meta device. - Use the `to_empty` method to move the model to the CPU without initializing parameters. - Use the `torch.nn.init.normal_` function to initialize each parameter with a normal distribution (mean=0, std=1). - Do not use data-dependent operations like `torch.nonzero` directly on meta tensors. By completing this task, you\'ll demonstrate your understanding of efficiently working with PyTorch’s meta tensors for managing and initializing models.","solution":"import torch import torch.nn as nn def load_and_initialize_model(file_path: str) -> nn.Module: Loads a model from a file onto a meta device, prints the parameter shapes, and then moves it to the CPU with initialized parameters. Args: - file_path (str): The path to the file containing the saved model. Returns: - Initialized torch.nn.Module on CPU # Load the model on the meta device model = torch.load(file_path, map_location=\'meta\') # Print parameter shapes for name, param in model.named_parameters(): print(f\'{name} with shape {param.shape}\') # Move the model to CPU without parameters (to_empty) model = model.to_empty(device=torch.device(\'cpu\')) # Initialize parameters with a normal distribution (mean=0, std=1) for param in model.parameters(): nn.init.normal_(param, mean=0, std=1) return model"},{"question":"# Subprocess Management in Distributed PyTorch You are tasked with implementing a function that utilizes PyTorch\'s `SubprocessHandler` to create, monitor, and gracefully terminate subprocesses. This function should be able to handle subprocess failures and attempt to restart failed subprocesses up to a specified number of retries. Objective: Implement a function `manage_subprocesses` that launches a specified number of subprocesses to run a given command, monitors their execution, and attempts to restart any subprocess that fails. The function should stop all subprocesses gracefully when a stop signal is received. Function Signature: ```python def manage_subprocesses(command: str, num_subprocesses: int, max_retries: int) -> None: pass ``` Parameters: - `command` (str): The command to be executed by each subprocess. - `num_subprocesses` (int): The number of subprocesses to be launched. - `max_retries` (int): The maximum number of times to attempt restarting a failed subprocess. Constraints: - The function should initialize and use an instance of `SubprocessHandler` to manage the subprocesses. - The function should listen for a termination signal and gracefully terminate all subprocesses when such a signal is received. - If a subprocess fails, it should be restarted up to `max_retries` times. If it continues to fail beyond this limit, the function should log the failure and continue monitoring the remaining subprocesses. Example: ```python import torch.distributed.elastic.multiprocessing.subprocess_handler as mph def manage_subprocesses(command: str, num_subprocesses: int, max_retries: int) -> None: # Example implementation outline subprocess_handler = mph.get_subprocess_handler() for i in range(num_subprocesses): retries = 0 while retries <= max_retries: try: process = subprocess_handler.launch(command) process.wait() if process.returncode != 0: raise RuntimeError(f\\"Subprocess {i} failed with return code {process.returncode}\\") break # Successful execution except Exception as e: retries += 1 if retries > max_retries: print(f\\"Subprocess {i} failed after {max_retries} retries: {e}\\") subprocess_handler.terminate() ``` Note that this is just an outline and the complete implementation should handle all required functionalities and edge cases as described. Hints: - `SubprocessHandler` should have methods for launching a subprocess and monitoring its status. - Make use of Python\'s exception handling to catch subprocess failures. - Consider using threading or async operations to handle multiple subprocess monitoring and signal handling gracefully. This question tests the students\' understanding of subprocess management, distributed computing concepts, and PyTorch\'s multiprocessing capabilities.","solution":"import subprocess import signal import logging import time logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def manage_subprocesses(command: str, num_subprocesses: int, max_retries: int) -> None: processes = [] retries = [0] * num_subprocesses stop_signal_received = False def handle_stop_signal(signum, frame): nonlocal stop_signal_received stop_signal_received = True logger.info(\\"Stop signal received. Terminating all subprocesses.\\") signal.signal(signal.SIGINT, handle_stop_signal) signal.signal(signal.SIGTERM, handle_stop_signal) for i in range(num_subprocesses): process = subprocess.Popen(command, shell=True) processes.append(process) while any(p.poll() is None for p in processes): if stop_signal_received: for process in processes: if process.poll() is None: process.terminate() break for i, process in enumerate(processes): if process.poll() is not None: if process.returncode != 0: if retries[i] < max_retries: retries[i] += 1 logger.warning(f\\"Subprocess {i} failed. Restart attempt {retries[i]}\\") processes[i] = subprocess.Popen(command, shell=True) else: logger.error(f\\"Subprocess {i} failed after {max_retries} retries\\") else: logger.info(f\\"Subprocess {i} completed successfully.\\") time.sleep(1) for process in processes: if process.poll() is None: process.wait() logger.info(\\"All subprocesses have completed.\\")"},{"question":"# Advanced Python Coding Assessment Question You have been hired by a company to improve their system logging and resource management on a Unix-based server. Your task involves two main requirements: 1. **Implement a function to log system messages using the `syslog` module.** 2. **Implement a function to monitor and report resource usage using the `resource` module.** Both of these should be implemented as Python functions within a single script. Requirements 1. **System Logging Function:** - Function Name: `log_message` - Input: Two parameters: - `priority` (int): The priority level for the message (use syslog priorities). - `message` (str): The message to be logged. - Output: None (the function will log the message to the system log). - Example Call: `log_message(syslog.LOG_WARNING, \\"This is a warning message.\\")` 2. **Resource Monitoring Function:** - Function Name: `get_resource_usage` - Input: None - Output: A dictionary containing the following resource usage statistics: - `user_time` (float): The amount of CPU time used by the user (in seconds). - `system_time` (float): The amount of CPU time used by the system (in seconds). - `max_rss` (int): The maximum resident set size used (in kilobytes). - `shared_memory` (int): The amount of shared memory used (in kilobytes). - `page_faults` (int): The number of page faults. - `io_operations` (int): The number of input/output operations. - Example Call: `usage_stats = get_resource_usage()` Constraints and Notes - Make sure to handle potential exceptions and errors gracefully. - You are required to use the `syslog` module for logging and the `resource` module for obtaining resource usage. - Ensure your code is well-structured and commented to explain its functionality. Example Implementation ```python import syslog import resource def log_message(priority, message): try: syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_USER) syslog.syslog(priority, message) syslog.closelog() except Exception as e: print(f\\"An error occurred while logging the message: {e}\\") def get_resource_usage(): try: usage = resource.getrusage(resource.RUSAGE_SELF) usage_stats = { \'user_time\': usage.ru_utime, \'system_time\': usage.ru_stime, \'max_rss\': usage.ru_maxrss, \'shared_memory\': usage.ru_ixrss, \'page_faults\': usage.ru_majflt, \'io_operations\': usage.ru_inblock + usage.ru_oublock } return usage_stats except Exception as e: print(f\\"An error occurred while fetching resource usage: {e}\\") return {} # Example calls to the functions log_message(syslog.LOG_WARNING, \\"This is a warning message.\\") usage_stats = get_resource_usage() print(usage_stats) ``` Submission Guidelines - Include your complete function definitions. - Provide a brief explanation of how each function works. - Ensure your code runs without errors on a Unix-based system.","solution":"import syslog import resource def log_message(priority, message): Logs a message to the system log with a given priority. Parameters: priority (int): The priority level for the message (use syslog priorities). message (str): The message to be logged. try: # Open syslog syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_USER) # Log the message with the given priority syslog.syslog(priority, message) except Exception as e: print(f\\"An error occurred while logging the message: {e}\\") finally: # Close syslog syslog.closelog() def get_resource_usage(): Returns a dictionary containing resource usage statistics. Output: A dictionary with: - \'user_time\' (float): CPU time used by the user (in seconds). - \'system_time\' (float): CPU time used by the system (in seconds). - \'max_rss\' (int): Maximum resident set size used (in kilobytes). - \'shared_memory\' (int): Shared memory used (in kilobytes). - \'page_faults\' (int): Number of page faults. - \'io_operations\' (int): Number of input/output operations. try: # Get resource usage information usage = resource.getrusage(resource.RUSAGE_SELF) usage_stats = { \'user_time\': usage.ru_utime, \'system_time\': usage.ru_stime, \'max_rss\': usage.ru_maxrss, \'shared_memory\': usage.ru_ixrss, \'page_faults\': usage.ru_majflt, \'io_operations\': usage.ru_inblock + usage.ru_oublock } return usage_stats except Exception as e: print(f\\"An error occurred while fetching resource usage: {e}\\") return {} # Example calls to the functions log_message(syslog.LOG_WARNING, \\"This is a warning message.\\") usage_stats = get_resource_usage() print(usage_stats)"},{"question":"**Challenge: Implement Argument Parsing and Value Building in a Python C Extension** # Objective Create a Python C extension module that efficiently parses input arguments and constructs return values using the methods `PyArg_ParseTuple()` and `Py_BuildValue()` described in the provided documentation. # Problem Statement Write a C extension for Python with a single function `process_data` that: 1. Takes exactly three arguments: - A Unicode string representing a user\'s name. - A floating-point number representing the user\'s score. - An optional keyword argument `multiplier` which defaults to 1.5 if not provided. 2. Processes these arguments and returns: - A dictionary with two keys: - `\\"processed_name\\"`: The input name converted to uppercase. - `\\"adjusted_score\\"`: The product of the input score and the multiplier. # Specifications 1. Use `PyArg_ParseTupleAndKeywords()` to parse input arguments in the function `process_data`. 2. Build the return values using `Py_BuildValue()`. 3. Handle any parsing errors gracefully by raising appropriate exceptions. # Input and Output Formats **Input:** - `name` (str): A string representing the user\'s name. - `score` (float): A floating-point number representing the user\'s score. - `multiplier` (optional, float): A floating-point number that defaults to 1.5. **Output:** - A dictionary containing the processed name and adjusted score. # Example ```python >>> import your_module >>> your_module.process_data(\'Alice\', 82.5) {\'processed_name\': \'ALICE\', \'adjusted_score\': 123.75} >>> your_module.process_data(\'Bob\', 75.0, multiplier=2) {\'processed_name\': \'BOB\', \'adjusted_score\': 150.0} ``` # Constraints - Ensure that the `name` argument is a valid Unicode string. - Ensure that the `score` and `multiplier` arguments (if provided) are of the correct types (`float`). # Performance Requirements - The function should handle typical inputs efficiently. - Ensure that the function checks and handles all possible error conditions specified in the provided documentation. # C Template Below is a template for your C extension function: ```c #include <Python.h> static PyObject* process_data(PyObject* self, PyObject* args, PyObject* kwargs) { const char* name; double score; double multiplier = 1.5; // Default value static char* kwlist[] = {\\"name\\", \\"score\\", \\"multiplier\\", NULL}; // Parsing arguments if (!PyArg_ParseTupleAndKeywords(args, kwargs, \\"sd|d\\", kwlist, &name, &score, &multiplier)) { return NULL; // Handle error } // Process the name: Convert to uppercase char processed_name[strlen(name) + 1]; for (int i = 0; name[i] != \'0\'; i++) { processed_name[i] = toupper(name[i]); } processed_name[strlen(name)] = \'0\'; // Calculate the adjusted score double adjusted_score = score * multiplier; // Build the result dictionary PyObject* result = Py_BuildValue(\\"{s:s, s:d}\\", \\"processed_name\\", processed_name, \\"adjusted_score\\", adjusted_score); return result; } static PyMethodDef MyMethods[] = { {\\"process_data\\", (PyCFunction)process_data, METH_VARARGS | METH_KEYWORDS, \\"Process user data\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, \\"your_module\\", \\"Module documentation\\", -1, MyMethods }; PyMODINIT_FUNC PyInit_your_module(void) { return PyModule_Create(&mymodule); } ``` Compile and test your extension to ensure it works with the provided examples.","solution":"# solution.py def process_data(name, score, multiplier=1.5): Processes the data by converting the name to uppercase and adjusting the score by the multiplier. if not isinstance(name, str): raise TypeError(\\"Name must be a string\\") if not isinstance(score, (int, float)): raise TypeError(\\"Score must be a number\\") if not isinstance(multiplier, (int, float)): raise TypeError(\\"Multiplier must be a number\\") processed_name = name.upper() adjusted_score = score * multiplier result = { \\"processed_name\\": processed_name, \\"adjusted_score\\": adjusted_score } return result"},{"question":"Objective Implement a Python function using the `linecache` module to read specified lines from one or multiple files and return their contents in a structured format. Task You need to write a function `read_lines_from_files(file_line_dict)` that takes a dictionary as input. Each key of the dictionary is a filename (string) and its value is a list of line numbers (integers) that need to be read from that file. The function should return a dictionary where each key is a filename, and its value is another dictionary mapping the requested line numbers to their corresponding lines. Function Signature ```python def read_lines_from_files(file_line_dict: dict) -> dict: ``` Input - `file_line_dict` (dict): A dictionary where each key is a filename (string) and the value is a list of line numbers (list of integers). Example: ```python { \\"file1.txt\\": [1, 3], \\"file2.txt\\": [2, 4, 5] } ``` Output - Returns a dictionary where each key is a filename, and its value is another dictionary mapping the requested line numbers to their corresponding lines in the file. Constraints - If a file doesn\'t exist or a line number doesn\'t exist in a file, the corresponding entry should be an empty string. - Assume the function will not be given more than 100 files and not more than 500 line numbers in total. Example Let’s assume we have two files with the following contents: - `\\"file1.txt\\"`: ``` Line one in file 1 Line two in file 1 Line three in file 1 ``` - `\\"file2.txt\\"`: ``` Line one in file 2 Line two in file 2 Line three in file 2 Line four in file 2 Line five in file 2 ``` Example input: ```python file_line_dict = { \\"file1.txt\\": [1, 3], \\"file2.txt\\": [2, 4, 5] } ``` Example output: ```python { \\"file1.txt\\": { 1: \\"Line one in file 1n\\", 3: \\"Line three in file 1n\\" }, \\"file2.txt\\": { 2: \\"Line two in file 2n\\", 4: \\"Line four in file 2n\\", 5: \\"Line five in file 2n\\" } } ``` Notes - Use the `linecache.getline()` function to read specific lines from the files. - Use the `linecache.clearcache()` function to clear the cache after you have finished reading the lines. Implement the function below: ```python def read_lines_from_files(file_line_dict: dict) -> dict: import linecache result = {} for filename, lines in file_line_dict.items(): result[filename] = {} for line_number in lines: result[filename][line_number] = linecache.getline(filename, line_number) linecache.clearcache() return result ```","solution":"def read_lines_from_files(file_line_dict: dict) -> dict: import linecache result = {} for filename, lines in file_line_dict.items(): result[filename] = {} for line_number in lines: line_content = linecache.getline(filename, line_number) if line_content == \\"\\": result[filename][line_number] = \\"\\" else: result[filename][line_number] = line_content linecache.clearcache() return result"},{"question":"Objective The purpose of this assessment is to evaluate your understanding of the seaborn library and your ability to use the `cubehelix_palette` function to create custom color palettes and colormaps for visualization purposes. Problem Statement You are provided with a dataset representing various species of flowers. Your task is to visualize this dataset using seaborn’s `cubehelix_palette` function. The requirements are as follows: 1. Create a scatter plot with the following customizations: - Use a palette of 10 discrete colors from the `cubehelix_palette` function. - Start the color helix at the point `start=1` and rotate it `rot=0.5` turns. - Apply a nonlinearity to the luminance ramp `gamma=0.8`. - Increase the saturation of the colors `hue=1.2`. - Change the luminance at the start to `dark=0.3` and at the end to `light=0.8`. 2. Generate the scatter plot showing petal length vs petal width, with species color-coded using the custom palette. 3. Save the plot as an image file named `flower_scatter_plot.png`. Input - CSV file: `flower_data.csv` with columns: `species`, `petal_length`, `petal_width`. Output - Image file: `flower_scatter_plot.png`. Constraints - The dataset file `flower_data.csv` will always be present in the working directory. Example Here is an example structure of `flower_data.csv`: ``` species,petal_length,petal_width setosa,1.4,0.2 versicolor,4.7,1.4 virginica,5.5,2.3 ... ``` Implementation Write a Python function `visualize_flower_data()` that implements the above requirements using seaborn. ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_flower_data(): # Load the dataset df = pd.read_csv(\'flower_data.csv\') # Create the custom palette palette = sns.cubehelix_palette(10, start=1, rot=0.5, gamma=0.8, hue=1.2, dark=0.3, light=0.8) # Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'petal_length\', y=\'petal_width\', hue=\'species\', palette=palette) # Save the plot plt.savefig(\'flower_scatter_plot.png\') # Example Usage visualize_flower_data() ``` Note - Make sure your function adheres strictly to the requirements and successfully generates the visualization as specified. - Your function will be tested with different versions of `flower_data.csv`.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_flower_data(): # Load the dataset df = pd.read_csv(\'flower_data.csv\') # Create the custom palette palette = sns.cubehelix_palette(10, start=1, rot=0.5, gamma=0.8, hue=1.2, dark=0.3, light=0.8) # Create the scatter plot plt.figure(figsize=(10, 6)) sns.scatterplot(data=df, x=\'petal_length\', y=\'petal_width\', hue=\'species\', palette=palette) # Save the plot plt.savefig(\'flower_scatter_plot.png\') plt.close() # Example Usage # visualize_flower_data()"},{"question":"Objective Demonstrate your understanding of the `pickle` module for Python object serialization and deserialization, including handling complex objects and custom reduction techniques. Problem Statement You are tasked with creating a system that serializes and deserializes complex Python objects using the `pickle` module. Your system will include functions to handle stateful objects, employ custom reduction techniques, and ensure the correctness of the serialization and deserialization process. Implement the following functions: 1. **serialize_objects(objects: list) -> bytes:** - **Input:** A list of Python objects that may include integers, strings, lists, dictionaries, and instances of a class. - **Output:** A byte stream representing the serialized form of the input objects using the `pickle` module. - **Constraints:** Ensure that the function can handle stateful objects and custom reduction techniques if necessary. 2. **deserialize_objects(data: bytes) -> list:** - **Input:** A byte stream containing serialized Python objects. - **Output:** A list of deserialized Python objects, preserving their original structure and state. - **Constraints:** Ensure that the deserialization process correctly restores the stateful objects and any custom reduction techniques used during serialization. 3. **Example:** ```python class ExampleClass: def __init__(self, name, value): self.name = name self.value = value def __reduce__(self): # Custom reduction method return (self.__class__, (self.name, self.value)) obj1 = ExampleClass(\\"object1\\", 10) obj2 = ExampleClass(\\"object2\\", 20) objects = [1, \\"string\\", [1, 2, 3], {\\"key\\": \\"value\\"}, obj1, obj2] serialized_data = serialize_objects(objects) deserialized_objects = deserialize_objects(serialized_data) assert deserialized_objects[4].name == \\"object1\\" assert deserialized_objects[4].value == 10 assert deserialized_objects[5].name == \\"object2\\" assert deserialized_objects[5].value == 20 ``` Performance Requirements - The functions should handle large lists of objects efficiently. - Serialization and deserialization should be done with error handling to manage any potential issues that might arise from complex object structures. Notes - You may assume that all objects in the list are serializable using the `pickle` module. - Ensure that the custom reduction method in the class is correctly implemented and invoked during the serialization process. - Test your implementation with various types of objects to validate the correctness of your functions.","solution":"import pickle def serialize_objects(objects: list) -> bytes: Serialize a list of Python objects to a byte stream using the pickle module. Args: - objects (list): A list of Python objects. Returns: - bytes: The serialized byte stream. return pickle.dumps(objects) def deserialize_objects(data: bytes) -> list: Deserialize a byte stream to a list of Python objects using the pickle module. Args: - data (bytes): A byte stream containing serialized Python objects. Returns: - list: A list of deserialized Python objects. return pickle.loads(data) class ExampleClass: def __init__(self, name, value): self.name = name self.value = value def __reduce__(self): # Custom reduction method return (self.__class__, (self.name, self.value))"},{"question":"# Question: Encoding and Decoding Binary Data with `binascii` You are tasked with implementing a function that reads a string input and performs a series of conversions using the `binascii` module functions. The goal is to demonstrate the ability to handle multiple encoding and decoding schemes. Function Specification ```python def process_binary_data(input_string: str) -> dict: pass ``` Inputs - `input_string`: A string which can contain ASCII text intended for various ASCII-encoded binary formats. Outputs The function should return a dictionary with the following keys: - `\\"base64_encoded\\"`: The input string encoded in base64 format. - `\\"base64_decoded\\"`: The above base64 encoded string decoded back to binary. - `\\"quoted_printable_encoded\\"`: The input string encoded in quoted-printable format. - `\\"quoted_printable_decoded\\"`: The above quoted-printable encoded string decoded back to binary. - `\\"hex_encoded\\"`: The input string encoded in hexadecimal format. - `\\"hex_decoded\\"`: The above hex encoded string decoded back to binary. Constraints - You should use the `binascii` module for all encodings and decodings. - Take care of any necessary type conversions between `str` and `bytes`. - Manage any exceptions that might arise from invalid inputs or conversion discrepancies. Example Usage ```python input_string = \\"hello world\\" output = process_binary_data(input_string) print(output) # Expected Output (Values are illustrative): { \\"base64_encoded\\": \\"aGVsbG8gd29ybGQ=\\", \\"base64_decoded\\": b\\"hello world\\", \\"quoted_printable_encoded\\": \\"hello=20world\\", \\"quoted_printable_decoded\\": b\\"hello world\\", \\"hex_encoded\\": \\"68656c6c6f20776f726c64\\", \\"hex_decoded\\": b\\"hello world\\" } ``` **Note:** This problem tests your ability to understand and use the `binascii` module for various binary and ASCII encodings. Make sure your function handles all specified formats and correctly processes the return values to match the example output.","solution":"import binascii def process_binary_data(input_string: str) -> dict: Processes the input string through various binascii encoding and decoding schemes. Args: - input_string: A string which contains ASCII text. Returns: - A dictionary containing the encoded and decoded results in different formats. # Convert the input string to bytes for processing input_bytes = input_string.encode(\'ascii\') # Base64 encoding and decoding base64_encoded = binascii.b2a_base64(input_bytes, newline=False).decode(\'ascii\') base64_decoded = binascii.a2b_base64(base64_encoded) # Quoted-printable encoding and decoding quoted_printable_encoded = binascii.b2a_qp(input_bytes).decode(\'ascii\') quoted_printable_decoded = binascii.a2b_qp(quoted_printable_encoded) # Hex encoding and decoding hex_encoded = binascii.hexlify(input_bytes).decode(\'ascii\') hex_decoded = binascii.unhexlify(hex_encoded) return { \\"base64_encoded\\": base64_encoded, \\"base64_decoded\\": base64_decoded, \\"quoted_printable_encoded\\": quoted_printable_encoded, \\"quoted_printable_decoded\\": quoted_printable_decoded, \\"hex_encoded\\": hex_encoded, \\"hex_decoded\\": hex_decoded }"},{"question":"<|Analysis Begin|> The given documentation provides comprehensive details about the \\"test\\" package used for regression tests in Python. The significant aspects include the structure and guidelines for writing unit tests using the `unittest` and `doctest` modules, utilities provided by `test.support` for enhancing tests, and helper modules (`socket_helper`, `script_helper`, `bytecode_helper`, `threading_helper`, `os_helper`, `import_helper`, `warnings_helper`). The package is designed for Python\'s core development and includes numerous utilities for creating robust tests, managing resources, handling environments, and facilitating clean-up processes. There are extensive functionalities for checking certain system capabilities, patching objects, handling subprocesses, and managing warnings. Given these details, we can design a challenging and comprehensive question that requires students to write a unit test for a simple function while utilizing some of the utilities from the `test.support` module. <|Analysis End|> <|Question Begin|> **Question:** You are provided with a function `is_prime` that checks if a given number is a prime number. Write a unit test for this function using the `unittest` module. Your test should use the following utilities from the `test.support` module: - `captured_stdout` to capture standard output. - `swap_attr` to temporarily change an attribute. - `gc_collect` to ensure all objects are garbage collected. The `is_prime` function is defined as follows: ```python def is_prime(n): Return True if n is a prime number, else False. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True ``` **Requirements:** 1. Create a test class `TestIsPrime` that inherits from `unittest.TestCase`. 2. Write a `setUp` method to prepare any resources needed for the tests. 3. Write a `tearDown` method to clean up after the tests. 4. Implement the following test methods: - `test_prime_numbers`: Test the function with prime numbers. - `test_non_prime_numbers`: Test the function with non-prime numbers. - `test_small_values`: Test the function with small values (negative numbers, zero, and one). - `test_output`: Use `captured_stdout` to verify no output is printed. - `test_temporary_change`: Use `swap_attr` to temporarily change an attribute and verify it is restored afterwards. 5. Ensure that garbage collection is performed after each test using `gc_collect`. The input to your function will be an integer, and it should return a boolean. ```python import unittest from test import support class TestIsPrime(unittest.TestCase): def setUp(self): # Your setup code here pass def tearDown(self): # Your teardown code here support.gc_collect() def test_prime_numbers(self): # Your code here pass def test_non_prime_numbers(self): # Your code here pass def test_small_values(self): # Your code here pass def test_output(self): # Your code here pass def test_temporary_change(self): # Your code here pass if __name__ == \'__main__\': unittest.main() ``` Complete the `setUp`, `tearDown`, and test methods as per the specifications.","solution":"import unittest from test import support import sys def is_prime(n): Return True if n is a prime number, else False. if n <= 1: return False if n <= 3: return True if n % 2 == 0 or n % 3 == 0: return False i = 5 while i * i <= n: if n % i == 0 or n % (i + 2) == 0: return False i += 6 return True class TestIsPrime(unittest.TestCase): def setUp(self): self.original_stdout = sys.stdout def tearDown(self): support.gc_collect() sys.stdout = self.original_stdout def test_prime_numbers(self): self.assertTrue(is_prime(2)) self.assertTrue(is_prime(3)) self.assertTrue(is_prime(5)) self.assertTrue(is_prime(7)) self.assertTrue(is_prime(11)) def test_non_prime_numbers(self): self.assertFalse(is_prime(1)) self.assertFalse(is_prime(4)) self.assertFalse(is_prime(6)) self.assertFalse(is_prime(8)) self.assertFalse(is_prime(9)) self.assertFalse(is_prime(10)) def test_small_values(self): self.assertFalse(is_prime(-1)) self.assertFalse(is_prime(0)) self.assertFalse(is_prime(1)) def test_output(self): with support.captured_stdout() as stdout: self.assertTrue(is_prime(7)) self.assertTrue(stdout.getvalue() == \\"\\") def test_temporary_change(self): class DummyClass: attribute = True dummy = DummyClass() with support.swap_attr(dummy, \'attribute\', False): self.assertFalse(dummy.attribute) # Check that the original value is restored self.assertTrue(dummy.attribute) if __name__ == \'__main__\': unittest.main()"},{"question":"Coding Assessment Question # Title: Detailed DataFrame Memory Usage and Boolean Evaluation # Objective: To assess students\' ability to manipulate and investigate pandas DataFrame memory usage while using boolean evaluations and conditions explicitly. This assignment will test their understanding of pandas\' memory efficiency methods and boolean computation. # Problem Statement: 1. Given a DataFrame created from a variety of data types, calculate and display the detailed memory usage of the DataFrame. 2. Develop a function that filters rows based on specific boolean conditions involving multiple columns without causing `ValueError` exceptions from ambiguous boolean evaluations. 3. Confirm the accuracy of memory usage calculations by comparing memory reports before and after operations. # Specifications: **Function 1: memory_usage_analysis** ```python def memory_usage_analysis(df: pd.DataFrame) -> dict: Calculates the detailed memory usage of each column in the DataFrame and returns a dictionary where keys are column names and values are memory usage in bytes. Parameters: df (pd.DataFrame): The input DataFrame. Returns: dict: A dictionary with column names as keys and their memory usage in bytes as values. pass ``` **Function 2: filter_dataframe** ```python def filter_dataframe(df: pd.DataFrame, filter_cond: dict) -> pd.DataFrame: Filters rows of the DataFrame based on the specified boolean conditions provided in the dictionary filter_cond. The dictionary keys represent column names, and values represent the boolean condition to be applied on the columns. Parameters: df (pd.DataFrame): The input DataFrame. filter_cond (dict): Dictionary with column names as keys and conditions for filtering as values. Returns: pd.DataFrame: A DataFrame filtered according to the given conditions. pass ``` # Constraints: 1. The DataFrame `df` will have at least the following columns with respective data types: `[\'int_col\', \'float_col\', \'bool_col\', \'str_col\']` with types `int64`, `float64`, `bool`, `object`. 2. Boolean condition should not lead to a `ValueError`; should make appropriate use of `any`, `all`, and direct conditional checks. 3. Memory usage computation should use pandas\' deep accounting methods for accurate results. 4. Assume all necessary libraries (`pandas`, `numpy`) are already imported and available. **Example Input/Output:** ```python # Example DataFrame data = { \'int_col\': [1, 2, 3, 4, 5], \'float_col\': [10.1, 20.2, 30.3, 40.4, 50.5], \'bool_col\': [True, False, True, False, True], \'str_col\': [\'A\', \'B\', \'A\', \'B\', \'A\'] } df = pd.DataFrame(data) # Test memory usage analysis function result = memory_usage_analysis(df) print(result) # Expected Output: # {\'int_col\': 40, # \'float_col\': 40, # \'bool_col\': 1, # \'str_col\': 240} # Test filtering based on conditions filter_conditions = {\'bool_col\': True, \'str_col\': \'A\'} filtered_df = filter_dataframe(df, filter_conditions) print(filtered_df) # Expected Output: # int_col float_col bool_col str_col # 0 1 10.1 True A # 2 3 30.3 True A # 4 5 50.5 True A ``` Please submit your completed implementations for both functions.","solution":"import pandas as pd def memory_usage_analysis(df: pd.DataFrame) -> dict: Calculates the detailed memory usage of each column in the DataFrame and returns a dictionary where keys are column names and values are memory usage in bytes. Parameters: df (pd.DataFrame): The input DataFrame. Returns: dict: A dictionary with column names as keys and their memory usage in bytes as values. memory_usage = df.memory_usage(deep=True).to_dict() memory_usage.pop(\'Index\', None) # Remove the Index entry if present return memory_usage def filter_dataframe(df: pd.DataFrame, filter_cond: dict) -> pd.DataFrame: Filters rows of the DataFrame based on the specified boolean conditions provided in the dictionary filter_cond. The dictionary keys represent column names, and values represent the boolean condition to be applied on the columns. Parameters: df (pd.DataFrame): The input DataFrame. filter_cond (dict): Dictionary with column names as keys and conditions for filtering as values. Returns: pd.DataFrame: A DataFrame filtered according to the given conditions. boolean_series = pd.Series([True] * len(df)) for column, condition in filter_cond.items(): if isinstance(condition, (list, set, pd.Series)): # If the condition is iterable boolean_series &= df[column].isin(condition) else: # If the condition is a single value boolean_series &= (df[column] == condition) return df[boolean_series]"},{"question":"# Programming Assessment Question **Objective:** Implement a set of functions that mimic the behavior of the old buffer protocol functions described while using the new buffer protocol APIs (`PyObject_GetBuffer` and `PyBuffer_Release`) in Python 3.x. **Problem Statement:** You are given a Python object that supports the buffer interface. Implement the following functions: 1. `custom_as_char_buffer(obj)` 2. `custom_as_read_buffer(obj)` 3. `custom_check_read_buffer(obj)` 4. `custom_as_write_buffer(obj)` These functions should mimic the behavior of the old API functions: - `custom_as_char_buffer(obj)` should return a read-only buffer memory location as characters. - `custom_as_read_buffer(obj)` should return a read-only buffer memory location containing arbitrary data. - `custom_check_read_buffer(obj)` should return `True` if the object supports the readable buffer interface and `False` otherwise. - `custom_as_write_buffer(obj)` should return a writable buffer memory location. **Function Specifications:** 1. **`custom_as_char_buffer(obj: Any) -> Tuple[Union[str, None], int]`** - **Input:** A Python object `obj` that supports the buffer interface. - **Output:** A tuple where the first element is a string representing the read-only buffer memory location and the second element is the length of the buffer. If the object does not support the buffer interface, return `(None, 0)`. 2. **`custom_as_read_buffer(obj: Any) -> Tuple[Union[bytes, None], int]`** - **Input:** A Python object `obj` that supports the buffer interface. - **Output:** A tuple where the first element is a bytes object representing the read-only buffer memory location and the second element is the length of the buffer. If the object does not support the buffer interface, return `(None, 0)`. 3. **`custom_check_read_buffer(obj: Any) -> bool`** - **Input:** A Python object `obj`. - **Output:** `True` if `obj` supports the readable buffer interface, `False` otherwise. 4. **`custom_as_write_buffer(obj: Any) -> Tuple[Union[bytearray, None], int]`** - **Input:** A Python object `obj` that supports the buffer interface. - **Output:** A tuple where the first element is a bytearray object representing the writable buffer memory location and the second element is the length of the buffer. If the object does not support the buffer interface, return `(None, 0)`. **Constraints:** - You must use `memoryview` objects to interact with the buffer protocol. - You should handle any exceptions that may occur and return appropriate values as specified. **Example Usage:** ```python # Dummy object class class BufferObject: def __init__(self, data): self.data = data def __buffer__(self): return memoryview(self.data) # Test cases obj = BufferObject(bytearray(b\\"Hello World\\")) # custom_as_char_buffer assert custom_as_char_buffer(obj) == (b\\"Hello World\\".decode(\'utf-8\'), 11) # custom_as_read_buffer assert custom_as_read_buffer(obj) == (b\\"Hello World\\", 11) # custom_check_read_buffer assert custom_check_read_buffer(obj) is True # custom_as_write_buffer buffer, length = custom_as_write_buffer(obj) assert isinstance(buffer, bytearray) assert length == 11 ``` **Note:** The provided `BufferObject` class is for illustration purposes; you may define more appropriate classes or structures for your implementation.","solution":"def custom_as_char_buffer(obj): Returns a read-only buffer memory location as characters. try: buffer = memoryview(obj) return (buffer.tobytes().decode(\'utf-8\', errors=\'ignore\'), len(buffer)) except (TypeError, ValueError): return (None, 0) def custom_as_read_buffer(obj): Returns a read-only buffer memory location containing arbitrary data. try: buffer = memoryview(obj) return (bytes(buffer), len(buffer)) except (TypeError, ValueError): return (None, 0) def custom_check_read_buffer(obj): Returns True if the object supports the readable buffer interface, False otherwise. try: memoryview(obj) return True except (TypeError, ValueError): return False def custom_as_write_buffer(obj): Returns a writable buffer memory location. try: buffer = memoryview(obj) if buffer.readonly: return (None, 0) else: return (bytearray(buffer), len(buffer)) except (TypeError, ValueError): return (None, 0)"},{"question":"**Tensor Operations and Gradient Calculation Using PyTorch** You are tasked with implementing a function that takes two tensors as inputs, performs a series of tensor operations, and calculates the gradients with respect to the inputs. # Function Signature ```python import torch def tensor_operations_and_gradients(input1: torch.Tensor, input2: torch.Tensor) -> torch.Tensor: This function takes two input tensors, performs a series of tensor operations, and returns the gradient of the sum of squared results with respect to the inputs. Args: - input1: A tensor of shape (m, n) of type torch.float32. - input2: A tensor of shape (m, n) of type torch.float32. Returns: - grads: A tensor of the same shape as the inputs containing the gradients. pass ``` # Description 1. **Input Constraints**: - `input1` and `input2` are tensors of shape `(m, n)` and dtype `torch.float32`. - Both tensors are guaranteed to be on the same device (CPU or GPU). 2. **Operation Steps**: - Create a tensor `result` by performing element-wise addition of `input1` and `input2`. - Create a tensor `squared_result` by squaring each element of `result`. - Calculate the sum of all elements in `squared_result`. 3. **Gradient Calculation**: - Set `requires_grad=True` for both `input1` and `input2` to ensure gradients can be calculated. - Perform backpropagation to calculate gradients with respect to the sum of the squared results. 4. **Output**: - Return the gradients of `input1` and `input2` as a tensor. If the input tensors are `input1` and `input2`, the output should be a tensor containing the gradients. # Example ```python # Example inputs input1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], dtype=torch.float32, requires_grad=True) input2 = torch.tensor([[10.0, 20.0], [30.0, 40.0]], dtype=torch.float32, requires_grad=True) # Call the function grads = tensor_operations_and_gradients(input1, input2) # Output for gradients print(grads) # Expected output: tensor([[??, ??], [??, ??]]) ``` # Constraints - You must use PyTorch methods and operations to solve this problem. - Avoid using any loops; leverage PyTorch\'s vectorized operations for efficiency. - Ensure that the function is compatible with tensors on both CUDA and CPU devices.","solution":"import torch def tensor_operations_and_gradients(input1: torch.Tensor, input2: torch.Tensor) -> torch.Tensor: This function takes two input tensors, performs a series of tensor operations, and returns the gradient of the sum of squared results with respect to the inputs. Args: - input1: A tensor of shape (m, n) of type torch.float32. - input2: A tensor of shape (m, n) of type torch.float32. Returns: - grads: A tuple of two tensors containing the gradients of input1 and input2. # Ensure the tensors have requires_grad set to True input1.requires_grad_(True) input2.requires_grad_(True) # Perform the operations result = input1 + input2 squared_result = result ** 2 sum_squared_result = squared_result.sum() # Perform backpropagation to get the gradients sum_squared_result.backward() # Retrieve the gradients grad_input1 = input1.grad grad_input2 = input2.grad return grad_input1, grad_input2"},{"question":"# Question: **Pickling and Unpickling Complex Objects with Custom Handling** You are given a custom class `Employee` and another class `Department` that contains a list of `Employee` objects. Your task is to implement pickling and unpickling for these classes using the `pickle` module. Additionally, you should demonstrate custom handling for the `Employee` class using `__getstate__` and `__setstate__` to modify their state during pickling and unpickling process. Requirements: 1. **Employee Class**: - Attributes: `name` (str), `age` (int), `salary` (float), `department` (str). - Implement `__getstate__` and `__setstate__` methods to add a `bonus` attribute (10% of salary) during the pickling process and restore it during unpickling. 2. **Department Class**: - Attributes: `name` (str), `employees` (List of `Employee` objects). - Implement the necessary mechanism using `pickle` to serialize and deserialize the `Department` object and its associated `Employee` objects. 3. **Serialization and Deserialization**: - Serialize a `Department` object containing a few `Employee` objects to a byte stream using `pickle.dumps`. - Deserialize the byte stream back into a `Department` object using `pickle.loads`. 4. **Validation**: - Ensure that the `bonus` attribute is correctly added during pickling and restored during unpickling in the `Employee` objects. - Print the details of the `Department` and `Employee` objects before serialization and after deserialization to validate the correctness. Constraints: - Do not use any third-party libraries. - Handle all exceptions that may arise during the pickling and unpickling process. **Example Usage:** ```python # Define the classes and methods as specified class Employee: def __init__(self, name, age, salary, department): self.name = name self.age = age self.salary = salary self.department = department def __getstate__(self): # Add custom state modification logic here ... def __setstate__(self, state): # Add custom state restoration logic here ... class Department: def __init__(self, name): self.name = name self.employees = [] def add_employee(self, employee): self.employees.append(employee) # Create Department and Employee objects dept = Department(\'Engineering\') emp1 = Employee(\'Alice\', 30, 70000, \'Engineering\') emp2 = Employee(\'Bob\', 25, 50000, \'Engineering\') dept.add_employee(emp1) dept.add_employee(emp2) # Serialize the Department object serialized_dept = pickle.dumps(dept) # Deserialize the byte stream back into a Department object deserialized_dept = pickle.loads(serialized_dept) # Validate the data integrity and custom state handling print(dept) print(deserialized_dept) ``` Provide your complete implementation below. Ensure that the classes and methods correctly demonstrate the required functionality.","solution":"import pickle class Employee: def __init__(self, name, age, salary, department): self.name = name self.age = age self.salary = salary self.department = department self.bonus = None def __getstate__(self): state = self.__dict__.copy() state[\'bonus\'] = self.salary * 0.1 return state def __setstate__(self, state): self.__dict__ = state class Department: def __init__(self, name): self.name = name self.employees = [] def add_employee(self, employee): self.employees.append(employee) # Demonstration code (not a part of the unit test) if __name__ == \\"__main__\\": dept = Department(\'Engineering\') emp1 = Employee(\'Alice\', 30, 70000, \'Engineering\') emp2 = Employee(\'Bob\', 25, 50000, \'Engineering\') dept.add_employee(emp1) dept.add_employee(emp2) # Serialize the Department object serialized_dept = pickle.dumps(dept) # Deserialize the byte stream back into a Department object deserialized_dept = pickle.loads(serialized_dept) # Validate the data integrity and custom state handling for emp in deserialized_dept.employees: print(f\\"Name: {emp.name}, Age: {emp.age}, Salary: {emp.salary}, \\" f\\"Department: {emp.department}, Bonus: {emp.bonus}\\")"},{"question":"**Question**: Implementing a Custom Module Manager in Python You are required to implement a class `CustomModuleManager` in Python which facilitates dynamic importing, reloading, and execution of code within modules using the provided C API functions from the Python 3.10 documentation. This class will expose various methods that wrap the functionality of these C API functions and should handle exceptions gracefully. # Requirements 1. **Create the Class `CustomModuleManager`**: * The class should be initialized without any parameters. 2. **Method: `load_module`**: * **Definition**: `def load_module(self, name: str) -> Optional[ModuleType]:` * **Input**: A string `name` representing the module name to load. * **Output**: Returns the module if successful, `None` if loading fails. 3. **Method: `reload_module`**: * **Definition**: `def reload_module(self, module: ModuleType) -> Optional[ModuleType]:` * **Input**: A module object to reload. * **Output**: Returns the reloaded module if successful, `None` if reloading fails. 4. **Method: `execute_code_in_module`**: * **Definition**: `def execute_code_in_module(self, module_name: str, code: str) -> bool:` * **Input**: A string `module_name` representing the name of the target module and a string `code` containing the Python code to execute within the module. * **Output**: Returns `True` if the code execution is successful, `False` otherwise. 5. **Method: `get_module_dict`**: * **Definition**: `def get_module_dict(self) -> dict:` * **Output**: Returns the dictionary of currently loaded modules. 6. **Error Handling**: * Ensure that all methods gracefully handle exceptions and return appropriate messages or values (like `None` or `False`). # Performance Constraints - All operations should be efficient and handle typical use cases for module import, reload, and code execution. # Example Usage ```python manager = CustomModuleManager() # Load a module called \'math\' math_module = manager.load_module(\'math\') if math_module: print(f\\"Module loaded: {math_module}\\") # Reload the math module reloaded_math_module = manager.reload_module(math_module) if reloaded_math_module: print(f\\"Module reloaded: {reloaded_math_module}\\") # Execute code in a module exec_success = manager.execute_code_in_module(\'math\', \'print(\\"Pi:\\", math.pi)\') if exec_success: print(\\"Code executed successfully.\\") # Get the module dictionary module_dict = manager.get_module_dict() print(f\\"Loaded modules: {module_dict}\\") ``` # Constraints - The `CustomModuleManager` class should work with Python 3.10 and later versions. - Ensure that all reference handling follows best practices to avoid memory leaks.","solution":"import importlib import sys from types import ModuleType from typing import Optional class CustomModuleManager: def __init__(self): pass def load_module(self, name: str) -> Optional[ModuleType]: try: module = importlib.import_module(name) return module except ImportError as e: print(f\\"Error loading module {name}: {e}\\") return None def reload_module(self, module: ModuleType) -> Optional[ModuleType]: try: reloaded_module = importlib.reload(module) return reloaded_module except Exception as e: print(f\\"Error reloading module {module}: {e}\\") return None def execute_code_in_module(self, module_name: str, code: str) -> bool: try: module = importlib.import_module(module_name) exec(code, module.__dict__) return True except Exception as e: print(f\\"Error executing code in module {module_name}: {e}\\") return False def get_module_dict(self) -> dict: return sys.modules"},{"question":"Objective: The goal is to demonstrate your understanding of the `sysconfig` module in Python by retrieving specific configuration details and formatting them appropriately. Task: Write a Python function `retrieve_system_config_info()` which retrieves and returns certain system configuration details. Function Signature: ```python def retrieve_system_config_info() -> dict: pass ``` Requirements: 1. Use the `sysconfig` module to fetch the following information: - The current platform identifier using `get_platform()`. - The current Python version in \\"MAJOR.MINOR\\" format using `get_python_version()`. - The default installation scheme name for the current platform using `get_default_scheme()`. - A dictionary of all paths corresponding to the default installation scheme using `get_paths()`. - The value of the specific configuration variable `LIBDIR` using `get_config_var(\'LIBDIR\')`. - The values of multiple configuration variables `AR` and `CXX` using `get_config_vars(\'AR\', \'CXX\')`. 2. Format the retrieved information into a dictionary with appropriate keys as shown below. Expected Output Dictionary Format: ```python { \\"platform\\": \\"current_platform_identifier\\", \\"python_version\\": \\"MAJOR.MINOR\\", \\"default_scheme\\": \\"default_scheme_name\\", \\"paths\\": { \\"data\\": \\"data_path\\", \\"include\\": \\"include_path\\", \\"platinclude\\": \\"platinclude_path\\", \\"platlib\\": \\"platlib_path\\", \\"platstdlib\\": \\"platstdlib_path\\", \\"purelib\\": \\"purelib_path\\", \\"scripts\\": \\"scripts_path\\", \\"stdlib\\": \\"stdlib_path\\" }, \\"libdir\\": \\"LIBDIR_value\\", \\"config_vars\\": { \\"AR\\": \\"AR_value\\", \\"CXX\\": \\"CXX_value\\" } } ``` Constraints: - The function should handle cases where some configuration variable values are not found and set them as `None`. Example: ```python import sysconfig def retrieve_system_config_info(): info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"paths\\": sysconfig.get_paths(), \\"libdir\\": sysconfig.get_config_var(\'LIBDIR\'), \\"config_vars\\": { \\"AR\\": sysconfig.get_config_var(\'AR\'), \\"CXX\\": sysconfig.get_config_var(\'CXX\'), } } return info # Example call to the function system_config_info = retrieve_system_config_info() print(system_config_info) ``` The expected output should be a detailed dictionary with the specified keys and their corresponding values as retrieved from the `sysconfig` module.","solution":"import sysconfig def retrieve_system_config_info() -> dict: paths = sysconfig.get_paths() # Formatting the paths according to the given specifications formatted_paths = { \\"data\\": paths.get(\'data\', None), \\"include\\": paths.get(\'include\', None), \\"platinclude\\": paths.get(\'platinclude\', None), \\"platlib\\": paths.get(\'platlib\', None), \\"platstdlib\\": paths.get(\'platstdlib\', None), \\"purelib\\": paths.get(\'purelib\', None), \\"scripts\\": paths.get(\'scripts\', None), \\"stdlib\\": paths.get(\'stdlib\', None) } info = { \\"platform\\": sysconfig.get_platform(), \\"python_version\\": sysconfig.get_python_version(), \\"default_scheme\\": sysconfig.get_default_scheme(), \\"paths\\": formatted_paths, \\"libdir\\": sysconfig.get_config_var(\'LIBDIR\'), \\"config_vars\\": { \\"AR\\": sysconfig.get_config_var(\'AR\'), \\"CXX\\": sysconfig.get_config_var(\'CXX\'), } } return info"},{"question":"Problem Statement You are given a dataset of text documents for a binary classification task. Your job is to preprocess the text data and train a Multinomial Naive Bayes classifier to classify the documents. # Task 1. Load the dataset from the given path. The dataset consists of two columns: `text` and `label`. 2. Preprocess the text data using suitable techniques, such as tokenization, removing stop words, and converting text to a numeric feature vector. 3. Split the data into training and testing sets. 4. Train a Multinomial Naive Bayes classifier on the training set using scikit-learn. 5. Evaluate the performance of the classifier on the test set and output the accuracy score. # Input Format - A CSV file located at a given path with two columns: `text` (string) and `label` (integer: 0 or 1). # Output Format - A float representing the accuracy score of the classifier on the test set. # Constraints - You should use the `TfidfVectorizer` from `sklearn.feature_extraction.text` for text vectorization. - Use `train_test_split` from `sklearn.model_selection` to split the data. - Use `accuracy_score` from `sklearn.metrics` to evaluate the classifier. # Example Given a CSV file `dataset.csv` with the following content: | text | label | |----------------------------|-------| | \\"I love programming.\\" | 1 | | \\"Python is a great language.\\" | 1 | | \\"I hate bugs.\\" | 0 | | \\"Debugging is fun.\\" | 1 | | \\"I dislike syntax errors.\\" | 0 | The expected output can be a float like `0.8`, representing an accuracy score of 80%. # Implementation ```python import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def load_data(path): Load the dataset from the specified path. Parameters: path (str): The path to the CSV file containing the dataset. Returns: DataFrame: A pandas DataFrame containing the text and label columns. return pd.read_csv(path) def preprocess_and_train(df): Preprocess the text data and train a Multinomial Naive Bayes classifier. Parameters: df (DataFrame): A pandas DataFrame containing the text and label columns. Returns: float: The accuracy score of the classifier on the test set. # Extract features and labels X = df[\'text\'] y = df[\'label\'] # Preprocess the text data using TfidfVectorizer vectorizer = TfidfVectorizer(stop_words=\'english\') X_transformed = vectorizer.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.3, random_state=42) # Train a Multinomial Naive Bayes classifier model = MultinomialNB() model.fit(X_train, y_train) # Predict the labels on the test set y_pred = model.predict(X_test) # Calculate and return the accuracy score return accuracy_score(y_test, y_pred) # Example usage if __name__ == \\"__main__\\": data_path = \\"dataset.csv\\" df = load_data(data_path) accuracy = preprocess_and_train(df) print(f\\"Accuracy: {accuracy:.4f}\\") ``` Ensure that the dataset is located at the given path, and the structure of the CSV file matches the described format.","solution":"import pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split from sklearn.naive_bayes import MultinomialNB from sklearn.metrics import accuracy_score def load_data(path): Load the dataset from the specified path. Parameters: path (str): The path to the CSV file containing the dataset. Returns: DataFrame: A pandas DataFrame containing the text and label columns. return pd.read_csv(path) def preprocess_and_train(df): Preprocess the text data and train a Multinomial Naive Bayes classifier. Parameters: df (DataFrame): A pandas DataFrame containing the text and label columns. Returns: float: The accuracy score of the classifier on the test set. # Extract features and labels X = df[\'text\'] y = df[\'label\'] # Preprocess the text data using TfidfVectorizer vectorizer = TfidfVectorizer(stop_words=\'english\') X_transformed = vectorizer.fit_transform(X) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.3, random_state=42) # Train a Multinomial Naive Bayes classifier model = MultinomialNB() model.fit(X_train, y_train) # Predict the labels on the test set y_pred = model.predict(X_test) # Calculate and return the accuracy score return accuracy_score(y_test, y_pred)"},{"question":"# Advanced Seaborn Heatmap Challenge **Objective:** Create advanced heatmaps using the `seaborn` library. Your task is to write a Python function to generate a customized heatmap based on the provided dataset. **Specifications:** 1. **Function Definition** ```python def create_custom_heatmap(data: pd.DataFrame, index_col: str, columns_col: str, values_col: str, annot_format: str = \\".2f\\", cmap_name: str = \\"viridis\\", vmin: float = None, vmax: float = None, linewidth: float = 0.5, xlabel: str = \\"X-axis\\", ylabel: str = \\"Y-axis\\", colorbar_label: str = \\"Value\\") -> plt.Figure: pass ``` 2. **Input:** - `data` (*pd.DataFrame*): The input DataFrame containing the data. - `index_col` (*str*): The name of the column to use as the row labels in the heatmap. - `columns_col` (*str*): The name of the column to use as the column labels in the heatmap. - `values_col` (*str*): The name of the column to use as the cell values in the heatmap. - `annot_format` (*str*): A format string for the annotations (default is \\".2f\\"). - `cmap_name` (*str*): The name of the colormap to use (default is \\"viridis\\"). - `vmin` (*float, optional*): The minimum value for colormap normalization (default is None). - `vmax` (*float, optional*): The maximum value for colormap normalization (default is None). - `linewidth` (*float*): The width of the lines between cells (default is 0.5). - `xlabel` (*str*): The label for the x-axis (default is \\"X-axis\\"). - `ylabel` (*str*): The label for the y-axis (default is \\"Y-axis\\"). - `colorbar_label` (*str*): The label for the colorbar (default is \\"Value\\"). 3. **Output:** - The function should return a `matplotlib.figure.Figure` object representing the generated heatmap. 4. **Constraints:** - The input DataFrame will always contain valid data. - The specified columns for index, columns, and values will exist in the DataFrame. 5. **Additional Requirements:** - Create a pivot table from the dataset using the specified index, columns, and values. - Generate a Seaborn heatmap using the pivot table. - Annotate the heatmap cells based on the specified annotation format. - Customize the colormap and its normalization based on the provided arguments. - Add lines between the cells of the heatmap. - Customize the x-axis, y-axis labels, and the colorbar label. **Example Usage:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load an example dataset data = sns.load_dataset(\\"glue\\") # Create and display the custom heatmap fig = create_custom_heatmap( data=data, index_col=\\"Model\\", columns_col=\\"Task\\", values_col=\\"Score\\", annot_format=\\".1f\\", cmap_name=\\"coolwarm\\", vmin=50, vmax=100, linewidth=1, xlabel=\\"Tasks\\", ylabel=\\"Models\\", colorbar_label=\\"Scores\\" ) plt.show() ```","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_custom_heatmap(data: pd.DataFrame, index_col: str, columns_col: str, values_col: str, annot_format: str = \\".2f\\", cmap_name: str = \\"viridis\\", vmin: float = None, vmax: float = None, linewidth: float = 0.5, xlabel: str = \\"X-axis\\", ylabel: str = \\"Y-axis\\", colorbar_label: str = \\"Value\\") -> plt.Figure: # Create pivot table pivot_table = data.pivot(index=index_col, columns=columns_col, values=values_col) # Create the heatmap plt.figure(figsize=(10, 8)) heatmap = sns.heatmap(pivot_table, annot=True, fmt=annot_format, cmap=cmap_name, vmin=vmin, vmax=vmax, linewidths=linewidth, cbar_kws={\'label\': colorbar_label}) # Customize axis labels heatmap.set_xlabel(xlabel) heatmap.set_ylabel(ylabel) # Return the figure associated with the heatmap return heatmap.get_figure()"},{"question":"# Custom Container Class **Objective**: Implement a custom container class that emulates both mapping (dictionary-like) and sequence (list-like) behaviors using Python’s special method names. Specifications: 1. **Class Name**: `CustomContainer` 2. **Initialization**: The class should be initialized with an optional dictionary. This dictionary will serve both sequence and mapping purposes. ```python class CustomContainer: def __init__(self, initial_dict=None): # Implementation here ``` 3. **Item access and assignment**: - Implement `__getitem__` to allow dictionary-like key access and list-like index access. - Implement `__setitem__` to allow setting values via both keys and indexes. - Implement `__delitem__` to allow deletion of items via both keys and indexes. 4. **Length and containment**: - Implement `__len__` to return the count of items in the container. - Implement `__contains__` to enable the use of the `in` operator for both keys and values. 5. **Methods for sequence behavior**: - Implement `append` method to add elements to the container as a list. - Implement `extend` method to add multiple elements to the container as a list. 6. **String representation**: - Implement `__str__` and `__repr__` for string representations. Example usage: ```python cc = CustomContainer({\'a\': 1, \'b\': 2}) cc[0] # Should behave like a list and output 1 cc[\'a\'] # Should behave like a dictionary and output 1 cc[0] = 10 # Should modify the list behavior, now cc[0] outputs 10 cc[\'c\'] = 3 # Should add a new key-value pair, cc[\'c\'] outputs 3 del cc[\'a\'] # Should remove the key \'a\' len(cc) # Should return the total count of items 10 in cc # Should check within values (should be True) \'a\' in cc # Should check within keys (should be False as key is deleted) cc.append(20) # Should act like a list append, adding 20 at the end cc.extend([30, 40]) # Should act like list extend, adding 30 and 40 at the end str(cc) # Should display the container as {\'c\': 3, 10, 20, 30, 40} ``` Constraints: - Ensure the operations are efficient and handle edge cases appropriately. - Assume all keys are hashable and values can be any type. - The class should handle mixed key/index access gracefully. Implement the `CustomContainer` class with the above specifications and behaviors.","solution":"class CustomContainer: def __init__(self, initial_dict=None): self.data = initial_dict if initial_dict else {} self.index_data = list(self.data.values()) def __getitem__(self, key): if isinstance(key, int): return self.index_data[key] else: return self.data[key] def __setitem__(self, key, value): if isinstance(key, int): self.index_data[key] = value # Update data for dictionary-like behavior synchronization dict_key = list(self.data.keys())[key] self.data[dict_key] = value else: self.data[key] = value self.index_data = list(self.data.values()) def __delitem__(self, key): if isinstance(key, int): dict_key = list(self.data.keys())[key] del self.data[dict_key] del self.index_data[key] else: del self.data[key] self.index_data = list(self.data.values()) def __len__(self): return len(self.data) def __contains__(self, item): return item in self.data or item in self.index_data def append(self, value): # Append value to index data self.index_data.append(value) # Find a unique key for dictionary mapping unique_key = f\\"auto_key_{len(self.data)}\\" self.data[unique_key] = value def extend(self, values): for value in values: self.append(value) def __str__(self): return str(self.data) def __repr__(self): return f\\"CustomContainer({self.data})\\""},{"question":"Coding Assessment Question # Context In this assessment, you are required to demonstrate your understanding of cross-validation techniques in scikit-learn by implementing and evaluating a machine learning model on a given dataset. You will be provided with the Iris dataset and your task will be to apply different cross-validation strategies to evaluate the performance of a Support Vector Machine (SVM) classifier. # Objective Write a Python function `evaluate_model_with_cv` that will: 1. Load the Iris dataset. 2. Split the dataset into features (`X`) and target labels (`y`). 3. Initialize a linear SVM classifier with `C=1` and `random_state=42`. 4. Evaluate the classifier using three different cross-validation techniques: - K-Fold Cross-Validation (with k=5) - Stratified K-Fold Cross-Validation (with k=5) - Time Series Split Cross-Validation (with n_splits=5) 5. Return a dictionary containing the mean accuracy and standard deviation of the accuracy for each cross-validation technique. # Input The function does not take any input parameters. # Output The function should return a dictionary with the following structure: ```python { \\"k_fold\\": { \\"mean_accuracy\\": float, # Mean accuracy from K-Fold Cross-Validation \\"std_accuracy\\": float # Standard deviation of accuracy from K-Fold Cross-Validation }, \\"stratified_k_fold\\": { \\"mean_accuracy\\": float, # Mean accuracy from Stratified K-Fold Cross-Validation \\"std_accuracy\\": float # Standard deviation of accuracy from Stratified K-Fold Cross-Validation }, \\"time_series_split\\": { \\"mean_accuracy\\": float, # Mean accuracy from Time Series Split Cross-Validation \\"std_accuracy\\": float # Standard deviation of accuracy from Time Series Split Cross-Validation } } ``` # Constraints - Use `sklearn.datasets.load_iris` to load the dataset. - Use `sklearn.svm.SVC` to initialize the SVM classifier. - Use `sklearn.model_selection.KFold`, `sklearn.model_selection.StratifiedKFold`, and `sklearn.model_selection.TimeSeriesSplit` for the cross-validation strategies. - Use `sklearn.model_selection.cross_val_score` to compute the cross-validation scores. # Implementation ```python def evaluate_model_with_cv(): # Load the Iris dataset from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit, cross_val_score X, y = datasets.load_iris(return_X_y=True) svm_clf = SVC(kernel=\'linear\', C=1, random_state=42) # K-Fold Cross-Validation kf = KFold(n_splits=5) kf_scores = cross_val_score(svm_clf, X, y, cv=kf) kf_mean_accuracy = kf_scores.mean() kf_std_accuracy = kf_scores.std() # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5) skf_scores = cross_val_score(svm_clf, X, y, cv=skf) skf_mean_accuracy = skf_scores.mean() skf_std_accuracy = skf_scores.std() # Time Series Split Cross-Validation tscv = TimeSeriesSplit(n_splits=5) tscv_scores = cross_val_score(svm_clf, X, y, cv=tscv) tscv_mean_accuracy = tscv_scores.mean() tscv_std_accuracy = tscv_scores.std() return { \\"k_fold\\": { \\"mean_accuracy\\": kf_mean_accuracy, \\"std_accuracy\\": kf_std_accuracy }, \\"stratified_k_fold\\": { \\"mean_accuracy\\": skf_mean_accuracy, \\"std_accuracy\\": skf_std_accuracy }, \\"time_series_split\\": { \\"mean_accuracy\\": tscv_mean_accuracy, \\"std_accuracy\\": tscv_std_accuracy } } # Example call to the function (Uncomment to run) # print(evaluate_model_with_cv()) ``` Evaluate your model\'s performance using different cross-validation techniques and compare the obtained accuracies.","solution":"def evaluate_model_with_cv(): # Load the Iris dataset from sklearn import datasets from sklearn.svm import SVC from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit, cross_val_score # Load the dataset X, y = datasets.load_iris(return_X_y=True) # Initialize the SVM classifier svm_clf = SVC(kernel=\'linear\', C=1, random_state=42) # K-Fold Cross-Validation kf = KFold(n_splits=5) kf_scores = cross_val_score(svm_clf, X, y, cv=kf) kf_mean_accuracy = kf_scores.mean() kf_std_accuracy = kf_scores.std() # Stratified K-Fold Cross-Validation skf = StratifiedKFold(n_splits=5) skf_scores = cross_val_score(svm_clf, X, y, cv=skf) skf_mean_accuracy = skf_scores.mean() skf_std_accuracy = skf_scores.std() # Time Series Split Cross-Validation tscv = TimeSeriesSplit(n_splits=5) tscv_scores = cross_val_score(svm_clf, X, y, cv=tscv) tscv_mean_accuracy = tscv_scores.mean() tscv_std_accuracy = tscv_scores.std() return { \\"k_fold\\": { \\"mean_accuracy\\": kf_mean_accuracy, \\"std_accuracy\\": kf_std_accuracy }, \\"stratified_k_fold\\": { \\"mean_accuracy\\": skf_mean_accuracy, \\"std_accuracy\\": skf_std_accuracy }, \\"time_series_split\\": { \\"mean_accuracy\\": tscv_mean_accuracy, \\"std_accuracy\\": tscv_std_accuracy } }"},{"question":"# Time Series Data Analysis with Pandas Objective: To assess your understanding and proficiency in handling time series data using pandas, you are tasked with implementing several functions that perform various operations on a time series dataset. Problem Statement: You are provided with data representing temperature readings taken every 15 minutes over a month. Implement the following functions: 1. **generate_time_series():** - **Input:** None - **Output:** A pandas DataFrame with two columns: `timestamp` and `temperature`. - `timestamp` should be a sequence of datetime values starting from `2023-01-01 00:00:00` to `2023-01-31 23:45:00` with a frequency of 15 minutes. - `temperature` should be randomly generated float values between -10 and 25 degrees Celsius. 2. **localize_timezone(df):** - **Input:** A pandas DataFrame with a `timestamp` column. - **Output:** The same DataFrame with the `timestamp` column localized to the `UTC` timezone. 3. **convert_timezone(df, target_tz):** - **Input:** A pandas DataFrame with a `timestamp` column localized to `UTC`, and a string `target_tz` representing the target timezone (e.g., `\\"US/Pacific\\"`). - **Output:** The same DataFrame with the `timestamp` column converted to the `target_tz` timezone. 4. **resample_data(df, freq):** - **Input:** A pandas DataFrame with a `timestamp` column, and a string `freq` representing the new frequency for resampling (e.g., `H` for hourly). - **Output:** A new DataFrame resampled to the new frequency, with the `temperature` column averaged over each resampling period. 5. **aggregate_data(df, start_date, end_date):** - **Input:** A pandas DataFrame with a `timestamp` column, and two strings `start_date` and `end_date` representing the date range for aggregation (in the format `YYYY-MM-DD`). - **Output:** A DataFrame aggregated daily over the specified date range, containing the columns: - `date`: the date of each day. - `max_temperature`: the maximum temperature of that day. - `min_temperature`: the minimum temperature of that day. - `mean_temperature`: the mean temperature of that day. Constraints: - Ensure that your solutions handle missing or erroneous inputs gracefully. - Maintain efficiency in terms of computation and memory usage, especially when dealing with large datasets. Example Usage: ```python # Assuming the pandas library is imported as pd import pandas as pd # 1. Generate the time series data df = generate_time_series() # 2. Localize the timezone to UTC df_utc = localize_timezone(df) # 3. Convert the timezone to \'US/Pacific\' df_pacific = convert_timezone(df_utc, \'US/Pacific\') # 4. Resample the data to hourly frequency df_hourly = resample_data(df_pacific, \'H\') # 5. Aggregate the data from 2023-01-01 to 2023-01-31 df_aggregated = aggregate_data(df_hourly, \'2023-01-01\', \'2023-01-31\') print(df_aggregated) ``` Submission: Submit your implementation of the five functions in a Python file (e.g., `time_series_analysis.py`). Ensure your code is well-documented, clean, and follows best practices for readability and efficiency.","solution":"import pandas as pd import numpy as np def generate_time_series(): Generate a time series DataFrame with timestamps every 15 minutes and random temperature values. date_range = pd.date_range(start=\'2023-01-01\', end=\'2023-01-31 23:45:00\', freq=\'15T\') temperature = np.random.uniform(-10, 25, size=len(date_range)) df = pd.DataFrame({\'timestamp\': date_range, \'temperature\': temperature}) return df def localize_timezone(df): Localize the \'timestamp\' column of the DataFrame to UTC timezone. df[\'timestamp\'] = df[\'timestamp\'].dt.tz_localize(\'UTC\') return df def convert_timezone(df, target_tz): Convert the timezone of the \'timestamp\' column to the target timezone. df[\'timestamp\'] = df[\'timestamp\'].dt.tz_convert(target_tz) return df def resample_data(df, freq): Resample the DataFrame to the given frequency and average the \'temperature\' values. df_resampled = df.resample(freq, on=\'timestamp\').mean().reset_index() return df_resampled def aggregate_data(df, start_date, end_date): Aggregate the temperature data daily over the specified date range. mask = (df[\'timestamp\'] >= start_date) & (df[\'timestamp\'] <= end_date) df_filtered = df.loc[mask] df_aggregated = df_filtered.resample(\'D\', on=\'timestamp\').agg({ \'temperature\': [\'max\', \'min\', \'mean\'] }).reset_index() df_aggregated.columns = [\'date\', \'max_temperature\', \'min_temperature\', \'mean_temperature\'] return df_aggregated"},{"question":"# Question: Customizing a Line Plot with Seaborn You are provided with a dataset containing monthly airline passenger data across 10 years. Your task is to create customized visualizations using seaborn\'s `lineplot` function to interpret the data effectively. Dataset The dataset has the following structure: ``` year month passengers 0 1949 January 112 1 1949 February 118 2 1949 March 132 ... ``` You can load the dataset using seaborn: ```python import seaborn as sns flights = sns.load_dataset(\\"flights\\") ``` Tasks 1. Write a function `plot_yearly_passenger_trends(data)` that takes the `flights` dataset and: - Plots the total number of passengers per year. - Adds error bars representing the 95% confidence interval. 2. Write a function `plot_monthly_passenger_trends(data)` that takes the `flights` dataset and: - Plots the monthly passenger trends for each year. - Uses different colors for each month. - Uses markers to differentiate between the months. 3. Write a function `plot_month_passenger_comparison(data, month1, month2)` that takes the `flights` dataset and two month names (e.g., \'January\', \'June\') and: - Plots the number of passengers across the years for the two specified months. - Use different styles (lines vs. markers) to differentiate the months. - Display a legend to indicate which month corresponds to which style. Requirements - Use seaborn\'s `lineplot` and other relevant functions to create the plots. - Customize the plots to make them informative and visually appealing. - Ensure the function names and parameters match the descriptions exactly. Expected Output - Each function should display the respective plots. - Each function should _not_ return any values. Example Usage ```python # Load dataset flights = sns.load_dataset(\\"flights\\") # Plot yearly passenger trends plot_yearly_passenger_trends(flights) # Plot monthly passenger trends plot_monthly_passenger_trends(flights) # Compare passenger trends for January and June plot_month_passenger_comparison(flights, \\"January\\", \\"June\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_yearly_passenger_trends(data): Plots the total number of passengers per year with error bars representing the 95% confidence interval. yearly_data = data.groupby(\'year\').agg({\'passengers\': \'sum\'}).reset_index() sns.lineplot(x=\'year\', y=\'passengers\', data=yearly_data, ci=95) plt.title(\\"Yearly Passenger Trends\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.show() def plot_monthly_passenger_trends(data): Plots the monthly passenger trends for each year using different colors and markers for each month. sns.lineplot(x=\'year\', y=\'passengers\', hue=\'month\', style=\'month\', data=data, markers=True, dashes=False) plt.title(\\"Monthly Passenger Trends\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show() def plot_month_passenger_comparison(data, month1, month2): Plots the number of passengers across the years for the two specified months using different styles. month_data = data[data[\'month\'].isin([month1, month2])] sns.lineplot(x=\'year\', y=\'passengers\', hue=\'month\', style=\'month\', markers=True, dashes=False, data=month_data) plt.title(f\\"Passenger Comparison: {month1} vs {month2}\\") plt.xlabel(\\"Year\\") plt.ylabel(\\"Number of Passengers\\") plt.legend(bbox_to_anchor=(1.05, 1), loc=\'upper left\') plt.show()"},{"question":"**Objective:** Your task is to implement a memory management utility using the `gc` module to analyze and manage garbage collection in a Python program. **Problem Statement:** You need to implement a function `analyze_garbage_collection(threshold0, threshold1, threshold2)` that performs the following operations: 1. Disables automatic garbage collection. 2. Sets new garbage collection thresholds. 3. Forces a garbage collection cycle for the specified generation and retrieves the number of objects collected and uncollectable. 4. Enables debugging flags to collect detailed statistics of the process. 5. Enables automatic garbage collection again. 6. Returns a dictionary containing the information obtained from the garbage collection process. **Function Signature:** ```python def analyze_garbage_collection(threshold0: int, threshold1: int, threshold2: int) -> dict: pass ``` **Input:** * `threshold0` - An integer setting the threshold for generation 0. * `threshold1` - An integer setting the threshold for generation 1. * `threshold2` - An integer setting the threshold for generation 2. **Output:** * A dictionary with: - `\\"collected_gen0\\"`: Number of objects collected from generation 0. - `\\"collected_gen1\\"`: Number of objects collected from generation 1. - `\\"collected_gen2\\"`: Number of objects collected from generation 2. - `\\"uncollectable\\"`: Number of uncollectable objects found. - `\\"gc_stats\\"`: Detailed statistics of the garbage collections performed. **Constraints:** * `threshold0`, `threshold1`, `threshold2` are positive integers. **Example Usage:** ```python result = analyze_garbage_collection(700, 10, 5) print(result) # Output Example: # { # \\"collected_gen0\\": 150, # \\"collected_gen1\\": 45, # \\"collected_gen2\\": 3, # \\"uncollectable\\": 0, # \\"gc_stats\\": [ # {\\"collections\\": 10, \\"collected\\": 150, \\"uncollectable\\": 0}, # {\\"collections\\": 7, \\"collected\\": 45, \\"uncollectable\\": 0}, # {\\"collections\\": 5, \\"collected\\": 3, \\"uncollectable\\": 0}, # ] # } ``` Please ensure that your function sets and then restores the garbage collection status (enabled/disabled) correctly. Utilize appropriate functions from the `gc` module to gather the necessary information. **Hints:** - Use `gc.enable()`, `gc.disable()`, and `gc.collect()` functions. - Use `gc.set_debug(gc.DEBUG_STATS)` and `gc.get_stats()` functions for gathering statistics. - Use `gc.set_threshold(threshold0, threshold1, threshold2)` to configure collection thresholds.","solution":"import gc def analyze_garbage_collection(threshold0: int, threshold1: int, threshold2: int) -> dict: Analyzes garbage collection activity and returns statistics. # Step 1: Disable automatic garbage collection gc.disable() # Step 2: Set new garbage collection thresholds previous_thresholds = gc.get_threshold() gc.set_threshold(threshold0, threshold1, threshold2) # Step 3: Force garbage collection cycles for each generation collected_gen0 = gc.collect(0) collected_gen1 = gc.collect(1) collected_gen2 = gc.collect(2) # Count uncollectable objects uncollectable = gc.collect() # Step 4: Enable debugging flags and collect detailed statistics gc.set_debug(gc.DEBUG_STATS) gc_stats = gc.get_stats() # Step 5: Enable automatic garbage collection again gc.enable() # Reset the garbage collection thresholds to their original values gc.set_threshold(*previous_thresholds) # Step 6: Return required statistics return { \\"collected_gen0\\": collected_gen0, \\"collected_gen1\\": collected_gen1, \\"collected_gen2\\": collected_gen2, \\"uncollectable\\": uncollectable, \\"gc_stats\\": gc_stats }"},{"question":"# Asynchronous Programming Assessment In this task, you will demonstrate your understanding of asynchronous programming using Python\'s `asyncio` package. You are required to implement a simple server-client application to simulate a basic chat system. Requirements 1. Implement an `AsyncChatServer` class that: - Listens for incoming connections on a specified host and port. - Manages multiple clients concurrently. - Broadcasts messages received from any client to all connected clients. - Uses asyncio logging to log connection and message events. 2. Implement a `start_server` function that: - Configures and starts an instance of `AsyncChatServer`. - Runs the event loop in debug mode. 3. Implement an `AsyncChatClient` class that: - Connects to a specified server. - Allows sending messages to the server. - Receives and displays messages from the server. - Uses asyncio logging to log connection and message events. 4. Write a short Python script demonstrating: - Starting the server. - Connecting multiple clients to the server. - Clients sending and receiving messages. Constraints - The server and clients should be able to handle continuous connections, disconnections, and message transmissions without blocking. - Ensure that proper exceptions are logged and appropriately handled. - The server should run indefinitely until manually stopped. - The code should use `asyncio` features, such as `asyncio.create_task`, `loop.call_soon_threadsafe`, and `asyncio.run_in_executor` for managing tasks and blocking operations. Example Usage ```python import asyncio # Example of server usage async def main(): server = start_server(\'localhost\', 8888) await server # Run the server indefinitely asyncio.run(main()) # Example of client usage async def main(): client = AsyncChatClient(\'localhost\', 8888) await client.connect() await client.send_message(\\"Hello World!\\") await client.receive_messages() # Run until disconnected asyncio.run(main()) ``` Expected Output - The server prints log messages for new connections, disconnections, and messages received. - The clients print log messages for connection events and display received messages. - Messages sent from one client should be broadcast to all connected clients. You should submit the following: - Full source code for `AsyncChatServer`, `start_server`, and `AsyncChatClient` classes/functions. - A script demonstrating the server and client interactions, as described in the example usage.","solution":"import asyncio import logging logging.basicConfig(level=logging.DEBUG) class AsyncChatServer: def __init__(self, host, port): self.host = host self.port = port self.clients = [] async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') logging.info(f\\"Connection from {addr}\\") self.clients.append(writer) while True: data = await reader.read(100) if not data: logging.info(f\\"Connection closed by {addr}\\") self.clients.remove(writer) writer.close() await writer.wait_closed() break message = data.decode() logging.info(f\\"Received message from {addr}: {message}\\") await self.broadcast_message(message, writer) async def broadcast_message(self, message, writer): for client in self.clients: if client != writer: client.write(message.encode()) await client.drain() async def start(self): server = await asyncio.start_server(self.handle_client, self.host, self.port) async with server: await server.serve_forever() def start_server(host, port): server = AsyncChatServer(host, port) return server.start() class AsyncChatClient: def __init__(self, host, port): self.host = host self.port = port async def connect(self): self.reader, self.writer = await asyncio.open_connection(self.host, self.port) logging.info(f\\"Client connected to {self.host}:{self.port}\\") async def send_message(self, message): logging.info(f\\"Sending message: {message}\\") self.writer.write(message.encode()) await self.writer.drain() async def receive_messages(self): while True: data = await self.reader.read(100) if not data: logging.info(\\"Disconnected from server\\") break message = data.decode() logging.info(f\\"Received message: {message}\\") print(message) # Example script demonstrating the server async def main(): server_task = asyncio.create_task(start_server(\'127.0.0.1\', 8888)) await server_task # Run the server indefinitely if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Custom Python Object Type Implementation** **Objective**: You are required to implement a custom Python type in a C extension module using the Python 3.10 C API. Your task involves creating a new type that demonstrates an understanding of `PyObject`, `PyVarObject`, and related structures and functions as outlined in the documentation. **Requirements**: 1. **Create a Custom Type**: - Define a new type named `CustomObject` which extends `PyObject`. - This type should have two attributes: `name` (a string) and `value` (an integer). - Implement the necessary type initialization and deallocation functions. 2. **Methods**: - Implement a method `set_name` to set the `name` attribute. - Implement a method `get_name` to get the `name` attribute. 3. **Functions and Macros**: - Utilize the `PyObject_HEAD` macro for the base part of your type. - Use the macros `Py_REFCNT`, `Py_TYPE`, `Py_SIZE`, `Py_SET_REFCNT`, and `Py_SET_TYPE` as needed in your implementation. 4. **Type Object Initialization**: - Initialize the type object with `PyTypeObject` structure. - Set up the method definitions using `PyMethodDef`. **Input and Output**: - Input: No specific input format. Functions within your type will be tested with appropriate input values in Python scripts. - Output: No specific output format. Output will be verified through function return values and state of the object attributes. **Constraints**: - Ensure proper reference counting to avoid memory leaks. - Handle error cases gracefully (e.g., invalid inputs). **Performance Requirements**: - The implementation should be efficient with respect to time and memory usage given the constraints of typical usage scenarios in Python. **Example Usage in C**: ```c #include <Python.h> typedef struct { PyObject_HEAD PyObject *name; int value; } CustomObject; static void CustomObject_dealloc(CustomObject *self) { Py_XDECREF(self->name); Py_TYPE(self)->tp_free((PyObject *)self); } static PyObject * CustomObject_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { CustomObject *self; self = (CustomObject *)type->tp_alloc(type, 0); if (self != NULL) { self->name = PyUnicode_FromString(\\"\\"); if (self->name == NULL) { Py_DECREF(self); return NULL; } self->value = 0; } return (PyObject *)self; } static PyObject * CustomObject_get_name(CustomObject *self, PyObject *Py_UNUSED(ignored)) { Py_INCREF(self->name); return self->name; } static PyObject * CustomObject_set_name(CustomObject *self, PyObject *args) { PyObject *temp, *name; if (!PyArg_ParseTuple(args, \\"U\\", &name)) return NULL; temp = self->name; Py_INCREF(name); self->name = name; Py_XDECREF(temp); Py_RETURN_NONE; } static PyMethodDef CustomObject_methods[] = { {\\"get_name\\", (PyCFunction)CustomObject_get_name, METH_NOARGS, \\"Get the name attribute\\"}, {\\"set_name\\", (PyCFunction)CustomObject_set_name, METH_VARARGS, \\"Set the name attribute\\"}, {NULL} /* Sentinel */ }; static PyTypeObject CustomObjectType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"custom.CustomObject\\", .tp_doc = \\"Custom object\\", .tp_basicsize = sizeof(CustomObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT, .tp_new = CustomObject_new, .tp_dealloc = (destructor)CustomObject_dealloc, .tp_methods = CustomObject_methods, }; static PyModuleDef custommodule = { PyModuleDef_HEAD_INIT, .m_name = \\"custom\\", .m_doc = \\"Example module that creates an extension type.\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_custom(void) { PyObject *m; if (PyType_Ready(&CustomObjectType) < 0) return NULL; m = PyModule_Create(&custommodule); if (m == NULL) return NULL; Py_INCREF(&CustomObjectType); if (PyModule_AddObject(m, \\"CustomObject\\", (PyObject *)&CustomObjectType) < 0) { Py_DECREF(&CustomObjectType); Py_DECREF(m); return NULL; } return m; } ``` **Instructions**: 1. Create the necessary `.c` file for your implementation. 2. Build and install your module using `setup.py` or any preferred method. 3. Test your module by importing it in Python and creating instances of `CustomObject`, then calling the methods to set and get the attributes.","solution":"def add(a, b): Returns the sum of a and b. return a + b"},{"question":"Objective Write a function `remove_duplicates_and_prevent_new_ones` that takes as input a pandas DataFrame and performs the following operations: 1. Detects duplicate row labels (index) in the DataFrame. 2. Removes duplicate rows by keeping the first occurrence. 3. Sets the attribute to disallow duplicate row labels for any future operations. 4. Returns the processed DataFrame. Input Format - A pandas DataFrame `df` with potentially duplicate row labels. Output Format - A pandas DataFrame `df` with duplicates removed and with the attribute set to disallow duplicate row labels. Constraints - The input DataFrame can have up to 1,000,000 rows and 100 columns. - The indices may contain strings, integers, or a combination of both. Performance Requirements - Operations should complete within reasonable time limits specified by the constraints. Example ```python import pandas as pd df = pd.DataFrame({\'A\': [1, 2, 3, 4], \'B\': [5, 6, 7, 8]}, index=[\'x\', \'y\', \'x\', \'z\']) result = remove_duplicates_and_prevent_new_ones(df) print(result) print(result.flags.allows_duplicate_labels) ``` Expected Output: ``` A B x 1 5 y 2 6 z 4 8 False ``` Function Signature ```python def remove_duplicates_and_prevent_new_ones(df: pd.DataFrame) -> pd.DataFrame: pass ```","solution":"import pandas as pd def remove_duplicates_and_prevent_new_ones(df: pd.DataFrame) -> pd.DataFrame: Removes duplicate row labels by keeping the first occurrence and sets the attribute to disallow duplicate row labels for any future operations. Parameters: df (pd.DataFrame): The input DataFrame with potentially duplicate row labels. Returns: pd.DataFrame: The processed DataFrame with duplicates removed and the attribute set to disallow duplicate row labels. # Remove duplicate indices, keeping the first occurrence df = df[~df.index.duplicated(keep=\'first\')] # Set the attribute to disallow duplicate row labels df.flags.allows_duplicate_labels = False return df"},{"question":"Objective: Write a Python program that performs the following operations on a Unix-based system: 1. Create a large file using the `posix` system calls. 2. Verify and display user and group information for the current process using the `pwd` and `grp` modules. 3. Handle terminal control using `termios` and `tty` by configuring terminal attributes for a custom input prompt. 4. Use `fcntl` to lock a portion of a file. 5. Record and demonstrate resource usage information using the `resource` module. Requirements: 1. Implement the function `create_large_file(filename: str, size_in_mb: int) -> None`: - This function should create a file of specified size in megabytes using low-level OS-specific calls. 2. Implement the function `get_user_group_info() -> Tuple[str, str]`: - This function should return a tuple containing the username and group name of the current process. 3. Implement the function `configure_terminal() -> None`: - This function should configure the terminal to disable echo and enable non-blocking mode. Use `termios` and `tty` for this purpose. 4. Implement the function `lock_file_region(filename: str, offset: int, length: int) -> None`: - This function should lock a specific region of a file, starting from `offset` and spanning `length` bytes using `fcntl`. 5. Implement the function `record_resource_usage(logfile: str) -> None`: - This function should log the current resource usage (CPU time, memory usage) to a specified log file. Constraints: - The solutions should use Unix-specific modules as documented. Make sure your code is compliant with POSIX. - Assume the system running the code has sufficient permissions to perform these operations. - Ensure proper exception handling for robust error management. Performance Requirements: - The large file creation should be efficient, minimizing unnecessary system calls. - User and group information retrieval should be optimized for minimal overhead. - Terminal configuration should not interfere with normal terminal operations post-execution. - File locking should be precise and avoid deadlocks. - Resource usage logging should be detailed yet concise to avoid excessive log sizes. Implement the functions as described and demonstrate their usage in a main program. The program should showcase a real-world scenario combining all the above functionalities in a cohesive manner.","solution":"import os import pwd import grp import termios import tty import fcntl import resource from typing import Tuple def create_large_file(filename: str, size_in_mb: int) -> None: Create a file of the specified size in megabytes. size_in_bytes = size_in_mb * 1024 * 1024 with open(filename, \'wb\') as f: f.truncate(size_in_bytes) def get_user_group_info() -> Tuple[str, str]: Retrieve and return the username and group name of the current process. uid = os.getuid() gid = os.getgid() user_info = pwd.getpwuid(uid) group_info = grp.getgrgid(gid) return user_info.pw_name, group_info.gr_name def configure_terminal() -> None: Configure the terminal to disable echo and enable non-blocking mode. fd = sys.stdin.fileno() old_settings = termios.tcgetattr(fd) tty.setraw(fd) new_settings = termios.tcgetattr(fd) new_settings[3] &= ~termios.ECHO # Disable echo termios.tcsetattr(fd, termios.TCSADRAIN, new_settings) # Restoring the settings after configuration termios.tcsetattr(fd, termios.TCSADRAIN, old_settings) def lock_file_region(filename: str, offset: int, length: int) -> None: Lock a specific region of a file. with open(filename, \'r+\') as f: fcntl.lockf(f, fcntl.LOCK_EX, length, offset) def record_resource_usage(logfile: str) -> None: Log the current resource usage to a specified log file. usage = resource.getrusage(resource.RUSAGE_SELF) with open(logfile, \'w\') as log: log.write(f\\"User time: {usage.ru_utime}n\\") log.write(f\\"System time: {usage.ru_stime}n\\") log.write(f\\"Max RSS: {usage.ru_maxrss}n\\") log.write(f\\"Shared memory size: {usage.ru_ixrss}n\\") log.write(f\\"Unshared memory size: {usage.ru_idrss}n\\") log.write(f\\"Unshared stack size: {usage.ru_isrss}n\\") log.write(f\\"Page reclaims: {usage.ru_minflt}n\\") log.write(f\\"Page faults: {usage.ru_majflt}n\\") log.write(f\\"Swaps: {usage.ru_nswap}n\\") log.write(f\\"Block input operations: {usage.ru_inblock}n\\") log.write(f\\"Block output operations: {usage.ru_oublock}n\\")"},{"question":"Problem Statement You are required to create a simple command-line authentication system using the `getpass` module. This system should prompt the user to enter their username and password. The system should verify the entered username and password against a predefined dictionary of usernames and passwords. If the username and password match the ones stored in this dictionary, the system should print a welcome message; otherwise, it should print an error message. Requirements 1. Implement a function `authenticate_user()`. 2. The function should first retrieve the username using `getpass.getuser()`. 3. Then it should prompt the user for their password using `getpass.getpass()`, without echoing the input. 4. You should define a dictionary `user_credentials` within your function that contains at least three username-password pairs for testing purposes. 5. Verify if the entered username (from `getpass.getuser()`) and the password match any pair in the `user_credentials` dictionary. 6. If the authentication is successful, print `\\"Welcome, [username]!\\"`. 7. If the authentication fails, print `\\"Authentication failed. Please try again.\\"`. Constraints and Notes - Ensure that the password input is not echoed. - Consider edge cases such as empty inputs and different cases of usernames. - Assume passwords are stored in plain text for simplicity (though this is not recommended for real-world applications). Input and Output - The function `authenticate_user()` does not take any parameters and does not return any value. It should handle all inputs and outputs within the function. Example ```python def authenticate_user(): Function to authenticate a user using the getpass module. import getpass # A predefined dictionary of username-password pairs user_credentials = { \'user1\': \'password123\', \'user2\': \'mysecurepassword\', \'admin\': \'adminpass\' } # Retrieve the current user\'s username username = getpass.getuser() # Prompt the user for their password password = getpass.getpass(prompt=\'Enter your password: \') if username in user_credentials and user_credentials[username] == password: print(f\\"Welcome, {username}!\\") else: print(\\"Authentication failed. Please try again.\\") # Example usage: authenticate_user() ``` When executing the function: - If the username is \\"user1\\" and the password is \\"password123\\", the output will be: `Welcome, user1!` - If the username or password is incorrect, the output will be: `Authentication failed. Please try again.` Ensure you test the function in an appropriate environment that supports `getpass`.","solution":"def authenticate_user(): Function to authenticate a user using the getpass module. import getpass # A predefined dictionary of username-password pairs user_credentials = { \'user1\': \'password123\', \'user2\': \'mysecurepassword\', \'admin\': \'adminpass\' } # Retrieve the current user\'s username username = getpass.getuser() # Prompt the user for their password password = getpass.getpass(prompt=\'Enter your password: \') if username in user_credentials and user_credentials[username] == password: print(f\\"Welcome, {username}!\\") else: print(\\"Authentication failed. Please try again.\\")"},{"question":"# **Question: Implement a File Encoder-Decoder using `uu` and `base64` libraries** In this task, you are required to implement functions to encode and decode files using both the `uu` and `base64` modules. The program should provide the functionality to convert a given file to another file using uuencoding and base64 encoding methods and then decode them back to their original form, ensuring that the content remains unchanged and correctly handled. **Requirements**: 1. Implement the following functions: 1. `uu_encode(infile: str, outfile: str, name: str = None, mode: int = None, backtick: bool = False) -> None` 2. `uu_decode(infile: str, outfile: str = None, mode: int = None, quiet: bool = False) -> None` 3. `base64_encode(infile: str, outfile: str) -> None` 4. `base64_decode(infile: str, outfile: str) -> None` 2. Handle exceptions appropriately and include meaningful error messages. 3. Validate that the original file content and the final decoded content are identical by comparing file content hashes. 4. Optimize file reading/writing to handle large files efficiently. **Input/Output Formats**: - `uu_encode(infile: str, outfile: str, name: str = None, mode: int = None, backtick: bool = False) -> None` - `infile`: Path to the input file to be uuencoded. - `outfile`: Path to the output file to store uuencoded content. - `name`: Optional. The name to be used in uuencoded file headers. - `mode`: Optional. The permission mode for the encoded output. - `backtick`: Optional. Whether to use backticks for representation of zeros. - `uu_decode(infile: str, outfile: str = None, mode: int = None, quiet: bool = False) -> None` - `infile`: Path to the uuencoded input file. - `outfile`: Optional. Path to the output file to store the decoded content. - `mode`: Optional. Permission mode for the decoded output file. - `quiet`: Optional. Suppress decoding warnings. - `base64_encode(infile: str, outfile: str) -> None` - `infile`: Path to the input file to be base64 encoded. - `outfile`: Path to the output file to store base64 encoded content. - `base64_decode(infile: str, outfile: str) -> None` - `infile`: Path to the base64 encoded input file. - `outfile`: Path to the output file to store the decoded content. # **Constraints**: - The functions should raise appropriate exceptions for invalid inputs, such as non-existent files or unsupported file formats. - Ensure efficient memory usage when handling large files. - Perform thorough validation to ensure file content integrity after encoding and decoding processes. # **Performance Requirements**: - The functions should be able to handle large files (up to several GB) without excessive memory consumption. - All file operations should be optimized for speed and efficiency using buffered reading and writing methods. ```python import uu import base64 import hashlib class FileEncoderDecoder: @staticmethod def uu_encode(infile: str, outfile: str, name: str = None, mode: int = None, backtick: bool = False) -> None: Encode a file using uuencoding. try: with open(infile, \'rb\') as in_f, open(outfile, \'wb\') as out_f: uu.encode(in_f, out_f, name, mode, backtick=backtick) except Exception as e: raise RuntimeError(f\\"Failed to uuencode file: {str(e)}\\") @staticmethod def uu_decode(infile: str, outfile: str = None, mode: int = None, quiet: bool = False) -> None: Decode a uuencoded file. try: with open(infile, \'rb\') as in_f: with open(outfile, \'wb\') as out_f: uu.decode(in_f, out_f, mode, quiet=quiet) except Exception as e: raise RuntimeError(f\\"Failed to uudecode file: {str(e)}\\") @staticmethod def base64_encode(infile: str, outfile: str) -> None: Encode a file using base64 encoding. try: with open(infile, \'rb\') as in_f, open(outfile, \'wb\') as out_f: base64.encode(in_f, out_f) except Exception as e: raise RuntimeError(f\\"Failed to base64 encode file: {str(e)}\\") @staticmethod def base64_decode(infile: str, outfile: str) -> None: Decode a base64 encoded file. try: with open(infile, \'rb\') as in_f, open(outfile, \'wb\') as out_f: base64.decode(in_f, out_f) except Exception as e: raise RuntimeError(f\\"Failed to base64 decode file: {str(e)}\\") @staticmethod def file_hash(filepath: str) -> str: Compute the hash of a file\'s content. hash_func = hashlib.sha256() with open(filepath, \'rb\') as f: while chunk := f.read(8192): hash_func.update(chunk) return hash_func.hexdigest() # Example usage: # Encode and then decode a file using uu and base64 methods, # ensuring the content hashes match after completing both round-trips. if __name__ == \\"__main__\\": infile = \'example.txt\' uu_encoded = \'example.uu\' uu_decoded = \'example_uu.txt\' base64_encoded = \'example.b64\' base64_decoded = \'example_b64.txt\' # uuencode and uudecode FileEncoderDecoder.uu_encode(infile, uu_encoded) FileEncoderDecoder.uu_decode(uu_encoded, uu_decoded) # base64 encode and decode FileEncoderDecoder.base64_encode(infile, base64_encoded) FileEncoderDecoder.base64_decode(base64_encoded, base64_decoded) original_hash = FileEncoderDecoder.file_hash(infile) uu_decoded_hash = FileEncoderDecoder.file_hash(uu_decoded) base64_decoded_hash = FileEncoderDecoder.file_hash(base64_decoded) assert original_hash == uu_decoded_hash == base64_decoded_hash, \\"File content mismatch after encoding and decoding!\\" print(\\"File content integrity validated.\\") ```","solution":"import uu import base64 import hashlib class FileEncoderDecoder: @staticmethod def uu_encode(infile: str, outfile: str, name: str = None, mode: int = None, backtick: bool = False) -> None: Encode a file using uuencoding. try: with open(infile, \'rb\') as in_f, open(outfile, \'wb\') as out_f: uu.encode(in_f, out_f, name, mode, backtick=backtick) except Exception as e: raise RuntimeError(f\\"Failed to uuencode file: {str(e)}\\") @staticmethod def uu_decode(infile: str, outfile: str = None, mode: int = None, quiet: bool = False) -> None: Decode a uuencoded file. try: with open(infile, \'rb\') as in_f: with open(outfile, \'wb\') as out_f: uu.decode(in_f, out_f, mode, quiet=quiet) except Exception as e: raise RuntimeError(f\\"Failed to uudecode file: {str(e)}\\") @staticmethod def base64_encode(infile: str, outfile: str) -> None: Encode a file using base64 encoding. try: with open(infile, \'rb\') as in_f, open(outfile, \'wb\') as out_f: base64.encode(in_f, out_f) except Exception as e: raise RuntimeError(f\\"Failed to base64 encode file: {str(e)}\\") @staticmethod def base64_decode(infile: str, outfile: str) -> None: Decode a base64 encoded file. try: with open(infile, \'rb\') as in_f, open(outfile, \'wb\') as out_f: base64.decode(in_f, out_f) except Exception as e: raise RuntimeError(f\\"Failed to base64 decode file: {str(e)}\\") @staticmethod def file_hash(filepath: str) -> str: Compute the hash of a file\'s content. hash_func = hashlib.sha256() with open(filepath, \'rb\') as f: while chunk := f.read(8192): hash_func.update(chunk) return hash_func.hexdigest()"},{"question":"# Advanced Python Multiprocessing Task **Objective:** To assess the understanding of Python\'s `multiprocessing` package, including advanced concepts like process synchronization, inter-process communication, and use of shared memory. **Problem Statement:** You are required to build a multiprocessing-based simulation where multiple worker processes perform computations and share results. This task involves creating worker processes, synchronizing them using locks, and passing data between them using queues. **Requirements:** 1. **Tasks:** - Implement a function `compute_task` which takes an integer `x` and returns `x ** 2` after a delay of `1` second. - Implement a function `logger_task` to log the results computed by worker processes. 2. **Process Management:** - Create a function `create_workers` that spawns `n` worker processes to perform computations on a list of integers. - Use synchronization primitives such as `Lock` to prevent race conditions while logging results. - Use a `Queue` for worker processes to send results back to the main process. 3. **Shared State:** - Use shared memory to count the number of completed tasks from worker processes. 4. **Main Function:** - Create a main function that initializes shared memory and synchronization primitives. - Spawn `n` worker processes to execute `compute_task` and collect results using a `Queue`. - Start a separate logging process to log the results. - Ensure all processes terminate gracefully. **Function Specifications:** 1. **compute_task(x: int) -> int:** - Input: An integer `x`. - Output: The square of `x` after a delay of 1 second. 2. **logger_task(queue: multiprocessing.Queue, log_lock: multiprocessing.Lock):** - Input: A `Queue` object to receive results from worker processes, and a `Lock` object for synchronized logging. - Output: None. The function should log the results received in the queue. 3. **create_workers(task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue, completed_tasks: multiprocessing.Value, n: int):** - Input: A queue for tasks, a queue for results, a shared memory `Value` for counting completed tasks, and an integer `n` for the number of workers. - Output: A list of worker processes. 4. **main(n: int, tasks: List[int]):** - Input: An integer `n` representing the number of worker processes, and a list of tasks for the workers. - Output: None. The main function should coordinate the entire process. **Constraints:** - The code should work on both Unix and Windows platforms. - Ensure that shared memory is safely managed. - The logging process should log results in the order they are processed. **Example:** ```python if __name__ == \'__main__\': # Initialize with 4 worker processes and a list of tasks [1, 2, 3, 4, 5] main(4, [1, 2, 3, 4, 5]) ``` **Expected Output:** - Logs of squared values processed by worker processes. - Example log entries: ``` Result: 1 Result: 4 Result: 9 Result: 16 Result: 25 ``` **Submission:** - Submit the code implementing the above functions and demonstrating their usage in the main function as specified.","solution":"import multiprocessing import time from typing import List def compute_task(x: int) -> int: Computes the square of x after a delay of 1 second. time.sleep(1) return x ** 2 def logger_task(queue: multiprocessing.Queue, log_lock: multiprocessing.Lock): Logs the results from the queue. Uses lock to synchronize access. while True: item = queue.get() if item is None: # Stop signal break with log_lock: print(f\\"Result: {item}\\") def worker(task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue, completed_tasks: multiprocessing.Value): Worker process that fetches tasks from task_queue, computes result and puts it to result_queue. Updates the completed_tasks shared counter. while True: task = task_queue.get() if task is None: # Stop signal break result = compute_task(task) result_queue.put(result) with completed_tasks.get_lock(): completed_tasks.value += 1 def create_workers(task_queue: multiprocessing.Queue, result_queue: multiprocessing.Queue, completed_tasks: multiprocessing.Value, n: int) -> List[multiprocessing.Process]: Creates worker processes to process tasks from task_queue and put results to result_queue. workers = [] for _ in range(n): p = multiprocessing.Process(target=worker, args=(task_queue, result_queue, completed_tasks)) p.start() workers.append(p) return workers def main(n: int, tasks: List[int]): task_queue = multiprocessing.Queue() result_queue = multiprocessing.Queue() completed_tasks = multiprocessing.Value(\'i\', 0) log_lock = multiprocessing.Lock() # Creating and starting the logger process logger_process = multiprocessing.Process(target=logger_task, args=(result_queue, log_lock)) logger_process.start() # Creating worker processes workers = create_workers(task_queue, result_queue, completed_tasks, n) # Adding tasks to the task queue for task in tasks: task_queue.put(task) # Add stop signals to the task queue for each worker for _ in range(n): task_queue.put(None) # Waiting for all workers to finish for worker in workers: worker.join() # Send stop signal to the logger process result_queue.put(None) logger_process.join() print(f\\"Total completed tasks: {completed_tasks.value}\\") if __name__ == \'__main__\': # Example usage main(4, [1, 2, 3, 4, 5])"},{"question":"**Coding Assessment Question: Implementing a Custom Iterator in Python** In this assignment, you are required to implement a custom iterator class in Python that behaves similarly to how iterators are managed in the C code described above. This custom iterator should simulate a sequence of even numbers starting from 0 up to, but not including, a given maximum value. # Instructions 1. Implement a class called `EvenIterator`. 2. This class should: - Be initialized with a maximum limit (`max_value`), establishing the upper bound for the iteration. - Implement the iterator protocol, including `__iter__()` and `__next__()` methods. 3. The `__next__()` method should: - Return the next even number in the sequence each time it is called. - Raise a `StopIteration` exception when the sequence reaches the maximum value. 4. Ensure proper error handling, similar to the C code example provided. # Example ```python class EvenIterator: def __init__(self, max_value): self.max_value = max_value self.current = 0 def __iter__(self): return self def __next__(self): if self.current >= self.max_value: raise StopIteration result = self.current self.current += 2 return result # Example usage max_value = 10 even_iterator = EvenIterator(max_value) for even in even_iterator: print(even) # Output: 0, 2, 4, 6, 8 ``` # Requirements - Your implementation should follow the iterator protocol guidelines as seen in the documentation given. - Ensure proper handling of the sequence and correct iteration logic. - Handle any potential exceptions, providing meaningful feedback when something goes wrong. # Constraints - The `max_value` passed to `EvenIterator` will always be a non-negative integer. - The iterator should only handle even numbers starting from 0 up to the given maximum value. Implement this iterator class in Python and test it with various scenarios to ensure it works as expected.","solution":"class EvenIterator: def __init__(self, max_value): if not isinstance(max_value, int) or max_value < 0: raise ValueError(\\"max_value must be a non-negative integer\\") self.max_value = max_value self.current = 0 def __iter__(self): return self def __next__(self): if self.current >= self.max_value: raise StopIteration result = self.current self.current += 2 return result"},{"question":"# Seaborn Coding Assessment Objective Demonstrate your understanding of `seaborn` for visualizing statistical relationships. Task Using the Seaborn library, complete the following tasks to visualize the relationships in the given dataset. Input Format The function signature should be: ```python def visualize_relationships(data: pd.DataFrame) -> None: ... ``` Where: - `data` (pd.DataFrame): A pandas DataFrame containing the dataset. Dataset You will use the built-in Seaborn dataset `tips` which can be loaded with `sns.load_dataset(\\"tips\\")`. Requirements 1. **Scatter Plot**: - Plot a scatter plot showing the relationship between `total_bill` and `tip`. - Color the points based on the `day` of the week (`hue` parameter). - Use different marker styles for `time` (Lunch or Dinner). - Customize the color palette to a sequential palette of your choice. - Adjust the size of the points based on the `size` parameter. 2. **Line Plot**: - Plot a line plot showing the relationship between `total_bill` and `tip`. - Aggregate the data to show the mean `tip` for each `total_bill` value, and display the 95% confidence interval. - Split the data into separate lines based on the `day` of the week (`hue` parameter). - Use different dash styles for each `time`. 3. **Facets**: - Create a facet grid of scatter plots showing the relationship between `total_bill` and `tip`: - Facet by `time` on the columns. - Facet by `day` on the rows. - Use different colors for points based on whether the smoker is a smoker or not (`hue` parameter). Constraints - Ensure that all plots are properly labeled with titles, axis labels, and legends as necessary. - Handle the sizes and aspects of the plots to ensure readability. Output Format The function should not return anything. Instead, it should display the required plots. Example ```python import seaborn as sns import pandas as pd def visualize_relationships(data: pd.DataFrame) -> None: sns.set_theme(style=\\"darkgrid\\") # Scatter Plot sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", palette=\\"ch:r=-.5,l=.75\\", sizes=(20, 200) ).set(title=\\"Scatter Plot of Total Bill vs Tip\\") # Line Plot sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", kind=\\"line\\", hue=\\"day\\", style=\\"time\\", ci=\\"sd\\" ).set(title=\\"Line Plot of Total Bill vs Tip\\") # Facet Grid g = sns.FacetGrid( data=data, col=\\"time\\", row=\\"day\\", hue=\\"smoker\\", margin_titles=True ) g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\").add_legend() g.set_titles(col_template=\\"{col_name}\\", row_template=\\"{row_name}\\") # Load dataset and call function tips = sns.load_dataset(\\"tips\\") visualize_relationships(tips) ``` Run the above function with the `tips` dataset to visualize the relationships as specified.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def visualize_relationships(data: pd.DataFrame) -> None: sns.set_theme(style=\\"darkgrid\\") # Scatter Plot scatter_plot = sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", style=\\"time\\", palette=\\"viridis\\", size=\\"size\\", sizes=(20, 200), ) scatter_plot.set(title=\\"Scatter Plot of Total Bill vs Tip\\") # Line Plot line_plot = sns.relplot( data=data, x=\\"total_bill\\", y=\\"tip\\", kind=\\"line\\", hue=\\"day\\", style=\\"time\\", ci=\\"sd\\", dashes=True, ) line_plot.set(title=\\"Line Plot of Total Bill vs Tip\\") # Facet Grid g = sns.FacetGrid( data=data, col=\\"time\\", row=\\"day\\", hue=\\"smoker\\", margin_titles=True, palette=\\"coolwarm\\", ) g.map(sns.scatterplot, \\"total_bill\\", \\"tip\\").add_legend() g.set_titles(col_template=\\"{col_name} Time\\", row_template=\\"{row_name} Day\\") plt.show()"},{"question":"Coding Assessment Question: Sales Data Analysis # Objective You are provided with a dataset containing daily sales records. You need to write a function that performs various analyses on this dataset to provide insights. The dataset is stored in a CSV file and contains the following columns: - `date`: The date of the sale. - `product`: The name of the product sold. - `quantity`: The number of units sold. - `price`: The price per unit of the product. # Data Example ```plaintext date,product,quantity,price 2023-01-01,Product_A,5,10.0 2023-01-01,Product_B,3,20.0 2023-01-02,Product_A,7,10.0 2023-01-02,Product_C,1,30.0 2023-01-03,Product_B,2,20.0 ``` # Requirements 1. **Read the Dataset**: Implement a function to read the CSV file into a pandas DataFrame. 2. **Add Columns**: Compute and add the following columns to the DataFrame: - `total_sale`: The total sale amount (`quantity` * `price`) for each row. 3. **Group and Analyze**: Group the data by `product` and compute the following: - Total `quantity` sold for each product. - Total `revenue` for each product (sum of `total_sale`). 4. **Time-based Analysis**: - Calculate the total quantity sold per day. - Calculate the average daily revenue for the overall dataset. # Input and Output - **Input**: Path to the CSV file containing the sales data. - **Output**: Two DataFrames: 1. A DataFrame with grouped product data including total quantity sold and total revenue. 2. A DataFrame with daily total quantity sales and daily revenue. # Constraints - The data in the CSV file is clean and does not contain missing values. - The function should handle cases where the CSV file is large by utilizing efficient pandas operations. # Performance Requirements - The function should be efficient and capable of handling datasets with up to 1 million rows within a reasonable time frame. # Function Signature ```python import pandas as pd def analyze_sales_data(file_path: str) -> (pd.DataFrame, pd.DataFrame): Analyze sales data from a CSV file. Parameters: file_path (str): Path to the CSV file containing sales data. Returns: (pd.DataFrame, pd.DataFrame): Two DataFrames with grouped product data and daily analysis. pass ``` # Detailed Steps 1. **Read the Dataset**: Use `pd.read_csv()` to read the data from the file into a DataFrame. 2. **Add Columns**: - Compute the `total_sale` column using vectorized operations. 3. **Group and Analyze**: - Group by `product` and compute the total quantity sold and total revenue. 4. **Time-based Analysis**: - Resample the data by day to calculate the daily total quantity sold. - Calculate the daily revenue and compute the average daily revenue. # Example Usage ```python df_product, df_daily = analyze_sales_data(\\"path/to/sales_data.csv\\") print(df_product) print(df_daily) ```","solution":"import pandas as pd def analyze_sales_data(file_path: str) -> (pd.DataFrame, pd.DataFrame): Analyze sales data from a CSV file. Parameters: file_path (str): Path to the CSV file containing sales data. Returns: (pd.DataFrame, pd.DataFrame): Two DataFrames with grouped product data and daily analysis. # Step 1: Read the CSV file into a DataFrame df = pd.read_csv(file_path) # Step 2: Compute the total sale amount and add it as a new column df[\'total_sale\'] = df[\'quantity\'] * df[\'price\'] # Step 3: Group by product and compute total quantity sold and total revenue df_grouped_product = df.groupby(\'product\').agg( total_quantity_sold=(\'quantity\', \'sum\'), total_revenue=(\'total_sale\', \'sum\') ).reset_index() # Step 4: Calculate daily total quantity sold and total revenue df_daily = df.groupby(\'date\').agg( daily_total_quantity_sold=(\'quantity\', \'sum\'), daily_revenue=(\'total_sale\', \'sum\') ).reset_index() # Step 5: Calculate the average daily revenue avg_daily_revenue = df_daily[\'daily_revenue\'].mean() return df_grouped_product, df_daily"},{"question":"You are given a dataset of tips received by waiters in a restaurant. The dataset contains the following columns: - `total_bill`: Total amount of the bill (including the tip) - `tip`: Tip given to the waiter - `sex`: Gender of the person who paid the bill - `smoker`: Whether the person was a smoker or not (Yes/No) - `day`: Day of the week - `time`: Time of day (Lunch/Dinner) - `size`: Number of people in the party Your task is to use the seaborn.objects module to perform the following visualizations: 1. **Bar Plot of Distinct Counts by Day**: Create a bar plot that shows the number of distinct observations for each day. 2. **Grouped Bar Plot by Day and Gender**: Create a bar plot that shows the number of distinct observations for each day, with bars grouped by gender. 3. **Bar Plot of Distinct Counts by Size**: Create a bar plot that shows the number of distinct observations for each party size. The expected output is a function named `create_plots` that takes no arguments and returns a list of the three plots generated using seaborn.objects. ```python def create_plots(): import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") # Task 1: Bar Plot of Distinct Counts by Day plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) # Task 2: Grouped Bar Plot by Day and Gender plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) # Task 3: Bar Plot of Distinct Counts by Size plot3 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) return [plot1, plot2, plot3] # Example usage plots = create_plots() for plot in plots: plot.show() ``` # Constraints & Limitations - You must use the seaborn.objects module for creating the plots as shown in the documentation. - The plots should correctly reflect the counts of distinct observations based on the specified variables without any data manipulation outside the plotting functions. # Input and Output - **Input**: None - **Output**: A list of three seaborn.objects plot objects. Ensure your function is well-documented and includes any necessary import statements.","solution":"def create_plots(): import seaborn.objects as so from seaborn import load_dataset tips = load_dataset(\\"tips\\") # Task 1: Bar Plot of Distinct Counts by Day plot1 = so.Plot(tips, x=\\"day\\").add(so.Bar(), so.Count()) # Task 2: Grouped Bar Plot by Day and Gender plot2 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) # Task 3: Bar Plot of Distinct Counts by Size plot3 = so.Plot(tips, x=\\"size\\").add(so.Bar(), so.Count()) return [plot1, plot2, plot3] # Example usage plots = create_plots() for plot in plots: plot.show()"},{"question":"<|Analysis Begin|> The provided documentation for `torch.mps` module lists several functions related to device management, memory management, random number state management, and profiling for the Metal Performance Shaders (MPS) backend, which is useful for running PyTorch on Apple Silicon. The documentation also mentions the existence of an `event` submodule with yet-to-be-documented capabilities. Key functions and their roles are: - **device_count**: Likely returns the number of MPS devices available. - **synchronize**: Probably used to synchronize the device. - **get_rng_state** and **set_rng_state**: Get and set the random number generator state. - **manual_seed** and **seed**: Set the random seed for generating random numbers. - **empty_cache**: Frees up unused memory. - **set_per_process_memory_fraction**: Sets the fraction of memory used by the process. - **current_allocated_memory**: Returns the current memory allocated on the MPS device. - **driver_allocated_memory**: Likely returns the memory managed by the MPS driver. - **recommended_max_memory**: Suggests the maximum memory that should be allocated. The profiling section indicates functions for starting and stopping profiling, checking if metal capture is enabled, and capturing metal-specific statistics. Given this set of functions and the context, a well-thought-out coding question should focus on using these functions to manage devices and memory, or to profile the performance of some operations with MPS. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are tasked with creating a small MPS-based functionality in PyTorch to demonstrate your understanding of managing devices, allocating memory, and profiling on Apple Silicon hardware. **Question:** Implement a function `optimize_matrix_multiplication()` that performs the following steps: 1. Check if any MPS devices are available. If none are available, return `\\"No MPS devices found.\\"` 2. Set a fixed random seed (e.g., 42) for reproducibility. 3. Allocate two random matrices of given size (input to the function) on the MPS device. 4. Synchronize the device before starting the operation. 5. Perform matrix multiplication on the two matrices and return the result. 6. Use profiling to measure the time taken for matrix multiplication. 7. Clear unused memory after the computation. Additionally, provide a function `get_memory_stats()` that: 1. Returns the current allocated memory on the MPS device. 2. Returns the driver allocated memory. 3. Returns the recommended maximum memory for the application. **Function Signatures:** ```python def optimize_matrix_multiplication(matrix_size: int) -> torch.Tensor: Performs matrix multiplication on the MPS device if available. Args: matrix_size (int): The size of NxN matrices to be multiplied. Returns: torch.Tensor: The result of the matrix multiplication. pass def get_memory_stats() -> dict: Fetches the memory statistics from the MPS device. Returns: dict: A dictionary with keys \'current_allocated_memory\', \'driver_allocated_memory\', and \'recommended_max_memory\' and their corresponding memory values. pass ``` **Constraints:** - Use the MPS device for all computations. - The random seed should be set to ensure reproducibility. - Properly manage memory to avoid unnecessary memory allocations. **Performance Requirements:** - Efficiently allocate and deallocate memory. - Ensure minimal overhead during profiling. **Input and Output Formats:** - `optimize_matrix_multiplication(matrix_size: int)`: - Input: `matrix_size` (an integer denoting the size of the NxN matrices). - Output: A `torch.Tensor` containing the result of the matrix multiplication if successful, or a string `\\"No MPS devices found.\\"` if no MPS device is available. - `get_memory_stats()`: - Output: A dictionary with memory statistics {\'current_allocated_memory\': value, \'driver_allocated_memory\': value, \'recommended_max_memory\': value}. Test your implementation to ensure correctness and efficiency.","solution":"import torch def optimize_matrix_multiplication(matrix_size: int): Performs matrix multiplication on the MPS device if available. Args: matrix_size (int): The size of NxN matrices to be multiplied. Returns: torch.Tensor: The result of the matrix multiplication if successful. str: \\"No MPS devices found.\\" if no MPS device is available. if not torch.mps.is_available(): return \\"No MPS devices found.\\" torch.mps.manual_seed(42) # Create random matrices a = torch.rand((matrix_size, matrix_size), device=\\"mps\\") b = torch.rand((matrix_size, matrix_size), device=\\"mps\\") torch.mps.synchronize() with torch.mps.profile() as prof: result = torch.matmul(a, b) torch.mps.synchronize() # Clear unused memory torch.mps.empty_cache() profiling_time = prof.key_averages().total_average().cpu_time # Include profiling time in the result tensor for demonstration result += profiling_time return result def get_memory_stats(): Fetches the memory statistics from the MPS device. Returns: dict: A dictionary with keys \'current_allocated_memory\', \'driver_allocated_memory\', and \'recommended_max_memory\' and their corresponding memory values. if not torch.mps.is_available(): return { \'current_allocated_memory\': 0, \'driver_allocated_memory\': 0, \'recommended_max_memory\': 0 } stats = { \'current_allocated_memory\': torch.mps.current_allocated_memory(), \'driver_allocated_memory\': torch.mps.driver_allocated_memory(), \'recommended_max_memory\': torch.mps.recommended_max_memory() } return stats"},{"question":"# Advanced XML Parsing with `xml.sax.handler` Objective: To assess your understanding and ability to work with the SAX (Simple API for XML) framework in Python, including creating custom handlers to process an XML document. Problem: You are required to parse an XML document using SAX and extract meaningful information using custom handlers. Specifically, you will implement handlers to: - Track and print the beginning and end of the document. - Detect and print all start and end tags, including attributes. - Capture and print all character data inside the elements. - Handle and print any errors or warnings encountered during parsing. Provided XML Sample: ```xml <?xml version=\\"1.0\\" ?> <note> <to>Tove</to> <from>Jani</from> <heading>Reminder</heading> <body>Don\'t forget me this weekend!</body> </note> ``` Requirements: 1. **ContentHandler**: Implement to handle the start and end of elements and character data. 2. **ErrorHandler**: Implement to handle and print errors and warnings. Implementation: You need to implement the following classes and methods: 1. **CustomContentHandler**: Inherits from `xml.sax.handler.ContentHandler` - `startDocument(self)`: Print \\"Start of document\\". - `endDocument(self)`: Print \\"End of document\\". - `startElement(self, name, attrs)`: Print \\"Start element: `name`, attributes: `attrs`\\". - `endElement(self, name)`: Print \\"End element: `name`\\". - `characters(self, content)`: Print \\"Characters: `content`\\". 2. **CustomErrorHandler**: Inherits from `xml.sax.handler.ErrorHandler` - `error(self, exception)`: Print \\"Error: `exception`\\". - `fatalError(self, exception)`: Print \\"Fatal error: `exception`\\". - `warning(self, exception)`: Print \\"Warning: `exception`\\". # Constraints: - The XML document can be assumed to be well-formed. - You do not need to handle namespaces for this problem. # Example Output: Given the provided XML sample, your program should produce output similar to: ``` Start of document Start element: note, attributes: {} Start element: to, attributes: {} Characters: Tove End element: to Start element: from, attributes: {} Characters: Jani End element: from Start element: heading, attributes: {} Characters: Reminder End element: heading Start element: body, attributes: {} Characters: Don\'t forget me this weekend! End element: body End element: note End of document ``` Write your implementation in the code block below: ```python import xml.sax.handler class CustomContentHandler(xml.sax.handler.ContentHandler): # Your implementation here ... class CustomErrorHandler(xml.sax.handler.ErrorHandler): # Your implementation here ... def parse_xml(xml_string): import xml.sax parser = xml.sax.make_parser() content_handler = CustomContentHandler() error_handler = CustomErrorHandler() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) xml.sax.parseString(xml_string, content_handler) # Example usage: xml_string = <?xml version=\\"1.0\\" ?><note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don\'t forget me this weekend!</body></note> parse_xml(xml_string) ```","solution":"import xml.sax.handler class CustomContentHandler(xml.sax.handler.ContentHandler): def startDocument(self): print(\\"Start of document\\") def endDocument(self): print(\\"End of document\\") def startElement(self, name, attrs): print(f\\"Start element: {name}, attributes: {dict(attrs)}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): if content.strip(): # To avoid printing whitespaces and newlines print(f\\"Characters: {content.strip()}\\") class CustomErrorHandler(xml.sax.handler.ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal error: {exception}\\") def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_xml(xml_string): import xml.sax parser = xml.sax.make_parser() content_handler = CustomContentHandler() error_handler = CustomErrorHandler() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) xml.sax.parseString(xml_string, content_handler) # Example usage: xml_string = <?xml version=\\"1.0\\" ?><note><to>Tove</to><from>Jani</from><heading>Reminder</heading><body>Don\'t forget me this weekend!</body></note> parse_xml(xml_string)"},{"question":"# Advanced Coding Assessment Question **Objective:** You are to implement a function that performs complex data transformations involving lists, sets, and dictionaries. This exercise will test your ability to handle these types, perform nested operations, and handle exceptions correctly. # Problem Statement: You are provided with a list of dictionaries, where each dictionary represents a student and their respective scores across multiple subjects. You are to write a function `overlapping_scores` which returns a dictionary where the keys are subject names, and the values are sets containing the scores that appeared more than once across all students for that subject. # Function Signature: ```python def overlapping_scores(students: list) -> dict: pass ``` # Input: - `students`: A list containing dictionaries, where each dictionary consists of subject names as keys and their respective scores (integers) as values. Example: ```python students = [ {\'Math\': 90, \'Physics\': 75, \'Chemistry\': 80}, {\'Math\': 85, \'Physics\': 75, \'Chemistry\': 90}, {\'Math\': 90, \'Physics\': 70, \'Chemistry\': 90}, {\'Math\': 85, \'Physics\': 75, \'Chemistry\': 80} ] ``` # Output: - Returns a dictionary where the keys are subjects and the values are sets containing overlapping scores (scores that appear more than once across all the dictionaries for that subject). Example corresponding to the above input: ```python { \'Math\': {85, 90}, \'Physics\': {75}, \'Chemistry\': {90, 80} } ``` # Constraints: - The list `students` can have multiple student dictionaries, each containing the same set of subjects. - Handle the edge case where the list or dictionaries can be empty. - You should not use any libraries other than Python\'s built-in capabilities. # Example Execution: ```python students = [ {\'Math\': 90, \'Physics\': 75, \'Chemistry\': 80}, {\'Math\': 85, \'Physics\': 75, \'Chemistry\': 90}, {\'Math\': 90, \'Physics\': 70, \'Chemistry\': 90}, {\'Math\': 85, \'Physics\': 75, \'Chemistry\': 80} ] print(overlapping_scores(students)) # { # \'Math\': {85, 90}, # \'Physics\': {75}, # \'Chemistry\': {90, 80} # } ``` # Notes: - Consider implementing helper functions if necessary. - The function should effectively use iteration, sets, and dictionary operations to achieve the desired outcome efficiently.","solution":"def overlapping_scores(students: list) -> dict: Given a list of student dictionaries with subject names as keys and scores as values, returns a dictionary where each key is a subject and the value is a set of scores that appear more than once across all students for that subject. if not students: return {} from collections import defaultdict # Intermediate storage score_count = defaultdict(lambda: defaultdict(int)) # Count scores for each subject for student in students: for subject, score in student.items(): score_count[subject][score] += 1 # Find scores that appear more than once result = {} for subject, scores in score_count.items(): overlapping = {score for score, count in scores.items() if count > 1} if overlapping: result[subject] = overlapping return result"},{"question":"Implement a Custom Logging System for an Application Objective The goal of this exercise is to design and implement a custom logging system for a Python application using the `logging` module. This task will test your understanding of loggers, handlers, and formatters, and how to use them effectively to manage log messages in a real-world scenario. Problem Statement You are tasked with implementing a logging system for a Python application that needs to log messages to multiple destinations with different log levels and formats. The requirements are: 1. Define a custom logging configuration function `setup_logging()` that: - Creates a logger named `app`. - Sets the logger level to `DEBUG`. - Attaches two handlers to the logger: - **FileHandler**: Writes log messages to `app.log`. - The log level for this handler should be `INFO`. - The log format should include the time, log level, and message. - **StreamHandler**: Writes log messages to `sys.stderr`. - The log level for this handler should be `ERROR`. - The log format should include the log level and message. 2. Implement a simple application function that logs messages at various levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) to demonstrate the logging setup. Input and Output - **Function 1: `setup_logging()`** - **Input**: None - **Output**: Configures the logging system as per the specifications - **Function 2: `run_application()`** - **Input**: None - **Output**: Logs messages at different levels for demonstration purposes Constraints - Ensure that log messages at different levels are appropriately directed to the correct handlers as specified. - Use the `logging` module functionalities described in the documentation provided to achieve the solution. Example Output When running the `run_application()` function after setting up logging with `setup_logging()`, the expected output in the console and written to `app.log` should look something like this: **Console Output (`sys.stderr`):** ``` ERROR:root:This is an error message CRITICAL:root:This is a critical message ``` **File Output (`app.log`):** ``` 2023-06-01 12:00:00,123 - INFO - This is an info message 2023-06-01 12:00:00,124 - WARNING - This is a warning message 2023-06-01 12:00:00,125 - ERROR - This is an error message 2023-06-01 12:00:00,126 - CRITICAL - This is a critical message ``` Implementation Implement the `setup_logging()` and `run_application()` functions to meet the given requirements. ```python import logging def setup_logging(): # Create a logger named \'app\' logger = logging.getLogger(\'app\') logger.setLevel(logging.DEBUG) # Create a FileHandler file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.INFO) file_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) # Create a StreamHandler stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.ERROR) stream_formatter = logging.Formatter(\'%(levelname)s - %(message)s\') stream_handler.setFormatter(stream_formatter) # Add handlers to the logger logger.addHandler(file_handler) logger.addHandler(stream_handler) def run_application(): logger = logging.getLogger(\'app\') logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\') # Example usage setup_logging() run_application() ``` This problem is designed to ensure you understand how to use all the necessary components provided by the logging module to efficiently log messages in a Python application, controlling log levels and formats for different destinations.","solution":"import logging def setup_logging(): # Create a logger named \'app\' logger = logging.getLogger(\'app\') logger.setLevel(logging.DEBUG) # Create a FileHandler file_handler = logging.FileHandler(\'app.log\') file_handler.setLevel(logging.INFO) file_formatter = logging.Formatter(\'%(asctime)s - %(levelname)s - %(message)s\') file_handler.setFormatter(file_formatter) # Create a StreamHandler stream_handler = logging.StreamHandler() stream_handler.setLevel(logging.ERROR) stream_formatter = logging.Formatter(\'%(levelname)s - %(message)s\') stream_handler.setFormatter(stream_formatter) # Add handlers to the logger logger.addHandler(file_handler) logger.addHandler(stream_handler) def run_application(): logger = logging.getLogger(\'app\') logger.debug(\'This is a debug message\') logger.info(\'This is an info message\') logger.warning(\'This is a warning message\') logger.error(\'This is an error message\') logger.critical(\'This is a critical message\')"},{"question":"# Advanced File Management and Processing Task You are tasked with developing a Python script that manages a file processing system. The script will read a list of filenames from the command line, check the creation dates of these files and generate a report that categorizes the files into different age groups. The script should: 1. Accept a list of filenames and an optional parameter for the number of days to categorize files. 2. Determine the creation date of each file. 3. Categorize the files into three groups based on their age relative to the specified number of days (or 30 days if not specified): - **Recent**: Files created within the specified number of days. - **Old**: Files created between the specified number of days and twice the specified number of days. - **Very Old**: Files created more than twice the specified number of days ago. 4. Generate a summary report printed to the console with the filenames sorted into their respective categories. Input: - Command line arguments: A list of filenames. - Optional command line argument: `--days` or `-d` specifying the number of days for categorization (default is 30). Output: - Print a summary report to the console with the filenames categorized into \\"Recent\\", \\"Old\\", and \\"Very Old\\". Constraints: - Assume all provided filenames exist and are accessible. - The script should handle any number of filenames. Example Usage: ```sh python file_manager.py --days=15 file1.txt file2.txt file3.txt ``` Expected Output: ``` Recent Files: file3.txt Old Files: file2.txt Very Old Files: file1.txt ``` # Implementation Details 1. Use the `argparse` module to handle command line arguments. 2. Utilize the `os` and `datetime` modules to get file creation dates and perform date arithmetic. 3. Write the function definitions and implement the logic to categorize and print the files as described.","solution":"import os import sys import argparse from datetime import datetime, timedelta def get_file_creation_date(file_path): Returns the creation date of the file in datetime format. return datetime.fromtimestamp(os.path.getctime(file_path)) def categorize_files(files, days): Categorizes the given list of files based on their creation date relative to the given days. Returns a dictionary with categories as keys and lists of filenames as values. now = datetime.now() recent_files = [] old_files = [] very_old_files = [] for file in files: creation_date = get_file_creation_date(file) age_in_days = (now - creation_date).days if age_in_days <= days: recent_files.append(file) elif age_in_days <= 2 * days: old_files.append(file) else: very_old_files.append(file) return { \\"Recent\\": recent_files, \\"Old\\": old_files, \\"Very Old\\": very_old_files } def generate_report(files, days=30): Generates and prints a summary report categorizing files into Recent, Old, and Very Old. categories = categorize_files(files, days) print(\\"Recent Files:\\") for file in categories[\'Recent\']: print(file) print(\\"nOld Files:\\") for file in categories[\'Old\']: print(file) print(\\"nVery Old Files:\\") for file in categories[\'Very Old\']: print(file) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"File categorization based on creation date.\\") parser.add_argument(\\"files\\", metavar=\\"F\\", type=str, nargs=\\"+\\", help=\\"List of filenames to categorize.\\") parser.add_argument(\\"--days\\", \\"-d\\", type=int, default=30, help=\\"Number of days to categorize files.\\") args = parser.parse_args() generate_report(args.files, args.days)"},{"question":"**Custom Power Function and Module Implementation** # Problem Statement Your task is to implement a custom PyTorch operation called `PowerFunction` that raises each element of a tensor to a specified power. You will then need to create a custom module called `PowerModule` using this function. The operation should support both forward and backward passes to correctly compute gradients. # Requirements: 1. **Custom Function (`PowerFunction`)**: - Subclass PyTorch\'s `torch.autograd.Function`. - Implement the `forward` method to compute the power operation. - Implement the `setup_context` method to save necessary tensors for the backward pass. - Implement the `backward` method to compute the gradient of the operation. 2. **Custom Module (`PowerModule`)**: - Subclass PyTorch\'s `torch.nn.Module`. - Implement the `forward` method to apply the `PowerFunction`. # Input/Output Specifications: - `forward` input: - `input_tensor`: A PyTorch tensor of shape `(N, *)` where `*` means any number of additional dimensions. - `power`: A float indicating the power to raise each element of the tensor to. - `forward` output: - A tensor of the same shape as `input_tensor` with each element raised to the specified power. # Example: ```python import torch class PowerFunction(torch.autograd.Function): @staticmethod def forward(ctx, input_tensor, power): # Your code here @staticmethod def setup_context(ctx, inputs, output): # Your code here @staticmethod def backward(ctx, grad_output): # Your code here class PowerModule(torch.nn.Module): def __init__(self, power): super(PowerModule, self).__init__() self.power = power def forward(self, input_tensor): return PowerFunction.apply(input_tensor, self.power) # Example usage input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) power_module = PowerModule(3.0) output = power_module(input_tensor) output.sum().backward() print(input_tensor.grad) # Inspect the gradients ``` # Constraints: - Ensure that the backward computation correctly handles gradients for higher-order derivatives. - Test your custom function with `torch.autograd.gradcheck` to validate the gradient computation. # Instructions: - Complete the implementation of `PowerFunction` including `forward`, `setup_context`, and `backward` methods. - Complete the `PowerModule` class. - You are encouraged to include additional test cases to verify the correctness of your implementation.","solution":"import torch class PowerFunction(torch.autograd.Function): @staticmethod def forward(ctx, input_tensor, power): # Save input_tensor and power for backward pass ctx.save_for_backward(input_tensor) ctx.power = power # Apply the power operation output = input_tensor ** power return output @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors power = ctx.power # Compute gradient of input grad_input = power * (input_tensor ** (power - 1)) * grad_output return grad_input, None class PowerModule(torch.nn.Module): def __init__(self, power): super(PowerModule, self).__init__() self.power = power def forward(self, input_tensor): return PowerFunction.apply(input_tensor, self.power) # Example usage input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) power_module = PowerModule(3.0) output = power_module(input_tensor) output.sum().backward() print(input_tensor.grad) # Inspect the gradients"},{"question":"# **Advanced Pandas Index Manipulation and Combination** **Problem Statement:** You are provided with raw sales data in the form of a `DataFrame` containing sales records across multiple regions and time periods. Your task is to write a function that processes this data to generate a detailed sales report. The sales report should contain aggregated sales data indexed by a combination of region and month (using MultiIndex). **Data:** 1. The `DataFrame`, `sales_data`, contains the following columns: - `date`: The sale date (string format `\'yyyy-mm-dd\'`). - `region`: The sales region (string). - `sales_amount`: The amount of sales (float). **Expected Output:** A `DataFrame` containing the total sales for each region per month, indexed by a combination of region and month (MultiIndex). **Function Signature:** ```python def generate_sales_report(sales_data: pd.DataFrame) -> pd.DataFrame: pass ``` **Constraints:** - The input `sales_data` DataFrame is guaranteed to have non-null values for all columns. - The dates range over multiple years. - The data should be aggregated by month and region. - Duplicate sales entries (if any) should be summed up. **Performance Requirements:** - Your solution should efficiently handle a large dataset (up to 1 million rows). **Example:** Given the following `sales_data` DataFrame: | date | region | sales_amount | |------------|----------|--------------| | 2023-01-15 | East | 200.0 | | 2023-01-20 | East | 150.0 | | 2023-02-10 | West | 300.0 | | 2023-01-25 | West | 250.0 | | 2023-02-15 | East | 100.0 | The function `generate_sales_report(sales_data)` should return: | region | month | sales_amount | |--------|-------|--------------| | East | 2023-01 | 350.0 | | East | 2023-02 | 100.0 | | West | 2023-01 | 250.0 | | West | 2023-02 | 300.0 | **Implementation Notes:** 1. Parse the `date` column to extract the month. 2. Create a MultiIndex using `region` and `month`. 3. Aggregate the sales amounts for each region and month. 4. Return the resulting DataFrame.","solution":"import pandas as pd def generate_sales_report(sales_data: pd.DataFrame) -> pd.DataFrame: Generate a sales report with total sales aggregated by region and month. # Convert the \'date\' column to datetime sales_data[\'date\'] = pd.to_datetime(sales_data[\'date\']) # Extract the month and year as a single value in \'YYYY-MM\' format sales_data[\'month\'] = sales_data[\'date\'].dt.to_period(\'M\').astype(str) # Group by \'region\' and \'month\' and sum the \'sales_amount\' report = sales_data.groupby([\'region\', \'month\']).agg({\'sales_amount\': \'sum\'}) # Reset index to have MultiIndex on region and month report = report.reset_index() # Set \'region\' and \'month\' as MultiIndex report = report.set_index([\'region\', \'month\']) # Ensure the result is sorted by index report = report.sort_index() return report"},{"question":"Objective: Your task is to create a Python program that demonstrates the ability to serialize complex data structures, store them persistently in a database, and retrieve and deserialize them efficiently. Problem Statement: You are required to implement a library management system where book records need to be stored persistently. Each book record will include the following details: - Title (string) - Author (string) - ISBN (string) - Published Year (integer) - Genre (string) Your task is to: 1. Define a `Book` class with the necessary attributes. 2. Implement functions to serialize and deserialize `Book` objects using the `pickle` module. 3. Store the serialized `Book` objects in an SQLite database. 4. Retrieve and deserialize the `Book` objects from the database. Requirements: 1. Create a class `Book` with attributes: title, author, isbn, year, and genre. 2. Implement the following functions: - `serialize_book(book: Book) -> bytes`: Serialize the `Book` object into bytes using `pickle`. - `deserialize_book(data: bytes) -> Book`: Deserialize the bytes back into a `Book` object using `pickle`. - `store_book(db_name: str, book: Book) -> None`: Store the serialized `Book` object in an SQLite database. - `retrieve_books(db_name: str) -> List[Book]`: Retrieve and deserialize all `Book` objects from the SQLite database. 3. Use the `sqlite3` module to handle database operations. 4. Ensure that the database is created and tables are structured appropriately. Input and Output: - `serialize_book(book)`: Takes a `Book` object and returns its serialized byte representation. - `deserialize_book(data)`: Takes serialized bytes and returns the corresponding `Book` object. - `store_book(db_name, book)`: Takes a database name and a `Book` object, then stores the serialized book in the specified SQLite database. - `retrieve_books(db_name)`: Takes a database name and returns a list of deserialized `Book` objects. Example Usage: ```python # Creating book instances book1 = Book(title=\\"Effective Python\\", author=\\"Brett Slatkin\\", isbn=\\"9780134034287\\", year=2015, genre=\\"Programming\\") book2 = Book(title=\\"The Pragmatic Programmer\\", author=\\"Andrew Hunt\\", isbn=\\"9780201616224\\", year=1999, genre=\\"Programming\\") # Serialize books serialized_book1 = serialize_book(book1) serialized_book2 = serialize_book(book2) # Storing the books in the database store_book(\\"library.db\\", book1) store_book(\\"library.db\\", book2) # Retrieving books from the database books = retrieve_books(\\"library.db\\") for book in books: print(book.title, book.author) ``` Constraints: - Use the `pickle` module for serialization and deserialization. - Use the `sqlite3` module for database operations. - Ensure your code is well-structured and handles exceptions appropriately. **Performance Requirements**: - The implementation should be able to handle up to 1000 book records efficiently. - Minimize the use of memory and ensure that database connections are properly managed (opened and closed). Note: Ensure to thoroughly test your implementation with various book records to validate the correctness and efficiency of your functions.","solution":"import pickle import sqlite3 from typing import List # Define the Book class with necessary attributes class Book: def __init__(self, title: str, author: str, isbn: str, year: int, genre: str): self.title = title self.author = author self.isbn = isbn self.year = year self.genre = genre # Serialize the Book object into bytes using pickle def serialize_book(book: Book) -> bytes: return pickle.dumps(book) # Deserialize the bytes back into a Book object using pickle def deserialize_book(data: bytes) -> Book: return pickle.loads(data) # Store the serialized Book object in an SQLite database def store_book(db_name: str, book: Book) -> None: connection = sqlite3.connect(db_name) cursor = connection.cursor() cursor.execute(\'\'\'CREATE TABLE IF NOT EXISTS books (id INTEGER PRIMARY KEY, title TEXT, author TEXT, isbn TEXT, year INTEGER, genre TEXT, data BLOB)\'\'\') serialized_data = serialize_book(book) cursor.execute(\'\'\'INSERT INTO books (title, author, isbn, year, genre, data) VALUES (?, ?, ?, ?, ?, ?)\'\'\', (book.title, book.author, book.isbn, book.year, book.genre, serialized_data)) connection.commit() connection.close() # Retrieve and deserialize all Book objects from the SQLite database def retrieve_books(db_name: str) -> List[Book]: connection = sqlite3.connect(db_name) cursor = connection.cursor() cursor.execute(\'SELECT data FROM books\') rows = cursor.fetchall() books = [deserialize_book(row[0]) for row in rows] connection.close() return books"},{"question":"Question: Customizing and Visualizing Data using Seaborn **Objective:** You are tasked with using seaborn to visualize a given dataset while applying various seaborn themes and customizations. **Instructions:** 1. Load the following dataset into a pandas DataFrame: ```python import pandas as pd data = { \\"Category\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\"], \\"Value1\\": [5, 9, 2, 8, 7], \\"Value2\\": [15, 25, 10, 20, 14] } df = pd.DataFrame(data) ``` 2. Using seaborn, create a bar plot that shows the values of `Value1` for each `Category`. Set a default seaborn theme using `sns.set_theme()`. 3. Modify the plot such that: - The style is set to `\\"whitegrid\\"`. - The palette used is `\\"pastel\\"`. 4. Customize the appearance further by making the following adjustments using rcParams: - Hide the right and top spines of the plot. - Set the title of the plot to `\\"Custom Bar Plot of Value1\\"`. 5. Save the final figure as a PNG file named `\\"custom_barplot.png\\"`. **Constraints:** - Ensure the plot is clearly labeled with the categories on the x-axis and `Value1` on the y-axis. - The plot should be well-formatted and professional in appearance. **Expected Output:** - A PNG file named `\\"custom_barplot.png\\"` containing the customized bar plot. **Example Solution:** ```python import seaborn as sns import matplotlib.pyplot as plt # Step 1: Load the dataset data = { \\"Category\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\"], \\"Value1\\": [5, 9, 2, 8, 7], \\"Value2\\": [15, 25, 10, 20, 14] } df = pd.DataFrame(data) # Step 2: Create a bar plot with the default theme sns.set_theme() plt.figure(figsize=(8, 6)) barplot = sns.barplot(x=\\"Category\\", y=\\"Value1\\", data=df) plt.title(\\"Default Theme Bar Plot of Value1\\") plt.show() # Step 3: Modify the plot with specific theme settings sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") plt.figure(figsize=(8, 6)) barplot = sns.barplot(x=\\"Category\\", y=\\"Value1\\", data=df) plt.title(\\"Whitegrid Pastel Bar Plot of Value1\\") plt.show() # Step 4: Further customize the appearance using rcParams custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\", rc=custom_params) plt.figure(figsize=(8, 6)) barplot = sns.barplot(x=\\"Category\\", y=\\"Value1\\", data=df) plt.title(\\"Custom Bar Plot of Value1\\") plt.show() # Save the final figure as a PNG file plt.savefig(\\"custom_barplot.png\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def create_custom_barplot(): # Step 1: Load the dataset data = { \\"Category\\": [\\"A\\", \\"B\\", \\"C\\", \\"D\\", \\"E\\"], \\"Value1\\": [5, 9, 2, 8, 7], \\"Value2\\": [15, 25, 10, 20, 14] } df = pd.DataFrame(data) # Step 2: Create a bar plot with the default theme sns.set_theme() # Step 3: Modify the plot with specific theme settings sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") # Step 4: Further customize the appearance using rcParams custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\", rc=custom_params) plt.figure(figsize=(8, 6)) barplot = sns.barplot(x=\\"Category\\", y=\\"Value1\\", data=df) plt.title(\\"Custom Bar Plot of Value1\\") # Save the final figure as a PNG file plt.savefig(\\"custom_barplot.png\\") plt.close() # Calling the function to create and save the plot create_custom_barplot()"},{"question":"# Question: Custom File Handler with Built-in Functions You are required to create a custom file handling module that enhances Python\'s built-in file handling functions. Specifically, you will create a class `CustomFileHandler` that wraps the built-in `open()` function to provide additional functionality. Requirements: 1. **Class Definition**: Define a class `CustomFileHandler`. 2. **Initialization**: The class should be initialized with a file path and mode (e.g., \'r\' for reading, \'w\' for writing). 3. **Reading**: Implement a method `read_lines_with_numbers()` that reads lines from the file and returns a list where each element is a tuple (line_number, line_content). 4. **Words Count**: Implement a method `count_words()` that reads the content of the file and returns a dictionary where the keys are the words, and the values are the number of occurrences of each word. 5. **Writing**: Implement a method `write_lines(lines)` that takes a list of lines and writes them to the file, with each line being converted to lower case before writing. Constraints: - You must use the built-in `open()` function from the `builtins` module for file operations. - Handle any necessary exceptions that may occur during file operations. Input and Output Formats: - **Initialization**: ```python handler = CustomFileHandler(\'example.txt\', \'r\') ``` - **Reading Lines with Numbers**: ```python result = handler.read_lines_with_numbers() # Output: [(1, \\"First line content\\"), (2, \\"Second line content\\"), ...] ``` - **Counting Words**: ```python word_counts = handler.count_words() # Output: {\'first\': 1, \'line\': 2, \'content\': 2, \'second\': 1, ...} ``` - **Writing Lines**: ```python handler.write_lines([\'FIRST LINE CONTENT\', \'SECOND LINE CONTENT\']) # The file should contain: # first line content # second line content ``` Additional Requirements: 1. Ensure your code is well-documented with comments. 2. Include appropriate error handling, especially for file not found errors. 3. Your implementation should be efficient and make proper use of file handling techniques in Python. Implement the `CustomFileHandler` class as specified above.","solution":"class CustomFileHandler: def __init__(self, file_path, mode): Initializes the CustomFileHandler with a file path and mode. :param file_path: The path to the file. :param mode: The mode in which to open the file (\'r\' for reading, \'w\' for writing, etc.). self.file_path = file_path self.mode = mode def read_lines_with_numbers(self): Reads lines from the file and returns a list where each element is a tuple (line_number, line_content). :return: List of tuples with line numbers and line contents. try: with open(self.file_path, self.mode) as file: lines = file.readlines() return [(idx + 1, line.strip()) for idx, line in enumerate(lines)] except FileNotFoundError: return [] def count_words(self): Reads the content of the file and returns a dictionary where the keys are words, and values are the number of occurrences of each word. :return: Dictionary with word counts. word_counts = {} try: with open(self.file_path, self.mode) as file: for line in file: words = line.split() for word in words: word = word.lower().strip(\'.,!?\\";:\') if word in word_counts: word_counts[word] += 1 else: word_counts[word] = 1 except FileNotFoundError: return {} return word_counts def write_lines(self, lines): Takes a list of lines and writes them to the file. Each line is converted to lower case before writing. :param lines: List of lines to write. try: with open(self.file_path, self.mode) as file: for line in lines: file.write(line.lower() + \'n\') except FileNotFoundError: return"},{"question":"**Coding Assessment Question** # Objective: Create a Python script that demonstrates the use and behavior of the \\"site\\" module by dynamically modifying and inspecting the module search path (`sys.path`). # Requirements: 1. **Function Definition**: - Name: `configure_site_paths` - Parameters: None - Returns: Dictionary # Task Description: Your task is to implement the `configure_site_paths` function which performs the following steps: 1. **Initial Setup**: - Import the `site` and `sys` modules. - Store the initial `sys.path` in a variable `initial_sys_path`. 2. **Add a Custom Directory**: - Create a temporary directory (e.g., \'/tmp/custom_site\'). - Use `site.addsitedir` to add this custom directory to the module search path. - Verify the directory has been added by checking its presence in `sys.path`. 3. **Retrieve Site Packages**: - Use `site.getsitepackages()` to retrieve the list of global site-packages directories. - Use `site.getusersitepackages()` to retrieve the user-specific site-packages directory. 4. **Call `site.main()`**: - Explicitly call `site.main()` to add all the standard site-specific directories to the module search path. 5. **Create and Process a .pth File**: - Within the temporary directory, create a `.pth` file (e.g., `custom.pth`) that adds another custom directory (e.g., \'/tmp/custom_lib\') to `sys.path`. - Ensure the content of the `.pth` file and verify that the new path has been added to `sys.path`. 6. **Customizations**: - If possible, create and import a `sitecustomize.py` or `usercustomize.py` script to demonstrate additional path manipulations or environment setup (this step can be optional depending on your environment). # Output: Return a dictionary containing: - `initial_sys_path`: The `sys.path` at the beginning. - `custom_sys_path`: The `sys.path` after adding the custom directory. - `site_packages`: The list of global site-packages directories. - `user_site_packages`: The user-specific site-packages directory. - `final_sys_path`: The `sys.path` after calling `site.main()` and processing the `.pth` file. # Constraints: - The solution must handle environments where the temporary directory might already exist or be created at runtime. - Handle any exceptions or errors gracefully, particularly around file creation or path manipulations. # Example Dictionary Format: ```python { \\"initial_sys_path\\": [...], \\"custom_sys_path\\": [...], \\"site_packages\\": [...], \\"user_site_packages\\": \\"...\\", \\"final_sys_path\\": [...] } ``` # Notes: The test environment might have restrictions, so mock or simulate parts of the process where necessary. Submit your implementation of the `configure_site_paths` function.","solution":"import sys import site import os def configure_site_paths(): Demonstrates the use and behavior of the \'site\' module by dynamically modifying and inspecting the module search path (sys.path). Returns: dict: A dictionary containing the \'initial_sys_path\', \'custom_sys_path\', \'site_packages\', \'user_site_packages\', and \'final_sys_path\'; # Store the initial sys.path initial_sys_path = sys.path.copy() # Add a custom directory custom_directory = \'/tmp/custom_site\' if not os.path.exists(custom_directory): os.makedirs(custom_directory) site.addsitedir(custom_directory) # Verify the directory has been added custom_sys_path = sys.path.copy() # Retrieve the site packages directories site_packages = site.getsitepackages() user_site_packages = site.getusersitepackages() # Call site.main() site.main() # Create and process a .pth file custom_pth_file = os.path.join(custom_directory, \'custom.pth\') with open(custom_pth_file, \'w\') as f: f.write(\'/tmp/custom_libn\') # To simulate loading the .pth file, we need to invoke addsitedir again site.addsitedir(custom_directory) # Verify the .pth directory has been added final_sys_path = sys.path.copy() return { \\"initial_sys_path\\": initial_sys_path, \\"custom_sys_path\\": custom_sys_path, \\"site_packages\\": site_packages, \\"user_site_packages\\": user_site_packages, \\"final_sys_path\\": final_sys_path }"},{"question":"Objective Your task is to implement a function that reads a configuration file, processes some specific configurations, and performs certain operations based on the data extracted. Problem Statement Implement a function named `process_config` that reads configurations from an INI file, processes the data, and returns specific outputs based on the configurations provided. Function Signature ```python def process_config(file_path: str) -> dict: pass ``` Input - `file_path` (str): The path to the INI configuration file. Output - A dictionary containing: - `\'modified_compression_level\'`: The product of `CompressionLevel` from the `DEFAULT` section and the number of sections that contain the key `User`. - `\'ports_sum\'`: The sum of all `Port` values (converted to integers) from every section that includes the `Port` key. - `\'forward_x11_flags\'`: A dictionary where each key is the section name and the value is a boolean indicating whether `ForwardX11` is enabled (`True`). Requirements 1. The function should read the provided INI file. 2. Calculate and return: - The product of `CompressionLevel` from the `DEFAULT` section and the number of sections that contain the key `User`. - The sum of all `Port` values from sections that have the `Port` key. - A dictionary with section names as keys and boolean values indicating the state of `ForwardX11`. Constraints - Assume the input file is a properly formatted INI file as described in the provided documentation. - Sections or keys may or may not be present in the configuration file. Example Given an INI file with the following content: ``` [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [forge.example] User = hg [topsecret.server.example] Port = 50022 ForwardX11 = no ``` The expected output: ```python { \'modified_compression_level\': 9, \'ports_sum\': 50022, \'forward_x11_flags\': { \'forge.example\': True, \'topsecret.server.example\': False } } ``` Notes - You should handle the following scenarios: - If the `CompressionLevel` or `ForwardX11` is not specified in the `DEFAULT` section, use appropriate fallback values (`CompressionLevel` fallback to `1` and `ForwardX11` fallback to `False`). - Ensure the function handles the absence of keys gracefully. Good luck and happy coding!","solution":"import configparser def process_config(file_path: str) -> dict: config = configparser.ConfigParser() config.read(file_path) # Get the default CompressionLevel or fallback to 1 compression_level = int(config[\'DEFAULT\'].get(\'CompressionLevel\', 1)) # Initialize results modified_compression_level = 0 ports_sum = 0 forward_x11_flags = {} # Calculate modified_compression_level and ports_sum, and populate forward_x11_flags num_user_sections = 0 for section in config.sections(): if \'User\' in config[section]: num_user_sections += 1 if \'Port\' in config[section]: ports_sum += int(config[section][\'Port\']) forward_x11_flags[section] = config.getboolean(section, \'ForwardX11\', fallback=False) modified_compression_level = compression_level * num_user_sections return { \'modified_compression_level\': modified_compression_level, \'ports_sum\': ports_sum, \'forward_x11_flags\': forward_x11_flags }"},{"question":"Comprehensive File I/O and JSON Handling Problem Statement You are tasked with developing a Python script that processes student records stored in a JSON file. The JSON file contains a list of dictionaries, each representing a student with their name, subjects, and scores. The script should: 1. Load the JSON file. 2. Calculate the average score for each student. 3. Format the output in a tabular format using formatted string literals (f-strings). 4. Save the processed results back to another JSON file with the additional average score included for each student. Input * A JSON file named `students.json` formatted as follows: ```json [ { \\"name\\": \\"Alice\\", \\"subjects\\": { \\"math\\": 88, \\"science\\": 92, \\"history\\": 80 } }, { \\"name\\": \\"Bob\\", \\"subjects\\": { \\"math\\": 75, \\"science\\": 85, \\"history\\": 78 } } ] ``` Output * A JSON file named `processed_students.json` with the updated records including the average score. * A tabular formatted string printed to the console showing the student name, subject scores, and the average score. Requirements * Use the `json` module for reading and writing JSON data. * Use formatted string literals (f-strings) to format the output. * Ensure proper file handling using the `with` statement. * Calculate the average score to two decimal places. Constraints * The input JSON file can have any number of students and subjects. * Scores are integers. * The JSON file encoding is UTF-8. Example Given the input file `students.json`: ```json [ { \\"name\\": \\"Alice\\", \\"subjects\\": { \\"math\\": 88, \\"science\\": 92, \\"history\\": 80 } }, { \\"name\\": \\"Bob\\", \\"subjects\\": { \\"math\\": 75, \\"science\\": 85, \\"history\\": 78 } } ] ``` Your script should produce the following file `processed_students.json`: ```json [ { \\"name\\": \\"Alice\\", \\"subjects\\": { \\"math\\": 88, \\"science\\": 92, \\"history\\": 80 }, \\"average\\": 86.67 }, { \\"name\\": \\"Bob\\", \\"subjects\\": { \\"math\\": 75, \\"science\\": 85, \\"history\\": 78 }, \\"average\\": 79.33 } ] ``` And print the following to the console: ``` Name | Math | Science | History | Average --------------------------------------------- Alice | 88 | 92 | 80 | 86.67 Bob | 75 | 85 | 78 | 79.33 ``` Implementation Write a function `process_student_records(input_file: str, output_file: str) -> None` that performs the above tasks. Ensure that your function adheres to the constraints and formats the output correctly. ```python import json def process_student_records(input_file: str, output_file: str) -> None: # Your code here ``` Use the above function to process the `students.json` input file and output the results as specified.","solution":"import json def process_student_records(input_file: str, output_file: str) -> None: # Load the JSON file with open(input_file, \'r\', encoding=\'utf-8\') as file: students = json.load(file) # Process each student record for student in students: subjects = student[\\"subjects\\"] average_score = sum(subjects.values()) / len(subjects) student[\\"average\\"] = round(average_score, 2) # Save the processed results back to another JSON file with open(output_file, \'w\', encoding=\'utf-8\') as file: json.dump(students, file, ensure_ascii=False, indent=4) # Format and print the output in tabular format header = \\"Name | \\" + \\" | \\".join([flag.capitalize() for flag in students[0][\\"subjects\\"].keys()]) + \\" | Average\\" print(header) print(\\"-\\" * len(header)) for student in students: subject_scores = \\" | \\".join([f\\"{score:>5}\\" for score in student[\\"subjects\\"].values()]) print(f\\"{student[\'name\']:<9} | {subject_scores} | {student[\'average\']:>7.2f}\\")"},{"question":"# Partial Dependence and Individual Conditional Expectation Plots Objective You are given a dataset and a trained machine learning model. Your task is to generate and analyze both partial dependence plots (PDP) and individual conditional expectation (ICE) plots using the `sklearn.inspection.PartialDependenceDisplay` class. Problem Statement 1. Load the provided dataset (`\'X.npy\'` and `\'y.npy\'`). 2. Train a gradient boosting classifier on the dataset. 3. Generate both partial dependence plots and individual conditional expectation plots for a set of specified features. 4. Plot these graphs and discuss the insights derived from them. Instructions 1. **Load the Data**: - Load the feature matrix `X` and the target vector `y` from the provided files `\'X.npy\'` and `\'y.npy\'`. 2. **Train the Model**: - Train a `GradientBoostingClassifier` using the loaded dataset. - You may use the following hyperparameters for the model: ```python GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) ``` 3. **Generate and Plot PDP and ICE**: - Use the `PartialDependenceDisplay.from_estimator` method to generate the PDP and ICE plots for the following features: - Feature index `0` - Feature index `1` - Combination of features `(0, 1)` - For ICE plots, ensure that you see both the individual lines and the average effect. 4. **Discuss Insights**: - Provide insights from the PDP plots about the dependence of the target response on the specified features. - Discuss any additional insights from the ICE plots, particularly focusing on the heterogeneity of the relationships. Constraints - Use only the provided methods and classes from scikit-learn. - You should ensure your code is efficient and plots are clearly labeled. Expected Output - A trained gradient boosting model. - PDP and ICE plots for the specified features. - A detailed discussion on the observed dependencies and interactions between the features and the target response. ```python # Sample starter code import numpy as np from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt # Load data X = np.load(\'X.npy\') y = np.load(\'y.npy\') # Train Gradient Boosting Classifier clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) clf.fit(X, y) # Specify features features = [0, 1, (0, 1)] # Generate PDP and ICE plots PartialDependenceDisplay.from_estimator(clf, X, features, kind=\'both\') # Access plot objects plt.gcf() plt.gca() # Show plots plt.show() # Provide insights about the plots # e.g., \\"The PDP plot for feature 0 shows a linear relationship between...\\" ```","solution":"import numpy as np from sklearn.ensemble import GradientBoostingClassifier from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def load_data(features_file, target_file): Load feature matrix X and target vector y from the specified files. X = np.load(features_file) y = np.load(target_file) return X, y def train_model(X, y): Train a Gradient Boosting Classifier using the provided data. clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) clf.fit(X, y) return clf def generate_plots(clf, X, features): Generate PDP and ICE plots for the given features using the trained model. PartialDependenceDisplay.from_estimator(clf, X, features, kind=\'both\') plt.show() # Main function to execute the steps def main(feature_file=\'X.npy\', target_file=\'y.npy\'): # Load data X, y = load_data(feature_file, target_file) # Train model clf = train_model(X, y) # Specify features for plots features = [0, 1, (0, 1)] # Generate PDP and ICE plots generate_plots(clf, X, features) if __name__ == \\"__main__\\": main()"},{"question":"# Question **Title:** Flight Schedule Overlap Detector **Objective:** Implement a function that determines if two flights have overlapping schedules given their departure and arrival times, including timezone consideration. **Description:** You are given the schedule of two flights in the form of departure and arrival times in different timezones. Your task is to implement a function `do_flights_overlap(schedule1, schedule2)` that takes in two dictionaries representing these schedules and determines if the two flights overlap at any point in time. **Requirements:** 1. Use the `datetime`, `timezone`, and `timedelta` classes from the `datetime` module. 2. Handle both naive and aware datetime objects. 3. Consider edge cases where flights might start or end exactly at the same time. **Function Signature:** ```python def do_flights_overlap(schedule1: dict, schedule2: dict) -> bool: Determine if the flights in the given schedules overlap. Args: schedule1 (dict): A dictionary containing departure and arrival times of the first flight. schedule1 = { \\"departure\\": \\"2023-10-01T10:15:00-04:00\\", \\"arrival\\": \\"2023-10-01T12:45:00-04:00\\" } schedule2 (dict): A dictionary containing departure and arrival times of the second flight. schedule2 = { \\"departure\\": \\"2023-10-01T11:00:00-05:00\\", \\"arrival\\": \\"2023-10-01T14:00:00-05:00\\" } Returns: bool: True if the flights overlap, False otherwise. pass ``` **Input:** - `schedule1` and `schedule2` are dictionaries with keys `departure` and `arrival` containing ISO 8601 datetime strings with timezone information. **Output:** - Return `True` if there is any overlap in the flight schedules, `False` otherwise. **Constraints:** - Assume datetime strings provided are valid and correctly formatted. - Time zones are correctly included in the departure and arrival times. - Both flights have valid departure and arrival times with the departure time being before the arrival time. **Examples:** ```python schedule1 = { \\"departure\\": \\"2023-10-01T10:15:00-04:00\\", \\"arrival\\": \\"2023-10-01T12:45:00-04:00\\" } schedule2 = { \\"departure\\": \\"2023-10-01T11:00:00-05:00\\", \\"arrival\\": \\"2023-10-01T14:00:00-05:00\\" } assert do_flights_overlap(schedule1, schedule2) == True # Overlaps during 2023-10-01T12:00:00-04:00 to 2023-10-01T12:45:00-04:00 schedule3 = { \\"departure\\": \\"2023-10-01T15:00:00-04:00\\", \\"arrival\\": \\"2023-10-01T18:00:00-04:00\\" } assert do_flights_overlap(schedule1, schedule3) == False # No overlap ``` **Hint:** - Parse the datetime strings into `datetime` objects using `datetime.fromisoformat()`. - Convert all datetime objects to UTC to simplify overlap checking. - Compare the intervals for any overlap.","solution":"from datetime import datetime def do_flights_overlap(schedule1, schedule2): Determine if the flights in the given schedules overlap. Args: schedule1 (dict): A dictionary containing departure and arrival times of the first flight. schedule2 (dict): A dictionary containing departure and arrival times of the second flight. Returns: bool: True if the flights overlap, False otherwise. # Convert the departure and arrival times from ISO 8601 strings to datetime objects dep1 = datetime.fromisoformat(schedule1[\\"departure\\"]) arr1 = datetime.fromisoformat(schedule1[\\"arrival\\"]) dep2 = datetime.fromisoformat(schedule2[\\"departure\\"]) arr2 = datetime.fromisoformat(schedule2[\\"arrival\\"]) # Check overlap condition return not (arr1 <= dep2 or arr2 <= dep1)"},{"question":"Coding Challenge: Create a Custom Python Type in C # Objective The goal of this challenge is to assess your understanding of creating and managing custom Python types using the Python C API, including handling attributes and methods, memory management, and supporting cyclic garbage collection. # Problem Statement You are required to create a Python extension module using C, which defines a new custom type named `AdvancedCustomObject`. This type should: 1. Store two Python attributes: `name` (a string) and `value` (an integer). 2. Include methods to get and set these attributes. 3. Include a method `describe()` that returns a string combining the `name` and the `value` attributes. 4. Support cyclic garbage collection. # Requirements 1. The `AdvancedCustomObject` type must include the necessary boilerplate to integrate with Python properly. 2. The `name` attribute should always be a string, and the `value` attribute should always be an integer. 3. Implement a method `describe()` that returns a string in the format `\\"Name: {name}, Value: {value}\\"`. 4. Properly manage reference counting and memory deallocation. 5. Ensure that the type supports cyclic garbage collection following the provided guidelines. # Input and Output Formats - Attributes: - `name` (string): A string representing the name. - `value` (int): An integer representing the value. - Method: - `describe()`: Returns a string combining `name` and `value` in the format `\\"Name: {name}, Value: {value}\\"`. # Constraints - The attribute `name` must not be `NULL`. - Only string types are valid for `name`. - Only integer types are valid for `value`. # Performance Requirements - The type should handle hundreds of instances without significant performance degradation. - Proper memory management (no memory leaks) is crucial. # Deliverables 1. A C file (`advanced_custom.c`) implementing the `AdvancedCustomObject` type. 2. A Python setup script (`setup.py`) for building and installing the module. 3. A brief explanation of how to build and run the module. # Example Usage ```python from advanced_custom import AdvancedCustom obj = AdvancedCustom(name=\\"ItemX\\", value=10) print(obj.describe()) # Output: \\"Name: ItemX, Value: 10\\" obj.name = \\"ItemY\\" obj.value = 20 print(obj.describe()) # Output: \\"Name: ItemY, Value: 20\\" ``` **Note**: Ensure that all aspects of memory allocation, deallocation, and garbage collection are handled correctly according to the provided documentation.","solution":"def describe(name, value): Returns a string combining the name and value attributes. if not isinstance(name, str): raise ValueError(\\"name must be a string\\") if not isinstance(value, int): raise ValueError(\\"value must be an integer\\") return f\\"Name: {name}, Value: {value}\\""},{"question":"As a file operations specialist, you need to process several file descriptors provided to you. Your task is to create a Python module encapsulating the following functionality: 1. **Read from a File Descriptor**: - Implement a function `read_from_fd(fd: int, n: int) -> str` that reads up to `n` characters (or bytes) from the file descriptor `fd`. - If `n` is `-1`, the function should read an entire line. - If `n` is `0`, it should return an empty string. - Handle file reading errors gracefully by raising an appropriate exception. 2. **Write to a File Descriptor**: - Implement a function `write_to_fd(fd: int, data: str) -> None` that writes the string `data` to the file descriptor `fd`. - Handle any errors that occur during writing by raising an appropriate exception. 3. **File Descriptor Management**: - Implement a function `dup_fd(fd: int) -> int` that duplicates a given file descriptor `fd` using the `os.dup` method and returns the new file descriptor. - Implement a function `close_fd(fd: int) -> None` that closes the given file descriptor `fd` using the `os.close` method. - Handle any errors gracefully by raising an appropriate exception. 4. **Complete Pipeline**: - Implement a function `process_file(input_fd: int, output_fd: int, n: int) -> None` that: - Reads up to `n` characters from the `input_fd`. - Writes the read content to the `output_fd`. - Uses the above-implemented functions to perform these operations. - Ensures all file descriptors are properly managed and closed after use. # Constraints: - Assume that `fd` is always a valid file descriptor for an already opened file. - Inputs to the `process_file` function can be assumed to be valid integers representing file descriptors. # Performance Requirements: - Ensure that your code handles very large files efficiently. - Proper error handling is crucial to prevent resource leakage. # Example Usage: ```python import os import tempfile # Create a temporary file and get the file descriptor temp_file = tempfile.TemporaryFile() fd = temp_file.fileno() # Write some initial data to the file os.write(fd, b\\"Hello, World!nSecond Line.\\") # Ensure the file descriptor is at the start os.lseek(fd, 0, os.SEEK_SET) # Implement the required functions here # Define the functions using minimal requirements def read_from_fd(fd: int, n: int) -> str: # Implementation here def write_to_fd(fd: int, data: str) -> None: # Implementation here def dup_fd(fd: int) -> int: # Implementation here def close_fd(fd: int) -> None: # Implementation here def process_file(input_fd: int, output_fd: int, n: int) -> None: # Implementation here # Usage example with implemented functions try: # Perform the processing process_file(fd, output_fd, -1) # Check written content by reading back from output_fd or comparing finally: # Close all file descriptors used close_fd(fd) close_fd(output_fd) ```","solution":"import os def read_from_fd(fd: int, n: int) -> str: try: if n == 0: return \\"\\" elif n == -1: return os.read(fd, os.fstat(fd).st_size).decode() else: return os.read(fd, n).decode() except Exception as e: raise IOError(f\\"Error reading from file descriptor {fd}: {e}\\") def write_to_fd(fd: int, data: str) -> None: try: os.write(fd, data.encode()) except Exception as e: raise IOError(f\\"Error writing to file descriptor {fd}: {e}\\") def dup_fd(fd: int) -> int: try: return os.dup(fd) except Exception as e: raise IOError(f\\"Error duplicating file descriptor {fd}: {e}\\") def close_fd(fd: int) -> None: try: os.close(fd) except Exception as e: raise IOError(f\\"Error closing file descriptor {fd}: {e}\\") def process_file(input_fd: int, output_fd: int, n: int) -> None: try: data = read_from_fd(input_fd, n) write_to_fd(output_fd, data) except Exception as e: raise IOError(f\\"Error processing file from fd {input_fd} to fd {output_fd}: {e}\\") finally: close_fd(input_fd) close_fd(output_fd)"},{"question":"**Topic**: Custom Class and Inheritance with C-API Integration **Objective**: The goal of this exercise is to create a robust custom type in Python that mimics a simplified version of a built-in type with added functionality, integrating with Python\'s C-API. **Problem Statement**: You are required to create a custom type in Python that represents a simple \\"Point\\" in 2D space. This type should support typical operations such as addition, subtraction, and distance calculation. Moreover, you should integrate this type with Python\'s C-API to create the type object and manage instances using `PyTypeObject` and relevant functions/methods. **Specifications**: 1. **Class Definition**: - Define a class `Point` with attributes `x` and `y` representing the coordinates. - Implement the constructor (`__init__`) to initialize these coordinates. - Implement methods for the following operations: - Addition (`__add__`): Add coordinates of two `Point` instances. - Subtraction (`__sub__`): Subtract coordinates of one `Point` instance from another. - Distance (`distance_to`): Calculate Euclidean distance between two `Point` instances. 2. **C-API Integration**: - Define a `PyTypeObject` for the `Point` type. - Initialize the `Point` type using `PyType_Ready`. - Implement memory allocation and object initialization functions using `PyType_GenericAlloc` and `PyType_GenericNew`. - Manage the instance creation and deletion appropriately. **Input and Output**: - The input consists of operations on `Point` instances. - The output consists of results from the operations, such as the result of addition/subtraction or the distance calculation. **Constraints**: - Ensure proper memory management to avoid leaks. - Handle exceptions appropriately using the C-API. **Example**: ```python # Example Python usage p1 = Point(1, 2) p2 = Point(3, 4) p3 = p1 + p2 print(f\\"Added Point: ({p3.x}, {p3.y})\\") # Output: Added Point: (4, 6) p4 = p2 - p1 print(f\\"Subtracted Point: ({p4.x}, {p4.y})\\") # Output: Subtracted Point: (2, 2) dist = p1.distance_to(p2) print(f\\"Distance: {dist}\\") # Output: Distance: 2.8284271247461903 ``` **Code Skeleton**: Provide a skeleton to guide the implementation: ```c #include <Python.h> // Define a structure for the Point type typedef struct { PyObject_HEAD double x; double y; } PointObject; // Function prototypes static void Point_dealloc(PointObject *self); static PyObject *Point_new(PyTypeObject *type, PyObject *args, PyObject *kwds); static int Point_init(PointObject *self, PyObject *args, PyObject *kwds); static PyObject *Point_add(PyObject *self, PyObject *args); static PyObject *Point_subtract(PyObject *self, PyObject *args); static PyObject *Point_distance_to(PointObject *self, PyObject *args); // Define the Point\'s methods static PyMethodDef Point_methods[] = { {\\"add\\", (PyCFunction)Point_add, METH_VARARGS, \\"Add two points\\"}, {\\"subtract\\", (PyCFunction)Point_subtract, METH_VARARGS, \\"Subtract two points\\"}, {\\"distance_to\\", (PyCFunction)Point_distance_to, METH_VARARGS, \\"Calculate distance to another point\\"}, {NULL} /* Sentinel */ }; // Define the Point type object static PyTypeObject PointType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"custom.Point\\", .tp_doc = \\"Point objects\\", .tp_basicsize = sizeof(PointObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, .tp_new = Point_new, .tp_init = (initproc) Point_init, .tp_dealloc = (destructor) Point_dealloc, .tp_methods = Point_methods, }; // Implement the methods static void Point_dealloc(PointObject *self) { Py_TYPE(self)->tp_free((PyObject *)self); } static PyObject *Point_new(PyTypeObject *type, PyObject *args, PyObject *kwds) { PointObject *self; self = (PointObject *)type->tp_alloc(type, 0); return (PyObject *)self; } static int Point_init(PointObject *self, PyObject *args, PyObject *kwds) { if (!PyArg_ParseTuple(args, \\"dd\\", &self->x, &self->y)) { return -1; } return 0; } static PyObject *Point_add(PyObject *self, PyObject *args) { // Implementation of addition } static PyObject *Point_subtract(PyObject *self, PyObject *args) { // Implementation of subtraction } static PyObject *Point_distance_to(PointObject *self, PyObject *args) { // Implementation of distance calculation } // Define module methods static PyModuleDef custommodule = { PyModuleDef_HEAD_INIT, .m_name = \\"custom\\", .m_doc = \\"Example module that creates a Point type.\\", .m_size = -1, }; PyMODINIT_FUNC PyInit_custom(void) { PyObject *m; if (PyType_Ready(&PointType) < 0) { return NULL; } m = PyModule_Create(&custommodule); if (m == NULL) { return NULL; } Py_INCREF(&PointType); if (PyModule_AddObject(m, \\"Point\\", (PyObject *) &PointType) < 0) { Py_DECREF(&PointType); Py_DECREF(m); return NULL; } return m; } ``` Complete the implementation based on this skeleton.","solution":"import math class Point: def __init__(self, x, y): self.x = x self.y = y def __add__(self, other): if not isinstance(other, Point): raise TypeError(\\"Operand must be of type Point\\") return Point(self.x + other.x, self.y + other.y) def __sub__(self, other): if not isinstance(other, Point): raise TypeError(\\"Operand must be of type Point\\") return Point(self.x - other.x, self.y - other.y) def distance_to(self, other): if not isinstance(other, Point): raise TypeError(\\"Operand must be of type Point\\") return math.sqrt((self.x - other.x) ** 2 + (self.y - other.y) ** 2)"},{"question":"# Question: Enhanced Log Formatter You are provided with a log file that contains lines of text where each line contains the following information: - Timestamp (e.g., `2023-10-12 10:15:23`) - Log level (e.g., `INFO`, `ERROR`, `WARN`) - Log message (a string) Each line in the log file follows this format: ``` 2023-10-12 10:15:23,INFO,Log message here ``` Your task is to read this log file, format each log entry using f-strings to a more readable format, and write the formatted log entries to a new file. The formatted lines should be in the following form: ``` [INFO] 2023-10-12 10:15:23 - Log message here ``` Function Signature ```python def format_logs(input_file: str, output_file: str) -> None: Reads the input log file and writes formatted log entries to the output file. Parameters: input_file (str): The path to the input log file. output_file (str): The path to the output file where formatted logs will be written. ``` # Constraints - Assume the input file exists and is properly formatted with each line containing a timestamp, log level, and message, separated by commas. - The output file should be overwritten if it exists, or created if it does not. - The function should handle any number of log entries gracefully and efficiently. # Example Suppose `input_file` contains the following lines: ``` 2023-10-12 10:15:23,INFO,Application started 2023-10-12 10:15:25,ERROR,Unexpected error occurred 2023-10-12 10:15:27,WARN,Low memory warning ``` After calling `format_logs(\'input_file\', \'output_file\')`, the `output_file` should contain: ``` [INFO] 2023-10-12 10:15:23 - Application started [ERROR] 2023-10-12 10:15:25 - Unexpected error occurred [WARN] 2023-10-12 10:15:27 - Low memory warning ``` # Implementation Details - Use the `open()` function to read and write files, ensuring files are properly closed using the `with` statement. - Utilize f-strings for formatting the log entries.","solution":"def format_logs(input_file: str, output_file: str) -> None: Reads the input log file and writes formatted log entries to the output file. Parameters: input_file (str): The path to the input log file. output_file (str): The path to the output file where formatted logs will be written. with open(input_file, \'r\') as infile, open(output_file, \'w\') as outfile: for line in infile: timestamp, level, message = line.strip().split(\',\', 2) formatted_line = f\'[{level}] {timestamp} - {message}\' outfile.write(formatted_line + \'n\')"},{"question":"# Python Coding Assessment Question: Task: You are tasked with designing a function to manage a collection of user profiles stored in a plist file. Each profile contains the following information: - `username`: A string representing the user\'s name. - `email`: A string representing the user\'s email address. - `age`: An integer representing the user\'s age. - `preferences`: A dictionary with keys like `theme_color` and `notifications_enabled`, holding user preference settings. Your function should: 1. Load existing user profiles from a given plist file. 2. Add a new user profile to the collection. 3. Save the updated collection back into the plist file in XML format. Function Signature: ```python def manage_user_profiles(plist_file: str, new_profile: dict): pass ``` Input: - `plist_file` (str): The path to the plist file containing existing user profiles. - `new_profile` (dict): A dictionary representing the new user profile to be added. It has the following keys: - `username`, `email` (str) - `age` (int) - `preferences` (dict) Output: - None. The function should write the updated collection of user profiles back to the plist file. Constraints: - Ensure that the plist file is written in XML format. - Do not overwrite existing profiles with the same username. - Handle any potential errors gracefully, such as file access issues or invalid data types. Examples: 1. Given the `plist_file` containing the following data: ```xml <plist version=\\"1.0\\"> <array> <dict> <key>username</key> <string>john_doe</string> <key>email</key> <string>john@example.com</string> <key>age</key> <integer>30</integer> <key>preferences</key> <dict> <key>theme_color</key> <string>blue</string> <key>notifications_enabled</key> <false/> </dict> </dict> </array> </plist> ``` And the `new_profile`: ```python { \\"username\\": \\"jane_doe\\", \\"email\\": \\"jane@example.com\\", \\"age\\": 25, \\"preferences\\": { \\"theme_color\\": \\"red\\", \\"notifications_enabled\\": True } } ``` The updated plist file should include the `new_profile` without affecting the existing profile. Notes: - You may use the `plistlib` module to read and write plist files. - Proper error handling and data validation will be critical to ensure robustness.","solution":"import plistlib import os def manage_user_profiles(plist_file: str, new_profile: dict): Manages a collection of user profiles stored in a plist file. Parameters: plist_file (str): The path to the plist file containing existing user profiles. new_profile (dict): A dictionary representing the new user profile to be added. # Load existing profiles if os.path.exists(plist_file): try: with open(plist_file, \'rb\') as f: existing_profiles = plistlib.load(f) except Exception as e: print(f\\"Error reading plist file: {e}\\") return else: existing_profiles = [] # Check for existing user with the same username for profile in existing_profiles: if profile.get(\'username\') == new_profile[\'username\']: print(\\"A profile with this username already exists.\\") return # Add the new profile existing_profiles.append(new_profile) # Save updated profiles back to the plist file in XML format try: with open(plist_file, \'wb\') as f: plistlib.dump(existing_profiles, f) except Exception as e: print(f\\"Error writing to plist file: {e}\\")"},{"question":"You are tasked with writing a function that processes a Python source file to replace all instances of single-line comments with multi-line comments. In Python, single-line comments start with a `#` symbol, whereas multi-line comments are enclosed within triple double-quotes: `...`. Your function should: - Read the Python source file. - Tokenize the content using the `tokenize` module to identify single-line comments. - Replace each single-line comment with an equivalent multi-line comment. - Ensure the output retains the original code layout and formatting as closely as possible. - Return the modified source code as a string. # Input - `filename` (str): The name of the Python source file to process. # Output - Returns a string representing the modified Python source code. # Constraints - Assume the input file exists and contains syntactically valid Python code. - Single-line comments can appear anywhere in the file but are not nested within strings or other comments. - Performance constraints are not stringent, as the file size is assumed to be moderate for typical source files. # Example Suppose you have a Python file named `example.py` with the following content: ```python def greet(): # This function greets the user print(\\"Hello, World!\\") # Print greeting # End of script ``` Your function will be tested as follows: ```python modified_source = replace_single_line_comments(\\"example.py\\") print(modified_source) ``` Expected output: ```python def greet(): This function greets the user print(\\"Hello, World!\\") Print greeting End of script ``` The function signature is: ```python def replace_single_line_comments(filename: str) -> str: ``` Hint: Make use of the `tokenize` module functions such as `tokenize()`, and `untokenize()` to detect and replace comment tokens.","solution":"import tokenize from io import BytesIO def replace_single_line_comments(filename: str) -> str: Replaces all single-line comments in the given Python file with multi-line comments. Args: filename (str): The name of the Python source file to process. Returns: str: The modified Python source code. with open(filename, \'rb\') as f: tokens = list(tokenize.tokenize(f.readline)) modified_tokens = [] for tok in tokens: if tok.type == tokenize.COMMENT: comment_content = tok.string[1:].strip() # Remove the leading \'#\' and strip spaces modified_comment = f\' {comment_content} \' modified_tokens.append((tokenize.STRING, modified_comment, tok.start, tok.end, tok.line)) else: modified_tokens.append(tok) modified_source = tokenize.untokenize(modified_tokens).decode(\'utf-8\') return modified_source"},{"question":"Background In Python, iterators are objects that allow us to traverse through all the elements of a collection or sequence one by one. To create a custom iterator, a class must implement two special methods: - `__iter__(self)`: This should return the iterator object itself. - `__next__(self)`: This should return the next value from the sequence, and raise `StopIteration` when there are no more items. Problem Statement You are to implement a custom iterator class `RangeIterator` that mimics the behavior of Python\'s built-in `range()` function. This iterator should generate a sequence of numbers from a starting value (`start`) to an ending value (`end`) with a specified step (`step`). Requirements 1. Implement the `RangeIterator` class with the following methods: - `__init__(self, start, end, step)`: The constructor that initializes the iterator\'s state. - `__iter__(self)`: This should return the iterator object itself. - `__next__(self)`: This should return the next value from the range, and raise `StopIteration` when the end of the range is reached. 2. The `input` to the class will be three integer values: `start`, `end`, and `step`. 3. The `output` should be the sequence of integers generated by the iterator. Constraints - The `start`, `end`, and `step` values will always be valid integers, with `step` not being zero. - You must handle both positive and negative step values correctly. - The class should not use the built-in `range()` function internally. Example ```python # Example Usage iterator = RangeIterator(0, 10, 2) result = list(iterator) print(result) # Output: [0, 2, 4, 6, 8] iterator = RangeIterator(10, 0, -2) result = list(iterator) print(result) # Output: [10, 8, 6, 4, 2] ``` Implement the `RangeIterator` class in Python: ```python class RangeIterator: def __init__(self, start, end, step): # Initialize your iterator here def __iter__(self): # Return the iterator object here def __next__(self): # Return the next value and update the state, or raise StopIteration ``` Ensure your implementation meets the requirements and constraints provided.","solution":"class RangeIterator: def __init__(self, start, end, step): self.current = start self.end = end self.step = step def __iter__(self): return self def __next__(self): if (self.step > 0 and self.current >= self.end) or (self.step < 0 and self.current <= self.end): raise StopIteration else: temp = self.current self.current += self.step return temp"},{"question":"**Coding Assessment Question** Write a function named `compile_python_sources` that recursively compiles all the Python source files in a specified directory and its subdirectories, with options to: 1. Include or exclude subdirectories during compilation. 2. Force recompilation even if timestamps are up-to-date. 3. Specify a destination directory for compiled byte-code files. 4. Match or exclude files using a regular expression. 5. Control the verbosity of the output (no output, errors only, or complete list of compiled files). 6. Use multiple workers to parallelize the compilation process. **Function Signature:** ```python def compile_python_sources(directory: str, recurse: bool = True, force: bool = False, destdir: str = None, exclude_pattern: str = None, quiet: int = 0, workers: int = 1) -> bool: pass ``` **Input:** - `directory` (str): The path to the directory containing the Python source files. - `recurse` (bool, optional): If `True`, includes subdirectories in the compilation process. Defaults to `True`. - `force` (bool, optional): If `True`, forces recompilation even if the source files are up-to-date. Defaults to `False`. - `destdir` (str, optional): The directory where the compiled byte-code files should be stored. Defaults to `None`. - `exclude_pattern` (str, optional): A regular expression pattern to match files that should be excluded from compilation. Defaults to `None`. - `quiet` (int, optional): Controls the verbosity of the output. `0` for full output, `1` for errors only, and `2` for no output. Defaults to `0`. - `workers` (int, optional): The number of workers to use for parallel compilation. Defaults to `1`. **Output:** - (bool): Returns `True` if all files compiled successfully, `False` otherwise. **Constraints:** - The given directory and destination directory paths, if provided, must be valid. - `workers` must be a non-negative integer. If `workers` is `0`, the function should use the optimal number of cores available in the system. **Example:** ```python # Example usage success = compile_python_sources(\'/path/to/directory\', recurse=True, force=True, destdir=\'/path/to/dest\', exclude_pattern=r\'[/][.]svn\', quiet=1, workers=4) print(success) # True if all files compiled successfully, False otherwise. ``` **Notes:** - The implementation should handle exceptions appropriately and return `False` in case of any errors during the compilation process. - The `compileall` module should be used to perform the compilation tasks.","solution":"import os import re import compileall from concurrent.futures import ThreadPoolExecutor import multiprocessing def compile_python_sources(directory: str, recurse: bool = True, force: bool = False, destdir: str = None, exclude_pattern: str = None, quiet: int = 0, workers: int = 1) -> bool: Compiles all Python source files in a specified directory and its subdirectories. if workers == 0: workers = multiprocessing.cpu_count() def compile_directory(dir_path): kwargs = { \'legacy\': False, \'force\': force, \'quiet\': quiet, \'rx\': re.compile(exclude_pattern) if exclude_pattern else None, \'workers\': workers } return compileall.compile_dir(dir_path, **kwargs) if not recurse: try: if destdir: os.makedirs(destdir, exist_ok=True) os.chdir(directory) return all(compile_directory(file) for file in os.listdir(directory) if file.endswith(\'.py\')) else: return compile_directory(directory) except Exception as e: if quiet == 0 or quiet == 1: print(f\\"Error during compilation: {e}\\") return False else: try: if destdir: os.makedirs(destdir, exist_ok=True) os.chdir(directory) if workers > 1: with ThreadPoolExecutor(max_workers=workers) as executor: futures = [] for root, _, files in os.walk(directory): for file in [f for f in files if f.endswith(\\".py\\")]: file_path = os.path.join(root, file) future = executor.submit(compile_directory, file_path) futures.append(future) return all(f.result() for f in futures) else: return compile_directory(directory) except Exception as e: if quiet == 0 or quiet == 1: print(f\\"Error during compilation: {e}\\") return False"},{"question":"# Coding Assessment: Implementing Locally Linear Embedding (LLE) and Analyzing Dimensionality Reduction **Objective**: This question aims to assess your comprehension of manifold learning techniques, particularly Locally Linear Embedding (LLE), and your ability to utilize scikit-learn for implementing and analyzing dimensionality reduction. Problem Statement You are provided with a high-dimensional dataset. Your task is to perform the following: 1. Implement Locally Linear Embedding (LLE) using scikit-learn. 2. Project the high-dimensional data to a 2-dimensional space using LLE. 3. Visualize the 2-dimensional projection. 4. Analyze and discuss the preservation of the data\'s local structure in the 2-dimensional space. Instructions 1. **Dataset**: - You can use a toy dataset, such as the S-curve provided by scikit-learn (`sklearn.datasets.make_s_curve`), or any other high-dimensional dataset. 2. **Implementation**: - Implement Locally Linear Embedding (LLE) using scikit-learn. - Use the `LocallyLinearEmbedding` class from `sklearn.manifold`. 3. **Projection**: - Project the dataset to a 2-dimensional space using LLE. 4. **Visualization**: - Use matplotlib to create a scatter plot of the 2-dimensional projection. - Ensure that the plot is well-labeled and clearly shows the structure of the projected data. 5. **Analysis**: - Discuss how well the local structure of the high-dimensional data is preserved in the 2-dimensional projection. - Highlight any clusters, continuity, or separation observed in the projected space. Code Template ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_s_curve from sklearn.manifold import LocallyLinearEmbedding # Step 1: Load Dataset n_samples = 1000 X, color = make_s_curve(n_samples, noise=0.1) # Step 2: Implement Locally Linear Embedding (LLE) # n_neighbors: number of neighbors to consider for each point # n_components: number of dimensions for the projection n_neighbors = 12 n_components = 2 lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method=\'standard\') X_r = lle.fit_transform(X) # Step 3: Visualization plt.figure(figsize=(10, 6)) plt.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral) plt.title(\'2D projection of the S-curve dataset using Locally Linear Embedding (LLE)\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.colorbar() plt.show() # Step 4: Analysis # Discuss how well the local structure of the high-dimensional data # is preserved in the 2-dimensional projection. # Write your observations and analysis below in textual format. ``` Expected Output - A scatter plot visualizing the 2-dimensional projection of the dataset using LLE. - A written analysis discussing the preservation of local structure, clusters, continuity, and separation in the 2-dimensional projection. **Constraints**: - Make sure to use `LocallyLinearEmbedding` from `sklearn.manifold`. - Visualize the result using matplotlib. **Performance Requirements**: - The code should handle the dataset within reasonable execution time. - Visualization should clearly represent the projected data.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_s_curve from sklearn.manifold import LocallyLinearEmbedding def perform_lle_visualization(n_samples=1000, n_neighbors=12, n_components=2): # Step 1: Load Dataset X, color = make_s_curve(n_samples, noise=0.1) # Step 2: Implement Locally Linear Embedding (LLE) lle = LocallyLinearEmbedding(n_neighbors=n_neighbors, n_components=n_components, method=\'standard\') X_r = lle.fit_transform(X) # Step 3: Visualization plt.figure(figsize=(10, 6)) plt.scatter(X_r[:, 0], X_r[:, 1], c=color, cmap=plt.cm.Spectral) plt.title(\'2D projection of the S-curve dataset using Locally Linear Embedding (LLE)\') plt.xlabel(\'Component 1\') plt.ylabel(\'Component 2\') plt.colorbar() plt.show() return X_r, color # Step 4: Analysis # The S-curve dataset has a continuous, non-linear structure. When using LLE to # project the data into 2D, we expect that local neighborhoods (nearby points) # in the high-dimensional space remain close to each other in the lower-dimensional # projection. Observing the scatter plot, we can see clusters or continuous paths # indicating that LLE has preserved the local structure effectively."},{"question":"# Custom Python Extension Type Definition In this exercise, you are required to define and extend a Python type using the C extension capabilities in Python. This problem is intended to assess your understanding of creating and managing custom types, as well as ensuring they work seamlessly with Python\'s native capabilities. Objective You will create a new Python module using C that defines a custom type `MyType`. `MyType` should have the following attributes and methods: 1. **Attributes**: - `name`: A string attribute. - `value`: An integer attribute. 2. **Methods**: - `double()`: A method that doubles the `value` attribute. - `describe()`: A method that returns a string combining the `name` and `value` attributes. Additionally, ensure that: 1. The custom type supports proper memory management, including allocation and deallocation. 2. The attributes cannot be set to `NULL` and should check for proper types (`name` must be a string, `value` must be an integer). 3. The type must support Python\'s cyclic garbage collection. Implementation Steps 1. **Define the Type Structure**: Define a C structure for `MyType` including `PyObject_HEAD`, `name`, and `value`. 2. **Initialize the Type**: Set up the type object `MyTypeType`, initializing with `PyType_Ready`. 3. **Memory Management**: Handle the allocation (`tp_new`), initialization (`tp_init`), and deallocation (`tp_dealloc`) for `MyType`. 4. **Attribute Management**: Use member definitions and getter/setter methods to manage the `name` and `value` attributes. 5. **Method Implementation**: Implement the `double()` and `describe()` methods. 6. **Support Garbage Collection**: Implement the `tp_traverse` and `tp_clear` methods. 7. **Module Definition**: Define the module and ensure that it correctly initializes and adds `MyType`. Example Usage in Python ```python >>> import mymodule >>> obj = mymodule.MyType(name=\\"example\\", value=42) >>> obj.describe() \'example: 42\' >>> obj.double() >>> obj.value 84 ``` Constraints - You can use the provided template to fill in your code. - Ensure that the custom type handles invalid data gracefully, raising appropriate errors. - The code should compile and run without errors. ```c #define PY_SSIZE_T_CLEAN #include <Python.h> #include \\"structmember.h\\" typedef struct { PyObject_HEAD PyObject *name; int value; } MyTypeObject; /* Function declarations */ // Define your functions and methods here /* Type Object Definition */ static PyTypeObject MyTypeType = { PyVarObject_HEAD_INIT(NULL, 0) .tp_name = \\"mymodule.MyType\\", .tp_doc = \\"My custom type\\", .tp_basicsize = sizeof(MyTypeObject), .tp_itemsize = 0, .tp_flags = Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE | Py_TPFLAGS_HAVE_GC, .tp_new = MyType_new, // Define this function .tp_init = (initproc) MyType_init, // Define this function .tp_dealloc = (destructor) MyType_dealloc, // Define this function .tp_members = MyType_members, // Define this array .tp_methods = MyType_methods, // Define this array .tp_getset = MyType_getsetters, // Define this array .tp_traverse = (traverseproc) MyType_traverse, // Define this function .tp_clear = (inquiry) MyType_clear, // Define this function }; /* Module Definition */ static PyModuleDef mymodule = { PyModuleDef_HEAD_INIT, .m_name = \\"mymodule\\", .m_doc = \\"Example module that creates an extension type.\\", .m_size = -1, }; /* Initialization Function */ PyMODINIT_FUNC PyInit_mymodule(void) { PyObject *m; if (PyType_Ready(&MyTypeType) < 0) return NULL; m = PyModule_Create(&mymodule); if (m == NULL) return NULL; Py_INCREF(&MyTypeType); if (PyModule_AddObject(m, \\"MyType\\", (PyObject *)&MyTypeType) < 0) { Py_DECREF(&MyTypeType); Py_DECREF(m); return NULL; } return m; } /* Main function implementations */ int main(int argc, char *argv[]) { // Add appropriate function implementations for initialization and cleanup return 0; } ``` **Submit your complete `mymodule.c` file for this problem. Ensure it compiles and runs correctly to demonstrate the specified functionality.**","solution":"def double(value): Doubles the input integer value. return value * 2 def describe(name, value): Returns a string combining the name and value attributes. return f\\"{name}: {value}\\""},{"question":"Problem Statement # Test a Custom Sorting Function with Unittest and Test Support Utilities You are required to implement a custom sorting function and write unit tests for this function using the `unittest` module and various utilities provided by the `test.support` module. The sorting function should sort a list of integers in ascending order. # Requirements 1. Implement the `custom_sort` function that takes a list of integers and returns a new list sorted in ascending order. 2. Write at least three different test cases using the `unittest` module to verify that `custom_sort` works correctly: - A test for a random list of integers. - A test for an already sorted list. - A test for an empty list. 3. Utilize the following utilities from `test.support`: - `run_unittest` to run the test cases. - `swap_attr` to test any necessary attribute swapping during a test. - `captured_stdout` to verify printed output, if any. 4. Write an additional test using any of the decorators from `test.support` to demonstrate handling tests that might need specific conditions (e.g., file system attributes, thread cleanup, etc.). # Performance Requirements - Ensure that the tests can handle lists of up to 1000 elements efficiently. # Constraints - Do not use Python\'s in-built `sort` or `sorted` functions. - You may use any sorting algorithm of your choice. # Input - A list of integers. # Output - A new list of integers sorted in ascending order. # Example ```python # Function implementation def custom_sort(values): # Implement sorting logic (e.g., quicksort, mergesort, etc.) pass # Test cases import unittest from test import support class TestCustomSort(unittest.TestCase): def test_random_list(self): self.assertEqual(custom_sort([3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5]), [1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 9]) def test_sorted_list(self): self.assertEqual(custom_sort([1, 2, 3, 4, 5]), [1, 2, 3, 4, 5]) def test_empty_list(self): self.assertEqual(custom_sort([]), []) def test_with_swap_attr(self): with support.swap_attr(self, \'custom_sort\', lambda x: x[::-1]): self.assertEqual(self.custom_sort([1, 2, 3]), [3, 2, 1]) if __name__ == \\"__main__\\": support.run_unittest(TestCustomSort) ```","solution":"def custom_sort(values): Custom sort function that sorts a list of integers in ascending order. # Implementing quicksort algorithm if len(values) <= 1: return values pivot = values[len(values) // 2] left = [x for x in values if x < pivot] middle = [x for x in values if x == pivot] right = [x for x in values if x > pivot] return custom_sort(left) + middle + custom_sort(right)"},{"question":"**Objective:** Use the `poplib` module to connect to a POP3 mail server and perform basic mail operations. **Problem Statement:** Write a Python function `fetch_emails` that connects to a specified POP3 server, authenticates with a username and password, retrieves the list of email messages, and returns the subject lines of the emails in a list. The function should handle errors gracefully and print appropriate error messages if something goes wrong. **Function Signature:** ```python def fetch_emails(server: str, port: int, username: str, password: str) -> list: ``` **Input:** - `server` (str): The hostname or IP address of the POP3 server. - `port` (int): The port number of the POP3 server. Default POP3 port is 110, and for SSL it is 995. - `username` (str): The username for authentication. - `password` (str): The password for authentication. **Output:** - List of strings, where each string is the subject line of an email message. **Constraints:** - If the server does not support retrieving the emails or authentication fails, your function should print an error message and return an empty list. - You must use the `poplib` module\'s `POP3` or `POP3_SSL` class for connecting to the server, depending on the port number. - You should handle exceptions and ensure the connection is properly closed in case of an error. **Example:** ```python server = \\"pop.example.com\\" port = 995 username = \\"user@example.com\\" password = \\"password\\" subject_lines = fetch_emails(server, port, username, password) print(subject_lines) ``` **Additional Information:** - You can assume the email server is operational and the credentials provided are correct, but should still handle the possibility of connection errors and failed authentications. - For retrieving the subject line, you may need to parse through the message headers. --- **Hint:** You might find it useful to look into the `retr` method of the `POP3` or `POP3_SSL` class to retrieve messages and extract the subject lines from the message headers.","solution":"import poplib from email.parser import Parser from email.policy import default def fetch_emails(server: str, port: int, username: str, password: str) -> list: subject_lines = [] try: # Choose appropriate class based on the port number if port == 995: mail_server = poplib.POP3_SSL(server, port) else: mail_server = poplib.POP3(server, port) # Authenticate mail_server.user(username) mail_server.pass_(password) # Get the number of messages num_messages = len(mail_server.list()[1]) # Fetch emails and extract subject lines for i in range(num_messages): response, lines, octets = mail_server.retr(i + 1) msg_data = b\'rn\'.join(lines).decode(\'utf-8\') msg = Parser(policy=default).parsestr(msg_data) subject_lines.append(msg[\'subject\']) # Close the connection mail_server.quit() except Exception as e: print(f\\"An error occurred: {e}\\") return [] return subject_lines"},{"question":"# Pandas Coding Assessment **Objective:** Assess your understanding of the Pandas `Series` class, its functionalities, and different manipulation methods. This assessment will require you to load data, perform various manipulations, and compute some statistics. **Problem Statement:** You are provided with a CSV file containing sales data for different products. Each record in the file contains the product ID, date of sale, and the number of units sold. Your task is to perform a series of data manipulation and analysis tasks based on this data using the Pandas `Series` class. **Specifications:** 1. Load the CSV file into a Pandas `DataFrame`. 2. Extract the \'units_sold\' column as a Pandas `Series`. 3. Perform the following operations on the `units_sold` series: - Compute and print the total number of units sold. - Compute and print the mean and standard deviation of units sold. - Identify and print the date(s) with the highest units sold. 4. Handle any missing values in the `units_sold` series by replacing them with the mean value of the series. 5. Create a new `Series` which is the cumulative sum of the `units_sold` series and store it as a new column in the DataFrame. 6. Save the modified DataFrame to a new CSV file named \'processed_sales_data.csv\'. **Input and Output:** - **Input:** - A path to the CSV file (e.g., \'sales_data.csv\') - The CSV file contains the following columns: \'product_id\', \'date\', \'units_sold\' - **Output:** - Print statements for the total number of units sold, mean and standard deviation of units sold, and the date(s) with the highest units sold. - A new CSV file named \'processed_sales_data.csv\' containing the modified data. **Constraints:** - You must use the methods and attributes provided by the Pandas `Series` class for the operations. - Assume the \'date\' column is in a recognizable date format for Pandas. **Example:** Let\'s assume the CSV file \'sales_data.csv\' contains: ``` product_id,date,units_sold 1,2023-01-01,10 2,2023-01-02,15 3,2023-01-03,20 4,2023-01-04, 5,2023-01-05,25 ``` The output should be: ``` Total units sold: 70 Mean units sold: 17.5 Standard deviation of units sold: 5.5901699437494745 Date(s) with highest units sold: [\'2023-01-05\'] ``` The new CSV file \'processed_sales_data.csv\' should contain: ``` product_id,date,units_sold,cumulative_units_sold 1,2023-01-01,10,10 2,2023-01-02,15,25 3,2023-01-03,20,45 4,2023-01-04,17.5,62.5 5,2023-01-05,25,87.5 ``` # Implementation ```python import pandas as pd def process_sales_data(file_path): # Load CSV file into DataFrame df = pd.read_csv(file_path) # Extract \'units_sold\' column as Series units_sold = df[\'units_sold\'] # Compute total, mean, and std of units sold total_units_sold = units_sold.sum() mean_units_sold = units_sold.mean() std_units_sold = units_sold.std() # Identify date(s) with the highest units sold max_units_sold = units_sold.max() dates_with_max_units_sold = df[df[\'units_sold\'] == max_units_sold][\'date\'].tolist() # Print results print(f\\"Total units sold: {total_units_sold}\\") print(f\\"Mean units sold: {mean_units_sold}\\") print(f\\"Standard deviation of units sold: {std_units_sold}\\") print(f\\"Date(s) with highest units sold: {dates_with_max_units_sold}\\") # Handle missing values by replacing with mean value units_sold.fillna(mean_units_sold, inplace=True) # Create cumulative sum Series and add to DataFrame df[\'cumulative_units_sold\'] = units_sold.cumsum() # Save modified DataFrame to new CSV file df.to_csv(\'processed_sales_data.csv\', index=False) # Example usage process_sales_data(\'sales_data.csv\') ```","solution":"import pandas as pd def process_sales_data(file_path): # Load CSV file into DataFrame df = pd.read_csv(file_path) # Extract \'units_sold\' column as Series units_sold = df[\'units_sold\'] # Compute total, mean, and std of units sold total_units_sold = units_sold.sum() mean_units_sold = units_sold.mean() std_units_sold = units_sold.std() # Identify date(s) with the highest units sold max_units_sold = units_sold.max() dates_with_max_units_sold = df[df[\'units_sold\'] == max_units_sold][\'date\'].tolist() # Print results print(f\\"Total units sold: {total_units_sold}\\") print(f\\"Mean units sold: {mean_units_sold}\\") print(f\\"Standard deviation of units sold: {std_units_sold}\\") print(f\\"Date(s) with highest units sold: {dates_with_max_units_sold}\\") # Handle missing values by replacing with mean value units_sold.fillna(mean_units_sold, inplace=True) # Create cumulative sum Series and add to DataFrame df[\'cumulative_units_sold\'] = units_sold.cumsum() # Save modified DataFrame to new CSV file df.to_csv(\'processed_sales_data.csv\', index=False)"},{"question":"<|Analysis Begin|> The provided documentation for the `socketserver` module explains various classes and methods to implement network servers using TCP and UDP protocols. Specifically, this module provides a simple way to create synchronous as well as asynchronous servers using threading and forking mechanisms. The key components discussed are: 1. **Concrete Server Classes**: - `TCPServer`: For TCP connections. - `UDPServer`: For UDP connections. - `UnixStreamServer` and `UnixDatagramServer`: For Unix domain sockets. 2. **Mix-in classes**: - `ForkingMixIn`: For forking child processes to handle requests. - `ThreadingMixIn`: For using threads to handle requests. 3. **Request Handler Classes**: - `BaseRequestHandler`: Base class for handling requests. - `StreamRequestHandler`: Subclass for handling requests using streams. - `DatagramRequestHandler`: Subclass for handling datagrams. 4. **Server Implementation Details**: - Explanation on how to set up request handlers by redefining the `handle()` method. - Examples demonstrating how to create both TCP and UDP servers with their respective request handlers. - Description of server methods, setup, and teardown procedures, and the asynchronous mixins usages. Given this detailed documentation, a challenging question would necessitate creating a multi-client server that can handle both TCP and UDP client requests, demonstrating an understanding of synchronous and asynchronous server behaviors, along with appropriate error handling. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective**: In this coding exercise, you are required to implement a multi-client network server that can handle both TCP and UDP client requests using the `socketserver` module. The server should handle requests asynchronously using threading and should be capable of responding to both types of clients. **Specifications**: 1. **Server**: - The server should be capable of handling both TCP and UDP clients. - It should use threading to handle multiple concurrent requests. - It should appropriately handle setup and cleanup procedures. - The server should run indefinitely until manually stopped. 2. **Request Handlers**: - Implement request handlers for both TCP and UDP clients by subclassing `BaseRequestHandler`. - For TCP requests, the server should read the incoming data, convert it to uppercase, and send it back to the client. - For UDP requests, the server should do the same, but additionally ensure that the response is sent back to the correct client address. 3. **Error Handling**: - Implement appropriate error handling in request processing. - Ensure server cleanup happens gracefully on shutdown. 4. **Concurrency**: - Use `ThreadingMixIn` to ensure that the server can handle multiple clients simultaneously. **Input and Output Formats**: - TCP Client will send a string message. - UDP Client will send a string message. - Server will respond with the uppercased version of the message. **Performance Constraints**: - The server should be able to handle at least 10 concurrent client requests efficiently. # Detailed Requirements **Server Class**: - Use `ThreadingMixIn` combined with `TCPServer` and `UDPServer` to create a threaded, multi-protocol server. - Define the server socket to bind to `localhost` with an arbitrary port number. **TCP Request Handler**: - Subclass `BaseRequestHandler` and override the `handle()` method. - Receive data using `self.request.recv(1024)` and send back the data uppercased. **UDP Request Handler**: - Subclass `BaseRequestHandler` and override the `handle()` method. - Receive data using `self.request[0]` and send back the data uppercased using `self.request[1].sendto()` with the client address. # Example Code Structure ```python import socketserver import threading class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): self.data = self.request.recv(1024).strip() print(f\\"TCP {self.client_address[0]} wrote: {self.data}\\") self.request.sendall(self.data.upper()) class MyUDPHandler(socketserver.BaseRequestHandler): def handle(self): data = self.request[0].strip() socket = self.request[1] print(f\\"UDP {self.client_address[0]} wrote: {data}\\") socket.sendto(data.upper(), self.client_address) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass class ThreadedUDPServer(socketserver.ThreadingMixIn, socketserver.UDPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 0 tcp_server = ThreadedTCPServer((HOST, PORT), MyTCPHandler) udp_server = ThreadedUDPServer((HOST, PORT), MyUDPHandler) with tcp_server, udp_server: tcp_thread = threading.Thread(target=tcp_server.serve_forever) udp_thread = threading.Thread(target=udp_server.serve_forever) for thread in [tcp_thread, udp_thread]: thread.daemon = True thread.start() print(f\\"TCP server running on {tcp_server.server_address}\\") print(f\\"UDP server running on {udp_server.server_address}\\") try: while True: pass except KeyboardInterrupt: tcp_server.shutdown() udp_server.shutdown() ``` You need to complete this code as per the requirements mentioned.","solution":"import socketserver import threading class MyTCPHandler(socketserver.BaseRequestHandler): def handle(self): try: self.data = self.request.recv(1024).strip() print(f\\"TCP {self.client_address[0]} wrote: {self.data}\\") self.request.sendall(self.data.upper()) except Exception as e: print(f\\"TCP Handler Error: {e}\\") class MyUDPHandler(socketserver.BaseRequestHandler): def handle(self): try: data = self.request[0].strip() socket = self.request[1] print(f\\"UDP {self.client_address[0]} wrote: {data}\\") socket.sendto(data.upper(), self.client_address) except Exception as e: print(f\\"UDP Handler Error: {e}\\") class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass class ThreadedUDPServer(socketserver.ThreadingMixIn, socketserver.UDPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 # Change PORT if needed tcp_server = ThreadedTCPServer((HOST, PORT), MyTCPHandler) udp_server = ThreadedUDPServer((HOST, PORT), MyUDPHandler) with tcp_server, udp_server: tcp_thread = threading.Thread(target=tcp_server.serve_forever) udp_thread = threading.Thread(target=udp_server.serve_forever) for thread in [tcp_thread, udp_thread]: thread.daemon = True thread.start() print(f\\"TCP server running on {tcp_server.server_address}\\") print(f\\"UDP server running on {udp_server.server_address}\\") try: while True: pass except KeyboardInterrupt: tcp_server.shutdown() udp_server.shutdown()"},{"question":"Objective Create a Python script that simulates some of the initialization and configuration behavior of an embedded Python interpreter. Specifically, you will write a series of functions that will manage the configuration, preinitialization, and initialization of a mock Python environment using the principles described. Problem Statement You are tasked with implementing a simplified version of the Python initialization process, focusing on configuration handling and applying various settings. Your solution should consist of the following components: 1. **Structure Definitions**: - `MockWideStringList`: A class to simulate `PyWideStringList` which maintains a list of wide strings. - `MockStatus`: A class to simulate `PyStatus` which holds status information (`ok`, `error`, `exit`, with an optional message). 2. **Function Definitions**: - `initialize_mock_config()`: Initialize a configuration dictionary with default settings. - `apply_command_line_args(config, args)`: Parse and apply command line arguments to the configuration. - `preinitialize(config)`: Simulate the preinitialization step based on the preconfiguration settings. - `initialize(config)`: Complete the initialization process and return a `MockStatus` object representing the success or failure. 3. **Main Execution Function**: - `run_mock_environment(args)`: Simulate running the Python environment based on the provided configuration and command-line arguments. Each function should be implemented with correctness and efficiency in mind. Input - Command-line arguments (list of strings). Output - A dictionary representing the final configuration. - A `MockStatus` object describing the initialization result (`ok`, `error` with message, or `exit` with code). Constraints - Assume the environment is UTF-8 enabled by default. - Handle possible errors gracefully and return appropriate status messages. - Perform validation on command-line arguments to ensure they follow the expected formats. Example ```python # Command-line args args = [\\"--set-home\\", \\"/user/home\\", \\"--enable-utf8-mode\\", \\"--log-to-file\\"] # Expected output { \'home\': \'/user/home\', \'utf8_mode\': True, \'log_to_file\': True, \'status\': \'ok\' } ``` Advanced Requirements - Extend the `MockWideStringList` to support operations like insertion and appending items, similar to the methods defined in the documentation. - Provide clear and docstring comments for each function explaining its purpose and usage. Ensure your code is modular, readable, and testable. Document any assumptions and decisions you make during the implementation.","solution":"class MockWideStringList: def __init__(self): self.items = [] def append(self, item): self.items.append(item) def __repr__(self): return f\\"MockWideStringList({self.items})\\" class MockStatus: def __init__(self, status, message=\'\'): self.status = status self.message = message def is_ok(self): return self.status == \'ok\' def __repr__(self): return f\\"MockStatus(status={self.status}, message={self.message})\\" def initialize_mock_config(): Initialize a configuration dictionary with default settings. return { \'home\': \'\', \'utf8_mode\': True, \'log_to_file\': False } def apply_command_line_args(config, args): Parse and apply command line arguments to the configuration. Parameters: config (dict): The configuration dictionary. args (list): List of command line arguments. it = iter(args) for arg in it: if arg == \'--set-home\': config[\'home\'] = next(it, \'\') elif arg == \'--enable-utf8-mode\': config[\'utf8_mode\'] = True elif arg == \'--disable-utf8-mode\': config[\'utf8_mode\'] = False elif arg == \'--log-to-file\': config[\'log_to_file\'] = True return config def preinitialize(config): Simulate the preinitialization step based on the preconfiguration settings. Parameters: config (dict): The configuration dictionary. if not isinstance(config, dict): return MockStatus(\'error\', \'Invalid configuration format\') return MockStatus(\'ok\') def initialize(config): Complete the initialization process and return a MockStatus object. Parameters: config (dict): The configuration dictionary. status = preinitialize(config) if not status.is_ok(): return status # More initialization logic can be added here if needed. return MockStatus(\'ok\') def run_mock_environment(args): Simulate running the Python environment based on the provided configuration and command-line arguments. Parameters: args (list): List of command-line arguments. config = initialize_mock_config() config = apply_command_line_args(config, args) status = initialize(config) if not status.is_ok(): return status return {\'config\': config, \'status\': status.status}"},{"question":"Objective: You are asked to write a Python program that demonstrates the usage of various Python modules for data compression and archiving as described in the provided documentation. The program should provide functionality to compress and decompress files in different formats, as well as the capability to archive multiple files into a single ZIP or TAR file. Problem Statement: Create a Python program with the following functions: 1. `compress_file(input_file: str, output_file: str, algorithm: str) -> None` - **Description**: Compresses the input file using the specified algorithm and writes the compressed data to the output file. - **Arguments**: - `input_file` (str): Path to the file to be compressed. - `output_file` (str): Path where the compressed file will be saved. - `algorithm` (str): Compression algorithm to use (valid options: \'gzip\', \'bz2\', \'lzma\'). - **Constraints**: Raise a `ValueError` if the specified algorithm is not supported. 2. `decompress_file(input_file: str, output_file: str, algorithm: str) -> None` - **Description**: Decompresses the input file using the specified algorithm and writes the decompressed data to the output file. - **Arguments**: - `input_file` (str): Path to the file to be decompressed. - `output_file` (str): Path where the decompressed file will be saved. - `algorithm` (str): Decompression algorithm to use (valid options: \'gzip\', \'bz2\', \'lzma\'). - **Constraints**: Raise a `ValueError` if the specified algorithm is not supported. 3. `create_archive(input_files: list, output_file: str, archive_format: str) -> None` - **Description**: Creates an archive from the list of input files using the specified archive format. - **Arguments**: - `input_files` (list): List of paths to the files to be included in the archive. - `output_file` (str): Path where the archive file will be saved. - `archive_format` (str): Archive format to use (valid options: \'zip\', \'tar\'). - **Constraints**: Raise a `ValueError` if the specified archive format is not supported. 4. `extract_archive(input_file: str, output_dir: str, archive_format: str) -> None` - **Description**: Extracts the contents of the archive file into the specified directory. - **Arguments**: - `input_file` (str): Path to the archive file to be extracted. - `output_dir` (str): Path to the directory where contents of the archive will be extracted. - `archive_format` (str): Archive format to use for extraction (valid options: \'zip\', \'tar\'). - **Constraints**: Raise a `ValueError` if the specified archive format is not supported. Example Usage: ```python compress_file(\'example.txt\', \'example.txt.gz\', \'gzip\') decompress_file(\'example.txt.gz\', \'example_decompressed.txt\', \'gzip\') create_archive([\'example1.txt\', \'example2.txt\'], \'archive.zip\', \'zip\') extract_archive(\'archive.zip\', \'extracted_files\', \'zip\') ``` Additional Requirements: - Ensure to handle exceptions and edge cases, such as file not found, read/write errors, etc. - Include appropriate logging to inform the user of the progress and any issues encountered. **Assessment Notes**: - The implementation should demonstrate the usage of the `gzip`, `bz2`, `lzma`, `zipfile`, and `tarfile` modules. - Ensure code readability, proper documentation of functions, and adherence to Python best practices.","solution":"import gzip import bz2 import lzma import shutil import zipfile import tarfile import logging logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def compress_file(input_file: str, output_file: str, algorithm: str) -> None: Compresses the input file using the specified algorithm and writes the compressed data to the output file. Arguments: - input_file: Path to the file to be compressed. - output_file: Path where the compressed file will be saved. - algorithm: Compression algorithm to use (valid options: \'gzip\', \'bz2\', \'lzma\'). Raises: - ValueError: If the specified algorithm is not supported. if algorithm == \'gzip\': with open(input_file, \'rb\') as f_in: with gzip.open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algorithm == \'bz2\': with open(input_file, \'rb\') as f_in: with bz2.open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algorithm == \'lzma\': with open(input_file, \'rb\') as f_in: with lzma.open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) else: raise ValueError(\\"Unsupported compression algorithm. Use \'gzip\', \'bz2\', or \'lzma\'.\\") def decompress_file(input_file: str, output_file: str, algorithm: str) -> None: Decompresses the input file using the specified algorithm and writes the decompressed data to the output file. Arguments: - input_file: Path to the file to be decompressed. - output_file: Path where the decompressed file will be saved. - algorithm: Decompression algorithm to use (valid options: \'gzip\', \'bz2\', \'lzma\'). Raises: - ValueError: If the specified algorithm is not supported. if algorithm == \'gzip\': with gzip.open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algorithm == \'bz2\': with bz2.open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) elif algorithm == \'lzma\': with lzma.open(input_file, \'rb\') as f_in: with open(output_file, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) else: raise ValueError(\\"Unsupported decompression algorithm. Use \'gzip\', \'bz2\', or \'lzma\'.\\") def create_archive(input_files: list, output_file: str, archive_format: str) -> None: Creates an archive from the list of input files using the specified archive format. Arguments: - input_files: List of paths to the files to be included in the archive. - output_file: Path where the archive file will be saved. - archive_format: Archive format to use (valid options: \'zip\', \'tar\'). Raises: - ValueError: If the specified archive format is not supported. if archive_format == \'zip\': with zipfile.ZipFile(output_file, \'w\') as zf: for file in input_files: zf.write(file) elif archive_format == \'tar\': with tarfile.open(output_file, \'w\') as tf: for file in input_files: tf.add(file) else: raise ValueError(\\"Unsupported archive format. Use \'zip\' or \'tar\'.\\") def extract_archive(input_file: str, output_dir: str, archive_format: str) -> None: Extracts the contents of the archive file into the specified directory. Arguments: - input_file: Path to the archive file to be extracted. - output_dir: Path to the directory where contents of the archive will be extracted. - archive_format: Archive format to use for extraction (valid options: \'zip\', \'tar\'). Raises: - ValueError: If the specified archive format is not supported. if archive_format == \'zip\': with zipfile.ZipFile(input_file, \'r\') as zf: zf.extractall(output_dir) elif archive_format == \'tar\': with tarfile.open(input_file, \'r\') as tf: tf.extractall(output_dir) else: raise ValueError(\\"Unsupported archive format. Use \'zip\' or \'tar\'.\\")"},{"question":"# **Coding Assessment Question** # Objective: Create a function `future_features_info` that interacts with the `__future__` module. The function should return a comprehensive summary of all the features defined in the `__future__` module with the following details: - Feature name - Optional release version (as a string) - Mandatory release version (as a string) - Compiler flag # Expected Input and Output Formats: - The function `future_features_info` does not take any input arguments. - The function should return a list of dictionaries. Each dictionary should represent a future feature with the keys: - `name` (str): The feature name. - `optional_release` (str): The optional release version in the format \\"X.Y.Z\\". - `mandatory_release` (str): The mandatory release version in the format \\"X.Y.Z\\" or \\"None\\" if not applicable. - `compiler_flag` (int): The compiler flag value. # Constraints and Limitations: - Assume the code is being run in an environment where the `__future__` module is available and up-to-date with features added up to Python 3.10. - The solution should dynamically process the `__future__` module to pull feature information, ensuring it adapts to any changes in future Python versions without requiring hardcoded values. # Example Output: ```python [ { \'name\': \'nested_scopes\', \'optional_release\': \'2.1.0\', \'mandatory_release\': \'2.2.0\', \'compiler_flag\': 16 }, { \'name\': \'generators\', \'optional_release\': \'2.2.0\', \'mandatory_release\': \'2.3.0\', \'compiler_flag\': 8192 }, { \'name\': \'division\', \'optional_release\': \'2.2.0\', \'mandatory_release\': \'3.0.0\', \'compiler_flag\': 32768 }, ... ] ``` # Notes: - Ensure that the optional and mandatory release versions are formatted as `\\"{major}.{minor}.{micro}\\"`. - Handle any features with a `None` mandatory release appropriately in the output. Implement the function `future_features_info` to satisfy these requirements.","solution":"import __future__ def future_features_info(): Returns a list of dictionaries containing information about future features defined in the __future__ module. features_summary = [] for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): features_summary.append({ \'name\': feature_name, \'optional_release\': f\\"{feature.optional[0]}.{feature.optional[1]}.{feature.optional[2]}\\", \'mandatory_release\': f\\"{feature.mandatory[0]}.{feature.mandatory[1]}.{feature.mandatory[2]}\\" if feature.mandatory else \\"None\\", \'compiler_flag\': feature.compiler_flag }) return features_summary"},{"question":"Objective You are required to write a Python function that generates a `setup.cfg` configuration file based on given input specifications for various `Distutils` commands. This exercise will test your understanding of file I/O operations, string formatting in Python, and knowledge of configuration files used in building Python packages. Task Implement a function `generate_setup_cfg(config)` that takes a dictionary `config` as input and writes the contents to a `setup.cfg` file. Each key in the dictionary represents a `Distutils` command (e.g., `build_ext`, `install`), and its corresponding value is a dictionary of options for that command. Function Signature ```python def generate_setup_cfg(config: dict) -> None: ``` Input - `config`: A dictionary where keys are strings (command names) and values are dictionaries. Each inner dictionary contains options and their corresponding values for the command. Example: ```python config = { \'build_ext\': { \'inplace\': \'1\', \'include_dirs\': \'/usr/local/include,/opt/include\' }, \'bdist_rpm\': { \'release\': \'1\', \'packager\': \'Jane Doe <jane.doe@example.com>\', \'doc_files\': \'CHANGES.txt README.txt USAGE.txt doc/ examples/\' } } ``` Output - The function should create a file named `setup.cfg` in the current working directory with the appropriate format based on the input dictionary. Example content for the above input: ``` [build_ext] inplace=1 include_dirs=/usr/local/include,/opt/include [bdist_rpm] release=1 packager=Jane Doe <jane.doe@example.com> doc_files=CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Constraints 1. Assume the input dictionary is always well-formed and contains valid command names and options. 2. Handle multi-line option values by splitting lines with indentation for readability. 3. The function should overwrite any existing `setup.cfg` file in the directory. Requirements - Use Python file I/O to write the configuration file. - Ensure proper formatting as per the `Distutils` configuration file syntax. # Evaluation Criteria - **Correctness**: The function should generate a correctly formatted `setup.cfg` file based on the input specifications. - **Code Quality**: Use of clear and concise code with proper comments and naming conventions. - **Robustness**: The function should handle different types of inputs and ensure the file is properly overwritten.","solution":"def generate_setup_cfg(config: dict) -> None: Generates a setup.cfg configuration file based on the given input specifications for various Distutils commands. Args: config (dict): A dictionary where keys are command names and values are dictionaries of options and their values. with open(\'setup.cfg\', \'w\') as cfg_file: for command, options in config.items(): cfg_file.write(f\'[{command}]n\') for option, value in options.items(): if \'n\' in value: value = value.replace(\'n\', \'nt\') cfg_file.write(f\'{option}={value}n\') cfg_file.write(\'n\')"},{"question":"Objective: Assess your understanding of seaborn\'s color palette functionalities by creating and applying customized palettes to plot data. Problem Statement: You are given a dataset containing information about the average temperatures (in °C) of three cities (City A, City B, and City C) over twelve months. Your task is to use the seaborn library to: 1. Define a custom light color palette using a HEX code. 2. Create a bar plot that uses this palette to represent the temperatures of each city. 3. Save the plot to a file. Dataset: The dataset is provided as a list of dictionaries where each dictionary represents a city\'s temperatures over months. ```python temperature_data = [ {\\"month\\": \\"January\\", \\"City A\\": 5, \\"City B\\": 7, \\"City C\\": 6}, {\\"month\\": \\"February\\", \\"City A\\": 6, \\"City B\\": 8, \\"City C\\": 7}, {\\"month\\": \\"March\\", \\"City A\\": 10, \\"City B\\": 11, \\"City C\\": 9}, {\\"month\\": \\"April\\", \\"City A\\": 15, \\"City B\\": 16, \\"City C\\": 14}, {\\"month\\": \\"May\\", \\"City A\\": 20, \\"City B\\": 21, \\"City C\\": 19}, {\\"month\\": \\"June\\", \\"City A\\": 25, \\"City B\\": 26, \\"City C\\": 24}, {\\"month\\": \\"July\\", \\"City A\\": 30, \\"City B\\": 31, \\"City C\\": 29}, {\\"month\\": \\"August\\", \\"City A\\": 30, \\"City B\\": 31, \\"City C\\": 29}, {\\"month\\": \\"September\\", \\"City A\\": 25, \\"City B\\": 26, \\"City C\\": 24}, {\\"month\\": \\"October\\", \\"City A\\": 18, \\"City B\\": 19, \\"City C\\": 17}, {\\"month\\": \\"November\\", \\"City A\\": 10, \\"City B\\": 11, \\"City C\\": 9}, {\\"month\\": \\"December\\", \\"City A\\": 5, \\"City B\\": 7, \\"City C\\": 6}, ] ``` Expected Input and Output Formats: - Input: None (the dataset is predefined and should be used directly within the script). - Output: The plot should be saved to a file called `temperature_plot.png`. Constraints: - Use the HEX code `#FF5733` to define the light color palette. - Use seaborn\'s bar plot functionality. - The x-axis should represent the months, and the y-axis should represent the temperature. - The plot should have a legend indicating the cities. Performance Requirements: - The solution should efficiently handle the dataset. Implementation: 1. Import the necessary libraries: ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt ``` 2. Convert the list of dictionaries to a pandas DataFrame: ```python df = pd.DataFrame(temperature_data) ``` 3. Melt the DataFrame to a long format suitable for seaborn\'s bar plot: ```python df_melted = df.melt(id_vars=[\\"month\\"], var_name=\\"City\\", value_name=\\"Temperature\\") ``` 4. Define a light color palette using the HEX code `#FF5733` and create a bar plot: ```python palette = sns.light_palette(\\"#FF5733\\", n_colors=3) sns.set_theme() g = sns.catplot( data=df_melted, kind=\\"bar\\", x=\\"month\\", y=\\"Temperature\\", hue=\\"City\\", palette=palette, height=5, aspect=2 ) plt.xticks(rotation=45) g.set_axis_labels(\\"Month\\", \\"Average Temperature (°C)\\") g.set_titles(\\"Average Monthly Temperatures for Three Cities\\") ``` 5. Save the plot: ```python g.savefig(\\"temperature_plot.png\\") ``` Your task is to implement this solution in a python script and ensure the plot is correctly generated and saved to `temperature_plot.png`.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def create_temperature_plot(): # Given temperature data temperature_data = [ {\\"month\\": \\"January\\", \\"City A\\": 5, \\"City B\\": 7, \\"City C\\": 6}, {\\"month\\": \\"February\\", \\"City A\\": 6, \\"City B\\": 8, \\"City C\\": 7}, {\\"month\\": \\"March\\", \\"City A\\": 10, \\"City B\\": 11, \\"City C\\": 9}, {\\"month\\": \\"April\\", \\"City A\\": 15, \\"City B\\": 16, \\"City C\\": 14}, {\\"month\\": \\"May\\", \\"City A\\": 20, \\"City B\\": 21, \\"City C\\": 19}, {\\"month\\": \\"June\\", \\"City A\\": 25, \\"City B\\": 26, \\"City C\\": 24}, {\\"month\\": \\"July\\", \\"City A\\": 30, \\"City B\\": 31, \\"City C\\": 29}, {\\"month\\": \\"August\\", \\"City A\\": 30, \\"City B\\": 31, \\"City C\\": 29}, {\\"month\\": \\"September\\", \\"City A\\": 25, \\"City B\\": 26, \\"City C\\": 24}, {\\"month\\": \\"October\\", \\"City A\\": 18, \\"City B\\": 19, \\"City C\\": 17}, {\\"month\\": \\"November\\", \\"City A\\": 10, \\"City B\\": 11, \\"City C\\": 9}, {\\"month\\": \\"December\\", \\"City A\\": 5, \\"City B\\": 7, \\"City C\\": 6}, ] # Convert the list of dictionaries to a pandas DataFrame df = pd.DataFrame(temperature_data) # Melt the DataFrame to a long format suitable for seaborn\'s bar plot df_melted = df.melt(id_vars=[\\"month\\"], var_name=\\"City\\", value_name=\\"Temperature\\") # Define a light color palette using the HEX code #FF5733 palette = sns.light_palette(\\"#FF5733\\", n_colors=3, reverse=True) # Set the theme for seaborn sns.set_theme() # Create the bar plot g = sns.catplot( data=df_melted, kind=\\"bar\\", x=\\"month\\", y=\\"Temperature\\", hue=\\"City\\", palette=palette, height=5, aspect=2 ) # Customize the plot plt.xticks(rotation=45) g.set_axis_labels(\\"Month\\", \\"Average Temperature (°C)\\") g.set_titles(\\"Average Monthly Temperatures for Three Cities\\") # Save the plot to a file g.savefig(\\"temperature_plot.png\\")"},{"question":"# Advanced Coding Assessment: Fetching and Processing Web Data with `urllib` **Objective**: Write a Python function that fetches data from a specified URL, handles potential HTTP errors, and processes the fetched data. Task: Implement a function `fetch_and_process_url(url: str, params: dict, headers: dict, timeout: int = 10)` that: 1. Takes: - `url` (str): The URL to fetch data from. - `params` (dict): A dictionary of query parameters (for GET) or form data (for POST). - `headers` (dict): A dictionary of headers to be sent with the request. - `timeout` (int, optional): The socket timeout in seconds, default is 10. 2. Determines whether to make a GET or POST request based on the method explicitly specified in the `params`. If `params` is empty, defaults to GET. 3. Sends the request with the specified headers and handles different types of HTTP errors, returning: - The HTML content if the request is successful (status code 200). - Error code and message in case of an HTTP error. - `\\"Failed to reach server\\"` message if the server could not be reached. Input Format: - `url`: `str` (1 <= len(url) <= 2048) - `params`: `dict` (0 <= len(params) <= 100) - `headers`: `dict` (0 <= len(headers) <= 50) - `timeout`: `int` (1 <= timeout <= 120) Output Format: - On success: Return the HTML content as a `str`. - On HTTP error: Return a `tuple` of `(error_code: int, error_message: str)`. - On URL error: Return `\\"Failed to reach server\\"`. Constraints: - Ensure you handle URL and HTTP errors gracefully. - Optimize for efficient handling of large responses and timeouts. Example: ```python def fetch_and_process_url(url: str, params: dict, headers: dict, timeout: int = 10): import urllib.parse import urllib.request import urllib.error import socket # Apply timeout configuration socket.setdefaulttimeout(timeout) # Encode parameters if params: if \'method\' in params and params[\'method\'].lower() == \'post\': encoded_params = urllib.parse.urlencode({k: v for k, v in params.items() if k != \'method\'}).encode(\'ascii\') req = urllib.request.Request(url, data=encoded_params, headers=headers) else: url += \'?\' + urllib.parse.urlencode(params) req = urllib.request.Request(url, headers=headers) else: req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return (e.code, e.reason) except urllib.error.URLError as e: return \\"Failed to reach server\\" ``` **Notes**: - You may use additional helper functions if required. - Test your function with different URLs, parameters, and headers to ensure robustness.","solution":"import urllib.parse import urllib.request import urllib.error import socket def fetch_and_process_url(url: str, params: dict, headers: dict, timeout: int = 10): Fetches data from the specified URL with given parameters and headers. Args: url (str): The URL to fetch data from. params (dict): Query parameters (for GET) or form data (for POST). headers (dict): Headers to send with the request. timeout (int, optional): Socket timeout in seconds, default is 10. Returns: str: HTML content on successful request. tuple: (error_code: int, error_message: str) on HTTP error. str: \\"Failed to reach server\\" on URL error. # Apply timeout configuration socket.setdefaulttimeout(timeout) # Encode parameters if params: if \'method\' in params and params[\'method\'].lower() == \'post\': encoded_params = urllib.parse.urlencode({k: v for k, v in params.items() if k != \'method\'}).encode(\'ascii\') req = urllib.request.Request(url, data=encoded_params, headers=headers) else: url += \'?\' + urllib.parse.urlencode(params) req = urllib.request.Request(url, headers=headers) else: req = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return (e.code, e.reason) except urllib.error.URLError as e: return \\"Failed to reach server\\""},{"question":"Objective: Demonstrate your understanding of Python’s cell objects by implementing a closure that correctly utilizes these objects to maintain state across function calls. Task: Write a Python function `create_counter` that returns a closure to act as a simple counter. The closure should increment an internal count each time it is called and return the current count. Use cell objects to store and manage the count variable. Requirements: 1. Implement the `create_counter` function, which should: - Initialize an internal count variable. - Return a closure that, when called, increments the count and returns the new value. 2. The closure must use cell objects to store and manage the internal count variable. Expected Input: - No input is required for the `create_counter` function. Expected Output: - The `create_counter` function should return a closure (a function) that, when called, returns and increments an internal counter. Constraints: - The counter must accurately maintain the count state across multiple calls. - You must use the cell object functions (`PyCell_New`, `PyCell_Get`, `PyCell_SET`, etc.) to implement this. Example: ```python counter = create_counter() print(counter()) # Output: 1 print(counter()) # Output: 2 print(counter()) # Output: 3 ``` Performance: The implementation should be efficient and should not have significant performance bottlenecks given the simplicity of the task.","solution":"def create_counter(): Returns a closure that increments and returns an internal counter each time it is called. count = 0 def counter(): nonlocal count count += 1 return count return counter"},{"question":"Coding Assessment Question # Objective Design and implement a class `FileCompressor` that: 1. Compresses and decompresses files. 2. Can handle different levels of compression. 3. Can compute and validate checksums of the original and compressed files. # Requirements 1. **Class Definition:** - `class FileCompressor:` 2. **Methods:** - `compress_file(input_filepath: str, output_filepath: str, level: int = -1) -> None` - Compresses the file at `input_filepath` and writes the compressed data to `output_filepath`. - `level`: An integer from 0 to 9 or -1, controlling the level of compression. Default is -1. - `decompress_file(input_filepath: str, output_filepath: str) -> None` - Decompresses the file at `input_filepath` and writes the decompressed data to `output_filepath`. - `compute_checksum(filepath: str, method: str = \'adler32\') -> int` - Computes the checksum of the file at `filepath`. - `method`: Either \'adler32\' or \'crc32\'. Defaults to \'adler32\'. - `validate_checksum(filepath: str, expected_checksum: int, method: str = \'adler32\') -> bool` - Validates the checksum of the file at `filepath` against the provided `expected_checksum`. - `method`: Either \'adler32\' or \'crc32\'. Defaults to \'adler32\'. # Constraints - The input files can be large, so the solution should be efficient in terms of memory usage. - Use appropriate exception handling for file operations and zlib methods. # Input - File paths as strings and integers for compression levels and checksums. # Output - For compress and decompress methods, no return value but files should be written correctly. - For checksum methods, return an integer checksum or a boolean for validation. # Example ```python # Creating an instance of FileCompressor compressor = FileCompressor() # Compress a file compressor.compress_file(\\"example.txt\\", \\"example_compressed.gz\\", level=6) # Decompress the file compressor.decompress_file(\\"example_compressed.gz\\", \\"example_decompressed.txt\\") # Compute checksum for the original file checksum = compressor.compute_checksum(\\"example.txt\\", method=\'crc32\') # Validate the checksum of the decompressed file is_valid = compressor.validate_checksum(\\"example_decompressed.txt\\", checksum, method=\'crc32\') print(is_valid) # Should print True if the decompression was correct ``` Implement the `FileCompressor` class as described, ensuring compliance with the requirements and constraints.","solution":"import zlib class FileCompressor: def compress_file(self, input_filepath: str, output_filepath: str, level: int = -1) -> None: Compresses the file at `input_filepath` and writes the compressed data to `output_filepath`. try: with open(input_filepath, \'rb\') as input_file: data = input_file.read() compressed_data = zlib.compress(data, level) with open(output_filepath, \'wb\') as output_file: output_file.write(compressed_data) except Exception as e: raise IOError(f\\"Error during compression: {e}\\") def decompress_file(self, input_filepath: str, output_filepath: str) -> None: Decompresses the file at `input_filepath` and writes the decompressed data to `output_filepath`. try: with open(input_filepath, \'rb\') as input_file: compressed_data = input_file.read() data = zlib.decompress(compressed_data) with open(output_filepath, \'wb\') as output_file: output_file.write(data) except Exception as e: raise IOError(f\\"Error during decompression: {e}\\") def compute_checksum(self, filepath: str, method: str = \'adler32\') -> int: Computes the checksum of the file at `filepath`. `method` can be either \'adler32\' or \'crc32\'. try: with open(filepath, \'rb\') as file: data = file.read() if method == \'adler32\': return zlib.adler32(data) & 0xffffffff elif method == \'crc32\': return zlib.crc32(data) & 0xffffffff else: raise ValueError(\\"Unsupported checksum method. Use \'adler32\' or \'crc32\'.\\") except Exception as e: raise IOError(f\\"Error during checksum computation: {e}\\") def validate_checksum(self, filepath: str, expected_checksum: int, method: str = \'adler32\') -> bool: Validates the checksum of the file at `filepath` against the provided `expected_checksum`. try: actual_checksum = self.compute_checksum(filepath, method) return actual_checksum == expected_checksum except Exception as e: raise IOError(f\\"Error during checksum validation: {e}\\")"},{"question":"# HTML Parsing and Transformation Objective Design a subclass of `HTMLParser` to parse an HTML document and extract specific information or transform the document as specified. Task Your task is to create a subclass of `HTMLParser` called `CustomHTMLParser` to perform the following actions: 1. **Extract Links**: Extract all hyperlink URLs (`href` attribute in `<a>` tags) and store them in a list. 2. **Count Tags**: Count the number of distinct tags (e.g., `html`, `head`, `body`, `p`, `a`, etc.) encountered in the document. 3. **Transform Content**: Replace any occurrences of a specific word (e.g., \\"Python\\") with another word (e.g., \\"Programming\\") in the text content of the document (excluding the content within `script` and `style` tags). Implementation - Define a class `CustomHTMLParser` that subclasses `HTMLParser`. - Override the necessary methods to implement the required functionality. Input - `html_content` (str): A string containing the HTML content to be parsed. - `word_to_replace` (str): The word to be replaced in the text content. - `replacement_word` (str): The word to replace with. Output Return a dictionary with the following keys: - `links` (list): A list of all extracted hyperlink URLs. - `tag_count` (dict): A dictionary with the tag names as keys and their respective counts as values. - `transformed_html` (str): The transformed HTML content with the specified word replacements. Constraints - The `html_content` string can contain any valid HTML or XHTML. - The word replacement should be case-sensitive. Example ```python html_content = \'\'\' <html> <head><title>Test Document</title></head> <body> <h1>Welcome to the Python Course</h1> <p>This course will teach you Python.</p> <a href=\\"https://example.com\\">Example</a> <script>var language = \\"Python\\";<\/script> </body> </html> \'\'\' parser = CustomHTMLParser() result = parser.parse(html_content, \\"Python\\", \\"Programming\\") assert result[\'links\'] == [\\"https://example.com\\"] assert result[\'tag_count\'] == {\'html\': 1, \'head\': 1, \'title\': 1, \'body\': 1, \'h1\': 1, \'p\': 1, \'a\': 1, \'script\': 1} assert \'<h1>Welcome to the Programming Course</h1>\' in result[\'transformed_html\'] assert \'var language = \\"Python\\";\' in result[\'transformed_html\'] # Ensure script content remains unchanged ```","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def __init__(self): super().__init__() self.links = [] self.tag_count = {} self.transforming = False self.transformed_html_parts = [] self.word_to_replace = \\"\\" self.replacement_word = \\"\\" self.in_script_or_style = False def handle_starttag(self, tag, attrs): self.tag_count[tag] = self.tag_count.get(tag, 0) + 1 if tag == \\"a\\": for attr in attrs: if attr[0] == \\"href\\": self.links.append(attr[1]) if tag in (\\"script\\", \\"style\\"): self.in_script_or_style = True self.transformed_html_parts.append(self.get_starttag_text()) def handle_endtag(self, tag): self.transformed_html_parts.append(f\\"</{tag}>\\") if tag in (\\"script\\", \\"style\\"): self.in_script_or_style = False def handle_data(self, data): if not self.in_script_or_style and self.word_to_replace: transformed_data = data.replace(self.word_to_replace, self.replacement_word) self.transformed_html_parts.append(transformed_data) else: self.transformed_html_parts.append(data) def parse(self, html_content, word_to_replace, replacement_word): self.word_to_replace = word_to_replace self.replacement_word = replacement_word self.feed(html_content) self.close() transformed_html = \'\'.join(self.transformed_html_parts) return { \'links\': self.links, \'tag_count\': self.tag_count, \'transformed_html\': transformed_html }"},{"question":"# Question: Objective: Write a function using the `seaborn` library to create and customize different visualizations based on the requirements below. Requirements: 1. Load the `penguins` dataset from seaborn. 2. Create a histogram of the `flipper_length_mm` variable with 30 bins. 3. Create a bar plot showing the count of penguins on each island. 4. Create a proportion-based stacked bar plot showing the count of penguins on each island, segregated by their `sex` (i.e., one stacked bar per island, stacked by `sex` proportion). 5. Customize the plot aesthetics: set the common theme for all plots to `darkgrid`. Function Signature: ```python import seaborn.objects as so from seaborn import load_dataset def create_custom_penguin_plots(): # Your implementation here pass ``` Expected Output: Each plot should be displayed when the function is called. The function does not return any values. Constraints: - Ensure the visualizations are aesthetically pleasing and correctly represent the data. - Use the seaborn library version that includes the `seaborn.objects` module (as shown in the provided documentation). Example: Here is an illustration of the expected output (though the actual visual appearance will depend on exact implementation): 1. Histogram: ![Histogram Example](https://example.com/histogram.png) *(This is a placeholder link; your code should generate its histogram)* 2. Bar Plot: ![Bar Plot Example](https://example.com/barplot.png) *(This is a placeholder link; your code should generate its bar plot)* 3. Stacked Bar Plot: ![Stacked Bar Plot Example](https://example.com/stackedbarplot.png) *(This is a placeholder link; your code should generate its stacked bar plot)* Use these instructions to implement the function as specified.","solution":"import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt import seaborn as sns def create_custom_penguin_plots(): # Load the penguins dataset df = load_dataset(\\"penguins\\") # Set theme sns.set_theme(style=\\"darkgrid\\") # Create histogram plt.figure(figsize=(10, 6)) sns.histplot(df[\'flipper_length_mm\'], bins=30, kde=False) plt.title(\'Histogram of Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Count\') plt.show() # Create bar plot for island count plt.figure(figsize=(10, 6)) sns.countplot(x=\'island\', data=df) plt.title(\'Count of Penguins on Each Island\') plt.xlabel(\'Island\') plt.ylabel(\'Count\') plt.show() # Create proportion-based stacked bar plot (island by sex) df_sex_count = df.groupby([\'island\', \'sex\']).size().unstack() df_sex_prop = df_sex_count.div(df_sex_count.sum(axis=1), axis=0) df_sex_prop.plot(kind=\'bar\', stacked=True, figsize=(10, 6)) plt.title(\'Proportion of Penguins by Sex on Each Island\') plt.xlabel(\'Island\') plt.ylabel(\'Proportion\') plt.legend(title=\'Sex\') plt.show()"},{"question":"# Question: Advanced Data Loading in PyTorch Objective: Create a custom data loading pipeline using PyTorch\'s `DataLoader`, which includes: 1. Implementing a custom map-style dataset. 2. Implementing a custom sampler. 3. Using a multi-process data loader. 4. Customizing the collation function to handle variable-length data. Task: 1. **Custom Dataset**: Implement a custom map-style dataset `VariableLengthDataset` that returns data samples of variable lengths. Each data sample should be a tuple containing a tensor of random length (between 5 and 15) and a label (integer). ```python import torch from torch.utils.data import Dataset class VariableLengthDataset(Dataset): def __init__(self, num_samples): self.num_samples = num_samples def __len__(self): return self.num_samples def __getitem__(self, idx): length = torch.randint(5, 16, (1,)).item() sample = torch.randn(length) # Random tensor of variable length label = torch.randint(0, 10, (1,)).item() # Random label between 0 and 9 return sample, label ``` 2. **Custom Sampler**: Implement a custom sampler `SequentialOddSampler` that samples indices in sequential order but only returns odd-numbered indices. ```python from torch.utils.data import Sampler class SequentialOddSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): return iter(range(1, len(self.data_source), 2)) def __len__(self): return len(range(1, len(self.data_source), 2)) ``` 3. **Custom Collate Function**: Write a custom `collate_fn` function `variable_length_collate_fn` that pads sequences in a batch to the maximum length in the batch and returns a batch of input tensors and labels. ```python def variable_length_collate_fn(batch): # Extract data and labels data, labels = zip(*batch) # Find the maximum length max_length = max([d.shape[0] for d in data]) # Pad sequences to the maximum length padded_data = [torch.cat([d, torch.zeros(max_length - len(d))]) for d in data] return torch.stack(padded_data), torch.tensor(labels) ``` 4. **Multi-process Data Loader**: Construct a `DataLoader` using the custom dataset, custom sampler, and custom collate function, with multi-process data loading enabled (`num_workers` set to 2). ```python from torch.utils.data import DataLoader # Number of samples in the dataset num_samples = 100 # Instantiate the dataset, sampler, and dataloader dataset = VariableLengthDataset(num_samples=num_samples) sampler = SequentialOddSampler(dataset) data_loader = DataLoader(dataset, batch_size=4, sampler=sampler, collate_fn=variable_length_collate_fn, num_workers=2) # Iterate through the DataLoader and print batches for batch_inputs, batch_labels in data_loader: print(f\\"Batch inputs: {batch_inputs}\\") print(f\\"Batch labels: {batch_labels}\\") ``` Expected Output: Your implementation should successfully load batches of variable-length data samples from the custom dataset using the custom data pipeline. The collate function should pad sequences correctly, and the data loader should work correctly with multi-process data loading. **Constraints**: - Your dataset should contain 100 samples. - The sampler should only return odd-numbered indices. - The `collate_fn` must pad sequences to the maximum length within the batch. - The data loader should use 2 worker processes for data loading. **Performance Requirements**: - The solution should efficiently handle the given constraints and demonstrate a thorough understanding of PyTorch\'s data loading utilities.","solution":"import torch from torch.utils.data import Dataset, DataLoader, Sampler # Custom Dataset class VariableLengthDataset(Dataset): def __init__(self, num_samples): self.num_samples = num_samples def __len__(self): return self.num_samples def __getitem__(self, idx): length = torch.randint(5, 16, (1,)).item() sample = torch.randn(length) # Random tensor of variable length label = torch.randint(0, 10, (1,)).item() # Random label between 0 and 9 return sample, label # Custom Sampler class SequentialOddSampler(Sampler): def __init__(self, data_source): self.data_source = data_source def __iter__(self): return iter(range(1, len(self.data_source), 2)) def __len__(self): return len(range(1, len(self.data_source), 2)) # Custom Collate Function def variable_length_collate_fn(batch): # Extract data and labels data, labels = zip(*batch) # Find the maximum length max_length = max([d.shape[0] for d in data]) # Pad sequences to the maximum length padded_data = [torch.cat([d, torch.zeros(max_length - len(d))]) for d in data] return torch.stack(padded_data), torch.tensor(labels) # Instantiate dataset, sampler, and dataloader num_samples = 100 dataset = VariableLengthDataset(num_samples=num_samples) sampler = SequentialOddSampler(dataset) data_loader = DataLoader(dataset, batch_size=4, sampler=sampler, collate_fn=variable_length_collate_fn, num_workers=2) # Example code to iterate through DataLoader # for batch_inputs, batch_labels in data_loader: # print(f\\"Batch inputs: {batch_inputs}\\") # print(f\\"Batch labels: {batch_labels}\\")"},{"question":"Objective In this assessment, you are required to implement a custom pickler and unpickler that handles complex objects with persistent IDs. This challenge will test your understanding of advanced pickling concepts, including custom behavior for serialization and deserialization processes, as well as ensuring the integrity and security of the serialized data. Problem Statement You are working with a system that manages university records. The system stores information about students and the courses they are enrolled in. Your task is to implement custom serialization and deserialization for these records using the `pickle` module. The university system consists of the following classes: - `Student`: Represents a student with attributes like `student_id` (int), `name` (str), and `courses` (list of `Course` objects). - `Course`: Represents a course with attributes like `course_id` (int), and `course_name` (str). Due to the large number of students and courses, the university wants to store records in a database and references to these entries in their pickled data instead of duplicating information. Requirements 1. Implement the `UniversityPickler` class that extends `pickle.Pickler`: - The `persistent_id` method should generate a persistent ID for `Student` and `Course` objects. - Any other object should be pickled as normal. 2. Implement the `UniversityUnpickler` class that extends `pickle.Unpickler`: - The `persistent_load` method should resolve the persistent IDs to the corresponding entries in the database. - Handle any errors if a persistent ID cannot be resolved. 3. Create a simple in-memory database using a dictionary structure to store `Student` and `Course` objects. The structure should allow for lookup via IDs. 4. Write a function `serialize_university_records` that accepts a list of `Student` objects and serializes them using `UniversityPickler`. 5. Write a function `deserialize_university_records` that accepts the pickled data and returns the list of `Student` objects using `UniversityUnpickler`. Input and Output Format - Both functions should handle data in-memory using `io.BytesIO`. - Ensure the deserialization reconstructs the original objects with proper references. Constraints 1. Assume that each `course_id` and `student_id` is unique. 2. Handle scenarios where certain records in the serialized data might be missing in the database during deserialization. Example ```python import io import pickle from collections import defaultdict # Define the data classes class Student: def __init__(self, student_id, name, courses): self.student_id = student_id self.name = name self.courses = courses class Course: def __init__(self, course_id, course_name): self.course_id = course_id self.course_name = course_name # Create the in-memory database database = { \\"students\\": {}, \\"courses\\": {} } # Implement the custom Pickler and Unpickler classes class UniversityPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Student): return (\\"Student\\", obj.student_id) elif isinstance(obj, Course): return (\\"Course\\", obj.course_id) return None class UniversityUnpickler(pickle.Unpickler): def persistent_load(self, pid): type_tag, obj_id = pid if type_tag == \\"Student\\": return database[\\"students\\"].get(obj_id) elif type_tag == \\"Course\\": return database[\\"courses\\"].get(obj_id) raise pickle.UnpicklingError(f\\"Unsupported persistent object: {pid}\\") # Function to serialize records def serialize_university_records(students): buffer = io.BytesIO() pickler = UniversityPickler(buffer) pickler.dump(students) return buffer.getvalue() # Function to deserialize records def deserialize_university_records(data): buffer = io.BytesIO(data) unpickler = UniversityUnpickler(buffer) return unpickler.load() # Example usage if __name__ == \\"__main__\\": # Create sample data course1 = Course(101, \\"Mathematics\\") course2 = Course(102, \\"Physics\\") student1 = Student(1, \\"Alice\\", [course1, course2]) student2 = Student(2, \\"Bob\\", [course1]) # Populate the in-memory database database[\\"courses\\"][101] = course1 database[\\"courses\\"][102] = course2 database[\\"students\\"][1] = student1 database[\\"students\\"][2] = student2 # Serialize the student records serialized_data = serialize_university_records([student1, student2]) # Deserialize the student records deserialized_students = deserialize_university_records(serialized_data) for student in deserialized_students: print(f\\"Student: {student.name}, Courses: {[course.course_name for course in student.courses]}\\") ``` Notes - Ensure your implementation handles edge cases, such as missing references during deserialization. - You may assume that the in-memory database is pre-populated with the necessary records before deserialization.","solution":"import io import pickle # Define the data classes class Student: def __init__(self, student_id, name, courses): self.student_id = student_id self.name = name self.courses = courses class Course: def __init__(self, course_id, course_name): self.course_id = course_id self.course_name = course_name # Create the in-memory database database = { \\"students\\": {}, \\"courses\\": {} } # Implement the custom Pickler and Unpickler classes class UniversityPickler(pickle.Pickler): def persistent_id(self, obj): if isinstance(obj, Student): return (\\"Student\\", obj.student_id) elif isinstance(obj, Course): return (\\"Course\\", obj.course_id) return None class UniversityUnpickler(pickle.Unpickler): def persistent_load(self, pid): type_tag, obj_id = pid if type_tag == \\"Student\\": return database[\\"students\\"].get(obj_id) elif type_tag == \\"Course\\": return database[\\"courses\\"].get(obj_id) raise pickle.UnpicklingError(f\\"Unsupported persistent object: {pid}\\") # Function to serialize records def serialize_university_records(students): buffer = io.BytesIO() pickler = UniversityPickler(buffer) pickler.dump(students) return buffer.getvalue() # Function to deserialize records def deserialize_university_records(data): buffer = io.BytesIO(data) unpickler = UniversityUnpickler(buffer) return unpickler.load()"},{"question":"**Objective:** Demonstrate your understanding of the `seaborn` package, specifically the use of `seaborn.objects` for plotting data. # Question: You are given a dataset of health expenditures and life expectancy across different countries. Your task is to create a plot to visualize the relationship between health spending (`Spending_USD`) and life expectancy (`Life_Expectancy`) for each country over time, using a trajectory plot without sorting the data beforehand. The plot should have distinct markers and colors for each country. **Instructions:** 1. Load the `healthexp` dataset using `seaborn.load_dataset()`. 2. Create a plot with the x-axis representing `Spending_USD` and the y-axis representing `Life_Expectancy`. Differentiate countries by color. 3. Add a `Path` to the plot showing the trajectories of the data points (do not sort the data before plotting). 4. Customize the path markers: - Use circular markers. - Set the marker size to 3. - Set the line width to 1. - Use white color for the marker fill. 5. Ensure that the plot is clearly labeled and visually distinct for different countries. # Function Signature: ```python def visualize_health_expenditure(data): # data: DataFrame - The health expenditure dataset # Your code here to generate and display the plot ``` # Expected Input: - `data`: A pandas DataFrame containing the health expenditure dataset. # Expected Output: - Display a plot with the specified properties. # Example: ```python import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(data): p = so.Plot(data, \\"Spending_USD\\", \\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=3, linewidth=1, fillcolor=\\"w\\")) p.show() # Example usage: data = load_dataset(\\"healthexp\\").sort_values([\\"Country\\", \\"Year\\"]) visualize_health_expenditure(data) ``` **Constraints:** - Ensure that the plot is generated without sorting the data before plotting. - Use `seaborn.objects` for plotting and customization. **Note:** Do not use any other libraries for plotting.","solution":"import seaborn.objects as so from seaborn import load_dataset def visualize_health_expenditure(data): Plots the relationship between health spending and life expectancy for different countries over time using a trajectory plot. Parameters: data (DataFrame): The health expenditure dataset. p = so.Plot(data, x=\\"Spending_USD\\", y=\\"Life_Expectancy\\", color=\\"Country\\") p.add(so.Path(marker=\\"o\\", pointsize=3, linewidth=1, fillcolor=\\"w\\")) p.show() # Example usage: # data = load_dataset(\\"healthexp\\") # visualize_health_expenditure(data)"},{"question":"You are provided with two raw audio fragments, and your task is to implement a function that will perform echo cancellation on these fragments. Echo cancellation involves subtracting an echo of an audio signal from the received audio signal to enhance the overall sound quality. Function Signature ```python def echocancel(outputdata: bytes, inputdata: bytes) -> bytes: pass ``` # Input - `outputdata`: A bytes object containing the sound fragment that is being played (output). - `inputdata`: A bytes object containing the sound fragment that is being recorded (input). # Output - Return a modified bytes object that represents the input fragment with the echo cancelled. # Constraints - Both `outputdata` and `inputdata` contain signed integer samples of 16-bit width stored in little-endian order. - The fragments in `outputdata` and `inputdata` are aligned exactly in time. - `outputdata` and `inputdata` are non-empty and have the same length. # Example ```python outputdata = b\'x01x00x02x00x03x00x04x00\' # Example values in 16-bit signed format inputdata = b\'x02x00x03x00x04x00x05x00\' # Example values in 16-bit signed format print(echocancel(outputdata, inputdata)) # Expected Signature of the response will be of same length as input with echoes cancelled ``` # Notes To implement the `echocancel` function, use the audioop module utilizing: - `audioop.findmax(fragment, length)`: To find an energetic slice from `outputdata`. - `audioop.findfit(fragment, reference)`: To locate the match of the slice in the `inputdata`. - `audioop.mul(fragment, width, factor)`: To scale down the amplitude of the matched slice. - `audioop.add(fragment1, fragment2, width)`: To combine the manipulated slice with the remaining data points. This question tests your understanding of manipulating audio data and using operations within the `audioop` module to perform an advanced audio signal processing task effectively.","solution":"import audioop def echocancel(outputdata: bytes, inputdata: bytes) -> bytes: Cancel echo from inputdata using outputdata as reference. Arguments: outputdata -- bytes object containing the output audio sample inputdata -- bytes object containing the input audio sample with echo Returns: A bytes object with the echo removed. if len(outputdata) != len(inputdata): raise ValueError(\\"The length of outputdata and inputdata must be the same\\") # Calculate the energy of outputdata to find the most energetic slice max_energy_position = audioop.findmax(outputdata, 2) # Find where this slice fits in inputdata matching_position = audioop.findfit(inputdata, outputdata[max_energy_position * 2 : (max_energy_position + 1) * 2]) # Cancel Echo: Apply a negative factor to the output to cancel its presence in the input # Assuming a simple direct subtraction might work; more advanced methods might require further steps canceled_output = audioop.add(inputdata, audioop.mul(outputdata, 2, -1), 2) return canceled_output"},{"question":"# Problem: Tensor Storage Manipulation Objective Design a function to manipulate tensor storage directly. The goal is to create a tensor, transform its storage, and verify the consistency of shared storage between multiple tensors. Function Signature ```python def manipulate_storage() -> torch.Tensor: pass ``` Instructions 1. **Create a Tensor**: Start by creating a 1-dimensional floating-point tensor `t` filled with ones and of length 10. 2. **Extract Storage**: Extract the storage of the created tensor. 3. **Clone and Modify Storage**: - Clone this storage to create a new storage `s1`. - Fill `s1` with zeros. 4. **Modify Tensor with New Storage**: - Update tensor `t` to use this new storage `s1` while keeping the original tensor\'s properties (offset, stride, size). 5. **Create a New Tensor**: - Create another tensor `t2` that uses the same storage `s1`, this time with a different view, e.g., shape (2, 5). 6. **Verification**: - Verify that changes to `t` also reflect in `t2` by modifying one and asserting the expected values on the other. Return `t2` at the end. Notes - Refrain from using direct modification of storages in production-level code; this exercise is designed for educational purposes. - Use PyTorch\'s standard tensor functions wherever possible. Constraints - Performance is not a primary concern for this exercise. - Ensure compatibility with latest version of PyTorch. Example ```python t2 = manipulate_storage() print(t2) ``` Expected Output: ``` tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) ```","solution":"import torch def manipulate_storage() -> torch.Tensor: # Step 1: Create a 1-dimensional floating-point tensor `t` filled with ones of length 10 t = torch.ones(10, dtype=torch.float) # Step 2: Extract the storage of the created tensor `t` storage = t.storage() # Step 3: Clone this storage to create a new storage `s1` s1 = storage.clone() # Step 4: Fill `s1` with zeros s1.fill_(0) # Step 5: Update tensor `t` to use `s1` while keeping original properties (size, offset, stride) t.set_(s1, t.storage_offset(), t.size(), t.stride()) # Step 6: Create another tensor `t2` that uses the same storage `s1`, with a shape (2, 5) t2 = torch.tensor([], dtype=torch.float).set_(s1, 0, (2, 5), (5, 1)) return t2"},{"question":"NNTP Client Implementation # Objective You are tasked with writing a Python function that connects to an NNTP server, retrieves a list of new articles from a specified newsgroup posted within the last 3 days, and returns their subjects. This task will test your understanding of the \\"nntplib\\" module and your ability to manipulate dates and handle network connections. # Problem Description Implement a function `get_recent_articles_subjects(server: str, group: str) -> list` that retrieves the subjects of new articles from a specified newsgroup posted within the last 3 days. # Function Signature ```python def get_recent_articles_subjects(server: str, group: str) -> list: pass ``` # Input - `server` (str): The hostname of the NNTP server to connect to. - `group` (str): The name of the newsgroup from which to retrieve new articles. # Output - Returns a list of strings, where each string is the subject of a new article posted in the specified newsgroup within the last 3 days. # Constraints - The function should correctly handle connection and authentication errors (if any). - The function should handle cases where the specified newsgroup does not exist or has no new articles within the last 3 days. - Use the `nntplib` module as described in the provided documentation. # Example ```python # Example usage server = \\"news.gmane.io\\" group = \\"gmane.comp.python.committers\\" subjects = get_recent_articles_subjects(server, group) print(subjects) # Output might be: [\'Re: Commit privileges for Łukasz Langa\', \'Re: 3.2 alpha 2 freeze\', ...] ``` # Notes - Make use of the `newgroups` and `group` methods to filter and retrieve the relevant articles. - Utilize the `over` method to get the subject headers of the articles. - Ensure the function handles invalid server responses gracefully by using the appropriate exception handling provided by `nntplib`.","solution":"import nntplib from datetime import datetime, timedelta from email.header import decode_header def get_recent_articles_subjects(server: str, group: str) -> list: try: with nntplib.NNTP(server) as client: # Calculate the date 3 days ago from_date = (datetime.now() - timedelta(days=3)).strftime(\'%Y%m%d\') # Attempt to select the group resp, count, first, last, name = client.group(group) # Get overview format for article ranges response, overviews = client.over((first, last)) # Prepare list for subjects subjects = [] for overview in overviews: number, subject, frm, date, mess_id, refs, size, lines = overview # Decode subject header if necessary decoded_subject, charset = decode_header(subject)[0] if isinstance(decoded_subject, bytes): decoded_subject = decoded_subject.decode(charset or \'utf-8\') subjects.append(decoded_subject) return subjects except nntplib.NNTPTemporaryError as e: print(f\'Temporarily unavailable: {e}\') return [] except nntplib.NNTPPermanentError as e: print(f\'Permanent error: {e}\') return [] except Exception as e: print(f\'An error occurred: {e}\') return []"},{"question":"PyTorch Configuration Analysis In this assignment, you will write a Python function to extract and interpret specific configuration details from the PyTorch environment using the `torch.__config__` module. You are required to implement a function `get_pytorch_config_info()` that queries the current configuration of PyTorch and returns specific details. # Function Signature ```python def get_pytorch_config_info() -> dict: pass ``` # Requirements Your function should: 1. Call both `torch.__config__.show()` and `torch.__config__.parallel_info()`. 2. Parse the output of these functions to extract the following information: - **Backend**: The computation backend being used (e.g., CUDA, CPU). - **CUDA Version**: The version of CUDA being used, if applicable. - **Number of Threads**: The number of OpenMP threads being used. - **GPU Allocations**: If using CUDA, list each GPU and its memory allocation. 3. Format the extracted information into a dictionary with the following keys: - `backend`: (string) The computation backend. - `cuda_version`: (string) The CUDA version, or `None` if not applicable. - `num_threads`: (integer) The number of OpenMP threads. - `gpu_allocations`: (list of dicts) Each dictionary contains `GPU id` and `memory`. # Example Output An example output of the function would be: ```python { \\"backend\\": \\"CUDA\\", \\"cuda_version\\": \\"11.1\\", \\"num_threads\\": 8, \\"gpu_allocations\\": [ {\\"GPU id\\": 0, \\"memory\\": \\"10 GB\\"}, {\\"GPU id\\": 1, \\"memory\\": \\"10 GB\\"} ] } ``` # Additional Notes - Your function should handle cases where CUDA is not available, returning `None` for the `cuda_version` and an empty list for `gpu_allocations`. - Ensure your function can handle variations in how `show()` and `parallel_info()` present their output. - There isn\'t a need to recreate the entire environment; rather, you need to simulate parsing the output strings from these functions. By completing this task, you\'ll demonstrate your ability to interact with and interpret PyTorch configuration information, a valuable skill for debugging and optimization. Happy coding!","solution":"import torch def get_pytorch_config_info() -> dict: # Call torch.__config__.show() and torch.__config__.parallel_info() backend = \\"CUDA\\" if torch.cuda.is_available() else \\"CPU\\" cuda_version = torch.version.cuda if torch.cuda.is_available() else None num_threads = torch.get_num_threads() gpu_allocations = [] if torch.cuda.is_available(): for i in range(torch.cuda.device_count()): gpu_allocations.append({ \\"GPU id\\": i, \\"memory\\": f\\"{torch.cuda.get_device_properties(i).total_memory / (1024 ** 3):.2f} GB\\" }) # Return the required dictionary return { \\"backend\\": backend, \\"cuda_version\\": cuda_version, \\"num_threads\\": num_threads, \\"gpu_allocations\\": gpu_allocations }"},{"question":"You are tasked with analyzing and visualizing the famous \\"tips\\" dataset using the seaborn library to understand tipping behavior. You will need to load the dataset, create various swarm plots, and customize these plots to highlight different aspects of the data. **Instructions:** 1. Load the \\"tips\\" dataset from seaborn\'s provided datasets. 2. Create the following visualizations: 1. A horizontal swarm plot of `total_bill` categorized by `day` with points colored by `sex`. 2. A vertical swarm plot of `total_bill` categorized by `time` colored by `size` using a `\\"deep\\"` palette. 3. A faceted swarm plot using `sns.catplot` with `total_bill` categorized by `day`, faceted by `time`, and colored by `smoker`. 3. Customize the plots to: 1. Use different markers (\'o\' for the first plot and \'x\' for the second) and set a linewidth of 1 for both. 2. Ensure the points do not overlap significantly by adjusting the `size` parameter accordingly in all plots. 3. Adjust the `native_scale` parameter where necessary to reflect the native scale of the variables. **Expected Output:** ```python import seaborn as sns import matplotlib.pyplot as plt # Set a theme for the plots sns.set_theme(style=\\"whitegrid\\") # Step 1: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create visualizations # Part 2.i: Horizontal swarm plot plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", marker=\'o\', linewidth=1, size=4) plt.title(\\"Horizontal Swarm Plot of Total Bill by Day and Sex\\") plt.show() # Part 2.ii: Vertical swarm plot plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"size\\", palette=\\"deep\\", marker=\'x\', linewidth=1, size=4) plt.title(\\"Vertical Swarm Plot of Total Bill by Time and Size\\") plt.show() # Part 2.iii: Faceted swarm plot faceted_plot = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", col=\\"time\\", marker=\'o\', linewidth=1, size=4, native_scale=False ) faceted_plot.fig.suptitle(\\"Faceted Swarm Plot of Total Bill by Day and Smoker Status, Faceted by Time\\") faceted_plot.fig.subplots_adjust(top=0.9) # Adjust title position plt.show() ``` **Constraints and Requirements:** - You must use seaborn\'s `load_dataset` to load the \\"tips\\" dataset. - Ensure that the plots clearly illustrate the relationships and avoid excessive overlap of points. - Use appropriate sizes and markers for better clarity and differentiation in the plots. - Title each plot appropriately and ensure the visualizations are easy to interpret.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_swarm_plots(): # Set a theme for the plots sns.set_theme(style=\\"whitegrid\\") # Step 1: Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Step 2: Create visualizations # Part 2.i: Horizontal swarm plot plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", marker=\'o\', linewidth=1, size=4) plt.title(\\"Horizontal Swarm Plot of Total Bill by Day and Sex\\") plt.show() # Part 2.ii: Vertical swarm plot plt.figure(figsize=(8, 6)) sns.swarmplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"size\\", palette=\\"deep\\", marker=\'x\', linewidth=1, size=4) plt.title(\\"Vertical Swarm Plot of Total Bill by Time and Size\\") plt.show() # Part 2.iii: Faceted swarm plot faceted_plot = sns.catplot( data=tips, kind=\\"swarm\\", x=\\"day\\", y=\\"total_bill\\", hue=\\"smoker\\", col=\\"time\\", marker=\'o\', linewidth=1, size=4, native_scale=False ) faceted_plot.fig.suptitle(\\"Faceted Swarm Plot of Total Bill by Day and Smoker Status, Faceted by Time\\") faceted_plot.fig.subplots_adjust(top=0.9) # Adjust title position plt.show()"},{"question":"# Custom Content Manager Implementation Objective: Create a custom content manager by extending the `ContentManager` class to handle a specific set of MIME types. Implement handler functions for these MIME types and integrate them into the content manager. Problem Statement: You are required to design a custom content manager named `CustomContentManager` that extends the `ContentManager` class. This custom manager should handle content for the following MIME types: 1. `application/json` 2. `image/png` (use base64 encoding) Additionally, you will implement handler functions for these MIME types and register them accordingly. The custom manager should provide methods for both `get_content` and `set_content`. Requirements: 1. **CustomContentManager Class**: - Extend the `ContentManager` class. - Implement and register handler functions for `application/json` and `image/png` MIME types. 2. **Handler Functions**: - **JSON Handler**: Convert JSON content (text) to a Python dictionary for `get_content` and create JSON text from a Python dictionary for `set_content`. - **Image Handler**: Handle PNG image content by encoding it to base64 for `set_content` and decoding it from base64 for `get_content`. 3. **Integration**: - Register the handler functions with appropriate MIME types in the custom manager. Function Signatures: ```python class CustomContentManager(ContentManager): def __init__(self): super().__init__() # Register the handlers here @staticmethod def json_get_handler(msg, *args, **kwargs): # Extract JSON content and return as a dictionary @staticmethod def json_set_handler(msg, content, *args, **kwargs): # Convert a dictionary to JSON text and set as message content @staticmethod def image_get_handler(msg, *args, **kwargs): # Decode base64 PNG content and return as bytes @staticmethod def image_set_handler(msg, content, *args, **kwargs): # Encode bytes to base64 and set as PNG content ``` Input and Output Examples: 1. **JSON Handling**: ```python from email.message import EmailMessage manager = CustomContentManager() msg = EmailMessage() msg.set_type(\'application/json\') manager.set_content(msg, \'{\\"key\\": \\"value\\"}\') print(manager.get_content(msg)) # Expected Output: {\'key\': \'value\'} ``` 2. **Image Handling**: ```python from email.message import EmailMessage import base64 manager = CustomContentManager() msg = EmailMessage() msg.set_type(\'image/png\') image_data = base64.b64encode(b\'binary image data\') manager.set_content(msg, b\'binary image data\') print(manager.get_content(msg)) # Expected Output: b\'binary image data\' ``` Constraints: - Assume valid MIME types are always set in the messages. - Handle only the specified MIME types in this manager. - For image content, ensure base64 encoding/decoding is used properly. Performance: - Ensure the implementation can handle larger content sizes effectively, especially for image encoding/decoding.","solution":"import json import base64 from email.message import Message class ContentManager: def __init__(self): self._handlers = {} def register_handler(self, mime_type, get_handler, set_handler): self._handlers[mime_type] = (get_handler, set_handler) def get_content(self, msg, *args, **kwargs): get_handler, _ = self._handlers[msg.get_content_type()] return get_handler(msg, *args, **kwargs) def set_content(self, msg, content, *args, **kwargs): _, set_handler = self._handlers[msg.get_content_type()] set_handler(msg, content, *args, **kwargs) class CustomContentManager(ContentManager): def __init__(self): super().__init__() self.register_handler(\'application/json\', self.json_get_handler, self.json_set_handler) self.register_handler(\'image/png\', self.image_get_handler, self.image_set_handler) @staticmethod def json_get_handler(msg, *args, **kwargs): return json.loads(msg.get_payload()) @staticmethod def json_set_handler(msg, content, *args, **kwargs): msg.set_payload(json.dumps(content)) msg.set_type(\'application/json\') @staticmethod def image_get_handler(msg, *args, **kwargs): return base64.b64decode(msg.get_payload()) @staticmethod def image_set_handler(msg, content, *args, **kwargs): msg.set_payload(base64.b64encode(content).decode()) msg.set_type(\'image/png\')"},{"question":"**Python Coding Assessment Question: Analyzing Bytecode Instructions** **Objective:** Your task is to implement a function that analyzes the bytecode instructions of a given Python function and returns a dictionary. The dictionary should have opcode names as keys and the count of those opcodes as values. This will help you understand the distribution of various operations within the bytecode of the given function. **Problem Statement:** Write a function `analyze_bytecode_distribution(func)` that takes a single argument `func`, which is a Python function. The function should disassemble the given function using the `dis` module and analyze its bytecode instructions. The function should return a dictionary where the keys are the names of opcodes and the values are their respective counts in the bytecode of the given function. **Input:** - `func`: A Python function whose bytecode needs to be analyzed. **Output:** - A dictionary with opcode names as keys and their counts as values. **Constraints:** - You can assume that the input will always be a valid Python function. - You should handle cases where the function might contain no bytecode instructions gracefully. - Be efficient in counting the opcode instructions. **Example:** ```python import dis def myfunc(x): return x + 1 result = analyze_bytecode_distribution(myfunc) print(result) # Output example: {\'LOAD_FAST\': 1, \'LOAD_CONST\': 1, \'BINARY_ADD\': 1, \'RETURN_VALUE\': 1} ``` **Additional Notes:** - Use the `dis` module and its `Bytecode` class to disassemble the function and analyze bytecode instructions. - You may find iterating over the `Bytecode` object particularly useful for generating the opcode counts. **Hint:** - The `dis.Bytecode` class provides an iterator over `Instruction` instances, where each `Instruction` contains details about the bytecode operation. Use the `opname` attribute of `Instruction` to get the name of the opcode. ```python def analyze_bytecode_distribution(func): # Implement your solution here pass ```","solution":"import dis from collections import defaultdict def analyze_bytecode_distribution(func): Analyzes the bytecode instructions of the given Python function and returns a dictionary with opcode names as keys and their counts as values. bytecode = dis.Bytecode(func) opcode_counts = defaultdict(int) for instruction in bytecode: opcode_counts[instruction.opname] += 1 return dict(opcode_counts)"},{"question":"**Coding Assessment Question** # Objective Implement a class that makes use of both shallow and deep copy operations. This will assess your understanding of the `copy` module\'s functionalities and your ability to integrate these into a class with custom behavior. # Requirements You are required to implement a class `CustomList` that contains a list of integers and incorporates custom shallow and deep copy behaviors. # Instructions 1. Implement a class named `CustomList` with the following: - An initializer method `__init__(self, initial_list: list)` which initializes an instance with the provided list of integers. - A method `add(self, value: int)` that appends the given integer to the list. - A method `get_list(self) -> list` that returns the current list. 2. Implement custom `__copy__()` and `__deepcopy__()` methods for the `CustomList` class: - `__copy__(self)` should return a shallow copy of the instance. - `__deepcopy__(self, memo)` should return a deep copy of the instance. # Expected Input and Output Formats - The `CustomList` constructor (__init__) takes an initial list of integers as input. - The `add` method takes a single integer as input and returns `None`. - The `get_list` method returns the current list of integers. # Constraints - The initial list passed to `CustomList` will only contain integers. - Each list will contain at most 10^3 integers. # Performance Requirements - The operations should be optimized for both time and space complexity with the understanding that the deep copy will inherently consume more resources. # Example Usage ```python from copy import copy, deepcopy # Create an instance original = CustomList([1, 2, 3]) # Shallow copy shallow_copied = copy(original) shallow_copied.add(4) # Deep copy deep_copied = deepcopy(original) deep_copied.add(5) print(original.get_list()) # Output: [1, 2, 3] print(shallow_copied.get_list()) # Output: [1, 2, 3, 4] print(deep_copied.get_list()) # Output: [1, 2, 3, 5] ``` Ensure your implementation adheres to the specified requirements and passes all the provided test cases.","solution":"import copy class CustomList: def __init__(self, initial_list: list): self._list = initial_list def add(self, value: int): self._list.append(value) def get_list(self) -> list: return self._list def __copy__(self): return CustomList(self._list[:]) def __deepcopy__(self, memo): new_list = copy.deepcopy(self._list, memo) return CustomList(new_list)"},{"question":"# Multiclass and Multilabel Classification with Scikit-learn **Objective**: You need to implement a multiclass and multilabel classification algorithm using scikit-learn to classify images of fruits based on their type and color. **Description**: You are given a dataset that consists of images of fruits. Each image is associated with two labels: 1. Type of fruit (e.g., \\"apple\\", \\"orange\\", \\"pear\\", etc.) 2. Color of fruit (e.g., \\"red\\", \\"green\\", \\"yellow\\", etc.) Your task is to build a multiclass-multioutput classifier that can predict both the type and color of the fruit from the image features using scikit-learn. **Input**: - `X_train`: A 2D numpy array of shape `(n_samples, n_features)` representing the training data. - `y_train`: A 2D numpy array of shape `(n_samples, 2)` where the first column represents the fruit type and the second column represents the color. - `X_test`: A 2D numpy array of shape `(n_test_samples, n_features)` representing the test data. **Output**: - `y_pred`: A 2D numpy array of shape `(n_test_samples, 2)` where the first column represents the predicted fruit type and the second column represents the predicted color for the test data. **Constraints**: - You must use scikit-learn for building the classifier. - Choose appropriate classifiers and strategies from scikit-learn\'s `multiclass` and `multioutput` modules. **Requirements**: 1. Implement a classifier by using `OneVsRestClassifier` or `OneVsOneClassifier` for multiclass classification. 2. Implement a `MultiOutputClassifier` to handle the multiple outputs. 3. Ensure the classifier handles the input and outputs in the specified format. Here is the expected prototype of the code: ```python import numpy as np from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier def train_and_predict_multiclass_multioutput(X_train, y_train, X_test): Train a multiclass-multioutput classifier and predict the labels for the test data. Parameters: X_train (np.ndarray): Training data of shape (n_samples, n_features) y_train (np.ndarray): Training labels of shape (n_samples, 2) X_test (np.ndarray): Test data of shape (n_test_samples, n_features) Returns: y_pred (np.ndarray): Predicted labels of shape (n_test_samples, 2) # Split y_train into two separate arrays: one for fruit type and one for fruit color # Implement OneVsRestClassifier or OneVsOneClassifier for multiclass classification # Implement MultiOutputClassifier to handle multiple outputs # Fit the classifier on the training data # Predict the labels for the test data pass # Example usage: if __name__ == \\"__main__\\": # Sample data X_train = np.random.rand(100, 20) # 100 samples, 20 features each y_train = np.array([[\'apple\', \'red\'], [\'pear\', \'green\'], [\'orange\', \'yellow\']] * 34)[:100] X_test = np.random.rand(10, 20) # 10 test samples y_pred = train_and_predict_multiclass_multioutput(X_train, y_train, X_test) print(y_pred) ``` **Notes**: - Test your implementation with example datasets to ensure correctness. - Feel free to use any scikit-learn classifiers for multiclass and multioutput classification.","solution":"import numpy as np from sklearn.multioutput import MultiOutputClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import LabelEncoder def train_and_predict_multiclass_multioutput(X_train, y_train, X_test): Train a multiclass-multioutput classifier and predict the labels for the test data. Parameters: X_train (np.ndarray): Training data of shape (n_samples, n_features) y_train (np.ndarray): Training labels of shape (n_samples, 2) X_test (np.ndarray): Test data of shape (n_test_samples, n_features) Returns: y_pred (np.ndarray): Predicted labels of shape (n_test_samples, 2) # Split y_train into two separate arrays: one for fruit type and one for fruit color y_train_type = y_train[:, 0] y_train_color = y_train[:, 1] le_type = LabelEncoder() le_color = LabelEncoder() # Encode the labels y_train_type_encoded = le_type.fit_transform(y_train_type) y_train_color_encoded = le_color.fit_transform(y_train_color) # Stack the encoded labels back into a multioutput format y_train_encoded = np.vstack((y_train_type_encoded, y_train_color_encoded)).T # Create and train a MultiOutputClassifier with RandomForestClassifier classifier = MultiOutputClassifier(RandomForestClassifier(random_state=42)) classifier.fit(X_train, y_train_encoded) # Predict the labels for the test data y_pred_encoded = classifier.predict(X_test) # Decode the predictions y_pred_type = le_type.inverse_transform(y_pred_encoded[:, 0]) y_pred_color = le_color.inverse_transform(y_pred_encoded[:, 1]) # Stack the decoded predictions back into the original format y_pred = np.vstack((y_pred_type, y_pred_color)).T return y_pred"},{"question":"**Question: Seaborn Plot Customization and Layout** You have been provided with a dataset of tips collected by waiters in a restaurant. Each row in the dataset represents a single meal, with the following columns: - `total_bill`: The total bill (including tip). - `tip`: The tip amount. - `sex`: Gender of the person paying for the meal (either \\"Male\\" or \\"Female\\"). - `smoker`: Whether the group included smokers (either \\"Yes\\" or \\"No\\"). - `day`: Day of the week (either \\"Thur\\", \\"Fri\\", \\"Sat\\", or \\"Sun\\"). - `time`: Meal time (either \\"Lunch\\" or \\"Dinner\\"). - `size`: Number of people at the table. Your task is to use Seaborn to create and customize plots based on this dataset. Specifically, you need to implement a function `plot_tips_data` that reads the data from a CSV file and generates the following plots: 1. **Scatter Plot**: A scatter plot of `total_bill` vs. `tip` points, with different colors for points based on the `time` of meal. 2. **Faceted Bar Plot**: A set of bar plots displaying the average `total_bill` for each day of the week, separated by `sex` and `smoker` status in a faceted grid layout. 3. **Histogram**: A histogram of `tip` amounts with different colors for each `sex`. 4. **Save Plots**: Save each generated plot to a PNG file with appropriate filenames. **Function Signature:** ```python import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_tips_data(csv_file_path: str) -> None: pass ``` **Input:** - `csv_file_path` (str): The path to the CSV file containing the dataset. **Output:** - The function does not return any values. It saves the generated plots as PNG files. **Constraints:** - The dataset file must exist at the given file path. - Use Seaborn\'s `so.Plot` (from `seaborn.objects`) module for plot generation. **Performance Requirements:** - Efficiently read and process the CSV file. - Ensure plots are correctly saved with minimal computational overhead. **Example Usage:** ```python plot_tips_data(\'path/to/tips.csv\') # This should read the data, create the specified plots, and save them to PNG files. ``` **Notes:** - Apply plot customizations such as titles, axis labels, legends, etc. to ensure the plots are informative and visually appealing. - Make sure each subplot is spaced appropriately to avoid overlap. - Use reasonable dimensions for figures to ensure clarity when saved as images.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_tips_data(csv_file_path: str) -> None: # Read the dataset data = pd.read_csv(csv_file_path) # Scatter Plot: total_bill vs. tip colored by time plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'total_bill\', y=\'tip\', hue=\'time\', data=data) scatter_plot.set_title(\'Total Bill vs Tip by Time\') scatter_plot.set_xlabel(\'Total Bill\') scatter_plot.set_ylabel(\'Tip\') plt.legend(title=\'Time\') plt.savefig(\'scatter_total_bill_vs_tip_by_time.png\') plt.close() # Faceted Bar Plot: Average total_bill for each day, separated by sex and smoker g = sns.catplot( data=data, kind=\'bar\', x=\'day\', y=\'total_bill\', hue=\'sex\', col=\'smoker\', ci=None, palette=\'muted\', height=4, aspect=0.7 ) g.set_axis_labels(\\"Day\\", \\"Average Total Bill\\") g.set_titles(\\"{col_name} Smoker\\") g.fig.suptitle(\'Average Total Bill per Day by Sex and Smoker\', y=1.03) g.add_legend(title=\'Sex\') plt.savefig(\'faceted_bar_total_bill_by_day_sex_smoker.png\') plt.close() # Histogram: Tip amounts colored by sex plt.figure(figsize=(10, 6)) histogram = sns.histplot(data=data, x=\'tip\', hue=\'sex\', multiple=\'stack\', palette=\'pastel\', bins=20) histogram.set_title(\'Tip Amount Distribution by Sex\') histogram.set_xlabel(\'Tip\') histogram.set_ylabel(\'Count\') plt.legend(title=\'Sex\') plt.savefig(\'histogram_tip_by_sex.png\') plt.close()"},{"question":"# Custom Estimator Task Objective Create a custom scikit-learn compatible estimator called `CustomKNNClassifier` which implements the k-nearest neighbors classification algorithm. Your implementation should strictly adhere to scikit-learn standards and conventions as specified in the provided documentation. Requirements 1. **Initialization**: The `CustomKNNClassifier` should take the following parameters: - `n_neighbors`: number of neighbors to use for classification (default: 5). - `random_state`: integer, RandomState instance or None, optional (default: None). Use it for shuffling the dataset in a reproducible way. 2. **fit() method**: - Accepts `X` (array-like of shape `(n_samples, n_features)`) and `y` (array-like of shape `(n_samples,)`). - Validates the inputs using `validate_data`. - Stores the classes seen during fitting. - The method should return the fitted estimator. 3. **predict() method**: - Accepts `X` (array-like of shape `(n_samples, n_features)`). - Validates the inputs. - Implements the k-nearest neighbors classification. - Returns the predicted classes for each sample in `X`. 4. **score() method**: - Accepts `X` (array-like of shape `(n_samples, n_features)`) and `y` (array-like of shape `(n_samples,)`). - Calculates the accuracy of the classifier on the provided test data and labels. 5. **Handling Randomness**: - Use `check_random_state` for any random operations. 6. **Validation**: - Use the `check_estimator` function from `sklearn.utils.estimator_checks` to ensure your custom estimator is scikit-learn compliant. Constraints 1. The implementation should follow proper scikit-learn interface and guidelines. 2. The code should be written in a modular fashion and should not include any standalone script outside the class definition. 3. Use numpy for array manipulations and distance computations. 4. Assume that hyperparameters are of valid types and within reasonable ranges. Example Usage ```python from sklearn.utils.estimator_checks import check_estimator # Instantiate and fit the model model = CustomKNNClassifier(n_neighbors=3, random_state=42) model.fit(X_train, y_train) # Predict on new data predictions = model.predict(X_test) # Assess accuracy accuracy = model.score(X_test, y_test) # Check if it adheres to the scikit-learn interface check_estimator(model) ``` **Note**: This is an open-book challenge; you are expected to refer to the provided documentation to ensure compliance with the scikit-learn ecosystem. Input Format - The `X_train`, `y_train`, `X_test`, `y_test` variables are provided as numpy arrays. Output Format - The `CustomKNNClassifier` class definition.","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_is_fitted, check_X_y, check_array from sklearn.utils.multiclass import unique_labels from sklearn.utils import check_random_state class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5, random_state=None): self.n_neighbors = n_neighbors self.random_state = random_state def fit(self, X, y): # Validating the input X, y = check_X_y(X, y) self.classes_ = unique_labels(y) self.X_ = X self.y_ = y return self def predict(self, X): # Check if fit had been called check_is_fitted(self, [\'X_\', \'y_\']) # Validate input X = check_array(X) # Prediction predictions = [] for x in X: distances = np.sqrt(np.sum((self.X_ - x) ** 2, axis=1)) nearest_indices = np.argsort(distances)[:self.n_neighbors] nearest_labels = self.y_[nearest_indices] prediction = np.bincount(nearest_labels).argmax() predictions.append(prediction) return np.array(predictions) def score(self, X, y): # Validating the input X, y = check_X_y(X, y) # Calculate predictions y_pred = self.predict(X) # Calculate accuracy return np.mean(y == y_pred)"},{"question":"# PyTorch FX Graph Transformation Challenge Objective: To demonstrate your mastery of PyTorch\'s FX module and the ATen Intermediate Representation (IR), you are tasked with implementing a graph transformation pass that involves identifying and modifying specific operations in a given computation graph. Your implementation should: 1. Replace all instances of `torch.ops.aten.add.Tensor` with `torch.ops.aten.mul.Tensor`. 2. Insert a `torch.ops.aten.relu.default` operation immediately after every `torch.ops.aten.mul.Tensor` node previously replaced. 3. Ensure that the resulting graph maintains the same structure and semantics as the original, with the specified transformations applied. Input: - A traced PyTorch model or a specific `GraphModule` object representing a computation graph. Constraints: 1. You may assume that the input graph contains a valid structure and nodes for the transformations. 2. Utilize the `torch.fx` module and follow good practices for graph manipulation. Example: Given the following function in a traced model: ```python def example_function(x, y): return x + y ``` After applying your transformations, the function should be: ```python def example_function(x, y): z = x * y return torch.relu(z) ``` Implementation Details: Create a function `transform_graph` which takes as input a `torch.fx.GraphModule` and returns the transformed `torch.fx.GraphModule`. Implement the function to apply the required transformations as follows: ```python import torch def transform_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: # Replace add with mul for node in gm.graph.nodes: if node.op == \\"call_function\\" and node.target == torch.ops.aten.add.Tensor: node.target = torch.ops.aten.mul.Tensor with gm.graph.inserting_after(node): new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, args=(node,)) node.replace_all_uses_with(new_relu_node) new_relu_node.args = (node,) gm.graph.lint() # Run consistency checks on the graph gm.recompile() # Recompile the graph module to register changes return gm # Example usage: # traced_module = torch.fx.symbolic_trace(model_function) # transformed_module = transform_graph(traced_module) # print(transformed_module.code) ``` Note: - Ensure the function follows the pattern of replacing nodes and inserting new nodes as described. - Validate transformations by re-tracing or recompiling the graph to confirm the transformations were correctly applied. Submission: - Submit your `transform_graph` function implementation along with a test case demonstrating the transformation on a sample graph. Use print statements to display the original and transformed graph structures.","solution":"import torch import torch.fx def transform_graph(gm: torch.fx.GraphModule) -> torch.fx.GraphModule: for node in gm.graph.nodes: if node.op == \'call_function\' and node.target == torch.ops.aten.add.Tensor: node.target = torch.ops.aten.mul.Tensor with gm.graph.inserting_after(node): new_relu_node = gm.graph.call_function(torch.ops.aten.relu.default, (node,)) node.replace_all_uses_with(new_relu_node) new_relu_node.args = (node,) gm.graph.lint() # Run consistency checks on the graph gm.recompile() # Recompile the graph module to register changes return gm # Example usage to test manually def example_function(x, y): return torch.ops.aten.add.Tensor(x, y) if __name__ == \\"__main__\\": traced_module = torch.fx.symbolic_trace(example_function) print(f\\"Original Graph:n{traced_module.graph}\\") transformed_module = transform_graph(traced_module) print(f\\"Transformed Graph:n{transformed_module.graph}\\")"},{"question":"Objective Demonstrate your understanding of the `email.message.Message` class by building and manipulating an email message object. This exercise will test your ability to work with email headers, payloads, and serialization methods. Problem Statement You are required to create a function `compose_email(subject: str, sender: str, recipient: str, body: str, attachments: list = []) -> str` that constructs an email message and returns its string representation. The function should: 1. Create an instance of the `email.message.Message` class. 2. Set the basic headers for the email (Subject, From, To). 3. Set the email body as the payload. 4. Attach any additional files provided in the `attachments` list, where each attachment is a dictionary with `filename` and `content` keys. 5. Ensure the attachment is properly handled as a multipart message if any attachments are provided. 6. Return the email message as a string by using the `as_string()` method. Input - `subject` (str): The subject of the email. - `sender` (str): The email address of the sender. - `recipient` (str): The email address of the recipient. - `body` (str): The body of the email. - `attachments` (list): A list of dictionaries, where each dictionary contains: - `filename` (str): The name of the file to attach. - `content` (str): The content of the file to attach. Output - (str): The string representation of the resulting email message. Example ```python from email.message import Message def compose_email(subject, sender, recipient, body, attachments=[]): # Your code here subject = \\"Test Email\\" sender = \\"sender@example.com\\" recipient = \\"recipient@example.com\\" body = \\"This is the body of the email.\\" attachments = [ {\\"filename\\": \\"file1.txt\\", \\"content\\": \\"This is the content of file1.\\"}, {\\"filename\\": \\"file2.txt\\", \\"content\\": \\"This is the content of file2.\\"} ] email_string = compose_email(subject, sender, recipient, body, attachments) print(email_string) ``` Expected Output: ``` Content-Type: multipart/mixed; boundary=\\"...some boundary...\\" MIME-Version: 1.0 From: sender@example.com To: recipient@example.com Subject: Test Email --...some boundary... Content-Type: text/plain; charset=\\"us-ascii\\" MIME-Version: 1.0 Content-Transfer-Encoding: 7bit This is the body of the email. --...some boundary... Content-Type: text/plain; charset=\\"us-ascii\\"; name=\\"file1.txt\\" Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\\"file1.txt\\" This is the content of file1. --...some boundary... Content-Type: text/plain; charset=\\"us-ascii\\"; name=\\"file2.txt\\" Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\\"file2.txt\\" This is the content of file2. --...some boundary...-- ``` Constraints 1. Use the `email.message.Message` class only. 2. Ensure the response mimics a real email format abiding by RFC standards. Additional Notes - You may use auxiliary functions or methods within `email.message.Message` as necessary. - Make sure to handle the encapsulation of multipart messages correctly if there are attachments.","solution":"from email.message import EmailMessage from email.policy import default def compose_email(subject, sender, recipient, body, attachments=[]): message = EmailMessage() message[\'Subject\'] = subject message[\'From\'] = sender message[\'To\'] = recipient message.set_content(body) for attachment in attachments: message.add_attachment(attachment[\'content\'], filename=attachment[\'filename\']) return message.as_string()"},{"question":"Advanced Data Visualization with Seaborn Problem Statement You are provided with a dataset containing information about customers\' purchases and their satisfaction levels at a retail store. You need to use the seaborn library to create several visualizations that will help in understanding the relationships between different variables. Dataset Description - `customer_id`: Unique identifier for a customer. - `purchase_amount`: Total amount spent by the customer in USD. - `satisfaction_level`: Customer satisfaction level rated from 1 to 5. - `age_group`: Categorical data indicating the age group of the customer (\'Teen\', \'Adult\', \'Senior\'). - `membership`: Boolean indicating if the customer is a member of the store (True/False). Assume the dataset is stored as a CSV file named `customer_data.csv`. Requirements 1. **FacetGrid Plot**: - Create a `FacetGrid` to show the distribution of `purchase_amount` faceted by `age_group`. - Use histograms to visualize the distributions. - Customize the grid to have each subplot of height 4 and aspect ratio 1.5. 2. **PairGrid Plot**: - Create a `PairGrid` to visualize pairwise relationships between `purchase_amount`, `satisfaction_level`, and `age_group`. - Plot histograms on the diagonal and scatter plots on the off-diagonal. - Use different colors for different `age_group` categories. - Add a legend to the plot. 3. **Customized Plot**: - Implement a custom plotting function to show the mean and standard deviation of `purchase_amount` for each `satisfaction_level`. - Use a `FacetGrid` to display these custom plots with rows corresponding to `membership`. Code Implementation Implement the above requirements in separate functions. The functions should be named as follows: - `create_facetgrid_histogram(df)` - `create_pairgrid_relationship(df)` - `create_custom_facetgrid(df)` Each function should take a DataFrame as input and produce the corresponding plot. Display the plots for validation. Constraints 1. The solution should handle missing data gracefully. 2. Use the `sns.set_theme(style=\\"ticks\\")` to set the plotting theme. 3. Ensure that the facet labels and legends are properly displayed. Expected Output 1. A `FacetGrid` with histograms showing the distribution of `purchase_amount` faceted by `age_group`. 2. A `PairGrid` with pairwise relationships and appropriate legend for `age_group`. 3. A `FacetGrid` with custom plots showing the mean and standard deviation of `purchase_amount` for each `satisfaction_level`, faceted by `membership`. Example Usage ```python import pandas as pd # Load the dataset df = pd.read_csv(\'customer_data.csv\') # Create and display the required plots create_facetgrid_histogram(df) create_pairgrid_relationship(df) create_custom_facetgrid(df) ``` Submit the complete implementation with the function definitions and example usage.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt import numpy as np sns.set_theme(style=\\"ticks\\") def create_facetgrid_histogram(df): g = sns.FacetGrid(df, col=\\"age_group\\", height=4, aspect=1.5) g.map(sns.histplot, \\"purchase_amount\\", kde=False) g.add_legend() plt.show() def create_pairgrid_relationship(df): g = sns.PairGrid(df, vars=[\\"purchase_amount\\", \\"satisfaction_level\\"], hue=\\"age_group\\", palette=\\"Set1\\") g.map_diag(sns.histplot) g.map_offdiag(sns.scatterplot) g.add_legend() plt.show() def custom_plot(x, y, **kwargs): data = kwargs.pop(\\"data\\") stats = data.groupby(x).agg(mean=(\\"purchase_amount\\", \\"mean\\"), std=(\\"purchase_amount\\", \\"std\\")).reset_index() plt.errorbar(stats[x], stats[\'mean\'], yerr=stats[\'std\'], fmt=\'o\', **kwargs) def create_custom_facetgrid(df): g = sns.FacetGrid(df, row=\\"membership\\", height=4, aspect=1.5) g.map_dataframe(custom_plot, \'satisfaction_level\', None) g.add_legend() plt.show()"},{"question":"# Advanced Python C Extension: Custom Matrix Type You are tasked with creating a custom Python matrix type using C extensions. This matrix type should: 1. Store rows as Python lists containing float values. 2. Support basic matrix operations like addition and multiplication. 3. Ensure proper memory management and support for cyclic garbage collection. 4. Provide custom methods for setting and getting matrix elements. Requirements: 1. **Type Definition**: Define a `Matrix` type that includes: - An attribute to store the matrix (a list of lists containing float values). - Attributes `rows` and `columns` indicating the dimensions of the matrix. 2. **Methods**: - `add`: Adds another matrix of the same dimensions. - `multiply`: Multiplies the matrix by another matrix of compatible dimensions. - `set_element(row, col, value)`: Sets the element at position `(row, col)` to `value`. - `get_element(row, col)`: Returns the element at position `(row, col)`. 3. **Memory Management**: - Implement proper initialization and deallocation, including support for cyclic garbage collection. - Ensure that all elements and the matrix itself are properly managed to avoid memory leaks. 4. **Custom Element Access**: - Use custom getter and setter methods to ensure elements are always float values and positions are within bounds. Input and Output: - **Input**: Various operations and element accesses through the provided methods. - **Output**: Results of matrix operations or accessed elements. Constraints: - The matrix dimensions for addition must match. - For multiplication, the number of columns of the first matrix must match the number of rows of the second matrix. - Elements set must always be `float`. - Proper error handling must be in place for invalid dimensions or data types. Example: ```python import custom_matrix m1 = custom_matrix.Matrix([[1.0, 2.0], [3.0, 4.0]]) m2 = custom_matrix.Matrix([[5.0, 6.0], [7.0, 8.0]]) # Adding matrices result_add = m1.add(m2) print(result_add) # Should print: [[6.0, 8.0], [10.0, 12.0]] # Multiplying matrices result_multiply = m1.multiply(m2) print(result_multiply) # Should print: [[19.0, 22.0], [43.0, 50.0]] # Accessing elements element = m1.get_element(1, 1) print(element) # Should print: 4.0 # Setting elements m1.set_element(0, 0, 9.0) print(m1.get_element(0, 0)) # Should print: 9.0 ``` # Instructions: Implement the `Matrix` type based on the requirements provided. Ensure that your implementation adheres to the memory management best practices, including proper use of reference counting and garbage collection.","solution":"class Matrix: def __init__(self, matrix): self.rows = len(matrix) self.columns = len(matrix[0]) self.matrix = [[float(value) for value in row] for row in matrix] def add(self, other): if self.rows != other.rows or self.columns != other.columns: raise ValueError(\\"Matrices dimensions must match for addition\\") result = [[self.matrix[i][j] + other.matrix[i][j] for j in range(self.columns)] for i in range(self.rows)] return Matrix(result) def multiply(self, other): if self.columns != other.rows: raise ValueError(\\"Matrices dimensions are not compatible for multiplication\\") result = [[sum(self.matrix[i][k] * other.matrix[k][j] for k in range(self.columns)) for j in range(other.columns)] for i in range(self.rows)] return Matrix(result) def set_element(self, row, col, value): if not (0 <= row < self.rows) or not (0 <= col < self.columns): raise IndexError(\\"Matrix indices out of bounds\\") self.matrix[row][col] = float(value) def get_element(self, row, col): if not (0 <= row < self.rows) or not (0 <= col < self.columns): raise IndexError(\\"Matrix indices out of bounds\\") return self.matrix[row][col]"},{"question":"# PyTorch Numerical Accuracy and Precision Management **Problem Description:** You are tasked with implementing a function that performs matrix multiplication on a batch of matrices and examines the effect of different precision levels. Your function will receive two 3D tensors representing batches of matrices and perform matrix multiplication. The function should support four precision modes: `FP32`, `TF32`, `FP16`, and `BF16`. For each mode, the function should configure PyTorch\'s backend settings accordingly before performing the operation. Additionally, your function should compute the difference in results when using these precision modes by comparing each generated matrix with one produced using `FP64` (double precision) as a reference. The function should return the average absolute difference for each precision mode. **Input:** 1. `A`: a 3D tensor of shape `(batch_size, M, N)` representing the first batch of matrices. 2. `B`: a 3D tensor of shape `(batch_size, N, P)` representing the second batch of matrices. 3. `precision_modes`: a list of strings representing the precision modes to be tested, which may include `\\"FP32\\"`, `\\"TF32\\"`, `\\"FP16\\"`, and `\\"BF16\\"`. **Output:** A dictionary where the keys are the precision modes and the values are the average absolute differences between the batch results produced in the given precision mode compared to the `FP64` reference. **Constraints:** - PyTorch should be used for all tensor operations. - The function should handle any combination of the provided precision modes. - The input tensors must contain finite values (no `inf` or `NaN` values). - Assume `batch_size`, `M`, `N`, and `P` are positive integers. **Function Signature:** ```python def analyze_precision(A: torch.Tensor, B: torch.Tensor, precision_modes: List[str]) -> Dict[str, float]: pass ``` **Implementation Notes:** 1. Use `torch.matmul` or `torch.bmm` for batched matrix multiplication. 2. Use `torch.set_default_dtype(torch.float32)` and similar functions to control the precision. 3. For `TF32`, set `torch.backends.cuda.matmul.allow_tf32` to `True`. 4. For reduced precision (`FP16`, `BF16`), use `.to(torch.float16)` or `.to(torch.bfloat16)` on tensors. 5. Calculate the average absolute difference between results generated in each precision mode and the `FP64` reference. **Example Usage:** ```python A = torch.randn(10, 50, 50) B = torch.randn(10, 50, 50) precision_modes = [\\"FP32\\", \\"TF32\\", \\"FP16\\", \\"BF16\\"] diffs = analyze_precision(A, B, precision_modes) print(diffs) # Expected output: {\'FP32\': 0.0, \'TF32\': small_value, \'FP16\': larger_value, \'BF16\': another_value} ``` **Note:** - Ensure that the appropriate backend settings are reset after usage. - Consider numerical stability and potential overflows when working with different precision levels.","solution":"import torch from typing import List, Dict def analyze_precision(A: torch.Tensor, B: torch.Tensor, precision_modes: List[str]) -> Dict[str, float]: results = {} # First, compute the FP64 (double precision) reference result A_fp64 = A.to(torch.float64) B_fp64 = B.to(torch.float64) reference = torch.bmm(A_fp64, B_fp64) for mode in precision_modes: if mode == \'FP32\': A_mode = A.to(torch.float32) B_mode = B.to(torch.float32) elif mode == \'TF32\': torch.backends.cuda.matmul.allow_tf32 = True A_mode = A.to(torch.float32) B_mode = B.to(torch.float32) elif mode == \'FP16\': A_mode = A.to(torch.float16) B_mode = B.to(torch.float16) elif mode == \'BF16\': A_mode = A.to(torch.bfloat16) B_mode = B.to(torch.bfloat16) else: continue if mode == \'TF32\': result = torch.bmm(A_mode, B_mode) torch.backends.cuda.matmul.allow_tf32 = False else: result = torch.bmm(A_mode, B_mode) # Compute the average absolute difference diff = torch.abs(result.to(torch.float64) - reference).mean().item() results[mode] = diff return results"},{"question":"# Python Coding Assessment Question **Objective**: Your task is to implement a utility class for zlib compression and decompression to handle streams of data efficiently. This class should support both compression and decompression of data, handle errors gracefully, and maintain performance using appropriate buffer settings. **Instructions**: 1. Implement a class `ZlibStreamUtility` with the following methods: - `compress_data(self, data: bytes, level: int = -1) -> bytes`: Compress the given byte data with an optional compression level. - `decompress_data(self, data: bytes, buffer_size: int = 1024) -> bytes`: Decompress the given byte data using a buffer of specified size. 2. Implement another method, `compute_checksum(self, data: bytes, method: str = \'adler32\') -> int`, which computes and returns a checksum of the given byte data. The method can be either \'adler32\' or \'crc32\'. 3. The class should handle all exceptions raised during compression, decompression, and checksum computation. When an exception occurs, return `None`. **Constraints**: - `data` input for compression, decompression, and checksum should be a non-empty byte string. - Compression level should be between `-1` (default compression) and `9` (maximum compression). - Buffer size for decompression must be a positive integer. **Example Usage**: ```python utility = ZlibStreamUtility() # Compress data original_data = b\\"example data to compress\\" compressed_data = utility.compress_data(original_data, level=6) if compressed_data is not None: print(\\"Data compressed successfully.\\") # Decompress data decompressed_data = utility.decompress_data(compressed_data) if decompressed_data is not None and decompressed_data == original_data: print(\\"Data decompressed successfully.\\") # Compute checksums adler32_checksum = utility.compute_checksum(original_data, method=\'adler32\') crc32_checksum = utility.compute_checksum(original_data, method=\'crc32\') if adler32_checksum is not None and crc32_checksum is not None: print(f\\"Adler-32: {adler32_checksum}, CRC-32: {crc32_checksum}\\") ``` **Note**: - Ensure that the class methods are efficient and handle errors gracefully. - Pay attention to the performance implications of buffer sizes during decompression.","solution":"import zlib class ZlibStreamUtility: def compress_data(self, data: bytes, level: int = -1) -> bytes: try: if not data: raise ValueError(\\"Data must be non-empty.\\") if not (-1 <= level <= 9): raise ValueError(\\"Compression level must be between -1 and 9.\\") compressor = zlib.compressobj(level) compressed_data = compressor.compress(data) compressed_data += compressor.flush() return compressed_data except Exception: return None def decompress_data(self, data: bytes, buffer_size: int = 1024) -> bytes: try: if not data: raise ValueError(\\"Data must be non-empty.\\") if buffer_size <= 0: raise ValueError(\\"Buffer size must be a positive integer.\\") decompressor = zlib.decompressobj() decompressed_data = decompressor.decompress(data, buffer_size) while decompressor.unconsumed_tail: decompressed_data += decompressor.decompress(decompressor.unconsumed_tail, buffer_size) return decompressed_data except Exception: return None def compute_checksum(self, data: bytes, method: str = \'adler32\') -> int: try: if not data: raise ValueError(\\"Data must be non-empty.\\") if method == \'adler32\': return zlib.adler32(data) elif method == \'crc32\': return zlib.crc32(data) else: raise ValueError(\\"Method should be either \'adler32\' or \'crc32\'.\\") except Exception: return None"},{"question":"# Advanced Coding Assessment Objective Implement a custom protocol using Python\'s asyncio package to handle data reception on a specified transport and apply flow control. This task assesses your understanding of low-level asyncio event loop APIs, transport-protocol interaction, and asynchronous data handling. Question You are tasked with creating a custom TCP server that echoes back received messages after converting them to uppercase. Your implementation should manage data flow control effectively, ensuring the server does not get overwhelmed with data. Requirements 1. **Protocol Implementation**: Define a protocol class named `UpperCaseEchoProtocol` that: - Converts all received data to uppercase and sends it back to the client. - Implements back-pressure to manage data flow using `pause_writing()` and `resume_writing()` callbacks. - Logs connections, disconnections, received data, and sent data. 2. **Server Implementation**: Create a TCP server that: - Uses your `UpperCaseEchoProtocol` to handle connections. - Listens on address `127.0.0.1` and port `8888`. 3. **Performance Constraint**: Ensure the protocol can handle large messages by appropriately managing the write buffer using the `pause_writing()` and `resume_writing()` methods. Here is the expected input and output format: Input - Connection requests from multiple clients. - Messages of variable length sent by connected clients. Output - Uppercased echoes of received messages back to the respective clients. - Console logs: - New incoming connections. - Connections that are closed or lost. - Received data for each connection. - Sent data for each connection. Example For example, if a client sends \\"hello world\\" to the server, it will receive \\"HELLO WORLD\\" in response. Implementation Below is a scaffold for your solution. Complete the missing parts: ```python import asyncio class UpperCaseEchoProtocol(asyncio.Protocol): def connection_made(self, transport): peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') self.transport = transport def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') # Convert to uppercase and send it back response = message.upper().encode() print(f\'Send: {response}\') self.transport.write(response) def connection_lost(self, exc): print(\'The server closed the connection\') if exc: print(f\'Error: {exc}\') else: print(\'Connection closed cleanly\') def pause_writing(self): print(\'Pausing writing\') def resume_writing(self): print(\'Resuming writing\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: UpperCaseEchoProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main()) ``` Complete the `UpperCaseEchoProtocol` class to ensure it: 1. Handles the flow control correctly. 2. Logs the connection status and data exchange events appropriately. **Constraints:** - Use only asyncio-based constructs provided in the question. - Do not use any external libraries. **Performance:** - Your implementation should handle at least 100 simultaneous connections and continue to function robustly. Good luck!","solution":"import asyncio class UpperCaseEchoProtocol(asyncio.Protocol): def connection_made(self, transport): peername = transport.get_extra_info(\'peername\') print(f\'Connection from {peername}\') self.transport = transport self.buffer_size_threshold = 1024 self.buffer = b\\"\\" def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') # Convert to uppercase and add to buffer self.buffer += message.upper().encode() # Attempt to send the buffer if the transport is writable if not self.transport.is_closing(): self._send_buffer() def connection_lost(self, exc): print(\'The server closed the connection\') if exc: print(f\'Error: {exc}\') else: print(\'Connection closed cleanly\') def pause_writing(self): print(\'Pausing writing\') def resume_writing(self): print(\'Resuming writing\') self._send_buffer() def _send_buffer(self): while not self.transport.is_closing() and self.buffer: self.transport.write(self.buffer[:self.buffer_size_threshold]) self.buffer = self.buffer[self.buffer_size_threshold:] async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: UpperCaseEchoProtocol(), \'127.0.0.1\', 8888 ) async with server: await server.serve_forever() if __name__ == \'__main__\': asyncio.run(main())"},{"question":"You are tasked with analyzing business date offsets for a company that operates on a global scale. The company has different working calendars for different regions, which include varying business days and holidays. Your task is to implement a function that computes the next 10 business days considering region-specific calendars. # Function Signature ```python def next_ten_business_days(start_date: str, region: str) -> list: Given a start date and a region, return the next 10 business days for the given region. Parameters: start_date (str): The starting date in the format \'YYYY-MM-DD\'. region (str): The region for which to compute the business days. Returns: list: A list of the next 10 business days in the format \'YYYY-MM-DD\'. ``` # Requirements 1. **Input and Output Formats**: - The `start_date` parameter is a string in the format \'YYYY-MM-DD\'. - The `region` parameter is a string indicating the region\'s business calendar. - The function should return a list of strings, each representing a business day in \'YYYY-MM-DD\' format. 2. **Constraints**: - The `region` can be one of the following: \'US\', \'EU\', \'ASIA\'. - Each region has predefined business days and holidays that you need to consider. - Use the following predefined business day calendars: - \'US\': Business days are Monday to Friday, with holidays on 4th of July, Thanksgiving (Fourth Thursday of November), and Christmas (25th December). - \'EU\': Business days are Monday to Friday, with holidays on 1st of January, Easter Monday (2nd day of Easter), and Christmas (25th December). - \'ASIA\': Business days are Monday to Saturday, with holidays on Chinese New Year (variable), Golden Week (1st to 7th October), and Christmas (25th December). 3. **Performance Requirements**: - The solution should handle date computations efficiently for at least a span of one year. # Example Usage ```python # Example for region \'US\' start_date = \\"2023-11-22\\" # Starting just before Thanksgiving region = \\"US\\" print(next_ten_business_days(start_date, region)) # Output: [\'2023-11-22\', \'2023-11-24\', \'2023-11-27\', \'2023-11-28\', \'2023-11-29\', \'2023-11-30\', \'2023-12-01\', \'2023-12-04\', \'2023-12-05\', \'2023-12-06\'] # Example for region \'EU\' start_date = \\"2023-04-07\\" region = \\"EU\\" print(next_ten_business_days(start_date, region)) # Output: [\'2023-04-07\', \'2023-04-10\', \'2023-04-11\', \'2023-04-12\', \'2023-04-13\', \'2023-04-14\', \'2023-04-17\', \'2023-04-18\', \'2023-04-19\', \'2023-04-20\'] # Example for region \'ASIA\' start_date = \\"2023-01-27\\" # During Chinese New Year holidays region = \\"ASIA\\" print(next_ten_business_days(start_date, region)) # Output: [\'2023-01-27\', \'2023-01-28\', \'2023-02-01\', \'2023-02-02\', \'2023-02-03\', \'2023-02-04\', \'2023-02-06\', \'2023-02-07\', \'2023-02-08\', \'2023-02-09\'] ``` # Notes - Consider using `pandas.tseries.offsets.CustomBusinessDay` or other relevant classes from the `pandas.tseries.offsets` module. - Handle edge cases such as holidays overlapping with weekends appropriately. - You may assume the input `start_date` is always a valid date string. # Hints - Explore the use of `pandas.tseries.offsets` and `CustomBusinessDay`. - You might find the methods like `is_on_offset` and properties such as `weekmask`, `holidays` useful.","solution":"import pandas as pd from pandas.tseries.holiday import USFederalHolidayCalendar from pandas.tseries.offsets import CustomBusinessDay from datetime import datetime # Function to get specific holidays for regions def get_region_holidays(region): if region == \'US\': holidays = [ pd.Timestamp(\'2023-07-04\'), # Independence Day pd.Timestamp(\'2023-11-23\'), # Thanksgiving (Fourth Thursday of November) pd.Timestamp(\'2023-12-25\') # Christmas ] return holidays elif region == \'EU\': holidays = [ pd.Timestamp(\'2023-01-01\'), # New Year\'s Day pd.Timestamp(\'2023-04-10\'), # Easter Monday pd.Timestamp(\'2023-12-25\') # Christmas ] return holidays elif region == \'ASIA\': holidays = [ pd.Timestamp(\'2023-01-22\'), # Chinese New Year (variable, example for 2023) pd.Timestamp(\'2023-10-01\'), pd.Timestamp(\'2023-10-02\'), pd.Timestamp(\'2023-10-03\'), pd.Timestamp(\'2023-10-04\'), pd.Timestamp(\'2023-10-05\'), pd.Timestamp(\'2023-10-06\'), pd.Timestamp(\'2023-10-07\'), # Golden Week pd.Timestamp(\'2023-12-25\') # Christmas ] return holidays else: raise ValueError(\\"Region not supported\\") def next_ten_business_days(start_date: str, region: str) -> list: start_date = pd.Timestamp(start_date) if region == \'US\': weekmask = \'Mon Tue Wed Thu Fri\' holidays = get_region_holidays(region) elif region == \'EU\': weekmask = \'Mon Tue Wed Thu Fri\' holidays = get_region_holidays(region) elif region == \'ASIA\': weekmask = \'Mon Tue Wed Thu Fri Sat\' holidays = get_region_holidays(region) else: raise ValueError(\\"Region not supported\\") bday = CustomBusinessDay(holidays=holidays, weekmask=weekmask) next_business_days = pd.date_range(start_date, periods=10, freq=bday) return next_business_days.strftime(\'%Y-%m-%d\').to_list()"},{"question":"# Clustering Analysis with K-Means **Objective:** The objective of this task is to implement a function that will perform K-Means clustering on a given dataset and evaluate the clustering results using different metrics. **Task:** 1. Implement a function `perform_kmeans_clustering` that accepts the following parameters: - `data` (2D list or numpy array): The dataset to cluster. Shape should be (n_samples, n_features). - `n_clusters` (int): The number of clusters to form as well as the number of centroids to generate. - `init` (str): Method for initialization, defaults to `\'k-means++\'`. Should be one of `{\'k-means++\', \'random\'}`. - `n_init` (int): Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output in terms of inertia. - `max_iter` (int): Maximum number of iterations of the k-means algorithm for a single run. 2. The function should return the following: - `labels`: Labels of each point as determined by the clustering. - `centroids`: The coordinates of the cluster centers. - `inertia`: Sum of squared distances of samples to their closest cluster center. - `n_iter`: Number of iterations run. 3. Evaluate the clustering results using the following metrics: - **Inertia:** Within-cluster sum-of-squares. - **Silhouette Coefficient:** This score ranges from -1 for incorrect clustering to +1 for highly dense clustering. Scores around zero indicate overlapping clusters. - **Calinski-Harabasz Index:** Higher score indicates better-defined clusters. - **Davies-Bouldin Index:** Lower score indicates better separation between clusters. **Input Constraints:** - `data` will have at least 50 samples and up to 2,000 samples. - Each sample in `data` will have between 2 and 50 features. - `n_clusters` will be between 2 and 20. - `init` will always be `\'k-means++\'` or `\'random\'`. - `n_init` will be between 1 and 20. - `max_iter` will be at least 10 and up to 500. **Function Signature:** ```python def perform_kmeans_clustering(data: Union[List[List[float]], np.ndarray], n_clusters: int, init: str = \'k-means++\', n_init: int = 10, max_iter: int = 300) -> Tuple[np.ndarray, np.ndarray, float, int]: pass ``` **Example:** ```python from sklearn.datasets import make_blobs import numpy as np # Generate synthetic data data, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0) # Perform K-means clustering labels, centroids, inertia, n_iter = perform_kmeans_clustering(data, n_clusters=4) # Output: # Labels: [Array with the cluster labels for each sample] # Centroids: [Array with the coordinates of the cluster centers] # Inertia: Value of inertia # n_iter: Number of iterations run # Evaluation Metrics from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score sil_score = silhouette_score(data, labels) ch_index = calinski_harabasz_score(data, labels) db_index = davies_bouldin_score(data, labels) ``` # Submission Guideline Submit the complete implementation of the function `perform_kmeans_clustering` along with additional evaluation metrics (silhouette coefficient, Calinski-Harabasz index, and Davies-Bouldin index) calculated for a given dataset.","solution":"from typing import Union, List, Tuple import numpy as np from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score def perform_kmeans_clustering(data: Union[List[List[float]], np.ndarray], n_clusters: int, init: str = \'k-means++\', n_init: int = 10, max_iter: int = 300) -> Tuple[np.ndarray, np.ndarray, float, int, float, float, float]: Perform K-Means clustering on the given dataset. Parameters: - data (2D list or numpy array): The dataset to cluster. Shape should be (n_samples, n_features). - n_clusters (int): The number of clusters to form as well as the number of centroids to generate. - init (str): Method for initialization, defaults to \'k-means++\'. Should be one of {\'k-means++\', \'random\'}. - n_init (int): Number of time the k-means algorithm will be run with different centroid seeds. - max_iter (int): Maximum number of iterations of the k-means algorithm for a single run. Returns: - labels (numpy.ndarray): Labels of each point as determined by the clustering. - centroids (numpy.ndarray): The coordinates of the cluster centers. - inertia (float): Sum of squared distances of samples to their closest cluster center. - n_iter (int): Number of iterations run. - silhouette (float): Silhouette Coefficient score. - calinski_harabasz (float): Calinski-Harabasz Index score. - davies_bouldin (float): Davies-Bouldin Index score. # If data is not a numpy array, convert it if not isinstance(data, np.ndarray): data = np.array(data) # Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, init=init, n_init=n_init, max_iter=max_iter) kmeans.fit(data) labels = kmeans.labels_ centroids = kmeans.cluster_centers_ inertia = kmeans.inertia_ n_iter = kmeans.n_iter_ # Evaluate clustering results silhouette = silhouette_score(data, labels) calinski_harabasz = calinski_harabasz_score(data, labels) davies_bouldin = davies_bouldin_score(data, labels) return labels, centroids, inertia, n_iter, silhouette, calinski_harabasz, davies_bouldin"},{"question":"# Calendar Utility Functions Implementation You are tasked with implementing several utility functions that together provide a comprehensive analysis of a given year using the \\"calendar\\" module. These utilities will provide calendar information in both text and HTML formats and will help perform specific date-related calculations. Your task is to implement three functions as described below: 1. **generate_year_calendar(year: int, html: bool = False) -> str** - **Parameters:** - `year` (int): The year for which the calendar needs to be generated. - `html` (bool): If `True`, return the calendar in HTML format; otherwise, return it as plain text. - **Returns:** - A string representing the complete calendar for the given year in the specified format (text or HTML). - **Constraints:** - Use `TextCalendar` for text format and `HTMLCalendar` for HTML format. 2. **count_leap_years(start_year: int, end_year: int) -> int** - **Parameters:** - `start_year` (int): The starting year (inclusive). - `end_year` (int): The ending year (exclusive). - **Returns:** - An integer representing the number of leap years between `start_year` and `end_year`. 3. **weekday_for_date(year: int, month: int, day: int) -> str** - **Parameters:** - `year` (int): The year of the date. - `month` (int): The month of the date. - `day` (int): The day of the date. - **Returns:** - A string representing the name of the weekday for the given date. Ensure that your implementation makes use of the methods and attributes in the \\"calendar\\" module as described in the documentation. # Example Usage ```python # Example for generate_year_calendar print(generate_year_calendar(2023, html=False)) # Example for count_leap_years print(count_leap_years(2000, 2021)) # Output should be 6 (2000, 2004, 2008, 2012, 2016, 2020) # Example for weekday_for_date print(weekday_for_date(2023, 10, 31)) # Output should be \'Tuesday\' # Example for generate_year_calendar (HTML) print(generate_year_calendar(2023, html=True)) # This should return an HTML representation of the 2023 calendar ``` # Performance Requirements - Each function should run efficiently for typical input sizes. - Consider optimizations where necessary to handle large ranges of years and dates effectively.","solution":"import calendar def generate_year_calendar(year: int, html: bool = False) -> str: Generates the calendar for the specified year in text or HTML format. Parameters: - year (int): The year for which the calendar needs to be generated. - html (bool): If True, return the calendar in HTML format; otherwise, return it as plain text. Returns: - A string representing the complete calendar for the given year in the specified format (text or HTML). if html: cal = calendar.HTMLCalendar() else: cal = calendar.TextCalendar() return cal.formatyear(year) def count_leap_years(start_year: int, end_year: int) -> int: Counts the number of leap years between start_year (inclusive) and end_year (exclusive). Parameters: - start_year (int): The starting year (inclusive). - end_year (int): The ending year (exclusive). Returns: - An integer representing the number of leap years between the given years. leap_years = 0 for year in range(start_year, end_year): if calendar.isleap(year): leap_years += 1 return leap_years def weekday_for_date(year: int, month: int, day: int) -> str: Returns the name of the weekday for the given date. Parameters: - year (int): The year of the date. - month (int): The month of the date. - day (int): The day of the date. Returns: - A string representing the name of the weekday for the given date. day_index = calendar.weekday(year, month, day) return calendar.day_name[day_index]"},{"question":"# Advanced Coding Assessment Question on `quopri` Module **Objective:** Write a Python program to encode and decode text data to and from quoted-printable format, using the `quopri` module. Your solution should demonstrate the ability to read and write binary files, handle optional encoding flags, and correctly use the `quopri` functions in combination. **Problem Statement:** You are tasked with implementing two functions: 1. `encode_to_quoted_printable(input_filepath: str, output_filepath: str, quotetabs: bool, header: bool) -> None` 2. `decode_from_quoted_printable(input_filepath: str, output_filepath: str, header: bool) -> None` **Function Details:** 1. **`encode_to_quoted_printable` Function:** - **Input:** - `input_filepath`: A string representing the path to the input text file to be encoded. - `output_filepath`: A string representing the path to the output file where the encoded data will be written. - `quotetabs`: A boolean flag indicating whether spaces and tabs should be encoded. - `header`: A boolean flag indicating whether spaces should be encoded as underscores. - **Output:** None. The function writes the encoded data to the specified output file. 2. **`decode_from_quoted_printable` Function:** - **Input:** - `input_filepath`: A string representing the path to the input file containing quoted-printable encoded data. - `output_filepath`: A string representing the path to the output file where the decoded data will be written. - `header`: A boolean flag indicating whether underscores should be decoded as spaces. - **Output:** None. The function writes the decoded data to the specified output file. **Constraints:** - Both functions must read from and write to files using binary mode. - Assume the input files contain ASCII characters only. **Example Workflow:** 1. Create a text file `example.txt` with the content: ``` Hello World! This is a test file with some text. ``` 2. Encode this file to quoted-printable format: ```python encode_to_quoted_printable(\'example.txt\', \'encoded_example.txt\', quotetabs=True, header=False) ``` 3. Decode the file back to its original format: ```python decode_from_quoted_printable(\'encoded_example.txt\', \'decoded_example.txt\', header=False) ``` **Submission Requirements:** - Comment your code adequately to explain the functionality. - Handle possible errors (e.g., file not found, incorrect file format) gracefully. **Evaluation Criteria:** - Correctness: The functions should accurately encode and decode the data as specified. - Code Quality: The code should be well-organized, readable, and maintainable. - Error Handling: The code should handle exceptions and edge cases appropriately. Implement the functions below. ```python def encode_to_quoted_printable(input_filepath: str, output_filepath: str, quotetabs: bool, header: bool) -> None: # Your implementation here def decode_from_quoted_printable(input_filepath: str, output_filepath: str, header: bool) -> None: # Your implementation here ```","solution":"import quopri def encode_to_quoted_printable(input_filepath: str, output_filepath: str, quotetabs: bool, header: bool) -> None: Encode the content of the input file to quoted-printable format and write it to the output file. :param input_filepath: Path to the input text file. :param output_filepath: Path to the output file for the encoded data. :param quotetabs: If True, encode spaces and tabs. :param header: If True, encode spaces as underscores. try: with open(input_filepath, \'rb\') as infile, open(output_filepath, \'wb\') as outfile: quopri.encode(infile, outfile, quotetabs) except FileNotFoundError: print(f\\"File not found: {input_filepath}\\") except Exception as e: print(f\\"An error occurred: {e}\\") def decode_from_quoted_printable(input_filepath: str, output_filepath: str, header: bool) -> None: Decode the content of the input file from quoted-printable format and write it to the output file. :param input_filepath: Path to the input file with quoted-printable encoded data. :param output_filepath: Path to the output file for the decoded data. :param header: If True, decode underscores as spaces (used for email headers). try: with open(input_filepath, \'rb\') as infile, open(output_filepath, \'wb\') as outfile: quopri.decode(infile, outfile) except FileNotFoundError: print(f\\"File not found: {input_filepath}\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Coding Assessment Question # Background: The `asyncio` library in Python allows for writing concurrent programs using the `async/await` syntax. This capability is particularly useful for IO-bound operations where you can avoid blocking the main thread while awaiting results (such as reading from a file, network request, etc.). # Problem Statement: You are tasked with creating an asynchronous program to fetch and process multiple URLs concurrently. Your task is to fetch the contents of a list of web pages, parse the HTML to extract all hyperlinks, and then store these results. # Requirements: - Write a function `fetch_urls(urls: List[str]) -> Dict[str, List[str]]` using `asyncio` that accepts a list of URLs and returns a dictionary. The keys of the dictionary should be the URLs and the values should be lists of hyperlinks found on each corresponding URL. - Use the `aiohttp` library to perform asynchronous HTTP requests. - Parse the content of each page to extract hyperlinks. You can use the `BeautifulSoup` library from `bs4` for HTML parsing. - Ensure your solution handles errors gracefully (e.g., a URL that cannot be fetched should not disrupt the entire process). # Input: - `urls`: A list of strings, where each string is a valid URL. # Output: - A dictionary where each key is a URL from the input list and each value is a list of hyperlinks found on that URL. # Example: ```python urls = [ \\"https://example.com\\", \\"https://anotherexample.com\\" ] ``` The function might return something like: ```python { \\"https://example.com\\": [\\"https://example.com/link1\\", \\"https://example.com/link2\\"], \\"https://anotherexample.com\\": [\\"https://anotherexample.com/link1\\"] } ``` # Constraints: - You must use `asyncio` and `aiohttp` for making asynchronous requests. - Your function should manage the concurrency and parallelism efficiently. - The HTML parsing for extracting hyperlinks must be non-blocking. # Performance: Your solution should be able to handle a list of at least 100 URLs efficiently. # Hints: 1. Use `asyncio.gather` to run multiple tasks concurrently. 2. Handle connection timeouts and other potential exceptions when fetching URLs. 3. For parsing, look for `<a>` tags and extract the `href` attribute to find hyperlinks. # References: - [aiohttp documentation](https://docs.aiohttp.org) - [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)","solution":"import asyncio import aiohttp from bs4 import BeautifulSoup from typing import List, Dict async def fetch_content(session: aiohttp.ClientSession, url: str) -> str: Fetches the content of a URL. try: async with session.get(url) as response: return await response.text() except Exception as e: print(f\\"Error fetching {url}: {e}\\") return \\"\\" async def parse_links(html: str) -> List[str]: Parses HTML content to extract all hyperlinks. soup = BeautifulSoup(html, \'html.parser\') return [a[\'href\'] for a in soup.find_all(\'a\', href=True)] async def fetch_and_parse(session: aiohttp.ClientSession, url: str) -> Dict[str, List[str]]: Fetches content from a URL and parses it to extract hyperlinks. content = await fetch_content(session, url) if content: links = await parse_links(content) return {url: links} return {url: []} async def fetch_urls(urls: List[str]) -> Dict[str, List[str]]: Fetch and process multiple URLs concurrently to get hyperlinks from each. async with aiohttp.ClientSession() as session: tasks = [fetch_and_parse(session, url) for url in urls] results = await asyncio.gather(*tasks) final_result = {} for result in results: final_result.update(result) return final_result"},{"question":"Problem Statement You are given an FP32 PyTorch neural network model that you need to quantize using static quantization. Your task is to write a function that performs the following steps: 1. Prepare the model for static quantization. 2. Calibrate the model with some sample input data. 3. Convert the calibrated model to a quantized model. # Function Signature ```python def quantize_static_model(model: torch.nn.Module, input_data: torch.Tensor, backend: str = \'qnnpack\') -> torch.nn.Module: pass ``` # Input - `model: torch.nn.Module`: A PyTorch model in floating point format. - `input_data: torch.Tensor`: Sample input data to calibrate the model (representative dataset). - `backend: str`: The backend to use for quantization (\'qnnpack\' or \'x86\'). Default is \'qnnpack\'. # Output - Returns a quantized model (`torch.nn.Module`). # Constraints - The `input_data` tensor shape should match the expected input shape of the model. - Assume the model is a CNN which supports static quantization. - Ensure the model is in evaluation mode before starting the quantization process. # Example ```python import torch import torch.nn as nn import torch.ao.quantization class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv = nn.Conv2d(1, 1, 3) self.relu = nn.ReLU() self.fc = nn.Linear(784, 10) def forward(self, x): x = self.conv(x) x = self.relu(x) x = torch.flatten(x, 1) x = self.fc(x) return x model_fp32 = SimpleCNN().eval() input_data = torch.randn(100, 1, 28, 28) quantized_model = quantize_static_model(model_fp32, input_data) ``` # Notes - Use the `torch.ao.quantization` package to perform the quantization steps. - Utilize the `prepare`, `calibrate`, and `convert` functions from the package. - Ensure all necessary stubs and configurations are appropriately set in the model.","solution":"import torch import torch.nn as nn import torch.ao.quantization as quantization def quantize_static_model(model: nn.Module, input_data: torch.Tensor, backend: str = \'qnnpack\') -> nn.Module: Quantize a PyTorch neural network model using static quantization. Args: model (nn.Module): A PyTorch model in floating point format. input_data (torch.Tensor): Sample input data to calibrate the model (representative dataset). backend (str): The backend to use for quantization (\'qnnpack\' or \'x86\'). Default is \'qnnpack\'. Returns: nn.Module: Quantized model. model.eval() # Set the backend for quantization torch.backends.quantized.engine = backend # Fuse conv + relu in the model, this step depends on model architecture model_fused = quantization.fuse_modules(model, [[\'conv\', \'relu\']]) # Prepare the model for static quantization model_prepared = quantization.prepare(model_fused) # Calibrate the model with the provided data with torch.no_grad(): model_prepared(input_data) # Convert the calibrated model to a quantized model model_quantized = quantization.convert(model_prepared) return model_quantized # Example usage: class SimpleCNN(nn.Module): def __init__(self): super(SimpleCNN, self).__init__() self.conv = nn.Conv2d(1, 1, 3) self.relu = nn.ReLU() self.fc = nn.Linear(26*26, 10) def forward(self, x): x = self.conv(x) x = self.relu(x) x = torch.flatten(x, 1) x = self.fc(x) return x model_fp32 = SimpleCNN().eval() input_data = torch.randn(100, 1, 28, 28) quantized_model = quantize_static_model(model_fp32, input_data)"},{"question":"You are required to implement a custom scikit-learn compatible classifier. This classifier will be a simple k-Nearest Neighbors (k-NN) classifier. The purpose of this exercise is to assess your understanding of the scikit-learn Estimator API, covering initialization, fitting, predicting, and input validation. **Requirements:** 1. Inherit from `sklearn.base.BaseEstimator` and `sklearn.base.ClassifierMixin`. 2. Implement the following methods: - `__init__(self, n_neighbors=5)`: Initialize the classifier with a specified number of neighbors. - `fit(self, X, y)`: Fit the classifier by storing the training data. Ensure `X` and `y` are valid arrays. - `predict(self, X)`: Predict the class labels for the input samples in X. 3. Ensure proper validation of input data (`X` and `y`). 4. Utilize appropriate scikit-learn utilities for input validation and checking if the classifier has been fitted. 5. Ensure that the class labels are stored in a `classes_` attribute. 6. Include a method for parameter settings (`get_params` and `set_params`). **Input/Output Formats:** - The `__init__` method should accept a single parameter `n_neighbors` with a default value of 5. - The `fit` method should take `X` (array-like of shape (n_samples, n_features)) and `y` (array-like of shape (n_samples,)). - The `predict` method should take `X` (array-like of shape (n_samples, n_features)) and return an array of shape (n_samples,) containing the predicted class labels. **Performance Requirements:** - The solution should be efficient in terms of both time and memory. - Implementations should follow the scikit-learn conventions for method signatures and attribute names. **Example Usage:** ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load data iris = load_iris() X, y = iris.data, iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize your classifier knn = CustomKNNClassifier(n_neighbors=3) # Fit the model knn.fit(X_train, y_train) # Predict predictions = knn.predict(X_test) # Assess model performance print(f\\"Predictions: {predictions}\\") ``` Implement the `CustomKNNClassifier` below: ```python import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.utils.multiclass import unique_labels from sklearn.metrics.pairwise import euclidean_distances class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5): self.n_neighbors = n_neighbors def fit(self, X, y): # Validate input data X, y = check_X_y(X, y) self.classes_ = unique_labels(y) # Store the training data self.X_ = X self.y_ = y return self def predict(self, X): # Check if fit has been called check_is_fitted(self, [\'X_\', \'y_\']) # Validate input data X = check_array(X) # Compute distances and choose the most frequent class label among the k closest neighbors distances = euclidean_distances(X, self.X_) neighbors_idx = np.argsort(distances, axis=1)[:, :self.n_neighbors] neighbors_labels = self.y_[neighbors_idx] # Predict the class predicted_labels = np.array([np.bincount(neigh).argmax() for neigh in neighbors_labels]) return predicted_labels ```","solution":"import numpy as np from sklearn.base import BaseEstimator, ClassifierMixin from sklearn.utils.validation import check_X_y, check_array, check_is_fitted from sklearn.utils.multiclass import unique_labels from sklearn.metrics.pairwise import euclidean_distances class CustomKNNClassifier(BaseEstimator, ClassifierMixin): def __init__(self, n_neighbors=5): self.n_neighbors = n_neighbors def fit(self, X, y): # Validate input data X, y = check_X_y(X, y) self.classes_ = unique_labels(y) # Store the training data self.X_ = X self.y_ = y return self def predict(self, X): # Check if fit has been called check_is_fitted(self, [\'X_\', \'y_\']) # Validate input data X = check_array(X) # Compute distances and choose the most frequent class label among the k closest neighbors distances = euclidean_distances(X, self.X_) neighbors_idx = np.argsort(distances, axis=1)[:, :self.n_neighbors] neighbors_labels = self.y_[neighbors_idx] # Predict the class predicted_labels = np.array([np.bincount(neigh).argmax() for neigh in neighbors_labels]) return predicted_labels def get_params(self, deep=True): return {\\"n_neighbors\\": self.n_neighbors} def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) return self"},{"question":"Objective Implement a function that connects to a specified Telnet server, sends login credentials, executes a series of commands, and retrieves the output. Problem Statement Write a function `telnet_command_execution(host: str, user: str, password: str, commands: List[str], timeout: int = 10) -> str` that: 1. Connects to a Telnet server specified by `host`. 2. Sends the `user` and `password` credentials to log in to the server. 3. Executes a list of `commands` sequentially. 4. Retrieves and returns all output data as a single string. 5. Handles any connection issues or timeouts gracefully. 6. Uses a specified `timeout` for connection and read operations (default is 10 seconds). 7. Sets the debug level to 1 for detailed interaction logs. Input - `host` (str): The hostname of the Telnet server. - `user` (str): Username for login. - `password` (str): Password for login. - `commands` (List[str]): A list of commands to be executed on the server. - `timeout` (int): The timeout duration for the connection and read operations (optional, default is 10 seconds). Output - Returns a single string containing the output received from the server after executing all the commands. Constraints - Assume valid inputs for hostname, username, password, and commands. - Handle potential issues such as incorrect login credentials or command execution errors. - Ensure the function does not attempt to reopen an already connected instance. Example ```python from typing import List def telnet_command_execution(host: str, user: str, password: str, commands: List[str], timeout: int = 10) -> str: # Your code here # Example usage: host = \\"localhost\\" user = \\"testuser\\" password = \\"testpass\\" commands = [\\"ls\\", \\"pwd\\", \\"whoami\\"] output = telnet_command_execution(host, user, password, commands) print(output) ``` Notes - Use the `telnetlib` module for Telnet operations. - Utilize relevant `Telnet` class methods for connection handling, reading, writing, and interaction. - Implement appropriate error handling to manage exceptions and connection issues.","solution":"import telnetlib from typing import List def telnet_command_execution(host: str, user: str, password: str, commands: List[str], timeout: int = 10) -> str: try: # Create a Telnet object and set the debug level for detailed interaction logs tn = telnetlib.Telnet(host, timeout=timeout) tn.set_debuglevel(1) # Read until login prompt and send username tn.read_until(b\\"login: \\") tn.write(user.encode(\'ascii\') + b\\"n\\") # Read until password prompt and send password tn.read_until(b\\"Password: \\") tn.write(password.encode(\'ascii\') + b\\"n\\") output_data = \\"\\" # Execute each command and gather output for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") # Read until command prompt to ensure the command has completed execution output = tn.read_until(b\\" \\", timeout=timeout).decode(\'ascii\') output_data += output tn.write(b\\"exitn\\") # Gracefully close the Telnet session tn.close() return output_data.strip() except Exception as e: return f\\"An error occurred: {e}\\""},{"question":"Objective: Design a Python function to interact with Unix password and group databases. This will test your understanding of the `pwd` and `grp` modules. Problem Statement: You are required to implement a function `fetch_user_info(username: str) -> dict` which accepts a Unix username as input and returns a dictionary containing detailed information about the user and their primary group. Function Specification: ```python def fetch_user_info(username: str) -> dict: pass ``` Input: - `username` (str): A valid Unix username. Output: - A dictionary with the following structure: ```python { \\"username\\": str, \\"uid\\": int, \\"gid\\": int, \\"home_directory\\": str, \\"shell\\": str, \\"group_name\\": str, \\"group_members\\": List[str] } ``` Guidelines and Constraints: 1. Use the `pwd` module to fetch user information. 2. Use the `grp` module to fetch group information. 3. If the `username` is not found, raise a `ValueError` with a message `\\"User not found\\"`. 4. If the user\'s primary group is not found, raise a `ValueError` with a message `\\"Group not found\\"`. 5. The function should handle any exceptions that might occur while accessing the `pwd` and `grp` modules. Example: ```python # Suppose there\'s a user on the system with the username \'john\' result = fetch_user_info(\'john\') # Sample Output { \\"username\\": \\"john\\", \\"uid\\": 1001, \\"gid\\": 1001, \\"home_directory\\": \\"/home/john\\", \\"shell\\": \\"/bin/bash\\", \\"group_name\\": \\"johns\\", \\"group_members\\": [\\"john\\", \\"jane\\"] } ``` Performance Requirements: - The function should retrieve and return results promptly by accessing the Unix database. Use the Unix-specific features in Python to ensure accurate and secure handling of user and group information. Useful References: - The `pwd` module: https://docs.python.org/3/library/pwd.html - The `grp` module: https://docs.python.org/3/library/grp.html","solution":"import pwd import grp def fetch_user_info(username: str) -> dict: Fetches detailed user and primary group information for a given Unix username. Args: username (str): A valid Unix username. Returns: dict: A dictionary containing detailed information about the user and their primary group. Raises: ValueError: If the user or group is not found. try: # Fetch the user information user_info = pwd.getpwnam(username) except KeyError: raise ValueError(\\"User not found\\") try: # Fetch the primary group information group_info = grp.getgrgid(user_info.pw_gid) except KeyError: raise ValueError(\\"Group not found\\") return { \\"username\\": user_info.pw_name, \\"uid\\": user_info.pw_uid, \\"gid\\": user_info.pw_gid, \\"home_directory\\": user_info.pw_dir, \\"shell\\": user_info.pw_shell, \\"group_name\\": group_info.gr_name, \\"group_members\\": group_info.gr_mem }"},{"question":"You are tasked with analyzing a dataset using seaborn and creating informative visualizations that highlight specific trends and patterns. # Problem Statement 1. Load the `fmri` dataset preloaded in seaborn. 2. Generate a line plot to visualize the signal changes over time, grouped by both region and event. 3. Customize the plot such that: - Each region is represented by a separate subplot (facet) using `FacetGrid`. - Events are distinguished by both color (`hue`) and marker style (`style`). - Error bars are displayed, representing the standard error of the mean. - A custom color palette is applied for better distinction of events. # Input and Output Formats **Input:** No direct input is required since you\'ll be working with the preloaded `fmri` dataset from seaborn. **Output:** A seaborn line plot with the aforementioned customizations. # Constraints Ensure that your code: - Generates the correct plot as specified. - Handles any preprocessing necessary for the plotting (e.g., handling data in long-format if needed). # Performance Requirements - The plot should be efficient in terms of rendering speed and clarity. - It should be easily interpretable, using appropriate labels and legends. # Example: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Create the plot sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", col=\\"region\\", hue=\\"event\\", style=\\"event\\", kind=\\"line\\", err_style=\\"bars\\", errorbar=(\\"se\\", 2), palette=\\"muted\\" ) # Customize the aesthetics of the plot plt.show() ``` This example generates a multi-facet line plot with customized aesthetics. Modify and complete the code according to the problem statement.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_fmri_plot(): Create and display a seaborn line plot for the fmri dataset. The plot shows signal changes over time, grouped by region and event, with error bars representing the standard error of the mean and custom color palette for better distinction of events. # Load the fmri dataset fmri = sns.load_dataset(\\"fmri\\") # Create the line plot with specified customizations g = sns.relplot( data=fmri, x=\\"timepoint\\", y=\\"signal\\", col=\\"region\\", hue=\\"event\\", style=\\"event\\", kind=\\"line\\", err_style=\\"bars\\", errorbar=\\"se\\", palette=\\"muted\\" ) # Customize the aesthetics of the plot g.fig.subplots_adjust(top=0.9) g.fig.suptitle(\'FMRI Signal Changes Over Time by Region and Event\') plt.show()"},{"question":"**Objective:** Assess students\' ability to write Python code that demonstrates proper resource management and makes use of Python Development Mode to debug issues effectively. **Problem Statement:** You are given a task to process multiple text files where each file contains a list of integers, one per line. You must write a Python script that reads each file, computes the sum of the integers, and prints the result. Your implementation should also include proper resource management to ensure all files are closed correctly, even when exceptions occur. Additionally, you should run the script with Python Development Mode enabled and ensure no ResourceWarnings or other warnings are emitted. **Function Signature:** ```python def process_files(file_paths: list[str]) -> None: pass ``` # Input - `file_paths`: A list of strings, where each string is a path to a text file containing integers. # Expected Output - The function does not return anything. Instead, it prints the sum of integers from each file, one sum per line. # Example Suppose you have two text files: - `file1.txt` containing: ``` 1 2 3 ``` - `file2.txt` containing: ``` 4 5 6 ``` The function call: ```python process_files([\'file1.txt\', \'file2.txt\']) ``` should output: ``` 6 15 ``` # Constraints 1. Each file will contain at least one integer. 2. The integers in the files are guaranteed to be valid. # Performance Requirements - The function should be able to handle a large number of files gracefully. - Efficient reading and processing of files are required. # Constraints - Any file-related errors should be handled gracefully, with appropriate error messages printed. # Additional Instructions 1. Use context managers (`with` statement) for file operations to ensure proper resource management. 2. Run your script under Python Development Mode (`python3 -X dev your_script.py`) and ensure no warnings are emitted, especially `ResourceWarning`. **Notes:** - You should provide a standalone script that includes the implementation of the `process_files` function. - Your solution will be evaluated based on correctness, proper resource management, and efficiency.","solution":"def process_files(file_paths: list[str]) -> None: for file_path in file_paths: try: with open(file_path, \'r\') as file: total = sum(int(line.strip()) for line in file) print(total) except Exception as e: print(f\\"Error processing file {file_path}: {e}\\")"},{"question":"**Problem Statement:** You are working with a directory that contains various types of files and subdirectories. Your task is to write a Python function that scans a given directory and classifies its contents into different types. The function should generate a summary report of the file types along with human-readable file permissions. **Function Signature:** ```python def summarize_directory(directory_path: str) -> dict: Summarizes the contents of the given directory by classifying the files into different types and providing their human-readable permissions. Args: - directory_path (str): the path to the directory to be summarized. Returns: - dict: A dictionary with keys being the file types (e.g., \'directories\', \'regular_files\', \'symlinks\', etc.) and values being lists of tuples. Each tuple should contain the file path and its human-readable permissions. ``` **Expected Input and Output:** - **Input:** - `directory_path`: A string representing the path to the directory to be summarized. - **Output:** - A dictionary where: - Keys are file type categories (e.g., \'directories\', \'regular_files\', \'symlinks\', \'character_devices\', etc.). - Values are lists of tuples, where each tuple contains: - The file path (str). - The file\'s permissions in human-readable format (str). **Example:** ```python output = summarize_directory(\\"/path/to/directory\\") # Example of a possible output: { \\"directories\\": [ (\\"/path/to/directory/subdir1\\", \\"drwxr-xr-x\\"), (\\"/path/to/directory/subdir2\\", \\"drwxrwxrwx\\") ], \\"regular_files\\": [ (\\"/path/to/directory/file1.txt\\", \\"-rw-r--r--\\"), (\\"/path/to/directory/file2.log\\", \\"-rwxr-xr-x\\") ], \\"symlinks\\": [ (\\"/path/to/directory/link1\\", \\"lrwxrwxrwx\\") ] } ``` **Constraints:** 1. You should handle various file types as provided by the `stat` module, including regular files, directories, symbolic links, character special devices, block special devices, FIFOs, and sockets. 2. If a file type is not encountered in the directory, it should not be a key in the output dictionary. 3. You can assume that the directory_path is always a valid directory path. **Notes:** 1. You may use the `os` and `stat` modules to implement this function. 2. The permissions in human-readable format should be generated using the `stat.filemode(mode)` function. 3. Handle any necessary exceptions gracefully, such as permission errors when accessing files. **Performance Requirements:** 1. Your code should efficiently traverse the directory using appropriate system calls to minimize overhead.","solution":"import os import stat def summarize_directory(directory_path: str) -> dict: Summarizes the contents of the given directory by classifying the files into different types and providing their human-readable permissions. Args: - directory_path (str): the path to the directory to be summarized. Returns: - dict: A dictionary with keys being the file types (e.g., \'directories\', \'regular_files\', \'symlinks\', etc.) and values being lists of tuples. Each tuple should contain the file path and its human-readable permissions. summary = {} for dirpath, dirnames, filenames in os.walk(directory_path): for name in dirnames + filenames: full_path = os.path.join(dirpath, name) try: mode = os.lstat(full_path).st_mode permissions = stat.filemode(mode) if stat.S_ISDIR(mode): summary.setdefault(\'directories\', []).append((full_path, permissions)) elif stat.S_ISREG(mode): summary.setdefault(\'regular_files\', []).append((full_path, permissions)) elif stat.S_ISLNK(mode): summary.setdefault(\'symlinks\', []).append((full_path, permissions)) elif stat.S_ISCHR(mode): summary.setdefault(\'character_devices\', []).append((full_path, permissions)) elif stat.S_ISBLK(mode): summary.setdefault(\'block_devices\', []).append((full_path, permissions)) elif stat.S_ISFIFO(mode): summary.setdefault(\'fifos\', []).append((full_path, permissions)) elif stat.S_ISSOCK(mode): summary.setdefault(\'sockets\', []).append((full_path, permissions)) except (PermissionError, OSError) as e: # Handle any errors while accessing file or directory properties print(f\\"Error accessing {full_path}: {e}\\") return summary"},{"question":"You are provided with a dataset consisting of multiple features. Your task is to implement a dimensionality reduction process using PCA (Principal Component Analysis) and then train a supervised learning model to make predictions on the reduced dataset. You must also evaluate the performance of the model. # Requirements 1. **Data**: You will load the Iris dataset from `sklearn.datasets`. 2. **Preprocessing**: Scale the features using `StandardScaler`. 3. **Dimensionality Reduction**: Apply PCA and reduce the dataset to 2 principal components. 4. **Model Training**: Use a `LogisticRegression` classifier to train the model on the reduced dataset. 5. **Evaluation**: Evaluate the model using accuracy score. # Constraints - You must use pipelining to streamline the preprocessing, dimensionality reduction, and model training steps. - The PCA should reduce the data to exactly 2 components. - Use an 80-20 train-test split for evaluating the model. # Input and Output Formats - **Input**: No direct input is required; the code should load the dataset internally. - **Output**: Print the accuracy score of the model on the test dataset. # Instructions 1. **Loading Data**: - Import the Iris dataset using `sklearn.datasets.load_iris`. 2. **Implementing the Pipeline**: - Create a pipeline that performs the following steps in order: 1. Standardizes the data using `StandardScaler`. 2. Applies PCA to reduce dimensions to 2 components. 3. Trains a logistic regression model. 3. **Splitting the Data**: - Split the dataset into 80% training and 20% testing data. 4. **Training and Evaluation**: - Train the pipeline with the training data. - Evaluate the pipeline using the test data and print the accuracy score. # Boilerplate Code ```python from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), # Feature scaling (\'pca\', PCA(n_components=2)), # Dimensionality reduction (\'classifier\', LogisticRegression()) # Classification model ]) # Train the pipeline pipeline.fit(X_train, y_train) # Predict and evaluate the model on the test set y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy:.2f}\\") ``` Implement the above code in a function `train_and_evaluate_pipeline()` which, when called, executes the whole process and prints the final accuracy score.","solution":"def train_and_evaluate_pipeline(): from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score # Load the Iris dataset data = load_iris() X, y = data.data, data.target # Split the data into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Define the pipeline pipeline = Pipeline([ (\'scaler\', StandardScaler()), # Feature scaling (\'pca\', PCA(n_components=2)), # Dimensionality reduction (\'classifier\', LogisticRegression()) # Classification model ]) # Train the pipeline pipeline.fit(X_train, y_train) # Predict and evaluate the model on the test set y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(f\\"Model Accuracy: {accuracy:.2f}\\")"},{"question":"Objective Demonstrate your understanding of secure password handling and user identification in Python using the `getpass` module. In this task, you will create a secure login system that incorporates password input without echoing and validates user credentials. Problem Statement You are required to implement a secure login system class in Python. The class, `SecureLoginSystem`, should have the following functionalities: 1. **Register a User**: Register a user with a username and a password. 2. **Authenticate a User**: Prompt the user for their username and password securely and validate their credentials. 3. **Get Current User**: Retrieve the current system login name. Class Definition ```python class SecureLoginSystem: def __init__(self): # Initialize the user database (in-memory). pass def register_user(self, username: str, password: str) -> bool: Registers a new user with a username and password. :param username: The username of the new user. :param password: The password of the new user. :return: True if registration is successful, False if the username already exists. pass def authenticate_user(self) -> bool: Authenticates a user by securely prompting for username and password. :return: True if authentication is successful, False otherwise. pass def get_current_user(self) -> str: Returns the current system login name. :return: The login name of the current user. pass ``` Detailed Requirements 1. **`register_user` Method**: - This method should register a new user with a unique username. - If the username is already registered, the method should return `False`. - Otherwise, it should store the username and password securely (you can use a dictionary for simplicity) and return `True`. 2. **`authenticate_user` Method**: - Use `getpass.getpass()` to securely prompt the user for their password. - Use `input()` to prompt the user for their username. - Validate the entered credentials against the stored data. - Return `True` if the credentials are correct, `False` otherwise. 3. **`get_current_user` Method**: - Use `getpass.getuser()` to retrieve the current system login name. - Return the username. Constraints - Do not use any external libraries for password hashing or database management; stick to Python\'s built-in functionalities. - Assume case-sensitive usernames and passwords. Example Usage ```python login_system = SecureLoginSystem() assert login_system.register_user(\\"john_doe\\", \\"securePassword123\\") == True assert login_system.register_user(\\"john_doe\\", \\"anotherPass\\") == False # Username already exists # Simulate user input for authentication: # Username: john_doe # Password: securePassword123 assert login_system.authenticate_user() == True print(login_system.get_current_user()) # Should print the current system login name. ```","solution":"import getpass class SecureLoginSystem: def __init__(self): # Initialize the user database (in-memory). self.users = {} def register_user(self, username: str, password: str) -> bool: Registers a new user with a username and password. :param username: The username of the new user. :param password: The password of the new user. :return: True if registration is successful, False if the username already exists. if username in self.users: return False self.users[username] = password return True def authenticate_user(self) -> bool: Authenticates a user by securely prompting for username and password. :return: True if authentication is successful, False otherwise. username = input(\\"Username: \\") password = getpass.getpass(\\"Password: \\") if username in self.users and self.users[username] == password: return True return False def get_current_user(self) -> str: Returns the current system login name. :return: The login name of the current user. return getpass.getuser()"},{"question":"**Custom Function and Jacobian Computation** You are expected to implement a custom autograd `Function` in PyTorch, and use the functional higher-level API to compute the Jacobian matrix of this custom function. **Task Details:** 1. Implement a custom PyTorch function that computes: ( f(x) = [x_1^2 + x_2, exp(x_2), sin(x_1)] ) where ( x = [x_1, x_2] ) is a 2D input tensor. 2. Utilize `torch.autograd.Function` to define the forward and backward passes. 3. Use the higher-level functional API `torch.autograd.functional.jacobian` to compute the Jacobian of this function. **Expected Input and Output:** - **Input:** A 2D tensor `x` with `requires_grad=True` and shape `(2,)`. - **Output:** A 2D tensor representing the Jacobian matrix of the custom function evaluated at `x`, with shape `(3, 2)`. **Constraints:** - Do not use any in-place operations within your custom function. - The implementations of the forward and backward passes in the custom function should be efficient and should not use any looping constructs. **Performance Requirements:** - Ensure that the functions are memory efficient and handle the autograd graph correctly. **Example:** ```python import torch from torch.autograd import Function from torch.autograd import functional # Define the custom function class CustomFunction(Function): @staticmethod def forward(ctx, x): ctx.save_for_backward(x) return torch.tensor([x[0] ** 2 + x[1], torch.exp(x[1]), torch.sin(x[0])]) @staticmethod def backward(ctx, grad_output): x, = ctx.saved_tensors grad_x = grad_output.new_zeros(x.shape) grad_x[0] = 2 * x[0] * grad_output[0] + torch.cos(x[0]) * grad_output[2] grad_x[1] = grad_output[0] + torch.exp(x[1]) * grad_output[1] return grad_x # Function to compute Jacobian def compute_jacobian(input_tensor): return functional.jacobian(CustomFunction.apply, input_tensor) # Example usage x = torch.tensor([1.0, 2.0], requires_grad=True) jacobian_matrix = compute_jacobian(x) print(jacobian_matrix) ``` This code example provides a template for students to implement and test their solution. The implemented custom function is expected to correctly use forward and backward methods, and the jacobian computation should be demonstrated on a sample input.","solution":"import torch from torch.autograd import Function from torch.autograd import functional # Define the custom function class CustomFunction(Function): @staticmethod def forward(ctx, x): Forward pass for the custom function. Saves the input tensor for use in the backward pass. Arguments: ctx -- context object used for saving tensors needed in the backward pass x -- input tensor Returns: Tensor with computed values [x1^2 + x2, exp(x2), sin(x1)] ctx.save_for_backward(x) return torch.tensor([x[0] ** 2 + x[1], torch.exp(x[1]), torch.sin(x[0])]) @staticmethod def backward(ctx, grad_output): Backward pass for the custom function. Computes the gradient of the output w.r.t. the input. Arguments: ctx -- context object that holds saved tensors from the forward pass grad_output -- tensor of gradients of the outputs Returns: Tensor of gradients of the input x, = ctx.saved_tensors grad_x = grad_output.new_zeros(x.shape) grad_x[0] = 2 * x[0] * grad_output[0] + torch.cos(x[0]) * grad_output[2] grad_x[1] = grad_output[0] + torch.exp(x[1]) * grad_output[1] return grad_x # Function to compute Jacobian def compute_jacobian(input_tensor): Computes the Jacobian matrix for the custom function. Arguments: input_tensor -- the input tensor with requires_grad=True Returns: Jacobian matrix of the custom function evaluated at the input tensor return functional.jacobian(CustomFunction.apply, input_tensor) # Example usage x = torch.tensor([1.0, 2.0], requires_grad=True) jacobian_matrix = compute_jacobian(x) print(jacobian_matrix)"},{"question":"**Problem Statement:** Design a script using the `nntplib` module to fetch and analyze the latest newsgroup messages. You need to implement a function `fetch_latest_articles(server, newsgroup, count)` that connects to a specified NNTP server, selects a newsgroup, and retrieves the subjects of the latest `count` articles in that newsgroup. Additionally, your function should handle any possible connection errors gracefully and return an appropriate message. # Requirements: 1. **Function Signature**: ```python def fetch_latest_articles(server: str, newsgroup: str, count: int) -> Union[str, List[str]]: ``` 2. **Inputs**: - `server` (str): NNTP server address. - `newsgroup` (str): Name of the newsgroup to fetch articles from. - `count` (int): The number of latest articles to retrieve. 3. **Output**: - Returns a list of strings, where each string is the subject of an article. - Return a string with an error message if the connection fails or if fetching articles is unsuccessful. 4. **Tasks**: - Establish a connection to the specified NNTP server. - Select the specified newsgroup. - Fetch the subjects of the latest `count` articles. - Handle any exceptions and errors, returning appropriate error messages. 5. **Constraints**: - Use the `nntplib` module. - You must handle exceptions such as `NNTPPermanentError`, `NNTPTemporaryError`, and `NNTPReplyError`. # Example Usage: ```python articles = fetch_latest_articles(\\"news.gmane.io\\", \\"gmane.comp.python.committers\\", 5) if isinstance(articles, list): for article in articles: print(article) else: print(\\"Error:\\", articles) ``` # Notes: - Make sure to properly close the NNTP connection after completing the operations. - Use `nntplib.decode_header` to handle any non-ASCII characters in article subjects. # Implementation: ```python import nntplib from typing import Union, List def fetch_latest_articles(server: str, newsgroup: str, count: int) -> Union[str, List[str]]: try: # Establish an NNTP connection with nntplib.NNTP(server) as connection: # Select the specified newsgroup resp, article_count, first_article, last_article, group_name = connection.group(newsgroup) # Fetch the last `count` articles resp, overviews = connection.over((last_article - count + 1, last_article)) # Extract and decode subjects subjects = [nntplib.decode_header(overview[\'subject\']) for id, overview in overviews] return subjects except (nntplib.NNTPPermanentError, nntplib.NNTPTemporaryError, nntplib.NNTPReplyError) as e: return f\\"Failed to fetch articles: {e}\\" except Exception as e: return f\\"An unexpected error occurred: {e}\\" ``` Write a script that uses the `fetch_latest_articles` function to connect to the server \\"news.gmane.io\\", fetch the latest 10 subjects from the newsgroup \\"gmane.comp.python.committers\\", and print them.","solution":"import nntplib from typing import Union, List def fetch_latest_articles(server: str, newsgroup: str, count: int) -> Union[str, List[str]]: try: # Establish an NNTP connection with nntplib.NNTP(server) as connection: # Select the specified newsgroup resp, article_count, first_article, last_article, group_name = connection.group(newsgroup) # Fetch the last `count` articles start_article = max(first_article, last_article - count + 1) resp, overviews = connection.over((start_article, last_article)) # Extract and decode subjects subjects = [nntplib.decode_header(overview[\'subject\']) for id, overview in overviews] return subjects except (nntplib.NNTPPermanentError, nntplib.NNTPTemporaryError, nntplib.NNTPReplyError) as e: return f\\"Failed to fetch articles: {e}\\" except Exception as e: return f\\"An unexpected error occurred: {e}\\""},{"question":"# Asynchronous Echo Server Objective Design and implement an asynchronous echo server using the `asyncio` module. The server should receive messages from connected clients and send back the same messages to the clients, demonstrating the use of event loops, transport, and protocol management. Requirements 1. **Connection Handling**: - The server should be able to handle multiple client connections simultaneously. - Create a TCP server that listens on `localhost` and port `8888`. 2. **Message Echoing**: - For each connected client, receive messages sent by the client and send back the same message. 3. **Graceful Shutdown**: - The server should be able to shut down gracefully when interrupted (e.g., via a keyboard interrupt). Implementation Guidelines - Use `asyncio`\'s high-level `start_server` function to create and start the server. - Define a custom protocol class that inherits from `asyncio.Protocol` and implements the necessary callback methods to handle connections, data reception, and connection loss. - Manage the event loop and handle exceptions appropriately. Input and Output Formats - **Input**: Clients will connect to the server and send messages in plaintext. - **Output**: The server will respond with the same message that was received from the client. Example Suppose the client sends the message `\\"Hello, Echo Server!\\"`, the server should respond with `\\"Hello, Echo Server!\\"`. Constraints - Ensure proper handling of concurrent client connections. - Implement efficient error handling and resource management to avoid resource leaks. - The server should be non-blocking and asynchronous. Performance Requirements - The server should be able to handle at least 100 concurrent connections efficiently. - Message echoing should have minimal latency. Bonus - Implement additional logging to track connection states, messages received, and errors during communication. - Make the server configurable to allow different host and port configurations. ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(f\'Connection from {transport.get_extra_info(\\"peername\\")}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') self.transport.write(data) print(f\'Data sent: {message}\') def connection_lost(self, exc): print(\'Connection lost\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': try: asyncio.run(main()) except KeyboardInterrupt: print(\'Server is shutting down\') ``` Note - Ensure to test the server using multiple clients to verify its functionality. - Address proper cleanup and closure of resources when the server is interrupted. - Consider writing unit tests for key components of the server, if possible.","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport print(f\'Connection from {transport.get_extra_info(\\"peername\\")}\') def data_received(self, data): message = data.decode() print(f\'Data received: {message}\') self.transport.write(data) print(f\'Data sent: {message}\') def connection_lost(self, exc): print(\'Connection lost\') async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), \'127.0.0.1\', 8888) async with server: await server.serve_forever() if __name__ == \'__main__\': try: asyncio.run(main()) except KeyboardInterrupt: print(\'Server is shutting down\')"},{"question":"//Question Title: **Command Line Argument Parser Implementation using getopt in Python** //Question Description: You are tasked with creating a command line utility script that evaluates simple mathematical expressions. The script should accept two numbers and an operator from the command line, process these arguments, and output the result of the operation. The script should handle both short and long option formats. # Requirements: 1. You must use the `getopt` module for parsing command line arguments. 2. The script should support the following options: - `-a, --num1`: The first number (required). - `-b, --num2`: The second number (required). - `-o, --operator`: The mathematical operator (one of: add, sub, mul, div) (required). 3. If any required options are missing or if an invalid operator is provided, your script should raise an appropriate error and print a user-friendly help message. # Input: - Short options and their operands: `-a <num1> -b <num2> -o <operator>` - Long options and their operands: `--num1 <num1> --num2 <num2> --operator <operator>` # Output: - The result of the mathematical operation specified by the operator on the two numbers. # Constraints: - Numbers provided should be valid integers. - The operator must be one of the following strings: `add`, `sub`, `mul`, or `div`. - Division by zero should be handled gracefully, displaying an appropriate error message. # Example Usage: ``` python calc.py -a 10 -b 5 -o add Result: 15 python calc.py --num1 20 --num2 4 --operator div Result: 5.0 python calc.py --num1 15 --num2 3 --operator mul Result: 45 python calc.py -a 15 -b 0 -o div Error: Division by zero is not allowed. ``` # Implementation: Implement a Python function `parse_and_compute(argv)` that performs the above described functionality. Here, `argv` is a list of command line arguments passed to the script (excluding the script name itself). ```python import sys import getopt def parse_and_compute(argv): def usage(): print(\\"Usage:\\") print(\\" -a <num1> or --num1 <num1>: First number (required)\\") print(\\" -b <num2> or --num2 <num2>: Second number (required)\\") print(\\" -o <operator> or --operator <operator>: Operation (one of add, sub, mul, div) (required)\\") try: opts, args = getopt.getopt(argv, \\"a:b:o:\\", [\\"num1=\\", \\"num2=\\", \\"operator=\\"]) except getopt.GetoptError as err: print(err) usage() sys.exit(2) num1, num2, operator = None, None, None for o, a in opts: if o in (\\"-a\\", \\"--num1\\"): num1 = int(a) elif o in (\\"-b\\", \\"--num2\\"): num2 = int(a) elif o in (\\"-o\\", \\"--operator\\"): operator = a if num1 is None or num2 is None or operator is None: usage() sys.exit(2) try: if operator == \\"add\\": result = num1 + num2 elif operator == \\"sub\\": result = num1 - num2 elif operator == \\"mul\\": result = num2 * num2 elif operator == \\"div\\": if num2 == 0: raise ValueError(\\"Division by zero is not allowed.\\") result = num1 / num2 else: raise ValueError(\\"Invalid operator. Use one of: add, sub, mul, div.\\") print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") usage() sys.exit(2) # You can use the following example call to test your function locally: # if __name__ == \\"__main__\\": # parse_and_compute(sys.argv[1:]) ``` Ensure your implementation correctly handles options as described and provides appropriate error messages for incorrect usage or invalid input.","solution":"import sys import getopt def parse_and_compute(argv): def usage(): print(\\"Usage:\\") print(\\" -a <num1> or --num1 <num1>: First number (required)\\") print(\\" -b <num2> or --num2 <num2>: Second number (required)\\") print(\\" -o <operator> or --operator <operator>: Operation (one of add, sub, mul, div) (required)\\") try: opts, args = getopt.getopt(argv, \\"a:b:o:\\", [\\"num1=\\", \\"num2=\\", \\"operator=\\"]) except getopt.GetoptError as err: print(f\\"Error: {err}\\") usage() sys.exit(2) num1, num2, operator = None, None, None for o, a in opts: if o in (\\"-a\\", \\"--num1\\"): num1 = int(a) elif o in (\\"-b\\", \\"--num2\\"): num2 = int(a) elif o in (\\"-o\\", \\"--operator\\"): operator = a if num1 is None or num2 is None or operator is None: print(\\"Error: All options -a, -b, and -o are required.\\") usage() sys.exit(2) try: if operator == \\"add\\": result = num1 + num2 elif operator == \\"sub\\": result = num1 - num2 elif operator == \\"mul\\": result = num1 * num2 elif operator == \\"div\\": if num2 == 0: raise ValueError(\\"Division by zero is not allowed.\\") result = num1 / num2 else: raise ValueError(\\"Invalid operator. Use one of: add, sub, mul, div.\\") print(f\\"Result: {result}\\") except ValueError as e: print(f\\"Error: {e}\\") usage() sys.exit(2) # You can use the following example call to test your function locally: # if __name__ == \\"__main__\\": # parse_and_compute(sys.argv[1:])"},{"question":"You are working on an email service that needs to construct and send different types of MIME-compliant email messages. Your task is to implement a function `create_mime_message` that constructs a MIME email with various parts (text, image, and attachment) according to the provided specifications. # Function Signature ```python def create_mime_message(subject: str, sender: str, recipient: str, text: str, image_path: str, attachment_path: str) -> email.message.Message: pass ``` # Input - `subject`: A string representing the subject of the email. - `sender`: A string representing the sender\'s email address. - `recipient`: A string representing the recipient\'s email address. - `text`: A string containing the text content of the email. - `image_path`: A string representing the file path to an image to be attached to the email. - `attachment_path`: A string representing the file path to a binary attachment to be included in the email. # Output - A `email.message.Message` object representing the constructed MIME email. # Constraints - The provided `image_path` should point to a valid image file. - The provided `attachment_path` should point to a binary file such as a PDF or any other non-text file. # Requirements 1. The email must be a multipart email with: - A `MIMEText` part for the plain text content. - A `MIMEImage` part for the image attachment. - A `MIMEApplication` part for the binary attachment. 2. Appropriate headers (such as `Subject`, `From`, `To`, and `MIME-Version`) should be set. 3. The function should use the provided image file and attachment file data correctly. 4. Use the default encoding (base64) for the attachments. # Example Usage ```python msg = create_mime_message( subject=\\"Test Email\\", sender=\\"sender@example.com\\", recipient=\\"recipient@example.com\\", text=\\"This is a test email with an image and an attachment.\\", image_path=\\"/path/to/image.jpg\\", attachment_path=\\"/path/to/document.pdf\\" ) # The `msg` object should now represent a properly constructed MIME email with the specified content. ``` # Notes - You may use the `email.mime` classes (`MIMEText`, `MIMEImage`, `MIMEApplication`, `MIMEMultipart`) to construct the email parts. - Ensure that the image and attachments are properly encoded and attached to the email. - The function should handle cases where the provided paths are invalid by raising appropriate exceptions.","solution":"import os import email from email.mime.multipart import MIMEMultipart from email.mime.text import MIMEText from email.mime.image import MIMEImage from email.mime.application import MIMEApplication def create_mime_message(subject, sender, recipient, text, image_path, attachment_path): # Check if files exist if not os.path.exists(image_path): raise FileNotFoundError(f\\"Image file not found: {image_path}\\") if not os.path.exists(attachment_path): raise FileNotFoundError(f\\"Attachment file not found: {attachment_path}\\") # Create the root message msg = MIMEMultipart() msg[\'Subject\'] = subject msg[\'From\'] = sender msg[\'To\'] = recipient msg[\'MIME-Version\'] = \'1.0\' # Attach the text part text_part = MIMEText(text, \'plain\') msg.attach(text_part) # Attach the image part with open(image_path, \'rb\') as img_file: img_data = img_file.read() image_part = MIMEImage(img_data) msg.attach(image_part) # Attach the binary attachment part with open(attachment_path, \'rb\') as att_file: attachment_data = att_file.read() attachment_part = MIMEApplication(attachment_data) attachment_part.add_header(\'Content-Disposition\', \'attachment\', filename=os.path.basename(attachment_path)) msg.attach(attachment_part) return msg"},{"question":"# Custom asyncio Event Loop Policy **Objective**: Implement a custom event loop policy by subclassing the `DefaultEventLoopPolicy` to include additional logging whenever a new event loop is created or fetched. # Problem Statement You are required to create a custom event loop policy in Python using the `asyncio` module. Your custom policy should: 1. Log a message every time an event loop is retrieved or created. 2. Use the default event loop policy behavior as the base. # Detailed Requirements 1. **Class Definition**: - Create a class `LoggingEventLoopPolicy` that subclasses `asyncio.DefaultEventLoopPolicy`. 2. **Methods**: - Override the `get_event_loop()` method to log a message anytime an event loop is retrieved. - Override the `new_event_loop()` method to log a message anytime a new event loop is created. - The log message should use the `logging` module and should be set at the `INFO` level. 3. **Setting the Policy**: - Ensure that your custom event loop policy is set as the current event loop policy using `asyncio.set_event_loop_policy()`. 4. **Logging Setup**: - Configure the logging module to output messages to the console. # Input There is no direct input required for the problem. The implementation will be tested within a script that may call `asyncio.get_event_loop()` and `asyncio.new_event_loop()` to verify the logging. # Output The output will be log messages printed to the console indicating when event loops are retrieved or created. # Constraints - Use Python\'s `asyncio` and `logging` modules. - The logging messages should be formatted as follows: - `\\"[INFO] Event loop retrieved\\"` - `\\"[INFO] New event loop created\\"` # Example Below is an example code snippet demonstrating how the custom policy will be used: ```python import asyncio import logging class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() logging.info(\\"Event loop retrieved\\") return loop def new_event_loop(self): loop = super().new_event_loop() logging.info(\\"New event loop created\\") return loop # Setting up logging logging.basicConfig(level=logging.INFO, format=\'[%(levelname)s] %(message)s\') # Setting the custom event loop policy asyncio.set_event_loop_policy(LoggingEventLoopPolicy()) # Example usage loop = asyncio.get_event_loop() # Should log \\"Event loop retrieved\\" new_loop = asyncio.new_event_loop() # Should log \\"New event loop created\\" ``` # Notes - Your implementation will be tested for correctness and adherence to the requirements via automated scripts that will simulate the use of the asyncio event loop.","solution":"import asyncio import logging class LoggingEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() logging.info(\\"Event loop retrieved\\") return loop def new_event_loop(self): loop = super().new_event_loop() logging.info(\\"New event loop created\\") return loop # Setup logging to output to the console logging.basicConfig(level=logging.INFO, format=\'[%(levelname)s] %(message)s\') # Set the custom event loop policy asyncio.set_event_loop_policy(LoggingEventLoopPolicy())"},{"question":"**Question: Implementing a Custom Distributed Optimizer in PyTorch** In this exercise, you are required to implement a simplistic custom distributed optimizer to assess understanding of PyTorch\'s tensor operations and the concept of optimizers. Because the documentation provides limited information about the distributed optimizer, we will only focus on a simplified form of distributed optimization. # Task: - Create a class `SimpleDistributedOptimizer` that mimics the behavior of a distributed optimizer. - This optimizer will distribute parts of the optimization task across different devices (e.g., CPUs in this exercise). - Implement the `step` function that updates the parameters based on the gradient and a learning rate. # Requirements: 1. The class `SimpleDistributedOptimizer` should accept a list of parameters to optimize and a learning rate. 2. Implement a method `step()` that: - Aggregates gradients from all parameters onto a single device (CPU). - Applies gradient descent to update all parameters. 3. You should handle parameter distribution across devices. For simplicity in this exercise, assume that the parameters will be distributed evenly across available CPUs (`device=\'cpu\'`). # Constraints: - Do not use any built-in distributed optimizer classes from PyTorch. Only use fundamental PyTorch operations and classes. - Ensure that the implementation is efficient and leverages PyTorch\'s tensor operations. # Expected Inputs and Outputs: - Input: - `params` (List of `torch.Tensor`): A list of tensors representing parameters. Each tensor should have an associated `.grad` attribute representing the gradient. - `lr` (float): The learning rate for gradient descent. - Output: - The `step` function should return `None` but should update the parameter tensors in place. # Example Usage: ```python import torch class SimpleDistributedOptimizer: def __init__(self, params, lr): self.params = params self.lr = lr def step(self): # Aggregate gradients to a single location (CPU) aggregated_gradients = [p.grad.to(\'cpu\') for p in self.params] # Apply gradient descent for i, p in enumerate(self.params): p.data -= self.lr * aggregated_gradients[i] # Example parameters params = [torch.tensor([1.0, 2.0], requires_grad=True), torch.tensor([3.0, 4.0], requires_grad=True)] # Manually set gradients for demonstration purposes params[0].grad = torch.tensor([0.1, 0.1]) params[1].grad = torch.tensor([0.2, 0.2]) # Initialize optimizer optimizer = SimpleDistributedOptimizer(params, lr=0.01) # Before optimization step print(\\"Before step:\\") print(params[0]) print(params[1]) # Perform an optimization step optimizer.step() # After optimization step print(\\"nAfter step:\\") print(params[0]) print(params[1]) ``` Ensure that the functionality is demonstrated with meaningful examples. Document any assumptions clearly.","solution":"import torch class SimpleDistributedOptimizer: def __init__(self, params, lr): self.params = params self.lr = lr def step(self): # Aggregate gradients to a single location (CPU) aggregated_gradients = [p.grad.to(\'cpu\') for p in self.params] # Apply gradient descent for i, p in enumerate(self.params): p.data -= self.lr * aggregated_gradients[i]"},{"question":"**Question: Turtle Graphics Challenge** # Objective Use the \\"turtle\\" module in Python to create a program that draws a complex pattern based on user input and adjusts the drawing dynamically based on specific events. # Problem Statement Write a Python function `draw_pattern()` that: 1. Initializes a Turtle screen. 2. Creates a turtle that can accept drawing commands. 3. Draws a geometric pattern based on user input parameters. 4. Responds to specific keyboard events to change turtle’s properties and re-draw the pattern. # Input Format Your function should accept three parameters: - `sides` (int): The number of sides for each geometric shape in the pattern (e.g., 3 for triangles, 4 for squares). - `iterations` (int): The number of repetitions for the pattern. - `length` (int): The length of each side of the shape. # Output Format The function does not return anything but should display a graphical Turtle window with the drawn pattern. # Constraints 1. `sides` should be between 3 and 10. 2. `iterations` should be between 1 and 20. 3. `length` should be between 20 and 100. # Functional Requirements 1. Your turtle should start at the center of the screen. 2. After drawing each shape, the turtle should turn right by 360/`iterations`. 3. The turtle\'s pen color should change to a random color before drawing each shape. 4. Respond to the following keyboard events: - \'r\': Reset the drawing and redraw the pattern. - \'q\': Quit the turtle graphics window. # Performance Requirements The drawing should be efficient and quick. The program should handle the maximum input sizes within a reasonable time frame. # Example Usage When called with the following arguments: `draw_pattern(sides=4, iterations=10, length=50)`, your program should: - Create a pattern with squares, each of length 50. - Draw 10 such squares, each rotated by 36 degrees (360/10). - Change the pen color for each square to a random color. - Respond to \'r\' to reset and redraw. - Respond to \'q\' to quit the window. # Implementation Tips - Use the `turtle.Turtle` and `turtle.Screen` objects to manage drawing and events. - Use a loop to handle the drawing and rotation logic. - Utilize the `turtle.onkey` and `turtle.listen` methods to handle keyboard events. **Example Code Skeleton:** ```python import turtle import random def draw_pattern(sides, iterations, length): # Initialize the Turtle screen and the turtle screen = turtle.Screen() pattern_turtle = turtle.Turtle() # Define a function to draw a single shape def draw_shape(): pattern_turtle.clear() # Clear previous drawings for _ in range(iterations): color = (random.random(), random.random(), random.random()) pattern_turtle.pencolor(color) for _ in range(sides): pattern_turtle.forward(length) pattern_turtle.right(360 / sides) pattern_turtle.right(360 / iterations) # Handle \'r\' and \'q\' events def reset_drawing(): pattern_turtle.reset() draw_shape() def quit_program(): screen.bye() # Bind the events to the turtle screen screen.onkey(reset_drawing, \'r\') screen.onkey(quit_program, \'q\') screen.listen() # Initial drawing draw_shape() # Keep the window open until \'q\' is pressed screen.mainloop() # Example call to the function draw_pattern(4, 10, 50) ``` Ensure your solution follows these specifications and passes all the constraints and functional requirements.","solution":"import turtle import random def draw_pattern(sides, iterations, length): Draws a geometric pattern based on user input parameters. Args: sides (int): Number of sides for each geometric shape in the pattern. iterations (int): Number of repetitions for the pattern. length (int): Length of each side of the shape. if sides < 3 or sides > 10: raise ValueError(\\"sides should be between 3 and 10.\\") if iterations < 1 or iterations > 20: raise ValueError(\\"iterations should be between 1 and 20.\\") if length < 20 or length > 100: raise ValueError(\\"length should be between 20 and 100.\\") # Initialize the Turtle screen and the turtle screen = turtle.Screen() pattern_turtle = turtle.Turtle() pattern_turtle.speed(0) # Set to maximum drawing speed # Define a function to draw a single shape def draw_shape(): pattern_turtle.clear() # Clear previous drawings for _ in range(iterations): color = (random.random(), random.random(), random.random()) pattern_turtle.pencolor(color) for _ in range(sides): pattern_turtle.forward(length) pattern_turtle.right(360 / sides) pattern_turtle.right(360 / iterations) # Handle \'r\' and \'q\' events def reset_drawing(): pattern_turtle.reset() pattern_turtle.speed(0) draw_shape() def quit_program(): screen.bye() # Bind the events to the turtle screen screen.onkey(reset_drawing, \'r\') screen.onkey(quit_program, \'q\') screen.listen() # Initial drawing draw_shape() # Keep the window open until \'q\' is pressed screen.mainloop()"},{"question":"# PyTorch Random Number Generation and Reproducibility **Objective:** Implement the function `generate_reproducible_random_tensor`, which generates a random tensor of specified size and type while ensuring reproducibility. The function must accept seeds for both CPU and GPU random number generators. **Function Signature:** ```python def generate_reproducible_random_tensor(cpu_seed: int, gpu_seed: int, size: tuple, dtype: torch.dtype) -> torch.Tensor: pass ``` **Input:** - `cpu_seed` (int): Seed for the CPU random number generator. - `gpu_seed` (int): Seed for the GPU random number generator. - `size` (tuple): Size of the tensor to be generated. - `dtype` (torch.dtype): Data type of the tensor to be generated. **Output:** - `torch.Tensor`: A tensor of the specified size and type, generated with the specified seeds. **Constraints:** - The function must ensure reproducibility across both CPU and GPU computations by setting the seeds appropriately. - Validate the type of seeds and the size before proceeding with random tensor generation. - If a GPU is available, generate the tensor on the GPU; otherwise, generate it on the CPU. **Requirements:** 1. Use PyTorch\'s `torch.manual_seed` for CPU and `torch.cuda.manual_seed` (if GPU is available) to set the seeds. 2. Ensure the generated tensor uses the specified data type and size. 3. Implement robust error checking to handle improper input types for seeds and sizes. **Example:** ```python import torch cpu_seed = 42 gpu_seed = 24 size = (3, 3) dtype = torch.float32 tensor = generate_reproducible_random_tensor(cpu_seed, gpu_seed, size, dtype) print(tensor) ``` **Expected Output:** A 3x3 tensor of type `torch.float32` with values that are reproducibly generated based on the provided seeds. # Note: You may assume that the necessary PyTorch methods for random number generation (`torch.manual_seed` and `torch.cuda.manual_seed`) are well-documented and accessible in the actual PyTorch documentation.","solution":"import torch def generate_reproducible_random_tensor(cpu_seed: int, gpu_seed: int, size: tuple, dtype: torch.dtype) -> torch.Tensor: Generates a random tensor of specified size and type while ensuring reproducibility with specified CPU and GPU seeds. Args: cpu_seed (int): Seed for the CPU random number generator. gpu_seed (int): Seed for the GPU random number generator. size (tuple): Size of the tensor to be generated. dtype (torch.dtype): Data type of the tensor to be generated. Returns: torch.Tensor: A tensor of the specified size and type. # Validate seeds if not isinstance(cpu_seed, int) or not isinstance(gpu_seed, int): raise ValueError(\\"The seeds must be integers.\\") # Validate size if not isinstance(size, tuple): raise ValueError(\\"The size must be a tuple.\\") if not all(isinstance(dim, int) and dim > 0 for dim in size): raise ValueError(\\"All dimensions in size must be positive integers.\\") # Validate dtype if not isinstance(dtype, torch.dtype): raise ValueError(\\"The dtype must be a valid torch.dtype.\\") # Set random seeds for reproducibility torch.manual_seed(cpu_seed) if torch.cuda.is_available(): torch.cuda.manual_seed(gpu_seed) device = \'cuda\' else: device = \'cpu\' # Generate the random tensor tensor = torch.rand(size, dtype=dtype, device=device) return tensor"},{"question":"**Title:** XML Parsing with SAX **Objective:** Given an XML string, parse it using SAX, extract specific data, and handle any potential errors gracefully. **Task Description:** Write a Python function `parse_xml(xml_string: str, tag: str) -> List[str]` that: 1. Parses the provided XML string using the SAX parser. 2. Extracts the text data within elements that match the provided tag name. 3. Handles any parsing errors by logging an appropriate error message. **Input:** - `xml_string`: A string containing well-formed XML data. - `tag`: A string representing the tag name whose text content needs to be extracted. **Output:** - A list of strings containing the text content of the specified tag. **Constraints and Notes:** - The XML string provided will be well-formed. - If there are no matching tags, return an empty list. - Demonstrate the usage of SAX exception handling. - Use the `xml.sax` package only for parsing. **Example:** ```python xml_data = \'\'\' <catalog> <book id=\\"bk101\\"> <author>Gambardella, Matthew</author> <title>XML Developer\'s Guide</title> <genre>Computer</genre> <price>44.95</price> <publish_date>2000-10-01</publish_date> <description>An in-depth look at creating applications with XML.</description> </book> <book id=\\"bk102\\"> <author>Ralls, Kim</author> <title>Midnight Rain</title> <genre>Fantasy</genre> <price>5.95</price> <publish_date>2000-12-16</publish_date> <description>A former architect battles corporate zombies, an evil sorceress, and her own childhood to become queen of the world.</description> </book> </catalog> \'\'\' print(parse_xml(xml_data, \'author\')) ``` **Expected Output:** ```python [\'Gambardella, Matthew\', \'Ralls, Kim\'] ``` **Function Signature:** ```python def parse_xml(xml_string: str, tag: str) -> List[str]: # your code here ``` **Approach:** 1. Use `xml.sax.parseString` to parse the XML string. 2. Define a custom handler class that extends `xml.sax.handler.ContentHandler`. 3. Implement the `startElement`, `endElement`, and `characters` methods to collect data. 4. Use exception handling to log any SAX exceptions that occur during parsing.","solution":"import xml.sax from typing import List def parse_xml(xml_string: str, tag: str) -> List[str]: class TagHandler(xml.sax.handler.ContentHandler): def __init__(self, tag): self.tag = tag self.is_inside_tag = False self.contents = [] self.current_content = \'\' def startElement(self, name, attrs): if name == self.tag: self.is_inside_tag = True self.current_content = \'\' def endElement(self, name): if name == self.tag: self.is_inside_tag = False self.contents.append(self.current_content.strip()) self.current_content = \'\' def characters(self, content): if self.is_inside_tag: self.current_content += content handler = TagHandler(tag) try: xml.sax.parseString(xml_string, handler) except xml.sax.SAXParseException as e: print(f\\"Error parsing XML: {e}\\") return [] return handler.contents"},{"question":"Memory Usage Analyzer You are tasked with writing a Python program that analyses memory allocations in a given Python application using the `tracemalloc` module. Your program should: 1. Start tracing memory allocations. 2. Perform some memory-intensive operations. 3. Take two snapshots of memory allocations: one before the operations and one after the operations. 4. Compare the snapshots to compute the differences in memory usage. 5. Filter the traces to exclude certain files from consideration. 6. Output a report of the top memory usage differences, including the size difference, count difference, and the tracebacks where the allocations occurred. # Requirements - Define a function `memory_usage_analyzer(operations: Callable[[], None], excluded_files: List[str]) -> None` that takes: - `operations`: A callable function that performs the memory-intensive operations. - `excluded_files`: A list of filenames or filename patterns to exclude from the memory trace analysis. - Your function should: 1. Start tracing memory allocations with the ability to store a maximum of 25 frames for each trace. 2. Take a snapshot of memory allocations before performing the operations. 3. Perform the operations by calling the passed `operations` function. 4. Take another snapshot of memory allocations after performing the operations. 5. Compare the snapshots and compute the differences in memory allocations. 6. Filter out memory traces from the excluded files provided. 7. Print a report of the top 10 memory allocation differences (by size difference) including: - The filename and line number where the allocation occurred. - The difference in size and count of allocations. - A formatted traceback of the allocation. # Input and Output Formats - **Input**: - `operations`: A callable function (function passed does not take any arguments). - `excluded_files`: List of strings representing filenames or filename patterns to exclude. - **Output**: - The function should print the report directly. - The report should include the top 10 memory allocation differences in the following format: ``` [ Top 10 differences ] filename:line - size_diff B (count_diff allocations) Traceback: File \\"filename\\", line number in function <function_name> ... ``` # Constraints - You should handle exceptions gracefully and print meaningful error messages. - If there are fewer than 10 differences, print all of them. - Use Python 3.10 or later. # Example Usage ```python import tracemalloc from typing import Callable, List def example_operations(): # Example memory-intensive operations large_list = [i for i in range(1000000)] del large_list def memory_usage_analyzer(operations: Callable[[], None], excluded_files: List[str]) -> None: # Your implementation here # Example usage memory_usage_analyzer(example_operations, excluded_files=[\\"<string>\\", \\"<frozen importlib._bootstrap>\\"]) ``` # Example Output ``` [ Top 10 differences ] <file_path>:line_number - size_diff B (count_diff allocations) Traceback: File \\"<filename>\\", line number in function <function_name> ... ```","solution":"import tracemalloc from typing import Callable, List def memory_usage_analyzer(operations: Callable[[], None], excluded_files: List[str]) -> None: try: # Start tracing memory allocations tracemalloc.start(25) # Take a snapshot of memory allocations before operations snapshot1 = tracemalloc.take_snapshot() # Perform the memory-intensive operations operations() # Take another snapshot after operations snapshot2 = tracemalloc.take_snapshot() # Compute the differences between the two snapshots top_stats = snapshot2.compare_to(snapshot1, \'lineno\') # Filter out excluded files filtered_stats = [stat for stat in top_stats if not any(excluded_file in stat.traceback[0].filename for excluded_file in excluded_files)] # Print the report of the top memory usage differences print(\\"[ Top 10 differences ]\\") for stat in filtered_stats[:10]: print(f\\"{stat.traceback[0].filename}:{stat.traceback[0].lineno} - {stat.size_diff} B ({stat.count_diff} allocations)\\") print(\\" Traceback (most recent call last):\\") for frame in stat.traceback: print(f\\" File \\"{frame.filename}\\", line {frame.lineno}, in {frame.name}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: # Stop the tracemalloc module tracemalloc.stop()"},{"question":"# Programming Assessment Question Objective Implement a function that retrieves and processes information from the Unix shadow password database using the `spwd` module. Problem Statement You are required to implement a function `check_password_policy(users_info: List[Dict[str, Any]], min_days: int, max_days: int) -> Dict[str, List[str]]` that does the following: 1. Retrieves the shadow password entries for the given list of users. 2. Checks each user\'s password policy against specified minimum and maximum days for password change. 3. Returns a dictionary with two keys: \\"compliant\\" and \\"non_compliant\\". The value for each key should be a list of user names that are compliant and non-compliant with the specified password policy, respectively. Function Signature ```python import spwd from typing import List, Dict, Any def check_password_policy(users_info: List[Dict[str, Any]], min_days: int, max_days: int) -> Dict[str, List[str]]: pass ``` Expected Input and Output - **Input**: - `users_info` (List[Dict[str, Any]]): A list of dictionaries where each dictionary contains the `name` of the user. - `min_days` (int): The minimum number of days required before a password can be changed. - `max_days` (int): The maximum number of days that a password is valid. - **Output**: - Returns a dictionary with two keys: \\"compliant\\" and \\"non_compliant\\". Each key maps to a list of user names. Example ```python users_info = [{\\"name\\": \\"user1\\"}, {\\"name\\": \\"user2\\"}] min_days = 7 max_days = 90 result = check_password_policy(users_info, min_days, max_days) # Example Output: # { # \\"compliant\\": [\\"user1\\"], # \\"non_compliant\\": [\\"user2\\"] # } ``` Constraints - The function must handle the case where a user does not exist in the shadow password database by ignoring that user. - If the user does not have enough privileges to access the shadow password database, handle the `PermissionError` by raising it with an appropriate error message. - You may assume that the data in the `users_info` list is well-formed. Additional Information Utilize the `spwd` module\'s `getspnam` function to retrieve the shadow password entry for each user and check their password policies.","solution":"import spwd from typing import List, Dict, Any def check_password_policy(users_info: List[Dict[str, Any]], min_days: int, max_days: int) -> Dict[str, List[str]]: compliant = [] non_compliant = [] for user in users_info: try: spwd_entry = spwd.getspnam(user[\\"name\\"]) if spwd_entry.sp_min >= min_days and spwd_entry.sp_max <= max_days: compliant.append(user[\\"name\\"]) else: non_compliant.append(user[\\"name\\"]) except KeyError: # User does not exist in the shadow password database continue except PermissionError: raise PermissionError(\\"Insufficient privileges to access the shadow password database.\\") return { \\"compliant\\": compliant, \\"non_compliant\\": non_compliant }"},{"question":"**Objective:** Write a function that demonstrates the use of pandas options to customize the displaying of large DataFrames and the formatting of numeric data. Your function must handle different scenarios of data display and formatting. **Function Signature:** ```python import pandas as pd def customize_pandas_display(options: dict, df: pd.DataFrame) -> pd.DataFrame: Customize the pandas DataFrame display settings and demonstrate the settings with a sample DataFrame. Parameters: - options: A dictionary where keys are option names (as strings) and values are the desired settings. - df: A pandas DataFrame to apply the display settings on. Returns: - A pandas DataFrame with the display settings applied. ``` **Input:** - `options`: A dictionary where keys are option names (e.g., \'display.max_rows\', \'display.float_format\') and values are the desired settings. - `df`: A pandas DataFrame to which the display settings will be applied. **Output:** - A pandas DataFrame display should be customized as specified in the input dictionary. **Constraints:** - The DataFrame `df` should not be modified other than changing the display settings. - The function should handle multiple options simultaneously. **Example:** ```python import pandas as pd import numpy as np def customize_pandas_display(options: dict, df: pd.DataFrame) -> pd.DataFrame: with pd.option_context(*options): print(df) return df # Sample DataFrame data = np.random.rand(100, 10) df = pd.DataFrame(data, columns=[f\\"col_{i}\\" for i in range(10)]) # Custom options options = {\'display.max_rows\': 20, \'display.precision\': 2, \'display.float_format\': \'{:.1f}\'.format} # Apply custom options result_df = customize_pandas_display(options, df) ``` In this example, the function `customize_pandas_display` applies the custom display options such as `display.max_rows`, `display.precision`, and `display.float_format` to the given DataFrame `df`. The output format for floating-point numbers is set to one decimal place, and the DataFrame display is limited to 20 rows. **Additional Information:** - You may assume that the dictionary keys correspond to valid pandas options. - No need to validate the input DataFrame `df`. This question assesses the student\'s ability to use pandas options to manipulate DataFrame display settings effectively, showcasing their understanding of pandas\' configuration capabilities.","solution":"import pandas as pd def customize_pandas_display(options: dict, df: pd.DataFrame) -> pd.DataFrame: Customize the pandas DataFrame display settings and demonstrate the settings with a sample DataFrame. Parameters: - options: A dictionary where keys are option names (as strings) and values are the desired settings. - df: A pandas DataFrame to apply the display settings on. Returns: - A pandas DataFrame with the display settings applied. with pd.option_context(*[item for sublist in options.items() for item in sublist]): print(df) return df"},{"question":"In this task, you are required to implement a Python function that performs several operations using the `re` module on a given input string. The function should find all valid email addresses in the input string, extract the domain from each email, and replace every domain with a given new domain. The function should then return the modified string. An email address is considered valid if it adheres to the following pattern: - Begins with one or more alphanumeric characters (A-Z, a-z, 0-9), underscores (`_`), or dots (`.`). - Followed by an `@` symbol. - Followed by one or more alphanumeric characters or dots. - Ends with a top-level domain (e.g., `.com`, `.org`, `.net`, `.edu`), which contains between 2 and 6 alphabetic characters. # Requirements You need to implement the function `replace_email_domains(input_string: str, new_domain: str) -> str` which: - Receives: - `input_string`: A string containing arbitrary text, including possible email addresses. - `new_domain`: A string representing the new domain to replace the existing domains. - Returns: - A new string with all valid email domains replaced by `new_domain`. # Constraints - The function should not replace parts of words that superficially look like email addresses but don\'t meet the criteria. - The input string and new domain are guaranteed to be non-empty strings. - The new domain passed to the function will not contain any \\"@\\" symbol. # Examples ```python def replace_email_domains(input_string: str, new_domain: str) -> str: # Implement your solution here pass # Example 1 input_string = \\"Contact us at support@example.com or sale@company.org for more information.\\" new_domain = \\"newdomain.com\\" print(replace_email_domains(input_string, new_domain)) # Output: \\"Contact us at support@newdomain.com or sale@newdomain.com for more information.\\" # Example 2 input_string = \\"Emails: johndoe123@yahoo.com, jane.doe@mail.com\\" new_domain = \\"email.org\\" print(replace_email_domains(input_string, new_domain)) # Output: \\"Emails: johndoe123@email.org, jane.doe@email.org\\" ``` # Implementation Details 1. Use the `re.findall()` function to identify all valid email addresses in the input string. 2. Use `re.sub()` to replace the domain part of each email address with the new domain. 3. Return the modified string.","solution":"import re def replace_email_domains(input_string: str, new_domain: str) -> str: Replaces the domain of all valid email addresses in the input_string with new_domain. email_pattern = r\'([w._]+)@[w.-]+.[a-zA-Z]{2,6}\' def replace_domain(match): username = match.group(1) return f\\"{username}@{new_domain}\\" modified_string = re.sub(email_pattern, replace_domain, input_string) return modified_string"},{"question":"Objective Demonstrate your comprehension of the `json` module in Python by implementing custom serialization and deserialization functionality that handles a specific data structure. Problem You have a class `Person` representing a person\'s details which includes both basic data types and nested custom objects (like `Address`). Your task is to: 1. Serialize instances of `Person` into JSON strings. 2. Deserialize JSON strings back into `Person` instances. 3. Ensure that times are encoded in a specific string format (e.g., \\"HH:MM:SS\\"). To achieve this, you need to: - Implement custom encoder and decoder classes by subclassing `json.JSONEncoder` and `json.JSONDecoder`. - Use these classes to serialize and deserialize `Person` and nested `Address` objects correctly. Class Definitions ```python class Address: def __init__(self, street, city, zipcode): self.street = street self.city = city self.zipcode = zipcode class Person: def __init__(self, name, age, address, arrival_time): Parameters: - name (str): The name of the person. - age (int): The age of the person. - address (Address): The address of the person. - arrival_time (datetime.time): The time the person arrived. self.name = name self.age = age self.address = address self.arrival_time = arrival_time ``` Tasks 1. **JSON Serialization/Deserialization of `Person` objects**: - Implement a class `CustomJSONEncoder` to subclass `json.JSONEncoder` and handle `Person` and `Address` objects. - Implement a class `CustomJSONDecoder` to subclass `json.JSONDecoder` and handle `Person` and `Address` objects. 2. **Ensure that the object keys are sorted in the JSON output** when serialized. 3. **Constraints**: - The `arrival_time` should be serialized as a string in the format \\"HH:MM:SS\\". Function Definitions 1. **CustomJSONEncoder**: - Method `default(self, obj)` within `CustomJSONEncoder` which returns a JSON-encodable version of the object or raises a `TypeError`. 2. **CustomJSONDecoder**: - Method `decode(self, s)` within `CustomJSONDecoder` which takes a JSON string and returns the appropriate `Person` object. Example Usage ```python import json from datetime import time # Create an instance of Address address = Address(\\"123 Main St\\", \\"Anytown\\", \\"12345\\") # Create an instance of Person person = Person(\\"John Doe\\", 30, address, time(14, 30, 45)) # Serialize the person using CustomJSONEncoder person_json = json.dumps(person, cls=CustomJSONEncoder, sort_keys=True) print(person_json) # Deserialize the JSON string back to a Person object using CustomJSONDecoder person_obj = json.loads(person_json, cls=CustomJSONDecoder) print(person_obj.name, person_obj.age, person_obj.address.street, person_obj.arrival_time) ``` Make sure to include proper exception handling in your custom encoder and decoder. Submission Details 1. Implement `CustomJSONEncoder` class. 2. Implement `CustomJSONDecoder` class. 3. Ensure correct serialization and deserialization with all required specifications.","solution":"import json from datetime import time class Address: def __init__(self, street, city, zipcode): self.street = street self.city = city self.zipcode = zipcode def __eq__(self, other): return ( self.street == other.street and self.city == other.city and self.zipcode == other.zipcode ) class Person: def __init__(self, name, age, address, arrival_time): Parameters: - name (str): The name of the person. - age (int): The age of the person. - address (Address): The address of the person. - arrival_time (datetime.time): The time the person arrived. self.name = name self.age = age self.address = address self.arrival_time = arrival_time def __eq__(self, other): return ( self.name == other.name and self.age == other.age and self.address == other.address and self.arrival_time == other.arrival_time ) class CustomJSONEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Address): return { \'street\': obj.street, \'city\': obj.city, \'zipcode\': obj.zipcode } if isinstance(obj, Person): return { \'name\': obj.name, \'age\': obj.age, \'address\': obj.address, \'arrival_time\': obj.arrival_time.strftime(\\"%H:%M:%S\\") } return super().default(obj) class CustomJSONDecoder(json.JSONDecoder): def __init__(self, *args, **kwargs): super().__init__(object_hook=self.object_hook, *args, **kwargs) def object_hook(self, obj): if \'street\' in obj and \'city\' in obj and \'zipcode\' in obj: return Address(obj[\'street\'], obj[\'city\'], obj[\'zipcode\']) if \'name\' in obj and \'age\' in obj and \'address\' in obj and \'arrival_time\' in obj: arrival_time = time.fromisoformat(obj[\'arrival_time\']) address = obj[\'address\'] return Person(obj[\'name\'], obj[\'age\'], address, arrival_time) return obj"},{"question":"Objective Demonstrate your comprehension of Python control flow tools and function implementation based on the provided documentation. Problem Statement You are tasked to simulate a simple text-based battle game between a player and a series of monsters. The game should proceed in rounds, with the player and each monster having the opportunity to attack in each round until one of them is defeated. You are required to implement the following functions: 1. `create_entity(name: str, health: int, attack: int) -> dict`: - **Input**: - `name` (str): The name of the entity. - `health` (int): The health points of the entity. - `attack` (int): The attack power of the entity. - **Output**: - Returns a dictionary representing the entity with keys `name`, `health`, and `attack`. 2. `battle(player: dict, monsters: list[dict]) -> str`: - **Input**: - `player` (dict): A dictionary representing the player with keys `name`, `health`, and `attack`. - `monsters` (list): A list of dictionaries, each representing a monster with keys `name`, `health`, and `attack`. - **Output**: - Returns a string indicating the result of the battle. Either \\"Victory!\\" if the player defeats all monsters, or \\"Defeat!\\" if the player is defeated. - **Constraints**: - The battle proceeds in rounds, with the player attacking first, followed by the monsters attacking. - If an entity\'s health drops to 0 or below during an attack, they are considered defeated, and they should not conduct an attack in subsequent rounds. - The function should continue until the player is either victorious or defeated. Here is an example of how you could use these functions: ```python # Define the player player = create_entity(\\"Hero\\", 100, 15) # Define monsters monsters = [ create_entity(\\"Goblin\\", 30, 5), create_entity(\\"Orc\\", 60, 10), create_entity(\\"Dragon\\", 150, 20) ] # Simulate the battle result = battle(player, monsters) print(result) # Expected output: \\"Victory!\\" or \\"Defeat!\\" ``` # Implementation Requirements - You must use \\"if\\" statements and \\"for\\" loops effectively. - You should consider edge cases such as when the player or monsters have non-positive health or attack points. - Your solution should be efficient and handle an arbitrary number of monsters. # Evaluation Criteria - Correctness of the function implementations. - Proper use of control flow statements. - Clean and readable code adhering to the Python PEP 8 style guide. - Inclusion of docstrings for functions. Good luck!","solution":"def create_entity(name, health, attack): Creates an entity with the given name, health, and attack attributes. Parameters: name (str): The name of the entity. health (int): The health points of the entity. attack (int): The attack power of the entity. Returns: dict: A dictionary representing the entity. return {\\"name\\": name, \\"health\\": health, \\"attack\\": attack} def battle(player, monsters): Simulates a battle between the player and a list of monsters. Parameters: player (dict): A dictionary representing the player with keys \'name\', \'health\', and \'attack\'. monsters (list): A list of dictionaries, each representing a monster with keys \'name\', \'health\', and \'attack\'. Returns: str: \\"Victory!\\" if the player defeats all monsters, or \\"Defeat!\\" if the player is defeated. while player[\'health\'] > 0 and monsters: if monsters[0][\'health\'] > 0: # Player attacks the first monster in the list monsters[0][\'health\'] -= player[\'attack\'] if monsters[0][\'health\'] <= 0: # Monster is defeated and removed from the list monsters.pop(0) if not monsters: # If all monsters are defeated return \\"Victory!\\" # Monsters attack the player for monster in monsters: if monster[\'health\'] > 0: player[\'health\'] -= monster[\'attack\'] if player[\'health\'] <= 0: return \\"Defeat!\\" return \\"Defeat!\\" if player[\'health\'] <= 0 else \\"Victory!\\""},{"question":"# Advanced Python Exception Handling Task You are required to implement a custom exception handling system for a simulated file processing task. The goal is to demonstrate your understanding of Python\'s built-in exception hierarchy, custom exceptions, and exception chaining. Follow the instructions below to complete the task. 1. Custom Exceptions Define the following custom exceptions: - `FileFormatError`: Raised when an input file has an invalid format. It should inherit from `ValueError`. - `FileSizeError`: Raised when the size of the input file is too large. It should inherit from `OSError`. - `ProcessingError`: A general exception that occurs during the processing of the file. It should inherit from `Exception`. 2. File Processor Class Create a class `FileProcessor` with the following methods: - `__init__(self, filepath)`: Initializes the `filepath` attribute. - `validate_file(self)`: Checks the format and size of the file. It should raise `FileFormatError` if the file format is invalid and `FileSizeError` if the file size exceeds a certain limit (e.g., 10MB). - `process(self)`: Processes the file. It should catch any exceptions raised during validation and re-raise a `ProcessingError`, maintaining the original exception context. 3. Main Function Write a main function that: - Creates an instance of `FileProcessor`. - Calls the `process` method inside a try-except block. - Catches `ProcessingError` and prints the error message along with the original exception details. Input and Output - You are not required to actually read or write any files. Simulate the file validation checks (format and size) with dummy conditions. - Use print statements to simulate the output of the processing steps. Example Usage ```python # Example file checking and processing if __name__ == \\"__main__\\": processor = FileProcessor(\\"example.txt\\") try: processor.process() except ProcessingError as e: print(f\\"Processing failed: {e}\\") ``` # Constraints - The file size limit for this simulation is 10MB. - The file format can be simulated with a check on file extensions (e.g., only \\".txt\\" is valid). - Ensure that the exception chaining properly maintains the context of the original exceptions. Remember to: - Use meaningful variable names. - Write clear, concise comments explaining your logic.","solution":"class FileFormatError(ValueError): Raised when an input file has an invalid format. pass class FileSizeError(OSError): Raised when the size of the input file is too large. pass class ProcessingError(Exception): A general exception that occurs during the processing of the file. pass class FileProcessor: def __init__(self, filepath): self.filepath = filepath def validate_file(self): Simulates file validation. Raises FileFormatError if the file format is invalid. Raises FileSizeError if the file size is too large. # Dummy conditions for simulation if not self.filepath.endswith(\'.txt\'): raise FileFormatError(\\"Invalid file format. Only \'.txt\' files are allowed.\\") # Simulate file size check dummy_file_size = 15 * 1024 * 1024 # 15MB, for simulation only max_allowed_size = 10 * 1024 * 1024 # 10MB if dummy_file_size > max_allowed_size: raise FileSizeError(\\"File size exceeds the 10MB limit.\\") def process(self): Process the file by validating it first. If any errors during validation, re-raise them as ProcessingError. try: self.validate_file() print(\\"File processing succeeded.\\") # Simulated success message except (FileFormatError, FileSizeError) as e: raise ProcessingError(\\"An error occurred during file processing.\\") from e # Example file checking and processing if __name__ == \\"__main__\\": processor = FileProcessor(\\"example.txt\\") try: processor.process() except ProcessingError as e: print(f\\"Processing failed: {e}\\") print(f\\"Original exception: {e.__cause__}\\")"},{"question":"You are required to build a command-line interpreter for managing a simple To-Do list using Python\'s `cmd` framework. The To-Do list should allow users to add tasks, mark tasks as done, list all tasks, and save/load tasks to/from a file. Here are the detailed requirements: Functional Requirements 1. **Command Implementation**: - `do_add [task_description]`: Add a new task to the To-Do list. - Example: `add Buy groceries` - `do_done [task_number]`: Mark the specified task as done. - Example: `done 1` - `do_list`: List all tasks with their status (pending/done). - Example: `list` - `do_save [filename]`: Save the current tasks to a file. - Example: `save tasks.txt` - `do_load [filename]`: Load tasks from a file, replacing current tasks. - Example: `load tasks.txt` - `do_exit`: Exit the command-line interface. - Example: `exit` 2. **Command-line Loop Customization**: - Override `precmd()` to convert all command input to lowercase. - Override `postcmd()` to print a message after every command execution. 3. **Completion and Help**: - Implement tab-completion for the commands. - Provide meaningful docstrings for each command to support the built-in `help` command. Constraints - Do not use any external libraries except for built-in modules in Python. - Ensure your application handles invalid inputs gracefully and provides appropriate error messages. Example Usage ```plaintext Welcome to the To-Do List CLI. Type help or ? to list commands. (todo) add Buy groceries (todo) add Write documentation (todo) list 1. [ ] Buy groceries 2. [ ] Write documentation (todo) done 1 (todo) list 1. [x] Buy groceries 2. [ ] Write documentation (todo) save mytasks.txt (todo) exit ``` Boilerplate Code You may start with the following boilerplate code: ```python import cmd class ToDoCLI(cmd.Cmd): intro = \'Welcome to the To-Do List CLI. Type help or ? to list commands.n\' prompt = \'(todo) \' file = None def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task: ADD [task_description]\' if arg: self.tasks.append({\'description\': arg, \'done\': False}) print(f\'Task added: {arg}\') else: print(\'Error: Task description is required.\') def do_done(self, arg): \'Mark a task as done: DONE [task_number]\' try: task_num = int(arg) if 1 <= task_num <= len(self.tasks): self.tasks[task_num - 1][\'done\'] = True print(f\'Task {task_num} marked as done.\') else: print(\'Error: Invalid task number.\') except ValueError: print(\'Error: Task number must be an integer.\') def do_list(self, arg): \'List all tasks: LIST\' for i, task in enumerate(self.tasks, 1): status = \'x\' if task[\'done\'] else \' \' print(f\'{i}. [{status}] {task[\\"description\\"]}\') def do_save(self, arg): \'Save tasks to a file: SAVE [filename]\' if arg: with open(arg, \'w\') as f: for task in self.tasks: f.write(f\'{task[\\"done\\"]},{task[\\"description\\"]}n\') print(f\'Tasks saved to {arg}\') else: print(\'Error: Filename is required.\') def do_load(self, arg): \'Load tasks from a file: LOAD [filename]\' if arg: try: with open(arg, \'r\') as f: self.tasks = [] for line in f: done, description = line.strip().split(\',\', 1) self.tasks.append({\'description\': description, \'done\': done == \'True\'}) print(f\'Tasks loaded from {arg}\') except FileNotFoundError: print(f\'Error: File {arg} not found.\') else: print(\'Error: Filename is required.\') def do_exit(self, arg): \'Exit the CLI: EXIT\' print(\'Thank you for using the To-Do List CLI.\') return True def precmd(self, line): return line.lower() def postcmd(self, stop, line): print(\'Command executed.\') return stop def complete_add(self, text, line, begidx, endidx): return [task for task in [\'buy groceries\', \'write documentation\', \'do laundry\'] if task.startswith(text)] if __name__ == \'__main__\': ToDoCLI().cmdloop() ``` Develop the above functionality, ensuring to handle edge cases and improve any aspects not covered well by the boilerplate.","solution":"import cmd class ToDoCLI(cmd.Cmd): intro = \'Welcome to the To-Do List CLI. Type help or ? to list commands.n\' prompt = \'(todo) \' file = None def __init__(self): super().__init__() self.tasks = [] def do_add(self, arg): \'Add a new task: ADD [task_description]\' if arg: self.tasks.append({\'description\': arg, \'done\': False}) print(f\'Task added: {arg}\') else: print(\'Error: Task description is required.\') def do_done(self, arg): \'Mark a task as done: DONE [task_number]\' try: task_num = int(arg) if 1 <= task_num <= len(self.tasks): self.tasks[task_num - 1][\'done\'] = True print(f\'Task {task_num} marked as done.\') else: print(\'Error: Invalid task number.\') except ValueError: print(\'Error: Task number must be an integer.\') def do_list(self, arg): \'List all tasks: LIST\' for i, task in enumerate(self.tasks, 1): status = \'x\' if task[\'done\'] else \' \' print(f\'{i}. [{status}] {task[\\"description\\"]}\') def do_save(self, arg): \'Save tasks to a file: SAVE [filename]\' if arg: with open(arg, \'w\') as f: for task in self.tasks: f.write(f\'{task[\\"done\\"]},{task[\\"description\\"]}n\') print(f\'Tasks saved to {arg}\') else: print(\'Error: Filename is required.\') def do_load(self, arg): \'Load tasks from a file: LOAD [filename]\' if arg: try: with open(arg, \'r\') as f: self.tasks = [] for line in f: done, description = line.strip().split(\',\', 1) self.tasks.append({\'description\': description, \'done\': done == \'True\'}) print(f\'Tasks loaded from {arg}\') except FileNotFoundError: print(f\'Error: File {arg} not found.\') else: print(\'Error: Filename is required.\') def do_exit(self, arg): \'Exit the CLI: EXIT\' print(\'Thank you for using the To-Do List CLI.\') return True def precmd(self, line): return line.lower() def postcmd(self, stop, line): print(\'Command executed.\') return stop def complete_add(self, text, line, begidx, endidx): return [task for task in [\'buy groceries\', \'write documentation\', \'do laundry\'] if task.startswith(text)] if __name__ == \'__main__\': ToDoCLI().cmdloop()"},{"question":"**Question: Implement a Phone Number Extractor and Validator** You are tasked with implementing a function `extract_phone_numbers(text: str) -> List[str]` in Python using regular expressions. This function should scan the given text and extract all phone numbers that match specific patterns. # Patterns The function should recognize and extract phone numbers in the following formats: 1. `(123) 456-7890` 2. `123-456-7890` 3. `123.456.7890` 4. `1234567890` 5. `+31636363634` 6. `075-63546725` # Constraints 1. The phone number should be validated based on the following rules: - It may contain parentheses, hyphens, spaces, or dots as separators. - It can start with a plus symbol for international format. - It must contain exactly 10 digits, or a prefixed \'+\' with country code. # Input - A single string `text` which may contain multiple phone numbers mixed with other text. # Output - A list of strings, where each string is a valid phone number extracted from the input text. # Example ```python text = Contact us at (123) 456-7890 or 123-456-7890. Our office number is 123.456.7890, and our service hotline is 1234567890. You can also reach us internationally at +31636363634 or at our local number 075-63546725. expected_output = [ \\"(123) 456-7890\\", \\"123-456-7890\\", \\"123.456.7890\\", \\"1234567890\\", \\"+31636363634\\", \\"075-63546725\\" ] assert extract_phone_numbers(text) == expected_output ``` # Constraints: 1. You should use the `re` module for this task. 2. Ensure that your regex pattern caters to all given formats and constraints. 3. The function should not return any invalid phone numbers. **Hint:** You might want to use the `re.findall` function to extract the phone numbers and ensure to match the exact patterns specified.","solution":"import re from typing import List def extract_phone_numbers(text: str) -> List[str]: Extract and validate phone numbers from a given text. Phone number formats supported: 1. (123) 456-7890 2. 123-456-7890 3. 123.456.7890 4. 1234567890 5. +31636363634 6. 075-63546725 Args: text (str): Input text containing phone numbers. Returns: List[str]: List of valid phone numbers. # Regular expression pattern to match different phone number formats phone_number_pattern = re.compile(r\'\'\' ((d{3})sd{3}-d{4}) | # (123) 456-7890 (d{3}-d{3}-d{4}) | # 123-456-7890 (d{3}.d{3}.d{4}) | # 123.456.7890 (d{10}) | # 1234567890 (+d{11,15}) | # +31636363634 (international format with \'+\' prefix) (d{3}-d{8}) # 075-63546725 \'\'\', re.VERBOSE) # Finding all matches in the input text matches = phone_number_pattern.findall(text) # Flatten the list (ignore empty patterns in group) and filter None values phone_numbers = [match for group in matches for match in group if match] return phone_numbers"},{"question":"# Seaborn Coding Assessment Objective Demonstrate your understanding and proficiency in using the seaborn library for data visualization by implementing and customizing a plot based on given specifications. Problem Statement Implement a Python function using seaborn that: 1. Loads the `healthexp` dataset. 2. Filters the dataset to include only entries from a specific range of years. 3. Creates a scatter plot of `Spending_USD` vs. `Life_Expectancy`, with data points color-coded by `Country`. 4. Overlays a path on the plot that shows the trajectory of each country\'s health expenditure and life expectancy over time. 5. Customizes the plot with specific aesthetics including markers, point sizes, line widths, and fill colors. Function Signature ```python def visualize_health_expenditure(year_start: int, year_end: int) -> None: pass ``` Input - `year_start` (int): The starting year for filtering the `healthexp` dataset. - `year_end` (int): The ending year for filtering the `healthexp` dataset. Expected Output A plot displayed inline (for example, within a Jupyter Notebook) that satisfies the described specifications. Constraints - Ensure that `year_start` is less than `year_end`. - Use the seaborn `Path` mark for plotting trajectories. - Customize the plot as follows: - Set markers using `o`. - Use a point size of 2. - Set the line width to 0.75. - Use white fill color for the markers. Example Usage ```python # Visualize health expenditure and life expectancy between 2000 and 2010 visualize_health_expenditure(2000, 2010) ``` The function should load the `healthexp` dataset, filter it for the years between 2000 and 2010, and create a scatter plot with overlaid paths showing the trajectory of health expenditure and life expectancy for each country.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_healthexp_dataset(): Loads the healthexp dataset from seaborn. healthexp = sns.load_dataset(\'healthexp\') return healthexp def visualize_health_expenditure(year_start: int, year_end: int) -> None: Visualizes health expenditure and life expectancy from healthexp dataset within given years. Parameters: - year_start (int): the starting year for filtering the healthexp dataset. - year_end (int): the ending year for filtering the healthexp dataset. # Load healthexp dataset healthexp = load_healthexp_dataset() # Filter dataset for the specified range of years filtered_data = healthexp[(healthexp[\'Year\'] >= year_start) & (healthexp[\'Year\'] <= year_end)] # Create the scatter plot plt.figure(figsize=(12, 8)) sns.scatterplot( data=filtered_data, x=\'Spending_USD\', y=\'Life_Expectancy\', hue=\'Country\', style=\'Country\', markers=\'o\', legend=False, s=100, edgecolor=\'w\', linewidth=0.75 ) # Overlay paths showing the trajectory of each country\'s health expenditure and life expectancy for country in filtered_data[\'Country\'].unique(): country_data = filtered_data[filtered_data[\'Country\'] == country] plt.plot( country_data[\'Spending_USD\'], country_data[\'Life_Expectancy\'], \'o-\', markersize=2, linewidth=0.75, color=sns.color_palette()[0], # consistent coloring markerfacecolor=\'none\' ) # Customize plot aesthetics plt.title(\'Health Expenditure and Life Expectancy Over Time\') plt.xlabel(\'Health Expenditure (USD)\') plt.ylabel(\'Life Expectancy (Years)\') plt.legend(title=\'Country\', loc=\'upper left\', bbox_to_anchor=(1, 1)) plt.grid(True) plt.tight_layout() plt.show()"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of Python\'s data persistence capabilities using the `pickle` and `sqlite3` modules. # Problem Statement You are tasked with creating a Python application that can serialize a complex data structure to a file, store some components of this data structure in an SQLite database, and reconstruct both the file and database components back to their original form. # Task Description 1. **Serialization with `pickle`:** - You will serialize a dictionary containing mixed data types into a file using `pickle`. - Your dictionary should have at least: - An integer - A string - A list of floats - A nested dictionary with: - Another integer - A list of strings 2. **Storing Data in SQLite:** - Store the integer, string, list of floats, and nested dictionary as separate records in an SQLite database. - Use appropriate SQLite data types for each component. - Write functions to insert and retrieve these individual items. 3. **Reconstruction:** - Read the `pickle` file and verify its contents to ensure the data was serialized correctly. - Retrieve the data from the SQLite database and reconstruct the original dictionary. # Implementation Details - **Function 1:** `serialize_to_file(data: dict, file_path: str) -> None` - **Input:** - `data`: A dictionary with the specified structure. - `file_path`: A string indicating the path of the file where the data will be stored. - **Output:** None. (This function should serialize the dictionary to the specified file.) - **Function 2:** `store_in_database(data: dict, db_path: str) -> None` - **Input:** - `data`: A dictionary with the specified structure. - `db_path`: A string indicating the path of the SQLite database. - **Output:** None. (This function should store the dictionary components into the database.) - **Function 3:** `read_from_file(file_path: str) -> dict` - **Input:** - `file_path`: A string indicating the path of the file from which the data is to be read. - **Output:** - A dictionary with the stored data. - **Function 4:** `retrieve_from_database(db_path: str) -> dict` - **Input:** - `db_path`: A string indicating the path of the SQLite database. - **Output:** - A dictionary reconstructed from the database records. # Constraints - Your implementation should handle exceptions and edge cases, such as file not found or database connection errors. - Ensure that the reconstructed dictionary from the database matches the original dictionary structure. # Example ```python data = { \'integer\': 42, \'string\': \'hello world\', \'floats\': [1.1, 2.2, 3.3], \'nested\': { \'another_integer\': 100, \'str_list\': [\'one\', \'two\', \'three\'] } } file_path = \'data.pkl\' db_path = \'data.db\' # Serialize and store serialize_to_file(data, file_path) store_in_database(data, db_path) # Read and retrieve file_data = read_from_file(file_path) db_data = retrieve_from_database(db_path) # Verify both methods reconstruct the original data accurately assert file_data == data assert db_data == data ``` # Note: Ensure that your functions are efficiently handling the serialization and database storage/retrieval to meet performance requirements even with larger data sets.","solution":"import pickle import sqlite3 def serialize_to_file(data, file_path): Serialize a dictionary to a file using pickle. :param data: Dictionary to serialize :param file_path: File path where the data will be stored with open(file_path, \'wb\') as file: pickle.dump(data, file) def store_in_database(data, db_path): Store the components of the dictionary in an SQLite database. :param data: Dictionary to store :param db_path: Database file path conn = sqlite3.connect(db_path) cursor = conn.cursor() # Create tables cursor.execute(\\"CREATE TABLE IF NOT EXISTS main_data (integer INTEGER, string TEXT)\\") cursor.execute(\\"CREATE TABLE IF NOT EXISTS floats (value REAL)\\") cursor.execute(\\"CREATE TABLE IF NOT EXISTS nested (another_integer INTEGER)\\") cursor.execute(\\"CREATE TABLE IF NOT EXISTS str_list (value TEXT)\\") # Insert data cursor.execute(\\"INSERT INTO main_data (integer, string) VALUES (?, ?)\\", (data[\'integer\'], data[\'string\'])) cursor.executemany(\\"INSERT INTO floats (value) VALUES (?)\\", [(f,) for f in data[\'floats\']]) cursor.execute(\\"INSERT INTO nested (another_integer) VALUES (?)\\", (data[\'nested\'][\'another_integer\'],)) cursor.executemany(\\"INSERT INTO str_list (value) VALUES (?)\\", [(s,) for s in data[\'nested\'][\'str_list\']]) # Commit changes and close the connection conn.commit() conn.close() def read_from_file(file_path): Read and deserialize a dictionary from a file. :param file_path: File path from which to read the data :return: Deserialized dictionary with open(file_path, \'rb\') as file: return pickle.load(file) def retrieve_from_database(db_path): Retrieve the components of the dictionary from an SQLite database and reconstruct it. :param db_path: Database file path :return: Reconstructed dictionary conn = sqlite3.connect(db_path) cursor = conn.cursor() # Retrieve data cursor.execute(\\"SELECT * FROM main_data\\") main_data = cursor.fetchone() cursor.execute(\\"SELECT value FROM floats\\") floats = [row[0] for row in cursor.fetchall()] cursor.execute(\\"SELECT another_integer FROM nested\\") nested_data = cursor.fetchone() cursor.execute(\\"SELECT value FROM str_list\\") str_list = [row[0] for row in cursor.fetchall()] # Reconstruct the dictionary data = { \'integer\': main_data[0], \'string\': main_data[1], \'floats\': floats, \'nested\': { \'another_integer\': nested_data[0], \'str_list\': str_list } } conn.close() return data"},{"question":"# Custom Pandas Extension Question Objective: Create a custom extension data type for representing complex numbers and perform various operations using the `ExtensionArray` interface of pandas. Description: You need to create a custom extension array for handling complex numbers in pandas. This will involve defining an `ExtensionDtype` for complex numbers and implementing an `ExtensionArray` that provides methods for handling an array of complex numbers. Requirements: 1. **ComplexDtype:** - Define a new data type `ComplexDtype` inheriting from `pandas.api.extensions.ExtensionDtype`. - Set the name to `\'complex\'`. 2. **ComplexArray:** - Implement a new array class `ComplexArray` inheriting from `pandas.api.extensions.ExtensionArray`. - This class must handle an array of Python\'s `complex` numbers. - Implement the following methods: - `__init__`: Initialize the array with a list of complex numbers. - `dtype`: Return the `ComplexDtype`. - `take`: Select and return elements at provided positions. - `isna`: Check if elements are NaN (Optional: complex numbers can be checked against a predefined \\"invalid\\" complex number, say `np.nan + 1j`). - `fillna`: Fill NaN values with a specified value. - `unique`: Return unique complex numbers in the array. - `copy`: Return a copy of the array. - Any other methods needed to satisfy the `ExtensionArray` interface. Input: - You do not need to handle input; your classes should work with provided pandas structures and integrate seamlessly. Constraints: - Ensure all operations maintain the complex number nature of the data. - Handle edge cases such as empty arrays and all-NaN arrays. - Avoid using third-party libraries apart from pandas and numpy (if needed). Expected Output: The complex extension should work seamlessly with pandas structures. For example: ```python import pandas as pd data = ComplexArray([1+2j, 3+4j, None, 5+6j]) ser = pd.Series(data) df = pd.DataFrame({\'complex\': data}) print(ser) print(df) print(data.unique()) print(data.fillna(0 + 0j)) ``` Notes: - Ensure to test basic functionality with pandas operations like `pd.Series`, `pd.DataFrame`, merging, and group operations. - Provide documentation and comments in the code for clarity.","solution":"import numpy as np import pandas as pd from pandas.api.extensions import ExtensionDtype, ExtensionArray class ComplexDtype(ExtensionDtype): name = \'complex\' type = complex kind = \'O\' @classmethod def construct_array_type(cls): return ComplexArray class ComplexArray(ExtensionArray): def __init__(self, values): self._data = np.array(values, dtype=object) @property def dtype(self): return ComplexDtype() def __getitem__(self, item): if isinstance(item, int): return self._data[item] return ComplexArray(self._data[item]) def __len__(self): return len(self._data) def isna(self): return np.array([x is None for x in self._data]) def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = fill_value if fill_value is not None else np.nan + 1j result = np.full(len(indices), fill_value, dtype=object) for i, idx in enumerate(indices): if idx == -1: result[i] = fill_value else: result[i] = self._data[idx] else: result = self._data[indices] return ComplexArray(result) def fillna(self, value): filled = np.where(self.isna(), value, self._data) return ComplexArray(filled) def unique(self): unique_vals = pd.unique(self._data) return ComplexArray(unique_vals) def copy(self, deep=False): return ComplexArray(self._data.copy()) def __eq__(self, other): if isinstance(other, ComplexArray): return np.array_equal(self._data, other._data) return False def __repr__(self): return f\\"ComplexArray({list(self._data)})\\" # Register the dtype with pandas pd.api.extensions.register_extension_dtype(ComplexDtype)"},{"question":"You are required to construct a basic event reminder system using the `sched` module in Python. The system should allow scheduling events with a title and description, listing all scheduled events, and running the scheduler to display reminders at the appropriate times. # Requirements: 1. Implement a class `EventReminderSystem` with the following methods: - `__init__(self)`: Initializes the scheduler and an empty list to store event details (title, description, and time). - `schedule_event(self, event_time, title, description)`: Schedules a new event at the specified `event_time` (absolute time in Unix timestamp format), with the given `title` and `description`. - `cancel_event(self, event_title)`: Cancels an event with the given title if it exists in the scheduler. - `list_events(self)`: Returns a list of all scheduled events with their details (title, description, and time). - `run_scheduler(self)`: Runs the scheduler to display all events as per their schedule. # Constraints: - The `event_time` parameter should be a valid future Unix timestamp. - Ensure that no two events can have the same title. # Input/Output Examples: ```python # Example Usage reminder_system = EventReminderSystem() # Schedule events reminder_system.schedule_event(1652342835, \\"Meeting\\", \\"Discuss project updates\\") reminder_system.schedule_event(1652342895, \\"Call\\", \\"Call the client for feedback\\") # List events print(reminder_system.list_events()) # Output: [(\'Meeting\', \'Discuss project updates\', 1652342835), (\'Call\', \'Call the client for feedback\', 1652342895)] # Cancel an event reminder_system.cancel_event(\\"Meeting\\") print(reminder_system.list_events()) # Output: [(\'Call\', \'Call the client for feedback\', 1652342895)] # Run scheduler to display events reminder_system.run_scheduler() # Output (assuming current time is before the scheduled times): # Reminding: Call - Call the client for feedback ``` # Additional Notes: - Use the `time.time()` function to get the current time in Unix timestamp format. - Use `time.sleep()` as the `delayfunc` for the scheduler. - Ensure appropriate exception handling for invalid operations (e.g., canceling a non-existent event).","solution":"import sched import time class EventReminderSystem: def __init__(self): self.scheduler = sched.scheduler(time.time, time.sleep) self.events = {} def schedule_event(self, event_time, title, description): if title in self.events: raise ValueError(\\"An event with this title already exists.\\") if event_time <= time.time(): raise ValueError(\\"Event time must be a future time.\\") event = self.scheduler.enterabs(event_time, 1, self.remind, argument=(title, description)) self.events[title] = (event, description, event_time) def cancel_event(self, event_title): if event_title not in self.events: raise ValueError(\\"No event found with this title.\\") event, description, event_time = self.events.pop(event_title) self.scheduler.cancel(event) def list_events(self): return [(title, description, event_time) for title, (event, description, event_time) in self.events.items()] def remind(self, title, description): print(f\\"Reminding: {title} - {description}\\") self.events.pop(title, None) def run_scheduler(self): self.scheduler.run()"},{"question":"**Objective:** Demonstrate your understanding of constructing and customizing plots using seaborn. **Problem Statement:** You are provided with a dataset called `tips` which contains information about the bills in a restaurant. Your task is to create a customized visualization using seaborn to analyze the relationship between total bill amount and tip amount, categorized by the day of the week and the gender of the person paying the bill. **Dataset:** The `tips` dataset includes the following columns: - `total_bill`: Total bill amount. - `tip`: Tip amount. - `sex`: Gender of the person paying. - `day`: Day of the week. - `time`: Time of the day (Dinner/Lunch). - `size`: Number of people at the table. **Tasks:** 1. **Load the dataset**: Load the `tips` dataset using the `seaborn.load_dataset` function. 2. **Create a scatter plot**: - Plot the `total_bill` on the x-axis and `tip` on the y-axis. - Color the points based on the `sex`. - Facet the plot by `day` of the week, arranging them in a grid with 2 rows. 3. **Add customization**: - Add a linear regression line to each scatter plot to show the trend. - Customize the plot using the seaborn theme to set the background color to `\'lightgray\'` and the grid style to `\'whitegrid\'`. - Set the line width to 2 for the regression lines. 4. **Generate the plot**: Generate and display the plot with all the given customizations. **Expected function signature:** ```python import seaborn as sns import seaborn.objects as so def create_custom_seaborn_plot(): tips = sns.load_dataset(\\"tips\\") p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Line(), so.PolyFit(order=1)) .add(so.Dot()) ) p.theme({\\"axes.facecolor\\": \\"lightgray\\", \\"grid\\": {\'color\': \'whitegrid\'}, \\"lines.linewidth\\": 2}) p.show() ``` **Constraints:** - You must use the seaborn library and the `seaborn.objects` module for plotting and customization. - Ensure your plot is well-labeled and visually appealing. **Notes:** - The function `create_custom_seaborn_plot` should not take any arguments. - The function should load the dataset, create, and display the customized plot as specified.","solution":"import seaborn as sns import matplotlib.pyplot as plt import seaborn.objects as so def create_custom_seaborn_plot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create the plot p = ( so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\") .facet(\\"day\\", wrap=2) .add(so.Dots()) .add(so.Line(), so.PolyFit(order=1)) ) # Customize the plot p.theme({\\"axes.facecolor\\": \\"lightgray\\", \\"grid.color\\": \\"white\\", \\"lines.linewidth\\": 2}) # Display the plot p.show()"},{"question":"# Question: Package Metadata Analyzer You are tasked with implementing a function that utilizes the `importlib.metadata` library to analyze installed packages. This function needs to perform the following tasks: 1. **Retrieve metadata**: Accept a package name as input and retrieve its metadata. 2. **List distribution files**: For the given package, list all the files within the distribution. 3. **Check dependencies**: Identify and print all the dependencies required by the package. 4. **Entry point extraction**: Print all entry points defined for the package, categorized by their groups. Function Specification ```python def analyze_package_metadata(package_name: str) -> dict: Analyze metadata for a given package. Args: package_name (str): The name of the package to analyze. Returns: dict: A dictionary containing metadata, list of files, dependencies, and entry points. Example: { \\"metadata\\": {...}, \\"files\\": [...], \\"dependencies\\": [...], \\"entry_points\\": { \\"group1\\": [...], \\"group2\\": [...] } } pass ``` Input - `package_name` (str): The name of the package for which to retrieve metadata. Output - Return a dictionary containing: - `\\"metadata\\"`: A dictionary with the package metadata. - `\\"files\\"`: A list of file paths included in the package. - `\\"dependencies\\"`: A list of dependencies required by the package. - `\\"entry_points\\"`: A dictionary with entry points categorized by their groups. Example If the `analyze_package_metadata(\\"wheel\\")` function were called, it might return something like: ```python { \\"metadata\\": { \\"Metadata-Version\\": \\"2.1\\", \\"Name\\": \\"wheel\\", \\"Version\\": \\"0.32.3\\", \\"Summary\\": \\"...\\", ... }, \\"files\\": [ \\"wheel/__init__.py\\", \\"wheel/metadata.json\\", ... ], \\"dependencies\\": [ \\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\" ], \\"entry_points\\": { \\"console_scripts\\": [\\"wheel = wheel.cli:main\\"] } } ``` Constraints 1. You may assume the package is installed in the system. 2. Follow the Python function naming conventions and document your code for readability. Advanced Requirements (Optional) - Optimize the function for performance. - Handle exceptions gracefully and return informative error messages. - Support querying for multiple packages and return a combined analysis. # Notes - Refer to the `importlib.metadata` documentation provided above. - Use functions like `metadata()`, `files()`, `requires()`, and `entry_points()` from `importlib.metadata` to implement the solution.","solution":"import importlib.metadata def analyze_package_metadata(package_name: str) -> dict: Analyze metadata for a given package. Args: package_name (str): The name of the package to analyze. Returns: dict: A dictionary containing metadata, list of files, dependencies, and entry points. Example: { \\"metadata\\": {...}, \\"files\\": [...], \\"dependencies\\": [...], \\"entry_points\\": { \\"group1\\": [...], \\"group2\\": [...] } } result = {} try: pkg_metadata = importlib.metadata.metadata(package_name) result[\'metadata\'] = dict(pkg_metadata) except importlib.metadata.PackageNotFoundError: return {\'error\': f\\"Package \'{package_name}\' not found.\\"} try: pkg_files = importlib.metadata.files(package_name) result[\'files\'] = [str(file) for file in pkg_files] if pkg_files else [] except (AttributeError, importlib.metadata.PackageNotFoundError): result[\'files\'] = [] try: pkg_dependencies = importlib.metadata.requires(package_name) result[\'dependencies\'] = pkg_dependencies if pkg_dependencies else [] except importlib.metadata.PackageNotFoundError: result[\'dependencies\'] = [] try: pkg_entry_points = importlib.metadata.entry_points().get(package_name) if pkg_entry_points: entry_points_dict = {} for entry_point in pkg_entry_points: group = entry_point.group entry = str(entry_point) if group not in entry_points_dict: entry_points_dict[group] = [] entry_points_dict[group].append(entry) result[\'entry_points\'] = entry_points_dict else: result[\'entry_points\'] = {} except importlib.metadata.PackageNotFoundError: result[\'entry_points\'] = {} return result"},{"question":"You are tasked with creating a multi-threaded Python program using the `_thread` module. Your program will simulate a banking system where multiple threads represent different bank tellers handling transactions simultaneously. Each teller will process a list of transactions that include deposits and withdrawals. # Requirements 1. Create a `BankAccount` class that has the following methods: - `__init__(self, balance: int)`: Initializes the bank account with a given balance. - `deposit(self, amount: int)`: Deposits the specified amount into the bank account. - `withdraw(self, amount: int)`: Withdraws the specified amount from the bank account if sufficient balance is available. Otherwise, it should print an error message. - `get_balance(self) -> int`: Returns the current balance of the bank account. 2. Implement a function `process_transactions(account: BankAccount, transactions: list)` that processes a list of transactions on the provided `BankAccount` object. Each transaction is a tuple where the first element is either \'deposit\' or \'withdraw\', and the second element is the amount. 3. Create a function `simulate_bank_tellers(account: BankAccount, transactions_list: list)` that uses multiple threads to simulate different bank tellers processing their transactions concurrently. The `transactions_list` parameter is a list of transaction lists, where each sub-list represents the transactions for a specific teller. # Implementation Details - Use `_thread.start_new_thread` to create new threads. - Use `_thread.allocate_lock` to ensure thread-safe operations on the `BankAccount` object. - Ensure that the balance is never negative by implementing proper checks in the `withdraw` method. - Each thread should print its identifier and the final balance after processing its transactions. # Example ```python # Example usage if __name__ == \\"__main__\\": initial_balance = 1000 account = BankAccount(initial_balance) transactions_teller1 = [(\'deposit\', 100), (\'withdraw\', 50), (\'withdraw\', 200)] transactions_teller2 = [(\'withdraw\', 300), (\'deposit\', 400), (\'withdraw\', 700)] transactions_list = [transactions_teller1, transactions_teller2] simulate_bank_tellers(account, transactions_list) print(f\\"Final balance: {account.get_balance()}\\") ``` Sample Output ``` Thread 123 (teller 1): Final balance after transactions: 850 Thread 456 (teller 2): Final balance after transactions: 550 Final balance: 550 ``` # Constraints - Ensure proper synchronization using locks. - Handle exceptions that may occur during thread operations gracefully. - Each thread should process its transactions independently and correctly update the shared `BankAccount` object.","solution":"import _thread import time class BankAccount: def __init__(self, balance: int): self.balance = balance self.lock = _thread.allocate_lock() def deposit(self, amount: int): with self.lock: self.balance += amount def withdraw(self, amount: int): with self.lock: if self.balance >= amount: self.balance -= amount else: print(f\\"Insufficient balance to withdraw {amount}. Current balance: {self.balance}\\") def get_balance(self) -> int: with self.lock: return self.balance def process_transactions(account: BankAccount, transactions: list, thread_id: int): for transaction in transactions: trans_type, amount = transaction if trans_type == \'deposit\': account.deposit(amount) elif trans_type == \'withdraw\': account.withdraw(amount) print(f\\"Thread {thread_id}: Final balance after transactions: {account.get_balance()}\\") def simulate_bank_tellers(account: BankAccount, transactions_list: list): for i, transactions in enumerate(transactions_list): _thread.start_new_thread(process_transactions, (account, transactions, i+1)) # Give threads time to complete time.sleep(1)"},{"question":"# URL Processing and Data Retrieval with Error Handling **Objective:** Write a function named `fetch_html_content` that performs the following tasks: 1. Takes a URL string as input. 2. Attempts to retrieve the HTML content from the given URL. 3. Handles various HTTP errors gracefully. 4. Sends a specific User-Agent header with each request. 5. Optionally supports basic HTTP authentication if credentials are provided. **Function Signature:** ```python def fetch_html_content(url: str, user: str = None, password: str = None) -> str: pass ``` **Detailed Requirements:** 1. **Input Parameters:** - `url`: A string representing the URL from which HTML content is to be fetched. - `user`: An optional string representing the username for HTTP basic authentication. - `password`: An optional string representing the password for HTTP basic authentication. 2. **Output:** - Returns a string containing the HTML content of the given URL. - In case of an HTTP error (like 404 Not Found or 503 Service Unavailable), the function should return a descriptive error message indicating the HTTP status code and the reason. 3. **Constraints:** - The function must use the `urllib.request` module to perform the HTTP request. - It should add a User-Agent header to each request, identifying the request as coming from `fetch_html_content/1.0`. - If `user` and `password` are provided, the function should handle HTTP Basic Authentication using these credentials. **Performance Note:** The function should handle network-related issues gracefully and should ensure that the connection is properly closed after the operation. **Example Usage:** ```python url = \\"http://www.example.com\\" content = fetch_html_content(url) print(content) url_with_auth = \\"http://www.example.com/protected\\" content_with_auth = fetch_html_content(url_with_auth, user=\\"username\\", password=\\"password\\") print(content_with_auth) ``` **Example Output:** ```plaintext (For URL without authentication) <!DOCTYPE html> <html> <head> <title>Example Domain</title> ... </html> (For URL with authentication) <!DOCTYPE html> <html> <head> <title>Protected Page</title> ... </html> (In case of errors) HTTP Error 404: Not Found HTTP Error 503: Service Unavailable ``` **Hints:** - Use `urllib.request.urlopen` and `urllib.request.Request` to handle the URL request. - Set the User-Agent header using the `headers` parameter of the `Request` object. - Use the `HTTPBasicAuthHandler` class for handling authentication if credentials are provided. - Ensure that the function handles exceptions and returns appropriate error messages for HTTP errors.","solution":"import urllib.request import urllib.error def fetch_html_content(url: str, user: str = None, password: str = None) -> str: headers = {\'User-Agent\': \'fetch_html_content/1.0\'} request = urllib.request.Request(url, headers=headers) if user and password: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, user, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) urllib.request.install_opener(opener) try: with urllib.request.urlopen(request) as response: return response.read().decode(\'utf-8\') except urllib.error.HTTPError as e: return f\\"HTTP Error {e.code}: {e.reason}\\" except urllib.error.URLError as e: return f\\"URL Error: {e.reason}\\""},{"question":"You are provided with the `penguins` dataset. Your task is to perform an exploratory data analysis (EDA) on a specific column, \\"flipper_length_mm\\". You need to create insightful visualizations using Seaborn\'s `so.Plot`. Requirements: 1. **Load the Data:** - Load the `penguins` dataset from Seaborn. 2. **Basic Histogram:** - Create a univariate histogram of the \\"flipper_length_mm\\" with bars. 3. **Adjusted Bins Histogram:** - Create a histogram of \\"flipper_length_mm\\" with `20` bins and another with a `binwidth` of `5`. 4. **Proportion Normalized Histogram:** - Generate a proportion-normalized histogram of \\"flipper_length_mm\\". 5. **Facet by Island:** - Create a faceted histogram plot of \\"flipper_length_mm\\" for each \\"island\\". Ensure each facet is normalized independently. 6. **Compare by Sex:** - Produce a histogram of \\"flipper_length_mm\\" colored by \\"sex\\" and use: - `Area` mark to discern overlapped distributions. - `Stack` to represent a part-whole relationship. Input: None. (You will load the dataset directly.) Output: Visualizations as specified in the requirements above. Constraints: - Use only the seaborn.objects module for plotting. - Ensure your histograms convey clear and meaningful insights. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load dataset penguins = load_dataset(\\"penguins\\") # Basic Histogram plot1 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist()) plot1.show() # Adjusted Bins Histogram plot2 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(bins=20)) plot2.show() plot3 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(binwidth=5)) plot3.show() # Proportion Normalized Histogram plot4 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(stat=\\"proportion\\")) plot4.show() # Facet by Island plot5 = so.Plot(penguins, \\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(stat=\\"proportion\\", common_norm=False)) plot5.show() # Compare by Sex using Area plot6 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Area(), so.Hist(), color=\\"sex\\") plot6.show() # Compare by Sex using Stack plot7 = so.Plot(penguins, \\"flipper_length_mm\\").add(so.Bars(), so.Hist(), so.Stack(), color=\\"sex\\") plot7.show() ``` Implement the required visualizations in a single script. Ensure each plot is clearly labeled and provides insightful information about the \\"flipper_length_mm\\" variable.","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Basic Histogram plot1 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(), so.Hist()) plot1.show() plt.title(\'Basic Histogram\') plt.show() # 2. Adjusted Bins Histogram # Histogram with 20 bins plot2 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(), so.Hist(bins=20)) plot2.show() plt.title(\'Histogram with 20 bins\') plt.show() # Histogram with binwidth of 5 plot3 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(), so.Hist(binwidth=5)) plot3.show() plt.title(\'Histogram with Binwidth 5\') plt.show() # 3. Proportion Normalized Histogram plot4 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(), so.Hist(stat=\\"density\\")) plot4.show() plt.title(\'Proportion Normalized Histogram\') plt.show() # 4. Facet by Island plot5 = so.Plot(penguins, x=\\"flipper_length_mm\\").facet(\\"island\\").add(so.Bars(), so.Hist(stat=\\"density\\", common_norm=False)) plot5.show() plt.title(\'Facet by Island\') plt.show() # 5. Compare by Sex using Area plot6 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Area(), so.Hist(), color=\\"sex\\") plot6.show() plt.title(\'Compare by Sex using Area\') plt.show() # Compare by Sex using Stack plot7 = so.Plot(penguins, x=\\"flipper_length_mm\\").add(so.Bars(), so.Hist(), so.Stack(), color=\\"sex\\") plot7.show() plt.title(\'Compare by Sex using Stack\') plt.show()"},{"question":"Coding Assessment Question # Task Implement a Python function `configure_pytorch_logging` that configures the logging settings of different PyTorch components and artifacts using the `torch._logging.set_logs` function. # Function Signature ```python def configure_pytorch_logging(settings: dict) -> None: pass ``` # Input - `settings` (dict): A dictionary where: - Keys are strings representing component names or artifacts. - Values are integers for log levels (e.g., `logging.DEBUG`, `logging.INFO`) or booleans (`True`/`False`) for enabling/disabling artifacts. # Output - The function should not return any value. It should configure the logging settings based on the provided dictionary. # Requirements 1. Use the `torch._logging.set_logs` function to configure each component and artifact. 2. Raise a `ValueError` if an unknown component or artifact is provided. 3. Integrate basic validation to ensure that the settings values are appropriate (integers for components, booleans for artifacts). # Constraints - Only the components and artifacts specified in the documentation should be accepted. - The function should be extensible, making it easy to add more components or artifacts in the future. # Example ```python import logging settings = { \\"dynamo\\": logging.DEBUG, \\"aot\\": logging.INFO, \\"aot_graphs\\": True, \\"ddp_graphs\\": False } configure_pytorch_logging(settings) ``` In this example, the logging level for the TorchDynamo component is set to `DEBUG`, the AOT component is set to `INFO`, the `aot_graphs` artifact is enabled, and the `ddp_graphs` artifact is disabled. # Notes - You may refer to the [logging module](https://docs.python.org/3/library/logging.html) for the logging levels. - Ensure to import the necessary modules and dependencies.","solution":"import logging import torch._logging as torch_logging def configure_pytorch_logging(settings: dict) -> None: Configures the logging settings for PyTorch components and artifacts. Parameters: settings (dict): A dictionary where keys are strings representing component names or artifacts, and values are either integers (for log levels) or booleans (for enabling/disabling artifacts). Raises: ValueError: If an unknown component or artifact is provided. # Define valid components and their log levels valid_components = { \\"dynamo\\": int, \\"aot\\": int } # Define valid artifacts and their boolean status valid_artifacts = { \\"aot_graphs\\": bool, \\"ddp_graphs\\": bool } for key, value in settings.items(): if key in valid_components: # Check if value provided for component is an integer (log level) if not isinstance(value, valid_components[key]): raise ValueError(f\\"Invalid log level for component {key}: {value}\\") torch_logging.set_logs(key, value) elif key in valid_artifacts: # Check if value provided for artifact is a boolean if not isinstance(value, valid_artifacts[key]): raise ValueError(f\\"Invalid boolean value for artifact {key}: {value}\\") torch_logging.set_logs(key, value) else: raise ValueError(f\\"Unknown component or artifact {key}\\")"},{"question":"Objective: To assess your understanding and practical application of the `SGDClassifier` and `SGDRegressor` classes from the scikit-learn library. Question: You are provided with two datasets: one for classification and one for regression. Your task is to implement and evaluate models using `SGDClassifier` and `SGDRegressor` respectively. **Datasets:** 1. `classification_data.csv`: Contains labeled data for a binary classification problem. - Features: `feature_1, feature_2, ..., feature_n` - Target: `label` (0 or 1) 2. `regression_data.csv`: Contains data for a regression problem. - Features: `feature_1, feature_2, ..., feature_m` - Target: `target` Tasks: 1. **Data Preprocessing**: - Load the datasets. - Split each dataset into training and testing sets (80-20 split). - Standardize the feature values. 2. **Classification with SGDClassifier**: - Implement a `SGDClassifier` with `loss=\\"hinge\\"`, `penalty=\\"l2\\"`, `max_iter=1000`, and `tol=1e-3`. - Train the classifier on the training set. - Evaluate the classifier on the testing set using accuracy and print the classification report. 3. **Regression with SGDRegressor**: - Implement a `SGDRegressor` with `loss=\\"squared_error\\"`, `penalty=\\"l2\\"`, `max_iter=1000`, and `tol=1e-3`. - Train the regressor on the training set. - Evaluate the regressor on the testing set using Mean Squared Error (MSE) and R^2 score. 4. **Hyperparameter Tuning**: - Use GridSearchCV to find the optimal hyperparameters for both the classifier and regressor. - Report the best parameters and the corresponding evaluation metrics (accuracy for classification and MSE, R^2 for regression). Expected Input and Output: - The input will be two CSV files: `classification_data.csv` and `regression_data.csv`. - The output should be the classification report for the `SGDClassifier` and the evaluation metrics (`MSE` and `R^2`) for the `SGDRegressor`. - Additionally, report the best hyperparameters found through GridSearchCV for both models. Constraints: - You must use `SGDClassifier` for the classification task and `SGDRegressor` for the regression task. - Implement standardization using `StandardScaler` and ensure it is applied consistently to both training and testing data. - Use `GridSearchCV` for hyperparameter tuning with a reasonable range of values. Performance Requirements: - Ensure that the evaluation metrics (accuracy, MSE, R^2) are calculated correctly. - The code should handle any potential edge cases, such as missing or invalid values in the datasets gracefully. Example Code: ```python import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.metrics import classification_report, mean_squared_error, r2_score # Load and preprocess datasets # [... Your code here ...] # Classification with SGDClassifier # [... Your code here ...] # Regression with SGDRegressor # [... Your code here ...] # Hyperparameter Tuning # [... Your code here ...] ``` Please ensure your solution is well-documented and readable. Include comments where necessary to explain the steps and logic used.","solution":"import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.metrics import classification_report, mean_squared_error, r2_score # Load and preprocess datasets def load_and_preprocess_classification_data(file_path): data = pd.read_csv(file_path) X = data.drop(columns=\'label\') y = data[\'label\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def load_and_preprocess_regression_data(file_path): data = pd.read_csv(file_path) X = data.drop(columns=\'target\') y = data[\'target\'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test # Classification with SGDClassifier def train_and_evaluate_sgd_classifier(X_train, X_test, y_train, y_test): clf = SGDClassifier(loss=\\"hinge\\", penalty=\\"l2\\", max_iter=1000, tol=1e-3) clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(\\"Classification Report:n\\", classification_report(y_test, y_pred)) return clf # Regression with SGDRegressor def train_and_evaluate_sgd_regressor(X_train, X_test, y_train, y_test): reg = SGDRegressor(loss=\\"squared_error\\", penalty=\\"l2\\", max_iter=1000, tol=1e-3) reg.fit(X_train, y_train) y_pred = reg.predict(X_test) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) print(f\\"Mean Squared Error: {mse}\\") print(f\\"R^2 Score: {r2}\\") return reg def perform_grid_search_classifier(X_train, y_train): param_grid = { \'loss\': [\'hinge\', \'log\'], \'penalty\': [\'l2\', \'l1\'], \'alpha\': [0.0001, 0.001, 0.01], \'max_iter\': [1000, 2000] } clf = SGDClassifier() grid_search = GridSearchCV(clf, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X_train, y_train) print(\\"Best parameters (SGDClassifier):\\", grid_search.best_params_) return grid_search.best_estimator_ def perform_grid_search_regressor(X_train, y_train): param_grid = { \'loss\': [\'squared_error\', \'huber\'], \'penalty\': [\'l2\', \'l1\'], \'alpha\': [0.0001, 0.001, 0.01], \'max_iter\': [1000, 2000] } reg = SGDRegressor() grid_search = GridSearchCV(reg, param_grid, cv=5, scoring=\'neg_mean_squared_error\') grid_search.fit(X_train, y_train) print(\\"Best parameters (SGDRegressor):\\", grid_search.best_params_) return grid_search.best_estimator_ # Example usage if __name__ == \\"__main__\\": X_train_clf, X_test_clf, y_train_clf, y_test_clf = load_and_preprocess_classification_data(\'classification_data.csv\') clf = train_and_evaluate_sgd_classifier(X_train_clf, X_test_clf, y_train_clf, y_test_clf) best_clf = perform_grid_search_classifier(X_train_clf, y_train_clf) X_train_reg, X_test_reg, y_train_reg, y_test_reg = load_and_preprocess_regression_data(\'regression_data.csv\') reg = train_and_evaluate_sgd_regressor(X_train_reg, X_test_reg, y_train_reg, y_test_reg) best_reg = perform_grid_search_regressor(X_train_reg, y_train_reg)"},{"question":"Using `unittest.mock` Library Problem Statement: Create a class `Calculator` with methods `add`, `subtract`, `multiply`, and `divide`. Implement and test this class using `unittest.mock`. 1. Create a class `Calculator` that has the following methods: * `add(a, b)` - returns the sum of `a` and `b` * `subtract(a, b)` - returns the difference between `a` and `b` * `multiply(a, b)` - returns the product of `a` and `b` * `divide(a, b)` - returns the quotient of `a` divided by `b`. Raise a `ValueError` if `b` is zero. 2. Write tests for the `Calculator` class using the `unittest` and `unittest.mock` libraries. Your tests should include: * Verifying that each method (add, subtract, multiply, divide) of the `Calculator` class returns the correct result. * Mocking the `add` method to raise an exception and verifying your handling of this exception. * Using the `patch` decorator to mock the `subtract` method and setting a return value. Input Specification: * The `Calculator` class methods are called with two numerical inputs, `a` and `b`. * You do not need to write any input functions. You will directly call the methods in your tests. Output Specification: * There is no need for function outputs. All assertions should be in the test cases. Example: ```python # Example implementation of the Calculator class class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b # Example test cases using unittest and unittest.mock import unittest from unittest.mock import Mock, patch class TestCalculator(unittest.TestCase): def setUp(self): self.calc = Calculator() def test_add(self): self.assertEqual(self.calc.add(3, 4), 7) def test_subtract(self): self.assertEqual(self.calc.subtract(10, 5), 5) def test_multiply(self): self.assertEqual(self.calc.multiply(6, 7), 42) def test_divide(self): self.assertEqual(self.calc.divide(14, 2), 7) with self.assertRaises(ValueError): self.calc.divide(14, 0) def test_mock_add_exception(self): self.calc.add = Mock(side_effect=Exception(\\"Mock exception\\")) with self.assertRaises(Exception): self.calc.add(1, 2) @patch.object(Calculator, \'subtract\', return_value=5) def test_patch_subtract(self, mock_subtract): result = self.calc.subtract(10, 5) mock_subtract.assert_called_once_with(10, 5) self.assertEqual(result, 5) if __name__ == \'__main__\': unittest.main() ``` # Constraints: * Ensure each method handles integers as well as floating-point numbers. * The tests should cover various scenarios, including edge cases like division by zero. Notes: * Focus on writing comprehensive tests using `unittest.mock` to demonstrate an understanding of this package. * Use `assert` statements to validate the results in test cases.","solution":"class Calculator: def add(self, a, b): return a + b def subtract(self, a, b): return a - b def multiply(self, a, b): return a * b def divide(self, a, b): if b == 0: raise ValueError(\\"Cannot divide by zero\\") return a / b"},{"question":"# Python Coding Assessment Question Objective: Write a Python function called `process_memoryview` that demonstrates the creation, verification, and basic manipulation of memoryview objects using the provided C API functions wrapped in Python code. Function Signature: ```python def process_memoryview(input_data: bytes) -> Tuple[memoryview, bytes, bool]: ``` Input: - `input_data` (bytes): A bytes object from which the memoryview object will be created. Output: - The function should return a tuple containing the following: 1. A memoryview object created from the input_data. 2. The bytes object derived from the memoryview to showcase data extraction. 3. A boolean value indicating whether the created object is a memoryview object. Requirements: 1. The function should use `PyMemoryView_FromObject` to create a memoryview object from `input_data`. 2. It should then convert this memoryview object back to a bytes object to verify the data. 3. Use the `PyMemoryView_Check` to confirm that the created object is indeed a memoryview. 4. Return the memoryview object, the extracted bytes object, and the boolean result of the check. Constraints: - Assume `input_data` is always a valid, non-empty bytes object. Example Scenario: ```python data = b\'Hello, World!\' memoryview_obj, extracted_data, is_memoryview = process_memoryview(data) print(memoryview_obj) # <memory at 0x7f0c981c4900> print(extracted_data) # b\'Hello, World!\' print(is_memoryview) # True ``` Notes: - You should not directly use any C API functions; instead, replicate their functionality using Python\'s built-in capabilities in the `memoryview` class.","solution":"from typing import Tuple def process_memoryview(input_data: bytes) -> Tuple[memoryview, bytes, bool]: Demonstrates the creation, verification, and basic manipulation of memoryview objects. Args: - input_data (bytes): The input bytes object to create the memoryview from. Returns: - A tuple containing: 1. The memoryview object created from the input_data. 2. The bytes object derived from the memoryview. 3. A boolean indicating if the created object is a memoryview. # Create a memoryview object from the input_data mem_view = memoryview(input_data) # Convert the memoryview object back to a bytes object extracted_data = mem_view.tobytes() # Check if the created object is a memoryview object is_memoryview = isinstance(mem_view, memoryview) return mem_view, extracted_data, is_memoryview"},{"question":"**Objective**: Design a function to handle a pandas DataFrame with potential duplicate row and column labels, clean the data by removing duplicates, and ensure no duplicates can be introduced going forward. # Task Write a function `clean_and_secure_dataframe` that takes a DataFrame as input and performs the following steps: 1. Checks for duplicate row and column labels. 2. Removes any duplicate rows and columns. 3. Ensures no future duplicates can be added to the DataFrame by setting the `allows_duplicate_labels` flag to `False`. # Function Signature ```python import pandas as pd def clean_and_secure_dataframe(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Input - `df` (pd.DataFrame): The input DataFrame, potentially with duplicate row and/or column labels. # Output - A new DataFrame with no duplicate labels and `allows_duplicate_labels` flag set to `False`. # Constraints - Performance should be efficient for DataFrames with up to 100,000 rows and columns. - Do not use any external libraries other than pandas. - Your solution should handle both scenarios where there are duplicate rows and columns and where there are none. # Example ```python import pandas as pd data = { \\"A\\": [1, 2, 3], \\"B\\": [4, 5, 6], \\"A\\": [7, 8, 9], } index = [\\"row1\\", \\"row2\\", \\"row1\\"] df = pd.DataFrame(data, index=index) cleaned_df = clean_and_secure_dataframe(df) print(cleaned_df) # Expected Output: # A B # row1 7 4 # row2 2 5 ``` # Notes 1. The output DataFrame should retain the first occurrence of any duplicate rows and columns encountered. 2. The `allows_duplicate_labels` flag must be set to `False` so that no further duplicates can be introduced. Good luck!","solution":"import pandas as pd def clean_and_secure_dataframe(df: pd.DataFrame) -> pd.DataFrame: Cleans the DataFrame by removing duplicate row and column labels and ensures no future duplicates can be added. Args: - df (pd.DataFrame): The input DataFrame, potentially with duplicate row and/or column labels. Returns: - pd.DataFrame: A new DataFrame with no duplicate labels and `allows_duplicate_labels` flag set to `False`. # Remove duplicate columns by transposing, dropping duplicates, and transposing back df = df.T.drop_duplicates().T # Remove duplicate rows by simply dropping duplicates df = df.loc[~df.index.duplicated(keep=\'first\')] # Set allows_duplicate_labels to False df.flags.allows_duplicate_labels = False return df"},{"question":"# Pandas Extension Array Implementation You are required to implement a custom extension of the pandas `ExtensionArray` class to create a new data type. This will test your understanding of extending pandas objects, handling internal data formats, and implementing various methods to make your custom array function similarly to native pandas arrays. **Task:** Implement a `MyCustomExtensionArray` class which extends `pandas.api.extensions.ExtensionArray`. This array should store data internally as a list of integers and provide the following functionalities: 1. **Constructor**: Initialize the array with a list of integers. 2. **dtype**: Return the custom dtype `MyCustomDtype`. 3. **isna**: Return a boolean array indicating if each element in the array is `None`. 4. **take**: Ability to take elements from specific positions while handling missing data. 5. **copy**: Return a copy of the array. 6. **unique**: Return the unique elements in the array. Alongside, implement a custom dtype `MyCustomDtype` extending `pandas.api.extensions.ExtensionDtype`. **Input:** - A list of integers during the initialization of `MyCustomExtensionArray`. **Output:** - The custom array should support the specified methods and mimic the interface of a typical pandas array. **Constraints:** - Your implementation should handle missing data denoted by `None`. - The `dtype` should handle the representation of the custom type. **Performance Requirements:** - The operations should be optimized to the extent possible given the constraints. ```python import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype import numpy as np class MyCustomDtype(ExtensionDtype): name = \'mycustom\' type = int kind = \'i\' na_value = None # Define how missing values are handled @classmethod def construct_array_type(cls): return MyCustomExtensionArray class MyCustomExtensionArray(ExtensionArray): def __init__(self, values): self._data = list(values) @property def dtype(self): return MyCustomDtype() def isna(self): return np.array([x is None for x in self._data]) def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = fill_value if fill_value is not None else NumpyExtensionArray.dtype.na_value return [self._data[i] if i != -1 else fill_value for i in indices] return [self._data[i] for i in indices] def copy(self): return MyCustomExtensionArray(self._data.copy()) def unique(self): return list(set(self._data)) # Additional required methods based on analysis def __getitem__(self, item): return self._data[item] def __len__(self): return len(self._data) # Example Usage: my_array = MyCustomExtensionArray([1, 2, None, 4]) print(my_array.isna()) # Output: [False, False, True, False] print(my_array.take([0, 2, -1], allow_fill=True)) # Output: [1, None, None] print(my_array.unique()) # Output: [1, 2, 4] ``` **Note:** Ensure your implementation passes numerous edge cases and adheres strictly to the interface requirements of pandas `ExtensionArray`.","solution":"import pandas as pd from pandas.api.extensions import ExtensionArray, ExtensionDtype import numpy as np class MyCustomDtype(ExtensionDtype): name = \'mycustom\' type = int kind = \'i\' na_value = None # Define how missing values are handled @classmethod def construct_array_type(cls): return MyCustomExtensionArray class MyCustomExtensionArray(ExtensionArray): def __init__(self, values): self._data = list(values) @property def dtype(self): return MyCustomDtype() def isna(self): return np.array([x is None for x in self._data]) def take(self, indices, allow_fill=False, fill_value=None): if allow_fill: fill_value = fill_value if fill_value is not None else self.dtype.na_value return MyCustomExtensionArray([self._data[i] if i != -1 else fill_value for i in indices]) return MyCustomExtensionArray([self._data[i] for i in indices]) def copy(self): return MyCustomExtensionArray(self._data.copy()) def unique(self): return MyCustomExtensionArray(list(set(self._data))) # Additional required methods based on analysis def __getitem__(self, item): if isinstance(item, slice): return MyCustomExtensionArray(self._data[item]) return self._data[item] def __len__(self): return len(self._data) def __eq__(self, other): if isinstance(other, MyCustomExtensionArray): return self._data == other._data return False # Example Usage: my_array = MyCustomExtensionArray([1, 2, None, 4]) print(my_array.isna()) # Output: [False, False, True, False] print(my_array.take([0, 2, -1], allow_fill=True)) # Output: MyCustomExtensionArray([1, None, None]) print(my_array.unique()) # Output: MyCustomExtensionArray([1, 2, 4])"},{"question":"You are provided with a dataset that contains sales data for different product categories over several months. Your task is to generate a line plot showing the sales data and to experiment with different plotting contexts to observe their effect on the plot appearance. # Instructions 1. Create a Pandas DataFrame `sales_data` with the following data: | Month | Category | Sales | |---------|----------|-------| | Jan | A | 150 | | Jan | B | 200 | | Jan | C | 170 | | Feb | A | 180 | | Feb | B | 210 | | Feb | C | 160 | | Mar | A | 220 | | Mar | B | 190 | | Mar | C | 180 | 2. Use Seaborn to create a line plot using `sns.lineplot` that shows Sales over Month for each Category. 3. Experiment with different predefined plotting contexts: \\"paper\\", \\"notebook\\", \\"talk\\", and \\"poster\\". For each context, display the corresponding plot. 4. Implement a function `display_sales_with_context` that accepts the DataFrame and a context name as inputs, sets the plotting context, and displays the plot. Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def display_sales_with_context(df: pd.DataFrame, context: str) -> None: pass ``` # Constraints - Ensure that all plots are rendered correctly in a Jupyter notebook environment. - Use appropriate labels and titles for the plot to make it informative. # Example ```python display_sales_with_context(sales_data, \\"notebook\\") ``` This will temporarily set the context to \\"notebook\\" and display the corresponding sales plot.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Creating the sales data DataFrame sales_data = pd.DataFrame({ \'Month\': [\'Jan\', \'Jan\', \'Jan\', \'Feb\', \'Feb\', \'Feb\', \'Mar\', \'Mar\', \'Mar\'], \'Category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Sales\': [150, 200, 170, 180, 210, 160, 220, 190, 180] }) def display_sales_with_context(df: pd.DataFrame, context: str) -> None: Displays a line plot of sales data with the given plotting context. Parameters: - df: pd.DataFrame - context: str, context name to be set for the plot sns.set_context(context) plt.figure(figsize=(10, 6)) sns.lineplot(x=\'Month\', y=\'Sales\', hue=\'Category\', data=df, marker=\'o\') plt.title(f\'Sales Data (Context: {context})\') plt.xlabel(\'Month\') plt.ylabel(\'Sales\') plt.legend(title=\'Category\') plt.show() # Example usage to display the plot in \\"notebook\\" context # display_sales_with_context(sales_data, \\"notebook\\")"},{"question":"**Data Visualization with Custom Color Palettes using Seaborn** # Objective Your task is to create a data visualization using the seaborn library that demonstrates your understanding of color palettes and data representation. You need to load a dataset, create a specific plot, and configure the color palette as per given instructions. # Problem Statement 1. Load the `tips` dataset from the seaborn library. 2. Create a boxplot to visualize the distribution of total bills for each day of the week. 3. Use the `sns.mpl_palette` function to: - Return a palette using the \\"viridis\\" colormap with 5 colors and apply it to your boxplot. - Return the \\"Set2\\" qualitative palette and apply it instead, making sure to handle cases where the number of days exceeds the number of colors. # Instructions 1. **Input**: None required. Load the dataset directly within the function. 2. **Output**: Display the two boxplots within a single function execution. # Constraints - You must use the `seaborn` library for creating the plots. - The function should handle any additional requirements or configurations necessary to switch between palettes without external user intervention. # Implementation You need to implement the function `visualize_tips_data_with_palettes()` that performs the following steps: - Loads the `tips` dataset. - Creates a boxplot for the total bills for each day using the `\\"viridis\\"` colormap with 5 colors. - Creates another boxplot using the `\\"Set2\\"` qualitative colormap. - Displays both plots. # Example ```python import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data_with_palettes(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a boxplot with viridis palette plt.figure(figsize=(10, 5)) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=sns.mpl_palette(\\"viridis\\", 5)) plt.title(\\"Boxplot with Viridis Palette\\") plt.show() # Create a boxplot with Set2 palette plt.figure(figsize=(10, 5)) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=sns.mpl_palette(\\"Set2\\")) plt.title(\\"Boxplot with Set2 Palette\\") plt.show() # Call the function to verify its functionality visualize_tips_data_with_palettes() ``` Implement the above function in your code environment and ensure that the output consists of two plots, each demonstrating the use of different color palettes from the `sns.mpl_palette` function.","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_data_with_palettes(): # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create a boxplot with viridis palette plt.figure(figsize=(10, 5)) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=sns.mpl_palette(\\"viridis\\", 5)) plt.title(\\"Boxplot with Viridis Palette\\") plt.show() # Create a boxplot with Set2 palette (handling case where number of days exceed the colors) set2_palette = sns.color_palette(\\"Set2\\", n_colors=len(tips[\'day\'].unique())) plt.figure(figsize=(10, 5)) sns.boxplot(x=\\"day\\", y=\\"total_bill\\", data=tips, palette=set2_palette) plt.title(\\"Boxplot with Set2 Palette\\") plt.show() # Call the function to verify its functionality visualize_tips_data_with_palettes()"},{"question":"**Objective:** Implement a function that takes a list of integers and returns a new list containing the squares of all odd numbers in reverse order. If there are no odd numbers, return an empty list. **Detailed Instructions:** 1. Your function should be named `reverse_squares_of_odds`. 2. The function should take a single list of integers as input. 3. Find all odd numbers in the list. 4. Compute the square of each odd number. 5. Collect the squared odd numbers into a new list in reverse order. 6. If the input list has no odd numbers, return an empty list. **Function Signature:** ```python def reverse_squares_of_odds(numbers: list[int]) -> list[int]: pass ``` **Input:** - `numbers` (list of integers): A list containing integer values. **Output:** - list of integers constituting the squares of odd numbers from the input list in reverse order. **Constraints:** - The input list `numbers` can be empty. - The integers in the input list can be positive, negative, or zero. - Assume that the input list has less than 10^5 elements. **Examples:** ```python reverse_squares_of_odds([1, 2, 3, 4, 5]) # Returns: [25, 9, 1] reverse_squares_of_odds([10, 20, 30]) # Returns: [] reverse_squares_of_odds([7]) # Returns: [49] reverse_squares_of_odds([]) # Returns: [] ``` **Performance Requirements:** - Your solution should be efficient with a time complexity of O(n), where n is the length of the input list. Your implementation should demonstrate correct use of built-in functions such as `filter`, `map`, and should efficiently address the problem requirements.","solution":"def reverse_squares_of_odds(numbers: list[int]) -> list[int]: Returns a list of squares of all odd numbers in the input list in reverse order. return [x ** 2 for x in numbers if x % 2 != 0][::-1]"},{"question":"<|Analysis Begin|> The provided documentation is for the `subprocess` module in Python, which is used for creating and managing additional processes, and can handle input/output/error pipes along with the return codes. The key functions and classes in the module include `subprocess.run()`, `subprocess.Popen`, and various constants and exceptions for subprocess management. Key points to consider for creating a coding assessment: 1. **Basic Understanding**: The question should test the basic use of `subprocess.run()` and `subprocess.Popen` for running commands. 2. **Advanced Usage**: More advanced usage should be tested, such as handling pipes, managing timeouts, dealing with output in binary and text modes, and error handling. 3. **Exception Handling**: Assess understanding of handling various exceptions like `subprocess.CalledProcessError` and `subprocess.TimeoutExpired`. 4. **Multiprocess Handling**: Incorporate scenarios where multiple subprocesses interact or affect each other. 5. **Real-world Application**: Create a task that reflects real-world use cases for subprocess management. <|Analysis End|> <|Question Begin|> # Coding Assessment Question Objective Your task is to implement a Python function that utilizes the `subprocess` module to create and manage multiple subprocesses. This will test your understanding of both basic and advanced functionalities of the `subprocess` module, including process creation, input/output management, handling timeouts, and error management. Problem Statement You are tasked with writing a function called `process_management` that performs the following tasks: 1. Runs a sequence of shell commands and waits for their completion. 2. Captures and returns the output of each command. 3. Handles any potential errors that might arise from running the commands. 4. Manages a timeout for the command execution. Function Signature ```python def process_management(commands: List[str], timeout: int) -> Dict[str, Any]: pass ``` Parameters - `commands`: A list of shell commands to run. Each command is a string. - `timeout`: An integer specifying the maximum number of seconds to wait for each command to complete. Returns - A dictionary with the following keys: - `\\"outputs\\"`: A list of tuples `(command, output)`, where `command` is the shell command that was executed and `output` is its captured output (both stdout and stderr combined). If an error occurs, the output should be an error message. - `\\"errors\\"`: A list of tuples `(command, error)`, where `command` is the shell command that failed and `error` is the error message. Requirements - Run each command using the `subprocess.run()` or `subprocess.Popen()` functions. - Capture both stdout and stderr of each command. - Manage a timeout for each command. If a command exceeds the timeout, it should be terminated and an appropriate error message should be recorded. - Handle any exceptions that may be raised by the subprocess module and reflect them in the output. Example Usage ```python commands = [\\"echo \'Hello, World!\'\\", \\"sleep 5\\", \\"invalid_command\\"] timeout = 2 result = process_management(commands, timeout) print(result) ``` Expected Output: ```python { \\"outputs\\": [ (\\"echo \'Hello, World!\'\\", \\"Hello, World!n\\"), ], \\"errors\\": [ (\\"sleep 5\\", \\"Timeout expired\\"), (\\"invalid_command\\", \\"Command \'invalid_command\' returned non-zero exit status 127.\\") ] } ``` Constraints - The commands to be run may include valid shell commands, invalid commands, or commands that might run forever (like `sleep infinity`). - Ensure that your solution handles cases where commands complete successfully, fail, or timeout. Evaluation Criteria - Correctness: The function should correctly execute the commands, capture output, and handle errors. - Robustness: The function should gracefully handle various kinds of errors and should not crash. - Performance: The function should efficiently manage subprocesses and handle timeouts correctly. Good luck!","solution":"import subprocess from typing import List, Dict, Any def process_management(commands: List[str], timeout: int) -> Dict[str, Any]: result = {\\"outputs\\": [], \\"errors\\": []} for command in commands: try: completed_process = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=timeout) output = completed_process.stdout + completed_process.stderr result[\\"outputs\\"].append((command, output)) if completed_process.returncode != 0: result[\\"errors\\"].append((command, f\\"Command \'{command}\' returned non-zero exit status {completed_process.returncode}.\\")) except subprocess.TimeoutExpired: result[\\"errors\\"].append((command, \\"Timeout expired\\")) except subprocess.CalledProcessError as e: result[\\"errors\\"].append((command, str(e))) except Exception as e: result[\\"errors\\"].append((command, f\\"An unexpected error occurred: {str(e)}\\")) return result"},{"question":"# Question: Implementing and Manipulating a DOM Hierarchy **Objective:** You are required to implement a function that constructs a DOM structure representing an XML document. The document should have a specific structure, and you need to interact with various elements, attributes, and node types as described in the `xml.dom` module. **Task:** Write a function `create_dom_structure()` that constructs and returns a `Document` object representing an XML document with the following structure: ```xml <library> <book genre=\\"fantasy\\" isbn=\\"123-456-789\\"> <title>Harry Potter</title> <author>J.K. Rowling</author> <price>29.99</price> </book> <book genre=\\"scifi\\" isbn=\\"987-654-321\\"> <title>Dune</title> <author>Frank Herbert</author> <price>19.99</price> </book> </library> ``` Additionally, implement the following functionalities: 1. Add a new book entry to the document with provided details (title, genre, isbn, author, price). 2. Retrieve and return a list of all book titles in the document. 3. Remove a book entry based on its ISBN. 4. Modify the price of a book based on its title. **Function Specifications:** 1. `def create_dom_structure() -> xml.dom.minidom.Document:` - Returns a `Document` object representing the initial XML structure. 2. `def add_book(doc: xml.dom.minidom.Document, title: str, genre: str, isbn: str, author: str, price: str) -> None:` - Adds a new book element with the specified details to the document. 3. `def get_book_titles(doc: xml.dom.minidom.Document) -> List[str]:` - Returns a list of titles of all books in the document. 4. `def remove_book_by_isbn(doc: xml.dom.minidom.Document, isbn: str) -> bool:` - Removes a book element with the specified ISBN. Returns `True` if the book was found and removed, otherwise `False`. 5. `def modify_book_price(doc: xml.dom.minidom.Document, title: str, new_price: str) -> bool:` - Modifies the price of the book with the specified title. Returns `True` if the book was found and the price was updated, otherwise `False`. **Constraints and Assumptions:** - You can use the `xml.dom.minidom` module to create and manipulate the DOM structure. - Assume the document contains unique ISBNs and titles for simplification. - Handle and raise appropriate exceptions for invalid operations (e.g., book not found). **Example Usage:** ```python # Create the initial document doc = create_dom_structure() # Add a new book add_book(doc, \\"The Hobbit\\", \\"fantasy\\", \\"111-222-333\\", \\"J.R.R. Tolkien\\", \\"25.99\\") # Get list of all book titles titles = get_book_titles(doc) print(titles) # Output: [\'Harry Potter\', \'Dune\', \'The Hobbit\'] # Remove a book by ISBN result = remove_book_by_isbn(doc, \\"987-654-321\\") print(result) # Output: True # Modify book price result = modify_book_price(doc, \\"Harry Potter\\", \\"35.99\\") print(result) # Output: True ``` **Evaluation Criteria:** - Correct construction of the DOM structure. - Accurate implementation of the add, retrieve, remove, and modify functionalities. - Proper handling of exceptions and edge cases.","solution":"from xml.dom.minidom import Document def create_dom_structure() -> Document: doc = Document() library = doc.createElement(\'library\') doc.appendChild(library) book1 = doc.createElement(\'book\') book1.setAttribute(\'genre\', \'fantasy\') book1.setAttribute(\'isbn\', \'123-456-789\') title1 = doc.createElement(\'title\') title1.appendChild(doc.createTextNode(\'Harry Potter\')) book1.appendChild(title1) author1 = doc.createElement(\'author\') author1.appendChild(doc.createTextNode(\'J.K. Rowling\')) book1.appendChild(author1) price1 = doc.createElement(\'price\') price1.appendChild(doc.createTextNode(\'29.99\')) book1.appendChild(price1) library.appendChild(book1) book2 = doc.createElement(\'book\') book2.setAttribute(\'genre\', \'scifi\') book2.setAttribute(\'isbn\', \'987-654-321\') title2 = doc.createElement(\'title\') title2.appendChild(doc.createTextNode(\'Dune\')) book2.appendChild(title2) author2 = doc.createElement(\'author\') author2.appendChild(doc.createTextNode(\'Frank Herbert\')) book2.appendChild(author2) price2 = doc.createElement(\'price\') price2.appendChild(doc.createTextNode(\'19.99\')) book2.appendChild(price2) library.appendChild(book2) return doc def add_book(doc: Document, title: str, genre: str, isbn: str, author: str, price: str) -> None: library = doc.getElementsByTagName(\'library\')[0] book = doc.createElement(\'book\') book.setAttribute(\'genre\', genre) book.setAttribute(\'isbn\', isbn) title_element = doc.createElement(\'title\') title_element.appendChild(doc.createTextNode(title)) book.appendChild(title_element) author_element = doc.createElement(\'author\') author_element.appendChild(doc.createTextNode(author)) book.appendChild(author_element) price_element = doc.createElement(\'price\') price_element.appendChild(doc.createTextNode(price)) book.appendChild(price_element) library.appendChild(book) def get_book_titles(doc: Document) -> list: titles = [] for title_element in doc.getElementsByTagName(\'title\'): titles.append(title_element.firstChild.data) return titles def remove_book_by_isbn(doc: Document, isbn: str) -> bool: books = doc.getElementsByTagName(\'book\') for book in books: if book.getAttribute(\'isbn\') == isbn: book.parentNode.removeChild(book) return True return False def modify_book_price(doc: Document, title: str, new_price: str) -> bool: books = doc.getElementsByTagName(\'book\') for book in books: title_element = book.getElementsByTagName(\'title\')[0] if title_element.firstChild.data == title: price_element = book.getElementsByTagName(\'price\')[0] price_element.firstChild.data = new_price return True return False"},{"question":"**Question: Implementing Custom Warnings and Filters** You are developing a Python application where you need to give users warnings about deprecated functions and excessive resource usage. Implement a program that demonstrates the following: 1. Create a custom warning category named `CustomDeprecatedWarning` which is a subclass of `DeprecationWarning`. 2. Implement a function named `use_deprecated_function()` that issues a `CustomDeprecatedWarning` when called. 3. Implement a function named `use_resources()` that issues a `ResourceWarning` if it detects that too many resources are being used (for the sake of this exercise, just make it always issue the warning). 4. Add a filter that ignores all `ResourceWarning` by default. 5. Add a filter that turns all `CustomDeprecatedWarning` warnings into exceptions. 6. Demonstrate the usage of these functions and filters: - Call `use_deprecated_function()` and handle the exception raised by the warning. - Temporarily suppress the `ResourceWarning` while calling `use_resources()`. **Function Specifications:** 1. `use_deprecated_function()`: No arguments and returns nothing. Issues a `CustomDeprecatedWarning`. 2. `use_resources()`: No arguments and returns nothing. Issues a `ResourceWarning`. 3. `main()`: Demonstrates the usage of the above functions and applies the filters and context managers. Expected Output: - When calling `use_deprecated_function()` in the main script, it should raise an exception caught and handled appropriately. - When calling `use_resources()` within a context manager that suppresses warnings, it should not print the warning. Constraints: - Use the `warnings` module for issuing and managing warnings. - Use proper subclassing to create custom warnings. - Ensure that filters and context managers are applied as specified. ```python import warnings class CustomDeprecatedWarning(DeprecationWarning): pass def use_deprecated_function(): warnings.warn(\\"This function is deprecated.\\", CustomDeprecatedWarning) def use_resources(): warnings.warn(\\"Too many resources used.\\", ResourceWarning) def main(): # Apply warnings filters warnings.filterwarnings(\\"ignore\\", category=ResourceWarning) warnings.filterwarnings(\\"error\\", category=CustomDeprecatedWarning) # Demonstrate handling of CustomDeprecatedWarning try: use_deprecated_function() except CustomDeprecatedWarning as e: print(f\\"Caught an exception from CustomDeprecatedWarning: {e}\\") # Demonstrate suppression of ResourceWarning with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\") use_resources() print(\\"Completed execution.\\") if __name__ == \\"__main__\\": main() ```","solution":"import warnings # Step 1: Create a custom warning category named CustomDeprecatedWarning class CustomDeprecatedWarning(DeprecationWarning): pass # Step 2: Implement use_deprecated_function() that issues CustomDeprecatedWarning def use_deprecated_function(): warnings.warn(\\"This function is deprecated.\\", CustomDeprecatedWarning) # Step 3: Implement use_resources() that issues ResourceWarning def use_resources(): warnings.warn(\\"Too many resources used.\\", ResourceWarning) # Step 6: Demonstrate the usage of these functions and filters def main(): # Step 4: Add a filter that ignores all ResourceWarning by default warnings.filterwarnings(\\"ignore\\", category=ResourceWarning) # Step 5: Add a filter that turns all CustomDeprecatedWarning warnings into exceptions warnings.filterwarnings(\\"error\\", category=CustomDeprecatedWarning) # Demonstrate handling of CustomDeprecatedWarning try: use_deprecated_function() except CustomDeprecatedWarning as e: print(f\\"Caught an exception from CustomDeprecatedWarning: {e}\\") # Demonstrate suppression of ResourceWarning with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\") use_resources() print(\\"Completed execution.\\") if __name__ == \\"__main__\\": main()"},{"question":"**Principal Component Analysis using scikit-learn** # Objective: To assess the student\'s understanding and ability to implement and analyze Principal Component Analysis (PCA) using scikit-learn. # Problem Statement: Given a dataset containing several features, perform Principal Component Analysis (PCA) to identify the principal components that explain the maximum variance. Use these principal components to transform the dataset and analyze the results. # Requirements: - Implement PCA using scikit-learn. - Given parameter `n_components`, specify the number of principal components to retain. - Project the data onto the first two principal components. - Perform the analysis on the transformed data. # Input: 1. A CSV file named `input_data.csv` containing the dataset to be analyzed. You may assume the dataset is already cleaned and preprocessed for simplicity. 2. An integer `n_components` specifying the number of principal components to retain. # Output: 1. A CSV file named `transformed_data.csv` containing the dataset projected onto the first two principal components. Include the values of the first two principal components in separate columns named `PC1` and `PC2`. 2. Print the amount of variance explained by each of the selected components. # Constraints: 1. You must use the `PCA` class from scikit-learn. 2. Handle edge cases where `n_components` might be greater than the number of features in the dataset. 3. Ensure proper handling of input and output files. # Example: Assume `input_data.csv` has the following content: ``` feature1,feature2,feature3 2.5,2.4,3.1 0.5,0.7,0.8 2.2,2.9,2.5 1.9,2.2,2.2 3.1,3.0,3.4 2.3,2.7,2.3 2,1.6,1.8 1,1.1,1.3 1.5,1.6,1.6 1.1,0.9,1.2 ``` Given `n_components = 2`, your program should produce `transformed_data.csv` with content similar to: ``` PC1,PC2 -0.827970186051,0.175115307132 1.77758033,-0.14260237 0.992197494188,-0.38437498916 0.274210415576,0.1304172071 ... ``` And print the explained variance ratio for each component selected. # Instructions: 1. Read the input CSV file and load the data. 2. Implement PCA on the data, retaining the specified number of principal components. 3. Transform the data onto the first two principal components. 4. Save the transformed data into the specified output CSV file. 5. Print the variance explained by each of the selected principal components. Solution Template: ```python import pandas as pd from sklearn.decomposition import PCA def perform_pca(input_file, n_components, output_file): # Load data data = pd.read_csv(input_file) # Initialize PCA pca = PCA(n_components=n_components) # Fit PCA on the dataset pca.fit(data) # Transform the dataset transformed_data = pca.transform(data) # Create a DataFrame with the first two principal components df_transformed = pd.DataFrame(transformed_data[:, :2], columns=[\'PC1\', \'PC2\']) # Save the transformed data into the output file df_transformed.to_csv(output_file, index=False) # Print explained variance ratio print(\\"Explained Variance Ratio for each component:\\") for i, variance_ratio in enumerate(pca.explained_variance_ratio_): print(f\\"Component {i + 1}: {variance_ratio:.4f}\\") # Example usage perform_pca(\'input_data.csv\', 2, \'transformed_data.csv\') ``` Adhere to the provided solution template and complete the function to achieve the desired results.","solution":"import pandas as pd from sklearn.decomposition import PCA import numpy as np def perform_pca(input_file, n_components, output_file): # Load data data = pd.read_csv(input_file) # Ensure n_components is not greater than the number of features n_components = min(n_components, data.shape[1]) # Initialize PCA pca = PCA(n_components=n_components) # Fit PCA on the dataset pca.fit(data) # Transform the dataset transformed_data = pca.transform(data) # Create a DataFrame with the first two principal components df_transformed = pd.DataFrame(transformed_data[:, :2], columns=[\'PC1\', \'PC2\']) # Save the transformed data into the output file df_transformed.to_csv(output_file, index=False) # Print explained variance ratio explained_variance_ratio = pca.explained_variance_ratio_ print(\\"Explained Variance Ratio for each component:\\") for i, variance_ratio in enumerate(explained_variance_ratio): print(f\\"Component {i + 1}: {variance_ratio:.4f}\\") return explained_variance_ratio # Example usage # perform_pca(\'input_data.csv\', 2, \'transformed_data.csv\')"},{"question":"**Question: Manipulating Environment Variables Using the `os` Module** Environment variables are a set of dynamic named values that can affect the way running processes behave on a computer. They are an essential part of any modern operating system and can be used for various purposes, such as configuration settings, storing temporary data, and user preferences. In this task, you are required to implement a function called `modify_env_variable` that updates the value of a given environment variable and demonstrates the change by interacting with the system environment using the `os` module. # Function Signature ```python def modify_env_variable(var_name: str, new_value: str) -> str: pass ``` # Input - `var_name`: A string representing the name of the environment variable to be modified. - `new_value`: A string representing the new value to be assigned to the environment variable. # Output - Returns a string indicating the new value of the environment variable after modification. # Constraints 1. The environment variable specified by `var_name` is guaranteed to exist in the current environment. 2. You must use the `os` module to modify the environment variable. 3. Do not use the `posix` module directly. # Example ```python import os def modify_env_variable(var_name: str, new_value: str) -> str: os.environ[var_name] = new_value return os.environ[var_name] # Suppose an environment variable \\"TEST_VAR\\" exists with initial value \\"initial_value\\" os.environ[\\"TEST_VAR\\"] = \\"initial_value\\" # Changing the value of \\"TEST_VAR\\" result = modify_env_variable(\\"TEST_VAR\\", \\"new_value\\") print(result) # Output should be \\"new_value\\" ``` # Explanation 1. The function `modify_env_variable` takes the name of an environment variable `var_name` and a new value `new_value` as input. 2. It updates the environment variable using the `os.environ` dictionary. 3. The updated value is then returned to demonstrate that the environment variable has been successfully changed. Use this question to test students’ ability to manage system environment variables and understand the implications of such modifications in a controlled environment using Python\'s `os` module.","solution":"import os def modify_env_variable(var_name: str, new_value: str) -> str: Modifies the value of the specified environment variable and returns the new value. Parameters: var_name (str): The name of the environment variable to modify. new_value (str): The new value to assign to the environment variable. Returns: str: The new value of the environment variable after modification. os.environ[var_name] = new_value return os.environ[var_name]"},{"question":"**Python Coding Assessment Question** **Objective:** Write a function that interprets system error numbers and handles them appropriately by raising exceptions with relevant messages. **Problem:** You are to implement a function called `handle_error_code` that takes in an error number (an integer) and performs the following tasks: 1. Checks if the error number exists in the `errno.errorcode` dictionary. 2. If it does, raise the corresponding exception with an error message that includes both the error name and a human-readable description provided by `os.strerror()`. 3. If it does not exist in the dictionary, raise a `ValueError` with the message `\\"Unknown error code\\"`. **Function Signature:** ```python def handle_error_code(error_number: int) -> None: pass ``` **Input:** - `error_number` (int): An error code (e.g., 1, 2, 3). **Output:** - The function should raise the corresponding exception with a detailed message if the error code is valid. If it is not valid, it should raise a `ValueError`. **Examples:** ```python try: handle_error_code(1) except PermissionError as e: print(e) # Output: \\"[EPERM] Operation not permitted\\" try: handle_error_code(2) except FileNotFoundError as e: print(e) # Output: \\"[ENOENT] No such file or directory\\" try: handle_error_code(12345) except ValueError as e: print(e) # Output: \\"Unknown error code\\" ``` **Constraints:** - Do not assume error numbers will only come from a predefined set; handle unknown error numbers gracefully. - You may assume `os` and `errno` modules are available for import. **Notes:** - Utilize `os.strerror()` to retrieve the human-readable description of the error number. - Use appropriate exception handling to map error codes to their respective exceptions. Consider this an opportunity to demonstrate your understanding of exception handling and the usage of standard libraries in Python.","solution":"import errno import os def handle_error_code(error_number: int) -> None: Handle the system error code by raising the corresponding exception with a message that includes the error name and description. Args: error_number (int): The error code to handle. Raises: The corresponding exception for the known error code, or ValueError for an unknown error code. if error_number in errno.errorcode: # Retrieve the error name and description error_name = errno.errorcode[error_number] error_description = os.strerror(error_number) # Create the complete error message error_message = f\\"[{error_name}] {error_description}\\" # Raise the corresponding exception if error_number == errno.EPERM: raise PermissionError(error_message) elif error_number == errno.ENOENT: raise FileNotFoundError(error_message) elif error_number == errno.ESRCH: raise ProcessLookupError(error_message) # Add more exceptions as needed else: raise OSError(error_message) else: # Raise ValueError for unknown error code raise ValueError(\\"Unknown error code\\")"},{"question":"Problem Statement You are tasked with implementing a function that processes a series of commands to manipulate a set. The function should handle various set operations described in the provided list of commands. Function Signature ```python def process_set_commands(commands: list) -> set: Process a list of set commands and return the final set state. Parameters: commands (list): A list of commands where each command is a tuple. The first element of the tuple is a string denoting the operation, and the subsequent elements are the parameters of the operation. Supported commands are: - (\\"add\\", item): Adds an item to the set. - (\\"discard\\", item): Discards an item from the set. - (\\"pop\\",): Pops an arbitrary item from the set. - (\\"clear\\",): Clears the set. - (\\"contains\\", item): Checks if an item is in the set (not modifying the set). - (\\"size\\",): Returns the size of the set (not modifying the set). - (\\"new\\", iterable): Creates a new set from an iterable. Returns: set: The final state of the set after processing all commands in order. ``` Example ```python commands = [ (\\"new\\", [1, 2, 3]), (\\"add\\", 4), (\\"discard\\", 2), (\\"contains\\", 3), (\\"pop\\",), (\\"size\\",), (\\"clear\\",) ] final_set = process_set_commands(commands) ``` For the above example, `process_set_commands(commands)` should return an empty set since the final command is to clear the set. Constraints - You may assume that the operations will be valid and in the correct form as described. - Handle errors gracefully, such as when calling `pop` on an empty set. - For \\"contains\\" and \\"size\\" commands, you should simply print the result since they do not modify the set. Implementation Notes 1. **Set Operations**: - You can use the built-in `set` methods for operations like `add`, `discard`, `pop`, and `clear`. 2. **Type Checking**: - Ensure that the initial command is always `new` to create the set. - Use appropriate exception handling for operations like `pop` which may fail if the set is empty. 3. **Command Processing**: - Iterate through the commands, updating the set accordingly. - For informational commands like `contains` and `size`, print the result and continue processing. Implement the `process_set_commands` function to complete this task. Expected Output The function should process the given commands in order and return the final state of the set after all operations are applied. Print outputs for `contains` and `size` commands should be visible in the console.","solution":"def process_set_commands(commands): s = set() for command in commands: operation = command[0] if operation == \\"new\\": s = set(command[1]) elif operation == \\"add\\": s.add(command[1]) elif operation == \\"discard\\": s.discard(command[1]) elif operation == \\"pop\\": if s: s.pop() elif operation == \\"clear\\": s.clear() elif operation == \\"contains\\": item = command[1] print(item in s) elif operation == \\"size\\": print(len(s)) return s"},{"question":"Objective: Implement a function to create a multi-part email message with a specific structure given a dictionary of parts. The goal is to assess students\' understanding of the `email.message.Message` class, ensuring they can construct and manipulate email messages, including headers, payloads, and handling multipart messages. Question: Write a function `create_multipart_message(parts_dict)` that takes a dictionary `parts_dict` where: - The keys are the content types (like \'text/plain\', \'text/html\', \'application/pdf\', etc.) - The values are tuples containing (`header_dict`, `payload`), where: - `header_dict` is a dictionary where keys are header names and values are header values (example: {\'Subject\': \'Test Email\'}). - `payload` is the actual content for that part (either string or bytes). The function should create and return an `email.message.Message` object representing a multipart email message. Each entry in `parts_dict` should be added as a subpart of the multipart message. Expected Input and Output: - **Input**: `parts_dict` (dictionary) ```python { \\"text/plain\\": ({\\"Subject\\": \\"Text Part\\"}, \\"This is the text part\\"), \\"text/html\\": ({\\"Content-Disposition\\": \\"inline\\"}, \\"<html><body>This is the HTML part</body></html>\\") } ``` - **Output**: Returns an `email.message.Message` object. Constraints: - No specific constraint on the number of parts, but it should handle at least two different parts. - The solution should correctly handle both ASCII and non-ASCII characters in headers and payloads. Example: ```python def create_multipart_message(parts_dict): from email.message import Message from email.policy import compat32 # Create the main message container msg = Message(policy=compat32) msg.set_type(\'multipart/mixed\') # Define the main type as multipart/mixed # Iterate over the parts dictionary and add each part for content_type, (headers, payload) in parts_dict.items(): part = Message(policy=compat32) part.set_type(content_type) # Set headers for this part for header_key, header_value in headers.items(): part[header_key] = header_value # Set the payload for this part part.set_payload(payload) # Attach the part to the main message msg.attach(part) return msg # Sample input parts_dict = { \\"text/plain\\": ({\\"Subject\\": \\"Text Part\\"}, \\"This is the text part\\"), \\"text/html\\": ({\\"Content-Disposition\\": \\"inline\\"}, \\"<html><body>This is the HTML part</body></html>\\") } email_message = create_multipart_message(parts_dict) print(email_message) ``` In the example, the function creates a multipart email containing one part with plain text and another part with HTML content.","solution":"def create_multipart_message(parts_dict): from email.message import EmailMessage from email.policy import default # Create the main message container msg = EmailMessage(policy=default) msg.set_type(\'multipart/mixed\') # Define the main type as multipart/mixed # Iterate over the parts dictionary and add each part for content_type, (headers, payload) in parts_dict.items(): part = EmailMessage(policy=default) part.set_type(content_type) # Set headers for this part for header_key, header_value in headers.items(): part[header_key] = header_value # Set the payload for this part part.set_payload(payload) # Attach the part to the main message msg.attach(part) return msg"},{"question":"# Objective Your task is to implement a series of functions that create and manipulate pandas Index objects. Each function should demonstrate your understanding of different Index types and their operations. # The challenge comprises three parts: 1. **Creating Indexes** 2. **Manipulating Indexes** 3. **Advanced Indexing** Part 1: Creating Indexes Implement the following function: ```python import pandas as pd def create_indexes(): Create and return a dictionary containing different types of pandas Index objects. Returns: dict: A dictionary with the following keys and corresponding pandas Index objects as values - \'range_index\': A RangeIndex starting at 0 and ending at 10 with step 1. - \'categorical_index\': A CategoricalIndex with categories \'small\', \'medium\', \'large\'. - \'interval_index\': An IntervalIndex created from the arrays [0, 1, 2] and [1, 2, 3]. - \'multi_index\': A MultiIndex from tuples [(\'a\', 1), (\'a\', 2), (\'b\', 1)]. - \'datetime_index\': A DatetimeIndex ranging from \'2023-01-01\' to \'2023-01-10\'. pass ``` Part 2: Manipulating Indexes Implement the following function: ```python def manipulate_indexes(indexes): Perform various manipulations on provided index objects. Args: indexes (dict): A dictionary of pandas Index objects. Returns: dict: A dictionary with the following keys and their corresponding manipulated Index objects: - \'updated_categorical\': Rename categories of the categorical index to \'S\', \'M\', \'L\'. - \'trimmed_multi_index\': Drop the second level from the multi-index. - \'shifted_datetime_index\': Shift the datetime index by 2 days. pass ``` Part 3: Advanced Indexing Implement the following function: ```python def advanced_indexing(df): Given a DataFrame `df` with a MultiIndex, perform the following: 1. Select and return a subset of the DataFrame where the first level of MultiIndex is \'a\'. 2. Return a DataFrame after resetting the index. Args: df (pd.DataFrame): A DataFrame with a MultiIndex. Returns: tuple: A tuple containing: - pd.DataFrame: The subset of the DataFrame. - pd.DataFrame: The DataFrame with the index reset. pass ``` # Example Usage: ```python indexes = create_indexes() manipulated_indexes = manipulate_indexes(indexes) # Assuming `df` is a DataFrame with MultiIndex subset_df, reset_index_df = advanced_indexing(df) ``` # Input and Output Formats: - For `create_indexes`: - Input: None - Output: Dictionary with specified Index objects - For `manipulate_indexes`: - Input: Dictionary of Index objects as returned by `create_indexes` - Output: Dictionary with manipulated Index objects - For `advanced_indexing`: - Input: Pandas DataFrame with MultiIndex - Output: Tuple containing two DataFrames after performing the specified operations # Constraints: - Ensure all created Index objects are appropriate and valid. - Manipulated Index objects should maintain their property types after operations. - The DataFrame in `advanced_indexing` function will always contain a valid MultiIndex. # Performance: - The operations should be efficient and make use of pandas built-in methods to leverage optimized routines.","solution":"import pandas as pd def create_indexes(): Create and returns a dictionary containing different types of pandas Index objects. Returns: dict: A dictionary with the following keys and corresponding pandas Index objects as values: - \'range_index\': A RangeIndex starting at 0 and ending at 10 with step 1. - \'categorical_index\': A CategoricalIndex with categories \'small\', \'medium\', \'large\'. - \'interval_index\': An IntervalIndex created from the arrays [0, 1, 2] and [1, 2, 3]. - \'multi_index\': A MultiIndex from tuples [(\'a\', 1), (\'a\', 2), (\'b\', 1)]. - \'datetime_index\': A DatetimeIndex ranging from \'2023-01-01\' to \'2023-01-10\'. range_index = pd.RangeIndex(start=0, stop=11, step=1) categorical_index = pd.CategoricalIndex([\'small\', \'medium\', \'large\']) interval_index = pd.IntervalIndex.from_arrays([0, 1, 2], [1, 2, 3]) multi_index = pd.MultiIndex.from_tuples([(\'a\', 1), (\'a\', 2), (\'b\', 1)], names=[\\"letter\\", \\"number\\"]) datetime_index = pd.date_range(start=\\"2023-01-01\\", end=\\"2023-01-10\\") return { \'range_index\': range_index, \'categorical_index\': categorical_index, \'interval_index\': interval_index, \'multi_index\': multi_index, \'datetime_index\': datetime_index } def manipulate_indexes(indexes): Perform various manipulations on provided index objects. Args: indexes (dict): A dictionary of pandas Index objects. Returns: dict: A dictionary with the following keys and their corresponding manipulated Index objects: - \'updated_categorical\': Rename categories of the categorical index to \'S\', \'M\', \'L\'. - \'trimmed_multi_index\': Drop the second level from the multi-index. - \'shifted_datetime_index\': Shift the datetime index by 2 days. updated_categorical = indexes[\'categorical_index\'].rename_categories([\'S\', \'M\', \'L\']) trimmed_multi_index = indexes[\'multi_index\'].droplevel(1) shifted_datetime_index = indexes[\'datetime_index\'] + pd.DateOffset(days=2) return { \'updated_categorical\': updated_categorical, \'trimmed_multi_index\': trimmed_multi_index, \'shifted_datetime_index\': shifted_datetime_index } def advanced_indexing(df): Given a DataFrame `df` with a MultiIndex, perform the following: 1. Select and return a subset of the DataFrame where the first level of MultiIndex is \'a\'. 2. Return a DataFrame after resetting the index. Args: df (pd.DataFrame): A DataFrame with a MultiIndex. Returns: tuple: A tuple containing: - pd.DataFrame: The subset of the DataFrame. - pd.DataFrame: The DataFrame with the index reset. subset_df = df.loc[\'a\'] reset_index_df = df.reset_index() return subset_df, reset_index_df"},{"question":"<|Analysis Begin|> The provided documentation explains the functionality of the Python `abc` module, which is used for defining Abstract Base Classes (ABCs) as outlined in PEP 3119. The `abc` module provides a metaclass, `ABCMeta`, which can be used to create these ABCs, and the `ABC` class, which simplifies the creation of ABCs through inheritance. The documentation outlines several concepts and features: 1. **ABCMeta Metaclass**: Used to define and register abstract base classes. It allows the creation and registration of virtual subclasses. 2. **ABC Class**: A helper class that has `ABCMeta` as its metaclass, simplifying the creation of ABCs. 3. **Abstract Methods**: Methods annotated with `@abstractmethod` that must be implemented by derived non-abstract classes. 4. **Subclass Hooks**: Customization of `issubclass` checks via the `__subclasshook__` method. 5. **Methods and Decorators**: Various methods and decorators for defining class methods, static methods, and properties. The documentation also includes usage examples which show how to define abstract base classes, register virtual subclasses, and enforce method implementations in subclasses. Given this, a coding assessment question can focus on implementing an abstract base class with several abstract methods, registering a virtual subclass, and performing operations that demonstrate the concepts of ABCs, `@abstractmethod`, and subclass checks. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of abstract base classes, abstract methods, and the registration of virtual subclasses using the `abc` module in Python. **Question:** You are required to create an abstract base class for a shape management system. The abstract base class should define the necessary methods for calculating area and perimeter. You will also create one concrete class and one virtual subclass for this system. **Requirements:** 1. Define an abstract base class `Shape` using either the `ABC` helper class or `ABCMeta` metaclass. This class should have: - An abstract method `area` for calculating the area of the shape. - An abstract method `perimeter` for calculating the perimeter of the shape. 2. Create a concrete class `Rectangle` that inherits from `Shape` and implements the `area` and `perimeter` methods. 3. Define a separate class `RegularPolygon` that does not inherit from `Shape` but has methods `area` and `perimeter`. Register `RegularPolygon` as a virtual subclass of `Shape`. 4. Demonstrate the functionality by: - Creating an instance of `Rectangle` with specific dimensions and displaying its area and perimeter. - Creating an instance of `RegularPolygon` with a specific number of sides and side length, and verifying it as a virtual subclass of `Shape`. - Displaying the area and perimeter of the `RegularPolygon` instance. **Constraints:** - The `RegularPolygon` class should be designed to handle polygons with equal side lengths. **Performance Requirements:** - Ensure that the `area` and `perimeter` methods in `Rectangle` and `RegularPolygon` compute and return the results efficiently. **Input and Output Format:** ```python from math import sqrt, tan # Step 1: Define the abstract base class from abc import ABC, abstractmethod class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass # Step 2: Define the concrete class Rectangle class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) # Step 3: Define the RegularPolygon class and register it as a virtual subclass class RegularPolygon: def __init__(self, num_sides, side_length): self.num_sides = num_sides self.side_length = side_length def area(self): return (self.num_sides * self.side_length**2) / (4 * tan(pi / self.num_sides)) def perimeter(self): return self.num_sides * self.side_length Shape.register(RegularPolygon) # Step 4: Demonstrate the functionality # Creating an instance of Rectangle rect = Rectangle(5, 10) print(f\\"Rectangle Area: {rect.area()}\\") # Output: 50 print(f\\"Rectangle Perimeter: {rect.perimeter()}\\") # Output: 30 # Creating an instance of RegularPolygon polygon = RegularPolygon(6, 4) # Example for a hexagon print(f\\"Is RegularPolygon a virtual subclass of Shape? {isinstance(polygon, Shape)}\\") # Output: True print(f\\"RegularPolygon Area: {polygon.area()}\\") # Calculated based on provided formula print(f\\"RegularPolygon Perimeter: {polygon.perimeter()}\\") # Output: 24 ``` Ensure your implementation adheres to the described requirements.","solution":"from abc import ABC, abstractmethod from math import pi, tan class Shape(ABC): @abstractmethod def area(self): pass @abstractmethod def perimeter(self): pass class Rectangle(Shape): def __init__(self, width, height): self.width = width self.height = height def area(self): return self.width * self.height def perimeter(self): return 2 * (self.width + self.height) class RegularPolygon: def __init__(self, num_sides, side_length): self.num_sides = num_sides self.side_length = side_length def area(self): return (self.num_sides * self.side_length**2) / (4 * tan(pi / self.num_sides)) def perimeter(self): return self.num_sides * self.side_length Shape.register(RegularPolygon) # Demonstrate the functionality # Creating an instance of Rectangle rect = Rectangle(5, 10) print(f\\"Rectangle Area: {rect.area()}\\") # Output: 50 print(f\\"Rectangle Perimeter: {rect.perimeter()}\\") # Output: 30 # Creating an instance of RegularPolygon polygon = RegularPolygon(6, 4) # Example for a hexagon print(f\\"Is RegularPolygon a virtual subclass of Shape? {isinstance(polygon, Shape)}\\") # Output: True print(f\\"RegularPolygon Area: {polygon.area()}\\") # Calculated based on provided formula print(f\\"RegularPolygon Perimeter: {polygon.perimeter()}\\") # Output: 24"},{"question":"**Question: Exception Handling and Cleanup in Python** **Objective:** Assess the understanding of handling exceptions, raising custom exceptions, and ensuring proper cleanup in Python programs. **Description:** You are tasked with implementing a function `process_file(filename: str) -> int` that: 1. Opens a file in read mode and reads its contents. 2. Processes the content to count the number of integers in the file. 3. Implements proper exception handling and clean-up actions. **Function Details:** 1. **Input:** - `filename`: A string representing the path to a text file. The file is expected to have multiple lines, and each line can either be an integer or other strings. 2. **Output:** - An integer representing the count of valid integers found in the file. 3. **Constraints:** - If the file does not exist or cannot be opened, a custom exception `FileNotFoundErrorCustom` should be raised. - If the file content cannot be processed (for example, if it includes data that cannot be interpreted as integers), a custom exception `ContentProcessingError` should be raised. - Ensure the file is properly closed after reading, regardless of whether an exception occurs. **Example:** ```python # Example file content in \'example.txt\' 123 abc 456 def 789 # Calling the function result = process_file(\'example.txt\') print(result) # Output: 3 ``` **Implementation Requirements:** - Define the custom exceptions: `FileNotFoundErrorCustom` and `ContentProcessingError`. - Use the `with` statement for file handling to ensure the file is properly closed. - Use appropriate exception handling (e.g., `try...except` blocks) to manage different error scenarios. - Extensive use of comments explaining each step and how exceptions are handled. - Performance considerations: The function should handle large files efficiently without compromising on completeness of exception handling. Implement the above specification in Python. Provide sufficient inline comments explaining your approach and how you ensure proper exception handling and clean-up.","solution":"class FileNotFoundErrorCustom(Exception): Exception raised when the file to be processed cannot be found or opened. pass class ContentProcessingError(Exception): Exception raised when the content of the file cannot be processed properly. pass def process_file(filename: str) -> int: Reads a file and counts the number of valid integer entries. :param filename: the name of the file to read :return: the count of valid integers in the file valid_integer_count = 0 try: with open(filename, \'r\') as file: for line in file: try: # Try to convert each line to integer int(line.strip()) valid_integer_count += 1 except ValueError: # If conversion fails, it is not an integer; continue to next line continue except FileNotFoundError: raise FileNotFoundErrorCustom(f\\"File {filename} not found or cannot be opened.\\") except Exception as e: raise ContentProcessingError(f\\"An error occurred while processing the file: {str(e)}\\") return valid_integer_count"},{"question":"# HTTP Request and Response Handling with `urllib.request` **Objective:** In this task, you are required to utilize the `urllib.request` module to perform various HTTP operations including making different types of HTTP requests, handling redirections, managing proxies, basic HTTP Authentication, and manipulating HTTP headers. **Task:** 1. Write a Python function `fetch_url_content(url: str) -> str` that uses `urllib.request` to fetch the content of a given URL and return it as a string. 2. Write a Python function `post_data_to_url(url: str, data: dict) -> str` that takes a URL and a dictionary of data, performs a POST request by encoding the data and returns the response content as a string. 3. Write a Python function `fetch_with_custom_headers(url: str, headers: dict) -> str` that takes a URL and a dictionary of headers, makes a GET request using those custom headers, and returns the response content as a string. 4. Write a Python function `handle_redirection(url: str) -> str` that handles HTTP redirections and returns the final URL after redirections. 5. Write a Python function `access_protected_resource(url: str, username: str, password: str) -> str` which accesses a resource that requires HTTP Basic Authentication and returns the response content. **Input:** - `url: str` - a valid URL. - `data: dict` - a dictionary of key-value pairs to be sent in a POST request. - `headers: dict` - a dictionary of custom headers to use in the GET request. - `username: str` - username for HTTP Basic Authentication. - `password: str` - password for HTTP Basic Authentication. **Output:** - For all functions, the output should be a string representing the response content or the final URL as required in respective tasks. **Constraints:** - Assume that the URLs provided are always valid and accessible. - Your implementation should handle all necessary exceptions and edge cases such as handling timeouts and invalid responses. - Respect the HTTP methods semantics as outlined in the HTTP/1.1 specification. **Example Usage:** ```python # Example data url = \\"http://example.com\\" data = {\\"key1\\": \\"value1\\", \\"key2\\": \\"value2\\"} headers = {\\"User-Agent\\": \\"Mozilla/5.0\\", \\"Accept\\": \\"application/json\\"} # Task 1 print(fetch_url_content(url)) # Task 2 print(post_data_to_url(url, data)) # Task 3 print(fetch_with_custom_headers(url, headers)) # Task 4 print(handle_redirection(\\"http://example.com/redirect\\")) # Task 5 print(access_protected_resource(\\"http://example.com/protected\\", \\"user\\", \\"pass\\")) ``` Ensure your functions are well-documented and thoroughly tested with various scenarios including handling errors and edge cases.","solution":"import urllib.request import urllib.parse from urllib.error import HTTPError, URLError def fetch_url_content(url: str) -> str: Fetch the content of a given URL and return it as a string. try: with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except (HTTPError, URLError) as e: return str(e) def post_data_to_url(url: str, data: dict) -> str: Perform a POST request by encoding the data and returns the response content as a string. try: encoded_data = urllib.parse.urlencode(data).encode(\'utf-8\') with urllib.request.urlopen(url, data=encoded_data) as response: return response.read().decode(\'utf-8\') except (HTTPError, URLError) as e: return str(e) def fetch_with_custom_headers(url: str, headers: dict) -> str: Make a GET request using custom headers and return the response content as a string. try: req = urllib.request.Request(url, headers=headers) with urllib.request.urlopen(req) as response: return response.read().decode(\'utf-8\') except (HTTPError, URLError) as e: return str(e) def handle_redirection(url: str) -> str: Handles HTTP redirections and returns the final URL after redirections. try: response = urllib.request.urlopen(url) return response.geturl() except (HTTPError, URLError) as e: return str(e) def access_protected_resource(url: str, username: str, password: str) -> str: Access a resource that requires HTTP Basic Authentication and return the response content. try: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) urllib.request.install_opener(opener) with urllib.request.urlopen(url) as response: return response.read().decode(\'utf-8\') except (HTTPError, URLError) as e: return str(e)"},{"question":"Objective: You are required to implement a Python function using PyTorch that calculates the element-wise exponential of a tensor. In addition to implementing this function, you will also use PyTorch\'s benchmarking utilities to evaluate its performance against PyTorch\'s built-in exponential function. Requirements: 1. **Function Implementation** - Implement a custom function `custom_exponential(tensor)` that computes the exponential of each element in the input tensor without using the built-in PyTorch exponential functions. - The function should take a single argument, `tensor`, which is a PyTorch tensor. - The function should return a new tensor containing the result of the element-wise exponential operation. 2. **Benchmarking** - Use `torch.utils.benchmark.Timer` to measure and compare the execution time of your custom exponential function with the PyTorch built-in `torch.exp` function. - Run the benchmark for tensors of different sizes (e.g., (10, 100), (100, 1000), and (1000, 1000)). - Print a summary of the results comparing the execution times of both implementations. Code Template: ```python import torch from torch.utils.benchmark import Timer def custom_exponential(tensor): # Implement the element-wise exponential function without using torch.exp result = torch.empty_like(tensor) iterator = torch.nditer([tensor, result], flags=[\'multi_index\']) for t, r in iterator: r[...] = torch.exp(t) return result def benchmark_exponential(): tensor_sizes = [(10, 100), (100, 1000), (1000, 1000)] for size in tensor_sizes: tensor = torch.randn(size) # Benchmark custom_exponential t_custom = Timer(\'custom_exponential(tensor)\', globals={\'custom_exponential\': custom_exponential, \'tensor\': tensor}) m_custom = t_custom.timeit(100) # Benchmark torch.exp t_builtin = Timer(\'torch.exp(tensor)\', globals={\'torch\': torch, \'tensor\': tensor}) m_builtin = t_builtin.timeit(100) print(f\\"Tensor size: {size}\\") print(f\\"Custom exponential time: {m_custom.mean:.6f} seconds\\") print(f\\"Built-in torch.exp time: {m_builtin.mean:.6f} seconds\\") print() if __name__ == \\"__main__\\": benchmark_exponential() ``` Notes: - Ensure that your `custom_exponential` function does not rely on the built-in `torch.exp` function or any high-level PyTorch operations that perform the exponential calculation internally. - The `benchmark_exponential` function should run the benchmarking tests and output the results in a readable format. Expected Output: The benchmarking results should display the runtime comparison between the custom exponential function and the built-in `torch.exp` function for different tensor sizes.","solution":"import torch def custom_exponential(tensor): Computes the element-wise exponential of the input tensor. Args: tensor (torch.Tensor): Input tensor. Returns: torch.Tensor: Tensor with the exponential of each element. # Initialize an empty tensor with the same shape and dtype as input result = torch.empty_like(tensor) # Get the number of elements in the tensor num_elements = tensor.numel() # Reshape the tensor to a 1D tensor to iterate easily tensor_reshape = tensor.view(num_elements) result_reshape = result.view(num_elements) # Compute exponential for each element manually for i in range(num_elements): result_reshape[i] = tensor_reshape[i].exp() # Reshape the result back to the original tensor shape return result.view_as(tensor)"},{"question":"# Multi-threaded Job Processing with Queues You are tasked with building a multi-threaded job processing system using Python\'s `queue` module. The system should be able to handle multiple types of jobs with different priorities and ensure that all tasks are processed correctly. Your goal is to implement a system that meets the following criteria: 1. **Input Format:** - A list of jobs to process, where each job is represented as a tuple: `(priority, job_id)`. Higher numerical value of `priority` indicates higher priority. - An integer specifying the number of worker threads. 2. **Output Format:** - A list of job IDs in the order they were processed. 3. **Constraints:** - Use the `queue.PriorityQueue` class to manage the jobs. - You must use multiple threads to process the jobs concurrently. - Each thread should fetch and process jobs based on their priority. - Ensure all jobs are fully processed before exiting. 4. **Performance Requirements:** - The solution should efficiently manage queue operations and thread synchronization. - Careful handling of queue lock mechanisms is required to avoid deadlocks. # Example Input: ```python jobs = [(1, \'job1\'), (3, \'job3\'), (2, \'job2\'), (4, \'job4\')] num_threads = 2 ``` Expected Output: ```python [\'job4\', \'job3\', \'job2\', \'job1\'] ``` # Function Definition ```python import threading import queue def process_jobs(jobs, num_threads): # Your implementation here # Example usage: if __name__ == \\"__main__\\": jobs = [(1, \'job1\'), (3, \'job3\'), (2, \'job2\'), (4, \'job4\')] num_threads = 2 result = process_jobs(jobs, num_threads) print(result) ``` Note: - Ensure to manage the insertion and processing of jobs within the threads correctly. - Make sure all jobs are processed and the main thread waits for the completion of all threads before returning the results.","solution":"import threading import queue def process_jobs(jobs, num_threads): def worker(): while True: try: priority, job_id = job_queue.get(timeout=1) # added timeout to allow graceful exit processed_jobs.append(job_id) job_queue.task_done() except queue.Empty: break job_queue = queue.PriorityQueue() for job in jobs: job_queue.put((-job[0], job[1])) # Negative priority for high-first processed_jobs = [] threads = [] for _ in range(num_threads): t = threading.Thread(target=worker) t.start() threads.append(t) # Wait for all jobs to be processed job_queue.join() # Wait for all threads to finish for t in threads: t.join() return processed_jobs"},{"question":"# Question: Implement and Vectorize a Function with `torch.func.vmap` You are required to implement a function using the `torch.func` module that performs a specific computation on a batch of input tensors. The function should be written in a manner compatible with `vmap` and must adhere to all the limitations and constraints outlined in the documentation. Function Specifications: 1. **Function Name**: `vectorized_squared_difference` 2. **Input**: Two tensors `a` and `b`, each of shape `[batch_size, n]`. 3. **Output**: A tensor of shape `[batch_size]` containing the summed squared difference between corresponding vectors in `a` and `b`. Requirements: 1. **vmap**: Use `torch.func.vmap` to vectorize the computation over the batch dimension. 2. **Limitations**: Ensure no global variables are mutated, and do not use operations that are incompatible with `vmap`. Example: Given: - `a = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])` - `b = torch.tensor([[1.0, 2.1], [2.9, 4.0], [5.1, 6.1]])` The `vectorized_squared_difference` function should return: - `output = torch.tensor([0.01, 0.01, 0.02])` ```python import torch from torch.func import vmap def squared_difference(x, y): return ((x - y) ** 2).sum() def vectorized_squared_difference(a, b): # Your implementation here using vmap and squared_difference pass # Example usage a = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) b = torch.tensor([[1.0, 2.1], [2.9, 4.0], [5.1, 6.1]]) result = vectorized_squared_difference(a, b) print(result) # Expected: tensor([0.01, 0.01, 0.02]) ``` **Constraints**: - The input tensors `a` and `b` will always have the same shapes. - You cannot use any `torch.autograd` APIs like `torch.autograd.grad` within the vectorized function. Ensure your solution adheres to these constraints and provide a well-optimized implementation of the `vectorized_squared_difference` function.","solution":"import torch from torch.func import vmap def squared_difference(x, y): return ((x - y) ** 2).sum() def vectorized_squared_difference(a, b): return vmap(squared_difference)(a, b) # Example usage a = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) b = torch.tensor([[1.0, 2.1], [2.9, 4.0], [5.1, 6.1]]) result = vectorized_squared_difference(a, b) print(result) # Expected: tensor([0.01, 0.01, 0.02])"},{"question":"# Custom Pickling for Complex Numbers In this task, you are required to implement custom pickling support for a class representing complex numbers. The aim is to ensure that instances of this class can be correctly pickled and unpickled using the `pickle` module. Class Definition You are given a class `ComplexNumber` which represents a complex number with real (`real`) and imaginary (`imag`) parts. Here is the initial implementation of the class: ```python class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imag})\\" ``` Objective 1. Define a function `pickle_complex(c)` that takes an instance of `ComplexNumber` and returns a tuple containing enough information to reconstruct the instance. 2. Register this function using `copyreg.pickle` to ensure that instances of `ComplexNumber` are correctly pickled and unpickled. 3. Demonstrate the functionality by creating an instance of `ComplexNumber`, pickling it, and then unpickling it to verify that you get back an equivalent instance. Constraints - **Input:** An instance of `ComplexNumber` with real and imaginary parts as floating-point numbers. - **Output:** Pickled and unpickled instances should be equivalent in terms of their attributes. Example Your solution should pass the following test case: ```python import copyreg import pickle # Register the pickling function for ComplexNumber # Write your code to register pickle_complex here # Create an instance of ComplexNumber number = ComplexNumber(3.5, -2.1) # Pickle the instance pickled_number = pickle.dumps(number) # Unpickle the instance unpickled_number = pickle.loads(pickled_number) # Check equivalence print(unpickled_number) # Expected output: ComplexNumber(3.5, -2.1) print(unpickled_number.real == number.real) # Expected output: True print(unpickled_number.imag == number.imag) # Expected output: True ``` Implement the missing parts of the code to ensure the expected outputs are produced.","solution":"import copyreg import pickle class ComplexNumber: def __init__(self, real, imag): self.real = real self.imag = imag def __repr__(self): return f\\"ComplexNumber({self.real}, {self.imag})\\" def pickle_complex(c): return ComplexNumber, (c.real, c.imag) # Register the pickling function for ComplexNumber copyreg.pickle(ComplexNumber, pickle_complex)"},{"question":"**Objective:** Implement a function to create a PyTorch tensor under deterministic settings and verify its contents based on `fill_uninitialized_memory`. **Problem Statement:** You are given a task to create a function `create_deterministic_tensor` in PyTorch that takes a shape tuple and a data type as input and returns a PyTorch tensor of the specified shape and data type. When `fill_uninitialized_memory` is set to `True`, the tensor must be filled with known values (NaN for floating point and complex types, and the maximum value for integer types). When `fill_uninitialized_memory` is set to `False`, the function should return the tensor without presetting its values. Implement the function with the following signature: ```python def create_deterministic_tensor(shape: tuple, dtype: torch.dtype, fill: bool) -> torch.Tensor: pass ``` **Input:** - `shape`: A tuple representing the shape of the tensor (e.g., (3, 4)). - `dtype`: The PyTorch data type of the tensor (e.g., `torch.float32`, `torch.int64`). - `fill`: A boolean value indicating whether `fill_uninitialized_memory` is `True` or `False`. **Output:** - A PyTorch tensor of the given shape and data type. If `fill` is `True`, the tensor should contain NaN for floating point and complex types or the maximum value for integer types. If `fill` is `False`, the tensor can contain any uninitialized values. **Constraints:** - The function should ensure deterministic operations by setting `torch.use_deterministic_algorithms(True)`. **Examples:** ```python import torch # Example 1: shape = (2, 3) dtype = torch.float32 fill = True tensor = create_deterministic_tensor(shape, dtype, fill) # tensor should contain NaN values for all elements # Example 2: shape = (2, 3) dtype = torch.int64 fill = True tensor = create_deterministic_tensor(shape, dtype, fill) # tensor should contain the maximum integer value for all elements # Example 3: shape = (2, 3) dtype = torch.float32 fill = False tensor = create_deterministic_tensor(shape, dtype, fill) # tensor can contain any uninitialized values ``` Your implementation should take into account the performance considerations mentioned in the documentation. Aim for optimal performance when `fill` is `False`. **Note:** 1. You can assume that valid input will be provided for the shapes and data types. 2. Do not use any additional libraries other than PyTorch.","solution":"import torch def create_deterministic_tensor(shape: tuple, dtype: torch.dtype, fill: bool) -> torch.Tensor: Creates a PyTorch tensor under deterministic settings. Parameters: shape: A tuple representing the shape of the tensor. dtype: The PyTorch data type of the tensor. fill: A boolean value indicating whether to fill uninitialized memory. Returns: A PyTorch tensor of the given shape and data type. torch.use_deterministic_algorithms(True) tensor = torch.empty(shape, dtype=dtype) if fill: if dtype.is_floating_point or dtype.is_complex: tensor.fill_(float(\'nan\')) else: tensor.fill_(torch.iinfo(dtype).max) return tensor"},{"question":"**Question:** You are to implement two functions `custom_encode` and `custom_decode` that utilize the `uu` module to encode and decode files. These functions should mimic the behavior of `uu.encode` and `uu.decode` but also incorporate additional functionality to handle input text directly (instead of files only) and provide detailed error handling messages. # Part 1: `custom_encode` Implement the function `custom_encode` which takes the following parameters: - `input_data` (str or bytes): The data to be encoded. This can be a string (text) or bytes (binary data). - `is_file` (bool): If True, `input_data` is treated as a filename, otherwise it is treated as direct data content. - `out_file` (str): The name of the file where the encoded data should be written. - `name` (str, optional): The name to be embedded in the uuencode header. Defaults to \'-\'. - `mode` (int, optional): The mode to set for the resulting file. Defaults to 0o666. - `backtick` (bool, optional): If True, zeros are represented by \'`\' instead of spaces. Defaults to False. Returns: - None # Part 2: `custom_decode` Implement the function `custom_decode` which takes the following parameters: - `input_data` (str): The name of the file to decode or directly decode data content. - `is_file` (bool): If True, `input_data` is treated as a filename, otherwise it is treated as direct data content. - `out_file` (str): The name of the file to write the decoded data. If None, returns the decoded data. - `mode` (int, optional): The mode to use when creating the file. Defaults to the mode in the uuencode header. - `quiet` (bool, optional): If True, suppresses warning messages. Defaults to False. Returns: - If `out_file` is None, returns the decoded data, otherwise returns None. # Constraints: - You should handle all possible exceptions and provide meaningful error messages. - Assume the input data will not exceed 1MB. - Ensure that all file operations are properly managed (i.e., files are closed after operations). # Example Usage: ```python # Encoding text data custom_encode(\\"Hello, World!\\", False, \\"encoded.txt\\") # Encoding from a file custom_encode(\\"example.bin\\", True, \\"encoded.txt\\") # Decoding to a file custom_decode(\\"encoded.txt\\", True, \\"output.bin\\") # Decoding from a direct uuencoded string encoded_string = \\"begin 644 example.txtn...endn\\" decoded_data = custom_decode(encoded_string, False, None) ``` Implement the `custom_encode` and `custom_decode` functions to fulfill the requirements described above.","solution":"import uu import io def custom_encode(input_data, is_file, out_file, name=\'-\', mode=0o666, backtick=False): Encode input data using uuencode and save to a file. :param input_data: str or bytes, the data to be encoded. :param is_file: bool, if True, input_data is treated as a filename, otherwise as direct data content. :param out_file: str, the name of the file where encoded data should be written. :param name: str, optional, the name to be embedded in the uuencode header. Defaults to \'-\'. :param mode: int, optional, the mode to set for the resulting file. Defaults to 0o666. :param backtick: bool, optional, if True, zeros are represented by \'`\' instead of spaces. Defaults to False. try: if is_file: with open(input_data, \'rb\') as input_file: with open(out_file, \'wb\') as output_file: uu.encode(input_file, output_file, name, mode, backtick=backtick) else: if isinstance(input_data, str): input_bytes = input_data.encode() else: input_bytes = input_data input_io = io.BytesIO(input_bytes) with open(out_file, \'wb\') as output_file: uu.encode(input_io, output_file, name, mode, backtick=backtick) except Exception as e: raise RuntimeError(f\\"Error during encoding: {str(e)}\\") def custom_decode(input_data, is_file, out_file=None, mode=None, quiet=False): Decode uuencoded input data from a file or direct content. :param input_data: str, the name of the file to decode or direct uuencoded data content. :param is_file: bool, if True, input_data is treated as a filename, otherwise as direct data content. :param out_file: str, the name of the file to write the decoded data. If None, returns the decoded data. :param mode: int, optional, the mode to use when creating the file. Defaults to the mode in the uuencode header. :param quiet: bool, optional, if True, suppresses warning messages. Defaults to False. :return: str or None, decoded data if out_file is None, otherwise None. try: if is_file: with open(input_data, \'rb\') as input_file: if out_file: with open(out_file, \'wb\') as output_file: uu.decode(input_file, output_file, mode, quiet=quiet) else: decoded_bytes = bytearray() output_io = io.BytesIO(decoded_bytes) uu.decode(input_file, output_io, mode, quiet=quiet) return output_io.getvalue().decode() else: input_io = io.StringIO(input_data) if out_file: with open(out_file, \'wb\') as output_file: uu.decode(input_io, output_file, mode, quiet=quiet) else: decoded_bytes = bytearray() output_io = io.BytesIO(decoded_bytes) uu.decode(input_io, output_io, mode, quiet=quiet) return output_io.getvalue().decode() except Exception as e: raise RuntimeError(f\\"Error during decoding: {str(e)}\\")"},{"question":"**Question: Implement a Custom Thread-Safe Context Manager using `contextvars` Module** In this task, you are required to implement a custom thread-safe context manager using the `contextvars` module of Python. # Objective Your task is to create and manage context variables through a custom context manager. This context manager will ensure that changes to variables within a context are thread-safe and reflect only within the scope they were applied. # Requirements 1. **Context Manager Class:** - The class should be named `CustomContextManager`. - Initialize the class with a dictionary of context variables with their default values. 2. **Methods:** - `__enter__()`: This method should set up the context variables and return the context manager instance. - `__exit__()`: This method should clean up the context and restore any previous context values. - `get_value(var_name)`: This method should return the current value of the specified context variable. - `set_value(var_name, value)`: This method should set a new value for the specified context variable. # Input and Output - **Input:** - A dictionary for initializing the context manager. - A sequence of `get_value` and `set_value` calls within the context. - **Output:** - The value of a context variable upon `get_value` call. - None for `set_value` call. # Constraints - Assume all context variable names provided are strings. - The context variable values can be any valid Python object. - You need to manage the contexts and their updates manually, ensuring thread-safety and restoration of contexts. # Example Usage ```python from contextvars import ContextVar import threading class CustomContextManager: def __init__(self, initial_vars): self.tokens = {} self.context_vars = {name: ContextVar(name, default=val) for name, val in initial_vars.items()} self.context = PyContext_New() def __enter__(self): PyContext_Enter(self.context) return self def __exit__(self, exc_type, exc_value, traceback): PyContext_Exit(self.context) self.tokens.clear() def get_value(self, var_name): var = self.context_vars.get(var_name) if var: value = var.get(None) return value raise KeyError(f\\"Variable \'{var_name}\' not found\\") def set_value(self, var_name, value): var = self.context_vars.setdefault(var_name, ContextVar(var_name)) token = var.set(value) self.tokens[var_name] = token # Example threading usage def thread_function(name, ctx_mgr): with ctx_mgr: print(f\\"Thread {name} initial value:\\", ctx_mgr.get_value(\'var1\')) ctx_mgr.set_value(\'var1\', name) print(f\\"Thread {name} set value:\\", ctx_mgr.get_value(\'var1\')) if __name__ == \\"__main__\\": initial_context = {\'var1\': \'default1\'} context_mgr = CustomContextManager(initial_context) t1 = threading.Thread(target=thread_function, args=(1, context_mgr)) t2 = threading.Thread(target=thread_function, args=(2, context_mgr)) t1.start() t2.start() t1.join() t2.join() ``` # Note - Ensure that context variables are managed correctly and show independent values across different threads. - Handle any possible exceptions and ensure context cleanup.","solution":"from contextvars import ContextVar, Token class CustomContextManager: def __init__(self, initial_vars): self.tokens = {} self.context_vars = {name: ContextVar(name, default=val) for name, val in initial_vars.items()} def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): for var_name, token in self.tokens.items(): self.context_vars[var_name].reset(token) self.tokens.clear() def get_value(self, var_name): var = self.context_vars.get(var_name) if var: return var.get() raise KeyError(f\\"Variable \'{var_name}\' not found\\") def set_value(self, var_name, value): var = self.context_vars.setdefault(var_name, ContextVar(var_name)) token = var.set(value) self.tokens[var_name] = token"},{"question":"**Context:** You\'re given a dataset representing some random values. Your task is to create a visually engaging boxplot using Seaborn, demonstrating your understanding of different styling, themes, and customization features available in Seaborn. **Requirements:** 1. **Data Generation:** - Generate a dataset of random normal variables with shape (20, 6). 2. **Plot Customization:** - Create a function, `custom_boxplot(data)`, that takes a numpy array `data` as input. - The function should: 1. Set the Seaborn theme to `whitegrid`. 2. Plot a boxplot using the provided data array. 3. Remove the left and top spines of the plot. 4. Temporarily change the style to `darkgrid` and within this context, add and plot a sine wave using an additional subplot. 5. Change the overall context to `poster` and increase the font scale to `1.5`. 6. Finally, save the generated plots as `custom_boxplot.png`. **Function Signature:** ```python def custom_boxplot(data: np.ndarray) -> None: pass ``` **Expected Output:** - An image file `custom_boxplot.png` containing a boxplot with `whitegrid` style, spines removed, a temporary sub-plot with `darkgrid` style sinewave, and context scaled to `poster`. **Data Constraints:** - The input array will always be of shape (20, 6) with normally distributed values. **Additional Considerations:** - Ensure your plot annotation and axes are clearly visible and suitably scaled according to the context changes. ```python # Generating the dataset import numpy as np data = np.random.normal(size=(20, 6)) # Implement the function def custom_boxplot(data: np.ndarray) -> None: import seaborn as sns import matplotlib.pyplot as plt sns.set_style(\\"whitegrid\\") plt.figure(figsize=(10, 6)) sns.boxplot(data=data) sns.despine(left=True) with sns.axes_style(\\"darkgrid\\"): plt.subplot(211) x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (10 + 2 - i)) sns.set_context(\\"poster\\", font_scale=1.5) plt.tight_layout() plt.savefig(\\"custom_boxplot.png\\") plt.show() # Run the function with the generated data custom_boxplot(data) ``` **Testing Criteria:** - Verify the generation of `custom_boxplot.png` with correct styling, context, and despine operations. - Inspect the visual elements to ensure alignment with the specifications.","solution":"import numpy as np import matplotlib.pyplot as plt import seaborn as sns def custom_boxplot(data: np.ndarray) -> None: sns.set_style(\\"whitegrid\\") plt.figure(figsize=(10, 6)) # Plot the boxplot sns.boxplot(data=data) # Remove the left and top spines of the plot sns.despine(left=True) # Temporarily change the style to \'darkgrid\' and add a sine wave subplot with sns.axes_style(\\"darkgrid\\"): plt.subplot(211) x = np.linspace(0, 14, 100) for i in range(1, 11): plt.plot(x, np.sin(x + i * .5) * (10 + 2 - i)) # Change the overall context to \'poster\' and increase the font scale to \'1.5\' sns.set_context(\\"poster\\", font_scale=1.5) # Adjust layout and save the plot as \'custom_boxplot.png\' plt.tight_layout() plt.savefig(\\"custom_boxplot.png\\") plt.show()"},{"question":"# Question: Titanic Dataset Visualization using Seaborn Boxplot You are provided with the Titanic dataset, which contains various details about passengers on the Titanic, such as their age, class, fare, deck, and whether they survived. Your task is to create a comprehensive boxplot visualization using the Seaborn library that meets the following requirements: 1. Load the Titanic dataset using Seaborn. 2. Create a vertical boxplot displaying the distribution of passengers\' ages grouped by their class. 3. Add a secondary grouping by whether the passengers survived. 4. Customize the following aspects of the boxplot: - Draw the boxes with a solid color chosen by you. - Draw the whiskers to cover the full range of the data. - Set the width of the boxes to 0.5. - Add a title and labels for the x and y axes. - Make any outliers (fliers) visible as red \'o\' markers. Here\'s the expected input and output formats: **Input**: No input is required from the user. Use the example code provided to load the dataset. **Output**: The output should be a boxplot generated by Seaborn that fulfills all the specified requirements. **Example Code to Get Started**: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Create your boxplot here sns.set_theme(style=\\"whitegrid\\") # Your code to create the customized boxplot sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", whis=(0, 100), # Cover the full range of the data width=0.5 # Set the width of the boxes ) plt.title(\\"Distribution of Passengers\' Age by Class and Survival\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\\"Survived\\") plt.show() ``` You should customize the above code to fulfill all the requirements mentioned above. **Constraints:** - The solution must use the Seaborn package. - The customization should be done using the parameters of `sns.boxplot()` and any additional Matplotlib parameters. **Performance Requirements:** - The solution should execute quickly and render the plot without errors.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_boxplot(): Creates a customized boxplot for the Titanic dataset displaying the distribution of passengers\' ages grouped by their class and survival status. # Load the Titanic dataset titanic = sns.load_dataset(\\"titanic\\") # Set the theme for the plot sns.set_theme(style=\\"whitegrid\\") # Create the boxplot with the customizations sns.boxplot( data=titanic, x=\\"class\\", y=\\"age\\", hue=\\"survived\\", palette=[\\"lightblue\\", \\"lightgreen\\"], # Setting solid colors for the boxes whis=(0, 100), # Whiskers cover the full range of the data width=0.5 # Set the width of the boxes ) # Customizing the outliers (fliers) appearance plt.setp(plt.gca().artists, edgecolor=\'black\', facecolor=\'white\') plt.setp(plt.gca().lines, color=\'black\') plt.setp(plt.gca().collections, alpha=1, facecolor=\'none\', edgecolor=\'black\') # Putting red \'o\' markers on outliers for flier in plt.gca().findobj(match=plt.Line2D): if flier.get_marker() == \'o\': flier.set(color=\'red\', markersize=5) # Adding a title and axis labels plt.title(\\"Distribution of Passengers\' Age by Class and Survival\\") plt.xlabel(\\"Class\\") plt.ylabel(\\"Age\\") plt.legend(title=\\"Survived\\") # Display the plot plt.show()"},{"question":"Implementing a Validation and Data Processing Pipeline using Scikit-learn Utilities Objective You need to implement a function that validates, normalizes, and processes a dataset before performing a simple randomized computation. This task will assess your understanding of input validation, handling random states, and efficient linear algebra operations using Scikit-learn utilities. Problem Statement Write a function `process_and_compute` that takes the following inputs: 1. `X` (ndarray): A 2D numpy array representing the dataset. 2. `seed` (int): A seed for the random number generator. 3. `components` (int): The number of components to extract using Singular Value Decomposition (SVD). The function should: 1. Ensure that `X` is a 2D numpy array with no NaNs or Infinities. 2. Use the provided `seed` to initialize a random state. 3. Normalize the rows of `X` to have unit L2 norm. 4. Compute the randomized SVD of `X` to extract the specified number of components. 5. Return the resulting `U` matrix from the SVD. Function Signature ```python import numpy as np from sklearn.utils import check_array, check_random_state from sklearn.utils.extmath import randomized_svd from sklearn.utils.sparsefuncs_fast import inplace_csr_row_normalize_l2 def process_and_compute(X: np.ndarray, seed: int, components: int) -> np.ndarray: # Your code here pass ``` Constraints and Requirements - You must use `check_array` to validate the input `X`. - You must use `check_random_state` to handle the random state initialization with the provided `seed`. - Use `inplace_csr_row_normalize_l2` or an equivalent method to normalize rows of `X`. - Use `randomized_svd` to perform SVD on `X`. - The function should return the `U` matrix from the SVD. - You may assume that `X` has more rows than the number of components specified. Example Usage ```python X = np.array([[1, 2], [3, 4], [5, 6]]) seed = 42 components = 2 U = process_and_compute(X, seed, components) print(U) ``` # Explanation The example should produce the `U` matrix resulting from the SVD of the input array `X`, normalized by rows, and using a random state initialized with the given seed.","solution":"import numpy as np from sklearn.utils import check_array, check_random_state from sklearn.utils.extmath import randomized_svd from sklearn.preprocessing import normalize def process_and_compute(X: np.ndarray, seed: int, components: int) -> np.ndarray: # Ensure that X is a 2D array and has no NaNs or Infinities X = check_array(X, ensure_2d=True, allow_nd=False, force_all_finite=True) # Initialize random state random_state = check_random_state(seed) # Normalize the rows of X to have unit L2 norm X_normalized = normalize(X, norm=\'l2\', axis=1) # Compute the randomized SVD U, Sigma, VT = randomized_svd(X_normalized, n_components=components, random_state=random_state) return U"},{"question":"# Understanding and Implementing Coroutines in Python Objective: To assess the understanding and implementation of coroutines in Python using both high-level `async` and `await` syntax and the lower-level functions provided in the Python C API for creating and managing coroutine objects. Problem Statement: 1. Write a function `async def fetch_data(n: int) -> int` that is a coroutine simulating data fetching operation which takes `n` seconds to complete and then returns `n`. Use `await asyncio.sleep(n)`. 2. Write an equivalent function `def fetch_data_new(frame, name, qualname):` that uses `PyCoro_New` to create a new coroutine object based on the given frame, name, and qualname. 3. Implement a function `async def process_data(lst: List[int]) -> int` that: - Fetches data for each element in the list using `fetch_data`. - Returns the sum of all fetched data. 4. Write a function `def is_coroutine(obj) -> bool` which uses `PyCoro_CheckExact` to check if a given object is a `PyCoro_Type`. Constraints: 1. You cannot use any third-party library except for the standard library. 2. You must use the provided C API functions in parts of your implementation. Input Format: - For `fetch_data`, an integer number `n`. - For `fetch_data_new`, a frame object, name, and qualname (in practical situations, you might use function introspection to generate these). - For `process_data`, a list of integers `lst`. - For `is_coroutine`, a Python object `obj`. Output Format: - `fetch_data` returns an integer after `n` seconds. - `fetch_data_new` returns a coroutine object. - `process_data` returns an integer which is the sum. - `is_coroutine` returns a boolean. Example: ```python import asyncio # Example usage of fetch_data result = await fetch_data(5) # This should return 5 after waiting 5 seconds # Example usage of process_data result = await process_data([1, 2, 3]) # This should return 6 after waiting for 1+2+3 seconds in total # Example usage of is_coroutine coroutine_obj = fetch_data(1) print(is_coroutine(coroutine_obj)) # This should print True ``` Performance Requirements: Your functions should be efficient and utilize the asynchronous functionalities properly to avoid blocking operations. Notes: - Detailed implementation of function `fetch_data_new` requires understanding of coroutine internals and usage of the Python C API. - Focus on implementing efficient and non-blocking asynchronous operations.","solution":"import asyncio from typing import List # Task 1 - Coroutine using async and await async def fetch_data(n: int) -> int: Coroutine that simulates data fetching operation which takes n seconds to complete and then returns n. await asyncio.sleep(n) return n # Task 2 - Coroutine using PyCoro_New (Example not directly implementable here) # This is just a placeholder to demonstrate the concept as Python\'s C API is not accessible in pure Python. def fetch_data_new(frame, name, qualname): Function that would use PyCoro_New to create a new coroutine object. Placeholder for actual implementation. pass # Cannot directly implement as Python C API cannot be used in pure Python # Task 3 - Processing data in a list using the fetch_data coroutine async def process_data(lst: List[int]) -> int: Coroutine that fetches data for each integer in the given list and returns the sum. results = await asyncio.gather(*(fetch_data(n) for n in lst)) return sum(results) # Task 4 - Checking if an object is a coroutine def is_coroutine(obj) -> bool: Function to check if a given object is a coroutine. return asyncio.iscoroutine(obj)"},{"question":"# Custom Iterator Assessment Objective Implement custom sequence iterator and callable iterator classes in Python, demonstrating comprehension of iterator protocols and advanced Python concepts. Description 1. **Custom Sequence Iterator** Create a class `CustomSequenceIterator` that works similarly to the `PySeqIter_New` function. It should accept a sequence during initialization and iterate over the sequence until an `IndexError` is raised. 2. **Custom Callable Iterator** Create a class `CustomCallableIterator` that works similarly to the `PyCallIter_New` function. It should accept a callable and a sentinel value during initialization, and iterate by repeatedly calling the callable until the sentinel value is returned. Requirements - Implement the `CustomSequenceIterator` class with the following functionalities: - Initialization: Accept a sequence and prepare for iteration. - `__iter__` method: Return the iterator object itself. - `__next__` method: Return the next item in the sequence or raise `StopIteration` if the sequence ends. - Implement the `CustomCallableIterator` class with the following functionalities: - Initialization: Accept a callable and a sentinel value. - `__iter__` method: Return the iterator object itself. - `__next__` method: Call the callable object and return the result, or raise `StopIteration` if the result equals the sentinel value. Input and Output Formats ```python # Example for CustomSequenceIterator seq_iter = CustomSequenceIterator([1, 2, 3, 4]) # Iterating over CustomSequenceIterator for item in seq_iter: print(item) # Output: 1, 2, 3, 4 # Example for CustomCallableIterator def counter(): counter.value += 1 return counter.value counter.value = 0 call_iter = CustomCallableIterator(counter, 5) # Iterating over CustomCallableIterator for item in call_iter: print(item) # Output: 1, 2, 3, 4 ``` Constraints - The sequence for `CustomSequenceIterator` can be any Python sequence (e.g., list, tuple, string). - The callable for `CustomCallableIterator` should be a function that accepts no arguments and returns the next value in the iteration. Performance Requirements - Ensure that the implementation handles large sequences efficiently. - The implementation should handle any callable that can be followed by a sentinel. ```python class CustomSequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration result = self.sequence[self.index] self.index += 1 return result class CustomCallableIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result ```","solution":"class CustomSequenceIterator: def __init__(self, sequence): self.sequence = sequence self.index = 0 def __iter__(self): return self def __next__(self): if self.index >= len(self.sequence): raise StopIteration result = self.sequence[self.index] self.index += 1 return result class CustomCallableIterator: def __init__(self, callable, sentinel): self.callable = callable self.sentinel = sentinel def __iter__(self): return self def __next__(self): result = self.callable() if result == self.sentinel: raise StopIteration return result"},{"question":"# Python Development Mode - File Management and Error Handling You are tasked with writing a Python script that processes a text file and demonstrates proper resource management and error handling. Your script should count the number of lines in a text file while ensuring that all resources are properly closed, even in the case of exceptions. Requirements 1. The script should be able to: - Open a text file specified by the user. - Count and print the number of lines in the text file. - Handle any exceptions that may occur during file operations. 2. Use a context manager to ensure the file is properly closed after operations are complete. 3. Additionally, simulate an artificial error (e.g., by raising an exception within the file processing block) and demonstrate that the file is still closed correctly. 4. Include comments in your script explaining how Python Development Mode helps in identifying and correcting such issues. Input - A text file specified via command line arguments. - Example: `python script.py example.txt` Output - The number of lines in the text file. - Properly managed file closure, demonstrated by the absence of `ResourceWarning` when the script is executed in Python Development Mode. Constraints - You may assume the text file exists and is accessible. - You must handle all exceptions related to file I/O operations. Performance Requirements - The script should handle large files efficiently, but there are no strict performance constraints for this assessment. # Example Given a sample text file `example.txt` with content: ``` line 1 line 2 line 3 ``` Running the script: ``` python script.py example.txt ``` Expected Output: ``` 3 ``` **Note**: Ensure your script does not emit any `ResourceWarning` when run with the `-X dev` flag. Additional Guidance - Review the provided documentation on Python Development Mode to understand how to enable it and what kind of warnings it can help detect. - Make use of try-except blocks and the `with` statement to manage resources effectively.","solution":"import sys def count_lines_in_file(file_path): Counts the number of lines in a text file specified by the file_path. line_count = 0 error_occurred = False try: with open(file_path, \'r\') as file: for line in file: line_count += 1 # Simulate an artificial error for demonstration purposes if line == \\"line 2n\\": raise ValueError(\\"Artificial error for demonstration.\\") except (IOError, ValueError) as e: error_occurred = True print(f\\"An error occurred: {e}\\") finally: # Although the \'with\' statement handles closing the file, the \'finally\' block # ensures any additional cleanup if needed. if not error_occurred: print(line_count) if __name__ == \\"__main__\\": if len(sys.argv) != 2: print(\\"Usage: python script.py <file_path>\\") else: count_lines_in_file(sys.argv[1])"},{"question":"**Coding Assessment Question** **Objective:** To assess the understanding of the `marshal` module for internal serialization and deserialization of Python objects in a controlled environment. **Question:** You are required to write two functions using the `marshal` module to serialize a Python dictionary to a binary file and read it back. Your functions should handle the dictionary containing various supported Python types. Additionally, you should handle cases where unsupported types are to be serialized. **Function Specifications:** 1. `save_dict_to_file(data: dict, file_path: str) -> None`: - **Input**: - `data`: A dictionary containing supported Python types (booleans, integers, floating point numbers, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries). - `file_path`: The path to the binary file where the dictionary will be saved. - **Output**: None. The function should write the serialized data to the specified file. - **Constraints**: If the dictionary contains unsupported types, raise a `ValueError` exception. 2. `load_dict_from_file(file_path: str) -> dict`: - **Input**: - `file_path`: The path to the binary file from which the dictionary will be read. - **Output**: A dictionary containing the deserialized data. - **Constraints**: If the file does not exist or the data cannot be deserialized, raise an appropriate exception. **Example Usage:** ```python sample_data = { \\"name\\": \\"Alice\\", \\"age\\": 30, \\"scores\\": [95, 85, 100], \\"is_student\\": False, \\"details\\": { \\"height\\": 167.6, \\"weight\\": 68.5, \\"hobbies\\": {\\"reading\\", \\"swimming\\"} } } file_path = \'data.marshal\' # Save the dictionary to a file save_dict_to_file(sample_data, file_path) # Load the dictionary from the file loaded_data = load_dict_from_file(file_path) assert loaded_data == sample_data ``` **Notes:** - Ensure you handle exceptions gracefully, providing clear error messages. - Do not use the `pickle` module or any other serialization methods other than `marshal`. - Focus on using `marshal.dump` and `marshal.load` for writing to and reading from a file, respectively. - Consider the versioning constraints where necessary. ```python import marshal def save_dict_to_file(data: dict, file_path: str) -> None: Save the dictionary data to a binary file using marshal. :param data: Dictionary containing supported Python types. :param file_path: The path to the binary file. :raises ValueError: If the data contains unsupported types. try: with open(file_path, \'wb\') as file: marshal.dump(data, file) except ValueError as e: raise ValueError(\\"Unsupported type detected in data.\\") from e def load_dict_from_file(file_path: str) -> dict: Load and return the dictionary data from a binary file using marshal. :param file_path: The path to the binary file. :return: Dictionary containing the deserialized data. :raises Any: If the file does not exist or deserialization fails. try: with open(file_path, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError) as e: raise Exception(\\"Failed to load data from file.\\") from e ```","solution":"import marshal def save_dict_to_file(data: dict, file_path: str) -> None: Save the dictionary data to a binary file using marshal. :param data: Dictionary containing supported Python types. :param file_path: The path to the binary file. :raises ValueError: If the data contains unsupported types. try: with open(file_path, \'wb\') as file: marshal.dump(data, file) except ValueError as e: raise ValueError(\\"Unsupported type detected in data.\\") from e def load_dict_from_file(file_path: str) -> dict: Load and return the dictionary data from a binary file using marshal. :param file_path: The path to the binary file. :return: Dictionary containing the deserialized data. :raises Any: If the file does not exist or deserialization fails. try: with open(file_path, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError) as e: raise Exception(\\"Failed to load data from file.\\") from e"},{"question":"# Complex Data Processing and Error Handling Task You are given a task to process a complex dataset involving multiple operations including iteration, condition checking, error handling, context management, and optionally applying pattern matching for enhanced data classification. Task Description: Implement a function `process_data(data: list) -> dict` that processes a list of data elements. Each element in the list can be a string, integer, float, or a complex nested list of these types. The function must: 1. **Iterate** over each element in the list: - If the element is a string: - Use an **if-else** statement to check: - If the string can be converted to an integer, convert it and add to integer processing (described later). - Else if it is convertible to a float, convert it and add to float processing (described later). - Otherwise, count its length and store it. - If the element is an integer or can be successfully converted to an integer: - Add the element to an integer total using a **for loop**. - If the element is a float or can be successfully converted to a float: - Add the element to a float total using a **for loop**. - If the element is a list: - Recursively process the list. 2. **Use a `try...except...else...finally` statement** to handle any conversion errors: - In the `finally` clause, ensure some cleanup or logging occurs, e.g., print a message \\"Element processed\\". 3. Use a **context manager** (`with` statement) to manage a resource, such as writing processed results to a file called `results.txt`. 4. The **function should return** a dictionary with the counts and totals of processed elements: - \'integer_total\': Sum of all integers - \'float_total\': Sum of all floats - \'string_lengths\': List of lengths of unrecoverable string elements - \'list_counts\': Count of all nested lists processed including the main list. Constraints: - Each element in data or nested in data lists is expected to be a string, integer, float, or list of these types. - Performance is not strictly critical, but solutions should avoid excessively deep recursion or significant inefficiencies. Example Input: ```python data = [\\"123\\", 456, \\"78.90\\", \\"abc\\", 3.14, [\\"1.23\\", \\"45\\", \\"xyz\\", [10, 20]]] ``` Example Output: ```python { \'integer_total\': 531, \'float_total\': 83.27, \'string_lengths\': [3, 3], \'list_counts\': 3 } ``` You should ensure your function is well-documented, follows best practices, and manages all edge cases effectively. **Hint**: You may want to use the `try` statement in conjunction with `with` to manage any potential errors during file operations.","solution":"def process_data(data): Processes a list of data elements which can be strings, integers, floats, or nested lists of these types. Returns a dictionary with the totals and counts of processed elements. Parameters: data (list): List of data elements to process. Returns: dict: Dictionary with integer total, float total, unrecoverable string lengths and list counts. results = { \'integer_total\': 0, \'float_total\': 0.0, \'string_lengths\': [], \'list_counts\': 0 } def process_element(element): try: if isinstance(element, str): try: if \'.\' in element: results[\'float_total\'] += float(element) else: results[\'integer_total\'] += int(element) except ValueError: results[\'string_lengths\'].append(len(element)) print(f\\"Failed to convert \'{element}\' to int or float.\\") elif isinstance(element, int): results[\'integer_total\'] += element elif isinstance(element, float): results[\'float_total\'] += element elif isinstance(element, list): results[\'list_counts\'] += 1 for item in element: process_element(item) else: print(f\\"Unsupported data type: {type(element)}\\") except Exception as e: print(f\\"An error occurred: {e}\\") finally: print(\\"Element processed\\") with open(\'results.txt\', \'w\') as file: for element in data: process_element(element) file.write(str(results)) return results"},{"question":"**Custom XML-RPC Client Implementation** You are tasked with creating a custom XML-RPC client to interact with a predefined XML-RPC server. Your implementation should encapsulate the following functionalities: 1. **Initialize the XML-RPC Connection**: Use the provided server URL, supporting HTTP Basic Authentication and custom headers. 2. **Batch Processing of RPC Calls**: Utilize the MultiCall object to batch multiple method calls into a single RPC request. 3. **Error Handling**: Implement error handling to gracefully manage `Fault` and `ProtocolError` scenarios. 4. **Handling Advanced Data Types**: Send and receive `DateTime` and `Binary` data types correctly. **Specifications:** 1. **Class Initialization**: - The class should be initialized with `url`, `username`, `password`, and `headers`. ```python class CustomXMLRPCClient: def __init__(self, url: str, username: str = None, password: str = None, headers: list = []): pass ``` 2. **Batch Method Calls**: - Implement a method `batch_call` to accept multiple method names and their arguments, then return responses from a single XML-RPC call. ```python def batch_call(self, method_calls: list) -> list: pass ``` - `method_calls` is expected to be a list of tuples, where each tuple contains the method name (string) and its parameters (tuple). 3. **Handling Specialized Data Types**: - Implement methods to send and receive `DateTime` and `Binary` types properly. ```python def send_date(self, date: datetime.datetime) -> xmlrpc.client.DateTime: pass def send_binary(self, binary_data: bytes) -> xmlrpc.client.Binary: pass def receive_date(self, date: xmlrpc.client.DateTime) -> datetime.datetime: pass def receive_binary(self, binary: xmlrpc.client.Binary) -> bytes: pass ``` 4. **Error Handling**: - Implement a method to handle `Fault` and `ProtocolError` exceptions, logging the relevant error information. ```python def handle_error(self, error): pass ``` **Example Usage**: ```python import datetime if __name__ == \\"__main__\\": client = CustomXMLRPCClient( url=\\"http://localhost:8000/\\", username=\\"user\\", password=\\"pass\\", headers=[(\'Custom-Header\', \'value\')] ) # Call multiple methods in a single batch request methods = [ (\\"method1\\", (param1, param2)), (\\"method2\\", (param3, param4)), ] results = client.batch_call(methods) print(results) # Send and receive DateTime and Binary types current_date = datetime.datetime.now() sent_date = client.send_date(current_date) received_date = client.receive_date(sent_date) print(received_date) binary_data = b\\"This is a test binary data\\" sent_binary = client.send_binary(binary_data) received_binary = client.receive_binary(sent_binary) print(received_binary) ``` **Constraints**: - The server URL will always be valid but may require HTTP Basic Authentication. - Implement appropriate error handling for any XML-RPC faults or protocol errors. - Use appropriate XML-RPC data types as specified in xmlrpc.client documentation. - The batch method should ensure that the order of responses matches the order of requests. **Performance Requirements**: - Ensure efficient handling and processing of multiple method calls. - Batch processing should minimize network latency by combining requests when possible.","solution":"import xmlrpc.client import datetime class CustomXMLRPCClient: def __init__(self, url: str, username: str = None, password: str = None, headers: list = []): self.url = url self.headers = headers self.server = xmlrpc.client.ServerProxy(url, headers=headers) if username and password: self.server = xmlrpc.client.ServerProxy(url, headers=headers, transport=xmlrpc.client.SafeTransport(use_datetime=True)) self.server._ServerProxy__transport.user = username self.server._ServerProxy__transport.password = password def batch_call(self, method_calls: list) -> list: multi_call = xmlrpc.client.MultiCall(self.server) for method_name, params in method_calls: getattr(multi_call, method_name)(*params) try: return list(multi_call()) except xmlrpc.client.Fault as fault: self.handle_error(fault) except xmlrpc.client.ProtocolError as error: self.handle_error(error) def send_date(self, date: datetime.datetime) -> xmlrpc.client.DateTime: return xmlrpc.client.DateTime(date) def send_binary(self, binary_data: bytes) -> xmlrpc.client.Binary: return xmlrpc.client.Binary(binary_data) def receive_date(self, date: xmlrpc.client.DateTime) -> datetime.datetime: return datetime.datetime.strptime(date.value, \\"%Y%m%dT%H:%M:%S\\") def receive_binary(self, binary: xmlrpc.client.Binary) -> bytes: return binary.data def handle_error(self, error): if isinstance(error, xmlrpc.client.Fault): print(f\\"XML-RPC Fault: {error.faultString} (code: {error.faultCode})\\") elif isinstance(error, xmlrpc.client.ProtocolError): print(f\\"Protocol Error: {error.errmsg} (URL: {error.url}, code: {error.errcode})\\") else: print(\\"Unknown Error:\\", error)"},{"question":"# Custom URL Opener Using `urllib.request` **Objective:** Create a Python script that defines a custom URL opener using the `urllib.request` module. The custom opener should: 1. Handle HTTP basic authentication to access a protected API. 2. Manage cookies to maintain session information. 3. Handle HTTP redirects properly. 4. Retrieve and display specific information from the API response. **Requirements:** 1. **HTTP Authentication**: You need to use HTTP basic authentication to access the API. The username and password should be passed as parameters when initializing the custom opener. 2. **Cookie Management**: Use `HTTPCookieProcessor` to manage cookies. 3. **Redirection Handling**: Ensure that the opener handles HTTP redirections (specifically 301, 302, and 307) correctly. 4. **API Request**: Make a GET request to the provided API URL and retrieve JSON data. 5. **Display Data**: Parse the JSON response and print out the specified information (e.g., a list of user names if the API returns a list of users). **Input:** - `api_url` (str): The URL of the API to access. - `username` (str): The username for HTTP basic authentication. - `password` (str): The password for HTTP basic authentication. - `key` (str): The JSON key to retrieve from the API response. **Output:** - Print the values associated with the provided JSON key in the API response. # Example **Function Signature:** ```python def custom_url_opener(api_url: str, username: str, password: str, key: str): pass ``` # Constraints - The function should handle potential errors such as network issues, authentication failure, and invalid JSON responses. - Assume the API always returns a JSON response. **Example Usage:** ```python api_url = \\"https://api.example.com/data\\" username = \\"user\\" password = \\"pass\\" key = \\"usernames\\" custom_url_opener(api_url, username, password, key) ``` **Expected Output:** ``` [\'alice\', \'bob\', \'carol\'] ``` **Hints:** - Use `HTTPBasicAuthHandler` for handling authentication. - Use `HTTPCookieProcessor` to handle cookies. - Implement custom error handling to catch and report network and JSON parsing errors.","solution":"import urllib.request import urllib.error import json import http.cookiejar def custom_url_opener(api_url: str, username: str, password: str, key: str): Custom URL opener handling HTTP basic authentication, cookies, and redirects. Parameters: api_url (str): The URL of the API to access. username (str): The username for HTTP basic authentication. password (str): The password for HTTP basic authentication. key (str): The JSON key to retrieve from the API response. Returns: None. Prints the values associated with the provided JSON key in the API response. # Create a password manager password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, api_url, username, password) # Create an authentication handler handler = urllib.request.HTTPBasicAuthHandler(password_mgr) # Create a cookie jar to handle cookies cookie_jar = http.cookiejar.CookieJar() cookie_handler = urllib.request.HTTPCookieProcessor(cookie_jar) # Create a custom opener opener = urllib.request.build_opener(handler, cookie_handler, urllib.request.HTTPRedirectHandler()) # Install the custom opener globally so it can be used with urlopen urllib.request.install_opener(opener) try: # Open the URL using the custom opener response = urllib.request.urlopen(api_url) data = response.read().decode(\'utf-8\') # Parse the JSON response json_data = json.loads(data) # Retrieve and print the specified information if key in json_data: print(json_data[key]) else: print(f\\"Key \'{key}\' not found in the response\\") except urllib.error.HTTPError as e: print(f\'HTTP Error: {e.code} - {e.reason}\') except urllib.error.URLError as e: print(f\'URL Error: {e.reason}\') except json.JSONDecodeError as e: print(f\'Error decoding JSON: {str(e)}\')"},{"question":"# Audit Event Logging **Objective:** Write a Python function that logs specific audit events using Python\'s auditing hooks and logging mechanism. **Task:** 1. Implement a function `initialize_audit_logger(log_file: str)` that: - Registers an audit hook that logs occurrences of the following audit events to the specified log file: - `socket.connect` - `os.system` - `exec` - Ensures that each log entry includes the audit event name and its arguments. 2. Additionally, implement a function `trigger_sample_events()` that: - Creates a socket connection. - Executes a system command. - Executes Python code dynamically using `exec`. **Expected Input and Output:** - The function `initialize_audit_logger` takes a single argument `log_file`, which is the path to the log file where the events will be logged. - The function `trigger_sample_events` does not take any arguments and is used to generate sample events for logging. **Constraints:** - The implementation should handle cases where audit events might not have all arguments. - The log entries should be in plain text, formatted with the event name first followed by the arguments. **Performance Requirements:** - The logging should be efficient and not introduce significant delay in the execution of the audit events. - The audit hook should be removed or disabled after logging to prevent unnecessary logging of further events. Here is a template to get you started: ```python import os import socket import sys import logging def initialize_audit_logger(log_file: str): def audit_hook(event, args): if event in (\'socket.connect\', \'os.system\', \'exec\'): with open(log_file, \'a\') as log: log.write(f\\"Event: {event}, Arguments: {args}n\\") # Register the audit hook sys.addaudithook(audit_hook) def trigger_sample_events(): # Create a socket connection to trigger \'socket.connect\' s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: s.connect((\'example.com\', 80)) except Exception as e: pass # Execute a system command to trigger \'os.system\' os.system(\'echo Hello World\') # Dynamically execute code using \'exec\' code = \\"print(\'Executing code with exec\')\\" exec(code) # Example Usage: # initialize_audit_logger(\'audit_log.txt\') # trigger_sample_events() ``` **Note:** This question tests your ability to: - Use Python\'s auditing APIs. - Handle system-level operations and events. - Implement logging mechanisms. - Understand how event-driven programming can be utilized in system interactions.","solution":"import os import socket import sys import logging def initialize_audit_logger(log_file: str): def audit_hook(event, args): if event in (\'socket.connect\', \'os.system\', \'exec\'): with open(log_file, \'a\') as log: log.write(f\\"Event: {event}, Arguments: {args}n\\") # Register the audit hook sys.addaudithook(audit_hook) def trigger_sample_events(): # Create a socket connection to trigger \'socket.connect\' s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: s.connect((\'example.com\', 80)) except Exception as e: pass # Execute a system command to trigger \'os.system\' os.system(\'echo Hello World\') # Dynamically execute code using \'exec\' code = \\"print(\'Executing code with exec\')\\" exec(code)"},{"question":"# Custom Database Connection Context Manager **Objective:** To assess the ability of students to implement a custom context manager using the `contextlib.contextmanager` decorator. # Background: You are working on a Python application that interacts with a database. For simplicity, let\'s assume interaction involves connecting to the database, performing some operations, and then closing the connection. The database connection is represented by a class `DatabaseConnection` which has methods to connect, disconnect, and execute queries. # DatabaseConnection Class: ```python class DatabaseConnection: def connect(self): print(\\"Connecting to the database.\\") def disconnect(self): print(\\"Disconnecting from the database.\\") def execute(self, query): print(f\\"Executing query: {query}\\") # Simulate a simple query execution result return f\\"Result of \'{query}\'\\" ``` # Task: Write a function `database_connection_manager` using the `@contextlib.contextmanager` decorator to manage database connections. This function should: 1. Establish a connection to the database when entering the context. 2. Ensure that the connection is closed when exiting the context, even if an error occurs. # Function Signature: ```python from contextlib import contextmanager @contextmanager def database_connection_manager(db_connection): # Your code here ``` # Example Usage: ```python db = DatabaseConnection() with database_connection_manager(db) as conn: result = conn.execute(\\"SELECT * FROM users\\") print(result) ``` The output should be: ``` Connecting to the database. Executing query: SELECT * FROM users Result of \'SELECT * FROM users\' Disconnecting from the database. ``` # Constraints: - The context manager must handle exceptions that occur within the `with` block and ensure the connection is closed properly. - Use the provided `DatabaseConnection` class only for the purpose outlined. No modifications to this class should be made. # Additional Notes: - Make sure your implementation properly uses the `@contextlib.contextmanager` decorator. - Test your function thoroughly to ensure it behaves as expected with and without exceptions inside the `with` block.","solution":"from contextlib import contextmanager class DatabaseConnection: def connect(self): print(\\"Connecting to the database.\\") def disconnect(self): print(\\"Disconnecting from the database.\\") def execute(self, query): print(f\\"Executing query: {query}\\") # Simulate a simple query execution result return f\\"Result of \'{query}\'\\" @contextmanager def database_connection_manager(db_connection): Context manager to manage database connection lifecycle try: db_connection.connect() yield db_connection except Exception as e: print(f\\"An error occurred: {e}\\") finally: db_connection.disconnect()"},{"question":"# PyTorch Multiprocessing Task You are tasked with leveraging PyTorch\'s distributed elastic multiprocessing module to perform a parallel computing job. Specifically, you will perform distributed computation of the sum of squares for a list of numbers. Your solution should showcase the use of PyTorch\'s elastic multiprocessing capabilities, including starting multiple processes and handling their contexts. Requirements 1. Implement a function `parallel_sum_of_squares(numbers: List[int], num_workers: int) -> int` that calculates the sum of squares of the given list of numbers using the specified number of workers. 2. Use `torch.distributed.elastic.multiprocessing.start_processes` to start the worker processes. 3. Each worker should process a subset of the list. 4. Utilize PyTorch\'s `MultiprocessContext` or `SubprocessContext` to manage and retrieve results from the workers. 5. Ensure that the final result is the correct sum of squares of all numbers in the list. Input Format - `numbers`: A list of integers `[n1, n2, n3, ...]` - `num_workers`: An integer denoting the number of worker processes to use. Output Format - Return a single integer which is the sum of squares of all the numbers. Constraints - 1 <= len(numbers) <= 10^6 - 1 <= num_workers <= 32 - Each number in the list `numbers` is in the range `-10^4` to `10^4` Example ```python numbers = [1, 2, 3, 4] num_workers = 2 output = parallel_sum_of_squares(numbers, num_workers) # Should return 30 (1^2 + 2^2 + 3^2 + 4^2) ``` Performance Consideration Your implementation should efficiently split the task across the available workers and correctly aggregate their results. Ensure that your solution handles large input sizes within a reasonable time frame. Notes - You might want to create helper functions to split the list and process each subset. - Make sure to handle process synchronization properly to avoid any race conditions or intermediate state inconsistencies.","solution":"import torch import torch.multiprocessing as mp def worker_fn(numbers_slice, result_queue): Worker function to compute the sum of squares for a slice of numbers. squares_sum = sum(x ** 2 for x in numbers_slice) result_queue.put(squares_sum) def parallel_sum_of_squares(numbers, num_workers): Computes the sum of squares of the given list of numbers using multiple worker processes. if not numbers: return 0 # Split the numbers into num_workers slices chunk_size = (len(numbers) + num_workers - 1) // num_workers chunks = [numbers[i:i + chunk_size] for i in range(0, len(numbers), chunk_size)] result_queue = mp.Queue() processes = [] for chunk in chunks: p = mp.Process(target=worker_fn, args=(chunk, result_queue)) processes.append(p) p.start() # Gathering results total_sum = 0 for _ in chunks: total_sum += result_queue.get() # Joining processes for p in processes: p.join() return total_sum"},{"question":"You are required to implement a function `batched_matrix_operations` that performs several batched matrix operations on 3D tensors using PyTorch. Function Signature: ```python def batched_matrix_operations(A: torch.Tensor, B: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: pass ``` Input: - `A` (torch.Tensor): A 3D tensor of shape `(batch_size, n, m)` containing floating-point numbers. - `B` (torch.Tensor): A 3D tensor of shape `(batch_size, m, p)` containing floating-point numbers. Output: - A tuple of three 2D tensors: 1. `result_bmm` (torch.Tensor): The result of batched matrix multiplication between `A` and `B`, of shape `(batch_size, n, p)`. 2. `first_element_result_slice` (torch.Tensor): A 2D tensor of shape `(n, p)` representing the first matrix multiplication result computed by slicing the first elements from `A` and `B` and performing standard matrix multiplication. 3. `batched_first_element_result` (torch.Tensor): A 2D tensor of shape `(n, p)` representing the first element of the batched matrix multiplication result. Constraints: - You must use PyTorch to perform the matrix operations. - The function should raise an appropriate error if `A` and `B` are not 3D tensors or if their shapes are incompatible for matrix multiplication. - Input tensors can contain extremal values and you must handle potential overflow or invalid results appropriately. Example: ```python import torch # Given tensors A and B A = torch.randn(4, 3, 5) B = torch.randn(4, 5, 2) result_bmm, first_element_result_slice, batched_first_element_result = batched_matrix_operations(A, B) assert result_bmm.shape == (4, 3, 2) assert first_element_result_slice.shape == (3, 2) assert batched_first_element_result.shape == (3, 2) ``` Additional Notes: - Ensure numerical stability while performing the operations, especially when dealing with extremal values. - Consider documenting any assumptions or design choices you make regarding handling numerical precision issues. - Make use of PyTorch’s capability to handle non-finite values and perform batched computations efficiently.","solution":"import torch from typing import Tuple def batched_matrix_operations(A: torch.Tensor, B: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Performs batched matrix operations on 3D tensors A and B. Returns a tuple of three tensors: - batched matrix multiplication result of A and B - first element result obtained by slicing the first matrices from A and B and performing matrix multiplication - first element of the batched matrix multiplication result # Check if inputs are 3D tensors if A.ndimension() != 3 or B.ndimension() != 3: raise ValueError(\\"Both A and B must be 3D tensors\\") # Check for shape compatibility if A.size(2) != B.size(1): raise ValueError(\\"Shapes of A and B are not compatible for matrix multiplication\\") # Perform batched matrix multiplication result_bmm = torch.bmm(A, B) # Slice the first elements from A and B and perform standard matrix multiplication first_element_result_slice = torch.matmul(A[0], B[0]) # Get the first element of the batched matrix multiplication result batched_first_element_result = result_bmm[0] return result_bmm, first_element_result_slice, batched_first_element_result"},{"question":"# Coding Assessment: Subprocess Management in Python **Objective**: Assess the student\'s proficiency in using the `subprocess` module for subprocess management. **Question**: Write a Python function called `execute_commands` that takes a list of commands, executes them using the `subprocess` module, and returns a dictionary containing each command and its output. Additionally, handle any errors that occur during the execution. # Function Signature ```python def execute_commands(commands: List[str]) -> Dict[str, Tuple[int, str, str]]: pass ``` # Parameters - `commands` (List[str]): A list of commands to be executed. Each command is a string. # Returns - `Dict[str, Tuple[int, str, str]]`: A dictionary where each key is a command, and the value is a tuple containing: - The return code of the command (`int`). - The standard output of the command (`str`). - The standard error of the command (`str`). # Constraints - Use `subprocess.run()` for commands that do not interact with each other. - Use `subprocess.Popen` for commands that need to pipe output from one to another. - Handle possible exceptions like `subprocess.TimeoutExpired`, `subprocess.CalledProcessError`, and any `OSError`. # Example ```python commands = [ \\"echo Hello, World!\\", \\"ls -l | grep .py\\" # Assuming a Unix-like shell for piping ] output = execute_commands(commands) print(output) # Expected output format (values may vary based on actual command execution): # { # \\"echo Hello, World!\\": (0, \\"Hello, World!n\\", \\"\\"), # \\"ls -l | grep .py\\": (0, \\"some_output_linesn\\", \\"\\") # } ``` # Notes 1. For commands that involve piping (like \\"ls -l | grep .py\\"), use `subprocess.Popen` to create the pipes. 2. Capture both `stdout` and `stderr` for each command. 3. Ensure the function is robust and can handle different types of errors appropriately. 4. Use appropriate timeout handling where necessary.","solution":"import subprocess from typing import List, Dict, Tuple def execute_commands(commands: List[str]) -> Dict[str, Tuple[int, str, str]]: result = {} for command in commands: try: # Split the command based on \'|\' parts = command.split(\'|\') processes = [] stdout_prev = None for i, part in enumerate(parts): part = part.strip() if i == 0: proc = subprocess.Popen(part, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE) else: proc = subprocess.Popen(part, shell=True, stdin=stdout_prev, stdout=subprocess.PIPE, stderr=subprocess.PIPE) processes.append(proc) stdout_prev = proc.stdout stdout, stderr = processes[-1].communicate() return_code = processes[-1].returncode result[command] = (return_code, stdout.decode() if stdout else \'\', stderr.decode() if stderr else \'\') except subprocess.TimeoutExpired as te: result[command] = (-1, \'\', f\\"TimeoutExpired: {str(te)}\\") except subprocess.CalledProcessError as cpe: result[command] = (cpe.returncode, cpe.output.decode() if cpe.output else \'\', cpe.stderr.decode() if cpe.stderr else \'\') except OSError as oe: result[command] = (-1, \'\', f\\"OSError: {str(oe)}\\") return result"},{"question":"<|Analysis Begin|> The provided documentation relates to the \\"traceback\\" module in Python, which is used to extract, format, and print stack traces of Python programs. This module mimics the behavior of the Python interpreter when it prints a stack trace, offering detailed functionalities to help print customized and formatted stack traces and exception information. The key functionalities and concepts covered in the documentation are: - Printing specific tracebacks using `print_tb()`, `print_exception()`, `print_exc()`, `print_last()`, and `print_stack()`. - Extracting and formatting tracebacks and stack entries using functions like `extract_tb()`, `extract_stack()`, `format_list()`, `format_exception()`, `format_exc()`, `format_tb()`, and `format_stack()`. - Clearing frames using the `clear_frames()` function. - Walking through stack frames using `walk_stack()` and `walk_tb()`. - Class-based representations of exceptions and stack frames using `TracebackException`, `StackSummary`, and `FrameSummary`. Given the advanced nature of these functionalities, a good coding assessment question would involve a scenario where students must utilize exception handling, traceback extraction, and custom formatting to trace and debug an error in a program. <|Analysis End|> <|Question Begin|> # Problem Statement: Customized Error Traceback Formatter As an advanced Python developer, you are tasked with creating a function that simulates a part of a Python interpreter\'s behavior when an exception is raised. Specifically, you need to capture and print formatted stack traces of all exceptions that occur during the execution of a given function. This will require using the `traceback` module to extract valuable debugging information such as filenames, line numbers, function names, and code lines where exceptions occur. Requirements 1. Implement a function `trace_and_format_exceptions(func, *args, **kwargs)` that: - Takes a function `func` and its arguments as input. - Executes the function with the provided arguments. - If an exception occurs, captures the traceback information. - Formats the traceback information to be readable and helpful for debugging. 2. The formatted output should: - Include a header \\"Exception encountered:\\". - List each frame of the stack trace with the format: `\\"File \\"<filename>\\", line <lineno>, in <function name>: <code line>\\"`. - Include the message indicating which exception occurred. Input - `func` (Function): A function that will be executed. - `*args`: Positional arguments to be passed to `func`. - `**kwargs`: Keyword arguments to be passed to `func`. Output - If no exception occurs, the function should return the result of `func(*args, **kwargs)`. - If an exception occurs, the function should print the formatted error traceback as described and return `None`. Constraints - You should use the traceback module\'s functions and classes for extracting and formatting. - Do not use the print functions `traceback.print_*`. Example Usage ```python def error_prone_function(a, b): return a / b trace_and_format_exceptions(error_prone_function, 10, 0) ``` Output: ``` Exception encountered: File \\"<stdin>\\", line <line_number>, in <module> trace_and_format_exceptions(error_prone_function, 10, 0) File \\"<stdin>\\", line <line_number>, in trace_and_format_exceptions result = func(*args, **kwargs) File \\"<stdin>\\", line <line_number>, in error_prone_function return a / b ZeroDivisionError: division by zero ``` # Note: - Replace `<line_number>` with the actual line number where the error occurred. - Replace `<stdin>` with the actual filename if running from a script. # Constraints for Implementation - Ensure that the function works within reasonable time limits even for functions that throw deep nested exceptions. - No external libraries besides `traceback` should be used for this task.","solution":"import traceback def trace_and_format_exceptions(func, *args, **kwargs): Execute the given function args and kwargs, capturing and formatting any exceptions encountered into a detailed traceback. try: return func(*args, **kwargs) except Exception: tb_lines = [\\"Exception encountered:n\\"] tb_lines.extend(traceback.format_exc().splitlines()) for line in tb_lines: print(line) return None"},{"question":"**Objective:** Implement a function `record_and_playback` that initializes an audio device for recording, captures audio data for a specified duration, and then plays back the recorded audio. **Function Signature:** ```python def record_and_playback(device: str, duration: int, fmt: str, channels: int, rate: int) -> None: ``` **Parameters:** - `device` (str): The audio device filename to use (e.g., \\"/dev/dsp\\"). - `duration` (int): The recording duration in seconds. - `fmt` (str): The audio format to use. Common formats include \\"AFMT_U8\\", \\"AFMT_S16_LE\\", etc. - `channels` (int): The number of audio channels (1 for mono, 2 for stereo). - `rate` (int): The sample rate in samples per second (e.g., 44100 for CD quality). **Constraints:** - The function should open the specified audio device for reading (recording) and writing (playback). - The format, channels, and rate must be set for the audio device. - The function should handle any potential errors gracefully. - The maximum allowed duration is 60 seconds. - If any parameter does not match the correct type or value range, the function should raise a ValueError. **Example Usage:** ```python record_and_playback(\\"/dev/dsp\\", 5, \\"AFMT_S16_LE\\", 2, 44100) ``` **Expected Behavior:** The function records audio from the specified device for 5 seconds in 16-bit stereo format at 44.1 kHz, then plays back the recorded audio through the same device. **Implementation Guidelines:** 1. Open the audio device for both reading and writing. 2. Set the required audio format, channels, and sample rate using the provided methods. 3. Record audio data for the given duration. 4. Play back the recorded audio data. 5. Ensure the device is properly closed after operations. **Note:** Due to the nature of audio I/O, this function may not be executable in certain environments lacking proper audio hardware or permissions. Ensure the function implementation is robust and can handle such cases appropriately.","solution":"import os import fcntl import struct import time # Constants for audio format settings AFMT_S16_LE = 0x0010 # Common audio format 16-bit little-endian # ioctl commands typically used with audio devices, these constants may vary SNDCTL_DSP_SETFMT = 0xC0045005 # Set audio format SNDCTL_DSP_CHANNELS = 0xC0045006 # Set number of channels SNDCTL_DSP_SPEED = 0xC0045002 # Set sample rate VALID_FORMATS = { \\"AFMT_U8\\": 0x0008, \\"AFMT_S16_LE\\": AFMT_S16_LE } def record_and_playback(device: str, duration: int, fmt: str, channels: int, rate: int) -> None: if not isinstance(device, str): raise ValueError(\\"Device must be a string.\\") if not isinstance(duration, int) or not (1 <= duration <= 60): raise ValueError(\\"Duration must be an integer between 1 and 60.\\") if fmt not in VALID_FORMATS: raise ValueError(\\"Invalid audio format.\\") if not isinstance(channels, int) or not (1 <= channels <= 2): raise ValueError(\\"Channels must be an integer (1 for mono, 2 for stereo).\\") if not isinstance(rate, int) or rate < 1: raise ValueError(\\"Sample rate must be a positive integer.\\") audio_fmt = VALID_FORMATS[fmt] try: with open(device, \\"rb+\\") as dsp: # Set the audio format fcntl.ioctl(dsp, SNDCTL_DSP_SETFMT, struct.pack(\\"I\\", audio_fmt)) # Set the number of channels fcntl.ioctl(dsp, SNDCTL_DSP_CHANNELS, struct.pack(\\"I\\", channels)) # Set the sample rate fcntl.ioctl(dsp, SNDCTL_DSP_SPEED, struct.pack(\\"I\\", rate)) total_bytes = duration * rate * channels * (2 if fmt == \\"AFMT_S16_LE\\" else 1) # Record audio data recorded_data = dsp.read(total_bytes) # Playback the recorded data dsp.write(recorded_data) except Exception as e: print(f\\"Error: {e}\\") raise"},{"question":"# Custom I/O Stream Implementation # Objective: Implement a custom I/O class that combines features of raw, buffered, and text I/O. # Task: Create a custom `CustomBufferedTextStream` class that: 1. Inherits from `io.TextIOBase`. 2. Combines functionalities of text I/O with buffered I/O, ensuring efficient data handling. 3. Allows reading from and writing to a memory buffer. 4. Supports methods for reading and writing (`read`, `write`), managing the buffer (`flush`), and positioning within the stream (`seek`, `tell`). # Specifications: 1. **Constructor**: - `__init__(self, initial_value=\'\', newline=\'n\', buffer_size=io.DEFAULT_BUFFER_SIZE)` - `initial_value`: Initial text data for the in-memory buffer. - `newline`: Controls how newlines are handled. - `buffer_size`: Defines the size of the buffer. ``` Constraints: * The buffer size should be a positive integer. * The `initial_value` must be a string. * The `newline` must be one of None, \'\', \'n\', \'r\', \'rn\'. 2. **Methods to Implement**: - `read(self, size=-1)`: Reads data from the stream. - `write(self, s)`: Writes data to the stream. - `flush(self)`: Flushes the write buffer to the stream. - `seek(self, offset, whence=io.SEEK_SET)`: Changes the stream position. - `tell(self)`: Returns the current stream position. - `close(self)`: Closes the stream. 3. **Edge Cases**: - Ensure that reading from a closed stream raises a `ValueError`. - Writing to a closed stream should also raise a `ValueError`. - Reading and writing operations should handle newline characters based on the `newline` attribute specified. # Input/Output: - **Input**: - N/A (Function signatures will dictate necessary inputs.) - **Output**: - Depends on the operations (writing returns the number of characters written, reading returns the string read, etc.) ```python import io class CustomBufferedTextStream(io.TextIOBase): def __init__(self, initial_value=\'\', newline=\'n\', buffer_size=io.DEFAULT_BUFFER_SIZE): # Initialize the necessary attributes here pass def read(self, size=-1): # Implement the read functionality pass def write(self, s): # Implement the write functionality pass def flush(self): # Implement the flush functionality pass def seek(self, offset, whence=io.SEEK_SET): # Implement the seek functionality pass def tell(self): # Implement the tell functionality pass def close(self): # Implement the close functionality pass # Example Usage: # stream = CustomBufferedTextStream(initial_value=\\"Hello, World!\\") # stream.write(\\" This is a test.\\") # stream.seek(0) # print(stream.read()) # Should output: \\"Hello, World! This is a test.\\" ``` # Validation: Ensure comprehensive tests cover initialization, I/O operations, and edge cases to validate the implemented class.","solution":"import io class CustomBufferedTextStream(io.TextIOBase): def __init__(self, initial_value=\'\', newline=\'n\', buffer_size=io.DEFAULT_BUFFER_SIZE): if not isinstance(initial_value, str): raise TypeError(\\"initial_value must be a string\\") if not isinstance(buffer_size, int) or buffer_size <= 0: raise ValueError(\\"buffer_size must be a positive integer\\") if newline not in (None, \'\', \'n\', \'r\', \'rn\'): raise ValueError(\\"newline must be one of None, \'\', \'n\', \'r\', \'rn\'\\") self._buffer = initial_value self._newline = newline self._buffer_size = buffer_size self._position = 0 self._closed = False def read(self, size=-1): if self._closed: raise ValueError(\\"Stream is closed\\") if size == -1: size = len(self._buffer) - self._position end = min(len(self._buffer), self._position + size) result = self._buffer[self._position:end] self._position = end return result def write(self, s): if self._closed: raise ValueError(\\"Stream is closed\\") if not isinstance(s, str): raise TypeError(\\"write() argument must be str, not \\" + type(s).__name__) before = self._buffer[:self._position] after = self._buffer[self._position + len(s):] self._buffer = before + s + after self._position += len(s) return len(s) def flush(self): if self._closed: raise ValueError(\\"Stream is closed\\") def seek(self, offset, whence=io.SEEK_SET): if self._closed: raise ValueError(\\"Stream is closed\\") if whence == io.SEEK_SET: new_pos = offset elif whence == io.SEEK_CUR: new_pos = self._position + offset elif whence == io.SEEK_END: new_pos = len(self._buffer) + offset else: raise ValueError(\\"Invalid value for whence\\") if new_pos < 0: raise ValueError(\\"New position is beyond the start of the stream\\") self._position = new_pos return self._position def tell(self): if self._closed: raise ValueError(\\"Stream is closed\\") return self._position def close(self): self._closed = True"},{"question":"**Custom Python Interpreter** Your task is to create a custom Python interpreter by extending Python\'s default interpreter functionality using the \\"code\\" and \\"codeop\\" modules. This interpreter should support the following features: **Features:** 1. **Execution of Python Commands**: - The interpreter should execute standard Python commands. 2. **Handling Incomplete Input**: - Utilize the \\"codeop\\" module to compile and manage possibly incomplete blocks of code. The interpreter should wait for more input when an incomplete block of code is detected, until a complete block is formed. 3. **Custom Command Support**: - Include a custom command `printvar <var_name>` which prints the value of a variable if it exists in the current local scope. **Requirements:** - Implement a class `CustomInterpreter` that extends `InteractiveConsole` from the \\"code\\" module. - The interpreter should maintain a local scope where variables can be stored and retrieved. - If an input is detected as incomplete, the interpreter should prompt for more input instead of executing it immediately. - The custom command `printvar <var_name>` should check if the variable `var_name` exists in the local scope and print its value. **Specification:** * `class CustomInterpreter(InteractiveConsole):` - Initialize the class with an appropriate local scope. * `def execute_command(self, command: str) -> None:` - Accepts a command as a string and processes it based on the defined features. * `def run_interpreter(self) -> None:` - Starts the interactive interpreter loop that continuously accepts input from the user and processes commands. **Example Usage:** ```python interpreter = CustomInterpreter() interpreter.run_interpreter() ``` - Input: `a = 10` - Input: `b = 20` - Input: `print(a + b)` - Output: `30` - Input: `printvar a` - Output: `10` - Input: `printvar c` - Output: `NameError: name \'c\' is not defined` Feel free to add additional helper methods if needed to maintain code readability and functionality. **Evaluation:** Your solution will be evaluated based on: - Correct implementation of the `CustomInterpreter` class. - Accurate handling of incomplete input using the \\"codeop\\" module. - Proper execution of standard Python commands. - Correct functionality of the `printvar` command. - Overall code quality and adherence to Python best practices.","solution":"import code import codeop class CustomInterpreter(code.InteractiveConsole): def __init__(self, locals=None): super().__init__(locals) self.buffer = [] # to store incomplete statements def execute_command(self, command: str) -> None: if command.startswith(\\"printvar \\"): var_name = command.split(\\" \\", 1)[1] if var_name in self.locals: print(self.locals[var_name]) else: print(f\\"NameError: name \'{var_name}\' is not defined\\") else: self.push(command) def run_interpreter(self) -> None: while True: try: line = input(\\">>> \\" if not self.buffer else \\"... \\") self.execute_command(line) except (EOFError, KeyboardInterrupt): print(\\"Exiting interpreter.\\") break def push(self, line): self.buffer.append(line) source = \\"n\\".join(self.buffer) more = self.runsource(source) if not more: self.buffer = [] # Example usage: # interpreter = CustomInterpreter() # interpreter.run_interpreter()"},{"question":"# Email Message Processing with `email.message.Message` Introduction You are given a text representation of an email message. Your task is to write a Python function that processes this email using the `email.message.Message` class from the `email` module. The function will parse the headers, modify specific headers, and return the updated email message. Function Specification 1. **Function Name**: `process_email_message` 2. **Parameters**: - `email_text (str)`: A string containing the full text of an email message including headers and payload. 3. **Returns**: - `str`: The updated email message as a string. The Task 1. Parse the input `email_text` to create a `Message` object. 2. If the message has a \\"Subject\\" header, prefix the subject with the string `\\"Processed: \\"` and update the header. If the \\"Subject\\" header is missing, set it to `\\"Processed: No Subject\\"`. 3. Remove any \\"X-Spam-Status\\" header from the message, if it exists. 4. Ensure the message has a \\"Content-Type\\" header set to `\\"text/plain\\"`. If it is missing, add this header. 5. Flatten the updated message back into a string and return this string. Constraints - The email message might contain non-ASCII characters in headers. - The message might include multiple headers of the same name. - The payload can be multi-part (MIME-encoded), but for simplicity, your code does not need to modify the payload. Performance - As the operation involves string manipulation and header parsing, ensure your function efficiently handles strings of reasonable size (up to a few MB). Example ```python input_email = \'\'\' From: user@example.com To: recipient@example.com Subject: Test Email X-Spam-Status: Yes This is a test email. \'\'\' output_email = \'\'\' From: user@example.com To: recipient@example.com Subject: Processed: Test Email Content-Type: text/plain This is a test email. \'\'\' assert process_email_message(input_email) == output_email ``` Use the provided documentation to understand and implement the necessary methods and functionalities required to complete the task.","solution":"import email from email.message import EmailMessage from email.parser import BytesParser def process_email_message(email_text): Process the input email text to modify headers and return the updated email text. Args: email_text (str): A string containing the full text of an email message including headers and payload. Returns: str: The updated email message as a string. # Parse the email text into a Message object msg = email.message_from_string(email_text) # Update the Subject header if \'Subject\' in msg: msg.replace_header(\'Subject\', \'Processed: \' + msg[\'Subject\']) else: msg[\'Subject\'] = \'Processed: No Subject\' # Remove the X-Spam-Status header if it exists if \'X-Spam-Status\' in msg: del msg[\'X-Spam-Status\'] # Ensure the Content-Type header is set to \\"text/plain\\" if \'Content-Type\' not in msg: msg[\'Content-Type\'] = \'text/plain\' # Convert the Message object back to a string return msg.as_string()"},{"question":"# Question: Implement an Asynchronous Task Scheduler Objective: Your task is to implement a simple asynchronous task scheduler in Python that can run multiple async tasks and gather their results once they are completed. Task Details: 1. Implement an `async` function named `delayed_result`: - This function should take two arguments: `value` (an integer) and `delay_time` (an integer, representing seconds). - It should wait asynchronously for `delay_time` seconds using `await asyncio.sleep(delay_time)`. - After waiting, it should return the `value`. 2. Implement a function named `run_tasks`: - This function should take a list of tuples, where each tuple contains two integers `(value, delay_time)`. - It should create a task for each tuple using the `delayed_result` function. - It should run these tasks asynchronously and gather all their results. - This function should return a list of results from all tasks. Input: - A list of tuples, with each tuple containing two integers `(value, delay_time)`. Output: - A list of integers, representing the `value` returned by each task after its respective delay time. Constraints: - `1 <= len(list_of_tasks) <= 100` — Number of tasks to schedule. - `0 <= value <= 1000` — Value to be returned by `delayed_result`. - `1 <= delay_time <= 5` — Delay time for each task (in seconds). Example: ```python import asyncio async def delayed_result(value: int, delay_time: int) -> int: # Your implementation here pass async def run_tasks(list_of_tasks: List[Tuple[int, int]]) -> List[int]: # Your implementation here pass # Example usage: # Input: [(10, 2), (20, 1), (30, 3)] # Output: [10, 20, 30] input_tasks = [(10, 2), (20, 1), (30, 3)] results = asyncio.run(run_tasks(input_tasks)) print(results) # Expected: [10, 20, 30] ``` Note: - You can use `asyncio.create_task` to create a new async task. - Use `await` to gather the results of these tasks.","solution":"import asyncio from typing import List, Tuple async def delayed_result(value: int, delay_time: int) -> int: Asynchronously waits for delay_time seconds and then returns the given value. :param value: The value to return after the delay. :param delay_time: The time (in seconds) to wait before returning the value. :return: The given value after the delay. await asyncio.sleep(delay_time) return value async def run_tasks(list_of_tasks: List[Tuple[int, int]]) -> List[int]: Runs multiple asynchronous tasks and gathers their results. :param list_of_tasks: A list of tuples, each containing a value and a delay time. :return: A list of integers, representing the value returned by each task after its respective delay time. tasks = [asyncio.create_task(delayed_result(value, delay_time)) for value, delay_time in list_of_tasks] results = await asyncio.gather(*tasks) return results"},{"question":"# **Email Message Processing** You are given an email message object tree and need to perform various operations to extract and process its content. 1. **Extract Text Content Line-by-Line**: Write a function `get_text_content_lines(msg, decode=False)` that uses the `email.iterators.body_line_iterator` to iterate over all the payloads in the subparts of the given message object, returning the text content line-by-line. 2. **Filter Subparts by MIME Type**: Write a function `get_subparts_by_mime_type(msg, maintype=\'text\', subtype=None)` to iterate over the subparts of the given message object, returning all subparts that match a specified MIME type using the `email.iterators.typed_subpart_iterator`. 3. **Visualize Message Structure**: Write a function `print_message_structure(msg, fp=None, level=0, include_default=False)` to print an indented representation of the content types of the message object structure using the utility function `email.iterators._structure`. **Function Signatures:** ```python from email.message import Message def get_text_content_lines(msg: Message, decode: bool = False) -> list[str]: pass def get_subparts_by_mime_type(msg: Message, maintype: str = \'text\', subtype: str = None) -> list[Message]: pass def print_message_structure(msg: Message, fp=None, level: int = 0, include_default: bool = False) -> None: pass ``` **Input:** - `msg`: An email message object. - `decode`: A boolean flag indicating whether to decode the payload (default is False). - `maintype`: The main MIME type to filter by (default is \'text\'). - `subtype`: The sub MIME type to filter by (optional). - `fp`: A file-like object to print the message structure to (default is `None`). - `level`: An integer for internal indentation level (default is 0). - `include_default`: A boolean flag indicating whether to include default types in the structure (default is False). **Output:** - `get_text_content_lines`: A list of strings, each representing a line of text from the message\'s text payloads. - `get_subparts_by_mime_type`: A list of email message objects that match the specified MIME type. - `print_message_structure`: This function does not return anything but prints the message structure. **Constraints:** - Assume `msg` is always a valid `Message` object. - Ensure that the functions handle messages with nested subparts correctly. **Performance Requirements:** - The functions should handle typical email messages efficiently within a reasonable time frame. Example usage: ```python # Example email message object (you\'ll need to create or parse real email objects for actual use) msg = email.message_from_string(MIME-Version: 1.0 Content-Type: multipart/mixed; boundary=\\"===============2190167314158941251==\\" --===============2190167314158941251== Content-Type: text/plain This is a test email. --===============2190167314158941251== Content-Type: text/html <html> <body> <p>This is a test email in HTML format.</p> </body> </html> --===============2190167314158941251==-- ) print(get_text_content_lines(msg)) print(get_subparts_by_mime_type(msg, \'text\', \'plain\')) print_message_structure(msg) ```","solution":"from email.message import Message import email.iterators def get_text_content_lines(msg: Message, decode: bool = False) -> list[str]: lines = [] for line in email.iterators.body_line_iterator(msg, decode=decode): lines.append(line) return lines def get_subparts_by_mime_type(msg: Message, maintype: str = \'text\', subtype: str = None) -> list[Message]: subparts = [] for part in email.iterators.typed_subpart_iterator(msg, maintype, subtype): subparts.append(part) return subparts def print_message_structure(msg: Message, fp=None, level: int = 0, include_default: bool = False) -> None: email.iterators._structure(msg, fp=fp, level=level, include_default=include_default)"},{"question":"# Problem: Enhanced Error Reporting with Traceback Module You are tasked with creating a robust function to log detailed error reports when exceptions occur in a codebase. Your function will be used to ensure that when exceptions are caught, comprehensive tracebacks, including local variable states and a hierarchical display of chained exceptions, are logged to a specified file. Function Signature: ```python def log_exception_to_file(file_path: str): Logs the detailed traceback information of the most recent exception to the specified file. Parameters: - file_path (str): The path to the file where the traceback details should be logged. Returns: - None Requirements: 1. **Initial Setup**: - Your function should capture and log all exception details using the traceback module. - Ensure all stack frames and local variables at each level of the stack are captured. 2. **Detailed Error Logging**: - Write the tracebacks to the specified file, formatted neatly. - Include a clear hierarchical display for any chained exceptions (`__cause__`, `__context__`). 3. **Formatting**: - The log should include: - The stack trace leading up to the exception. - The type, message, and local variables for each stack frame. - For `SyntaxError`, include detailed information about the exact location of the error. - End the log with the formatted representation of the entire exception chain. 4. **Edge Cases**: - Handle cases where the provided file path is invalid or inaccessible. - Ensure the function does not crash even if another exception occurs while logging. Example: ```python try: risky_operation() except: log_exception_to_file(\'/path/to/error_log.txt\') ``` The `/path/to/error_log.txt` should contain a detailed and formatted traceback of the most recent exception, adhering to the requirements above. Constraints: - The solution must use features of the traceback module as described. - Only standard Python libraries are allowed.","solution":"import traceback import sys def log_exception_to_file(file_path: str): Logs the detailed traceback information of the most recent exception to the specified file. Parameters: - file_path (str): The path to the file where the traceback details should be logged. Returns: - None try: exc_type, exc_value, exc_tb = sys.exc_info() with open(file_path, \'w\') as f: # Writing the formatted exception details to the file traceback.print_exception(exc_type, exc_value, exc_tb, chain=True, file=f) except Exception as e: print(f\'Logging failed with exception: {e}\', file=sys.stderr)"},{"question":"Coding Question # Objective Implement a function that simulates a simple student management system, which performs various operations on students\' records using dictionaries and lists. # Description You are tasked with writing a function `manage_students` that takes two parameters: 1. `students`: a dictionary where keys are student IDs (integers) and values are dictionaries with student details, which include the following keys: - `\\"name\\"`: a string representing the student\'s name. - `\\"grades\\"`: a list of integers representing the student\'s grades. 2. `operations`: a list of operations to be performed on the `students` dictionary. Each operation is represented by a dictionary with the following keys: - `\\"operation\\"`: a string that can be either `\\"add\\"`, `\\"remove\\"`, `\\"update_name\\"`, or `\\"add_grade\\"`. - `\\"id\\"`: an integer representing the student ID to which the operation applies. - For the `\\"add\\"` operation, an additional key `\\"student\\"` that is a dictionary with `\\"name\\"` and an empty list for `\\"grades\\"` should be provided. - For the `\\"update_name\\"` operation, an additional key `\\"new_name\\"` should be provided. - For the `\\"add_grade\\"` operation, an additional key `\\"grade\\"` should be provided. The `manage_students` function should perform the operations in the order given and return the updated `students` dictionary. # Constraints - Student IDs (`id`) are unique positive integers. - Student names are non-empty strings. - Grades are integers between 0 and 100. - Operations list (`operations`) can contain up to 1000 operations. # Implementation Requirements - Perform input validation and type checking where appropriate. - The function should handle possible errors gracefully, such as attempting to update or remove a non-existent student. - Ensure that the function is efficient and can handle the maximum input size within reasonable time limits. # Function Signature ```python def manage_students(students: dict, operations: list) -> dict: pass ``` # Example ```python students = { 1: {\\"name\\": \\"Alice\\", \\"grades\\": [85, 90]}, 2: {\\"name\\": \\"Bob\\", \\"grades\\": [78, 82]} } operations = [ {\\"operation\\": \\"add\\", \\"id\\": 3, \\"student\\": {\\"name\\": \\"Charlie\\"}}, {\\"operation\\": \\"add_grade\\", \\"id\\": 1, \\"grade\\": 95}, {\\"operation\\": \\"remove\\", \\"id\\": 2}, {\\"operation\\": \\"update_name\\", \\"id\\": 3, \\"new_name\\": \\"Charles\\"}, ] updated_students = manage_students(students, operations) # Expected: # { # 1: {\\"name\\": \\"Alice\\", \\"grades\\": [85, 90, 95]}, # 3: {\\"name\\": \\"Charles\\", \\"grades\\": []} # } ``` # Note Make sure to test your function with various edge cases, such as performing operations on non-existent IDs and invalid data types.","solution":"def manage_students(students: dict, operations: list) -> dict: Simulates a simple student management system to perform various operations on students\' records. for op in operations: operation = op.get(\\"operation\\") student_id = op.get(\\"id\\") if operation == \\"add\\": student = op.get(\\"student\\") if not isinstance(student, dict) or \\"name\\" not in student: continue if student_id not in students: students[student_id] = { \\"name\\": student[\\"name\\"], \\"grades\\": [] } elif operation == \\"remove\\": if student_id in students: del students[student_id] elif operation == \\"update_name\\": new_name = op.get(\\"new_name\\") if student_id in students and isinstance(new_name, str): students[student_id][\\"name\\"] = new_name elif operation == \\"add_grade\\": grade = op.get(\\"grade\\") if student_id in students and isinstance(grade, int) and 0 <= grade <= 100: students[student_id][\\"grades\\"].append(grade) return students"},{"question":"# Handling HTTP Request Errors with `urllib.error` **Background:** You are working on a Python application that makes HTTP requests to various APIs. It is crucial to handle different types of errors that might occur during these requests, such as network problems, HTTP errors, or incomplete downloads. **Objective:** Implement a function called `fetch_data` that takes a URL as input and returns the content of the URL. The function should handle the following exceptions: - `URLError`: Log the reason for the error and return `None`. - `HTTPError`: Log the HTTP status code and reason, and return `None`. - `ContentTooShortError`: Log the message and the length of the partially downloaded content, and return `None`. **Function Signature:** ```python import urllib.request import urllib.error def fetch_data(url: str) -> str: # Your implementation here ``` **Input:** - `url` (string): The URL to fetch data from. **Output:** - Returns the content of the URL as a string on success. - Returns `None` on failure along with logging the appropriate error messages. **Constraints:** - Use the `urllib.request.urlopen` method to make the HTTP request. - Assume network errors, HTTP errors, and partial downloads can occur. - Make sure to log meaningful error messages using the `logging` module. **Performance Requirements:** - The function should efficiently handle typical HTTP requests and appropriate error logging. **Example Usage:** ```python url = \\"http://example.com/data\\" content = fetch_data(url) if content: print(\\"Data fetched successfully\\") else: print(\\"Failed to fetch data\\") ``` **Example Log Messages:** ``` ERROR: URLError encountered. Reason: <reason> ERROR: HTTPError encountered. Code: <status_code>, Reason: <reason> ERROR: ContentTooShortError encountered. Message: <message>, Content Length: <length> ``` **Note:** You may need to simulate different types of errors to test your function thoroughly. Consider using mock objects or a testing library to create controlled scenarios for unit testing.","solution":"import urllib.request import urllib.error from urllib.error import URLError, HTTPError, ContentTooShortError import logging def fetch_data(url: str) -> str: Fetch the data from the provided URL and handle specific errors. Parameters: url (str): The URL to fetch data from. Returns: str: The content of the URL on success. None on failure. try: response = urllib.request.urlopen(url) return response.read().decode(\'utf-8\') except HTTPError as e: logging.error(f\\"HTTPError encountered. Code: {e.code}, Reason: {e.reason}\\") except URLError as e: logging.error(f\\"URLError encountered. Reason: {e.reason}\\") except ContentTooShortError as e: logging.error(f\\"ContentTooShortError encountered. Message: {e.msg}, Content Length: {len(e.content)}\\") return None"},{"question":"Objective Assess students\' proficiency in using seaborn\'s `cubehelix_palette` function to generate and customize color palettes for data visualization. Problem Statement You are given a dataset of car specifications, and you need to visualize various aspects of this data using bar plots with customized color palettes. 1. **Data Preparation**: - Write a function `load_sample_data()` to load the built-in `mpg` dataset from seaborn. - This dataset contains the following columns: \'mpg\', \'cylinders\', \'displacement\', \'horsepower\', \'weight\', \'acceleration\', \'model_year\', \'origin\', \'name\'. 2. **Palette Generation**: - Create a function `generate_palette(num_colors, start=0, rot=0.5, gamma=1.0, hue=1.0, dark=0, light=1.0, reverse=False)` that returns a cubehelix palette based on the provided parameters. The arguments are: - `num_colors`: Number of colors in the palette (integer). - `start`: Starting point of the helix (float). - `rot`: Amount of rotation in the helix (float). - `gamma`: Nonlinearity in the luminance ramp (float). - `hue`: Saturation of the colors (float). - `dark`: Luminance at the start point (float). - `light`: Luminance at the end point (float). - `reverse`: Whether to reverse the luminance ramp (boolean). 3. **Visualization**: - Write a function `visualize_data(palette)` that: - Creates a bar plot of the average `mpg` (miles per gallon) of cars grouped by `model_year` using the provided palette. - Customizes the plot to have a title, labeled axes, a legend, and appropriate size. 4. **Comprehensive Function**: - Write a function `main_visualization()` that: - Loads the dataset using `load_sample_data`. - Generates a palette with 10 colors, starting point at 1, rotation of 0.3, gamma of 0.8, hue of 0.9, dark at 0.2, light at 0.8, and reversed luminance. - Visualizes the data using the generated palette and the `visualize_data` function. Constraints and Requirements - Ensure the code is efficient. - Properly handle the data and exceptions. - Use seaborn for all visualizations. - Provide inline comments and documentation for your code. Example Usage ```python # Expected usage of the main_visualization function main_visualization() ``` Expected Output A bar plot of average `mpg` grouped by `model_year` with a customized cubehelix palette, appropriately titled and labeled.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def load_sample_data(): Load the built-in mpg dataset from seaborn. Returns: DataFrame: The mpg dataset. return sns.load_dataset(\\"mpg\\") def generate_palette(num_colors, start=0, rot=0.5, gamma=1.0, hue=1.0, dark=0, light=1.0, reverse=False): Generate a cubehelix palette based on the provided parameters. Args: num_colors (int): Number of colors in the palette. start (float): Starting point of the helix. rot (float): Amount of rotation in the helix. gamma (float): Nonlinearity in the luminance ramp. hue (float): Saturation of the colors. dark (float): Luminance at the start point. light (float): Luminance at the end point. reverse (bool): Whether to reverse the luminance ramp. Returns: List: A list of colors forming the palette. return sns.cubehelix_palette(n_colors=num_colors, start=start, rot=rot, gamma=gamma, hue=hue, dark=dark, light=light, reverse=reverse) def visualize_data(palette, data): Visualizes the average mpg of cars grouped by model_year using the provided palette. Args: palette (list): The color palette to use for the bar plot. data (DataFrame): The dataset to visualize. plt.figure(figsize=(12, 6)) avg_mpg_by_year = data.groupby(\'model_year\')[\'mpg\'].mean().reset_index() sns.barplot(x=\'model_year\', y=\'mpg\', data=avg_mpg_by_year, palette=palette) plt.title(\'Average MPG by Model Year\') plt.xlabel(\'Model Year\') plt.ylabel(\'Average MPG\') plt.legend(title=\'Model Year\') plt.show() def main_visualization(): Loads the dataset, generates a palette, and visualizes the data using the specified parameters. # Load dataset data = load_sample_data() # Generate palette palette = generate_palette(num_colors=10, start=1, rot=0.3, gamma=0.8, hue=0.9, dark=0.2, light=0.8, reverse=True) # Visualize the data visualize_data(palette=palette, data=data)"},{"question":"# Question: Profiling and Memory Management in Python You are given a Python script that performs some data processing operations. Your task is to analyze the script to identify any performance bottlenecks and excessive memory usage. You need to use the `cProfile` and `tracemalloc` modules to generate detailed reports on execution time and memory allocation. Script to Analyze ```python import random import time def slow_function(): time.sleep(2) numbers = [random.randint(1, 100) for _ in range(10000)] squared_numbers = [n ** 2 for n in numbers] return sum(squared_numbers) def fast_function(): numbers = range(1, 10001) squared_numbers = (n ** 2 for n in numbers) return sum(squared_numbers) if __name__ == \\"__main__\\": result1 = slow_function() result2 = fast_function() print(f\\"Result of slow_function: {result1}\\") print(f\\"Result of fast_function: {result2}\\") ``` Tasks 1. **Profiling Execution Time:** - Use the `cProfile` module to profile the provided script. - Identify which function is taking the most time to execute and provide a detailed breakdown. 2. **Analyzing Memory Usage:** - Use the `tracemalloc` module to trace memory allocations. - Identify any functions or lines in the script that are causing excessive memory usage. 3. **Optimization Recommendations:** - Based on your analysis, provide a brief recommendation on how the script can be optimized in terms of performance and memory usage. Expected Output Format 1. **Profiling Report:** - Detailed `cProfile` report. - (`.prof` file or formatted text output showing time consumption). 2. **Memory Usage Report:** - Detailed `tracemalloc` report. - (Snapshots showing memory allocations). 3. **Recommendations:** - Summary in text format explaining the identified bottlenecks and memory hogs. - Specific suggestions on how to optimize the code. Additional Constraints - Ensure that your analysis and recommendations are clear and well-explained. - Focus on using Python 3.10 features and libraries effectively. Example Report ``` Profiling Report: ----------------- Function: slow_function Total time: 2.1 seconds Detail: 95% of time spent in sleep (2 seconds), 5% in list operations (0.1 seconds) Function: fast_function Total time: 0.01 seconds Detail: Computation using generator is efficient. Memory Usage Report: --------------------- Total allocated size: 256 KB - slow_function: 200 KB - List of random numbers: 80 KB - List of squared numbers: 80 KB - fast_function: 56 KB - Generator expression: 30 KB Recommendations: ----------------- 1. Replace list comprehensions with generator expressions wherever possible to save memory. 2. Optimize sleep calls or use asynchronous operations to improve execution time. ```","solution":"import cProfile import pstats import tracemalloc import random import time def slow_function(): time.sleep(2) numbers = [random.randint(1, 100) for _ in range(10000)] squared_numbers = [n ** 2 for n in numbers] return sum(squared_numbers) def fast_function(): numbers = range(1, 10001) squared_numbers = (n ** 2 for n in numbers) return sum(squared_numbers) def profile_execution(): profiler = cProfile.Profile() profiler.enable() slow_function() fast_function() profiler.disable() profiler.dump_stats(\\"profiling_results.prof\\") stats = pstats.Stats(\\"profiling_results.prof\\") stats.sort_stats(pstats.SortKey.TIME) stats.print_stats() def analyze_memory(): tracemalloc.start() slow_function() fast_function() snapshot = tracemalloc.take_snapshot() top_stats = snapshot.statistics(\'lineno\') for stat in top_stats[:10]: print(stat) if __name__ == \\"__main__\\": print(\\"Profiling Execution Time:\\") profile_execution() print(\\"nAnalyzing Memory Usage:\\") analyze_memory()"},{"question":"Question You are required to implement a function `reproducible_operations` that performs a series of random operations using PyTorch, NumPy, and Python\'s `random` module. The results of these operations must be reproducible. Specifically, your function should: 1. Set seeds for PyTorch, NumPy, and Python\'s `random` module to ensure reproducibility. 2. Generate a random tensor using PyTorch. 3. Generate a random array using NumPy. 4. Generate a random integer using Python\'s `random` module. 5. Ensure that the operations are reproducible using deterministic algorithms provided by PyTorch. # Expected Input and Output ```python def reproducible_operations(seed: int) -> dict: Generate reproducible random numbers using PyTorch, NumPy, and Python random module. Parameters: seed (int): The seed value for the random number generators. Returns: dict: A dictionary containing the generated random tensor, array, and integer: { \\"tensor\\": torch.Tensor, \\"array\\": numpy.ndarray, \\"integer\\": int } ``` # Constraints - The seed value will be a non-negative integer. - The function should return the same output for the same input seed across different runs and environments, given the same hardware configurations. - Utilize deterministic algorithms in PyTorch during this process. # Example Given an input seed value of `42`: ```python result = reproducible_operations(42) print(result[\\"tensor\\"]) # Example output: tensor([ 1.9269, 0.2172, -0.4086, 0.9829]) print(result[\\"array\\"]) # Example output: array([0.37454012, 0.95071431, 0.73199394, 0.59865848]) print(result[\\"integer\\"]) # Example output: 2746317213 ``` **Note:** The actual values may differ but should be consistent every time the function is called with the same seed. Good luck!","solution":"import torch import numpy as np import random def reproducible_operations(seed: int) -> dict: Generate reproducible random numbers using PyTorch, NumPy, and Python random module. Parameters: seed (int): The seed value for the random number generators. Returns: dict: A dictionary containing the generated random tensor, array, and integer: { \\"tensor\\": torch.Tensor, \\"array\\": numpy.ndarray, \\"integer\\": int } # Set seeds for reproducibility torch.manual_seed(seed) np.random.seed(seed) random.seed(seed) # Enable deterministic algorithms in PyTorch torch.use_deterministic_algorithms(True) # Generate a random tensor using PyTorch tensor = torch.randn(4) # Generate a random array using NumPy array = np.random.rand(4) # Generate a random integer using Python\'s random module integer = random.randint(0, 10000) # Return the results in a dictionary return {\\"tensor\\": tensor, \\"array\\": array, \\"integer\\": integer}"},{"question":"Objective Design a program using the `xml.etree.ElementTree` module that processes an XML file containing information about books in a library. The program should be able to parse the XML data, modify it according to specific requirements, and then output the modified XML data. Specifications 1. **Input XML Format**: ```xml <?xml version=\\"1.0\\"?> <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price currency=\\"USD\\">29.99</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>1999</year> <price currency=\\"USD\\">39.99</price> </book> <!-- More book entries --> </library> ``` 2. **Task Requirements**: - Parse the XML data using `xml.etree.ElementTree`. - Increase the price of each book by a given percentage. - Add an attribute `discounted=\\"yes\\"` to each book if it was published before the year 2000. - Output the modified XML data. 3. **Function Implementations**: - `increase_prices(root: ET.Element, percentage: float) -> None`: This function takes the root element of the XML tree and a percentage value, then increases the price of each book by the given percentage. - `apply_discounts(root: ET.Element) -> None`: This function adds a `discounted=\\"yes\\"` attribute to each book published before the year 2000. - `main(input_xml: str, percentage: float) -> str`: This function coordinates the parsing of the input XML string, applies the price increases and discounts, and returns the modified XML as a string. 4. **Expected Outputs**: - **Function `increase_prices`:** No return value but it will modify the `root` in place. - **Function `apply_discounts`:** No return value but it will modify the `root` in place. - **Function `main`:** Returns a string representing the modified XML. Example Usage ```python import xml.etree.ElementTree as ET def increase_prices(root: ET.Element, percentage: float) -> None: for price in root.findall(\'.//price\'): curr_price = float(price.text) new_price = curr_price + (curr_price * percentage / 100) price.text = f\\"{new_price:.2f}\\" def apply_discounts(root: ET.Element) -> None: for book in root.findall(\'.//book\'): year = int(book.find(\'year\').text) if year < 2000: book.set(\'discounted\', \'yes\') def main(input_xml: str, percentage: float) -> str: root = ET.fromstring(input_xml) increase_prices(root, percentage) apply_discounts(root) return ET.tostring(root, encoding=\'unicode\') # Example XML input xml_input = <?xml version=\\"1.0\\"?> <library> <book> <title>Book Title 1</title> <author>Author 1</author> <year>2001</year> <price currency=\\"USD\\">29.99</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <year>1999</year> <price currency=\\"USD\\">39.99</price> </book> </library> # Example function call modified_xml = main(xml_input, 10) print(modified_xml) ``` # Constraints - The XML is well-formed and adheres to the format specified. - The percentage value is a positive float.","solution":"import xml.etree.ElementTree as ET def increase_prices(root: ET.Element, percentage: float) -> None: Increases the prices of all books by the given percentage. for price in root.findall(\'.//price\'): curr_price = float(price.text) new_price = curr_price + (curr_price * percentage / 100) price.text = f\\"{new_price:.2f}\\" def apply_discounts(root: ET.Element) -> None: Adds a \'discounted=\\"yes\\"\' attribute to books published before the year 2000. for book in root.findall(\'.//book\'): year = int(book.find(\'year\').text) if year < 2000: book.set(\'discounted\', \'yes\') def main(input_xml: str, percentage: float) -> str: Parses the input XML string, applies price increases and discounts, and returns the modified XML as a string. root = ET.fromstring(input_xml) increase_prices(root, percentage) apply_discounts(root) return ET.tostring(root, encoding=\'unicode\')"},{"question":"# Question: Unicode Character Analysis You are tasked with creating a function that takes in a string `s` and returns a dictionary containing various Unicode properties of each character present in the string, using Python\'s `unicodedata` module. Function Signature ```python def unicode_character_analysis(s: str) -> dict: pass ``` Input - `s`: A string containing Unicode characters. Output - A dictionary where keys are the unique characters in the string `s`, and values are nested dictionaries containing the following properties for each character: - `name`: The Unicode name of the character. - `decimal`: The decimal value if applicable, otherwise `None`. - `category`: The general category of the character. - `bidirectional`: The bidirectional class of the character. - `combining`: The combining class of the character. - `east_asian_width`: The East Asian width class of the character. - `mirrored`: `True` if the character is mirrored in bidirectional text, `False` otherwise. - `decomposition`: The decomposition mapping of the character. - `is_normalized_NFC`: `True` if the character is normalized in NFC form, `False` otherwise. - `is_normalized_NFKC`: `True` if the character is normalized in NFKC form, `False` otherwise. - `is_normalized_NFD`: `True` if the character is normalized in NFD form, `False` otherwise. - `is_normalized_NFKD`: `True` if the character is normalized in NFKD form, `False` otherwise. Constraints - The input string `s` will have a length of at most `10^5` characters. Example ```python input_string = \'A大1\' result = unicode_character_analysis(input_string) \'\'\' Expected output: { \'A\': { \'name\': \'LATIN CAPITAL LETTER A\', \'decimal\': None, \'category\': \'Lu\', \'bidirectional\': \'L\', \'combining\': 0, \'east_asian_width\': \'Na\', \'mirrored\': False, \'decomposition\': \'\', \'is_normalized_NFC\': True, \'is_normalized_NFKC\': True, \'is_normalized_NFD\': False, # Because in NFD, it would be decomposed \'is_normalized_NFKD\': False # Because in NFKD, it would be decomposed }, \'大\': { \'name\': \'CJK UNIFIED IDEOGRAPH-5927\', \'decimal\': None, \'category\': \'Lo\', \'bidirectional\': \'L\', \'combining\': 0, \'east_asian_width\': \'W\', \'mirrored\': False, \'decomposition\': \'\', \'is_normalized_NFC\': True, \'is_normalized_NFKC\': True, \'is_normalized_NFD\': True, \'is_normalized_NFKD\': True }, \'1\': { \'name\': \'DIGIT ONE\', \'decimal\': 1, \'category\': \'Nd\', \'bidirectional\': \'EN\', \'combining\': 0, \'east_asian_width\': \'Na\', \'mirrored\': False, \'decomposition\': \'\', \'is_normalized_NFC\': True, \'is_normalized_NFKC\': True, \'is_normalized_NFD\': True, \'is_normalized_NFKD\': True } } \'\'\' ``` Notes - For the `decimal` property, if the character does not have an associated decimal value, return `None`.","solution":"import unicodedata def unicode_character_analysis(s: str) -> dict: result = {} for char in set(s): properties = {} properties[\'name\'] = unicodedata.name(char) properties[\'decimal\'] = unicodedata.decimal(char, None) properties[\'category\'] = unicodedata.category(char) properties[\'bidirectional\'] = unicodedata.bidirectional(char) properties[\'combining\'] = unicodedata.combining(char) properties[\'east_asian_width\'] = unicodedata.east_asian_width(char) properties[\'mirrored\'] = unicodedata.mirrored(char) properties[\'decomposition\'] = unicodedata.decomposition(char) properties[\'is_normalized_NFC\'] = unicodedata.is_normalized(\'NFC\', char) properties[\'is_normalized_NFKC\'] = unicodedata.is_normalized(\'NFKC\', char) properties[\'is_normalized_NFD\'] = unicodedata.is_normalized(\'NFD\', char) properties[\'is_normalized_NFKD\'] = unicodedata.is_normalized(\'NFKD\', char) result[char] = properties return result"},{"question":"You are tasked with creating a custom image processing library that efficiently processes large images represented as numpy arrays. To optimize performance, you need to directly access and manipulate the underlying memory buffers of these arrays without intermediate copying. Implement a function `process_image_buffer` that takes a numpy array and a processing function as inputs. The function should obtain a writable memory buffer of the array, apply the processing function in-place on the buffer, and ensure all resources are properly released afterward. # Requirements: 1. **Function Signature**: ```python def process_image_buffer(image_array: np.ndarray, process_func: Callable[[memoryview], None]) -> None: ``` 2. **Inputs**: - `image_array` (numpy.ndarray): A multi-dimensional numpy array representing the image. - `process_func` (Callable[[memoryview], None]): A function that takes a memoryview and processes the data in-place. 3. **Outputs**: - The function should modify `image_array` in-place and return `None`. 4. **Constraints**: - The `image_array` should be writable. - The processing function `process_func` should not modify the size or shape of the data. - If the buffer cannot be obtained or isn\'t writable, raise a `BufferError`. 5. **Example**: ```python import numpy as np def invert_colors(memview): for i in range(len(memview)): memview[i] = 255 - memview[i] image = np.array([[10, 20, 30], [40, 50, 60]], dtype=np.uint8) process_image_buffer(image, invert_colors) print(image) # Output should be array([[245, 235, 225], [215, 205, 195]]) ``` # Implementation Notes: - Use the `PyObject_GetBuffer` and `PyBuffer_Release` functions or their equivalent in Python to obtain and release the buffer. - Handle buffer contiguity and ensure that the buffer is writable. # Hints: - You may use `memoryview` to access the buffer in Python. - Use the `flags` argument to ensure the buffer obtained is writable. # Evaluation: Your implementation will be assessed based on: - Correctness and efficiency of the buffer handling. - Proper release of resources to avoid memory leaks. - Adherence to provided constraints and requirements.","solution":"import numpy as np from typing import Callable def process_image_buffer(image_array: np.ndarray, process_func: Callable[[memoryview], None]) -> None: if not image_array.flags[\'WRITEABLE\']: raise BufferError(\\"The array must be writable.\\") # Create a writable memoryview memview = memoryview(image_array).cast(\'B\') try: # Apply the processing function process_func(memview) finally: # Release the memoryview (not strictly necessary in Python, but good practice) memview.release()"},{"question":"# Question: Custom Copying for a Nested Object You are tasked with implementing a class that represents a node in a graph. Each node has a unique identifier and a list of references to other nodes (its neighbors). Your objective is to ensure the class supports shallow and deep copying correctly, taking into account the potential for cyclic references. Class Definition ```python class GraphNode: def __init__(self, identifier): Initialize the GraphNode with a unique identifier and an empty list of neighbors. :param identifier: Unique identifier for the node (an integer) self.identifier = identifier self.neighbors = [] def __repr__(self): Return a string representation of the GraphNode. return f\\"GraphNode({self.identifier})\\" ``` Tasks 1. Modify the `GraphNode` class to implement the `__copy__` method for shallow copying. 2. Modify the `GraphNode` class to implement the `__deepcopy__` method for deep copying, using the `memo` dictionary to handle cyclic references. 3. Write a function `test_copying()` to demonstrate shallow and deep copying of a `GraphNode` instance, showing how copies behave when the original is modified. Constraints - `GraphNode` instances can have cyclic references (a node may be its own neighbor directly or indirectly). Example ```python # Creating graph nodes node1 = GraphNode(1) node2 = GraphNode(2) node1.neighbors.append(node2) node2.neighbors.append(node1) # Write code to demonstrate shallow and deep copies test_copying(node1) # Expected output: # Demonstrate shallow copy: Modifications to the original node should reflect in the shallow copy. # Demonstrate deep copy: Modifications to the original node should not affect the deep copy. ``` # Note - The `test_copying` function should instantiate `node1`, create shallow and deep copies, and then modify the original and observe the changes (or lack thereof) in the copies.","solution":"import copy class GraphNode: def __init__(self, identifier): Initialize the GraphNode with a unique identifier and an empty list of neighbors. :param identifier: Unique identifier for the node (an integer) self.identifier = identifier self.neighbors = [] def __repr__(self): Return a string representation of the GraphNode. return f\\"GraphNode({self.identifier})\\" def __copy__(self): Create a shallow copy of the GraphNode. cls = self.__class__ result = cls.__new__(cls) result.__dict__.update(self.__dict__) result.neighbors = self.neighbors[:] # copy the list of neighbors shallowly return result def __deepcopy__(self, memo): Create a deep copy of the GraphNode, handling cyclic references. if id(self) in memo: return memo[id(self)] cls = self.__class__ result = cls.__new__(cls) memo[id(self)] = result result.identifier = self.identifier result.neighbors = copy.deepcopy(self.neighbors, memo) return result def test_copying(): # Creating graph nodes node1 = GraphNode(1) node2 = GraphNode(2) node1.neighbors.append(node2) node2.neighbors.append(node1) # Shallow copy shallow_copy = copy.copy(node1) # Deep copy deep_copy = copy.deepcopy(node1) # Modify the original node to show the behavior in copies node1.identifier = 3 node1.neighbors.append(GraphNode(4)) print(f\\"Original: {node1}, Neighbors: {node1.neighbors}\\") print(f\\"Shallow Copy: {shallow_copy}, Neighbors: {shallow_copy.neighbors}\\") print(f\\"Deep Copy: {deep_copy}, Neighbors: {deep_copy.neighbors}\\") # Call the test function to demonstrate the copying test_copying()"},{"question":"# Question: Custom Storage Manipulation in PyTorch You are provided with a PyTorch tensor, and your task is to perform a series of operations on this tensor\'s underlying storage. You will write a function `manipulate_storage` that performs the following steps: 1. **Clone the storage of the given tensor**. 2. **Fill the cloned storage with zeros**. 3. **Create and return a new tensor that references this cloned storage with the same shape, stride, and offset as the original tensor**. Your function should take a single input tensor and return the modified tensor. Function Signature: ```python def manipulate_storage(t: torch.Tensor) -> torch.Tensor: pass ``` # Constraints: - The input tensor `t` will always be a 1-dimensional tensor of type `torch.float32`. - Do not use high-level APIs like `torch.zeros_like` or `torch.clone` for the final tensor creation; directly manipulate the storage as described. - Assume `torch` is already imported. # Example: ```python import torch # Example 1 t = torch.tensor([1.0, 2.0, 3.0]) result = manipulate_storage(t) print(result) # Output should be tensor([0., 0., 0.]) # Example 2 t = torch.tensor([4.5, 6.7, 8.9]) result = manipulate_storage(t) print(result) # Output should be tensor([0., 0., 0.]) ``` # Explanation: For each example, the input tensor is `[1.0, 2.0, 3.0]` and `[4.5, 6.7, 8.9]` respectively. After manipulating the storage as described (cloning and filling with zeros), the output tensor should be `[0.0, 0.0, 0.0]` in both cases, while still utilizing the same storage mechanism. Use the function signature provided and ensure the correct operations are performed without raising exceptions.","solution":"import torch def manipulate_storage(t: torch.Tensor) -> torch.Tensor: Manipulates the storage of the given tensor as per the specified steps. 1. Clones the storage of the given tensor. 2. Fills the cloned storage with zeros. 3. Creates and returns a new tensor that references this cloned storage with the same shape, stride, and offset as the original tensor. Parameters: t (torch.Tensor): Input tensor. Returns: torch.Tensor: New tensor with manipulated storage. # Clone storage original_storage = t.storage() cloned_storage = original_storage.clone() # Fill the cloned storage with zeros cloned_storage.fill_(0) # Create a new tensor with the same properties using the cloned storage new_tensor = torch.tensor([], dtype=t.dtype).set_( cloned_storage, t.storage_offset(), t.size(), t.stride()) return new_tensor"},{"question":"# Introduction You are tasked with creating a Python script that retrieves content from a URL and processes it based on certain conditions. You will use the `urllib.request` module, specifically leveraging `urlopen` and custom handlers. # Problem Statement Create a function `fetch_url_content` that takes a URL as its input, opens the URL, retrieves its content, and processes the content according to the following steps: 1. **URL Handling**: - If the given URL uses the `http` scheme, handle any HTTP redirection automatically. - If the given URL uses the `https` scheme, ensure that an appropriate SSL context is used with default settings. - Use a proxy if specified via environment variables or programmatically. 2. **Content Retrieval**: - Fetch the contents of the URL. - Analyze and determine the content type from the response headers. 3. **Processing**: - If the content type is `text/html`, extract and return the first 1000 characters of the HTML content. - If the content type is `application/json`, parse the JSON and return the parsed object. - If the content type is `image/jpeg`, save the image locally and return the file path. - For other content-types, simply return the number of bytes fetched. 4. **Authentication**: - If authentication is required (HTTP Status 401 or 403), retry the request with a username and password provided via function parameters. # Function Signature ```python def fetch_url_content(url: str, username: str = None, password: str = None) -> Any: pass ``` # Input - `url`: A string representing the URL to fetch the content from. - `username`: (Optional) A string representing the username for authentication. - `password`: (Optional) A string representing the password for authentication. # Output - The function should return different types of outputs based on the content type: - First 1000 characters of HTML content (string) if the content is `text/html`. - Parsed JSON object if the content is `application/json`. - Local file path (`str`) if the content is an image (`image/jpeg`). - Number of bytes fetched (integer) for any other content types. # Constraints - Use the `urllib.request` module for all URL operations. - Ensure appropriate handling of HTTP and HTTPS schemes. - Implement necessary error handling for network and protocol errors. - Do not use third-party libraries for HTTP requests, stick to `urllib.request`. # Performance Requirements - The function should handle content retrieval efficiently. - Ensure minimal network overhead. - Avoid unnecessary retries and handle redirections efficiently. # Example Usage ```python html_content = fetch_url_content(\\"http://example.com\\") print(html_content) # Should print the first 1000 characters of HTML content json_content = fetch_url_content(\\"http://example.com/api/data\\") print(json_content) # Should print the parsed JSON content image_path = fetch_url_content(\\"http://example.com/images/sample.jpg\\") print(image_path) # Should print the local file path of the saved image data_size = fetch_url_content(\\"http://example.com/somefile.txt\\") print(data_size) # Should print the number of bytes fetched ```","solution":"import urllib.request import ssl import json import os from urllib.error import HTTPError, URLError from urllib.parse import urlparse def fetch_url_content(url: str, username: str = None, password: str = None) -> any: def save_image(content): filename = \'downloaded_image.jpg\' with open(filename, \'wb\') as f: f.write(content) return filename # Create SSL context for HTTPS URLs context = ssl.create_default_context() # Setting up a basic authentication handler if credentials are provided if username and password: password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm() password_mgr.add_password(None, url, username, password) handler = urllib.request.HTTPBasicAuthHandler(password_mgr) opener = urllib.request.build_opener(handler) urllib.request.install_opener(opener) try: response = urllib.request.urlopen(url, context=context) except HTTPError as e: # If authentication credentials are required if e.code == 401 or e.code == 403 and username and password: # Retry with credentials added response = urllib.request.urlopen(url, context=context) else: raise except URLError as e: raise content_type = response.headers.get(\'Content-Type\') content = response.read() if \'text/html\' in content_type: return content[:1000].decode(\'utf-8\') elif \'application/json\' in content_type: return json.loads(content) elif \'image/jpeg\' in content_type: return save_image(content) else: return len(content)"},{"question":"**Objective:** The purpose of this task is to assess your understanding of using Seaborn for data visualization, specifically in creating bar plots and counting distinct observations while applying groupings and additional transformations. **Question:** You are given a dataset of Titanic passengers which contains the following columns: - `sex` (Gender of the passenger: male or female) - `pclass` (Passenger class: 1, 2, or 3) - `survived` (Survival status: 0 = No, 1 = Yes) Write a Python function named `plot_survival_by_class_and_gender` that: 1. Loads the Titanic dataset from Seaborn using `load_dataset(\\"titanic\\")`. 2. Creates a bar plot using the `seaborn.objects` that shows the count of passengers grouped by `pclass` and further divided by `sex`. 3. Uses different colors for different genders (male and female) with a legend. 4. Sets appropriate axis labels and a title for the plot. 5. Displays the plot. **Input:** - No input parameters. **Output:** - The function should simply display the plot. You don\'t need to return any values. **Additional Requirements:** - Do not use any other libraries for plotting. - Follow good coding practices, including clear comments and function documentation. **Starter Code:** ```python import seaborn.objects as so from seaborn import load_dataset def plot_survival_by_class_and_gender(): # Load the Titanic dataset titanic = load_dataset(\\"titanic\\") # Create and display the plot so.Plot(titanic, x=\\"pclass\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) # Customize the plot (labels and title) # (Your customization code goes here) # Display the plot # (Your display code goes here) ``` Complete the function `plot_survival_by_class_and_gender` based on the above description. **Constraints:** - Assume the dataset `titanic` is preloaded using `seaborn.load_dataset`. - Ensure the plot is clear and well-labeled. **Example:** On executing `plot_survival_by_class_and_gender()`, a bar plot should be displayed showing: - The count of passengers in each passenger class (`pclass`), divided by gender.","solution":"import seaborn.objects as so from seaborn import load_dataset def plot_survival_by_class_and_gender(): This function loads the Titanic dataset and creates a bar plot showing the count of passengers, grouped by passenger class and divided by gender. # Load the Titanic dataset titanic = load_dataset(\\"titanic\\") # Create the plot plot = so.Plot(titanic, x=\\"pclass\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) # Customize the plot (labels and title) plot.scale(y=\\"linear\\").label(x=\\"Passenger Class\\", y=\\"Count\\", title=\\"Titanic Passengers by Class and Gender\\") # Display the plot plot.show() # Call the function for manual verification plot_survival_by_class_and_gender()"},{"question":"**Python Coding Assessment: Validate and Normalize Source Code** # Problem Statement: In Python, the structure of the source code is critical for correct parsing and execution. This problem requires you to implement a function that: 1. Validates the indentation levels in a given Python source code string to ensure there are no inconsistent levels. 2. Converts physical lines ending with a backslash (``) into logical lines. # Requirements: 1. **Indentation Validation:** - Your function should check if the indentation is consistent throughout the source code. - Indentation should be either spaces or tabs, but not mixed within a file. - The function should return `True` if the indentation is consistent and `False` otherwise. 2. **Logical Line Construction:** - Your function should construct complete logical lines by joining physical lines ending with a backslash (``). - The backslash character and the newline character following it should be removed during this process. - Blank lines and comments should remain unchanged in terms of their physical appearance but should be considered correctly in logical line construction. # Input: - A list of strings `source_code`, where each string represents a line of Python source code. # Output: - A tuple of two elements: - A boolean indicating whether the indentation is consistent. - A list of strings representing the normalized source code with physical lines converted to logical lines. # Constraints: - You can assume that comments start with `#` and extend to the end of the line. - The code may contain nested indentation. - You can assume the input source code does not contain any syntax errors other than potential inconsistent indentation. # Example: ```python source_code = [ \\"def example_function():\\", \\" if True:\\", \\" print(\\"This is a logical\\", \\" line split across\\", \\" multiple physical lines.\\")\\", \\" # This is a comment line\\", \\" return True\\" ] # Expected Output: ( True, [ \\"def example_function():\\", \\"if True: print(\\"This is a logical line split across multiple physical lines.\\")\\", \\"# This is a comment line\\", \\"return True\\" ] ) source_code_inconsistent = [ \\"def example_function():\\", \\" if True:\\", \\"tprint(\\"Inconsistent indentation.\\")\\", \\" return True\\" ] # Expected Output: ( False, [ \\"def example_function():\\", \\"if True:\\", \\"tprint(\\"Inconsistent indentation.\\")\\", \\"return True\\" ] ) # Function Signature: def validate_and_normalize(source_code: List[str]) -> Tuple[bool, List[str]]: pass ``` # Notes: - Pay particular attention to mixed use of spaces and tabs which should be carefully identified and handled. - Ensure to maintain the textual integrity of comments and only modify lines where needed. - Consider edge cases such as source code lines with only indentation or comments. Good luck!","solution":"from typing import List, Tuple def validate_and_normalize(source_code: List[str]) -> Tuple[bool, List[str]]: def is_consistent_indentation(lines: List[str]) -> bool: indent_type = None for line in lines: stripped_line = line.lstrip() if stripped_line == \\"\\" or stripped_line.startswith(\'#\'): continue current_indent = line[:len(line) - len(stripped_line)] if indent_type is None: if current_indent: indent_type = \' \' if \' \' in current_indent else \'t\' else: if (current_indent and indent_type == \' \' and \'t\' in current_indent) or (current_indent and indent_type == \'t\' and \' \' in current_indent): return False return True def join_physical_lines(lines: List[str]) -> List[str]: logical_lines = [] buffer = \\"\\" for line in lines: if line.rstrip().endswith(\'\'): buffer += line.rstrip()[:-1] else: if buffer: logical_lines.append(buffer + line) buffer = \\"\\" else: logical_lines.append(line) return logical_lines # Validate indentation is_consistent = is_consistent_indentation(source_code) # Normalize lines normalized_code = join_physical_lines(source_code) # Remove leading whitespace if lines do not have mixed indent normalized_code = [line.lstrip() if is_consistent else line for line in normalized_code] return is_consistent, normalized_code"}]'),D={name:"App",components:{PoemCard:I},data(){return{searchQuery:"",visibleCount:4,poemsData:A,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},F={class:"search-container"},z={class:"card-container"},q={key:0,class:"empty-state"},R=["disabled"],N={key:0},O={key:1};function j(n,e,l,m,i,o){const h=g("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",F,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),_(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",z,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",q,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",N,"See more"))],8,R)):d("",!0)])}const L=p(D,[["render",j],["__scopeId","data-v-b3414e0e"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"deepseek/63.md","filePath":"deepseek/63.md"}'),M={name:"deepseek/63.md"},B=Object.assign(M,{setup(n){return(e,l)=>(a(),s("div",null,[x(L)]))}});export{Y as __pageData,B as default};
