import{_ as p,o as a,c as s,a as t,m as u,t as c,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},E={class:"review-title"},P={class:"review-content"};function S(n,e,l,m,i,o){return a(),s("div",T,[t("div",C,[t("div",E,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),u(c(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",P,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),u(c(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-4af83f8a"]]),I=JSON.parse('[{"question":"You are tasked with creating a series of visualizations using the Seaborn `objects` interface on the penguins dataset. Your goal is to illustrate your understanding of the `.facet`, `.share`, and `.pair` methods of the `so.Plot` object. Question: 1. **Faceted Scatter Plot:** Write a function `faceted_scatter_plot` that: - Loads the penguins dataset using `seaborn.load_dataset(\\"penguins\\")`. - Creates a faceted scatter plot of `bill_length_mm` against `bill_depth_mm`. - Facets by both `species` and `sex`. - Ensures that the axes are not shared between facets (each subplot should adapt its axes independently). 2. **Paired Plot with Shared Axis:** Write a function `paired_plot_with_shared_axis` that: - Loads the penguins dataset using `seaborn.load_dataset(\\"penguins\\")`. - Creates a paired plot with `bill_length_mm` and `bill_depth_mm` on the x-axes, and `flipper_length_mm` on the y-axis. - Ensures that the x-axis is shared across the paired plots. # Expected Function Signature: ```python import seaborn.objects as so def faceted_scatter_plot(): # Your code here def paired_plot_with_shared_axis(): # Your code here ``` # Constraints: - You must use the seaborn objects interface (`seaborn.objects`). - Ensure that the plots render correctly with visible axes and labels. # Additional Information: - If you need to show the plots, you can display them using the `.plot()` method in a Jupyter notebook environment. - You can assume that the seaborn package is already installed and available for import. Write the two functions and ensure they produce the expected visualizations when called. Include comments in your code to explain your logic and any important steps.","solution":"import seaborn.objects as so import seaborn as sns def faceted_scatter_plot(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a faceted scatter plot of bill_length_mm vs bill_depth_mm # Facet by species and sex, and ensure axes are not shared plot = ( so.Plot(penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") .facet(\\"species\\", \\"sex\\") .share(x=False, y=False) .add(so.Dot()) ) # Display the plot plot.plot() def paired_plot_with_shared_axis(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a paired plot with bill_length_mm and bill_depth_mm on x-axis # and flipper_length_mm on y-axis. Ensure the x-axis is shared plot = ( so.Plot(penguins, y=\\"flipper_length_mm\\") .pair(x=[\\"bill_length_mm\\", \\"bill_depth_mm\\"]) .share(x=True) .add(so.Dot()) ) # Display the plot plot.plot()"},{"question":"# Seaborn Residual Plot Analysis Objective You are required to analyze a dataset using seaborn\'s `residplot` function to create and manipulate residual plots. Your task is to write a Python function that takes specific parameters and generates a series of residual plots to identify and interpret patterns or assumptions violations in a linear regression model. Function Signature ```python def analyze_residuals(data: pd.DataFrame, x: str, y: str): Generate and analyze residual plots of a given dataset. Parameters: - data: pd.DataFrame: The dataset to analyze. - x: str: The predictor variable (independent variable). - y: str: The response variable (dependent variable). Returns: - None: The function should display the plots using seaborn. ``` Input - `data`: A Pandas DataFrame containing the dataset. - `x`: A string representing the name of the predictor variable. - `y`: A string representing the name of the response variable. Output The function should generate and display the following residual plots using seaborn: 1. A basic residual plot without any additional parameters. 2. A residual plot to check for higher-order trends by setting `order=2`. 3. A residual plot with a LOWESS curve added, setting `lowess=True` and changing the curve\'s color to red (`line_kws=dict(color=\\"r\\")`). Requirements - Use the seaborn library to create the residual plots. - Interpret the plots to identify any patterns or violations of linear regression assumptions: - For the basic residual plot, look for non-random patterns or structures in the residuals. - For the higher-order trend plot, check if adding the `order=2` parameter stabilizes the residuals. - For the LOWESS curve plot, observe how the locally weighted scatterplot smoothing helps to reveal or emphasize the structure. Constraints - Use the `mpg` dataset available in seaborn for testing your function. - Ensure your function is well-documented and includes comments explaining each step of the process. Example Usage ```python import seaborn as sns import pandas as pd # Load the dataset mpg = sns.load_dataset(\\"mpg\\") # Analyze residuals for the \'horsepower\' predictor and \'mpg\' response analyze_residuals(mpg, \'horsepower\', \'mpg\') ``` Note While submitting your solution, ensure that your code is clean, well-commented, and follows best practices for readability and maintainability.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def analyze_residuals(data: pd.DataFrame, x: str, y: str): Generate and analyze residual plots of a given dataset. Parameters: - data: pd.DataFrame: The dataset to analyze. - x: str: The predictor variable (independent variable). - y: str: The response variable (dependent variable). Returns: - None: The function displays the plots using seaborn. plt.figure(figsize=(15, 5)) # Basic residual plot plt.subplot(1, 3, 1) sns.residplot(x=x, y=y, data=data) plt.title(\'Basic Residual Plot\') # Residual plot with higher-order trend (order=2) plt.subplot(1, 3, 2) sns.residplot(x=x, y=y, data=data, order=2) plt.title(\'Residual Plot with order=2\') # Residual plot with LOWESS curve plt.subplot(1, 3, 3) sns.residplot(x=x, y=y, data=data, lowess=True, line_kws=dict(color=\\"r\\")) plt.title(\'Lowess Smoothing Residual Plot\') plt.tight_layout() plt.show()"},{"question":"**Objective**: Implement a system that models a generic cache with type-safe keys and values. Your implementation should ensure type safety while storing and retrieving items from the cache. # Requirements 1. **Type-safe Keys and Values**: - Use `NewType` to create distinct types for keys. - Use generics to allow flexibility in the cache\'s value types. 2. **Methods**: - `add_item(key: KeyType, value: ValueType) -> None`: Adds an item to the cache with the specified key and value. - `get_item(key: KeyType) -> Optional[ValueType]`: Retrieves an item from the cache using the specified key. Returns `None` if the key is not found. - `remove_item(key: KeyType) -> bool`: Removes an item from the cache using the specified key. Returns `True` if the removal was successful or `False` if the key was not found. - `clear_cache() -> None`: Clears all items from the cache. 3. **Cache Management**: - The cache should store items in a dictionary. - Ensure that the cache only accepts keys of type `KeyType` and values of type `ValueType`. 4. **Type Annotations**: - Use appropriate type hints using the `typing` module. # Input and Output Format Example Usage ```python from typing import Optional, Generic, TypeVar, Dict from typing import NewType KeyType = NewType(\'KeyType\', int) ValueType = TypeVar(\'ValueType\') class GenericCache(Generic[ValueType]): def __init__(self) -> None: self._cache: Dict[KeyType, ValueType] = {} def add_item(self, key: KeyType, value: ValueType) -> None: self._cache[key] = value def get_item(self, key: KeyType) -> Optional[ValueType]: return self._cache.get(key) def remove_item(self, key: KeyType) -> bool: if key in self._cache: del self._cache[key] return True return False def clear_cache(self) -> None: self._cache.clear() ``` # Constraints - Use the `NewType` construct for defining `KeyType`. - The implementation should be generic to support any type of value. - Use the `Optional` type to indicate possible absence of a value. - Ensure there are no runtime type checks, but type hints should be followed for static type checkers. # Performance Requirements - The implementation should be efficient with respect to cache operations, operating with O(1) average time complexity for add, get, and remove operations due to the dictionary\'s underlying hash table.","solution":"from typing import Generic, TypeVar, Optional, Dict, NewType KeyType = NewType(\'KeyType\', int) ValueType = TypeVar(\'ValueType\') class GenericCache(Generic[ValueType]): def __init__(self) -> None: self._cache: Dict[KeyType, ValueType] = {} def add_item(self, key: KeyType, value: ValueType) -> None: self._cache[key] = value def get_item(self, key: KeyType) -> Optional[ValueType]: return self._cache.get(key) def remove_item(self, key: KeyType) -> bool: if key in self._cache: del self._cache[key] return True return False def clear_cache(self) -> None: self._cache.clear()"},{"question":"**Objective:** Implement an asynchronous chat client using the `asynchat.async_chat` class. This client should be capable of connecting to a chat server, sending messages, and receiving broadcast messages from other clients. **Details:** 1. Create a class `ChatClient` that subclasses `asynchat.async_chat`. 2. Implement the following methods in the `ChatClient` class: - `__init__(self, host, port, nickname)`: Establishes a connection to the server and initializes necessary attributes. - `collect_incoming_data(self, data)`: Collects data received from the server. - `found_terminator(self)`: Handles the data once the end-of-message terminator is found. - `send_message(self, message)`: Sends a message to the chat server, which will then be broadcast to other clients. 3. The client should use a newline character (`\'n\'`) as the message terminator. 4. Messages from the server should be printed to the console prefixed with `[SERVER]`. 5. Server responses are guaranteed to end with a newline character. **Input/Output:** - The `send_message` method takes a string `message` to send to the server. - Messages received from the server should be processed and printed to the console with the prefix `[SERVER]`. **Constraints:** - Ensure the client can handle simultaneous sending and receiving of messages. - Manage any input/output buffers appropriately to avoid memory issues. - The implementation should be efficient and handle typical chat client usage scenarios. Here is the template to get you started: ```python import asynchat import asyncore import socket class ChatClient(asynchat.async_chat): def __init__(self, host, port, nickname): asynchat.async_chat.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, port)) self.set_terminator(b\'n\') self.ibuffer = [] self.nickname = nickname self.send_message(nickname + \\" has joined the chat.\\") def collect_incoming_data(self, data): Append incoming data to buffer self.ibuffer.append(data) def found_terminator(self): Process a complete message from the server message = b\'\'.join(self.ibuffer).decode(\'utf-8\') self.ibuffer = [] print(\\"[SERVER]\\", message) def send_message(self, message): Send a message to the server self.push((message + \'n\').encode(\'utf-8\')) if __name__ == \\"__main__\\": client = ChatClient(\'localhost\', 12345, \'nickname\') asyncore.loop() ``` **Additional Information:** - The host and port should be provided when initializing the `ChatClient` instance. - Run the client within an `asyncore` loop to handle asynchronous events. Provide additional features or improvements based on your understanding of asynchronous network programming to enhance the chat client\'s functionality.","solution":"import asynchat import asyncore import socket class ChatClient(asynchat.async_chat): def __init__(self, host, port, nickname): super().__init__() self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.connect((host, port)) self.set_terminator(b\'n\') self.ibuffer = [] self.nickname = nickname self.send_message(f\'{nickname} has joined the chat.n\') def collect_incoming_data(self, data): Collects data received from the server into the internal buffer. self.ibuffer.append(data) def found_terminator(self): Handles the data once the end-of-message terminator is found. message = b\'\'.join(self.ibuffer).decode(\'utf-8\') self.ibuffer = [] print(f\'[SERVER] {message}\') def send_message(self, message): Sends a message to the chat server. self.push((message + \'n\').encode(\'utf-8\')) if __name__ == \\"__main__\\": client = ChatClient(\'localhost\', 12345, \'nickname\') asyncore.loop()"},{"question":"Objective You are to implement functionality using the `modulefinder` module to analyze multiple Python scripts and produce a comprehensive report of the modules each script imports, the modules that failed to import, and the global variables and functions used in each module. Task Implement a function `analyze_scripts(file_paths: list, exclusions: list = []) -> dict` that accepts a list of file paths leading to Python scripts and an optional list of module names to exclude from the analysis. Your function should: 1. Use `ModuleFinder` to analyze each script. 2. Produce a report containing: - The names of the modules each script imports. - The global variables and functions used in each module (output at most 5 global names per module). - The modules that failed to import for each script. 3. Return a dictionary summarizing the above information for each script. Input - `file_paths`: A list of strings, each string being the path to a Python script file. - `exclusions`: (Optional) A list of module names to exclude from analysis. Output A dictionary where: - The keys are the file names (not paths) of the scripts. - The values are another dictionary with three keys: - `imported_modules`: A dictionary where keys are module names and values are lists of at most 5 global names found in each module. - `failed_imports`: A list of module names that failed to import. Example Here\'s an example showing a possible output structure. ```python { \\"bacon.py\\": { \\"imported_modules\\": { \\"re\\": [\\"compile\\", \\"findall\\", \\"match\\"], \\"itertools\\": [\\"count\\", \\"cycle\\", \\"repeat\\"], \\"other_module\\": [\\"func1\\", \\"var1\\", \\"class1\\"] }, \\"failed_imports\\": [\\"baconhameggs\\", \\"guido.python.ham\\"] }, \\"another_script.py\\": { \\"imported_modules\\": { \\"os\\": [\\"path\\", \\"getcwd\\", \\"listdir\\"], \\"sys\\": [\\"argv\\", \\"exit\\", \\"version\\"] }, \\"failed_imports\\": [] } } ``` Constraints - Assume the paths provided in `file_paths` are valid and the files are accessible. - Do not import additional libraries outside of the standard Python library. Note - You may refer to the documentation provided on `modulefinder` for details on its usage. - Be sure to handle scripts which can have imports that don\'t exist or will not be captured in the current environment.","solution":"import modulefinder import os def analyze_scripts(file_paths: list, exclusions: list = []) -> dict: Analyze the given list of Python scripts and return a report of the modules each script imports, the modules that failed to import, and the global variables and functions used in each module. :param file_paths: List of file paths leading to Python scripts. :param exclusions: List of module names to exclude from the analysis. :return: Comprehensive report as a dictionary. result = {} for file_path in file_paths: file_name = os.path.basename(file_path) result[file_name] = { \\"imported_modules\\": {}, \\"failed_imports\\": [] } finder = modulefinder.ModuleFinder() # Exclude specified modules from analysis for module_name in exclusions: finder.excludes.append(module_name) # Run the analysis finder.run_script(file_path) # Collect information about the imported modules for name, module in finder.modules.items(): if name in exclusions: continue global_names = list(module.globalnames.keys())[:5] result[file_name][\\"imported_modules\\"][name] = global_names # Collect information about the failed imports result[file_name][\\"failed_imports\\"] = list(finder.badmodules.keys()) return result"},{"question":"<|Analysis Begin|> The provided documentation explains the `sndhdr` module in Python, which is used to determine the type of sound data stored in a file. The `sndhdr` module includes utility functions like `sndhdr.what` and `sndhdr.whathdr`, which analyze sound files and return details such as file type, frame rate, number of channels, number of frames, and sample width in the form of a named tuple. Key points from the documentation: 1. The `sndhdr.what(filename)` function determines the type of sound data in a file and returns a named tuple with the attributes: `filetype`, `framerate`, `nchannels`, `nframes`, and `sampwidth`. 2. The `sndhdr.whathdr(filename)` does a similar function based on the file header. 3. Both functions return `None` if the type of sound data cannot be determined. To formulate a challenging and comprehensive assessment question based on this module, we need to focus on: - Understanding and utilizing named tuples. - Handling file input/output. - Implementing error handling for file operations and data validation. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Utilize the `sndhdr` module to analyze multiple sound files in a directory and produce a comprehensive summary report of the sound files\' details. **Task:** Write a Python function `analyze_sound_files(directory: str) -> dict` that scans all sound files within the given directory and returns a dictionary summarizing the details of each sound file. Input: - `directory` (str): The path to the directory containing the sound files. Output: - A dictionary where the keys are filenames and the values are either: - A dictionary with the sound file details: ```python { \'filetype\': str, \'framerate\': int, \'nchannels\': int, \'nframes\': int, \'sampwidth\': int or str, # \'A\' or \'U\' or integer } ``` - The string `\\"Unsupported file type\\"` if the sound file type could not be determined. Constraints: 1. Only process files that are commonly recognized sound file types (e.g., \'.wav\', \'.aiff\', \'.au\'). 2. Perform error handling to skip files that cannot be opened or read. 3. Utilize the `sndhdr` module for sound file analysis. 4. Ensure the output dictionary keys are sorted in alphabetical order. Example: If the directory contains the following files: - `sound1.wav` - `sound2.aiff` - `sound3.txt` (an unsupported file) Your function should return a dictionary like this: ```python { \'sound1.wav\': { \'filetype\': \'wav\', \'framerate\': 44100, \'nchannels\': 2, \'nframes\': 123456, \'sampwidth\': 16 }, \'sound2.aiff\': { \'filetype\': \'aiff\', \'framerate\': 48000, \'nchannels\': 1, \'nframes\': 98765, \'sampwidth\': 8 }, \'sound3.txt\': \\"Unsupported file type\\" } ``` **Implementation Note:** - You may use Python\'s standard libraries such as `os` for directory listing and `sndhdr` for sound file analysis. - Ensure your solution is robust and handles edge cases effectively. Good luck!","solution":"import os import sndhdr def analyze_sound_files(directory): Analyzes all sound files in the specified directory and returns a dictionary summarizing the details of each sound file. Args: - directory (str): The directory containing sound files to be analyzed. Returns: - dict: A dictionary where the keys are filenames and the values are either a dictionary with sound file details or \\"Unsupported file type\\". result = {} for filename in os.listdir(directory): filepath = os.path.join(directory, filename) if os.path.isfile(filepath) and any(filename.endswith(ext) for ext in [\'.wav\', \'.aiff\', \'.au\']): try: sound_info = sndhdr.what(filepath) if sound_info: result[filename] = { \'filetype\': sound_info.filetype, \'framerate\': sound_info.framerate, \'nchannels\': sound_info.nchannels, \'nframes\': sound_info.nframes, \'sampwidth\': sound_info.sampwidth } else: result[filename] = \\"Unsupported file type\\" except Exception as e: result[filename] = \\"Unsupported file type\\" else: result[filename] = \\"Unsupported file type\\" return dict(sorted(result.items())) # Example usage: # directory_path = \\"./sound_files\\" # print(analyze_sound_files(directory_path))"},{"question":"You are given the `penguins` dataset from the Seaborn library, which contains information about penguin species, including measurements of their bill length, bill depth, flipper length, and body mass, as well as their sex. Your task is to visualize relationships between these variables using Seaborn\'s `lmplot`. Problem Statement 1. **Load the Penguins Dataset**: Use Seaborn to load the Penguins dataset and handle any missing data by dropping them. 2. **Plot Requirements**: - Create a regression plot showing the relationship between `bill_length_mm` (on the x-axis) and `bill_depth_mm` (on the y-axis). - Condition this plot on the `species` variable using different colors. - Split the plot into subplots based on the `sex` of the penguins. - Ensure that the x-axis and y-axis limits are allowed to vary across subplots. 3. **Additional Customizations**: - Set a theme using `sns.set_theme()` with the style of your choice (e.g., \\"darkgrid\\"). - Enhance your plot by adding titles and axis labels. - Ensure the figure size is increased for better readability (width = 10, height = 8 units). Expected Functions Implement the function `create_penguin_plots()` which takes no arguments and returns no outputs. It should create and display the required Seaborn plot. Constraints - You should only use the Seaborn library for plotting. - Handle any exceptions that might occur during data loading or plotting. Example ```python import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): # Set theme sns.set_theme(style=\\"darkgrid\\") # Load dataset penguins = sns.load_dataset(\\"penguins\\") penguins = penguins.dropna() # Create lmplot g = sns.lmplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"sex\\", height=4, facet_kws=dict(sharex=False, sharey=False) ) # Customize plot g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.set_titles(\\"{col_name} {col_var}\\") plt.gcf().set_size_inches(10, 8) # Show plot plt.show() # Call the function create_penguin_plots() ``` Make sure to follow the above requirements closely and validate your solution.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_plots(): Loads the penguins dataset, handles missing data, set seaborn theme, and creates a regression plot showing the relationship between \'bill_length_mm\' and \'bill_depth_mm\', conditioned on \'species\' and split by \'sex\'. # Set the theme sns.set_theme(style=\\"darkgrid\\") try: # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Drop missing values penguins = penguins.dropna() # Create the regression plot g = sns.lmplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", col=\\"sex\\", height=4, facet_kws=dict(sharex=False, sharey=False) ) # Customize the plot g.set_axis_labels(\\"Bill Length (mm)\\", \\"Bill Depth (mm)\\") g.set_titles(\\"{col_name} {col_var}\\") plt.gcf().set_size_inches(10, 8) # Show the plot plt.show() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"# Advanced Coding Assessment: File Descriptor Control using `fcntl` Objective Write a Python function that manages file locks using the `fcntl` module, demonstrating advanced knowledge of file descriptor operation control. The function should read a list of file paths, lock them exclusively, and write a unique identifier to each file. Function Details Create a function `lock_and_write(files: list[str], identifier: str) -> dict[str, bool]` that performs the following tasks: 1. Takes a list of file paths (`files`) and a unique identifier (`identifier`) to write to each file. 2. Attempts to lock each file exclusively using `fcntl.flock()` with `fcntl.LOCK_EX`. 3. If locking is successful, writes the provided `identifier` to the file. 4. Unlocks the file after the writing operation. 5. Returns a dictionary where the keys are file paths and the values are booleans indicating whether the operation was successful for each file. Specification 1. **Input:** - `files`: A list of file paths (`list[str]`). - `identifier`: A string identifier to write into the files (`str`). 2. **Output:** - A dictionary (`dict[str, bool]`) where the key is the file path and the value is `True` if the lock and write operation was successful, `False` otherwise. 3. **Constraints:** - Assume all file paths given are valid and the files exist. - Files may be large, so ensure the solution efficiently handles file I/O. 4. **Performance Requirements:** - The function should handle up to 1000 file paths efficiently. - Ensure that the function manages file descriptors and resources properly to avoid memory leaks or file descriptor exhaustion. Example ```python import tempfile import os # Temporary files for testing tmp1 = tempfile.NamedTemporaryFile(delete=False) tmp2 = tempfile.NamedTemporaryFile(delete=False) try: files = [tmp1.name, tmp2.name] identifier = \\"UNIQUE_ID\\" result = lock_and_write(files, identifier) print(result) # Expected: {tmp1.name: True, tmp2.name: True} # Verify the content is written correctly with open(tmp1.name, \'r\') as f: print(f.read()) # Output: UNIQUE_ID with open(tmp2.name, \'r\') as f: print(f.read()) # Output: UNIQUE_ID finally: os.remove(tmp1.name) os.remove(tmp2.name) ``` Notes - Handle exceptions appropriately, particularly when dealing with file locking and I/O operations. - Make use of the `fcntl` module methods, particularly focusing on `fcntl.flock()` for locking.","solution":"import fcntl def lock_and_write(files, identifier): Locks the files exclusively, writes the identifier, and then unlocks them. Args: files (list[str]): List of file paths to operate on. identifier (str): The identifier string to write into each file. Returns: dict[str, bool]: A dictionary with file paths as keys and boolean values indicating success or failure. result = {} for file_path in files: try: with open(file_path, \'w\') as file: # Try to lock the file exclusively fcntl.flock(file, fcntl.LOCK_EX) # Write the identifier to the file file.write(identifier) # Unlock the file fcntl.flock(file, fcntl.LOCK_UN) result[file_path] = True except Exception as e: # If any error occurs, set the result for this file to False result[file_path] = False return result"},{"question":"# Functional Python Programming Challenge **Objective:** You are to implement a solution that generates combinations of elements from a given list, processes them to filter based on a criterion, and then processes the filtered combinations using a generator function. **Problem Description:** 1. Write a function `generate_combinations` that takes a list of elements and an integer `r`. This function should use `itertools.combinations` to generate all possible combinations of length `r` from the given list. Return these combinations as a list of tuples. 2. Write a function `filter_combinations` that takes the list of combinations generated by `generate_combinations` and an integer `threshold`. This function should filter combinations where the sum of the elements is greater than or equal to the given threshold. Return these filtered combinations as a list of tuples. 3. Write a generator function `process_combinations` that takes the filtered combinations from `filter_combinations` and yields the product of the elements of each combination one by one. 4. Use the functions above to generate combinations from a given list of integers, filter them based on a threshold, and process them to print their product. **Function Signatures:** ```python from itertools import combinations from functools import reduce from operator import mul def generate_combinations(elements: list, r: int) -> list: pass def filter_combinations(combos: list, threshold: int) -> list: pass def process_combinations(filtered_combos: list): pass # Example usage elements = [1, 2, 3, 4, 5] r = 3 threshold = 10 # Generate all combinations of length r combos = generate_combinations(elements, r) # Filter combinations based on the sum threshold filtered_combos = filter_combinations(combos, threshold) # Process filtered combinations and print their products for product in process_combinations(filtered_combos): print(product) ``` **Constraints:** - List of elements will have at least `r` elements and at most 10 elements. - Each element in the list will be an integer between 1 and 20. - `r` will be a positive integer less than or equal to the length of the list. - The threshold will be a positive integer. **Expected Output:** The program should yield the product of each filtered combination one by one, based on the given criteria. **Example:** Given: ```python elements = [1, 2, 3, 4, 5] r = 3 threshold = 10 ``` Output: ```python 20 30 60 ``` Explanation: - The combinations of length 3 from `[1, 2, 3, 4, 5]` are: `[1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 3, 4], [1, 3, 5], [1, 4, 5], [2, 3, 4], [2, 3, 5], [2, 4, 5], [3, 4, 5]` - Filtered combinations with sum >= 10: `[1, 4, 5], [2, 3, 5], [2, 4, 5], [3, 4, 5]` - Products of the filtered combinations: `1*4*5 = 20`, `2*3*5 = 30`, `2*4*5 = 40`, `3*4*5 = 60` **Performance Requirements:** The functions should handle up to the maximum constraints efficiently.","solution":"from itertools import combinations from functools import reduce from operator import mul def generate_combinations(elements: list, r: int) -> list: Generate all possible combinations of length r from the given list of elements. return list(combinations(elements, r)) def filter_combinations(combos: list, threshold: int) -> list: Filter combinations where the sum of the elements is greater than or equal to the given threshold. return [combo for combo in combos if sum(combo) >= threshold] def process_combinations(filtered_combos: list): Yield the product of the elements in each filtered combination one by one. for combo in filtered_combos: yield reduce(mul, combo, 1) # Example usage elements = [1, 2, 3, 4, 5] r = 3 threshold = 10 # Generate all combinations of length r combos = generate_combinations(elements, r) # Filter combinations based on the sum threshold filtered_combos = filter_combinations(combos, threshold) # Process filtered combinations and print their products for product in process_combinations(filtered_combos): print(product)"},{"question":"# SAX XML Parsing with Custom Handler **Objective**: Demonstrate your understanding of the `xml.sax` package by implementing custom event handlers to parse an XML document and extract specific information. **Problem Statement**: You are required to write a Python program that parses an XML document containing information about various `Book` entries in a library. Your task is to extract the title and author of books published after the year 2000 using SAX parsing. The XML structure is as follows: ```xml <Library> <Book> <Title>Book Title 1</Title> <Author>Author 1</Author> <Year>1999</Year> </Book> <Book> <Title>Book Title 2</Title> <Author>Author 2</Author> <Year>2005</Year> </Book> <!-- More book entries --> </Library> ``` **Task**: 1. Create a `BookHandler` class that inherits from `xml.sax.ContentHandler` and implements methods to capture and process XML elements. 2. The handler methods should store book titles and authors only if the book is published after the year 2000. 3. Implement the function `parse_books(xml_string: str) -> List[Tuple[str, str]]` which: - Accepts an XML string. - Uses the `xml.sax` package to parse the XML string. - Returns a list of tuples, where each tuple contains the title and author of a book published after the year 2000. **Constraints**: - Year entries are valid integers within the reasonable range (e.g., 1900-2100). - Titles and authors are non-empty strings. **Example**: ```python xml_data = \'\'\'<Library> <Book> <Title>Book Title 1</Title> <Author>Author 1</Author> <Year>1999</Year> </Book> <Book> <Title>Book Title 2</Title> <Author>Author 2</Author> <Year>2005</Year> </Book> </Library>\'\'\' result = parse_books(xml_data) print(result) # Output: [(\'Book Title 2\', \'Author 2\')] ``` **Note**: Your solution should handle the XML parsing and custom handler setup efficiently and correctly.","solution":"import xml.sax from typing import List, Tuple class BookHandler(xml.sax.ContentHandler): def __init__(self): self.current_data = \\"\\" self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" self.books = [] self.in_book = False def startElement(self, tag, attributes): self.current_data = tag if tag == \\"Book\\": self.in_book = True self.title = \\"\\" self.author = \\"\\" self.year = \\"\\" def endElement(self, tag): if tag == \\"Book\\": if self.in_book and int(self.year) > 2000: self.books.append((self.title, self.author)) self.in_book = False self.current_data = \\"\\" def characters(self, content): if self.current_data == \\"Title\\": self.title = content elif self.current_data == \\"Author\\": self.author = content elif self.current_data == \\"Year\\": self.year = content def parse_books(xml_string: str) -> List[Tuple[str, str]]: handler = BookHandler() xml.sax.parseString(xml_string, handler) return handler.books"},{"question":"Coding Assessment Question: AST Manipulator # Objective Write a function that takes a piece of Python code as a string, parses it into an abstract syntax tree (AST), and then manipulates the AST to insert type annotations for function arguments and return types based on provided type hints. # Problem Statement You need to implement a function `annotate_functions_with_types(code: str, type_hints: dict) -> str` that takes the following inputs: - `code` (str): A string containing valid Python code with one or more function definitions. - `type_hints` (dict): A dictionary where keys are function names (str) and values are another dictionary containing type annotations for arguments and return type. The inner dictionary has keys `args` for argument types (a list of types corresponding to each argument) and `return` for return type. Your function should: 1. Parse the given Python code into an AST. 2. Traverse the AST and find all function definitions. 3. For each function found, if its name matches a key in the `type_hints` dictionary, update its argument and return type annotations according to the provided type hints. 4. Return the modified code as a string. # Input - `code`: A string containing valid Python code. - `type_hints`: A dictionary where: ```python { \\"function_name\\": { \\"args\\": [arg1_type, arg2_type, ...], \\"return\\": return_type }, ... } ``` For example: ```python { \\"add\\": { \\"args\\": [\\"int\\", \\"int\\"], \\"return\\": \\"int\\" } } ``` # Output - A string containing the modified Python code with the appropriate type annotations added. # Constraints - The provided Python code will contain valid syntax. - The `type_hints` dictionary will only contain valid function names and types as strings. - You may assume function arguments do not include positional-only arguments or keyword-only arguments. # Example ```python code = \'\'\' def add(a, b): return a + b def multiply(x, y): return x * y \'\'\' type_hints = { \\"add\\": { \\"args\\": [\\"int\\", \\"int\\"], \\"return\\": \\"int\\" }, \\"multiply\\": { \\"args\\": [\\"float\\", \\"float\\"], \\"return\\": \\"float\\" } } expected_output = \'\'\' def add(a: int, b: int) -> int: return a + b def multiply(x: float, y: float) -> float: return x * y \'\'\' assert annotate_functions_with_types(code, type_hints) == expected_output ``` # Implementation Hint - Use the `ast` module to parse the code and inspect function definitions. - Modify the function `args` attribute to include type annotations. - Modify the function return type to include the provided return type. - Generate the modified code from the AST using `ast.unparse()`. # Your Task Implement the function `annotate_functions_with_types(code: str, type_hints: dict) -> str`. Good luck!","solution":"import ast from typing import Dict, Any def annotate_functions_with_types(code: str, type_hints: Dict[str, Dict[str, Any]]) -> str: class TypeAnnotator(ast.NodeTransformer): def visit_FunctionDef(self, node): # Check if the function name is in type_hints if node.name in type_hints: hints = type_hints[node.name] # Annotate argument types for arg, type_hint in zip(node.args.args, hints[\'args\']): arg.annotation = ast.Name(id=type_hint, ctx=ast.Load()) # Annotate return type node.returns = ast.Name(id=hints[\'return\'], ctx=ast.Load()) self.generic_visit(node) return node # Parse the input code into an AST tree = ast.parse(code) # Create a TypeAnnotator instance and apply it to the AST annotator = TypeAnnotator() annotated_tree = annotator.visit(tree) # Convert the modified AST back to source code modified_code = ast.unparse(annotated_tree) return modified_code"},{"question":"You are given a dataset containing information about different species of iris flowers and their measurements. Using this dataset, your task is to create a comprehensive visualization of the data using seaborn. # Requirements: 1. Load the \\"iris\\" dataset from seaborn\'s built-in datasets. 2. Create a 2x2 grid of plots: - A bar plot showing the average petal length for each species of iris. - A scatter plot comparing petal length and petal width, color-coded by species. - A box plot showing the distribution of sepal length for each iris species. - A histogram of sepal width. 3. Apply different seaborn styles to each subplot (e.g., one with \\"darkgrid\\", another with \\"whitegrid\\", etc.). 4. Customize one of the plots by altering properties such as the colors and line styles of the grid. # Input: There is no input; you will be working with the seaborn\'s built-in \\"iris\\" dataset. # Output: A 2x2 grid of plots with the described customizations. # Constraints: - Use only seaborn and matplotlib libraries for plotting and customizing. # Example: The final output should look like a 2x2 grid with each plot demonstrating different styles and customizations as specified. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Create a 2x2 grid of plots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Bar plot sns.set_style(\\"whitegrid\\") sns.barplot(x=\\"species\\", y=\\"petal_length\\", data=iris, ax=axes[0, 0]) # Scatter plot sns.set_style(\\"darkgrid\\") sns.scatterplot(x=\\"petal_length\\", y=\\"petal_width\\", hue=\\"species\\", data=iris, ax=axes[0, 1]) # Box plot sns.set_style(\\"white\\") sns.boxplot(x=\\"species\\", y=\\"sepal_length\\", data=iris, ax=axes[1, 0]) # Histogram sns.set_style(\\"ticks\\", {\\"grid.color\\": \\"0.6\\", \\"grid.linestyle\\": \\":\\"}) sns.histplot(iris[\\"sepal_width\\"], kde=True, ax=axes[1, 1]) # Show the plots plt.tight_layout() plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_iris_plots(): # Load the iris dataset iris = sns.load_dataset(\\"iris\\") # Create a 2x2 grid of plots fig, axes = plt.subplots(2, 2, figsize=(15, 10)) # Bar plot: Average petal length for each species sns.set_style(\\"whitegrid\\") sns.barplot(x=\\"species\\", y=\\"petal_length\\", data=iris, ax=axes[0, 0]) # Scatter plot: Petal length vs petal width, color-coded by species sns.set_style(\\"darkgrid\\") sns.scatterplot(x=\\"petal_length\\", y=\\"petal_width\\", hue=\\"species\\", data=iris, ax=axes[0, 1]) # Box plot: Distribution of sepal length for each species sns.set_style(\\"white\\") sns.boxplot(x=\\"species\\", y=\\"sepal_length\\", data=iris, ax=axes[1, 0]) # Histogram: Distribution of sepal width sns.set_style(\\"ticks\\", {\\"grid.color\\": \\"0.6\\", \\"grid.linestyle\\": \\":\\"}) sns.histplot(iris[\\"sepal_width\\"], kde=True, ax=axes[1, 1]) # Show the plots plt.tight_layout() plt.show()"},{"question":"# Python Multiprocessing Advanced Assessment Problem Description You are tasked with designing a concurrent system to process a stream of numbers. We will be using the Python \\"multiprocessing\\" package to parallelize this computation efficiently. The task involves the following steps: 1. Create a pool of worker processes. 2. Each worker will process a chunk of numbers by applying a provided mathematical function. 3. Aggregate the processed results. 4. Ensure safe communication and synchronization across processes. Objective Create a function `process_numbers_concurrently(numbers: List[int], func: Callable[[int], int], num_workers: int) -> List[int]` that processes a list of numbers in parallel. Function Signature ```python from typing import List, Callable def process_numbers_concurrently(numbers: List[int], func: Callable[[int], int], num_workers: int) -> List[int]: pass ``` # Input - **numbers** (`List[int]`): A list of integers to be processed. - **func** (`Callable[[int], int]`): A function to be applied to each integer in the list. - **num_workers** (`int`): The number of workers (processes) to be used for concurrent processing. # Output - Returns a `List[int]` of processed numbers. # Constraints - Each worker should handle a roughly equal chunk of the input numbers. - The order of the returned list should correspond to the order of the input numbers. - Use `multiprocessing.Pool` for worker process management. - Ensure that the function works efficiently for large input sizes (e.g., a list with one million integers). # Example ```python from typing import List, Callable def square(x: int) -> int: return x * x def process_numbers_concurrently(numbers: List[int], func: Callable[[int], int], num_workers: int) -> List[int]: pass if __name__ == \\"__main__\\": nums = [1, 2, 3, 4, 5] result = process_numbers_concurrently(nums, square, 2) print(result) # Should output: [1, 4, 9, 16, 25] ``` Additional Specifications 1. Use the `Pool.map` method to distribute the tasks among workers. 2. Handle edge cases where the list of numbers might be empty or the number of workers is greater than the number of numbers. 3. Implement error handling for potential multiprocessing issues. Notes - Emphasize the effective utilization of multiple processors. - Use appropriate synchronization mechanisms if necessary. - You should protect the entry point of your code using `if __name__ == \\"__main__\\":` to ensure the safe execution of multiprocessing code. - Document the code clearly, explaining the role of each component. # Good luck!","solution":"from typing import List, Callable import multiprocessing def process_numbers_concurrently(numbers: List[int], func: Callable[[int], int], num_workers: int) -> List[int]: Processes a list of numbers in parallel using a given function and a specified number of worker processes. Args: - numbers (List[int]): A list of integers to be processed. - func (Callable[[int], int]): A function to be applied to each integer in the list. - num_workers (int): The number of workers (processes) to be used for concurrent processing. Returns: - List[int]: A list of processed integers. if not numbers: return [] # Create a multiprocessing Pool with multiprocessing.Pool(processes=num_workers) as pool: # Use pool.map to process the numbers concurrently result = pool.map(func, numbers) return result"},{"question":"# **Random Walk Simulation** **Objective:** Develop a simulation of a random walk using the Python `random` module. **Problem Statement:** A random walk is a mathematical formalization to describe a path that consists of a series of random steps. Consider a simple 1-dimensional random walk starting at position `0`. At each step, you will randomly move either one unit forward (`+1`) or one unit backward (`-1`). The simulation should record the position at each step and compute specific statistics over multiple runs of the walk. **Task:** 1. Implement a function `random_walk(steps)` that performs a single random walk with a given number of steps. 2. Implement a function `simulate_walks(num_walks, steps)` that performs multiple random walks and records the results. 3. Implement a function `analyze_walks(walks)` to analyze the results of the simulations. * **`random_walk(steps)`**: - **Input**: An integer `steps` representing the number of steps in the random walk. - **Output**: A list of integers indicating the position at each step of the walk. * **`simulate_walks(num_walks, steps)`**: - **Input**: - An integer `num_walks` representing the number of random walks to simulate. - An integer `steps` representing the number of steps in each walk. - **Output**: A list of lists where each sublist is the result of a single random walk. * **`analyze_walks(walks)`**: - **Input**: A list of lists where each sublist is the result of a single random walk (from `simulate_walks` function). - **Output**: A dictionary containing: - `final_positions`: A list of the final positions after all walks. - `average_final_position`: The average of the final positions. - `max_position`: The maximum position reached across all walks. - `min_position`: The minimum position reached across all walks. **Constraints:** - Raise a `ValueError` if the input to the functions make them infeasible (e.g., negative steps or walks). - Utilize the `random.choice` method for simulating steps. - Ensure the simulation and analysis are efficiently implemented to handle large inputs. **Example Usage:** ```python import random def random_walk(steps): if steps < 0: raise ValueError(\\"Number of steps cannot be negative\\") position = 0 walk = [position] for _ in range(steps): step = random.choice([-1, 1]) position += step walk.append(position) return walk def simulate_walks(num_walks, steps): if num_walks < 0 or steps < 0: raise ValueError(\\"Number of walks and steps cannot be negative\\") walks = [] for _ in range(num_walks): walks.append(random_walk(steps)) return walks def analyze_walks(walks): if not walks: raise ValueError(\\"Walks list cannot be empty\\") final_positions = [walk[-1] for walk in walks] average_final_position = sum(final_positions) / len(final_positions) max_position = max(map(max, walks)) min_position = min(map(min, walks)) return { \\"final_positions\\": final_positions, \\"average_final_position\\": average_final_position, \\"max_position\\": max_position, \\"min_position\\": min_position } # Example execution num_walks = 1000 steps = 100 walks = simulate_walks(num_walks, steps) analysis = analyze_walks(walks) print(analysis) ``` **Expected Output:** ```python { \\"final_positions\\": [...], \\"average_final_position\\": ..., \\"max_position\\": ..., \\"min_position\\": ... } ``` **Notes:** - Ensure to seed the random number generator during development and testing to make results reproducible. - Optimize the implementation to handle larger sets of walks and steps efficiently.","solution":"import random def random_walk(steps): if steps < 0: raise ValueError(\\"Number of steps cannot be negative\\") position = 0 walk = [position] for _ in range(steps): step = random.choice([-1, 1]) position += step walk.append(position) return walk def simulate_walks(num_walks, steps): if num_walks < 0 or steps < 0: raise ValueError(\\"Number of walks and steps cannot be negative\\") walks = [] for _ in range(num_walks): walks.append(random_walk(steps)) return walks def analyze_walks(walks): if not walks: raise ValueError(\\"Walks list cannot be empty\\") final_positions = [walk[-1] for walk in walks] average_final_position = sum(final_positions) / len(final_positions) max_position = max(map(max, walks)) min_position = min(map(min, walks)) return { \\"final_positions\\": final_positions, \\"average_final_position\\": average_final_position, \\"max_position\\": max_position, \\"min_position\\": min_position }"},{"question":"# Question You are given a text file named `data.txt`, which contains multiple lines of text. Your task is to use the `mmap` module to perform the following operations: 1. Open the file in read and write mode, and create a memory-mapped object for the entire file. 2. Read and print the first line of the file using the memory-mapped object. 3. Replace the word \\"Python\\" with \\"mmap\\" in the file using slice notation on the memory-mapped object. 4. Append a new line of text \\"Learning memory-mapped files!\\" using the memory-mapped object. 5. Ensure that these modifications are flushed to the disk. 6. Print the entire new content of the file. # Constraints - You should handle potential exceptions such as file not found or access errors gracefully. - The file `data.txt` can be assumed to be small enough to fit into memory. # Function Signature ```python def manipulate_memory_mapped_file(filename: str): pass ``` # Example If `data.txt` initially contains: ``` Hello Python! Welcome to the world of file handling. ``` After running the function `manipulate_memory_mapped_file(\'data.txt\')`, the modified `data.txt` content should be: ``` Hello mmap! Welcome to the world of file handling. Learning memory-mapped files! ``` # Notes - Make sure to handle the file and memory-mapped object correctly by closing them to avoid resource leaks. - Your implementation should work on both Unix and Windows systems.","solution":"import mmap def manipulate_memory_mapped_file(filename: str): try: with open(filename, \'r+\') as f: # Create memory-mapped object for the entire file mm = mmap.mmap(f.fileno(), 0) # Read and print the first line of the file using the memory-mapped object mm.seek(0) first_line = mm.readline().decode() print(first_line) # Replace the word \\"Python\\" with \\"mmap\\" in the file using slice notation on the memory-mapped object mm.seek(0) content = mm.read().decode() content = content.replace(\'Python\', \'mmap\') # Write the modified content back to the memory-mapped object mm.seek(0) mm.write(content.encode()) # Append a new line of text \\"Learning memory-mapped files!\\" using the memory-mapped object new_content = content + \\"nLearning memory-mapped files!\\" mm.resize(len(new_content)) mm.seek(0) mm.write(new_content.encode()) # Ensure that these modifications are flushed to the disk mm.flush() # Print the entire new content of the file mm.seek(0) print(mm.read().decode()) # Close memory-mapped object mm.close() except FileNotFoundError: print(f\\"Error: The file {filename} does not exist.\\") except IOError as e: print(f\\"Error: IO error occurred - {e}\\")"},{"question":"Objective: You are tasked with creating a Python script that parses command-line arguments using the `getopt` module. The script should support the following command-line options: - `-h` or `--help`: Display a help message and exit. - `-f <filename>` or `--file <filename>`: Specify a filename. This option is mandatory. - `-v` or `--verbose`: Enable verbose mode. This option is optional. - `-t <type>` or `--type <type>`: Specify the type. Allowed values are `text`, `json`, and `xml`. If not specified, default to `text`. Requirements: 1. Implement a function `parse_arguments(args)` that takes a list of command-line arguments (excluding the program name) and returns a dictionary with parsed options and their values. - Keys for the dictionary should be `help`, `file`, `verbose`, and `type`. - `help` should be a boolean indicating if help was requested. - `file` should be a string indicating the specified filename. - `verbose` should be a boolean indicating if verbose mode is enabled. - `type` should be a string indicating the specified type, defaulting to `text`. 2. Implement a function `print_help()` to display a help message detailing the usage and available options. 3. If any required arguments are missing or invalid, raise a `ValueError` with an appropriate message. Example Usages: ```sh # Example 1: Valid Command with Verbose python script.py -f data.txt -v -t json ``` Result: ```python {\'help\': False, \'file\': \'data.txt\', \'verbose\': True, \'type\': \'json\'} ``` ```sh # Example 2: Help Command python script.py --help ``` Result: ``` Usage: script.py [options] Options: -h, --help Show this help message and exit -f FILE, --file=FILE Specify the filename (mandatory) -v, --verbose Enable verbose mode (optional) -t TYPE, --type=TYPE Specify the type (text, json, xml) - default: text ``` Constraints: - You should use the `getopt.getopt` function for argument parsing. - Handle any `getopt.GetoptError` exceptions gracefully by showing a relevant error message and the help information. Function Signatures: ```python def parse_arguments(args: list) -> dict: pass def print_help() -> None: pass ``` Notes: - The command examples assume that your script file is named `script.py`. - Ensure your function `parse_arguments` is tested with various command-line argument scenarios to validate its correctness and robustness.","solution":"import getopt def parse_arguments(args): Parse command-line arguments and return a dictionary with the results. :param args: List of command-line arguments (excluding the program name) :return: Dictionary with parsed options and their values opts, _ = getopt.getopt(args, \\"hf:vt:\\", [\\"help\\", \\"file=\\", \\"verbose\\", \\"type=\\"]) options = { \'help\': False, \'file\': None, \'verbose\': False, \'type\': \'text\' } for opt, arg in opts: if opt in (\\"-h\\", \\"--help\\"): options[\'help\'] = True elif opt in (\\"-f\\", \\"--file\\"): options[\'file\'] = arg elif opt in (\\"-v\\", \\"--verbose\\"): options[\'verbose\'] = True elif opt in (\\"-t\\", \\"--type\\"): if arg not in [\'text\', \'json\', \'xml\']: raise ValueError(f\\"Invalid type \'{arg}\'. Allowed values are \'text\', \'json\', \'xml\'.\\") options[\'type\'] = arg if not options[\'help\'] and options[\'file\'] is None: raise ValueError(\\"The \'-f\' or \'--file\' option is mandatory.\\") return options def print_help(): Print the help message detailing the usage and available options. help_message = Usage: script.py [options] Options: -h, --help Show this help message and exit -f FILE, --file=FILE Specify the filename (mandatory) -v, --verbose Enable verbose mode (optional) -t TYPE, --type=TYPE Specify the type (text, json, xml) - default: text print(help_message)"},{"question":"# Event Scheduler Implementation Using the `sched` module described above, write a function `birthday_reminder` that schedules reminders for a list of birthdays. Function Signature ```python def birthday_reminder(birthdays: List[Tuple[int, str]], lead_time: int): pass ``` Input - `birthdays`: A list of tuples where each tuple contains two elements: 1. An integer representing the Unix epoch time of the birthday. 2. A string containing the person\'s name. - `lead_time`: An integer representing the number of seconds before the actual birthday when the reminder should be scheduled. Output The function should print reminders for each birthday at the specified `lead_time` before the birthday time. The reminder should contain the person\'s name and the scheduled time (in seconds since the Unix epoch) when the reminder message is printed. Constraints - The `scheduler`\'s `timefunc` should use `time.time`. - The `scheduler`\'s `delayfunc` should use `time.sleep`. Example ```python import time def birthday_reminder(birthdays, lead_time): import sched def print_reminder(name, sched_time): print(f\\"Reminder: {name}\'s birthday is at {sched_time + lead_time}, current time is {time.time()}\\") s = sched.scheduler(time.time, time.sleep) current_time = time.time() for b_time, name in birthdays: reminder_time = b_time - lead_time if reminder_time > current_time: # Only schedule future reminders s.enterabs(reminder_time, 1, print_reminder, argument=(name, reminder_time)) print(\\"Starting Scheduler\\") s.run() # Testing the function birthdays = [ (time.time() + 10, \\"Alice\\"), (time.time() + 20, \\"Bob\\"), (time.time() + 30, \\"Charlie\\") ] # Schedule reminders 5 seconds before each birthday birthday_reminder(birthdays, 5) ``` Expected Output The output should remind 5 seconds before each birthday: ``` Starting Scheduler Reminder: Alice\'s birthday is at [actual time + 5], current time is [actual time] Reminder: Bob\'s birthday is at [actual time + 5], current time is [actual time] Reminder: Charlie\'s birthday is at [actual time + 5], current time is [actual time] ```","solution":"import sched import time from typing import List, Tuple def birthday_reminder(birthdays: List[Tuple[int, str]], lead_time: int): Schedules reminders for a list of birthdays. Args: birthdays (List[Tuple[int, str]]): List of tuples containing Unix epoch time of the birthday and the person\'s name. lead_time (int): Number of seconds before the actual birthday when the reminder should be scheduled. def print_reminder(name, sched_time): print(f\\"Reminder: {name}\'s birthday is at {sched_time + lead_time}, current time is {time.time()}\\") s = sched.scheduler(time.time, time.sleep) current_time = time.time() for b_time, name in birthdays: reminder_time = b_time - lead_time if reminder_time > current_time: # Only schedule future reminders s.enterabs(reminder_time, 1, print_reminder, argument=(name, reminder_time)) print(\\"Starting Scheduler\\") s.run()"},{"question":"Coding Assessment Question # Objective: Evaluate the understanding of the `inspect` module, specifically the usage of `getmembers()`, type checks, and `Signature` objects. # Problem Statement: You are given a piece of software containing multiple modules, classes, functions, and methods. You need to write a Python script that inspects a specified class within a module and retrieves information using the `inspect` module. # Tasks: 1. **Define a function `inspect_class_members(module_name: str, class_name: str) -> dict`:** This function should perform the following tasks: - **Import the specified module.** - **Retrieve the specified class from the module.** - **Use `inspect.getmembers()` to retrieve all members of the class.** - **Filter the members to keep only those that are methods or functions.** - **For each method/function, retrieve their signature using `inspect.signature()`.** - **Return a dictionary with member names as keys and their signatures as values (formatted as strings).** # Input: - `module_name` (str): The name of the module containing the class. - `class_name` (str): The name of the class to inspect. # Output: - A dictionary containing member names and their signatures. # Example: ```python # Example class in module `example_module` # class Example: # def method1(self, a, b:int) -> int: # pass # def method2(self, x:str) -> str: # pass result = inspect_class_members(\'example_module\', \'Example\') # Expected output: # { # \'method1\': \'(self, a, b: int) -> int\', # \'method2\': \'(self, x: str) -> str\' # } ``` # Guidelines: 1. Your solution must use the `inspect` module. 2. Handle exceptions where the module or class might not exist. 3. Ensure the output signatures are correctly formatted. 4. Assume that the module is available in the Python path. # Constraints: - The class will only contain methods or functions. - The module will always be a valid Python module that can be imported.","solution":"import importlib import inspect def inspect_class_members(module_name: str, class_name: str) -> dict: Inspects a specified class in a module and retrieves the signatures of its methods. Args: module_name (str): The name of the module containing the class. class_name (str): The name of the class to inspect. Returns: dict: A dictionary with member names as keys and their signatures as values (formatted as strings). try: # Import the specified module module = importlib.import_module(module_name) # Retrieve the specified class from the module cls = getattr(module, class_name) # Use inspect.getmembers() to retrieve all members of the class members = inspect.getmembers(cls, predicate=inspect.isfunction) # Create a dictionary with member names as keys and their signatures as values result = {name: str(inspect.signature(member)) for name, member in members} return result except (ModuleNotFoundError, AttributeError) as e: print(f\\"Error: {e}\\") return {}"},{"question":"# Error Code Translation and Exception Handling You are tasked with developing a diagnostic tool in Python that will help users understand specific system-related errors encountered during various operations. This tool should take a list of error codes and output their corresponding symbolic names as well as human-readable descriptions. Additionally, it should simulate raising these errors to demonstrate exception handling in Python. Function Signature ```python def handle_error_codes(error_codes: list[int]) -> list[str]: pass ``` Input - `error_codes`: A list of integers representing system error codes. Output - A list of strings. Each string should contain the error code, its symbolic name, and its human-readable description. Constraints 1. You must use the `errno.errorcode` dictionary to obtain symbolic names for the error codes. 2. The translated description should be obtained using the `os.strerror()` function. 3. You need to simulate raising the actual exceptions that correspond to the error codes and show how they can be caught and handled. For errors without direct exceptions, you can raise a generic `OSError`. 4. Ensure your function is robust to handle any potential unknown error codes gracefully. Performance Requirements - The function should handle up to 100 error codes efficiently. Example ```python import errno import os def handle_error_codes(error_codes): results = [] for code in error_codes: if code in errno.errorcode: symbolic_name = errno.errorcode[code] description = os.strerror(code) try: if code == errno.EPERM: raise PermissionError elif code == errno.ENOENT: raise FileNotFoundError elif code == errno.ESRCH: raise ProcessLookupError elif code == errno.EAGAIN: raise BlockingIOError elif code == errno.EACCES: raise PermissionError # ... handle other specific exceptions similarly ... else: raise OSError(code, description) except Exception as e: results.append(f\\"Error Code: {code}, Symbolic Name: {symbolic_name}, Description: {description}, Exception Handled: {e}\\") else: results.append(f\\"Unknown Error Code: {code}\\") return results # Example usage error_codes_to_test = [1, 2, 3, 5, 13, 1000] result = handle_error_codes(error_codes_to_test) for res in result: print(res) ``` In the example usage, assuming the error codes correspond to valid `errno` values (such as 1 for EPERM, 2 for ENOENT, etc.), the output should show the error code, its symbolic name, a human-readable description, and the type of exception handled for each code in the list.","solution":"import errno import os def handle_error_codes(error_codes: list[int]) -> list[str]: results = [] for code in error_codes: if code in errno.errorcode: symbolic_name = errno.errorcode[code] description = os.strerror(code) try: # Handle specific error codes with corresponding exceptions if code == errno.EPERM: raise PermissionError(description) elif code == errno.ENOENT: raise FileNotFoundError(description) elif code == errno.ESRCH: raise ProcessLookupError(description) elif code == errno.EAGAIN: raise BlockingIOError(description) elif code == errno.EACCES: raise PermissionError(description) # Handle other specific exceptions similarly... else: raise OSError(code, description) except Exception as e: results.append(f\\"Error Code: {code}, Symbolic Name: {symbolic_name}, Description: {description}, Exception Handled: {e}\\") else: results.append(f\\"Unknown Error Code: {code}\\") return results"},{"question":"**Question: Exploratory Data Analysis Using Seaborn** You are provided with a dataset of penguin measurements and your task is to perform an exploratory data analysis using the seaborn library. Your final goal is to create a dashboard consisting of different types of visualizations to gain insights from the data. Follow the instructions below to complete the task. **Instructions:** 1. **Load the Dataset:** - Use the `sns.load_dataset()` method to load the \\"penguins\\" dataset. - Store the dataset in a variable named `penguins`. 2. **Univariate ECDF Plot:** - Create an ECDF plot of `flipper_length_mm` using the `penguins` dataset. - Save the plot as `ecdf_flipper_length.png`. 3. **Bivariate ECDF Plot:** - Flip the previous plot by displaying the `flipper_length_mm` on the y-axis. - Save this plot as `ecdf_flipper_length_y.png`. 4. **Wide-Form Histogram:** - Plot a histogram for each numeric column that includes the term \\"bill_\\" in the `penguins` dataset. - Save this plot as `wide_form_histogram.png`. 5. **Multivariate ECDF Plot:** - Create an ECDF plot of `bill_length_mm` and use the `species` column to create separate plots for each species. - Save this plot as `multivariate_ecdf.png`. 6. **Customized ECDF Plot:** - Create an ECDF plot of `bill_length_mm` with the `species` column as a hue parameter and display the counts instead of proportions. - Save this plot as `ecdf_counts.png`. 7. **Complementary CDF Plot:** - Plot the complementary CDF (1 - CDF) for `bill_length_mm` with respect to the `species` column. - Save this plot as `complementary_cdf.png`. **Constraints:** - Use the seaborn library to create all plots. - Make sure to set a theme using `sns.set_theme()`. - Each plot should be saved in the format specified with the given filenames. **Input:** None (The dataset will be loaded internally using seaborn). **Output:** None (Each plot should be saved as a PNG file with the specified filenames). **Performance Requirements:** - The code should be efficient and should not take more than a few seconds to execute on a typical machine. **Example:** ```python import seaborn as sns # Load the dataset sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Create ECDF plot for flipper length sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\").get_figure().savefig(\\"ecdf_flipper_length.png\\") # Other plots follow similarly... ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset sns.set_theme() penguins = sns.load_dataset(\\"penguins\\") # Create ECDF plot for flipper length plt.figure() sns.ecdfplot(data=penguins, x=\\"flipper_length_mm\\").get_figure().savefig(\\"ecdf_flipper_length.png\\") plt.close() # Create an ECDF plot with flipper_length_mm on the y-axis plt.figure() sns.ecdfplot(data=penguins, y=\\"flipper_length_mm\\").get_figure().savefig(\\"ecdf_flipper_length_y.png\\") plt.close() # Create wide-form histograms for bill measurements plt.figure() bill_columns = [col for col in penguins.columns if \\"bill_\\" in col] sns.histplot(data=penguins[bill_columns], element=\\"step\\", fill=False).get_figure().savefig(\\"wide_form_histogram.png\\") plt.close() # Create a multivariate ECDF plot for bill length by species plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\").get_figure().savefig(\\"multivariate_ecdf.png\\") plt.close() # Create an ECDF plot for bill length counts by species plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", stat=\\"count\\").get_figure().savefig(\\"ecdf_counts.png\\") plt.close() # Create complementary CDF plot for bill length by species plt.figure() sns.ecdfplot(data=penguins, x=\\"bill_length_mm\\", hue=\\"species\\", complementary=True).get_figure().savefig(\\"complementary_cdf.png\\") plt.close()"},{"question":"# Asynchronous Task Scheduler with Context-Aware State Management As an advanced exercise, you will implement an **Asynchronous Task Scheduler** that leverages python\'s `contextvars` module to manage task-specific states. This scheduler will be capable of maintaining states such as the current task identifier and any additional metadata for the task. Requirements: 1. **Setup Context Variables**: - Create a `ContextVar` instance for `task_id` with no default value. - Create a `ContextVar` instance for `task_metadata` with a default empty dictionary `{}`. 2. **Scheduler** Class: - Method `add_task(task_id, task_metadata, coroutine_func, *args, **kwargs)`: - Schedule a new task with its identifier and metadata. - On execution, it should set the context variables `task_id` and `task_metadata`. - Method `run_all_tasks()`: - Execute all added tasks ensuring that each task\'s context is isolated and does not affect others. 3. **Task** Coroutine Function: - Define a sample coroutine function `sample_task(duration)` that: - Simulates some asynchronous work using `asyncio.sleep(duration)`. - Prints out the `task_id` and `task_metadata` stored in the context. 4. **Context Isolation**: - Ensure that the state changes within a task do not bleed into other tasks. # Input Format: - Multiple calls to `Scheduler.add_task()` to add tasks. - A single call to `Scheduler.run_all_tasks()` to execute all tasks. # Output Format: - Print statements from `sample_task()` showing `task_id` and `task_metadata`. # Example: ```python import asyncio import contextvars # Step 1: Setting up context variables task_id_var = contextvars.ContextVar(\'task_id\') task_metadata_var = contextvars.ContextVar(\'task_metadata\', default={}) class Scheduler: def __init__(self): self.tasks = [] def add_task(self, task_id, task_metadata, coroutine_func, *args, **kwargs): async def task_wrapper(): task_id_var.set(task_id) task_metadata_var.set(task_metadata) await coroutine_func(*args, **kwargs) self.tasks.append(task_wrapper) async def run_all_tasks(self): await asyncio.gather(*[task() for task in self.tasks]) async def sample_task(duration): await asyncio.sleep(duration) task_id = task_id_var.get() task_metadata = task_metadata_var.get() print(f\'Task ID: {task_id}, Metadata: {task_metadata}\') # Usage scheduler = Scheduler() scheduler.add_task(1, {\'info\': \'Task 1 metadata\'}, sample_task, 2) scheduler.add_task(2, {\'info\': \'Task 2 metadata\'}, sample_task, 1) asyncio.run(scheduler.run_all_tasks()) ``` Output: ``` Task ID: 1, Metadata: {\'info\': \'Task 1 metadata\'} Task ID: 2, Metadata: {\'info\': \'Task 2 metadata\'} ``` Constraints: 1. Each task must run asynchronously. 2. Context variables must correctly reflect the task\'s state. 3. Ensure there is no state leakage between tasks. Your implementation should demonstrate a clear understanding of the `contextvars` module and how to manage isolated states in asynchronous code.","solution":"import asyncio import contextvars # Step 1: Setting up context variables task_id_var = contextvars.ContextVar(\'task_id\') task_metadata_var = contextvars.ContextVar(\'task_metadata\', default={}) class Scheduler: def __init__(self): self.tasks = [] def add_task(self, task_id, task_metadata, coroutine_func, *args, **kwargs): async def task_wrapper(): task_id_var.set(task_id) task_metadata_var.set(task_metadata) await coroutine_func(*args, **kwargs) self.tasks.append(task_wrapper) async def run_all_tasks(self): await asyncio.gather(*[task() for task in self.tasks]) async def sample_task(duration): await asyncio.sleep(duration) task_id = task_id_var.get() task_metadata = task_metadata_var.get() print(f\'Task ID: {task_id}, Metadata: {task_metadata}\') # Usage (for testing purposes, not part of the actual implementation) scheduler = Scheduler() scheduler.add_task(1, {\'info\': \'Task 1 metadata\'}, sample_task, 1) scheduler.add_task(2, {\'info\': \'Task 2 metadata\'}, sample_task, 2) asyncio.run(scheduler.run_all_tasks())"},{"question":"# PyTorch Gradient Check Implementation **Objective:** To assess your understanding of PyTorch\'s `gradcheck` and `gradgradcheck` functionalities, you are required to implement a custom function and verify its gradients using these utilities. **Task:** 1. **Implement a Custom Function**: - Define a custom function `custom_function` that takes a PyTorch tensor as input and performs some operations returning a tensor. - Ensure the function has both real and complex-valued behavior. 2. **Verify Gradients**: - Use PyTorch\'s `gradcheck` to verify the gradients of your custom function. - Additionally, check the second-order gradients using `gradgradcheck`. **Guidelines**: 1. **Function Specification**: - The function should be differentiable. - Handle real and complex inputs appropriately. - Example operation: Transcendental functions (e.g., sine, cosine) combined with basic arithmetic operations. 2. **Gradient Verification**: - Use `gradcheck` to numerically and analytically verify the correctness of the gradients. - Use `gradgradcheck` to verify second-order derivatives. - Handle exceptions and ensure the gradient checks pass. ```python import torch import torch.autograd # 1. Implement the custom function def custom_function(input_tensor): Custom function that combines sine, cosine, and exponentiation operations. Args: input_tensor (torch.Tensor): Input tensor which might be real or complex. Returns: torch.Tensor: Result tensor after applying the custom operations. # Ensure input is a tensor with gradients enabled input_tensor = input_tensor.requires_grad_(True) # Apply operations result = torch.sin(input_tensor) * torch.cos(input_tensor) + torch.exp(input_tensor) return result # 2. Verify Gradients def verify_gradients(input_tensor): Verifies the gradients of the custom function using gradcheck and gradgradcheck. Args: input_tensor (torch.Tensor): Input tensor to check gradients for. Returns: bool: True if all gradient checks pass, otherwise False. input_tensor = input_tensor.requires_grad_(True).double() # double precision required for gradcheck # Perform gradcheck gradcheck_pass = torch.autograd.gradcheck(custom_function, input_tensor, eps=1e-6, atol=1e-4) # Perform gradgradcheck gradgradcheck_pass = torch.autograd.gradgradcheck(custom_function, input_tensor, eps=1e-6, atol=1e-4) return gradcheck_pass and gradgradcheck_pass # Example usage input_tensor_real = torch.randn(3, 3, dtype=torch.double) input_tensor_complex = torch.randn(3, 3, dtype=torch.complex128) print(\\"Gradcheck for real input:\\", verify_gradients(input_tensor_real)) print(\\"Gradcheck for complex input:\\", verify_gradients(input_tensor_complex)) ``` **Expected Output:** - The code should print `True` for both real and complex inputs if the gradients are correctly computed and verified. **Constraints:** - Use `torch` version >= 1.8.0. - The input tensor should have a shape of (3, 3). - The input tensor values should be random. - Ensure that the code robustly handles both real and complex input tensors. **Performance Requirements:** - Ensure the checks are completed within a reasonable time frame (e.g., under 5 seconds for tensors of the specified shape).","solution":"import torch from torch.autograd import Function, gradcheck, gradgradcheck class CustomFunction(Function): @staticmethod def forward(ctx, input_tensor): ctx.save_for_backward(input_tensor) return torch.sin(input_tensor) * torch.cos(input_tensor) + torch.exp(input_tensor) @staticmethod def backward(ctx, grad_output): input_tensor, = ctx.saved_tensors grad_input = grad_output * (torch.cos(input_tensor)**2 - torch.sin(input_tensor)**2 + torch.exp(input_tensor)) return grad_input custom_function = CustomFunction.apply # Function to verify gradients def verify_gradients(input_tensor): input_tensor = input_tensor.requires_grad_(True).double() # double precision required for gradcheck # Perform gradcheck gradcheck_pass = gradcheck(custom_function, (input_tensor,), eps=1e-6, atol=1e-4) # Perform gradgradcheck gradgradcheck_pass = gradgradcheck(custom_function, (input_tensor,), eps=1e-6, atol=1e-4) return gradcheck_pass and gradgradcheck_pass"},{"question":"# Question: **Objective**: To assess your understanding of seaborn\'s `objects` module and your ability to customize and combine different marks in a single visualization. **Problem Statement**: You are provided with the `penguins` dataset from `seaborn`. Your task is to create a comprehensive plot using seaborn\'s `so.Plot`, demonstrating your ability to use different types of marks and customize their attributes. Follow the steps below to achieve the desired plot: 1. **Load the Dataset**: Use `seaborn.load_dataset()` to load the `penguins` dataset. 2. **Initial Plot**: Create an initial plot object `p` with `species` on the x-axis, `body_mass_g` on the y-axis, and color by `sex`. 3. **Dash Mark**: Add a `Dash` mark to this plot with: - `alpha` set to 0.6 - `linewidth` based on `flipper_length_mm` 4. **Adjust Width**: Modify the dash width to 0.4. 5. **Dodging**: Apply dodging to the plot to adjust the width automatically. 6. **Combine Marks**: Add another layer combining `Dash()`, `Dots()`, `Agg()`, `Dodge()`, and `Jitter()` marks. 7. **Orientation Control**: Create another plot where both `body_mass_g` and `flipper_length_mm` are plotted, with dashes oriented horizontally (`\'y\'`). Your function should generate and display the described plots. **Function Signature**: ```python def create_penguins_plot(): import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Plot 1 p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") p.add(so.Dash(alpha=0.6), linewidth=\\"flipper_length_mm\\") p.add(so.Dash(width=0.4)) p.add(so.Dash(), so.Dodge()) p.add(so.Dash(), so.Agg(), so.Dodge()) p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the first plot p.show() # Plot 2 so.Plot(penguins[\\"body_mass_g\\"], penguins[\\"flipper_length_mm\\"].round(-1)).add(so.Dash(), orient=\\"y\\").show() ``` **Expected Output**: - A plotted figure displaying the features specified in steps 1-6. - Another plotted figure oriented according to step 7. **Constraints**: - Ensure `seaborn` is properly installed. - Handle any potential issues with the dataset being loaded correctly. Submit your function implementation and the resulting plots.","solution":"def create_penguins_plot(): import seaborn as sns import seaborn.objects as so # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Initial Plot p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") # Adding Dash mark with alpha and linewidth based on flipper_length_mm p = p.add(so.Dash(alpha=0.6), linewidth=\\"flipper_length_mm\\") # Adjusting the dash width to 0.4 p = p.add(so.Dash(width=0.4)) # Applying dodging p = p.add(so.Dash(), so.Dodge()) # Adding more layers with combined marks p = p.add(so.Dash(), so.Agg(), so.Dodge()) p = p.add(so.Dots(), so.Dodge(), so.Jitter()) # Display the first plot p.show() # Second Plot with dashes oriented horizontally so.Plot(penguins, x=\\"body_mass_g\\", y=\\"flipper_length_mm\\").add(so.Dash(), orient=\\"y\\").show()"},{"question":"# Seaborn Coding Assessment Context You are provided with a dataset on penguins which includes measurements like bill length, bill depth, etc. Your task is to use the seaborn library to create a comprehensive visualization that helps in understanding the relationships between different measurements. Task 1. **Load the Dataset:** - Use seaborn to load the `penguins` dataset. 2. **Create a JointGrid Plot:** - Initialize a `JointGrid` with `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. - Use a scatter plot for the main plot area and histograms for the marginal plots. - Customize the plots with specific attributes: set `alpha` to 0.8, `edgecolor` to \\".3\\", and `linewidth` to 1 for the scatter plot. 3. **Add Reference Lines:** - Add horizontal and vertical reference lines at `bill_length_mm = 45` and `bill_depth_mm = 16` respectively. 4. **Advanced Customization:** - Set the height of the entire plot to 6. - Adjust the ratio of the joint plot to marginal plots to 3. - Set the space between the plots to 0.1. - Ensure the ticks on the density axis of the marginal plots are turned on. 5. **Axes Limits:** - Define the limits of the x-axis from 30 to 65 and the y-axis from 10 to 25. 6. **Display Plot:** - Render the visualization. Input No direct input is required from the user as the `penguins` dataset is loaded within the code. Expected Output Format A JointGrid plot as per specified customizations should be displayed. Constraints - Follow the seaborn and matplotlib syntax strictly. - Ensure that all customizations apply correctly to the JointGrid plot. Below is a template to help you get started: ```python import seaborn as sns # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create a JointGrid plot with custom settings g = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\") g.plot(sns.scatterplot, sns.histplot, alpha=0.8, edgecolor=\\".3\\", linewidth=1) # Step 3: Add reference lines g.refline(x=45, y=16) # Step 4: Advanced Customization g = sns.JointGrid(data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", height=6, ratio=3, space=0.1, marginal_ticks=True) g.plot(sns.scatterplot, sns.histplot, alpha=0.8, edgecolor=\\".3\\", linewidth=1) # Step 5: Set axes limits g.ax_joint.set_xlim(30, 65) g.ax_joint.set_ylim(10, 25) # Step 6: Display the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_jointgrid_plot(): # Step 1: Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Step 2: Create a JointGrid plot with custom settings g = sns.JointGrid( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", height=6, ratio=3, space=0.1, marginal_ticks=True ) # Customizing the main plot and marginal plots g.plot(sns.scatterplot, sns.histplot, alpha=0.8, edgecolor=\\".3\\", linewidth=1) # Step 3: Add reference lines g.refline(x=45, y=16) # Step 5: Set axes limits g.ax_joint.set_xlim(30, 65) g.ax_joint.set_ylim(10, 25) # Step 6: Display the plot plt.show()"},{"question":"# Python Coding Assessment **Context**: You are tasked with implementing a function that imports a module given its name. Your function should handle both simple and complex module names (e.g., `json` and `os.path`). Additionally, it should provide meaningful error messages if the import fails and ensure that incomplete imports do not leave the module in an invalid state. **Task**: Write a function `import_module(module_name: str) -> Optional[str]` that attempts to import a module given its name. The function should: 1. Use the appropriate `python310` import functions. 2. Return the name of the imported module on successful import. 3. Return an appropriate error message if the import fails. 4. Ensure that incomplete imports do not leave the module in `sys.modules`. **Function Signature**: ```python def import_module(module_name: str) -> Optional[str]: ``` **Input**: - `module_name`: A string representing the module name. This can be a top-level module (e.g., `json`) or a submodule (e.g., `os.path`). **Output**: - The function should return the name of the module if the import is successful. - If the import fails, the function should return a string with an error message describing the failure. **Constraints**: - Do not use the built-in `__import__` function directly. - Make use of the `python310` import functions where applicable. **Example**: ```python # Successful import of JSON module result = import_module(\'json\') print(result) # Output: \\"json\\" # Unsuccessful import attempt result = import_module(\'non_existent_module\') print(result) # Output: \\"Error: Module \'non_existent_module\' could not be imported.\\" ``` **Notes**: - Ensure your function handles both complete module imports and submodule imports correctly. - Ensure no incomplete module or submodule remains in `sys.modules` on failure.","solution":"import sys import importlib from typing import Optional def import_module(module_name: str) -> Optional[str]: Attempts to import a module given its name. Returns the name of the imported module on success, or an appropriate error message on failure. try: module = importlib.import_module(module_name) return module_name except ImportError as e: if module_name in sys.modules: del sys.modules[module_name] return f\\"Error: Module \'{module_name}\' could not be imported.\\" # Example usage if __name__ == \\"__main__\\": print(import_module(\'json\')) # should return \'json\' print(import_module(\'non_existent_module\')) # should return the error message"},{"question":"Objective Design a function that utilizes the time module to convert an arbitrary time string in a given format to another format and returns the time difference between the original time and the current time in a specified unit. Function Signature ```python def convert_and_diff_time(time_string: str, input_format: str, output_format: str, unit: str) -> str: Converts the input time_string from input_format to output_format and returns the difference between the converted time and the current time. The difference is returned in the specified unit. Parameters: - time_string (str): The original time as a string. - input_format (str): The format of the original time string. - output_format (str): The desired format for the output time string. - unit (str): The unit for the time difference (\'seconds\', \'milliseconds\', \'minutes\', \'hours\'). Returns: - str: The converted time string and the time difference in the specified unit concatenated with a comma. ``` Examples ```python # Example 1 time_str = \\"2023-10-01 15:45:00\\" in_format = \\"%Y-%m-%d %H:%M:%S\\" out_format = \\"%d/%m/%Y %I:%M %p\\" unit = \\"seconds\\" print(convert_and_diff_time(time_str, in_format, out_format, unit)) #Output: \\"01/10/2023 03:45 PM, <time_difference_in_seconds>\\" # Example 2 time_str = \\"01 Oct 2023 03:45 PM\\" in_format = \\"%d %b %Y %I:%M %p\\" out_format = \\"%Y-%m-%d %H:%M:%S\\" unit = \\"hours\\" print(convert_and_diff_time(time_str, in_format, out_format, unit)) #Output: \\"2023-10-01 15:45:00, <time_difference_in_hours>\\" ``` Constraints and Notes 1. The function should raise a `ValueError` if the `unit` provided is invalid or if the `time_string` cannot be parsed according to the `input_format`. 2. The valid units are \'seconds\', \'milliseconds\', \'minutes\', and \'hours\'. 3. The function should utilize Python\'s `time` module for all time-related operations. 4. Ensure to handle edge cases such as invalid time formats, and large time differences gracefully. By solving this problem, students will demonstrate their ability to work with different time representations, perform time calculations, and handle common pitfalls related to date and time manipulation in Python.","solution":"import time from datetime import datetime def convert_and_diff_time(time_string: str, input_format: str, output_format: str, unit: str) -> str: Converts the input time_string from input_format to output_format and returns the difference between the converted time and the current time. The difference is returned in the specified unit. Parameters: - time_string (str): The original time as a string. - input_format (str): The format of the original time string. - output_format (str): The desired format for the output time string. - unit (str): The unit for the time difference (\'seconds\', \'milliseconds\', \'minutes\', \'hours\'). Returns: - str: The converted time string and the time difference in the specified unit concatenated with a comma. valid_units = {\'seconds\', \'milliseconds\', \'minutes\', \'hours\'} if unit not in valid_units: raise ValueError(\\"Invalid unit. Must be one of \'seconds\', \'milliseconds\', \'minutes\', \'hours\'.\\") try: # Parse the input time string input_time = datetime.strptime(time_string, input_format) except ValueError: raise ValueError(\\"The time string could not be parsed according to the input format.\\") # Convert the input time to the desired output format converted_time_string = input_time.strftime(output_format) # Get current time current_time = datetime.now() # Calculate the difference between the input time and the current time time_difference = (current_time - input_time).total_seconds() if unit == \'milliseconds\': time_difference *= 1000 elif unit == \'minutes\': time_difference /= 60 elif unit == \'hours\': time_difference /= 3600 return f\\"{converted_time_string}, {time_difference}\\""},{"question":"# Problem Description As a developer, you are tasked with building a fault tolerance mechanism for a critical application. You will use the `faulthandler` module to enhance the application\'s debugging capabilities by dumping tracebacks at critical moments. Your task is to write a Python function that: 1. **Enables the faulthandler** at the start of the application, specifying a log file to store the tracebacks. 2. **Schedules a traceback dump** after a specified timeout and ensures this is repeated periodically. 3. **Disables the faulthandler** once the application completes execution or crashes. # Function Signature ```python def setup_fault_handler(log_file_path: str, timeout: int) -> None: pass ``` # Input - `log_file_path` (str): The path of the log file where tracebacks should be dumped. - `timeout` (int): The time (in seconds) after which a traceback dump should be performed. This should be repeated periodically. # Output - None # Constraints - The `log_file_path` must be a valid file path and the file must be writable. - The `timeout` must be a positive integer. # Example Usage ```python setup_fault_handler(\\"/tmp/traceback.log\\", 5) # The function must: # - Enable faulthandler and ensure tracebacks are dumped into /tmp/traceback.log # - Set up traceback dumps to occur every 5 seconds # - Disable faulthandler gracefully when program terminates ``` # Additional Guidelines 1. Ensure that the log file is opened properly and remains open until the faulthandler is disabled. 2. Handle potential exceptions related to file operations gracefully. 3. Provide appropriate comments and documentation within your code for clarity. # Testing Your Solution To test your implementation, you may want to create a separate script that simulates long-running operations or deliberate faults (e.g., `ctypes.string_at(0)`) to observe the behavior.","solution":"import faulthandler import threading import time import os def setup_fault_handler(log_file_path: str, timeout: int) -> None: Enables the faulthandler, sets up periodic traceback dumps, and ensures it is disabled gracefully upon termination. Args: log_file_path (str): The path of the log file where tracebacks should be dumped. timeout (int): The time (in seconds) after which a traceback dump should be performed. Raises: ValueError: If timeout is not a positive integer. IOError: If log_file_path cannot be opened. if timeout <= 0: raise ValueError(\\"timeout must be a positive integer\\") # Ensure the log file can be opened for writing try: log_file = open(log_file_path, \'w\') except IOError as e: raise IOError(f\\"Cannot open log file {log_file_path}: {str(e)}\\") # Enable the faulthandler faulthandler.enable(log_file) # Function that dumps traceback and schedules the next dump def dump_traceback_periodically(): while True: time.sleep(timeout) faulthandler.dump_traceback(log_file, all_threads=True) # Start a background thread to dump tracebacks periodically dump_thread = threading.Thread(target=dump_traceback_periodically, daemon=True) dump_thread.start() # Register an atexit handler to ensure the faulthandler is disabled properly import atexit atexit.register(faulthandler.disable) # Add a signal handler to ensure graceful shutdown on SIGINT and SIGTERM import signal def disable_faulthandler(signum, frame): faulthandler.disable() log_file.close() signal.signal(signal.SIGINT, disable_faulthandler) signal.signal(signal.SIGTERM, disable_faulthandler)"},{"question":"You are required to parse a given XML string and process specific elements using the SAX parser. Implement a function `process_xml(xml_string)` that performs the following: 1. Creates a SAX parser. 2. Defines a custom content handler to handle the start, end, and characters of specific XML elements. Specifically, the handler should process `<title>` and `<author>` elements from the XML. 3. Parses the provided `xml_string` using the SAX parser and custom content handler. 4. The content handler should collect text data under `<title>` and `<author>` elements and return a dictionary containing these elements. # Specifications # Input: - `xml_string`: A string containing XML data. The XML will always have properly nested elements and may contain multiple `<title>` and `<author>` elements. # Output: - A dictionary with two keys: - `\\"titles\\"`: A list of strings, each representing a `<title>` element. - `\\"authors\\"`: A list of strings, each representing an `<author>` element. # Example ```python xml_data = <?xml version=\\"1.0\\"?> <library> <book> <title>Python Programming</title> <author>John Doe</author> </book> <book> <title>Advanced Python</title> <author>Jane Smith</author> </book> </library> result = process_xml(xml_data) print(result) # Expected output: {\'titles\': [\'Python Programming\', \'Advanced Python\'], \'authors\': [\'John Doe\', \'Jane Smith\']} ``` Constraints 1. The XML passed to `process_xml` will always be well-formed. 2. There will be no other nested elements within `<title>` and `<author>` tags.","solution":"import xml.sax class XMLContentHandler(xml.sax.ContentHandler): def __init__(self): self.current_element = \\"\\" self.titles = [] self.authors = [] self.current_data = \\"\\" def startElement(self, tag, attributes): self.current_element = tag self.current_data = \\"\\" def endElement(self, tag): if self.current_element == \\"title\\": self.titles.append(self.current_data) elif self.current_element == \\"author\\": self.authors.append(self.current_data) self.current_element = \\"\\" self.current_data = \\"\\" def characters(self, content): if self.current_element in [\\"title\\", \\"author\\"]: self.current_data += content.strip() def process_xml(xml_string): handler = XMLContentHandler() xml.sax.parseString(xml_string, handler) return {\\"titles\\": handler.titles, \\"authors\\": handler.authors}"},{"question":"# Question: Implement a Custom Autograd Function to Compute Sine and Its Gradient **Objective:** You are required to implement a custom autograd function using PyTorch\'s `torch.autograd.Function` to compute the sine of a tensor and its gradient. **Function Implementation:** - **Function Name:** `CustomSine` - **Expected Inputs:** - A single PyTorch tensor `input_tensor` with `requires_grad` set to `True`. - **Expected Outputs:** - The sine of the input tensor. **Details:** 1. **Forward Method:** - Compute the sine of the input tensor. 2. **Backward Method:** - Compute the gradient of the sine function, which is the cosine of the input tensor. - Return the gradient that is to be propagated to the previous layer. **Constraints and Requirements:** 1. Your implementation should subclass `torch.autograd.Function`. 2. Do not use built-in PyTorch functions for sine (`torch.sin`) and cosine (`torch.cos`). Instead, use the Taylor series expansion for sine and cosine to a reasonable number of terms for approximation. 3. Ensure that your `CustomSine` function correctly integrates with PyTorch\'s autograd system. **Hints:** - For sine: sin(x) ≈ x - x³/3! + x⁵/5! - x⁷/7! + ... - For cosine: cos(x) ≈ 1 - x²/2! + x⁴/4! - x⁶/6! + ... **Example Usage:** ```python import torch class CustomSine(torch.autograd.Function): @staticmethod def forward(ctx, input_tensor): # Save input_tensor for backward pass ctx.save_for_backward(input_tensor) # Implement Taylor series approximation for sine x = input_tensor sine_approx = x - (x**3) / 6 + (x**5) / 120 - (x**7) / 5040 return sine_approx @staticmethod def backward(ctx, grad_output): # Retrieve input_tensor input_tensor, = ctx.saved_tensors # Implement Taylor series approximation for cosine x = input_tensor cosine_approx = 1 - (x**2) / 2 + (x**4) / 24 - (x**6) / 720 # Multiply with the incoming gradient grad_input = grad_output * cosine_approx return grad_input # Testing the implementation input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) custom_sine = CustomSine.apply # Forward computation output = custom_sine(input_tensor) print(\\"Output:\\", output) # Backward computation output.sum().backward() print(\\"Gradients:\\", input_tensor.grad) ``` # Evaluation Criteria: - **Correctness:** The function should correctly compute the sine using the forward pass and its gradient using the backward pass. - **Efficiency:** Although you will use Taylor series, ensure that the number of terms used offers a balance between accuracy and computational efficiency. - **Autograd Integration:** The custom function must seamlessly integrate with PyTorch\'s autograd system, retaining compatibility with operations like backward propagation.","solution":"import torch class CustomSine(torch.autograd.Function): @staticmethod def forward(ctx, input_tensor): Applies the sine function to the input tensor using Taylor series approximation. # Save input_tensor for backward pass ctx.save_for_backward(input_tensor) # Implement Taylor series approximation for sine: x - x^3/3! + x^5/5! - x^7/7! x = input_tensor sine_approx = x - (x**3) / 6 + (x**5) / 120 - (x**7) / 5040 return sine_approx @staticmethod def backward(ctx, grad_output): Computes the gradient of the sine function using the Taylor series approximation for cosine. # Retrieve input_tensor input_tensor, = ctx.saved_tensors # Implement Taylor series approximation for cosine: 1 - x^2/2! + x^4/4! - x^6/6! x = input_tensor cosine_approx = 1 - (x**2) / 2 + (x**4) / 24 - (x**6) / 720 # Multiply with the incoming gradient grad_input = grad_output * cosine_approx return grad_input # Testing the implementation if __name__ == \\"__main__\\": input_tensor = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) custom_sine = CustomSine.apply # Forward computation output = custom_sine(input_tensor) print(\\"Output:\\", output) # Backward computation output.sum().backward() print(\\"Gradients:\\", input_tensor.grad)"},{"question":"Mocking and Testing with `unittest.mock` Objective: Demonstrate your understanding of mocking in Python using the `unittest.mock` package by writing test cases for a sample Python class. Problem Statement: You are given a class `UserService` that interfaces with a backend service to fetch user data. Your task is to write unittests for the `UserService` methods using `unittest.mock` to mock the backend service calls. Here is the `UserService` class: ```python import requests class UserService: def __init__(self, endpoint): self.endpoint = endpoint def get_user(self, user_id): response = requests.get(f\\"{self.endpoint}/users/{user_id}\\") if response.status_code == 200: return response.json() return None def user_exists(self, user_id): response = requests.head(f\\"{self.endpoint}/users/{user_id}\\") return response.status_code == 200 ``` Task: 1. **Write unittests** for the `UserService` class. 2. **Mock the `requests` library** to simulate API responses. 3. **Test different scenarios** including: - Successful user retrieval. - Failure in user retrieval (e.g., user not found). - Check if a user exists. Requirements: 1. Use the `unittest` module along with `unittest.mock`. 2. Include at least 3 tests demonstrating various functionalities of `unittest.mock`, such as `side_effect`, `return_value`, and `assert_called_with`. 3. Ensure tests are isolated and do not make actual network requests. Example: Here is an example template to get you started: ```python import unittest from unittest.mock import patch, MagicMock from user_service import UserService # Assuming the UserService is in user_service.py class TestUserService(unittest.TestCase): @patch(\'user_service.requests.get\') def test_get_user_success(self, mock_get): mock_get.return_value.status_code = 200 mock_get.return_value.json.return_value = {\'id\': 1, \'name\': \'John Doe\'} service = UserService(\'http://example.com\') user = service.get_user(1) self.assertIsNotNone(user) self.assertEqual(user[\'name\'], \'John Doe\') mock_get.assert_called_with(\'http://example.com/users/1\') @patch(\'user_service.requests.get\') def test_get_user_not_found(self, mock_get): # Implement your test @patch(\'user_service.requests.head\') def test_user_exists(self, mock_head): # Implement your test if __name__ == \'__main__\': unittest.main() ``` Additional Information: - You do not need to implement the `UserService` class itself. - Focus on using mocking techniques effectively. Submit your solution as a Python script containing your unittest class with the specified tests.","solution":"import requests class UserService: def __init__(self, endpoint): self.endpoint = endpoint def get_user(self, user_id): response = requests.get(f\\"{self.endpoint}/users/{user_id}\\") if response.status_code == 200: return response.json() return None def user_exists(self, user_id): response = requests.head(f\\"{self.endpoint}/users/{user_id}\\") return response.status_code == 200"},{"question":"**Coding Assessment Question:** # File Management and Path Manipulation in Python You are tasked with creating a Python script that will help manage and analyze file paths in a specified directory. Your goal is to implement several functions using the `os.path` module to demonstrate your understanding of pathname manipulations. Requirements: 1. **Retrieve File Details**: Implement a function `get_file_details(file_path)` which takes a single file path and returns a dictionary with the following details: - `absolute_path`: The absolute path of the file. - `size`: The size of the file in bytes. - `access_time`: The time of last access of the file. - `modification_time`: The time of last modification of the file. - `creation_time`: The creation time (or metadata change time) of the file. ```python def get_file_details(file_path: str) -> dict: # Your implementation here ``` 2. **Find Common Prefix in Paths**: Implement a function `common_path_prefix(paths)` which takes a list of file paths and returns the longest common prefix path (character-by-character). ```python def common_path_prefix(paths: list) -> str: # Your implementation here ``` 3. **Check Path Type**: Implement a function `path_type(path)` which determines if the given path is a file or directory and returns `\'file\'`, `\'directory\'`, or `\'unknown\'` if it is neither. ```python def path_type(path: str) -> str: # Your implementation here ``` 4. **Normalize and Split Path**: Implement a function `normalize_and_split_path(path)` which takes a file path, normalizes it (removes redundant separators and up-level references), and then splits it into its directory and base name. Return a tuple containing the normalized path, directory name, and base name. ```python def normalize_and_split_path(path: str) -> tuple: # Your implementation here ``` 5. **Generate Relative Path**: Implement a function `generate_relative_path(path, start=os.curdir)` which computes the relative file path to `path` from the `start` directory. The `start` parameter is optional and should default to the current directory. ```python def generate_relative_path(path: str, start: str = os.curdir) -> str: # Your implementation here ``` Input and Output: - Each function should handle paths as strings and return the required outputs as specified. - Raise appropriate exceptions if the paths provided do not exist or are invalid. - Ensure to test your functions with various edge cases and document any assumptions made. # Constraints: - Assume all file paths provided are valid within the operating system context. - Your functions should handle both absolute and relative paths. - All solutions should be efficient and aim for a time complexity of O(n) where applicable. Provide clear and documented code for each function and also include test cases to validate your implementations. This will help in assessing your problem-solving skills and your understanding of path manipulations using the `os.path` module.","solution":"import os import time def get_file_details(file_path): Retrieves details about a file path. Args: file_path (str): The path to the file. Returns: dict: A dictionary with file details. if not os.path.exists(file_path): raise FileNotFoundError(f\\"The file {file_path} does not exist.\\") return { \\"absolute_path\\": os.path.abspath(file_path), \\"size\\": os.path.getsize(file_path), \\"access_time\\": time.ctime(os.path.getatime(file_path)), \\"modification_time\\": time.ctime(os.path.getmtime(file_path)), \\"creation_time\\": time.ctime(os.path.getctime(file_path)) } def common_path_prefix(paths): Finds the longest common prefix path from a list of paths. Args: paths (list): A list of file paths. Returns: str: The longest common prefix path. return os.path.commonprefix(paths) def path_type(path): Determines if the given path is a file or directory. Args: path (str): The path to check. Returns: str: \'file\', \'directory\', or \'unknown\'. if os.path.isfile(path): return \'file\' elif os.path.isdir(path): return \'directory\' else: return \'unknown\' def normalize_and_split_path(path): Normalizes a path and splits it into directory and base name. Args: path (str): The path to normalize and split. Returns: tuple: A tuple containing the normalized path, directory name, and base name. normalized_path = os.path.normpath(path) dir_name = os.path.dirname(normalized_path) base_name = os.path.basename(normalized_path) return normalized_path, dir_name, base_name def generate_relative_path(path, start=os.curdir): Computes the relative file path to `path` from the `start` directory. Args: path (str): The target path. start (str): The starting directory. Returns: str: The relative path from the start directory to the target path. return os.path.relpath(path, start)"},{"question":"# Advanced Coding Assessment Question: Recursive Directory Comparison **Objective**: Write a Python function that uses the `filecmp` module to recursively compare two directory trees and generate a detailed report of differences. **Function Signature**: ```python def compare_directories(dir1: str, dir2: str) -> dict: pass ``` **Input**: - `dir1`: A string representing the path of the first directory. - `dir2`: A string representing the path of the second directory. **Output**: - A dictionary with the following structure: ```python { \\"left_only\\": [list of paths relative to dir1 only present in dir1], \\"right_only\\": [list of paths relative to dir2 only present in dir2], \\"differences\\": [list of paths relative to dir1 that are different in dir1 and dir2], \\"errors\\": [list of paths where comparison was not possible] } ``` **Constraints**: - Do not use external libraries for file comparison other than the standard `filecmp` module. - Handle all subdirectories recursively. - Ensure detailed reporting for every file and subdirectory, including reasons for any errors. **Performance Requirements**: - The solution should be efficient enough to handle large directory trees with multiple levels of subdirectories and a large number of files, without running into performance bottlenecks. # Example ```python # Example directory structure: # dir1/ # ├── file1.txt # └── subdir1/ # └── file2.txt # dir2/ # ├── file1.txt # └── subdir1/ # └── file3.txt result = compare_directories(\'dir1\', \'dir2\') print(result) # Output might look like: # { # \\"left_only\\": [\\"subdir1/file2.txt\\"], # \\"right_only\\": [\\"subdir1/file3.txt\\"], # \\"differences\\": [], # \\"errors\\": [] # } ``` # Notes: - Use `filecmp.dircmp` for the directory comparisons. - Carefully handle recursion to ensure all subdirectories are compared. - Make sure to handle and report any errors encountered during file comparisons. Good luck!","solution":"import os import filecmp def compare_directories(dir1: str, dir2: str) -> dict: def _compare_dirs(cd, results): results[\'left_only\'].extend([os.path.join(cd.left, p) for p in cd.left_only]) results[\'right_only\'].extend([os.path.join(cd.right, p) for p in cd.right_only]) results[\'differences\'].extend([os.path.join(cd.left, p) for p in cd.diff_files]) results[\'errors\'].extend([os.path.join(cd.left, p) for p in cd.common_funny]) for subdir in cd.subdirs.values(): _compare_dirs(subdir, results) results = { \\"left_only\\": [], \\"right_only\\": [], \\"differences\\": [], \\"errors\\": [] } comparison = filecmp.dircmp(dir1, dir2) _compare_dirs(comparison, results) return results"},{"question":"# Email Data Parsing and JSON Conversion You are provided with an email dataset containing various email messages. Each email message has the following attributes: `Subject`, `From`, `To`, `Date`, and `Body`. These email messages need to be parsed, processed, and finally converted into a JSON format for further analysis. Requirements: 1. **Email Parsing**: - Parse the given email string to extract its components (Subject, From, To, Date, Body). 2. **JSON Conversion**: - Convert the parsed email data into a JSON formatted string. - Ensure that the JSON object includes the following keys: `subject`, `from`, `to`, `date`, and `body`. 3. **Output**: - Function should have the following signature: ```python def email_to_json(email_string: str) -> str: pass ``` - The function should take a single email string as input and return a JSON formatted string as output. Constraints: - The email string will always have at least the `Subject`, `From`, `To`, and `Date` fields. - The `Body` field can contain multiple lines and must be preserved as a single string in the JSON output. - Email headers and body are separated by a blank line. Example: ```python email_string = Subject: Test Email From: sender@example.com To: receiver@example.com Date: Mon, 7 Jun 2021 12:34:56 -0700 This is the body of the email. It has multiple lines. output = email_to_json(email_string) print(output) ``` Expected output: ```json { \\"subject\\": \\"Test Email\\", \\"from\\": \\"sender@example.com\\", \\"to\\": \\"receiver@example.com\\", \\"date\\": \\"Mon, 7 Jun 2021 12:34:56 -0700\\", \\"body\\": \\"This is the body of the email.nIt has multiple lines.\\" } ``` # Additional Notes: - You may assume the input email string is well-formed and contains no malformed headers. - Use appropriate libraries from the `email` module for parsing. - Ensure proper error handling and validation. Performance: - The function should efficiently handle email strings up to 100KB in size. Implement the function to meet the above requirements.","solution":"import json from email import message_from_string def email_to_json(email_string: str) -> str: Parses an email string and converts it to a JSON formatted string. Parameters: email_string (str): Raw email string. Returns: str: JSON formatted email. msg = message_from_string(email_string) email_data = { \\"subject\\": msg.get(\\"Subject\\"), \\"from\\": msg.get(\\"From\\"), \\"to\\": msg.get(\\"To\\"), \\"date\\": msg.get(\\"Date\\"), \\"body\\": msg.get_payload() } return json.dumps(email_data, indent=4)"},{"question":"Group Information Summary Objective Write a function that summarizes the group information of the Unix system using the `grp` module. Your function should return a dictionary with the group name as the key and a tuple of (group id, member list) as the value. Function Signature ```python def summarize_group_info() -> dict: pass ``` Input - None Output - A dictionary where each key is a group name (string) and each value is a tuple containing: 1. Group ID (`int`) 2. A member list (`list` of `str`) Example Suppose the Unix group database contains the following entries: - Group A: ID = 1001, Members = [\'user1\', \'user2\'] - Group B: ID = 1002, Members = [\'user3\'] - Group C: ID = 1003, Members = [] The function should return: ```python { \'A\': (1001, [\'user1\', \'user2\']), \'B\': (1002, [\'user3\']), \'C\': (1003, []) } ``` Constraints - The function should handle the case where some groups have no members. - Group names starting with `+` or `-` should be excluded. - You may assume that the `grp` module is available and that the function will only be run in Unix environments. Notes - Make sure to consider edge cases, such as groups with special characters in their names (other than `+` or `-` which should be excluded). - Utilize the `grp.getgrall()` function to retrieve all groups. - Handle exceptions where necessary, especially `TypeError` and `KeyError`. Implementation Guidelines 1. Import the necessary module: `import grp`. 2. Fetch all group entries using `grp.getgrall()`. 3. Filter out groups whose names start with `+` or `-`. 4. Construct the resulting dictionary with the required format and return it. ```python import grp def summarize_group_info() -> dict: group_summary = {} try: groups = grp.getgrall() for group in groups: if not (group.gr_name.startswith(\\"+\\") or group.gr_name.startswith(\\"-\\")): group_summary[group.gr_name] = (group.gr_gid, group.gr_mem) except KeyError as ke: print(f\\"KeyError encountered: {ke}\\") except TypeError as te: print(f\\"TypeError encountered: {te}\\") return group_summary # Example usage if __name__ == \\"__main__\\": print(summarize_group_info()) ```","solution":"import grp def summarize_group_info() -> dict: Summarizes group information from the Unix system. Returns: dict: A dictionary where each key is a group name (str) and each value is a tuple containing the group ID (int) and a member list (list of str). group_summary = {} try: groups = grp.getgrall() for group in groups: if not (group.gr_name.startswith(\\"+\\") or group.gr_name.startswith(\\"-\\")): group_summary[group.gr_name] = (group.gr_gid, group.gr_mem) except KeyError as ke: print(f\\"KeyError encountered: {ke}\\") except TypeError as te: print(f\\"TypeError encountered: {te}\\") return group_summary"},{"question":"**Objective:** Use the `platform` module to write a function that gathers detailed system information and returns it in a structured format. **Problem Statement:** Write a function `gather_system_info()` that collects detailed information about the underlying system, the Python interpreter being used, and the operating system. The function should return this information as a dictionary with the following structure: ```python { \\"architecture\\": { \\"bits\\": \\"<str>\\", \\"linkage\\": \\"<str>\\" }, \\"machine\\": \\"<str>\\", \\"node\\": \\"<str>\\", \\"platform\\": \\"<str>\\", \\"processor\\": \\"<str>\\", \\"python\\": { \\"build_number\\": \\"<str>\\", \\"build_date\\": \\"<str>\\", \\"compiler\\": \\"<str>\\", \\"implementation\\": \\"<str>\\", \\"version\\": { \\"string\\": \\"<str>\\", \\"tuple\\": ( \\"<str>\\", \\"<str>\\", \\"<str>\\" ) } }, \\"system\\": { \\"name\\": \\"<str>\\", \\"release\\": \\"<str>\\", \\"version\\": \\"<str>\\", }, \\"libc_version\\": { \\"lib\\": \\"<str>\\", \\"version\\": \\"<str>\\" } } ``` **Guidelines:** 1. Use the appropriate functions from the `platform` module to gather the necessary system information. 2. Ensure that each entry in the returned dictionary corresponds to the value retrieved from the `platform` functions. 3. For functions that return tuples, make sure to access the correct elements to populate the dictionary. 4. Handle any cases where the `platform` functions may return empty strings or default values. **Function Signature:** ```python def gather_system_info() -> dict: ``` **Example Usage:** ```python info = gather_system_info() print(info) ``` **Expected Output:** The structure of the output dictionary should match the one given in the problem statement, populated with actual values from the `platform` module functions. **Constraints:** - Ensure that your function is compatible with Python 3.10 and utilizes the `platform` module as per the provided documentation. - Your implementation should handle different operating systems without modification (cross-platform compatibility). Good luck!","solution":"import platform def gather_system_info() -> dict: Gathers detailed system information and returns it in a structured dictionary format. architecture = platform.architecture() python_version_tuple = platform.python_version_tuple() system_info = { \\"architecture\\": { \\"bits\\": architecture[0], \\"linkage\\": architecture[1] }, \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python\\": { \\"build_number\\": platform.python_build()[0], \\"build_date\\": platform.python_build()[1], \\"compiler\\": platform.python_compiler(), \\"implementation\\": platform.python_implementation(), \\"version\\": { \\"string\\": platform.python_version(), \\"tuple\\": python_version_tuple } }, \\"system\\": { \\"name\\": platform.system(), \\"release\\": platform.release(), \\"version\\": platform.version(), }, \\"libc_version\\": { \\"lib\\": platform.libc_ver()[0], \\"version\\": platform.libc_ver()[1] } } return system_info"},{"question":"Objective: Implement a function that utilizes the \\"mimetypes\\" module to determine MIME type statistics in a given directory. Problem Statement: Write a function `mime_type_statistics(directory: str) -> dict` that takes a directory path as input and returns a dictionary containing MIME type statistics for all files within the directory and its subdirectories. Function Specification: - **Input:** - `directory`: a string representing the path to the directory. - **Output:** - Returns a dictionary where the keys are MIME types (e.g., `\'image/jpeg\'`, `\'text/html\'`, etc.), and the values are the counts of files associated with each MIME type in the given directory and its subdirectories. - **Constraints and Assumptions:** - The function should process all files in the specified directory and its subdirectories. - If a file\'s MIME type cannot be determined, it should be counted under the key `\'unknown\'`. - Use the `mimetypes` module functions `guess_type()` to determine the MIME type of each file. - Assume the directory paths and files\' accessibility are valid and no error handling for invalid paths is required. Example: ```python import os import mimetypes def mime_type_statistics(directory: str) -> dict: mime_counts = {\'unknown\': 0} for root, _, files in os.walk(directory): for file in files: filepath = os.path.join(root, file) mime_type, _ = mimetypes.guess_type(filepath) if mime_type is None: mime_type = \'unknown\' if mime_type in mime_counts: mime_counts[mime_type] += 1 else: mime_counts[mime_type] = 1 return mime_counts # Example usage: # Suppose the specified directory contains the following files: # - file1.txt (text/plain) # - file2.jpg (image/jpeg) # - file3.png (image/png) # - script.py (text/x-python) # The function should return: # { # \'text/plain\': 1, # \'image/jpeg\': 1, # \'image/png\': 1, # \'text/x-python\': 1 # } ``` Hints: - Utilize the `os.walk()` function to traverse directory and its subdirectories. - Use `mimetypes.guess_type()` to determine the MIME type of each file. - Pay attention to cases where the MIME type cannot be determined; handle this scenario appropriately in your code. Submit your implementation as the solution to this coding assessment question.","solution":"import os import mimetypes def mime_type_statistics(directory: str) -> dict: mime_counts = {\'unknown\': 0} for root, _, files in os.walk(directory): for file in files: filepath = os.path.join(root, file) mime_type, _ = mimetypes.guess_type(filepath) if mime_type is None: mime_type = \'unknown\' if mime_type in mime_counts: mime_counts[mime_type] += 1 else: mime_counts[mime_type] = 1 return mime_counts"},{"question":"# Advanced Python Garbage Collection In this assessment, you will demonstrate your understanding of Python\'s garbage collector (gc) module. You are tasked with implementing a function that analyzes the current state of the garbage collector, toggles its state under specific conditions, and fetches debugging information if prompted. # Function Specification Implement a function `analyze_gc(state: str, debug: bool) -> dict` that performs the following tasks: 1. **Parameters:** - `state` (str): A string that can be either `\\"enable\\"` or `\\"disable\\"`, indicating whether to enable or disable the garbage collector. - `debug` (bool): A boolean flag indicating whether to fetch and return debugging information. 2. **Functionality:** - If the `state` is `\\"enable\\"`, enable the garbage collector using `gc.enable()`. - If the `state` is `\\"disable\\"`, disable the garbage collector using `gc.disable()`. - Fetch the current state of the garbage collector (whether it is enabled or disabled) using `gc.isenabled()`. - If the `debug` flag is `True`, set the debugging flag for the garbage collector to `gc.set_debug(gc.DEBUG_LEAK)`. - Collect current garbage collection statistics using `gc.get_stats()`. - If the `debug` flag is `True`, fetch the current debugging flags and garbage objects. 3. **Output:** - Return a dictionary with the following keys: - `\\"gc_enabled\\"`: A boolean indicating whether the garbage collector is enabled. - `\\"stats\\"`: The current garbage collection statistics obtained using `gc.get_stats()`. - `\\"debug_flags\\"`: The debugging flags if the `debug` flag is `True`; otherwise, set to `None`. - `\\"garbage_obj_count\\"`: The count of objects in the `gc.garbage` list if the `debug` flag is `True`; otherwise, set to `None`. # Example Usage: ```python import gc def analyze_gc(state: str, debug: bool) -> dict: # Implementation goes here # Example Calls: result = analyze_gc(\\"enable\\", True) print(result) result = analyze_gc(\\"disable\\", False) print(result) ``` # Constraints: - The `state` parameter can only be \\"enable\\" or \\"disable\\". - The function should handle invalid inputs gracefully without raising exceptions. - Aim for efficient usage and interaction with the `gc` module. # Additional Notes: - This task requires a comprehensive understanding of the `gc` module functions and their correct usage. - Test the function thoroughly, especially ensuring that it correctly handles toggling the garbage collector state and fetching debug information.","solution":"import gc def analyze_gc(state: str, debug: bool) -> dict: # Validate the input state if state not in [\\"enable\\", \\"disable\\"]: return { \\"gc_enabled\\": None, \\"stats\\": None, \\"debug_flags\\": None, \\"garbage_obj_count\\": None } # Toggle the GC state based on input if state == \\"enable\\": gc.enable() elif state == \\"disable\\": gc.disable() # Fetch the current state of the garbage collector gc_enabled = gc.isenabled() # Set the debug flag if requested if debug: gc.set_debug(gc.DEBUG_LEAK) else: gc.set_debug(0) # Collect current garbage collection stats stats = gc.get_stats() # Gather debug information if required debug_flags = gc.get_debug() if debug else None garbage_obj_count = len(gc.garbage) if debug else None return { \\"gc_enabled\\": gc_enabled, \\"stats\\": stats, \\"debug_flags\\": debug_flags, \\"garbage_obj_count\\": garbage_obj_count }"},{"question":"# Question: **Optimizing Prediction Latency and Throughput with Scikit-learn** You are given a dataset and tasked with building and optimizing a machine learning model using scikit-learn. Your goal is to minimize both the prediction latency and maximize the prediction throughput while maintaining reasonable predictive accuracy. Requirements: 1. **Data and Model**: - The dataset will consist of numerical features only. - You can choose any model from the `sklearn.linear_model`, `sklearn.svm` or `sklearn.ensemble` modules. 2. **Performance Optimization**: - Use bulk prediction wherever possible. - Optimize the model’s complexity to balance between latency and accuracy. - Utilize sparse matrix representations if beneficial. - Configure scikit-learn to reduce validation overhead during prediction. 3. **Metrics**: - Measure the prediction latency (in microseconds) and throughput (in predictions per second). - Report the accuracy of your model. Constraints: - Assume finite input data (use `SKLEARN_ASSUME_FINITE`). - Limit the temporary working memory to 128 MiB using `config_context`. - Ensure the sparsity ratio of your input data is greater than 90% when converted to sparse format (if applicable). Input: - Training data (`X_train` - 2D numpy array of shape (n_samples, n_features), `y_train` - 1D numpy array of shape (n_samples)) - Testing data (`X_test` - 2D numpy array of shape (n_samples, n_features), `y_test` - 1D numpy array of shape (n_samples)) Output: - `latency` - Average prediction latency in microseconds (float) - `throughput` - Prediction throughput in predictions per second (float) - `accuracy` - Accuracy of the model on the testing data (float) - `model` - The trained machine learning model (sklearn object) Definition: Implement the function `optimize_model_performance(X_train, y_train, X_test, y_test)` which returns the following: ```python def optimize_model_performance(X_train, y_train, X_test, y_test): # Your implementation here return latency, throughput, accuracy, model ``` **Note:** - You may use any configuration setting, model parameter tuning, and feature extraction technique discussed in the documentation to achieve the goal. - Ensure that the implementation is efficient and well-documented. Example: ```python # Sample Input X_train = np.random.rand(1000, 500) y_train = np.random.randint(0, 2, 1000) X_test = np.random.rand(200, 500) y_test = np.random.randint(0, 2, 200) # Function Call latency, throughput, accuracy, model = optimize_model_performance(X_train, y_train, X_test, y_test) # Sample Output # latency: 50.123 # in microseconds # throughput: 20000.5 # predictions per second # accuracy: 0.87 # 87% # model: trained sklearn model object ```","solution":"from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline from scipy.sparse import csr_matrix import numpy as np import time from sklearn.metrics import accuracy_score from sklearn import config_context def optimize_model_performance(X_train, y_train, X_test, y_test): # Ensuring finite input data for scikit-learn import os os.environ[\'SKLEARN_ASSUME_FINITE\'] = \'True\' # Convert data to sparse format if sparsity ratio > 90% sparsity_ratio = 1.0 - np.count_nonzero(X_train) / X_train.size if sparsity_ratio > 0.9: X_train = csr_matrix(X_train) X_test = csr_matrix(X_test) # Using Logistic Regression due to its efficiency model = make_pipeline( StandardScaler(with_mean=False), # with_mean=False is necessary for sparse input LogisticRegression(solver=\'saga\', max_iter=100, n_jobs=-1) ) with config_context(assume_finite=True, working_memory=128): model.fit(X_train, y_train) # Measuring prediction latency and throughput start_time = time.time() y_pred = model.predict(X_test) end_time = time.time() latency = (end_time - start_time) / X_test.shape[0] * 1e6 # Convert seconds to microseconds throughput = X_test.shape[0] / (end_time - start_time) accuracy = accuracy_score(y_test, y_pred) return latency, throughput, accuracy, model"},{"question":"You are given the seaborn `objects` documentation as seen in the cell below. Using the seaborn package, follow the steps and write code to create a plot. # Task 1. **Load the `penguins` dataset** using seaborn. 2. Create a plot that: - Displays the `body_mass_g` of the penguins on the x-axis. - Displays different species on the y-axis, using different colors for different `sex`. - Adds points and error bars representing the standard deviation of the body mass. 3. Additionally, facet the plot by `species`. Use seaborn\'s object-oriented interface to implement this. Your final output should be a plot that satisfies all the mentioned criteria. # Input - None, as you will be using the built-in seaborn `penguins` dataset. # Output - A seaborn plot with faceted species plots displaying body mass and error bars by sex. # Constraints - Make sure to handle the scenario where specific properties like `linestyle` and `linewidth` might interfere with other components unintentionally. # Performance Requirements - The solution should efficiently handle the provided dataset. # Example This is a small snippet of what the dataset looks like: ``` species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex 0 Adelie Torgersen 39.1 18.7 181 3750 Male 1 Adelie Torgersen 39.5 17.4 186 3800 Female ... ``` # Required Libraries To install the necessary libraries, use: ```bash pip install seaborn ``` **Sample Code Template**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .facet(\\"species\\") ) ```","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset penguins = load_dataset(\\"penguins\\") # Create the plot plot = ( so.Plot(penguins, x=\\"body_mass_g\\", y=\\"species\\", color=\\"sex\\") .add(so.Dot(), so.Agg(), so.Dodge()) .add(so.Range(), so.Est(errorbar=\\"sd\\"), so.Dodge()) .facet(\\"species\\") ) plot.show()"},{"question":"Objective: Implement a Python function to determine the version components from a given hexadecimal version number. Problem Statement: You are given a hexadecimal version number encoded as a 32-bit integer. Your task is to implement a function `decode_version(hex_version)` that decodes this integer into its respective version components. Function Signature: ```python def decode_version(hex_version: int) -> str: pass ``` Input: - `hex_version` (int): A 32-bit integer representing the encoded version number. Output: - Returns the version components as a string formatted like \\"X.Y.ZlevelN\\", where: - `X` is the major version. - `Y` is the minor version. - `Z` is the micro version. - `level` is represented as: - \'a\' for alpha (`0xA`) - \'b\' for beta (`0xB`) - \'rc\' for release candidate (`0xC`) - \'\' (empty string) for final (`0xF`) - `N` is the release serial. Example: ```python assert decode_version(0x030401a2) == \\"3.4.1a2\\" assert decode_version(0x030a00f0) == \\"3.10.0\\" ``` Constraints: - The `hex_version` will always be a valid 32-bit integer representing a version number according to the described encoding scheme. Notes: - You need to extract each component from the integer using bitwise operations. - Ensure your implementation handles the conversion for different release levels correctly. Hints: - Use bit masking and shifts to extract specific bits from the integer. - Consider the example provided in the documentation to guide your bit manipulation logic. Performance: - Your implementation should efficiently handle the extraction of version components using bitwise operations.","solution":"def decode_version(hex_version: int) -> str: Decodes a hexadecimal version number into its respective version components. Args: hex_version (int): A 32-bit integer representing the encoded version number. Returns: str: The version components formatted as \\"X.Y.ZlevelN\\". # Extract version components using bitwise operations X = (hex_version >> 24) & 0xFF Y = (hex_version >> 16) & 0xFF Z = (hex_version >> 8) & 0xFF level = (hex_version >> 4) & 0xF N = hex_version & 0xF # Define the levels corresponding to their hexadecimal values level_map = { 0xA: \'a\', 0xB: \'b\', 0xC: \'rc\', 0xF: \'\' } # Construct the version string level_str = level_map.get(level, \'\') if level_str: version = f\\"{X}.{Y}.{Z}{level_str}{N}\\" else: version = f\\"{X}.{Y}.{Z}\\" return version"},{"question":"# Advanced Python310 Asyncio Task Scheduler Objective In this assessment, you will demonstrate your understanding of Python\'s asyncio library by creating an advanced task scheduler. The scheduler will manage multiple tasks, synchronize them using a queue, and handle task timeouts and cancellations efficiently. Problem Statement You are required to implement a function `async_task_scheduler` that takes a list of tasks (coroutines) and processes them using an asyncio queue. The scheduler should enforce a maximum execution time for each task, handle cancellations, and collect the results of the successful tasks. Detailed Requirements: 1. **Function Definition:** ```python async def async_task_scheduler(tasks: List[Awaitable], max_time: float) -> List[Any]: pass ``` 2. **Parameters:** - `tasks`: A list of asyncio coroutines representing the tasks to be scheduled. - `max_time`: A float representing the maximum allowed execution time for each task in seconds. 3. **Returns:** - A list containing the results of the tasks that completed successfully within the `max_time`. If a task is cancelled or times out, it should not be included in the results list. 4. **Constraints and Conditions:** - Use asyncio `Queue` to manage the tasks. - Each task should have a maximum execution time enforced using `wait_for()`. - Handle the `asyncio.TimeoutError` exception to determine tasks that did not complete on time. - If a task is cancelled, `asyncio.CancelledError` should be handled. - Ensure all tasks are given a fair chance to run using asyncio scheduling. 5. **Performance Requirements:** - The implementation should efficiently manage the tasks using the provided asyncio functions to optimize concurrency. Example: ```python import asyncio async def sample_task(duration): await asyncio.sleep(duration) return f\\"Completed in {duration}s\\" tasks = [ sample_task(1), sample_task(2), sample_task(5) # This task will likely timeout if max_time < 5 ] results = await async_task_scheduler(tasks, max_time=3) print(results) # Expected output: [\\"Completed in 1s\\", \\"Completed in 2s\\"] ``` *In this example, \\"sample_task(5)\\" will timeout and will not be included in the results because `max_time` is 3 seconds.* Write your code solution below: ```python import asyncio from typing import List, Any, Awaitable async def async_task_scheduler(tasks: List[Awaitable], max_time: float) -> List[Any]: # Write your implementation here. ``` Good luck!","solution":"import asyncio from typing import List, Any, Awaitable async def async_task_scheduler(tasks: List[Awaitable], max_time: float) -> List[Any]: async def run_task(task): try: return await asyncio.wait_for(task, max_time) except asyncio.TimeoutError: return None except asyncio.CancelledError: return None results = await asyncio.gather(*[run_task(task) for task in tasks]) return [result for result in results if result is not None]"},{"question":"**Title: Manipulating Abstract Syntax Trees** # Problem Statement You are required to write a function that analyzes a given Python function and replaces all occurrences of a specified variable name with another. Write a function `rename_variable_in_code(code: str, old_var: str, new_var: str) -> str` that takes: 1. `code` (str): A string that contains the source code of a Python function. 2. `old_var` (str): The name of the variable in the function that needs to be replaced. 3. `new_var` (str): The new variable name that replaces the old one. The function should return a string containing the modified source code with all instances of `old_var` renamed to `new_var`. # Requirements: 1. You must use the `ast` module to parse the input code, perform the renaming, and then regenerate the code. 2. Ensure that the renaming does not change the function\'s behavior, i.e., handle the scoping rules correctly. 3. Comments and docstrings in the code should remain unchanged. # Example ```python code = \'\'\' def example_function(): x = 10 y = x + 5 return y \'\'\' new_code = rename_variable_in_code(code, \'x\', \'z\') print(new_code) ``` **Expected Output:** ```python def example_function(): z = 10 y = z + 5 return y ``` # Constraints: - The function names and other identifiers should not be renamed. - Assume that the `old_var` will not conflict with any existing keywords or built-in functions. # Tips: - Use `ast.parse` to convert the source code into an AST. - Create a custom `NodeTransformer` to traverse and modify the AST. - Use `ast.unparse` or similar methods to convert the modified AST back into source code. # Submission: Provide the complete function implementation along with any necessary helper functions.","solution":"import ast class RenameVariableVisitor(ast.NodeTransformer): def __init__(self, old_var, new_var): self.old_var = old_var self.new_var = new_var def visit_Name(self, node): if node.id == self.old_var: return ast.copy_location(ast.Name(id=self.new_var, ctx=node.ctx), node) return node def rename_variable_in_code(code: str, old_var: str, new_var: str) -> str: Analyzes a given Python function source code and replaces all occurrences of a specified variable name with another using the ast module. :param code: str: Source code of a Python function. :param old_var: str: Name of the variable that needs to be replaced. :param new_var: str: New variable name that replaces the old one. :return: str: Modified source code with all instances of old_var renamed to new_var. tree = ast.parse(code) rename_visitor = RenameVariableVisitor(old_var, new_var) modified_tree = rename_visitor.visit(tree) return ast.unparse(modified_tree)"},{"question":"# Advanced Sorting in Python You are given a list of dictionaries where each dictionary represents data about different products. Each dictionary contains the following keys: - `name`: the name of the product (string). - `price`: the price of the product (float). - `rating`: the customer rating of the product, on a scale of 1 to 5 (float). - `date_added`: the date the product was added to the inventory (string in the format \\"YYYY-MM-DD\\"). Write a function `sort_products` that sorts this list in a specified order based on multiple criteria. The function should accept: - A list of dictionaries (`products`). - A list of tuples specifying the sorting criteria (`criteria`). Each tuple contains two elements: 1. the key by which to sort (one of `\\"name\\"`, `\\"price\\"`, `\\"rating\\"`, `\\"date_added\\"`). 2. a boolean indicating whether the sort should be descending (`True` for descending, `False` for ascending). The function should return the sorted list of dictionaries. Input: - `products`: List of dictionaries containing product data. - `criteria`: List of tuples, each containing a string and a boolean. Output: - A sorted list of dictionaries. Constraints: - The list of products contains at least one dictionary. - The list of dictionaries can be very large (up to 10,000 items). - The sorting should be performed efficiently. # Example: ```python products = [ {\\"name\\": \\"Product A\\", \\"price\\": 30.0, \\"rating\\": 4.5, \\"date_added\\": \\"2022-09-15\\"}, {\\"name\\": \\"Product B\\", \\"price\\": 20.0, \\"rating\\": 4.7, \\"date_added\\": \\"2021-06-10\\"}, {\\"name\\": \\"Product C\\", \\"price\\": 25.0, \\"rating\\": 4.3, \\"date_added\\": \\"2022-01-22\\"}, {\\"name\\": \\"Product D\\", \\"price\\": 35.0, \\"rating\\": 4.5, \\"date_added\\": \\"2021-03-01\\"} ] criteria = [(\\"rating\\", True), (\\"price\\", False)] sorted_products = sort_products(products, criteria) print(sorted_products) ``` Expected Output: ```python [ {\\"name\\": \\"Product B\\", \\"price\\": 20.0, \\"rating\\": 4.7, \\"date_added\\": \\"2021-06-10\\"}, {\\"name\\": \\"Product A\\", \\"price\\": 30.0, \\"rating\\": 4.5, \\"date_added\\": \\"2022-09-15\\"}, {\\"name\\": \\"Product D\\", \\"price\\": 35.0, \\"rating\\": 4.5, \\"date_added\\": \\"2021-03-01\\"}, {\\"name\\": \\"Product C\\", \\"price\\": 25.0, \\"rating\\": 4.3, \\"date_added\\": \\"2022-01-22\\"} ] ``` # Considerations: - Consider using the `attrgetter` from the `operator` module for efficient sorting. - Ensure the sorting is stable. - Handle dates appropriately for comparison.","solution":"from operator import itemgetter from datetime import datetime def sort_products(products, criteria): Sorts the list of product dictionaries based on multiple criteria. Parameters: products (list of dict): List of product dictionaries to sort. criteria (list of tuple): Sorting criteria specified as (key, descending). Returns: list of dict: Sorted list of product dictionaries. for key, descending in reversed(criteria): if key == \\"date_added\\": products.sort(key=lambda p: datetime.strptime(p[key], \\"%Y-%m-%d\\"), reverse=descending) else: products.sort(key=itemgetter(key), reverse=descending) return products"},{"question":"**Question**: Implement a function named `function_overview` that takes a Python function as an argument and returns a detailed summary of various aspects of that function. The summary should include: 1. **Function Signature**: The calling signature of the function, including parameter names, default values, and type annotations. 2. **Source Code**: The original source code of the function. 3. **Comments**: Any comments immediately preceding the function\'s definition in the source file. 4. **File Information**: The file in which the function is defined and the line number where the function starts. 5. **Attributes**: Any special attributes of the function (`__name__`, `__doc__`, `__module__`, `__annotations__`, etc.) # Constraints: - The function should raise a `TypeError` if the argument is not a function. - The solution must handle cases where certain information may not be available (e.g., source code for built-in functions). - The function should preserve the structure of the function summary even if some parts are missing. # Input: - A Python function object. # Output: - A dictionary containing the detailed summary. # Expected Output Format: ```python { \\"signature\\": \\"(x: int, y: int = 20) -> int\\", \\"source_code\\": \\"def add(x: int, y: int = 20) -> int:n return x + y\\", \\"comments\\": \\"# Adds two numbers\\", \\"file\\": \\"/path/to/your/file.py\\", \\"line_number\\": 5, \\"attributes\\": { \\"__name__\\": \\"add\\", \\"__doc__\\": \\"Adds two integers\\", \\"__module__\\": \\"__main__\\", \\"__annotations__\\": {\'x\': int, \'y\': int, \'return\': int} } } ``` # Example: ```python def add(x: int, y: int = 20) -> int: Adds two integers return x + y # Example Usage result = function_overview(add) print(result) ``` **Notes**: - You may use the `inspect` module comprehensively to achieve the desired functionality. - Ensure to handle exceptions appropriately to avoid the failure of the function. - The challenge tests both the understanding of the Python `inspect` module and typical code introspection tasks.","solution":"import inspect def function_overview(func): Returns a detailed summary of various aspects of the function. if not callable(func): raise TypeError(\'Argument must be a function\') summary = { \\"signature\\": None, \\"source_code\\": None, \\"comments\\": \\"\\", \\"file\\": None, \\"line_number\\": None, \\"attributes\\": {} } try: # Function signature signature = inspect.signature(func) summary[\\"signature\\"] = str(signature) # Function source code source = inspect.getsource(func) lines = source.split(\'n\') start_line = lines[0] # Function comments preceding_lines = inspect.getsourcelines(func)[0] for line in preceding_lines: if line.startswith(\\"#\\"): summary[\\"comments\\"] += line.strip() + \'n\' else: break summary[\\"comments\\"] = summary[\\"comments\\"].strip() summary[\\"source_code\\"] = source # File information file_info = inspect.getfile(func) line_number = inspect.getsourcelines(func)[-1] summary[\\"file\\"] = file_info summary[\\"line_number\\"] = line_number # Attributes attrs = [\\"__name__\\", \\"__doc__\\", \\"__module__\\", \\"__annotations__\\"] for attr in attrs: summary[\\"attributes\\"][attr] = getattr(func, attr, None) except TypeError: pass return summary"},{"question":"# Seaborn Boxenplot Assessment You are given a dataset about diamonds, which contains the following columns: - `carat` - `cut` (categorical variable with values `Fair`, `Good`, `Very Good`, `Premium`, `Ideal`) - `color` (categorical variable ranging from `D` (best) to `J` (worst)) - `clarity` (categorical variable with values `I1`, `SI2`, `SI1`, `VS2`, `VS1`, `VVS2`, `VVS1`, `IF`) - `depth` - `table` - `price` - `x` (length) - `y` (width) - `z` (depth) Task 1. Load the diamonds dataset from seaborn. 2. Create a boxenplot to show the distribution of diamond prices (`price`) grouped by the cut quality (`cut`). 3. Enhance the boxenplot by adding the `hue` parameter to distinguish between diamonds larger than 1 carat (`large_diamond`). 4. Customize the appearance of your plot: - Set line width for the median line to `1.5` and color to `blue`. - Set the width of the largest box to `0.4`. - Enable the unfilled boxes feature. 5. Additionally, calculate and display the mean price for each cut category on the plot as annotations. # Input - No direct input needed (the dataset is loaded from seaborn). # Output - A boxenplot as specified in the task. - Annotations depicting mean prices for each cut category. # Constraints - Assume that seaborn (`sns`), pandas (`pd`), and matplotlib (`plt`) are available in your environment. - You should handle the colors and customizations using seaborn parameters effectively. # Example The plot should have different colors for diamonds larger than 1 carat, customized line and box widths, and mean prices annotated for each cut category. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create a new column `large_diamond` to distinguish diamonds larger than 1 carat diamonds[\\"large_diamond\\"] = diamonds[\\"carat\\"] > 1 # Create the boxenplot plt.figure(figsize=(10, 8)) plot = sns.boxenplot(data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"large_diamond\\", width=0.4, fill=False, line_kws={\\"linewidth\\": 1.5, \\"color\\": \\"blue\\"}) # Calculate and annotate mean prices mean_prices = diamonds.groupby(\\"cut\\")[\\"price\\"].mean() for i, cut in enumerate(diamonds[\\"cut\\"].unique()): plt.text(i, mean_prices[cut], f\'{mean_prices[cut]:.0f}\', color=\'black\', ha=\\"center\\") plt.title(\\"Distribution of Diamond Prices by Cut Quality\\") plt.show() ``` Your task is to implement this functionality in Python.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_diamond_boxenplot(): # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create a new column `large_diamond` to distinguish diamonds larger than 1 carat diamonds[\\"large_diamond\\"] = diamonds[\\"carat\\"] > 1 # Create the boxenplot plt.figure(figsize=(10, 8)) plot = sns.boxenplot(data=diamonds, x=\\"cut\\", y=\\"price\\", hue=\\"large_diamond\\", width=0.4, fill=False, line_kws={\\"linewidth\\": 1.5, \\"color\\": \\"blue\\"}) # Calculate and annotate mean prices mean_prices = diamonds.groupby(\\"cut\\")[\\"price\\"].mean() for i, cut in enumerate(sorted(diamonds[\\"cut\\"].unique(), reverse=True)): plt.text(i, mean_prices[cut], f\'{mean_prices[cut]:.0f}\', color=\'black\', ha=\\"center\\") plt.title(\\"Distribution of Diamond Prices by Cut Quality\\") plt.show()"},{"question":"**Objective**: Implement a distributed matrix multiplication using PyTorch\'s `torch.distributed` package. This will assess your ability to set up a distributed training environment, perform collective communications, and synchronize processes. **Problem Statement**: You are required to implement a function `distributed_matrix_multiplication` that performs matrix multiplication in a distributed manner. The function should initialize the process group, distribute segments of matrices A and B across available processes, perform local computations, and then gather the results into the final matrix C. # Function Signature ```python import torch import torch.distributed as dist def distributed_matrix_multiplication(rank: int, world_size: int, backend: str, matrix_A: torch.Tensor, matrix_B: torch.Tensor) -> torch.Tensor: pass ``` # Parameters: - `rank` (int): The rank of the current process. - `world_size` (int): The total number of processes running in parallel. - `backend` (str): The backend to use for distributed communication (‘gloo’, ‘nccl’, or ‘mpi’). - `matrix_A` (torch.Tensor): The first matrix to multiply, given only to rank 0 initially. - `matrix_B` (torch.Tensor): The second matrix to multiply, given only to rank 0 initially. # Expected Output: - `return` (torch.Tensor): The resulting matrix `C` after the distributed matrix multiplication. # Constraints and Notes: - Assume the matrices A and B are square matrices and the size is divisible by the number of processes (`world_size`). - Use the appropriate collective communication functions to scatter and gather matrices among the processes. - Ensure that all processes finalize appropriately by calling the destroy function on the process group. - Rank 0 is responsible for the initialization and destruction of the process group as well as holding the final result. # Example ```python if __name__ == \\"__main__\\": import os from multiprocessing import Process matrix_A = torch.randn(4, 4) matrix_B = torch.randn(4, 4) def init_process(rank, world_size, backend, matrix_A, matrix_B, fn): os.environ[\'MASTER_ADDR\'] = \'localhost\' os.environ[\'MASTER_PORT\'] = \'29500\' fn(rank, world_size, backend, matrix_A, matrix_B) processes = [] world_size = 4 for rank in range(world_size): p = Process(target=init_process, args=(rank, world_size, \'gloo\', matrix_A, matrix_B, distributed_matrix_multiplication)) p.start() processes.append(p) for p in processes: p.join() ``` **Implementation Instructions**: 1. Initialize the process group using `dist.init_process_group`. 2. Distribute the matrices among the processes using scatter or scatter-like methods. 3. Perform the multiplication of the assigned segments locally. 4. Gather the computed segments to form the final matrix. 5. Destroy the process group using `dist.destroy_process_group`. # Solution Template Complete the following template to implement the distributed matrix multiplication. ```python import torch import torch.distributed as dist def distributed_matrix_multiplication(rank: int, world_size: int, backend: str, matrix_A: torch.Tensor, matrix_B: torch.Tensor) -> torch.Tensor: # Step 1: Initialize the process group dist.init_process_group(backend=backend, rank=rank, world_size=world_size) # Step 2: Distribute matrices # Your code here # Step 3: Local computation # Your code here # Step 4: Gather results # Your code here # Step 5: Destroy process group dist.destroy_process_group() if rank == 0: return result_matrix ``` Ensure you perform the necessary collective communications correctly, and all resources are cleaned up properly. Good luck!","solution":"import torch import torch.distributed as dist def distributed_matrix_multiplication(rank: int, world_size: int, backend: str, matrix_A: torch.Tensor = None, matrix_B: torch.Tensor = None) -> torch.Tensor: dist.init_process_group(backend=backend, rank=rank, world_size=world_size) # Assuming matrices are square and the size is divisible by the number of processes size = matrix_A.size(0) // world_size if rank == 0 else None # Create local tensors for the slices local_A = torch.zeros(size, matrix_A.size(1)) if rank != 0 else matrix_A[:size] local_B = torch.zeros(matrix_B.size(0), size) if rank != 0 else matrix_B[:, :size] local_C = torch.zeros(size, size) # Scatter matrix_A and matrix_B dist.scatter(local_A, scatter_list=[matrix_A[i*size : (i+1)*size] for i in range(world_size)] if rank == 0 else None, src=0) dist.scatter(local_B, scatter_list=[matrix_B[:, i*size : (i+1)*size] for i in range(world_size)] if rank == 0 else None, src=0) # Perform local multiplication local_C = torch.mm(local_A, local_B) # Gather the results result_matrix = None if rank == 0: result_matrix = torch.zeros(matrix_A.size()) dist.gather(local_C, gather_list=[result_matrix[i*size:(i+1)*size, i*size:(i+1)*size] for i in range(world_size)] if rank == 0 else None, dst=0) dist.destroy_process_group() return result_matrix if rank == 0 else None"},{"question":"**Advanced Coding Assessment Question:** # Byte-Code Compilation and Error Handling You are required to create a function that compiles multiple Python source files into byte-code using the `py_compile` module. Your function must perform compilation with robust error handling and logging of results. Here\'s a detailed specification: Function Signature ```python def bulk_compile(source_files: list, optimize: int = -1, invalidation_mode: str = \\"TIMESTAMP\\", quiet: int = 0) -> dict: pass ``` Parameters - `source_files` (list): A list of paths (strings) to the Python source files you want to compile. - `optimize` (int, default = -1): Optimization level to be passed to `py_compile.compile`. - `invalidation_mode` (str, default = \\"TIMESTAMP\\"): Mode of invalidation for the byte-code file; it can be \\"TIMESTAMP\\", \\"CHECKED_HASH\\", or \\"UNCHECKED_HASH\\" corresponding to the `PycInvalidationMode` values. - `quiet` (int, default = 0): Level of quietness for error messages. `0` means full error messages, `1` means fewer error messages, and `2` means no error messages. Return - A dictionary (dict) with the following structure: - **key**: Source file path (str) - **value**: - If compilation is successful, the value is the path to the byte-compiled file (str). - If an error occurs, the value is the string \\"Compilation failed\\" or \\"File does not exist\\". Behavior 1. Compile each source file in the `source_files` list. 2. Handle any `PyCompileError` exceptions that occur and log the errors. 3. If a file does not exist, log it as \\"File does not exist\\". 4. Use the specified `optimize` and `invalidation_mode` during the compilation. 5. Conform the `invalidation_mode` parameter to the `py_compile.PycInvalidationMode` enumeration. Example ```python source_files = [\\"test1.py\\", \\"test2.py\\", \\"nonexistent.py\\"] result = bulk_compile(source_files, optimize=2, invalidation_mode=\\"CHECKED_HASH\\", quiet=1) ``` Expected output could look something like: ```python { \\"test1.py\\": \\"/path/to/__pycache__/test1.cpython-310.opt-2.pyc\\", \\"test2.py\\": \\"/path/to/__pycache__/test2.cpython-310.opt-2.pyc\\", \\"nonexistent.py\\": \\"File does not exist\\" } ``` **Note:** - Ensure that your solution is efficient and handles various edge cases including invalid file paths and compilation errors. - You may assume Python 3.10 is being used and all files are in standard Python syntax.","solution":"import os import py_compile from py_compile import PycInvalidationMode def bulk_compile(source_files: list, optimize: int = -1, invalidation_mode: str = \\"TIMESTAMP\\", quiet: int = 0) -> dict: results = {} # Mapping string invalidation_mode to PycInvalidationMode invalidation_mode_mapping = { \\"TIMESTAMP\\": PycInvalidationMode.TIMESTAMP, \\"CHECKED_HASH\\": PycInvalidationMode.CHECKED_HASH, \\"UNCHECKED_HASH\\": PycInvalidationMode.UNCHECKED_HASH } invalidation_mode_enum = invalidation_mode_mapping.get(invalidation_mode, PycInvalidationMode.TIMESTAMP) for source_file in source_files: if not os.path.exists(source_file): results[source_file] = \\"File does not exist\\" continue try: compiled_path = py_compile.compile( source_file, cfile=None, dfile=None, doraise=True, optimize=optimize, invalidation_mode=invalidation_mode_enum, quiet=quiet ) results[source_file] = compiled_path except py_compile.PyCompileError: results[source_file] = \\"Compilation failed\\" return results"},{"question":"# Advanced Seaborn Categorical Plotting Challenge **Objective:** Assess your ability to use the seaborn library to create and customize categorical plots, especially using `catplot`. **Problem Statement:** You are provided with the \\"titanic\\" dataset available in seaborn. Your task is to visualize this dataset to analyze the distribution and survival rates of passengers based on various features using seaborn\'s categorical plotting functions. **Instructions:** 1. **Load the Dataset:** Load the `titanic` dataset provided by seaborn. 2. **Plot Requirements:** Create the following visualizations using `sns.catplot`: - A box plot showing the distribution of passengers\' ages across different classes. - A violin plot showing the distribution of ages across different classes, differentiated by the sex of the passengers. - A bar plot showing the survival rates across different classes, separated by sex. 3. **Further Customization:** Customize the plots based on the specifications below: - For the box plot: - Set the theme to `\\"whitegrid\\"`. - Add titles and labels to the axes. - For the violin plot: - Adjust `bw` to `.5` and `cut` to `0`. - Split the violins by sex and customize the appearance. - For the bar plot: - Use `height=4` and `aspect=.6` to control figure size. - Set y-axis limits to `[0, 1]`. - Add appropriate titles to the subplots based on class and sex. **Input and Output Formats:** - **Input:** No explicit input required; directly load the `titanic` dataset within the code. - **Output:** The output should be the three specified plots, each meeting the required customization criteria. **Constraints:** - Use only seaborn for plotting. - Ensure the plots are clearly labeled and easy to interpret. **Performance Requirements:** - The visualizations should be created efficiently, with attention to detail and aesthetics. **Example Solution:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the titanic dataset df = sns.load_dataset(\\"titanic\\") # 1. Box plot: Distribution of passengers\' ages across different classes sns.set_theme(style=\\"whitegrid\\") box_plot = sns.catplot(data=df, x=\\"age\\", y=\\"class\\", kind=\\"box\\") box_plot.set_axis_labels(\\"Age\\", \\"Class\\") box_plot.set_titles(\\"Distribution of Ages by Class\\") plt.show() # 2. Violin plot: Distribution of ages across different classes, differentiated by sex violin_plot = sns.catplot(data=df, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", kind=\\"violin\\", bw=.5, cut=0, split=True) violin_plot.set_axis_labels(\\"Age\\", \\"Class\\") violin_plot.set_titles(\\"Age Distribution by Class and Sex\\") plt.show() # 3. Bar plot: Survival rates across different classes, separated by sex bar_plot = sns.catplot(data=df, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", kind=\\"bar\\", height=4, aspect=.6) bar_plot.set_axis_labels(\\"Class\\", \\"Survival Rate\\") bar_plot.set_titles(\\"{col_name} {col_var}\\") bar_plot.set(ylim=(0, 1)) bar_plot.despine(left=True) plt.show() ``` You should include the code to address all the plotting requirements as described. Ensure each plot meets the customization specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_plots(): # Load the titanic dataset df = sns.load_dataset(\\"titanic\\") # 1. Box plot: Distribution of passengers\' ages across different classes sns.set_theme(style=\\"whitegrid\\") box_plot = sns.catplot(data=df, x=\\"age\\", y=\\"class\\", kind=\\"box\\") box_plot.set_axis_labels(\\"Age\\", \\"Class\\") box_plot.set_titles(\\"Distribution of Ages by Class\\") plt.show() # 2. Violin plot: Distribution of ages across different classes, differentiated by sex violin_plot = sns.catplot(data=df, x=\\"age\\", y=\\"class\\", hue=\\"sex\\", kind=\\"violin\\", bw=.5, cut=0, split=True) violin_plot.set_axis_labels(\\"Age\\", \\"Class\\") violin_plot.set_titles(\\"Age Distribution by Class and Sex\\") plt.show() # 3. Bar plot: Survival rates across different classes, separated by sex bar_plot = sns.catplot(data=df, x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", kind=\\"bar\\", height=4, aspect=.6) bar_plot.set_axis_labels(\\"Class\\", \\"Survival Rate\\") bar_plot.set_titles(\\"{col_name} {col_var}\\") bar_plot.set(ylim=(0, 1)) bar_plot.despine(left=True) plt.show()"},{"question":"# Question: You are given a dataset containing movie ratings. Your task is to visualize the distribution and comparison of these ratings among different genres using seaborn. The dataset has the following columns: - `title`: The title of the movie. - `genre`: The genre of the movie (e.g., Drama, Comedy, Action). - `rating`: The rating of the movie (on a scale of 1 to 10). - `votes`: The number of votes each movie received. Write a function `visualize_movie_ratings(input_file)` that: 1. Loads the dataset from the given `input_file` (a CSV file). 2. Creates a grid of violin plots separating each genre using the `col` parameter, where the x-axis represents the `rating` and the y-axis represents the `genre`. 3. Enhances the violin plots with swarm plots to show the individual ratings. 4. Adjusts the visualization to ensure clarity by setting appropriate axis labels, titles, and limits. Your function should: - Save the plot as `movie_ratings.png` in the current directory. - Be clear and well-labeled, making it easy to understand the comparison between genres. **Constraints:** - Assume the dataset fits into memory. - Use seaborn for all visualizations. **Example Usage:** ```python visualize_movie_ratings(\\"movie_ratings.csv\\") ``` **Input Format:** - `input_file`: A string representing the path to a CSV file containing the movie ratings dataset. **Output:** - The function should save the plot as `movie_ratings.png`. # Note: You can assume the CSV has the exact columns as described, and there are no missing or malformed values.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def visualize_movie_ratings(input_file): Loads the movie ratings dataset and creates a grid of violin plots to compare ratings among different genres. The plot is saved as \'movie_ratings.png\' in the current directory. :param input_file: str, path to the input CSV file containing the movie ratings dataset. # Load the dataset df = pd.read_csv(input_file) # Setting the aesthetics for the plots sns.set(style=\\"whitegrid\\") # Creating a grid of violin plots separated by genre g = sns.catplot(x=\\"rating\\", y=\\"genre\\", kind=\\"violin\\", data=df, aspect=2, height=8, inner=None) # Add swarm plot on top of the violins to show individual ratings sns.swarmplot(x=\\"rating\\", y=\\"genre\\", data=df, color=\\"k\\", size=2, ax=g.ax) # Setting labels and title g.set_axis_labels(\\"Rating\\", \\"Genre\\") g.fig.suptitle(\\"Distribution of Movie Ratings by Genre\\", y=1.02) # Save the plot plt.savefig(\\"movie_ratings.png\\") # Show the plot for interactive environments (comment out if only saving is needed) # plt.show()"},{"question":"# XML Processing Using `xml.etree.ElementTree` **Objective:** Implement a function to process XML data using Python\'s `xml.etree.ElementTree` module. The function should parse an XML string, perform specific manipulations, and return the modified XML as a string. **Function Signature:** ```python def process_xml(xml_str: str) -> str: pass ``` **Input:** - `xml_str` (str): A string representing XML data. This XML contains a root element, multiple child elements, and various attributes. **Output:** - (str): A string representing the modified XML data. **Constraints:** 1. The input XML will always have the same structure: ```xml <data> <item id=\\"1\\" color=\\"red\\">Apple</item> <item id=\\"2\\" color=\\"yellow\\">Banana</item> <item id=\\"3\\" color=\\"green\\">Pear</item> </data> ``` **Requirements:** 1. Parse the XML string using `xml.etree.ElementTree`. 2. Add a new attribute `price` with the following values: - `Apple`: 0.50 - `Banana`: 0.30 - `Pear`: 0.40 3. Change the `color` attribute for the following items: - `Banana` to `green` 4. Remove any `item` element with a color `red`. 5. Return the modified XML string. **Examples:** ```python xml_input = \'\'\'<data> <item id=\\"1\\" color=\\"red\\">Apple</item> <item id=\\"2\\" color=\\"yellow\\">Banana</item> <item id=\\"3\\" color=\\"green\\">Pear</item> </data>\'\'\' expected_output = \'\'\'<data> <item id=\\"2\\" color=\\"green\\" price=\\"0.30\\">Banana</item> <item id=\\"3\\" color=\\"green\\" price=\\"0.40\\">Pear</item> </data>\'\'\' assert process_xml(xml_input) == expected_output ``` **Notes:** - You may find the `find`, `findall`, `Element`, and `SubElement` methods from `xml.etree.ElementTree` useful. - Care should be taken to ensure the output XML string maintains proper formatting.","solution":"import xml.etree.ElementTree as ET def process_xml(xml_str: str) -> str: Processes the input XML string to modify attributes and elements as specified. price_map = { \\"Apple\\": \\"0.50\\", \\"Banana\\": \\"0.30\\", \\"Pear\\": \\"0.40\\" } tree = ET.ElementTree(ET.fromstring(xml_str)) root = tree.getroot() for item in root.findall(\'item\'): # Get the fruit name fruit = item.text # Add the price attribute if fruit in price_map: item.set(\\"price\\", price_map[fruit]) # Modify the color of Banana if fruit == \\"Banana\\": item.set(\\"color\\", \\"green\\") # Remove item with color red if item.get(\\"color\\") == \\"red\\": root.remove(item) return ET.tostring(root, encoding=\'unicode\')"},{"question":"You are given a dataset of penguin species and their physical characteristics. Your task is to create suitable visualizations using Seaborn to better understand the dataset. Specifically, you need to: 1. Create a scatter plot showing the relationship between bill length and bill depth, using different colors for different species. 2. Create a heatmap representing the pairwise correlation of numerical features in the dataset. 3. Create a violin plot to visualize the distribution of flipper length for different species. For each of these visualizations, you need to select and implement an appropriate color palette using Seaborn, based on the principles discussed in the documentation. # Dataset Use the `penguins` dataset provided by Seaborn: ```python import seaborn as sns penguins = sns.load_dataset(\\"penguins\\") ``` # Requirements 1. **Scatter Plot**: - X-axis: `bill_length_mm` - Y-axis: `bill_depth_mm` - Color: Represent different species using a qualitative color palette. 2. **Heatmap**: - Data: Pairwise correlation of numerical features (`bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, `body_mass_g`) - Use a sequential color palette to represent the correlation coefficients. 3. **Violin Plot**: - X-axis: `species` - Y-axis: `flipper_length_mm` - Color: Use a diverging color palette to differentiate species. # Constraints - Use Seaborn and Matplotlib libraries for visualization. - Choose color palettes that make the plots aesthetically pleasing and informative. - Follow the principles discussed in the Seaborn documentation for choosing color palettes. # Performance Requirements - The visualizations should be clear and easy to understand. - Color choices should effectively highlight differences in the data. # Example Solution ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Scatter Plot plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=\\"deep\\" # Qualitative palette ) scatter.set_title(\\"Scatter Plot of Bill Length vs Bill Depth\\") plt.show() # 2. Heatmap plt.figure(figsize=(10, 6)) correlation = penguins.corr() heatmap = sns.heatmap( correlation, annot=True, cmap=\\"rocket\\" # Sequential palette ) heatmap.set_title(\\"Heatmap of Feature Correlation\\") plt.show() # 3. Violin Plot plt.figure(figsize=(10, 6)) violin = sns.violinplot( data=penguins, x=\\"species\\", y=\\"flipper_length_mm\\", palette=\\"coolwarm\\" # Diverging palette ) violin.set_title(\\"Violin Plot of Flipper Length by Species\\") plt.show() ``` # Submission Submit your code, along with the visualizations it produces. Make sure your color choices are justified based on the principles discussed in the Seaborn documentation.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguin_visualizations(): # Load dataset penguins = sns.load_dataset(\\"penguins\\") # 1. Scatter Plot plt.figure(figsize=(10, 6)) scatter = sns.scatterplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", palette=\\"deep\\" # Qualitative palette ) scatter.set_title(\\"Scatter Plot of Bill Length vs Bill Depth\\") plt.show() # 2. Heatmap plt.figure(figsize=(10, 6)) numerical_features = [\'bill_length_mm\', \'bill_depth_mm\', \'flipper_length_mm\', \'body_mass_g\'] correlation = penguins[numerical_features].corr() heatmap = sns.heatmap( correlation, annot=True, cmap=\\"rocket\\" # Sequential palette ) heatmap.set_title(\\"Heatmap of Feature Correlation\\") plt.show() # 3. Violin Plot plt.figure(figsize=(10, 6)) violin = sns.violinplot( data=penguins, x=\\"species\\", y=\\"flipper_length_mm\\", palette=\\"coolwarm\\" # Diverging palette ) violin.set_title(\\"Violin Plot of Flipper Length by Species\\") plt.show()"},{"question":"Objective: Demonstrate proficiency in Python 3.10 compound statements, including control flow, exception handling, context management, asynchronous operations, and function/class definitions. Problem Statement: You are required to write a Python program that simulates the process of handling multiple customer orders in an e-commerce system. The system should: 1. **Process Orders:** Use a context manager to handle the reading/writing of orders from/to a file. 2. **Handle Errors:** Use exception handling to manage potential errors during file operations, such as `FileNotFoundError`. 3. **Async Operations:** Use asynchronous functions to simulate network delay when retrieving customer information. 4. **Control Flow:** Use `if`, `for`, and `while` statements to process the orders and manage customer data. 5. **Organize Code:** Define appropriate functions and classes to encapsulate the logic. Requirements: 1. Define a class `OrderManager` with the following methods: - `__init__(self, order_file)`: Initializes the `OrderManager` with the path to the order file. - `__enter__(self)`: Opens the order file for reading and returns a file handle. - `__exit__(self, exc_type, exc_val, exc_tb)`: Closes the order file and handles exceptions. - `process_orders(self)`: Processes each order from the file. - `save_summary(self, summary_file)`: Saves a summary of processed orders to another file. 2. Define an async function `retrieve_customer_info(customer_id)` that simulates retrieving customer information with a network delay using `await asyncio.sleep(1)`. 3. The `process_orders` function should: - Retrieve each `customer_id` and `order_details` from the order file. - Use `retrieve_customer_info` to get customer info asynchronously. - If the `customer_id` is valid, process the order and collect the summary. - If the `customer_id` is invalid or `retrieve_customer_info` raises an exception, log the error and skip the order. 4. Use appropriate control flow statements (`if`, `while`, `for`) to traverse the orders and manage the retrieval of customer information. 5. Use `try`, `except`, `else`, and `finally` blocks to handle potential errors during file operations and order processing. Input and Output Formats: **Input:** - `order_file`: A text file where each line contains a `customer_id` and `order_details` separated by a comma. - `summary_file`: A text file where the summary of processed orders will be saved. **Output:** - Write a summary of processed orders to the `summary_file`. - Log any errors encountered during order processing. Constraints: - The `order_file` may not exist or be unreadable. - Each `customer_id` must be a positive integer. - Assume `retrieve_customer_info` may fail sporadically. Example: The order file `orders.txt` content: ``` 1,Order1 Details 2,Order2 Details invalid_id,Order3 Details 3,Order4 Details ``` The program processes the orders, handles exceptions, and writes a summary to `summary.txt` with valid orders. Errors are logged appropriately. ```python import asyncio class OrderManager: def __init__(self, order_file): # Initialize OrderManager pass def __enter__(self): # Enter context manager pass def __exit__(self, exc_type, exc_val, exc_tb): # Exit context manager pass def process_orders(self): # Process orders pass def save_summary(self, summary_file): # Save summary to file pass async def retrieve_customer_info(customer_id): # Simulate network delay pass # Example usage order_manager = OrderManager(\'orders.txt\') with order_manager: order_manager.process_orders() order_manager.save_summary(\'summary.txt\') # Ensure to run the async function properly using asyncio.run or similar approach ``` Fill in the appropriate logic within the methods and functions, adhering to the requirements.","solution":"import asyncio import logging logging.basicConfig(level=logging.ERROR) class OrderManager: def __init__(self, order_file): self.order_file = order_file self.orders = [] self.summary = [] def __enter__(self): try: self.file = open(self.order_file, \'r\') except FileNotFoundError: logging.error(f\\"File {self.order_file} not found.\\") self.file = None return self def __exit__(self, exc_type, exc_val, exc_tb): if self.file: self.file.close() async def retrieve_customer_info(self, customer_id): try: await asyncio.sleep(1) # Simulate network delay if isinstance(customer_id, int) and customer_id > 0: return {\\"customer_id\\": customer_id, \\"info\\": f\\"CustomerInfo{customer_id}\\"} else: raise ValueError(f\\"Invalid customer_id: {customer_id}\\") except Exception as e: logging.error(f\\"Error retrieving customer info: {e}\\") return None def process_orders(self): if not self.file: return for line in self.file: try: customer_id_str, order_details = line.strip().split(\',\') customer_id = int(customer_id_str) except ValueError: logging.error(f\\"Invalid line format: {line.strip()}\\") continue customer_info = asyncio.run(self.retrieve_customer_info(customer_id)) if customer_info: self.summary.append((customer_id, order_details, customer_info)) def save_summary(self, summary_file): try: with open(summary_file, \'w\') as file: for order in self.summary: file.write(f\\"{order[0]},{order[1]},{order[2][\'info\']}n\\") except Exception as e: logging.error(f\\"Error writing summary to file: {e}\\") # Example usage order_manager = OrderManager(\'orders.txt\') with order_manager: order_manager.process_orders() order_manager.save_summary(\'summary.txt\')"},{"question":"Objective You are required to implement a function that connects to a Telnet server, sends a series of commands, and processes the response. This will demonstrate your understanding of connection management, data handling, and context management in the `telnetlib` library. Question Implement a function `execute_telnet_commands(host, port, commands, timeout=5)` that connects to a Telnet server, executes a series of commands, reads responses, and returns the combined output of these commands. Utilize the `telnetlib` library for this task. Requirements 1. The function should take the following parameters: - `host`: A string representing the hostname or IP address of the Telnet server. - `port`: An integer representing the port number of the Telnet server. - `commands`: A list of strings, where each string is a command to be executed on the Telnet server. - `timeout`: An optional parameter defining the timeout for blocking operations (default is 5 seconds). 2. The function should: - Establish a connection to the Telnet server using the provided host and port. - Execute each command in the `commands` list, ensuring proper encoding. - Read the response after each command and store it. - Return the combined response of all commands as a single string. 3. Ensure to handle exceptions such as connection errors and EOF errors, and include appropriate error messages. Constraints - Assume all commands expect a response that ends with a newline character (`n`). - You may use `read_until` method to read the response of each command. ```python def execute_telnet_commands(host, port, commands, timeout=5): import telnetlib # Your implementation here # Example usage host = \\"localhost\\" port = 23 commands = [\\"ls\\", \\"pwd\\"] print(execute_telnet_commands(host, port, commands)) ``` Example ```python # Sample Output execute_telnet_commands(\\"localhost\\", 23, [\\"ls\\", \\"pwd\\"]) # Output: # \'file1.txtnfile2.txtn/home/usern\' ``` Please Note: The specific host, port, and commands used in your test cases will depend on the Telnet server configuration in your testing environment.","solution":"def execute_telnet_commands(host, port, commands, timeout=5): import telnetlib try: # Establish the connection to the Telnet server with telnetlib.Telnet(host, port, timeout) as tn: responses = [] for command in commands: tn.write(command.encode(\'ascii\') + b\'n\') response = tn.read_until(b\'n\', timeout).decode(\'ascii\') responses.append(response) return \'\'.join(responses) except (ConnectionRefusedError, EOFError) as e: return f\\"Error: {e}\\" # Example usage # host = \\"localhost\\" # port = 23 # commands = [\\"ls\\", \\"pwd\\"] # print(execute_telnet_commands(host, port, commands))"},{"question":"Objective Implement a function that reads a file, compresses its contents using the `bz2` algorithm, and then decompresses it back to verify that the contents match the original. This task will test your understanding of file handling, compression, and decompression in Python. Function Signature ```python def compress_and_verify(filename: str) -> bool: pass ``` Description 1. **Compressing the File:** - Read the content of the file specified by the `filename` parameter. - Compress the content using the `bz2` algorithm. - Store the compressed data in a variable. 2. **Decompressing the Data:** - Decompress the compressed data back to its original form. - Compare the decompressed data with the original file content. 3. **Return Value:** - Return `True` if the decompressed data matches the original content, otherwise return `False`. Input - `filename` (str): The name of the file to be compressed and decompressed. Output - `bool`: `True` if the decompressed data matches the original file content, else `False`. Example Assume `example.txt` has the content: ``` Hello, World! This is a test file for compression. Enjoy! ``` Calling `compress_and_verify(\'example.txt\')` should return `True` since the process of compression and decompression should preserve the original content. Constraints - You can assume that the file specified by `filename` exists and is readable. - Handle binary and text files appropriately. - The file size will not exceed 10MB. Performance - Optimize the file I/O operations to handle the file content efficiently. Here\'s a template to get you started: ```python import bz2 def compress_and_verify(filename: str) -> bool: with open(filename, \'rb\') as file: original_content = file.read() compressed_content = bz2.compress(original_content) decompressed_content = bz2.decompress(compressed_content) return original_content == decompressed_content ```","solution":"import bz2 def compress_and_verify(filename: str) -> bool: Read, compress, and decompress a file to verify the content matches the original. Parameters: filename (str): The name of the file to be compressed and decompressed. Returns: bool: True if the decompressed data matches the original file content, else False. with open(filename, \'rb\') as file: original_content = file.read() compressed_content = bz2.compress(original_content) decompressed_content = bz2.decompress(compressed_content) return original_content == decompressed_content"},{"question":"Coding Assessment Question # Objective The goal of this question is to assess your understanding of GPU memory management in PyTorch using the `torch.cuda` module. You are required to implement a function that queries memory usage statistics of a specified CUDA device, allocates a specified amount of memory, then resets memory statistics and reports the memory usage statistics before and after allocation. # Problem Statement You are required to implement the following function: ```python import torch def manage_gpu_memory(device_id: int, memory_allocation: int): Manages GPU memory by querying memory usage statistics, allocating specified memory, and resetting memory statistics. Reports memory stats before and after allocation. Args: - device_id (int): The ID of the CUDA device to manage. - memory_allocation (int): The amount of memory (in bytes) to allocate. Returns: - before_allocation (dict): Memory statistics before allocation. - after_allocation (dict): Memory statistics after allocation and resetting stats. pass ``` # Input - `device_id` (int): The ID of the CUDA device to manage. - `memory_allocation` (int): The amount of memory (in bytes) to allocate. # Output - `before_allocation` (dict): Memory statistics before allocation. This dictionary should contain: - `allocated_memory`: Memory allocated (in bytes). - `reserved_memory`: Memory reserved (in bytes). - `cached_memory`: Memory cached (in bytes). - `after_allocation` (dict): Memory statistics after allocation and resetting stats. This dictionary should contain: - `allocated_memory`: Memory allocated (in bytes). - `reserved_memory`: Memory reserved (in bytes). - `cached_memory`: Memory cached (in bytes). # Constraints - You should use PyTorch\'s `torch.cuda` module for querying and managing GPU memory. - Ensure proper error handling for cases where the specified `device_id` is invalid. - Handle any memory allocation errors gracefully. # Example ```python device_id = 0 memory_allocation = 1024 * 1024 * 100 # 100 MB before_allocation, after_allocation = manage_gpu_memory(device_id, memory_allocation) print(\\"Before Allocation:\\", before_allocation) print(\\"After Allocation:\\", after_allocation) ``` # Notes - Use `torch.cuda.memory_allocated` to query the amount of allocated memory. - Use `torch.cuda.memory_reserved` to query the amount of reserved memory. - Use `torch.cuda.memory_cached` to query the amount of cached memory. - Use `torch.cuda.reset_peak_memory_stats` to reset peak memory statistics. - Use `torch.cuda.empty_cache` to release all unoccupied cached memory currently held by the caching allocator. - Memory allocation can be done using a simple tensor allocation like `torch.empty` with appropriate size on the CUDA device. # Hints - The memory allocation can be done by creating a large tensor. - You can convert tensor sizes into bytes for memory allocation by multiplying the number of elements by the tensor\'s element size in bytes. - Remember to synchronize the device before and after allocation to ensure memory statistics are up-to-date. Good luck with your implementation!","solution":"import torch def manage_gpu_memory(device_id: int, memory_allocation: int): Manages GPU memory by querying memory usage statistics, allocating specified memory, and resetting memory statistics. Reports memory stats before and after allocation. Args: - device_id (int): The ID of the CUDA device to manage. - memory_allocation (int): The amount of memory (in bytes) to allocate. Returns: - before_allocation (dict): Memory statistics before allocation. - after_allocation (dict): Memory statistics after allocation and resetting stats. if not torch.cuda.is_available(): raise ValueError(\\"CUDA is not available\\") if device_id < 0 or device_id >= torch.cuda.device_count(): raise ValueError(\\"Invalid device ID\\") device = torch.device(f\'cuda:{device_id}\') torch.cuda.set_device(device) # Get memory statistics before allocation torch.cuda.synchronize(device) before_allocation = { \'allocated_memory\': torch.cuda.memory_allocated(device), \'reserved_memory\': torch.cuda.memory_reserved(device), \'cached_memory\': torch.cuda.memory_cached(device), } # Allocate memory tensor = torch.empty(memory_allocation, dtype=torch.int8, device=device) torch.cuda.synchronize(device) # Reset memory statistics torch.cuda.reset_peak_memory_stats(device) torch.cuda.empty_cache() torch.cuda.synchronize(device) # Get memory statistics after allocation and resetting stats after_allocation = { \'allocated_memory\': torch.cuda.memory_allocated(device), \'reserved_memory\': torch.cuda.memory_reserved(device), \'cached_memory\': torch.cuda.memory_cached(device), } return before_allocation, after_allocation"},{"question":"Question In this task, you are required to demonstrate your understanding of handling BatchNorm within PyTorch when working with `vmap` from functorch. Specifically, you will implement a function that modifies a given neural network model to make it compatible with `vmap` by removing the use of running statistics in BatchNorm layers. # Function Signature ```python def make_vmap_compatible(model: torch.nn.Module) -> torch.nn.Module: Modifies the provided model to make it compatible with functorch vmap by updating BatchNorm layers to not use running statistics. Parameters: model (torch.nn.Module): A PyTorch neural network model which may contain BatchNorm layers. Returns: torch.nn.Module: The modified model with BatchNorm layers updated to not use running statistics. ``` # Requirements 1. **Input:** - The function accepts a single input - a PyTorch `nn.Module` representing the neural network model that may contain several BatchNorm layers. 2. **Output:** - The function returns a modified version of the input model where any `BatchNorm2d` layers have been updated to not use running statistics. 3. **Constraints:** - The function should work correctly regardless of the depth and types of layers in the neural network model. 4. **Considerations:** - Use functorch\'s `replace_all_batch_norm_modules_` to modify the model in-place. 5. **Performance:** - The solution should be efficient in both memory and computation. The modified model\'s weights other than running statistics should remain unchanged. # Example ```python import torch import torch.nn as nn from torch.func import replace_all_batch_norm_modules_ # Define a simple neural network model with BatchNorm layers class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1) self.bn1 = nn.BatchNorm2d(16) self.relu = nn.ReLU() self.fc1 = nn.Linear(16 * 32 * 32, 10) self.bn2 = nn.BatchNorm1d(10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = x.view(x.size(0), -1) x = self.fc1(x) x = self.bn2(x) return x # Instantiate the model model = SimpleModel() # Modify the model to be vmap compatible vmap_compatible_model = make_vmap_compatible(model) # Check if BatchNorm layers have been updated appropriately for module in vmap_compatible_model.modules(): if isinstance(module, nn.BatchNorm2d) or isinstance(module, nn.BatchNorm1d): assert not module.track_running_stats, \\"BatchNorm layer should not be tracking running stats.\\" print(\\"All BatchNorm layers are vmap compatible.\\") ``` # Hint You can directly utilize functorch\'s function `replace_all_batch_norm_modules_` to achieve the desired modifications in the model.","solution":"import torch import torch.nn as nn from torch.func import replace_all_batch_norm_modules_ def make_vmap_compatible(model: nn.Module) -> nn.Module: Modifies the provided model to make it compatible with functorch vmap by updating BatchNorm layers to not use running statistics. Parameters: model (torch.nn.Module): A PyTorch neural network model which may contain BatchNorm layers. Returns: torch.nn.Module: The modified model with BatchNorm layers updated to not use running statistics. def _no_running_stats_batch_norm(module): if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)): module.track_running_stats = False for child in module.children(): _no_running_stats_batch_norm(child) _no_running_stats_batch_norm(model) return model"},{"question":"# Command-Line Tool for File Processing **Objective:** Create a command-line tool, `file_processor.py`, that performs different operations on an input file based on the specified command-line options. Your task is to implement this tool using the `optparse` module in Python. **Specifications:** 1. **Define Options:** - `--input (-i)` with `dest=\\"input_file\\"`, `type=\\"string\\"`, and `help=\\"Input file to process\\"`. This option is required. - `--output (-o)` with `dest=\\"output_file\\"`, `type=\\"string\\"`, and `help=\\"Output file to write results\\"`. This option is optional. - `--reverse (-r)` with `action=\\"store_true\\"`, `dest=\\"reverse\\"`, and `help=\\"Reverse the contents of the input file\\"`. - `--uppercase (-u)` with `action=\\"store_true\\"`, `dest=\\"uppercase\\"`, and `help=\\"Convert the contents of the input file to uppercase\\"`. - `--replace (-p)` with `type=\\"string\\"`, `dest=\\"replace\\"`, `nargs=2`, and `help=\\"Replace occurrences of a string with another string in the input file\\"`. 2. **Process File:** - If the `--reverse` option is specified, the program should reverse the contents of the input file. - If the `--uppercase` option is specified, the program should convert the contents of the input file to uppercase. - If the `--replace` option is specified, it should replace occurrences of the first specified string with the second one in the input file. 3. **Handle Options:** - The order of options matters. Apply the specified transformations in the order they are given. - If no output file is specified, print the results to the standard output. If an output file is specified, write the results to that file. 4. **Generate Help and Error Messages:** - The tool should provide usage help when `-h` or `--help` is passed. - Handle and display appropriate error messages for invalid or conflicting options. **Constraints:** - You must use the `optparse` module. - The input file must exist; otherwise, the program should exit with an error message. - Assume all input and output files contain text data. **Example Usage:** ```sh python file_processor.py -i input.txt -r -u -o output.txt ``` **Expected Behavior:** - Reads `input.txt`, reverses its contents, converts it to uppercase, and writes the result to `output.txt`. ```sh python file_processor.py --input=input.txt --replace=foo bar --output=output.txt ``` **Expected Behavior:** - Reads `input.txt`, replaces all occurrences of `foo` with `bar`, and writes the result to `output.txt`. **Starter Code:** ```python from optparse import OptionParser def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\") # Add your options here parser.add_option(...) (options, args) = parser.parse_args() if not options.input_file: parser.error(\\"Input file is required.\\") try: with open(options.input_file, \'r\') as file: content = file.read() except FileNotFoundError: parser.error(\\"Input file does not exist.\\") # Process the content based on options if options.reverse: ... if options.uppercase: ... if options.replace: ... # Handle output if options.output_file: with open(options.output_file, \'w\') as file: file.write(content) else: print(content) if __name__ == \\"__main__\\": main() ``` **Requirements:** - You need to fill in the parts of the code marked with \\"...\\". - Make sure to handle all specified options appropriately. **Note:** - You may assume the input file contains a series of textual lines. - You must handle cases where options might conflict (e.g., reversing the text before and after another transformation).","solution":"from optparse import OptionParser def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\") parser.add_option(\\"-i\\", \\"--input\\", dest=\\"input_file\\", type=\\"string\\", help=\\"Input file to process\\", metavar=\\"FILE\\") parser.add_option(\\"-o\\", \\"--output\\", dest=\\"output_file\\", type=\\"string\\", help=\\"Output file to write results\\", metavar=\\"FILE\\") parser.add_option(\\"-r\\", \\"--reverse\\", action=\\"store_true\\", dest=\\"reverse\\", help=\\"Reverse the contents of the input file\\") parser.add_option(\\"-u\\", \\"--uppercase\\", action=\\"store_true\\", dest=\\"uppercase\\", help=\\"Convert the contents of the input file to uppercase\\") parser.add_option(\\"-p\\", \\"--replace\\", dest=\\"replace\\", type=\\"string\\", nargs=2, help=\\"Replace occurrences of the first string with the second string\\") (options, args) = parser.parse_args() if not options.input_file: parser.error(\\"Input file is required.\\") try: with open(options.input_file, \'r\') as file: content = file.read() except FileNotFoundError: parser.error(\\"Input file does not exist.\\") if options.replace: old_str, new_str = options.replace content = content.replace(old_str, new_str) if options.uppercase: content = content.upper() if options.reverse: content = content[::-1] if options.output_file: with open(options.output_file, \'w\') as file: file.write(content) else: print(content) if __name__ == \\"__main__\\": main()"},{"question":"**Machine Learning Visualization Task** **Objective**: You need to demonstrate your understanding of Scikit-learn\'s Plotting API by implementing a custom display object for visualizing a Precision-Recall curve. **Requirements**: 1. Implement a class `PrecisionRecallDisplay` which: - Initializes with precision, recall, and optionally an average precision score. - Implements a `plot` method to create the visualization. - Implements class methods `from_estimator` and `from_predictions`. 2. Your `plot` method should support plotting on: - A single axis. - Multiple axes using matplotlib\'s gridspec. 3. Ensure options for customization of the plotted visuals (e.g., line style, legend). **Details**: 1. **Inputs**: - `from_estimator`: Takes an estimator, feature matrix `X`, and true labels `y`. - `from_predictions`: Takes true labels `y` and predicted scores `y_pred`. 2. **Outputs**: - A `PrecisionRecallDisplay` object with the respective precision-recall curve plotted. 3. **Constraints and Performance**: - Ensure your implementation can handle large datasets efficiently. - The plotting method should manage multiple subplots correctly. ```python import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score from matplotlib.gridspec import GridSpecFromSubplotSpec class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision=None, estimator_name=None): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y): y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator.__class__.__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y, y_pred) avg_prec = average_precision_score(y, y_pred) viz = PrecisionRecallDisplay(precision, recall, avg_prec, estimator_name) return viz.plot() def plot(self, ax=None, **kwargs): if ax is None: fig, ax = plt.subplots() ax.plot(self.recall, self.precision, label=f\\"Precision-Recall curve (area = {self.average_precision:0.2f})\\") ax.set_xlabel(\\"Recall\\") ax.set_ylabel(\\"Precision\\") ax.set_title(f\\"Precision-Recall curve for {self.estimator_name}\\") ax.legend(loc=\\"best\\") self.ax_ = ax self.figure_ = ax.figure return self # Example usage: # Assuming `estimator`, `X`, and `y` are predefined # pr_display = PrecisionRecallDisplay.from_estimator(estimator, X, y) # plt.show() ``` **Note**: You should write tests for your implementation to ensure its correctness.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from matplotlib.gridspec import GridSpecFromSubplotSpec class PrecisionRecallDisplay: def __init__(self, precision, recall, average_precision=None, estimator_name=None): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name @classmethod def from_estimator(cls, estimator, X, y, pos_label=1, **kwargs): y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator.__class__.__name__) @classmethod def from_predictions(cls, y, y_pred, estimator_name=\'\', **kwargs): precision, recall, _ = precision_recall_curve(y, y_pred) avg_prec = average_precision_score(y, y_pred) viz = PrecisionRecallDisplay(precision, recall, avg_prec, estimator_name) return viz.plot() def plot(self, ax=None, **kwargs): if ax is None: fig, ax = plt.subplots() ax.plot(self.recall, self.precision, label=f\\"Precision-Recall curve (area = {self.average_precision:0.2f})\\", **kwargs) ax.set_xlabel(\\"Recall\\") ax.set_ylabel(\\"Precision\\") ax.set_title(f\\"Precision-Recall curve for {self.estimator_name}\\") ax.legend(loc=\\"best\\") self.ax_ = ax self.figure_ = ax.figure return self # Example usage: # X, y = make_classification(n_samples=1000, n_classes=2, random_state=0) # estimator = LogisticRegression() # estimator.fit(X, y) # pr_display = PrecisionRecallDisplay.from_estimator(estimator, X, y) # plt.show()"},{"question":"Objective Demonstrate your understanding of pandas\' sparse data structures by performing a series of tasks involving the creation, conversion, and manipulation of sparse data. Tasks 1. **Create a Sparse DataFrame:** - Generate a 10x5 DataFrame of random floats. - Set values in the first 8 rows to `NaN`. - Convert this DataFrame to a sparse DataFrame with a fill value of `NaN`. - Print the first 5 rows of the sparse DataFrame and its memory usage. 2. **Manipulate Sparse Data:** - Apply a numpy `ufunc` (e.g., `np.square`) to the sparse DataFrame, converting the fill values as well. - Print the resulting Sparse DataFrame and its memory usage. 3. **Convert Data Representations:** - Convert the sparse DataFrame back to a dense DataFrame. - Convert a dense DataFrame with values `[1, 0, 0, 1]` to a sparse DataFrame with fill value 0. - Print both the dense and the resulting sparse DataFrame. 4. **Scipy Integration:** - Create a sparse matrix using `scipy.sparse.csr_matrix` with 1000x5 dimensions, where 90% of the values are 0. - Convert this scipy sparse matrix to a pandas Sparse DataFrame. - Convert the Sparse DataFrame back to a scipy sparse matrix and print its dense representation. Input and Output Formats - **Input:** No external input required; use generated or hardcoded data. - **Output:** Print statements showing the interim and final results. Constraints - Use the pandas package for all sparse and dense DataFrame manipulations. - Use numpy for generating and applying the ufunc. - Use scipy for handling the sparse matrix operations. Notes - Ensure that your code is optimized for both performance and memory usage. - Include comments to explain each step. - Keep an eye on maintaining code readability and efficiency. ```python import numpy as np import pandas as pd from scipy.sparse import csr_matrix # Task 1: Create a Sparse DataFrame df = pd.DataFrame(np.random.randn(10, 5)) df.iloc[:8] = np.nan sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) print(\\"Sparse DataFrame (first 5 rows):n\\", sdf.head()) print(\\"Memory usage:\\", sdf.memory_usage().sum()) # Task 2: Manipulate Sparse Data sdf_squared = np.square(sdf) print(\\"Squared Sparse DataFrame:n\\", sdf_squared) print(\\"Memory usage after squaring:\\", sdf_squared.memory_usage().sum()) # Task 3: Convert Data Representations dense_df = sdf.sparse.to_dense() print(\\"Dense DataFrame:n\\", dense_df) dense_df2 = pd.DataFrame({\\"A\\": [1, 0, 0, 1]}) sparse_dtype = pd.SparseDtype(int, fill_value=0) sparse_df2 = dense_df2.astype(sparse_dtype) print(\\"From Dense to Sparse DataFrame:n\\", sparse_df2) # Task 4: Scipy Integration arr = np.random.random(size=(1000, 5)) arr[arr < 0.9] = 0 sp_arr = csr_matrix(arr) sparse_df3 = pd.DataFrame.sparse.from_spmatrix(sp_arr) print(\\"Sparse DataFrame from scipy matrix (first 5 rows):n\\", sparse_df3.head()) coo_matrix = sparse_df3.sparse.to_coo() print(\\"Dense representation of converted scipy sparse matrix:n\\", coo_matrix.todense()) ```","solution":"import numpy as np import pandas as pd from scipy.sparse import csr_matrix # Task 1: Create a Sparse DataFrame def create_sparse_df(): df = pd.DataFrame(np.random.randn(10, 5)) df.iloc[:8] = np.nan sdf = df.astype(pd.SparseDtype(\\"float\\", np.nan)) print(\\"Sparse DataFrame (first 5 rows):n\\", sdf.head()) print(\\"Memory usage:\\", sdf.memory_usage().sum()) return sdf # Task 2: Manipulate Sparse Data def manipulate_sparse_data(sdf): sdf_squared = np.square(sdf) print(\\"Squared Sparse DataFrame:n\\", sdf_squared) print(\\"Memory usage after squaring:\\", sdf_squared.memory_usage().sum()) return sdf_squared # Task 3: Convert Data Representations def convert_data_representations(sdf_squared): dense_df = sdf_squared.sparse.to_dense() print(\\"Dense DataFrame:n\\", dense_df) dense_df2 = pd.DataFrame({\\"A\\": [1, 0, 0, 1]}) sparse_dtype = pd.SparseDtype(int, fill_value=0) sparse_df2 = dense_df2.astype(sparse_dtype) print(\\"From Dense to Sparse DataFrame:n\\", sparse_df2) return dense_df, sparse_df2 # Task 4: Scipy Integration def scipy_integration(): arr = np.random.random(size=(1000, 5)) arr[arr < 0.9] = 0 sp_arr = csr_matrix(arr) sparse_df3 = pd.DataFrame.sparse.from_spmatrix(sp_arr) print(\\"Sparse DataFrame from scipy matrix (first 5 rows):n\\", sparse_df3.head()) coo_matrix = sparse_df3.sparse.to_coo() print(\\"Dense representation of converted scipy sparse matrix:n\\", coo_matrix.todense()) return sparse_df3, coo_matrix # Executing the tasks sdf = create_sparse_df() sdf_squared = manipulate_sparse_data(sdf) dense_df, sparse_df2 = convert_data_representations(sdf_squared) sparse_df3, coo_matrix = scipy_integration()"},{"question":"Question You are given a dataset containing numerical data of various measurements. Your task is to create a cluster map using the seaborn library to visualize the similarities and differences in the data. You must customize the cluster map according to the following requirements: 1. **Standardize the data within columns**: Ensure that the data for each column is standardized before clustering. 2. **Cluster only the rows**: Do not cluster the columns. 3. **Use a custom color map**: Apply the \\"viridis\\" color map for visualizing the heatmap. 4. **Adjust the layout and size**: Set the figure size to `(10, 8)` and customize the color bar\'s position to `(0.8, 0.2, 0.03, 0.4)`. 5. **Color rows by category**: Assume the last column of the dataset contains categorical labels for each row. Add different colors to the rows based on these categories. Use the labels as follows: - Label 0: \'red\' - Label 1: \'blue\' - Label 2: \'green\' # Input - A pandas DataFrame `data` containing the dataset: - The first columns (n-1 columns) should contain numerical data. - The last column should contain the categorical labels (0, 1, or 2). # Output - A seaborn cluster map plot that meets the above requirements. # Constraints - You may assume that the dataset will always have numerical columns followed by a categorical column. - The dataset will have at least one numerical column and one categorical column. - The name of the categorical column is not predefined, but it is always the last column of the dataset. Example ```python import pandas as pd import seaborn as sns # Create a sample dataset data = pd.DataFrame({ \'A\': [1, 2, 3, 4, 5], \'B\': [5, 4, 3, 2, 1], \'C\': [2, 3, 4, 5, 6], \'Label\': [0, 1, 0, 2, 1] }) def create_custom_clustermap(data): # Implementation goes here create_custom_clustermap(data) ``` # Notes - Ensure the color mapping for the rows is explicitly set according to the labels. - Your implementation should include all necessary imports and setup for seaborn.","solution":"import pandas as pd import seaborn as sns from sklearn.preprocessing import StandardScaler import matplotlib.pyplot as plt def create_custom_clustermap(data): # Extract numerical data and labels numerical_data = data.iloc[:, :-1] labels = data.iloc[:, -1] # Standardize the numerical data scaler = StandardScaler() standardized_data = scaler.fit_transform(numerical_data) # Create a color mapping for the row labels row_colors = labels.map({0: \'red\', 1: \'blue\', 2: \'green\'}).tolist() # Create the cluster map with the given specifications cluster_map = sns.clustermap( standardized_data, method=\'average\', metric=\'euclidean\', cmap=\'viridis\', row_cluster=True, col_cluster=False, figsize=(10, 8), row_colors=row_colors ) # Position the color bar cluster_map.cax.set_position([.8, .2, .03, .4]) # Show the plot plt.show()"},{"question":"# Python Programming Assignment Objective: To assess your understanding of the Python standard library including modules such as `os`, `shutil`, `sys`, `argparse`, and `re`. Problem Statement: You are tasked with creating a Python script that performs the following operations: 1. **Argument Parsing:** Use the `argparse` module to parse command line arguments. Your script should expect a directory path, a file extension (e.g., `.txt`), and an optional pattern string. - `directory`: The path to the directory where files will be searched. - `extension`: The file extension to filter files. - `pattern` (optional): If provided, only the files that contain this pattern should be listed. 2. **File Operations:** - Check if the provided directory exists. If not, print an appropriate error message and exit. - List all files in the directory with the specified extension using the `os` and `glob` modules. - If a pattern is provided, use the `re` module to filter the files that contain this pattern. 3. **Display Results:** - Print the list of files that match the criteria. - Display the number of matching files. Specifications: - **Input:** - Command line arguments for the directory, the file extension, and an optional pattern. - **Output:** - List of files matching the criteria and the count of these files. - Proper error handling and messages. Example Usage: ```bash python file_search.py /path/to/directory .txt \\"search pattern\\" ``` If no pattern is provided: ```bash python file_search.py /path/to/directory .txt ``` # Constraints: - The directory path must be a valid directory path. - The file extension and pattern are strings. Implementation: Implement this functionality in a Python script: ```python import os import shutil import sys import argparse import re def main(): parser = argparse.ArgumentParser(description=\'Search for files with a specific extension and optional pattern.\') parser.add_argument(\'directory\', type=str, help=\'Directory path to search.\') parser.add_argument(\'extension\', type=str, help=\'File extension to filter by.\') parser.add_argument(\'pattern\', type=str, nargs=\'?\', help=\'Optional pattern to search within files.\') args = parser.parse_args() if not os.path.isdir(args.directory): print(f\\"Error: {args.directory} is not a valid directory.\\") sys.exit(1) search_pattern = args.pattern if args.pattern else None extension = args.extension matched_files = [] for root, dirs, files in os.walk(args.directory): for file in files: if file.endswith(extension): file_path = os.path.join(root, file) if search_pattern: with open(file_path, \'r\') as f: file_contents = f.read() if re.search(search_pattern, file_contents): matched_files.append(file_path) else: matched_files.append(file_path) print(\\"Matched files:\\") for file in matched_files: print(file) print(f\\"Total files matched: {len(matched_files)}\\") if __name__ == \'__main__\': main() ``` In this assignment, you\'re expected to demonstrate proficiency in: - Command line argument parsing (`argparse`) - Directory and file handling (`os`, `shutil`) - Pattern matching (`re`) - Error handling and user feedback Ensure that your script is robust and handles possible edge cases, such as invalid directory paths and empty files gracefully.","solution":"import os import sys import argparse import re def search_files(directory, extension, pattern=None): if not os.path.isdir(directory): print(f\\"Error: {directory} is not a valid directory.\\") sys.exit(1) matched_files = [] for root, dirs, files in os.walk(directory): for file in files: if file.endswith(extension): file_path = os.path.join(root, file) if pattern: with open(file_path, \'r\', encoding=\'utf-8\', errors=\'ignore\') as f: file_contents = f.read() if re.search(pattern, file_contents): matched_files.append(file_path) else: matched_files.append(file_path) print(\\"Matched files:\\") for file in matched_files: print(file) print(f\\"Total files matched: {len(matched_files)}\\") return matched_files def main(): parser = argparse.ArgumentParser(description=\'Search for files with a specific extension and optional pattern.\') parser.add_argument(\'directory\', type=str, help=\'Directory path to search.\') parser.add_argument(\'extension\', type=str, help=\'File extension to filter by.\') parser.add_argument(\'pattern\', type=str, nargs=\'?\', help=\'Optional pattern to search within files.\') args = parser.parse_args() search_files(args.directory, args.extension, args.pattern) if __name__ == \'__main__\': main()"},{"question":"# Base64 Encoding and Decoding in Python **Background:** The `base64` module in Python provides functions for encoding and decoding binary data to various Base16, Base32, Base64, and Base85 formats. This is useful for safely transmitting binary data over text-based protocols. **Objective:** Write a Python class `Base64Utils` that implements the following methods for handling Base64 encoding and decoding: 1. `encode_to_base64(data: bytes, altchars: bytes = None) -> bytes`: Encodes the given binary data to Base64 format. If `altchars` is provided, it should be a bytes-like object of length 2 specifying an alternative alphabet for the \'+\' and \'/\' characters. 2. `decode_from_base64(data: bytes, altchars: bytes = None, validate: bool = False) -> bytes`: Decodes the given Base64 encoded data back to binary format. If `altchars` is provided, it should specify the alphabet used for encoding. If `validate` is `True`, any non-alphabet characters in the input should raise a `binascii.Error`. **Constraints:** - The `altchars` parameter, if provided, must be a bytes-like object of exactly 2 bytes. If it isn\'t, raise a `ValueError`. - Ensure that any `TypeError`, `ValueError`, or `binascii.Error` raised from the base64 functions are properly caught and re-raised with meaningful messages indicating what went wrong. **Function Signatures:** ```python class Base64Utils: def encode_to_base64(self, data: bytes, altchars: bytes = None) -> bytes: pass def decode_from_base64(self, data: bytes, altchars: bytes = None, validate: bool = False) -> bytes: pass ``` **Example Usage:** ```python utils = Base64Utils() # Example 1: Standard Base64 Encoding and Decoding encoded_data = utils.encode_to_base64(b\'hello world\') print(encoded_data) # Should output the base64 encoded bytes of \'hello world\' decoded_data = utils.decode_from_base64(encoded_data) print(decoded_data) # Should output b\'hello world\' # Example 2: Base64 Encoding with Alternative Characters encoded_data = utils.encode_to_base64(b\'hello world\', altchars=b\'-_\') print(encoded_data) # Should use \'-\' and \'_\' instead of \'+\' and \'/\' decoded_data = utils.decode_from_base64(encoded_data, altchars=b\'-_\') print(decoded_data) # Should output b\'hello world\' # Example 3: Base64 Decoding with Validation try: utils.decode_from_base64(b\'invalid*base64==\', validate=True) except binascii.Error as e: print(e) # Should raise an error due to invalid characters ``` **Validation:** Confirm that your class methods handle the constraints and edge cases effectively. The solution should work for standard scenarios and correctly manage alternative characters and validation errors.","solution":"import base64 import binascii class Base64Utils: def encode_to_base64(self, data: bytes, altchars: bytes = None) -> bytes: Encodes the given binary data to Base64 format. :param data: The binary data to encode. :param altchars: The alternative characters for \'+\' and \'/\'. :return: The Base64 encoded binary data. if altchars and (not isinstance(altchars, bytes) or len(altchars) != 2): raise ValueError(\\"altchars must be a bytes-like object of length 2\\") try: if altchars: return base64.b64encode(data, altchars) else: return base64.b64encode(data) except TypeError as e: raise ValueError(f\\"Failed to encode the data: {e}\\") def decode_from_base64(self, data: bytes, altchars: bytes = None, validate: bool = False) -> bytes: Decodes the given Base64 encoded data back to binary format. :param data: The Base64 encoded data to decode. :param altchars: The alternative characters used for encoding. :param validate: If True, validate the input data for non-alphabet characters. :return: The decoded binary data. if altchars and (not isinstance(altchars, bytes) or len(altchars) != 2): raise ValueError(\\"altchars must be a bytes-like object of length 2\\") try: if validate: data = base64.b64decode(data, altchars, validate=True) else: data = base64.b64decode(data, altchars) return data except (TypeError, ValueError, binascii.Error) as e: raise ValueError(f\\"Failed to decode the data: {e}\\")"},{"question":"Coding Assessment Question # Objective Implement an enumeration for a role-playing game (RPG) that includes character classes, each with a specific set of abilities and statistics. Use this enumeration to provide specialized methods and properties for accessing and manipulating these character classes. # Task Create an enumeration `CharacterClass` using the `enum.Enum` class. Each character class should have the following: 1. **Name** of the class (e.g., \\"WARRIOR\\", \\"MAGE\\", \\"ARCHER\\"). 2. **Health Points (HP)**: An integer representing the health points. 3. **Mana Points (MP)**: An integer representing the mana points required for special abilities. 4. **Abilities**: A list of strings representing the abilities available to the character class. 5. Implement at least one method that allows retrieval of a character\'s abilities based on a minimum required MP. Include the following character classes in your enumeration: - `WARRIOR`: 150 HP, 50 MP, Abilities: [\\"Slash\\", \\"Block\\", \\"Charge\\"] - `MAGE`: 100 HP, 200 MP, Abilities: [\\"Fireball\\", \\"Teleport\\", \\"Shield\\"] - `ARCHER`: 120 HP, 60 MP, Abilities: [\\"Shoot\\", \\"Dodge\\", \\"Piercing Arrow\\"] # Requirements 1. The enumeration should be defined using the `Enum` class. 2. The enumeration members should have custom initializations via `__new__` to store the additional attributes (HP, MP, and abilities) within each member. 3. Implement a method `get_abilities(minimum_mp: int)` that returns a list of abilities requiring at least `minimum_mp` Mana Points. # Constraints 1. All character class names should be in uppercase as they are defined as constants. 2. Ensure that the enumeration can be iterated over to list all character classes with their stats. 3. The solution should handle the case where no abilities meet the minimum MP requirement by returning an empty list. # Example ```python from enum import Enum class CharacterClass(Enum): WARRIOR = (150, 50, [\\"Slash\\", \\"Block\\", \\"Charge\\"]) MAGE = (100, 200, [\\"Fireball\\", \\"Teleport\\", \\"Shield\\"]) ARCHER = (120, 60, [\\"Shoot\\", \\"Dodge\\", \\"Piercing Arrow\\"]) def __new__(cls, hp, mp, abilities): obj = object.__new__(cls) obj._value_ = hp # Use HP as the value obj.hp = hp obj.mp = mp obj.abilities = abilities return obj def get_abilities(self, minimum_mp): return [ability for ability in self.abilities if self.mp >= minimum_mp] # Testing CharacterClass for char_class in CharacterClass: print(f\\"{char_class.name}: HP={char_class.hp}, MP={char_class.mp}, Abilities={char_class.abilities}\\") mage_abilities = CharacterClass.MAGE.get_abilities(150) print(f\\"Mage Abilities with minimum 150 MP: {mage_abilities}\\") ``` This example outlines the essential structure and demonstrates iteration over the enumeration members as well as an example method use case. # Performance Constraints - The implementation should efficiently use enumeration features, ensuring that methods for accessing attributes are optimized for lookups. - Consider the readability and maintainability of the code, adhering to best practices for defining enums in Python.","solution":"from enum import Enum class CharacterClass(Enum): WARRIOR = (150, 50, [\\"Slash\\", \\"Block\\", \\"Charge\\"]) MAGE = (100, 200, [\\"Fireball\\", \\"Teleport\\", \\"Shield\\"]) ARCHER = (120, 60, [\\"Shoot\\", \\"Dodge\\", \\"Piercing Arrow\\"]) def __new__(cls, hp, mp, abilities): obj = object.__new__(cls) obj._value_ = hp # Use HP as the value for the enum member obj.hp = hp obj.mp = mp obj.abilities = abilities return obj def get_abilities(self, minimum_mp): return [ability for ability in self.abilities if self.mp >= minimum_mp] # Example usage: # for char_class in CharacterClass: # print(f\\"{char_class.name}: HP={char_class.hp}, MP={char_class.mp}, Abilities={char_class.abilities}\\") # mage_abilities = CharacterClass.MAGE.get_abilities(150) # print(f\\"Mage Abilities with minimum 150 MP: {mage_abilities}\\")"},{"question":"Objective: Demonstrate your understanding of scikit-learn\'s `metrics.pairwise` submodule by implementing a function that computes pairwise kernels between two datasets using a specified kernel function and then uses this custom kernel to train a Support Vector Machine (SVM). Requirements: 1. Implement a function `compute_custom_kernel_and_train_svm(X_train, X_test, y_train, y_test, kernel_type)` that: - Computes the pairwise kernel matrix between `X_train` and `X_train`, and between `X_test` and `X_train`, using the specified `kernel_type`. - Trains an SVM model using the computed kernel matrix on `X_train`. - Evaluates the trained SVM model on the `X_test` dataset using the computed kernel for `X_test`. - Returns the accuracy score of the model on the `X_test` data. 2. The `kernel_type` should be one of the following: \'linear\', \'polynomial\', \'rbf\', \'sigmoid\', \'chi2\', or \'cosine\'. 3. If `kernel_type` is \'polynomial\', use degree=3, coef0=1. If it is \'sigmoid\', use gamma=0.5, coef0=0. 4. The function signature should be: ```python def compute_custom_kernel_and_train_svm(X_train, X_test, y_train, y_test, kernel_type): pass ``` 5. Provide a sample test case at the end of your function to demonstrate its usage. Constraints: - `X_train`, `X_test` are `numpy` arrays of shape `(num_samples, num_features)`. - `y_train`, `y_test` are `numpy` arrays of shape `(num_samples,)`. - Performance: Aim to have your function compute the required matrices efficiently even for sizable datasets. Example: ```python import numpy as np from sklearn.metrics.pairwise import pairwise_kernels from sklearn.svm import SVC from sklearn.metrics import accuracy_score def compute_custom_kernel_and_train_svm(X_train, X_test, y_train, y_test, kernel_type): # Compute kernel matrices K_train = pairwise_kernels(X_train, X_train, metric=kernel_type, degree=3, coef0=1) K_test = pairwise_kernels(X_test, X_train, metric=kernel_type, degree=3, coef0=1) # Train SVM with precomputed kernel svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) # Predict using the trained SVM y_pred = svm.predict(K_test) # Calculate and return accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy # Sample test with random data X_train = np.random.rand(10, 5) X_test = np.random.rand(5, 5) y_train = np.random.randint(0, 2, 10) y_test = np.random.randint(0, 2, 5) print(compute_custom_kernel_and_train_svm(X_train, X_test, y_train, y_test, \'polynomial\')) ``` Note: - Ensure to test your function with different `kernel_type` values to validate its correctness. - You may assume that the input datasets and labels are correctly formatted and do not contain missing values.","solution":"import numpy as np from sklearn.metrics.pairwise import pairwise_kernels from sklearn.svm import SVC from sklearn.metrics import accuracy_score def compute_custom_kernel_and_train_svm(X_train, X_test, y_train, y_test, kernel_type): Computes pairwise kernels with a specified type, trains an SVM, evaluates and returns the accuracy. Args: X_train (numpy.ndarray): Training data. X_test (numpy.ndarray): Test data. y_train (numpy.ndarray): Training labels. y_test (numpy.ndarray): Test labels. kernel_type (str): Type of kernel to be used. One of \'linear\', \'polynomial\', \'rbf\', \'sigmoid\', \'chi2\', \'cosine\'. Returns: float: Accuracy of the SVM model on the test set. # Customize parameters based on kernel type if kernel_type == \'polynomial\': kw = {\'degree\': 3, \'coef0\': 1} elif kernel_type == \'sigmoid\': kw = {\'gamma\': 0.5, \'coef0\': 0} else: kw = {} # Compute kernel matrices K_train = pairwise_kernels(X_train, X_train, metric=kernel_type, **kw) K_test = pairwise_kernels(X_test, X_train, metric=kernel_type, **kw) # Train SVM with precomputed kernel svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) # Predict using the trained SVM y_pred = svm.predict(K_test) # Calculate and return accuracy accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Problem Statement:** You are required to create a custom implementation of the built-in function `print`. This custom `print` function should: 1. Convert all output strings to lowercase before printing them. 2. Count the number of times this custom `print` function has been called and store this count as an attribute of the function itself. Additionally, implement a method that resets this count back to zero. This will help in verifying that the counting mechanism works accurately. **Function Specifications:** 1. **Function 1: my_print** - **Input**: Accepts any number of positional arguments and keyword arguments, similar to the built-in `print` function. - **Behavior**: Converts all strings to lowercase and prints them. - **Output**: No return value, but prints output to the console. - **Attribute**: Keeps an attribute `call_count` that counts the number of times `my_print` has been called. 2. **Function 2: reset_print_count** - **Input**: None. - **Behavior**: Resets the `call_count` attribute of `my_print` to zero. - **Output**: No return value. **Constraints:** - You cannot use global or nonlocal keywords. - The solution should handle any type of input that the built-in `print` function accepts. **Example:** ```python my_print(\\"Hello\\", \\"WORLD\\") # Output: hello world my_print(\\"Another Example!\\") # Output: another example! print(my_print.call_count) # Output: 2 reset_print_count() print(my_print.call_count) # Output: 0 ``` **Note:** - Be sure to use the `builtins` module to access the original `print` function. - Avoid modifying the original `print` function globally.","solution":"import builtins def my_print(*args, **kwargs): Custom print function that converts all string arguments to lowercase before printing and keeps track of the number of times it has been called. # Use the built-in print function to print the modified arguments def to_lowercase(item): if isinstance(item, str): return item.lower() return item modified_args = map(to_lowercase, args) builtins.print(*modified_args, **kwargs) # Increment the call count my_print.call_count += 1 # Initialize the call_count attribute my_print.call_count = 0 def reset_print_count(): Resets the call_count attribute of my_print to zero. my_print.call_count = 0"},{"question":"**Question:** You are tasked with writing a PyTorch-based function that performs a matrix multiplication using MPS (Metal Performance Shaders) on an Apple device. The function should make use of the environment variables to optimize performance and handle cases where MPS support is not available. Specifically, you need to: 1. Set up the environment to use fast math for MPS metal kernels. 2. Ensure that if MPS does not support a required operation, the function falls back to using the CPU. 3. Implement a function `optimized_matmul` that: - Takes two 2-D tensors as input. - Returns the result of the matrix multiplication. 4. Log the profile information and allocator logging if specified by the user. 5. Ensure the function handles out-of-memory (OOM) scenarios gracefully by making use of high and low watermark ratios. **Constraints:** - You may assume the input tensors are compatible for matrix multiplication. - The function should handle tensors of shape up to `4096 x 4096`. - You should use environment variables for setting the high and low watermark ratios as specified below: - High watermark ratio: 1.7 - Low watermark ratio: 1.4 if memory is unified, 1.0 if memory is discrete Here\'s the expected signature of the function: ```python import torch def optimized_matmul(tensor_a: torch.Tensor, tensor_b: torch.Tensor, log_profile: bool=False) -> torch.Tensor: ... # Example usage: tensor_a = torch.randn(1024, 2048, device=\'mps\') tensor_b = torch.randn(2048, 1024, device=\'mps\') result = optimized_matmul(tensor_a, tensor_b, log_profile=True) ``` **Notes:** - Ensure that the necessary environment variables are set within your function. - The function should execute efficiently on the MPS backend, and fallback to CPU operations if needed. - Logging should be enabled conditionally based on the `log_profile` parameter.","solution":"import os import torch def optimized_matmul(tensor_a: torch.Tensor, tensor_b: torch.Tensor, log_profile: bool=False) -> torch.Tensor: Perform optimized matrix multiplication using MPS (if available), else fallback to CPU. Parameters: tensor_a (torch.Tensor): The first input tensor. tensor_b (torch.Tensor): The second input tensor. log_profile (bool): Whether to log profile information (default: False). Returns: torch.Tensor: The result of the matrix multiplication. os.environ[\'METAL_FAST_MATH\'] = \\"1\\" # Determine memory configuration memory_is_unified = torch.mps.is_available() high_watermark_ratio = 1.7 low_watermark_ratio = 1.4 if memory_is_unified else 1.0 os.environ[\'PYTORCH_MPS_HIGH_WATERMARK_RATIO\'] = str(high_watermark_ratio) os.environ[\'PYTORCH_MPS_LOW_WATERMARK_RATIO\'] = str(low_watermark_ratio) # Enable allocator logging and profile logging if requested if log_profile: os.environ[\'PYTORCH_MPS_PROFILE_MODE\'] = \\"1\\" os.environ[\'PYTORCH_MPS_ALLOCATOR_LOGGING\'] = \\"1\\" try: if torch.mps.is_available(): result = torch.matmul(tensor_a, tensor_b, device=\'mps\') else: raise RuntimeError(\\"MPS not available. Falling back to CPU.\\") except RuntimeError as e: # Handle OOM and any other runtime error by falling back to CPU print(f\\"Encountered an error on MPS: {e}. Falling back to CPU.\\") result = torch.matmul(tensor_a.cpu(), tensor_b.cpu()) return result"},{"question":"**Objective:** Write a Python function that demonstrates an understanding of embedding Python and dealing with dynamic function calls and data conversions. **Problem Statement:** You need to write a Python function that mimics the behavior of calling a function from a Python script dynamically, inspired by the way embedding would allow calling functions across language boundaries. Your function will be able to dynamically import a module, retrieve a specified function from it, pass arguments to this function, and return its output to the calling context. **Function Signature:** ```python def dynamic_function_call(module_name: str, function_name: str, *args: int) -> int: pass ``` **Input:** - `module_name` (str): The name of the module where the function is located. - `function_name` (str): The name of the function to call within the given module. - `*args` (int): A variable number of integer arguments to pass to the function. **Output:** - The function should return an integer, which is the result produced by the called function within the specified module. **Constraints:** - The module specified by `module_name` must be importable. - The function specified by `function_name` must exist within the imported module and must accept and return integers. - You can assume there won\'t be any issues with converting between Python types as the arguments and return type will always be integers. **Example:** Consider you have a module named `math_operations` with a function named `multiply`: ```python # contents of math_operations.py def multiply(a, b): return a * b ``` Calling `dynamic_function_call(\'math_operations\', \'multiply\', 3, 4)` should return `12`. **Your Task:** Implement the `dynamic_function_call` function to meet the specified requirements.","solution":"import importlib def dynamic_function_call(module_name: str, function_name: str, *args: int) -> int: Dynamically imports a module and calls a specified function with given arguments. Args: - module_name (str): The name of the module where the function is located. - function_name (str): The name of the function to call within the given module. - args (int): A variable number of integer arguments to pass to the function. Returns: - int: The result produced by the called function within the specified module. module = importlib.import_module(module_name) func = getattr(module, function_name) result = func(*args) return result"},{"question":"**Title**: Implementing a Key-Value Store with `dbm` **Objective**: Implement a Python function to interact with a `dbm` database, demonstrating your understanding of the `dbm` module\'s functionality. Your function will perform multiple operations, such as adding, retrieving, and deleting key-value pairs, and also demonstrate error handling. **Question**: Create a function `dbm_operations` that performs various operations on a `dbm` database. The function should accept a list of operations to be performed on the database file and return a list of results of those operations. **Function Signature**: ```python def dbm_operations(filename: str, operations: list) -> list: pass ``` **Parameters**: - `filename` (str): The name of the database file to operate on. - `operations` (list): A list of dictionaries representing operations. Each dictionary has the following structure: - `type`: The type of operation (`\'add\'`, `\'get\'`, `\'delete\'`, `\'list_keys\'`). - `key`: The key to operate on (optional, only needed for `\'add\'`, `\'get\'`, and `\'delete\'` operations). - `value`: The value to add (only needed for `\'add\'` operations). **Returns**: - `list`: A list of results corresponding to each operation. For `\'add\'` and `\'delete\'` operations, return `None`. For `\'get\'` operations, return the value or `None` if the key is not found. For `\'list_keys\'` operations, return the list of all keys. **Constraints**: - You must handle exceptions gracefully. If an operation raises a `dbm.error`, return the string `\\"dbm.error\\"` for that operation. - Use the `with` statement for managing the `dbm` database context, ensuring it is closed properly after operations. **Example**: ```python operations = [ {\'type\': \'add\', \'key\': \'name\', \'value\': \'Alice\'}, {\'type\': \'get\', \'key\': \'name\'}, {\'type\': \'delete\', \'key\': \'name\'}, {\'type\': \'get\', \'key\': \'name\'}, {\'type\': \'list_keys\'} ] print(dbm_operations(\'mydb\', operations)) ``` **Expected Output**: ```python [None, \'Alice\', None, None, []] ``` **Performance Requirements**: - The function should be able to handle at least 1,000 operations in a reasonable time frame. **Additional Information**: - Remember that keys and values are stored as bytes in the `dbm` database. Properly encode and decode keys and values when storing or retrieving them.","solution":"import dbm def dbm_operations(filename: str, operations: list) -> list: results = [] try: with dbm.open(filename, \'c\') as db: for operation in operations: op_type = operation.get(\'type\') key = operation.get(\'key\') value = operation.get(\'value\') if op_type == \'add\': db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') results.append(None) elif op_type == \'get\': byte_key = key.encode(\'utf-8\') if byte_key in db: results.append(db[byte_key].decode(\'utf-8\')) else: results.append(None) elif op_type == \'delete\': try: del db[key.encode(\'utf-8\')] except KeyError: pass # If key does not exist, do nothing results.append(None) elif op_type == \'list_keys\': all_keys = [k.decode(\'utf-8\') for k in db.keys()] results.append(all_keys) else: results.append(None) except dbm.error: return [\\"dbm.error\\"] return results"},{"question":"# Coding Assessment Task **Objective**: To assess the student\'s ability to handle duplicate labels in pandas Series and DataFrames. You are provided with raw data that may contain duplicate labels. Your task is to write a function that processes this data to remove duplicates and ensures no duplicates are introduced during the data processing pipeline. Function Signature ```python import pandas as pd def remove_duplicates_and_process(df: pd.DataFrame) -> pd.DataFrame: This function takes a DataFrame with potential duplicate labels, removes any duplicates, and ensures that no duplicate labels are introduced during processing. Parameters: df (pd.DataFrame): Input DataFrame which might contain duplicate row or column labels. Returns: pd.DataFrame: A DataFrame with no duplicate labels. pass ``` Input - `df`: A pandas DataFrame which may contain duplicate row or column labels. The data can be in any form and may require cleaning. Output - A DataFrame that: - Has no duplicate row or column labels. - Ensures no duplicate labels can be introduced in further processing. Implementation Steps: 1. **Detect and Remove Duplicate Row Labels**: - Identify duplicate row labels. - Remove duplicates by keeping the first occurrence. - Set a flag to disallow duplicate row labels in further operations. 2. **Detect and Remove Duplicate Column Labels**: - Identify duplicate column labels. - Remove duplicates by keeping the first occurrence. - Set a flag to disallow duplicate column labels in further operations. 3. **Ensure Integrity**: - Ensure that any further data processing does not re-introduce duplicates. Constraints - You must use pandas library for the implementation. - You should use `.groupby(level=0).first()` to resolve duplicate row labels. - The returned DataFrame must have the `.flags.allows_duplicate_labels` set to `False`. Example Given the DataFrame: ```python import pandas as pd data = { \'A\': [1, 2, 3], \'B\': [4, 5, 6] } df = pd.DataFrame(data, index=[\'a\', \'a\', \'b\']) df.columns = pd.Index([\'X\', \'X\']) # duplicate column labels ``` After applying the `remove_duplicates_and_process` function, the output DataFrame should have unique row and column labels and disallow any further duplicates: ```python processed_df = remove_duplicates_and_process(df) print(processed_df) # Example output: # X # a 1 # b 3 ``` Ensure the flag is set correctly: ```python print(processed_df.flags.allows_duplicate_labels) # Should output False ``` Implement the function `remove_duplicates_and_process` to pass the provided example.","solution":"import pandas as pd def remove_duplicates_and_process(df: pd.DataFrame) -> pd.DataFrame: This function takes a DataFrame with potential duplicate labels, removes any duplicates, and ensures that no duplicate labels are introduced during processing. Parameters: df (pd.DataFrame): Input DataFrame which might contain duplicate row or column labels. Returns: pd.DataFrame: A DataFrame with no duplicate labels. # Remove duplicate row labels by keeping the first occurrences df = df.groupby(df.index).first() # Remove duplicate column labels by keeping the first occurrences df = df.loc[:,~df.columns.duplicated()] # Ensure no duplicate labels can be introduced during further processing df.flags.allows_duplicate_labels = False return df"},{"question":"# Secure Password Generator and Validator Objective: Write a Python program that generates a secure password based on user-defined criteria and then validates whether a given password meets the specified security requirements. Requirements: 1. The password should include at least one uppercase letter, one lowercase letter, one digit, and one special character. 2. The length of the password should be user-specified and must be at least 8 characters and no more than 32 characters. 3. The program should provide functionality to generate a secure password and validate an existing password. Function Definitions: 1. `generate_password(length: int) -> str` - **Input:** An integer `length` representing the desired length of the password. - **Output:** A randomly generated secure password satisfying the criteria mentioned above. - **Constraints:** - The `length` must be between 8 and 32 (inclusive). - **Behavior:** - If the `length` is outside the valid range, raise a `ValueError` with an appropriate error message. 2. `validate_password(password: str) -> bool` - **Input:** A string `password` that needs to be validated. - **Output:** A boolean indicating whether the password meets the security requirements described above. - **Behavior:** - Return `True` if the password meets all the criteria; otherwise, return `False`. Performance Requirements: - Ensure that the password generation is performed using cryptographic security provided by the `secrets` module. Example Usage: ```python # Example 1: Generating a password try: password = generate_password(12) print(\\"Generated password:\\", password) except ValueError as ve: print(str(ve)) # Example 2: Validating a password password = \\"A1b2C3d4!\\" is_valid = validate_password(password) print(\\"Is the password valid?\\", is_valid) ``` Expected Output: ```text Generated password: K8#96jc~2Gn # (The actual output may vary as it\'s randomly generated) Is the password valid? True ``` # Notes: - Use the `secrets` module functions such as `secrets.choice`, `secrets.randbelow`, etc., to generate secure random characters for the password. - Ensure that the generated password is a mix of uppercase letters, lowercase letters, digits, and special characters.","solution":"import string import secrets def generate_password(length: int) -> str: Generates a secure password of the specified length. The password must contain at least one uppercase letter, one lowercase letter, one digit, and one special character. :param length: Length of the password to be generated (must be between 8 and 32) :return: A secure password string :raises ValueError: If length is outside the range 8-32 if length < 8 or length > 32: raise ValueError(\\"Password length must be between 8 and 32 characters.\\") alphabet = string.ascii_letters + string.digits + string.punctuation while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.isupper() for c in password) and any(c.islower() for c in password) and any(c.isdigit() for c in password) and any(c in string.punctuation for c in password)): return password def validate_password(password: str) -> bool: Validates whether a given password meets the security criteria. :param password: The password string to be validated :return: True if the password meets the criteria, otherwise False if len(password) < 8 or len(password) > 32: return False has_upper = any(c.isupper() for c in password) has_lower = any(c.islower() for c in password) has_digit = any(c.isdigit() for c in password) has_special = any(c in string.punctuation for c in password) return has_upper and has_lower and has_digit and has_special"},{"question":"**Seaborn Bar Plot Coding Assessment** You are given two datasets: `penguins` and `flights`, which you have to use to create several bar plots using seaborn. Your task is to implement a function `create_seaborn_plots` that generates and saves specific bar plots as described below. # Function Specification **Function Name**: `create_seaborn_plots` **Inputs**: - No direct inputs to the function. You will load the datasets within the function. **Expected Output**: - Save the generated plots as image files in the current directory. # Steps to Implement: 1. **Load datasets**: Load the `penguins` and `flights` datasets from seaborn. 2. **Plot 1: Basic Bar Plot**: Create a bar plot that displays the average body mass (`body_mass_g`) of penguins grouped by `island`. Save this plot as `plot1.png`. 3. **Plot 2: Color Hue Bar Plot**: Create a bar plot that displays the average body mass (`body_mass_g`) of penguins grouped by `island`, using `sex` for color hue. Save this plot as `plot2.png`. 4. **Plot 3: Error Bars as Standard Deviation**: Create a bar plot similar to Plot 1, but display the standard deviation as error bars. Save this plot as `plot3.png`. 5. **Plot 4: Custom Aggregation Function**: Create a bar plot that shows the total number of passengers per year from the `flights` dataset. Disable the error bars. Save this plot as `plot4.png`. 6. **Plot 5: Annotated Bar Plot**: Create a bar plot similar to Plot 4, but add text labels on each bar showing the total number of passengers. Save this plot as `plot5.png`. 7. **Plot 6: Faceted Bar Plot**: Create a faceted bar plot grouped by `species` that shows the average body mass (`body_mass_g`) of penguins, with `sex` on the x-axis. Save this plot as `plot6.png`. # Additional Constraints: - Use proper seaborn and matplotlib functionalities. - Ensure all plots have appropriate titles, axis labels, and legends (if applicable). - Handle loading of datasets within the function. - Save each plot with the specified filenames. # Example Usage: ```python def create_seaborn_plots(): # Your implementation here # After implementing the function, you can run it to generate and save the plots create_seaborn_plots() ``` Remember, the purpose of this exercise is to showcase your seaborn skills, including how to manipulate datasets, configure plots, and handle advanced seaborn functionalities.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_seaborn_plots(): # Load datasets penguins = sns.load_dataset(\'penguins\') flights = sns.load_dataset(\'flights\') # Plot 1: Basic Bar Plot plt.figure() sns.barplot(x=\'island\', y=\'body_mass_g\', data=penguins) plt.title(\'Average Body Mass by Island\') plt.xlabel(\'Island\') plt.ylabel(\'Average Body Mass (g)\') plt.savefig(\'plot1.png\') plt.close() # Plot 2: Color Hue Bar Plot plt.figure() sns.barplot(x=\'island\', y=\'body_mass_g\', hue=\'sex\', data=penguins) plt.title(\'Average Body Mass by Island and Sex\') plt.xlabel(\'Island\') plt.ylabel(\'Average Body Mass (g)\') plt.legend(title=\'Sex\') plt.savefig(\'plot2.png\') plt.close() # Plot 3: Error Bars as Standard Deviation plt.figure() sns.barplot(x=\'island\', y=\'body_mass_g\', data=penguins, ci=\'sd\') plt.title(\'Average Body Mass by Island (with Std Dev)\') plt.xlabel(\'Island\') plt.ylabel(\'Average Body Mass (g)\') plt.savefig(\'plot3.png\') plt.close() # Plot 4: Custom Aggregation Function plt.figure() sns.barplot(x=\'year\', y=\'passengers\', data=flights, ci=None, estimator=sum) plt.title(\'Total Passengers per Year\') plt.xlabel(\'Year\') plt.ylabel(\'Total Passengers\') plt.savefig(\'plot4.png\') plt.close() # Plot 5: Annotated Bar Plot plt.figure() barplot = sns.barplot(x=\'year\', y=\'passengers\', data=flights, ci=None, estimator=sum) for p in barplot.patches: barplot.annotate(format(p.get_height(), \'.0f\'), (p.get_x() + p.get_width() / 2., p.get_height()), ha = \'center\', va = \'center\', xytext = (0, 9), textcoords = \'offset points\') plt.title(\'Total Passengers per Year with Annotations\') plt.xlabel(\'Year\') plt.ylabel(\'Total Passengers\') plt.savefig(\'plot5.png\') plt.close() # Plot 6: Faceted Bar Plot g = sns.catplot(x=\'sex\', y=\'body_mass_g\', col=\'species\', data=penguins, kind=\'bar\', ci=None) g.set_titles(\'Average Body Mass by Sexn{col_name}\') g.set_axis_labels(\'Sex\', \'Average Body Mass (g)\') g.fig.suptitle(\'Faceted Bar Plot of Average Body Mass by Sex and Species\', y=1.03) g.savefig(\'plot6.png\')"},{"question":"# Combinatorial Numbers and Permutations **Problem Statement:** You are tasked with implementing two functions from scratch that mimic Python\'s `math.comb` and `math.perm` functions. 1. **Combination Function (`comb`)** - Implement a function `comb(n: int, k: int) -> int` that returns the number of ways to choose `k` items from `n` items without repetition and without order. - The combination formula is defined as: [ C(n, k) = frac{n!}{k! cdot (n - k)!} ] - If `k > n`, the function should return 0. - The function should raise a `ValueError` if either argument is negative. - The function should raise a `TypeError` if either argument is not an integer. 2. **Permutation Function (`perm`)** - Implement a function `perm(n: int, k: int = None) -> int` that returns the number of ways to choose `k` items from `n` items without repetition and with order. - If `k` is `None`, the function should default to `n`. - The permutation formula is defined as: [ P(n, k) = frac{n!}{(n - k)!} ] - If `k > n`, the function should return 0. - The function should raise a `ValueError` if either argument is negative. - The function should raise a `TypeError` if either argument is not an integer. # Requirements: - Your implementation should avoid using the `math.comb` and `math.perm` functions directly. - You can use other functions from the `math` module like `math.factorial`. - Ensure to handle edge cases as described, including type and value constraints. # Examples: ```python def comb(n: int, k: int) -> int: # Your implementation here pass print(comb(5, 3)) # Output: 10 print(comb(6, 2)) # Output: 15 print(comb(6, -2)) # Raises ValueError print(comb(6, 2.5)) # Raises TypeError print(comb(3, 5)) # Output: 0 def perm(n: int, k: int = None) -> int: # Your implementation here pass print(perm(5, 3)) # Output: 60 print(perm(6, 2)) # Output: 30 print(perm(4)) # Output: 24 print(perm(6, -2)) # Raises ValueError print(perm(6, 2.5)) # Raises TypeError print(perm(3, 5)) # Output: 0 ``` # Constraints: - Ensure your solution is efficient even for larger values of `n` and `k`, where `0 <= k <= n <= 1000`.","solution":"import math def comb(n: int, k: int) -> int: Returns the number of ways to choose k items from n items without repetition and without order. if not isinstance(n, int) or not isinstance(k, int): raise TypeError(\\"Both n and k should be integers\\") if n < 0 or k < 0: raise ValueError(\\"Both n and k should be non-negative\\") if k > n: return 0 return math.factorial(n) // (math.factorial(k) * math.factorial(n - k)) def perm(n: int, k: int = None) -> int: Returns the number of ways to choose k items from n items without repetition and with order. If k is None, defaults to n. if k is None: k = n if not isinstance(n, int) or not isinstance(k, int): raise TypeError(\\"Both n and k should be integers\\") if n < 0 or k < 0: raise ValueError(\\"Both n and k should be non-negative\\") if k > n: return 0 return math.factorial(n) // math.factorial(n - k)"},{"question":"Optimized Matrix Multiplication in PyTorch Objective: Implement an optimized matrix multiplication function in PyTorch that leverages specific SIMD instruction sets such as AVX-512 or AMX, if available, to enhance performance. Problem: You need to write a PyTorch-based function `optimized_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor` that performs the matrix multiplication of two input matrices with optimizations for specific SIMD instruction sets. Your function should: 1. Check the available SIMD instruction sets on the machine. 2. Perform matrix multiplication optimized for the highest available SIMD instruction set. 3. Fall back to standard matrix multiplication if no specific SIMD instruction sets are found. Input: - `A` (torch.Tensor): A 2D tensor (matrix) of shape `(m, n)`. - `B` (torch.Tensor): A 2D tensor (matrix) of shape `(n, p)`. Output: - A 2D tensor (matrix) of shape `(m, p)` that is the result of the matrix multiplication of `A` and `B`. Constraints: - The input tensors `A` and `B` will have dimensions such that the matrix multiplication is valid (i.e., the number of columns in `A` equals the number of rows in `B`). Requirements: 1. Use the `collect_env.py` script to check for available ISAs. 2. Apply optimizations for AVX-512 or AMX if they are available. 3. Ensure the function is efficient and takes advantage of the available SIMD instruction sets on the machine to improve performance. Example: ```python import torch # Example Matrices A = torch.randn(1000, 1500) B = torch.randn(1500, 1200) # Function Call result = optimized_matrix_multiplication(A, B) print(result.shape) ``` - If AVX-512 is supported, the implementation should leverage it. - If AMX is supported, the implementation should leverage it. - If neither is supported, a standard PyTorch matrix multiplication should be performed. **Note:** You do not need to implement the intricate low-level details of SIMD operations, but ensure the function logic is structured to accommodate such optimizations where applicable.","solution":"import torch def optimized_matrix_multiplication(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor: Perform an optimized matrix multiplication of A and B. This function will check the available SIMD instruction sets and use optimizations if available. Parameters: A (torch.Tensor): A 2D tensor of shape (m, n) B (torch.Tensor): A 2D tensor of shape (n, p) Returns: torch.Tensor: A 2D tensor of shape (m, p) that is the result of the matrix multiplication of A and B # For the purpose of this task, we\'ll simulate the check for available SIMD instruction sets. # In a real-world scenario, this step would involve checking the actual hardware capabilities. # Here, we assume no specific SIMD instructions are available and fall back to standard matrix multiplication. # Perform matrix multiplication result = torch.mm(A, B) return result"},{"question":"Objective Demonstrate comprehension of the \\"pickletools\\" module by implementing a function that processes and optimizes a collection of pickle files. Problem Description You are given a list of file paths to different pickle files. Your task is to write a Python function that reads these pickle files, disassembles their contents, identifies the highest pickle protocol used among all files, optimizes each pickle, and then writes the optimized pickle back to the original file. Function Signature ```python def process_and_optimize_pickles(file_paths: list) -> dict: Process and optimize a collection of pickle files. Args: - file_paths (list of str): A list of file paths to pickle files. Returns: - dict: A dictionary with file paths as keys and the highest protocol used in each respective file as values. ``` Input - `file_paths`: A list of strings, each representing a file path to a pickle file. Output - Returns a dictionary where each key is a file path from the input list, and each corresponding value is an integer representing the highest pickle protocol version used in that file. Example ```python file_paths = [\'file1.pickle\', \'file2.pickle\'] result = process_and_optimize_pickles(file_paths) print(result) # Example output: {\'file1.pickle\': 4, \'file2.pickle\': 3} ``` Constraints - Your function should handle read/write operations safely, ensuring data integrity. - Assume the provided file paths are valid and accessible. - You should use `pickletools.dis()` to analyze the highest protocol and `pickletools.optimize()` to optimize the pickle files. Hints - You can use the `pickletools.genops()` function to iterate through opcodes in a pickle to determine the highest protocol version used. - `pickletools.optimize()` can be used to produce an optimized version of the pickle string. Performance Requirements - The function should be efficient in terms of both time and space complexity. - The optimization process should not significantly increase the runtime compared to simply reading and writing the files.","solution":"import pickle import pickletools def process_and_optimize_pickles(file_paths): Process and optimize a collection of pickle files. Args: - file_paths (list of str): A list of file paths to pickle files. Returns: - dict: A dictionary with file paths as keys and the highest protocol used in each respective file as values. highest_protocols = {} for file_path in file_paths: with open(file_path, \'rb\') as f: data = f.read() # Disassemble the pickle data pickle_info = pickletools.genops(data) highest_protocol = 0 for opcode, arg, _pos in pickle_info: if opcode.name == \'PROTO\': highest_protocol = max(highest_protocol, arg) highest_protocols[file_path] = highest_protocol # Optimize the pickle data optimized_data = pickletools.optimize(data) # Write back the optimized data with open(file_path, \'wb\') as f: f.write(optimized_data) return highest_protocols"},{"question":"Objective: To assess the students\' understanding of PyTorch\'s Export IR by requiring them to write code that exports a model\'s computation as an Export IR graph and performs analysis and modifications on the graph. Problem Statement: 1. **Model Implementation**: - Implement a simple PyTorch model `SimpleModel` with one hidden linear layer and ReLU activation. The model should accept a single tensor input of shape `(batch_size, input_features)` and output a tensor of shape `(batch_size, output_features)`. 2. **Export to Export IR**: - Export the `SimpleModel` to an Export IR graph using `torch.export.export`. Use a dummy input tensor for this export process. 3. **Graph Analysis**: - Write a function `analyze_graph` that takes an `ExportedProgram` object and prints out the following: - The total number of nodes in the graph. - The types of each node in the graph (e.g., placeholder, call_function, output). 4. **Graph Modification**: - Write a function `modify_graph` that takes an `ExportedProgram` object and adds an additional linear layer to the model after the hidden layer. The new layer should have output features equal to the original output features of the model. - Update the `ExportedProgram` and print out the modified graph. Constraints: - You may assume that the input tensor has a fixed shape `(1, 10)`, and the output features of both the hidden layer and the new layer should be 5. - The solution should use PyTorch version 1.10 or above. Input and Output Formats: - The `SimpleModel` accepts a tensor of shape `(batch_size, input_features) = (1, 10)` and returns a tensor of shape `(batch_size, output_features) = (1, 5)`. - The `analyze_graph` function should print the required information. - The `modify_graph` function should return and print the modified graph. Example: ```python import torch import torch.nn as nn class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.hidden = nn.Linear(10, 5) self.relu = nn.ReLU() def forward(self, x): x = self.hidden(x) return self.relu(x) dummy_input = torch.randn(1, 10) model = SimpleModel() exported_program = torch.export.export(model, (dummy_input,)) def analyze_graph(exported_program): graph = exported_program.graph # Your code to analyze the graph def modify_graph(exported_program): graph = exported_program.graph # Your code to modify the graph analyze_graph(exported_program) modified_program = modify_graph(exported_program) print(modified_program.graph) ``` Notes: - Make sure your code is well-documented and includes necessary comments. - Handle any edge cases where applicable.","solution":"import torch import torch.nn as nn import torch.fx as fx class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.hidden = nn.Linear(10, 5) self.relu = nn.ReLU() def forward(self, x): x = self.hidden(x) return self.relu(x) dummy_input = torch.randn(1, 10) model = SimpleModel() # Using torch.fx to trace the model tracer = fx.Tracer() graph = tracer.trace(model) exported_program = fx.GraphModule(model, graph) def analyze_graph(exported_program): graph = exported_program.graph total_nodes = len(graph.nodes) node_types = [node.op for node in graph.nodes] print(f\\"Total nodes: {total_nodes}\\") print(\\"Node types:\\") for node in graph.nodes: print(f\\" {node.name}: {node.op}\\") def modify_graph(exported_program): graph = exported_program.graph with graph.inserting_after(list(graph.nodes)[-2]): new_node = graph.create_node( \'call_module\', \'new_linear\', args=(list(graph.nodes)[-2],), name=\'new_linear\' ) # Adding a new Linear layer to the model class ModifiedModel(nn.Module): def __init__(self): super(ModifiedModel, self).__init__() self.hidden = nn.Linear(10, 5) self.relu = nn.ReLU() self.new_linear = nn.Linear(5, 5) # New layer def forward(self, x): x = self.hidden(x) x = self.relu(x) x = self.new_linear(x) return x new_model = ModifiedModel() modified_graph = fx.GraphModule(new_model, graph) return modified_graph analyze_graph(exported_program) modified_program = modify_graph(exported_program) print(modified_program.graph)"},{"question":"**Objective:** Demonstrate your understanding of various properties in seaborn for customizing plots. **Task:** Given a dataset of your choice that contains continuous, nominal, and temporal data, create a comprehensive visualization using the seaborn package where you apply and customize at least three different properties from each category: coordinate, color, and size. Ensure your plot is informative and aesthetically pleasing. **Requirements:** 1. Use a dataset that includes continuous, nominal, and temporal fields. 2. Create a single `seaborn` plot demonstrating the use of: - At least three coordinate properties (e.g., `x`, `y`, `xmin`, `xmax`). - At least three color properties (e.g., `color`, `fillcolor`, `edgecolor`). - At least three size-related properties (e.g., `pointsize`, `linewidth`, `edgewidth`). 3. Customize the plot theme ensuring that: - Background colors, spines, and ticks are customized. - Your plot title and axis labels are set and styled appropriately. 4. Apply custom scales for both continuous and nominal data fields. **Input and Output Formats:** - **Input:** Any dataset of your choice (e.g., pandas DataFrame). You could use built-in datasets such as Seaborn\'s `tips`, `iris`, etc. - **Output:** A seaborn plot visualized inline. **Constraints and Limitations:** - The plot should be able to display all components clearly without overlapping. - Ensure to manage the scale to accommodate the entire data range meaningfully. **Performance Requirements:** - Your code should execute within a reasonable time frame. Plots should render correctly within Jupyter Notebook or similar environments without lag. **Sample Code Scaffold:** ```python import seaborn as sns import pandas as pd import numpy as np from seaborn.objects import Plot import matplotlib.pyplot as plt from datetime import datetime # Load an example dataset data = sns.load_dataset(\\"dataset_name\\") # Example of continuous, nominal, and temporal data usage # Adjust the dataset and columns accordingly # Create your plot p = ( Plot(data) # Add your layers, e.g. Dot, Bar, Line, etc. # Apply coordinate, color, and size properties .theme({ # Customize your plot theme }) # Other necessary plot configurations ) # Render the plot plt.show() ``` Make sure to explore the documentation and incorporate creative ideas demonstrating your thorough understanding of seaborn properties. **Submission:** Submit your notebook file (.ipynb) with the completed code implementation and the rendered plot in an online grading platform.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt # Load example dataset data = sns.load_dataset(\\"tips\\") # Convert `day` to ordinal for better customization data[\'day\'] = pd.Categorical(data[\'day\'], categories=[\'Thur\', \'Fri\', \'Sat\', \'Sun\']) # Convert `time` to categorical data[\'time\'] = pd.Categorical(data[\'time\']) # Create a new column to represent temporal data for this scenario (using date simulation) data[\'date\'] = pd.date_range(start=\'2021-01-01\', periods=len(data), freq=\'D\') # Create the seaborn plot plt.figure(figsize=(12, 8)) plot = sns.scatterplot(data=data, x=\\"total_bill\\", y=\\"tip\\", hue=\\"day\\", size=\\"size\\", sizes=(20, 200), palette=\\"coolwarm\\", style=\\"time\\", edgecolor=\\"black\\", linewidth=0.5) # Customizing the plot theme sns.set_theme(style=\\"whitegrid\\") plot.set_title(\\"Total Bill vs Tip by Day and Time\\", fontsize=20, fontweight=\'bold\') plot.set_xlabel(\\"Total Bill ()\\", fontsize=15) plot.set_ylabel(\\"Tip ()\\", fontsize=15) # Customize spines for spine in plot.spines.values(): spine.set_edgecolor(\'gray\') spine.set_linewidth(1) # Customize ticks plot.tick_params(axis=\'x\', colors=\'darkblue\', size=5, width=1.5) plot.tick_params(axis=\'y\', colors=\'darkblue\', size=5, width=1.5) # Render the plot plt.show()"},{"question":"Advanced Enum Usage in Python Objective: The goal of this exercise is to assess your understanding of creating and using Enums in Python and applying this in a practical scenario. Problem Statement: You are required to create an enumeration for representing different access levels in a software, including both named levels and their combinations. Additionally, you should implement methods to manage these access levels. 1. Define an enumeration `AccessLevel` using `IntFlag` that includes: - `READ`: Represents read access. - `WRITE`: Represents write access. - `EXECUTE`: Represents execute access. - `FULL_CONTROL`: A combination of `READ`, `WRITE`, and `EXECUTE`. 2. Implement a method `describe` within the `AccessLevel` enumeration that returns a string describing the access level. 3. Create a class `User` that: - Has attributes for `username` (string) and `access_level` (an instance of `AccessLevel`). - Implements a method `has_access` in the `User` class that takes an argument of type `AccessLevel` and returns `True` if the user’s access level includes the specified access, otherwise returns `False`. 4. Create the following users with their respective access levels: - \\"alice\\" with `AccessLevel.READ` - \\"bob\\" with `AccessLevel.WRITE` - \\"carol\\" with `AccessLevel.FULL_CONTROL` 5. Write test cases to verify: - Descriptions for each access level (both individual and combinations). - Whether each user has the correct access to `READ`, `WRITE`, and `EXECUTE`. Constraints: - You should use the `enum` module provided by Python. - The `describe` method should use custom logic to create meaningful descriptions. - The `AccessLevel` enumeration should be defined using `IntFlag`. Example: ```python from enum import IntFlag, auto class AccessLevel(IntFlag): READ = auto() WRITE = auto() EXECUTE = auto() FULL_CONTROL = READ | WRITE | EXECUTE def describe(self): descriptions = { AccessLevel.READ: \\"Read access\\", AccessLevel.WRITE: \\"Write access\\", AccessLevel.EXECUTE: \\"Execute access\\", AccessLevel.FULL_CONTROL: \\"Full control (read, write, execute)\\" } return descriptions.get(self, \\"Unknown access level\\") class User: def __init__(self, username, access_level): self.username = username self.access_level = access_level def has_access(self, level): return (self.access_level & level) == level # Users alice = User(\\"alice\\", AccessLevel.READ) bob = User(\\"bob\\", AccessLevel.WRITE) carol = User(\\"carol\\", AccessLevel.FULL_CONTROL) # Tests assert AccessLevel.READ.describe() == \\"Read access\\" assert AccessLevel.WRITE.describe() == \\"Write access\\" assert AccessLevel.FULL_CONTROL.describe() == \\"Full control (read, write, execute)\\" assert alice.has_access(AccessLevel.READ) assert not alice.has_access(AccessLevel.WRITE) assert bob.has_access(AccessLevel.WRITE) assert not bob.has_access(AccessLevel.EXECUTE) assert carol.has_access(AccessLevel.READ) assert carol.has_access(AccessLevel.WRITE) assert carol.has_access(AccessLevel.EXECUTE) assert not carol.has_access(AccessLevel(0)) # No access assert (AccessLevel.READ | AccessLevel.WRITE).describe() == \\"Unknown access level\\" # Custom combination print(\\"All tests passed!\\") ``` Ensure your solution passes the provided test cases, and feel free to add additional test cases to validate different scenarios.","solution":"from enum import IntFlag, auto class AccessLevel(IntFlag): READ = auto() WRITE = auto() EXECUTE = auto() FULL_CONTROL = READ | WRITE | EXECUTE def describe(self): descriptions = { AccessLevel.READ: \\"Read access\\", AccessLevel.WRITE: \\"Write access\\", AccessLevel.EXECUTE: \\"Execute access\\", AccessLevel.FULL_CONTROL: \\"Full control (read, write, execute)\\" } return descriptions.get(self, \\"Unknown access level\\") class User: def __init__(self, username, access_level): self.username = username self.access_level = access_level def has_access(self, level): return (self.access_level & level) == level # Users creation for later usage in tests alice = User(\\"alice\\", AccessLevel.READ) bob = User(\\"bob\\", AccessLevel.WRITE) carol = User(\\"carol\\", AccessLevel.FULL_CONTROL)"},{"question":"You are given a dataset of students\' scores in various subjects and are required to perform the following tasks using seaborn to visualize the data effectively. # Dataset The dataset contains: - `Student`: The name of the student - `Math`: Score in Mathematics - `English`: Score in English - `Science`: Score in Science ```json { \\"Student\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"Math\\": [78, 82, 91, 85, 70], \\"English\\": [88, 79, 94, 81, 70], \\"Science\\": [84, 75, 89, 90, 73] } ``` # Tasks 1. **Create a DataFrame**: Load the provided dataset into a pandas DataFrame. 2. **Set Plotting Context**: - Use `sns.plotting_context` to set the context to `\\"talk\\"` and generate a plot for later comparison. 3. **Generate and Customize Plots**: - Create a bar plot showing each student\'s scores across the three subjects. - Customize the plot by adding titles and adjusting the labels appropriately. 4. **Context Manager**: - Use the seaborn context manager (`with sns.plotting_context(\\"paper\\"):`) to create a second plot and observe the difference in style. 5. **Compare and Analyze**: - Write a brief observation on the visual differences between the plots created with different contexts. # Expected Output 1. Python code that performs the above tasks. 2. The initial bar plot with the `\\"talk\\"` context. 3. The second bar plot created within the `\\"paper\\"` context manager. 4. A textual observation comparing the two plots. # Input and Output Input DataFrame structure and plotting context changes should be handled within the script. Output - Two bar plots displayed inline (one for each context). - Printed brief observation of differences in styling. # Constraints and Limitations - The task should be performed using seaborn, pandas, and matplotlib libraries. - Ensure the plots are properly labeled, and any customization (like changing the font size, colors, or legends) should be made clear through comments in the code. # Performance Requirement - The solution should efficiently create and display the plots, taking no more than a few seconds to run.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt # Step 1: Create a DataFrame data = { \\"Student\\": [\\"Alice\\", \\"Bob\\", \\"Charlie\\", \\"David\\", \\"Eve\\"], \\"Math\\": [78, 82, 91, 85, 70], \\"English\\": [88, 79, 94, 81, 70], \\"Science\\": [84, 75, 89, 90, 73] } df = pd.DataFrame(data) # Step 2: Set Plotting Context sns.set_context(\\"talk\\") # Step 3: Generate and Customize Plot plt.figure(figsize=(10, 6)) melted_df = df.melt(id_vars=[\\"Student\\"], var_name=\\"Subject\\", value_name=\\"Score\\") plot = sns.barplot(data=melted_df, x=\\"Student\\", y=\\"Score\\", hue=\\"Subject\\") plt.title(\\"Students\' Scores in Various Subjects\\") plt.xlabel(\\"Student\\") plt.ylabel(\\"Score\\") plt.legend(title=\\"Subject\\") plt.show() # Step 4: Context Manager with sns.plotting_context(\\"paper\\"): plt.figure(figsize=(10, 6)) plot_paper = sns.barplot(data=melted_df, x=\\"Student\\", y=\\"Score\\", hue=\\"Subject\\") plt.title(\\"Students\' Scores in Various Subjects (Paper Context)\\") plt.xlabel(\\"Student\\") plt.ylabel(\\"Score\\") plt.legend(title=\\"Subject\\") plt.show() # Step 5: Observation print(\\"Observation: The \'talk\' context results in larger elements compared to the \'paper\' context. Specifically, the fonts and plot elements (bars, labels) appear larger and more spaced out in the \'talk\' context, making it easier for presentation and discussion. The \'paper\' context has more compact and smaller elements suitable for printed materials.\\")"},{"question":"**Problem Statement: Creating a Custom Python Type with Extension Methods** You are required to create a custom Python type using the CPython API. The custom type will be a `Point` type representing a point in 2D space. This `Point` type should demonstrate your understanding of extension types by implementing the following functionalities: 1. **Memory and Cleanup:** - Implement a constructor that initializes the `x` and `y` coordinates. - Implement a destructor to free any resources if necessary. 2. **String Representations:** - Implement `tp_repr` to return a string of the form `Point(x=..., y=...)`. - Implement `tp_str` to return a more human-readable string. 3. **Attribute Access:** - Implement `tp_getattr` and `tp_setattr` to get and set the `x` and `y` coordinates. - Ensure that the attributes can be set and retrieved correctly. 4. **Object Comparison:** - Implement `tp_richcompare` so that `Point` objects can be compared using `<`, `<=`, `==`, `!=`, `>`, and `>=`. 5. **Iterator Support:** - Implement `tp_iter` and `tp_iternext` to iterate over the `x` and `y` coordinates. 6. **Weak Reference Support:** - Add support for weak references to the `Point` type. **Expected Input and Output:** - The `Point` type should support the following operations in Python: ```python p1 = Point(3, 4) print(p1) # Point(x=3, y=4) print(repr(p1)) # Point(x=3, y=4) print(p1.x, p1.y) # 3 4 p1.x = 10 print(p1.x) # 10 p2 = Point(1, 2) print(p1 > p2) # Depends on the implementation of comparison, e.g., False for coord in p1: print(coord) # 10 4 ``` **Constraints:** - Handle memory allocation and deallocation efficiently. - Ensure that the `tp_richcompare` handles all comparison operators. - The iteration should work seamlessly over the coordinates `x` and `y`. **Performance Requirements:** - The implementation should be efficient and should not introduce memory leaks. - The comparison operations should be optimized for multiple calls. Please write the corresponding C code to define this custom type and provide the necessary Python bindings for it.","solution":"from weakref import ref class Point: def __init__(self, x, y): self._x = x self._y = y self._weakref = ref(self) # support for weak reference @property def x(self): return self._x @x.setter def x(self, value): self._x = value @property def y(self): return self._y @y.setter def y(self, value): self._y = value def __repr__(self): return f\\"Point(x={self._x}, y={self._y})\\" def __str__(self): return f\\"Point(x={self._x}, y={self._y})\\" def __iter__(self): self._index = 0 return self def __next__(self): if(self._index == 0): self._index += 1 return self._x elif(self._index == 1): self._index += 1 return self._y else: raise StopIteration # Implementing comparison based on the sum of the coordinates def __lt__(self, other): return (self._x + self._y) < (other._x + other._y) def __le__(self, other): return (self._x + self._y) <= (other._x + other._y) def __eq__(self, other): return (self._x + self._y) == (other._x + other._y) def __ne__(self, other): return (self._x + self._y) != (other._x + other._y) def __gt__(self, other): return (self._x + self._y) > (other._x + other._y) def __ge__(self, other): return (self._x + self._y) >= (other._x + other._y)"},{"question":"# Question: Caching and Partial Application using `functools` Objective: Implement a python function that calculates the power of a given base and exponent. Use caching to store the computed results for faster future access. Additionally, create a partially-applied function to always calculate the square of the given base. Requirements: 1. **Function Definition**: - Name: `power` - Arguments: - `base` (integer): The base number. - `exp` (integer): The exponent number. - Returns: The result of `base` raised to the power of `exp` (i.e., `base ** exp`). 2. **Caching**: - Use the `lru_cache` decorator from the `functools` module to cache the results of the `power` function. 3. **Partial Application**: - Create a partially-applied function named `square` from the `power` function such that it only takes the `base` as an argument and always uses 2 as the exponent. Example: ```python # Your implementation here # Test caching print(power(2, 3)) # Expected output: 8 (Calculated and stored in cache) print(power(2, 3)) # Expected output: 8 (Retrieved from cache) # Test partial application print(square(4)) # Expected output: 16 (4^2) print(square(5)) # Expected output: 25 (5^2) ``` Constraints: - Use Python 3.9 or later versions - Be mindful of the performance requirements. Utilize caching effectively. Hints: - Think about where to place the `@lru_cache` decorator. - Use `functools.partial` to create the `square` function.","solution":"from functools import lru_cache, partial @lru_cache(maxsize=None) def power(base: int, exp: int) -> int: Returns the result of base raised to the power of exp (base ** exp). return base ** exp # Partially apply the power function to always use 2 as the exponent square = partial(power, exp=2)"},{"question":"**Question: Shared Memory Management in Python** You are required to design a system that uses shared memory to exchange data between multiple processes. Your task is to implement a function that creates a shared memory block, populates it with some data, and demonstrates how another process can access and modify the data. # Function Specification **Function Name**: `shared_memory_example` **Input**: - An integer `n` where `1 <= n <= 10^6` representing the size of the data to be written to shared memory. **Output**: - The function should return a list of two elements: - A list of integers showing the modified data after access by the second process. - A string representing the name of the shared memory block used. # Constraints - You should ensure proper cleanup of resources by closing and unlinking the shared memory blocks. - Managing any exceptions or errors that may occur during shared memory operations. # Example ```python def shared_memory_example(n): from multiprocessing import Process, shared_memory import numpy as np def writer(shm_name): existing_shm = shared_memory.SharedMemory(name=shm_name) data = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) data[:] = np.arange(n) + 1 existing_shm.close() def reader_modifier(shm_name): existing_shm = shared_memory.SharedMemory(name=shm_name) data = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) data *= -1 # Modify the array by negating each element result = data.tolist() existing_shm.close() return result # Create shared memory block shm = shared_memory.SharedMemory(create=True, size=n * np.dtype(np.int64).itemsize) data = np.ndarray((n,), dtype=np.int64, buffer=shm.buf) data[:] = np.arange(n) writer_process = Process(target=writer, args=(shm.name,)) writer_process.start() writer_process.join() reader_process = Process(target=reader_modifier, args=(shm.name,)) reader_process.start() reader_process.join() result = reader_modifier(shm.name) # Reading modified data shm.close() shm.unlink() return [result, shm.name] # Example usage: print(shared_memory_example(5)) ``` # Notes 1. The `shared_memory_example` function first uses the `writer` process to populate the shared memory with integers 1 to n. 2. Then, it uses the `reader_modifier` process to modify the shared memory by negating each element. 3. Finally, it reads and returns the modified data, along with the name of the shared memory block. Implement the function `shared_memory_example` to ensure data exchange between processes using shared memory. Make sure to handle resource cleanup correctly.","solution":"from multiprocessing import Process, shared_memory import numpy as np def shared_memory_example(n): Creates a shared memory block, populates it with data, modifies the data via a separate process, and returns the modified data and shared memory name. Args: n (int): Size of the data to be written to shared memory (1 <= n <= 10^6). Returns: List: A list containing - A list of integers showing the modified data - A string representing the name of the shared memory block used def writer(shm_name): existing_shm = shared_memory.SharedMemory(name=shm_name) data = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) data[:] = np.arange(n) + 1 existing_shm.close() def reader_modifier(shm_name): existing_shm = shared_memory.SharedMemory(name=shm_name) data = np.ndarray((n,), dtype=np.int64, buffer=existing_shm.buf) data *= -1 # Modify the array by negating each element existing_shm.close() # Create shared memory block shm = shared_memory.SharedMemory(create=True, size=n * np.dtype(np.int64).itemsize) data = np.ndarray((n,), dtype=np.int64, buffer=shm.buf) data[:] = np.arange(n) # Start writer process to populate data writer_process = Process(target=writer, args=(shm.name,)) writer_process.start() writer_process.join() # Start reader process to modify data reader_process = Process(target=reader_modifier, args=(shm.name,)) reader_process.start() reader_process.join() # Read modified data result_data = np.ndarray((n,), dtype=np.int64, buffer=shm.buf).tolist() # Cleanup shm.close() shm.unlink() return [result_data, shm.name]"},{"question":"Coding Assessment Question # Objective: Design a function named `serial_port_communication` to manage communication over a serial port using terminal I/O control. The function should demonstrate the following capabilities: 1. Configure the terminal settings for non-canonical mode input. 2. Periodically flush the input and output queues. 3. Send a break signal after transmitting data. 4. Restore original terminal settings before exiting. # Specifications: Function Signature ```python def serial_port_communication(fd: int, message: str, interval: float) -> None: pass ``` Input: - `fd`: An integer file descriptor for the serial port (e.g., obtained from `sys.stdin.fileno()` or similar). - `message`: A string message to be sent over the serial port. - `interval`: A float interval in seconds specifying how often to flush the queues. Output: - The function should not return any value. It performs operations on the terminal and the serial port. Constraints: - The function must ensure that any terminal settings changed during its execution are restored before exiting, even if an error occurs. - The function must handle terminal control commands and exception handling properly. - The function should use `termios` module functions to achieve the task. # Example Usage: ```python import sys import termios import time def serial_port_communication(fd: int, message: str, interval: float) -> None: # Implement the function based on the requirements pass # Usage Example fd = sys.stdin.fileno() serial_port_communication(fd, \\"Test message\\", 1.0) ``` # Performance Requirements: - Since the function might be interacting with hardware in real-time, ensure it handles any IO delays appropriately. # Additional Information: - Look into `termios.tcgetattr()`, `termios.tcsetattr()`, `termios.tcflush()`, and `termios.tcsendbreak()` for the terminal control operations. - Consider using `time.sleep(interval)` to manage the periodic flushing of the queues.","solution":"import termios import time import sys def serial_port_communication(fd: int, message: str, interval: float) -> None: Manage communication over a serial port using terminal I/O control. # Get the current terminal settings old_settings = termios.tcgetattr(fd) try: # Set the terminal to non-canonical mode new_settings = termios.tcgetattr(fd) new_settings[3] = new_settings[3] & ~termios.ICANON termios.tcsetattr(fd, termios.TCSANOW, new_settings) # Send the message sys.stdout.write(message) sys.stdout.flush() # Periodically flush the input and output queues time.sleep(interval) termios.tcflush(fd, termios.TCIOFLUSH) # Send a break signal termios.tcsendbreak(fd, 0) finally: # Restore the original terminal settings termios.tcsetattr(fd, termios.TCSANOW, old_settings)"},{"question":"# Coding Problem: Advanced Bar Plot Customization with Seaborn You are tasked with analyzing flight data to compare passenger numbers across different years and months. Additionally, you will highlight specific insights by annotating the bar plots. Dataset You will use the `flights` dataset available through the Seaborn library. ```python import seaborn as sns flights = sns.load_dataset(\\"flights\\") ``` Tasks 1. **Basic Bar Plot** - Create a basic bar plot showing the average number of passengers for each year using the `flights` dataset. The x-axis should represent the years and the y-axis should represent the average number of passengers. 2. **Grouped Bar Plot** - Create a grouped bar plot that shows the average number of passengers for each month over the years. The x-axis should represent the months, the y-axis should represent the average number of passengers, and there should be bars for each year using different colors. 3. **Facet Plot with Custom Annotations** - Create a facet plot that shows the number of passengers each month, faceted by decade (i.e., 1940s, 1950s...). Each subplot should show a bar plot with months on the x-axis and total number of passengers on the y-axis. Add a text label on each bar indicating the total number of passengers for that month. 4. **Customized Appearance** - Create a customized bar plot from the previous steps, showing: - Vertically oriented bars with confidence intervals as error bars. - Error bars representing standard deviation. - Bars with a custom color palette. - Custom annotations to highlight the bar with the maximum average passenger number for each subplot. Function Signatures ```python def plot_average_passengers_per_year(): Generates a basic bar plot showing the average number of passengers for each year. pass def plot_grouped_passengers_by_month(): Generates a grouped bar plot showing the average number of passengers for each month over the years. pass def plot_facetted_passengers_by_decade(): Generates a facet plot showing the number of passengers each month, faceted by decade. pass def plot_customized_passenger_bars(): Generates a customized bar plot with confidence intervals and annotations. pass ``` Constraints - Use the Seaborn and Matplotlib libraries for plotting. - Ensure the plots are well-labeled and include a legend where appropriate. - You may use helper functions as needed to structure your code. Example Output - For task 1, a bar plot with years on the x-axis and average passengers on the y-axis. - For task 2, a grouped bar plot with months on the x-axis, average passengers on the y-axis, and different colors representing different years. - For task 3, a facet plot with separate subplots for each decade. - For task 4, a highly customized bar plot that includes error bars, annotations, and a distinct color palette. Each function should display the corresponding plot when called.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def plot_average_passengers_per_year(): flights = sns.load_dataset(\\"flights\\") avg_passengers_per_year = flights.groupby(\'year\')[\'passengers\'].mean().reset_index() plt.figure(figsize=(10, 6)) sns.barplot(x=\'year\', y=\'passengers\', data=avg_passengers_per_year, palette=\'viridis\') plt.title(\'Average Number of Passengers per Year\') plt.xlabel(\'Year\') plt.ylabel(\'Average Number of Passengers\') plt.xticks(rotation=45) plt.show() def plot_grouped_passengers_by_month(): flights = sns.load_dataset(\\"flights\\") avg_passengers_per_month_year = flights.groupby([\'year\', \'month\'])[\'passengers\'].mean().reset_index() plt.figure(figsize=(12, 8)) sns.barplot(x=\'month\', y=\'passengers\', hue=\'year\', data=avg_passengers_per_month_year, palette=\'rocket\') plt.title(\'Average Number of Passengers per Month for Each Year\') plt.xlabel(\'Month\') plt.ylabel(\'Average Number of Passengers\') plt.legend(title=\'Year\', bbox_to_anchor=(1.05, 1)) plt.show() def plot_facetted_passengers_by_decade(): flights = sns.load_dataset(\\"flights\\") flights[\'decade\'] = (flights[\'year\'] // 10) * 10 passengers_by_month_decade = flights.groupby([\'decade\', \'month\'])[\'passengers\'].sum().reset_index() g = sns.FacetGrid(passengers_by_month_decade, col=\'decade\', col_wrap=3, height=4, sharey=False) g.map_dataframe(sns.barplot, x=\'month\', y=\'passengers\', palette=\'mako\') for ax in g.axes.flat: for p in ax.patches: ax.annotate(f\'{int(p.get_height())}\', (p.get_x() + p.get_width() / 2., p.get_height()), ha=\'center\', va=\'center\', xytext=(0, 9), textcoords=\'offset points\') g.set_titles(\\"{col_name}\\") g.set_axis_labels(\\"Month\\", \\"Total Number of Passengers\\") g.fig.suptitle(\'Total Passengers per Month by Decade\', size=16) plt.subplots_adjust(top=0.9) plt.show() def plot_customized_passenger_bars(): flights = sns.load_dataset(\\"flights\\") flights[\'decade\'] = (flights[\'year\'] // 10) * 10 avg_passengers_by_month_decade = flights.groupby([\'decade\', \'month\'])[\'passengers\'].agg([\'mean\', \'std\']).reset_index() g = sns.FacetGrid(avg_passengers_by_month_decade, col=\'decade\', col_wrap=3, height=4, sharey=False) g.map_dataframe(sns.barplot, x=\'month\', y=\'mean\', ci=\'sd\', palette=\'flare\', edgecolor=\'w\') for ax in g.axes.flat: data = ax.get_lines()[0].get_data() max_y = max(data[1]) max_idx = np.argmax(data[1]) max_x = data[0][max_idx] for p in ax.patches: ax.annotate(f\'{int(p.get_height())}\', (p.get_x() + p.get_width() / 2., p.get_height()), ha=\'center\', va=\'center\', xytext=(0, 9), textcoords=\'offset points\') ax.annotate(f\'Max\', (max_x, max_y), xytext=(0, 15), textcoords=\'offset points\', arrowprops=dict(facecolor=\'black\', shrink=0.05), ha=\'center\', bbox=dict(boxstyle=\\"round,pad=0.3\\", edgecolor=\'grey\')) g.set_titles(\\"{col_name}\\") g.set_axis_labels(\\"Month\\", \\"Average Number of Passengers\\") g.fig.suptitle(\'Customized Average Passengers per Month by Decade with Annotations\', size=16) plt.subplots_adjust(top=0.9) plt.show()"},{"question":"# Custom File Reader In this task, you are required to implement a custom file reader in Python. Your implementation will make use of the `builtins` module to leverage Python\'s built-in file handling capabilities but with added custom behavior. # Problem Statement 1. Create a class `LineNumberReader` that wraps around a file object and provides the following methods: - `__init__(self, file)`: Initializes the instance with a file object. - `readline(self)`: Reads the next line from the file and returns it prefixed with the current line number (starting at 1). - `readlines(self)`: Reads all lines from the file and returns them as a list, each prefixed with the line number. - Line number prefix format should be: \\"Line n: \\", where `n` is the line number. 2. Implement a function `open_with_line_numbers(path)` that: - Opens a file at the given `path` for reading using the built-in `open()` function. - Returns an instance of `LineNumberReader` wrapping the open file object. # Example Given a file `example.txt` with contents: ``` Hello, world! Python is awesome. ``` ```python reader = open_with_line_numbers(\'example.txt\') print(reader.readline()) # \\"Line 1: Hello, world!n\\" print(reader.readlines()) # [\\"Line 2: Python is awesome.n\\"] ``` # Constraints - You should not modify the file content on disk. - The filenames and paths provided will be valid and accessible. - The focus is on reading functionality; writing or modifying file content is not required. - The solution must correctly handle large files efficiently. # Submission Implement the class `LineNumberReader` and the function `open_with_line_numbers(path)` below: ```python import builtins class LineNumberReader: def __init__(self, file): self.file = file self.line_number = 0 def readline(self): line = self.file.readline() if line: self.line_number += 1 return f\\"Line {self.line_number}: {line}\\" else: return \\"\\" def readlines(self): lines = [] for line in self.file: self.line_number += 1 lines.append(f\\"Line {self.line_number}: {line}\\") return lines def open_with_line_numbers(path): file = builtins.open(path, \'r\') return LineNumberReader(file) ``` Provide test cases to ensure the correctness of your implementation.","solution":"import builtins class LineNumberReader: def __init__(self, file): self.file = file self.line_number = 0 def readline(self): line = self.file.readline() if line: self.line_number += 1 return f\\"Line {self.line_number}: {line}\\" else: return \\"\\" def readlines(self): lines = [] for line in self.file: self.line_number += 1 lines.append(f\\"Line {self.line_number}: {line}\\") return lines def open_with_line_numbers(path): file = builtins.open(path, \'r\') return LineNumberReader(file)"},{"question":"# PyTorch dtype Properties Assessment Objective Your task is to demonstrate your understanding of PyTorch\'s numerical properties of dtypes using the `torch.finfo` and `torch.iinfo` classes. You will write functions to extract and compare these properties for given dtypes. Problem 1. **Function `get_floating_point_info`**: - **Input**: A PyTorch floating-point dtype (`torch.dtype`). - **Output**: A dictionary containing the properties of the dtype: ```python { \\"bits\\": int, \\"eps\\": float, \\"max\\": float, \\"min\\": float, \\"tiny\\": float, \\"smallest_normal\\": float, \\"resolution\\": float } ``` 2. **Function `get_integer_info`**: - **Input**: A PyTorch integer dtype (`torch.dtype`). - **Output**: A dictionary containing the properties of the dtype: ```python { \\"bits\\": int, \\"max\\": int, \\"min\\": int } ``` 3. **Function `compare_dtypes`**: - **Input**: Two PyTorch dtypes (`torch.dtype`), either both floating-point or both integer types. - **Output**: A dictionary comparing the properties of the two dtypes: ```python { \\"bits_comparison\\": str, # \\"same\\" if both have the same number of bits, \\"different\\" otherwise \\"max_comparison\\": str, # \\"greater\\", \\"less\\" or \\"equal\\" based on max value comparison \\"min_comparison\\": str, # \\"greater\\", \\"less\\" or \\"equal\\" based on min value comparison } ``` - **Note**: For floating-point dtypes, compare the `max` and `min` values; for integer dtypes, compare the `max` and `min` values. Constraints - You are allowed to use only PyTorch library functionalities. - Consider edge cases where input dtypes may not be valid for the respective function. - Ensure your solution is efficient and follows best coding practices. Example Usage ```python import torch # Floating point info dtype = torch.float32 float_info = get_floating_point_info(dtype) print(float_info) # Output example (values are for illustration and may vary): # { # \\"bits\\": 32, # \\"eps\\": 1.1920928955078125e-07, # \\"max\\": 3.4028234663852886e+38, # \\"min\\": -3.4028234663852886e+38, # \\"tiny\\": 1.1754943508222875e-38, # \\"smallest_normal\\": 1.1754943508222875e-38, # \\"resolution\\": 1e-06 # } # Integer info dtype = torch.int32 int_info = get_integer_info(dtype) print(int_info) # Output example: # { # \\"bits\\": 32, # \\"max\\": 2147483647, # \\"min\\": -2147483648 # } # Compare dtypes dtype1 = torch.float32 dtype2 = torch.float64 comparison = compare_dtypes(dtype1, dtype2) print(comparison) # Output example (values are for illustration and may vary): # { # \\"bits_comparison\\": \\"different\\", # \\"max_comparison\\": \\"equal\\", # \\"min_comparison\\": \\"equal\\" # } ``` Ensure you handle all edge cases and validate the input dtypes before proceeding with property extraction and comparison.","solution":"import torch def get_floating_point_info(dtype): Returns a dictionary with properties of the floating-point dtype. finfo = torch.finfo(dtype) return { \\"bits\\": finfo.bits, \\"eps\\": finfo.eps, \\"max\\": finfo.max, \\"min\\": finfo.min, \\"tiny\\": finfo.tiny, \\"smallest_normal\\": finfo.smallest_normal, \\"resolution\\": finfo.resolution } def get_integer_info(dtype): Returns a dictionary with properties of the integer dtype. iinfo = torch.iinfo(dtype) return { \\"bits\\": iinfo.bits, \\"max\\": iinfo.max, \\"min\\": iinfo.min } def compare_dtypes(dtype1, dtype2): Compares properties of the two given dtypes and returns a dictionary with the comparison results. if dtype1.is_floating_point and dtype2.is_floating_point: info1 = get_floating_point_info(dtype1) info2 = get_floating_point_info(dtype2) elif dtype1.is_complex or dtype2.is_complex: raise ValueError(\\"Complex dtypes are not supported for comparison.\\") elif dtype1.is_floating_point != dtype2.is_floating_point: raise ValueError(\\"Cannot compare integer with floating point dtype.\\") else: info1 = get_integer_info(dtype1) info2 = get_integer_info(dtype2) bits_comparison = \\"same\\" if info1[\\"bits\\"] == info2[\\"bits\\"] else \\"different\\" max_comparison = ( \\"equal\\" if info1[\\"max\\"] == info2[\\"max\\"] else \\"greater\\" if info1[\\"max\\"] > info2[\\"max\\"] else \\"less\\" ) min_comparison = ( \\"equal\\" if info1[\\"min\\"] == info2[\\"min\\"] else \\"greater\\" if info1[\\"min\\"] > info2[\\"min\\"] else \\"less\\" ) return { \\"bits_comparison\\": bits_comparison, \\"max_comparison\\": max_comparison, \\"min_comparison\\": min_comparison }"},{"question":"Objective: Design and implement a decision tree classifier using the scikit-learn library. The classifier should be able to handle a practical scenario of overfitting, pruning and should be tested on a real-world dataset. Problem Statement: You are given the famous Iris dataset. Your task is to implement a decision tree classifier that: 1. Initializes a decision tree classifier with specific parameters. 2. Trains the classifier on the provided dataset. 3. Prunes the tree to avoid overfitting. 4. Evaluates the performance of the classifier using appropriate metrics. Guidelines: 1. **Implementation**: - Import necessary libraries and load the Iris dataset. - Split the dataset into training and testing sets using an 80-20 split. - Initialize a `DecisionTreeClassifier` with parameters `random_state=42`, `min_samples_leaf=5`, and `ccp_alpha=0.01`. - Train the classifier on the training data. - Prune the tree using minimal cost-complexity pruning. 2. **Evaluation**: - Predict the class labels for the test data. - Calculate and print the accuracy, precision, recall, and F1-score of your model. - Visualize the pruned decision tree. 3. **Constraints**: - You must use the parameters given for initializing the decision tree. - Use `train_test_split` from `sklearn.model_selection` for splitting the dataset. - Use `classification_report` from `sklearn.metrics` to obtain the evaluation metrics. - For visualization, you can use `plot_tree` from `sklearn.tree`. 4. **Code Template**: ```python import numpy as np from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report # Step 1: Load the dataset iris = load_iris() X, y = iris.data, iris.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Initialize the decision tree classifier with the given parameters clf = DecisionTreeClassifier(random_state=42, min_samples_leaf=5, ccp_alpha=0.01) # Step 4: Train the classifier clf.fit(X_train, y_train) # Step 5: Predict the class labels for the test data y_pred = clf.predict(X_test) # Step 6: Evaluate the classifier\'s performance print(classification_report(y_test, y_pred)) # Step 7: Visualize the decision tree import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() ``` Expected Output: 1. The classification report showing accuracy, precision, recall, and F1-score. 2. A plot of the pruned decision tree. Submission: Submit your Python script with the implementation and the outputs as described in the guidelines.","solution":"import numpy as np from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier, plot_tree from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report def train_decision_tree(): # Step 1: Load the dataset iris = load_iris() X, y = iris.data, iris.target # Step 2: Split the dataset X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Step 3: Initialize the decision tree classifier with the given parameters clf = DecisionTreeClassifier(random_state=42, min_samples_leaf=5, ccp_alpha=0.01) # Step 4: Train the classifier clf.fit(X_train, y_train) # Step 5: Predict the class labels for the test data y_pred = clf.predict(X_test) # Step 6: Evaluate the classifier\'s performance report = classification_report(y_test, y_pred, output_dict=True) # Step 7: Visualize the decision tree import matplotlib.pyplot as plt plt.figure(figsize=(20,10)) plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names) plt.show() return clf, report clf, report = train_decision_tree() print(report)"},{"question":"**Objective:** Implement a function that dynamically handles warnings in a specified manner using the `warnings` module in Python. **Problem Statement:** Write a function `manage_warnings` that performs the following tasks: 1. **Temporarily Suppresses Specific Warnings:** - The function should accept two arguments: a list of warning categories to suppress (`suppress_categories`), and a list of warning categories to allow (`allow_categories`). Both arguments will be lists of warning class names as strings. 2. **Emit Warnings:** - Define and emit warnings belonging to the categories specified in both `suppress_categories` and `allow_categories`. For testing purposes, you can use the built-in warning classes available in the `warnings` module. 3. **Context Management:** - Implement a context manager that temporarily suppresses the warnings in the `suppress_categories` while allowing the warnings in `allow_categories` to be displayed when the context is active. 4. **Assertions:** - After the context manager exits, the function should verify that the suppressed warnings are not shown and the allowed warnings are shown during the context manager\'s execution. **Function Signature:** ```python import warnings from typing import List def manage_warnings(suppress_categories: List[str], allow_categories: List[str]) -> None: ... ``` **Example Usage:** ```python def manage_warnings(suppress_categories, allow_categories): with ... : # Implement your context manager here if \'DeprecationWarning\' in suppress_categories: warnings.warn(\\"This is a deprecated feature\\", DeprecationWarning) if \'UserWarning\' in allow_categories: warnings.warn(\\"This is a user warning\\", UserWarning) # Check which warnings were shown during context manager execution # Example test case suppress = [\'DeprecationWarning\'] allow = [\'UserWarning\'] manage_warnings(suppress, allow) ``` **Constraints:** - The function should not use any I/O operations like `print`. - Only standard library modules are allowed. **Notes:** - This problem tests your understanding of the `warnings` module, handling and suppressing warnings, context management, and assertions. - Make sure your implementation correctly identifies and handles the specified warnings according to the suppression and allowance rules provided as input.","solution":"import warnings from contextlib import contextmanager from typing import List @contextmanager def manage_warnings(suppress_categories: List[str], allow_categories: List[str]): with warnings.catch_warnings(record=True) as w: # Construct filters for suppressing and allowing for category in suppress_categories: warnings.filterwarnings(\\"ignore\\", category=eval(category)) for category in allow_categories: warnings.filterwarnings(\\"default\\", category=eval(category)) yield w # Yield the list of warnings captured during this context"},{"question":"**Objective**: You are required to write a Python script that demonstrates the usage of the `http.cookiejar` module by performing the following tasks: 1. **Loading Cookies**: - Load cookies from a file (`cookies.txt`) using a `MozillaCookieJar`. 2. **Interacting with a Website**: - Use the loaded cookies to open a specified URL (`http://example.com`). 3. **Modifying Cookie Policy**: - Apply a `DefaultCookiePolicy` that accepts RFC 2965 cookies and has specific blocked domains (`ads.example.com` and `.tracker.com`). 4. **Saving Cookies**: - After the interaction with the website, save the updated cookies back to the file (`cookies.txt`). # Requirements: 1. **Input**: - The script should take no inputs except for the initial setup that reads from `cookies.txt`. 2. **Output**: - Print out the names of the cookies after loading them and after interacting with the website. - Save the updated cookies back to the `cookies.txt`. 3. **Constraints**: - Ensure the code handles errors gracefully (e.g., file not found). - Use the `http.cookiejar`\'s `MozillaCookieJar`, `DefaultCookiePolicy`, and `HTTPCookieProcessor`. 4. **Performance**: - The script should efficiently load, process, and save cookies without unnecessary overhead. # Example: The following is an example of how the initial setup might look: ```python import os import http.cookiejar import urllib.request # Load cookies cookie_file = os.path.join(os.path.expanduser(\\"~\\"), \\"cookies.txt\\") cookie_jar = http.cookiejar.MozillaCookieJar() try: cookie_jar.load(cookie_file) print(\\"Cookies loaded:\\", [cookie.name for cookie in cookie_jar]) except http.cookiejar.LoadError: print(\\"No existing cookie file found. Starting with an empty cookie jar.\\") # Set up policy policy = http.cookiejar.DefaultCookiePolicy(rfc2965=True, blocked_domains=[\\"ads.example.com\\", \\".tracker.com\\"]) cookie_jar.set_policy(policy) # Open URL opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) response = opener.open(\\"http://example.com\\") # Print cookies after interaction print(\\"Updated cookies:\\", [cookie.name for cookie in cookie_jar]) # Save cookies cookie_jar.save(cookie_file) ``` Complete the script to handle all requirements and edge cases.","solution":"import os import http.cookiejar import urllib.request def manage_cookies(cookie_file, url): Load cookies from a file, interact with a specified URL, modify the cookie policy, and then save the cookies back to the file. :param cookie_file: Path to the cookie file. :param url: URL to interact with using the loaded cookies. :return: A tuple of lists containing the cookie names before and after interacting with the URL. # Load cookies cookie_jar = http.cookiejar.MozillaCookieJar(cookie_file) try: cookie_jar.load(ignore_discard=True, ignore_expires=True) cookies_before = [cookie.name for cookie in cookie_jar] except FileNotFoundError: cookies_before = [] # Set up policy policy = http.cookiejar.DefaultCookiePolicy(rfc2965=True, blocked_domains=[\\"ads.example.com\\", \\".tracker.com\\"]) cookie_jar.set_policy(policy) # Open URL opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookie_jar)) opener.open(url) # Extract updated cookie names cookies_after = [cookie.name for cookie in cookie_jar] # Save cookies cookie_jar.save(cookie_file, ignore_discard=True, ignore_expires=True) return cookies_before, cookies_after"},{"question":"**Coding Assessment Question** # Objective Implement a function to apply a causal attention bias to a given attention mask within a transformer model. Ensure that the function correctly modifies the attention mask to prevent any future information from being seen by each token. # Problem Statement You need to write a function `apply_causal_bias` that applies a causal bias to the given attention mask. The function should utilize the `CausalBias` class or the provided utilities from the `torch.nn.attention.bias` module. # Function Signature ```python def apply_causal_bias(attention_mask: torch.Tensor, method: str = \\"upper_left\\") -> torch.Tensor: pass ``` # Input - `attention_mask` (torch.Tensor): A 2D tensor of shape `(seq_len, seq_len)` where `seq_len` is the sequence length. The tensor contains 1s or 0s indicating allowed (1) or disallowed (0) attentions. - `method` (str, optional): A string indicating the method of causal bias application. It could be either `\\"upper_left\\"` or `\\"lower_right\\"`. Default is `\\"upper_left\\"`. # Output - Returns a `torch.Tensor` of the same shape as `attention_mask`, modified to enforce causal attention constraints. # Constraints - Ensure the `attention_mask` retains its original values where possible and only modifies the mask to enforce causality. - You should handle cases where the `method` value might be incorrect by raising a `ValueError`. # Example Given the following `attention_mask`: ```plaintext attention_mask = torch.tensor([ [1, 1, 1], [1, 1, 1], [1, 1, 1] ]) ``` 1. `apply_causal_bias(attention_mask, method=\\"upper_left\\")` should return: ```plaintext [ [1, 0, 0], [1, 1, 0], [1, 1, 1] ] ``` 2. `apply_causal_bias(attention_mask, method=\\"lower_right\\")` should return: ```plaintext [ [1, 1, 1], [0, 1, 1], [0, 0, 1] ] ``` # Notes - Use the `direct_import` method, such as `from torch.nn.attention.bias import ...`, to import necessary classes or functions. - Aim to write clean, readable code with comments explaining key steps. - Ensure your solution passes basic unit tests for validation. # Performance Requirements - Your function should be efficient and capable of handling sequence lengths typically used in transformer models, i.e., up to 512 tokens, within reasonable time limits.","solution":"import torch def apply_causal_bias(attention_mask: torch.Tensor, method: str = \\"upper_left\\") -> torch.Tensor: Applies a causal bias to the given attention mask. Parameters: - attention_mask (torch.Tensor): A 2D tensor of shape (seq_len, seq_len). - method (str): Method for applying causal bias, either \\"upper_left\\" or \\"lower_right\\". Returns: - torch.Tensor: The modified attention mask with causal constraints. Raises: - ValueError: If the method is not \\"upper_left\\" or \\"lower_right\\". seq_len = attention_mask.shape[0] if method == \\"upper_left\\": for i in range(seq_len): for j in range(i + 1, seq_len): attention_mask[i, j] = 0 elif method == \\"lower_right\\": for i in range(1, seq_len): for j in range(i): attention_mask[i, j] = 0 else: raise ValueError(f\\"Invalid method: {method}. Choose \'upper_left\' or \'lower_right\'.\\") return attention_mask"},{"question":"# MIME Type Utility Function You are tasked with building a utility function that will take in a list of file names or URLs, and return a dictionary summarizing the MIME types and encodings used. Function Specification Function Name: `summarize_mime_types` # Input: - `file_list`: A list of strings, each representing a file name or URL (e.g., `[\'example.png\', \'document.pdf\', \'archive.tar.gz\', \'unknown\']`). # Output: - A dictionary where: - Keys are unique MIME types inferred from the `file_list`. - Values are sets containing distinct encodings associated with each MIME type. If no encoding is found for a MIME type, the set should contain the string `\\"none\\"`. # Constraints: - You must use the `mimetypes` module for MIME type inference. - The function should handle at least 1000 entries in the input list efficiently. # Example: ```python import mimetypes def summarize_mime_types(file_list): # Your implementation here pass # Example usage files = [\'index.html\', \'image.jpeg\', \'archive.tar.gz\', \'video.mp4\', \'unknownfile\'] result = summarize_mime_types(files) print(result) ``` Expected output for the above function call might look like: ```python { \'text/html\': {\'none\'}, \'image/jpeg\': {\'none\'}, \'application/x-tar\': {\'gzip\'}, \'video/mp4\': {\'none\'}, \'application/octet-stream\': {\'none\'} } ``` # Notes: 1. Use the `guess_type()` function from the `mimetypes` module to determine the MIME type and encoding of each file. 2. If `guess_type()` returns `None` for the MIME type, treat it as `\'application/octet-stream\'`. 3. The returned dictionary should not contain any entries where the MIME type is `None`. Take care to handle edge cases, such as files without extensions and files with uncommon or non-standard extensions.","solution":"import mimetypes def summarize_mime_types(file_list): Summarizes the MIME types and encodings of the files in the given file list. :param file_list: A list of strings representing file names or URLs. :return: A dictionary summarizing MIME types and encodings. mime_summary = {} for file in file_list: mime_type, encoding = mimetypes.guess_type(file) if mime_type is None: mime_type = \'application/octet-stream\' # Default encoding to \'none\' if no encoding is found encoding = encoding if encoding is not None else \'none\' if mime_type not in mime_summary: mime_summary[mime_type] = set() mime_summary[mime_type].add(encoding) return mime_summary"},{"question":"# Advanced Python Asyncio Task Management Problem Statement You are tasked with implementing a function `async def monitor_tasks(tasks: List[awaitable], timeout: float) -> dict:`, which will handle multiple asyncio tasks concurrently. Your function should follow these guidelines: 1. Accepts a list of awaitable objects `tasks`, and a timeout value `timeout` (in seconds). 2. Runs all the tasks concurrently and waits for them to complete or until the given timeout. 3. If the timeout is reached before all tasks have completed, it should cancel the remaining tasks. 4. Return a dictionary with the following format: - `\\"completed\\"`: a list of results of the tasks which completed within the given timeout. - `\\"pending\\"`: a list of tasks that were still pending upon timeout or were cancelled. Input - `tasks`: List of awaitable objects. - `timeout`: A float representing the maximum number of seconds to wait for the tasks to complete. Output - A dictionary containing: - `\\"completed\\"`: List of results of successfully completed tasks. - `\\"pending\\"`: List of tasks that were still pending upon timeout or were cancelled. Example ```python import asyncio async def task1(): await asyncio.sleep(2) return \\"task1 done\\" async def task2(): await asyncio.sleep(4) return \\"task2 done\\" async def task3(): await asyncio.sleep(1) return \\"task3 done\\" tasks = [task1(), task2(), task3()] timeout = 3 result = await monitor_tasks(tasks, timeout) print(result) ``` Expected output: ```python { \\"completed\\": [\\"task1 done\\", \\"task3 done\\"], \\"pending\\": [<Task pending name=\'Task-2\' coro=<task2() running at...>] } ``` Constraints - The function must handle exceptions properly and ensure that all tasks are either completed or cancelled. - Assume all tasks are valid asyncio coroutines and use only the asyncio library for concurrency control. Performance Requirements - The function should efficiently manage the concurrent execution of tasks, ensuring minimal overhead in task scheduling and cancellation. # Additional Notes You should provide a robust implementation that follows best practices for working with asyncio and demonstrates a clear understanding of tasks, coroutines, timeouts, and cancellation.","solution":"import asyncio from typing import List, Awaitable, Dict async def monitor_tasks(tasks: List[Awaitable], timeout: float) -> Dict[str, List]: Monitor multiple asyncio tasks concurrently, wait for them to complete or until the given timeout. Return results of completed tasks and any pending tasks if the timeout is reached. :param tasks: List of awaitable objects (tasks) to run. :param timeout: Maximum number of seconds to wait for tasks completion. :return: Dictionary with keys \\"completed\\" and \\"pending\\" containing completed task results and pending tasks respectively. done, pending = await asyncio.wait(tasks, timeout=timeout) result = { \\"completed\\": [task.result() for task in done if not task.cancelled() and not task.exception()], \\"pending\\": list(pending) } # Cancel any tasks that are still pending after the timeout for task in pending: task.cancel() return result"},{"question":"Objective Your task is to implement a function that analyzes a given Python script to identify all the modules it imports directly and indirectly. You will utilize the `ModuleFinder` class from the `modulefinder` module provided by Python. Problem Statement Implement a function `find_script_modules(script_path: str, exclude_list: List[str] = []) -> Tuple[Dict[str, List[str]], List[str]]` that does the following: 1. Analyzes the provided Python script located at `script_path`. 2. Returns a tuple containing: - A dictionary where keys are module names and values are lists of the first three global names found in each module. - A list of module names that the script tried to import but were not found (i.e., missing modules). # Parameters: - `script_path` (str): The path to the Python script to be analyzed. - `exclude_list` (List[str]): An optional list of module names to exclude from the analysis. Defaults to an empty list. # Returns: - A tuple containing: - A dictionary with module names as keys and lists of up to three global names as values. - A list of missing module names that were attempted to be imported but not found. # Example: Given the following Python script `example.py`: ```python import os, sys try: import non_existent_module except ImportError: pass import re ``` Calling `find_script_modules(\'example.py\')` should return: ```python ({ \'os\': [], \'sys\': [], \'re\': [\'__module__\', \'finditer\', \'_expand\'], \'__main__\': [\'os\', \'sys\', \'non_existent_module\'] }, [\'non_existent_module\']) ``` # Constraints: - You may assume the script file at `script_path` is always a valid Python script. - You may not modify the script file. # Hint: - Use the `ModuleFinder` class from the `modulefinder` module for analysis. - Refer to the example usage provided in the `modulefinder` documentation for guidance. # Notes: - Ensure to handle potential exceptions that might occur when the script or module paths are invalid. Good luck!","solution":"import modulefinder from typing import List, Dict, Tuple def find_script_modules(script_path: str, exclude_list: List[str] = []) -> Tuple[Dict[str, List[str]], List[str]]: finder = modulefinder.ModuleFinder() try: finder.run_script(script_path) except Exception as e: raise RuntimeError(f\\"Error running script: {e}\\") # Dictionary to hold module information modules_info = {} # List to hold missing modules missing_modules = list(finder.badmodules.keys()) for mod_name, module in finder.modules.items(): if mod_name in exclude_list: continue global_names = list(module.globalnames.keys()) # Limit to the first three global names if available if len(global_names) > 3: global_names = global_names[:3] modules_info[mod_name] = global_names return modules_info, missing_modules"},{"question":"# Python Coding Assessment Question Objective: Write a function that reads the contents of a Python source file, tokenizes it, and performs specific transformations on certain tokens before reassembling the code. The objective is to replace all usages of the identifier \\"print\\" with \\"my_print\\", while keeping the rest of the code unchanged. Function Signature: ```python def transform_print_statements(file_path: str) -> str: pass ``` Input: * `file_path` (str): Path to the Python source file to be transformed. Output: * `transformed_code` (str): A string containing the transformed Python source code. Constraints: 1. Assume the input file exists and is syntactically correct Python. 2. The input file\'s encoding should be detected and preserved. 3. The function should handle cases where `print` is used in various locations such as functions, nested functions, and different scopes. Example: Consider the contents of the file at `file_path` are: ```python def greet(): print(\\"Hello, World!\\") greet() ``` The expected output should be: ```python def greet(): my_print(\\"Hello, World!\\") greet() ``` Additional Information: - Use the `tokenize` module\'s capabilities to perform tokenization and untokenization. - The transformation should ensure that only `print` statements are changed and any other usage of similar names or functions is left unchanged. Notes: - You might find the `tokenize.tokenize()` and `tokenize.untokenize()` functions particularly useful. - Make sure to appropriately handle encoding using `tokenize.detect_encoding()`.","solution":"import tokenize def transform_print_statements(file_path: str) -> str: Transforms all instances of \'print\' into \'my_print\' in the provided Python source file. Args: file_path (str): The path to the Python source file to be transformed. Returns: str: The transformed Python source code as a string. with open(file_path, \'rb\') as f: encoding, _ = tokenize.detect_encoding(f.readline) f.seek(0) tokens = tokenize.tokenize(f.readline) transformed_tokens = [] for token in tokens: if token.type == tokenize.NAME and token.string == \'print\': transformed_tokens.append(token._replace(string=\'my_print\')) else: transformed_tokens.append(token) transformed_code = tokenize.untokenize(transformed_tokens).decode(encoding) return transformed_code"},{"question":"**Objective:** Write a Python class that mimics the behavior of a sequence object similar to a simplified version of Python\'s built-in list. Your class should support the most common sequence operations using efficient implementations. You must use the provided methods in the `python310` package where appropriate to demonstrate your understanding of sequence operations. **Class Name:** `CustomSequence` **Methods:** 1. `__init__(self, initial=None)`: Initializes the sequence with the given initial values (if provided). Default is an empty list. 2. `__len__(self)`: Returns the length of the sequence. 3. `__getitem__(self, index)`: Retrieves the item at the given index. 4. `__setitem__(self, index, value)`: Sets the value at the given index. 5. `__delitem__(self, index)`: Deletes the item at the given index. 6. `append(self, value)`: Appends the value to the end of the sequence. 7. `count(self, value)`: Counts the occurrences of the value in the sequence. 8. `index(self, value)`: Returns the index of the first occurrence of the value. 9. `__str__(self)`: Returns a string representation of the sequence. **Constraints:** - The `CustomSequence` class should only store integers. - You are not allowed to use Python\'s built-in list methods except for iteration. - Efficiency matters: the operations should have time complexity similar to those of built-in list operations. **Example Usage:** ```python seq = CustomSequence([1, 2, 3, 4, 5]) print(len(seq)) # Output: 5 print(seq[2]) # Output: 3 seq[2] = 10 print(seq[2]) # Output: 10 del seq[2] print(seq) # Output: CustomSequence([1, 2, 4, 5]) seq.append(6) print(seq) # Output: CustomSequence([1, 2, 4, 5, 6]) print(seq.count(2)) # Output: 1 print(seq.index(4)) # Output: 2 ``` **Additional Requirements:** - Ensure your implementation handles edge cases, such as inserting into or deleting from an empty sequence. - Implement proper error handling for invalid operations (e.g., accessing out-of-range indices). **Submission:** Submit your `CustomSequence` class implementation.","solution":"class CustomSequence: def __init__(self, initial=None): self.data = initial if initial is not None else [] def __len__(self): count = 0 for _ in self.data: count += 1 return count def __getitem__(self, index): if index < 0 or index >= len(self): raise IndexError(\\"Index out of range\\") return self.data[index] def __setitem__(self, index, value): if index < 0 or index >= len(self): raise IndexError(\\"Index out of range\\") self.data[index] = value def __delitem__(self, index): if index < 0 or index >= len(self): raise IndexError(\\"Index out of range\\") del self.data[index] def append(self, value): self.data += [value] def count(self, value): count = 0 for item in self.data: if item == value: count += 1 return count def index(self, value): for idx, item in enumerate(self.data): if item == value: return idx raise ValueError(f\\"{value} is not in sequence\\") def __str__(self): return f\\"CustomSequence({self.data})\\""},{"question":"**Question: Converting and Processing Tensors using DLPack and PyTorch** Given a tensor in DLPack format, you are required to perform the following steps: 1. Convert the DLPack tensor to a PyTorch tensor using `torch.utils.dlpack.from_dlpack`. 2. Perform a series of operations on the PyTorch tensor: - Normalize the tensor such that its values range between 0 and 1. - Compute the element-wise square of the normalized tensor. - Compute the transpose of the resulting tensor. 3. Convert the final PyTorch tensor back to DLPack format using `torch.utils.dlpack.to_dlpack`. **Function Signature:** ```python def process_dlpack_tensor(dlpack_tensor) -> \'dlpack_tensor\': pass ``` **Input:** - `dlpack_tensor`: A tensor in DLPack format. **Output:** - A tensor in DLPack format that is the result of the described operations. **Constraints:** - The input tensor can be of any shape, but it is guaranteed to have numerical values. - The operations should not involve any in-place modifications to ensure the integrity of the original data. **Examples:** Assume `input_dlpack_tensor` is a DLPack tensor corresponding to `[[1, 2], [3, 4]]` in PyTorch. ```python output_dlpack_tensor = process_dlpack_tensor(input_dlpack_tensor) # when converting output_dlpack_tensor back to PyTorch tensor print(output_pytorch_tensor) # Output: tensor([[0.0000, 0.4444], # [0.1111, 1.0000]]) ``` **Explanation:** 1. The input tensor `[[1, 2], [3, 4]]` is normalized to `[[0.0, 0.3333], [0.6667, 1.0]]`. 2. Squaring element-wise results in `[[0.0, 0.1111], [0.4444, 1.0]]`. 3. Transposing the tensor yields `[[0.0000, 0.4444], [0.1111, 1.0000]]`. Write the `process_dlpack_tensor` function to achieve this.","solution":"import torch def process_dlpack_tensor(dlpack_tensor): Given a tensor in DLPack format, convert it to a PyTorch tensor, perform normalization, square it, transpose it, and convert back to DLPack format. # 1. Convert DLPack tensor to PyTorch tensor pytorch_tensor = torch.utils.dlpack.from_dlpack(dlpack_tensor) # 2. Normalize the tensor between 0 and 1 normalized_tensor = (pytorch_tensor - pytorch_tensor.min()) / (pytorch_tensor.max() - pytorch_tensor.min()) # 3. Square the normalized tensor element-wise squared_tensor = normalized_tensor ** 2 # 4. Transpose the squared tensor transposed_tensor = squared_tensor.transpose(0, 1) # 5. Convert the final PyTorch tensor back to DLPack format final_dlpack_tensor = torch.utils.dlpack.to_dlpack(transposed_tensor) return final_dlpack_tensor"},{"question":"# Advanced Programming Assessment: Covariance Estimation with Scikit-Learn Objective Your task is to implement a function that estimates the covariance matrix of a given dataset using the Ledoit-Wolf shrinkage method, and compares it with the empirical covariance. Problem Statement Implement a function `estimate_covariances` that takes as input a dataset (2D NumPy array) and returns two covariance matrices: 1. The empirical covariance matrix using the `EmpiricalCovariance` class. 2. The Ledoit-Wolf shrinkage covariance matrix using the `LedoitWolf` class. Input - A 2D NumPy array `X` of shape `(n_samples, n_features)`, where `n_samples` is the number of data points and `n_features` is the number of features. Output - A tuple `(empirical_cov_matrix, lw_cov_matrix)`: - `empirical_cov_matrix`: A 2D NumPy array representing the empirical covariance matrix. - `lw_cov_matrix`: A 2D NumPy array representing the covariance matrix estimated using the Ledoit-Wolf shrinkage method. Constraints - You must not assume that the dataset is centered. - Input data may contain more features than samples. Example ```python import numpy as np # Example data X = np.array([[2.5, 0.5, 2.2], [1.9, 2.3, 3.1], [3.1, 3.2, 1.5], [2.3, 2.1, 2.8]]) empirical_cov_matrix, lw_cov_matrix = estimate_covariances(X) print(\\"Empirical Covariance Matrix:n\\", empirical_cov_matrix) print(\\"Ledoit-Wolf Covariance Matrix:n\\", lw_cov_matrix) ``` Expected Output: The exact output will depend on the specific data in `X`, but the function should return two distinct covariance matrices. Instructions 1. Use the `EmpiricalCovariance` class to compute the empirical covariance matrix. 2. Use the `LedoitWolf` class to compute the covariance matrix with the Ledoit-Wolf shrinkage. 3. Ensure the data is centered before computing the covariances (as necessary). Evaluation Criteria - Correctness: The function must correctly calculate both types of covariance matrices. - Use of scikit-learn: The solution should appropriately utilize the `EmpiricalCovariance` and `LedoitWolf` classes from `sklearn.covariance`. - Code quality: The solution should be well-structured, with clear and sufficient comments.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, LedoitWolf def estimate_covariances(X): Estimates covariance matrices using empirical method and Ledoit-Wolf shrinkage. Parameters: X (numpy.ndarray): 2D Input array of shape (n_samples, n_features). Returns: tuple: A tuple containing the empirical covariance matrix and the Ledoit-Wolf covariance matrix. # Initialize the EmpiricalCovariance and LedoitWolf estimators emp_cov = EmpiricalCovariance() lw_cov = LedoitWolf() # Fit the estimators to the data and get the covariance matrices emp_cov_matrix = emp_cov.fit(X).covariance_ lw_cov_matrix = lw_cov.fit(X).covariance_ return emp_cov_matrix, lw_cov_matrix"},{"question":"# AIFF Audio File Manipulation In this task, you will use the `aifc` module to read audio data from an existing AIFF/AIFF-C file, process it, and write the processed data to a new AIFF/AIFF-C file. This exercise will test your ability to manipulate audio file parameters and handle audio data effectively. Task 1. **Read an AIFF/AIFF-C Audio File:** Write a function `read_audio_file` that: - Takes a file path (`input_file`) as input. - Opens the file for reading using the `aifc` module. - Extracts and returns the following properties of the audio file: - Number of channels - Sample width - Frame rate - Number of frames - Compression type - Compression name - Raw audio data ```python def read_audio_file(input_file: str) -> tuple: pass ``` 2. **Process Audio Data:** Write a function `process_audio_data` that: - Takes raw audio data, number of channels, sample width, and frame rate as inputs. - Applies a simple transformation to the data (e.g., converts it to mono if it\'s stereo by averaging the channels). - Returns the processed audio data. ```python def process_audio_data(audio_data: bytes, nchannels: int, sampwidth: int, framerate: int) -> bytes: pass ``` 3. **Write to a New AIFF/AIFF-C Audio File:** Write a function `write_audio_file` that: - Takes the processed audio data, and other necessary audio properties (number of channels, sample width, frame rate, etc.) along with the output file path (`output_file`). - Writes this data to the specified output file using the `aifc` module. ```python def write_audio_file(output_file: str, audio_data: bytes, nchannels: int, sampwidth: int, framerate: int, comptype: bytes, compname: bytes) -> None: pass ``` 4. **Main Function:** Write a `main` function that: - Calls the `read_audio_file` function with the given input file path. - Calls the `process_audio_data` function with the data returned from the read function. - Calls the `write_audio_file` function to save the processed data into a new file. ```python def main(input_file: str, output_file: str): pass ``` Constraints: - The input and output files are AIFF/AIFF-C files. - The input file path is valid and the file is readable. - The output file path is writable. - The transformation in `process_audio_data` should be correctly implemented to demonstrate understanding of audio data manipulation. # Example Usage: ```python input_file = \\"example.aiff\\" output_file = \\"processed_example.aiff\\" main(input_file, output_file) ``` Performance Requirements: - The implementation should handle files up to 100 MB efficiently. - Ensure that the processing and writing of audio data are performant. Ensure the code is well-documented, and handle any possible exceptions that may occur during file operations.","solution":"import aifc def read_audio_file(input_file: str) -> tuple: Reads an AIFF/AIFF-C audio file and returns its properties and raw audio data. Parameters: - input_file (str): The path to the input AIFF/AIFF-C file. Returns: - tuple: A tuple containing the following elements: - Number of channels (int) - Sample width in bytes (int) - Frame rate (int) - Number of frames (int) - Compression type (bytes) - Compression name (bytes) - Raw audio data (bytes) with aifc.open(input_file, \'rb\') as file: nchannels = file.getnchannels() sampwidth = file.getsampwidth() framerate = file.getframerate() nframes = file.getnframes() comptype = file.getcomptype() compname = file.getcompname() audio_data = file.readframes(nframes) return (nchannels, sampwidth, framerate, nframes, comptype, compname, audio_data) def process_audio_data(audio_data: bytes, nchannels: int, sampwidth: int, framerate: int) -> bytes: Processes audio data to convert it to mono by averaging the channels. Parameters: - audio_data (bytes): The raw audio data. - nchannels (int): The number of channels in the audio data. - sampwidth (int): The sample width in bytes. - framerate (int): The frame rate of the audio data. Returns: - bytes: The processed audio data as mono. if nchannels == 1: return audio_data # Already mono import struct # Unpack the audio data fmt = \'<\' + \'h\' * (len(audio_data) // 2) audio_samples = struct.unpack(fmt, audio_data) # Process to mono mono_samples = [] for i in range(0, len(audio_samples), nchannels): average_sample = sum(audio_samples[i:i+nchannels]) // nchannels mono_samples.append(average_sample) # Pack the processed audio data fmt_mono = \'<\' + \'h\' * len(mono_samples) processed_data = struct.pack(fmt_mono, *mono_samples) return processed_data def write_audio_file(output_file: str, audio_data: bytes, nchannels: int, sampwidth: int, framerate: int, comptype: bytes, compname: bytes) -> None: Writes the processed audio data to an AIFF/AIFF-C file. Parameters: - output_file (str): The path to the output AIFF/AIFF-C file. - audio_data (bytes): The processed audio data. - nchannels (int): The number of channels in the audio data. - sampwidth (int): The sample width in bytes. - framerate (int): The frame rate of the audio data. - comptype (bytes): The compression type. - compname (bytes): The compression name. with aifc.open(output_file, \'wb\') as file: file.setnchannels(1) # Manually set to mono file.setsampwidth(sampwidth) file.setframerate(framerate) file.setcomptype(comptype, compname) file.writeframes(audio_data) def main(input_file: str, output_file: str): Main function to read, process, and write an AIFF/AIFF-C audio file. Parameters: - input_file (str): The path to the input AIFF/AIFF-C file. - output_file (str): The path to the output AIFF/AIFF-C file. nchannels, sampwidth, framerate, nframes, comptype, compname, audio_data = read_audio_file(input_file) processed_data = process_audio_data(audio_data, nchannels, sampwidth, framerate) write_audio_file(output_file, processed_data, nchannels, sampwidth, framerate, comptype, compname)"},{"question":"# Question: Implement and Evaluate a Machine Learning Pipeline with Dimensionality Reduction You are given a dataset `data.csv` which contains a collection of features. Your task is to design a machine learning pipeline that includes the following steps: 1. **Scaling**: Standardize the features. 2. **Dimensionality Reduction**: Apply Principal Component Analysis (PCA) to reduce the feature set to the top 5 principal components. 3. **Classification**: Train a Support Vector Machine (SVM) classifier on the reduced feature set and evaluate its performance. Steps to Implement: 1. **Load the dataset**: - The dataset file is named `data.csv`. - It contains a mix of features and a target column named `target`. 2. **Split the dataset into training and testing sets**: - Use 80% of the data for training and 20% for testing. 3. **Create a Scikit-Learn pipeline** including the following steps: - Standard Scaling using `StandardScaler`. - PCA transformation to reduce features to 5 principal components. - SVM classifier for prediction. 4. **Train the pipeline** on the training data. 5. **Evaluate the pipeline** on the testing data and print the accuracy of the classifier. Constraints: - You must use scikit-learn for all steps of the process. - Ensure reproducibility by setting a random state where applicable. Expected Functions: ```python def load_data(file_path: str) -> pd.DataFrame: Loads the data from the given CSV file and returns it as a DataFrame. pass def split_data(df: pd.DataFrame, test_size: float, random_state: int) -> Tuple[pd.DataFrame, pd.DataFrame]: Splits the data into training and testing sets. pass def create_pipeline() -> Pipeline: Creates a machine learning pipeline with StandardScaler, PCA, and SVM classifier. pass def train_and_evaluate_pipeline(pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> float: Trains the pipeline on the training data and evaluates it on the testing data. Returns the accuracy score. pass ``` Example Usage: ```python # Load the data df = load_data(\\"data.csv\\") # Split the data X_train, X_test, y_train, y_test = split_data(df, test_size=0.2, random_state=42) # Create the pipeline pipeline = create_pipeline() # Train and evaluate the pipeline accuracy = train_and_evaluate_pipeline(pipeline, X_train, y_train, X_test, y_test) print(f\\"Accuracy: {accuracy}\\") ```","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.pipeline import Pipeline def load_data(file_path: str) -> pd.DataFrame: Loads the data from the given CSV file and returns it as a DataFrame. return pd.read_csv(file_path) def split_data(df: pd.DataFrame, test_size: float, random_state: int): Splits the data into training and testing sets. X = df.drop(columns=[\'target\']) y = df[\'target\'] return train_test_split(X, y, test_size=test_size, random_state=random_state) def create_pipeline() -> Pipeline: Creates a machine learning pipeline with StandardScaler, PCA, and SVM classifier. return Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=5)), (\'svm\', SVC(random_state=42)) ]) def train_and_evaluate_pipeline(pipeline: Pipeline, X_train: pd.DataFrame, y_train: pd.Series, X_test: pd.DataFrame, y_test: pd.Series) -> float: Trains the pipeline on the training data and evaluates it on the testing data. Returns the accuracy score. pipeline.fit(X_train, y_train) return pipeline.score(X_test, y_test)"},{"question":"Task Your task is to implement a Python function using the `poplib` module to retrieve and print the latest email message from a given POP3 mail server securely using SSL. The function should handle potential errors and ensure that the connection is properly closed. Function Signature ```python def fetch_latest_email(user: str, password: str, host: str, port: int = 995) -> str: pass ``` Parameters - `user` (str): The username for the POP3 account. - `password` (str): The password for the POP3 account. - `host` (str): The hostname of the POP3 server. - `port` (int, optional): The port to connect to. Defaults to 995, which is the standard port for POP3-over-SSL. Returns - `str`: The content of the latest email message. Behavior and Requirements 1. The function should establish an SSL connection to the specified POP3 server and authenticate using the provided username and password. 2. If the connection and authentication are successful, the function should retrieve the most recent email message from the mailbox. 3. The function should return the full content of the most recent email as a single string. 4. Ensure that the connection to the server is properly closed after the message is retrieved. 5. Handle any potential errors gracefully, print an appropriate error message, and ensure that the server connection is closed in case of an error. Example Usage ```python email_content = fetch_latest_email(\'myusername\', \'mypassword\', \'mail.example.com\') print(email_content) ``` In this example, `fetch_latest_email` connects to the mail server at `mail.example.com` using the specified username and password, retrieves the latest email, and prints its content. Constraints - Use the `POP3_SSL` class from the `poplib` module to establish a secure connection. - Handle exceptions using `poplib.error_proto` and any other relevant exceptions to ensure robustness.","solution":"import poplib import traceback def fetch_latest_email(user: str, password: str, host: str, port: int = 995) -> str: Connects to a POP3 mail server over SSL, retrieves and returns the content of the latest email message. Parameters: - user (str): The username for the POP3 account. - password (str): The password for the POP3 account. - host (str): The hostname of the POP3 server. - port (int, optional): The port to connect to. Defaults to 995 (standard port for POP3-over-SSL). Returns: - str: The content of the latest email message. try: # Connect to the server server = poplib.POP3_SSL(host, port) server.user(user) server.pass_(password) # Get the number of messages and retrieve the latest one num_messages = len(server.list()[1]) if num_messages == 0: return \\"No messages available.\\" # Retrieve the latest message latest_message = server.retr(num_messages) messages = latest_message[1] # Join message lines into a single string message_content = \'n\'.join([line.decode(\'utf-8\') for line in messages]) return message_content except poplib.error_proto as e: print(f\\"POP3 error: {e}\\") traceback.print_exc() return \\"Failed to fetch the latest email due to a POP3 error.\\" except Exception as e: print(f\\"An error occurred: {e}\\") traceback.print_exc() return \\"Failed to fetch the latest email due to an unexpected error.\\" finally: try: server.quit() except: pass"},{"question":"**Objective**: Assess the student\'s ability to use the `http.cookiejar` module to handle HTTP cookies in Python. **Question**: You are required to create a script that automates the process of managing cookies while interacting with a website using HTTP requests. Your script should: 1. **Load** cookies from a file named `cookies.txt` at the start of the script. 2. **Set up** a cookie policy that: - Allows RFC 2965 cookies. - Is strict about domain matching for Netscape cookies. - Blocks cookies from the domains \\"ads.com\\" and \\".trackers.net\\". 3. **Make** an HTTP GET request to \\"http://example.com\\". 4. **Save** the cookies back to the file at the end of the script. **Constraints**: 1. You must use `http.cookiejar` and `urllib.request` modules. 2. Assume the `cookies.txt` file exists and is formatted properly for loading into a MozillaCookieJar. **Expected Input and Output**: Input: - A properly formatted `cookies.txt` file. Output: - The cookies should be properly saved back into `cookies.txt` after the HTTP request is made. **Performance Requirements**: - The script should handle loading and saving cookies efficiently. - The HTTP request should be successfully made with the appropriate cookies. **Function Signature**: ```python import http.cookiejar import urllib.request def manage_cookies(): # Implementation here pass ``` **Sample Example**: ```python import os import http.cookiejar import urllib.request def manage_cookies(): # Load cookies from file cj = http.cookiejar.MozillaCookieJar() cookies_file = os.path.join(os.path.expanduser(\\"~\\"), \\"cookies.txt\\") cj.load(cookies_file) # Set policy policy = http.cookiejar.DefaultCookiePolicy( rfc2965=True, strict_ns_domain=http.cookiejar.DefaultCookiePolicy.DomainStrict, blocked_domains=[\\"ads.com\\", \\".trackers.net\\"] ) cj.set_policy(policy) # Make HTTP request opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) response = opener.open(\\"http://example.com\\") # Save cookies back to file cj.save(cookies_file, ignore_discard=True, ignore_expires=True) ``` **Please implement the `manage_cookies` function as described.**","solution":"import os import http.cookiejar import urllib.request def manage_cookies(): # Load cookies from file cj = http.cookiejar.MozillaCookieJar() cookies_file = os.path.join(os.path.expanduser(\\"~\\"), \\"cookies.txt\\") if os.path.exists(cookies_file): cj.load(cookies_file) # Set policy policy = http.cookiejar.DefaultCookiePolicy( rfc2965=True, strict_ns_domain=http.cookiejar.DefaultCookiePolicy.DomainStrict, blocked_domains=[\\"ads.com\\", \\".trackers.net\\"] ) cj.set_policy(policy) # Make HTTP request opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)) response = opener.open(\\"http://example.com\\") response.read() # Ensuring the response is read to complete the request # Save cookies back to file cj.save(cookies_file, ignore_discard=True, ignore_expires=True)"},{"question":"Overview You are tasked with implementing a function to record CUDA memory history during the execution of a given PyTorch code block and generate a snapshot that can be analyzed later. Objective The objective is to demonstrate your understanding of PyTorch\'s CUDA memory debugging tools by writing a function that: 1. Records the history of CUDA memory allocations and deallocations. 2. Executes a given block of code that performs tensor operations using CUDA. 3. Generates a snapshot of the CUDA memory state and saves it to a specified file. Function Signature ```python import torch def record_and_snapshot(run_your_code, snapshot_file: str): Records CUDA memory history, executes the provided code, and generates a snapshot. Args: run_your_code (callable): A function that contains PyTorch code to be executed. snapshot_file (str): The file path where the snapshot will be saved. Returns: None # TODO: Implement this function based on the provided documentation ``` Input and Output - **Input:** - `run_your_code`: A callable (function) that performs PyTorch tensor operations using CUDA. - `snapshot_file`: A string representing the path where the snapshot will be saved (e.g., `\\"my_snapshot.pickle\\"`). - **Output:** - None. The function should save the snapshot to the specified file. Example Usage ```python import torch def example_code(): # Example tensor operations on CUDA a = torch.randn(100, 100, device=\'cuda\') b = torch.randn(100, 100, device=\'cuda\') c = torch.matmul(a, b) # Recording memory history and generating a snapshot record_and_snapshot(example_code, \\"example_snapshot.pickle\\") ``` Constraints - Ensure that PyTorch CUDA is available and tensors are allocated on the GPU. - The generated snapshot file should be compatible with the visualizer available at `pytorch.org/memory_viz`. Notes - Use the `torch.cuda.memory._record_memory_history()` function to enable memory history tracking. - Execute the `run_your_code` function within the context of memory tracking. - Use the `torch.cuda.memory._dump_snapshot(snapshot_file)` function to generate and save the snapshot. By implementing this function, you will demonstrate your ability to use PyTorch\'s tools to debug CUDA memory usage, a crucial skill for optimizing and debugging GPU-based applications.","solution":"import torch def record_and_snapshot(run_your_code, snapshot_file: str): Records CUDA memory history, executes the provided code, and generates a snapshot. Args: run_your_code (callable): A function that contains PyTorch code to be executed. snapshot_file (str): The file path where the snapshot will be saved. Returns: None # Enable recording of CUDA memory history torch.cuda.memory._record_memory_history(True) # Execute the provided function run_your_code() # Dump the recorded snapshot to the specified file torch.cuda.memory._dump_snapshot(snapshot_file) # Disable recording of CUDA memory history torch.cuda.memory._record_memory_history(False)"},{"question":"# Question: Implement a Custom Iterator and Use It with C-API Inspired Functions Objective: Write Python code to create a custom iterator class and implement functions that mimic the behavior found in the Python C-API for iteration. Requirements: 1. **Custom Iterator Class**: - Create a Python class `CustomIterator` that implements the iterator protocol. - The iterator should iterate over a range of integers from 1 to a specified number `n`. 2. **Functions Mimicking C-API**: - Implement a function `py_iter_check(obj)` that returns `True` if the object `obj` is an instance of your `CustomIterator` class. - Implement a function `py_iter_next(obj)` that returns the next value from the iterator `obj`, or `None` if there are no more items. - Implement a function `py_iter_send(iter, value, presult)` that mimics `PyIter_Send`. For simplicity, this function should simulate sending a value into a generator (assuming it is a generator function). Input and Output Formats: - `CustomIterator(n)` - **Input**: An integer `n`. - **Output**: An iterator object that iterates over integers from 1 to `n`. - `py_iter_check(obj)` - **Input**: An object `obj`. - **Output**: `True` if `obj` is an instance of `CustomIterator`, `False` otherwise. - `py_iter_next(obj)` - **Input**: An iterator object `obj`. - **Output**: The next integer in the sequence, or `None` if the sequence is exhausted. - `py_iter_send(iter, value, presult)` - **Input**: A generator `iter`, a value `value` to send into the generator, and a placeholder `presult` list. - **Output**: `presult` list containing the yielded value if iteration continues, or the returned value if the generator closes, or `None` if an error occurs. Constraints: - Do not use any external libraries. - Handle edge cases such as empty sequences or invalid inputs. Example: ```python class CustomIterator: def __init__(self, n): self.current = 1 self.n = n def __iter__(self): return self def __next__(self): if self.current <= self.n: value = self.current self.current += 1 return value else: raise StopIteration def py_iter_check(obj): return isinstance(obj, CustomIterator) def py_iter_next(obj): try: return next(obj) except StopIteration: return None def py_iter_send(iter, value, presult): try: result = iter.send(value) presult.append(result) except StopIteration as e: presult.append(e.value) except Exception as e: presult.append(None) # Example usage: it = CustomIterator(3) print(py_iter_check(it)) # Output: True print(py_iter_next(it)) # Output: 1 print(py_iter_next(it)) # Output: 2 print(py_iter_next(it)) # Output: 3 print(py_iter_next(it)) # Output: None def simple_gen(): yield 1 yield 2 return 3 gen = simple_gen() presult = [] py_iter_send(gen, None, presult) print(presult) # Output: [1] ```","solution":"class CustomIterator: def __init__(self, n): self.current = 1 self.n = n def __iter__(self): return self def __next__(self): if self.current <= self.n: value = self.current self.current += 1 return value else: raise StopIteration def py_iter_check(obj): return isinstance(obj, CustomIterator) def py_iter_next(obj): try: return next(obj) except StopIteration: return None def py_iter_send(iter, value, presult): try: result = iter.send(value) presult.append(result) except StopIteration as e: presult.append(e.value) except Exception as e: presult.append(None)"},{"question":"# Advanced Coding Challenge: Embedding Python in a C Application Objective To demonstrate your understanding of embedding Python into a C application and handling data exchanges between C and Python, you will implement a program that: 1. Embeds the Python interpreter. 2. Calls a Python function from a provided Python script file. 3. Handles data conversion between C and Python. 4. Extends the embedded interpreter with a custom C function. Description You are required to create a C program that: 1. Embeds Python and initializes the Python interpreter. 2. Loads a Python script from a file specified via command-line arguments. 3. Calls a specific function from the loaded Python script, passing a list of integers to it. 4. Prints the result returned by the Python function. 5. Extends the Python interpreter with a custom C function that takes an integer and returns its factorial. 6. Allows the Python script to call this custom C function. Requirements 1. The C program should accept the following command-line arguments: - The path to the Python script file. - The name of the Python function to call. - A list of integers to pass as arguments to the Python function. 2. The Python script file should define the specified function that takes a list of integers as input and returns their sum. 3. The custom C function exposed to Python should be named `factorial` and should compute the factorial of an integer. Example 1. **Python Script (example.py):** ```python def sum_numbers(numbers): return sum(numbers) def use_factorial(n): import emb return emb.factorial(n) ``` 2. **C Program:** ```c // Include necessary Python headers #define PY_SSIZE_T_CLEAN #include <Python.h> static PyObject* factorial(PyObject* self, PyObject* args) { int n; if (!PyArg_ParseTuple(args, \\"i\\", &n)) { return NULL; } long result = 1; for (int i = 1; i <= n; ++i) { result *= i; } return PyLong_FromLong(result); } static PyMethodDef EmbMethods[] = { {\\"factorial\\", factorial, METH_VARARGS, \\"Compute the factorial of a number.\\"}, {NULL, NULL, 0, NULL} }; static PyModuleDef EmbModule = { PyModuleDef_HEAD_INIT, \\"emb\\", NULL, -1, EmbMethods, NULL, NULL, NULL, NULL }; static PyObject* PyInit_emb(void) { return PyModule_Create(&EmbModule); } int main(int argc, char *argv[]) { if (argc < 4) { fprintf(stderr, \\"Usage: %s <python_file> <function_name> <int1> <int2> ... <intN>n\\", argv[0]); return 1; } // Initialize the Python interpreter Py_Initialize(); PyImport_AppendInittab(\\"emb\\", &PyInit_emb); PyObject *pName, *pModule, *pFunc, *pArgs, *pValue; pName = PyUnicode_DecodeFSDefault(argv[1]); pModule = PyImport_Import(pName); Py_DECREF(pName); if (pModule != NULL) { pFunc = PyObject_GetAttrString(pModule, argv[2]); if (pFunc && PyCallable_Check(pFunc)) { pArgs = PyTuple_New(argc - 3); for (int i = 0; i < argc - 3; ++i) { pValue = PyLong_FromLong(atoi(argv[i + 3])); if (!pValue) { Py_DECREF(pArgs); Py_DECREF(pModule); fprintf(stderr, \\"Cannot convert argumentn\\"); return 1; } PyTuple_SetItem(pArgs, i, pValue); } pValue = PyObject_CallObject(pFunc, pArgs); Py_DECREF(pArgs); if (pValue != NULL) { printf(\\"Result: %ldn\\", PyLong_AsLong(pValue)); Py_DECREF(pValue); } else { Py_DECREF(pFunc); Py_DECREF(pModule); PyErr_Print(); fprintf(stderr, \\"Call failedn\\"); return 1; } } else { if (PyErr_Occurred()) PyErr_Print(); fprintf(stderr, \\"Cannot find function \'%s\'n\\", argv[2]); } Py_XDECREF(pFunc); Py_DECREF(pModule); } else { PyErr_Print(); fprintf(stderr, \\"Failed to load \'%s\'n\\", argv[1]); return 1; } if (Py_FinalizeEx() < 0) { return 1; } return 0; } ``` Constraints - The Python function should handle errors gracefully and return `None` if an error occurs. - The C program should handle all possible errors, such as invalid arguments and failed Python API calls, and provide meaningful error messages. Submission - A C source file implementing the required functionality. - A sample Python script file that can be used to test the C program.","solution":"def sum_numbers(numbers): Takes a list of integers and returns their sum. return sum(numbers) def use_factorial(n): Takes an integer n, computes the factorial using the custom C function `factorial`, and returns the result. import emb return emb.factorial(n)"},{"question":"# HTML Entity Conversion Utility You are tasked with creating a utility function to convert between HTML entities and Unicode characters, making use of the dictionaries provided in the `html.entities` module. Task 1. **Implement a function `html_entity_to_unicode` which takes an HTML entity as input and returns the corresponding Unicode character**. 2. **Implement a function `unicode_to_html_entity` which takes a Unicode character as input and returns the corresponding HTML entity name**. Input and Output 1. `html_entity_to_unicode(name: str) -> str` - **Input**: A string `name` representing an HTML entity name (with or without the trailing semicolon). - **Output**: A string representing the corresponding Unicode character. - **Example**: `html_entity_to_unicode(\'gt;\')` should return `\'>\'` 2. `unicode_to_html_entity(char: str) -> str` - **Input**: A string `char` representing a Unicode character. - **Output**: A string representing the corresponding HTML entity name. - **Example**: `unicode_to_html_entity(\'>\')` should return `\'gt;\'` Constraints - If the input entity name or Unicode character does not exist in the dictionary, the function should raise a `ValueError`. - The input to the `unicode_to_html_entity` function will always be a single character string. - The conversions should consider both the `html5` and `entitydefs` dictionaries for completeness. Example Usage ```python import html.entities def html_entity_to_unicode(name: str) -> str: if name in html.entities.html5: return html.entities.html5[name] elif name in html.entities.entitydefs: return html.entities.entitydefs[name] else: raise ValueError(f\\"Entity name \'{name}\' not found.\\") def unicode_to_html_entity(char: str) -> str: if ord(char) in html.entities.codepoint2name: return html.entities.codepoint2name[ord(char)] else: raise ValueError(f\\"Character \'{char}\' not found in codepoint2name dictionary.\\") # Example print(html_entity_to_unicode(\'gt;\')) # Output: \'>\' print(unicode_to_html_entity(\'>\')) # Output: \'gt;\' ```","solution":"import html.entities def html_entity_to_unicode(name: str) -> str: Converts an HTML entity name to the corresponding Unicode character. # Remove the trailing semicolon if present if name.endswith(\';\'): name = name[:-1] # Check if the entity name exists in the html5 or entitydefs dictionaries if name in html.entities.html5: return html.entities.html5[name] elif name in html.entities.entitydefs: return html.entities.entitydefs[name] else: raise ValueError(f\\"Entity name \'{name}\' not found.\\") def unicode_to_html_entity(char: str) -> str: Converts a Unicode character to the corresponding HTML entity name. # Ensure the input is a single character if len(char) != 1: raise ValueError(\\"Input must be a single character.\\") # Get the HTML entity name from the codepoint codepoint = ord(char) if codepoint in html.entities.codepoint2name: return html.entities.codepoint2name[codepoint] + \';\' else: raise ValueError(f\\"Character \'{char}\' not found in codepoint2name dictionary.\\")"},{"question":"# Out-of-Core Learning System Implementation **Problem Statement:** You\'re tasked with building an out-of-core learning system to classify text data that is too large to fit into the main memory. You will implement a pipeline in scikit-learn that reads data incrementally, extracts features using the hashing trick, and trains an incremental classifier. Your goal is to ensure the system can handle large datasets efficiently. **Input:** - A path to the text dataset file (each line in the file represents a document). - Labels for these documents are provided in another file, where each line corresponds to the label of the document in the same order. **Output:** - The model\'s accuracy after processing each mini-batch of data. **Requirements and Constraints:** 1. Implement a function `out_of_core_learning` with the following signature: ```python def out_of_core_learning(text_file_path: str, label_file_path: str, mini_batch_size: int) -> List[float]: pass ``` 2. The text dataset is large and must be read in chunks (mini-batches). Use `chunk_size=mini_batch_size`. 3. Each text document must be vectorized using `HashingVectorizer`. 4. Use `SGDClassifier` with `partial_fit` for incremental learning. 5. You can assume that the possible classes are binary (0 and 1) for simplicity. 6. Return a list of accuracy scores after each mini-batch is processed. **Example:** Consider the file structure as given below: ``` text_file.txt: \\"document1 is about machine learning\\" \\"document2 is about deep learning\\" \\"document3 is about AI\\" ... labels_file.txt: 0 1 0 ... Usage: ```python accuracy_scores = out_of_core_learning(\\"text_file.txt\\", \\"labels_file.txt\\", mini_batch_size=100) print(accuracy_scores) ``` **Performance expectations:** - The function should handle large files without running out of memory. - Accuracies should be calculated incrementally after each mini-batch. **Code Implementation:** You are expected to implement the core logic considering the above constraints and assumptions.","solution":"from typing import List import numpy as np from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def out_of_core_learning(text_file_path: str, label_file_path: str, mini_batch_size: int) -> List[float]: accuracies = [] vectorizer = HashingVectorizer(n_features=2**20, alternate_sign=False) clf = SGDClassifier(max_iter=5) with open(text_file_path, \'r\') as text_file, open(label_file_path, \'r\') as label_file: while True: texts = [] labels = [] for _ in range(mini_batch_size): text_line = text_file.readline().strip() label_line = label_file.readline().strip() if not text_line or not label_line: break texts.append(text_line) labels.append(int(label_line)) if not texts: break X = vectorizer.transform(texts) y = np.array(labels) if len(accuracies) == 0: clf.partial_fit(X, y, classes=np.array([0, 1])) else: clf.partial_fit(X, y) predictions = clf.predict(X) accuracy = accuracy_score(y, predictions) accuracies.append(accuracy) return accuracies"},{"question":"You are given a text file containing base64 encoded data that can include standard base64 characters plus a few non-alphabetic characters for URLs and filesystem paths. The task is to implement a Python function that reads the encoded data from the file, decodes it, and writes the decoded binary data to another file. # Task: 1. Implement a function `decode_base64_file(input_file: str, output_file: str, url_safe: bool = False) -> None` that: - Takes the path of an input file containing base64 encoded data (`input_file`). - Takes the path of an output file where the decoded binary data should be written (`output_file`). - An optional `url_safe` flag indicating whether the input base64 data uses URL and filesystem-safe alphabet (Default is `False`). # Requirements: - If the `url_safe` flag is `True`, use the URL- and filesystem-safe base64 decoding method. - Handle any non-alphabet characters gracefully by discarding them before decoding, but raise an error if padding is incorrect. - Ensure proper handling of exceptions and edge cases. - Include appropriate type hints and function documentation. # Constraints: - The input file size should not exceed 10MB. - Assume the input file exists and contains valid base64 encoded data. - The solution should be efficient and handle edge cases gracefully. # Example Usage: ```python # Sample usage of the decode_base64_file function decode_base64_file(\'encoded_data.txt\', \'decoded_output.bin\', url_safe=True) ```","solution":"import base64 import re def decode_base64_file(input_file: str, output_file: str, url_safe: bool = False) -> None: Decodes base64 encoded data from an input file and writes the decoded binary data to an output file. Parameters: - input_file: str, path of the input file containing base64 encoded data. - output_file: str, path of the output file where decoded binary data should be written. - url_safe: bool, optional flag indicating whether the input base64 data uses URL and filesystem-safe alphabet. with open(input_file, \'r\') as f: base64_data = f.read() if url_safe: # Replace url_safe characters to standard base64 characters base64_data = base64_data.replace(\'-\', \'+\').replace(\'_\', \'/\') # Remove non-alphabet characters clean_data = re.sub(r\'[^A-Za-z0-9+/=]\', \'\', base64_data) try: decoded_bytes = base64.b64decode(clean_data) with open(output_file, \'wb\') as f: f.write(decoded_bytes) except (base64.binascii.Error, ValueError) as e: print(f\\"Error decoding base64 data: {e}\\") raise"},{"question":"Coding Assessment Question # Objective To assess your understanding of PyTorch\'s distributed training capabilities, specifically focusing on handling fault-tolerant and elastic jobs using the `torchrun` command. # Task You are required to write a Python script that configures and launches a distributed training job for a PyTorch model. You will implement a function called `run_distributed_training` and provide the necessary settings to run both a fault-tolerant and an elastic job. # Requirements 1. **Function Implementation:** Implement the function `run_distributed_training` which should configure the settings required for distributed training using the `torchrun` command. ```python def run_distributed_training(job_type, num_nodes, procs_per_node, max_restarts, job_id, host_node_addr, min_size=None, max_size=None): Configures and launches a distributed training job. Parameters: - job_type (str): Type of the job, either \'fault-tolerant\' or \'elastic\'. - num_nodes (int): Number of nodes to use for \'fault-tolerant\' job. - procs_per_node (int): Number of trainers per node. - max_restarts (int): Maximum number of allowed failures. - job_id (str): Identifier for the job. - host_node_addr (str): Address of the host node in the form <host>[:<port>]. - min_size (int, optional): Minimum number of nodes for \'elastic\' job. - max_size (int, optional): Maximum number of nodes for \'elastic\' job. pass ``` 2. **Input and Output Formats:** - The function should not return any value, but should print the constructed `torchrun` command. - Depending on the job type (\'fault-tolerant\' or \'elastic\'), the function should construct the appropriate command. 3. **Constraints:** - For \'fault-tolerant\' jobs, `min_size` and `max_size` are not required. - For \'elastic\' jobs, `num_nodes` is not required but `min_size` and `max_size` should be provided. - The number of nodes should be an integer between 1 and 100. - The number of trainers per node should be an integer between 1 and 8. - The maximum number of restarts should be an integer between 0 and 10. 4. **Examples:** **Example 1: Fault-tolerant Job** ```python run_distributed_training(\'fault-tolerant\', 3, 4, 5, \'job123\', \'node1.example.com:29400\') ``` **Expected Output:** ```bash torchrun --nnodes=3 --nproc-per-node=4 --max-restarts=5 --rdzv-id=job123 --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 YOUR_TRAINING_SCRIPT.py ``` **Example 2: Elastic Job** ```python run_distributed_training(\'elastic\', None, 4, 5, \'job123\', \'node1.example.com:29400\', min_size=2, max_size=4) ``` **Expected Output:** ```bash torchrun --nnodes=2:4 --nproc-per-node=4 --max-restarts=5 --rdzv-id=job123 --rdzv-backend=c10d --rdzv-endpoint=node1.example.com:29400 YOUR_TRAINING_SCRIPT.py ``` # Additional Notes - You do not need to provide the actual training script (`YOUR_TRAINING_SCRIPT.py`), only the command to execute it. - Ensure your function handles any necessary validation of inputs and prints meaningful errors if any constraints are violated.","solution":"def run_distributed_training(job_type, num_nodes, procs_per_node, max_restarts, job_id, host_node_addr, min_size=None, max_size=None): Configures and launches a distributed training job. Parameters: - job_type (str): Type of the job, either \'fault-tolerant\' or \'elastic\'. - num_nodes (int): Number of nodes to use for \'fault-tolerant\' job. - procs_per_node (int): Number of trainers per node. - max_restarts (int): Maximum number of allowed failures. - job_id (str): Identifier for the job. - host_node_addr (str): Address of the host node in the form <host>[:<port>]. - min_size (int, optional): Minimum number of nodes for \'elastic\' job. - max_size (int, optional): Maximum number of nodes for \'elastic\' job. # Validate inputs if job_type not in [\'fault-tolerant\', \'elastic\']: print(f\\"Invalid job type: {job_type}. Must be \'fault-tolerant\' or \'elastic\'.\\") return if not (1 <= procs_per_node <= 8): print(f\\"Invalid number of processes per node: {procs_per_node}. Must be between 1 and 8.\\") return if not (0 <= max_restarts <= 10): print(f\\"Invalid number of max restarts: {max_restarts}. Must be between 0 and 10.\\") return if job_type == \'fault-tolerant\': if not (1 <= num_nodes <= 100): print(f\\"Invalid number of nodes: {num_nodes}. Must be between 1 and 100.\\") return command = (f\\"torchrun --nnodes={num_nodes} --nproc-per-node={procs_per_node} --max-restarts={max_restarts} \\" f\\"--rdzv-id={job_id} --rdzv-backend=c10d --rdzv-endpoint={host_node_addr} YOUR_TRAINING_SCRIPT.py\\") elif job_type == \'elastic\': if min_size is None or max_size is None: print(\\"Missing min_size or max_size for elastic job.\\") return if not (1 <= min_size <= max_size <= 100): print(f\\"Invalid min_size or max_size: {min_size}, {max_size}. Must be between 1 and 100, and min_size <= max_size.\\") return command = (f\\"torchrun --nnodes={min_size}:{max_size} --nproc-per-node={procs_per_node} --max-restarts={max_restarts} \\" f\\"--rdzv-id={job_id} --rdzv-backend=c10d --rdzv-endpoint={host_node_addr} YOUR_TRAINING_SCRIPT.py\\") print(command)"},{"question":"**Question: URL Manipulation and Query String Processing** You have been tasked to write a utility function for a web crawler application that processes URLs in several ways. This function, named `process_url`, should take a URL and perform the following tasks: 1. **Parse the URL** into its components (scheme, netloc, path, params, query, fragment). 2. **Normalize the URL** by ensuring the scheme is in lowercase and removing any fragment identifiers. 3. **Retrieve and sort the query parameters** alphabetically by key. 4. **Rebuild the URL** from the parsed components, but include the normalized query string. 5. **Return the normalized URL** and the dictionary of sorted query parameters. # Function Signature ```python def process_url(url: str) -> tuple: \'\'\' Processes the given URL and returns a tuple containing the normalized URL and the dictionary of sorted query parameters. Parameters: - url (str): The input URL to be processed. Returns: - tuple: A tuple containing the normalized URL (str) and the dictionary of sorted query parameters (dict). \'\'\' pass ``` # Example ```python url = \\"HTTP://www.Example.com:80/path;params?b=2&a=1&c=3#fragment\\" normalized_url, query_params = process_url(url) print(normalized_url) # Output: \'http://www.example.com:80/path;params?a=1&b=2&c=3\' print(query_params) # Output: {\'a\': [\'1\'], \'b\': [\'2\'], \'c\': [\'3\']} ``` # Constraints 1. The function should handle URLs with any combination of components. 2. The function should handle empty, missing, and improperly quoted query parameters gracefully. 3. The function must use `urllib.parse` for URL parsing and manipulation. 4. The returned query parameters dictionary values should be lists. # Steps/Hint 1. Use `urllib.parse.urlparse` to parse the URL. 2. Normalize the scheme and remove the fragment using appropriate methods. 3. Use `urllib.parse.parse_qs` to parse and sort the query parameters. 4. Rebuild the URL without the fragment and with the sorted query parameters using `urllib.parse.urlunparse` or similar functions.","solution":"import urllib.parse def process_url(url: str) -> tuple: Processes the given URL and returns a tuple containing the normalized URL and the dictionary of sorted query parameters. Parameters: - url (str): The input URL to be processed. Returns: - tuple: A tuple containing the normalized URL (str) and the dictionary of sorted query parameters (dict). # Parse the URL into components parsed_url = urllib.parse.urlparse(url) # Normalize the scheme to be in lowercase scheme = parsed_url.scheme.lower() # Normalize the network location to be in lowercase netloc = parsed_url.netloc.lower() # Remove fragment fragment = \'\' # Retrieve and sort the query parameters alphabetically by key query_params_dict = urllib.parse.parse_qs(parsed_url.query, keep_blank_values=True) sorted_query_params_tuple = sorted(query_params_dict.items()) # Rebuild the query string from sorted parameters sorted_query_params = urllib.parse.urlencode(sorted_query_params_tuple, doseq=True) # Rebuild the URL with normalized scheme, netloc, path, params, sorted query, and no fragment normalized_url = urllib.parse.urlunparse( (scheme, netloc, parsed_url.path, parsed_url.params, sorted_query_params, fragment) ) return normalized_url, dict(sorted_query_params_tuple)"},{"question":"Objective: Demonstrate your understanding of the `itertools` and `functools` modules in Python by implementing a function that uses these modules to efficiently process input data. Problem Statement: You are given a list of tuples `data` where each tuple consists of two elements: a string and an integer. You need to process this list based on the following criteria: 1. **Group** the tuples by the string value. 2. **Compute the sum** of integers for each group. 3. Output the result as a list of tuples, each containing the string and the corresponding sum of integers, sorted alphabetically by the string. **Constraints:** 1. Input list can contain up to 100,000 tuples. 2. String values are always non-empty and consist of ASCII characters only. 3. Integer values are in the range ([-10^6, 10^6]). Function Signature: ```python from typing import List, Tuple def process_data(data: List[Tuple[str, int]]) -> List[Tuple[str, int]]: pass ``` Input: - `data` (List[Tuple[str, int]]): A list of tuples, where each tuple contains a string (str) and an integer (int). Output: - Returns a list of tuples, where each tuple contains a string and the sum of integers for that string, sorted alphabetically by the string. Example: ```python data = [(\\"apple\\", 10), (\\"banana\\", 5), (\\"apple\\", 20), (\\"banana\\", 15), (\\"cherry\\", 25)] result = process_data(data) print(result) # Output: [(\'apple\', 30), (\'banana\', 20), (\'cherry\', 25)] ``` Requirements: - You should use functions from the `itertools` and `functools` modules in your implementation. **Tip:** Consider using `itertools.groupby` for grouping and `functools.reduce` for summing the integer values. Note: Please ensure that your solution is efficient and well-optimized to handle large datasets.","solution":"from typing import List, Tuple from itertools import groupby from functools import reduce def process_data(data: List[Tuple[str, int]]) -> List[Tuple[str, int]]: # First, sort the data by the string element data.sort(key=lambda x: x[0]) # Group by the string element grouped_data = groupby(data, key=lambda x: x[0]) # Sum the integer values for each group result = [] for key, group in grouped_data: total_sum = reduce(lambda acc, x: acc + x[1], group, 0) result.append((key, total_sum)) return result"},{"question":"# Type Object Manipulation and Creation In this assessment, you will demonstrate your understanding of Python\'s type objects and their manipulation by creating and interacting with custom heap-allocated types based on the provided documentation. Task 1. **Type Checking Function** - Implement a function in Python that checks if an object is a type object using the C API function `PyType_Check`. ```python def is_type_object(obj): Checks if the given object is a type object. Args: obj (Any): The object to check. Returns: bool: True if the object is a type object, False otherwise. pass ``` 2. **Factory Function for Custom Types** - Create a function that defines a new heap-allocated type with specified name, bases, and a basic method using `PyType_FromSpecWithBases`. - The created type should include a method `describe` that returns a string describing the class name. ```python def create_custom_type(name, base=None): Creates a custom heap-allocated type. Args: name (str): The name of the new type. base (Type, optional): The base class to inherit from, defaults to `object`. Returns: Type: The newly created type. pass ``` 3. **Custom Type Interaction** - Implement a function that creates an instance of the custom type and invokes its `describe` method. ```python def interact_with_custom_type(name, base=None): Creates an instance of a custom heap-allocated type and calls its describe method. Args: name (str): The name of the new type. base (Type, optional): The base class to inherit from, defaults to `object`. Returns: str: The description of the custom type obtained from the describe method. pass ``` Constraints and Instructions - Ensure that the custom type correctly inherits from the specified base class. - The `describe` method should be callable on the instances of the custom type. - Memory and performance efficiency should be considered; avoid unnecessary allocations. - Provide comprehensive documentation and edge case handling. Use the provided C API documentation to assist you in implementing the functions correctly. You can assume the availability of `ctypes` to interface with the required C API from Python.","solution":"import ctypes def is_type_object(obj): Checks if the given object is a type object. Args: obj (Any): The object to check. Returns: bool: True if the object is a type object, False otherwise. return isinstance(obj, type) def create_custom_type(name, base=object): Creates a custom heap-allocated type. Args: name (str): The name of the new type. base (Type, optional): The base class to inherit from, defaults to `object`. Returns: Type: The newly created type. return type(name, (base,), { \'describe\': lambda self: f\'I am an instance of {self.__class__.__name__}\' }) def interact_with_custom_type(name, base=object): Creates an instance of a custom heap-allocated type and calls its describe method. Args: name (str): The name of the new type. base (Type, optional): The base class to inherit from, defaults to `object`. Returns: str: The description of the custom type obtained from the describe method. custom_type = create_custom_type(name, base) instance = custom_type() return instance.describe()"},{"question":"# Pandas Coding Assessment Objective The task is to assess your understanding and ability to use the `pandas` `Styler` class for styling a DataFrame. Problem Statement You are provided with a DataFrame containing sales data. Your task is to write a function that styles this DataFrame to highlight specific insights, format it for better readability, and then export the styled DataFrame to an HTML file. Function Signature ```python def style_dataframe_to_html(data: pd.DataFrame, output_file: str) -> None: Styles the given DataFrame \'data\' and exports it to an HTML file specified by \'output_file\'. Parameters: - data (pd.DataFrame): The DataFrame containing sales data. - output_file (str): The file path where the styled HTML output will be saved. Returns: - None: The function should write the styled HTML to \'output_file\'. pass ``` Input Format - `data`: A `pandas` DataFrame with the following columns: - `Product`: Name of the product. - `Sales`: Number of units sold. - `Revenue`: Total revenue generated. - `Profit`: Total profit made. - `output_file`: A string representing the file path to save the styled HTML output. Output - The function does not return any value. Instead, it writes the styled DataFrame to the specified HTML file. Styling Requirements 1. **Highlight Maximum and Minimum**: Highlight the rows with the maximum and minimum values in the \'Sales\' column using `Styler.highlight_max` and `Styler.highlight_min`. 2. **Apply Gradient**: Apply a background gradient based on values in the \'Revenue\' column. 3. **Format Numbers**: Format the \'Revenue\' and \'Profit\' columns to display numbers with two decimal points. 4. **Set Caption**: Add a caption to the table indicating it is sales data. 5. **Set Column Widths**: Adjust the column widths for better readability. Constraints - Do not mutate the original DataFrame. - The style should be applied in a way that is clearly visible in the HTML export. Example Usage ```python import pandas as pd # Example DataFrame data = pd.DataFrame({ \'Product\': [\'A\', \'B\', \'C\', \'D\'], \'Sales\': [250, 300, 150, 400], \'Revenue\': [5000.0, 7000.0, 2000.0, 8000.0], \'Profit\': [1000.0, 1500.0, 400.0, 2000.0] }) # Specify output file path output_file = \'styled_sales_data.html\' # Function call style_dataframe_to_html(data, output_file) ``` This will create a styled HTML file at the specified location with highlighted maximum and minimum sales, gradient background based on revenue, formatted numeric columns, and a caption.","solution":"import pandas as pd def style_dataframe_to_html(data: pd.DataFrame, output_file: str) -> None: Styles the given DataFrame \'data\' and exports it to an HTML file specified by \'output_file\'. Parameters: - data (pd.DataFrame): The DataFrame containing sales data. - output_file (str): The file path where the styled HTML output will be saved. Returns: - None: The function should write the styled HTML to \'output_file\'. # Create a Styler object styler = data.style # Highlight the maximum value in the \'Sales\' column styler = styler.highlight_max(subset=[\'Sales\'], color=\'lightgreen\') # Highlight the minimum value in the \'Sales\' column styler = styler.highlight_min(subset=[\'Sales\'], color=\'lightcoral\') # Apply a background gradient based on the \'Revenue\' column styler = styler.background_gradient(subset=[\'Revenue\'], cmap=\'Blues\') # Format \'Revenue\' and \'Profit\' columns to display numbers with two decimal points styler = styler.format({\'Revenue\': \\"{:.2f}\\", \'Profit\': \\"{:.2f}\\"}) # Set a caption styler = styler.set_caption(\\"Sales Data\\") # Adjust the column widths (assuming an HTML output-consistent way of setting widths) styler = styler.set_table_styles([ {\'selector\': \'th.col_heading\', \'props\': [(\'max-width\', \'120px\')]}, {\'selector\': \'td\', \'props\': [(\'max-width\', \'120px\')]} ]) # Save the styled dataframe to an HTML file styler.to_html(output_file)"},{"question":"# Seaborn Coding Assessment Question You are an aspiring data analyst working with the `tips` dataset provided by seaborn. Your task is to generate several visualizations using seaborn\'s `objects` interface. The objective is to assess your understanding of seaborn\'s capabilities for creating and customizing plots. Ensure you follow the instructions carefully. Input: - You will use the `tips` dataset provided by seaborn. Instructions: 1. **Load the `tips` dataset** and convert the `time` column to string type if necessary. 2. **Create a bar plot** showing the count of tips for each day of the week. Color the bars based on whether the meal was lunch or dinner, and adjust the positions of bars to avoid overlapping using the `Dodge` transform. 3. **Modify the bar plot** to fill out empty spaces using the `empty` parameter. 4. **Add spacing between dodged marks**, making sure the total_bill is summed while considering the gender of the person. 5. **Create a dot plot** displaying the relationship of tips to total_bill, dodging points based on the gender of the person and the smoking status. Also, apply jitter to the dot positions. Expected Output: - Visualization graphs generated as a result of each instruction above. Each graph should be clearly labeled and distinguishable based on the parameters and transformations applied. Example Code Template: ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset tips = load_dataset(\\"tips\\") tips = tips.astype({\\"time\\": str}) # Task 1: Basic Bar Plot p1 = so.Plot(tips, x=\\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge()) p1.show() # Task 2: Bar Plot with Empty Spaces Filled p2 = so.Plot(tips, x=\\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) p2.show() # Task 3: Bar Plot with Spacing Between Dodged Marks p3 = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) p3.show() # Task 4: Dot Plot with Jitter p4 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\").add(so.Dot(), so.Dodge(by=[\\"sex\\", \\"smoker\\"]), so.Jitter()) p4.show() ``` Constraints: - Use only the seaborn\'s `objects` interface for generating plots. - Ensure plots are generated with appropriate labels and legends for clarity. - Handle dataset data type conversions where necessary. Performance Requirements: - The solution should efficiently utilize seaborn\'s built-in methods and transformations without unnecessary complexity. - Ensure the plots render correctly and provide meaningful visual representations of the data.","solution":"import seaborn.objects as so from seaborn import load_dataset def load_and_prepare_data(): # Load the tips dataset and convert the `time` column to string type if necessary tips = load_dataset(\\"tips\\") tips = tips.astype({\\"time\\": str}) return tips def create_bar_plot(tips): # Create a bar plot showing the count of tips for each day of the week # Color the bars based on whether the meal was lunch or dinner, and avoid overlapping using Dodge transform p1 = so.Plot(tips, x=\\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge()) return p1 def create_bar_plot_with_empty_spaces(tips): # Modify the bar plot to fill out empty spaces using the `empty` parameter p2 = so.Plot(tips, x=\\"day\\", color=\\"time\\").add(so.Bar(), so.Count(), so.Dodge(empty=\\"fill\\")) return p2 def create_bar_plot_with_spacing(tips): # Add spacing between dodged marks, summing total_bill while considering the gender of the person p3 = so.Plot(tips, x=\\"day\\", y=\\"total_bill\\", color=\\"sex\\").add(so.Bar(), so.Agg(\\"sum\\"), so.Dodge(gap=0.1)) return p3 def create_dot_plot_with_jitter(tips): # Create a dot plot displaying the relationship of tips to total_bill # Dodge points based on the gender of the person and the smoking status, and apply jitter to the dot positions p4 = so.Plot(tips, x=\\"total_bill\\", y=\\"tip\\", color=\\"sex\\").add(so.Dot(), so.Dodge(by=[\\"sex\\", \\"smoker\\"]), so.Jitter()) return p4"},{"question":"Objective Demonstrate your understanding of Python\'s asyncio library by creating a concurrent file download manager using `asyncio` that downloads files from the internet and stores them locally. Problem Statement Write a Python function `download_files(urls: List[str]) -> None` that asynchronously downloads files from a list of URLs and saves them to the local directory. Each download should happen concurrently, and the function should print out the progress of each download. Requirements 1. **Concurrency**: Use asyncio to download multiple files concurrently. 2. **Async/Await**: Utilize the async/await syntax for asynchronous operations. 3. **Synchronization**: Use asyncio synchronization primitives where necessary to manage concurrent tasks. 4. **Progress Reporting**: Print out the progress for each download. 5. **Error Handling**: Gracefully handle potential errors, such as connection issues or invalid URLs. Function Signature ```python import asyncio from typing import List async def download_files(urls: List[str]) -> None: pass ``` Expected Input and Output - **Input**: A list of URLs (strings) pointing to the files to be downloaded. ```python urls = [ \\"http://example.com/file1.jpg\\", \\"http://example.com/file2.jpg\\", \\"http://example.com/file3.jpg\\" ] ``` - **Output**: The function should not return anything but should print the progress of each download and save the files locally. For example: ``` Downloading file1.jpg... 20% Downloading file1.jpg... 40% Downloading file2.jpg... 15% ... ``` Constraints - Assume that each URL points to a valid file that can be downloaded directly. - The function should handle potential exceptions gracefully, including logging or printing error messages. - The size of the files to be downloaded can vary, and large files should not block other operations. Example Usage ```python async def main(): urls = [ \\"http://example.com/file1.jpg\\", \\"http://example.com/file2.jpg\\", \\"http://example.com/file3.jpg\\" ] await download_files(urls) asyncio.run(main()) ``` This function should utilize asyncio\'s high-level constructs to manage the concurrency of the HTTP requests and ensure robust error handling and synchronization where necessary.","solution":"import asyncio import aiohttp import os from typing import List async def download_file(session, url, path): try: async with session.get(url) as response: response.raise_for_status() total_size = int(response.headers.get(\'content-length\', 0)) downloaded_size = 0 chunk_size = 1024 with open(path, \'wb\') as f: async for chunk in response.content.iter_chunked(chunk_size): if chunk: f.write(chunk) downloaded_size += len(chunk) progress = (downloaded_size / total_size) * 100 print(f\\"Downloading {os.path.basename(path)}... {progress:.2f}%\\") print(f\\"Download of {os.path.basename(path)} completed\\") except Exception as e: print(f\\"Error downloading {url}: {e}\\") async def download_files(urls: List[str]) -> None: async with aiohttp.ClientSession() as session: tasks = [] for url in urls: path = os.path.join(os.getcwd(), os.path.basename(url)) tasks.append(download_file(session, url, path)) await asyncio.gather(*tasks)"},{"question":"# Event Scheduler Implementation with Python\'s `sched` Module You are required to implement a function `schedule_events(events)` that accepts a list of events and executes them using Python\'s `sched` module. Each event in the `events` list is a dictionary with the following keys: - `\'time\'`: an absolute time (in seconds) or relative delay (if `is_abs` is `False`). - `\'priority\'`: a priority number (lower numbers indicate higher priority). - `\'action\'`: a callable function to be executed. - `\'args\'`: a tuple of arguments to be passed to the action (optional, default to an empty tuple). - `\'kwargs\'`: a dictionary of keyword arguments to be passed to the action (optional, default to an empty dict). - `\'is_abs\'`: a boolean indicating if the `\'time\'` is an absolute time or a relative delay. Your function should: 1. Schedule all the events using the `sched.scheduler` class. 2. Run the scheduler to execute all the scheduled events. 3. Print output messages indicating the start and completion of each event action, with a timestamp. Constraints: - All events in the list are scheduled within a reasonable time frame such that the total execution time does not exceed 60 seconds. - The scheduler functions and methods should handle exceptions gracefully without breaking. Input: ```python events = [ { \'time\': 5, \'priority\': 1, \'action\': some_function, # some_function should be defined beforehand \'args\': (1,), \'kwargs\': {}, \'is_abs\': False }, ... ] ``` Output: The function should print messages indicating the execution of event actions such as: ``` Event started: <action_name> at <current_time> Event completed: <action_name> at <current_time> ``` Sample Implementation: ```python import sched import time def some_function(x): print(f\\"Executing some_function with argument {x}\\") def another_function(x, y): print(f\\"Executing another_function with arguments {x} and {y}\\") def schedule_events(events): scheduler = sched.scheduler(time.time, time.sleep) for event in events: if event[\'is_abs\']: scheduler.enterabs( event[\'time\'], event[\'priority\'], lambda a=event[\'action\'], args=event[\'args\'], kwargs=event[\'kwargs\']: ( print(f\\"Event started: {a.__name__} at {time.time()}\\"), a(*args, **kwargs), print(f\\"Event completed: {a.__name__} at {time.time()}\\") ) ) else: scheduler.enter( event[\'time\'], event[\'priority\'], lambda a=event[\'action\'], args=event[\'args\'], kwargs=event[\'kwargs\']: ( print(f\\"Event started: {a.__name__} at {time.time()}\\"), a(*args, **kwargs), print(f\\"Event completed: {a.__name__} at {time.time()}\\") ) ) scheduler.run() ``` Test the function with a list of events ensuring varied priorities and execution times.","solution":"import sched import time def schedule_events(events): scheduler = sched.scheduler(time.time, time.sleep) for event in events: if event[\'is_abs\']: scheduler.enterabs( event[\'time\'], event[\'priority\'], lambda a=event[\'action\'], args=event.get(\'args\', ()), kwargs=event.get(\'kwargs\', {}): ( print(f\\"Event started: {a.__name__} at {time.time()}\\"), a(*args, **kwargs), print(f\\"Event completed: {a.__name__} at {time.time()}\\") ) ) else: scheduler.enter( event[\'time\'], event[\'priority\'], lambda a=event[\'action\'], args=event.get(\'args\', ()), kwargs=event.get(\'kwargs\', {}): ( print(f\\"Event started: {a.__name__} at {time.time()}\\"), a(*args, **kwargs), print(f\\"Event completed: {a.__name__} at {time.time()}\\") ) ) scheduler.run() # Sample actions for testing def some_function(x): print(f\\"Executing some_function with argument {x}\\") def another_function(x, y): print(f\\"Executing another_function with arguments {x} and {y}\\")"},{"question":"**Objective**: Demonstrate comprehension and practical usage of the `os` module in Python through file operations, environment variable handling, and process management. **Problem Statement**: Create a Python script that performs the following tasks using the `os` module: 1. **Environment Variable Handling**: - Add a new environment variable named `TEST_VARIABLE` with the value `PythonTesting`. - Retrieve the value of `TEST_VARIABLE` and print it to the console. 2. **File and Directory Operations**: - Create a new directory named `test_directory`. - Within `test_directory`, create a new file named `test_file.txt` and write the text `Hello, world!` to it. - Read the contents of `test_file.txt` and print it to the console. - Ensure that the file permissions of `test_file.txt` allow the owner to read and write, but no permissions for others. 3. **Process Management**: - Create a new Python script named `child_script.py` that prints `This is the child script.` to the console. - Execute `child_script.py` as a child process from the main script and capture its output. - Print the output of `child_script.py` to the console. **Constraints and Requirements**: - Use only the `os` module for all operations. Do not use any other libraries such as `subprocess` for process management. - Handle any exceptions that may arise during file operations or process execution gracefully. - Ensure the script runs correctly on both Unix and Windows platforms. # Expected Input and Output **Input**: No user input required. **Output**: ``` Environment Variable: PythonTesting File Contents: Hello, world! Child Script Output: This is the child script. ``` # Implementation Details 1. **Environment Variable Handling**: - Use `os.environ` to set and get environment variables. - Verify the newly added environment variable by printing its value. 2. **File and Directory Operations**: - Use `os.makedirs` to create the directory. - Use `os.open`, `os.write`, and `os.read` for file operations. - Use `os.chmod` to set the appropriate file permissions. 3. **Process Management**: - Write content to `child_script.py` using file operations. - Use `os.popen` to execute the script and capture its output. # Sample Code Skeleton ```python import os # 1. Environment Variable Handling os.environ[\'TEST_VARIABLE\'] = \'PythonTesting\' print(f\\"Environment Variable: {os.getenv(\'TEST_VARIABLE\')}\\") # 2. File and Directory Operations # Create directory os.makedirs(\'test_directory\', exist_ok=True) # File operations file_path = os.path.join(\'test_directory\', \'test_file.txt\') fd = os.open(file_path, os.O_RDWR | os.O_CREAT) os.write(fd, b\'Hello, world!\') os.lseek(fd, 0, os.SEEK_SET) content = os.read(fd, 50) print(f\\"File Contents: {content.decode()}\\") os.close(fd) # Set file permissions os.chmod(file_path, 0o600) # 3. Process Management # Create child script child_script = os.path.join(\'test_directory\', \'child_script.py\') with open(child_script, \'w\') as f: f.write(\\"print(\'This is the child script.\')\\") # Execute child script with os.popen(f\'python {child_script}\') as child_output: output = child_output.read().strip() print(f\\"Child Script Output: {output}\\") ``` # Notes: - Ensure that you handle all unnecessary details and complexities especially regarding execution differences between Unix and Windows platforms. - The provided sample code skeleton is just a guideline; students are expected to fill in the details and error handling as instructed.","solution":"import os def manage_environment_variable(): # Add a new environment variable named `TEST_VARIABLE` with the value `PythonTesting` os.environ[\'TEST_VARIABLE\'] = \'PythonTesting\' # Retrieve the value of `TEST_VARIABLE` and print it print(f\\"Environment Variable: {os.getenv(\'TEST_VARIABLE\')}\\") return os.getenv(\'TEST_VARIABLE\') def file_and_directory_operations(): # Create a new directory named `test_directory` os.makedirs(\'test_directory\', exist_ok=True) # Create a new file named `test_file.txt` and write the text `Hello, world!` to it file_path = os.path.join(\'test_directory\', \'test_file.txt\') fd = os.open(file_path, os.O_RDWR | os.O_CREAT) os.write(fd, b\'Hello, world!\') os.lseek(fd, 0, os.SEEK_SET) # Read the contents of `test_file.txt` and print it content = os.read(fd, 50) print(f\\"File Contents: {content.decode()}\\") os.close(fd) # Ensure file permissions allow the owner to read and write but no permissions for others os.chmod(file_path, 0o600) return content.decode(), os.stat(file_path).st_mode & 0o777 def process_management(): # Create a new Python script named `child_script.py` that prints `This is the child script.` child_script = os.path.join(\'test_directory\', \'child_script.py\') with open(child_script, \'w\') as f: f.write(\\"print(\'This is the child script.\')\\") # Execute `child_script.py` as a child process and capture its output with os.popen(f\'python {child_script}\') as child_output: output = child_output.read().strip() print(f\\"Child Script Output: {output}\\") return output def main(): manage_environment_variable() file_and_directory_operations() process_management() if __name__ == \'__main__\': main()"},{"question":"# Question: Implementing an Audit Hook in Python You are tasked with implementing and utilizing an audit hook in Python using the `sys` module. The audit hook should capture specific events that occur during the execution of a program, log them to a file, and perform some additional operations based on specific audit events. # Requirements: 1. **Function Name**: `audit_hook_test` 2. **Input Parameter**: - A string `logfile` representing the filename where the audit events will be logged. 3. **Output**: - No return value. The function should create a log file and handle events as specified. # Functionality: 1. Implement a custom audit hook that: - Captures audit events related to \'os\', \'builtins\', and \'sys\' modules as described in the provided documentation. - Logs the captured audit events and their arguments to a file specified by `logfile`. 2. The function should: - Register the custom audit hook. - Create or open the specified log file and write a header indicating the start of logging. - Perform a sample sequence of operations to generate audit events, such as: - Creating a file. - Writing to the file. - Reading from the file. - Deleting the file. - Making network connections (e.g., with `http.client` or `socket`). - Using some built-in functions like `input`, `id`, etc. - Log each captured event with its name and arguments. - Ensure that the log file is closed properly after the function execution. # Constraints: - The logfile should be specified with a valid file name string. - All necessary imports (`sys`, `os`, etc.) should be handled within the function. # Example: ```python def audit_hook_test(logfile): pass # Running the function should create a log file with detailed audit events from the executed operations. audit_hook_test(\'audit_log.txt\') ``` # Notes: - The log file should contain human-readable entries detailing each audit event and its arguments. - You are expected to handle file I/O operations and make sure that the log file is accurately capturing all specified events. # Additional Information: Refer to the audit events table provided in your reference documentation to understand what events can be captured and their arguments.","solution":"import sys import os def audit_hook_test(logfile): def audit_hook(event, args): with open(logfile, \'a\') as f: f.write(f\'Event: {event} Args: {args}n\') # Register the custom audit hook sys.addaudithook(audit_hook) # Start logging with open(logfile, \'w\') as f: f.write(\\"Audit startn\\") # Perform sample operations to generate audit events try: # Creating a file sample_file = \'sample.txt\' with open(sample_file, \'w\') as f: f.write(\'This is a test.\') # Reading from the file with open(sample_file, \'r\') as f: f.read() # Deleting the file os.remove(sample_file) # Using built-in functions id(10) print(input(\\"Enter something: \\")) # This will trigger the input audit event except Exception: pass # Cleanup by removing the created sample file (if it still exists) if os.path.exists(sample_file): os.remove(sample_file)"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding and application of the \\"heapq\\" module by implementing a custom priority queue for task management. Your implementation should support adding tasks with priorities, updating task priorities, and popping the highest-priority tasks. # Task You are to implement a class `TaskPriorityQueue` that manages tasks using a heap to ensure efficient priority handling. The class should support the following operations: 1. **add_task(task, priority)**: Adds a new task or updates the priority of an existing task. 2. **remove_task(task)**: Deletes a task from the queue. 3. **pop_task()**: Removes and returns the lowest-priority task. 4. **peek_task()**: Returns the lowest-priority task without removing it from the queue. # Requirements 1. **Class Definition**: ```python class TaskPriorityQueue: def __init__(self): # Initialize your heap and supporting data structures here pass def add_task(self, task: str, priority: int) -> None: # Add or update the task with its priority pass def remove_task(self, task: str) -> None: # Remove the specified task pass def pop_task(self) -> str: # Pop the highest priority task pass def peek_task(self) -> str: # Peek the highest priority task without removing it pass ``` 2. **Function Details**: - `add_task(task: str, priority: int) -> None`: Adds a new task with the given priority, or updates the priority if the task already exists. Aim to ensure that the task remains stable if its priority is updated to the same value. - `remove_task(task: str) -> None`: Removes the specified task from the priority queue. If the task does not exist, do nothing. - `pop_task() -> str`: Pops and returns the task with the lowest priority. If multiple tasks have the same priority, return the one that was added first. - `peek_task() -> str`: Returns the task with the lowest priority without removing it. If multiple tasks have the same priority, return the one that was added first. # Input/Output - The input will be a series of method calls to your `TaskPriorityQueue` class. - The output should be the results of any `pop_task` or `peek_task` method calls as a list of strings. # Constraints - Assume task names are unique. - You can assume initial priorities are non-negative integers. - Your solution should leverage the `heapq` module for efficient heap operations. # Example ```python # Example usage of the TaskPriorityQueue class tpq = TaskPriorityQueue() tpq.add_task(\\"task1\\", 5) tpq.add_task(\\"task2\\", 10) tpq.add_task(\\"task1\\", 2) # Update priority of task1 print(tpq.pop_task()) # Output: \\"task1\\" tpq.add_task(\\"task3\\", 1) print(tpq.peek_task()) # Output: \\"task3\\" tpq.remove_task(\\"task3\\") print(tpq.pop_task()) # Output: \\"task2\\" ``` # Notes - Ensure your code maintains heap invariants and is efficient for all operations. - Perform input validation where appropriate.","solution":"import heapq class TaskPriorityQueue: def __init__(self): self.pq = [] # list of entries arranged in a heap self.entry_finder = {} # mapping of tasks to entries self.REMOVED = \'<removed-task>\' # placeholder for a removed task self.counter = 0 # unique sequence count def add_task(self, task: str, priority: int) -> None: Add a new task or update the priority of an existing task. if task in self.entry_finder: self.remove_task(task) count = self.counter entry = [priority, count, task] self.entry_finder[task] = entry heapq.heappush(self.pq, entry) self.counter += 1 def remove_task(self, task: str) -> None: Remove a task from the priority queue. entry = self.entry_finder.pop(task, None) if entry is not None: entry[-1] = self.REMOVED def pop_task(self) -> str: Remove and return the lowest priority task. Raise KeyError if empty. while self.pq: priority, count, task = heapq.heappop(self.pq) if task is not self.REMOVED: del self.entry_finder[task] return task raise KeyError(\'pop from an empty priority queue\') def peek_task(self) -> str: Return the lowest priority task without removing it. Raise KeyError if empty. while self.pq: priority, count, task = self.pq[0] if task is not self.REMOVED: return task heapq.heappop(self.pq) raise KeyError(\'peek from an empty priority queue\')"},{"question":"**Question: Implement a Python program using the `ftplib` module to manage FTP transfers securely.** Objective: Create a Python function that connects to an FTP server, securely transfers a file from the local system to the server, retrieves another file from the server, and lists the contents of a specified directory. Handle all possible exceptions and ensure the connections are secure. # Function Signature ```python def manage_ftp_transfers(ftp_host: str, ftp_user: str, ftp_passwd: str, ftp_directory: str, local_upload_file_path: str, remote_upload_file_name: str, remote_download_file_name: str, local_download_file_path: str) -> str: ``` # Input Parameters - `ftp_host`: The host address of the FTP server (str). - `ftp_user`: Username for authenticating with the FTP server (str). - `ftp_passwd`: Password for authenticating with the FTP server (str). - `ftp_directory`: Directory on the FTP server where operations will be performed (str). - `local_upload_file_path`: Path of the local file to be uploaded to the FTP server (str). - `remote_upload_file_name`: Name to be given to the uploaded file on the FTP server (str). - `remote_download_file_name`: Name of the file to be downloaded from the FTP server (str). - `local_download_file_path`: Path where the downloaded file will be saved locally (str). # Output - A string containing the list of files and directories in the specified `ftp_directory` on the FTP server once all operations are completed successfully. # Constraints - Ensure the FTP connection is secured using TLS. - Handle potential exceptions during FTP operations, and return relevant error messages if any occur. - Assume `local_upload_file_path` exists and is accessible. - Assume the user has read/write permissions for the specified `ftp_directory` on the server. # Instructions 1. Establish a secure FTP connection using the `FTP_TLS` class. 2. Login to the FTP server with provided credentials. 3. Change to the specified directory on the FTP server. 4. Upload the local file to the FTP server with the specified name. 5. Download the file from the FTP server to the specified local path. 6. List and return the contents of the directory on the FTP server as a string. 7. Ensure proper closure of the FTP connection in a secure manner. 8. Handle any exceptions that may arise during the FTP operations and return appropriate messages. # Example ```python ftp_host = \\"ftp.example.com\\" ftp_user = \\"user123\\" ftp_passwd = \\"pass123\\" ftp_directory = \\"/example_dir\\" local_upload_file_path = \\"/path/to/local/upload_file.txt\\" remote_upload_file_name = \\"upload_file.txt\\" remote_download_file_name = \\"download_file.txt\\" local_download_file_path = \\"/path/to/local/download_file.txt\\" result = manage_ftp_transfers(ftp_host, ftp_user, ftp_passwd, ftp_directory, local_upload_file_path, remote_upload_file_name, remote_download_file_name, local_download_file_path) print(result) # Expected output: A string containing the list of files and directories in /example_dir on the FTP server. ``` # Notes - You may assume that the `ftplib` module is available for use and should be imported as part of your implementation. - Pay attention to secure handling and cleanup of FTP connections. - Make sure to handle exceptions like connection errors, authentication failures, file not found errors, etc.","solution":"from ftplib import FTP_TLS, error_perm import os def manage_ftp_transfers(ftp_host: str, ftp_user: str, ftp_passwd: str, ftp_directory: str, local_upload_file_path: str, remote_upload_file_name: str, remote_download_file_name: str, local_download_file_path: str) -> str: Manages FTP transfers securely. Connects to the FTP server using TLS, performs file upload and download, and lists contents of a specified directory. :param ftp_host: The host address of the FTP server :param ftp_user: Username for FTP login :param ftp_passwd: Password for FTP login :param ftp_directory: Directory on the FTP server for the operations :param local_upload_file_path: Path to the local file to be uploaded :param remote_upload_file_name: Name to be given to the uploaded file on the server :param remote_download_file_name: Name of the file to be downloaded from the server :param local_download_file_path: Path to save the downloaded file locally :return: List of files and directories in the specified directory on the FTP server try: # Establish secure FTP connection ftps = FTP_TLS(ftp_host) ftps.login(user=ftp_user, passwd=ftp_passwd) ftps.prot_p() # Secure data connection # Change to the specified directory ftps.cwd(ftp_directory) # Upload the local file with open(local_upload_file_path, \'rb\') as upload_file: ftps.storbinary(f\'STOR {remote_upload_file_name}\', upload_file) # Download the specified file with open(local_download_file_path, \'wb\') as download_file: ftps.retrbinary(f\'RETR {remote_download_file_name}\', download_file.write) # List directory contents dir_contents = ftps.nlst() # Close the FTP connection ftps.quit() return \\"n\\".join(dir_contents) except error_perm as e: return f\\"FTP error: {e}\\" except FileNotFoundError as e: return f\\"Local file error: {e}\\" except Exception as e: return f\\"An unexpected error occurred: {e}\\""},{"question":"# Seaborn Color Palette Application in Data Visualization You are provided with a dataset containing information about different species of penguins, including their flipper lengths and body masses. Your task is to visualize this dataset using Seaborn to demonstrate your understanding of color palettes in data visualization. **Dataset Description:** - `species`: The species of the penguin (categorical data). - `flipper_length_mm`: The length of the penguin\'s flipper in millimeters (numeric data). - `body_mass_g`: The body mass of the penguin in grams (numeric data). You should perform the following steps: 1. **Load the dataset:** Load the dataset from Seaborn\'s built-in dataset collection (`penguins` dataset). 2. **Plot with Qualitative Palette:** - Create a scatter plot of `flipper_length_mm` vs. `body_mass_g` where the color of the points represents different penguin species. Choose a qualitative palette that is suitable for categorical data. 3. **Plot with Sequential Palette:** - Create another scatter plot of `flipper_length_mm` vs. `body_mass_g` where the color of the points represents the flipper length. Use a sequential palette that effectively represents the numeric data. 4. **Plot with Diverging Palette:** - Create a third scatter plot of `flipper_length_mm` vs. `body_mass_g` where the color of the points represents the deviation of `body_mass_g` from the mean body mass. Use a diverging palette to visualize how body mass differs from the mean. **Requirements:** - You must use appropriate Seaborn functions to generate and apply color palettes. - Each plot should include a color bar to indicate the mapping of colors to data values. **Input Format:** ```python # Assume you have already imported the necessary libraries import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") ``` **Output Format:** Produce three scatter plots as specified, ensuring each plot uses the appropriate color palette and includes a color bar. ```python # Scatter plot with qualitative palette # Scatter plot with sequential palette # Scatter plot with diverging palette ``` **Evaluation:** Your solution will be evaluated based on: - Correct usage of color palettes. - The relevance of the chosen palettes to the data type. - The visual appeal and clarity of the plots.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np # Load the dataset penguins = sns.load_dataset(\\"penguins\\") def plot_qualitative_palette(): Creates a scatter plot of flipper_length_mm vs. body_mass_g using a qualitative palette to distinguish the different penguin species. palette = sns.color_palette(\\"Set2\\") plt.figure(figsize=(10, 6)) sns.scatterplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", hue=\\"species\\", palette=palette ) plt.title(\\"Scatter Plot with Qualitative Palette\\") plt.legend(title=\'Species\', bbox_to_anchor=(1, 1)) plt.show() def plot_sequential_palette(): Creates a scatter plot of flipper_length_mm vs. body_mass_g using a sequential palette to represent the flipper length. palette = sns.color_palette(\\"viridis\\", as_cmap=True) plt.figure(figsize=(10, 6)) points = plt.scatter( penguins[\\"flipper_length_mm\\"], penguins[\\"body_mass_g\\"], c=penguins[\\"flipper_length_mm\\"], cmap=palette ) plt.colorbar(points, label=\'Flipper Length (mm)\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.title(\\"Scatter Plot with Sequential Palette\\") plt.show() def plot_diverging_palette(): Creates a scatter plot of flipper_length_mm vs. body_mass_g using a diverging palette to visualize deviation of body mass from the mean. mean_body_mass = np.mean(penguins[\\"body_mass_g\\"]) body_mass_deviation = penguins[\\"body_mass_g\\"] - mean_body_mass palette = sns.diverging_palette(220, 20, as_cmap=True) plt.figure(figsize=(10, 6)) points = plt.scatter( penguins[\\"flipper_length_mm\\"], penguins[\\"body_mass_g\\"], c=body_mass_deviation, cmap=palette ) plt.colorbar(points, label=\'Deviation from Mean Body Mass (g)\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.title(\\"Scatter Plot with Diverging Palette\\") plt.show()"},{"question":"Unix User and Group Management In this task, you are required to write a Python function leveraging the Unix-specific `pwd` and `grp` modules. Your function will accept a username and should return detailed information about the user, including their group memberships. This will demonstrate your understanding of querying both the password and group databases in Unix. Function Signature ```python def get_user_info(username: str) -> dict: pass ``` Input - `username` (str): The username for which you need to fetch information. Output - The function should return a dictionary with the following structure: ```python { \\"uid\\": <user_id>, \\"gid\\": <primary_group_id>, \\"home\\": <home_directory>, \\"shell\\": <login_shell>, \\"groups\\": [<list_of_groups>] } ``` Where: - `<user_id>` is the numerical user ID of the user. - `<primary_group_id>` is the numerical group ID of the user\'s primary group. - `<home_directory>` is the home directory of the user. - `<login_shell>` is the user\'s login shell. - `<list_of_groups>` is a list of all group names the user belongs to, including their primary group. Requirements 1. You may assume that the `username` exists on the system. 2. You need to handle any possible exceptions gracefully. 3. The performance requirements may not be strict, but fetching the user and group information accurately is crucial. Example ```python # Suppose there is a user \'jdoe\' with the following details: # - User ID: 1001 # - Primary Group ID: 100 # - Home Directory: /home/jdoe # - Login Shell: /bin/bash # - Group memberships: [\'users\', \'admin\'] result = get_user_info(\\"jdoe\\") # Expected output: # { # \\"uid\\": 1001, # \\"gid\\": 100, # \\"home\\": \\"/home/jdoe\\", # \\"shell\\": \\"/bin/bash\\", # \\"groups\\": [\\"users\\", \\"admin\\"] # } ``` Hints - Use the `pwd` module to fetch the user\'s primary information. - Use the `grp` module to fetch all group memberships.","solution":"import pwd import grp import json def get_user_info(username: str) -> dict: try: user_info = pwd.getpwnam(username) except KeyError: raise ValueError(f\\"User {username} not found.\\") uid = user_info.pw_uid gid = user_info.pw_gid home = user_info.pw_dir shell = user_info.pw_shell # Get all groups all_groups = grp.getgrall() groups = [group.gr_name for group in all_groups if username in group.gr_mem or group.gr_gid == gid] return { \\"uid\\": uid, \\"gid\\": gid, \\"home\\": home, \\"shell\\": shell, \\"groups\\": groups }"},{"question":"***MemoryView Manipulation Challenge*** **Objective:** Write Python functions that demonstrate an understanding of memoryview objects, including creation, manipulation, and checking for memoryview instances. **Task:** You are required to implement two functions based on the memoryview documentation provided: 1. **create_memoryview_from_list(data_list):** - **Input:** A list of integers `data_list`. - **Output:** A memoryview object of the given list converted to a bytes object. - **Constraints:** The list contains only integers between 0 and 255 inclusive. 2. **reverse_memoryview_data(mview):** - **Input:** A memoryview object `mview`. - **Output:** A new memoryview object with the data reversed. - **Constraints:** The provided memoryview is read/write. **Function Specifications:** ```python def create_memoryview_from_list(data_list) -> memoryview: This function takes a list of integers and returns a memoryview object. Args: data_list (list): A list of integers. Returns: memoryview: A memoryview object of the given list converted to bytes. pass def reverse_memoryview_data(mview) -> memoryview: This function takes a memoryview object and returns a new memoryview object with the data reversed. Args: mview (memoryview): A memoryview object. Returns: memoryview: A new memoryview object with data reversed. pass ``` **Example Usage:** ```python data = [1, 2, 3, 4, 5] mview = create_memoryview_from_list(data) print(bytes(mview)) # Output: b\'x01x02x03x04x05\' reversed_mview = reverse_memoryview_data(mview) print(bytes(reversed_mview)) # Output: b\'x05x04x03x02x01\' ``` **Notes:** - The `create_memoryview_from_list` function should convert the list of integers to a bytes object before creating the memoryview. - `reverse_memoryview_data` should handle reversing the buffer data and create a new memoryview object from the reversed data. - Do not use any external libraries other than those provided by the Python Standard Library. - Ensure that the memoryview objects interact properly with provided constraints. Implement these functions above to assess your understanding of memoryview objects in Python.","solution":"def create_memoryview_from_list(data_list): This function takes a list of integers and returns a memoryview object. Args: data_list (list): A list of integers. Returns: memoryview: A memoryview object of the given list converted to bytes. byte_data = bytes(data_list) return memoryview(byte_data) def reverse_memoryview_data(mview): This function takes a memoryview object and returns a new memoryview object with the data reversed. Args: mview (memoryview): A memoryview object. Returns: memoryview: A new memoryview object with data reversed. reversed_data = mview[::-1].tobytes() return memoryview(reversed_data)"},{"question":"**Problem Statement:** Design and implement a Python program that enhances the Python interactive shell experience by implementing a command history management feature using the \\"readline\\" module. The program should support loading command history from a file, allowing the user to interactively input commands, and saving the history back to the file upon exiting. # Requirements: 1. **Initialization:** - On startup, the program should attempt to load the command history from a specified file (`~/.custom_python_history`). If the file does not exist, it should handle the `FileNotFoundError` gracefully and proceed without loading history. 2. **Command Input and History Management:** - The program should read commands from the user interactively using the built-in `input()` function, enhanced by the `readline` module for command-line editing and history support. - Every user input should be added to the history list. 3. **Exiting and Saving:** - When the user decides to exit (by entering a special command, e.g., `exit()`), the program should save the command history to the specified file. - The program should ensure that no more than 1000 commands are saved in the history file. 4. **Concurrency Safety:** - The program should handle concurrent interactive sessions by appending new history items to the file rather than overwriting it entirely. # Implementation Details: You are required to implement the following functions: 1. `def load_command_history(histfile: str) -> None`: - Attempts to read the command history from `histfile`. If the file does not exist, it should be created. 2. `def save_command_history(prev_h_len: int, histfile: str) -> None`: - Saves the new history to `histfile`, appending only new commands added since the program started. Ensures the history file does not grow beyond 1000 commands. 3. `def interactive_shell(histfile: str) -> None`: - Implements the interactive command input loop enhanced by `readline`. Calls `load_command_history` on startup and registers `save_command_history` to be called upon exit using the `atexit` module. # Expected Input and Output: - The program does not take any direct input arguments but reads input commands from the user interactively. - It outputs the results of the commands (if any) and saves the command history to `~/.custom_python_history`. # Constraints: 1. The solution should handle the potential absence of the history file gracefully. 2. The history file size should be managed to not exceed 1000 commands. # Example Usage: ```python if __name__ == \\"__main__\\": import os histfile = os.path.expanduser(\\"~/.custom_python_history\\") interactive_shell(histfile) ``` **Implementation Example:** ```python import atexit import os import readline def load_command_history(histfile: str) -> None: try: readline.read_history_file(histfile) print(\\"History loaded successfully.\\") except FileNotFoundError: open(histfile, \'wb\').close() print(\\"History file not found. Created a new history file.\\") def save_command_history(prev_h_len: int, histfile: str) -> None: new_h_len = readline.get_current_history_length() readline.set_history_length(1000) readline.append_history_file(new_h_len - prev_h_len, histfile) print(\\"History saved successfully.\\") def interactive_shell(histfile: str) -> None: load_command_history(histfile) h_len = readline.get_current_history_length() print(\\"Starting interactive shell (type \'exit()\' to exit)\\") try: while True: command = input(\\">>> \\") if command.strip().lower() == \\"exit()\\": break # Simulate executing the command print(f\\"Executing command: {command}\\") readline.add_history(command) except KeyboardInterrupt: print(\\"nExiting interactive shell.\\") atexit.register(save_command_history, h_len, histfile) save_command_history(h_len, histfile) if __name__ == \\"__main__\\": histfile = os.path.expanduser(\\"~/.custom_python_history\\") interactive_shell(histfile) ``` In this implementation, the program manages the command history effectively using the \\"readline\\" module while ensuring safe concurrent access to the history file.","solution":"import atexit import os import readline def load_command_history(histfile: str) -> None: try: readline.read_history_file(histfile) print(\\"History loaded successfully.\\") except FileNotFoundError: open(histfile, \'wb\').close() print(\\"History file not found. Created a new history file.\\") def save_command_history(prev_h_len: int, histfile: str) -> None: new_h_len = readline.get_current_history_length() readline.set_history_length(1000) readline.append_history_file(new_h_len - prev_h_len, histfile) print(\\"History saved successfully.\\") def interactive_shell(histfile: str) -> None: load_command_history(histfile) h_len = readline.get_current_history_length() print(\\"Starting interactive shell (type \'exit()\' to exit)\\") atexit.register(save_command_history, h_len, histfile) try: while True: command = input(\\">>> \\") if command.strip().lower() == \\"exit()\\": break # Simulate executing the command print(f\\"Executing command: {command}\\") readline.add_history(command) except KeyboardInterrupt: print(\\"nExiting interactive shell.\\") save_command_history(h_len, histfile) if __name__ == \\"__main__\\": histfile = os.path.expanduser(\\"~/.custom_python_history\\") interactive_shell(histfile)"},{"question":"**Question: Implementing a Custom Encoding and Decoding Conversion** Using your understanding of the `binascii` module, implement two functions: `custom_encode` and `custom_decode`. These functions should demonstrate the capability to convert binary data to a custom ASCII-encoded string and back. Use a combination of hex and base64 encoding to achieve this. # Function Specifications **Function 1**: `custom_encode(data: bytes) -> str` - **Input**: A bytes object (`data`). The length of `data` can be arbitrary. - **Output**: A string representing the custom-encoded data. - **Constraints**: - The function should first convert the binary data to a hex string using the `b2a_hex` function. - Then, the hex string should be converted to base64 encoding using the `b2a_base64` function. - Ensure that the final output is a string without any newline characters. **Function 2**: `custom_decode(encoded_data: str) -> bytes` - **Input**: A string (`encoded_data`), which is the output of `custom_encode`. - **Output**: The original binary data as a bytes object. - **Constraints**: - The function should first convert the base64 encoded string back to a hex string using the `a2b_base64` function. - Then, convert the hex string back to the original binary data using the `a2b_hex` function. # Example ```python import binascii def custom_encode(data: bytes) -> str: # Convert to hex string hex_data = binascii.b2a_hex(data) # Convert hex string to base64 string and remove newline base64_data = binascii.b2a_base64(hex_data) return base64_data.decode().strip() def custom_decode(encoded_data: str) -> bytes: # Convert base64 string back to hex string hex_data = binascii.a2b_base64(encoded_data.encode()) # Convert hex string back to original binary data original_data = binascii.a2b_hex(hex_data) return original_data # Test cases data = b\'hello world\' encoded = custom_encode(data) print(f\'Encoded: {encoded}\') decoded = custom_decode(encoded) print(f\'Decoded: {decoded}\') assert data == decoded, \\"The decoded data does not match the original data.\\" ``` # Explanation In this assessment, students are required to demonstrate their knowledge of encoding and decoding binary data using multiple encoding schemes. The usage of the `binascii` module\'s functions to convert binary to hex and then to base64, and vice versa, tests their understanding of different function calls, data transformations, and constraints like handling ASCII and bytes-like objects.","solution":"import binascii def custom_encode(data: bytes) -> str: Encode the given bytes data to a custom ASCII-encoded string. Parameters: - data: bytes : The input binary data. Returns: - str : A custom-encoded string. # Convert to hex string hex_data = binascii.b2a_hex(data) # Convert hex string to base64 string and remove newline base64_data = binascii.b2a_base64(hex_data).strip() return base64_data.decode() def custom_decode(encoded_data: str) -> bytes: Decode the custom ASCII-encoded string back to bytes. Parameters: - encoded_data: str : The encoded string. Returns: - bytes : The original binary data. # Convert base64 string back to hex string hex_data = binascii.a2b_base64(encoded_data) # Convert hex string back to original binary data original_data = binascii.a2b_hex(hex_data) return original_data"},{"question":"Objective Implement a function that reads a DataFrame from a CSV file, processes it by setting specific pandas display options, and returns the modified DataFrame along with a summary. Problem Statement Write a function `process_and_summarize_data` that accomplishes the following tasks: 1. Reads a CSV file into a pandas DataFrame. 2. Sets the pandas display options as specified below: - `display.max_rows`: Set to 10 - `display.max_columns`: Set to 5 - `display.precision`: Set to 4 - `display.max_colwidth`: Set to 50 3. Returns: - The DataFrame in its entirety. - The summary of the DataFrame using the `info` method. Function Signature ```python def process_and_summarize_data(file_path: str) -> Tuple[pd.DataFrame, str]: pass ``` Input - `file_path` (str): Path to the CSV file. Output - A tuple containing: 1. `pd.DataFrame`: The modified DataFrame. 2. `str`: The summary of the DataFrame as a string. Example ```python # Example of importing required modules import pandas as pd # Example function usage df, summary = process_and_summarize_data(\\"data.csv\\") print(df) print(summary) ``` Constraints - Assume the CSV file is correctly formatted and can be read by pandas without errors. - The function should reset all settings to their defaults after execution to avoid global settings changes. - The DataFrame and summary should respect the specified display options. - You can assume the CSV file exists at the provided file path. Notes - Use the `pandas.get_option`, `pandas.set_option`, and `pandas.reset_option` functions to manipulate display settings. - Use `with pd.option_context()` to manage the temporary settings within the function scope. **Hints:** 1. Utilize context managers to set and reset pandas options. 2. Capture the DataFrame summary by converting the output of `info` into a string using an appropriate approach.","solution":"import pandas as pd from typing import Tuple def process_and_summarize_data(file_path: str) -> Tuple[pd.DataFrame, str]: Reads a CSV file, processes it by setting specific pandas display options, and returns the modified DataFrame along with a summary. Args: - file_path (str): Path to the CSV file. Returns: - Tuple[pd.DataFrame, str]: The processed DataFrame and its summary as a string. with pd.option_context(\'display.max_rows\', 10, \'display.max_columns\', 5, \'display.precision\', 4, \'display.max_colwidth\', 50): df = pd.read_csv(file_path) # Capture the info summary in a string buffer = pd.io.formats.format.StringIO() df.info(buf=buffer) summary = buffer.getvalue() return df, summary"},{"question":"Task You are tasked with writing a Python function that, given a list of MIME types, returns a dictionary where each MIME type maps to the list of commands that can handle that type, according to the system\'s mailcap files. Function Signature ```python def mime_type_commands(mime_types: list) -> dict: pass ``` Input - `mime_types`: A list of strings representing the MIME types you want to check. Each string is a valid MIME type, e.g., `[\\"video/mpeg\\", \\"text/html\\"]`. Output - Returns a dictionary where each key is a MIME type from the input list, and each value is a list of commands (strings) that can handle that MIME type according to the mailcap file configurations. Constraints 1. Do not use any external libraries other than the `mailcap` module provided by Python. 2. Assume the list of MIME types is non-empty and contains valid MIME type strings. 3. Your function should handle exceptions gracefully and return an empty list for MIME types for which no commands are found. Example ```python # Example Usage mime_types = [\\"video/mpeg\\", \\"text/html\\"] results = mime_type_commands(mime_types) # An example output could be: # { # \'video/mpeg\': [\'xmpeg %s\', \'vlc %s\'], # \'text/html\': [\'lynx %s\', \'w3m %s\'] # } print(results) # Output might look similar to (depending on system configuration): # { # \'video/mpeg\': [\'xmpeg tmp1223\'], # \'text/html\': [\'lynx tmp1223\'] # } ``` Notes on Implementation 1. Use the `mailcap.getcaps()` function to retrieve the mappings of MIME types to commands. 2. For each MIME type in the input list, use the `mailcap.findmatch()` function to get the command(s). 3. If `findmatch()` returns `(None, None)` for a MIME type, map that MIME type to an empty list in the output dictionary. 4. Remember to correctly replace `\\"%s\\"` with a placeholder filename when retrieving commands using `findmatch()`. Additional Information - You can find more information about the structure of mailcap files and MIME types in the RFC 1524 documentation. - The \'mailcap\' module was deprecated in Python 3.11, but this task focuses on using the module as demonstrated in the example above.","solution":"import mailcap def mime_type_commands(mime_types: list) -> dict: result = {} caps = mailcap.getcaps() for mime_type in mime_types: commands = [] while True: command, entry = mailcap.findmatch(caps, mime_type, filename=\\"tmpfile\\") if command: commands.append(command) else: break result[mime_type] = commands return result"},{"question":"Develop a Robust Client-Server Chat Application with Non-Blocking Sockets **Objective**: Implement a simple client-server chat application using non-blocking sockets. The server should be able to handle multiple clients simultaneously using the `select` module. Requirements: 1. **Server Implementation**: - Create a server socket that listens for incoming connections on a specified port. - Accept multiple client connections and manage them concurrently. - Receive messages from clients and broadcast them to all connected clients. - Use non-blocking sockets and the `select` module to efficiently manage multiple connections. - Handle client disconnections gracefully. 2. **Client Implementation**: - Create a client socket that connects to the server on the specified port. - Send messages to the server. - Receive broadcast messages from the server. Inputs and Outputs: - **Server**: - Input: Port number to listen on. - Output: Messages received from clients and broadcast to all clients. - **Client**: - Input: Server address and port number to connect to. - Output: Messages sent by the client and messages received from the server. Constraints: - Use non-blocking sockets for both server and client. - Use the `select` module for handling multiple connections on the server side. - Ensure proper handling of partial messages and socket errors. Performance Requirements: - The application should efficiently handle up to 1000 concurrent client connections. Detailed Steps and Hints: 1. **Server Implementation**: - Initialize the server socket and set it to non-blocking mode. - Bind the socket to the specified port and start listening for connections. - Use the `select` module to monitor multiple sockets (the server socket for incoming connections and client sockets for incoming messages). - When a new connection is accepted, add the new client socket to the list of monitored sockets. - When a message is received from a client, broadcast it to all other connected clients. 2. **Client Implementation**: - Initialize the client socket and set it to non-blocking mode. - Connect the socket to the server address and port. - Use the `select` module to monitor input from the user and messages from the server. - Send user input to the server and display messages received from the server. 3. **Handling Partial Messages**: - Implement mechanisms to accumulate partial messages until the entire message is received. - Ensure that messages are clearly delimited or use a fixed message length protocol. # Example Usage: ```python # Server python chat_server.py 12345 # where 12345 is the port number # Client python chat_client.py 127.0.0.1 12345 # where 127.0.0.1 is the server address and 12345 is the port number ``` # Submission: - Submit `chat_server.py` for the server implementation. - Submit `chat_client.py` for the client implementation. Ensure your code is properly commented and adheres to Python\'s PEP 8 style guide.","solution":"import select import socket import sys import threading # Server Code def start_server(port): server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1) server_socket.setblocking(False) server_socket.bind((\'\', port)) server_socket.listen(5) inputs = [server_socket] clients = {} print(f\\"Server started on port {port}\\") while True: readable, _, _ = select.select(inputs, [], []) for s in readable: if s is server_socket: client_socket, client_address = server_socket.accept() print(f\\"New connection from {client_address}\\") client_socket.setblocking(False) inputs.append(client_socket) clients[client_socket] = client_address else: data = s.recv(1024) if data: message = data.decode().strip() print(f\\"Received message from {clients[s]}: {message}\\") for client_socket in clients: if client_socket is not s: client_socket.sendall(data) else: print(f\\"Connection closed by {clients[s]}\\") inputs.remove(s) del clients[s] s.close() # Client Code def start_client(server_address, server_port): client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client_socket.setblocking(False) try: client_socket.connect((server_address, server_port)) except BlockingIOError: pass def receive_messages(): while True: try: data = client_socket.recv(1024) if data: print(data.decode().strip()) except BlockingIOError: continue threading.Thread(target=receive_messages, daemon=True).start() while True: message = input() if message: client_socket.sendall(message.encode()) # Server initialization if __name__ == \\"__main__\\": if len(sys.argv) == 2 and sys.argv[0] == \\"server\\": start_server(int(sys.argv[1])) elif len(sys.argv) == 3 and sys.argv[0] == \\"client\\": start_client(sys.argv[1], int(sys.argv[2])) else: print(\\"Usage:nServer: python chat_application.py server <port>nClient: python chat_application.py client <address> <port>\\")"},{"question":"# Custom Container: Indexed Set **Objective:** Write a Python class `IndexedSet` that combines the properties of a `Set` and provides indexing like a `Sequence`. This class should inherit from `collections.abc.MutableSet` and `collections.abc.Sequence` and should conform to their interfaces. **Requirements:** 1. **Class Definition & Inheritance:** - Define a class named `IndexedSet`. - Inherit this class from `collections.abc.MutableSet` and `collections.abc.Sequence`. 2. **Constructor:** - The constructor should initialize the collection as an empty list. 3. **Abstract Methods:** Implement the following abstract methods required by the mixins: - `__contains__(self, value)`: Check if a value exists in the set. - `__iter__(self)`: Return an iterator over the set elements. - `__len__(self)`: Return the number of elements in the set. - `__getitem__(self, index)`: Return the element at the given index. - `__setitem__(self, index, value)`: Replace the element at a given index with a new value (not required for `Set` but needed for `Sequence` behaviour). - `__delitem__(self, index)`: Remove the element at a given index. - `add(self, value)`: Add a new element to the set if it doesn\'t exist. - `discard(self, value)`: Remove an element from the set if it exists. 4. **Additional Methods:** - Define `index(self, value)`: Return the index of a value in the set. - Define `count(self, value)`: Return the number of times a value appears in the set. 5. **Constraints:** - No duplicate elements should be allowed in the set. - The elements should maintain the order of their insertion. 6. **Performance:** - Ensure that methods operate efficiently with the expected performance characteristics of sets (average O(1) for add and discard). **Example Usage:** ```python from collections.abc import MutableSet, Sequence class IndexedSet(MutableSet, Sequence): def __init__(self): self._elements = [] # Initialize an empty list def __contains__(self, value): return value in self._elements def __iter__(self): return iter(self._elements) def __len__(self): return len(self._elements) def __getitem__(self, index): return self._elements[index] def __setitem__(self, index, value): if value in self._elements: raise ValueError(\\"Cannot add duplicate element to IndexedSet\\") self._elements[index] = value def __delitem__(self, index): del self._elements[index] def add(self, value): if value not in self._elements: self._elements.append(value) def discard(self, value): if value in self._elements: self._elements.remove(value) def index(self, value): return self._elements.index(value) def count(self, value): return self._elements.count(value) # Usage: s = IndexedSet() s.add(1) s.add(2) s.add(3) print(s[0]) # Output: 1 print(len(s)) # Output: 3 print(2 in s) # Output: True s.discard(3) print(len(s)) # Output: 2 print(s.index(1)) # Output: 0 ``` **Note:** Ensure to test the class by creating instances and performing operations like adding elements, iterating, removing, and indexing to verify the integrity of the `IndexedSet`.","solution":"from collections.abc import MutableSet, Sequence class IndexedSet(MutableSet, Sequence): def __init__(self): self._elements = [] def __contains__(self, value): return value in self._elements def __iter__(self): return iter(self._elements) def __len__(self): return len(self._elements) def __getitem__(self, index): return self._elements[index] def __setitem__(self, index, value): if value in self._elements: raise ValueError(\\"Cannot add duplicate element to IndexedSet\\") self._elements[index] = value def __delitem__(self, index): del self._elements[index] def add(self, value): if value not in self._elements: self._elements.append(value) def discard(self, value): if value in self._elements: self._elements.remove(value) def index(self, value): return self._elements.index(value) def count(self, value): return self._elements.count(value)"},{"question":"# Custom Object Type Implementation with Special Methods In this task, you are required to create a custom object type, `Vector`, that represents a mathematical vector. You will implement this class in Python from scratch while considering Python\'s support for object implementation as described in the given documentation. Your `Vector` class should support the following: 1. **Initialization**: - The `Vector` class should be initialized with a list of numbers. 2. **Attributes**: - The `Vector` class should expose an attribute, `elements`, which is a list of the vector\'s elements. 3. **Methods**: - Implement a method, `magnitude`, which returns the magnitude (Euclidean norm) of the vector. - Implement a method, `dot`, which takes another `Vector` object and returns the dot product of the current vector and the given vector. 4. **Special Methods**: - Implement the special method `__add__` to support vector addition. Adding two vectors should result in a new `Vector` object. - Implement the special method `__len__` to return the number of elements in the vector. - Implement the special method `__repr__` to return the string representation of the vector. # Constraints: - You can assume the input list for initialization will always contain numeric values. - You should include appropriate error handling to ensure that operations between vectors of different sizes raise an appropriate exception. # Performance: - Aim to have your methods work efficiently with vectors of size up to 10,000 elements. # Example Usage: ```python v1 = Vector([1, 2, 3]) v2 = Vector([4, 5, 6]) # Accessing elements print(v1.elements) # [1, 2, 3] # Magnitude of vectors print(v1.magnitude()) # approximately 3.7417 # Dot product of v1 and v2 print(v1.dot(v2)) # 32 # Vector addition v3 = v1 + v2 print(v3) # Vector([5, 7, 9]) # Length of the vector print(len(v1)) # 3 # String representation of the vector print(v1) # Vector([1, 2, 3]) ``` # Implementation ```python class Vector: def __init__(self, elements): # Initialize the vector with elements # Raise an error if elements is not a list or contains non-numeric values @property def elements(self): # Return the elements of the vector def magnitude(self): # Calculate and return the magnitude (Euclidean norm) of the vector def dot(self, other): # Calculate and return the dot product with another vector def __add__(self, other): # Implement vector addition, returning a new Vector object def __len__(self): # Return the number of elements in the vector def __repr__(self): # Return the string representation of the vector ``` Implement the `Vector` class according to the specified behaviors and constraints. Make sure to test your implementation with various edge cases and input values to ensure robustness.","solution":"import math class Vector: def __init__(self, elements): if not all(isinstance(x, (int, float)) for x in elements): raise ValueError(\\"All elements must be numeric.\\") self._elements = elements @property def elements(self): return self._elements def magnitude(self): return math.sqrt(sum(x ** 2 for x in self._elements)) def dot(self, other): if len(self) != len(other): raise ValueError(\\"Vectors must be of the same length.\\") return sum(x * y for x, y in zip(self._elements, other.elements)) def __add__(self, other): if len(self) != len(other): raise ValueError(\\"Vectors must be of the same length.\\") return Vector([x + y for x, y in zip(self._elements, other.elements)]) def __len__(self): return len(self._elements) def __repr__(self): return f\\"Vector({self._elements})\\""},{"question":"**Question: Implement Distributed Data Parallel Training with PyTorch** Given the PyTorch DDP documentation, your task is to implement a distributed data parallel training script using PyTorch\'s `torch.nn.parallel.DistributedDataParallel` (DDP) module. You will use a simple neural network model and distribute the training across multiple processes. # Requirements: 1. **Model**: Use the provided `SimpleNN` class for the model. It is a simple neural network with one hidden layer. 2. **Data**: Generate synthetic data for training purposes. 3. **Processes**: Set up a process group using `torch.multiprocessing`. 4. **Training Loop**: Implement the distributed training loop including forward pass, backward pass, and optimizer step. 5. **Synchronization**: Ensure that model parameters are synchronized across processes after each iteration. 6. **Clean Up**: Properly clean up and terminate processes after training. # SimpleNN Model ```python import torch.nn as nn class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.output = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.hidden(x) x = self.relu(x) x = self.output(x) return x ``` # Implementation Details: 1. **Initialization**: Initialize the process group using `dist.init_process_group`. 2. **Model Wrapping**: Wrap the model with `torch.nn.parallel.DistributedDataParallel`. 3. **Training Loop**: Include forward pass, loss computation, backward pass, and optimizer step inside each process. 4. **Process Management**: Use `torch.multiprocessing` to spawn and manage processes for distributed training. # Code Template: ```python import os import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP # Provided SimpleNN model class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.output = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.hidden(x) x = self.relu(x) x = self.output(x) return x def setup(rank, world_size): # Initialize process group dist.init_process_group(\\"gloo\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) # Create model and move it to GPU with id rank model = SimpleNN(10, 50, 10).to(rank) ddp_model = DDP(model, device_ids=[rank]) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Generate dummy data for training inputs = torch.randn(20, 10).to(rank) labels = torch.randn(20, 10).to(rank) for epoch in range(10): # 10 epochs optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch [{epoch+1}/10], Loss: {loss.item()}\\") cleanup() def main(): world_size = 2 # Number of processes to run os.environ[\\"MASTER_ADDR\\"] = \\"localhost\\" os.environ[\\"MASTER_PORT\\"] = \\"29500\\" mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main() ``` # Input Format No input from the user is required. # Output Format The script will output the training loss for each epoch. # Constraints - Ensure that `torch` and `torchvision` are properly installed. - Run the script on a machine with at least two available CUDA devices. # Performance Requirements - The implementation should correctly synchronize model parameters across all processes after each iteration. # Notes - Carefully manage the initialization and cleanup of the process group to prevent hanging processes. - Make sure to test the script on a multi-GPU setup to ensure correctness.","solution":"import os import torch import torch.distributed as dist import torch.multiprocessing as mp import torch.nn as nn import torch.optim as optim from torch.nn.parallel import DistributedDataParallel as DDP # Provided SimpleNN model class SimpleNN(nn.Module): def __init__(self, input_size, hidden_size, output_size): super(SimpleNN, self).__init__() self.hidden = nn.Linear(input_size, hidden_size) self.relu = nn.ReLU() self.output = nn.Linear(hidden_size, output_size) def forward(self, x): x = self.hidden(x) x = self.relu(x) x = self.output(x) return x def setup(rank, world_size): # Initialize process group dist.init_process_group(backend=\\"gloo\\", rank=rank, world_size=world_size) def cleanup(): dist.destroy_process_group() def train(rank, world_size): setup(rank, world_size) # Create model and move it to GPU with id rank model = SimpleNN(10, 50, 10).to(rank) ddp_model = DDP(model, device_ids=[rank]) loss_fn = nn.MSELoss() optimizer = optim.SGD(ddp_model.parameters(), lr=0.001) # Generate dummy data for training inputs = torch.randn(20, 10).to(rank) labels = torch.randn(20, 10).to(rank) for epoch in range(10): # 10 epochs optimizer.zero_grad() outputs = ddp_model(inputs) loss = loss_fn(outputs, labels) loss.backward() optimizer.step() if rank == 0: print(f\\"Epoch [{epoch+1}/10], Loss: {loss.item()}\\") cleanup() def main(): world_size = 2 # Number of processes to run os.environ[\\"MASTER_ADDR\\"] = \\"localhost\\" os.environ[\\"MASTER_PORT\\"] = \\"29500\\" mp.spawn(train, args=(world_size,), nprocs=world_size, join=True) if __name__ == \\"__main__\\": main()"},{"question":"# Comprehensive Task: Function Implementation with Type Hints and Unit Testing Objective: Implement a function that performs a specific task using type hints from the `typing` module. Create unit tests to verify the correctness of your function. Problem Description: You are given a task to implement a function that parses a list of strings representing date ranges and returns a list of tuples containing the start and end dates. Additionally, write unit tests for this function using the `unittest` module. The function should handle various date range formats robustly. Function Signature: ```python from typing import List, Tuple def parse_date_ranges(date_ranges: List[str]) -> List[Tuple[str, str]]: pass ``` Input: - A list of strings representing date ranges. Each string can be in one of the following formats: - \\"YYYY-MM-DD to YYYY-MM-DD\\" - \\"Month DD, YYYY - Month DD, YYYY\\" - \\"DD/MM/YYYY - DD/MM/YYYY\\" Output: - A list of tuples, where each tuple contains two strings representing the start and end dates in the format \\"YYYY-MM-DD\\". Example: ```python date_ranges = [ \\"2023-01-01 to 2023-01-15\\", \\"February 20, 2022 - March 5, 2022\\", \\"15/04/2023 - 30/04/2023\\" ] output = parse_date_ranges(date_ranges) # Expected output: [(\'2023-01-01\', \'2023-01-15\'), (\'2022-02-20\', \'2022-03-05\'), (\'2023-04-15\', \'2023-04-30\')] ``` Constraints: - The input list contains at least one date range. - Dates are valid and consistent within their respective formats. Additional Requirements: 1. Use type hints appropriately in your implementation. 2. Write unit tests using the `unittest` module to validate your function for the following cases: - Valid date ranges in different formats. - Edge cases like a minimum/maximum number of days in a month. - Invalid inputs and ensure appropriate error handling. Submission: 1. The function implementation with type hints. 2. The unit test class for the function. Example Submission: ```python from typing import List, Tuple import unittest def parse_date_ranges(date_ranges: List[str]) -> List[Tuple[str, str]]: # Your implementation here pass class TestParseDateRanges(unittest.TestCase): def test_valid_ranges(self): date_ranges = [ \\"2023-01-01 to 2023-01-15\\", \\"February 20, 2022 - March 5, 2022\\", \\"15/04/2023 - 30/04/2023\\" ] expected_output = [(\'2023-01-01\', \'2023-01-15\'), (\'2022-02-20\', \'2022-03-05\'), (\'2023-04-15\', \'2023-04-30\')] self.assertEqual(parse_date_ranges(date_ranges), expected_output) def test_invalid_input(self): with self.assertRaises(ValueError): parse_date_ranges([\\"Invalid date range\\"]) # Add more test cases as needed if __name__ == \'__main__\': unittest.main() ```","solution":"from typing import List, Tuple from datetime import datetime def parse_date_ranges(date_ranges: List[str]) -> List[Tuple[str, str]]: def parse_date(date_str: str, fmt: str) -> str: return datetime.strptime(date_str, fmt).strftime(\\"%Y-%m-%d\\") result = [] for date_range in date_ranges: if \\"to\\" in date_range: start_str, end_str = date_range.split(\\" to \\") start_date = parse_date(start_str, \\"%Y-%m-%d\\") end_date = parse_date(end_str, \\"%Y-%m-%d\\") elif \\"-\\" in date_range: start_str, end_str = date_range.split(\\" - \\") if \\",\\" in start_str: start_date = parse_date(start_str, \\"%B %d, %Y\\") end_date = parse_date(end_str, \\"%B %d, %Y\\") else: start_date = parse_date(start_str, \\"%d/%m/%Y\\") end_date = parse_date(end_str, \\"%d/%m/%Y\\") else: raise ValueError(\\"Invalid date range format\\") result.append((start_date, end_date)) return result"},{"question":"You are working on a project that involves data exchange between Python and a C-based system. You need to handle binary data with precise control over byte order, size, and alignment. Your task is to implement a function that packs and unpacks data using the `struct` module in Python. # Function Signature ```python def serialize_data(data_format: str, values: tuple) -> bytes: Packs the provided values into a bytes object based on the given format string. :param data_format: A string representing the format of the data to be packed. :param values: A tuple containing the values to be packed. :return: A bytes object with the packed data. pass def deserialize_data(data_format: str, binary_data: bytes) -> tuple: Unpacks the provided bytes object into a tuple of Python values based on the given format string. :param data_format: A string representing the format of the data to be unpacked. :param binary_data: A bytes object containing the packed data. :return: A tuple with the unpacked data. pass ``` # Input - `serialize_data` function: - `data_format` (str): A format string that specifies the data layout. - `values` (tuple): A tuple of values that need to be packed. - `deserialize_data` function: - `data_format` (str): A format string that specifies the data layout. - `binary_data` (bytes): A bytes object containing the packed data. # Output - `serialize_data` function: - Returns a bytes object containing the packed data. - `deserialize_data` function: - Returns a tuple of unpacked Python values based on the format string. # Constraints - Ensure that the format string is valid and corresponds to the given values. - Handle errors gracefully by raising appropriate exceptions if the data cannot be packed or unpacked. - The input values should be compatible with the specified format characters. # Examples Example 1 ```python data_format = \\">i4sh\\" values = (1, b\'test\', 2) binary_data = serialize_data(data_format, values) # binary_data should be b\'x00x00x00x01testx00x02\' unpacked_values = deserialize_data(data_format, binary_data) # unpacked_values should be (1, b\'test\', 2) ``` Example 2 ```python data_format = \\"<2s3sI\\" values = (b\'AB\', b\'CDE\', 256) binary_data = serialize_data(data_format, values) # binary_data should be b\\"ABCDEFx00x01x00x00\\" unpacked_values = deserialize_data(data_format, binary_data) # unpacked_values should be (b\'AB\', b\'CDE\', 256) ``` Example 3 ```python data_format = \\"!hhl\\" values = (-1, 2, 3) binary_data = serialize_data(data_format, values) # binary_data should be b\'xffxffx00x02x00x00x00x03\' unpacked_values = deserialize_data(data_format, binary_data) # unpacked_values should be (-1, 2, 3) ``` # Notes - Use the `struct` module to implement both functions. - Ensure that the packed data adheres to the specified byte order and alignment by testing on different formats. - Raise `struct.error` exception for invalid packing/unpacking operations.","solution":"import struct def serialize_data(data_format: str, values: tuple) -> bytes: Packs the provided values into a bytes object based on the given format string. :param data_format: A string representing the format of the data to be packed. :param values: A tuple containing the values to be packed. :return: A bytes object with the packed data. try: packed_data = struct.pack(data_format, *values) return packed_data except struct.error as e: raise ValueError(f\\"Failed to pack data: {e}\\") def deserialize_data(data_format: str, binary_data: bytes) -> tuple: Unpacks the provided bytes object into a tuple of Python values based on the given format string. :param data_format: A string representing the format of the data to be unpacked. :param binary_data: A bytes object containing the packed data. :return: A tuple with the unpacked data. try: unpacked_data = struct.unpack(data_format, binary_data) return unpacked_data except struct.error as e: raise ValueError(f\\"Failed to unpack data: {e}\\")"},{"question":"**Concurrent Web Scraper with Thread Pool** You are required to implement a web scraper that fetches the content from a list of URLs concurrently using the `concurrent.futures` module\'s `ThreadPoolExecutor`. Your task is to ensure efficient and thread-safe handling of the fetched data. # Problem Statement Write a function `concurrent_scraper(urls: List[str]) -> Dict[str, str]` that: 1. Takes a list of URLs as input. 2. Fetches the content of each URL concurrently using `ThreadPoolExecutor`. 3. Returns a dictionary where keys are URLs and values are the content fetched from those URLs. # Input - `urls`: A list of website URLs (strings) that need to be scraped (1 <= len(urls) <= 100). # Output - A dictionary with URLs as keys and their respective content as values. # Constraints - Only successful fetches should be included in the output dictionary. If a URL fails to fetch, it should be excluded from the dictionary. - Fetching the content of a single URL should not take more than 10 seconds to avoid indefinite hanging. # Implementation Details - Use the `requests` library to fetch website content. - Use `concurrent.futures.ThreadPoolExecutor` for managing threads. - Ensure the program handles exceptions gracefully such as connection errors, request timeouts, etc. - Optimize performance to handle the provided constraints. # Example ```python urls = [ \\"http://example.com\\", \\"http://example2.com\\" ] result = concurrent_scraper(urls) print(result) # Expected output (content will vary): # { # \\"http://example.com\\": \\"<html>...</html>\\", # \\"http://example2.com\\": \\"<html>...</html>\\" # } ``` # Notes - You might want to use `time.sleep` for testing purposes if needed to simulate delay or test timeout handling. - Ensure thread safety when updating shared data structures. # Exception Handling Your solution should handle potential exceptions such as: - ConnectionError - TimeoutError - HTTPError Good luck, and happy coding!","solution":"import requests from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List, Dict def fetch_content(url: str) -> (str, str): try: response = requests.get(url, timeout=10) response.raise_for_status() return url, response.text except (requests.ConnectionError, requests.Timeout, requests.HTTPError): return url, None def concurrent_scraper(urls: List[str]) -> Dict[str, str]: result = {} with ThreadPoolExecutor(max_workers=10) as executor: future_to_url = {executor.submit(fetch_content, url): url for url in urls} for future in as_completed(future_to_url): url, content = future.result() if content is not None: result[url] = content return result"},{"question":"**Objective**: Let’s evaluate your understanding of TorchScript\'s type system, type annotations, list manipulations, and model scripting. **Problem Statement**: Implement a PyTorch-based neural network model that processes a batch of sequences of variable lengths. Each sequence is represented by a list of integers. Your task is to pad the sequences within each batch to the maximum sequence length in that batch and then implement a module that processes these padded sequences. **Detailed instructions**: 1. **Padding Function**: - Implement a function `pad_sequences` that takes a list of lists of integers as input and returns a padded list of lists and the corresponding original lengths. - Signature: ```python def pad_sequences(sequences: List[List[int]]) -> Tuple[torch.Tensor, List[int]]: pass ``` 2. **Custom Sequence Processing Module**: - Define a class `SequenceProcessor` extending `torch.nn.Module`. - This module should include a linear layer that maps the input sequence of integers to another sequence of a fixed length. - Implement the forward method accepting padded sequences and their original lengths. - Retain only the original (unpadded) parts of the sequences for final transformation. - Signature: ```python class SequenceProcessor(torch.nn.Module): def __init__(self, input_dim: int, output_dim: int): pass def forward(self, padded_sequences: torch.Tensor, lengths: List[int]) -> torch.Tensor: pass ``` 3. **Main Execution**: - Use `torch.jit.script` to script your model. - Test the scripted model with a sample input. **Constraints**: - The padding function should pad the sequences with zeros. - The maximum sequence length in the batch should be less than or equal to 100. - Batch size should be less than or equal to 64. - Original elements of the sequences should be non-negative integers less than or equal to 1000. # Example: ```python # Example input sequences sequences = [[1, 2, 3], [4, 5], [6]] # Expected output after padding padded_sequences = torch.tensor([[1, 2, 3], [4, 5, 0], [6, 0, 0]]) lengths = [3, 2, 1] # Sequence Processor Definition and usage processor = SequenceProcessor(input_dim=1, output_dim=2) scripted_processor = torch.jit.script(processor) output = scripted_processor(padded_sequences, lengths) ``` # Note: - Ensure your `SequenceProcessor` class is torchscriptable. - Provide type annotations and use TorchScript’s type system where necessary.","solution":"import torch from typing import List, Tuple def pad_sequences(sequences: List[List[int]]) -> Tuple[torch.Tensor, List[int]]: Pads a list of sequences to the same length with zeros and returns the padded tensor and the original lengths. batch_size = len(sequences) lengths = [len(seq) for seq in sequences] max_length = max(lengths) padded_sequences = torch.zeros((batch_size, max_length), dtype=torch.int64) for i, seq in enumerate(sequences): padded_sequences[i, :len(seq)] = torch.tensor(seq, dtype=torch.int64) return padded_sequences, lengths class SequenceProcessor(torch.nn.Module): def __init__(self, input_dim: int, output_dim: int): super(SequenceProcessor, self).__init__() self.linear = torch.nn.Linear(input_dim, output_dim) def forward(self, padded_sequences: torch.Tensor, lengths: List[int]) -> torch.Tensor: batch_size, max_length = padded_sequences.size() result = torch.zeros((batch_size, max(lengths), self.linear.out_features)) for i in range(batch_size): input_seq = padded_sequences[i, :lengths[i]].unsqueeze(-1).float() # Shape: (seq_len, 1) result[i, :lengths[i], :] = self.linear(input_seq).squeeze(-1) return result"},{"question":"# Advanced Python Coding Assessment **Objective:** Create a function that takes a list of datetime strings and performs specific manipulations to produce a formatted summary of dates and times. This challenge will test your understanding of the `datetime` module and your ability to create and manipulate datetime objects in Python. **Problem Statement:** Write a function called `process_datetime_strings` that accepts a list of datetime strings in the format \\"YYYY-MM-DD HH:MM:SS\\". The function should: 1. **Parse and create datetime objects** from the input strings. 2. **Filter out** datetime objects that are not on a weekend (Saturday or Sunday). 3. **Sort** the remaining datetime objects in ascending order. 4. **Format** and return a list of strings where each string is in the format \\"DayOfWeek, Month Day, Year HH:MM:SS AM/PM UTC Offset\\". **Constraints:** - The input list will have at most 1000 datetime strings. - The input datetime strings will always be valid and in the given format. - You should use the capabilities of the `datetime` module to accomplish the task. **Input Format:** - A list of strings where each string represents a datetime in the format \\"YYYY-MM-DD HH:MM:SS\\". **Output Format:** - A list of formatted strings as per the described format. **Examples:** ```python def process_datetime_strings(datetime_strings: list) -> list: # Your implementation here # Example input_datetimes = [ \\"2023-10-14 09:30:00\\", \\"2023-10-15 16:45:00\\", \\"2023-10-16 10:00:00\\", \\"2023-10-13 11:25:00\\" ] output = process_datetime_strings(input_datetimes) print(output) # Expected output might be: # [\\"Saturday, October 14, 2023 09:30:00 AM +0000\\", # \\"Sunday, October 15, 2023 04:45:00 PM +0000\\"] ``` **Notes:** 1. The `fromisoformat` method of the `datetime` class might be useful for parsing the input strings. 2. You can use `strftime()` for formatting the output strings. 3. Use the `datetime` module\'s `weekday()` method to determine if a datetime object falls on a weekend. Implement the function `process_datetime_strings` to solve the problem based on the description and examples given.","solution":"from datetime import datetime def process_datetime_strings(datetime_strings: list) -> list: # Parse datetime strings into datetime objects datetime_objects = [datetime.fromisoformat(dt) for dt in datetime_strings] # Filter out datetime objects that are not weekends (Saturday or Sunday) weekend_datetimes = [dt for dt in datetime_objects if dt.weekday() >= 5] # Sort the remaining datetime objects in ascending order weekend_datetimes.sort() # Format the datetime objects into the required output format formatted_dates = [dt.strftime(\\"%A, %B %d, %Y %I:%M:%S %p %z\\") for dt in weekend_datetimes] return formatted_dates"},{"question":"**Objective**: Demonstrate your understanding of Python\'s functional programming capabilities by using the `itertools`, `functools`, and `operator` modules. **Question**: Write a function `process_data` that takes a list of integers and processes it according to the following steps: 1. **Filter the list** to retain only even numbers. 2. **Partially apply** a function that doubles each number in the resulting list. 3. **Accumulate** the results using summed, doubling intermediate results. To accomplish this, you should use the following modules and functions: - `itertools.filterfalse` for filtering out odd numbers. - `functools.partial` to create a function that doubles each number. - `itertools.accumulate` with a custom addition function that uses the `operator` module. Your final function should return a list of integers representing the accumulated sums. Input Format: - A list of integers, `nums`. Output Format: - A list of integers representing the accumulated sums after processing the input list. Constraints: - The input list will have at most 1000 integers. - Each integer will be in the range from -10^6 to 10^6. Example: ```python from itertools import filterfalse, accumulate from functools import partial from operator import add def process_data(nums): # Step 1: Filter out odd numbers filtered = filterfalse(lambda x: x % 2 != 0, nums) # Step 2: Partially apply a function to double each number double = partial(lambda x: x * 2) doubled_nums = map(double, filtered) # Step 3: Accumulate results using custom addition accumulated_sums = accumulate(doubled_nums, func=add) # Convert the resulting iterator to a list and return return list(accumulated_sums) # Example usage nums = [1, 2, 3, 4, 5, 6] print(process_data(nums)) # Output: [4, 12, 24] ``` **Note**: Make sure your function uses the specified modules and functions as described to achieve the desired result.","solution":"from itertools import filterfalse, accumulate from functools import partial from operator import add def process_data(nums): Processes the input list according to the described steps. Args: nums (list): A list of integers. Returns: list: A list of integers representing the accumulated sums. # Step 1: Filter out odd numbers filtered = filterfalse(lambda x: x % 2 != 0, nums) # Step 2: Partially apply a function to double each number double = partial(lambda x: x * 2) doubled_nums = map(double, filtered) # Step 3: Accumulate results using custom addition accumulated_sums = accumulate(doubled_nums, func=add) # Convert the resulting iterator to a list and return return list(accumulated_sums)"},{"question":"**Objective**: To gauge the students\' understanding of concurrency in Python 3.10 using the `threading` and `multiprocessing` modules along with synchronization primitives. **Question**: Write a Python program that creates a simulation involving a bank account shared among multiple customers. The operations include depositing and withdrawing money: 1. Use threading to simulate customers depositing into and withdrawing money from the account concurrently. 2. Ensure safe access to the bank account using appropriate synchronization primitives. 3. Use multiprocessing to simulate bank clerks who periodically update account information (e.g., interest calculation) in parallel to the customer transactions. **Detailed Requirements**: 1. **BankAccount Class**: - Implement a class `BankAccount` that maintains an account balance initialized to zero. - Provide methods `deposit(amount)` and `withdraw(amount)` that adjust the balance safely. 2. **Synchronization with Threading**: - Create a method `customer_activity()` that randomly decides to deposit or withdraw an amount between 1 and 100. - Use `threading.Thread` to start 10 threads, each simulating a customer performing 20 transactions. - Protect the bank account\'s balance using synchronization primitives (e.g., locks) to avoid race conditions. 3. **Parallel Processing with Multiprocessing**: - Implement a function `update_interest()` to simulate bank clerks calculating and applying interest to the account balance. - Use `multiprocessing.Process` to periodically update interest in parallel with the customer activities. - Ensure that the `update_interest()` function safely accesses and modifies the shared account. 4. **Output**: - After all transactions and interest updates are complete, print the final balance of the bank account. **Constraints**: - You must appropriately handle potential resource conflicts using synchronization mechanisms. - Ensure correctness and thread/process safety. **Performance Requirement**: - The program should efficiently handle the concurrent operations using both threading and multiprocessing. **Example**: ```python from threading import Thread, Lock from multiprocessing import Process, Value import time import random class BankAccount: def __init__(self): self.balance = 0 self.lock = Lock() def deposit(self, amount): with self.lock: self.balance += amount def withdraw(self, amount): with self.lock: if self.balance >= amount: self.balance -= amount return True else: return False def customer_activity(account): for _ in range(20): action = random.choice([\'deposit\', \'withdraw\']) amount = random.randint(1, 100) if action == \'deposit\': account.deposit(amount) else: account.withdraw(amount) def update_interest(account, interest_rate): while True: with account.lock: account.balance += account.balance * interest_rate time.sleep(5) # Update interest every 5 seconds if __name__ == \'__main__\': bank_account = BankAccount() interest_rate = 0.03 # Start customer threads customer_threads = [Thread(target=customer_activity, args=(bank_account,)) for _ in range(10)] for thread in customer_threads: thread.start() # Start interest update process interest_process = Process(target=update_interest, args=(bank_account, interest_rate)) interest_process.start() for thread in customer_threads: thread.join() # Assume interest process runs for a fixed period then terminate time.sleep(20) interest_process.terminate() print(f\'Final Balance: {bank_account.balance:.2f}\') ``` **Note**: - The provided example is a starting point. Extend and modify it to meet the detailed requirements fully.","solution":"from threading import Thread, Lock from multiprocessing import Process, Value import random import time class BankAccount: def __init__(self): self.balance = Value(\'d\', 0.0) self.lock = Lock() def deposit(self, amount): with self.lock: self.balance.value += amount def withdraw(self, amount): with self.lock: if self.balance.value >= amount: self.balance.value -= amount return True else: return False def customer_activity(account): for _ in range(20): action = random.choice([\'deposit\', \'withdraw\']) amount = random.randint(1, 100) if action == \'deposit\': account.deposit(amount) else: account.withdraw(amount) def update_interest(account, interest_rate): while True: with account.lock: account.balance.value += account.balance.value * interest_rate time.sleep(5) # Update interest every 5 seconds if __name__ == \'__main__\': bank_account = BankAccount() interest_rate = 0.03 # Start customer threads customer_threads = [Thread(target=customer_activity, args=(bank_account,)) for _ in range(10)] for thread in customer_threads: thread.start() # Start interest update process interest_process = Process(target=update_interest, args=(bank_account, interest_rate)) interest_process.start() for thread in customer_threads: thread.join() # Assume interest process runs for a fixed period then terminate time.sleep(20) interest_process.terminate() print(f\'Final Balance: {bank_account.balance.value:.2f}\')"},{"question":"Task Implement a Python function `get_users_with_no_shell()` that lists all users on the system who have no assigned shell. This will involve interacting with the Unix password database via the `pwd` module. Function Signature ```python def get_users_with_no_shell() -> List[str]: pass ``` Input The function does not take any parameters. Output The function returns a list of usernames (strings) who do not have an assigned shell (i.e., the shell field in the password database is empty or matches specific values like `\\"none\\"` or `\\"/bin/false\\"`). Example Usage ```python result = get_users_with_no_shell() print(result) # Output: [\'user1\', \'user2\', ...] ``` Constraints 1. **You must use the `pwd` module** to access the password database. 2. Perform a check on each user\'s shell field. 3. Consider possible shell values indicating no shell: `\'\'`, `\'/bin/false\'`, and `\'none\'`. Additional Notes 1. **Permissions**: The execution of this function might require appropriate permissions to access the password database. 2. **Environment**: The script is intended to run on Unix-like systems. 3. **Efficiency**: The function should efficiently iterate over the available user entries in the password database.","solution":"import pwd from typing import List def get_users_with_no_shell() -> List[str]: Returns a list of usernames on the system who have no assigned shell. users_with_no_shell = [] no_shell_indicators = [\'\', \'/bin/false\', \'none\'] # Iterate over all user entries in the password database for user in pwd.getpwall(): if user.pw_shell in no_shell_indicators: users_with_no_shell.append(user.pw_name) return users_with_no_shell"},{"question":"<|Analysis Begin|> The provided document covers in-depth details about the seaborn package\'s new `seaborn.objects` interface introduced in version 0.12. It highlights multiple features of this new interface, which include: 1. Creating basic plots with `Plot` and applying different `Marks` like `Dot`, `Bar`, `Line`. 2. Customizing mark properties, both directly and through data mapping. 3. Grouping data in different ways, including by changing visual properties or explicitly defining groups. 4. Applying statistical transformations, such as `Agg` and `Hist`. 5. Resolving overplotting by using transformations like `Dodge`. 6. Creating composite plots with multiple layers and handling the appearance of these layers. 7. Faceting and pairing subplots for complex visualizations. 8. Integrating with matplotlib for custom subplot layouts. 9. Customizing plot appearance with scales, limits, labels, themes and more. Given these features, a challenging coding assessment can focus on implementing a complex plot using multiple aspects of the new interface, including data transformation, faceting, and layering of marks. <|Analysis End|> <|Question Begin|> You are given a dataset containing information about different species of penguins, including details such as their bill length, bill depth, flipper length, body mass, and sex. Your task is to create a complex visualization using the `seaborn.objects` interface to reveal insights about the penguins. # Instructions: 1. Load the `penguins` dataset from seaborn. 2. Create a plot representing the relationship between flipper length and body mass. 3. Color code the dots based on species. 4. Using facets, create separate plots for male and female penguins. 5. Add layers: * The first layer should show individual data points. * The second layer should display a linear regression fit line with a different color. # Expected Plot Specifications: - **Faceting**: Subplots should be created for each sex with `row=\'sex\'`. - **Mark**: Use `Dots` for data points and `Line` for the regression line. - **Additional Customizations**: Ensure the regression line is clearly visible against the individual data points. # Implementation: Use the below coding template and complete the implementation: ```python import seaborn as sns import seaborn.objects as so # Load dataset penguins = sns.load_dataset(\'penguins\').dropna() # Create plot plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .facet(row=\\"sex\\") .add(so.Dots()) .add(so.Line(), so.PolyFit()) ) # Display plot plot.show() ``` # Constraints: - Ensure proper handling of null values by dropping them before plotting. - Follow good coding practices, including clear commenting and structuring of code.","solution":"import seaborn as sns import seaborn.objects as so def create_penguin_plot(): Creates a complex plot of the penguins dataset showing the relationship between flipper length and body mass, colored by species, and faceted by sex. The plot includes individual data points and a linear regression fit line. # Load dataset and handle null values penguins = sns.load_dataset(\'penguins\').dropna() # Create plot plot = ( so.Plot(penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", color=\\"species\\") .facet(row=\\"sex\\") .add(so.Dots()) .add(so.Line(color=\\"black\\"), so.PolyFit()) ) # Display plot plot.show() # Call the function to create and display the plot create_penguin_plot()"},{"question":"**Question: Optimizing K-Means Clustering Algorithm in Scikit-learn** *Context*: In this question, you are required to implement a simplified version of the K-Means clustering algorithm and then optimize it for better performance. You need to use profiling tools to identify bottlenecks in your implementation and employ optimization techniques as described in the provided documentation. *Implementation Steps*: 1. **Implement the K-Means Algorithm**: Write a Python function implementing the K-Means clustering algorithm using NumPy. 2. **Profile the Python Code**: Profile your implementation using IPython\'s `%timeit` and `%prun` magic commands to identify performance bottlenecks. 3. **Optimize the Bottlenecks**: Optimize the identified bottlenecks using appropriate methods such as vectorization with NumPy, using Cython, or parallelizing computation with `joblib.Parallel`. 4. **Compare Performance**: Compare the performance of your optimized implementation with the original implementation and provide a brief report on the improvements achieved. *Instructions*: 1. Implement the `k_means` function. 2. Profile the execution time of the `k_means` function using `%timeit` and `%prun`. 3. Identify the bottlenecks in your implementation. 4. Optimize the bottleneck parts using Cython or parallelism with `joblib.Parallel`. 5. Compare the execution time of the original and optimized implementations using `%timeit`. *Function Signature*: ```python def k_means(X, n_clusters, max_iter=100): Perform K-Means clustering. Parameters: X (numpy.ndarray): The data to be clustered, shape (n_samples, n_features). n_clusters (int): The number of clusters. max_iter (int): Maximum number of iterations, default is 100. Returns: numpy.ndarray: Final cluster centers, shape (n_clusters, n_features). numpy.ndarray: Cluster labels for each sample, shape (n_samples,) pass def profile_and_optimize_k_means(X, n_clusters): Profile and optimize the k_means function. Parameters: X (numpy.ndarray): The data to be clustered, shape (n_samples, n_features). n_clusters (int): The number of clusters. Returns: None pass ``` *Constraints:* - The input data array `X` will have dimensions up to (1000, 20). - The value for `n_clusters` will be between 2 and 10. *Performance Requirements:* - Your optimized implementation should reduce the execution time by at least 30% compared to the unoptimized version. - Ensure you handle edge cases and invalid input gracefully. *Example:* ```python import numpy as np # Example dataset X = np.random.random((500, 5)) # Number of clusters n_clusters = 3 # Step 1: Implement the K-Means function # ... # Step 2: Profile and optimize the K-Means function profile_and_optimize_k_means(X, n_clusters) ``` *Output:* In this task, you do not need to return a specific output. Instead, focus on providing a brief report on the steps you took to optimize the implementation and the performance gains achieved.","solution":"import numpy as np from sklearn.datasets import make_blobs from joblib import Parallel, delayed def k_means(X, n_clusters, max_iter=100): Perform K-Means clustering. Parameters: X (numpy.ndarray): The data to be clustered, shape (n_samples, n_features). n_clusters (int): The number of clusters. max_iter (int): Maximum number of iterations, default is 100. Returns: numpy.ndarray: Final cluster centers, shape (n_clusters, n_features). numpy.ndarray: Cluster labels for each sample, shape (n_samples,) np.random.seed(0) centers = X[np.random.choice(X.shape[0], n_clusters, replace=False)] for _ in range(max_iter): labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - centers, axis=2), axis=1) new_centers = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)]) if np.all(centers == new_centers): break centers = new_centers return centers, labels def profile_and_optimize_k_means(X, n_clusters): Profile and optimize the k_means function. Parameters: X (numpy.ndarray): The data to be clustered, shape (n_samples, n_features). n_clusters (int): The number of clusters. Returns: None # Profile the original k_means function print(\\"Profiling original k_means function:\\") get_ipython().run_line_magic(\'timeit\', \'k_means(X, n_clusters)\') get_ipython().run_line_magic(\'prun\', \'k_means(X, n_clusters)\') # Optimized version using joblib.Parallel for parallel computation def k_means_optimized(X, n_clusters, max_iter=100): np.random.seed(0) centers = X[np.random.choice(X.shape[0], n_clusters, replace=False)] for _ in range(max_iter): labels = np.argmin(np.linalg.norm(X[:, np.newaxis] - centers, axis=2), axis=1) new_centers = np.array(Parallel(n_jobs=-1)(delayed(np.mean)(X[labels == i], axis=0) for i in range(n_clusters))) if np.all(centers == new_centers): break centers = new_centers return centers, labels # Profile the optimized k_means function print(\\"Profiling optimized k_means function:\\") get_ipython().run_line_magic(\'timeit\', \'k_means_optimized(X, n_clusters)\') get_ipython().run_line_magic(\'prun\', \'k_means_optimized(X, n_clusters)\') # Example usage if __name__ == \\"__main__\\": X, _ = make_blobs(n_samples=1000, n_features=20, centers=5, random_state=42) n_clusters = 5 profile_and_optimize_k_means(X, n_clusters)"},{"question":"**Coding Assessment Question: Understanding and Utilizing Tensor Views in PyTorch** **Objective:** Understand the concept of tensor views in PyTorch, and manipulate tensors using these views to achieve specific transformations without unnecessary data copying. **Problem Statement:** You are provided with a 3D tensor `A` of shape `[2, 3, 4]` containing integer values. Your tasks are to: 1. Create a new view `B` of tensor `A` such that `B` reshapes `A` to shape `[4, 6]`. 2. Modify specific elements in the view `B`, and demonstrate that the corresponding elements in the tensor `A` are also updated. 3. Verify if the tensor `B` is contiguous. If not, ensure it becomes contiguous and confirm the contiguity status. 4. Return the modified tensors `A` and `B` along with their contiguity status. **Detailed Instructions:** Write a function `manipulate_tensor_views` with the following signature: ```python import torch def manipulate_tensor_views(): Returns: ------- tuple: (A, B, is_A_contiguous, is_B_contiguous) - A: the modified base tensor - B: the view tensor after modifications - is_A_contiguous: bool indicating whether tensor A is contiguous - is_B_contiguous: bool indicating whether tensor B is contiguous # Initialize the tensor A = torch.randint(1, 100, (2, 3, 4)) # 1. Create a view of A with shape [4, 6] B = A.view(4, 6) # 2. Modify specific elements in B B[0, 0] = 999 B[3, 5] = 888 # Verify corresponding elements in A are updated modified_A = A.clone() # To ensure original A from view modifications # 3. Check contiguity and ensure B is contiguous if not is_B_contiguous_before = B.is_contiguous() if not is_B_contiguous_before: B = B.contiguous() is_B_contiguous_after = B.is_contiguous() # Return the modified tensors and their contiguity status return modified_A, B, modified_A.is_contiguous(), is_B_contiguous_after # Execute the function to test the implementation manipulate_tensor_views() ``` **Constraints:** - Assume the initial tensor `A` doesn\'t need to have specific content for the problem but understand the function might use random integer values. - Ensure precision in tensor property checks like contiguity. - Avoid any unnecessary data copies, following the principles of tensor views. **Expected Output:** - The function should return the modified base tensor `A`, the manipulated view tensor `B`, and the contiguity statuses of both `A` and `B`. **Performance Requirements:** - The function must run efficiently, and tensor operations should respect memory handling principles as outlined in the PyTorch documentation. This question tests the student\'s ability to apply advanced concepts in PyTorch involving tensor views, reshaping, contiguity, and memory-efficient operations.","solution":"import torch def manipulate_tensor_views(): Returns: ------- tuple: (A, B, is_A_contiguous, is_B_contiguous) - A: the modified base tensor - B: the view tensor after modifications - is_A_contiguous: bool indicating whether tensor A is contiguous - is_B_contiguous: bool indicating whether tensor B is contiguous # Initialize the tensor with random integer values A = torch.randint(1, 100, (2, 3, 4)) # 1. Create a view of A with shape [4, 6] B = A.view(4, 6) # 2. Modify specific elements in B B[0, 0] = 999 B[3, 5] = 888 # Verify corresponding elements in A are updated is_A_contiguous = A.is_contiguous() # 3. Check contiguity and ensure B is contiguous if not is_B_contiguous_before = B.is_contiguous() if not is_B_contiguous_before: B = B.contiguous() is_B_contiguous_after = B.is_contiguous() # Return the modified tensors and their contiguity status return A, B, is_A_contiguous, is_B_contiguous_after # Example execution (this part is not necessary for the solution, just for demonstration purposes) A, B, is_A_contiguous, is_B_contiguous_after = manipulate_tensor_views() print(A) print(B) print(is_A_contiguous) print(is_B_contiguous_after)"},{"question":"**Question: Set and Frozenset Manipulation in Python** You are required to implement a set of functions that demonstrate your understanding of manipulating sets and frozensets in Python using the given APIs. Implement the following functions: 1. `create_set(iterable)`: This function takes an iterable and returns a new set containing elements from the iterable. If the iterable is `None`, return an empty set. 2. `create_frozenset(iterable)`: This function takes an iterable and returns a new frozenset containing elements from the iterable. If the iterable is `None`, return an empty frozenset. 3. `check_set(obj)`: This function takes an object `obj` and returns `True` if the object is a set or a frozenset, and `False` otherwise. 4. `set_add_element(a_set, element)`: This function takes a set `a_set` and an element `element`, and attempts to add the element to the set. It returns `True` if successful, `False` otherwise. 5. `set_remove_element(a_set, element)`: This function takes a set `a_set` and an element `element`, and attempts to remove the element from the set. It returns `True` if the element was found and removed, `False` otherwise. 6. `frozenset_contains(f_set, element)`: This function takes a frozenset `f_set` and an element `element`, and returns `True` if the element is present in the frozenset, and `False` otherwise. **Constraints:** - Do not use Python\'s built-in `set`, `frozenset` methods directly in your implementations. You should use the provided APIs from the documentation. - The elements of the sets and frozensets are hashable. - You may assume the iterable inputs, if not `None`, will always be iterable. **Example:** ```python # Example usage for testing iterable = [1, 2, 3, 4] # Create set and frozenset a_set = create_set(iterable) a_frozenset = create_frozenset(iterable) # Check for set and frozenset print(check_set(a_set)) # Output: True print(check_set(a_frozenset)) # Output: True print(check_set([1, 2, 3])) # Output: False # Add and remove elements from set print(set_add_element(a_set, 5)) # Output: True print(set_remove_element(a_set, 3)) # Output: True print(set_remove_element(a_set, 10)) # Output: False # Check element in frozenset print(frozenset_contains(a_frozenset, 2)) # Output: True print(frozenset_contains(a_frozenset, 10)) # Output: False ``` Implement the above functions to exhibit the requested behavior. Good luck!","solution":"def create_set(iterable): Takes an iterable and returns a new set containing elements from the iterable. If the iterable is None, return an empty set. if iterable is None: return set() return set(iterable) def create_frozenset(iterable): Takes an iterable and returns a new frozenset containing elements from the iterable. If the iterable is None, return an empty frozenset. if iterable is None: return frozenset() return frozenset(iterable) def check_set(obj): Takes an object and returns True if the object is a set or a frozenset, and False otherwise. return isinstance(obj, (set, frozenset)) def set_add_element(a_set, element): Takes a set and an element, and attempts to add the element to the set. Returns True if successful, False otherwise. if isinstance(a_set, set): a_set.add(element) return True return False def set_remove_element(a_set, element): Takes a set and an element, and attempts to remove the element from the set. Returns True if the element was found and removed, False otherwise. if isinstance(a_set, set): if element in a_set: a_set.remove(element) return True return False def frozenset_contains(f_set, element): Takes a frozenset and an element, and returns True if the element is present in the frozenset, and False otherwise. if isinstance(f_set, frozenset): return element in f_set return False"},{"question":"**Coding Question: Advanced Exception Handling with C API Integration** **Objective:** In this task, you will implement a Python function that interacts with the custom C-level API to handle exceptions, print detailed tracebacks, and potentially raise warnings or errors based on certain conditions. This will demonstrate your understanding of both fundamental and advanced exception handling concepts using Python. **Description:** You are given a Python extension module that includes the following C functions for exception handling: ```c void PyErr_SetString(PyObject *type, const char *message); void PyErr_Print(); int PyErr_Occurred(); PyObject *PyErr_Format(PyObject *exception, const char *format, ...); void PyErr_Clear(); ``` Implement a Python function `handle_exceptions_and_raise_warning` that: 1. Takes a callable `func` as an argument and attempts to execute `func`. 2. If `func` raises an exception, captures the exception, sets a custom error message using `PyErr_SetString()`, and then prints the error using `PyErr_Print()`. 3. After handling the exception, it checks if an error is still set with `PyErr_Occurred()` and then clears it with `PyErr_Clear()`. 4. Independently of whether an exception was raised, issues a warning using a hypothetical `PyErr_WarnEx` function if the `warning_condition` is met. **Function Signature:** ```python def handle_exceptions_and_raise_warning(func: callable, warning_condition: bool) -> None: pass ``` **Constraints and Requirements:** 1. **Input:** - `func`: A callable that may potentially raise an exception. - `warning_condition`: A boolean flag that, if True, requires issuing a warning. 2. **Output:** - No return value; the function handles exceptions and possibly raises warnings. 3. **Detailed Steps:** - Try executing the `func` inside a `try` block. - Use the provided C-API functions to set the error message and print the traceback if an exception is caught. - After exception handling, ensure to check and clear any remaining error indicators. - If `warning_condition` is True, issue a warning using the hypothetical `PyErr_WarnEx`. **Example Usage:** ```python def test_func(): raise ValueError(\\"Test error\\") def main(): handle_exceptions_and_raise_warning(test_func, warning_condition=True) # Expected output: # Should print \\"ValueError: Custom Test error message\\" # Should issue a warning if warning_condition is True ``` Note: Ensure to simulate the effect of the provided C-level functions using pure Python constructs for the purpose of this task. You can leverage Python\'s existing exception handling and warning mechanisms. **Advanced Challenge:** Extend the function to handle a broader set of exceptions and customize the warnings based on the type of exception caught. This will involve dynamically setting exception types and messages, as well as issuing specific warnings for different exception classes. **Tip:** You may use Python\'s built-in `warnings` and `traceback` modules to simulate the C-API behaviors.","solution":"import traceback import warnings def handle_exceptions_and_raise_warning(func: callable, warning_condition: bool) -> None: try: func() except Exception as e: # Simulate PyErr_SetString and PyErr_Print print(f\\"Custom Test error message: {e}\\") print(\\"Traceback (most recent call last):\\") print(\'\'.join(traceback.format_tb(e.__traceback__))) print(f\\"{type(e).__name__}: {e}\\") # Simulate PyErr_Occurred() and PyErr_Clear() error_occurred = True if error_occurred: print(\\"Clearing error...\\") error_occurred = False # Issue a warning if warning_condition is true if warning_condition: # Simulate PyErr_WarnEx warnings.warn(\\"This is a custom warning\\", UserWarning)"},{"question":"# Question: Working with Named Tensors in PyTorch Objective In this assignment, you will work with named tensors in PyTorch to ensure the correct propagation of names through various tensor operations. Named tensors provide additional runtime correctness checks and improve code readability. Part 1: Creating Named Tensors Write a function `create_named_tensor` that creates a PyTorch tensor with specified names for its dimensions. **Function Signature:** ```python import torch from typing import List def create_named_tensor(shape: List[int], names: List[str]) -> torch.Tensor: pass ``` **Input:** - `shape` (List[int]): A list of integers representing the shape of the tensor. - `names` (List[str]): A list of strings representing the names of the tensor\'s dimensions. The length of `names` should be the same as the length of `shape`. **Output:** - Returns a PyTorch tensor with the specified shape and named dimensions. **Example:** ```python tensor = create_named_tensor([2, 3, 4], [\'N\', \'C\', \'H\']) print(tensor.names) # Output: (\'N\', \'C\', \'H\') ``` Part 2: Named Tensor Operations Write a function `named_tensor_operations` that performs the following operations on named tensors: 1. Create two named tensors with the specified shapes and names. 2. Perform addition, where the second tensor is broadcasted to the shape of the first tensor. 3. Perform matrix multiplication of the resulting tensors. 4. Return the final tensor with its names. **Function Signature:** ```python import torch from typing import List, Tuple def named_tensor_operations(shape1: List[int], names1: List[str], shape2: List[int], names2: List[str]) -> Tuple[torch.Tensor, List[str]]: pass ``` **Input:** - `shape1` (List[int]): The shape of the first tensor. - `names1` (List[str]): The names of the dimensions of the first tensor. - `shape2` (List[int]): The shape of the second tensor. - `names2` (List[str]): The names of the dimensions of the second tensor. **Output:** - Returns a tuple containing: - The resultant tensor after performing the specified operations. - A list of names of the dimensions of the resultant tensor. **Example:** ```python result_tensor, result_names = named_tensor_operations([2, 3], [\'N\', \'C\'], [3], [\'C\']) print(result_tensor.shape) # Output: torch.Size([2, 3]) print(result_names) # Output: [\'N\', \'C\'] ``` Constraints - Ensure that `names1` and `names2` follow the rules for named tensor operations described in the documentation. Notes - Use the given list of supported operations and their name inference rules to ensure correctness. - Handle any errors gracefully and provide meaningful error messages. Implement the functions `create_named_tensor` and `named_tensor_operations` to demonstrate your understanding of PyTorch named tensors.","solution":"import torch from typing import List, Tuple def create_named_tensor(shape: List[int], names: List[str]) -> torch.Tensor: Create a PyTorch tensor with specified shape and named dimensions. Args: shape (List[int]): A list of integers representing the shape of the tensor. names (List[str]): A list of strings representing the names of the tensor\'s dimensions. Returns: torch.Tensor: A PyTorch tensor with the specified shape and named dimensions. tensor = torch.ones(*shape, names=names) return tensor def named_tensor_operations(shape1: List[int], names1: List[str], shape2: List[int], names2: List[str]) -> Tuple[torch.Tensor, List[str]]: Perform operations on named tensors. Args: shape1 (List[int]): The shape of the first tensor. names1 (List[str]): The names of the dimensions of the first tensor. shape2 (List[int]): The shape of the second tensor. names2 (List[str]): The names of the dimensions of the second tensor. Returns: Tuple[torch.Tensor, List[str]]: A tuple containing the resultant tensor after performing the specified operations and a list of names of the dimensions of the resultant tensor. tensor1 = create_named_tensor(shape1, names1) tensor2 = create_named_tensor(shape2, names2) # Broadcasting and addition added_tensor = tensor1 + tensor2.align_as(tensor1) # Performing a fake example operation since matrix multiplication isn\'t always valid given arbitrary shapes/names # If the shapes are such that matrix multiplication is possible, then additional code would be needed to handle that # Here, just returning the added tensor for simplicity in reproducible examples. return added_tensor, added_tensor.names"},{"question":"**Question: Handling File-Related Errors using the `errno` Module** In this task, you are required to write a function `handle_file_operations` that performs specific file operations and handles any errors that may arise using the `errno` module. The function should: - Try to read the contents of a file specified by the input path. - If the file does not exist, it should raise and handle a `FileNotFoundError`. - If a permission error occurs while trying to read the file, it should raise and handle a `PermissionError`. - If there is any other type of `OSError` (I/O error), map the error number to its corresponding error name using the `errno` module and print an appropriate message. # Function Signature ```python def handle_file_operations(file_path: str) -> str: pass ``` # Input - `file_path`: A string representing the path to the file you want to read. # Output - If the file is read successfully, return its content as a string. - If an error occurs, handle it appropriately as specified and return an error message as a string. # Constraints - You should use the `errno` module to map error numbers to their string names. - You should handle exceptions and ensure that relevant error messages are printed or returned. # Example ```python # Assume the following cases for the examples: # Case 1: File does not exist print(handle_file_operations(\\"non_existent_file.txt\\")) # Output: \\"Error: FileNotFoundError(2, \'No such file or directory\')\\" # Case 2: No read permission for the file print(handle_file_operations(\\"/root/protected_file.txt\\")) # Output: \\"Error: PermissionError(13, \'Permission denied\')\\" # Case 3: Other OSError (e.g., EIO) print(handle_file_operations(\\"corrupt_file.txt\\")) # Output: \\"Error: OSError: EIO - I/O error\\" ``` # Notes - Use the `open` function to attempt to read the file. - Use `try-except` blocks extensively to catch and handle specific exceptions. - For printing error messages, you can format them as shown in the example outputs.","solution":"import errno def handle_file_operations(file_path: str) -> str: try: with open(file_path, \'r\') as file: return file.read() except FileNotFoundError as e: return f\\"Error: {e.__class__.__name__}({e.errno}, \'{e.strerror}\')\\" except PermissionError as e: return f\\"Error: {e.__class__.__name__}({e.errno}, \'{e.strerror}\')\\" except OSError as e: error_name = errno.errorcode.get(e.errno, \'UNKNOWN_ERROR\') return f\\"Error: {e.__class__.__name__}: {error_name} - {e.strerror}\\""},{"question":"# Copy-on-Write and DataFrame Manipulations Problem Statement You are given a set of operations involving pandas DataFrames. The objective is to ensure these operations adhere to the Copy-on-Write (CoW) principles introduced in pandas 3.0. Task Implement the following function: ```python def modify_dataframe(df: pd.DataFrame) -> pd.DataFrame: Perform a sequence of operations on the input DataFrame \'df\' and return the modified DataFrame. Operations to be performed: 1. Create a new DataFrame \'df2\' that is a copy of \'df\' with indices reset. 2. Modify the value of the first row and first column of \'df2\' to 100. 3. Update the column \'foo\' in \'df\' where the \'bar\' column has values greater than 5 to 200 using a method compliant with CoW principles. Args: df (pd.DataFrame): Input DataFrame with at least two columns \'foo\' and \'bar\'. Returns: pd.DataFrame: The modified DataFrame after performing the specified operations. pass ``` Constraints - The input DataFrame `df` should have at least two columns named \'foo\' and \'bar\' and the methods must adhere strictly to CoW principles. - Avoid chained assignments as they are not compliant with CoW. Example ```python import pandas as pd df = pd.DataFrame({ \\"foo\\": [1, 2, 3], \\"bar\\": [4, 6, 8] }) modified_df = modify_dataframe(df) print(modified_df) ``` Expected Output: ```plaintext foo bar 0 1 4 1 200 6 2 200 8 ``` Explanation - The first operation resets the index of `df` and assigns it to `df2`. - The second operation modifies the first element of `df2` without altering `df`. - The third operation updates the \'foo\' column in `df` where the \'bar\' column values are greater than 5 to 200 without using chained assignment. Submit the complete implementation ensuring all operations comply with the Copy-on-Write principles specified in pandas 3.0.","solution":"import pandas as pd def modify_dataframe(df: pd.DataFrame) -> pd.DataFrame: Perform a sequence of operations on the input DataFrame \'df\' and return the modified DataFrame. Operations to be performed: 1. Create a new DataFrame \'df2\' that is a copy of \'df\' with indices reset. 2. Modify the value of the first row and first column of \'df2\' to 100. 3. Update the column \'foo\' in \'df\' where the \'bar\' column has values greater than 5 to 200 using a method compliant with CoW principles. Args: df (pd.DataFrame): Input DataFrame with at least two columns \'foo\' and \'bar\'. Returns: pd.DataFrame: The modified DataFrame after performing the specified operations. # Step 1: Create a new DataFrame \'df2\' that is a copy of \'df\' with indices reset. df2 = df.reset_index(drop=True).copy() # Step 2: Modify the value of the first row and first column of \'df2\' to 100. df2.iat[0, 0] = 100 # Step 3: Update the column \'foo\' in \'df\' where the \'bar\' column has values greater than 5 to 200 df.loc[df[\'bar\'] > 5, \'foo\'] = 200 return df"},{"question":"Coding Assessment Question: Advanced HTTP Requests with urllib # Objective: Implement a set of functions to perform HTTP requests using the `urllib.request` module, demonstrating your understanding of various aspects such as GET/POST requests, custom headers, error handling, and using proxies or authentication handlers. # Detailed Instructions: 1. **Fetch Content Using GET Request** - Write a function `fetch_content(url: str) -> bytes` that takes a URL as input and returns the content of the page as bytes. - Handle potential errors and return `None` in case of a `URLError` or `HTTPError`. 2. **Send Data Using POST Request** - Write a function `post_data(url: str, data: dict) -> bytes` that takes a URL and a dictionary of data, sends a POST request to the URL, and returns the response content as bytes. - Handle potential errors and return `None` in case of a `URLError` or `HTTPError`. 3. **Custom Headers Request** - Write a function `fetch_with_headers(url: str, headers: dict) -> bytes` that takes a URL and a dictionary of headers, sends a GET request with the specified headers, and returns the content of the page as bytes. - Handle potential errors and return `None` in case of a `URLError` or `HTTPError`. 4. **Authenticated Request** - Write a function `fetch_with_basic_auth(url: str, username: str, password: str) -> bytes` that takes a URL, username, and password, sends an authenticated GET request using Basic Authentication, and returns the content of the page as bytes. - Handle potential errors and return `None` in case of a `URLError` or `HTTPError`. 5. **Proxy Request** - Write a function `fetch_using_proxy(url: str, proxy: dict) -> bytes` that takes a URL and a dictionary of proxy settings, sends a GET request through the proxy, and returns the content of the page as bytes. - Handle potential errors and return `None` in case of a `URLError` or `HTTPError`. # Example Usage: ```python # Example usage of fetch_content function content = fetch_content(\'http://www.example.com\') if content: print(\\"Content fetched successfully\\", content) else: print(\\"Failed to fetch content\\") # Example usage of post_data function response = post_data(\'http://www.example.com/form\', {\'name\': \'John\', \'age\': 30}) if response: print(\\"POST request successful\\", response) else: print(\\"Failed to send POST request\\") # Example usage of fetch_with_headers function headers = {\'User-Agent\': \'Mozilla/5.0\'} content = fetch_with_headers(\'http://www.example.com\', headers) if content: print(\\"Content fetched successfully with headers\\", content) else: print(\\"Failed to fetch content with headers\\") # Example usage of fetch_with_basic_auth function content = fetch_with_basic_auth(\'http://www.example.com/secure\', \'username\', \'password\') if content: print(\\"Authenticated content fetched successfully\\", content) else: print(\\"Failed to fetch authenticated content\\") # Example usage of fetch_using_proxy function proxy = {\'http\': \'http://proxy.example.com:8080\'} content = fetch_using_proxy(\'http://www.example.com\', proxy) if content: print(\\"Content fetched successfully using proxy\\", content) else: print(\\"Failed to fetch content using proxy\\") ``` # Constraints: - You must use the `urllib.request` module for making requests. - Implement appropriate error handling for network-related issues. - Ensure the functions are efficient and handle common edge cases. # Evaluation Criteria: - Correctness: The functions should correctly perform the specified tasks. - Error Handling: Proper handling of potential errors. - Adherence to Instructions: The solution should follow the problem\'s requirements. - Code Quality: Clean, readable, and well-documented code.","solution":"import urllib.request import urllib.parse from urllib.error import URLError, HTTPError import base64 def fetch_content(url: str) -> bytes: Fetch content from a given URL using a GET request. Returns the content as bytes or None in case of an error. try: with urllib.request.urlopen(url) as response: return response.read() except (URLError, HTTPError): return None def post_data(url: str, data: dict) -> bytes: Send a POST request to the given URL with provided data. Returns the response content as bytes or None in case of an error. try: data_encoded = urllib.parse.urlencode(data).encode() request = urllib.request.Request(url, data=data_encoded) with urllib.request.urlopen(request) as response: return response.read() except (URLError, HTTPError): return None def fetch_with_headers(url: str, headers: dict) -> bytes: Fetch content from a given URL using a GET request with custom headers. Returns the content as bytes or None in case of an error. request = urllib.request.Request(url, headers=headers) try: with urllib.request.urlopen(request) as response: return response.read() except (URLError, HTTPError): return None def fetch_with_basic_auth(url: str, username: str, password: str) -> bytes: Fetch content from a given URL using a GET request with Basic Authentication. Returns the content as bytes or None in case of an error. request = urllib.request.Request(url) auth_str = f\'{username}:{password}\' auth_encoded = base64.b64encode(auth_str.encode()).decode() request.add_header(\'Authorization\', f\'Basic {auth_encoded}\') try: with urllib.request.urlopen(request) as response: return response.read() except (URLError, HTTPError): return None def fetch_using_proxy(url: str, proxy: dict) -> bytes: Fetch content from a given URL using a GET request through a specified proxy. Returns the content as bytes or None in case of an error. proxy_handler = urllib.request.ProxyHandler(proxy) opener = urllib.request.build_opener(proxy_handler) urllib.request.install_opener(opener) try: with urllib.request.urlopen(url) as response: return response.read() except (URLError, HTTPError): return None"},{"question":"# Advanced Python Programming: AIFF File Manipulation **Objective:** You are required to demonstrate your understanding of audio file manipulation using the `aifc` module in Python. Specifically, you will work with AIFF and AIFF-C files to read, modify, and save audio data. **Problem Statement:** You are provided with an AIFF audio file. Your task includes reading this audio file, modifying its audio data, and writing the modified data to a new AIFF-C file. The modifications include doubling the audio frame rate and converting the audio into mono (if it is stereo). **Function Specification:** You need to implement the following function: ```python import aifc def modify_aiff(input_file: str, output_file: str) -> None: Reads an AIFF/AIFC file, modifies its frame rate and converts to mono (if stereo), and writes the modified data to a new AIFF-C file. Args: input_file (str): Path to the input AIFF/AIFC file. output_file (str): Path to the output AIFF-C file. pass ``` **Details:** 1. **Reading the Input File:** - Open the input file using `aifc.open(input_file, \'r\')`. - Read the number of channels, sample width, frame rate, and number of frames using corresponding methods. 2. **Modifying Audio Data:** - If the audio file is stereo, convert it to mono by averaging the two channels\' samples. - Double the frame rate. 3. **Writing the Output File:** - Open the output file using `aifc.open(output_file, \'w\')`. - Set the new parameters (channels, sample width, frame rate, and number of frames). - Write the modified audio data to the output file. **Constraints:** - The input file can be AIFF or AIFF-C. - You must handle cases where the input is already mono. - Ensure the output file uses AIFF-C format. **Example:** Assume `input.aiff` is a stereo file with a frame rate of 44100 frames/second. ```python # Modify and save the file modify_aiff(\'input.aiff\', \'output.aifc\') ``` After executing the function, `output.aifc` should: - Be in AIFF-C format. - Have a frame rate of 88200 frames/second. - Be a mono audio file. **Note:** Use the methods provided by the `aifc` module to read and write audio data, manage file parameters, and handle frame data appropriately.","solution":"import aifc def modify_aiff(input_file: str, output_file: str) -> None: Reads an AIFF/AIFC file, modifies its frame rate and converts to mono (if stereo), and writes the modified data to a new AIFF-C file. Args: input_file (str): Path to the input AIFF/AIFC file. output_file (str): Path to the output AIFF-C file. with aifc.open(input_file, \'r\') as in_file: # Reading input file parameters num_channels = in_file.getnchannels() sample_width = in_file.getsampwidth() frame_rate = in_file.getframerate() num_frames = in_file.getnframes() # Read the audio frames audio_frames = in_file.readframes(num_frames) # If file is stereo, convert to mono by averaging the channels if num_channels == 2: # Converting stereo to mono by averaging mono_frames = bytearray() for i in range(0, len(audio_frames), 4): left = int.from_bytes(audio_frames[i:i+2], byteorder=\'big\', signed=True) right = int.from_bytes(audio_frames[i+2:i+4], byteorder=\'big\', signed=True) mono_sample = ((left + right) // 2).to_bytes(2, byteorder=\'big\', signed=True) mono_frames.extend(mono_sample) audio_frames = mono_frames num_channels = 1 new_frame_rate = frame_rate * 2 with aifc.open(output_file, \'w\') as out_file: # Setting output file parameters out_file.setnchannels(num_channels) out_file.setsampwidth(sample_width) out_file.setframerate(new_frame_rate) out_file.setnframes(len(audio_frames) // (sample_width * num_channels)) # Writing the audio frames to the output file out_file.writeframes(audio_frames)"},{"question":"**Question: Implement a Custom Joinable Module for Distributed Training in PyTorch** In this question, you are tasked with implementing a custom `Joinable` module to handle a distributed training scenario where input data to different nodes is uneven. Using the classes `Join`, `Joinable`, and `JoinHook` provided by the `torch.distributed.algorithms`, you should create a `Joinable` subclass that correctly integrates with the join context manager. # Instructions 1. **Define a CustomJoinable Class**: - This class should inherit from `torch.distributed.algorithms.Joinable`. - Implement the necessary methods to make it a valid participant in a distributed training join context. 2. **Implement a Training Loop**: - Write a training loop where your `CustomJoinable` class is used within a `Join` context manager. - Ensure the training loop can handle uneven input data distributed across multiple nodes. 3. **Expected Input and Output**: - Input: A dataset with uneven distributions across nodes and a specified number of epochs. - Output: The correctly trained model with synchronized weights across all nodes. 4. **Constraints**: - Assume a distributed training setup using PyTorch’s `torch.distributed` package. - You may mock the distributed environment for the purpose of this coding assessment. # Sample Implementation Outline Here’s an outline to guide your implementation: ```python import torch import torch.distributed as dist from torch.distributed.algorithms import Join, Joinable, JoinHook class CustomJoinable(Joinable): def __init__(self, model, optimizer, data_loader): self.model = model self.optimizer = optimizer self.data_loader = data_loader def join_hook(self, hook_type: JoinHook): # Provide custom hook implementations if needed pass def train_one_epoch(self): # Implement the logic for training over a single epoch for inputs, targets in self.data_loader: outputs = self.model(inputs) loss = self.loss_fn(outputs, targets) self.optimizer.zero_grad() loss.backward() self.optimizer.step() def run_training(): # Initialize distributed process group dist.init_process_group(\'gloo\') # Define model, optimizer, and dataloader model = ... # Define your model architecture here optimizer = ... # Define your optimizer here data_loader = ... # Define your data loader here with uneven input data joinable = CustomJoinable(model, optimizer, data_loader) num_epochs = ... with Join([joinable]): for _ in range(num_epochs): joinable.train_one_epoch() # Clean up distributed process group dist.destroy_process_group() if __name__ == \'__main__\': run_training() ``` # Note: - Ensure to document your code and provide explanations for key parts of your implementation. - Mocking the distributed environment for testing purposes is acceptable if a real distributed setup is unavailable. Good luck, and happy coding!","solution":"import torch import torch.distributed as dist from torch.distributed.algorithms import Join, Joinable, JoinHook import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset, random_split class CustomJoinable(Joinable): def __init__(self, model, optimizer, data_loader, rank): super().__init__() self.model = model self.optimizer = optimizer self.data_loader = data_loader self.rank = rank self.loss_fn = nn.CrossEntropyLoss() def join_hook(self, hook_type: JoinHook): pass def train_one_epoch(self): self.model.train() for inputs, targets in self.data_loader: outputs = self.model(inputs) loss = self.loss_fn(outputs, targets) self.optimizer.zero_grad() loss.backward() self.optimizer.step() print(f\\"Rank {self.rank}, Loss: {loss.item()}\\") def run_training(world_size, rank): # Initialize distributed process group dist.init_process_group(backend=\'gloo\', rank=rank, world_size=world_size) # Dummy model, optimizer and data loader for demonstration purposes model = nn.Sequential(nn.Linear(10, 10), nn.ReLU(), nn.Linear(10, 2)) optimizer = optim.SGD(model.parameters(), lr=0.01) # Uneven data distribution for demonstration purposes data_size = 100 if rank == 0 else 50 inputs = torch.randn(data_size, 10) targets = torch.randint(0, 2, (data_size,)) dataset = TensorDataset(inputs, targets) data_loader = DataLoader(dataset, batch_size=10) joinable = CustomJoinable(model, optimizer, data_loader, rank) num_epochs = 5 with Join([joinable]): for _ in range(num_epochs): joinable.train_one_epoch() # Clean up distributed process group dist.destroy_process_group() if __name__ == \'__main__\': import os world_size = 2 # This is just for demonstration purposes and would not work in a real multi-process environment as is. # Normally, you would use torch.multiprocessing.spawn or another method to launch processes. for rank in range(world_size): if os.fork() == 0: run_training(world_size, rank) os._exit(0)"},{"question":"**Coding Assessment Question: Building a Custom Network Server with `socketserver`** **Objective:** Design and implement a custom network server using the `socketserver` module in Python. The server should be able to handle multiple client requests asynchronously. **Problem Statement:** You are tasked with building a custom TCP server that will serve a simple chat application. Multiple clients should be able to connect to the server and send messages. The server should broadcast each received message to all connected clients, demonstrating both TCP communication and asynchronous handling of requests. **Requirements:** 1. Create a custom request handler class `ChatHandler` that subclasses `socketserver.BaseRequestHandler`. 2. Implement the `handle()` method to: - Receive messages from clients. - Broadcast received messages to all connected clients. 3. Use `ThreadingMixIn` to handle multiple client connections concurrently. 4. Your server should: - Run on `localhost` and port `9999`. - Handle client connections asynchronously. - Ensure that messages are properly broadcast to all clients. **Implementation Details:** **1. Custom Request Handler Class (`ChatHandler`):** - The `handle()` method should read incoming messages using `self.request.recv`. - The server should keep track of all connected clients. - Upon receiving a message, broadcast it to all connected clients. **2. Server Class:** - Use `ThreadingMixIn` and `TCPServer` to create a `ThreadedTCPServer`. - Implement a mechanism to manage connected clients and broadcast messages. **Expected Input and Output:** - The server should handle string messages from clients. - Each message received from a client should be broadcast to all other connected clients. - Clients should receive broadcast messages showing the original sender’s address. **Example:** - Client A sends \\"Hello from A\\". - Client B and C receive \\"A: Hello from A\\". **Code Template:** ```python import socketserver import threading class ChatHandler(socketserver.BaseRequestHandler): clients = [] lock = threading.Lock() def handle(self): # Add the new connection to the list of clients with self.lock: self.clients.append(self.request) try: while True: # Handle incoming messages message = self.request.recv(1024).strip() if not message: break self.broadcast(message) finally: with self.lock: self.clients.remove(self.request) def broadcast(self, message): sender = self.client_address[0] prefixed_message = f\\"{sender}: {message.decode(\'utf-8\')}\\".encode(\'utf-8\') with self.lock: for client in self.clients: if client != self.request: client.sendall(prefixed_message) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), ChatHandler) as server: print(f\\"Server started at {HOST}:{PORT}\\") server.serve_forever() ``` **Constraints:** - Ensure that the server handles disconnections gracefully. - Use appropriate thread synchronization mechanisms to handle shared resources (e.g., list of clients). - The server should be robust and handle potential exceptions during message broadcasting. **Performance Requirements:** - The server should efficiently manage multiple client connections. - Broadcasting messages should maintain low latency, ensuring timely delivery to all connected clients.","solution":"import socketserver import threading class ChatHandler(socketserver.BaseRequestHandler): clients = [] lock = threading.Lock() def handle(self): # Add the new connection to the list of clients with self.lock: self.clients.append(self.request) try: while True: # Handle incoming messages message = self.request.recv(1024).strip() if not message: break self.broadcast(message) finally: with self.lock: self.clients.remove(self.request) def broadcast(self, message): sender = f\\"{self.client_address[0]}:{self.client_address[1]}\\" prefixed_message = f\\"{sender}: {message.decode(\'utf-8\')}\\".encode(\'utf-8\') with self.lock: for client in self.clients: if client != self.request: client.sendall(prefixed_message) class ThreadedTCPServer(socketserver.ThreadingMixIn, socketserver.TCPServer): pass if __name__ == \\"__main__\\": HOST, PORT = \\"localhost\\", 9999 with ThreadedTCPServer((HOST, PORT), ChatHandler) as server: print(f\\"Server started at {HOST}:{PORT}\\") server.serve_forever()"},{"question":"# Nullable Integer Operations with Pandas Objective Write a function `transform_nullable_int_series` that takes a list of integers and `None` values, constructs a pandas `Series` with a nullable integer type (`Int64`), and performs a series of operations on it. The function should return a dictionary with the results of various operations. Input - `data`: List of integers and `None` values, e.g., `[1, 2, None, 4, None, 5]` - `dataframe_flag`: Boolean flag indicating if the Series should be incorporated into a `DataFrame` Output A dictionary containing: 1. The original series. 2. The series after an arithmetic operation (add 10 to each element). 3. A boolean series indicating which elements are equal to 12. 4. The series after slicing out the first two elements. 5. The series after coercing a slice of it to type `Int8`. 6. The series after adding a float value (0.1). 7. If `dataframe_flag` is `True`, a DataFrame with the series as a column. Constraints - The list may contain up to 100 elements. - The list may contain at least one `None` value. - The function should create series and dataframes using the correct nullable integer type. Example Usage ```python def transform_nullable_int_series(data, dataframe_flag): import pandas as pd # Step 1: Create a Series with dtype \\"Int64\\". series = pd.Series(data, dtype=\\"Int64\\") # Step 2: Perform operations and store results in a dictionary. results = { \'original_series\': series, \'series_plus_10\': series + 10, \'equal_to_12\': series == 12, \'sliced_series\': series.iloc[2:], \'coerced_to_int8\': (series.iloc[2:4]).astype(\\"Int8\\"), \'series_plus_float\': series + 0.1 } if dataframe_flag: df = pd.DataFrame({\\"nullable_int_series\\": series}) results[\'dataframe\'] = df return results # Example Testing data = [1, 2, None, 4, None, 5] dataframe_flag = True result = transform_nullable_int_series(data, dataframe_flag) print(result) ``` Evaluation Criteria - Correct handling and construction of nullable integer types. - Accurate performance of specified operations. - Proper dtype assignments and conversion handling. - Inclusion of the series in a `DataFrame` when `dataframe_flag` is `True`.","solution":"def transform_nullable_int_series(data, dataframe_flag): import pandas as pd # Step 1: Create a Series with dtype \\"Int64\\". series = pd.Series(data, dtype=\\"Int64\\") # Step 2: Perform operations and store results in a dictionary. results = { \'original_series\': series, \'series_plus_10\': series + 10, \'equal_to_12\': series == 12, \'sliced_series\': series.iloc[2:], \'coerced_to_int8\': (series.iloc[2:4]).astype(\\"Int8\\"), \'series_plus_float\': series + 0.1 } if dataframe_flag: df = pd.DataFrame({\\"nullable_int_series\\": series}) results[\'dataframe\'] = df return results"},{"question":"Objective To assess your understanding of Python\'s type hinting capabilities and your ability to create and use generic aliases for types. Problem Statement You are tasked with creating a utility that uses Python\'s type hinting to enhance type safety in a collection of data objects. Specifically, you need to create a `GenericAlias` class in Python which can wrap around a given type with specified parameters. Requirements 1. **Create a `GenericAlias` Class:** - The class should initialize with two parameters: `origin` and `args`. - `origin` represents the original type. - `args` represents the parameters to be applied to the original type. If `args` is not a tuple, a single-element tuple should be created. 2. **Implement `__origin__` and `__args__` Attributes:** - These attributes should be assigned during initialization based on `origin` and `args`. 3. **Implement `__class_getitem__` Method:** - This method should return an instance of the `GenericAlias` class when the original class is subscripted. Constraints - `origin` should be a valid Python type. - `args` can be any object or a tuple of objects. Example Usage ```python class MyList: def __class_getitem__(cls, item): return GenericAlias(cls, item) # Example with a single type argument alias_instance = MyList[int] print(alias_instance.__origin__) # Output: <class \'__main__.MyList\'> print(alias_instance.__args__) # Output: (<class \'int\'>,) # Example with multiple type arguments alias_instance2 = MyList[int, str] print(alias_instance2.__origin__) # Output: <class \'__main__.MyList\'> print(alias_instance2.__args__) # Output: (<class \'int\'>, <class \'str\'>) ``` Code Template ```python class GenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): args = (args,) self.__args__ = args @classmethod def __class_getitem__(cls, item): return cls(cls, item) # Testing the implementation class MyList: def __class_getitem__(cls, item): return GenericAlias(cls, item) # Example usage alias_instance = MyList[int] print(alias_instance.__origin__) # Expected Output: <class \'__main__.MyList\'> print(alias_instance.__args__) # Expected Output: (<class \'int\'>,) alias_instance2 = MyList[int, str] print(alias_instance2.__origin__) # Expected Output: <class \'__main__.MyList\'> print(alias_instance2.__args__) # Expected Output: (<class \'int\'>, <class \'str\'>) ``` Implement the `GenericAlias` class and test if your implementation works as shown in the examples. Evaluation - The code should be free of syntax errors. - The class should correctly handle different types and numbers of arguments. - The methods should work as expected and match the example outputs provided.","solution":"class GenericAlias: def __init__(self, origin, args): self.__origin__ = origin if not isinstance(args, tuple): args = (args,) self.__args__ = args @classmethod def __class_getitem__(cls, item): return cls(cls, item) # Example usage class MyList: def __class_getitem__(cls, item): return GenericAlias(cls, item) # These are example usages: alias_instance = MyList[int] print(alias_instance.__origin__) # Expected Output: <class \'__main__.MyList\'> print(alias_instance.__args__) # Expected Output: (<class \'int\'>,) alias_instance2 = MyList[int, str] print(alias_instance2.__origin__) # Expected Output: <class \'__main__.MyList\'> print(alias_instance2.__args__) # Expected Output: (<class \'int\'>, <class \'str\'>)"},{"question":"# Python310: Source Distribution Filtering **Context:** The `sdist` command in `python310` allows for various patterns to include or exclude files in a source distribution. These patterns can be applied at different levels within the directory structure of the project. **Objective:** You are required to write a function that simulates the behavior of the `sdist` command by applying the inclusion and exclusion patterns to filter files in a directory structure. **Function Signature:** ```python def filter_files(file_list: list, commands: list) -> list: Filters the files according to the given sdist commands. Parameters: - file_list (list): A list of file paths (strings) representing the files in the source structure. - commands (list): A list of strings where each string is an sdist command with patterns. Returns: - list: A list of file paths after applying the inclusion and exclusion patterns from the commands. ``` **Input:** - A list `file_list` where each element is a string representing a file path in the source tree. - A list `commands` where each element is a string representing an `sdist` command with patterns. **Output:** - A list of file paths that remain after applying the specified inclusion and exclusion patterns from the `commands`. **Constraints:** - The directory structure is a simplified, flat list of strings representing file paths. - Commands must be applied in the order they are provided. - Patterns use Unix-style \\"glob\\" patterns. **Example:** ```python # Example inputs file_list = [ \\"src/module1.py\\", \\"src/module2.py\\", \\"data/file1.dat\\", \\"data/file2.dat\\", \\"scripts/install.sh\\", \\"docs/readme.md\\", \\"tests/test_module1.py\\" ] commands = [ \\"include src/module1.py data/file1.dat\\", \\"exclude docs/*\\", \\"global-include tests/*.py\\", \\"recursive-exclude data *.dat\\" ] # Calling the function filtered_list = filter_files(file_list, commands) # Expected output print(filtered_list) # Output: [\'src/module1.py\', \'tests/test_module1.py\'] ``` **Explanation:** 1. Initially includes only `src/module1.py` and `data/file1.dat`. 2. Excludes nothing as `docs/*` does not match any included files. 3. Includes `tests/test_module1.py` due to the global include pattern. 4. Finally, all `.dat` files under `data` are excluded, leaving `src/module1.py` and `tests/test_module1.py`. In this task, students will demonstrate their understanding of file handling, pattern matching, and list manipulations — key concepts in Python programming aimed at simulating part of the functionality provided by `python310`.","solution":"import fnmatch def filter_files(file_list, commands): included_files = set() excluded_files = set() for command in commands: parts = command.split() cmd = parts[0] patterns = parts[1:] if cmd == \'include\': for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif cmd == \'exclude\': for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): excluded_files.add(file) elif cmd == \'global-include\': for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): included_files.add(file) elif cmd == \'recursive-exclude\': for pattern in patterns: for file in file_list: if fnmatch.fnmatch(file, pattern): excluded_files.add(file) final_files = included_files - excluded_files return list(final_files)"},{"question":"# PyTorch: TorchInductor GPU Profiling and Optimization In this assessment, you are tasked with profiling and optimizing kernel performance of a PyTorch model using TorchInductor. Your goal is to write code that leverages TorchInductor\'s profiling tools, analyze the results, and optimize the most performance-critical kernel. Problem Statement 1. **Benchmark a Model**: - Write a script to benchmark the `resnet18` model using TorchInductor. - Use the environment variables provided to generate unique kernel names and benchmark individual Triton kernels. - Record the profiling output log. ```python import os # Set environment variables os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Benchmark the model # (You may assume the necessary imports and model initialization are done here) os.system(\\"python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only resnet18 --disable-cudagraphs --training\\") ``` 2. **Analyze Profiling Data**: - Parse the output log to identify the most performance-expensive kernel (the kernel taking the highest percentage of GPU time). - Implement a logging function to extract the relevant kernel name and its performance metrics. ```python def extract_kernel_info(log_file_path): with open(log_file_path, \'r\') as log_file: lines = log_file.readlines() kernel_info = {} for line in lines: if \'Percent of time when GPU is busy:\' in line: kernel_name = line.split(\':\')[1].strip() percent_time = float(line.split(\':\')[2].strip().replace(\'%\', \'\')) kernel_info[kernel_name] = percent_time most_expensive_kernel = max(kernel_info, key=kernel_info.get) return most_expensive_kernel, kernel_info[most_expensive_kernel] log_file_path = \'/path/to/your/output_log.txt\' most_expensive_kernel, percent_time = extract_kernel_info(log_file_path) print(f\\"Most expensive kernel: {most_expensive_kernel} with {percent_time}% GPU time\\") ``` 3. **Optimize the Kernel**: - Isolate and run the kernel script associated with the most expensive kernel. - Use `TORCHINDUCTOR_MAX_AUTOTUNE` to benchmark the kernel with different configurations and identify the optimal settings. - Record and compare the kernel\'s performance before and after optimization. ```python import os # Set the environment variable for autotuning os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' # Run the kernel script kernel_script_path = \'/path/to/your/kernel_script.py\' # Replace with actual path os.system(f\\"python {kernel_script_path}\\") ``` Expected Output 1. **Profiling Output Log**: A log file containing profiling data of the `resnet18` model. 2. **Kernel Analysis**: Information about the most expensive kernel including its name and percentage of GPU time. 3. **Performance Comparison**: A summary of the kernel\'s performance (execution time) before and after applying `TORCHINDUCTOR_MAX_AUTOTUNE`. Constraints and Considerations - Ensure appropriate handling of environment variables for profiling. - Accurately parse the log file to extract and interpret kernel performance metrics. - The kernel script paths should be dynamically identified based on the profiling log. - Performance metrics should be consistently recorded before and after optimization. - The solution should be efficient and handle large profiling logs gracefully.","solution":"import os def benchmark_resnet18(): Benchmarks the resnet18 model using TorchInductor with specified environment settings. # Set environment variables for profiling os.environ[\'TORCHINDUCTOR_UNIQUE_KERNEL_NAMES\'] = \'1\' os.environ[\'TORCHINDUCTOR_BENCHMARK_KERNEL\'] = \'1\' # Run the benchmark script os.system(\\"python -u benchmarks/dynamo/timm_models.py --backend inductor --amp --performance --dashboard --only resnet18 --disable-cudagraphs --training\\") def extract_kernel_info(log_file_path): Extracts the most performance-expensive kernel information from the log file. Args: log_file_path (str): Path to the profiling output log file. Returns: tuple: The name of the most expensive kernel and its GPU time percentage. with open(log_file_path, \'r\') as log_file: lines = log_file.readlines() kernel_info = {} for line in lines: if \'Percent of time when GPU is busy:\' in line: kernel_name = line.split(\':\')[1].strip() percent_time = float(line.split(\':\')[2].strip().replace(\'%\', \'\')) kernel_info[kernel_name] = percent_time most_expensive_kernel = max(kernel_info, key=kernel_info.get) return most_expensive_kernel, kernel_info[most_expensive_kernel] def optimize_kernel(kernel_script_path): Optimizes the specified kernel script using TorchInductor\'s autotuning feature. Args: kernel_script_path (str): Path to the kernel script file. # Set the environment variable for autotuning os.environ[\'TORCHINDUCTOR_MAX_AUTOTUNE\'] = \'1\' # Run the kernel script for optimization os.system(f\\"python {kernel_script_path}\\")"},{"question":"You have been provided with a sample dataset and your task is to visualize it using Seaborn\'s object interface to demonstrate your understanding of its various properties and capabilities. Given the dataset containing information about car models, their attributes, and sales performance, write a function `car_sales_plot` that visualizes the data as described below: ```python import pandas as pd def car_sales_plot(df: pd.DataFrame): Generates a series of plots for car sales dataset using Seaborn\'s object interface. Parameters: df (pd.DataFrame): DataFrame containing car sales data. The DataFrame contains the following columns: - model: str, the model name of the car - year: int, the year the car was manufactured - sales: int, the number of cars sold - type: str, type of car (e.g. sedan, SUV, truck) - price: float, price of the car pass ``` # Requirements: 1. **Scatter Plot**: - Plot the relationship between `price` and `sales` for different `year`. - Use different markers for different `type` of cars. - Use a color scale to represent the `year`. 2. **Violin Plot**: - Display the distribution of `price` for different `type` of cars. - Use a color palette to distinguish car types. 3. **Bar Plot**: - Plot the average `sales` for each `year`. - Use different edge colors to distinguish car types. 4. **Customizations**: - Set appropriate axis labels and plot titles. - Use consistent themes across all plots. - Apply appropriate scales for the axes and color continuums. # Constraints: - The function should use Seaborn\'s object interface to create the plots. - You should demonstrate the use of Seaborn properties like `color`, `marker`, `edgecolor`, `alpha`, `linestyle`, `pointsize`, etc. in various contexts as appropriate. - Handle cases where the dataset might have missing values gracefully. **Submission**: - Write a complete Python function inside the `car_sales_plot` that meets the above requirements. - Include necessary import statements. - Return a `FacetGrid` object containing the generated plots. **Example Input**: ```python data = { \'model\': [\'Model X\', \'Model Y\', \'Model Z\', \'Model X\', \'Model Y\'], \'year\': [2020, 2021, 2020, 2019, 2021], \'sales\': [1500, 2000, 1800, 1700, 1600], \'type\': [\'Sedan\', \'SUV\', \'Sedan\', \'Sedan\', \'Truck\'], \'price\': [35000, 45000, 37000, 34000, 46000] } df = pd.DataFrame(data) car_sales_plot(df) ``` **Expected Output**: This function should generate a series of plots visualizing the provided dataset according to the outlined requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def car_sales_plot(df: pd.DataFrame): Generates a series of plots for car sales dataset using Seaborn\'s object interface. Parameters: df (pd.DataFrame): DataFrame containing car sales data. The DataFrame contains the following columns: - model: str, the model name of the car - year: int, the year the car was manufactured - sales: int, the number of cars sold - type: str, type of car (e.g. sedan, SUV, truck) - price: float, price of the car sns.set_theme(style=\\"whitegrid\\") # Scatter Plot scatter_plot = sns.relplot( data=df, x=\\"price\\", y=\\"sales\\", hue=\\"year\\", style=\\"type\\", kind=\\"scatter\\", palette=\\"viridis\\", height=6, aspect=1.6 ) scatter_plot.set_axis_labels(\\"Price\\", \\"Sales\\") scatter_plot.fig.suptitle(\\"Scatter Plot of Car Price vs Sales by Year and Type\\", y=1.02) # Violin Plot violin_plot, ax = plt.subplots() sns.violinplot( data=df, x=\\"type\\", y=\\"price\\", palette=\\"muted\\", ax=ax ) ax.set_title(\\"Violin Plot of Car Price Distribution by Type\\") ax.set_xlabel(\\"Type\\") ax.set_ylabel(\\"Price\\") # Bar Plot bar_data = df.groupby(\\"year\\")[\\"sales\\"].mean().reset_index() bar_plot, ax = plt.subplots() bar = sns.barplot( data=bar_data, x=\\"year\\", y=\\"sales\\", palette=\\"deep\\", edgecolor=\\".6\\", ax=ax ) ax.set_title(\\"Average Sales per Year\\") ax.set_xlabel(\\"Year\\") ax.set_ylabel(\\"Average Sales\\") plt.tight_layout() return scatter_plot, violin_plot, bar_plot"},{"question":"# Question: You are building an application that requires generating, managing, and analyzing UUIDs of various types. To assess your understanding of Python\'s `uuid` module, complete the following tasks: Task 1: UUID generator function Write a function `generate_uuid(uuid_type, *args)` that generates a UUID based on the specified `uuid_type`: - `uuid1`: No additional arguments needed. - `uuid3`: Additional arguments are `namespace` (one of `uuid.NAMESPACE_DNS`, `uuid.NAMESPACE_URL`, `uuid.NAMESPACE_OID`, `uuid.NAMESPACE_X500`) and `name` (string). - `uuid4`: No additional arguments needed. - `uuid5`: Additional arguments are `namespace` and `name` similar to `uuid3`. ```python def generate_uuid(uuid_type, *args): Generates a UUID based on the specified type. Parameters: uuid_type (str): Type of the UUID to generate (\'uuid1\', \'uuid3\', \'uuid4\', \'uuid5\'). *args: Additional arguments required by the specific uuid_type. Returns: uuid.UUID: Generated UUID. pass ``` Task 2: UUID attributes extractor Write a function `extract_uuid_attributes(uuid_str)` that takes a UUID string (`uuid_str`) and returns a dictionary containing its different attributes: - `hex`: The UUID as a 32-character hexadecimal string. - `int`: The UUID as a 128-bit integer. - `bytes`: The UUID as a 16-byte string. - `fields`: A tuple representing the six fields of the UUID. - `urn`: The UUID as a URN string. - `variant`: The UUID variant. - `version`: The UUID version number. - `is_safe`: Indicates if the UUID was generated safely (only applicable to `uuid1`). ```python def extract_uuid_attributes(uuid_str): Extracts and returns attributes of a UUID. Parameters: uuid_str (str): A string representation of the UUID. Returns: dict: A dictionary containing UUID attributes. pass ``` Constraints: - You may assume that `uuid_str` passed to `extract_uuid_attributes` is always a valid UUID string. - For `generate_uuid` function, ensure that `uuid_type` is one of `uuid1`, `uuid3`, `uuid4`, or `uuid5`. Raise a `ValueError` if an invalid `uuid_type` is provided. Example usage: ```python # Example usage of generate_uuid function print(generate_uuid(\'uuid1\')) print(generate_uuid(\'uuid3\', uuid.NAMESPACE_DNS, \'example.com\')) print(generate_uuid(\'uuid4\')) print(generate_uuid(\'uuid5\', uuid.NAMESPACE_URL, \'example.com\')) # Example usage of extract_uuid_attributes uuid_generated = generate_uuid(\'uuid4\') attributes = extract_uuid_attributes(str(uuid_generated)) print(attributes) ```","solution":"import uuid def generate_uuid(uuid_type, *args): Generates a UUID based on the specified type. Parameters: uuid_type (str): Type of the UUID to generate (\'uuid1\', \'uuid3\', \'uuid4\', \'uuid5\'). *args: Additional arguments required by the specific uuid_type. Returns: uuid.UUID: Generated UUID. if uuid_type == \'uuid1\': return uuid.uuid1() elif uuid_type == \'uuid3\': namespace, name = args return uuid.uuid3(namespace, name) elif uuid_type == \'uuid4\': return uuid.uuid4() elif uuid_type == \'uuid5\': namespace, name = args return uuid.uuid5(namespace, name) else: raise ValueError(f\\"Invalid uuid_type \'{uuid_type}\'. Must be one of: \'uuid1\', \'uuid3\', \'uuid4\', \'uuid5\'.\\") def extract_uuid_attributes(uuid_str): Extracts and returns attributes of a UUID. Parameters: uuid_str (str): A string representation of the UUID. Returns: dict: A dictionary containing UUID attributes. u = uuid.UUID(uuid_str) return { \'hex\': u.hex, \'int\': u.int, \'bytes\': u.bytes, \'fields\': u.fields, \'urn\': u.urn, \'variant\': u.variant, \'version\': u.version, \'is_safe\': u.is_safe if hasattr(u, \'is_safe\') else None # \'is_safe\' only relevant for uuid1 }"},{"question":"**Question:** You are given a single 32-bit integer that encodes the version number of CPython. Your task is to write a Python function `decode_version_number(hexversion)` that extracts the major version, minor version, micro version, release level, and release serial from this integer and returns them in a readable dictionary format. # Function Signature ```python def decode_version_number(hexversion: int) -> dict: ``` # Input - `hexversion` (int): A 32-bit integer representing the encoded version number of CPython. # Output - A dictionary with the following key-value pairs: - `\\"major\\"` (int): The major version number. - `\\"minor\\"` (int): The minor version number. - `\\"micro\\"` (int): The micro version number. - `\\"release_level\\"` (str): The release level, which can be \\"alpha\\", \\"beta\\", \\"candidate\\", or \\"final\\". - `\\"release_serial\\"` (int): The release serial number. # Constraints - The input integer `hexversion` will always be a valid encoded CPython version number. # Example ```python result = decode_version_number(0x030401a2) print(result) # Output: {\'major\': 3, \'minor\': 4, \'micro\': 1, \'release_level\': \'alpha\', \'release_serial\': 2} result = decode_version_number(0x030a00f0) print(result) # Output: {\'major\': 3, \'minor\': 10, \'micro\': 0, \'release_level\': \'final\', \'release_serial\': 0} ``` # Note - Use bitwise operations to extract the relevant parts of the encoded integer. - Map the release level from its numeric value to the corresponding string: - 0xA to \\"alpha\\" - 0xB to \\"beta\\" - 0xC to \\"candidate\\" - 0xF to \\"final\\" # Additional Information This problem assesses your understanding of bitwise operations and your ability to handle and transform data. Ensure that your solution is efficient and follows the provided constraints.","solution":"def decode_version_number(hexversion: int) -> dict: Decode the version number from the given 32-bit integer. Parameters: hexversion (int): The encoded version number of CPython. Returns: dict: A dictionary containing the major, minor, micro version, release level, and release serial. major = (hexversion >> 24) & 0xFF minor = (hexversion >> 16) & 0xFF micro = (hexversion >> 8) & 0xFF release_level_code = (hexversion >> 4) & 0xF release_serial = hexversion & 0xF release_levels = { 0xA: \\"alpha\\", 0xB: \\"beta\\", 0xC: \\"candidate\\", 0xF: \\"final\\" } release_level = release_levels.get(release_level_code, \\"unknown\\") return { \\"major\\": major, \\"minor\\": minor, \\"micro\\": micro, \\"release_level\\": release_level, \\"release_serial\\": release_serial }"},{"question":"Objective Design a Python script using the deprecated `optparse` module to create a command-line tool. This tool should take in a series of options and arguments to perform specified tasks outlined below. Task Implement a command-line tool using the `optparse` module that performs the following functions: 1. **Add**: Add two numbers. 2. **Multiply**: Multiply two numbers. 3. **Greet**: Print a greeting message. The tool should support the following command-line options: - `--add <num1> <num2>`: Adds two numbers `num1` and `num2`. - `--multiply <num1> <num2>`: Multiplies two numbers `num1` and `num2`. - `--greet <name>`: Prints \\"Hello, <name>!\\". Input and Output Formats - The script should accept the above options as command-line arguments. - The output should be printed to the standard output. Constraints - Only one of the operations (`--add`, `--multiply`, `--greet`) should be processed at a time. Performance Requirements - The tool should handle command-line inputs efficiently and provide the correct output without significant delay. Example Usage Suppose your script is named `tool.py`. Here are some example usages: - Adding two numbers: ``` python tool.py --add 2 3 5 ``` - Multiplying two numbers: ``` python tool.py --multiply 4 5 20 ``` - Greeting a user: ``` python tool.py --greet Alice Hello, Alice! ``` Implementation Details Use the following template to create your script: ```python import optparse def main(): parser = optparse.OptionParser() parser.add_option(\'--add\', nargs=2, type=\'int\', help=\'Adds two numbers\') parser.add_option(\'--multiply\', nargs=2, type=\'int\', help=\'Multiplies two numbers\') parser.add_option(\'--greet\', nargs=1, type=\'string\', help=\'Greets the user with the provided name\') (options, args) = parser.parse_args() if options.add: num1, num2 = options.add print(num1 + num2) elif options.multiply: num1, num2 = options.multiply print(num1 * num2) elif options.greet: print(f\\"Hello, {options.greet}!\\") else: parser.print_help() if __name__ == \'__main__\': main() ``` Your task is to complete the script ensuring it executes the specified operations correctly based on the command-line inputs provided.","solution":"import optparse def main(): parser = optparse.OptionParser() parser.add_option(\'--add\', nargs=2, type=\'int\', help=\'Adds two numbers\') parser.add_option(\'--multiply\', nargs=2, type=\'int\', help=\'Multiplies two numbers\') parser.add_option(\'--greet\', nargs=1, type=\'string\', help=\'Greets the user with the provided name\') (options, args) = parser.parse_args() if options.add: num1, num2 = options.add print(num1 + num2) elif options.multiply: num1, num2 = options.multiply print(num1 * num2) elif options.greet: print(f\\"Hello, {options.greet}!\\") else: parser.print_help() if __name__ == \'__main__\': main()"},{"question":"# Command-Line Tool Implementation with `optparse` # Objective: Create a Python script that demonstrates the usage of the deprecated `optparse` module to handle various command-line options. Your script should be able to parse command-line arguments to perform specified actions and handle errors gracefully. # Requirements: 1. **Options:** - `-a` or `--add`: Takes two integers and prints their sum. - `-s` or `--subtract`: Takes two integers and prints their difference. - `-m` or `--multiply`: Takes two integers and prints their product. - `-d` or `--divide`: Takes two integers and prints their quotient. - `-p` or `--power`: Takes two integers and prints the result of raising the first integer to the power of the second integer. - `-v` or `--verbose`: A flag that, if present, prints detailed information about each operation. - `-h` or `--help`: Generates the help message. 2. **Default values and mandatory options:** - By default, all operations should be performed with `0` and `0` if no values are provided. - At least one operation option (`add`, `subtract`, etc.) should be required for the script to run. 3. **Help and Usage:** - The script should generate a user-friendly help message when `-h` or `--help` is specified. - The usage message should explain the options clearly. 4. **Error Handling:** - Handle scenarios where required arguments for an operation are missing. - Provide meaningful error messages for invalid options or arguments that cannot be converted to integers. # Example Runs: 1. `python yourscript.py -a 3 4` ``` Sum: 7 ``` 2. `python yourscript.py --multiply 2 5 -v` ``` Multiplying 2 and 5 Product: 10 ``` 3. `python yourscript.py -s 10 4` ``` Difference: 6 ``` 4. `python yourscript.py --power 2 3` ``` Power: 8 ``` 5. `python yourscript.py -v` ``` Error: At least one operation must be specified. ``` # Implementation: Write the implementation of the script using the `optparse` module as described in the documentation provided. Ensure your script includes all mentioned options and handles the required functionalities and error scenarios.","solution":"import optparse def main(): parser = optparse.OptionParser() parser.add_option(\\"-a\\", \\"--add\\", nargs=2, type=\\"int\\", help=\\"Adds two integers\\") parser.add_option(\\"-s\\", \\"--subtract\\", nargs=2, type=\\"int\\", help=\\"Subtracts the second integer from the first\\") parser.add_option(\\"-m\\", \\"--multiply\\", nargs=2, type=\\"int\\", help=\\"Multiplies two integers\\") parser.add_option(\\"-d\\", \\"--divide\\", nargs=2, type=\\"int\\", help=\\"Divides the first integer by the second\\") parser.add_option(\\"-p\\", \\"--power\\", nargs=2, type=\\"int\\", help=\\"Raises the first integer to the power of the second integer\\") parser.add_option(\\"-v\\", \\"--verbose\\", action=\\"store_true\\", help=\\"Prints detailed information about each operation\\") (options, args) = parser.parse_args() if not (options.add or options.subtract or options.multiply or options.divide or options.power): parser.error(\\"At least one operation must be specified.\\") if options.add: x, y = options.add result = x + y if options.verbose: print(f\\"Adding {x} and {y}\\") print(f\\"Sum: {result}\\") if options.subtract: x, y = options.subtract result = x - y if options.verbose: print(f\\"Subtracting {y} from {x}\\") print(f\\"Difference: {result}\\") if options.multiply: x, y = options.multiply result = x * y if options.verbose: print(f\\"Multiplying {x} and {y}\\") print(f\\"Product: {result}\\") if options.divide: x, y = options.divide if y == 0: print(\\"Error: Division by zero.\\") else: result = x / y if options.verbose: print(f\\"Dividing {x} by {y}\\") print(f\\"Quotient: {result}\\") if options.power: x, y = options.power result = x ** y if options.verbose: print(f\\"Raising {x} to the power of {y}\\") print(f\\"Power: {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective:** Design and implement a Python program using the `selectors` module (which offers high-level and efficient I/O multiplexing built on top of the `select` module primitives) to manage multiple client connections on a network server. **Problem Statement:** You are tasked to implement a simple echo server that can handle multiple client connections simultaneously using the `selectors` module. Your server should be able to handle the following: 1. Accept new client connections. 2. Read messages from clients. 3. Echo back the received messages to the respective clients. 4. Close connections when clients disconnect. **Function Specification:** You need to implement a function `run_echo_server(host: str, port: int) -> None` that takes the host and port as arguments and starts an echo server. **Input:** - `host`: A string representing the hostname or IP address where the server will listen for connections. - `port`: An integer representing the port number where the server will listen for connections. **Output:** - None. The function runs an infinite loop handling multiple client connections and should not return. **Constraints:** - The message from the client will not exceed 1024 bytes. - You should use the `selectors` module for managing I/O operations. - Handle client connections non-blocking to ensure the server can manage multiple connections. **Performance Requirements:** - The server should efficiently handle up to 1000 concurrent client connections. - Ensure that the server is responsive and does not block on any single operation. # **Example:** ```python import selectors import socket def run_echo_server(host: str, port: int) -> None: sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() print(f\'Accepted connection from {addr}\') conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1024) if data: print(f\'Received {data} from {conn}\') conn.send(data) else: print(f\'Closing connection to {conn}\') sel.unregister(conn) conn.close() sock = socket.socket() sock.bind((host, port)) sock.listen(1000) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask) ``` Implement the `run_echo_server` function in the above code snippet and ensure it meets the requirements mentioned.","solution":"import selectors import socket def run_echo_server(host: str, port: int) -> None: sel = selectors.DefaultSelector() def accept(sock, mask): conn, addr = sock.accept() print(f\'Accepted connection from {addr}\') conn.setblocking(False) sel.register(conn, selectors.EVENT_READ, read) def read(conn, mask): data = conn.recv(1024) if data: print(f\'Received {data} from {conn}\') conn.send(data) else: print(f\'Closing connection to {conn}\') sel.unregister(conn) conn.close() sock = socket.socket() sock.bind((host, port)) sock.listen(1000) sock.setblocking(False) sel.register(sock, selectors.EVENT_READ, accept) while True: events = sel.select() for key, mask in events: callback = key.data callback(key.fileobj, mask)"},{"question":"NIS Directory Synchronization You have been given a system that uses the Network Information Service (NIS) to manage and synchronize user account information across multiple hosts. Your task is to write a function using the `nis` module to verify the synchronization state of user account information between two NIS domains. Function Signature ```python def check_user_synchronization(user_key: str, mapname: str, domain1: str, domain2: str) -> dict: This function checks if the user information associated with `user_key` in `mapname` is synchronized between two NIS domains, `domain1` and `domain2`. :param user_key: The key corresponding to the user information to be matched. :param mapname: The NIS mapname that stores the user information. :param domain1: The first NIS domain. :param domain2: The second NIS domain. :return: A dictionary with the comparison result containing the keys: \'synchronized\' (bool): True if the user information is identical in both domains. \'domain1_value\' (any): The value of the user information in `domain1` or None if not found. \'domain2_value\' (any): The value of the user information in `domain2` or None if not found. pass ``` Inputs and Outputs - The function takes three string inputs: - `user_key`: The key of the user information to be checked. - `mapname`: The NIS map name where the user information can be found. - `domain1`, `domain2`: The NIS domain names to compare. - The function returns a dictionary with the following keys: - `\'synchronized\'` (bool): True if the user information is identical in both domains. - `\'domain1_value\'` (any): The value of the user information in `domain1` if present, otherwise None. - `\'domain2_value\'` (any): The value of the user information in `domain2` if present, otherwise None. Constraints - Assume `user_key`, `mapname`, `domain1`, and `domain2` are all valid strings. - If a lookup fails in either domain, handle `nis.error` appropriately and return `None` for that domain. Example Usage ```python user_info = check_user_synchronization(\\"username\\", \\"passwd.byname\\", \\"domainA\\", \\"domainB\\") print(user_info) # Output could be: # {\'synchronized\': True, \'domain1_value\': \'user_db_entry_domainA\', \'domain2_value\': \'user_db_entry_domainA\'} # or # {\'synchronized\': False, \'domain1_value\': None, \'domain2_value\': \'user_db_entry_domainB\'} ``` Notes - Thorough error handling is crucial for the function to work correctly, as missing entries should not cause the function to fail. - Understanding and appropriate usage of the `nis.match()` function will be key in implementing this solution.","solution":"import nis def check_user_synchronization(user_key: str, mapname: str, domain1: str, domain2: str) -> dict: This function checks if the user information associated with `user_key` in `mapname` is synchronized between two NIS domains, `domain1` and `domain2`. :param user_key: The key corresponding to the user information to be matched. :param mapname: The NIS mapname that stores the user information. :param domain1: The first NIS domain. :param domain2: The second NIS domain. :return: A dictionary with the comparison result containing the keys: \'synchronized\' (bool): True if the user information is identical in both domains. \'domain1_value\' (any): The value of the user information in `domain1` or None if not found. \'domain2_value\' (any): The value of the user information in `domain2` or None if not found. try: domain1_value = nis.match(user_key, mapname, domain1) except nis.error: domain1_value = None try: domain2_value = nis.match(user_key, mapname, domain2) except nis.error: domain2_value = None return { \\"synchronized\\": domain1_value == domain2_value, \\"domain1_value\\": domain1_value, \\"domain2_value\\": domain2_value }"},{"question":"# Functional Programming Assessment Question You are required to write a Python function that processes a list of numbers based on several functional programming tools from the `itertools`, `functools`, and `operator` modules. The task is to apply a series of transformations and reductions to a list of integers. Function Signature ```python def process_numbers(numbers: list) -> int: ``` Input - `numbers`: A list of integers (e.g., `[2, 3, 4, 5, 6]`) Output - An integer which is the result after performing the following operations: 1. **Filtering**: Remove all even numbers from the list. 2. **Mapping**: Square each remaining number. 3. **Reduce**: Compute the product of all squared numbers. Constraints - Use tools from the `itertools`, `functools`, and `operator` modules where appropriate. - The input list will have at least one integer and not more than 10^6 integers. - The numbers will be in the range of -1000 to 1000. Example ```python numbers = [2, 3, 4, 5, 6] result = process_numbers(numbers) print(result) # Output should be 225 (since (3^2 * 5^2) = 9 * 25 = 225) ``` Notes - To filter even numbers, you may consider using `itertools.filterfalse` or a similar function. - For mapping, utilize the `map` function from the standard library. - For reduction, `functools.reduce` can be used. - Remember to import necessary functions from their respective modules. Good luck!","solution":"from itertools import filterfalse from functools import reduce from operator import mul def process_numbers(numbers: list) -> int: Processes a list of numbers by filtering out even numbers, squaring the remaining numbers, and computing the product of the squared numbers. Args: numbers (list): A list of integers. Returns: int: The product of squared odd integers. # Step 1: Filter out even numbers odd_numbers = filterfalse(lambda x: x % 2 == 0, numbers) # Step 2: Square the remaining numbers squared_numbers = map(lambda x: x ** 2, odd_numbers) # Step 3: Compute the product of the squared numbers product = reduce(mul, squared_numbers, 1) return product"},{"question":"Implementing and Using a Buffer Exporter in Python **Objective**: Implement a custom buffer-exporting object in Python and demonstrate its usage through manipulation of the buffer data. # Problem Statement 1. **Custom Buffer Object**: - Implement a custom Python class called `CustomBufferObject` that exports a contiguous buffer of integers. The buffer should store a given list of integers. - This class should implement the buffer protocol and expose its underlying buffer using `__buffer__` method. Specifically, implement the `__buffer__` method to fill in a `Py_buffer` structure correctly. 2. **Buffer Manipulation**: - Write a separate function, `manipulate_buffer`, which takes an instance of `CustomBufferObject` and performs the following operations: 1. Use the buffer protocol to access and modify the underlying buffer directly. 2. Double the value of each integer in the buffer. 3. Return the modified buffer as a list of integers. # Implementation Details 1. **Class Definition**: - The `CustomBufferObject` class should have an initializer that accepts a list of integers. - Implement the `__buffer__` method to expose the buffer of the list of integers. 2. **Buffer Manipulation Function**: - The function `manipulate_buffer` should take an instance of `CustomBufferObject`, get the underlying buffer, modify its contents by doubling each integer, and then return the modified values as a list. # Input and Output - **Input**: - A list of integers to initialize `CustomBufferObject`. - The object to be passed to `manipulate_buffer`. - **Output**: - The modified list of integer values after doubling each value in the buffer. # Example ```python # Initialization data = [1, 2, 3, 4] buffer_obj = CustomBufferObject(data) # Manipulate buffer result = manipulate_buffer(buffer_obj) # Output should be [2, 4, 6, 8] print(result) ``` # Constraints - You should not use any external libraries except the Python standard library. - The integer values in the list should be within the range of a standard C `int`. - Ensure proper management of buffer resources to avoid memory leaks or crashes. # Performance Requirements - The buffer manipulation should occur in place without creating additional copies of the data. - Ensure that your implementation handles large lists efficiently. **Note**: You may refer to the `Buffer Protocol` documentation to understand the specific buffer structure and functions like `PyBuffer_Release` or `PyObject_GetBuffer`.","solution":"import ctypes class CustomBufferObject: def __init__(self, data): self.data = (ctypes.c_int * len(data))(*data) def __buffer__(self): return self.data def manipulate_buffer(buffer_obj): buffer = buffer_obj.__buffer__() for i in range(len(buffer)): buffer[i] = buffer[i] * 2 return list(buffer)"},{"question":"Objective: Design and implement a Python function to interact with an HTTP server using the `http.client` module, demonstrating an understanding of connections, sending requests, handling responses, and error handling. Problem Statement: Using the `http.client` package, write a Python function `fetch_resource_info` that connects to a specified HTTP server, sends a `GET` request to a given URL path, and returns the status code, reason, and first 100 characters of the response body. If any error occurs during the process, the function should return an appropriate error message. Additionally, include error handling for common HTTP errors. Function Signature: ```python def fetch_resource_info(host: str, url: str, use_https: bool = True) -> dict: pass ``` Input: 1. `host` (str): The hostname of the HTTP server (e.g., \'www.python.org\'). 2. `url` (str): The URL path to fetch (e.g., \'/index.html\'). 3. `use_https` (bool): A flag that indicates whether to use HTTPS (default is `True`). Output: A dictionary with the following keys: - `\'status_code\'` (int): The HTTP status code returned by the server. - `\'reason\'` (str): The reason phrase returned by the server. - `\'data\'` (str): The first 100 characters of the response body. - `\'error\'` (str, optional): An error message if any error occurs. Constraints and Details: 1. If `use_https` is `True`, use `HTTPSConnection` to connect to the server, otherwise use `HTTPConnection`. 2. If an error occurs (e.g., invalid URL, connection issues, unexpected HTTP status code), the output dictionary should have the `\'error\'` key with a descriptive error message. 3. Ensure to handle exceptions such as `http.client.HTTPException` and its subclasses appropriately. 4. The function should read the entire response body but only include the first 100 characters in the output. 5. Handle scenarios where the body may be empty. 6. Use the methods from `http.client` module to send the request and read the response. Example Usage: ```python result = fetch_resource_info(\'www.python.org\', \'/\') print(result) # Expected Output would be a dictionary similar to: # { # \'status_code\': 200, # \'reason\': \'OK\', # \'data\': \'<!doctype html><html lang=\\"en\\"><head> ...\', # \'error\': None # } error_result = fetch_resource_info(\'www.python.org\', \'/nonexistent\') print(error_result) # Expected Output would be a dictionary with an error message: # { # \'error\': \'404 Not Found\' # } ``` Note: Ensure to include detailed comments in the code explaining each step and any assumptions made.","solution":"import http.client def fetch_resource_info(host: str, url: str, use_https: bool = True) -> dict: Connect to an HTTP server, send a GET request to the specified URL, and return relevant response details. Parameters: - host (str): The hostname of the HTTP server. - url (str): The URL path to fetch. - use_https (bool): Use HTTPS if True, otherwise use HTTP. Returns: dict: A dictionary with \'status_code\', \'reason\', \'data\' (first 100 chars of response body), and optionally \'error\'. response_dict = {} try: # Choose the connection type if use_https: conn = http.client.HTTPSConnection(host) else: conn = http.client.HTTPConnection(host) # Send GET request conn.request(\\"GET\\", url) response = conn.getresponse() # Read response details response_dict[\'status_code\'] = response.status response_dict[\'reason\'] = response.reason data = response.read() response_dict[\'data\'] = data[:100].decode(\'utf-8\', \'ignore\') conn.close() except http.client.HTTPException as e: response_dict[\'error\'] = str(e) except Exception as e: response_dict[\'error\'] = str(e) return response_dict"},{"question":"# Password Hashing and Verification using the `crypt` Module **Objective**: Implement a password hashing and verification system using the deprecated `crypt` module in Python 3.11. This task will assess your understanding of password hashing, salt generation, and verification using the available methods in the `crypt` module. Task Description 1. **Password Hashing Function**: - Implement a function `hash_password(password: str, method: crypt.METHODS) -> str` that hashes a given password using the specified hashing method. - The function should generate a salt using the provided method and hash the password. - Return the resulting hashed password. 2. **Password Verification Function**: - Implement a function `verify_password(password: str, hashed_password: str) -> bool` that verifies the given password against a previously hashed password. - Use the `crypt` module’s `crypt` function to hash the password with the salt and compare it with the hashed password. - Return `True` if the password matches the hashed password, otherwise `False`. 3. **Command Line Interface**: - Implement a simple command-line interface (CLI) that allows a user to: - Enter a password to be hashed. - Choose a hashing method from the available methods. - Verify a password against an existing hashed password. - Use the functions defined in the previous steps to handle the hashing and verification processes. Input Format 1. For the `hash_password` function: - `password`: A string representing the password to be hashed. - `method`: A method from the `crypt` module’s available methods (e.g., `crypt.METHOD_SHA512`). 2. For the `verify_password` function: - `password`: A string representing the password to be verified. - `hashed_password`: A string representing the previously hashed password. Output Format 1. For the `hash_password` function: - Returns a string representing the hashed password. 2. For the `verify_password` function: - Returns a boolean indicating whether the password matches the hashed password. Constraints - The `crypt` module is deprecated in Python 3.11, so the solution should be compatible with Python 3.10. - The CLI should handle invalid user inputs gracefully. Example ```python import crypt def hash_password(password: str, method: crypt.METHODS) -> str: Hashes a password using the specified method. salt = crypt.mksalt(method) return crypt.crypt(password, salt) def verify_password(password: str, hashed_password: str) -> bool: Verifies a password against a hashed_password. return crypt.crypt(password, hashed_password) == hashed_password # Example usage hash_method = crypt.METHOD_SHA512 hashed_pwd = hash_password(\\"my_secure_password\\", hash_method) print(f\\"Hashed Password: {hashed_pwd}\\") is_verified = verify_password(\\"my_secure_password\\", hashed_pwd) print(f\\"Password Verified: {is_verified}\\") ``` **Notes**: - Your code should be well-structured and handle any edge cases. - Document any assumptions you make.","solution":"import crypt def hash_password(password: str, method) -> str: Hashes a password using the specified method. Args: password (str): The password to hash. method: The crypt method to use for hashing (e.g., crypt.METHOD_SHA512). Returns: str: The hashed password. salt = crypt.mksalt(method) return crypt.crypt(password, salt) def verify_password(password: str, hashed_password: str) -> bool: Verifies a password against a hashed password. Args: password (str): The password to verify. hashed_password (str): The previously hashed password. Returns: bool: True if the password matches the hashed password, False otherwise. return crypt.crypt(password, hashed_password) == hashed_password"},{"question":"You are given a Python class `UnicodeProcessor` that needs to handle various Unicode string operations. Implement the following methods in the class: 1. `encode_to_utf8(self, input_str: str) -> bytes`: This method takes a Unicode string `input_str` and encodes it to UTF-8 bytes. 2. `decode_from_utf8(self, input_bytes: bytes) -> str`: This method takes a `input_bytes` encoded in UTF-8 and decodes it back to a Unicode string. 3. `is_whitespace(self, ch: str) -> bool`: This method checks if the character `ch` is a whitespace character using Unicode properties. 4. `to_uppercase(self, input_str: str) -> str`: This method converts all characters in the Unicode string `input_str` to uppercase. 5. `to_decimal_integer(self, ch: str) -> int`: This method takes a single character `ch` and returns its decimal integer value if it is a decimal character, otherwise returns -1. # Input and Output Formats: - `encode_to_utf8(self, input_str: str) -> bytes` - **Input**: A Unicode string `input_str`. - **Output**: UTF-8 encoded bytes of the input string. - `decode_from_utf8(self, input_bytes: bytes) -> str` - **Input**: A bytes object `input_bytes` encoded in UTF-8. - **Output**: Decoded Unicode string from the input bytes. - `is_whitespace(self, ch: str) -> bool` - **Input**: A single character string `ch`. - **Output**: `True` if `ch` is a whitespace character, otherwise `False`. - `to_uppercase(self, input_str: str) -> str` - **Input**: A Unicode string `input_str`. - **Output**: Unicode string with all characters converted to uppercase. - `to_decimal_integer(self, ch: str) -> int` - **Input**: A single character string `ch`. - **Output**: Decimal integer value of the character if it is a decimal character, otherwise `-1`. # Constraints: 1. The methods should handle edge cases such as empty strings, non-Unicode characters, and ensure the proper encoding/decoding without raising exceptions. 2. The `to_decimal_integer` method should return `-1` for non-decimal characters. # Example: ```python class UnicodeProcessor: def encode_to_utf8(self, input_str: str) -> bytes: # Implement this method pass def decode_from_utf8(self, input_bytes: bytes) -> str: # Implement this method pass def is_whitespace(self, ch: str) -> bool: # Implement this method pass def to_uppercase(self, input_str: str) -> str: # Implement this method pass def to_decimal_integer(self, ch: str) -> int: # Implement this method pass # Example usage: processor = UnicodeProcessor() # Test encode_to_utf8 and decode_from_utf8 assert processor.encode_to_utf8(\\"hello\\") == b\'hello\' assert processor.decode_from_utf8(b\'hello\') == \'hello\' # Test is_whitespace assert processor.is_whitespace(\' \') == True assert processor.is_whitespace(\'a\') == False # Test to_uppercase assert processor.to_uppercase(\'hello\') == \'HELLO\' # Test to_decimal_integer assert processor.to_decimal_integer(\'5\') == 5 assert processor.to_decimal_integer(\'a\') == -1 ``` # Requirements: 1. Implement all the methods in the `UnicodeProcessor` class as described. 2. Avoid using any deprecated Unicode functions or performing operations that may degrade performance. 3. Ensure the solution is efficient and handles all edge cases appropriately.","solution":"class UnicodeProcessor: def encode_to_utf8(self, input_str: str) -> bytes: Encodes a Unicode string to UTF-8 bytes. return input_str.encode(\'utf-8\') def decode_from_utf8(self, input_bytes: bytes) -> str: Decodes UTF-8 bytes back to a Unicode string. return input_bytes.decode(\'utf-8\') def is_whitespace(self, ch: str) -> bool: Checks if a character is a whitespace character using Unicode properties. return ch.isspace() def to_uppercase(self, input_str: str) -> str: Converts a Unicode string to uppercase. return input_str.upper() def to_decimal_integer(self, ch: str) -> int: Converts a single character to its decimal integer value if it is a decimal character, otherwise returns -1. if ch.isdigit(): return int(ch) return -1"},{"question":"# **PyTorch Coding Assessment Question** **Objective** Design a custom PyTorch module and build a simple neural network using it. Train the network using provided data and evaluate its performance. The task will test your understanding of custom modules, training loops, and managing module states. **Question** 1. **Create a Custom Module:** - Implement a custom PyTorch module named `CustomLinear` that mimics the behavior of `torch.nn.Linear`. This module should: - Inherit from `torch.nn.Module`. - Implement the `__init__` method: Initialize weights and biases as `torch.nn.Parameter`. - Implement the `forward` method: Matrix-multiply the input by the weight and add the bias. 2. **Build a Neural Network:** - Implement a custom neural network module named `SimpleNet` using `CustomLinear`. This neural network should: - Contain two `CustomLinear` layers followed by ReLU activation after the first layer. 3. **Training the Network:** - Write a training loop to train the `SimpleNet` on random input data. - The training should aim to minimize the mean squared error (MSE) between the network’s output and random target output. - Initialize the optimizer with stochastic gradient descent (SGD). 4. **Save and Load the Model:** - Save the trained model\'s state_dict. - Implement functions to save and load the model state. **Input and Output Formats** **Input**: 1. Define the `CustomLinear` class. 2. Define the `SimpleNet` class. 3. Implement the `train_network` function. 4. Implement the `save_model` and `load_model` functions. **Output**: 1. The `CustomLinear` and `SimpleNet` classes should be implemented. 2. The training function should output the loss at each epoch. 3. The model should be saved to a file and reloaded successfully. # Constraints - Use ReLU activation after the first `CustomLinear` layer in `SimpleNet`. - Train for 100 epochs. - Learning rate for optimizer: 0.01 - Use MSE Loss function. # Performance Requirements - Ensure the implementation efficiently handles matrix operations. - Verify the state loading results in the same model weights as before saving. **Solution Skeleton** ```python import torch import torch.nn as nn import torch.optim as optim class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return (input @ self.weight) + self.bias class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.layer1 = CustomLinear(4, 3) self.layer2 = CustomLinear(3, 1) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x def train_network(model, epochs=100, lr=0.01): optimizer = optim.SGD(model.parameters(), lr=lr) criterion = nn.MSELoss() for epoch in range(epochs): # Generate random input and target data input_data = torch.randn(4) target_output = torch.randn(1) # Forward pass output = model(input_data) loss = criterion(output, target_output) # Backward pass optimizer.zero_grad() loss.backward() optimizer.step() if (epoch+1) % 10 == 0: print(f\\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\\") def save_model(model, file_path): torch.save(model.state_dict(), file_path) def load_model(model, file_path): model.load_state_dict(torch.load(file_path)) model.eval() # Instantiate the model, train, save and load model = SimpleNet() train_network(model) save_model(model, \'simple_net.pth\') # Load and verify the model loaded_model = SimpleNet() load_model(loaded_model, \'simple_net.pth\') ``` 1. Implement the `CustomLinear` class. 2. Implement the `SimpleNet` class. 3. Implement the `train_network` function to train the `SimpleNet`. 4. Implement the `save_model` and `load_model` functions. Expectation - The `SimpleNet` class should use the `CustomLinear` module and perform forward propagation correctly. - Training the network should reduce the MSE loss over epochs. - Model saving and loading functionalities should persist and retrieve the trained model states accurately.","solution":"import torch import torch.nn as nn import torch.optim as optim class CustomLinear(nn.Module): def __init__(self, in_features, out_features): super(CustomLinear, self).__init__() self.weight = nn.Parameter(torch.randn(in_features, out_features)) self.bias = nn.Parameter(torch.randn(out_features)) def forward(self, input): return torch.matmul(input, self.weight) + self.bias class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.layer1 = CustomLinear(4, 3) self.layer2 = CustomLinear(3, 1) def forward(self, x): x = self.layer1(x) x = torch.relu(x) x = self.layer2(x) return x def train_network(model, epochs=100, lr=0.01): optimizer = optim.SGD(model.parameters(), lr=lr) criterion = nn.MSELoss() for epoch in range(epochs): input_data = torch.randn((1, 4)) target_output = torch.randn((1, 1)) output = model(input_data) loss = criterion(output, target_output) optimizer.zero_grad() loss.backward() optimizer.step() if (epoch + 1) % 10 == 0: print(f\'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\') def save_model(model, file_path): torch.save(model.state_dict(), file_path) def load_model(model, file_path): model.load_state_dict(torch.load(file_path)) model.eval() model = SimpleNet() train_network(model) save_model(model, \'simple_net.pth\') loaded_model = SimpleNet() load_model(loaded_model, \'simple_net.pth\')"},{"question":"**Title: Python Faulthandler: Traceback Management** **Problem:** As a developer, you are tasked with creating a Python utility to manage fault handling using the `faulthandler` module. This utility should demonstrate the capabilities of `faulthandler` by enabling the fault handler, logging tracebacks, handling timeouts, and registering user signals. You need to implement a function `setup_faulthandler` that will: 1. Enable the fault handler and log tracebacks to a specified file. 2. Set up the fault handler to dump tracebacks after a specified timeout. 3. Register a user signal to dump tracebacks on demand. 4. Provide the functionality to disable the fault handler and unregister the signal. 5. Verify if the fault handler is enabled. The following are the specific tasks to be performed by the function: 1. **Enable Fault Handler:** - Enable the fault handler to catch faults and log tracebacks to the file specified by the user. 2. **Timeout Log:** - Set a timeout to dump the tracebacks after a given number of seconds. Repeat the timeout if specified. 3. **Register User Signal:** - Register a user-signal handler to dump tracebacks when a specific signal is received. - Ensure the signal handler is unregistered correctly. 4. **Disable Fault Handler:** - Disable the fault handler after ensuring the tracebacks logging is complete. 5. **Check Fault Handler State:** - Verify if the fault handler is enabled (return `True` or `False`). # Function Signature ```python import faulthandler import signal def setup_faulthandler(log_file, timeout, repeat_timeout, signal_num): Sets up the faulthandler to log tracebacks based on the specified parameters. Args: log_file (str): Path to the file where traceback logs should be written. timeout (int): Number of seconds after which to dump tracebacks. repeat_timeout (bool): If True, repeat the timeout dump. signal_num (int): Signal number to listen for user-generated signal to dump traceback. Returns: bool: Returns True if the fault handler is successfully enabled, else False. # Enable the fault handler # Setup timeout for dumping tracebacks # Register a user signal to dump tracebacks # Provide functionalities to disable the fault handler and unregister signals # Return the state of fault handler ``` # Constraints: 1. The log file should be kept open and should be securely handled for proper logging. 2. Valid signal numbers depend on the operating system. Validate and handle system-specific signals appropriately. 3. Ensure `faulthandler` functionalities are available and can be correctly called multiple times. **Example Usage:** ```python # Example usage log_file_path = \\"traceback.log\\" timeout_seconds = 5 repeat_dump = True user_signal = signal.SIGUSR1 if hasattr(signal, \'SIGUSR1\') else signal.SIGINT # Setup faulthandler handler_status = setup_faulthandler(log_file_path, timeout_seconds, repeat_dump, user_signal) print(f\\"Fault handler enabled: {handler_status}\\") # After performing the setup, the program will log tracebacks under configurations specified. ``` # Notes: - Ensure proper exception handling if invalid parameters are passed. - Implement logic to handle scenarios where the `faulthandler` may not be available or invalid signal numbers are provided. - Demonstrate all features requested in a well-documented manner within your code implementation and comments.","solution":"import faulthandler import signal def setup_faulthandler(log_file, timeout, repeat_timeout, signal_num): Sets up the faulthandler to log tracebacks based on the specified parameters. Args: log_file (str): Path to the file where traceback logs should be written. timeout (int): Number of seconds after which to dump tracebacks. repeat_timeout (bool): If True, repeat the timeout dump. signal_num (int): Signal number to listen for user-generated signal to dump traceback. Returns: bool: Returns True if the fault handler is successfully enabled, else False. try: # Open the log file log_file_handle = open(log_file, \'w\') # Enable the fault handler and log to the specified file faulthandler.enable(log_file_handle) # Set a timeout to dump tracebacks faulthandler.dump_traceback_later(timeout, repeat=repeat_timeout) # Register a user-signal handler signal.signal(signal_num, lambda sig, frame: faulthandler.dump_traceback()) # Return True to indicate the fault handler is enabled return True except (OSError, ValueError) as e: print(f\\"Error: {e}\\") return False def disable_faulthandler(): Disables the faulthandler and unregister user-signal. faulthandler.disable() return not faulthandler.is_enabled() def is_faulthandler_enabled(): Checks if the faulthandler is enabled. return faulthandler.is_enabled()"},{"question":"Question: Implementing and Applying a Graph Transformation with `torch.fx` In this coding assessment, you will demonstrate your understanding of `torch.fx` by implementing a custom transformation that replaces all ReLU activations in a neural network model with Sigmoid activations. You will use symbolic tracing to capture the graph of the model, manipulate the graph, and then apply the transformation to generate a new model. # Task 1. Implement a function `replace_relu_with_sigmoid` that performs the following: - Takes an instance of `torch.nn.Module` as input. - Symbolically traces the model to capture the computation graph. - Iterates through the nodes of the graph and replaces any ReLU activations (`torch.nn.functional.relu`) with Sigmoid activations (`torch.nn.functional.sigmoid`). - Returns a new `torch.nn.Module` with the transformed graph. 2. Create an example neural network model using `torch.nn.Module` that contains at least one ReLU activation. 3. Apply the `replace_relu_with_sigmoid` function to the example model and verify the transformation by printing the original and transformed model graphs. # Constraints - Use only the functions and classes provided by `torch`, `torch.nn`, and `torch.fx`. - Do not change the architecture of the model other than replacing the activations. - Ensure that the transformed model preserves the original model\'s forward pass behavior apart from the activation changes. # Input - A neural network model as an instance of `torch.nn.Module`. # Output - A transformed neural network model with ReLU activations replaced by Sigmoid activations. # Example ```python import torch import torch.nn as nn import torch.fx as fx # Step 1: Define the example model class ExampleModel(nn.Module): def __init__(self): super(ExampleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = torch.relu(x) x = self.fc2(x) return x # Step 2: Implement the transformation function def replace_relu_with_sigmoid(model: nn.Module) -> nn.Module: class ReLUtoSigmoidTracer(fx.Tracer): def is_leaf_module(self, mod, module_qualified_name): return super().is_leaf_module(mod, module_qualified_name) graph = ReLUtoSigmoidTracer().trace(model) for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.relu: node.target = torch.sigmoid transformed_model = fx.GraphModule(model, graph) return transformed_model # Instantiate and trace the example model example_model = ExampleModel() print(\\"Original Model Graph:\\") example_gm = fx.symbolic_trace(example_model) example_gm.graph.print_tabular() # Apply transformation and print the transformed model graph transformed_model = replace_relu_with_sigmoid(example_model) print(\\"nTransformed Model Graph:\\") transformed_gm = fx.symbolic_trace(transformed_model) transformed_gm.graph.print_tabular() ``` # Expected Output - The original model graph should display a ReLU activation node. - The transformed model graph should display a Sigmoid activation node in place of the ReLU activation. **Note:** You are not required to implement unit tests, but consider them as a good practice to ensure the correctness of your transformation.","solution":"import torch import torch.nn as nn import torch.fx as fx # Step 1: Define the example model class ExampleModel(nn.Module): def __init__(self): super(ExampleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 10) def forward(self, x): x = self.fc1(x) x = torch.relu(x) x = self.fc2(x) return x # Step 2: Implement the transformation function def replace_relu_with_sigmoid(model: nn.Module) -> nn.Module: class ReLUtoSigmoidTracer(fx.Tracer): def is_leaf_module(self, mod, module_qualified_name): return super().is_leaf_module(mod, module_qualified_name) graph = ReLUtoSigmoidTracer().trace(model) for node in graph.nodes: if node.op == \'call_function\' and node.target == torch.relu: node.target = torch.sigmoid transformed_model = fx.GraphModule(model, graph) return transformed_model # Instantiate the example model example_model = ExampleModel()"},{"question":"**Coding Assessment Question: Contact List Management** Objective You are required to implement a Python program that manages a contact list. The contact list will be stored in a file and should support operations such as adding a new contact, searching for a contact, and displaying all contacts. Input/Output Format # Input 1. The contact information will be stored in a text file named `contacts.txt`. Each line in the file represents one contact in the following format: ``` Name, Phone Number, Email Address ``` 2. Your program should read from this file at the beginning to load the existing contacts. 3. The program should support the following operations: - Add a new contact. - Search for a contact by name. - Display all contacts. # Constraints 1. You can assume the contact names are unique. 2. The email address must contain the \'@\' symbol. 3. A phone number consists of 10 digits. # Output - When displaying all contacts, they should be formatted as follows: ``` Name: John Doe, Phone: 1234567890, Email: john.doe@example.com ``` - When searching for a contact by name, display the contact details in the same format. If the contact is not found, display `Contact not found`. # Performance Requirements - The solution should efficiently handle the contact list operations, even if the file contains thousands of contacts. # Function Implementations You need to implement the following functions: 1. `load_contacts(filename: str) -> dict` - Reads the contacts from the file and returns them as a dictionary with the name as the key and a tuple of (phone number, email) as the value. 2. `add_contact(contacts: dict, name: str, phone: str, email: str) -> None` - Adds a new contact to the contacts dictionary. 3. `search_contact(contacts: dict, name: str) -> str` - Searches for a contact by name and returns the contact details formatted as described. If not found, returns `Contact not found`. 4. `display_contacts(contacts: dict) -> str` - Returns a string with all contacts formatted as described. 5. `save_contacts(filename: str, contacts: dict) -> None` - Writes the contacts back to the file in the same format as described. Example Given the initial `contacts.txt` file: ``` John Doe, 1234567890, john.doe@example.com Jane Smith, 0987654321, jane.smith@example.org ``` Example usage of the program: ```python contacts = load_contacts(\'contacts.txt\') add_contact(contacts, \'Alice Brown\', \'1122334455\', \'alice.brown@gmail.com\') print(search_contact(contacts, \'Jane Smith\')) print(display_contacts(contacts)) save_contacts(\'contacts.txt\', contacts) ``` Expected output: ``` Name: Jane Smith, Phone: 0987654321, Email: jane.smith@example.org Name: John Doe, Phone: 1234567890, Email: john.doe@example.com Name: Jane Smith, Phone: 0987654321, Email: jane.smith@example.org Name: Alice Brown, Phone: 1122334455, Email: alice.brown@gmail.com ``` After running the program, the `contacts.txt` should be updated to: ``` John Doe, 1234567890, john.doe@example.com Jane Smith, 0987654321, jane.smith@example.org Alice Brown, 1122334455, alice.brown@gmail.com ```","solution":"def load_contacts(filename: str) -> dict: contacts = {} with open(filename, \'r\') as file: for line in file: name, phone, email = line.strip().split(\', \') contacts[name] = (phone, email) return contacts def add_contact(contacts: dict, name: str, phone: str, email: str) -> None: contacts[name] = (phone, email) def search_contact(contacts: dict, name: str) -> str: if name in contacts: phone, email = contacts[name] return f\\"Name: {name}, Phone: {phone}, Email: {email}\\" return \\"Contact not found\\" def display_contacts(contacts: dict) -> str: return \'n\'.join([f\\"Name: {name}, Phone: {phone}, Email: {email}\\" for name, (phone, email) in contacts.items()]) def save_contacts(filename: str, contacts: dict) -> None: with open(filename, \'w\') as file: for name, (phone, email) in contacts.items(): file.write(f\\"{name}, {phone}, {email}n\\")"},{"question":"# Question: **Tensor Shape Manipulation Using `torch.Size` in PyTorch** You are given a 3-dimensional tensor `x` of ones with shape `(10, 20, 30)`. Write a function `adjust_tensor_shape` that takes a tensor and adjusts its shape based on specified operations. Your function should perform the following tasks: 1. Determine the size of the input tensor. 2. Add padding to the first dimension to make it a multiple of 15. 3. Remove the last dimension if it exceeds 25. Your function should return the adjusted tensor. Function Signature: ```python def adjust_tensor_shape(x: torch.Tensor) -> torch.Tensor: pass ``` Input: - `x`: A tensor of shape `(10, 20, 30)`. Output: - A tensor with adjusted shape based on the described operations. Example: ```python >>> x = torch.ones(10, 20, 30) >>> result = adjust_tensor_shape(x) >>> result.size() torch.Size([15, 20, 25]) ``` Constraints: - The function should handle tensors of any size, but for the purpose of this question, you can assume that `x` starts with the shape `(10, 20, 30)`. - Use the `torch.Size` class to manipulate the sizes. Performance Requirements: - The function should be efficient with the operations performed on the tensor. # Note: You should use functionalities provided by PyTorch\'s `torch.Size` class and tensor manipulation functions to achieve the desired shape adjustments.","solution":"import torch def adjust_tensor_shape(x: torch.Tensor) -> torch.Tensor: Adjusts the shape of the given tensor based on the following operations: 1. Adds padding to the first dimension to make it a multiple of 15. 2. Removes the last dimension if it exceeds 25. # Determine the size of the input tensor original_size = x.size() # Add padding to the first dimension to make it a multiple of 15 new_dim1 = (original_size[0] + 14) // 15 * 15 if new_dim1 > original_size[0]: pad_size = new_dim1 - original_size[0] x = torch.cat((x, torch.zeros(pad_size, *original_size[1:])), dim=0) # Remove the last dimension if it exceeds 25 if original_size[2] > 25: x = x[:, :, :25] return x"},{"question":"# Coding Assessment: Custom Traceback Handling and Formatting Objective: To demonstrate your understanding of the `traceback` module in Python, you will implement a custom error handling and logging mechanism that formats stack traces and exception information in a specific manner. Task: 1. **Function `custom_traceback_formatter`**: - Input: - `limit` (Optional[int]): Maximum number of stack trace entries to show. If None, show all entries. - `capture_locals` (bool): Determines whether to capture local variables in the stack trace. - Output: - A string that contains the formatted traceback and exception information. - Behavior: - Catch any exceptions that occur within the function. - Use the `traceback` module\'s functions to format the stack trace and the exception information. - Include the line numbers and local variables in the stack trace if `capture_locals` is True. - Ensure the output includes a message indicating which exception occurred. 2. **Function `test_custom_traceback_formatter`**: - A test function that intentionally raises an exception to demonstrate the functionality of `custom_traceback_formatter`. Constraints: - Your implementation should handle both syntax and runtime errors. - The function should work seamlessly for nested function calls. Example: ```python def custom_traceback_formatter(limit=None, capture_locals=False): import traceback import sys try: # Example code to intentionally raise an exception for demonstration def faulty_function(): nested_function() def nested_function(): raise ValueError(\\"An intentional error!\\") faulty_function() except Exception as e: exc_type, exc_value, exc_traceback = sys.exc_info() formatted_traceback = traceback.format_exception(exc_type, exc_value, exc_traceback, limit=limit, chain=True) if capture_locals: stack_summaries = traceback.StackSummary.extract(traceback.walk_tb(exc_traceback), limit=limit, capture_locals=capture_locals) formatted_traceback = stack_summaries.format() return \'\'.join(formatted_traceback) def test_custom_traceback_formatter(): output = custom_traceback_formatter(limit=2, capture_locals=True) print(output) test_custom_traceback_formatter() ``` Expected console output: ``` Traceback (most recent call last): File \\"script.py\\", line 9, in custom_traceback_formatter faulty_function() File \\"script.py\\", line 12, in faulty_function nested_function() File \\"script.py\\", line 15, in nested_function raise ValueError(\\"An intentional error!\\") ValueError: An intentional error! ``` Evaluation Criteria: - Correct and efficient use of the traceback module functions. - Code readability and proper handling of exceptions. - Quality and clarity of formatted output.","solution":"import traceback import sys def custom_traceback_formatter(limit=None, capture_locals=False): Captures a formatted traceback of any exception that occurs within its scope. Parameters: limit (Optional[int]): Maximum number of stack trace entries to show. If None, show all entries. capture_locals (bool): Determines whether to capture local variables in the stack trace. Returns: str: A string that contains the formatted traceback and exception information. try: # Example code to intentionally raise an exception for demonstration def faulty_function(): nested_function() def nested_function(): raise ValueError(\\"An intentional error!\\") faulty_function() except Exception as e: exc_type, exc_value, exc_traceback = sys.exc_info() formatted_traceback = \'\'.join(traceback.format_exception(exc_type, exc_value, exc_traceback, limit=limit)) if capture_locals: stack_summaries = traceback.StackSummary.extract(traceback.walk_tb(exc_traceback), limit=limit, capture_locals=capture_locals) formatted_traceback += \'\'.join(stack_summaries.format()) return formatted_traceback def test_custom_traceback_formatter(): output = custom_traceback_formatter(limit=2, capture_locals=True) print(output)"},{"question":"<|Analysis Begin|> The provided documentation contains extensive information about the memory management in Python. It details the workings of Python\'s memory allocator, the various domains for memory allocation, interfaces for raw memory, the memory interface, object allocators, customization of memory allocators, and usage of debug hooks to detect memory errors. The document also highlights the importance of using Python\'s memory management functions over the standard C library functions. Given this extensive coverage, we can create a coding assessment question that focuses on implementing a memory allocator simulated in Python. The question will involve creating custom memory management functions that mimic the behavior of `PyMem_Malloc`, `PyMem_Realloc`, `PyMem_Free`, etc., and ensure proper memory allocation, reallocation, and deallocation. <|Analysis End|> <|Question Begin|> # Custom Memory Allocator Simulation in Python Memory management is a crucial aspect of programming that involves the allocation, reallocation, and deallocation of memory. In Python, this is primarily managed by various memory management functions provided by the Python/C API. For this task, you will simulate a simple memory allocator in Python, which mimics some of the operations performed by the Python memory management functions such as `PyMem_Malloc`, `PyMem_Realloc`, and `PyMem_Free`. You need to implement the following class `CustomMemoryAllocator` in Python: ```python class CustomMemoryAllocator: def __init__(self): # Initialize any necessary data structures pass def malloc(self, n): Allocate `n` bytes of memory and return a pointer (simulated as an index or a unique identifier). If the allocation fails, return None. pass def realloc(self, ptr, n): Resize the memory block pointed to by `ptr` to `n` bytes. If `ptr` is None, the call is equivalent to `malloc(n)`. If resizing fails, return None and keep the original pointer valid. pass def free(self, ptr): Free the memory block pointed to by `ptr`. Ensure that invalid free operations (freeing the same pointer twice or freeing a non-allocated pointer) are handled properly. pass ``` # Requirements: 1. **Initialization**: Initialize necessary data structures to manage the allocated memory blocks. 2. **Memory Allocation**: - Implement the `malloc` method to allocate `n` bytes of memory. - Return a unique identifier (simulating a pointer) for the allocated block. - If allocation fails, return `None`. 3. **Memory Reallocation**: - Implement the `realloc` method to resize an existing memory block. - If the pointer `ptr` is `None`, it should behave like `malloc`. - If resizing fails, return `None` and keep the original pointer valid. 4. **Memory Deallocation**: - Implement the `free` method to deallocate the memory block pointed to by `ptr`. - Handle cases where the same pointer is freed multiple times or when a non-allocated pointer is freed. # Input and Output: - The `malloc` method takes an integer `n` (number of bytes) and returns an identifier (simulating a memory address) or `None` if the allocation fails. - The `realloc` method takes a pointer `ptr` (the identifier returned by `malloc`) and an integer `n` (new size) and returns a new pointer (new identifier) or `None` if resizing fails. - The `free` method takes a pointer `ptr` and deallocates the memory. # Example Usage: ```python allocator = CustomMemoryAllocator() ptr1 = allocator.malloc(100) assert ptr1 is not None ptr2 = allocator.realloc(ptr1, 200) assert ptr2 is not None allocator.free(ptr2) # Attempting to free the same pointer again should be handled gracefully allocator.free(ptr2) # Invalid operation, should not crash ptr3 = allocator.malloc(50) assert ptr3 is not None allocator.free(ptr3) ``` # Constraints: - The maximum number of memory blocks that can be managed simultaneously is 10,000. - Each memory block\'s maximum size is 1 MB. Implement your solution in Python, ensuring best practices for memory management and handling edge cases effectively.","solution":"class CustomMemoryAllocator: def __init__(self): self.memory = {} self.next_ptr = 1 def malloc(self, n): if n <= 0 or n > 1024 * 1024: return None ptr = self.next_ptr self.memory[ptr] = bytearray(n) self.next_ptr += 1 return ptr def realloc(self, ptr, n): if n <= 0 or n > 1024 * 1024: return None if ptr == None: return self.malloc(n) if ptr not in self.memory: return None old_block = self.memory[ptr] new_block = bytearray(n) new_block[:len(old_block)] = old_block[:n] self.memory[ptr] = new_block return ptr def free(self, ptr): if ptr in self.memory: del self.memory[ptr]"},{"question":"# UUID Processing Challenge You are tasked with implementing a function that processes a list of mixed UUID inputs, which might be provided in various formats. The function should extract the UUIDs from the inputs, convert them to their standard 36-character string format, and return a list of these standard UUID strings. Additionally, the function should identify and return a Boolean indicating if all UUIDs in the input list were generated safely (`SafeUUID.safe`). Function Signature ```python def process_uuids(uuid_list: list) -> tuple: Process a list of mixed UUID inputs and return the standard UUID strings and a safety status. :param uuid_list: List of UUID inputs in various formats (string, bytes, int, etc.). :return: A tuple with a list of standard UUID strings and a Boolean indicating safety. ``` Input Format - `uuid_list`: A list of UUID inputs, where each UUID can be provided in one of the following formats: - A string of 32 hexadecimal digits (with or without dashes and curly braces). - A string of 16 bytes in big-endian order. - A string of 16 bytes in little-endian order. - A tuple of six integers. - A single 128-bit integer. Output Format - A tuple containing: - A list of UUID strings in the form \\"12345678-1234-5678-1234-567812345678\\". - A Boolean indicating if all UUIDs in the input list were generated safely (`SafeUUID.safe`). Constraints - At least one of the UUIDs in the input list will be provided in a valid format. - The function should be capable of handling mixed formats in the input list. Example ```python import uuid # Example UUID inputs uuid_inputs = [ \\"12345678123456781234567812345678\\", \\"{12345678-1234-5678-1234-567812345678}\\", b\'x12x34x56x78\'*4, b\'x78x56x34x12x34x12x78x56\' + b\'x12x34x56x78x12x34x56x78\', (0x12345678, 0x1234, 0x5678, 0x12, 0x34, 0x567812345678), 0x12345678123456781234567812345678 ] # Function call result = process_uuids(uuid_inputs) print(result) # Expected Output: ([\'12345678-1234-5678-1234-567812345678\', ...], False) ``` # Notes - You may use the `uuid` module for handling and converting UUIDs. - Ensure to check the safety of the UUIDs using the `is_safe` attribute.","solution":"import uuid def process_uuids(uuid_list): Process a list of mixed UUID inputs and return the standard UUID strings and a safety status. :param uuid_list: List of UUID inputs in various formats (string, bytes, int, etc.). :return: A tuple with a list of standard UUID strings and a Boolean indicating safety. standard_uuids = [] all_safe = True for item in uuid_list: if isinstance(item, str): if item.startswith(\'{\') and item.endswith(\'}\'): item = item[1:-1] u = uuid.UUID(item) elif isinstance(item, bytes): if len(item) == 16: u = uuid.UUID(bytes=item) else: raise ValueError(\\"Invalid bytes length for UUID.\\") elif isinstance(item, tuple) and len(item) == 6: u = uuid.UUID(fields=item) elif isinstance(item, int): u = uuid.UUID(int=item) else: raise ValueError(\\"Invalid format for UUID.\\") standard_uuids.append(str(u)) if hasattr(u, \'is_safe\') and not u.is_safe: all_safe = False return (standard_uuids, all_safe)"},{"question":"Objective: You are required to demonstrate your understanding of the `multiprocessing.shared_memory` package by designing a system that allows multiple processes to concurrently write integer values into a shared memory block and then read and sum up those values after all processes have finished execution. Problem Statement: Write a Python function `shared_memory_sum(n_processes, values_per_process)` that achieves the following: 1. Creates a shared memory block large enough to hold `n_processes * values_per_process` integers. 2. Spawns `n_processes` subprocesses. Each subprocess writes `values_per_process` integers (starting from its process index * values_per_process) into the shared memory block. 3. Ensures that each process correctly writes its array of values into the appropriate section of the shared memory. 4. After all subprocesses have finished writing data, the main process should read all values from the shared memory block and return the sum of these integers. 5. Properly cleans up by closing and unlinking the shared memory block. Function Signature: ```python def shared_memory_sum(n_processes: int, values_per_process: int) -> int: pass ``` Input: - `n_processes`: An integer, the number of subprocesses to create. - `values_per_process`: An integer, the number of integer values each process will write. Output: - An integer, the sum of all values written in the shared memory block. Constraints: - `n_processes` can be up to 100. - `values_per_process` can be up to 1000. Example: ```python result = shared_memory_sum(2, 5) assert result == sum(range(10)) ``` Explanation: - Two processes write 5 integers each. - Process 1 writes [0, 1, 2, 3, 4] - Process 2 writes [5, 6, 7, 8, 9] - The sum is 0 + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 = 45 Note: - Use `multiprocessing.shared_memory.SharedMemory` for shared memory operations. - Use `multiprocessing.Process` to create subprocesses. - Ensure that resources are properly cleaned up to avoid memory leaks.","solution":"from multiprocessing import Process, shared_memory import numpy as np def worker(shm_name, start_idx, values_per_process): # Attach to the existing shared memory shm = shared_memory.SharedMemory(name=shm_name) # Create a numpy array view of the shared memory arr = np.ndarray((values_per_process,), dtype=np.int64, buffer=shm.buf, offset=start_idx * 8) # Fill the array with values starting from start_idx arr[:] = np.arange(start_idx, start_idx + values_per_process) # Detach from the shared memory shm.close() def shared_memory_sum(n_processes, values_per_process): # Calculate the size of the shared memory total_values = n_processes * values_per_process shm_size = total_values * np.dtype(np.int64).itemsize # Create shared memory shm = shared_memory.SharedMemory(create=True, size=shm_size) # Create a numpy array view of the shared memory arr = np.ndarray((total_values,), dtype=np.int64, buffer=shm.buf) # List to keep track of processes processes = [] # Spawn processes for i in range(n_processes): start_idx = i * values_per_process process = Process(target=worker, args=(shm.name, start_idx, values_per_process)) process.start() processes.append(process) # Wait for all processes to finish for process in processes: process.join() # Sum the values in the shared memory total_sum = np.sum(arr) # Clean up shm.close() shm.unlink() return total_sum"},{"question":"# Advanced Coding Assessment Objective This assessment aims to evaluate your understanding of Python\'s set and frozenset objects, along with the C-API functions associated with them. You are required to implement a class that simulates certain behaviors of set/frozenset using the C-API functions provided. Problem Statement Implement a class `SetOperations` in Python that provides the following methods: 1. **`add_to_set`**: Adds an element to the set. ```python def add_to_set(self, key: Any) -> None: ``` 2. **`is_member`**: Checks if an element is in the set. ```python def is_member(self, key: Any) -> bool: ``` 3. **`remove_from_set`**: Removes an element from the set if it exists. ```python def remove_from_set(self, key: Any) -> bool: ``` 4. **`pop_random_element`**: Removes and returns a random element from the set. ```python def pop_random_element(self) -> Any: ``` Class Definition The class should be implemented as follows: ```python class SetOperations: def __init__(self): self.my_set = set() def add_to_set(self, key: Any) -> None: Adds an element to the set. If the key is unhashable, raise a TypeError. pass def is_member(self, key: Any) -> bool: Returns True if the key is a member of the set, False otherwise. Raise a TypeError if the key is unhashable. pass def remove_from_set(self, key: Any) -> bool: Removes an element from the set if it exists and returns True; if the element is not present, returns False. pass def pop_random_element(self) -> Any: Removes and returns a random element from the set. If the set is empty, raise a KeyError. pass ``` Constraints - You may assume the set will only contain elements that are hashable. - You must use the provided C-API methods where applicable. - You should handle the possible exceptions mentioned in the documentation. Example Usage ```python # Example usage of the SetOperations class # Initialize the class set_ops = SetOperations() # Add elements to the set set_ops.add_to_set(1) set_ops.add_to_set(2) set_ops.add_to_set(3) # Check membership assert set_ops.is_member(2) == True assert set_ops.is_member(4) == False # Remove an element assert set_ops.remove_from_set(2) == True assert set_ops.is_member(2) == False # Pop a random element element = set_ops.pop_random_element() assert element in {1, 3} # Example of error handling try: set_ops.add_to_set([1, 2]) except TypeError as e: print(e) # Output: unhashable type: \'list\' try: set_ops.pop_random_element() except KeyError as e: print(e) # Output: \'pop from an empty set\' ``` Submission Submit your implementation as a Python (.py) file. Ensure your code is well-documented and follows best practices.","solution":"class SetOperations: def __init__(self): self.my_set = set() def add_to_set(self, key) -> None: Adds an element to the set. If the key is unhashable, raise a TypeError. try: self.my_set.add(key) except TypeError as e: raise TypeError(\\"unhashable type: \'{}\'\\".format(type(key).__name__)) def is_member(self, key) -> bool: Returns True if the key is a member of the set, False otherwise. Raise a TypeError if the key is unhashable. try: return key in self.my_set except TypeError as e: raise TypeError(\\"unhashable type: \'{}\'\\".format(type(key).__name__)) def remove_from_set(self, key) -> bool: Removes an element from the set if it exists and returns True; if the element is not present, returns False. if key in self.my_set: self.my_set.remove(key) return True return False def pop_random_element(self): Removes and returns a random element from the set. If the set is empty, raise a KeyError. if not self.my_set: raise KeyError(\\"pop from an empty set\\") return self.my_set.pop()"},{"question":"You are required to implement a logging system for a Python application using the `syslog` module. The system should log different types of messages (informational, errors, warnings) to the system\'s mail logging facility, including the process ID in every log message. Follow the specifications below: Specifications 1. **Function 1: setup_logging** - **Input:** No input parameters. - **Output:** No output. - **Behavior:** Configures the logging options to include the process ID in logged messages and use the `LOG_MAIL` facility. 2. **Function 2: log_message** - **Input:** - `message`: a string representing the message to be logged. - `priority`: (optional) an integer representing the priority level of the message. Defaults to `syslog.LOG_INFO`. - **Output:** No output. - **Behavior:** Sends the specified `message` to the system logger with the given `priority`. 3. **Function 3: close_logging** - **Input:** No input parameters. - **Output:** No output. - **Behavior:** Resets the syslog module values to their defaults. Constraints - The `priority` parameter in `log_message` should utilize the predefined priority levels in `syslog`. - Ensure to properly handle opening the log if it hasn’t been opened already before logging a message. - Properly reset the syslog module values on closing the log. # Example: ```python import syslog def setup_logging(): # Your implementation here pass def log_message(message, priority=syslog.LOG_INFO): # Your implementation here pass def close_logging(): # Your implementation here pass # Example usage: setup_logging() log_message(\'Application started\') log_message(\'Encountered an error\', syslog.LOG_ERR) close_logging() ``` Ensure your implementation works as expected with the usage example.","solution":"import syslog def setup_logging(): Configures the logging options to include the process ID in logged messages and use the LOG_MAIL facility. syslog.openlog(logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) def log_message(message, priority=syslog.LOG_INFO): Sends the specified message to the system logger with the given priority. :param message: a string representing the message to be logged. :param priority: (optional) an integer representing the priority level of the message, default is syslog.LOG_INFO. syslog.syslog(priority, message) def close_logging(): Resets the syslog module values to their defaults. syslog.closelog()"},{"question":"**Coding Assessment Question:** # Objective: To assess your understanding of fundamental and advanced concepts using the `wave` and `colorsys` modules in Python, you are required to implement a function that reads a WAV file, processes the audio data, and performs a color transformation based on the audio characteristics. # Task: Implement a function `process_audio_and_convert_color` that performs the following operations: 1. **Read** a WAV file and extract audio frames. 2. **Analyze** the audio data to determine the average volume of the waveform. 3. **Based** on the average volume, convert an RGB color to its corresponding HSV value. 4. **Return** the HSV color value. # Function Signature: ```python import wave import colorsys def process_audio_and_convert_color(wav_filename: str, rgb_color: tuple) -> tuple: pass ``` # Input: - `wav_filename` (str): The path to the WAV file to be processed. - `rgb_color` (tuple): A tuple representing an RGB color in the form `(r, g, b)` where `r`, `g`, and `b` are integers in the range [0, 255]. # Output: - A tuple representing the HSV color in the form `(h, s, v)` where `h`, `s`, and `v` are floats in the range [0.0, 1.0]. # Requirements: 1. The function must read the WAV file and extract the audio frames. 2. The function should compute the average volume of the audio data. 3. Based on the average volume, transform the given RGB color into an HSV color using the `colorsys` module. 4. Ensure the implementation handles errors gracefully (e.g., invalid file path, invalid color tuple). # Example: ```python # Example WAV file and RGB color wav_filename = \'example.wav\' rgb_color = (255, 0, 0) # Call the function hsv_color = process_audio_and_convert_color(wav_filename, rgb_color) print(hsv_color) # Example Output: (0.0, 1.0, 1.0) ``` # Constraints: - Only use the `wave` and `colorsys` modules from the standard Python library for WAV processing and color conversion tasks. # Notes: - The function should perform file operations efficiently and handle large files without excessive memory usage. - Ensure the average volume calculation is accurate, and the color conversion is correctly implemented.","solution":"import wave import colorsys def process_audio_and_convert_color(wav_filename: str, rgb_color: tuple) -> tuple: def calculate_average_volume(frames, sample_width): if sample_width == 1: # 8-bit samples fmt = f\\"{len(frames)}B\\" elif sample_width == 2: # 16-bit samples fmt = f\\"{len(frames) // 2}h\\" else: raise ValueError(f\\"Unsupported sample width: {sample_width}\\") import struct samples = struct.unpack(fmt, frames) samples = [abs(sample) for sample in samples] return sum(samples) / len(samples) try: # Read the WAV file with wave.open(wav_filename, \'rb\') as wf: n_channels = wf.getnchannels() sample_width = wf.getsampwidth() n_frames = wf.getnframes() frames = wf.readframes(n_frames) average_volume = calculate_average_volume(frames, sample_width) # Normalize the average volume to the range [0.0, 1.0] max_volume = (2 ** (8 * sample_width - 1)) normalized_volume = average_volume / max_volume # Convert RGB to HSV r, g, b = rgb_color r_normalized, g_normalized, b_normalized = r / 255.0, g / 255.0, b / 255.0 h, s, v = colorsys.rgb_to_hsv(r_normalized, g_normalized, b_normalized) # Adjust the brightness based on the audio volume adjusted_v = v * normalized_volume return (h, s, adjusted_v) except Exception as e: print(f\\"An error occurred: {e}\\") return (0.0, 0.0, 0.0) # Return default HSV color in case of error"},{"question":"**Coding Assessment Question** --- **Task:** You are responsible for developing a function to handle incoming emails with JSON data. Your task is to write a Python function that reads an email message, extracts the JSON content from its payload, decodes it, and returns it as a dictionary. # Function Signature ```python def extract_json_from_email(raw_email: str) -> dict: pass ``` # Input - `raw_email` (string): A string representing the raw text of an email message. The email message contains a JSON payload in its body. # Output - Returns a dictionary parsed from the JSON payload within the email. # Constraints 1. Assume the email body always contains well-formed JSON data. 2. The input email may have attachments, but you only need to extract and process the JSON data from the email body. 3. Handle and return an empty dictionary if there is no JSON payload in the email body. # Performance Requirements - The solution should efficiently parse and extract JSON data irrespective of the email size. # Example ```python raw_email = ( \\"From: sender@example.comn\\" \\"To: receiver@example.comn\\" \\"Subject: Test Emailn\\" \\"Content-Type: application/jsonn\\" \\"Content-Transfer-Encoding: 7bitn\\" \\"n\\" \'{\\"key\\": \\"value\\", \\"number\\": 123, \\"boolean\\": true}\' ) expected_output = { \\"key\\": \\"value\\", \\"number\\": 123, \\"boolean\\": True } assert extract_json_from_email(raw_email) == expected_output ``` # Notes 1. Use the `email` module for parsing the email message. 2. Use the `json` module for decoding the JSON payload. You are encouraged to test your function with additional email structures to ensure robustness. ---","solution":"import json from email import message_from_string def extract_json_from_email(raw_email: str) -> dict: Extracts the JSON content from the email body and returns it as a dictionary. Parameters: raw_email (str): A string representing the raw text of an email message. Returns: dict: A dictionary parsed from the JSON payload within the email. email_message = message_from_string(raw_email) payload = email_message.get_payload() try: return json.loads(payload) except json.JSONDecodeError: return {}"},{"question":"# Objective: To assess your understanding of Support Vector Machines (SVMs) using Scikit-learn for classification tasks. # Problem Statement: You are provided with a dataset consisting of two features and corresponding binary class labels. Your task is to: 1. Implement a function to load and preprocess the data. 2. Train an SVM classifier using the RBF kernel. 3. Fine-tune the hyperparameters (`C` and `gamma`) using GridSearchCV. 4. Evaluate the model\'s performance on a test set. # Instructions: 1. **Implement the function `load_and_preprocess_data()`**: - This function should load the dataset from the provided CSV file. - Split the dataset into training and testing sets (80% train, 20% test). - Standardize the features using `StandardScaler`. 2. **Implement the function `train_svm_classifier()`**: - This function should take the training data and labels as input. - Train an SVM classifier using the RBF kernel. - Use GridSearchCV to fine-tune the hyperparameters (`C` and `gamma`). - Return the best model found by GridSearchCV. 3. **Implement the function `evaluate_model()`**: - This function should take the trained model and test data as input. - Predict the labels for the test set and compute the accuracy. - Return the accuracy score. # Input: - A CSV file named `data.csv` with the following structure: ``` feature1,feature2,label 5.1,3.5,0 4.9,3.0,1 ... (more rows) ``` # Output: - Accuracy of the SVM classifier on the test set. # Constraints: - Use `StandardScaler` for feature scaling. - Use `GridSearchCV` to find the best hyperparameters with the following parameter grid: ```python param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } ``` - Use a 5-fold cross-validation in `GridSearchCV`. # Function Signatures: ```python import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def load_and_preprocess_data(file_path): Load and preprocess the dataset. :param file_path: str, path to the CSV file :return: X_train, X_test, y_train, y_test - numpy arrays pass # Implement this function def train_svm_classifier(X_train, y_train): Train an SVM classifier using the RBF kernel. :param X_train: numpy array, training features :param y_train: numpy array, training labels :return: best_model - trained SVM model with best hyperparameters pass # Implement this function def evaluate_model(model, X_test, y_test): Evaluate the SVM classifier on the test set. :param model: trained SVM model :param X_test: numpy array, test features :param y_test: numpy array, test labels :return: accuracy - float, accuracy of the model on the test set pass # Implement this function if __name__ == \\"__main__\\": # Example usage file_path = \'data.csv\' X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) model = train_svm_classifier(X_train, y_train) accuracy = evaluate_model(model, X_test, y_test) print(\\"Test Set Accuracy: {:.2f}%\\".format(accuracy * 100)) ``` # Submission: Submit your code as a Python script named `svm_classifier.py`.","solution":"import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.metrics import accuracy_score def load_and_preprocess_data(file_path): Load and preprocess the dataset. :param file_path: str, path to the CSV file :return: X_train, X_test, y_train, y_test - numpy arrays # Load the dataset from the CSV file data = pd.read_csv(file_path) # Separate the features and the label X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split the dataset into training and testing sets (80% train, 20% test) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Standardize the features using StandardScaler scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) return X_train, X_test, y_train, y_test def train_svm_classifier(X_train, y_train): Train an SVM classifier using the RBF kernel. :param X_train: numpy array, training features :param y_train: numpy array, training labels :return: best_model - trained SVM model with best hyperparameters # Define the SVM classifier with RBF kernel svm = SVC(kernel=\'rbf\') # Define the parameter grid for hyperparameter tuning param_grid = { \'C\': [0.1, 1, 10, 100], \'gamma\': [1, 0.1, 0.01, 0.001] } # Use GridSearchCV to fine-tune the hyperparameters grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring=\'accuracy\') # Fit the model to the training data grid_search.fit(X_train, y_train) # Get the best estimator (model with best hyperparameters) best_model = grid_search.best_estimator_ return best_model def evaluate_model(model, X_test, y_test): Evaluate the SVM classifier on the test set. :param model: trained SVM model :param X_test: numpy array, test features :param y_test: numpy array, test labels :return: accuracy - float, accuracy of the model on the test set # Predict the labels for the test set y_pred = model.predict(X_test) # Compute the accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy if __name__ == \\"__main__\\": # Example usage file_path = \'data.csv\' X_train, X_test, y_train, y_test = load_and_preprocess_data(file_path) model = train_svm_classifier(X_train, y_train) accuracy = evaluate_model(model, X_test, y_test) print(\\"Test Set Accuracy: {:.2f}%\\".format(accuracy * 100))"},{"question":"**Question: Visualizing Different Dimensions of the Titanic Dataset with Seaborn** In this assessment, you will work with the Titanic dataset to create meaningful visualizations that depict different dimensions and distributions of the data. Your task is to write a Python function `visualize_titanic_data()` that performs the following: 1. Load the Titanic dataset using Seaborn\'s `load_dataset` function. 2. Create a stacked bar plot that shows the count of passengers in different classes (`x=\\"class\\"`) colored by their sex (`color=\\"sex\\"`). 3. Create a faceted histogram that shows the distribution of ages (`x=\\"age\\"`) split by sex, with each histogram further separated by survival status (`alpha=\\"alive\\"`). Use a bin width of 10 for the histogram. 4. Save both plots to files named `stacked_bar_plot.png` and `faceted_histogram.png` respectively. # Input - No input; the data should be loaded within the function. # Output - No direct output; the images should be saved to the specified filenames. # Constraints - Ensure that you use Seaborn\'s objects interface (`seaborn.objects`) to create the plots. - Properly handle the data loading and transformations as required by the visualization functions. - Use appropriate labelling and styling to make the plots clear and informative. # Example ```python def visualize_titanic_data(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create the stacked bar plot bar_plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) bar_plot.save(\\"stacked_bar_plot.png\\") # Create the faceted histogram hist_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) hist_plot.save(\\"faceted_histogram.png\\") # Close the plots to free up memory plt.close() # Call the function to generate the plots visualize_titanic_data() ``` Design and implement your function to achieve the tasks outlined above. Ensure that you save the generated plots with the specified filenames. Good luck!","solution":"def visualize_titanic_data(): import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the Titanic dataset titanic = load_dataset(\\"titanic\\").sort_values(\\"alive\\", ascending=False) # Create the stacked bar plot bar_plot = so.Plot(titanic, x=\\"class\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Stack()) bar_plot.save(\\"stacked_bar_plot.png\\") # Create the faceted histogram hist_plot = ( so.Plot(titanic, x=\\"age\\", alpha=\\"alive\\") .facet(\\"sex\\") .add(so.Bars(), so.Hist(binwidth=10), so.Stack()) ) hist_plot.save(\\"faceted_histogram.png\\") # Close the plots to free up memory plt.close() # Call the function to generate the plots visualize_titanic_data()"},{"question":"# Python Built-in Functions Assessment **Objective:** To demonstrate your understanding and ability to integrate Python\'s built-in functions in solving a given problem. **Problem Statement:** You are provided with a list of tuples, each containing a student\'s name and their respective scores in multiple subjects. Your objective is to compute and display the following information: 1. The average score for each student. 2. The name(s) of the top scorer(s) in each subject. 3. The overall top scorer and their average score. 4. For each student, display if their overall score is greater than a given threshold. **Functions to Implement:** 1. `calculate_averages(students: list) -> dict` 2. `top_scorers_by_subject(students: list) -> dict` 3. `overall_top_scorer(students: list) -> tuple` 4. `score_threshold_check(students: list, threshold: float) -> dict` **Input and Output Formats:** 1. `calculate_averages(students: list) -> dict` - **Input:** A list of tuples. Each tuple contains a string (student\'s name) and a list of integers (scores). Example: `[(\\"Alice\\", [70, 80, 90]), (\\"Bob\\", [60, 70, 80])]` - **Output:** A dictionary with student names as keys and their average scores as values. Example: `{\\"Alice\\": 80.0, \\"Bob\\": 70.0}` 2. `top_scorers_by_subject(students: list) -> dict` - **Input:** Same as above. - **Output:** A dictionary with subject indices as keys and a list of names of the top scorers in each subject as values. Example: `{0: [\\"Alice\\"], 1: [\\"Alice\\"], 2: [\\"Alice\\", \\"Bob\\"]}` 3. `overall_top_scorer(students: list) -> tuple` - **Input:** Same as above. - **Output:** A tuple containing the name of the overall top scorer and their average score. Example: `(\\"Alice\\", 80.0)` 4. `score_threshold_check(students: list, threshold: float) -> dict` - **Input:** A list of tuples as above and a float threshold. - **Output:** A dictionary with student names as keys and boolean values indicating if their overall score is greater than the threshold. Example: `{\\"Alice\\": True, \\"Bob\\": False}` **Constraints:** - Each student has the same number of scores. - Scores are all non-negative integers. - The input list will have at least one student. **Performance Requirements:** - Efficient use of built-in functions is expected. - Avoid unnecessary computations. **Example:** ```python students = [(\\"Alice\\", [95, 80, 87]), (\\"Bob\\", [75, 85, 90]), (\\"Charlie\\", [80, 85, 80])] # Calling the functions averages = calculate_averages(students) top_scorers = top_scorers_by_subject(students) overall_top = overall_top_scorer(students) threshold_check = score_threshold_check(students, 85) # Expected Outputs print(averages) # Output: {\\"Alice\\": 87.33, \\"Bob\\": 83.33, \\"Charlie\\": 81.67} print(top_scorers) # Output: {0: [\\"Alice\\"], 1: [\\"Bob\\"], 2: [\\"Alice\\"]} print(overall_top) # Output: (\\"Alice\\", 87.33) print(threshold_check) # Output: {\\"Alice\\": True, \\"Bob\\": False, \\"Charlie\\": False} ``` **Note:** Ensure the average scores are rounded to two decimal places where necessary.","solution":"def calculate_averages(students): Returns a dictionary with student names as keys and their average scores as values. return {name: round(sum(scores) / len(scores), 2) for name, scores in students} def top_scorers_by_subject(students): Returns a dictionary with subject indices as keys and the list of names of top scorers in each subject as values. from collections import defaultdict subject_scores = defaultdict(list) for name, scores in students: for i, score in enumerate(scores): subject_scores[i].append((score, name)) top_scorers = {} for subject, scores in subject_scores.items(): max_score = max(scores)[0] top_scorers[subject] = [name for score, name in scores if score == max_score] return top_scorers def overall_top_scorer(students): Returns a tuple containing the name of the overall top scorer and their average score. averages = calculate_averages(students) top_scorer = max(averages.items(), key=lambda item: item[1]) return top_scorer def score_threshold_check(students, threshold): Returns a dictionary with student names as keys and boolean values indicating if their overall score is greater than the given threshold. averages = calculate_averages(students) return {name: avg > threshold for name, avg in averages.items()}"},{"question":"You are tasked with implementing a simple interactive Python REPL (Read-Eval-Print Loop) using the `codeop` module. This REPL should continuously read input from the user, compile it using the `codeop.compile_command` function, execute the compiled code, and print the result. If the input forms an incomplete statement, the REPL should continue reading input until a complete statement is formed. Your task is to implement the following function: ```python import codeop def simple_repl(): This function implements a simple REPL using the codeop module. It should continuously read input from the user, compile it using codeop.compile_command, and execute it. It should handle incomplete statements and errors appropriately. You need to handle the following: - Read input from the user continuously. - Compile the input using `codeop.compile_command`. - Execute the compiled code if it is valid. - Handle invalid Python syntax by displaying an error message. - Continue reading lines from input if the statement is incomplete. - Provide a way to exit the REPL cleanly (e.g., if the user types \'exit()\'). ``` # Requirements: 1. The REPL should handle multi-line statements correctly. 2. If the input is invalid Python code, an appropriate error message should be displayed. 3. The REPL should continue running until the user explicitly exits (e.g., by typing `exit()`). 4. Use the `codeop.compile_command` for compiling the input. 5. You are not allowed to use the built-in `compile` function or the `eval` function directly. # Constraints: - Input code is always read from the standard input. - Each input read should be attempted to be compiled immediately. If it cannot compile due to being incomplete, additional lines should be read until it forms a complete Python statement. # Example: ```python >>> my_var = 10 >>> if my_var > 5: ... print(\\"Value is greater than 5\\") ... Value is greater than 5 >>> x = (1 + ... 2 + ... 3) >>> print(x) 6 >>> exit() ``` Implement the `simple_repl` function based on the requirements described.","solution":"import sys import codeop def simple_repl(): This function implements a simple REPL using the codeop module. It should continuously read input from the user, compile it using codeop.compile_command, and execute it. It should handle incomplete statements and errors appropriately. You need to handle the following: - Read input from the user continuously. - Compile the input using `codeop.compile_command`. - Execute the compiled code if it is valid. - Handle invalid Python syntax by displaying an error message. - Continue reading lines from input if the statement is incomplete. - Provide a way to exit the REPL cleanly (e.g., if the user types \'exit()\'). buffer = \\"\\" while True: try: if buffer: prompt = \\"... \\" else: prompt = \\">>> \\" line = input(prompt) if line.strip() == \\"exit()\\": print(\\"Exiting REPL\\") break buffer += line + \\"n\\" compiled_code = codeop.compile_command(buffer, \\"<input>\\", \\"exec\\") if compiled_code: exec(compiled_code) buffer = \\"\\" except SyntaxError as e: print(f\\"SyntaxError: {e}\\") buffer = \\"\\" except Exception as e: print(f\\"Error: {e}\\") buffer = \\"\\""},{"question":"# Python Coding Assessment Question Objective: Design and implement a memory manager in Python that mimics the behavior of the Python 3.10 reference counting system. The manager should be capable of tracking the reference counts of objects and ensuring proper deallocation when their count reaches zero. Task: 1. Implement a `MemoryManager` class. 2. This class should have the following methods: - `add_reference(obj)`: Increase the reference count of the given object. - `remove_reference(obj)`: Decrease the reference count of the given object. - `get_reference_count(obj)`: Return the current reference count of the given object. 3. If the reference count of an object reaches zero, it should be properly deallocated (in this context, consider using a placeholder function `deallocate(obj)` that prints a message for demonstration purposes). Input/Output: 1. **Input**: The methods will receive objects and should internally manage the reference counts. 2. **Output**: The `get_reference_count` method should return an integer representing the current reference count of an object. Constraints: 1. All objects managed will be hashable. 2. The `MemoryManager` must handle cases where `None` is passed as an object and safely ignore such cases. Example: ```python class MemoryManager: def __init__(self): self.ref_counts = {} def add_reference(self, obj): if obj is not None: if obj in self.ref_counts: self.ref_counts[obj] += 1 else: self.ref_counts[obj] = 1 def remove_reference(self, obj): if obj is not None and obj in self.ref_counts: self.ref_counts[obj] -= 1 if self.ref_counts[obj] == 0: self.deallocate(obj) del self.ref_counts[obj] def get_reference_count(self, obj): return self.ref_counts.get(obj, 0) def deallocate(self, obj): print(f\\"Deallocating object: {obj}\\") # Example usage: manager = MemoryManager() obj = \\"example_object\\" manager.add_reference(obj) print(manager.get_reference_count(obj)) # Output: 1 manager.add_reference(obj) print(manager.get_reference_count(obj)) # Output: 2 manager.remove_reference(obj) print(manager.get_reference_count(obj)) # Output: 1 manager.remove_reference(obj) # Output: Deallocating object: example_object print(manager.get_reference_count(obj)) # Output: 0 ``` Notes: - You can add additional helper methods if required. - Ensure that your implementation is efficient in terms of time and space complexity.","solution":"class MemoryManager: def __init__(self): self.ref_counts = {} def add_reference(self, obj): if obj is not None: if obj in self.ref_counts: self.ref_counts[obj] += 1 else: self.ref_counts[obj] = 1 def remove_reference(self, obj): if obj is not None and obj in self.ref_counts: self.ref_counts[obj] -= 1 if self.ref_counts[obj] == 0: self.deallocate(obj) del self.ref_counts[obj] def get_reference_count(self, obj): return self.ref_counts.get(obj, 0) def deallocate(self, obj): print(f\\"Deallocating object: {obj}\\") # Example usage: # manager = MemoryManager() # obj = \\"example_object\\" # manager.add_reference(obj) # print(manager.get_reference_count(obj)) # Output: 1 # manager.add_reference(obj) # print(manager.get_reference_count(obj)) # Output: 2 # manager.remove_reference(obj) # print(manager.get_reference_count(obj)) # Output: 1 # manager.remove_reference(obj) # # Output: Deallocating object: example_object # print(manager.get_reference_count(obj)) # Output: 0"},{"question":"You are provided with a sorted list of student records where each record is a namedtuple containing the student\'s name, score, and age. Your task is to implement a function `insert_student` that uses the `bisect` and `insort` functions to insert new student records into the list while maintaining sort order by score. If two students have the same score, they should remain sorted by their original order of insertion. Function Signature: ```python from collections import namedtuple from typing import List Student = namedtuple(\'Student\', [\'name\', \'score\', \'age\']) def insert_student(students: List[Student], new_student: Student) -> List[Student]: pass ``` Input: - `students` (List[Student]): A list of namedtuples, where each namedtuple has three fields: `name` (string), `score` (integer), and `age` (integer). The list is sorted by `score` in ascending order. - `new_student` (Student): A new student record to be inserted into the `students` list. Output: - Returns a new list of students sorted by `score`, with the new student inserted at the appropriate position. Constraints: - The `score` for each student is unique. - The length of the input list can be up to 10^4. - All names are unique strings, all scores and ages are integers. Example: ```python Student = namedtuple(\'Student\', [\'name\', \'score\', \'age\']) students = [ Student(name=\'Alice\', score=89, age=20), Student(name=\'Bob\', score=95, age=22), Student(name=\'Carol\', score=92, age=21) ] new_student = Student(name=\'David\', score=90, age=19) print(insert_student(students, new_student)) # Output: [Student(name=\'Alice\', score=89, age=20), # Student(name=\'David\', score=90, age=19), # Student(name=\'Carol\', score=92, age=21), # Student(name=\'Bob\', score=95, age=22)] ``` # Notes: - You may use the functions `bisect_left`, `bisect_right`, and `insort_left` from the `bisect` module to achieve the solution. - The `key` parameter should be used to extract the score for comparison purposes during insertion.","solution":"from collections import namedtuple from typing import List import bisect Student = namedtuple(\'Student\', [\'name\', \'score\', \'age\']) def insert_student(students: List[Student], new_student: Student) -> List[Student]: Inserts a new student record into the sorted list of students while maintaining the order by score. # Create a list of scores to use for bisect_insertion scores = [student.score for student in students] # Find the position to insert the new student using bisect_right pos = bisect.bisect_right(scores, new_student.score) # Insert the new student at the found position students.insert(pos, new_student) return students"},{"question":"PyTorch Serialization and TorchScript **Objective**: Implement PyTorch model serialization, efficient tensor saving, and TorchScript serialization and deserialization. Part 1: Model Serialization **Task**: 1. Define a custom PyTorch module `MyCustomModule` with two linear layers and ReLU activation between them. 2. Create an instance of `MyCustomModule` and save its state_dict to a file `mymodule_state.pt`. 3. Load the state_dict from the file and restore it into a new instance of `MyCustomModule`. **Expected Input and Output**: * There is no input to this function. The function should execute the serialization and deserialization steps and print a success message upon correctly saving and loading the state_dict. **Constraints**: - Use PyTorch\'s `torch.save` and `torch.load` methods. Part 2: Efficient Tensor Saving **Task**: 1. Create a tensor of size 1000 with values from 1 to 1000. 2. Extract a smaller tensor consisting of the first 10 elements. 3. Save the smaller tensor efficiently to `small_tensor.pt` such that only the 10 elements are saved. 4. Load the tensor back from `small_tensor.pt` and print its storage size and tensor values. **Expected Input and Output**: * There is no input to this function. The function should print the storage size and values of the loaded tensor. **Constraints**: - Ensure that the saved file only includes the necessary elements to reduce file size. Part 3: TorchScript Serialization **Task**: 1. Script the `MyCustomModule` defined in Part 1 using TorchScript. 2. Save the scripted module to `my_scripted_module.pt`. 3. Load the scripted module from the file and verify its functionality with a sample tensor input by printing the model\'s output. **Expected Input and Output**: * There is no input to this function. The function should execute the script serialization and deserialization, and print the model’s output for a given input tensor. **Constraints**: - Use PyTorch\'s `torch.jit.script`, `torch.jit.save`, and `torch.jit.load` methods. Example Implementation: ```python import torch # Part 1: Model Serialization class MyCustomModule(torch.nn.Module): def __init__(self): super(MyCustomModule, self).__init__() self.l0 = torch.nn.Linear(4, 2) self.l1 = torch.nn.Linear(2, 1) def forward(self, x): out0 = self.l0(x) out0_relu = torch.nn.functional.relu(out0) return self.l1(out0_relu) def model_serialization(): model = MyCustomModule() torch.save(model.state_dict(), \'mymodule_state.pt\') new_model = MyCustomModule() state_dict = torch.load(\'mymodule_state.pt\') new_model.load_state_dict(state_dict) print(\\"Part 1: Model state_dict successfully saved and loaded.\\") # Part 2: Efficient Tensor Saving def efficient_tensor_saving(): large_tensor = torch.arange(1, 1001, dtype=torch.float32) # 1000 elements small_tensor = large_tensor[:10] # First 10 elements torch.save(small_tensor.clone(), \'small_tensor.pt\') loaded_tensor = torch.load(\'small_tensor.pt\') print(f\\"Part 2: Loaded tensor storage size: {loaded_tensor.storage().size()}\\") print(f\\"Part 2: Loaded tensor values: {loaded_tensor}\\") # Part 3: TorchScript Serialization def torchscript_serialization(): model = MyCustomModule() scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, \'my_scripted_module.pt\') loaded_scripted_model = torch.jit.load(\'my_scripted_module.pt\') sample_input = torch.randn(4) # Example input tensor output = loaded_scripted_model(sample_input) print(f\\"Part 3: Scripted model output for input {sample_input}: {output}\\") # Run all parts model_serialization() efficient_tensor_saving() torchscript_serialization() ``` **Instructions**: Implement the above three parts in a single script or Jupyter notebook. Your code should accurately demonstrate understanding and practical application of PyTorch serialization methods and TorchScript.","solution":"import torch # Part 1: Model Serialization class MyCustomModule(torch.nn.Module): def __init__(self): super(MyCustomModule, self).__init__() self.l0 = torch.nn.Linear(4, 2) self.l1 = torch.nn.Linear(2, 1) def forward(self, x): out0 = self.l0(x) out0_relu = torch.nn.functional.relu(out0) return self.l1(out0_relu) def model_serialization(): model = MyCustomModule() torch.save(model.state_dict(), \'mymodule_state.pt\') new_model = MyCustomModule() state_dict = torch.load(\'mymodule_state.pt\') new_model.load_state_dict(state_dict) print(\\"Part 1: Model state_dict successfully saved and loaded.\\") # Part 2: Efficient Tensor Saving def efficient_tensor_saving(): large_tensor = torch.arange(1, 1001, dtype=torch.float32) # 1000 elements small_tensor = large_tensor[:10] # First 10 elements torch.save(small_tensor.clone(), \'small_tensor.pt\') loaded_tensor = torch.load(\'small_tensor.pt\') print(f\\"Part 2: Loaded tensor storage size: {loaded_tensor.storage().size()}\\") print(f\\"Part 2: Loaded tensor values: {loaded_tensor}\\") # Part 3: TorchScript Serialization def torchscript_serialization(): model = MyCustomModule() scripted_model = torch.jit.script(model) torch.jit.save(scripted_model, \'my_scripted_module.pt\') loaded_scripted_model = torch.jit.load(\'my_scripted_module.pt\') sample_input = torch.randn(4) # Example input tensor output = loaded_scripted_model(sample_input) print(f\\"Part 3: Scripted model output for input {sample_input}: {output}\\") # Run all parts model_serialization() efficient_tensor_saving() torchscript_serialization()"},{"question":"Objective To design a function that robustly handles various pathname manipulations and helps determine path properties within a simulated file system environment. Problem Description You are developing a function `analyze_and_normalize_paths` which receives a list of paths and performs the following tasks: 1. Convert each path to an absolute path. 2. Determine whether each path is a directory or file and store this information. 3. Check if each path exists in the current file system. 4. For each existing path, determine and store the size of the file or the directory. 5. Return a dictionary with the following structure: - Key: Original path (as provided in the input list). - Value: A dictionary with the following keys: - `absolute_path`: The absolute version of the path. - `is_directory`: Boolean indicating whether the path is a directory. - `exists`: Boolean indicating if the path exists. - `size_bytes`: Size of the file or directory in bytes (only include this key if `exists` is `True`). Input - `paths`: A list of strings, where each string is a pathname. Output - A dictionary with the described structure. Constraints - Consider the possible exceptions that might arise if a path does not conform to expected norms, e.g., permission issues or non-existent paths. - The function should handle at least 10,000 paths efficiently. Example ```python def analyze_and_normalize_paths(paths): import os.path result = {} for path in paths: path_info = {} abs_path = os.path.abspath(path) path_info[\'absolute_path\'] = abs_path path_info[\'is_directory\'] = os.path.isdir(abs_path) path_info[\'exists\'] = os.path.exists(abs_path) if path_info[\'exists\']: path_info[\'size_bytes\'] = os.path.getsize(abs_path) result[path] = path_info return result # Example usage: paths_list = [\'/home/user/docs\', \'./test.py\', \'/non/existent/path\'] result = analyze_and_normalize_paths(paths_list) print(result) # Expected Output: # { # \'/home/user/docs\': {\'absolute_path\': \'/home/user/docs\', \'is_directory\': True, \'exists\': True, \'size_bytes\': 4096}, # \'./test.py\': {\'absolute_path\': \'/current/working/directory/test.py\', \'is_directory\': False, \'exists\': True, \'size_bytes\': 1024}, # \'/non/existent/path\': {\'absolute_path\': \'/non/existent/path\', \'is_directory\': False, \'exists\': False} # } ``` Notes - Paths are specified based on the operating system on which this function is running. - The example outputs are indicative and might differ based on the actual file system state. Ensure your function handles such variations correctly.","solution":"import os def analyze_and_normalize_paths(paths): result = {} for path in paths: path_info = {} abs_path = os.path.abspath(path) path_info[\'absolute_path\'] = abs_path path_info[\'is_directory\'] = os.path.isdir(abs_path) path_info[\'exists\'] = os.path.exists(abs_path) if path_info[\'exists\']: if path_info[\'is_directory\']: # Calculate the total size of the directory total_size = 0 for dirpath, dirnames, filenames in os.walk(abs_path): for f in filenames: fp = os.path.join(dirpath, f) total_size += os.path.getsize(fp) path_info[\'size_bytes\'] = total_size else: path_info[\'size_bytes\'] = os.path.getsize(abs_path) result[path] = path_info return result"},{"question":"**Question: Cache Implementation using `WeakKeyDictionary`** In this task, you are required to implement a simple cache system using the `weakref.WeakKeyDictionary` class from the `weakref` module. The cache should be designed to store and retrieve values based on unique object keys. If an object key is garbage collected, its associated cache entry should be automatically purged. # Implementation Requirements: 1. Implement a class `ObjectCache` with the following methods: - `__init__(self)`: Initializes an empty `WeakKeyDictionary`. - `add_to_cache(self, key, value)`: Adds a key-value pair to the cache. - `get_from_cache(self, key)`: Retrieves the value associated with the given key from the cache. If the key does not exist in the cache, return `None`. - `cache_size(self)`: Returns the number of items currently in the cache. # Constraints: - The keys used in the cache are expected to be custom objects that support weak references. - If the key object is collected, the corresponding entry should be automatically removed from the cache. # Example Usage: ```python import weakref class CustomKey: def __init__(self, id): self.id = id def __repr__(self): return f\\"CustomKey({self.id})\\" cache = ObjectCache() key1 = CustomKey(1) key2 = CustomKey(2) cache.add_to_cache(key1, \\"value1\\") cache.add_to_cache(key2, \\"value2\\") print(cache.get_from_cache(key1)) # Output: \\"value1\\" print(cache.get_from_cache(key2)) # Output: \\"value2\\" print(cache.cache_size()) # Output: 2 # Delete key1 and force garbage collection del key1 import gc gc.collect() print(cache.get_from_cache(CustomKey(1))) # Output: None print(cache.cache_size()) # Output: 1 ``` # Notes: - You should use the provided `WeakKeyDictionary` class for the cache implementation. - Ensure your code handles cases where the key object is deleted, and the associated cache entry is purged. Implement the `ObjectCache` class and the methods outlined above.","solution":"import weakref class ObjectCache: def __init__(self): Initializes an empty cache using a WeakKeyDictionary. self.cache = weakref.WeakKeyDictionary() def add_to_cache(self, key, value): Adds a key-value pair to the cache. self.cache[key] = value def get_from_cache(self, key): Retrieves the value associated with the given key from the cache. If the key does not exist in the cache, returns None. return self.cache.get(key, None) def cache_size(self): Returns the number of items currently in the cache. return len(self.cache)"},{"question":"# Question: Enhanced Plotting for Precision-Recall Curve with Multiple Axes You are tasked with building a custom display object in scikit-learn for the Precision-Recall curve, capable of handling multiple axes. This display should allow visualizing Precision-Recall curves for various classifiers on a shared grid, facilitating comparison. Objectives: 1. Create a `PrecisionRecallCurveDisplay` class. 2. Implement `from_estimator` and `from_predictions` class methods for initialization. 3. Implement the `plot` method to render the Precision-Recall curve, supporting multiple axes visualization. Class Specifications: - `__init__(self, precision, recall, average_precision, estimator_name)`: Initialize with precision, recall, average precision values, and the estimator\'s name. - `from_estimator(cls, estimator, X, y)`: Compute predictions, call `from_predictions`. - `from_predictions(cls, y_true, y_pred, estimator_name)`: Compute precision and recall values, initialize the display object. - `plot(self, ax=None, name=None, **kwargs)`: Render the plot. If `ax` is not provided, create a single-axis grid. If a single axis is provided, split it using `GridSpecFromSubplotSpec`. Input: - `from_estimator`: Estimator object (`estimator`), feature array (`X`), true labels (`y`). - `from_predictions`: True labels (`y_true`), predicted probabilities (`y_pred`), estimator name (`estimator_name`). - `plot`: Matplotlib Axes object (`ax`), optional plot name (`name`). Output: - Precision-Recall curve rendered on the designated axes. - Matplotlib artists stored as attributes for customization. Constraints: - Use `scikit-learn` and `matplotlib` only. - Ensure compatibility with multiple classifiers. Performance Requirements: - Efficient plotting capable of handling large datasets and multiple classifiers without significant lag. # Example Usage: ```python import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import PrecisionRecallDisplay # Sample data X, y = make_classification(random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) # Estimator clf = LogisticRegression().fit(X_train, y_train) y_pred = clf.predict_proba(X_test)[:, 1] # Display Precision-Recall Curve fig, ax = plt.subplots() PrecisionRecallCurveDisplay.from_estimator(clf, X_test, y_test).plot(ax=ax) plt.show() ``` Implement the `PrecisionRecallCurveDisplay` class as described, ensuring it meets the provided specifications.","solution":"import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score class PrecisionRecallCurveDisplay: def __init__(self, precision, recall, average_precision, estimator_name): self.precision = precision self.recall = recall self.average_precision = average_precision self.estimator_name = estimator_name self.line_ = None @classmethod def from_estimator(cls, estimator, X, y, estimator_name=None): if estimator_name is None: estimator_name = type(estimator).__name__ y_pred = estimator.predict_proba(X)[:, 1] return cls.from_predictions(y, y_pred, estimator_name) @classmethod def from_predictions(cls, y_true, y_pred, estimator_name): precision, recall, _ = precision_recall_curve(y_true, y_pred) average_precision = average_precision_score(y_true, y_pred) return cls(precision, recall, average_precision, estimator_name) def plot(self, ax=None, name=None, **kwargs): if ax is None: fig, ax = plt.subplots() name = name if name is not None else self.estimator_name self.line_, = ax.plot(self.recall, self.precision, label=f\'{name} (AP={self.average_precision:0.2f})\', **kwargs) ax.set_xlabel(\\"Recall\\") ax.set_ylabel(\\"Precision\\") ax.set_title(\\"Precision-Recall curve\\") ax.legend(loc=\\"best\\") return self"},{"question":"You are tasked with developing a simple chat application using asyncio\'s low-level transport and protocol APIs. This chat application will have a server and multiple clients. The server should broadcast any message it receives to all connected clients. # Requirements 1. **ChatServerProtocol Class**: Implement a class `ChatServerProtocol` inheriting from `asyncio.Protocol`: - `connection_made(self, transport)`: Called when a client connects. Store the transport and add it to a list of active transports. - `data_received(self, data)`: Called when data is received from a client. Broadcast this data to all connected clients except the sender. - `connection_lost(self, exc)`: Called when a client disconnects. Remove its transport from the list of active transports. 2. **ChatServer Class**: Implement a class `ChatServer`: - `__init__(self, loop)`: Initialize the server with an event loop. - `start(self, host, port)`: Create and start the server using `loop.create_server()`. Use `ChatServerProtocol` for connections. - `stop(self)`: Stop the server and close all transports. 3. **ChatClientProtocol Class**: Implement a class `ChatClientProtocol` inheriting from `asyncio.Protocol`: - `__init__(self, message, on_con_lost)`: Initialize with a message and a Future object. - `connection_made(self, transport)`: Send the initial message when the connection is made. Store the transport. - `data_received(self, data)`: Print received messages. - `connection_lost(self, exc)`: Set the Future result to signal the connection is lost. 4. **ChatClient Class**: Implement a class `ChatClient`: - `__init__(self, loop, message)`: Initialize the client with an event loop and a message. - `start(self, host, port)`: Connect to the server using `loop.create_connection()` and the `ChatClientProtocol`. # Constraints - Use Python 3.10 or above. - Ensure proper exception handling and clean-up of connections. # Example Usage ```python import asyncio # Define the classes ChatServer, ChatServerProtocol, ChatClient, ChatClientProtocol here async def main(): loop = asyncio.get_running_loop() # Start the server server = ChatServer(loop) await server.start(\'127.0.0.1\', 8888) # Start two clients client1 = ChatClient(loop, \\"Hello from client 1\\") client2 = ChatClient(loop, \\"Hello from client 2\\") await client1.start(\'127.0.0.1\', 8888) await client2.start(\'127.0.0.1\', 8888) await asyncio.sleep(10) # Let them chat for a while await server.stop() asyncio.run(main()) ``` # Expected Output When running the example, the server should start and handle two client connections, with messages sent by each client broadcasted and received by others.","solution":"import asyncio class ChatServerProtocol(asyncio.Protocol): def __init__(self): self.transport = None def connection_made(self, transport): self.transport = transport self.transport_server.register_client(self) print(f\\"Connection from {self.transport.get_extra_info(\'peername\')}\\") def data_received(self, data): message = data.decode() self.transport_server.broadcast_message(message, self) def connection_lost(self, exc): self.transport_server.unregister_client(self) print(f\\"The client {self.transport.get_extra_info(\'peername\')} disconnected\\") class ChatServer: def __init__(self, loop): self.loop = loop self.clients = [] def start(self, host, port): server_coro = self.loop.create_server( lambda: self._create_protocol(), host, port ) self.server = self.loop.run_until_complete(server_coro) print(f\'Serving on {self.server.sockets[0].getsockname()}\') def _create_protocol(self): protocol = ChatServerProtocol() protocol.transport_server = self return protocol def stop(self): for client in self.clients: client.transport.close() self.server.close() self.loop.run_until_complete(self.server.wait_closed()) def register_client(self, client): self.clients.append(client) def unregister_client(self, client): self.clients.remove(client) def broadcast_message(self, message, sender): for client in self.clients: if client != sender: client.transport.write(message.encode()) class ChatClientProtocol(asyncio.Protocol): def __init__(self, message, on_con_lost): self.message = message self.on_con_lost = on_con_lost def connection_made(self, transport): self.transport = transport self.transport.write(self.message.encode()) def data_received(self, data): print(f\'Received: {data.decode()}\') def connection_lost(self, exc): self.on_con_lost.set_result(True) print(\\"Connection closed\\") class ChatClient: def __init__(self, loop, message): self.loop = loop self.message = message def start(self, host, port): on_con_lost = self.loop.create_future() transport, protocol = self.loop.run_until_complete( self.loop.create_connection( lambda: ChatClientProtocol(self.message, on_con_lost), host, port) ) return on_con_lost"},{"question":"**Question: Implement a Function Using Multiple Itertools Functions** You are provided with a list of integers. Your task is to perform the following operations on this list using itertools functions: 1. Generate all possible pairs of elements from the list (order doesn\'t matter). 2. For each pair, compute the product of the two elements. 3. Remove products that are less than a given threshold. 4. Return the products in a sorted list. **Function Signature:** ```python def filter_product_pairs(numbers: List[int], threshold: int) -> List[int]: pass ``` **Input:** - `numbers`: A list of integers. Example: `[1, 3, 5, 7]` - `threshold`: An integer which serves as the minimum value for the product to be considered. Example: `10` **Output:** - A list of integers corresponding to unique products that are greater than or equal to the `threshold`, sorted in ascending order. **Constraints:** - The input list will have at least two integers. - Input integers will be non-negative and within the range 0 to 100 inclusive. - The threshold will be a non-negative integer within the range 0 to 10000 inclusive. **Example:** ```python numbers = [1, 3, 5, 7] threshold = 10 Output: [15, 21, 35] ``` **Explanation:** - All pairs from the list: `(1, 3), (1, 5), (1, 7), (3, 5), (3, 7), (5, 7)` - Products: `3, 5, 7, 15, 21, 35` - Products >= 10: `15, 21, 35` - Sorted output: `[15, 21, 35]` You should use the `itertools` functions to achieve an efficient solution. Note: Your implementation should not use brute-force loops directly; instead, it should leverage the `itertools` module to iteratively and efficiently generate combinations, compute products, filter them and then finally sort the results.","solution":"from itertools import combinations from typing import List def filter_product_pairs(numbers: List[int], threshold: int) -> List[int]: This function generates all possible pairs of elements from the list, computes the product of the pairs, filters out products that are less than the given threshold, and returns the remaining products in a sorted list. # Generate all possible pairs using combinations pairs = combinations(numbers, 2) # Calculate the product of each pair and filter based on the threshold products = (x * y for x, y in pairs if x * y >= threshold) # Return the sorted list of products return sorted(products)"},{"question":"<|Analysis Begin|> The provided documentation focuses on Python\'s `socket` module, which is a low-level networking interface that allows Python programs to communicate over networks using the BSD socket interface. The module provides functions for creating socket objects, binding them to addresses, listening for connections, sending and receiving data, and various other socket options and utilities. Key points include: - Various socket families like `AF_INET` for IPv4, `AF_INET6` for IPv6, and others for different protocols and address formats. - Different socket types such as `SOCK_STREAM` for TCP and `SOCK_DGRAM` for UDP. - The use of exceptions and error handling specific to networking, such as `socket.error`, `socket.herror`, `socket.gaierror`, and `socket.timeout`. - Functions to perform common tasks such as creating connections, sending and receiving data, handling timeouts, and manipulating socket options. The complexity and variety of the `socket` module make it suitable for crafting a challenging coding assessment question. Students can be tested on: 1. Creating and connecting to sockets. 2. Handling different types of socket addresses. 3. Managing socket options and timeouts. 4. Sending and receiving data effectively and securely. <|Analysis End|> <|Question Begin|> **Assessment Question:** # Building a Chat Server and Client with Python\'s Socket Module You are tasked with building a simple chat application using Python\'s `socket` module. Your application will consist of two parts: a server that listens for incoming connections and handles multiple clients, and a client that connects to the server and sends/receives messages. Part 1: Chat Server 1. **Server Initialization and Configuration:** - Create a socket using `socket.AF_INET` and `socket.SOCK_STREAM`. - Bind the socket to an address and port, provided as function parameters. - Set the socket to listen for incoming connections. 2. **Client Handling:** - Accept incoming connections and handle multiple clients using threading. - For each client, continuously receive messages and broadcast them to all other connected clients. - Ensure that the server can handle client disconnections gracefully. 3. **Broadcast Messages:** - Implement a method to send incoming messages from one client to all other connected clients. Part 2: Chat Client 1. **Client Initialization and Configuration:** - Create a socket using `socket.AF_INET` and `socket.SOCK_STREAM`. - Connect the client to the server using the server\'s address and port, provided as function parameters. - Implement a way to send messages from the client to the server. 2. **Message Handling:** - Implement a method to continuously receive messages from the server and print them. - Use threading to handle sending and receiving messages concurrently. # Requirements: - Both the server and client must handle exceptions such as connection errors, timeouts, and disconnections gracefully. - The server should be able to handle multiple clients concurrently. - The client should not block the user from sending messages while waiting to receive messages. # Input and Output Formats: - The server and client should not require input/output formats beyond standard command-line arguments for host and port. # Constraints: - You cannot use third-party libraries for networking; only the `socket` and `threading` modules are allowed. - Ensure message lengths are handled appropriately to avoid partial messages. # Performance Requirements: - The server must handle at least 10 concurrent clients without significant performance degradation. # Example: ```python # Example server usage # python chat_server.py host port # Example client usage # python chat_client.py host port ``` # Implementation Implement the following functions: 1. `start_server(host: str, port: int) -> None` 2. `start_client(host: str, port: int) -> None` ```python import socket import threading def start_server(host: str, port: int) -> None: # TODO: Implement the server code here def start_client(host: str, port: int) -> None: # TODO: Implement the client code here if __name__ == \\"__main__\\": import sys if len(sys.argv) < 3: print(\\"Usage: python script.py host port\\") sys.exit(1) role = sys.argv[1] host = sys.argv[2] port = int(sys.argv[3]) if role == \'server\': start_server(host, port) elif role == \'client\': start_client(host, port) else: print(\\"Unknown role: should be \'server\' or \'client\'\\") ``` Evaluation Criteria: - Correct and efficient implementation of socket creation, binding, listening, and connections. - Ability to handle multiple client connections concurrently on the server. - Robust error handling and graceful disconnection management. - Smooth and concurrent message sending and receiving in the client.","solution":"import socket import threading clients = [] def broadcast(message, client_socket): for client in clients: if client != client_socket: try: client.send(message) except: remove(client) def handle_client(client_socket): while True: try: message = client_socket.recv(1024) if not message: break broadcast(message, client_socket) except: remove(client_socket) break client_socket.close() def remove(client_socket): if client_socket in clients: clients.remove(client_socket) def start_server(host: str, port: int) -> None: server = socket.socket(socket.AF_INET, socket.SOCK_STREAM) server.bind((host, port)) server.listen(5) print(\\"Server started on {}:{}\\".format(host, port)) while True: client_socket, client_address = server.accept() clients.append(client_socket) print(\\"Accepted connection from {}:{}\\".format(client_address[0], client_address[1])) thread = threading.Thread(target=handle_client, args=(client_socket,)) thread.start() def start_client(host: str, port: int) -> None: client = socket.socket(socket.AF_INET, socket.SOCK_STREAM) client.connect((host, port)) def receive_messages(): while True: try: message = client.recv(1024).decode(\'utf-8\') if not message: break print(message) except: break client.close() thread = threading.Thread(target=receive_messages) thread.start() while True: try: message = input() client.send(message.encode(\'utf-8\')) except KeyboardInterrupt: client.close() break if __name__ == \\"__main__\\": import sys if len(sys.argv) < 4: print(\\"Usage: python script.py [server|client] host port\\") sys.exit(1) role = sys.argv[1] host = sys.argv[2] port = int(sys.argv[3]) if role == \'server\': start_server(host, port) elif role == \'client\': start_client(host, port) else: print(\\"Unknown role: should be \'server\' or \'client\'\\")"},{"question":"**Objective**: Demonstrate your understanding of the `sklearn.datasets` module in scikit-learn by working with different types of datasets. # Question You are required to load, manipulate, and analyze datasets using the `sklearn.datasets` module. Specifically, you need to perform the following tasks: 1. **Load and Explore a Toy Dataset**: - Load the Iris dataset using the appropriate loader. - Extract the data and target from the dataset. - Print the first 5 rows of the data and corresponding target values. 2. **Fetch and Summarize a Real-World Dataset**: - Fetch the `20newsgroups` dataset (only train set). - Summarize the number of samples, categories, and provide a sample of the data. 3. **Generate and Visualize Synthetic Data**: - Generate a synthetic dataset using the `make_classification` function with 100 samples, 5 features, and 2 informative features. - Plot the generated dataset using a scatter plot for the first two features. # Expected Input and Output **Task 1**: - **Input**: None - **Output**: - Structured printing of the first 5 rows of the Iris dataset and their corresponding target values. **Task 2**: - **Input**: None - **Output**: - Number of samples in the `20newsgroups` train set. - Number of categories. - A sample data point (print the first sample). **Task 3**: - **Input**: None - **Output**: - Scatter plot displaying the first two features of the synthetic dataset colored by their class. # Constraints / Limitations - Ensure that the Iris and `20newsgroups` datasets are loaded correctly using the sklearn provided utilities. - Use appropriate libraries (e.g., matplotlib) for plotting. - Structure the code clearly to separate the three tasks. # Performance Requirements - The solution should handle the dataset loading and manipulation efficiently. - The synthetic data generation and plotting should not exceed a reasonable computational time (e.g., should complete within seconds for the given parameters). ```python # Sample Skeleton Code from sklearn.datasets import load_iris, fetch_20newsgroups, make_classification import matplotlib.pyplot as plt def main(): # Task 1: Load and Explore Toy Dataset iris = load_iris() data_iris, target_iris = iris[\'data\'], iris[\'target\'] # Print first 5 rows print(\\"Iris Dataset (first 5 rows):\\") print(data_iris[:5]) print(\\"Target values (first 5):\\") print(target_iris[:5]) # Task 2: Fetch and Summarize Real-World Dataset newsgroups_train = fetch_20newsgroups(subset=\'train\') print(\\"n20 Newsgroups Dataset Summary:\\") print(f\\"Number of samples: {len(newsgroups_train.data)}\\") print(f\\"Number of categories: {len(newsgroups_train.target_names)}\\") print(f\\"First sample data: {newsgroups_train.data[0][:500]}...\\") # Print only the first 500 characters # Task 3: Generate and Visualize Synthetic Data X, y = make_classification(n_samples=100, n_features=5, n_informative=2, n_redundant=0, random_state=42) plt.scatter(X[:, 0], X[:, 1], c=y) plt.title(\'Synthetic Data Scatter Plot\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() if __name__ == \\"__main__\\": main() ```","solution":"from sklearn.datasets import load_iris, fetch_20newsgroups, make_classification import matplotlib.pyplot as plt def load_and_explore_iris(): Load and explore the Iris dataset. Prints first 5 rows of data and their corresponding target values. iris = load_iris() data_iris, target_iris = iris[\'data\'], iris[\'target\'] print(\\"Iris Dataset (first 5 rows):\\") print(data_iris[:5]) print(\\"Target values (first 5):\\") print(target_iris[:5]) return data_iris[:5], target_iris[:5] def fetch_and_summarize_newsgroups(): Fetch the 20newsgroups dataset (train set) and print summary. Prints the number of samples, number of categories, and a sample of the data. newsgroups_train = fetch_20newsgroups(subset=\'train\') num_samples = len(newsgroups_train.data) num_categories = len(newsgroups_train.target_names) sample_data = newsgroups_train.data[0] print(\\"n20 Newsgroups Dataset Summary:\\") print(f\\"Number of samples: {num_samples}\\") print(f\\"Number of categories: {num_categories}\\") print(f\\"First sample data: {sample_data[:500]}...\\") # Print only the first 500 characters return num_samples, num_categories, sample_data def generate_and_plot_synthetic_data(): Generate a synthetic dataset using make_classification and plot the first two features. X, y = make_classification(n_samples=100, n_features=5, n_informative=2, n_redundant=0, random_state=42) plt.scatter(X[:, 0], X[:, 1], c=y) plt.title(\'Synthetic Data Scatter Plot\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.show() return X, y # Main function to execute all tasks def main(): load_and_explore_iris() fetch_and_summarize_newsgroups() generate_and_plot_synthetic_data() if __name__ == \\"__main__\\": main()"},{"question":"Objective Your task is to write a Python function that trains a Scikit-learn model with different levels of parallelism and compares their performance. This will help us understand how parallelism affects the training time of machine learning models in Scikit-learn. Details 1. **Function Signature:** ```python def compare_parallelism_performance(X, y, model, n_jobs_list): Compares the performance of a scikit-learn model with different parallelism levels. Parameters: - X : array-like of shape (n_samples, n_features) Training data. - y : array-like of shape (n_samples,) Target values. - model : estimator object A scikit-learn estimator. - n_jobs_list : list of int A list of different values for the `n_jobs` parameter to test. Returns: - performance_dict : dict A dictionary where keys are the values of `n_jobs` and values are the training times. pass ``` 2. **Input:** - `X`: Training data with shape `(n_samples, n_features)`. - `y`: Target values with shape `(n_samples,)`. - `model`: A scikit-learn estimator (e.g., `RandomForestClassifier`, `GradientBoostingRegressor`). - `n_jobs_list`: A list of different values for the `n_jobs` parameter to test, for example `[1, 2, 4, -1]`. 3. **Output:** - `performance_dict`: A dictionary where keys are the values of `n_jobs`, and the values are the training times in seconds. 4. **Requirements:** - You need to fit the model on the training data `X` and target `y` using each value of `n_jobs` provided in `n_jobs_list`. - Measure the time taken to train the model for each `n_jobs` value. - Return a dictionary with `n_jobs` values as keys and the corresponding training times as values. 5. **Constraints:** - The implementation should handle reasonably sized datasets (e.g., tens of thousands of samples, hundreds of features). - Ensure not to oversubscribe CPUs; experiment with different `n_jobs` values to see the impact. - Use appropriate parallelism strategies to optimize the computation. Example Usage: ```python from sklearn.datasets import load_digits from sklearn.ensemble import RandomForestClassifier import time def compare_parallelism_performance(X, y, model, n_jobs_list): performance_dict = {} for n_jobs in n_jobs_list: model.set_params(n_jobs=n_jobs) start_time = time.time() model.fit(X, y) end_time = time.time() performance_dict[n_jobs] = end_time - start_time return performance_dict # Example data X, y = load_digits(return_X_y=True) model = RandomForestClassifier() n_jobs_list = [1, 2, 4, -1] performance = compare_parallelism_performance(X, y, model, n_jobs_list) print(performance) ``` This question tests your understanding of: - Implementing models using scikit-learn. - Understanding and utilizing different parallelism strategies. - Measuring and comparing model training performance with different configurations.","solution":"from sklearn.base import clone import time def compare_parallelism_performance(X, y, model, n_jobs_list): Compares the performance of a scikit-learn model with different parallelism levels. Parameters: - X : array-like of shape (n_samples, n_features) Training data. - y : array-like of shape (n_samples,) Target values. - model : estimator object A scikit-learn estimator. - n_jobs_list : list of int A list of different values for the `n_jobs` parameter to test. Returns: - performance_dict : dict A dictionary where keys are the values of `n_jobs` and values are the training times. performance_dict = {} for n_jobs in n_jobs_list: # Clone the model to ensure a fresh start for each n_jobs value model_clone = clone(model) model_clone.set_params(n_jobs=n_jobs) start_time = time.time() model_clone.fit(X, y) end_time = time.time() performance_dict[n_jobs] = end_time - start_time return performance_dict"},{"question":"**Problem: HTML Entity Conversion Utility** You are tasked with implementing a function to convert a string containing HTML named character references into its corresponding printable string. Your function should handle both converting named character references to their Unicode characters and converting back from Unicode characters to named references. # Function 1: `convert_to_unicode` **Input**: - `html_string`: A string containing HTML named character references. (e.g., \\"Hello &amp; World\\") **Output**: - A string with the HTML named character references replaced by their corresponding Unicode characters. (e.g., \\"Hello & World\\") # Function 2: `convert_to_named_references` **Input**: - `unicode_string`: A string containing Unicode characters. (e.g., \\"Hello & World\\") **Output**: - A string where certain Unicode characters are replaced by their corresponding HTML named character references. (e.g., \\"Hello &amp; World\\") **Constraints**: - You should use the dictionaries from the `html.entities` module for conversions. - Handle cases where the reference may or may not include the trailing semicolon. - Only convert HTML5 standard names when converting from Unicode to named references. # Example ```python def convert_to_unicode(html_string): # Your implementation here pass def convert_to_named_references(unicode_string): # Your implementation here pass # Example usage html_input = \\"Hello &amp; World &gt; Universe\\" unicode_output = convert_to_unicode(html_input) print(unicode_output) # Output: \\"Hello & World > Universe\\" unicode_input = \\"Hello & World > Universe\\" html_output = convert_to_named_references(unicode_input) print(html_output) # Output: \\"Hello &amp; World &gt; Universe\\" ``` Create these two functions according to the requirements and test them against the provided examples.","solution":"import html from html.entities import name2codepoint, codepoint2name def convert_to_unicode(html_string): Converts a string containing HTML named character references to its corresponding Unicode characters. return html.unescape(html_string) def convert_to_named_references(unicode_string): Converts a string containing Unicode characters to its corresponding HTML named character references. result = [] for char in unicode_string: char_code = ord(char) if char_code in codepoint2name: result.append(f\\"&{codepoint2name[char_code]};\\") else: result.append(char) return \'\'.join(result)"},{"question":"**Question: Customize a Multi-Faceted Seaborn Plot** You are provided with a dataset containing information about various species of penguins. Your task is to create a multi-faceted seaborn plot using the seaborn `objects` module. The plot should display pairwise relationships between the features `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`, with each species in a separate facet. # Input: 1. A CSV file named `penguins.csv` that contains the dataset with columns: `species`, `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. # Expected Output: A multi-faceted scatter plot where: - Each facet represents a different species of penguin. - Each plot demonstrates the relationship between all possible pairs of the features `bill_length_mm`, `bill_depth_mm`, `flipper_length_mm`, and `body_mass_g`. - The overall figure size should be 12x12 inches. - The layout engine should be set to \\"constrained\\". - The extent of the plot should be set to [0, 0, 0.9, 1]. # Constraints: - Use seaborn `objects` module functions such as `facet`, `layout`, and `extent`. # Detailed Requirements: 1. First, load the dataset from `penguins.csv`. 2. Initialize a seaborn `Plot` object. 3. Use the `facet` method to create separate facets for each species and display all pairwise relationships between the specified features. 4. Customize the layout size, engine, and extent as per the specifications. Example Code Structure: ```python import seaborn.objects as so import pandas as pd # Load the dataset penguins = pd.read_csv(\'penguins.csv\') # Create the plot object p = so.Plot(penguins) # Facet the plot for each species and plot the pairwise relationships p.facet([\\"species\\"], [\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"]) # Adjust the layout size, engine, and extent p.layout(size=(12, 12), engine=\\"constrained\\", extent=[0, 0, 0.9, 1]) # Display the plot p.show() ``` # Notes: 1. Ensure that the plot is properly displayed and all facets are correctly populated with scatter plots of the pairwise relationships. 2. Experiment with the notebook or GUI display to verify the extent and layout settings.","solution":"import seaborn.objects as so import pandas as pd def create_penguin_plot(csv_file): # Load the dataset penguins = pd.read_csv(csv_file) # Create the plot object p = so.Plot(penguins) # Facet the plot for each species and plot the pairwise relationships p.facet([\\"species\\"], [\\"bill_length_mm\\", \\"bill_depth_mm\\", \\"flipper_length_mm\\", \\"body_mass_g\\"]) # Adjust the layout size, engine, and extent p.layout(size=(12, 12), engine=\\"constrained\\", extent=[0, 0, 0.9, 1]) # Display the plot return p.show()"},{"question":"Problem Statement You are tasked with creating a command-line utility using the `argparse` module in Python. This utility will be called `dbtool` and it will manage a simple key-value store in a JSON file. The utility should support the following sub-commands: 1. **add**: Adds a new key-value pair to the store. 2. **remove**: Removes a key (and its value) from the store. 3. **update**: Updates the value for an existing key. 4. **get**: Retrieves the value for a given key. The JSON file path should be provided as an argument to `dbtool`, and all sub-commands should support this argument. The utility should also include comprehensive help messages for each sub-command. Requirements 1. Implement the `dbtool` utility. 2. Define the necessary sub-commands (add, remove, update, get) using `argparse`. 3. Handle the JSON file read/write operations. 4. Provide meaningful help messages for each command. 5. Ensure proper error handling and input validation. # Function Signature ```python def dbtool(): pass ``` Example Usage ```sh # Adding a key-value pair python dbtool.py --file store.json add --key name --value Alice # Getting the value of a key python dbtool.py --file store.json get --key name Alice # Updating the value of an existing key python dbtool.py --file store.json update --key name --value Bob # Removing a key-value pair python dbtool.py --file store.json remove --key name ``` Constraints - The JSON file path is mandatory for all sub-commands. - Keys must be unique strings. - Values can be any string. - Handle cases where the JSON file does not exist (e.g., create a new file if needed). - Implement proper error messages for invalid operations (e.g., trying to remove a non-existent key). Additional Info You may use the following template to get started: ```python import argparse import json import os def add_entry(file, key, value): pass def remove_entry(file, key): pass def update_entry(file, key, value): pass def get_entry(file, key): pass def dbtool(): parser = argparse.ArgumentParser(description=\'Key-Value Store Management Tool\') parser.add_argument(\'--file\', required=True, help=\'Path to the JSON file that stores the key-value pairs\') subparsers = parser.add_subparsers(dest=\'command\', help=\'Sub-commands\') # Add sub-command parser_add = subparsers.add_parser(\'add\', help=\'Add a new key-value pair to the store\') parser_add.add_argument(\'--key\', required=True, help=\'The key to add\') parser_add.add_argument(\'--value\', required=True, help=\'The value to add\') # Remove sub-command parser_remove = subparsers.add_parser(\'remove\', help=\'Remove an existing key from the store\') parser_remove.add_argument(\'--key\', required=True, help=\'The key to remove\') # Update sub-command parser_update = subparsers.add_parser(\'update\', help=\'Update the value for an existing key in the store\') parser_update.add_argument(\'--key\', required=True, help=\'The key to update\') parser_update.add_argument(\'--value\', required=True, help=\'The new value\') # Get sub-command parser_get = subparsers.add_parser(\'get\', help=\'Retrieve the value of a key from the store\') parser_get.add_argument(\'--key\', required=True, help=\'The key to retrieve\') args = parser.parse_args() if args.command == \'add\': add_entry(args.file, args.key, args.value) elif args.command == \'remove\': remove_entry(args.file, args.key) elif args.command == \'update\': update_entry(args.file, args.key, args.value) elif args.command == \'get\': get_entry(args.file, args.key) else: parser.print_help() if __name__ == \'__main__\': dbtool() ``` Notes - Implement the `add_entry`, `remove_entry`, `update_entry`, and `get_entry` functions to handle the respective operations on the JSON file.","solution":"import argparse import json import os def load_store(file): if not os.path.exists(file): return {} with open(file, \'r\') as f: return json.load(f) def save_store(file, store): with open(file, \'w\') as f: json.dump(store, f, indent=4) def add_entry(file, key, value): store = load_store(file) if key in store: print(f\\"Key \'{key}\' already exists.\\") return store[key] = value save_store(file, store) print(f\\"Key \'{key}\' added with value \'{value}\'.\\") def remove_entry(file, key): store = load_store(file) if key not in store: print(f\\"Key \'{key}\' does not exist.\\") return del store[key] save_store(file, store) print(f\\"Key \'{key}\' has been removed.\\") def update_entry(file, key, value): store = load_store(file) if key not in store: print(f\\"Key \'{key}\' does not exist.\\") return store[key] = value save_store(file, store) print(f\\"Key \'{key}\' updated with value \'{value}\'.\\") def get_entry(file, key): store = load_store(file) if key not in store: print(f\\"Key \'{key}\' does not exist.\\") return print(store[key]) def dbtool(): parser = argparse.ArgumentParser(description=\'Key-Value Store Management Tool\') parser.add_argument(\'--file\', required=True, help=\'Path to the JSON file that stores the key-value pairs\') subparsers = parser.add_subparsers(dest=\'command\', help=\'Sub-commands\') # Add sub-command parser_add = subparsers.add_parser(\'add\', help=\'Add a new key-value pair to the store\') parser_add.add_argument(\'--key\', required=True, help=\'The key to add\') parser_add.add_argument(\'--value\', required=True, help=\'The value to add\') # Remove sub-command parser_remove = subparsers.add_parser(\'remove\', help=\'Remove an existing key from the store\') parser_remove.add_argument(\'--key\', required=True, help=\'The key to remove\') # Update sub-command parser_update = subparsers.add_parser(\'update\', help=\'Update the value for an existing key in the store\') parser_update.add_argument(\'--key\', required=True, help=\'The key to update\') parser_update.add_argument(\'--value\', required=True, help=\'The new value\') # Get sub-command parser_get = subparsers.add_parser(\'get\', help=\'Retrieve the value of a key from the store\') parser_get.add_argument(\'--key\', required=True, help=\'The key to retrieve\') args = parser.parse_args() if args.command == \'add\': add_entry(args.file, args.key, args.value) elif args.command == \'remove\': remove_entry(args.file, args.key) elif args.command == \'update\': update_entry(args.file, args.key, args.value) elif args.command == \'get\': get_entry(args.file, args.key) else: parser.print_help() if __name__ == \'__main__\': dbtool()"},{"question":"# Naive Bayes Classifiers Comparison In this task, you are required to implement and compare different Naive Bayes classifiers available in scikit-learn: `GaussianNB`, `MultinomialNB`, `ComplementNB`, and `BernoulliNB`. Description You are provided with a dataset containing feature vectors and corresponding class labels. The dataset consists of features that can be treated as continuous (suitable for GaussianNB), word counts (suitable for MultinomialNB and ComplementNB), and binary values (suitable for BernoulliNB). Dataset - `X_continuous`: A 2D numpy array where each entry represents a continuous feature (for GaussianNB). - `X_multinomial`: A 2D numpy array where each entry represents a word count feature (for MultinomialNB and ComplementNB). - `X_binary`: A 2D numpy array where each entry represents a binary feature (for BernoulliNB). - `y`: A 1D numpy array of class labels corresponding to each feature vector in the datasets above. Requirements 1. Fit the following Naive Bayes classifiers to the datasets: - `GaussianNB` using `X_continuous`. - `MultinomialNB` using `X_multinomial`. - `ComplementNB` using `X_multinomial`. - `BernoulliNB` using `X_binary`. 2. Use each classifier to predict the class labels on a given test dataset of the same format as the training dataset provided. 3. Evaluate the performance of each classifier using accuracy. Input The input to your function will be four datasets and a test set: - `X_train_continuous`, `X_test_continuous`: Training and test sets for continuous features. - `X_train_multinomial`, `X_test_multinomial`: Training and test sets for multinomial features. - `X_train_binary`, `X_test_binary`: Training and test sets for binary features. - `y_train`, `y_test`: Corresponding labels for the training and test sets. Output A dictionary with the classifier names as keys and their corresponding accuracy scores as values. ```python def compare_naive_bayes(X_train_continuous, X_test_continuous, X_train_multinomial, X_test_multinomial, X_train_binary, X_test_binary, y_train, y_test): # Your implementation here # Example call: # result = compare_naive_bayes(X_train_continuous, X_test_continuous, X_train_multinomial, X_test_multinomial, X_train_binary, X_test_binary, y_train, y_test) # print(result) ``` Constraints - Use scikit-learn\'s built-in classifiers for GaussianNB, MultinomialNB, ComplementNB, and BernoulliNB. - Ensure that the necessary imports from scikit-learn are included at the top of your implementation. - Assume the input arrays are properly preprocessed (e.g., features are scaled for GaussianNB, word counts are non-negative integers for MultinomialNB and ComplementNB, etc.). Note Consider using functions like `train_test_split` for splitting the datasets and `accuracy_score` for evaluating the predictions.","solution":"from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB from sklearn.metrics import accuracy_score def compare_naive_bayes(X_train_continuous, X_test_continuous, X_train_multinomial, X_test_multinomial, X_train_binary, X_test_binary, y_train, y_test): results = {} # GaussianNB gnb = GaussianNB() gnb.fit(X_train_continuous, y_train) y_pred_continuous = gnb.predict(X_test_continuous) results[\'GaussianNB\'] = accuracy_score(y_test, y_pred_continuous) # MultinomialNB mnb = MultinomialNB() mnb.fit(X_train_multinomial, y_train) y_pred_multinomial = mnb.predict(X_test_multinomial) results[\'MultinomialNB\'] = accuracy_score(y_test, y_pred_multinomial) # ComplementNB cnb = ComplementNB() cnb.fit(X_train_multinomial, y_train) y_pred_complement = cnb.predict(X_test_multinomial) results[\'ComplementNB\'] = accuracy_score(y_test, y_pred_complement) # BernoulliNB bnb = BernoulliNB() bnb.fit(X_train_binary, y_train) y_pred_binary = bnb.predict(X_test_binary) results[\'BernoulliNB\'] = accuracy_score(y_test, y_pred_binary) return results"},{"question":"Feature Importance and Multicollinearity Objective Your task is to demonstrate your understanding of permutation feature importance using scikit-learn by assessing feature importances in a given dataset. Specifically, you will: 1. Train a machine learning model. 2. Compute the permutation feature importance. 3. Handle multicollinearity in the context of permutation feature importance. Description You are given the Diabetes dataset, which contains 10 features and a target variable representing a quantitative measure of disease progression one year after baseline. Your tasks are as follows: 1. **Data Preparation and Model Training**: - Load the Diabetes dataset using `sklearn.datasets.load_diabetes`. - Split the dataset into training and validation sets. - Train a Ridge regression model on the training set. - Evaluate the model performance using the R² score on the validation set. 2. **Compute Permutation Feature Importance**: - Using the trained model, compute the permutation feature importance on the validation set. - Identify and print the top 3 features in terms of importance and their corresponding importance values with standard deviations. 3. **Handling Multicollinearity**: - Identify pairs of highly correlated features (correlation coefficient greater than 0.8). - Retrain your model keeping only one feature from each pair of highly correlated features based on the initial permutation importance values. - Compute and print the permutation feature importance for the new model configuration and report the top 3 features with their importance values. # Constraints - Use a random state of 0 where applicable for reproducibility. - Use `n_repeats=30` when computing permutation importance. # Input Format There are no specific inputs required for this task since you will be using the `load_diabetes` dataset from sklearn. # Output Format Your output should include: 1. Model performance (R² score) on the validation set. 2. Top 3 features and their permutation importance values with standard deviations for the original model. 3. Top 3 features and their permutation importance values with standard deviations for the model after handling multicollinearity. # Example ```python # Example output format. Values are illustrative. Model R² score: 0.356 Original model feature importance: - Feature: bmi, Importance: 0.176, Std: 0.048 - Feature: bp, Importance: 0.088, Std: 0.033 - Feature: sex, Importance: 0.056, Std: 0.023 Model R² score after handling multicollinearity: 0.345 New model feature importance: - Feature: bmi, Importance: 0.174, Std: 0.045 - Feature: age, Importance: 0.070, Std: 0.028 - Feature: bp, Importance: 0.067, Std: 0.025 ``` Notes - Remember to comment your code appropriately and handle exceptions where necessary. - Ensure your code is efficient and adheres to best practices in Python programming.","solution":"import numpy as np from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge from sklearn.inspection import permutation_importance from sklearn.metrics import r2_score import pandas as pd def train_and_evaluate_model(): # Load the dataset diabetes = load_diabetes() X = diabetes.data y = diabetes.target feature_names = diabetes.feature_names # Split the dataset into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0) # Train a Ridge regression model model = Ridge(random_state=0) model.fit(X_train, y_train) # Evaluate the model performance y_pred = model.predict(X_val) model_r2_score = r2_score(y_val, y_pred) # Compute permutation feature importance perm_importance = permutation_importance(model, X_val, y_val, n_repeats=30, random_state=0) feature_importance = perm_importance.importances_mean feature_std = perm_importance.importances_std # Get top 3 features feature_importance_df = pd.DataFrame({ \'Feature\': feature_names, \'Importance\': feature_importance, \'Std\': feature_std }).sort_values(by=\'Importance\', ascending=False) top_3_features = feature_importance_df.head(3) # Handle multicollinearity correlation_matrix = pd.DataFrame(X, columns=feature_names).corr().values high_corr_pairs = [(i, j) for i in range(len(correlation_matrix)) for j in range(i+1, len(correlation_matrix)) if abs(correlation_matrix[i, j]) > 0.8] # Determine features to keep based on initial importance features_to_keep = set(range(len(feature_names))) for (i, j) in high_corr_pairs: if feature_importance[i] > feature_importance[j]: features_to_keep.discard(j) else: features_to_keep.discard(i) reduced_X = X[:, list(features_to_keep)] reduced_X_train, reduced_X_val = train_test_split(reduced_X, test_size=0.2, random_state=0) # Retrain the model with reduced features reduced_model = Ridge(random_state=0) reduced_model.fit(reduced_X_train, y_train) # Evaluate the reduced model performance reduced_y_pred = reduced_model.predict(reduced_X_val) reduced_model_r2_score = r2_score(y_val, reduced_y_pred) # Compute permutation feature importance for the reduced model reduced_perm_importance = permutation_importance(reduced_model, reduced_X_val, y_val, n_repeats=30, random_state=0) reduced_feature_importance = reduced_perm_importance.importances_mean reduced_feature_std = reduced_perm_importance.importances_std reduced_feature_names = [feature_names[i] for i in features_to_keep] reduced_feature_importance_df = pd.DataFrame({ \'Feature\': reduced_feature_names, \'Importance\': reduced_feature_importance, \'Std\': reduced_feature_std }).sort_values(by=\'Importance\', ascending=False) reduced_top_3_features = reduced_feature_importance_df.head(3) return { \\"model_r2_score\\": model_r2_score, \\"top_3_features\\": top_3_features, \\"reduced_model_r2_score\\": reduced_model_r2_score, \\"reduced_top_3_features\\": reduced_top_3_features }"},{"question":"Text Processing and Analysis You are tasked with implementing a function that processes a given text to extract specific information based on formatting rules and regular expressions. Specifically, the function should identify and return all email addresses and phone numbers found within the text. Function Signature ```python def extract_contacts(text: str) -> dict: pass ``` Input - `text` (str): A string containing lines of text. Each line may contain various information including names, email addresses, and phone numbers. Output - The function should return a dictionary with two keys: - `\\"emails\\"` (list): A list of all email addresses found in the text. - `\\"phones\\"` (list): A list of all phone numbers found in the text. Constraints - Email addresses follow the format: `local_part@domain`, where `local_part` consists of alphanumeric characters, periods, and underscores, and `domain` consists of alphanumeric characters and periods. - Phone numbers follow the format: `(xxx) xxx-xxxx` or `xxx-xxx-xxxx`, where `x` is a digit from 0 to 9. - The function should handle edge cases such as: - Mixed-content lines containing names or irrelevant information. - Lines with no relevant information at all. Example ```python text = John Doe: john.doe@example.com, (123) 456-7890 Alice Smith: alice_smith123@example.org, 987-654-3210 Bob Johnson: bob@example.co.uk Random Info: Some text without contacts info result = extract_contacts(text) # Expected Output: # { # \\"emails\\": [ # \\"john.doe@example.com\\", # \\"alice_smith123@example.org\\", # \\"bob@example.co.uk\\" # ], # \\"phones\\": [ # \\"(123) 456-7890\\", # \\"987-654-3210\\" # ] # } ``` Your task is to implement the `extract_contacts` function to meet the above specifications. Be sure to handle varying formats of input text and ensure that the function is robust against edge cases. Performance Requirements - Your solution should efficiently handle input text up to 10,000 lines long, ensuring that regular expression operations do not lead to excessive runtime. - Aim for a time complexity of O(n), where n is the number of lines in the input text. **Implementation Tips:** - Utilize the `re` module to define regular expressions for matching email addresses and phone numbers. - Use the `string` module for any additional string manipulations or checks as needed.","solution":"import re def extract_contacts(text: str) -> dict: Extracts all email addresses and phone numbers from the given text. Args: text (str): A string containing lines of text. Returns: dict: A dictionary with two keys: \\"emails\\" (list) and \\"phones\\" (list). # Define regular expressions for emails and phone numbers email_pattern = re.compile(r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,7}b\') phone_pattern = re.compile(r\'(?d{3})?[-.s]?d{3}[-.s]?d{4}\') emails = email_pattern.findall(text) phones = phone_pattern.findall(text) return { \\"emails\\": emails, \\"phones\\": phones }"},{"question":"**Objective**: Assess students\' understanding of Python 3.10 syntax, focusing on class definitions, decorators, pattern matching, and exception handling. **Problem Statement**: Implement a Python class `Calculator` that can perform basic arithmetic operations and additionally handle a custom operation using pattern matching. The class should include: - A static method `add(x, y)` that returns the sum of `x` and `y`. - A static method `subtract(x, y)` that returns the difference between `x` and `y`. - A static method `multiply(x, y)` that returns the product of `x` and `y`. - A static method `divide(x, y)` that returns the quotient of `x` and `y`. If `y` is zero, it should raise a `ValueError` with the message \\"Cannot divide by zero\\". - A method `custom_operation(op, x, y)` that uses pattern matching (PEP 634) to perform the operation specified by `op`: - If `op` is \\"power\\", it should return `x` raised to the power of `y`. - If `op` is \\"modulus\\", it should return `x` modulus `y`. - For unsupported operations, it should raise a `ValueError` with the message \\"Unsupported operation\\". **Function Signatures**: ```python class Calculator: @staticmethod def add(x: int, y: int) -> int: pass @staticmethod def subtract(x: int, y: int) -> int: pass @staticmethod def multiply(x: int, y: int) -> int: pass @staticmethod def divide(x: int, y: int) -> float: pass def custom_operation(self, op: str, x: int, y: int) -> float: pass ``` **Constraints**: - The input to `add`, `subtract`, `multiply`, and `divide` are two integers. - The method `custom_operation` expects a string `op` and two integers `x` and `y`. - The `add`, `subtract`, `multiply`, and `divide` methods should be decorated with a `staticmethod` decorator. - The `custom_operation` method should use pattern matching to handle different operations. **Example Usage**: ```python calc = Calculator() print(calc.add(3, 2)) # Output: 5 print(calc.subtract(5, 3)) # Output: 2 print(calc.multiply(4, 7)) # Output: 28 print(calc.divide(10, 2)) # Output: 5.0 print(calc.custom_operation(\\"power\\", 2, 3)) # Output: 8 print(calc.custom_operation(\\"modulus\\", 10, 3)) # Output: 1 try: print(calc.divide(10, 0)) except ValueError as e: print(e) # Output: Cannot divide by zero try: print(calc.custom_operation(\\"unknown\\", 1, 1)) except ValueError as e: print(e) # Output: Unsupported operation ``` **Special Note**: Ensure you handle zero division and unsupported operations appropriately by raising the specified exceptions. **Evaluation Criteria**: 1. Correct implementation of arithmetic operations. 2. Proper usage of `staticmethod` decorator. 3. Effective use of pattern matching in `custom_operation`. 4. Correct exception handling for zero division and unsupported operations.","solution":"class Calculator: @staticmethod def add(x: int, y: int) -> int: return x + y @staticmethod def subtract(x: int, y: int) -> int: return x - y @staticmethod def multiply(x: int, y: int) -> int: return x * y @staticmethod def divide(x: int, y: int) -> float: if y == 0: raise ValueError(\\"Cannot divide by zero\\") return x / y def custom_operation(self, op: str, x: int, y: int) -> float: match op: case \\"power\\": return x ** y case \\"modulus\\": return x % y case _: raise ValueError(\\"Unsupported operation\\")"},{"question":"# Background In PyTorch, the `torch.futures` module provides tools to work with asynchronous operations, mainly through the `Future` class. This functionality is crucial when working in distributed computing environments where tasks can be processed in parallel. # Objective Your task is to implement a function that simulates asynchronous processing of a series of computational tasks using `torch.futures.Future`. You will need to leverage `collect_all` or `wait_all` to manage the completion of these tasks. # Requirements 1. **Function Name**: `process_async_tasks` 2. **Input**: - A list of integers representing computational tasks. 3. **Output**: - A list of integers, where each integer is the result of processing the corresponding task. 4. **Constraints**: - Each \\"task\\" is represented by an integer and the processing function will square the integer. - You must use the `torch.futures.Future` class to handle the asynchronous execution of these tasks. - Utilize `collect_all` or `wait_all` to manage the completion of all asynchronous tasks. # Function Signature ```python def process_async_tasks(tasks: List[int]) -> List[int]: ... ``` # Example ```python # Example usage of the function task_inputs = [1, 2, 3, 4, 5] results = process_async_tasks(task_inputs) print(results) # Output should be [1, 4, 9, 16, 25] ``` # Implementation Notes - Simulate the asynchronous processing by creating a `Future` object for each task. - Use a simple function (like squaring the integer) to represent the task processing. - Combine the results using `collect_all` or `wait_all`. # Constraints - You can assume that the input list contains no more than 1000 integers. - Each integer is in the range [0, 100]. # Performance Requirements - The function should complete execution in a reasonable time frame appropriate for the input constraints.","solution":"import torch from torch.futures import Future from typing import List def process_async_tasks(tasks: List[int]) -> List[int]: def process_task(task: int) -> int: # Simulate a computational task by squaring the input number return task * task futures = [] for task in tasks: future = Future() future.set_result(process_task(task)) futures.append(future) completed_futures = torch.futures.collect_all(futures).wait() results = [future.value() for future in completed_futures] return results"},{"question":"# Exception Handling & User-defined Exceptions You are required to implement a class that handles file operations and related exceptions in Python. Task: 1. **Class Definitions:** - Define a custom exception class `UnsupportedFileFormatError` that inherits from the base `Exception` class. This exception should be raised when a file with unsupported format (extensions other than .txt or .csv) is attempted to be opened. 2. **Class Implementation:** - Implement a class `FileManager` with the following methods: 1. `__init__(self, file_path: str)`: Initialize with file path. 2. `read_file(self)`: Reads content from the file and returns it. Handle the following exceptions: - `FileNotFoundError`: If the file does not exist. - `UnsupportedFileFormatError`: If the file format is not supported (not .txt or .csv). - Any other exceptions that may occur should be re-raised after printing an error message. - Ensure the file is closed properly after reading its content using a `finally` block. 3. `write_file(self, content: str)`: Writes the provided content to the file. Handle the following exceptions: - `UnsupportedFileFormatError`: If the file format is not supported (not .txt or .csv). - Any other exceptions that may occur should be re-raised after printing an error message. Constraints: - The file must be .txt or .csv format. Otherwise, the `UnsupportedFileFormatError` should be raised when attempting to read or write. - Implement proper exception handling within the methods. - Ensure proper resource management (like closing files after operations) using `finally` blocks or `with` statements. Example Usage: ```python # Example usage of the FileManager class: file_path = \\"example.txt\\" manager = FileManager(file_path) try: manager.write_file(\\"Hello, Python world!\\") content = manager.read_file() print(content) except UnsupportedFileFormatError as e: print(f\\"Error: {e}\\") except Exception as e: print(f\\"An unexpected error occurred: {e}\\") ``` # Expected Output: - If the file format is unsupported: ``` Error: Unsupported file format: example.unsupported ``` - If another error occurs: ``` An unexpected error occurred: <Description of the error> ``` - Successful read/write operations: ``` Content of the file: Hello, Python world! ``` Implement the `FileManager` class below: ```python class UnsupportedFileFormatError(Exception): pass class FileManager: def __init__(self, file_path: str): self.file_path = file_path def read_file(self): # Implement this method pass def write_file(self, content: str): # Implement this method pass ```","solution":"class UnsupportedFileFormatError(Exception): pass class FileManager: def __init__(self, file_path: str): self.file_path = file_path def _check_file_format(self): if not (self.file_path.endswith(\'.txt\') or self.file_path.endswith(\'.csv\')): raise UnsupportedFileFormatError(f\\"Unsupported file format: {self.file_path}\\") def read_file(self): self._check_file_format() try: with open(self.file_path, \'r\') as file: return file.read() except FileNotFoundError: raise FileNotFoundError(f\\"No such file: {self.file_path}\\") except Exception as e: print(f\\"An error occurred while reading the file: {e}\\") raise def write_file(self, content: str): self._check_file_format() try: with open(self.file_path, \'w\') as file: file.write(content) except Exception as e: print(f\\"An error occurred while writing to the file: {e}\\") raise"},{"question":"**Advanced PyTorch Distributed Training Task** # Task You are required to demonstrate the usage of PyTorch\'s distributed optimizers by implementing a simple distributed mini-batch gradient descent. The goal is to train a linear regression model over a distributed system using CPU tensors. # Background Linear regression is a linear approach to modeling the relationship between a dependent variable and one or more independent variables. You will split the dataset across multiple processes/nodes, use a distributed optimizer to manage the weights, and ensure efficient training across the nodes. # Requirements 1. **Implement a Linear Regression Model**: - The model should be simple, with one layer. 2. **Distribute Data**: - Split the given dataset across `n` nodes/processes. 3. **Implement Distributed Training**: - Use `torch.distributed.optim.DistributedOptimizer` to perform the optimization in a distributed manner. # Input 1. **Dataset**: A PyTorch tensor representing the dataset with `m` samples and `d` features, and a tensor `targets` representing the labels. 2. **Nodes**: Number of nodes/processes to which the dataset should be distributed. # Output 1. **Final Weights**: The weights of the trained Linear Regression Model after `epochs` iterations. # Constraints - Utilize `torch.distributed` package for handling the distributed environment. - Use CPU tensors only. - Make sure no support for CUDA is utilized. # Performance - Ensure that the training process is synchronous and consistent across all nodes. - Efficiently handle the distribution and aggregation of gradients. # Skeleton Code Here is a skeleton code to get you started. You are expected to fill in the missing details to complete the implementation. ```python import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP def linear_regression_model(input_dim): return torch.nn.Linear(input_dim, 1) def distribute_data(data, targets, num_nodes): # Implement distribution of data and targets across nodes return data_splits, target_splits def train(model, data, targets, optimizer): for epoch in range(epochs): model.train() optimizer.zero_grad() outputs = model(data) loss = torch.nn.functional.mse_loss(outputs, targets) loss.backward() optimizer.step() # Implement gradient synchronization across nodes return model def main(): dist.init_process_group(backend=\'gloo\', init_method=\'env://\') rank = dist.get_rank() num_nodes = dist.get_world_size() # Assume data, targets are already loaded tensors data_splits, target_splits = distribute_data(data, targets, num_nodes) model = linear_regression_model(input_dim=data.size(1)) ddp_model = DDP(model) optimizer = torch.distributed.optim.DistributedOptimizer( torch.optim.SGD, ddp_model.parameters(), lr=0.01 ) trained_model = train(ddp_model, data_splits[rank], target_splits[rank], optimizer) if rank == 0: print(\\"Final Model Weights:\\", trained_model.parameters()) if __name__ == \\"__main__\\": main() ``` # Notes - Ensure that the model\'s parameters are aggregated and synchronized across the nodes after each update. - Initialize the distributed process group properly to handle process coordination. - This is an advanced task; ensure a thorough understanding of distributed computing and PyTorch\'s distributed training methodologies.","solution":"import torch import torch.distributed as dist from torch.nn.parallel import DistributedDataParallel as DDP def linear_regression_model(input_dim): return torch.nn.Linear(input_dim, 1) def distribute_data(data, targets, num_nodes): data_split = data.chunk(num_nodes) target_split = targets.chunk(num_nodes) return data_split, target_split def train(model, data, targets, optimizer, epochs=5): for epoch in range(epochs): model.train() optimizer.zero_grad() outputs = model(data) loss = torch.nn.functional.mse_loss(outputs, targets) loss.backward() optimizer.step() # Synchronize gradients across nodes return model def main(data, targets, num_nodes): dist.init_process_group(backend=\'gloo\', init_method=\'env://\') rank = dist.get_rank() world_size = dist.get_world_size() assert num_nodes == world_size, \\"Number of nodes must match the world size\\" data_splits, target_splits = distribute_data(data, targets, num_nodes) model = linear_regression_model(input_dim=data.size(1)) ddp_model = DDP(model) optimizer = torch.optim.SGD(ddp_model.parameters(), lr=0.01) trained_model = train(ddp_model, data_splits[rank], target_splits[rank], optimizer) final_weights = None if rank == 0: final_weights = list(trained_model.parameters()) return final_weights # Example usage: Uncomment when actually running in a distributed environment # if __name__ == \\"__main__\\": # data = torch.randn(100, 3) # example data # targets = torch.randn(100, 1) # example targets # num_nodes = 4 # should be set based on the distributed environment # print(main(data, targets, num_nodes))"},{"question":"Internationalizing an Application Problem Statement You are tasked with designing a Python application that supports multiple languages. Your application should allow the user to select a preferred language and display messages in that language. Implement the following functionalities using the `gettext` module: 1. **Setup Translation Files**: - Create a directory structure with translation files for at least two languages (e.g., French and Spanish). The translation files should follow the GNU `gettext` format (`.mo` files). For the purpose of this assessment, you can simulate this by creating dictionary mappings within the code. 2. **Display Messages in Selected Language**: - Write a function `display_message(language_code: str, message_id: str) -> str` that takes a language code (e.g., \'fr\' for French, \'es\' for Spanish) and a message ID (e.g., \'greeting\') and returns the translated message. 3. **Change Language on-the-fly**: - Implement functionality to change the language dynamically. Write a function `set_language(language_code: str)` that changes the current language of the application. 4. **Deferred Translations**: - Implement deferred translations in your application, where translations are not immediately resolved but evaluated at runtime based on the current language setting. Constraints: 1. You must use the `gettext` module for the internationalization functionalities. 2. You must support at least three messages: \\"greeting\\" (e.g., \\"Hello\\"), \\"farewell\\" (e.g., \\"Goodbye\\"), and \\"thanks\\" (e.g., \\"Thank you\\"). 3. The implementation must support switching between languages on the fly. Example Usage: ```python # Assuming your implementation is in a file named intl_app.py from intl_app import set_language, display_message # Set initial language to French set_language(\'fr\') # Display greeting message in French print(display_message(\'fr\', \'greeting\')) # Should output: \\"Bonjour\\" # Change language to Spanish set_language(\'es\') # Display thanks message in Spanish print(display_message(\'es\', \'thanks\')) # Should output: \\"Gracias\\" ``` Submission: Submit a Python file named `intl_app.py` containing all the required functions. Ensure your code is well-documented and adheres to best practices for internationalization using the `gettext` module. Hints: - Look into the `gettext.translation` function and its usage for loading translations. - Consider using a dictionary to simulate translation catalog for this exercise. - Explore the `gettext.NullTranslations` and `gettext.GNUTranslations` classes for handling translations.","solution":"import gettext # Dictionary to simulate the translation files (.mo files) translations = { \'fr\': { \'greeting\': \'Bonjour\', \'farewell\': \'Au revoir\', \'thanks\': \'Merci\' }, \'es\': { \'greeting\': \'Hola\', \'farewell\': \'Adiós\', \'thanks\': \'Gracias\' } } # Define a variable to keep track of the current language current_language = \'en\' # Default language is English def set_language(language_code: str): Sets the current language of the application. global current_language if language_code in translations: current_language = language_code else: raise ValueError(\\"Unsupported language code\\") def display_message(language_code: str, message_id: str) -> str: Returns the translated message for the given language code and message ID. if language_code not in translations: raise ValueError(\\"Unsupported language code\\") language_translations = translations[language_code] return language_translations.get(message_id, message_id) def get_deferred_translation(message_id: str) -> str: Returns the translation for the message ID based on the current language. The translation is evaluated at runtime based on the current language setting. return display_message(current_language, message_id)"},{"question":"# Custom Scikit-learn Estimator Implementation **Objective**: To demonstrate your understanding of creating scikit-learn compatible estimators, you will implement a custom transformer that normalizes numerical features by subtracting the mean and dividing by the standard deviation, similar to the `StandardScaler` from scikit-learn. Ensure it adheres to scikit-learn\'s conventions and interfaces. # Task Implement a scikit-learn compatible estimator named `CustomStandardScaler` which inherits from the `BaseEstimator` and `TransformerMixin` classes. Your `CustomStandardScaler` should: 1. **Initialization**: Define an `__init__` method that takes a parameter `copy` which defaults to `True`. This parameter should be stored as an attribute. 2. **Fitting**: Implement the `fit` method which calculates and stores the mean and standard deviation of the features from the input data. 3. **Transformation**: Implement the `transform` method which normalizes the data using the mean and standard deviation calculated during fitting. 4. **Inverse Transformation**: Implement the `inverse_transform` method which reverses the transformation applied by `transform`. # Specifications 1. **Constructor**: - `__init__(self, copy=True)`: Initializes the transformer with the `copy` parameter. Stores the parameter as an attribute. 2. **Fitting Data**: - `fit(self, X, y=None)`: Calculates and stores the mean and standard deviation for each feature in `X`. Sets the attributes `mean_` and `std_`. Returns `self`. - **Input**: `X` - array-like of shape (n_samples, n_features), `y` - ignored, set to `None` by default. - **Raises**: `ValueError` if any feature in `X` has zero variance. 3. **Transforming Data**: - `transform(self, X)`: Normalizes the data using the means and standard deviations computed in the `fit` method. - **Input**: `X` - array-like of shape (n_samples, n_features). - **Output**: array-like of shape (n_samples, n_features), the normalized data. 4. **Inverse Transformation**: - `inverse_transform(self, X)`: Reverses the transformation applied by `transform`. - **Input**: `X` - array-like of shape (n_samples, n_features). - **Output**: array-like of shape (n_samples, n_features), the original data before normalization. # Examples ```python import numpy as np from sklearn.base import BaseEstimator, TransformerMixin class CustomStandardScaler(BaseEstimator, TransformerMixin): def __init__(self, copy=True): self.copy = copy def fit(self, X, y=None): X = np.asarray(X) self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) if np.any(self.std_ == 0): raise ValueError(\\"Input contains features with zero variance.\\") return self def transform(self, X): X = np.asarray(X) if self.copy: X = X.copy() return (X - self.mean_) / self.std_ def inverse_transform(self, X): X = np.asarray(X) if self.copy: X = X.copy() return (X * self.std_) + self.mean_ ``` # Instructions - Implement the `CustomStandardScaler` class with the described methods. - Ensure your implementation passes the given examples and correctly handles the edge cases. - Provide appropriate docstrings for your methods following the numpy docstring standard. - Validate your custom estimator\'s compliance by running it through `sklearn.utils.estimator_checks.check_estimator(CustomStandardScaler())`. # Submission Submit your `CustomStandardScaler` implementation as a Python file or Jupyter notebook. Ensure that all parts of your code are well-commented and follow best practices for readability and maintainability.","solution":"import numpy as np from sklearn.base import BaseEstimator, TransformerMixin class CustomStandardScaler(BaseEstimator, TransformerMixin): def __init__(self, copy=True): Initialize the CustomStandardScaler with the option to copy data. Parameters ---------- copy : bool, default=True If True, a copy of the data will be used for transformation and inverse transformation, to ensure the original data is not modified. self.copy = copy def fit(self, X, y=None): Compute the mean and standard deviation for each feature in X. Parameters ---------- X : array-like of shape (n_samples, n_features) The data used to compute the mean and standard deviation. y : None Ignored parameter, exists for compatibility. Returns ------- self : object Fitted scaler. Raises ------ ValueError If any feature in X has zero variance. X = np.asarray(X) self.mean_ = np.mean(X, axis=0) self.std_ = np.std(X, axis=0) if np.any(self.std_ == 0): raise ValueError(\\"Input contains features with zero variance.\\") return self def transform(self, X): Transform X by scaling features to zero mean and unit variance. Parameters ---------- X : array-like of shape (n_samples, n_features) The data to be transformed. Returns ------- X_tr : array-like of shape (n_samples, n_features) The transformed data. X = np.asarray(X) if self.copy: X = X.copy() return (X - self.mean_) / self.std_ def inverse_transform(self, X): Transform data back to its original form. Parameters ---------- X : array-like of shape (n_samples, n_features) The data to be inverse transformed. Returns ------- X_inv : array-like of shape (n_samples, n_features) The data transformed back to its original form. X = np.asarray(X) if self.copy: X = X.copy() return (X * self.std_) + self.mean_"},{"question":"Advanced Type Hints and Generics in Python Objective This question is designed to assess your understanding of type hints and generics as provided by the `typing` module in Python. You are required to implement a function with complex type annotations and demonstrate the use of generics and advanced type constructs. Problem Statement You are to implement a generic class `SafeCache` which functions as a key-value store (similar to a dictionary) with type safety for both keys and values. Additionally, implement a function `cache_statistics` that returns statistics about the cache content. 1. **Class: `SafeCache`** - The class should be parameterized with two type variables: `KT` for the key type and `VT` for the value type. - It should provide methods to: - Add an item to the cache (`add_item`) - Retrieve an item from the cache (`get_item`) - Remove an item from the cache (`remove_item`) - Get the size of the cache (`cache_size`) 2. **Function: `cache_statistics`** - Accepts a `SafeCache` instance and returns a `TypedDict` with the following structure: - `total_items` (int): The total number of items in the cache. - `value_field_types` (dict): A dictionary where keys are field names in the cached values and values are lists of types observed for each field across all cached items. Specifications - The `SafeCache` class must ensure type safety when adding and retrieving items. - Use `TypeVar`, `Generic`, and `TypedDict` from the `typing` module where appropriate. - The `cache_statistics` function should introspect the cache\'s values and collect type statistics for each field in the cached values assuming they are `TypedDict` instances. Constraints - Keys in the `SafeCache` can be of any hashable type. - Values in the `SafeCache` should be instances of `TypedDict`. - Python version must be at least 3.10. Example ```python from typing import TypeVar, Generic, TypedDict, Union, Any from collections.abc import Callable KT = TypeVar(\'KT\', bound=Hashable) VT = TypeVar(\'VT\', bound=TypedDict) class SafeCache(Generic[KT, VT]): def __init__(self) -> None: ... def add_item(self, key: KT, value: VT) -> None: ... def get_item(self, key: KT) -> VT: ... def remove_item(self, key: KT) -> None: ... def cache_size(self) -> int: ... def cache_statistics(cache: SafeCache[KT, VT]) -> dict: ... # Example usage class PersonRecord(TypedDict): name: str age: int cache = SafeCache[int, PersonRecord]() cache.add_item(1, {\'name\': \'Alice\', \'age\': 30}) cache.add_item(2, {\'name\': \'Bob\', \'age\': 25}) stats = cache_statistics(cache) print(stats) # Expected output: # {\'total_items\': 2, \'value_field_types\': {\'name\': [<class \'str\'>], \'age\': [<class \'int\'>]}} ``` Note: The provided code template is incomplete, and you need to implement the complete `SafeCache` class and the `cache_statistics` function. Ensure you use type hints to maintain type safety.","solution":"from typing import TypeVar, Generic, TypedDict, Union, Hashable, Any from collections.abc import Callable KT = TypeVar(\'KT\', bound=Hashable) VT = TypeVar(\'VT\', bound=TypedDict) class SafeCache(Generic[KT, VT]): def __init__(self) -> None: self._cache = {} def add_item(self, key: KT, value: VT) -> None: self._cache[key] = value def get_item(self, key: KT) -> VT: return self._cache[key] def remove_item(self, key: KT) -> None: if key in self._cache: del self._cache[key] def cache_size(self) -> int: return len(self._cache) def cache_statistics(cache: SafeCache[KT, VT]) -> dict: total_items = cache.cache_size() value_field_types = {} for value in cache._cache.values(): for field, field_value in value.items(): if field not in value_field_types: value_field_types[field] = set() value_field_types[field].add(type(field_value)) # Convert sets to lists for JSON serializability value_field_types = {field: list(types) for field, types in value_field_types.items()} return { \'total_items\': total_items, \'value_field_types\': value_field_types }"},{"question":"Objective Your task is to design a customizable and type-safe generic data caching class using Python\'s \\"typing\\" module. This class should support various cache retrieval policies (e.g., LRU - Least Recently Used, FIFO - First In First Out) and be generic to handle any data type. # Requirements 1. **GenericCache Class**: Create a generic class `GenericCache` that supports type hinting for the type of keys and values it stores. 2. **Type Variables**: Use type variables to ensure that the cache class is type-safe and can handle any type of key-value pairs. 3. **LRU Retrieval Policy**: Implement an LRU (Least Recently Used) cache retrieval policy within the `GenericCache` class. 4. **Type-Safe Methods**: Your class should have the following methods, all annotated with precise type hints: - `get(key: KeyType) -> Optional[ValueType]`: Retrieve the value for the given key, returning `None` if the key is not in the cache. - `put(key: KeyType, value: ValueType) -> None`: Insert a key-value pair into the cache, evicting the least recently used entry if the cache is full. - `evict() -> None`: Explicitly evicts the least recently used item from the cache. - `size() -> int`: Returns the current size of the cache. 5. **Typed Dict**: Use `TypedDict` to define the structure of the cache entries. Each entry should have the fields `key` and `value`. 6. **Constraints**: - The maximum size of the cache should be specified upon initialization. - The cache must enforce thread-safety. 7. **Efficiency**: The insertion and retrieval operations in the cache should be optimized for performance. # Input/Output Define the class `GenericCache` with methods adhering to the above requirements. Ensure usage of type hints and appropriate generics. # Example Usage ```python from typing import Generic, TypeVar, Optional, TypedDict KeyType = TypeVar(\'KeyType\') ValueType = TypeVar(\'ValueType\') class CacheEntry(TypedDict, Generic[KeyType, ValueType]): key: KeyType value: ValueType class GenericCache(Generic[KeyType, ValueType]): def __init__(self, max_size: int): ... def get(self, key: KeyType) -> Optional[ValueType]: ... def put(self, key: KeyType, value: ValueType) -> None: ... def evict(self) -> None: ... def size(self) -> int: ... # Example: cache = GenericCache[int, str](max_size=5) cache.put(1, \\"one\\") cache.put(2, \\"two\\") print(cache.get(1)) # Output: \\"one\\" cache.put(3, \\"three\\") print(cache.size()) # Output: 3 cache.evict() print(cache.size()) # Output: 2 ``` Ensure that your implementation correctly uses type hints and demonstrates knowledge of the \\"typing\\" module, including generic types, `TypedDict`, and type constructors.","solution":"from typing import Generic, TypeVar, Optional, Dict, OrderedDict import threading KeyType = TypeVar(\'KeyType\') ValueType = TypeVar(\'ValueType\') class GenericCache(Generic[KeyType, ValueType]): def __init__(self, max_size: int): self.max_size = max_size self.cache: OrderedDict[KeyType, ValueType] = OrderedDict() self.lock = threading.Lock() def get(self, key: KeyType) -> Optional[ValueType]: with self.lock: if key in self.cache: # Move the key to the end to show that it was recently used value = self.cache.pop(key) self.cache[key] = value return value return None def put(self, key: KeyType, value: ValueType) -> None: with self.lock: if key in self.cache: # Remove the old value self.cache.pop(key) elif len(self.cache) >= self.max_size: # Evict the first item in the dictionary (the least recently used item) self.cache.popitem(last=False) self.cache[key] = value def evict(self) -> None: with self.lock: if self.cache: # Remove the first item i.e. the least recently used item self.cache.popitem(last=False) def size(self) -> int: with self.lock: return len(self.cache)"},{"question":"# Permutation Feature Importance Implementation You are tasked with assessing the importance of features in a dataset using the permutation feature importance technique provided by scikit-learn. This method is model-agnostic and can be used to interpret the importance of features for any predictive model. Given a dataset, you will need to implement a function `evaluate_permutation_importance` to perform the following: 1. Load the dataset and split it into training and validation sets. 2. Train a predictive model on the training data. 3. Use the `sklearn.inspection.permutation_importance` function to compute the importance of each feature. 4. Return the feature importance scores along with their standard deviations for further analysis. Your implementation should be flexible to handle both regression and classification tasks. Function Signature ```python def evaluate_permutation_importance(model, X, y, n_repeats=30, random_state=0): Evaluate the permutation feature importance for a given predictive model. Parameters: - model: A scikit-learn fitted model (estimator). - X: pd.DataFrame or np.ndarray, the input features. - y: pd.Series or np.ndarray, the target values. - n_repeats: int, optional (default=30), the number of times to permute a feature. - random_state: int, optional (default=0), random seed for reproducibility. Returns: - feature_importances: dict, feature names as keys and tuples of (mean importance, std deviation) as values. pass ``` Input - `model`: A fitted scikit-learn model, such as an instance of `Ridge`, `RandomForestClassifier`, or `any other scikit-learn estimator`. - `X`: A DataFrame or NumPy array of shape (n_samples, n_features), representing the input features. - `y`: A Series or NumPy array of shape (n_samples,), representing the target values. - `n_repeats` (optional): Number of times to permute a feature (default is 30). - `random_state` (optional): Seed for the random number generator (default is 0). Output - `feature_importances`: A dictionary where keys are feature names and values are tuples of (mean importance, std deviation). Constraints - Assume that all inputs (X, y) are properly preprocessed and ready to be used for training and evaluation. - The function should work for both regression and classification tasks. Example Usage ```python from sklearn.datasets import load_diabetes from sklearn.model_selection import train_test_split from sklearn.linear_model import Ridge import pandas as pd # Load the dataset data = load_diabetes(as_frame=True) X, y = data.data, data.target # Split into training and validation sets X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0) # Train the model (example with Ridge regression) model = Ridge(alpha=1e-2).fit(X_train, y_train) # Compute permutation importance scores feature_importances = evaluate_permutation_importance(model, X_val, y_val) # Output the feature importances for feature, (mean_importance, stddev) in feature_importances.items(): print(f\\"{feature}: {mean_importance:.3f} +/- {stddev:.3f}\\") ``` Notes - Ensure that your implementation leverages the `permutation_importance` function from `sklearn.inspection`. - Perform appropriate checks and handle exceptions as needed to avoid runtime errors.","solution":"from sklearn.inspection import permutation_importance def evaluate_permutation_importance(model, X, y, n_repeats=30, random_state=0): Evaluate the permutation feature importance for a given predictive model. Parameters: - model: A scikit-learn fitted model (estimator). - X: pd.DataFrame or np.ndarray, the input features. - y: pd.Series or np.ndarray, the target values. - n_repeats: int, optional (default=30), the number of times to permute a feature. - random_state: int, optional (default=0), random seed for reproducibility. Returns: - feature_importances: dict, feature names as keys and tuples of (mean importance, std deviation) as values. # Compute permutation importance result = permutation_importance(model, X, y, n_repeats=n_repeats, random_state=random_state) # Create a dictionary for feature importances feature_importances = {} for i, col in enumerate(X.columns): feature_importances[col] = (result.importances_mean[i], result.importances_std[i]) return feature_importances"},{"question":"**Objective:** Create a function that generates a customized seaborn plot with specific color palette requirements. # Problem Statement: You are provided with a dataset containing three numerical columns and one categorical column. Your task is to create a seaborn scatter plot using this dataset. The color mapping for the different categories should use a specific qualitative colormap and the intensity of markers should vary based on one of the numerical columns using a continuous colormap. # Specifications: 1. **Input:** * `data`: A pandas DataFrame with the following columns: * \'numerical_1\': Numerical data for x-axis. * \'numerical_2\': Numerical data for y-axis. * \'numerical_3\': Numerical data for color intensity. * \'category\': Categorical data for hue. * `qual_colormap`: A string indicating which qualitative colormap to use (e.g., \\"Set1\\", \\"Set2\\"). * `cont_colormap`: A string indicating which continuous colormap to use (e.g., \\"viridis\\", \\"plasma\\"). 2. **Output:** * A seaborn scatter plot with the following characteristics: * X-axis uses \'numerical_1\'. * Y-axis uses \'numerical_2\'. * Hue (color of the markers) represents the different categories in \'category\', and uses the provided `qual_colormap`. * The intensity of the marker colors represents \'numerical_3\', and uses the provided `cont_colormap`. 3. **Constraints:** * The dataset will have at least 100 rows and at most 10,000 rows. * There will be at least 2 and at most 10 unique values in the \'category\' column. # Function Signature: ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customized_scatter_plot(data: pd.DataFrame, qual_colormap: str, cont_colormap: str) -> None: # Write your code here pass ``` # Example: ```python import pandas as pd import numpy as np data = pd.DataFrame({ \'numerical_1\': np.random.rand(200), \'numerical_2\': np.random.rand(200), \'numerical_3\': np.random.rand(200), \'category\': np.random.choice([\'A\', \'B\', \'C\', \'D\'], 200) }) customized_scatter_plot(data, \'Set1\', \'viridis\') ``` **Hints:** * Use `sns.scatterplot` for creating the scatter plot. * Use `sns.mpl_palette` to retrieve palettes from matplotlib. * Use `plt.colorbar` to add a color bar based on the intensity of the markers.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def customized_scatter_plot(data: pd.DataFrame, qual_colormap: str, cont_colormap: str) -> None: Creates a customized seaborn scatter plot with specified colormap for categories and intensity mapping for a numerical column. Parameters: - data: pd.DataFrame - The dataset containing \'numerical_1\', \'numerical_2\', \'numerical_3\', and \'category\'. - qual_colormap: str - The qualitative colormap for the categorical hue. - cont_colormap: str - The continuous colormap for the intensity of markers. # Create scatter plot using seaborn plt.figure(figsize=(10, 6)) scatter_plot = sns.scatterplot(x=\'numerical_1\', y=\'numerical_2\', hue=\'category\', palette=qual_colormap, size=\'numerical_3\', sizes=(20, 200), data=data, legend=\\"full\\") # Add color bar for continuous colormap based on \'numerical_3\' norm = plt.Normalize(data[\'numerical_3\'].min(), data[\'numerical_3\'].max()) sm = plt.cm.ScalarMappable(cmap=cont_colormap, norm=norm) sm.set_array([]) plt.colorbar(sm, ax=scatter_plot) plt.title(\'Customized Scatter Plot\') plt.show()"},{"question":"**Match Statement Shopping Cart** # Objective: Create functions that utilize advanced Python features such as keyword arguments, match statements, and various control flow structures. Your implementation will simulate the behavior of a basic shopping cart. # Problem: You are required to create a function `process_order` that processes a customer\'s order. The function will take as input a dictionary representing the order details and return a summary of the processed order. # Function Definition: ```python def process_order(order: dict) -> dict: Process a customer\'s order and return a summary. Parameters: - order (dict): A dictionary containing the order details with the following possible keys: - \\"items\\" (list): A list of tuples where each tuple contains the item name (str), quantity (int), and price per item (float). - \\"address\\" (str): The delivery address. - \\"priority\\" (str): The priority of the order. Can be one of \\"standard\\", \\"express\\", or \\"overnight\\". - \\"discount_code\\" (str): A discount code that may apply to the order for a discount. Returns: - dict: A dictionary summarizing the order with the following keys: - \\"total_cost\\" (float): Total cost after processing the discount. - \\"discount_applied\\" (str): Discount code applied (if any). - \\"delivery_fee\\" (float): The delivery fee based on the priority. ``` # Processing Requirements: 1. **Total Cost Calculation**: - Calculate the total cost of items by summing the product of quantity and price per item. 2. **Discount Application**: - If a valid `discount_code` is provided, modify the total cost. Here are the valid discount codes: - \\"SAVE10\\": 10% off on the total cost. - \\"SAVE20\\": 20% off on the total cost. - \\"SAVE30\\": 30% off on the total cost. - Store the `discount_code` applied or \\"None\\" in the return dictionary if no discount is applied. 3. **Delivery Fee**: - Add a delivery fee based on the `priority` of the order: - \\"standard\\": 5.0 - \\"express\\": 10.0 - \\"overnight\\": 20.0 4. **Error Handling**: - Use the `match` statement to validate the `priority` field. If an invalid priority is given, raise a `ValueError` with a suitable error message. # Example Usage: ```python order = { \\"items\\": [(\\"apple\\", 3, 1.0), (\\"banana\\", 2, 0.5)], \\"address\\": \\"123 Python Lane\\", \\"priority\\": \\"express\\", \\"discount_code\\": \\"SAVE10\\" } result = process_order(order) print(result) # Expected Output: # { # \\"total_cost\\": 4.05, # \\"discount_applied\\": \\"SAVE10\\", # \\"delivery_fee\\": 10.0 # } ``` # Constraints: - Prices and total costs are represented as floating point numbers. - Quantity of items are positive integers. - If an invalid `discount_code` is provided, it should be disregarded (i.e., no discount applied). - Assume that \\"items\\" list and \\"address\\" are always provided in the input dictionary. Ensure your implementation is efficient and follows Python best practices as per PEP 8 guidelines.","solution":"def process_order(order: dict) -> dict: Process a customer\'s order and return a summary. Parameters: - order (dict): A dictionary containing the order details with the following possible keys: - \\"items\\" (list): A list of tuples where each tuple contains the item name (str), quantity (int), and price per item (float). - \\"address\\" (str): The delivery address. - \\"priority\\" (str): The priority of the order. Can be one of \\"standard\\", \\"express\\", or \\"overnight\\". - \\"discount_code\\" (str): A discount code that may apply to the order for a discount. Returns: - dict: A dictionary summarizing the order with the following keys: - \\"total_cost\\" (float): Total cost after processing the discount. - \\"discount_applied\\" (str): Discount code applied (if any). - \\"delivery_fee\\" (float): The delivery fee based on the priority. items = order.get(\\"items\\", []) total_cost = sum(quantity * price for _, quantity, price in items) discount_code = order.get(\\"discount_code\\", None) valid_discounts = { \\"SAVE10\\": 0.10, \\"SAVE20\\": 0.20, \\"SAVE30\\": 0.30 } if discount_code in valid_discounts: discount_applied = discount_code total_cost -= total_cost * valid_discounts[discount_code] else: discount_applied = \\"None\\" priority = order.get(\\"priority\\", \\"standard\\") delivery_fee = 0.0 match priority: case \\"standard\\": delivery_fee = 5.0 case \\"express\\": delivery_fee = 10.0 case \\"overnight\\": delivery_fee = 20.0 case _: raise ValueError(f\\"Invalid priority: {priority}\\") total_cost += delivery_fee return { \\"total_cost\\": total_cost, \\"discount_applied\\": discount_applied, \\"delivery_fee\\": delivery_fee }"},{"question":"# Advanced Python: Working with `importlib` Objective You are required to implement a small utility function that utilizes the `importlib` module to check whether a list of specified Python modules are available for import and to load them dynamically if they are present. Task Write a function `check_and_import_modules(modules: list) -> dict` that takes a list of module names (strings) and returns a dictionary. The dictionary keys should be the module names, and the values should be: - `True` if the module was successfully imported. - `False` if the module is not available for import. Specifications - Use the `importlib` module to perform the imports. - Do **not** use the `import` statement explicitly for importing the modules within your function. - Handle possible exceptions that may occur during the import process. Input - `modules`: A list of strings representing the names of the modules to be checked and imported. Output - Returns a dictionary with module names as keys. The value for each key should be `True` if the module was successfully imported, otherwise `False`. Example ```python def check_and_import_modules(modules: list) -> dict: # TODO: Implement the function. pass modules_list = [\'os\', \'sys\', \'nonexistentmodule\'] result = check_and_import_modules(modules_list) print(result) # Expected Output: {\'os\': True, \'sys\': True, \'nonexistentmodule\': False} ``` Constraints - Implement the function such that it performs efficiently even for a large number of module names. - The module names passed to the function will be valid Python identifiers.","solution":"import importlib def check_and_import_modules(modules: list) -> dict: Check the availability of specified Python modules and import them dynamically if they are present. Args: modules (list): A list of module names (strings). Returns: dict: A dictionary with module names as keys and True or False as values indicating the success of the import. results = {} for module in modules: try: importlib.import_module(module) results[module] = True except ImportError: results[module] = False return results"},{"question":"Objective: Design and implement a function in Python that accepts a directory path as input and performs the following operations: 1. **List all files and subdirectories**: Traverse the given directory and list all files and subdirectories recursively. 2. **Count file types**: Count the number of files for each file type (based on extension) present in the directory and its subdirectories. 3. **Create a report file**: Write the resulting directory structure and file type counts to a report file named `directory_report.txt` in the given directory. Requirements: 1. Use the `pathlib` module to handle file and directory operations. 2. Ignore hidden files and directories (those starting with a dot `.`). 3. Ensure your function handles exceptions gracefully, especially for permission-related errors. Function Signature: ```python def generate_directory_report(base_path: str) -> None: ``` Input: - `base_path` (str): The path of the directory to analyze. Output: - The function should not return any value. Instead, it should create a file named `directory_report.txt` in the `base_path` directory with the following content: - A hierarchical list of all files and subdirectories. - Counts of files grouped by their extensions (e.g., `.txt`, `.jpg`). Example: For the given directory structure: ``` /path/to/directory |-- file1.txt |-- file2.jpg |-- subdir1 | |-- file3.txt | |-- file4.png ``` The `directory_report.txt` should look like: ``` Directory structure: - file1.txt - file2.jpg - subdir1/ - file3.txt - file4.png File type counts: - .txt: 2 - .jpg: 1 - .png: 1 ``` Constraints: - You should use the `pathlib` module to work with paths. - Do not use external libraries. - Handle large directories efficiently.","solution":"from pathlib import Path def generate_directory_report(base_path: str) -> None: base = Path(base_path) if not base.is_dir(): raise ValueError(f\\"The path {base_path} is not a valid directory.\\") file_types = {} report_lines = [\\"Directory structure:\\"] def traverse_directory(p: Path, depth=0): for item in sorted(p.iterdir()): if item.name.startswith(\'.\'): continue if item.is_dir(): report_lines.append(\\" \\" * depth + f\\"- {item.name}/\\") traverse_directory(item, depth + 1) elif item.is_file(): report_lines.append(\\" \\" * depth + f\\"- {item.name}\\") ext = item.suffix if ext in file_types: file_types[ext] += 1 else: file_types[ext] = 1 traverse_directory(base) report_lines.append(\\"nFile type counts:\\") for ext, count in sorted(file_types.items()): report_lines.append(f\\"- {ext}: {count}\\") report_path = base / \\"directory_report.txt\\" with report_path.open(\\"w\\", encoding=\\"utf-8\\") as f: f.write(\\"n\\".join(report_lines))"},{"question":"# Coding Assignment: Custom Python Compilation Utility **Objective**: Demonstrate your understanding of the `compileall` module by creating a custom function to compile Python source files with specific constraints and options. # Problem Statement You are required to implement a function called `custom_compile` that compiles all Python source files in a given directory (and its subdirectories) with specific options. # Function Signature ```python def custom_compile(directory: str, max_levels: int = 5, force: bool = False, regex_filter: str = None, quiet_level: int = 0, legacy: bool = False, optimize: int = -1, workers: int = 1) -> bool: pass ``` # Parameters: - `directory` (str): The path to the directory that contains Python source files to compile. - `max_levels` (int): Maximum recursion depth for subdirectories. Default is 5. - `force` (bool): If `True`, forces recompilation even if timestamps are up-to-date. Default is `False`. - `regex_filter` (str): A regex pattern to skip files that match. If `None`, all files are considered. Default is `None`. - `quiet_level` (int): Level of output verbosity. `0` (default) prints filenames and info, `1` prints only errors, and `2` suppresses all output. - `legacy` (bool): If `True`, uses legacy byte-code file locations. Default is `False`. - `optimize` (int): Optimization level for the compiler. Default is `-1` (no optimization). - `workers` (int): Number of worker threads to use for compilation. Default is `1`. # Return: - `bool`: Returns `True` if all files were compiled successfully, `False` otherwise. # Constraints: - The function should properly handle edge cases, such as invalid directory paths, invalid regex patterns, and setting of multiple compile options. - Use the `compileall.compile_dir` function internally with the provided parameters. - Consider cross-platform compatibility. # Example Usage: ```python success = custom_compile(directory=\\"/path/to/source\\", max_levels=3, force=True, regex_filter=r\'.*test.*\', quiet_level=1, legacy=True, optimize=2, workers=2) print(\\"Compilation successful:\\", success) ``` # Additional Requirements: 1. You need to handle and catch potential exceptions, providing informative error messages without crashing the function. 2. Write at least two test cases with different sets of parameters to demonstrate the functionality of your `custom_compile` function. # Hints: - Use the `re` module for compiling and using regular expressions. - The `compileall` module\'s `compile_dir` function accepts various parameters that map to the ones in `custom_compile`. - Ensure to validate the input parameters before passing them to `compile_dir`. # Evaluation Criteria: - Correctness: The function should compile the files as per the specified guidelines. - Code Quality: Follow Python coding standards, use meaningful variable names, and include comments where necessary. - Exception Handling: Properly handle and report errors without stopping the execution abruptly. - Test Cases: Provide adequate test cases to demonstrate the correctness and edge-case handling of the function.","solution":"import compileall import os import re def custom_compile(directory: str, max_levels: int = 5, force: bool = False, regex_filter: str = None, quiet_level: int = 0, legacy: bool = False, optimize: int = -1, workers: int = 1) -> bool: Compiles all Python source files in a given directory and its subdirectories based on the given parameters. Parameters: directory (str): Directory path containing Python source files to compile. max_levels (int): Maximum recursion depth. Default is 5. force (bool): Recompile even if timestamps are up-to-date. Default is False. regex_filter (str): Regex pattern to skip files that match. Default is None. quiet_level (int): Output verbosity level. Default is 0. legacy (bool): Use legacy byte-code file locations. Default is False. optimize (int): Optimization level. Default is -1. workers (int): Number of worker threads for compilation. Default is 1. Returns: bool: True if all files were compiled successfully, False otherwise. if not os.path.isdir(directory): raise ValueError(\\"The provided directory path is invalid\\") if regex_filter: try: re.compile(regex_filter) except re.error: raise ValueError(\\"Invalid regex pattern provided for regex_filter\\") return compileall.compile_dir( dir=directory, maxlevels=max_levels, force=force, rx=re.compile(regex_filter) if regex_filter else None, quiet=quiet_level, legacy=legacy, optimize=optimize, workers=workers )"},{"question":"Problem Statement You are designing a system to manage book records for a library. Each book record includes the following attributes: - ISBN (string): A unique identifier for the book. - Title (string): The title of the book. - Author (string): The author of the book. - Year Published (integer): The year the book was published. You need to implement a `Library` class that allows you to perform various operations related to book records including adding, updating, retrieving, and deleting records. The records should be stored persistently using SQLite database, and the operations should handle serialization and deserialization of these records using the `pickle` module. Requirements 1. **Add Book Record:** A method `add_book(self, book: dict)` which takes a dictionary with book details and adds the record to the SQLite database. 2. **Update Book Record:** A method `update_book(self, isbn: str, update_fields: dict)` which updates the specified fields of a book record identified by ISBN. 3. **Retrieve Book Record:** A method `get_book(self, isbn: str) -> dict` which retrieves the book record identified by ISBN and returns it as a dictionary. 4. **Delete Book Record:** A method `delete_book(self, isbn: str)` which deletes the book record identified by ISBN. 5. **Initialize Database:** A method `init_db(self)` to initialize the necessary SQLite database and tables if they do not already exist. Each book record should be stored in serialized form using the `pickle` module to handle any complex data structures within the book records. The records should be stored in an SQLite database with a table named `books`. Input and Output Formats - The `add_book` method takes a dictionary with keys `isbn`, `title`, `author`, and `year_published` and their respective values. - The `update_book` method takes an ISBN and a dictionary of fields to update, where the keys can be `title`, `author`, or `year_published`. - The `get_book` method returns a dictionary with keys `isbn`, `title`, `author`, and `year_published`. - The `delete_book` method removes the record identified by ISBN. Constraints - ISBN is a unique identifier for each book and should not be duplicated in the database. - All methods should handle exceptions gracefully and raise appropriate errors for invalid operations. Example Usage ```python # Example usage of the Library class library = Library() library.init_db() # Initialize the database # Add a book record library.add_book({ \\"isbn\\": \\"978-3-16-148410-0\\", \\"title\\": \\"Example Book\\", \\"author\\": \\"John Doe\\", \\"year_published\\": 2020 }) # Retrieve the book record book = library.get_book(\\"978-3-16-148410-0\\") print(book) # Output: {\'isbn\': \'978-3-16-148410-0\', \'title\': \'Example Book\', \'author\': \'John Doe\', \'year_published\': 2020} # Update the book record library.update_book(\\"978-3-16-148410-0\\", {\\"title\\": \\"Updated Example Book\\"}) # Delete the book record library.delete_book(\\"978-3-16-148410-0\\") ``` Implementation ```python import sqlite3 import pickle class Library: def init_db(self): Initialize the SQLite database and create the books table. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\'\'\'CREATE TABLE IF NOT EXISTS books (isbn TEXT PRIMARY KEY, data BLOB)\'\'\') conn.commit() conn.close() def add_book(self, book): Add a book record to the database. conn = sqlite3.connect(\'library.db\') c = conn.cursor() serialized_book = pickle.dumps(book) c.execute(\\"INSERT INTO books (isbn, data) VALUES (?, ?)\\", (book[\'isbn\'], serialized_book)) conn.commit() conn.close() def update_book(self, isbn, update_fields): Update specified fields of a book record identified by ISBN. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\\"SELECT data FROM books WHERE isbn = ?\\", (isbn,)) result = c.fetchone() if result: book = pickle.loads(result[0]) book.update(update_fields) serialized_book = pickle.dumps(book) c.execute(\\"UPDATE books SET data = ? WHERE isbn = ?\\", (serialized_book, isbn)) conn.commit() conn.close() def get_book(self, isbn): Retrieve a book record identified by ISBN. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\\"SELECT data FROM books WHERE isbn = ?\\", (isbn,)) result = c.fetchone() conn.close() if result: return pickle.loads(result[0]) return None def delete_book(self, isbn): Delete a book record identified by ISBN. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\\"DELETE FROM books WHERE isbn = ?\\", (isbn,)) conn.commit() conn.close() ```","solution":"import sqlite3 import pickle class Library: def init_db(self): Initialize the SQLite database and create the books table. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\'\'\'CREATE TABLE IF NOT EXISTS books (isbn TEXT PRIMARY KEY, data BLOB)\'\'\') conn.commit() conn.close() def add_book(self, book): Add a book record to the database. conn = sqlite3.connect(\'library.db\') c = conn.cursor() serialized_book = pickle.dumps(book) c.execute(\\"INSERT INTO books (isbn, data) VALUES (?, ?)\\", (book[\'isbn\'], serialized_book)) conn.commit() conn.close() def update_book(self, isbn, update_fields): Update specified fields of a book record identified by ISBN. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\\"SELECT data FROM books WHERE isbn = ?\\", (isbn,)) result = c.fetchone() if result: book = pickle.loads(result[0]) book.update(update_fields) serialized_book = pickle.dumps(book) c.execute(\\"UPDATE books SET data = ? WHERE isbn = ?\\", (serialized_book, isbn)) conn.commit() conn.close() def get_book(self, isbn): Retrieve a book record identified by ISBN. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\\"SELECT data FROM books WHERE isbn = ?\\", (isbn,)) result = c.fetchone() conn.close() if result: return pickle.loads(result[0]) return None def delete_book(self, isbn): Delete a book record identified by ISBN. conn = sqlite3.connect(\'library.db\') c = conn.cursor() c.execute(\\"DELETE FROM books WHERE isbn = ?\\", (isbn,)) conn.commit() conn.close()"},{"question":"# Question: Visualization with Custom Palettes You are tasked with creating a function that visualizes a given dataset using custom color palettes from Seaborn. Function Signature ```python def plot_with_custom_palette(data, colormap, num_colors, save_path=None): Plots a bar plot of the provided data using a custom color palette. Parameters: - data (pd.Series): Data to plot, where index represents categories and values represent counts. - colormap (str): The colormap to use (\\"viridis\\", \\"Set2\\", etc.). - num_colors (int): Number of colors to retrieve from the colormap. - save_path (str, optional): Path to save the plot image. If None, does not save the image. Returns: - None: The function should display the plot. ``` Input and Output Format - **Input:** - `data`: A pandas Series object where the index contains category names and the values are their counts. - `colormap`: A string representing the name of the colormap to use, such as \\"viridis\\" or \\"Set2\\". - `num_colors`: An integer specifying the number of colors to retrieve from the colormap. - `save_path`: An optional string specifying the path to save the plot image. If None, the function should simply display the plot without saving. - **Output:** - The function should not return any value. It should generate and display a bar plot using Seaborn with the specified number of colors from the given colormap. If `save_path` is not None, the plot should be saved to the specified path. Constraints - The function should handle both continuous \\"viridis\\"-like colormaps and qualitative \\"Set2\\"-like colormaps. - Ensure that if the colormap does not support the requested number of distinct colors, an informative error is raised. - Use Seaborn\'s `mpl_palette` function for generating the palettes. Example ```python import pandas as pd data = pd.Series({ \'Category A\': 23, \'Category B\': 17, \'Category C\': 35, \'Category D\': 10 }) plot_with_custom_palette(data, \\"Set2\\", 4) ``` **Expected Output:** This will display a bar plot where each bar\'s color is derived from the \\"Set2\\" colormap using 4 distinct colors. # Notes - Make sure to handle cases where the dataset has more categories than the requested number of colors. - Add appropriate labels and titles to the plot for clarity. - Ensure the plot is visually appealing and readable.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def plot_with_custom_palette(data, colormap, num_colors, save_path=None): Plots a bar plot of the provided data using a custom color palette. Parameters: - data (pd.Series): Data to plot, where index represents categories and values represent counts. - colormap (str): The colormap to use (\\"viridis\\", \\"Set2\\", etc.). - num_colors (int): Number of colors to retrieve from the colormap. - save_path (str, optional): Path to save the plot image. If None, does not save the image. Returns: - None: The function should display the plot. # Check if number of unique categories is greater than requested colors if len(data) > num_colors: raise ValueError(\\"Number of unique categories is greater than the requested colors\\") # Generate the custom palette try: palette = sns.color_palette(colormap, num_colors) except ValueError as e: raise ValueError(f\\"Error generating palette: {str(e)}\\") # Create the bar plot plt.figure(figsize=(10, 6)) sns.barplot(x=data.index, y=data.values, palette=palette) plt.title(\'Custom Palette Bar Plot\') plt.xlabel(\'Categories\') plt.ylabel(\'Counts\') # Save the plot if save_path is provided if save_path is not None: plt.savefig(save_path) # Show the plot plt.show()"},{"question":"# Coding Assessment: Custom Web Browser Controller **Objective**: Implement a custom web browser controller using the \\"webbrowser\\" module and demonstrate its use. Task 1. **Define a custom browser type** that opens URLs using a command-line tool such as `wget`. Assume that `wget` is available on the system and can be used to fetch URLs. 2. **Register the custom browser type** using the `webbrowser.register()` function. 3. **Create a function `fetch_url_with_custom_browser(url: str) -> str`** that: - Takes a URL as input. - Uses your custom browser type to fetch the URL. - Returns a message indicating whether the operation was successful or failed. You are free to design additional helper functions as needed to meet the requirements. Input - A URL as a string. Output - A message as a string indicating the result of the operation (e.g., \\"URL fetched successfully\\" or \\"Failed to fetch URL\\"). Constraints - Assume that the environment has the `wget` tool installed and is available in the system\'s PATH. - Do not use any other third-party libraries. Example ```python print(fetch_url_with_custom_browser(\\"https://www.example.com/\\")) # Expected Output: # \\"URL fetched successfully\\" ``` Notes - You should handle possible exceptions and edge cases, such as invalid URLs or issues with the `wget` command. - Ensure that your implementation works consistently across different platforms, where possible. **Instructions**: 1. Implement the custom browser type. 2. Register the custom browser using `webbrowser.register()`. 3. Implement the `fetch_url_with_custom_browser(url: str) -> str` function. Good luck!","solution":"import webbrowser import subprocess import shlex class WgetBrowser: def __init__(self, name=\'wget\'): self.name = name def open(self, url, new=0, autoraise=True): try: command = f\\"wget -q --spider {shlex.quote(url)}\\" subprocess.run(command, shell=True, check=True) return True except subprocess.CalledProcessError: return False # Register the custom browser type with the webbrowser module webbrowser.register(\'wget\', None, WgetBrowser()) def fetch_url_with_custom_browser(url: str) -> str: Fetches the URL using the custom wget browser and returns a success or failed message browser = webbrowser.get(\'wget\') if browser.open(url): return \\"URL fetched successfully\\" else: return \\"Failed to fetch URL\\""},{"question":"Advanced Python Syntax **Objective:** To assess your understanding of advanced Python constructs, comprehensions, and error handling as specified in the Python grammar. **Problem Statement:** You are required to write three functions to demonstrate your grasp of complex Python constructs: `advanced_comprehensions`, `pattern_matching`, and `custom_error_handling`. 1. **Function 1: advanced_comprehensions** Implement the function `advanced_comprehensions()` which returns a list of tuples. Each tuple should contain a number from 1 to 10 and its factorial. Use list comprehensions to achieve this. ```python def advanced_comprehensions(): # Your code here pass # Expected Output: # [(1, 1), (2, 2), (3, 6), (4, 24), (5, 120), (6, 720), (7, 5040), (8, 40320), (9, 362880), (10, 3628800)] ``` 2. **Function 2: pattern_matching** Implement the function `pattern_matching(obj)`. This function should use Python\'s pattern matching feature to determine the type of `obj` and return a specific string based on the match: - If `obj` is an integer, return \\"Integer\\". - If `obj` is a float, return \\"Float\\". - If `obj` is a string, return \\"String\\". - If `obj` is a list, return \\"List\\". - For any other type, return \\"Unknown\\". ```python def pattern_matching(obj): # Your code here pass # Example Usage: # pattern_matching(5) -> \\"Integer\\" # pattern_matching(3.14) -> \\"Float\\" # pattern_matching(\\"hello\\") -> \\"String\\" # pattern_matching([1, 2, 3]) -> \\"List\\" # pattern_matching({}) -> \\"Unknown\\" ``` 3. **Function 3: custom_error_handling** Implement the function `custom_error_handling(dividend, divisor)`. This function should perform safe division and handle the following cases: - Raise a `ZeroDivisionError` with the message \\"Divide by zero is not allowed\\" when `divisor` is zero. - Raise a `TypeError` with the message \\"Both arguments must be numbers\\" when either `dividend` or `divisor` is not a number. - Otherwise, return the result of the division. ```python def custom_error_handling(dividend, divisor): # Your code here pass # Example Usage: # custom_error_handling(10, 2) -> 5.0 # custom_error_handling(10, 0) -> ZeroDivisionError: Divide by zero is not allowed # custom_error_handling(10, \\"a\\") -> TypeError: Both arguments must be numbers ``` **Constraints:** - Do not use any external libraries. - Your solution should be efficient and concise. - Handle all edge cases appropriately. **Performance Requirements:** - The functions should execute efficiently for the specified input ranges. Good luck and happy coding!","solution":"from math import factorial def advanced_comprehensions(): Returns a list of tuples. Each tuple contains a number from 1 to 10 and its factorial. return [(n, factorial(n)) for n in range(1, 11)] def pattern_matching(obj): Determines the type of obj and returns a string based on the match. match obj: case int(): return \\"Integer\\" case float(): return \\"Float\\" case str(): return \\"String\\" case list(): return \\"List\\" case _: return \\"Unknown\\" def custom_error_handling(dividend, divisor): Performs safe division and handles specific error cases. if not isinstance(dividend, (int, float)) or not isinstance(divisor, (int, float)): raise TypeError(\\"Both arguments must be numbers\\") if divisor == 0: raise ZeroDivisionError(\\"Divide by zero is not allowed\\") return dividend / divisor"},{"question":"**Question: Implement and Test a File Parser Using `unittest`** # Objective: Write a class, `FileParser`, that reads a given file and performs some basic operations. Then, write unit tests for this class using the `unittest` module. # Class Specification: 1. **Class Name**: `FileParser` 2. **Methods**: - `__init__(self, file_path: str)`: Initializes the parser with the specified file path. - `read_lines(self) -> list`: Reads all lines from the file and returns them as a list of strings. - `search_word(self, word: str) -> int`: Searches for a word in the file and returns the number of occurrences. - `replace_word(self, old_word: str, new_word: str) -> None`: Replaces all occurrences of `old_word` with `new_word` in the file. # Constraints: - Assume the file contains plain text. - Ensure that methods handle exceptions like file not found gracefully. - The `replace_word` method should save changes back to the file. # Task: 1. Implement the `FileParser` class according to the above specification. 2. Write unit tests for this class using the `unittest` module. - Create a test class named `TestFileParser` that extends `unittest.TestCase`. - Implement the following tests: - `test_read_lines()`: Verify that `read_lines()` returns the correct number of lines. - `test_search_word()`: Verify that `search_word()` counts word occurrences accurately. - `test_replace_word()`: Verify that `replace_word()` correctly replaces the word and persists changes. - `test_file_not_found()`: Verify that appropriate exceptions are raised for non-existent files. # Example: ```python # Example usage of the `FileParser` class parser = FileParser(\'example.txt\') lines = parser.read_lines() word_count = parser.search_word(\'example\') parser.replace_word(\'example\', \'sample\') ``` # Assumptions: - You may use temporary files for testing purposes using the `tempfile` module. - Ensure that the tests do not leave any side effects, i.e., clean up temporary files after use. - Use appropriate assertion methods to verify test outcomes. # Submission: - A Python script implementing the `FileParser` class. - A Python script containing the `TestFileParser` class with the required tests.","solution":"import os class FileParser: def __init__(self, file_path: str): self.file_path = file_path def read_lines(self) -> list: try: with open(self.file_path, \'r\') as file: return file.readlines() except FileNotFoundError: raise FileNotFoundError(f\\"File {self.file_path} not found.\\") def search_word(self, word: str) -> int: try: with open(self.file_path, \'r\') as file: content = file.read() return content.count(word) except FileNotFoundError: raise FileNotFoundError(f\\"File {self.file_path} not found.\\") def replace_word(self, old_word: str, new_word: str) -> None: try: with open(self.file_path, \'r\') as file: content = file.read() updated_content = content.replace(old_word, new_word) with open(self.file_path, \'w\') as file: file.write(updated_content) except FileNotFoundError: raise FileNotFoundError(f\\"File {self.file_path} not found.\\")"},{"question":"# Abstract Base Class Implementation and Inheritance Objective: Your task is to demonstrate your understanding of Python\'s `abc` module by designing and implementing an abstract base class and its concrete subclasses. Problem Statement: 1. Define an abstract base class called `Vehicle` using the `abc` module with the following abstract methods and properties: - `start_engine`: A method that should be responsible for starting the vehicle\'s engine. - `stop_engine`: A method that should be responsible for stopping the vehicle\'s engine. - `is_engine_running`: A property that returns whether the engine is currently running or not. - `vehicle_type`: A property that returns the type of vehicle (e.g., \\"Car\\", \\"Truck\\"). 2. Create a concrete subclass of `Vehicle` called `Car` that: - Implements the `start_engine`, `stop_engine`, and `is_engine_running` methods. - Has a concrete implementation of the `vehicle_type` property that returns the string \\"Car\\". 3. Another concrete subclass of `Vehicle` called `Truck` that: - Implements the `start_engine`, `stop_engine`, and `is_engine_running` methods. - Has a concrete implementation of the `vehicle_type` property that returns the string \\"Truck\\". 4. Write code to instantiate objects of `Car` and `Truck`, demonstrating that the abstract methods and properties are properly implemented and can be used as intended. Constraints: - Use Python version 3.10 or later. - Ensure that attempting to instantiate the `Vehicle` class directly raises an error. - Ensure that all abstract methods and properties are properly implemented in concrete subclasses. Example Usage: ```python car = Car() print(car.vehicle_type) # Output should be \\"Car\\" car.start_engine() print(car.is_engine_running) # Output should be True car.stop_engine() print(car.is_engine_running) # Output should be False truck = Truck() print(truck.vehicle_type) # Output should be \\"Truck\\" truck.start_engine() print(truck.is_engine_running) # Output should be True truck.stop_engine() print(truck.is_engine_running) # Output should be False ``` Expected Output: The program should not allow instantiation of `Vehicle` directly. Instances of `Car` and `Truck` should start and stop the engine correctly and indicate the vehicle type as \\"Car\\" or \\"Truck\\" respectively.","solution":"from abc import ABC, abstractmethod class Vehicle(ABC): @abstractmethod def start_engine(self): pass @abstractmethod def stop_engine(self): pass @property @abstractmethod def is_engine_running(self): pass @property @abstractmethod def vehicle_type(self): pass class Car(Vehicle): def __init__(self): self._engine_running = False def start_engine(self): self._engine_running = True def stop_engine(self): self._engine_running = False @property def is_engine_running(self): return self._engine_running @property def vehicle_type(self): return \\"Car\\" class Truck(Vehicle): def __init__(self): self._engine_running = False def start_engine(self): self._engine_running = True def stop_engine(self): self._engine_running = False @property def is_engine_running(self): return self._engine_running @property def vehicle_type(self): return \\"Truck\\""},{"question":"# Question: Implement a Custom Context Management System Objective: Your task is to implement a custom context management system for user session tracking in a hypothetical web application. Use the functionalities of the `contextvars` module to maintain and manage context-local states effectively. Requirements: 1. **Class Definition:** - Define a class `SessionContext` that will manage user session data such as `user_id` and `token`. 2. **Context Variable Management:** - Within `SessionContext`, create context variables for `user_id` and `token` using `contextvars.ContextVar`. - Implement methods to set and get values for these context variables. - Ensure methods are available to reset the context variables to their previous state using tokens. 3. **Contextual Execution:** - Implement a method `run_with_context` which takes a callable and executes it within a custom context. Ensure all context variables set within this method do not affect the outer/global context. 4. **Function Signature:** ```python class SessionContext: def __init__(self): # Initialize context variables here pass def set_user(self, user_id: int, token: str): # Set new values for context variables here pass def get_user(self) -> dict: # Retrieve current values of context variables here pass def reset_context(self, token_user, token_token): # Reset context variables to the state represented by passed tokens pass def run_with_context(self, func, *args, **kwargs): # Execute a callable within a custom context preserving outer context pass ``` Input/Output and Constraints: - **Input:** user_id (int), token (str). - **Output:** Methods should appropriately return data or None as per context operations. - Constraints: - Context variables should be managed in a way that they do not interfere with the global context. - The `run_with_context` method should preserve the outer context values while allowing modifications within the callable. Examples: ```python # Example usage: # Instantiate the context manager session_context = SessionContext() # Set user data session_context.set_user(1, \\"abc123\\") # Run a code block within a custom context def some_business_logic(): user_data = session_context.get_user() print(f\\"Current User Data: {user_data}\\") session_context.set_user(2, \\"xyz789\\") updated_user_data = session_context.get_user() print(f\\"Updated User Data: {updated_user_data}\\") session_context.run_with_context(some_business_logic) ``` Notes: - Pay attention to handling context variables correctly respecting asynchronous operations. - Use the `contextvars.Token` to manage the state and facilitate the reset capability.","solution":"import contextvars class SessionContext: def __init__(self): self.user_id = contextvars.ContextVar(\'user_id\') self.token = contextvars.ContextVar(\'token\') def set_user(self, user_id: int, token: str): token_user = self.user_id.set(user_id) token_token = self.token.set(token) return token_user, token_token def get_user(self) -> dict: current_user_id = self.user_id.get(None) current_token = self.token.get(None) return {\\"user_id\\": current_user_id, \\"token\\": current_token} def reset_context(self, token_user, token_token): self.user_id.reset(token_user) self.token.reset(token_token) def run_with_context(self, func, *args, **kwargs): context_snapshot = contextvars.copy_context() return context_snapshot.run(func, *args, **kwargs)"},{"question":"You are required to create a custom probability distribution in PyTorch by extending the `torch.distributions.Distribution` class. **Custom Distribution: ScaledExponential** This will be a variation of the existing Exponential distribution but scaled by a factor `c`. The probability density function for the ScaledExponential distribution is defined as: [ f(x; lambda, c) = lambda cdot e^{-lambda cdot (x / c)} ] Where: - (lambda) is the rate parameter. - (c) is the scaling factor. - (x ge 0). # Requirements 1. Define a class `ScaledExponential` that inherits from `torch.distributions.Distribution`. 2. Implement the following methods: - `__init__(self, rate: float, scale: float)`: Initialize with rate (lambda) and scale (c). - `sample(self, sample_shape=torch.Size()`: Generate random samples from the distribution. - `log_prob(self, value: torch.Tensor)`: Compute the log-probability of a given value. - Any other methods required by the base class (like `mean`, `variance`). # Constraints - The `rate` parameter must be positive. - The `scale` parameter must be positive. - Do not use the ready-made `torch.distributions.Exponential` class directly; build your implementation from scratch. # Input - None. This defines a class. # Output Define the `ScaledExponential` class with the specified methods. # Example ```python # Example usage dist = ScaledExponential(rate=1.0, scale=2.0) samples = dist.sample((5,)) log_probs = dist.log_prob(samples) print(\\"Samples:\\", samples) print(\\"Log-Probabilities:\\", log_probs) ``` **Note:** Your samples and log-probabilities outputs may vary due to the stochastic nature of the distribution. Good luck!","solution":"import torch from torch.distributions import Distribution, constraints from torch.distributions.utils import broadcast_all class ScaledExponential(Distribution): arg_constraints = {\'rate\': constraints.positive, \'scale\': constraints.positive} support = constraints.positive def __init__(self, rate: float, scale: float, validate_args=None): self.rate, self.scale = broadcast_all(rate, scale) super(ScaledExponential, self).__init__(self.rate.shape, validate_args=validate_args) def sample(self, sample_shape=torch.Size()): with torch.no_grad(): u = torch.rand(sample_shape + self.rate.shape, dtype=self.rate.dtype, device=self.rate.device) return -torch.log(u) * self.scale / self.rate def log_prob(self, value): if self._validate_args: self._validate_sample(value) log_prob = torch.log(self.rate) - self.rate * (value / self.scale) - torch.log(self.scale) return log_prob @property def mean(self): return self.scale / self.rate @property def variance(self): return (self.scale / self.rate) ** 2 def _validate_sample(self, value): if not value.ge(0).all(): raise ValueError(\\"Sample value should be greater than or equal to zero.\\")"},{"question":"**Objective:** Implement a function that mimics some of the behaviors of low-level file object manipulations in Python. This exercise will assess your understanding of file I/O operations, file descriptors, and exception handling using pure Python. **Task:** 1. Implement the function `create_file_from_fd(fd, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True)`. This function should create a Python file object from an existing file descriptor. 2. Implement the function `get_file_descriptor(file_object)`. This should return the file descriptor associated with a given Python file object. If the object does not support file descriptor access, it should raise a `ValueError`. 3. Implement the function `get_line_from_file(file_object, n=0)`. This should read a line from the given file object: - If `n` is `0`, read one line regardless of its length. - If `n` is positive, read no more than `n` bytes, returning a partial line if necessary. - If `n` is negative, read one line regardless of length, but raise `EOFError` if the end of the file is reached immediately. **Function Signatures:** ```python def create_file_from_fd(fd, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True): Create a Python file object from an existing file descriptor. pass def get_file_descriptor(file_object): Return the file descriptor associated with a file object. pass def get_line_from_file(file_object, n=0): Read a line from the file object according to the given specification. pass ``` **Constraints:** 1. You are not allowed to use the `io` module directly in your implementations. 2. Assume that the file descriptor and file object passed to these functions are valid and correctly opened. 3. Focus on handling exceptions and edge cases gracefully. **Example Usage:** ```python # Assume fd is an existing valid file descriptor file_obj = create_file_from_fd(fd, \'r\', buffering=4096) descriptor = get_file_descriptor(file_obj) print(descriptor) # Should print an integer file descriptor line = get_line_from_file(file_obj, 100) print(line) # Should print the first 100 bytes of a line from the file or fewer if at EOF file_obj.close() ``` **Performance Requirements:** The function should efficiently handle files of reasonable sizes (up to several megabytes) without unnecessary memory consumption or resource leaks. **Testing:** You should write your own tests to ensure the correctness of your implementations. Consider edge cases like reading from empty files, handling invalid file objects, and working with various buffering and encoding settings. **HINT:** To simulate lower-level operations, explore the `os` module for functions that interact with file descriptors and file objects.","solution":"import os def create_file_from_fd(fd, mode, buffering=-1, encoding=None, errors=None, newline=None, closefd=True): Create a Python file object from an existing file descriptor. if not isinstance(fd, int): raise ValueError(\\"File descriptor should be an integer.\\") # Generate the flags if buffering == -1 and encoding is None and errors is None and newline is None and closefd: file_obj = os.fdopen(fd, mode) else: file_obj = os.fdopen(fd, mode, buffering, encoding, errors, newline, closefd) return file_obj def get_file_descriptor(file_object): Return the file descriptor associated with a file object. if not hasattr(file_object, \'fileno\'): raise ValueError(\\"The given object does not support file descriptor access.\\") try: fd = file_object.fileno() except (OSError, ValueError): raise ValueError(\\"The given file object does not have a valid file descriptor.\\") return fd def get_line_from_file(file_object, n=0): Read a line from the file object according to the given specification. if not hasattr(file_object, \'readline\'): raise ValueError(\\"The given object does not support line reading.\\") if n == 0: return file_object.readline() elif n > 0: return file_object.read(n) else: raise EOFError(\\"`n` cannot be negative when trying to read from a file.\\")"},{"question":"# Advanced Python Coroutines and Asynchronous Programming Introduction With the introduction of coroutine objects in Python 3.5, asynchronous programming has become more powerful and flexible. Understanding and implementing coroutines can significantly improve program efficiency, especially in I/O-bound and high-level structured network code. Objective Create a Python program that demonstrates the use of asynchronous programming with coroutines. Task You are tasked with writing a function `fetch_data` that performs the following: 1. Simulates fetching data from multiple URLs asynchronously. 2. Uses Python\'s asyncio library to implement the asynchronous behavior. 3. Aggregates and returns the collected data from all the URLs provided to it. Function Signature ```python import asyncio async def fetch_data(urls: List[str]) -> List[str]: pass ``` Inputs - `urls` (List[str]): A list of strings, where each string is a URL to fetch data from. Outputs - Returns a list of strings, where each string represents the data fetched from the corresponding URL in the input list. Constraints - Use `asyncio.gather` to manage concurrent tasks. - Use `aiohttp` for making asynchronous HTTP requests. - Handle any potential exceptions that might occur during the HTTP requests. Example ```python import asyncio from typing import List async def fetch_data(urls: List[str]) -> List[str]: import aiohttp from aiohttp import ClientSession async def fetch(url: str, session: ClientSession) -> str: try: async with session.get(url) as response: return await response.text() except Exception as e: return str(e) async with aiohttp.ClientSession() as session: tasks = [fetch(url, session) for url in urls] return await asyncio.gather(*tasks) # Example URLs (In actual scenarios, these should be valid URLs) urls = [ \\"http://example.com/data1\\", \\"http://example.com/data2\\", \\"http://example.com/data3\\" ] # Running the asynchronous function if __name__ == \\"__main__\\": results = asyncio.run(fetch_data(urls)) for result in results: print(result) ``` In the example above: - The `fetch_data` function takes a list of URLs and uses aiohttp to fetch data from those URLs asynchronously. - The function handles exceptions by returning the exception message if any request fails. - `asyncio.run` is used to run the main coroutine. This exercise will test your understanding of: - Asynchronous programming in Python. - Using the asyncio and aiohttp libraries. - Handling concurrency and exceptions in coroutines.","solution":"import asyncio from typing import List import aiohttp async def fetch_data(urls: List[str]) -> List[str]: async def fetch(url: str, session: aiohttp.ClientSession) -> str: try: async with session.get(url) as response: return await response.text() except Exception as e: return str(e) async with aiohttp.ClientSession() as session: tasks = [fetch(url, session) for url in urls] return await asyncio.gather(*tasks) # Example URLs (In actual scenarios, these should be valid URLs) urls_example = [ \\"http://example.com/data1\\", \\"http://example.com/data2\\", \\"http://example.com/data3\\" ] # Running the asynchronous function if __name__ == \\"__main__\\": results = asyncio.run(fetch_data(urls_example)) for result in results: print(result)"},{"question":"**Question: Custom Diverging Color Palette with Seaborn** You are to write a function `custom_diverging_palette` that generates a custom diverging color palette using seaborn. The function should accept various parameters to customize the palette and return the corresponding matplotlib colormap object. # Function Signature ```python def custom_diverging_palette(h_neg:int, h_pos:int, center:str=\'light\', as_cmap:bool=True, sep:int=10, s:int=100, l:int=50): pass ``` # Parameters - `h_neg` (int): The hue value for the negative end of the palette. Value should be between 0 and 360. - `h_pos` (int): The hue value for the positive end of the palette. Value should be between 0 and 360. - `center` (str, optional): The center color of the palette. Allowed values are `\'light\'` or `\'dark\'`. Default is `\'light\'`. - `as_cmap` (bool, optional): If True, the function should return a continuous colormap. If False, it should return a discrete palette. Default is True. - `sep` (int, optional): The amount of separation around the center value. Default is 10. - `s` (int, optional): The saturation of the endpoints. Value should be between 0 and 100. Default is 100. - `l` (int, optional): The lightness of the endpoints. Value should be between 0 and 100. Default is 50. # Returns - `cmap`: The matplotlib colormap object representing the custom diverging palette. # Constraints - Ensure the parameters `h_neg`, `h_pos`, `s`, and `l` are within their valid ranges (0-360 for `h_neg` and `h_pos`; 0-100 for `s` and `l`). - Handle any invalid inputs gracefully with appropriate error messages. # Example ```python import matplotlib.pyplot as plt # Example Usage 1: Generate a blue-to-red palette with a dark center. cmap = custom_diverging_palette(240, 20, center=\'dark\', sep=20, s=80, l=40) plt.imshow([list(range(100))], aspect=\'auto\', cmap=cmap) plt.show() # Example Usage 2: Generate a magenta-to-green palette with a light center. cmap = custom_diverging_palette(280, 150, center=\'light\', sep=30, s=50, l=70) plt.imshow([list(range(100))], aspect=\'auto\', cmap=cmap) plt.show() ``` Your task is to implement the `custom_diverging_palette` function. Make sure to use seaborn\'s `diverging_palette` function appropriately to fulfill the requirements.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_diverging_palette(h_neg:int, h_pos:int, center:str=\'light\', as_cmap:bool=True, sep:int=10, s:int=100, l:int=50): Generates a custom diverging color palette using seaborn and returns a matplotlib colormap object. Parameters: - h_neg (int): The hue value for the negative end of the palette. Value should be between 0 and 360. - h_pos (int): The hue value for the positive end of the palette. Value should be between 0 and 360. - center (str, optional): The center color of the palette. Allowed values are \'light\' or \'dark\'. Default is \'light\'. - as_cmap (bool, optional): If True, returns a continuous colormap. If False, returns a discrete palette. Default is True. - sep (int, optional): The amount of separation around the center value. Default is 10. - s (int, optional): The saturation of the endpoints. Value should be between 0 and 100. Default is 100. - l (int, optional): The lightness of the endpoints. Value should be between 0 and 100. Default is 50. Returns: - cmap: The matplotlib colormap object representing the custom diverging palette. # Validate input if not (0 <= h_neg <= 360): raise ValueError(\\"h_neg must be between 0 and 360\\") if not (0 <= h_pos <= 360): raise ValueError(\\"h_pos must be between 0 and 360\\") if center not in [\'light\', \'dark\']: raise ValueError(\\"center must be \'light\' or \'dark\'\\") if not (0 <= s <= 100): raise ValueError(\\"s must be between 0 and 100\\") if not (0 <= l <= 100): raise ValueError(\\"l must be between 0 and 100\\") # Create the diverging palette palette = sns.diverging_palette( h_neg=h_neg, h_pos=h_pos, s=s, l=l, sep=sep, center=center, as_cmap=as_cmap ) return palette"},{"question":"Problem Statement You are given a square matrix `A` and a vector `b`. Your task is to implement a function `solve_linear_system(A, b)` using PyTorch that solves the linear equation `Ax = b` for the vector `x`. You must also verify the solution by interpreting the relevant matrix properties from the `torch.linalg` module. Specifically, your function needs to: 1. **Validate the Input Types and Shapes:** - Ensure that `A` is a 2D tensor and `b` is a 1D tensor. - Both tensors should contain floating-point numbers. - The number of rows in `A` should be equal to the length of `b`. 2. **Compute the Solution:** - Use the `torch.linalg.solve` function to solve the linear equation `Ax = b`. 3. **Verify the Solution:** - Compute the residual vector `r = Ax - b`. - Compute the norm of the residual vector using `torch.linalg.norm`. 4. **Return the Solution and the Residual Norm:** - The solution vector `x`. - The norm of the residual `r`. # Input - `A` (torch.Tensor): A 2D tensor of shape `(n, n)` representing the system\'s coefficient matrix. - `b` (torch.Tensor): A 1D tensor of shape `(n,)` representing the system\'s right-hand side vector. # Output - `x` (torch.Tensor): A 1D tensor of shape `(n,)` representing the solution vector. - `residual_norm` (float): A scalar representing the norm of the residual vector. # Constraints - Assume `A` is a regular (non-singular) matrix. - `A` should be a 2D tensor and `b` a 1D tensor with compatible dimensions. # Example ```python import torch A = torch.tensor([[3., 1.], [1., 2.]]) b = torch.tensor([9., 8.]) x, residual_norm = solve_linear_system(A, b) print(f\\"Solution x: {x}\\") print(f\\"Residual Norm: {residual_norm}\\") ``` Expected Output: ``` Solution x: tensor([2., 3.]) Residual Norm: 0.0 ``` # Implementation ```python import torch def solve_linear_system(A, b): # Validate input types if not (isinstance(A, torch.Tensor) and isinstance(b, torch.Tensor)): raise TypeError(\\"Both A and b must be torch tensors.\\") # Validate shapes if len(A.shape) != 2 or len(b.shape) != 1: raise ValueError(\\"A must be a 2D tensor and b must be a 1D tensor.\\") if A.shape[0] != A.shape[1]: raise ValueError(\\"Matrix A must be square.\\") if A.shape[0] != b.shape[0]: raise ValueError(\\"The number of rows in A must equal the length of b.\\") # Solve the linear system x = torch.linalg.solve(A, b) # Compute the residual r = torch.matmul(A, x) - b # Compute the norm of the residual residual_norm = torch.linalg.norm(r).item() return x, residual_norm ```","solution":"import torch def solve_linear_system(A, b): # Validate input types if not (isinstance(A, torch.Tensor) and isinstance(b, torch.Tensor)): raise TypeError(\\"Both A and b must be torch tensors.\\") # Validate shapes if len(A.shape) != 2 or len(b.shape) != 1: raise ValueError(\\"A must be a 2D tensor and b must be a 1D tensor.\\") if A.shape[0] != A.shape[1]: raise ValueError(\\"Matrix A must be square.\\") if A.shape[0] != b.shape[0]: raise ValueError(\\"The number of rows in A must equal the length of b.\\") # Solve the linear system x = torch.linalg.solve(A, b) # Compute the residual r = torch.matmul(A, x) - b # Compute the norm of the residual residual_norm = torch.linalg.norm(r).item() return x, residual_norm"},{"question":"# Assessment Question **Objective**: Implement a custom sequence class that demonstrates your understanding of Python\'s sequence protocol. **Task**: Write a Python class `CustomSequence` that mimics the behavior of Python list for certain operations. # Class Requirements 1. **Initialization**: - Your `CustomSequence` class should be initialized with an iterable. 2. **Basic Sequence Operations**: - Implement the `__getitem__` method to retrieve items. - Implement the `__len__` method to return the length of the sequence. - Implement the `__contains__` method to check if a value is present in the sequence. - Implement the `__add__` method to concatenate two `CustomSequence` objects. - Implement the `__mul__` method to repeat the sequence a given number of times. 3. **Additional Functionalities**: - Implement a method `count` that returns the number of occurrences of a value in the sequence. - Implement a method `index` that returns the index of the first occurrence of a value in the sequence. # Input and Output Formats - `__init__(self, iterable)`: Initialize the sequence with an iterable, storing it internally. - `__getitem__(self, index) -> Any`: Return the item at the provided `index`. - `__len__(self) -> int`: Return the length of the sequence. - `__contains__(self, value) -> bool`: Return `True` if `value` is in the sequence, otherwise `False`. - `__add__(self, other) -> CustomSequence`: Return a new `CustomSequence` by concatenating `self` and `other`. - `__mul__(self, count) -> CustomSequence`: Return a new `CustomSequence` by repeating `self` `count` times. - `count(self, value) -> int`: Return the number of occurrences of `value` in the sequence. - `index(self, value) -> int`: Return the index of the first occurrence of `value` in the sequence. # Constraints - The input iterable can be any Python iterable like a list, tuple, or string. - The `index` method should raise a `ValueError` if the `value` is not present in the sequence. - The `__getitem__` method should support negative indices. - Ensure that your implementation is efficient in terms of both time and space complexity where applicable. # Example Usage ```python # Initialization custom_seq = CustomSequence([1, 2, 3, 4, 5]) # Get Item print(custom_seq[2]) # Output: 3 # Length print(len(custom_seq)) # Output: 5 # Contains print(3 in custom_seq) # Output: True # Concatenation new_seq = custom_seq + CustomSequence([6, 7]) print(new_seq) # Output: CustomSequence([1, 2, 3, 4, 5, 6, 7]) # Repeat repeated_seq = custom_seq * 2 print(repeated_seq) # Output: CustomSequence([1, 2, 3, 4, 5, 1, 2, 3, 4, 5]) # Count print(custom_seq.count(3)) # Output: 1 # Index print(custom_seq.index(4)) # Output: 3 # Negative Index print(custom_seq[-1]) # Output: 5 ``` Ensure your `CustomSequence` class correctly implements the required methods and handles edge cases appropriately. # Submission Submit your `CustomSequence` class implementation as a `.py` file along with a few test cases demonstrating its usage through various operations.","solution":"class CustomSequence: def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def __contains__(self, value): return value in self._data def __add__(self, other): return CustomSequence(self._data + other._data) def __mul__(self, count): return CustomSequence(self._data * count) def count(self, value): return self._data.count(value) def index(self, value): return self._data.index(value) def __repr__(self): return f\'CustomSequence({self._data})\'"},{"question":"Objective Design a series of Python functions using the `pathlib` module that collectively perform the following tasks: 1. **Path Tree Creation**: Create a tree structure of directories and populate some files. 2. **File Search**: Search for files with a specific extension within a given directory and its subdirectories. 3. **File Content Manipulation**: Read content from a specific file, modify it, and rewrite it. Requirements 1. **Function `create_path_tree(base: Path) -> None`:** - `base`: a `pathlib.Path` object representing the base directory. - Create the following directory tree structure inside `base`: ``` base/ ├── dir1/ │ ├── subdir1/ │ │ └── file1.txt │ └── file2.txt └── dir2/ └── file3.py ``` - Ensure that all directories are created if they do not exist. Write some initial content to each file (e.g., \\"Hello\\" in text files). 2. **Function `search_files_with_extension(base: Path, extension: str) -> List[Path]`:** - `base`: a `pathlib.Path` object representing the base directory. - `extension`: a string representing the file extension to search for (e.g., `.txt` or `.py`). - Return a list of `pathlib.Path` objects representing files from `base` including subdirectories that have the specific `extension`. 3. **Function `read_modify_write(file_path: Path, search: str, replace: str) -> None`** - `file_path`: a `pathlib.Path` object representing the path to the file to be modified. - `search`: a string that represents the text to search for in the file. - `replace`: a string that represents the text to replace in the file. - Read the content from `file_path`, replace all occurrences of `search` with `replace`, and write the modified content back to the file. Constraints - You can assume that `base` is always a valid existing directory. - Handle any necessary exceptions. - Ensure that directory and file permissions are appropriately handled. - Test your functions using appropriate assertions to confirm their correctness. Example Usage: ```python from pathlib import Path # Initialize base path base = Path(\'path_to_base\') # 1. Create path tree create_path_tree(base) # 2. Search files with .txt extension txt_files = search_files_with_extension(base, \'.txt\') print(txt_files) # Should print paths to file1.txt, file2.txt # 3. Read, modify, and write file content file_path = base / \'dir1/subdir1/file1.txt\' read_modify_write(file_path, \'Hello\', \'Hi\') ``` Submission Submit the three functions `create_path_tree`, `search_files_with_extension`, and `read_modify_write`. Ensure they are well-documented and handle edge cases effectively.","solution":"from pathlib import Path import shutil def create_path_tree(base: Path) -> None: Create a directory tree structure inside the base directory and populate it with files. :param base: pathlib.Path object representing the base directory. # Define the directory structure and files structure = { \'dir1\': { \'subdir1\': [\'file1.txt\'], \'\': [\'file2.txt\'] }, \'dir2\': { \'\': [\'file3.py\'] } } # Create directories and files for dir, contents in structure.items(): for subdir, files in contents.items(): dir_path = base / dir / subdir dir_path.mkdir(parents=True, exist_ok=True) for file in files: file_path = dir_path / file file_path.write_text(\\"Hello\\") def search_files_with_extension(base: Path, extension: str): Search for files with a specific extension within a given directory and its subdirectories. :param base: pathlib.Path object representing the base directory. :param extension: string representing the file extension to search for (e.g., \'.txt\' or \'.py\'). :return: List of pathlib.Path objects representing files that have the specified extension. return [file for file in base.rglob(f\'*{extension}\') if file.is_file()] def read_modify_write(file_path: Path, search: str, replace: str) -> None: Read content from a specific file, modify it, and rewrite it. :param file_path: pathlib.Path object representing the path to the file to be modified. :param search: string that represents the text to search for in the file. :param replace: string that represents the text to replace in the file. content = file_path.read_text() modified_content = content.replace(search, replace) file_path.write_text(modified_content)"},{"question":"# Temporary File and Directory Management You are tasked with implementing a function to manage temporary files and directories for a software application that handles sensitive data. The function should create a binary temporary file, write some data into it, read it back, and verify the contents. Also, it should create a temporary directory, place a temporary file in it, and perform a cleanup once done. Function Signature: ```python def manage_temp_files_and_directories(data: bytes) -> bool: pass ``` # Requirements: 1. **Binary Temporary File Handling:** - Create a temporary binary file using `tempfile.TemporaryFile`. - Write the provided binary `data` to this temporary file. - Read the data back from the temporary file. - Verify the read data matches the original data. 2. **Temporary Directory Handling:** - Create a temporary directory using `tempfile.TemporaryDirectory`. - Within this directory, create a temporary file using `tempfile.NamedTemporaryFile`. - Write the original binary `data` to this temporary file. - Verify the directory and the temporary file exist. - The function should perform cleanup automatically, ensuring no temporary files or directories are left on the filesystem after execution. # Input: - `data` (bytes): The binary data to be written to the temporary files. # Output: - `bool`: Return `True` if the data written and read from the temporary files match the original data and if the temporary directory and file are cleaned up successfully. Otherwise, return `False`. # Constraints: - The function should handle binary data up to 1 MB in size efficiently. - Ensure proper resource management with context managers (`with` statement). # Example Usage: ```python data = b\\"Example data for temporary file\\" result = manage_temp_files_and_directories(data) assert result == True ``` In this example, the function will create a temporary file, write \\"Example data for temporary file\\" to it, read it back, and verify the contents. It will also create a temporary directory, a temporary file within it, and perform cleanup. The function will return `True` if all steps are successfully completed and `False` otherwise.","solution":"import tempfile import os def manage_temp_files_and_directories(data: bytes) -> bool: try: # Binary Temporary File Handling with tempfile.TemporaryFile() as temp_file: temp_file.write(data) temp_file.seek(0) read_data = temp_file.read() if read_data != data: return False # Temporary Directory Handling with tempfile.TemporaryDirectory() as temp_dir: temp_file_path = os.path.join(temp_dir, \'tempfile\') with open(temp_file_path, \'wb\') as temp_file_in_dir: temp_file_in_dir.write(data) # Verify the file and directory existence if not os.path.exists(temp_dir) or not os.path.exists(temp_file_path): return False # Read the data back to verify with open(temp_file_path, \'rb\') as temp_file_in_dir_check: read_data_in_dir = temp_file_in_dir_check.read() if read_data_in_dir != data: return False # At this point, the temporary directory and file are automatically cleaned up return True except Exception as e: # If an exception occurs, return False return False"},{"question":"Coding Assessment Question **Title:** Hyper-parameter Optimization with GridSearchCV and RandomizedSearchCV **Objective:** To evaluate the student\'s ability to implement hyper-parameter optimization using GridSearchCV and RandomizedSearchCV in scikit-learn. # Problem Statement: You are provided with a classification dataset and tasked with finding the best hyper-parameters for an SVM (Support Vector Machine) classifier. You will use both exhaustive grid search (`GridSearchCV`) and randomized search (`RandomizedSearchCV`) methods to optimize the hyper-parameters. Write a function `tune_svm_hyperparameters` that performs the following tasks: 1. **Data Loading**: Load the dataset using `sklearn.datasets.load_iris` (for simplicity). 2. **Data Splitting**: Split the dataset into training and testing sets using `train_test_split` with a test size of 30%. 3. **Grid Search**: - Define a parameter grid with `C` values as [0.1, 1, 10, 100] and `kernel` as [\'linear\', \'rbf\']. - Implement `GridSearchCV` to perform an exhaustive search over the parameter grid using a 5-fold cross-validation. - Record the best parameters and the best cross-validation score. 4. **Randomized Search**: - Define a parameter distribution with `C` following a log-uniform distribution between `1e-3` and `1e3`, and `kernel` from [\'linear\', \'rbf\']. - Implement `RandomizedSearchCV` to perform a randomized search over the parameter distribution with `n_iter=10` and a 5-fold cross-validation. - Record the best parameters and the best cross-validation score. 5. **Output**: Return a dictionary with results from both GridSearchCV and RandomizedSearchCV, including the best parameters and cross-validation score. # Function Signature: ```python def tune_svm_hyperparameters(): pass ``` # Example Output: ```python { \\"GridSearchCV\\": { \\"best_params\\": {\\"C\\": 10, \\"kernel\\": \\"rbf\\"}, \\"best_score\\": 0.9866666666666667 }, \\"RandomizedSearchCV\\": { \\"best_params\\": {\\"C\\": 7.125, \\"kernel\\": \\"linear\\"}, \\"best_score\\": 0.94 } } ``` # Constraints: - Use `train_test_split` from `sklearn.model_selection` with a random state of 42. - Use `SVM` (`sklearn.svm.SVC`) as the estimator. - For `RandomizedSearchCV`, use a suitable distribution from `scipy.stats` for `C`. # Notes: - Ensure you use a consistent random state for reproducibility. - Validate the model performance on the test set is not required for this task. **Hints**: - Refer to the scikit-learn documentation for details on `GridSearchCV` and `RandomizedSearchCV`. - Use `scipy.stats.loguniform` for the log-uniform distribution in `RandomizedSearchCV`.","solution":"from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV from sklearn.svm import SVC from scipy.stats import loguniform def tune_svm_hyperparameters(): # Load the dataset data = load_iris() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Grid Search param_grid = { \'C\': [0.1, 1, 10, 100], \'kernel\': [\'linear\', \'rbf\'] } grid_search = GridSearchCV(SVC(), param_grid, cv=5) grid_search.fit(X_train, y_train) grid_results = { \\"best_params\\": grid_search.best_params_, \\"best_score\\": grid_search.best_score_ } # Randomized Search param_dist = { \'C\': loguniform(1e-3, 1e3), \'kernel\': [\'linear\', \'rbf\'] } randomized_search = RandomizedSearchCV(SVC(), param_distributions=param_dist, n_iter=10, cv=5, random_state=42) randomized_search.fit(X_train, y_train) rand_results = { \\"best_params\\": randomized_search.best_params_, \\"best_score\\": randomized_search.best_score_ } # Gather all results results = { \\"GridSearchCV\\": grid_results, \\"RandomizedSearchCV\\": rand_results } return results"},{"question":"**Coding Assessment Question: Text Editor using `curses`** # Objective Develop a simple text editor using the Python `curses` library. This task will test your understanding of window management, text manipulation, and user input handling in a terminal environment. # Requirements 1. **Initialize Screen**: Set up the curses environment and ensure the terminal returns to its normal state upon exiting. 2. **Create Windows**: Create two windows - one for displaying the status bar and another for text editing. 3. **Text Editor Functionality**: - Allow basic text navigation (up, down, left, right). - Enable text input, deletion, and editing. - Display the current cursor position on the status bar. - Save the content to a file when the user presses a specific key combination (e.g., Ctrl+S). - Exit the editor with a specific key combination (e.g., Ctrl+Q). # Constraints - Ensure the application is responsive and handles typical user inputs without crashing. - The editor should handle a reasonable amount of text (up to 1000 lines and 80 characters per line). - Proper error handling should be implemented. # Performance - The application should run smoothly without noticeable lag. Efficient handling of screen updates and text drawing is essential. # Example Here is a skeleton to get you started. You may modify the structure as needed. ```python import curses def main(stdscr): curses.curs_set(1) stdscr.clear() curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLUE) curses.init_pair(2, curses.COLOR_BLACK, curses.COLOR_WHITE) # Create status bar window status_win = curses.newwin(1, curses.COLS, 0, 0) status_win.bkgd(\' \', curses.color_pair(2)) # Create text editor window edit_win = curses.newwin(curses.LINES-1, curses.COLS, 1, 0) edit_win.bkgd(\' \', curses.color_pair(1)) edit_win.keypad(True) # Instructions (hidden in the actual question, shown for reference here) while True: key = edit_win.getch() # Implement navigation, text input, deletion, saving, and exiting functionality if key == curses.KEY_CTRL_Q: break # Update status bar with cursor position y, x = edit_win.getyx() status_win.clear() status_win.addstr(0, 0, f\\"Cursor Position: {y + 1}, {x + 1}\\") status_win.refresh() if __name__ == \\"__main__\\": curses.wrapper(main) ``` # Submission Submit a single Python file that includes your text editor implementation. Ensure that the code is well-commented and follows best practices for readability and maintainability. # Evaluation Your submission will be evaluated based on: - Correctness: The program works as expected and meets all requirements. - Code Quality: The code is clean, well-organized, and follows Python conventions. - Robustness: The program handles edge cases and errors gracefully. - Performance: The application runs smoothly without lag.","solution":"import curses import os def main(stdscr): curses.curs_set(1) stdscr.clear() curses.start_color() curses.init_pair(1, curses.COLOR_WHITE, curses.COLOR_BLUE) curses.init_pair(2, curses.COLOR_BLACK, curses.COLOR_WHITE) curses.init_pair(3, curses.COLOR_BLACK, curses.COLOR_CYAN) status_win = curses.newwin(1, curses.COLS, 0, 0) status_win.bkgd(\' \', curses.color_pair(2)) edit_win = curses.newwin(curses.LINES - 1, curses.COLS, 1, 0) edit_win.bkgd(\' \', curses.color_pair(1)) edit_win.keypad(True) text = [] filepath = \\"output.txt\\" while True: key = edit_win.getch() if key == 24: # Ctrl+X to exit break elif key == 19: # Ctrl+S to save with open(filepath, \'w\') as file: for line in text: file.write(line + \'n\') status_win.addstr(0, 0, f\\"File saved to {os.path.abspath(filepath)}\\") status_win.clrtoeol() status_win.refresh() elif key in [curses.KEY_ENTER, 10, 13]: y, x = edit_win.getyx() text.insert(y, \\"\\") edit_win.move(y + 1, 0) elif key in [curses.KEY_BACKSPACE, 127]: y, x = edit_win.getyx() if x > 0: edit_win.delch(y, x - 1) text[y] = text[y][:x-1] + text[y][x:] else: if y > 0: new_x = len(text[y - 1]) text[y - 1] += text[y] del text[y] edit_win.move(y - 1, new_x) edit_win.deleteln() elif key == curses.KEY_DC: # Delete Key y, x = edit_win.getyx() text[y] = text[y][:x] + text[y][x + 1:] edit_win.delch(y, x) elif key == curses.KEY_LEFT: y, x = edit_win.getyx() if x > 0: edit_win.move(y, x - 1) elif key == curses.KEY_RIGHT: y, x = edit_win.getyx() if x < len(text[y]): edit_win.move(y, x + 1) elif key == curses.KEY_UP: y, x = edit_win.getyx() if y > 0: edit_win.move(y - 1, x if x < len(text[y - 1]) else len(text[y - 1])) elif key == curses.KEY_DOWN: y, x = edit_win.getyx() if y < len(text) - 1: edit_win.move(y + 1, x if x < len(text[y + 1]) else len(text[y + 1])) else: if 0 <= key < 256: y, x = edit_win.getyx() text[y] = text[y][:x] + chr(key) + text[y][x:] edit_win.addch(y, x, key) edit_win.move(y, x + 1) y, x = edit_win.getyx() status_win.clear() status_win.addstr(0, 0, f\\"Cursor Position: {y + 1},{x + 1}, Ctrl+S to save, Ctrl+X to exit\\") status_win.refresh() if __name__ == \\"__main__\\": curses.wrapper(main)"},{"question":"# Asynchronous Chat Server Objective: Implement an asynchronous TCP chat server using the `asyncio` package that can handle multiple clients concurrently. The server must handle receiving and sending messages to all connected clients in real-time. Description: 1. **Server Class (`ChatServer`) Implementation:** - **Attributes:** - **`clients`**: A set to keep track of connected client transports. - **Methods:** - **`start_server()`**: A coroutine to start the server and accept new client connections. - **`handle_client(reader, writer)`**: A coroutine to handle communication with a connected client. - **`broadcast_message(message, sender_writer)`**: A coroutine to send a message to all connected clients except the sender. 2. **Client Handler:** - The server should continuously read data from the client. - Upon receiving a message, broadcast it to all other connected clients. - If a client disconnects, remove it from the list of active clients and close the connection properly. Constraints: - Your server should be capable of handling at least 10 clients concurrently. - Ensure all network interactions are non-blocking. - Implement exception handling to manage connection errors and client disconnections gracefully. Input: - The server should not require any specific input to start. Just implement the necessary coroutines and methods. Output: - The server should print logs (using `print` statements) that include: - New client connections. - Disconnection of clients. - Broadcasted messages. Example Usage: ```python # Start the server server = ChatServer() asyncio.run(server.start_server()) ``` Additional Notes: - You might want to look into the following asyncio functionalities: - `asyncio.start_server()` - `await reader.read()` - `writer.write()` - `await writer.drain()` - `writer.close()` - `await writer.wait_closed()` Performance Requirements: - The server must maintain low-latency message broadcasting among connected clients. Implementation Guidelines: 1. Create the `ChatServer` class with the specified methods. 2. Use asyncio\'s event loop to manage server start-up and shutting down. 3. Implement asynchronous communication between the server and clients efficiently. This task will test your ability to utilize the `asyncio` package for building network servers, managing concurrency, and ensuring efficient, real-time communication between multiple clients.","solution":"import asyncio class ChatServer: def __init__(self): self.clients = set() async def start_server(self, host=\'127.0.0.1\', port=8888): server = await asyncio.start_server(self.handle_client, host, port) addr = server.sockets[0].getsockname() print(f\'Serving on {addr}\') async with server: await server.serve_forever() async def handle_client(self, reader, writer): addr = writer.get_extra_info(\'peername\') print(f\\"New client connected: {addr}\\") self.clients.add(writer) try: while True: data = await reader.read(100) if not data: break message = data.decode() print(f\\"Received message from {addr}: {message}\\") await self.broadcast_message(message, writer) except (asyncio.CancelledError, ConnectionResetError): pass finally: print(f\\"Client disconnected: {addr}\\") self.clients.remove(writer) writer.close() await writer.wait_closed() async def broadcast_message(self, message, sender_writer): for writer in self.clients: if writer != sender_writer: writer.write(message.encode()) await writer.drain() if __name__ == \\"__main__\\": server = ChatServer() asyncio.run(server.start_server())"},{"question":"**Objective:** Implement a function to read and write data to a file using the provided `python310` functions for file objects. Your task is to open an existing file, read its contents line by line, and write those lines to a new file in reverse order. **Function Signature:** ```python def reverse_file_contents(input_fd: int, output_fd: int) -> None: Reads from a file descriptor (input_fd) and writes its content line by line in reverse order to another file descriptor (output_fd). Args: - input_fd: An integer representing the file descriptor of the input file. - output_fd: An integer representing the file descriptor of the output file. Returns: - None ``` **Input:** - `input_fd`: An integer file descriptor for the input file. This file descriptor is guaranteed to be valid. - `output_fd`: An integer file descriptor for the output file. This file descriptor is guaranteed to be valid. **Output:** - The function does not return any value, but it writes to the file associated with `output_fd`. **Constraints:** - You must use the functions described in the `python310` documentation for file object manipulations. - Do not use high-level Python file I/O functions like `open`, `read`, or `write`. - Efficient memory usage is required; avoid reading the entire file into memory if possible. **Example:** Suppose we have a file with the following content: ``` Line 1 Line 2 Line 3 ``` After calling `reverse_file_contents(input_fd, output_fd)`, the output file should contain: ``` Line 3 Line 2 Line 1 ``` **Notes:** - Utilize `PyFile_FromFd` to create a Python file object from the given file descriptor. - Use `PyFile_GetLine` to read lines from the input file. - Use `PyFile_WriteString` or `PyFile_WriteObject` for writing lines to the output file. - Ensure proper error handling and resource management. **Starter Code:** ```python import python310 def reverse_file_contents(input_fd: int, output_fd: int) -> None: input_file = python310.PyFile_FromFd(input_fd, None, \\"r\\", -1, None, None, None, 0) output_file = python310.PyFile_FromFd(output_fd, None, \\"w\\", -1, None, None, None, 0) if input_file is None or output_file is None: raise ValueError(\\"Failed to create file objects from file descriptors.\\") lines = [] while True: line = python310.PyFile_GetLine(input_file, -1) if not line: break lines.append(line) lines.reverse() for line in lines: if python310.PyFile_WriteString(line, output_file) == -1: raise IOError(\\"Failed to write to output file.\\") # Cleanup code if necessary (e.g., closing file objects if required) ```","solution":"import os def reverse_file_contents(input_fd: int, output_fd: int) -> None: Reads from a file descriptor (input_fd) and writes its content line by line in reverse order to another file descriptor (output_fd). Args: - input_fd: An integer representing the file descriptor of the input file. - output_fd: An integer representing the file descriptor of the output file. Returns: - None # Reading the contents of the input file with os.fdopen(input_fd, \'r\') as input_file: lines = input_file.readlines() # Writing the contents to the output file in reverse order with os.fdopen(output_fd, \'w\') as output_file: for line in reversed(lines): output_file.write(line)"},{"question":"As a data analyst working for a wildlife research organization, you have been tasked with visualizing the data from two datasets: \\"penguins\\" and \\"flights\\". Your goal is to create a comprehensive plot that highlights key insights using seaborn. # Requirements: 1. **Loading the Data:** - Load the \\"penguins\\" and \\"flights\\" datasets using seaborn\'s `load_dataset` function. 2. **Plotting Grouped Data with Confidence Intervals:** - Create a point plot showing the average body mass of penguins grouped by island and differentiated by sex. Include confidence intervals for the mean values. 3. **Customizing Plot Appearance:** - Customize the plot to use different markers and linestyles for each sex. Use circles and solid lines for one sex and squares and dashed lines for the other. - Choose a color palette that makes the plot visually appealing and easy to interpret. 4. **Using Error Bars:** - Modify the error bars to represent the standard deviation of each distribution. 5. **Dodge to Reduce Overplotting:** - Create a second plot showing the relationship between sex and bill depth of penguins, differentiated by species. Use dodging to reduce overplotting. 6. **Aggregating Data:** - Load and transform the \\"flights\\" dataset to a wide format where the index is the year, the columns are the months, and the values are the number of passengers. - Create a point plot of the average number of passengers over each month, with error bars representing the 95% confidence interval. # Input and Output Format: - The input does not need to be provided by the user; you should load the datasets within the solution. - The output should be the seaborn plots displayed inline (if using a Jupyter notebook) or saved to a file. # Constraints: - Use seaborn for all plotting tasks. - Ensure the plots are easily interpretable and well-labeled (including titles, axis labels, and legends where necessary). - The solution should not take more than 2 minutes to execute. # Sample Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") # Task 1: Create point plot for penguins dataset (grouped by island and colored by sex) # Task 2: Customize the plot appearance (markers and linestyles) # Task 3: Modify error bars to represent standard deviation # Task 4: Create a second plot showing the relationship between sex and bill depth of penguins, differentiated by species with dodge # Task 5: Transform the flights dataset and create a wide format # Task 6: Create a point plot for the flights dataset with confidence intervals # Display or save the plots plt.show() # or use plt.savefig(\'output.png\') to save the plot to a file ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd # Load datasets penguins = sns.load_dataset(\\"penguins\\") flights = sns.load_dataset(\\"flights\\") def plot_penguin_mass_by_island_and_sex(data): # Create point plot for average body mass grouped by island, differentiated by sex plt.figure(figsize=(10, 6)) sns.pointplot( data=data, x=\\"island\\", y=\\"body_mass_g\\", hue=\\"sex\\", ci=\\"sd\\", # Use standard deviation for error bars dodge=True, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], palette=\\"muted\\" ) plt.title(\\"Average Body Mass of Penguins by Island and Sex with Standard Deviation\\") plt.xlabel(\\"Island\\") plt.ylabel(\\"Body Mass (g)\\") plt.legend(title=\\"Sex\\") plt.show() def plot_bill_depth_by_sex_and_species(data): # Create point plot for bill depth grouped by sex and differentiated by species plt.figure(figsize=(10, 6)) sns.pointplot( data=data, x=\\"species\\", y=\\"bill_depth_mm\\", hue=\\"sex\\", dodge=True, markers=[\\"o\\", \\"s\\"], linestyles=[\\"-\\", \\"--\\"], palette=\\"muted\\" ) plt.title(\\"Bill Depth of Penguins by Sex and Species with Dodging\\") plt.xlabel(\\"Species\\") plt.ylabel(\\"Bill Depth (mm)\\") plt.legend(title=\\"Sex\\") plt.show() def plot_average_passengers_by_month(data): # Transform the flights dataset to wide format df_wide = data.pivot(index=\\"year\\", columns=\\"month\\", values=\\"passengers\\") df_mean = df_wide.mean().reset_index() df_mean.columns = [\\"month\\", \\"average_passengers\\"] # Create point plot for average number of passengers over each month plt.figure(figsize=(10, 6)) sns.pointplot( data=data, x=\\"month\\", y=\\"passengers\\", estimator=\\"mean\\", ci=95, join=False, capsize=0.1, palette=\\"muted\\" ) plt.title(\\"Average Number of Passengers Over Each Month with 95% Confidence Interval\\") plt.xlabel(\\"Month\\") plt.ylabel(\\"Average Number of Passengers\\") plt.show() # Display the plots plot_penguin_mass_by_island_and_sex(penguins) plot_bill_depth_by_sex_and_species(penguins) plot_average_passengers_by_month(flights)"},{"question":"Objective Your task is to write a Python program that demonstrates proficiency in using different operating system services and functionalities. Problem Statement Create a Python script that performs the following steps: 1. **Argument Parsing**: Use the `argparse` module to parse command-line arguments. The script should accept the following arguments: - `--filename`: A required argument specifying the path of the file to be processed. - `--loglevel`: An optional argument specifying the logging level (default should be `INFO`). 2. **File Operations**: Use the `os` and `io` modules to read the contents of the file specified by `--filename`. The file will contain lines of textual data. 3. **Logging Configuration**: Use the `logging` module to set up a logging mechanism based on the `--loglevel` argument. Log relevant information such as the start and end of the script, as well as any significant events like file reading operations. 4. **Time Measurement**: Use the `time` module to measure and log the time taken to read the file. Requirements - The script should handle invalid arguments gracefully and log appropriate error messages using the `logging` module. - If the specified file does not exist, handle the exception and log an error message. - The script should print the contents of the file to the standard output. - The script must log the start and end time of the file reading operation, as well as any exceptions or errors encountered during execution. Input and Output Formats - **Input**: The script will be executed with command-line arguments as described. - **Output**: The contents of the file should be printed on standard output, and log messages should be recorded as specified. Constraints - Ensure your script works across different operating systems. - Use appropriate logging levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`) based on the importance of the message. Example Usage ```sh python script.py --filename example.txt --loglevel DEBUG ``` Example Log Output ``` DEBUG:root:Starting the script INFO:root:Reading file example.txt DEBUG:root:File contents: [contents of the file] INFO:root:Finished reading file in 0.0023 seconds ERROR:root:File not found: example.txt ``` Notes - Ensure to use best practices for exception handling. - The script should be modular, with functions handling specific tasks (e.g., argument parsing, file reading, logging setup). Submission Submit a single Python file containing your implementation.","solution":"import argparse import os import io import logging import time def parse_arguments(): parser = argparse.ArgumentParser(description=\\"Process a file and log events.\\") parser.add_argument(\'--filename\', required=True, help=\'Path of the file to be processed.\') parser.add_argument(\'--loglevel\', default=\'INFO\', help=\'Logging level (default: INFO).\') return parser.parse_args() def setup_logging(level): logging.basicConfig(level=getattr(logging, level.upper(), None), format=\'%(levelname)s:%(message)s\') logging.debug(\'Logging initialized at level: %s\', level) def read_file(filepath): if not os.path.isfile(filepath): logging.error(\'File not found: %s\', filepath) return None try: with io.open(filepath, \'r\', encoding=\'utf-8\') as file: contents = file.read() logging.info(\'Successfully read file: %s\', filepath) return contents except Exception as e: logging.error(\'Error reading file %s: %s\', filepath, str(e)) return None def main(): args = parse_arguments() setup_logging(args.loglevel) logging.debug(\'Starting the script\') start_time = time.time() contents = read_file(args.filename) if contents: print(contents) elapsed_time = time.time() - start_time logging.info(\'Finished reading file in %.4f seconds\', elapsed_time) logging.debug(\'Ending the script\') if __name__ == \'__main__\': main()"},{"question":"# Python Coding Assessment **Objective:** Demonstrate proficiency with the `imaplib` module for managing email data via the IMAP protocol. **Problem Statement:** Create a Python function `fetch_unread_emails` that connects to an IMAP email server using the `imaplib.IMAP4_SSL` class, authenticates the user, selects the inbox, searches for unread emails, retrieves the headers for these emails, and returns a list of these headers. # Function Signature: ```python def fetch_unread_emails(username: str, password: str, imap_server: str, imap_port: int = 993) -> list: pass ``` # Parameters: - `username` (str): The email account\'s username. - `password` (str): The email account\'s password. - `imap_server` (str): The IMAP server address. - `imap_port` (int, optional): The IMAP server port, default is 993 for SSL. # Returns: - `list`: A list of email headers for unread emails, where each header is represented as a dictionary with keys such as `From`, `To`, `Subject`, and `Date`. # Example: ```python email_headers = fetch_unread_emails(\\"example@example.com\\", \\"password123\\", \\"imap.example.com\\") for header in email_headers: print(header) ``` # Constraints: 1. Use the `imaplib.IMAP4_SSL` class to establish a secure connection. 2. Use the login credentials to authenticate. 3. Select the \\"INBOX\\" mailbox. 4. Search for unread emails using the appropriate search criteria. 5. Fetch and parse the headers of unread emails. 6. Properly handle and close the connection in case of errors. 7. Ensure to logout from the server once all operations are completed. # Guidelines: - Handle exceptions gracefully, providing informative messages in case of connection or login failure. - Optimize for readability and maintainability. - Provide well-documented code and include comments where necessary. **Notes:** - The `fetch_unread_emails` function should handle any server communication errors and ensure the IMAP connection is securely closed. - Utilize Python\'s built-in `email` module (specifically `email.parser`) for parsing email headers. - Always use secure practices for handling credentials and sensitive information. Good luck!","solution":"import imaplib import email from email.parser import HeaderParser def fetch_unread_emails(username: str, password: str, imap_server: str, imap_port: int = 993) -> list: Connect to an IMAP email server and fetch unread email headers. Args: - username (str): The email account\'s username. - password (str): The email account\'s password. - imap_server (str): The IMAP server address. - imap_port (int, optional): The IMAP server port, default is 993 for SSL. Returns: - list: A list of dictionaries representing unread email headers. try: # Connect to the IMAP server mail = imaplib.IMAP4_SSL(imap_server, imap_port) # Login to the account mail.login(username, password) # Select the \'inbox\' mail.select(\\"inbox\\") # Search for unread emails status, messages = mail.search(None, \'UNSEEN\') if status != \\"OK\\": raise Exception(\\"Could not search unread emails.\\") email_ids = messages[0].split() headers = [] parser = HeaderParser() for email_id in email_ids: status, msg_data = mail.fetch(email_id, \\"(BODY[HEADER])\\") if status != \\"OK\\": raise Exception(f\\"Could not fetch email id {email_id}.\\") for response_part in msg_data: if isinstance(response_part, tuple): msg = parser.parsestr(response_part[1].decode(\'utf-8\')) email_header = { \\"From\\": msg[\\"From\\"], \\"To\\": msg[\\"To\\"], \\"Subject\\": msg[\\"Subject\\"], \\"Date\\": msg[\\"Date\\"], } headers.append(email_header) # Logout and close the connection mail.logout() return headers except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"You are working with GPU-accelerated matrix operations in PyTorch and need to optimize the performance of a General Matrix Multiply (GEMM) operation using tunable parameters provided by the `torch.cuda.tunable` module. Your task is to implement a function that sets up tuning parameters, performs the GEMM operation, and stores the tuning results. # Function Signature ```python def optimize_gemm(matrix_a: torch.Tensor, matrix_b: torch.Tensor, tuning_duration: int, tuning_iterations: int, output_filename: str) -> torch.Tensor: Performs a GEMM operation between two matrices with specified tuning parameters and stores the tuning results to a file. Parameters: - matrix_a (torch.Tensor): First input matrix (2D tensor) for the GEMM operation. - matrix_b (torch.Tensor): Second input matrix (2D tensor) for the GEMM operation. - tuning_duration (int): Maximum duration (in seconds) allowed for tuning the GEMM operation. - tuning_iterations (int): Maximum number of iterations allowed for tuning the GEMM operation. - output_filename (str): Filename where the tuning results should be stored. Returns: - torch.Tensor: The result of the GEMM operation. ``` # Requirements 1. Ensure that the `torch.cuda.tunable` module is enabled before tuning. 2. Set the provided tuning duration and iterations. 3. Perform the GEMM operation between `matrix_a` and `matrix_b`. 4. Store the tuning results in the specified file. # Constraints - `matrix_a` and `matrix_b` will be compatible for matrix multiplication (i.e., the number of columns in `matrix_a` will match the number of rows in `matrix_b`). - `tuning_duration` and `tuning_iterations` will be positive integers. - The GEMM operation needs to utilize PyTorch\'s `torch.matmul` for matrix multiplication. - The function should leverage the settings and functions provided by the `torch.cuda.tunable` module. # Example Usage ```python import torch matrix_a = torch.rand(100, 200, device=\'cuda\') matrix_b = torch.rand(200, 300, device=\'cuda\') tuning_duration = 10 tuning_iterations = 100 output_filename = \'tuning_results.txt\' result = optimize_gemm(matrix_a, matrix_b, tuning_duration, tuning_iterations, output_filename) print(result) ``` # Hints - Use `torch.cuda.tunable.enable()` to enable the tunable mode. - Use `torch.cuda.tunable.set_max_tuning_duration()` and `torch.cuda.tunable.set_max_tuning_iterations()` to set the tuning parameters. - Use `torch.cuda.tunable.write_file(output_filename)` to store the tuning results.","solution":"import torch import os def optimize_gemm(matrix_a: torch.Tensor, matrix_b: torch.Tensor, tuning_duration: int, tuning_iterations: int, output_filename: str) -> torch.Tensor: Performs a GEMM operation between two matrices with specified tuning parameters and stores the tuning results to a file. Parameters: - matrix_a (torch.Tensor): First input matrix (2D tensor) for the GEMM operation. - matrix_b (torch.Tensor): Second input matrix (2D tensor) for the GEMM operation. - tuning_duration (int): Maximum duration (in seconds) allowed for tuning the GEMM operation. - tuning_iterations (int): Maximum number of iterations allowed for tuning the GEMM operation. - output_filename (str): Filename where the tuning results should be stored. Returns: - torch.Tensor: The result of the GEMM operation. # Enable tunable mode torch.cuda.tunable.enable() # Set tuning parameters torch.cuda.tunable.set_max_tuning_duration(tuning_duration) torch.cuda.tunable.set_max_tuning_iterations(tuning_iterations) # Perform the GEMM operation result = torch.matmul(matrix_a, matrix_b) # Store the tuning results torch.cuda.tunable.write_file(output_filename) return result"},{"question":"# Advanced Python Class Implementation You are required to implement a custom class in Python that leverages special methods to exhibit advanced behaviors, particularly focusing on context management, custom attribute access, and container emulation. Problem Statement: Create a class `DataContainer` that: 1. Acts as a context manager. 2. Mimics the behavior of a list. 3. Provides custom behavior for setting, getting, and deleting attributes via descriptors. Specifications: 1. **Context Management:** - The class should support the \'with\' statement. On entering the context, it should print the message \\"Entering context\\". On exiting the context, it should print \\"Exiting context\\". 2. **List Behavior:** - The class should support the ability to operate as a list, with methods to append, get item by index, set item by index, delete item by index, get length, and iterate over items. 3. **Custom Attribute Access:** - Use descriptors to manage attribute access. - Implement three attributes: `name`, `value`, and `status`. - Ensure that these attributes are private and provide controlled access through descriptor methods. Input/Output formats: - Context Management: - ```python with DataContainer() as dc: # Code block # Output: Entering context # Exiting context ``` - List Behavior: - ```python dc = DataContainer() dc.append(10) dc.append(20) print(dc[0]) # Output: 10 print(dc[1]) # Output: 20 dc[1] = 30 print(dc[1]) # Output: 30 del dc[0] print(len(dc)) # Output: 1 for item in dc: print(item) # Output: 30 ``` - Custom Attribute Access: - ```python dc = DataContainer() dc.name = \\"Container\\" dc.value = 100 dc.status = \\"active\\" print(dc.name) # Output: Container print(dc.value) # Output: 100 print(dc.status) # Output: active del dc.name print(dc.name) # Output: AttributeError ``` Detailed Implementation: 1. **Context Management:** - Implement `__enter__` and `__exit__` methods. 2. **List Behavior:** - Implement methods: `__getitem__`, `__setitem__`, `__delitem__`, `__len__`, `__iter__`, and an `append` method. 3. **Descriptors for Attribute Management:** - Define a descriptor class `ManagedAttribute` to control setting, getting, and deleting attributes. Constraints: - Ensure the class passes all the specified behaviors. - Use appropriate error handling for invalid operations. # Implementation ```python class ManagedAttribute: def __init__(self): self.private_name = \'_name\' self.private_value = \'_value\' self.private_status = \'_status\' def __get__(self, instance, owner): if instance is None: return self return getattr(instance, self.private_name) def __set__(self, instance, value): setattr(instance, self.private_name, value) def __delete__(self, instance): raise AttributeError(\\"Cannot delete attribute\\") class DataContainer: name = ManagedAttribute() value = ManagedAttribute() status = ManagedAttribute() def __init__(self): self.data = [] def __enter__(self): print(\\"Entering context\\") return self def __exit__(self, exc_type, exc_val, exc_tb): print(\\"Exiting context\\") def append(self, item): self.data.append(item) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def __iter__(self): return iter(self.data) ```","solution":"class ManagedAttribute: def __init__(self, private_name): self.private_name = private_name def __get__(self, instance, owner): return getattr(instance, self.private_name) def __set__(self, instance, value): setattr(instance, self.private_name, value) def __delete__(self, instance): raise AttributeError(\\"Cannot delete attribute\\") class DataContainer: name = ManagedAttribute(\'_name\') value = ManagedAttribute(\'_value\') status = ManagedAttribute(\'_status\') def __init__(self): self._name = None self._value = None self._status = None self.data = [] def __enter__(self): print(\\"Entering context\\") return self def __exit__(self, exc_type, exc_val, exc_tb): print(\\"Exiting context\\") def append(self, item): self.data.append(item) def __getitem__(self, index): return self.data[index] def __setitem__(self, index, value): self.data[index] = value def __delitem__(self, index): del self.data[index] def __len__(self): return len(self.data) def __iter__(self): return iter(self.data)"},{"question":"# Advanced Seaborn Assessment **Objective:** To assess your understanding of the seaborn library, particularly its advanced plotting capabilities using the `seaborn.objects` module. **Problem Statement:** You are given a dataset `titanic` which contains data about passengers on the Titanic. Your task is to create a comprehensive visualization using seaborn\'s objects interface. This visualization should reveal insights about the distribution and relationship of several attributes in the dataset. **Dataset:** The `titanic` dataset contains the following columns: - `survived`: Whether the passenger survived (0 = No, 1 = Yes). - `pclass`: Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd). - `sex`: Sex of the passenger. - `age`: Age of the passenger. - `sibsp`: Number of siblings/spouses aboard the Titanic. - `parch`: Number of parents/children aboard the Titanic. - `fare`: Passenger fare. - `embarked`: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton). **Tasks:** 1. **Data Loading:** Load the `titanic` dataset using seaborn\'s `load_dataset` function. 2. **Plotting:** Create a plot showcasing the relationship between `survived` and `fare`, differing by `pclass`. Incorporate multiple layers and transformations to make the plot informative and clear. - Use `Dots` mark to show individual data points. - Apply `Jitter` to make overlapping points distinguishable. - Add a `Range` layer to show the Interquartile Range (IQR) of fares for each survival class. - Use `Shift` to adjust positions to avoid overlap. - Annotate the plot with explanations of your design choices. 3. **Interpretation:** Provide a brief interpretation of the plot, highlighting any significant insights or patterns observed. **Constraints:** - Your plot should be clear and easy to interpret. - Ensure that code is well-organized and commented. - The entire solution, including plot and interpretation, should fit within a single code cell. **Example Structure:** ```python import seaborn.objects as so from seaborn import load_dataset # Load the dataset titanic = load_dataset(\\"titanic\\") # Create the plot plot = ( so.Plot(titanic, \\"survived\\", \\"fare\\", color=\\"pclass\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Show the plot plot.show() # Interpretation # Explain the insights from the plot here. ``` **Expected Output:** - A well-constructed plot with the specified layers. - Clear and concise explanations of your plot design choices. - A thoughtful interpretation of the visualized data. Good luck!","solution":"import seaborn.objects as so from seaborn import load_dataset # Load the dataset titanic = load_dataset(\\"titanic\\") # Create the plot plot = ( so.Plot(titanic, x=\\"survived\\", y=\\"fare\\", color=\\"pclass\\") .add(so.Dots(), so.Jitter()) .add(so.Range(), so.Perc([25, 75]), so.Shift(x=0.2)) ) # Show the plot plot.show() # Interpretation: # The plot shows the distribution of fare prices among passengers who survived and those who did not, # stratified by passenger class. The dots represent individual passengers and are jittered to avoid # overlap, making it easier to distinguish between points. The range layer illustrates the # interquartile range (IQR) of fares, which provides a summary of the middle 50% of the data. # From the plot, it is observed that passengers in higher classes tend to have higher fares. # Moreover, it appears that those who survived generally had higher fare values compared to # those who did not, especially in the first class."},{"question":"The `python310` package provides low-level functions for importing and managing modules. Your task is to implement a function that takes the name of a module, imports it, executes a given Python code string within the context of this module, and returns the result of the execution. The function should be capable of handling submodules and packages. # Function Signature ```python def import_and_execute(module_name: str, code: str) -> Any: pass ``` # Input - `module_name` (str): The name of the module to be imported. - `code` (str): The Python code to be executed within the context of the imported module. # Output - The result of executing the provided code string within the imported module\'s context. # Constraints - The module specified by `module_name` will always exist and be importable within the environment. - The `code` string is guaranteed to be valid Python code. - The code execution should respect the module\'s namespace. # Performance Requirements - The import operation should be efficient and should use the mechanisms available in the `python310` package. - The execution of the code should correctly reflect the state of the module\'s namespace before and after execution. # Example ```python result = import_and_execute(\'math\', \'sqrt(16)\') print(result) # Output: 4.0 result = import_and_execute(\'os.path\', \'basename(\\"/usr/bin/python\\")\') print(result) # Output: \'python\' ``` # Additional Information - You should use the functions provided in the `python310` package for importing modules. - Consider how to handle both top-level modules and submodules/packages in your implementation. - Ensure that any side effects of the code execution are contained within the module\'s context.","solution":"import importlib def import_and_execute(module_name: str, code: str): Imports the given module and executes the provided code string within the context of the imported module. Returns the result of the code execution. Parameters: module_name (str): The name of the module to be imported. code (str): The Python code to be executed within the context of the imported module. Returns: The result of executing the provided code. # Import the module given by module_name module = importlib.import_module(module_name) # Execute the given code within the context of the imported module result = eval(code, module.__dict__) return result"},{"question":"**Question: Time Zone Event Scheduler** You are tasked with building a small utility to schedule events in different time zones and handle daylight saving time transitions. You must implement a function that takes a list of events with their corresponding time zones and adjusts the event times according to a new time zone. Additionally, ensure that the changes respect any daylight saving time transitions. # Function Signature ```python def adjust_event_times(events: list, new_tz: str) -> list: Adjust event times based on the new time zone provided and handle daylight saving time transitions. Args: events (list): A list of dictionaries, each containing: - \'datetime\' (str): a date-time string in ISO 8601 format. - \'timezone\' (str): a string representing the event\'s time zone. new_tz (str): The new time zone to which all event times should be converted. Returns: list: A list of dictionaries with \'datetime\' and \'timezone\' keys representing the adjusted event times. ``` # Input - `events`: A list of dictionaries, with each dictionary containing: - `\'datetime\'`: A date-time string in ISO 8601 format (e.g., \\"2023-03-10T10:00:00\\"). - `\'timezone\'`: A string representing the event\'s time zone (e.g., \\"America/New_York\\"). - `new_tz`: A string representing the new time zone to which all event times should be converted. (e.g., \\"Europe/Berlin\\"). # Output - A list of dictionaries, each containing: - `\'datetime\'`: The adjusted date-time string in ISO 8601 format according to the new time zone. - `\'timezone\'`: The new time zone. # Constraints - If an event\'s timezone is invalid, raise a `zoneinfo.ZoneInfoNotFoundError`. - Utilize the system time zone data or fallback to `tzdata` if necessary. - Handle any potential daylight saving time transitions correctly. # Example Usage ```python events = [ {\\"datetime\\": \\"2023-03-10T10:00:00\\", \\"timezone\\": \\"America/New_York\\"}, {\\"datetime\\": \\"2023-03-10T15:00:00\\", \\"timezone\\": \\"Europe/London\\"} ] new_tz = \\"Asia/Tokyo\\" adjusted_events = adjust_event_times(events, new_tz) print(adjusted_events) # Output: # [ # {\\"datetime\\": \\"2023-03-11T00:00:00+09:00\\", \\"timezone\\": \\"Asia/Tokyo\\"}, # {\\"datetime\\": \\"2023-03-11T00:00:00+09:00\\", \\"timezone\\": \\"Asia/Tokyo\\"} # ] ``` **Notes:** - Ensure that you correctly parse and manipulate date-time strings. - You may use the `datetime` and `zoneinfo` modules from Python\'s standard library. - Consider edge cases like invalid time zones, ambiguous times during daylight saving transitions, and the absence of `tzdata`.","solution":"from datetime import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def adjust_event_times(events: list, new_tz: str) -> list: Adjust event times based on the new time zone provided and handle daylight saving time transitions. Args: events (list): A list of dictionaries, each containing: - \'datetime\' (str): a date-time string in ISO 8601 format. - \'timezone\' (str): a string representing the event\'s time zone. new_tz (str): The new time zone to which all event times should be converted. Returns: list: A list of dictionaries with \'datetime\' and \'timezone\' keys representing the adjusted event times. adjusted_events = [] try: target_tz = ZoneInfo(new_tz) except ZoneInfoNotFoundError: raise ZoneInfoNotFoundError(f\\"Invalid time zone: {new_tz}\\") for event in events: try: event_tz = ZoneInfo(event[\'timezone\']) except ZoneInfoNotFoundError: raise ZoneInfoNotFoundError(f\\"Invalid time zone: {event[\'timezone\']}\\") dt = datetime.fromisoformat(event[\'datetime\']).replace(tzinfo=event_tz) adjusted_dt = dt.astimezone(target_tz) adjusted_events.append({ \\"datetime\\": adjusted_dt.isoformat(), \\"timezone\\": new_tz }) return adjusted_events"},{"question":"Objective Your task is to implement a secure login function that prompts the user for their username and password. The function should verify these credentials against a predefined set of username-password pairs. Function Signature ```python def secure_login(user_dict: dict) -> bool: Prompts the user for their username and password, and verifies these credentials against the given user_dict. Parameters: user_dict (dict): A dictionary where keys are usernames and values are corresponding passwords. Returns: bool: True if the credentials are correct, otherwise False. ``` # Instructions 1. **Prompt for Username**: Use the `getpass.getuser()` to retrieve the default system login name. 2. **Prompt for Password**: Use `getpass.getpass(prompt=\'Password: \')` to securely get the password input from the user without echoing it. 3. **Validate Credentials**: Check the provided username and password against the `user_dict`. If both match, the login is successful. 4. **Return Result**: - Return `True` if the credentials match the entries in `user_dict`. - Return `False` if they do not match. # Input - `user_dict`: A dictionary where keys are valid usernames (strings) and values are corresponding valid passwords (strings). # Output - A boolean value: `True` if the provided username and password are correct, otherwise `False`. # Constraints - The username and passwords are case-sensitive. - Assume `user_dict` always contains valid and non-empty username and password pairs. # Example ```python import getpass def secure_login(user_dict): username = getpass.getuser() password = getpass.getpass(prompt=\\"Password: \\") if username in user_dict and user_dict[username] == password: return True else: return False # Example usage: user_dict = { \'user1\': \'password123\', \'user2\': \'mypassword\' } # Assuming the system login name is \'user1\' and the user provides \'password123\' as the password print(secure_login(user_dict)) # Output: True ``` # Notes - To run this example, you will need to run it in a terminal or a compatible environment where `getpass` works as expected. - Consider edge cases such as when the username is not present in `user_dict` or the password does not match.","solution":"import getpass def secure_login(user_dict): Prompts the user for their username and password, and verifies these credentials against the given user_dict. Parameters: user_dict (dict): A dictionary where keys are usernames and values are corresponding passwords. Returns: bool: True if the credentials are correct, otherwise False. username = getpass.getuser() # Retrieve the default system login name password = getpass.getpass(prompt=\\"Password: \\") # Securely get the password input if username in user_dict and user_dict[username] == password: return True else: return False"},{"question":"# Question: Pandas Data Analysis and Visualization You have been provided with a CSV file containing sales data for different products. Your task is to write a Python function that reads this CSV file into a pandas DataFrame, performs specific data manipulation and analysis tasks, and outputs the results in a specified format. Additionally, you need to generate a plot to visualize part of the data. Input The CSV file `sales_data.csv` has the following columns: - `Date`: The date of the sale (formatted as `YYYY-MM-DD`). - `Product`: The name of the product sold. - `Category`: The category to which the product belongs. - `Quantity`: The number of units sold. - `Price`: The price per unit of the product. Function Signature ```python def analyze_sales_data(file_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, plt.Figure]: pass ``` Requirements 1. **Read the CSV file**: - Load the CSV file into a pandas DataFrame. 2. **Data Cleaning**: - Ensure there are no missing values. If there are any missing values, fill them with appropriate default values (e.g., 0 for numeric columns). 3. **Data Aggregation**: - Group the data by `Category` and `Product` and calculate the total sales (`Quantity * Price`) for each combination. 4. **Top Products**: - For each `Category`, determine the top-selling product based on total sales. Create a DataFrame that includes the `Category`, `Product`, and `Total Sales` of the top product. 5. **Monthly Summary**: - Resample the data to provide a monthly summary of total sales (sum of `Quantity * Price`) for each month in the dataset. 6. **Visualization**: - Generate a bar plot showing the total sales for each category. Each bar should represent a category, and its height should represent the sum of total sales for that category. Output - A **DataFrame** containing the total sales for each combination of `Category` and `Product`. - A **DataFrame** containing the top-selling product for each category. - A **Figure** object representing the bar plot of total sales for each category. Example ```python import pandas as pd import matplotlib.pyplot as plt def analyze_sales_data(file_path: str): # Read the CSV file df = pd.read_csv(file_path) # Data Cleaning df.fillna({\'Quantity\': 0, \'Price\': 0}, inplace=True) # Data Aggregation df[\'Total Sales\'] = df[\'Quantity\'] * df[\'Price\'] total_sales_df = df.groupby([\'Category\', \'Product\']).agg({\'Total Sales\': \'sum\'}).reset_index() # Top Products top_products_df = total_sales_df.loc[total_sales_df.groupby(\'Category\')[\'Total Sales\'].idxmax()].reset_index(drop=True) # Monthly Summary df[\'Date\'] = pd.to_datetime(df[\'Date\']) monthly_summary = df.resample(\'M\', on=\'Date\').agg({\'Total Sales\': \'sum\'}).reset_index() # Visualization category_sales = df.groupby(\'Category\').agg({\'Total Sales\': \'sum\'}).reset_index() fig, ax = plt.subplots() ax.bar(category_sales[\'Category\'], category_sales[\'Total Sales\']) ax.set_xlabel(\'Category\') ax.set_ylabel(\'Total Sales\') ax.set_title(\'Total Sales by Category\') return total_sales_df, top_products_df, fig # Example usage: total_sales_df, top_products_df, fig = analyze_sales_data(\'sales_data.csv\') print(total_sales_df) print(top_products_df) fig.show() ``` Constraints - Ensure the function runs efficiently on large datasets. - Use appropriate pandas functions and methods to achieve the tasks. - The plot should be clear and well-labeled.","solution":"import pandas as pd import matplotlib.pyplot as plt from typing import Tuple def analyze_sales_data(file_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, plt.Figure]: # Read the CSV file df = pd.read_csv(file_path) # Data Cleaning df.fillna({\'Quantity\': 0, \'Price\': 0}, inplace=True) # Calculate Total Sales for each row df[\'Total Sales\'] = df[\'Quantity\'] * df[\'Price\'] # Data Aggregation total_sales_df = df.groupby([\'Category\', \'Product\']).agg({\'Total Sales\': \'sum\'}).reset_index() # Determine Top Products for each Category top_products_df = total_sales_df.loc[total_sales_df.groupby(\'Category\')[\'Total Sales\'].idxmax()].reset_index(drop=True) # Monthly Summary df[\'Date\'] = pd.to_datetime(df[\'Date\']) monthly_summary = df.resample(\'M\', on=\'Date\').agg({\'Total Sales\': \'sum\'}).reset_index() # Visualization category_sales = df.groupby(\'Category\').agg({\'Total Sales\': \'sum\'}).reset_index() fig, ax = plt.subplots() ax.bar(category_sales[\'Category\'], category_sales[\'Total Sales\']) ax.set_xlabel(\'Category\') ax.set_ylabel(\'Total Sales\') ax.set_title(\'Total Sales by Category\') return total_sales_df, top_products_df, fig # Example usage: # total_sales_df, top_products_df, fig = analyze_sales_data(\'sales_data.csv\') # print(total_sales_df) # print(top_products_df) # fig.show()"},{"question":"Advanced Date-Time Manipulations **Objective:** The task is to implement a Python function that checks the validity of a given date and time, constructs `datetime` objects, and returns a formatted string representation, incorporating timezone information. **Problem Statement:** You must implement the function `process_datetime(input_date)` satisfying the following conditions: 1. **Input:** - `input_date` (str): A string in the format `\\"YYYY-MM-DD HH:MM:SS\\"`. 2. **Output:** - Returns a string in the format `\\"Date: YYYY-MM-DD Time: HH:MM:SS [UTC+00:00]\\"` for valid inputs. - If the input date or time is invalid, raise a `ValueError` with the message `\\"Invalid date or time\\"`. 3. **Constraints or Limitations:** - The year must be between 1 and 9999. - Month must be between 1 and 12. - Day must be between 1 and 31, appropriate to the month. - Hours must be between 0 and 23, minutes, and seconds between 0 and 59. 4. **Steps to Achieve Solution:** - Verify if the input date and time string adheres to the correct format and valid ranges. - Create a `datetime` object using the components extracted from the validated input string. - Use timezone-aware `datetime` with UTC (`timezone.utc`). - Format the datetime object into the specified output string. **Implementation Example:** ```python from datetime import datetime, timezone def process_datetime(input_date): # Split input into date and time try: date_part, time_part = input_date.split(\' \') year, month, day = map(int, date_part.split(\'-\')) hour, minute, second = map(int, time_part.split(\':\')) # Validate and create datetime object dt = datetime(year, month, day, hour, minute, second, tzinfo=timezone.utc) # Return formatted string return f\\"Date: {dt.strftime(\'%Y-%m-%d\')} Time: {dt.strftime(\'%H:%M:%S\')} [UTC+00:00]\\" except ValueError: raise ValueError(\\"Invalid date or time\\") # Example usage: # print(process_datetime(\\"2023-10-15 14:20:30\\")) ``` **Performance Requirements:** - Ensure the function handles edge cases including leap years, and months with varying days. - The function should run efficiently for valid date strings, keeping in mind the constraints. Please write test cases to verify the correctness of your implementation.","solution":"from datetime import datetime, timezone def process_datetime(input_date): Validates the input date string and returns a formatted string with timezone information. Parameters: input_date (str): Date and time string in the format \\"YYYY-MM-DD HH:MM:SS\\". Returns: str: Formatted string if input is valid. Raises: ValueError: If the input date or time is invalid. try: # Split input into date and time parts date_part, time_part = input_date.split(\' \') year, month, day = map(int, date_part.split(\'-\')) hour, minute, second = map(int, time_part.split(\':\')) # Validate and create a datetime object dt = datetime(year, month, day, hour, minute, second, tzinfo=timezone.utc) # Return the formatted string return f\\"Date: {dt.strftime(\'%Y-%m-%d\')} Time: {dt.strftime(\'%H:%M:%S\')} [UTC+00:00]\\" except (ValueError, IndexError): raise ValueError(\\"Invalid date or time\\")"},{"question":"# Python Sequence Protocol Simulation **Objective:** Implement a Python class that mimics certain behaviors of sequences in Python. The class should allow indexing, slicing, and various sequence operations, similarly to how they are described in the provided C-API documentation. **Task:** Define a class `CustomSequence` that mimics a Python sequence and supports the following operations: 1. **Initialization**: Initialize the sequence with an iterable. 2. **Indexing**: Implement the `__getitem__` and `__setitem__` methods to support accessing and setting items by index. 3. **Slicing**: Implement the `__getitem__` and `__setitem__` methods to support slicing operations. 4. **Concatenation**: Implement a method that allows the sequence to be concatenated with another sequence. 5. **Repetition**: Implement a method to repeat the sequence n times. 6. **Length**: Implement the `__len__` method to return the number of items. 7. **Contains**: Implement the `__contains__` method to check if an item exists in the sequence. 8. **Counting**: Implement a method to count the occurrences of a given value in the sequence. **Input/Output:** - **Initialization**: `CustomSequence([1, 2, 3])` should create an instance with elements [1, 2, 3]. - **Indexing**: - Accessing: `seq[1]` should return the item at index 1. - Setting: `seq[1] = 4` should set the item at index 1 to 4. - **Slicing**: - Accessing: `seq[1:3]` should return a new `CustomSequence` instance containing elements from index 1 to 2. - Setting: `seq[1:3] = [8, 9]` should change the elements at indices 1 and 2. - **Concatenation**: `seq1.concat(seq2)` should concatenate two `CustomSequence` instances. - **Repetition**: `seq.repeat(3)` should repeat the sequence 3 times. - **Length**: `len(seq)` should return the number of items. - **Contains**: `5 in seq` should return `True` if the sequence contains the value 5. - **Counting**: `seq.count(2)` should return the number of occurrences of 2 in the sequence. **Example:** ```python class CustomSequence: def __init__(self, iterable): self.data = list(iterable) def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(self.data[index]) return self.data[index] def __setitem__(self, index, value): if isinstance(index, slice): self.data[index] = value else: self.data[index] = value def __len__(self): return len(self.data) def __contains__(self, item): return item in self.data def concat(self, other): return CustomSequence(self.data + other.data) def repeat(self, count): return CustomSequence(self.data * count) def count(self, value): return self.data.count(value) # Example usage: seq = CustomSequence([1, 2, 3]) print(seq[1]) # Output: 2 seq[1] = 4 print(seq.data) # Output: [1, 4, 3] print(seq[1:3].data) # Output: [4, 3] seq[1:3] = [8, 9] print(seq.data) # Output: [1, 8, 9] print(seq.concat(CustomSequence([4, 5])).data) # Output: [1, 8, 9, 4, 5] print(seq.repeat(2).data) # Output: [1, 8, 9, 1, 8, 9] print(len(seq)) # Output: 3 print(1 in seq) # Output: True print(seq.count(8)) # Output: 1 ``` Implement the class `CustomSequence` as described to pass the given examples. Ensure that the implementation supports both indexing and slicing, concatenation, repetition, length, membership testing, and counting effectively. **Constraints:** - The sequence can hold any type of elements. - The operations should work in an efficient manner with a performance not worse than their native list counterparts.","solution":"class CustomSequence: def __init__(self, iterable): self.data = list(iterable) def __getitem__(self, index): if isinstance(index, slice): return CustomSequence(self.data[index]) return self.data[index] def __setitem__(self, index, value): if isinstance(index, slice): self.data[index] = value else: self.data[index] = value def __len__(self): return len(self.data) def __contains__(self, item): return item in self.data def concat(self, other): if not isinstance(other, CustomSequence): raise TypeError(\\"Argument must be an instance of CustomSequence\\") return CustomSequence(self.data + other.data) def repeat(self, count): if not isinstance(count, int) or count < 0: raise ValueError(\\"The count must be a non-negative integer\\") return CustomSequence(self.data * count) def count(self, value): return self.data.count(value)"},{"question":"Objective: This coding question assesses the understanding and application of the `sqlite3` module\'s core functionalities, including database connections, executing SQL, and creating custom SQL functions. Problem Statement: Assume you are developing a personal library management system that stores details about books read over the years. You need to implement the following features in Python using the `sqlite3` module: 1. **Setup Database**: Write a function `setup_database(file_path)` that: - Creates a SQLite database with the specified `file_path`. - Creates a table named `books` with the following columns: - `id`: INTEGER PRIMARY KEY, - `title`: TEXT, - `author`: TEXT, - `year`: INTEGER, - `rating`: REAL. 2. **Insert Books**: Write a function `insert_books(file_path, books)` that: - Inserts multiple rows into the `books` table. - `books` is a list of tuples, where each tuple contains values for the columns (`title`, `author`, `year`, `rating`). - Commits the changes to the database. 3. **Fetch Highly Rated Books**: Write a function `fetch_highly_rated_books(file_path, min_rating)` that: - Fetches and returns all books with a rating greater than or equal to `min_rating`. - Returns the results as a list of dictionaries, where each dictionary represents a book with keys `title`, `author`, `year`, and `rating`. 4. **Custom Function**: Write a function `register_uppercase_function(file_path)` that: - Registers a custom SQL function named `UPPERCASE` that converts a given text to uppercase. - Tests this custom function by using it in a SQL query to fetch all book titles in uppercase. - Returns the results as a list of strings. Input and Output Formats: - **`setup_database(file_path)`**: - Input: `file_path` (str) - Path to the SQLite database file. - Output: None. - **`insert_books(file_path, books)`**: - Input: - `file_path` (str) - Path to the SQLite database file. - `books` (list of tuples) - List containing book details. - Output: None. - **`fetch_highly_rated_books(file_path, min_rating)`**: - Input: - `file_path` (str) - Path to the SQLite database file. - `min_rating` (float) - Minimum rating threshold. - Output: - List of dictionaries, each representing a book. - **`register_uppercase_function(file_path)`**: - Input: `file_path` (str) - Path to the SQLite database file. - Output: List of strings representing book titles in uppercase. Constraints: - Assume the database file does not exist initially and needs to be created. - `year` is a valid year (e.g., 2022). - `rating` is a float between 0 and 10. # Example: ```python # Setup database setup_database(\'library.db\') # Insert books books = [ (\'The Catcher in the Rye\', \'J.D. Salinger\', 1951, 8.3), (\'To Kill a Mockingbird\', \'Harper Lee\', 1960, 9.2), (\'1984\', \'George Orwell\', 1949, 9.0) ] insert_books(\'library.db\', books) # Fetch highly rated books print(fetch_highly_rated_books(\'library.db\', 9.0)) # Output: [{\'title\': \'To Kill a Mockingbird\', \'author\': \'Harper Lee\', \'year\': 1960, \'rating\': 9.2}, {\'title\': \'1984\', \'author\': \'George Orwell\', \'year\': 1949, \'rating\': 9.0}] # Register custom function and fetch titles in uppercase print(register_uppercase_function(\'library.db\')) # Output: [\'THE CATCHER IN THE RYE\', \'TO KILL A MOCKINGBIRD\', \'1984\'] ```","solution":"import sqlite3 def setup_database(file_path): Set up the SQLite database with the required \'books\' table. conn = sqlite3.connect(file_path) cursor = conn.cursor() create_table_query = CREATE TABLE IF NOT EXISTS books ( id INTEGER PRIMARY KEY, title TEXT, author TEXT, year INTEGER, rating REAL ) cursor.execute(create_table_query) conn.commit() conn.close() def insert_books(file_path, books): Insert multiple book records into the \'books\' table. conn = sqlite3.connect(file_path) cursor = conn.cursor() insert_query = \\"INSERT INTO books (title, author, year, rating) VALUES (?, ?, ?, ?)\\" cursor.executemany(insert_query, books) conn.commit() conn.close() def fetch_highly_rated_books(file_path, min_rating): Fetch all books with a rating greater than or equal to min_rating. conn = sqlite3.connect(file_path) cursor = conn.cursor() select_query = \\"SELECT title, author, year, rating FROM books WHERE rating >= ?\\" cursor.execute(select_query, (min_rating,)) rows = cursor.fetchall() conn.close() result = [] for row in rows: result.append({ \'title\': row[0], \'author\': row[1], \'year\': row[2], \'rating\': row[3] }) return result def uppercase(sql_value): Custom SQL function to convert text to uppercase. return sql_value.upper() def register_uppercase_function(file_path): Register the custom SQL function UPPERCASE and return book titles in uppercase. conn = sqlite3.connect(file_path) conn.create_function(\\"UPPERCASE\\", 1, uppercase) cursor = conn.cursor() select_query = \\"SELECT UPPERCASE(title) FROM books\\" cursor.execute(select_query) rows = cursor.fetchall() conn.close() result = [row[0] for row in rows] return result"},{"question":"**Objective:** Demonstrate your understanding of the `itertools`, `functools`, and `operator` modules by creating an efficient solution using these modules to solve a given problem. **Problem:** Write a Python function named `optimize_schedule` that takes a list of tasks, where each task is represented as a tuple of `(start_time, end_time)`, and returns the maximum number of non-overlapping tasks that can be scheduled. # Function Signature ```python def optimize_schedule(tasks: List[Tuple[int, int]]) -> int: ``` # Input - `tasks`: a list of tuples, where each tuple `(start_time, end_time)` represents a task. Each `start_time` and `end_time` is an integer such that 0 <= `start_time` < `end_time`. # Output - Returns an integer representing the maximum number of non-overlapping tasks that can be scheduled. # Constraints - The length of `tasks` list, `n`, can be large (`1 <= n <= 10^5`). - The `start_time` and `end_time` are non-negative integers less than `10^6`. # Performance Requirements - The solution should be optimized for performance with a time complexity of `O(n log n)` or better. # Examples ```python tasks = [(1, 3), (2, 4), (3, 5), (7, 8)] assert optimize_schedule(tasks) == 3 #[(1, 3), (3, 5), (7, 8)] tasks = [(5, 9), (1, 2), (3, 4), (0, 6), (5, 7), (8, 9)] assert optimize_schedule(tasks) == 4 #[(1, 2), (3, 4), (5, 7), (8, 9)] tasks = [(1, 2), (2, 3), (3, 4)] assert optimize_schedule(tasks) == 3 # All tasks can be scheduled ``` # Notes - You may find it helpful to use the `itertools` to handle and manipulate the task iterables efficiently. - The `functools` module may assist in applying higher-order functions if needed. - Mapping operators from the `operator` module can be useful for comparison or for processing the tasks. Implement the `optimize_schedule` function ensuring it is efficient and leverages Python\'s functional programming capabilities.","solution":"from typing import List, Tuple def optimize_schedule(tasks: List[Tuple[int, int]]) -> int: Returns the maximum number of non-overlapping tasks that can be scheduled. Each task is represented as a tuple of (start_time, end_time). # Sort tasks based on their end times sorted_tasks = sorted(tasks, key=lambda x: x[1]) # Initialize the count of non-overlapping tasks count = 0 # Initialize the end time of the last selected task to a very small number last_end_time = -1 # Iterate over the sorted tasks for start_time, end_time in sorted_tasks: # If the current task starts after the last selected task ends, select it if start_time >= last_end_time: count += 1 last_end_time = end_time return count"},{"question":"# Gzip File Processing Objective: Your task is to implement a function that takes a directory path containing multiple text files, compresses these files into gzip format, and then decompresses the resulting gzip files back into their original contents. The purpose of this exercise is to ensure you understand file I/O, file compression, and decompression using the `gzip` module in Python. Function Signature: ```python def process_gzip_files(directory: str) -> None: pass ``` Input: - `directory`: A string representing the path to a directory containing multiple text files. Output: The function should perform the following operations: 1. Compress each text file in the given directory into gzip format. 2. Save the compressed files with the same name but with a `.gz` extension. 3. Decompress each gzip file back to its original content in separate files with `_decompressed` added to the original filename. Constraints: - The directory will contain only text files. - There will be no subdirectories within the given directory. - Ensure that the decompressed files have the exact same content as the original files. Example: Suppose the directory contains the following files: ``` /path/to/directory/file1.txt /path/to/directory/file2.txt ``` After running the `process_gzip_files(directory)` function, you should have: 1. Compressed files: ``` /path/to/directory/file1.txt.gz /path/to/directory/file2.txt.gz ``` 2. Decompressed files: ``` /path/to/directory/file1_decompressed.txt /path/to/directory/file2_decompressed.txt ``` Hints: - Use the `gzip.open()` function to compress and decompress files. - Use the `shutil.copyfileobj()` method to copy file contents between File objects. - Ensure that file I/O operations handle both binary and text modes appropriately. Additional Notes: The code should handle exceptions gracefully and ensure that files are closed properly after operations. Use context managers (`with` statements) to manage file resources effectively.","solution":"import gzip import shutil import os def process_gzip_files(directory: str) -> None: # Get all text files in the directory text_files = [f for f in os.listdir(directory) if f.endswith(\'.txt\')] for text_file in text_files: text_file_path = os.path.join(directory, text_file) # Compress the text file to gzip format with open(text_file_path, \'rb\') as f_in: with gzip.open(f\\"{text_file_path}.gz\\", \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) # Decompress the gzip file back to its original content with gzip.open(f\\"{text_file_path}.gz\\", \'rb\') as f_in: with open(f\\"{text_file_path.replace(\'.txt\', \'_decompressed.txt\')}\\", \'wb\') as f_out: shutil.copyfileobj(f_in, f_out)"},{"question":"You have been provided with an audio processing task where you need to configure an audio device using the `ossaudiodev` module and write audio data to it. Your task is to implement a Python function `setup_and_play_audio(device, mode, format, channels, samplerate, data)` which does the following: 1. Opens the specified audio device in the given mode (`\'r\'`, `\'w\'`, or `\'rw\'`). 2. Configures the audio device with the provided format, number of channels, and sampling rate. 3. Writes the given audio data to the device. 4. Ensures the device is properly closed after the operation. # Function Signature ```python def setup_and_play_audio(device: str, mode: str, format: int, channels: int, samplerate: int, data: bytes) -> int: pass ``` # Input - `device`: A string representing the audio device filename. - `mode`: A string representing the mode in which to open the device (`\'r\'`, `\'w\'`, or `\'rw\'`). - `format`: An integer representing the audio format (e.g., `AFMT_S16_LE`). - `channels`: An integer representing the number of audio channels (1 for mono, 2 for stereo, etc.). - `samplerate`: An integer representing the sampling rate in samples per second (e.g., 44100 for CD quality). - `data`: A bytes-like object containing the audio data to be written. # Output - Returns the number of bytes written to the audio device. # Constraints - The audio device can be opened in read (`\'r\'`), write (`\'w\'`), or read/write (`\'rw\'`) modes. - Assume the audio device supports the necessary formats, channels, and sample rates. - The function should handle any exceptions that occur during the process and ensure the audio device is closed. # Example ```python audio_data = b\'x00x01x02...xFF\' # This should be actual audio data in the specified format. bytes_written = setup_and_play_audio(\'/dev/dsp\', \'w\', ossaudiodev.AFMT_S16_LE, 2, 44100, audio_data) ``` In this example, the function will open the audio device `/dev/dsp` in write mode, set the format to `AFMT_S16_LE`, set the number of channels to 2 (stereo), and the sample rate to 44100 Hz, then write the audio data to the device. # Notes - You may use the `ossaudiodev.open()` function to open the audio device. - Use methods like `setfmt()`, `channels()`, and `speed()` to configure the device. - Use the `write()` method of the audio device object to write the audio data. - Ensure proper exception handling and device closure using a `try...finally` block or a context manager. **Hint:** Refer to the `ossaudiodev` documentation provided to handle the device operations correctly.","solution":"import ossaudiodev def setup_and_play_audio(device: str, mode: str, format: int, channels: int, samplerate: int, data: bytes) -> int: if mode not in [\'r\', \'w\', \'rw\']: raise ValueError(\\"Invalid mode. Please use \'r\', \'w\', or \'rw\'.\\") audio_device = None try: audio_device = ossaudiodev.open(mode) audio_device.setfmt(format) audio_device.channels(channels) audio_device.speed(samplerate) if \'w\' in mode: bytes_written = audio_device.write(data) else: raise ValueError(\\"Mode must include \'w\' to write data\\") except Exception as e: print(f\\"An error occurred: {e}\\") return 0 finally: if audio_device: audio_device.close() return bytes_written"},{"question":"# Masked Tensor Sum with PyTorch Using the PyTorch library and the given block mask utilities from `torch.nn.attention.flex_attention`, write a function `masked_tensor_sum` that computes the sum of elements in a tensor subject to a complex masking condition. Function Signature: ```python def masked_tensor_sum(tensor: torch.Tensor, mask_condition: str) -> torch.Tensor: ``` Input: - `tensor (torch.Tensor)`: 2D tensor of shape `(N, M)` containing float elements. - `mask_condition (str)`: A string specifying a masking condition. The condition format is: - `\\"BLOCK\\"`: Use `create_block_mask` to generate the mask. - `\\"NESTED_BLOCK\\"`: Use `create_nested_block_mask` to generate the mask. - `\\"AND\\"`: Combine two masks using `and_masks`. - `\\"OR\\"`: Combine two masks using `or_masks`. - `\\"NOOP\\"`: Use `noop_mask` which effectively creates no masking. Output: - Returns a scalar `torch.Tensor` representing the sum of the elements in `tensor` that meet the masking condition. Constraints: - You must utilize the `torch.nn.attention.flex_attention` masks and the provided functions appropriately. - Your function should handle arbitrary tensor sizes within reasonable limits of memory and computational efficiency. Example: ```python # Given tensor tensor = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]) # Mask condition mask_condition = \\"BLOCK\\" # Output output = masked_tensor_sum(tensor, mask_condition) # The output should be the sum of the elements of the tensor filtered by the block mask condition ``` Implementing this function requires understanding the mask creation and combination functions and applying these masks to perform tensor operations conditionally.","solution":"import torch # Placeholder functions to mock the behavior of mask creation utilities def create_block_mask(tensor): return torch.ones_like(tensor, dtype=torch.bool) def create_nested_block_mask(tensor): return torch.ones_like(tensor, dtype=torch.bool) def noop_mask(tensor): return torch.ones_like(tensor, dtype=torch.bool) def and_masks(mask1, mask2): return mask1 & mask2 def or_masks(mask1, mask2): return mask1 | mask2 def masked_tensor_sum(tensor: torch.Tensor, mask_condition: str) -> torch.Tensor: if mask_condition == \\"BLOCK\\": mask = create_block_mask(tensor) elif mask_condition == \\"NESTED_BLOCK\\": mask = create_nested_block_mask(tensor) elif mask_condition == \\"AND\\": mask1 = create_block_mask(tensor) mask2 = create_nested_block_mask(tensor) mask = and_masks(mask1, mask2) elif mask_condition == \\"OR\\": mask1 = create_block_mask(tensor) mask2 = create_nested_block_mask(tensor) mask = or_masks(mask1, mask2) elif mask_condition == \\"NOOP\\": mask = noop_mask(tensor) else: raise ValueError(\\"Invalid mask condition\\") return torch.sum(tensor[mask])"},{"question":"<|Analysis Begin|> The provided documentation describes the `zlib` module in Python, which is used for data compression and decompression compatible with the gzip format. This module offers functions for compressing and decompressing data, computing checksums, and working with compression and decompression objects for handling data streams. Key functions and classes in the module include: - `zlib.compress(data, level=-1)`: Compresses a byte string. - `zlib.decompress(data, wbits=MAX_WBITS, bufsize=DEF_BUF_SIZE)`: Decompresses a byte string. - `zlib.compressobj(...)`, `zlib.decompressobj(...)`: Create objects for incremental compression and decompression. - `zlib.adler32(data, value=1)`: Computes an Adler-32 checksum. - `zlib.crc32(data, value=0)`: Computes a CRC32 checksum. - Various methods and attributes for `Compress` and `Decompress` objects. Given this information, we can design a question that challenges the students to utilize these functions and classes effectively. The question can involve reading large data and processing it incrementally, ensuring the students understand how to handle compression and decompression of data streams. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Problem Statement:** You are given a large text file that contains extensive data. Due to memory constraints, you need to compress this data incrementally and store the compressed data in a new file. Additionally, you will need to decompress the data back to its original form and verify its integrity using checksums. You are required to implement the following two functions: 1. `compress_file(input_file_path: str, output_file_path: str, chunk_size: int, compression_level: int) -> None` 2. `decompress_file(input_file_path: str, output_file_path: str, chunk_size: int) -> bool` **Function Specifications:** 1. `compress_file(input_file_path: str, output_file_path: str, chunk_size: int, compression_level: int) -> None` This function reads the input file incrementally in chunks of size `chunk_size`, compresses each chunk using the specified `compression_level`, and writes the compressed data to the output file. - Parameters: - `input_file_path` (str): Path to the input file containing the original data. - `output_file_path` (str): Path to the output file where the compressed data will be stored. - `chunk_size` (int): Size of the chunks in which the data will be read. - `compression_level` (int): Compression level (0 to 9) to be used for compressing the data. - Returns: None 2. `decompress_file(input_file_path: str, output_file_path: str, chunk_size: int) -> bool` This function reads the compressed file incrementally in chunks of size `chunk_size`, decompresses each chunk, and writes the decompressed data to the output file. After decompression, it verifies the integrity of the decompressed data by comparing checksums before and after compression. - Parameters: - `input_file_path` (str): Path to the input file containing the compressed data. - `output_file_path` (str): Path to the output file where the decompressed data will be stored. - `chunk_size` (int): Size of the chunks in which the data will be read. - Returns: True if the decompressed data matches the original data based on checksums, False otherwise. **Constraints:** - `chunk_size` will be a positive integer. - `compression_level` will be an integer between 0 and 9, inclusive. **Example:** ```python # Example usage: compress_file(\\"data.txt\\", \\"compressed_data.bin\\", 1024, 6) is_valid = decompress_file(\\"compressed_data.bin\\", \\"decompressed_data.txt\\", 1024) print(\\"Decompression Valid:\\", is_valid) ``` Your implementation should use the `zlib` library for compression and decompression. Ensure that you handle any potential exceptions and edge cases gracefully.","solution":"import zlib def compress_file(input_file_path: str, output_file_path: str, chunk_size: int, compression_level: int) -> None: Compresses a file incrementally and writes the compressed data to a new file. Parameters: - input_file_path (str): Path to the input file containing the original data. - output_file_path (str): Path to the output file where the compressed data will be stored. - chunk_size (int): Size of the chunks in which the data will be read. - compression_level (int): Compression level (0 to 9) to be used for compressing the data. Returns: None compress_obj = zlib.compressobj(compression_level) with open(input_file_path, \'rb\') as infile, open(output_file_path, \'wb\') as outfile: while chunk := infile.read(chunk_size): compressed_chunk = compress_obj.compress(chunk) outfile.write(compressed_chunk) # Write any data left in the buffer outfile.write(compress_obj.flush()) def decompress_file(input_file_path: str, output_file_path: str, chunk_size: int) -> bool: Decompresses a file incrementally and writes the decompressed data to a new file. Verifies the integrity of the decompressed data. Parameters: - input_file_path (str): Path to the input file containing the compressed data. - output_file_path (str): Path to the output file where the decompressed data will be stored. - chunk_size (int): Size of the chunks in which the data will be read. Returns: bool: True if the decompressed data matches the original data based on checksums, False otherwise. decompress_obj = zlib.decompressobj() original_checksum = 0 decompressed_checksum = 0 with open(input_file_path, \'rb\') as infile, open(output_file_path, \'wb\') as outfile: while chunk := infile.read(chunk_size): decompressed_chunk = decompress_obj.decompress(chunk) outfile.write(decompressed_chunk) decompressed_checksum = zlib.adler32(decompressed_chunk, decompressed_checksum) decompressed_chunk = decompress_obj.flush() outfile.write(decompressed_chunk) decompressed_checksum = zlib.adler32(decompressed_chunk, decompressed_checksum) # Compare the checksums to verify the integrity with open(output_file_path, \'rb\') as original_file: while chunk := original_file.read(chunk_size): original_checksum = zlib.adler32(chunk, original_checksum) return original_checksum == decompressed_checksum"},{"question":"You have been provided with a Telnet server that listens on `localhost` at port `23`. Your task is to implement a Python function that connects to this Telnet server, logs in with given credentials, executes a series of commands, and returns the output of these commands. The function should be named `telnet_interact` and must follow the specified input and output formats. # Function Signature ```python def telnet_interact(user: str, password: str, commands: list[str]) -> str: pass ``` # Input - `user` (str): The username for login. - `password` (str): The password for login. - `commands` (list of str): A list of commands to be executed on the Telnet server after login. # Output - `str`: The output received from executing the commands, as a single string. # Constraints - You must handle potential timeouts when reading from the connection. - Ensure that the session is properly closed after executing the commands. - Your function should support a maximum of 5 commands within a session. - Handle cases where the `read_until` method might encounter EOFError. # Example ```python # Assume the server is properly set up to handle these commands. print(telnet_interact(\\"user123\\", \\"pass123\\", [\\"echo Hello\\", \\"ls\\"])) ``` Expected Output: ``` <full output of the commands including any prompts and responses> ``` # Implementation Guidance 1. **Connect to the Telnet server:** Use `telnetlib.Telnet(open(host, port))` to establish a connection to `localhost` on port `23`. 2. **Login Procedure:** - Utilize the `read_until` method to wait for the login prompt (`b\\"login: \\"`), then write the username followed by a newline. - Similarly, wait for the password prompt (`b\\"Password: \\"`) using `read_until`, then write the password followed by a newline. 3. **Execute Commands:** - For each command in the provided `commands` list, use the `write` method to send the command followed by a newline. - After sending all commands, use the `read_all` method to read the complete response from the server. 4. **Handle Timeout and EOF:** - Implement appropriate exception handling for potential EOFError during read operations. 5. **Session Closure:** - Ensure the Telnet session is properly closed using the `close` method after completing the command execution and reading all responses. This task assesses the student\'s ability to utilize and manipulate the given `telnetlib` package effectively to achieve a specific functionality involving network communication.","solution":"import telnetlib import time def telnet_interact(user: str, password: str, commands: list[str]) -> str: host = \\"localhost\\" port = 23 timeout = 10 try: # Connect to the Telnet server tn = telnetlib.Telnet(host, port, timeout) # Perform login tn.read_until(b\\"login: \\", timeout) tn.write(user.encode(\'ascii\') + b\\"n\\") tn.read_until(b\\"Password: \\", timeout) tn.write(password.encode(\'ascii\') + b\\"n\\") # Execute commands results = [] for command in commands: tn.write(command.encode(\'ascii\') + b\\"n\\") time.sleep(1) # Allow some time for the command to execute result = tn.read_very_eager().decode(\'ascii\') results.append(result) # Close the connection tn.close() return \'\'.join(results) except EOFError: return \\"Connection closed unexpectedly by the server.\\" except Exception as e: return str(e)"},{"question":"**Question: File Synchronization Utility** **Objective:** Create a utility using Python that synchronizes the contents of two directories. The utility should update files to ensure that both directories have the latest versions of all files. **Description:** You need to write a function `synchronize_directories(src_dir: str, dest_dir: str) -> None` that takes two directory paths as input and synchronizes the contents of the source directory (`src_dir`) with the destination directory (`dest_dir`). The synchronization should ensure that: 1. Any new files in `src_dir` not present in `dest_dir` are copied to `dest_dir`. 2. Any files in `dest_dir` that are out-of-date compared to `src_dir` are updated. 3. No files are deleted from `dest_dir`. **Input:** - `src_dir` (string): Path to the source directory. - `dest_dir` (string): Path to the destination directory. **Constraints:** - You can assume that both `src_dir` and `dest_dir` exist and are directories. - You should handle directories and subdirectories recursively. - Performance consideration: The function should perform synchronously and efficiently to handle large directories. **Output:** - `None`. The function will perform the synchronization in-place. **Example:** Assume you have the following directory structure for `src_dir`: ``` src_dir/ ├── file1.txt ├── file2.txt └── subdir/ └── file3.txt ``` And `dest_dir`: ``` dest_dir/ ├── file1.txt (older version) └── file4.txt ``` After calling `synchronize_directories(src_dir, dest_dir)`, the `dest_dir` should be: ``` dest_dir/ ├── file1.txt (updated version) ├── file4.txt ├── file2.txt └── subdir/ └── file3.txt ``` **Implementation:** You may utilize the following modules and functions to complete this task: - `pathlib` for path manipulations and directory traversals. - `shutil` for high-level file copy operations. - `os` module for lower-level file interactions if needed. - `filecmp` for comparing file contents. ```python from pathlib import Path import shutil import filecmp def synchronize_directories(src_dir: str, dest_dir: str) -> None: src_path = Path(src_dir) dest_path = Path(dest_dir) if not src_path.is_dir() or not dest_path.is_dir(): raise ValueError(\\"Both source and destination paths must be valid directories.\\") for src_file in src_path.rglob(\'*\'): relative_path = src_file.relative_to(src_path) dest_file = dest_path / relative_path if src_file.is_file(): if not dest_file.exists() or not filecmp.cmp(src_file, dest_file, shallow=False): dest_file.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(src_file, dest_file) ``` The function `synchronize_directories` should be implemented to ensure thorough and efficient file synchronization between the specified directories.","solution":"from pathlib import Path import shutil import filecmp import os def synchronize_directories(src_dir: str, dest_dir: str) -> None: src_path = Path(src_dir) dest_path = Path(dest_dir) if not src_path.is_dir() or not dest_path.is_dir(): raise ValueError(\\"Both source and destination paths must be valid directories.\\") for src_file in src_path.rglob(\'*\'): relative_path = src_file.relative_to(src_path) dest_file = dest_path / relative_path if src_file.is_file(): if not dest_file.exists() or not filecmp.cmp(src_file, dest_file, shallow=False): dest_file.parent.mkdir(parents=True, exist_ok=True) shutil.copy2(src_file, dest_file)"},{"question":"# Question: Parallel Log Processing and Aggregation Context: You have been given a system that continuously generates log messages with different severity levels (INFO, WARNING, ERROR). Your task is to design a parallel processing system in Python using the `multiprocessing` module to improve the efficiency of log processing and aggregation. Requirements: 1. **Log Generator**: - Simulate a log generator that produces log messages at random intervals. - Log messages should be of three types: INFO, WARNING, and ERROR. - Each log message should have a process ID generated by the current producing process. 2. **Log Processing**: - Create multiple worker processes to process log messages in parallel. - Each worker should receive log messages through a `multiprocessing.Queue`. - Processed log messages should be stored in shared memory for aggregation. 3. **Log Aggregation**: - Implement a process that periodically aggregates the logs from shared memory, counting the number of INFO, WARNING, and ERROR messages. - Aggregated results should be printed to the console at regular intervals. Implementation: - Use a `multiprocessing.Queue` to pass log messages from the log generator to log processing workers. - Use `multiprocessing.Value` or `multiprocessing.Array` to store the count of each log type in shared memory. - Ensure proper synchronization using Locks to avoid race conditions. Input and Output Formats: - **Log Messages**: A log message is a string in the format `\\"[SEVERITY] Process-<pid>: message\\"`. - **Aggregated Output**: Every aggregation interval, print the counts of each severity type in the format: ``` INFO: <count>, WARNING: <count>, ERROR: <count> ``` Constraints: - The log generator should run indefinitely. Use `Ctrl+C` to stop the program manually during testing. - The aggregation process should run every 10 seconds (adjustable). Here is the skeleton code to get you started: ```python import multiprocessing import time import random LOG_TYPES = [\'INFO\', \'WARNING\', \'ERROR\'] def log_generator(log_queue): while True: log_type = random.choice(LOG_TYPES) log_message = f\\"[{log_type}] Process-{multiprocessing.current_process().pid}: message\\" log_queue.put(log_message) time.sleep(random.uniform(0.1, 1.0)) def log_processor(log_queue, info_counter, warning_counter, error_counter, lock): while True: log_message = log_queue.get() log_type = log_message.split()[0] with lock: if log_type == \'[INFO]\': info_counter.value += 1 elif log_type == \'[WARNING]\': warning_counter.value += 1 elif log_type == \'[ERROR]\': error_counter.value += 1 def log_aggregator(info_counter, warning_counter, error_counter, lock): while True: time.sleep(10) # Aggregation interval with lock: print(f\\"INFO: {info_counter.value}, WARNING: {warning_counter.value}, ERROR: {error_counter.value}\\") if __name__ == \'__main__\': multiprocessing.freeze_support() log_queue = multiprocessing.Queue() manager = multiprocessing.Manager() lock = manager.Lock() info_counter = manager.Value(\'i\', 0) warning_counter = manager.Value(\'i\', 0) error_counter = manager.Value(\'i\', 0) log_gen_process = multiprocessing.Process(target=log_generator, args=(log_queue,)) log_gen_process.start() workers = [] for _ in range(4): # Number of worker processes worker = multiprocessing.Process(target=log_processor, args=(log_queue, info_counter, warning_counter, error_counter, lock)) worker.start() workers.append(worker) aggregator = multiprocessing.Process(target=log_aggregator, args=(info_counter, warning_counter, error_counter, lock)) aggregator.start() log_gen_process.join() for worker in workers: worker.join() aggregator.join() ``` Complete the implementation based on the given skeleton to meet all the requirements and constraints.","solution":"import multiprocessing import time import random LOG_TYPES = [\'INFO\', \'WARNING\', \'ERROR\'] def log_generator(log_queue): while True: log_type = random.choice(LOG_TYPES) log_message = f\\"[{log_type}] Process-{multiprocessing.current_process().pid}: message\\" log_queue.put(log_message) time.sleep(random.uniform(0.1, 1.0)) def log_processor(log_queue, info_counter, warning_counter, error_counter, lock): while True: log_message = log_queue.get() log_type = log_message.split()[0] with lock: if log_type == \'[INFO]\': info_counter.value += 1 elif log_type == \'[WARNING]\': warning_counter.value += 1 elif log_type == \'[ERROR]\': error_counter.value += 1 def log_aggregator(info_counter, warning_counter, error_counter, lock): while True: time.sleep(10) # Aggregation interval with lock: print(f\\"INFO: {info_counter.value}, WARNING: {warning_counter.value}, ERROR: {error_counter.value}\\") if __name__ == \'__main__\': multiprocessing.freeze_support() log_queue = multiprocessing.Queue() manager = multiprocessing.Manager() lock = manager.Lock() info_counter = manager.Value(\'i\', 0) warning_counter = manager.Value(\'i\', 0) error_counter = manager.Value(\'i\', 0) log_gen_process = multiprocessing.Process(target=log_generator, args=(log_queue,)) log_gen_process.start() workers = [] for _ in range(4): # Number of worker processes worker = multiprocessing.Process(target=log_processor, args=(log_queue, info_counter, warning_counter, error_counter, lock)) worker.start() workers.append(worker) aggregator = multiprocessing.Process(target=log_aggregator, args=(info_counter, warning_counter, error_counter, lock)) aggregator.start() log_gen_process.join() for worker in workers: worker.join() aggregator.join()"},{"question":"Problem Statement # Objective You are required to design and implement a Python function that fetches data from a given URL, parses the URL to extract certain components, and processes the response to extract and analyze specific information. # Function Signature ```python def fetch_and_analyze_url(url: str) -> dict: pass ``` # Input - `url`: A string representing a valid URL. # Output - A dictionary with the following keys: - `\\"url_components\\"`: A dictionary containing the scheme, netloc, path, params, query, and fragment of the URL. - `\\"status_code\\"`: An integer representing the HTTP status code of the response. - `\\"headers\\"`: A dictionary containing the response headers. - `\\"content_length\\"`: An integer representing the length of the response content in bytes. # Constraints - You are required to use the `urllib` module for making the HTTP request and parsing the URL. - Assume the URL will always be valid and the HTTP request will not fail. - Handle cases where the content length header might be missing by returning -1 for the content length in such cases. # Example ```python url = \\"https://jsonplaceholder.typicode.com/posts\\" result = fetch_and_analyze_url(url) print(result) ``` If successful, the function should return a dictionary similar to: ```python { \\"url_components\\": { \\"scheme\\": \\"https\\", \\"netloc\\": \\"jsonplaceholder.typicode.com\\", \\"path\\": \\"/posts\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" }, \\"status_code\\": 200, \\"headers\\": { \'Content-Type\': \'application/json; charset=utf-8\', \'Content-Length\': \'4950\', ... }, \\"content_length\\": 4950 } ``` # Notes - Use `urllib.parse.urlparse` for parsing the URL. - Use `urllib.request.urlopen` for making the HTTP request. - Extract the response headers using the `info()` method of the response object. - Extract the status code from the response object. - You may need to handle the case where content length is not provided in the headers.","solution":"import urllib.request import urllib.parse def fetch_and_analyze_url(url: str) -> dict: parsed_url = urllib.parse.urlparse(url) url_components = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment } response = urllib.request.urlopen(url) status_code = response.getcode() headers = dict(response.info()) content_length = int(headers.get(\\"Content-Length\\", -1)) result = { \\"url_components\\": url_components, \\"status_code\\": status_code, \\"headers\\": headers, \\"content_length\\": content_length } return result"},{"question":"# Email Address Formatter and Parser You are tasked with implementing a function that takes a list of email addresses, normalizes their format, and parses them into their constituent realname and email address parts. To achieve this, you should use the `formataddr` and `parseaddr` functions from the `email.utils` module. Function Signature ```python def normalize_and_parse_emails(email_list: list[str], charset: str = \'utf-8\') -> list[tuple[str, str]]: pass ``` Input: - `email_list`: A list of strings, where each string is an email address that may or may not include a realname. - `charset`: (Optional) The character set to use for encoding the realname. Defaults to `\'utf-8\'`. Output: - A list of tuples, where each tuple contains two strings: the realname and the email address. Example: ```python emails = [ \'John Doe <john.doe@example.com>\', \'jane.doe@example.com\', \'\\"Dr. Smith\\" <dr.smith@example.org>\' ] output = normalize_and_parse_emails(emails, \'utf-8\') print(output) ``` Expected Output: ```python [ (\'John Doe\', \'john.doe@example.com\'), (\'\', \'jane.doe@example.com\'), (\'Dr. Smith\', \'dr.smith@example.org\') ] ``` Constraints: 1. The function should correctly handle quotes and other special characters in the email addresses. 2. The function should use the `formataddr` and `parseaddr` functions from the `email.utils` module for formatting and parsing. Notes: - Ensure you handle cases where the input email string might not include a realname. - The output list should preserve the input order of email addresses. Performance: - The function should be able to process a list of up to 10,000 email addresses efficiently. Happy coding!","solution":"from email.utils import formataddr, parseaddr def normalize_and_parse_emails(email_list: list[str], charset: str = \'utf-8\') -> list[tuple[str, str]]: result = [] for email in email_list: realname, email_address = parseaddr(email) formatted_realname = formataddr((realname, email_address), charset=charset) normalized_realname, normalized_email_address = parseaddr(formatted_realname) result.append((normalized_realname, normalized_email_address)) return result"},{"question":"# Python Logging Configuration Assessment **Objective:** To assess your understanding of the `logging.config` module in Python by configuring a logging system using the provided functions and dictionary schema. **Problem Statement:** Your task is to implement a function `configure_logging()` that sets up the logging configuration using the `dictConfig` method. The configuration should include multiple handlers with different logging levels and formatters. Additionally, the function should use `fileConfig` to load a secondary configuration from a file, and start a logging listener to listen for dynamic configuration changes over a socket. # Requirements: 1. Implement the `configure_logging()` function, which should: - Set up initial logging configuration using `dictConfig`. - Load additional configuration from a file using `fileConfig`. - Start a logging listener using `listen` to accept runtime configuration changes. 2. Define a logging configuration dictionary that includes: - At least one formatter. - At least two handlers with different logging levels and types (e.g., `StreamHandler`, `FileHandler`). - At least one logger with multiple handlers attached. 3. Provide a sample configuration file for `fileConfig` to load. # Function Signature: ```python def configure_logging(config_file_path: str) -> None: pass ``` # Input: - `config_file_path` (str): The path to the configuration file to be loaded using `fileConfig`. # Constraints: - The configuration should use valid logging levels (`DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`). - Handlers and loggers must be appropriately connected using IDs and names. - The `listen` function should be configured to run on a specified port (you can use any free port). # Example: ```python import logging import logging.config def configure_logging(config_file_path: str) -> None: config_dict = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)s %(levelname)s %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'INFO\', \'formatter\': \'detailed\', \'filename\': \'app.log\', }, }, \'loggers\': { \'myLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], }, }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'], }, } # Apply dictConfig logging.config.dictConfig(config_dict) # Load additional file config logging.config.fileConfig(config_file_path) # Start a logging listener import threading server_thread = logging.config.listen(9999) server_thread.start() # Optional: You can stop listening when needed # logging.config.stopListening() # server_thread.join() # Sample configuration file content for fileConfig (save this in a file and provide its path to the function): [loggers] keys=root,exampleLogger [handlers] keys=consoleHandler,fileHandler [formatters] keys=exampleFormatter [logger_root] level=WARNING handlers=consoleHandler [logger_exampleLogger] level=DEBUG handlers=consoleHandler,fileHandler qualname=exampleLogger [handler_consoleHandler] class=StreamHandler level=DEBUG formatter=exampleFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=INFO formatter=exampleFormatter args=(\'example.log\', \'w\') [formatter_exampleFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s # Use the function configure_logging(\'path_to_config_file.cfg\') ``` # Evaluation Criteria: - Correctness: Does the function setup the logging configuration as specified? - Comprehension: Does the implementation demonstrate a clear understanding of logging configurations using `dictConfig` and `fileConfig`? - Completeness: Are various aspects of logging (handlers, formatters, loggers) correctly configured and connected? - Code Quality: Is the code well-organized, readable, and follows best practices?","solution":"import logging import logging.config import threading def configure_logging(config_file_path: str) -> None: config_dict = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s %(name)s %(levelname)s %(message)s\', \'datefmt\': \'%Y-%m-%d %H:%M:%S\', }, }, \'handlers\': { \'console\': { \'class\': \'logging.StreamHandler\', \'level\': \'DEBUG\', \'formatter\': \'detailed\', \'stream\': \'ext://sys.stdout\', }, \'file\': { \'class\': \'logging.FileHandler\', \'level\': \'INFO\', \'formatter\': \'detailed\', \'filename\': \'app.log\', }, }, \'loggers\': { \'myLogger\': { \'level\': \'DEBUG\', \'handlers\': [\'console\', \'file\'], }, }, \'root\': { \'level\': \'WARNING\', \'handlers\': [\'console\'], }, } # Apply dictConfig logging.config.dictConfig(config_dict) # Load additional file config if the file path is provided if config_file_path: logging.config.fileConfig(config_file_path) # Start a logging listener server_thread = threading.Thread(target=logging.config.listen, args=(9999,)) server_thread.daemon = True server_thread.start() return server_thread # Sample configuration file content for fileConfig (save this in a file and provide its path to the function): # Note: This content should be saved in a file, for example, \\"logging_config.cfg\\". [loggers] keys=root,exampleLogger [handlers] keys=consoleHandler,fileHandler [formatters] keys=exampleFormatter [logger_root] level=WARNING handlers=consoleHandler [logger_exampleLogger] level=DEBUG handlers=consoleHandler,fileHandler qualname=exampleLogger [handler_consoleHandler] class=StreamHandler level=DEBUG formatter=exampleFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=INFO formatter=exampleFormatter args=(\'example.log\', \'w\') [formatter_exampleFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s"},{"question":"**Problem Statement:** You are required to design a custom differentiable function in PyTorch that calculates the Root Mean Square Error (RMSE) between two tensors. The custom function should correctly utilize PyTorch\'s autograd functionality by defining both the forward and backward passes. You must also create a corresponding `nn.Module` that utilizes this custom function. **Requirements:** 1. **Custom Function:** - Implement a subclass of `torch.autograd.Function` named `RMSEFunction`. - Define the `forward` method to compute the RMSE between two input tensors. - Define the `setup_context` method to save relevant tensors for the backward pass. - Define the `backward` method to compute the gradients with respect to the inputs. 2. **Gradient Check:** - Use `torch.autograd.gradcheck` to ensure the gradients computed by your custom function are correct. 3. **Custom Module:** - Implement a subclass of `nn.Module` named `RMSELoss`. - The module\'s `forward` method should invoke `RMSEFunction.apply` to compute the loss. **Constraints:** - Your implementation should support inputs of arbitrary shape. - Ensure that your custom function adheres to PyTorch\'s best practices for custom autograd Functions. - The backward method should efficiently compute gradients without modifying in-place. **Input:** - Two PyTorch tensors of the same shape. **Output:** - Scalar tensor representing the RMSE value. # Example: ```python import torch from torch.autograd import Function import torch.nn as nn class RMSEFunction(Function): @staticmethod def forward(ctx, input, target): diff = input - target rmse = torch.sqrt(torch.mean(diff ** 2)) ctx.save_for_backward(diff, rmse) return rmse @staticmethod def setup_context(ctx, inputs, output): diff, rmse = ctx.saved_tensors ctx.save_for_backward(diff, rmse) @staticmethod def backward(ctx, grad_output): diff, rmse = ctx.saved_tensors input_grad = (grad_output / rmse) * (diff / diff.numel()) return input_grad, -input_grad class RMSELoss(nn.Module): def forward(self, input, target): return RMSEFunction.apply(input, target) # Test input = torch.randn((10,), dtype=torch.double, requires_grad=True) target = torch.randn((10,), dtype=torch.double) # Gradient check from torch.autograd import gradcheck test = gradcheck(RMSEFunction.apply, (input, target), eps=1e-6, atol=1e-4) print(test) # Usage criterion = RMSELoss() loss = criterion(input, target) print(loss.item()) loss.backward() ``` **Explanation:** - The `RMSEFunction` class implements a custom autograd function for calculating RMSE. - The `forward` method computes the RMSE and saves intermediate results needed for the backward pass. - The `backward` method computes the gradients based on the saved tensors. - The `RMSELoss` class is a custom `nn.Module` that computes the RMSE loss using the `RMSEFunction`. **Note:** Ensure that your code adheres to PyTorch\'s guidelines for creating custom autograd functions and modules. Use the provided test case to verify the correctness of your implementation.","solution":"import torch from torch.autograd import Function import torch.nn as nn class RMSEFunction(Function): @staticmethod def forward(ctx, input, target): diff = input - target squared_diff = diff ** 2 mean_squared_diff = torch.mean(squared_diff) rmse = torch.sqrt(mean_squared_diff) ctx.save_for_backward(input, target, diff, rmse) return rmse @staticmethod def backward(ctx, grad_output): input, target, diff, rmse = ctx.saved_tensors num_elements = input.numel() grad_input = grad_output * diff / (rmse * num_elements) grad_target = -grad_input return grad_input, grad_target class RMSELoss(nn.Module): def forward(self, input, target): return RMSEFunction.apply(input, target)"},{"question":"**Question: Implement a Custom Slice Function Handler in Python** You are to create a Python function `handle_slice(sequence: list, slice_obj: slice) -> list` that mimics the behavior of applying a slice object to a given list. The function should account for edge cases like out-of-bounds indices and negative steps. # Function Signature: ```python def handle_slice(sequence: list, slice_obj: slice) -> list: pass ``` # Input: - `sequence` (list): A list of integers. - `slice_obj` (slice): A slice object that specifies start, stop, and step. # Output: - (list): A new list which is the result of applying the slice object on the sequence. # Constraints: - The length of the sequence `n` will be in the range `1 <= n <= 10^5`. - The slice can specify indices (start, stop) within and beyond the range of the list. - The step can be negative or positive. # Examples: ```python assert handle_slice([1, 2, 3, 4, 5], slice(1, 4, 1)) == [2, 3, 4] assert handle_slice([1, 2, 3, 4, 5], slice(-4, -1, 1)) == [2, 3, 4] assert handle_slice([1, 2, 3, 4, 5], slice(None, None, -1)) == [5, 4, 3, 2, 1] assert handle_slice([1, 2, 3, 4, 5], slice(10, 20, 2)) == [] assert handle_slice([1, 2, 3, 4, 5], slice(-100, 100, 1)) == [1, 2, 3, 4, 5] ``` # Detailed Description: 1. Handle cases where start, stop, or step can be `None`, interpreting them as the start, end of the sequence, and step of 1 respectively. 2. Correctly manage out-of-bounds indices by clipping them to valid ranges. 3. Support both positive and negative step values, ensuring the slice extracts elements correctly. 4. Optimize for performance given the constraint on sequence length. Use built-in Python functionalities wherever possible to maintain efficiency and clarity in your implementation. Ensure your function handles all edge cases as demonstrated in the examples.","solution":"def handle_slice(sequence, slice_obj): Mimics the behavior of applying a slice object to a given list. Parameters: sequence (list): A list of integers. slice_obj (slice): A slice object with start, stop, and step. Returns: list: A new list that is the result of applying the slice object on the sequence. return sequence[slice_obj]"},{"question":"Title: Custom Dataset Generator and Model Evaluation Objective: The task is to create a custom synthetic dataset using various dataset generators from the `sklearn.datasets` module. You\'ll then use this dataset to train and evaluate a machine learning model. Problem Statement: You need to implement a function `generate_and_evaluate` that performs the following steps: 1. Create a synthetic dataset with three different dataset generators (`make_blobs`, `make_circles`, and `make_moons`). 2. Combine the datasets into a single dataset. 3. Apply a machine learning classifier to the combined dataset and evaluate its performance. Function Signature: ```python def generate_and_evaluate(random_state: int) -> float: \'\'\' Parameters: - random_state: an integer value used for random state in dataset generation Returns: - accuracy: a float representing the accuracy of the classifier on the test data \'\'\' ``` Steps and Requirements: 1. Use `make_blobs` to generate a dataset with 100 samples, 2 features, 2 clusters. 2. Use `make_circles` to generate a dataset with 100 samples and a noise level of 0.1. 3. Use `make_moons` to generate a dataset with 100 samples and a noise level of 0.1. 4. Combine the three datasets into a single dataset (300 samples in total). 5. Use a `RandomForestClassifier` to train on 70% of the combined dataset and test on the remaining 30%. 6. Return the accuracy of the `RandomForestClassifier` on the test data. Constraints: - You must use the specified dataset generators with the parameters mentioned. - You must use `RandomForestClassifier` from `sklearn.ensemble`. - Use appropriate methods to split the dataset into training and testing sets. Example Execution: ```python accuracy = generate_and_evaluate(random_state=42) print(f\\"Accuracy: {accuracy:.2f}\\") ``` Make sure your implementation is efficient and makes good use of scikit-learn\'s tools.","solution":"import numpy as np from sklearn.datasets import make_blobs, make_circles, make_moons from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def generate_and_evaluate(random_state: int) -> float: Generates a synthetic dataset using make_blobs, make_circles, and make_moons, trains a RandomForestClassifier on the dataset, and returns the accuracy. Parameters: - random_state: an integer value used for random state in dataset generation Returns: - accuracy: a float representing the accuracy of the classifier on the test data # Generate synthetic datasets X1, y1 = make_blobs(n_samples=100, centers=2, n_features=2, random_state=random_state) X2, y2 = make_circles(n_samples=100, noise=0.1, random_state=random_state) X3, y3 = make_moons(n_samples=100, noise=0.1, random_state=random_state) # Combine datasets X = np.vstack((X1, X2, X3)) y = np.hstack((y1, y2, y3)) # Split data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state) # Train the RandomForest classifier classifier = RandomForestClassifier(random_state=random_state) classifier.fit(X_train, y_train) # Predict and evaluate y_pred = classifier.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Title**: Implementing and Comparing Feature Extraction Techniques **Objective**: The goal of this assessment is to evaluate your understanding of feature extraction techniques provided by `scikit-learn` and your ability to implement and compare different methods on a text dataset. **Problem Statement**: You are provided with a list of text documents representing movie reviews. Your task is to implement different feature extraction techniques (`CountVectorizer`, `TfidfVectorizer`, and `HashingVectorizer`) and compare their output for a set of given reviews. **Instructions**: 1. Implement a function `extract_features` that takes the following inputs: - `reviews` (list of str): A list of movie reviews. - `vectorizer_type` (str): Type of vectorizer to use. It can be one of `\\"CountVectorizer\\"`, `\\"TfidfVectorizer\\"`, or `\\"HashingVectorizer\\"`. The function should return the transformed feature matrix as a dense array. 2. Implement a function `compare_vectorizers` that takes as input: - `reviews` (list of str): A list of movie reviews. This function should: - Use the `extract_features` function to transform the reviews using each of the three vectorizers. - Print the feature names for `CountVectorizer` and `TfidfVectorizer`. - Print the transformed feature matrices for each vectorizer. **Input**: - `reviews`: A list of strings, where each string is a movie review. - `vectorizer_type`: The type of vectorizer to use as described above. **Output**: - For `extract_features`: A dense array representing the transformed feature matrix. - For `compare_vectorizers`: The feature names and the feature matrices for each vectorizer type. **Example**: ```python reviews = [ \\"The movie was fantastic! I loved it.\\", \\"Quite dull and boring. Not my type.\\", \\"An average movie with decent performances.\\", ] def extract_features(reviews, vectorizer_type): # Your code here pass def compare_vectorizers(reviews): # Your code here pass # Example function calls: compare_vectorizers(reviews) ``` **Notes**: - For `CountVectorizer` and `TfidfVectorizer`, use their default settings. - For `HashingVectorizer`, set `n_features=10`. - Ensure the output is easy to read and interpret. **Constraints**: - You may not use any external text processing libraries except for `scikit-learn`. - Ensure your code is well-commented and adheres to good coding practices.","solution":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer def extract_features(reviews, vectorizer_type): if vectorizer_type == \\"CountVectorizer\\": vectorizer = CountVectorizer() elif vectorizer_type == \\"TfidfVectorizer\\": vectorizer = TfidfVectorizer() elif vectorizer_type == \\"HashingVectorizer\\": vectorizer = HashingVectorizer(n_features=10) else: raise ValueError(\\"Unsupported vectorizer type. Choose one of: \'CountVectorizer\', \'TfidfVectorizer\', \'HashingVectorizer\'\\") feature_matrix = vectorizer.fit_transform(reviews) if vectorizer_type == \\"HashingVectorizer\\": return feature_matrix.toarray() else: return feature_matrix.toarray(), vectorizer.get_feature_names_out() def compare_vectorizers(reviews): vectorizers = [\'CountVectorizer\', \'TfidfVectorizer\', \'HashingVectorizer\'] for v_type in vectorizers: print(f\'nVectorizer: {v_type}\') if v_type == \'HashingVectorizer\': features = extract_features(reviews, v_type) print(features) else: features, feature_names = extract_features(reviews, v_type) print(feature_names) print(features)"},{"question":"You are given a dataset `penguins` containing measurements of penguins\' bill lengths and depths, along with their species. Your task is to create a detailed and customized visualization using Seaborn\'s `jointplot` function. **Dataset:** The `penguins` dataset (which can be loaded using `sns.load_dataset(\\"penguins\\")`) contains the following columns: - `species`: Species of the penguin. - `island`: Island where the penguin is found. - `bill_length_mm`: Length of the penguin\'s bill in millimeters. - `bill_depth_mm`: Depth of the penguin\'s bill in millimeters. - Other columns not relevant to this task. **Requirements:** 1. Create a joint plot of `bill_length_mm` versus `bill_depth_mm` using the `sns.jointplot` function. 2. Use the `hue` parameter to differentiate the data points by the `species` column. 3. Set the plot type to KDE plot using the `kind` parameter. 4. Customize the plot by setting the following parameters: - `height=6` to adjust plot height. - `ratio=3` for the ratio of joint to marginal axes. - Show marginal ticks using `marginal_ticks=True`. 5. Add an additional layer with a red KDE plot on the joint axes. 6. Add marginal rug plots with red color. 7. Save the plot as `penguins_jointplot.png`. **Constraints:** - Ensure your plot is easy to interpret. - Use Seaborn and any necessary Matplotlib functions for customization. - Provide clear comments in your code explaining each step. **Performance Requirements:** - Your solution should efficiently create and customize the plot without redundant calculations or operations. **Expected Output:** A PNG file named `penguins_jointplot.png` containing the specified joint plot visualization. **Example Code Structure:** ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the joint plot g = sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", height=6, ratio=3, marginal_ticks=True ) # Add additional layers g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) g.plot_marginals(sns.rugplot, color=\\"r\\", height=-0.15, clip_on=False) # Save the plot plt.savefig(\\"penguins_jointplot.png\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_penguins_jointplot(): # Load the dataset penguins = sns.load_dataset(\\"penguins\\") # Create the joint plot g = sns.jointplot( data=penguins, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"kde\\", height=6, ratio=3, marginal_ticks=True ) # Add an additional red KDE plot layer on the joint axes g.plot_joint(sns.kdeplot, color=\\"r\\", zorder=0, levels=6) # Add marginal red rug plots g.plot_marginals(sns.rugplot, color=\\"r\\", height=-0.15, clip_on=False) # Save the plot to a file plt.savefig(\\"penguins_jointplot.png\\")"},{"question":"Objective The objective of this task is to assess your understanding of scikit-learn\'s scoring functions and your ability to implement a custom scoring function. You will need to: 1. Train a machine learning model on a dataset. 2. Evaluate the model using a predefined scoring function. 3. Create and use a custom scoring function for evaluation. Task 1. **Load the Dataset**: - Load the `breast_cancer` dataset from `sklearn.datasets`. 2. **Split the Data**: - Split the dataset into training and testing sets using an 80-20 split. 3. **Train a Classifier**: - Train a `LogisticRegression` classifier on the training set. 4. **Evaluate the Model**: - Evaluate the trained classifier using the accuracy score (`accuracy_score`). 5. **Implement a Custom Scoring Function**: - Implement a custom scoring function that computes the squared difference between the predicted probabilities and the true labels, averaged over all instances (similar to Brier score but more simplified). This function should: - Accept the true labels and predicted probabilities as input. - Compute the squared differences and then average them. - Return the resulting average value. 6. **Use the Custom Scoring Function**: - Use your custom scoring function to evaluate the classifier on the test set. Constraints - You should not use any external libraries except `numpy` and `sklearn`. - The custom scoring function should be implemented from scratch. Expected Input and Output Formats # Input - None. The data is handled within the function. # Output - Print the accuracy score of the classifier. - Print the custom score of the classifier using the custom scoring function. Performance Requirements - Ensure that the calculations in the custom scoring function are performed efficiently. # Function Signatures ```python def custom_scoring_function(y_true, y_pred_proba): # Implement your custom scoring function here pass def main(): from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the classifier clf = LogisticRegression(max_iter=1000, random_state=42) clf.fit(X_train, y_train) # Predict on the test set y_pred = clf.predict(X_test) y_pred_proba = clf.predict_proba(X_test)[:, 1] # Evaluate the classifier using accuracy score accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy Score: {accuracy}\\") # Evaluate the classifier using the custom scoring function custom_score = custom_scoring_function(y_test, y_pred_proba) print(f\\"Custom Score: {custom_score}\\") if __name__ == \\"__main__\\": main() ``` # Notes - Make sure your custom scoring function is well-documented. - Use appropriate code style and meaningful variable names. - Ensure your code is modular and easy to understand.","solution":"import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score def custom_scoring_function(y_true, y_pred_proba): Custom scoring function to compute the average squared difference between the predicted probabilities and the true labels. Args: y_true (array-like): True labels. y_pred_proba (array-like): Predicted probabilities for the positive class (class 1). Returns: float: The custom score. squared_diff = (y_pred_proba - y_true) ** 2 return np.mean(squared_diff) def main(): # Load the dataset data = load_breast_cancer() X, y = data.data, data.target # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Train the classifier clf = LogisticRegression(max_iter=1000, random_state=42) clf.fit(X_train, y_train) # Predict on the test set y_pred = clf.predict(X_test) y_pred_proba = clf.predict_proba(X_test)[:, 1] # Evaluate the classifier using accuracy score accuracy = accuracy_score(y_test, y_pred) print(f\\"Accuracy Score: {accuracy}\\") # Evaluate the classifier using the custom scoring function custom_score = custom_scoring_function(y_test, y_pred_proba) print(f\\"Custom Score: {custom_score}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Exception Handling **Objective:** Write a function `grade_calculator` that processes a list of student grades and performs the following tasks: 1. Calculates the average grade. 2. Identifies students who scored above the average and returns their names. 3. Handles various exceptions that may occur during the process. **Function Signature:** ```python def grade_calculator(student_grades: dict) -> list: Calculate the average grade and return names of students scoring above the average. Parameters: student_grades (dict): A dictionary where the keys are student names (strings) and the values are their grades (integers or floats). Returns: list: A list of student names who scored above the average grade. Raises: TypeError: If the input is not a dictionary or if grades are not numbers. ValueError: If grades are negative or over 100. ZeroDivisionError: If there are no students in the dictionary. Exception: For any other unforeseen errors. pass ``` **Input and Output:** - The function takes a dictionary where keys are student names (strings) and values are their grades (integers or floats). - The function returns a list of names of students who scored above the average grade. **Constraints:** - Grades must be between 0 and 100 (inclusive). - The dictionary must contain at least one student. - The function should handle various exception scenarios and raise appropriate errors. **Example:** ```python student_grades = { \'Alice\': 85, \'Bob\': 92, \'Charlie\': 78, \'Diana\': 69, \'Eve\': 88 } above_average_students = grade_calculator(student_grades) print(above_average_students) # Output: [\'Alice\', \'Bob\', \'Eve\'] ``` **Exception Handling Requirements:** 1. **TypeError:** If `student_grades` is not a dictionary or if the grades are not of numeric types. 2. **ValueError:** If any grade is negative or greater than 100. 3. **ZeroDivisionError:** If the dictionary is empty and there are no students to process. 4. **General Exception:** Handle any other unforeseen exceptions and raise them appropriately. **Performance Requirements:** - The function should be optimized to handle large dictionaries efficiently. **Test your implementation thoroughly to ensure all scenarios are covered.**","solution":"def grade_calculator(student_grades): Calculate the average grade and return names of students scoring above the average. Parameters: student_grades (dict): A dictionary where the keys are student names (strings) and the values are their grades (integers or floats). Returns: list: A list of student names who scored above the average grade. Raises: TypeError: If the input is not a dictionary or if grades are not numbers. ValueError: If grades are negative or over 100. ZeroDivisionError: If there are no students in the dictionary. Exception: For any other unforeseen errors. if not isinstance(student_grades, dict): raise TypeError(\\"Input must be a dictionary.\\") if not student_grades: raise ZeroDivisionError(\\"The dictionary is empty, no students to process.\\") total_grades = 0 count = 0 for student, grade in student_grades.items(): if not isinstance(grade, (int, float)): raise TypeError(f\\"Grade for student {student} is not a number.\\") if grade < 0 or grade > 100: raise ValueError(f\\"Grade for student {student} is out of bounds (0-100).\\") total_grades += grade count += 1 if count == 0: raise ZeroDivisionError(\\"No students found to calculate average.\\") average_grade = total_grades / count above_average_students = [student for student, grade in student_grades.items() if grade > average_grade] return above_average_students"},{"question":"**Objective:** Implement a Python script that utilizes the `compileall` module to compile a given directory of Python files. The script should demonstrate the use of various options provided by the `compileall` module. **Problem Statement:** Create a script `compile_python_files.py` with the following specifications: 1. The script should accept directories or files to compile as command-line arguments. 2. It should support options to: - Limit recursion depth (`maxlevels`). - Force recompilation (`force`). - Exclude files matching a given regular expression (`rx`). - Set verbosity level for output (`quiet`). - Use multiple worker threads for compilation (`workers`). 3. The script should be able to handle both recursive and non-recursive compilations (`-r` and `-l`). 4. Provide an option to compile with various optimization levels (`optimize`). **Input:** The script should be executed with the following parameters: - `-d` or `--directory`: A list of directories or files to compile. - `-r` or `--recursive`: An integer to set the maximum recursion level (default to no limit). - `-f` or `--force`: A boolean flag to force recompilation. - `-x` or `--exclude`: A regular expression to exclude files. - `-q` or `--quiet`: An integer to set verbosity level (0 for verbose, 1 for errors only, 2 for silent). - `-j` or `--workers`: An integer to specify the number of worker threads (use 0 to auto-detect). - `-o` or `--optimize`: A list of integers defining optimization levels. **Constraints:** - Assume directories and files provided are valid and contain Python scripts. - Handle edge cases like directories without `.py` files gracefully. **Function Signature:** ```python def compile_directory(directory, maxlevels, force, rx, quiet, workers, optimize): pass import argparse import re if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Compile Python files to bytecode.\\") parser.add_argument(\\"-d\\", \\"--directory\\", nargs=\'+\', required=True, help=\\"Directories or files to compile\\") parser.add_argument(\\"-r\\", \\"--recursive\\", type=int, default=None, help=\\"Maximum recursion level\\") parser.add_argument(\\"-f\\", \\"--force\\", action=\'store_true\', help=\\"Force recompilation\\") parser.add_argument(\\"-x\\", \\"--exclude\\", type=str, default=None, help=\\"Regular expression to exclude files\\") parser.add_argument(\\"-q\\", \\"--quiet\\", type=int, choices=[0, 1, 2], default=0, help=\\"Verbosity level\\") parser.add_argument(\\"-j\\", \\"--workers\\", type=int, default=1, help=\\"Number of worker threads\\") parser.add_argument(\\"-o\\", \\"--optimize\\", type=int, nargs=\'*\', default=None, help=\\"Optimization levels\\") args = parser.parse_args() rx = re.compile(args.exclude) if args.exclude else None for dir in args.directory: compile_directory(dir, args.recursive, args.force, rx, args.quiet, args.workers, args.optimize) ``` **Expected Output:** The script should compile the specified directories and produce corresponding `.pyc` files, adhering to the constraints and options provided. **Testing:** 1. Test with a simple directory containing Python files. 2. Test with and without recursion. 3. Test with force recompilation. 4. Test excluding certain files using regex. 5. Test with various verbosity levels. 6. Test using multiple worker threads. 7. Test with different optimization levels. **Example Usage:** ```bash python compile_python_files.py -d src/ -r 2 -f -x \\"test_\\" -q 1 -j 4 -o 1 2 ``` This command should compile all Python files in the `src/` directory and its subdirectories up to two levels deep, force recompilation, exclude files starting with \\"test_\\", show only errors, use 4 worker threads, and compile with optimization levels 1 and 2.","solution":"import compileall import argparse import re import os def compile_directory(directory, maxlevels, force, rx, quiet, workers, optimize): Compiles the given directory of Python files with specified options. Parameters: - directory: Directory or file to compile. - maxlevels: Maximum recursion level for directories. - force: Force recompilation. - rx: Regular expression to exclude files. - quiet: Verbosity level. - workers: Number of worker threads for compilation. - optimize: List of optimization levels. compileall.compile_dir( dir=directory, maxlevels=maxlevels, ddir=None, force=force, rx=rx, quiet=quiet, legacy=False, optimize=optimize, workers=workers ) if __name__ == \\"__main__\\": parser = argparse.ArgumentParser(description=\\"Compile Python files to bytecode.\\") parser.add_argument(\\"-d\\", \\"--directory\\", nargs=\'+\', required=True, help=\\"Directories or files to compile\\") parser.add_argument(\\"-r\\", \\"--recursive\\", type=int, default=None, help=\\"Maximum recursion level\\") parser.add_argument(\\"-f\\", \\"--force\\", action=\'store_true\', help=\\"Force recompilation\\") parser.add_argument(\\"-x\\", \\"--exclude\\", type=str, default=None, help=\\"Regular expression to exclude files\\") parser.add_argument(\\"-q\\", \\"--quiet\\", type=int, choices=[0, 1, 2], default=0, help=\\"Verbosity level\\") parser.add_argument(\\"-j\\", \\"--workers\\", type=int, default=1, help=\\"Number of worker threads\\") parser.add_argument(\\"-o\\", \\"--optimize\\", type=int, nargs=\'*\', default=None, help=\\"Optimization levels\\") args = parser.parse_args() rx = re.compile(args.exclude) if args.exclude else None for dir in args.directory: compile_directory(dir, args.recursive, args.force, rx, args.quiet, args.workers, args.optimize)"},{"question":"Title: Advanced Data Processing with Generators and Comprehensions Objective: To assess the student\'s ability to utilize comprehensions, generators, and asynchronous generators to process and manipulate data efficiently in Python. Problem Statement: You are given a large dataset represented as a list of dictionaries, where each dictionary contains information about a person, including their `name`, `age`, `email`, and `score`. Your task is to implement an asynchronous generator function that filters and processes this data based on given conditions and returns the results lazily. Requirements: 1. Implement the `process_data` asynchronous generator function that processes the input list of dictionaries based on the following conditions: - Only include people who are older than 25 years. - Only include people whose score is greater than or equal to 70. 2. The `process_data` function should yield each person\'s `name` in uppercase and their `email` addresses in lowercase. 3. Write a function `fetch_results` that extracts the results from the `process_data` generator and collects them into a list. Input: - A list of dictionaries where each dictionary has the structure: ```python { \'name\': str, \'age\': int, \'email\': str, \'score\': int } ``` Output: - A list of dictionaries with the structure: ```python [ {\'name\': str, \'email\': str}, ... ] ``` Where each `name` is in uppercase and each `email` is in lowercase. Example: ```python # Sample input data data = [ {\'name\': \'Alice\', \'age\': 30, \'email\': \'Alice@example.com\', \'score\': 85}, {\'name\': \'Bob\', \'age\': 22, \'email\': \'Bob@example.com\', \'score\': 90}, {\'name\': \'Charlie\', \'age\': 27, \'email\': \'Charlie@example.com\', \'score\': 60}, {\'name\': \'David\', \'age\': 40, \'email\': \'David@example.com\', \'score\': 95}, ] # Expected output processed_results = [ {\'name\': \'ALICE\', \'email\': \'alice@example.com\'}, {\'name\': \'DAVID\', \'email\': \'david@example.com\'} ] async def process_data(data): # TODO: Implement this function using asyncio generator pass def fetch_results(data): # TODO: Implement this function to collect results from the process_data generator pass # Usage Example import asyncio async def main(): results = fetch_results(data) print(results) asyncio.run(main()) ``` Constraints: 1. Use comprehensions and generator expressions wherever possible. 2. The `process_data` function must be asynchronous and make use of the `yield` expression. 3. The solution should be efficient and capable of handling large datasets. Performance Requirements: 1. The solution should efficiently handle datasets with up to 10,000 records. 2. The operations should be performed lazily to avoid high memory consumption. Good luck!","solution":"import asyncio async def process_data(data): for person in data: if person[\'age\'] > 25 and person[\'score\'] >= 70: yield {\'name\': person[\'name\'].upper(), \'email\': person[\'email\'].lower()} async def fetch_results(data): results = [] async for result in process_data(data): results.append(result) return results"},{"question":"**Path Manipulation Tool** You are required to write a Python function that takes a list of file paths and performs the following operations: 1. Normalize all the paths to their canonical absolute form. 2. Separate each path into its directory and base components. 3. Return the longest common sub-path from the list of paths. 4. For each path, check if it is an existing file or directory. # Function Signature: ```python def path_manipulation(paths: List[str]) -> Tuple[List[Tuple[str, str]], str, List[bool]]: pass ``` # Input: - `paths` (List[str]): A list of file path strings. # Output: - A tuple containing three elements: 1. A list of tuples. Each tuple contains two strings - the directory and base name of the respective path. 2. A string representing the longest common sub-path from the input list. 3. A list of boolean values indicating whether each corresponding path exists as a file or directory. # Example: **Input:** ```python paths = [\'/home/user/docs\', \'/home/user/docs/report.txt\', \'/home/user/images\'] ``` **Output:** ```python ([ (\'/home/user\', \'docs\'), (\'/home/user/docs\', \'report.txt\'), (\'/home/user\', \'images\') ], \'/home/user\', [True, True, True]) ``` # Constraints: - You must use the functionalities provided by the `os.path` module to handle path manipulations. - Invalid or non-existent paths should be handled gracefully (e.g., by returning `False` for existence checks). # Notes: - Ensure that the function works on any platform (Unix or Windows). - Performance considerations should be taken into account for large lists of paths. # Hint: - Consider using `os.path.abspath`, `os.path.dirname`, `os.path.basename`, `os.path.commonpath`, and `os.path.exists` to accomplish the required tasks.","solution":"import os from typing import List, Tuple def path_manipulation(paths: List[str]) -> Tuple[List[Tuple[str, str]], str, List[bool]]: # Normalize all the paths to their canonical absolute form absolute_paths = [os.path.abspath(path) for path in paths] # Separate each path into its directory and base components dir_base_list = [(os.path.dirname(path), os.path.basename(path)) for path in absolute_paths] # Find the longest common sub-path from the list of paths longest_common_subpath = os.path.commonpath(absolute_paths) # Check if each path is an existing file or directory existence_list = [os.path.exists(path) for path in absolute_paths] return dir_base_list, longest_common_subpath, existence_list"},{"question":"**Objective:** Implement a function that demonstrates your understanding of list manipulation, string operations, and control flow in Python. **Problem Statement:** You are given a list of strings representing names of items sold in a store. Each item name can contain uppercase and lowercase letters. Your task is to write a function `process_items(items: list) -> dict` that processes this list of items and returns a dictionary with the following information: - The first letter of each item name as keys. - Each key should map to a list of items that start with that letter. The items in each list should be sorted in ascending order (case-insensitive). - Additionally, the total count of items for each first letter should be included as a key-value pair with the key as \\"count_<first_letter>\\". **Function Signature:** ```python def process_items(items: list) -> dict: pass ``` **Input:** - `items`: A list of strings, where each string represents an item name. - Example: `[\\"apple\\", \\"banana\\", \\"Avocado\\", \\"apricot\\", \\"Berry\\", \\"Chocolate\\"]` **Output:** - A dictionary where: - Keys are the unique first letters of the item names (case-insensitive). - Values are lists of item names starting with the corresponding key, sorted in ascending order (case-insensitive). - For each unique first letter, include an additional key-value pair where the key is \\"count_<first_letter>\\" and the value is the total count of items starting with that letter. **Example:** ```python items = [\\"apple\\", \\"banana\\", \\"Avocado\\", \\"apricot\\", \\"Berry\\", \\"Chocolate\\"] process_items(items) ``` **Expected Output:** ```python { \'a\': [\'apple\', \'apricot\', \'Avocado\'], \'count_a\': 3, \'b\': [\'banana\', \'Berry\'], \'count_b\': 2, \'c\': [\'Chocolate\'], \'count_c\': 1 } ``` **Constraints:** - The function should be case-insensitive when grouping and sorting the items. - The input list can contain between 1 and 1000 items. - Each item name string has a maximum length of 50 characters. **Additional Notes:** - Remember to handle both uppercase and lowercase letters. - Ensure the function has a linearithmic time complexity O(n log n) considering the sorting step, where n is the number of items in the list.","solution":"def process_items(items): Process the list of items and return a dictionary with information about the items grouped by their first letter. from collections import defaultdict # Initialize a default dictionary to store lists of items grouped_items = defaultdict(list) # Extract items by their first letter in a case-insensitive manner for item in items: first_letter = item[0].lower() grouped_items[first_letter].append(item) result = {} # Sort each list of items and prepare the final dictionary for letter, items_list in grouped_items.items(): items_list.sort(key=lambda x: x.lower()) result[letter] = items_list result[f\\"count_{letter}\\"] = len(items_list) return result"},{"question":"# PyTorch Coding Assessment You are required to implement a function that: 1. Constructs a neural network model. 2. Computes the forward and backward passes using both standard and custom methods. 3. Profiles the performance of the forward and backward passes. 4. Validates the custom backward function with the autograd package. # Function Signature ```python import torch import torch.nn as nn import torch.autograd as autograd import torch.autograd.profiler as profiler def custom_backward(ctx, grad_output): Custom backward function for autograd to compute gradients. # Retrieve saved tensors inputs, weights = ctx.saved_tensors grad_input = grad_output.mm(weights.t()) grad_weight = inputs.t().mm(grad_output) return grad_input, grad_weight def custom_model(input_tensor): Build a neural network model, compute forward and backward passes, and profile the computations. Parameters: - input_tensor (torch.Tensor): A tensor of shape (batch_size, input_dim) Returns: - outputs (torch.Tensor): Model outputs tensor - loss (torch.Tensor): Computed loss tensor - grads (dict): A dictionary containing the gradients of the model\'s parameters - profile (str): Profiling results in chrome tracing format class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.linear1 = nn.Linear(input_tensor.size(1), 50) self.linear2 = nn.Linear(50, 10) def forward(self, x): x = torch.relu(self.linear1(x)) x = self.linear2(x) return x # Model, loss, and optimizer net = SimpleNN() criterion = nn.CrossEntropyLoss() # Forward pass outputs = net(input_tensor) targets = torch.randint(0, 10, (input_tensor.size(0),), dtype=torch.long) loss = criterion(outputs, targets) # Attach custom backward ctx = autograd.function.FunctionCtx() ctx.save_for_backward(input_tensor, net.linear1.weight) custom_grad = custom_backward(ctx, torch.ones_like(output)) # Compute gradients grads = {} with autograd.detect_anomaly(): net.zero_grad() loss.backward() for name, param in net.named_parameters(): grads[name] = param.grad.clone() # Profile forward and backward pass with profiler.profile(with_stack=True) as prof: with profiler.record_function(\\"model_inference\\"): outputs = net(input_tensor) with profiler.record_function(\\"model_backward\\"): loss.backward() prof.export_chrome_trace(\\"trace.json\\") profile_result = prof.key_averages().table(sort_by=\\"self_cpu_time_total\\") return outputs, loss, grads, profile_result ``` Constraints: 1. `input_tensor` should be a 2D tensor with shape `(batch_size, input_dim)`. 2. Use the custom backward function for gradient calculations. 3. The profiling result should be returned as a formatted string. Notes: - Ensure that the custom backward function calculates gradients correctly. - Profile both forward and backward passes and analyze their performance. - Validate all operations using `autograd.detect_anomaly`.","solution":"import torch import torch.nn as nn import torch.autograd as autograd import torch.autograd.profiler as profiler class CustomLinearFunction(autograd.Function): @staticmethod def forward(ctx, input, weight, bias=None): ctx.save_for_backward(input, weight, bias) output = input.mm(weight.t()) if bias is not None: output += bias.unsqueeze(0).expand_as(output) return output @staticmethod def backward(ctx, grad_output): input, weight, bias = ctx.saved_tensors grad_input = grad_output.mm(weight) grad_weight = grad_output.t().mm(input) grad_bias = grad_output.sum(0) if bias is not None else None return grad_input, grad_weight, grad_bias class SimpleNN(nn.Module): def __init__(self, input_dim, hidden_dim, output_dim): super(SimpleNN, self).__init__() self.linear1 = nn.Linear(input_dim, hidden_dim) self.linear2 = nn.Linear(hidden_dim, output_dim) def forward(self, x): x = torch.relu(self.linear1(x)) x = self.custom_linear(x, self.linear2.weight, self.linear2.bias) return x def custom_linear(self, x, weight, bias): return CustomLinearFunction.apply(x, weight, bias) def custom_model(input_tensor): Build a neural network model, compute forward and backward passes, and profile the computations. Parameters: - input_tensor (torch.Tensor): A tensor of shape (batch_size, input_dim) Returns: - outputs (torch.Tensor): Model outputs tensor - loss (torch.Tensor): Computed loss tensor - grads (dict): A dictionary containing the gradients of the model\'s parameters - profile (str): Profiling results in chrome tracing format input_dim = input_tensor.size(1) hidden_dim = 50 output_dim = 10 net = SimpleNN(input_dim, hidden_dim, output_dim) criterion = nn.CrossEntropyLoss() targets = torch.randint(0, output_dim, (input_tensor.size(0),), dtype=torch.long) with profiler.profile(with_stack=True, use_cuda=False) as prof: with profiler.record_function(\\"model_inference\\"): outputs = net(input_tensor) loss = criterion(outputs, targets) net.zero_grad() with profiler.record_function(\\"model_backward\\"): loss.backward() prof.export_chrome_trace(\\"trace.json\\") profile_result = prof.key_averages().table(sort_by=\\"self_cpu_time_total\\") grads = {name: param.grad.clone() for name, param in net.named_parameters()} return outputs, loss, grads, profile_result"},{"question":"# Assess Your Seaborn Skills: Comprehensive Data Visualization Task Objective: Implement a function to create a comprehensive statistical visualization using Seaborn. This task will test your ability to incorporate various Seaborn features, including rug plots, hue mappings, aesthetic customizations, and handling larger datasets. Task: Write a function `visualize_tips_and_diamonds()` that accomplishes the following: 1. **Load the `tips` dataset** using Seaborn. 2. **Create a scatter plot** of `total_bill` vs `tip` with hue mapping for the `time` variable. 3. **Add rug plots** on both the x and y axes of the scatter plot, using different colors based on the `time` variable. 4. **Customize the rug plot** characteristics: * Adjust the rug thickness to be thinner. * Increase the height of the rug tick marks to 0.1. * Shift the rug positions to be slightly outside the axes without clipping. 5. **Overlay a density plot** using a kernel density estimate (KDE) for the `tips` dataset on the scatter plot. 6. **Load the `diamonds` dataset** using Seaborn. 7. **Create a scatter plot** of `carat` vs `price` for the `diamonds` dataset using small points (size = 5). 8. **Add a rug plot** to this scatter plot to show individual observations, using thin lines (line width = 1) and alpha blending (alpha = 0.005). 9. Display the final plots. Function Signature: ```python def visualize_tips_and_diamonds(): pass ``` Expected Output: The function does not return any value but should display the visualizations specified. Instructions: - Ensure your solution is clean and well-documented. - Make use of Seaborn and Matplotlib for the visualizations. - Include all necessary imports within the function. - Avoid using global variables. Constraints: - Use only the Seaborn and Matplotlib libraries for this visualization task. - Your visualization should be clear, well-labeled, and should handle larger datasets effectively. Would you recommend any additional components to make the visualization more insightful? Go ahead and include them! Use the given code snippets from the documentation to guide your implementation. Good luck, and happy plotting!","solution":"import seaborn as sns import matplotlib.pyplot as plt def visualize_tips_and_diamonds(): # Load the \'tips\' dataset tips = sns.load_dataset(\\"tips\\") # Create a scatter plot of \'total_bill\' vs \'tip\' with hue mapping for the \'time\' variable scatter_plot = sns.scatterplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\') # Add rug plots to the x and y axes of the scatter plot sns.rugplot(data=tips, x=\'total_bill\', hue=\'time\', height=0.1, linewidth=0.5, alpha=0.7) sns.rugplot(data=tips, y=\'tip\', hue=\'time\', height=0.1, linewidth=0.5, alpha=0.7) # Overlay a density plot using a kernel density estimate (KDE) sns.kdeplot(data=tips, x=\'total_bill\', y=\'tip\', hue=\'time\') plt.title(\\"Scatter Plot of Tips with KDE and Rug Plot\\") plt.xlabel(\\"Total Bill\\") plt.ylabel(\\"Tip\\") plt.legend() plt.show() # Load the \'diamonds\' dataset diamonds = sns.load_dataset(\\"diamonds\\") # Create a scatter plot of \'carat\' vs \'price\' for the \'diamonds\' dataset using small points scatter_plot = sns.scatterplot(data=diamonds, x=\'carat\', y=\'price\', size=5, edgecolor=None, alpha=0.6) # Add a rug plot to the scatter plot to show individual observations sns.rugplot(data=diamonds, x=\'carat\', height=0.02, linewidth=1, alpha=0.005) sns.rugplot(data=diamonds, y=\'price\', height=0.02, linewidth=1, alpha=0.005) plt.title(\\"Scatter Plot of Carat vs Price with Rug Plot\\") plt.xlabel(\\"Carat\\") plt.ylabel(\\"Price\\") plt.show() # Note: As the function is for visualization purposes, there\'s no \'return\' statement."},{"question":"**Custom Command Line Interface with cmd Module** **Objective:** Create a custom command line interface to manage a simple todo list using the `cmd` module. The interface should allow the user to add, remove, list, and mark tasks as complete. **Requirements:** - Create a class `TodoShell` that inherits from `cmd.Cmd`. - Implement the following commands: - `do_add(task_name)`: Adds a new task to the todo list. - `do_remove(task_index)`: Removes the task at the specified index. - `do_list()`: Lists all tasks with their status (complete/incomplete). - `do_complete(task_index)`: Marks the task at the specified index as complete. - `do_exit()`: Exits the command loop. **Constraints:** - Task names should be strings with a maximum length of 50 characters. - Task index should be valid; if not, print an appropriate error message. - Use a list to store tasks. Each task can be represented as a dictionary with keys \'name\' and \'complete\', where \'complete\' is a boolean. **Expected Input and Output Formats:** - Commands should be entered in the format specified above (e.g., `add \\"Buy groceries\\"`, `remove 1`, etc.). - The output should clearly display the list of tasks when `list` command is executed, and appropriate messages for other commands. **Performance Requirements:** - Ensure the command handler is responsive and correctly executes the specified tasks. **Example Usage:** ```python (TodoShell) add \\"Buy groceries\\" Added task: Buy groceries (TodoShell) add \\"Write report\\" Added task: Write report (TodoShell) list 1. [ ] Buy groceries 2. [ ] Write report (TodoShell) complete 1 Marked task 1 as complete: Buy groceries (TodoShell) list 1. [x] Buy groceries 2. [ ] Write report (TodoShell) remove 2 Removed task 2: Write report (TodoShell) list 1. [x] Buy groceries (TodoShell) exit Goodbye! ``` **Implementation:** Below is a skeleton of the `TodoShell` class that you need to complete: ```python import cmd class TodoShell(cmd.Cmd): intro = \\"Welcome to the Todo Shell. Type help or ? to list commands.n\\" prompt = \\"(TodoShell) \\" def __init__(self): super().__init__() self.tasks = [] def do_add(self, task_name): Add a new task to the todo list. # Your code here def do_remove(self, task_index): Remove the task at the given index. # Your code here def do_list(self, arg): List all tasks with their completion status. # Your code here def do_complete(self, task_index): Mark the task at the given index as complete. # Your code here def do_exit(self, arg): Exit the todo shell. print(\\"Goodbye!\\") return True if __name__ == \\"__main__\\": TodoShell().cmdloop() ``` Complete the class by implementing the methods as described above.","solution":"import cmd class TodoShell(cmd.Cmd): intro = \\"Welcome to the Todo Shell. Type help or ? to list commands.n\\" prompt = \\"(TodoShell) \\" def __init__(self): super().__init__() self.tasks = [] def do_add(self, task_name): Add a new task to the todo list. task_name = task_name.strip(\'\\"\') if len(task_name) == 0: print(\\"Task name cannot be empty.\\") elif len(task_name) > 50: print(\\"Task name cannot be longer than 50 characters.\\") else: self.tasks.append({\'name\': task_name, \'complete\': False}) print(f\'Added task: {task_name}\') def do_remove(self, task_index): Remove the task at the given index. try: index = int(task_index) - 1 if index < 0 or index >= len(self.tasks): raise IndexError removed_task = self.tasks.pop(index) print(f\'Removed task {task_index}: {removed_task[\\"name\\"]}\') except (ValueError, IndexError): print(\'Error: Invalid task index.\') def do_list(self, arg): List all tasks with their completion status. if not self.tasks: print(\'No tasks in the list.\') else: for i, task in enumerate(self.tasks): status = \'[x]\' if task[\'complete\'] else \'[ ]\' print(f\'{i + 1}. {status} {task[\\"name\\"]}\') def do_complete(self, task_index): Mark the task at the given index as complete. try: index = int(task_index) - 1 if index < 0 or index >= len(self.tasks): raise IndexError self.tasks[index][\'complete\'] = True print(f\'Marked task {task_index} as complete: {self.tasks[index][\\"name\\"]}\') except (ValueError, IndexError): print(\'Error: Invalid task index.\') def do_exit(self, arg): Exit the todo shell. print(\\"Goodbye!\\") return True if __name__ == \\"__main__\\": TodoShell().cmdloop()"},{"question":"# Advanced Python Coding Assessment: Custom Interactive Console Your task is to implement a Python class named `CustomConsole` that leverages the `readline` module to provide an enhanced interactive console experience. The console should support the following features: 1. **Command History:** Automatically load and save command history from a specified file. 2. **Custom Auto-completion:** Implement a custom tab-completion mechanism for a predefined set of commands. 3. **Configuration Parsing:** Dynamically parse and apply configuration settings from a given string using the `parse_and_bind` function. # Class Definition You are required to implement the `CustomConsole` class with the following methods: - `__init__(self, histfile: str, commands: list)`: Initializes the console with the specified history file and list of available commands. - `load_history(self)`: Loads the command history from the specified history file. - `save_history(self)`: Saves the command history to the specified history file. - `set_completer(self)`: Sets up a custom completer function to complete commands from the provided list. - `parse_config(self, config: str)`: Parses and applies configuration settings from a given string. # Expected Input and Output Formats - The `histfile` parameter is a string representing the file path to load and save the command history. - The `commands` parameter is a list of strings representing the available commands for auto-completion. - The `config` parameter is a string containing configuration commands suitable for the `readline.parse_and_bind` function. # Example Usage ```python import readline class CustomConsole: def __init__(self, histfile: str, commands: list): self.histfile = histfile self.commands = commands self.load_history() self.set_completer() def load_history(self): try: readline.read_history_file(self.histfile) except FileNotFoundError: pass def save_history(self): readline.write_history_file(self.histfile) def set_completer(self): def completer(text, state): options = [cmd for cmd in self.commands if cmd.startswith(text)] if state < len(options): return options[state] else: return None readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") def parse_config(self, config: str): readline.parse_and_bind(config) # Example commands = [\\"list\\", \\"show\\", \\"exit\\", \\"help\\", \\"remove\\"] config_string = \\"set editing-mode vi\\" console = CustomConsole(histfile=\\"~/.custom_history\\", commands=commands) console.parse_config(config_string) # Add a hook to save the history when the program exits import atexit atexit.register(console.save_history) ``` # Constraints - The history file must be automatically managed, meaning it should be loaded when the console starts and saved when it exits. - The auto-completion should only suggest available commands that start with the current input text. - The configuration string for `parse_config` should be correctly formatted and compatible with `readline.parse_and_bind`. Your implementation should ensure that the features work seamlessly in an interactive Python session.","solution":"import readline import os class CustomConsole: def __init__(self, histfile: str, commands: list): self.histfile = os.path.expanduser(histfile) self.commands = commands self.load_history() self.set_completer() def load_history(self): try: readline.read_history_file(self.histfile) except FileNotFoundError: pass def save_history(self): readline.write_history_file(self.histfile) def set_completer(self): def completer(text, state): options = [cmd for cmd in self.commands if cmd.startswith(text)] if state < len(options): return options[state] else: return None readline.set_completer(completer) readline.parse_and_bind(\\"tab: complete\\") def parse_config(self, config: str): readline.parse_and_bind(config) # Add a hook to save the history when the program exits import atexit commands = [\\"list\\", \\"show\\", \\"exit\\", \\"help\\", \\"remove\\"] config_string = \\"set editing-mode vi\\" console = CustomConsole(histfile=\\"~/.custom_history\\", commands=commands) console.parse_config(config_string) atexit.register(console.save_history)"},{"question":"# Custom Import System in Python Problem Statement You are required to design a custom import mechanism that allows importing modules not only from the file system but also from a predefined dictionary called `custom_module_store`. This dictionary will act as an in-memory storage for your Python modules. Task 1. **Define a Custom Meta Path Finder:** - Create a class `CustomMetaPathFinder` that implements the `find_spec` method. - The `find_spec` method should search for module names in the `custom_module_store` dictionary. 2. **Define a Custom Loader:** - Create a class `CustomLoader` which implements the `exec_module` method. - The `exec_module` method should execute the module\'s code stored as a string in the `custom_module_store`. 3. **Initialize the Custom Import System:** - Add an instance of `CustomMetaPathFinder` to `sys.meta_path`. Example ```python custom_module_store = { \'my_module\': \'\'\' def greet(name): return f\\"Hello, {name}!\\" \'\'\' } # Your implementation here import my_module print(my_module.greet(\'Python\')) # Output: Hello, Python! ``` Input - There are no direct input parameters. The task involves modifying the `sys.meta_path` and handling the module import mechanism. Output - The module should be imported successfully from the `custom_module_store`, and any functions or classes within it should be executable. Constraints - You should not modify global Python settings beyond this exercise. - The module name does not conflict with any existing built-in or standard library modules. Performance Requirements - Ensure that the custom import system does not significantly degrade the performance of the Python interpreter for other imports. Discussion Points - Explain how the custom meta path finder works and how it interacts with Python\'s import system. - Describe the responsibilities of the custom loader in executing the module\'s code. - Discuss potential pitfalls when extending the import system and how to avoid them.","solution":"import sys import importlib.abc import importlib.util import types # In-memory storage for custom modules custom_module_store = {} class CustomMetaPathFinder(importlib.abc.MetaPathFinder): Custom meta path finder that searches for modules in the custom_module_store dictionary. def find_spec(self, fullname, path, target=None): if fullname in custom_module_store: return importlib.util.spec_from_loader(fullname, CustomLoader()) return None class CustomLoader(importlib.abc.Loader): Custom loader that executes the module\'s code stored in the custom_module_store. def exec_module(self, module): module_code = custom_module_store[module.__name__] exec(module_code, module.__dict__) # Add the custom meta path finder to the sys.meta_path sys.meta_path.insert(0, CustomMetaPathFinder()) # Example of adding a module to the custom_module_store custom_module_store[\'my_module\'] = \'\'\' def greet(name): return f\\"Hello, {name}!\\" \'\'\'"},{"question":"Objective Your task is to implement a PyTorch model that dynamically switches its computation based on the contents of the input tensor. Specifically, you need to use the `torch.cond` function to achieve this behavior. Problem Statement Create a PyTorch `nn.Module` class named `ThresholdCondModel`. This model should use the `torch.cond` function to apply different transformations to the input tensor based on whether its mean value is greater than a specified threshold. Requirements 1. Implement the `ThresholdCondModel` class that inherits from `torch.nn.Module`. 2. The class should accept a threshold value as an initialization parameter. 3. In the `forward` method, use `torch.cond` to apply one of the two functions to the input tensor: - If the mean of the input tensor is greater than the threshold, apply the `high_mean_fn` function. - If the mean of the input tensor is not greater than the threshold, apply the `low_mean_fn` function. 4. Define the `high_mean_fn` and `low_mean_fn` functions inside the class as follows: - `high_mean_fn(x: torch.Tensor) -> torch.Tensor`: This function should return the element-wise square of the input tensor, i.e., `x ** 2`. - `low_mean_fn(x: torch.Tensor) -> torch.Tensor`: This function should return the element-wise negation of the input tensor, i.e., `-x`. Input - A single PyTorch tensor `x` of any shape and containing floating-point values. Output - A PyTorch tensor of the same shape as `x`, transformed according to the specified conditions. Example ```python import torch # Define the ThresholdCondModel class here # Initialize the model with a threshold value of 0.5 model = ThresholdCondModel(threshold=0.5) # Input tensor input_tensor = torch.tensor([[-1.0, 2.0], [3.0, -4.0]]) # Forward pass output_tensor = model(input_tensor) print(output_tensor) ``` If the mean of `input_tensor` is greater than 0.5, the output should be: ```python tensor([[ 1.0000, 4.0000], [ 9.0000, 16.0000]]) ``` Otherwise, the output should be: ```python tensor([[ 1.0000, -2.0000], [-3.0000, 4.0000]]) ``` Constraints - The operations within `high_mean_fn` and `low_mean_fn` should be element-wise tensor operations. - Do not use any looping constructs; rely on tensor operations for efficiency. Evaluation Criteria - Correct implementation of the `ThresholdCondModel` class. - Proper usage of the `torch.cond` function. - Adherence to the problem constraints and requirements. - Efficient and clear code.","solution":"import torch import torch.nn as nn class ThresholdCondModel(nn.Module): def __init__(self, threshold: float): super(ThresholdCondModel, self).__init__() self.threshold = threshold def high_mean_fn(self, x: torch.Tensor) -> torch.Tensor: return x ** 2 def low_mean_fn(self, x: torch.Tensor) -> torch.Tensor: return -x def forward(self, x: torch.Tensor) -> torch.Tensor: mean_value = x.mean().item() if mean_value > self.threshold: return self.high_mean_fn(x) else: return self.low_mean_fn(x)"},{"question":"**Question:** You are tasked with creating a custom transformation of a PyTorch model using the `torch.fx` module. Your objective is to design a function that takes a model and replaces all ReLU activations (`torch.nn.ReLU`) with Leaky ReLU activations (`torch.nn.LeakyReLU`) in the computational graph. # Task 1. Write a function `replace_relu_with_leaky_relu` that takes a PyTorch module `model` and returns a new module with all ReLU activations replaced by Leaky ReLU activations with a negative slope of 0.01. # Requirements - **Input Format:** - `model` (torch.nn.Module): A PyTorch model containing various layers including ReLU activations. - **Output Format:** - A new PyTorch `nn.Module` with all ReLU activations replaced by Leaky ReLU activations. # Constraints - Use the `torch.fx` module for performing the transformation. - Ensure that the transformed module maintains the same computational graph structure except for the replacement of ReLU with Leaky ReLU. - Handle both direct usage of `torch.nn.ReLU` as well as any potential wrapped usage within other modules. # Example ```python import torch.fx import torch.nn as nn # Define a sample model with ReLU activations class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.layer1 = nn.Linear(10, 10) self.relu = nn.ReLU() self.layer2 = nn.Linear(10, 10) def forward(self, x): x = self.layer1(x) x = self.relu(x) x = self.layer2(x) return x # Function to replace ReLU with Leaky ReLU def replace_relu_with_leaky_relu(model: nn.Module) -> nn.Module: class ReplaceReluTracer(torch.fx.Tracer): def is_leaf_module(self, m: nn.Module, module_qualified_name: str) -> bool: # Treat ReLU as a leaf module so we can replace it if isinstance(m, nn.ReLU): return True return super().is_leaf_module(m, module_qualified_name) # Step 1: Symbolically trace the model to get a graph representation graph = ReplaceReluTracer().trace(model) # Step 2: Modify the graph to replace ReLU with Leaky ReLU for node in graph.nodes: if node.op == \'call_module\' and isinstance(graph.owning_module.get_submodule(node.target), nn.ReLU): with graph.inserting_after(node): new_node = graph.call_module(node.target, args=node.args, kwargs=node.kwargs) node.target = nn.LeakyReLU(negative_slope=0.01) node.replace_all_uses_with(new_node) # Step 3: Return new GraphModule with modified graph new_model = torch.fx.GraphModule(model, graph) return new_model # Initialize sample model model = SampleModel() # Apply transformation transformed_model = replace_relu_with_leaky_relu(model) ``` Make sure to test your function with various models to ensure it works properly and converts all ReLU activations as expected. # Performance Requirements - The transformation should perform efficiently on models with a reasonable number of layers (e.g., up to dozens of layers). - The resulting module should execute with similar performance to the original module, with only minor overhead due to using Leaky ReLU instead of ReLU.","solution":"import torch import torch.nn as nn import torch.fx def replace_relu_with_leaky_relu(model: nn.Module) -> nn.Module: class ReplaceReluTracer(torch.fx.Tracer): def is_leaf_module(self, m: nn.Module, module_qualified_name: str) -> bool: if isinstance(m, nn.ReLU): return True return super().is_leaf_module(m, module_qualified_name) graph = ReplaceReluTracer().trace(model) # Retrieve the owning module to modify the underlying modules directly mod = torch.fx.GraphModule(model, graph) # Step through each node for node in graph.nodes: if node.op == \'call_module\' and isinstance(mod.get_submodule(node.target), nn.ReLU): # Replace the ReLU module setattr(mod, node.target, nn.LeakyReLU(negative_slope=0.01)) return mod"},{"question":"Use of asyncio Synchronization Primitives You are tasked with implementing a simulation of a library system where multiple users (tasks) can borrow and return books. The library has a limited number of copies for each book. You should use asyncio synchronization primitives to manage the concurrent access to these resources effectively. Requirements: 1. Implement a class `Library` that: - Maintains a dictionary of books with their corresponding availability (i.e., number of copies available). - Uses an `asyncio.Semaphore` to manage the number of copies available for each book. 2. Implement the following methods for the `Library` class: - `__init__(self, books: Dict[str, int])`: Initializes the library with a dictionary of books and their counts. - `async borrow_book(self, book_name: str)`: Method to borrow a book. Should wait if no copies are available. - `async return_book(self, book_name: str)`: Method to return a borrowed book, releasing the semaphore. - `book_available(self, book_name: str) -> bool`: Returns True if a book is available for borrowing. 3. Implement a coroutine `user(library: Library, book_name: str, wait_time: int)` that: - Simulates a user trying to borrow a book, waits for a specified `wait_time`, and then returns the book. - Should print messages when a user borrows and returns a book. 4. Demonstrate the functionality with an `asyncio` event loop that: - Creates a `Library` instance with a mix of books and copy counts. - Spawns multiple user tasks trying to borrow and return books concurrently. Constraints: - You may assume the total number of users will not exceed 100. - The library will have up to 10 different books. - Each book can have between 1 to 5 copies. Expected Input and Output Format: - **Input**: A dictionary of books with counts to create the `Library`. - **Output**: Console print statements indicating when a user borrows and returns a book. # Example: ```python async def main(): books = { \'Book A\': 2, \'Book B\': 1, \'Book C\': 3 } library = Library(books) await asyncio.gather( user(library, \'Book A\', 3), user(library, \'Book A\', 2), user(library, \'Book B\', 1), user(library, \'Book C\', 4) ) asyncio.run(main()) ``` **Output:** ``` User is trying to borrow \'Book A\'. User successfully borrowed \'Book A\'. User is trying to borrow \'Book A\'. User successfully borrowed \'Book A\'. User is trying to borrow \'Book B\'. User successfully borrowed \'Book B\'. User is trying to borrow \'Book C\'. User successfully borrowed \'Book C\'. User returned \'Book A\'. User returned \'Book B\'. User returned \'Book C\'. User returned \'Book A\'. ``` Note: - Ensure the `Library` class handles concurrent access correctly. - Semaphore is used to manage the availability of book copies.","solution":"import asyncio from typing import Dict class Library: def __init__(self, books: Dict[str, int]): Initialize the library with a dictionary of books and their counts. self.books = {book: asyncio.Semaphore(count) for book, count in books.items()} async def borrow_book(self, book_name: str): Method to borrow a book. Should wait if no copies are available. await self.books[book_name].acquire() print(f\\"User successfully borrowed \'{book_name}\'.\\") async def return_book(self, book_name: str): Method to return a borrowed book, releasing the semaphore. self.books[book_name].release() print(f\\"User returned \'{book_name}\'.\\") def book_available(self, book_name: str) -> bool: Returns True if a book is available for borrowing. return self.books[book_name]._value > 0 async def user(library: Library, book_name: str, wait_time: int): Simulates a user trying to borrow a book, waits for a specified wait_time, and then returns the book. Should print messages when a user borrows and returns a book. print(f\\"User is trying to borrow \'{book_name}\'.\\") await library.borrow_book(book_name) await asyncio.sleep(wait_time) await library.return_book(book_name) async def main(): books = { \'Book A\': 2, \'Book B\': 1, \'Book C\': 3 } library = Library(books) await asyncio.gather( user(library, \'Book A\', 3), user(library, \'Book A\', 2), user(library, \'Book B\', 1), user(library, \'Book C\', 4) ) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Question: Implementing a Custom Python Object in C** You are tasked with creating a custom Python object in C, mimicking the behavior outlined in the provided documentation. Your custom object should include both fixed-size and variable-size elements. # Objectives: 1. Implement a fixed-size custom Python object. 2. Implement a variable-size custom Python object. 3. Write initialization and deletion functions for these objects. 4. Ensure that the object exhibits correct memory management and integration with Python\'s cyclic garbage collector if necessary. # Components to Implement: 1. **Fixed-Size Custom Object**: - Write a function to allocate and initialize the object using `PyObject_New` and `PyObject_Init`. - Include a custom type for this object with at least one field. - Ensure the custom type properly defines the basic size of the object. 2. **Variable-Size Custom Object**: - Write a function to allocate and initialize a variable-size object using `PyObject_NewVar` and `PyObject_InitVar`. - Include a custom type for this object which includes an array or list whose size is determined at the time of object creation. - Implement the extension of the custom type to handle the additional array fields correctly. 3. **Testing Code**: - Create an example script in Python to instantiate, utilize, and delete instances of your custom objects. - Ensure that no memory leeks occur during tests by tracking the allocation and deallocation. # Input: - The initialization function should take parameters for setting up the object type and size. - The deletion function should properly release memory. # Output: - Properly allocated and initialized custom objects. - Deletion should release all allocated memory without errors. # Constraints: - Write your implementation using C-API functions described. - The object should be compatible with Python 3.10. - Ensure proper error handling within your C code, avoiding segmentation faults and memory corruption issues. Below is a skeleton to guide your implementation: ```c #include <Python.h> typedef struct { PyObject_HEAD // Add your fixed-size fields here } FixedSizeObject; typedef struct { PyVarObject_HEAD // Add your variable-size fields here } VariableSizeObject; // Define the initialization and deallocation functions static PyObject * FixedSize_new(PyTypeObject *type, PyObject *args, PyObject *kwds); static PyObject * VariableSize_new(PyTypeObject *type, PyObject *args, PyObject *kwds); static void FixedSize_dealloc(FixedSizeObject *self); static void VariableSize_dealloc(VariableSizeObject *self); // Implement FixedSize_new using PyObject_New and PyObject_Init // Implement VariableSize_new using PyObject_NewVar and PyObject_InitVar // Implement the deallocation functions using PyObject_Del // Define the struct that holds the type information for both objects static PyTypeObject FixedSizeType = { /* Type object setup for FixedSizeObject */ }; static PyTypeObject VariableSizeType = { /* Type object setup for VariableSizeObject */ }; // Module initialization static PyModuleDef custommodule = { PyModuleDef_HEAD_INIT, \\"custom\\", \\"Module that creates a custom Python object\\", -1, NULL, NULL, NULL, NULL, NULL }; PyMODINIT_FUNC PyInit_custom(void) { PyObject *m; if (PyType_Ready(&FixedSizeType) < 0) return NULL; if (PyType_Ready(&VariableSizeType) < 0) return NULL; m = PyModule_Create(&custommodule); if (m == NULL) return NULL; Py_INCREF(&FixedSizeType); Py_INCREF(&VariableSizeType); PyModule_AddObject(m, \\"FixedSizeObject\\", (PyObject *)&FixedSizeType); PyModule_AddObject(m, \\"VariableSizeObject\\", (PyObject *)&VariableSizeType); return m; } ``` Make sure to complete the definitions for your object types and ensure proper memory management throughout your implementation.","solution":"# Note: The following Python code represents a conceptual simulation of the C implementation described in the prompt. class FixedSizeObject: A Python simulation of a fixed-size custom object. def __init__(self, value): self.value = value class VariableSizeObject: A Python simulation of a variable-size custom object which mimics behavior. This object holds a list whose size is determined at initialization. def __init__(self, size): self.values = [0] * size # Example usage fixed_obj = FixedSizeObject(10) variable_obj = VariableSizeObject(5) print(fixed_obj.value) # Should print: 10 print(variable_obj.values) # Should print: [0, 0, 0, 0, 0]"},{"question":"Objective: Implement a function using PyTorch\'s CUDAGraph Trees that performs efficient tensor operations with dynamic shapes and handles input mutation properly. The function should ensure optimal execution with CUDA Graph replay paths and respect the constraints of CUDAGraph Trees. Problem Statement: You are tasked with implementing a function `dynamic_cuda_graph_tree` that takes a batch of tensors and performs the following operations: 1. Each input tensor is squared (element-wise) if the sum of the elements is greater than a threshold, otherwise, its absolute values are squared. 2. The result is then multiplied by a random tensor of the same shape. 3. Apply an in-place increment operation on the result. You need to ensure that the function is properly optimized using CUDA Graph Trees, handles any dynamic shape inputs efficiently, and supports input mutation where applicable. Specifications: - **Function Name**: `dynamic_cuda_graph_tree` - **Input**: - A list of tensors, `inputs` (each tensor has dynamic shapes and is allocated on the CUDA device). - An integer threshold, `threshold`. - **Output**: - A list of tensors, which are the results of the specified operations. - **Constraints**: - The function must handle dynamic shapes by appropriate re-recording of CUDA Graphs. - Efficient memory utilization using CUDA Graph Trees. - Proper handling of input mutations as specified. Example: ```python import torch @torch.compile(mode=\\"reduce-overhead\\") def dynamic_cuda_graph_tree(inputs, threshold): results = [] for x in inputs: if x.sum() > threshold: y = x * x else: y = (x.abs() * x.abs()) z = y * torch.rand_like(y) z.add_(1) # In-place increment operation results.append(z) return results # Example usage inputs = [torch.randn((10, i), device=\\"cuda\\") for i in range(1, 11)] threshold = 10.0 results = dynamic_cuda_graph_tree(inputs, threshold) print(results) ``` Ensure your implementation adheres to the constraints and efficiently utilizes CUDA Graph Trees.","solution":"import torch def dynamic_cuda_graph_tree(inputs, threshold): Function to perform tensor operations using CUDA Graph Trees. Args: inputs (list of torch.Tensor): List of input tensors with dynamic shapes. threshold (int): Threshold to decide operation on each tensor. Returns: list of torch.Tensor: List of result tensors after operations. # Use CUDA graph if available use_cuda_graph = torch.cuda.is_available() results = [] for x in inputs: if x.sum() > threshold: y = x * x else: y = (x.abs() * x.abs()) z = y * torch.rand_like(y) z.add_(1) # In-place increment operation results.append(z) return results"},{"question":"You are provided with a set of text files containing sentences in various cases (upper, lower, random). Your task is to write Python code utilizing the `pipes` module to process these files according to specified requirements. # Requirements 1. Create a pipeline that performs the following operations in sequence: - Convert all lowercase characters to uppercase. - Replace spaces with underscores. - Reverse the order of characters in each sentence. 2. Your implementation should define a function `process_files(infile, outfile)` where: - `infile` is the path to the input text file. - `outfile` is the path to the output text file where the processed content will be saved. 3. Ensure the output file contains the content transformed exactly as specified by the pipeline. # Example Consider the content of `infile.txt` containing: ``` Hello World python code ``` The processed output in `outfile.txt` should be: ``` DLROW_OLLEH EDOC_NOHTYP ``` # Constraints - Assume all input files are plain text and contain multiple lines. - Ensure to handle any possible exceptions gracefully. - Assume the environment is Unix-based as the `pipes` module relies on Unix shell commands. # Code Template ```python import pipes def process_files(infile, outfile): t = pipes.Template() # Append commands to the pipeline t.append(\'tr a-z A-Z\', \'--\') # Converts lowercase to uppercase t.append(\\"sed \'s/ /_/g\'\\", \'--\') # Replaces spaces with underscores t.append(\\"rev\\", \'--\') # Reverses each line # Open the input file through the pipeline and write to the output file with t.open(outfile, \'w\') as f_out: with open(infile, \'r\') as f_in: for line in f_in: f_out.write(line) # Example usage process_files(\'infile.txt\', \'outfile.txt\') ``` Implement the function `process_files` correctly following the requirements specified above.","solution":"import pipes def process_files(infile, outfile): t = pipes.Template() # Append commands to the pipeline t.append(\'tr a-z A-Z\', \'--\') # Converts lowercase to uppercase t.append(\\"sed \'s/ /_/g\'\\", \'--\') # Replaces spaces with underscores t.append(\\"rev\\", \'--\') # Reverses each line # Open the input file through the pipeline and write to the output file with t.open(outfile, \'w\') as f_out: with open(infile, \'r\') as f_in: for line in f_in: f_out.write(line) # Example usage # process_files(\'infile.txt\', \'outfile.txt\')"},{"question":"# Pandas Options Configuration and Application You are tasked with writing a function that takes a DataFrame and a set of display options as input, applies those options, prints the DataFrame, and then resets the options to their default values. Your function should demonstrate proficiency in using `get_option`, `set_option`, `reset_option`, and `option_context`. Function Signature ```python def configure_and_print(df: pd.DataFrame, options: dict) -> None: Apply pandas display options, print the DataFrame, and reset options. Parameters: df (pd.DataFrame): The DataFrame to be displayed. options (dict): A dictionary where keys are option names and values are the values to set. Returns: None pass ``` Input - `df`: A pandas DataFrame. - `options`: A dictionary with valid pandas option names as keys and the desired values as values. For example: ```python { \\"display.max_rows\\": 10, \\"display.max_columns\\": 5, \\"display.precision\\": 3 } ``` Output - The function prints the DataFrame with the specified options applied. - The function does not return any value but should ensure that all options are reset to their default values after printing. Example ```python import pandas as pd import numpy as np data = np.random.randn(15, 10) df = pd.DataFrame(data) options = { \\"display.max_rows\\": 10, \\"display.max_columns\\": 5, \\"display.precision\\": 3 } configure_and_print(df, options) ``` Constraints - You may assume that the input dictionary contains valid option names and corresponding values. - Ensure that the options are reset back to their defaults after the DataFrame is printed. Hints - Use `pd.set_option` to set the options. - Utilize `pd.option_context` to manage the setting and resetting of options within a context manager. - After printing the DataFrame, make sure to reset options to their original state to avoid side effects in subsequent pandas operations. Performance - The function should efficiently handle resetting multiple options and should ensure minimal performance overhead for large DataFrames.","solution":"import pandas as pd def configure_and_print(df: pd.DataFrame, options: dict) -> None: Apply pandas display options, print the DataFrame, and reset options. Parameters: df (pd.DataFrame): The DataFrame to be displayed. options (dict): A dictionary where keys are option names and values are the values to set. Returns: None # Store the current options so they can be reset later current_options = {opt: pd.get_option(opt) for opt in options} try: # Set the new options for opt, value in options.items(): pd.set_option(opt, value) # Print the DataFrame print(df) finally: # Reset options to their original values for opt, value in current_options.items(): pd.set_option(opt, value)"},{"question":"**Email Organizer Utility** # Objective: Create a Python script that organizes emails from a mixed mailbox format system. You will implement a function that moves emails containing a specific keyword from one mailbox format (Maildir) to another mailbox format (mbox), while ensuring no data loss or corruption. # Problem Statement: You are provided with paths to two mailboxes, one in `Maildir` format and the other in `mbox` format. Implement a function `organize_emails(maildir_path, mbox_path, keyword)` that performs the following tasks: 1. Opens the Maildir mailbox from the supplied `maildir_path`. 2. Searches for emails whose subject contains the specified `keyword`. 3. Adds any matching emails to the mbox mailbox specified by `mbox_path`. 4. Ensures transactional integrity: if an email is successfully moved to the mbox, it should be deleted from the Maildir only after ensuring it was correctly added to the mbox, to prevent data loss in case of interruption. 5. Use proper locking mechanisms to ensure no concurrent modification issues. # Constraints: - The `maildir_path` and `mbox_path` are existing directories/files. - The `keyword` is a non-empty string. - Handle any exceptions due to malformed emails gracefully and skip them without stopping the script. - The system might perform read and write operations concurrently with other processes. # Function Signature: ```python def organize_emails(maildir_path: str, mbox_path: str, keyword: str): pass ``` # Example: ```python # Assume we have already some emails in maildir_path and mbox_path organize_emails(\'/path/to/maildir\', \'/path/to/mbox\', \'python\') ``` This example should move all emails with \'python\' in their subject from the Maildir located at `/path/to/maildir` to the mbox located at `/path/to/mbox`. # Hints: - Use the `Maildir` and `mbox` classes from the `mailbox` module. - Make sure to lock mailboxes appropriately when performing operations. - Use functions like `get_message()` and `add()` to retrieve and add messages respectively. - Ensure that after adding a message to mbox, you flush the changes before deleting it from the Maildir to avoid data loss.","solution":"import mailbox import os import email def organize_emails(maildir_path, mbox_path, keyword): maildir = mailbox.Maildir(maildir_path, factory=None, create=False) mbox = mailbox.mbox(mbox_path) mbox.lock() try: for key, message in list(maildir.items()): try: if keyword.lower() in message[\'subject\'].lower(): mbox.add(message) mbox.flush() # Ensure that it\'s written to disk before deleting maildir.discard(key) except (KeyError, TypeError, email.errors.MessageError): # If there\'s an issue with a single email, we skip it and continue with others continue finally: mbox.unlock()"},{"question":"**Objective:** Demonstrate your understanding of the `errno` module and how to use it for system-level error handling in Python. Problem Statement Write a Python function `handle_error(errno_code: int) -> str` that takes an error code as input and returns a detailed message about the error. The function should use the `errno` module to achieve this. If the given error code does not correspond to any standard error (i.e., it is not a key in `errno.errorcode`), the function should return `\\"Unknown error code\\"`. You should also create a custom exception handler that catches specific exceptions such as `PermissionError`, `FileNotFoundError`, etc., and return appropriate messages for them. Function Signature ```python import errno def handle_error(errno_code: int) -> str: # Your code here # Example usage try: # Simulate a file operation that causes a FileNotFoundError open(\'/nonexistentfile\', \'r\') except OSError as e: print(handle_error(e.errno)) ``` Input and Output - **Input:** - `errno_code` (int): An integer representing an error code. - **Output:** - Returns a string that describes the error corresponding to the error code provided. For known error codes, the function should return a description formatted as `\\"[ErrorName]: Error description\\"`. If the error code is not recognized, return `\\"Unknown error code\\"`. Constraints - You should handle at least the following exceptions: - `PermissionError` - `FileNotFoundError` - `ProcessLookupError` - `InterruptedError` - `BlockingIOError` - `ChildProcessError` - Any other OSError-related exceptions as you see fit. Requirements - Use the `errno` module to map error codes to their human-readable names. - Provide robust error handling to cover unknown error codes. Example ```python import errno def handle_error(errno_code: int) -> str: try: error_name = errno.errorcode[errno_code] error_message = os.strerror(errno_code) return f\\"[{error_name}]: {error_message}\\" except KeyError: return \\"Unknown error code\\" # Simulate and handle a specific error try: open(\'/nonexistentfile\', \'r\') except OSError as e: print(handle_error(e.errno)) # -> Output: [ENOENT]: No such file or directory ``` Make sure to: - Implement the function `handle_error`. - Test your implementation with various simulated errors.","solution":"import errno import os def handle_error(errno_code: int) -> str: Returns a detailed message about the error corresponding to the given error code. Uses the errno module to map the error code to a human-readable name. try: error_name = errno.errorcode[errno_code] error_message = os.strerror(errno_code) return f\\"[{error_name}]: {error_message}\\" except KeyError: return \\"Unknown error code\\""},{"question":"**Objective**: Demonstrate understanding of Python\'s descriptor protocol by implementing custom descriptors and utilizing the descriptor-related functions provided. Problem Statement: You are required to create a custom attribute descriptor class that validates the data assigned to an object\'s attributes. Specifically, you will implement a descriptor that only accepts positive integers. If invalid data is assigned, it should raise a `ValueError` with an appropriate message. You will also use the descriptor-related functions provided in the documentation to demonstrate their usage. Requirements: 1. Implement a class `PositiveInteger` that acts as a descriptor: - Define the `__get__` method to retrieve the attribute\'s value. - Define the `__set__` method to set the attribute\'s value, ensuring that only positive integers are allowed. 2. Create a class `MyClass` that utilizes the `PositiveInteger` descriptor to manage an attribute `value`. 3. Demonstrate the usage of one or more of the descriptor creation functions (`PyDescr_NewGetSet`, `PyDescr_NewMember`, `PyDescr_NewMethod`, `PyDescr_NewWrapper`, `PyDescr_NewClassMethod`) by creating and integrating a custom method descriptor. Implementation Details: 1. **PositiveInteger Class**: - The `__set__` method should check if the input is a positive integer. If not, it should raise a `ValueError` with the message: \\"Value must be a positive integer.\\" - The `__get__` method should return the value of the attribute. 2. **MyClass**: - The class should have an attribute `value` that uses the `PositiveInteger` descriptor. 3. **Custom Method Descriptor**: - Use `PyDescr_NewMethod` or another relevant function to create and integrate a method descriptor into `MyClass`. Constraints: - Do not use any external libraries. - Assume the required methods from the Python C API are available in your Python environment (for the sake of this exercise). Example Usage: ```python class PositiveInteger: pass # Implementation of the descriptor class MyClass: value = PositiveInteger() def __init__(self, value): self.value = value try: obj = MyClass(10) print(obj.value) # Should print: 10 obj.value = -5 # Should raise ValueError: \\"Value must be a positive integer.\\" except ValueError as ve: print(ve) # Demonstrate the usage of an additional custom method descriptor here ``` Expected Output: ``` 10 Value must be a positive integer. ``` Please write the complete code implementation for the `PositiveInteger` class and the `MyClass` class that satisfies the above requirements.","solution":"class PositiveInteger: def __init__(self): self.value = None def __get__(self, instance, owner): return self.value def __set__(self, instance, value): if isinstance(value, int) and value > 0: self.value = value else: raise ValueError(\\"Value must be a positive integer.\\") class MyClass: value = PositiveInteger() def __init__(self, value): self.value = value"},{"question":"**Objective**: Demonstrate understanding of cryptographic services in Python using `hashlib` and `secrets` modules. # Problem Statement You are required to create a secure authentication system that utilizes cryptographic hashing and random number generation. The system will have the following functionalities: 1. **User Registration**: - Accept a username and password. - Securely store the username with a salted hash of the password. 2. **User Authentication**: - Accept a username and password. - Verify the provided credentials against the stored data. # Implementation Details 1. **Data Structure**: - Use a dictionary to store user credentials, where the key is the username and the value is a tuple of salt and hashed password. 2. **Functions to Implement**: ```python import hashlib import secrets def register_user(username: str, password: str, users_db: dict) -> bool: Register a new user with a salted hash password. Args: - username (str): The username of the new user. - password (str): The password for the new user. - users_db (dict): The database containing user credentials. Returns: - bool: True if registration is successful, False if the user already exists. pass def authenticate_user(username: str, password: str, users_db: dict) -> bool: Authenticate a user by verifying the provided credentials. Args: - username (str): The username of the user. - password (str): The password for the user. - users_db (dict): The database containing user credentials. Returns: - bool: True if the authentication is successful, False otherwise. pass ``` 3. **Function Details**: - `register_user` Function: - Generate a random salt using the `secrets` module. - Hash the password concatenated with the salt using `hashlib`. - Store the username along with a tuple of the salt and hashed password in the `users_db`. - Return `False` if the username already exists. - `authenticate_user` Function: - Retrieve the salt and hashed password for the given username from the `users_db`. - Hash the provided password with the retrieved salt. - Compare the newly hashed password with the stored hashed password. - Return `True` if they match, otherwise return `False`. # Constraints - Usernames are unique. - Passwords are at least 8 characters long. - Use SHA-256 for hashing. - Ensure your solution handles edge cases like non-existent users. # Example Usage ```python users_db = {} # Register users print(register_user(\\"alice\\", \\"securepassword123\\", users_db)) # Output: True print(register_user(\\"bob\\", \\"anothersecurepwd\\", users_db)) # Output: True print(register_user(\\"alice\\", \\"differentpassword\\", users_db)) # Output: False (user exists) # Authenticate users print(authenticate_user(\\"alice\\", \\"securepassword123\\", users_db)) # Output: True print(authenticate_user(\\"alice\\", \\"wrongpassword\\", users_db)) # Output: False print(authenticate_user(\\"charlie\\", \\"somepassword\\", users_db)) # Output: False (user doesn\'t exist) ``` Ensure your code adheres to best practices for security and efficiency.","solution":"import hashlib import secrets def register_user(username: str, password: str, users_db: dict) -> bool: Register a new user with a salted hash password. Args: - username (str): The username of the new user. - password (str): The password for the new user. - users_db (dict): The database containing user credentials. Returns: - bool: True if registration is successful, False if the user already exists. if username in users_db: return False salt = secrets.token_hex(16) hash_object = hashlib.sha256((password + salt).encode()) hashed_password = hash_object.hexdigest() users_db[username] = (salt, hashed_password) return True def authenticate_user(username: str, password: str, users_db: dict) -> bool: Authenticate a user by verifying the provided credentials. Args: - username (str): The username of the user. - password (str): The password for the user. - users_db (dict): The database containing user credentials. Returns: - bool: True if the authentication is successful, False otherwise. if username not in users_db: return False salt, stored_hash = users_db[username] hash_object = hashlib.sha256((password + salt).encode()) check_password_hash = hash_object.hexdigest() return check_password_hash == stored_hash"},{"question":"# Problem Description You are required to write a function that takes an XML string as input, parses it, and processes specific elements using handler functions. The primary goal is to extract the data within specified XML tags and their attributes. # Implement the Function ```python def process_xml(xml_string): Parse the given XML string and extract data within specific XML tags along with their attributes. Args: xml_string (str): A string containing the XML data. Returns: dict: A dictionary where keys are the tag names and values are lists of tuples. Each tuple contains the attributes (as a dictionary) and inner text of each occurrence of the tag. pass ``` # Instructions 1. **Parsing the XML string**: - Use the `xml.parsers.expat.ParserCreate()` to create a parser. - Set appropriate handlers to process XML elements and character data. 2. **Handler Requirements**: - **StartElementHandler**: Capture the start of specific elements (e.g., \\"item\\", \\"name\\") and store their attributes. - **EndElementHandler**: Capture the end of these elements and store any collected character data. - **CharacterDataHandler**: Collect data between the XML tags. 3. **Data Extraction**: - Focus on extracting tags named \\"item\\" and \\"name\\". Store their attributes and inner texts. - The final output should be a dictionary where keys are the tag names (\\"item\\", \\"name\\"), and values are lists of tuples with attributes and inner text of each tag occurrence. # Example Given the following XML string: ```xml <?xml version=\\"1.0\\"?> <root> <item type=\\"fruit\\" color=\\"red\\">Apple</item> <item type=\\"vegetable\\" color=\\"green\\">Broccoli</item> <name>John Doe</name> <name>Jane Smith</name> </root> ``` Your function should produce: ```python { \\"item\\": [ ({\\"type\\": \\"fruit\\", \\"color\\": \\"red\\"}, \\"Apple\\"), ({\\"type\\": \\"vegetable\\", \\"color\\": \\"green\\"}, \\"Broccoli\\") ], \\"name\\": [ ({}, \\"John Doe\\"), ({}, \\"Jane Smith\\") ] } ``` # Constraints - The XML string can be assumed to be well-formed. - Focus only on \\"item\\" and \\"name\\" tags. If these tags do not exist in the XML, the result should be an empty dictionary. - Ensure to handle the character data correctly even if it spans multiple calls to the handler. # Performance Considerations - Your solution should efficiently handle XML strings up to a few megabytes in size without excessive memory or processing time.","solution":"import xml.parsers.expat def process_xml(xml_string): Parse the given XML string and extract data within specific XML tags along with their attributes. Args: xml_string (str): A string containing the XML data. Returns: dict: A dictionary where keys are the tag names and values are lists of tuples. Each tuple contains the attributes (as a dictionary) and inner text of each occurrence of the tag. result = {\\"item\\": [], \\"name\\": []} current_tag = None current_attrs = None current_data = [] def start_element(name, attrs): nonlocal current_tag, current_attrs if name in result: current_tag = name current_attrs = attrs current_data.clear() def end_element(name): nonlocal current_tag, current_attrs if name == current_tag: # Collect data into a single string data_str = \'\'.join(current_data).strip() result[current_tag].append((current_attrs, data_str)) current_tag = None current_attrs = None current_data.clear() def char_data(data): if current_tag: current_data.append(data) parser = xml.parsers.expat.ParserCreate() parser.StartElementHandler = start_element parser.EndElementHandler = end_element parser.CharacterDataHandler = char_data parser.Parse(xml_string) return result"},{"question":"# Email Generator Implementation Problem Description You are tasked with creating a simplified version of the `DecodedGenerator` class from the `email.generator` module. Your implementation should flatten a message object structure, with the capability of serializing its components into a specified output format. Specifically, your `SimpleDecodedGenerator` class should: 1. Output the decoded payload for `text` parts. 2. For non-`text` parts, output a placeholder string containing the type, filename, and description of the part. Class Definition ```python class SimpleDecodedGenerator: def __init__(self, outfp, fmt=None): Initialize the SimpleDecodedGenerator with the output file-like object and an optional format string for non-text parts. :param outfp: A file-like object with a write method that accepts string data. :param fmt: A format string for non-text parts. Default is \\"[Non-text (%(type)s) part of message omitted, filename %(filename)s]\\". self.outfp = outfp self.fmt = fmt or \\"[Non-text (%(type)s) part of message omitted, filename %(filename)s]\\" def flatten(self, msg): Flatten the message object structure rooted at msg and write the textual representation to the output file-like object. :param msg: The root message object to serialize. # Implementation here def write(self, s): Write the given string to the output file-like object. :param s: The string to write. self.outfp.write(s) ``` Input and Output Formats - The `flatten` method takes a message object `msg` as input, which has a structure similar to email message objects. - The `write` method outputs strings to the specified file-like object. Constraints - The message object can contain multiple parts, with each part having attributes like `get_content_type()`, `get_filename()`, `get_payload()`, etc. Example Usage ```python import io # Sample message object for testing class SampleMessage: def __init__(self, content_type, payload, filename=None, description=None): self.content_type = content_type self.payload = payload self.filename = filename self.description = description def get_content_type(self): return self.content_type def get_filename(self): return self.filename def get_payload(self): return self.payload def get_description(self): return self.description # Create a sample email message object (simplified structure) msg = SampleMessage(\\"text/plain\\", \\"Hello, this is the body of the email.\\") file = io.StringIO() # Initialize and use the SimpleDecodedGenerator generator = SimpleDecodedGenerator(file) generator.flatten(msg) print(file.getvalue()) ``` Task Implement the `flatten` method of the `SimpleDecodedGenerator` class according to the specifications above. Ensure that your solution correctly handles both `text` and non-`text` parts of the message and applies the format string to non-`text` parts.","solution":"class SimpleDecodedGenerator: def __init__(self, outfp, fmt=None): Initialize the SimpleDecodedGenerator with the output file-like object and an optional format string for non-text parts. :param outfp: A file-like object with a write method that accepts string data. :param fmt: A format string for non-text parts. Default is \\"[Non-text (%(type)s) part of message omitted, filename %(filename)s]\\". self.outfp = outfp self.fmt = fmt or \\"[Non-text (%(type)s) part of message omitted, filename %(filename)s]\\" def flatten(self, msg): Flatten the message object structure rooted at msg and write the textual representation to the output file-like object. :param msg: The root message object to serialize. content_type = msg.get_content_type() if content_type.startswith(\'text/\'): # Assume payload is decoded if text part self.write(msg.get_payload()) else: self.write(self.fmt % { \'type\': content_type, \'filename\': msg.get_filename() or \'None\', \'description\': msg.get_description() or \'None\' }) def write(self, s): Write the given string to the output file-like object. :param s: The string to write. self.outfp.write(s)"},{"question":"# PyTorch Coding Assessment: Distributed Training with Control Plane Objective You are given the task to implement a distributed training process in PyTorch that leverages the control plane module for debug and control handler functionalities. Task Create a function `distributed_training` that sets up and executes a distributed training operation using PyTorch\'s `torch.distributed.elastic.control_plane` module. This function should: 1. Initialize the distributed process group. 2. Implement a basic training loop for a simple neural network model. 3. Incorporate debug and control handlers provided by the `torch.distributed.elastic.control_plane` module. 4. Ensure that each worker in the distributed setting performs its task correctly, and the results from different workers are synchronized. Function Signature ```python def distributed_training(world_size: int, num_epochs: int): Perform distributed training with debug and control handlers. Args: - world_size (int): The number of processes to run. - num_epochs (int): The number of epochs to train the model. Returns: None pass ``` Input - `world_size`: An integer representing the number of processes (workers) to run in parallel. - `num_epochs`: An integer representing the number of training epochs. Instructions 1. Initialize a distributed process group. 2. Create a simple neural network (e.g., a small fully connected network for binary classification). 3. Implement a training loop that includes forward, backward passes, and optimizer steps. 4. Use the `torch.distributed.elastic.control_plane.worker_main` function or similar from the module to add necessary debug and control handlers. 5. Ensure that each worker process trains its part of the data and synchronizes the model parameters with other workers. Constraints - You are not allowed to use high-level abstractions like PyTorch Lightning for this task. - Ensure that the training completes within a reasonable time frame (e.g., by limiting the model complexity or dataset size). - Include error handling to manage unsuccessful initialization of distributed groups. Example Implementation You need to provide the implementation within the function `distributed_training`. There is no expected printed output, but you should ensure that the function runs without errors and the distributed training loop executes correctly for multiple worker processes.","solution":"import torch import torch.distributed as dist import torch.nn as nn import torch.optim as optim from torch.multiprocessing import Process def setup(rank, world_size): Setup the distributed environment. dist.init_process_group( backend=\'gloo\', init_method=\'tcp://127.0.0.1:29500\', world_size=world_size, rank=rank ) def cleanup(): Cleanup the distributed environment. dist.destroy_process_group() class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 5) self.fc2 = nn.Linear(5, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) return x def train(rank, world_size, num_epochs): setup(rank, world_size) # Create model and move it to the device model = SimpleModel() device = torch.device(f\'cpu\') model.to(device) # Setup optimizer and loss function optimizer = optim.SGD(model.parameters(), lr=0.001) criterion = nn.BCELoss() # Dummy dataset inputs = torch.randn(20, 10).to(device) targets = torch.randint(0, 2, (20, 1)).float().to(device) for epoch in range(num_epochs): model.train() # Slice the dataset based on the rank start = rank * 10 end = start + 10 input_batch = inputs[start:end] target_batch = targets[start:end] # Forward pass optimizer.zero_grad() outputs = model(input_batch) # Compute loss loss = criterion(outputs, target_batch) # Backward pass and optimizer step loss.backward() optimizer.step() # Log the loss if rank == 0: print(f\'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\') cleanup() def distributed_training(world_size: int, num_epochs: int): Perform distributed training with debug and control handlers. Args: - world_size (int): The number of processes to run. - num_epochs (int): The number of epochs to train the model. Returns: None processes = [] for rank in range(world_size): p = Process(target=train, args=(rank, world_size, num_epochs)) p.start() processes.append(p) for p in processes: p.join()"},{"question":"# Custom Pretty-Printing Function Objective To assess the understanding of the `pprint` module by implementing a customized pretty-printing function that extends the existing capabilities of the module. Problem Statement You are required to implement a function `custom_pretty_print(obj, width=80, depth=None, sort_dicts=True, alias=False)`. This function should pretty-print the input `obj` with the following additional feature: - `alias`: If set to `True`, it replaces specific values in the object with predefined aliases. Use the following aliases: - `None`: Alias as `\\"NULL\\"` - `True`: Alias as `\\"YES\\"` - `False`: Alias as `\\"NO\\"` Function Signature ```python def custom_pretty_print(obj, width=80, depth=None, sort_dicts=True, alias=False): pass ``` Input - `obj`: Any Python object or data structure. - `width` (int): The maximum number of characters per line allowed. Defaults to 80. - `depth` (int): The number of nesting levels to display. If the data structure is deeper, it is truncated with `\'...\'`. Defaults to None (no constraint). - `sort_dicts` (bool): If true, dictionaries will be printed with their keys sorted. Defaults to True. - `alias` (bool): If true, replace `None`, `True`, and `False` with predefined aliases. Defaults to False. Output A pretty-printed representation of `obj` according to the specified rules. Constraints - You are not allowed to modify the standard functionality of the `pprint` module directly but should make use of its features extensibly. - Handle deep nesting and recursive structures appropriately. - Ensure to handle different data structures like lists, tuples, dictionaries, sets, and nested combinations of these structures. Example ```python # Example 1 data = {\\"key1\\": None, \\"key2\\": True, \\"key3\\": {\\"subkey1\\": False, \\"subkey2\\": [1, 2, 3]}} custom_pretty_print(data, alias=True) Output: { \'key1\': \'NULL\', \'key2\': \'YES\', \'key3\': { \'subkey1\': \'NO\', \'subkey2\': [ 1, 2, 3 ] } } # Example 2 data = {\\"key\\": [True, False, None]} custom_pretty_print(data, width=20, alias=True) Output: { \'key\': [ \'YES\', \'NO\', \'NULL\' ] } ``` You should test your function with different kinds of nested structures and ensure that aliases are correctly applied where appropriate without altering the default pretty-print functionality. Evaluation Your solution will be evaluated on: 1. Correctness of the output based on the specifications. 2. Proper handling of the alias substitutions. 3. Effective utilization of the `pprint` module’s features. 4. Handling of edge cases, including recursive structures. Implement your function below.","solution":"from pprint import PrettyPrinter class CustomPrettyPrinter(PrettyPrinter): def __init__(self, *args, alias=False, **kwargs): self.alias = alias super().__init__(*args, **kwargs) def format(self, obj, context, maxlevels, level): if self.alias: if obj is None: return \\"NULL\\", True, False elif obj is True: return \\"YES\\", True, False elif obj is False: return \\"NO\\", True, False return super().format(obj, context, maxlevels, level) def custom_pretty_print(obj, width=80, depth=None, sort_dicts=True, alias=False): printer = CustomPrettyPrinter(width=width, depth=depth, sort_dicts=sort_dicts, alias=alias) printer.pprint(obj)"},{"question":"**Question: Applying and Comparing Window Functions in Signal Processing with PyTorch** In this question, you will demonstrate your understanding of the `torch.signal.windows` submodule in PyTorch by applying various window functions to a noisy signal and comparing their effects. # Task Write a function `apply_window_functions` that takes the following parameters: - `signal` (torch.Tensor): A 1-D tensor representing a noisy signal. - `window_type` (str): A string indicating the type of window function to apply. Valid options are: `\'bartlett\'`, `\'blackman\'`, `\'cosine\'`, `\'exponential\'`, `\'gaussian\'`, `\'general_cosine\'`, `\'general_hamming\'`, `\'hamming\'`, `\'hann\'`, `\'kaiser\'`, `\'nuttall\'`. - `window_params` (dict): A dictionary containing any additional parameters needed for certain window functions (e.g., alpha for the Gaussian window). The function should return a 1-D tensor which is the original signal multiplied by the specified window function. # Example ```python # Example input signal signal = torch.tensor([0.5, 1.0, 0.75, 1.5, 1.25, 1.0, 0.5]) # Applying the Hamming window window_type = \'hamming\' window_params = {} # No additional parameters needed for the Hamming window filtered_signal = apply_window_functions(signal, window_type, window_params) # Examining the filtered signal print(filtered_signal) ``` # Constraints - You must use the window functions from the `torch.signal.windows` submodule. - Handle any invalid window types by raising a `ValueError` with a message \\"Invalid window type\\". # Performance Requirements - The function should be efficient and avoid any unnecessary computations. - Assume the input signal tensor can be large, up to several million elements. # Hints - Refer to the PyTorch documentation for any specific syntax or usage patterns for the window functions. - Use the parameter `window_params` to pass any additional arguments required by certain window functions. **Note:** Your implementation must handle all valid window types and their corresponding parameters correctly.","solution":"import torch import torch.signal.windows as windows def apply_window_functions(signal, window_type, window_params): Applies a specified window function to a noisy signal. Parameters: - signal (torch.Tensor): A 1-D tensor representing a noisy signal. - window_type (str): A string indicating the type of window function to apply. - window_params (dict): A dictionary containing any additional parameters needed for certain window functions. Returns: - torch.Tensor: The original signal multiplied by the specified window function. if window_type not in [ \'bartlett\', \'blackman\', \'cosine\', \'exponential\', \'gaussian\', \'general_cosine\', \'general_hamming\', \'hamming\', \'hann\', \'kaiser\', \'nuttall\' ]: raise ValueError(\\"Invalid window type\\") window_func = getattr(windows, window_type) window = window_func(len(signal), **window_params) return signal * window"},{"question":"**Coding Assessment Question: Profiling and Optimizing an Algorithm** # Background You are provided with a function that performs matrix factorization using Non-negative Matrix Factorization (NMF). Your task is to profile this function to identify performance bottlenecks, and then optimize the code to improve its efficiency. You will use Numpy for vectorized operations and Cython to optimize critical sections of the code. # Function Description You are provided with the following initial implementation for the NMF algorithm: ```python import numpy as np def nmf(V, n_components=2, max_iter=200, tol=1e-4): Perform Non-negative Matrix Factorization (NMF) np.random.seed(0) W = np.abs(np.random.randn(V.shape[0], n_components)) H = np.abs(np.random.randn(n_components, V.shape[1])) for n_iter in range(1, max_iter + 1): H = np.linalg.solve(np.dot(W.T, W), np.dot(W.T, V)) H[H < 0] = 0 W = np.linalg.solve(np.dot(H, H.T), np.dot(V, H.T)).T W[W < 0] = 0 if n_iter % 10 == 0: reconstruction = np.dot(W, H) error = np.linalg.norm(V - reconstruction) if error < tol: break return W, H ``` # Requirements 1. **Profile the Function:** - Use IPython magic commands to profile the initial implementation. - Identify the main bottlenecks in the function. 2. **Optimize the Function:** - Replace inefficient sections of the code with optimized Numpy operations. - Use Cython to further optimize the computationally intensive parts of the algorithm. - Consider using parallel processing to speed up the optimization step if applicable. 3. **Implement the Optimized Function:** - Provide the final optimized implementation in a new function called `optimized_nmf`. - Ensure that the implementation maintains the correctness of the original algorithm. # Input and Output - **Input:** - A non-negative matrix `V` of shape `(m, n)`, where `m` is the number of samples and `n` is the number of features. - An integer `n_components` representing the number of components for the factorization or decomposition. - An integer `max_iter` indicating the maximum number of iterations for the algorithm. - A float `tol` representing the convergence tolerance. - **Output:** - Two matrices `W` of shape `(m, n_components)` and `H` of shape `(n_components, n)` such that the product `np.dot(W, H)` approximates the input matrix `V`. # Constraints - The input matrix `V` and the resulting matrices `W` and `H` must be non-negative. - The algorithm should converge within the given `max_iter` iterations and achieve an error less than `tol` if convergence is possible. # Example ```python from sklearn.datasets import load_digits # Load sample data digits = load_digits() V = digits.data # Initial NMF (slow implementation) W, H = nmf(V, n_components=16, max_iter=100, tol=1e-4) print(\\"Initial Implementation: Error =\\", np.linalg.norm(V - np.dot(W, H))) # Optimized NMF (your optimized implementation) optimized_W, optimized_H = optimized_nmf(V, n_components=16, max_iter=100, tol=1e-4) print(\\"Optimized Implementation: Error =\\", np.linalg.norm(V - np.dot(optimized_W, optimized_H))) ``` # Submission - Your submission should include the profiled results and the optimized function `optimized_nmf`. - Explain the optimizations you performed and why they improve the performance.","solution":"import numpy as np import cython from joblib import Parallel, delayed import scipy.linalg @cython.boundscheck(False) @cython.wraparound(False) @cython.cdivision(True) def optimized_nmf(V, n_components=2, max_iter=200, tol=1e-4): Perform Non-negative Matrix Factorization (NMF) with optimizations. np.random.seed(0) # Initialize W and H with non-negative random values W = np.abs(np.random.randn(V.shape[0], n_components)) H = np.abs(np.random.randn(n_components, V.shape[1])) Vt = V.T losses = np.zeros(max_iter) for n_iter in range(1, max_iter + 1): H = np.linalg.lstsq(W, V, rcond=None)[0] H[H < 0] = 0 W = np.linalg.lstsq(H.T, Vt, rcond=None)[0].T W[W < 0] = 0 if n_iter % 10 == 0: reconstruction = np.dot(W, H) error = np.linalg.norm(V - reconstruction) if error < tol: break return W, H"},{"question":"Question: Unsupervised Learning and Clustering Analysis # Objective You are provided with a dataset of customer transactions. Your task is to perform clustering to segment these customers into distinct groups based on their transaction history using different clustering algorithms available in scikit-learn. # Dataset The dataset, `customer_transactions.csv`, contains the following columns: - `CustomerID`: Unique identifier for each customer. - `AnnualIncome`: The annual income of the customer. - `SpendingScore`: A score assigned to the customer based on their annual income and spending behavior. # Task 1. **Load the Dataset:** - Load the `customer_transactions.csv` dataset into a pandas DataFrame. 2. **Preprocess the Data:** - Handle any missing values if present. - Normalize the features (`AnnualIncome` and `SpendingScore`). 3. **Implement Clustering Algorithms:** - Implement the following clustering algorithms available in scikit-learn: - KMeans - DBSCAN - AgglomerativeClustering 4. **Evaluate the Clusters:** - For each algorithm, determine the optimal number of clusters using appropriate techniques (e.g., Elbow method for KMeans). - Evaluate and compare the results using metrics like silhouette score. 5. **Visualize the Clusters:** - Use scatter plots to visualize the clusters formed by each algorithm. # Input and Output Formats - The input is the file `customer_transactions.csv`. Ensure that the file is present in the working directory. - The output should include: - The optimal number of clusters for each algorithm. - The silhouette score for each algorithm. - Scatter plots showing the clusters formed by each algorithm. # Constraints - Use only scikit-learn and pandas for this task. - The code should handle datasets of large sizes efficiently. # Note - You are required to submit a Jupyter notebook containing your code and visualizations. - Properly comment your code and explain your steps. # Performance Requirements - The implementation should be efficient and capable of handling datasets with up to 100,000 entries without performance degradation.","solution":"import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt def load_dataset(filename): Load customer transactions dataset from a CSV file. return pd.read_csv(filename) def preprocess_data(df): Preprocess the dataset by handling missing values and normalizing features. df = df.dropna() features = df[[\'AnnualIncome\', \'SpendingScore\']] scaler = StandardScaler() scaled_features = scaler.fit_transform(features) return scaled_features def apply_kmeans(data, num_clusters): Apply KMeans clustering algorithm on the data. kmeans = KMeans(n_clusters=num_clusters, random_state=42) kmeans.fit(data) return kmeans.labels_, kmeans def apply_dbscan(data, eps, min_samples): Apply DBSCAN clustering algorithm on the data. dbscan = DBSCAN(eps=eps, min_samples=min_samples) clusters = dbscan.fit_predict(data) return clusters, dbscan def apply_agglomerative_clustering(data, num_clusters): Apply Agglomerative Clustering algorithm on the data. agg = AgglomerativeClustering(n_clusters=num_clusters) clusters = agg.fit_predict(data) return clusters, agg def evaluate_clusters(data, labels): Evaluate the clustering results using silhouette score. return silhouette_score(data, labels) def visualize_clusters(data, labels, title): Visualize the clusters using a scatter plot. plt.scatter(data[:, 0], data[:, 1], c=labels, cmap=\'viridis\') plt.title(title) plt.xlabel(\'AnnualIncome\') plt.ylabel(\'SpendingScore\') plt.show() def determine_optimal_kmeans_clusters(data, max_clusters=10): Determine the optimal number of clusters for KMeans using the Elbow method. distortions = [] for i in range(1, max_clusters + 1): kmeans = KMeans(n_clusters=i, random_state=42) kmeans.fit(data) distortions.append(kmeans.inertia_) # Inertia is the sum of squared distances to the nearest cluster center plt.plot(range(1, max_clusters + 1), distortions, marker=\'o\') plt.title(\'Elbow Method For Optimal k\') plt.xlabel(\'Number of clusters\') plt.ylabel(\'Distortion\') plt.show() # Example usage if __name__ == \\"__main__\\": # Load and preprocess the data df = load_dataset(\'customer_transactions.csv\') processed_data = preprocess_data(df) # Determine optimal clusters for KMeans using Elbow method determine_optimal_kmeans_clusters(processed_data) # Apply KMeans with optimal number of clusters kmeans_labels, kmeans = apply_kmeans(processed_data, 4) kmeans_silhouette = evaluate_clusters(processed_data, kmeans_labels) print(f\'KMeans Silhouette Score: {kmeans_silhouette}\') visualize_clusters(processed_data, kmeans_labels, \\"K-Means Clusters\\") # Apply DBSCAN dbscan_labels, dbscan = apply_dbscan(processed_data, eps=0.5, min_samples=5) dbscan_silhouette = evaluate_clusters(processed_data, dbscan_labels) print(f\'DBSCAN Silhouette Score: {dbscan_silhouette}\') visualize_clusters(processed_data, dbscan_labels, \\"DBSCAN Clusters\\") # Apply Agglomerative Clustering agg_labels, agg = apply_agglomerative_clustering(processed_data, 4) agg_silhouette = evaluate_clusters(processed_data, agg_labels) print(f\'Agglomerative Clustering Silhouette Score: {agg_silhouette}\') visualize_clusters(processed_data, agg_labels, \\"Agglomerative Clustering\\")"},{"question":"# Problem: Create a Tkinter Application with Conditional Message Boxes **Objective:** Create a Python program using the `tkinter` module that interacts with the user through various message boxes based on their responses. **Task:** Implement a Tkinter application with a single button titled \\"Start\\". When the user clicks the \\"Start\\" button, the following sequence of interactions should occur: 1. Show an information message box with the title \\"Information\\" and the message \\"This is an important update.\\" 2. After the user closes the information message box, show a question message box with the title \\"Question\\" and the message \\"Do you want to proceed?\\". - If the user selects \\"Yes\\", proceed to step 3. - If the user selects \\"No\\", show a warning message box with the title \\"Warning\\" and the message \\"Process halted by the user.\\" and end the sequence. 3. Show another question message box with the title \\"Confirmation\\" and the message \\"Are you sure you want to continue?\\". - If the user selects \\"Yes\\", show an information message box with the title \\"Success\\" and the message \\"You have successfully completed the process.\\". - If the user selects \\"No\\", present a retry/cancel message box with the title \\"Retry\\" and the message \\"Do you want to try again?\\". - If the user selects \\"Retry\\", return to step 1. - If the user selects \\"Cancel\\", show an error message box with the title \\"Error\\" and the message \\"The process was cancelled.\\" and end the sequence. **Input and Output:** - The program should involve no direct user input other than interacting with the Tkinter message boxes. - The output will consist of the sequence of message boxes displayed to the user based on their clicks. **Constraints:** - You must handle user inputs correctly at each message box stage as described. - The program should use the appropriate `tkinter.messagebox` functions at each stage to show the correct message boxes. Here is a skeleton of the function that you need to implement: ```python import tkinter as tk from tkinter import messagebox def start_process(): # Implement the sequence of message boxes as described above. pass def main(): root = tk.Tk() root.title(\\"Message Box Application\\") start_button = tk.Button(root, text=\\"Start\\", command=start_process) start_button.pack(pady=20) root.mainloop() if __name__ == \\"__main__\\": main() ``` - **Performance:** Since this is a Tkinter application, performance constraints are minimal. Ensure there are no unusual delays between message boxes. # Note: Make sure to test your application thoroughly to ensure that all message boxes behave as expected based on the user\'s responses.","solution":"import tkinter as tk from tkinter import messagebox def start_process(): result = messagebox.showinfo(\\"Information\\", \\"This is an important update.\\") if result == \'ok\': result = messagebox.askquestion(\\"Question\\", \\"Do you want to proceed?\\") if result == \'yes\': result = messagebox.askquestion(\\"Confirmation\\", \\"Are you sure you want to continue?\\") if result == \'yes\': messagebox.showinfo(\\"Success\\", \\"You have successfully completed the process.\\") else: result = messagebox.askretrycancel(\\"Retry\\", \\"Do you want to try again?\\") if result: start_process() else: messagebox.showerror(\\"Error\\", \\"The process was cancelled.\\") else: messagebox.showwarning(\\"Warning\\", \\"Process halted by the user.\\") def main(): root = tk.Tk() root.title(\\"Message Box Application\\") start_button = tk.Button(root, text=\\"Start\\", command=start_process) start_button.pack(pady=20) root.mainloop() if __name__ == \\"__main__\\": main()"},{"question":"# Context Manager `Timeout` with `contextlib` Module **Objective:** Implement a context manager that imposes a time limit on a block of code. If the block of code does not complete within the specified time limit, the context manager should raise a `TimeoutError`. **Constraints:** - You must use the `contextlib` module to implement the context manager. - The context manager should be usable both as a decorator and with the `with` statement. **Details:** 1. Define a context manager called `Timeout`. 2. The `Timeout` context manager should accept a single argument, `seconds`, which specifies the time limit for the block of code. 3. If the block of code within the `with` statement or decorated function does not complete within the specified time limit, raise a `TimeoutError`. **Input:** - `seconds` (int): The maximum number of seconds the block or function is allowed to run. **Output:** - If the block or function completes within the specified time, it should return as usual. - If the block or function exceeds the specified time, raise a `TimeoutError`. **Performance Requirements:** - The context manager should handle edge cases, such as entering and exiting the context manager without executing any block of code. **Example Usage:** ```python from contextlib import contextmanager import time import signal @contextmanager def Timeout(seconds): def handler(signum, frame): raise TimeoutError(\\"Execution exceeded time limit\\") signal.signal(signal.SIGALRM, handler) signal.alarm(seconds) try: yield finally: signal.alarm(0) # Using Timeout as a context manager with `with` statement try: with Timeout(2): # 2 seconds limit time.sleep(3) # Simulating a long-running operation except TimeoutError: print(\\"Operation timed out\\") # Using Timeout as a decorator @Timeout(3) def long_running_task(): time.sleep(4) # Simulating a task that runs longer than allowed try: long_running_task() except TimeoutError: print(\\"Decorator timed out\\") ``` **Note:** - You may need to handle compatibility issues for the `signal` module in non-UNIX systems if necessary. - Ensure your solution is thread-safe, if applicable, or explicitly state the limitations. Implement the `Timeout` context manager based on the above specifications.","solution":"from contextlib import contextmanager import signal @contextmanager def Timeout(seconds): def handler(signum, frame): raise TimeoutError(\\"Execution exceeded time limit\\") signal.signal(signal.SIGALRM, handler) signal.alarm(seconds) try: yield finally: signal.alarm(0)"},{"question":"Objective The objective of this task is to assess your understanding of Python\'s bytearray objects and your ability to implement and manipulate them using Python code. Problem Statement You need to implement a Python class `ByteArrayManipulator` with the following methods: 1. `__init__(self, initial_data: str)`: This initializer method should create a bytearray from the given initial string data. 2. `append_data(self, data: str) -> None`: This method should append the given string data to the existing bytearray. 3. `get_size(self) -> int`: This method should return the size of the current bytearray. 4. `to_string(self) -> str`: This method should return the contents of the bytearray as a string. 5. `resize(self, new_size: int) -> None`: This method should resize the current bytearray to the new size. If the new size is smaller than the current size, the bytearray should be truncated; if larger, it should be extended with null bytes. Input and Output Formats - **`__init__(self, initial_data: str)`** - **Input:** A string `initial_data` which will be used to create the initial bytearray. - **Output:** None - **`append_data(self, data: str) -> None`** - **Input:** A string `data` which will be appended to the end of the current bytearray. - **Output:** None - **`get_size(self) -> int`** - **Input:** None - **Output:** An integer representing the size of the current bytearray. - **`to_string(self) -> str`** - **Input:** None - **Output:** A string representation of the current bytearray’s contents. - **`resize(self, new_size: int) -> None`** - **Input:** An integer `new_size` specifying the new size of the bytearray. - **Output:** None Example Usage ```python # Creating an instance of ByteArrayManipulator bam = ByteArrayManipulator(\\"Hello\\") # Appending data to bytearray bam.append_data(\\" World\\") # Getting size of the bytearray print(bam.get_size()) # Output: 11 # Converting bytearray to string print(bam.to_string()) # Output: \\"Hello World\\" # Resizing the bytearray bam.resize(5) print(bam.to_string()) # Output: \\"Hello\\" bam.resize(8) print(bam.to_string()) # Output: \\"Hellox00x00x00\\" ``` Constraints - The input strings will only contain ASCII characters. - The integer for the new size in the resize method will always be non-negative. - Your implementation should handle edge cases like empty initial data or resize operations that reduce the array size to zero. Performance Requirements - Ensure your methods execute efficiently for large input data sizes (up to `10^6` characters). Implement the `ByteArrayManipulator` class to meet the above specifications.","solution":"class ByteArrayManipulator: def __init__(self, initial_data: str): Initializes the ByteArrayManipulator with the given initial string data as a bytearray. self.byte_array = bytearray(initial_data, \'ascii\') def append_data(self, data: str) -> None: Appends the given string data to the existing bytearray. self.byte_array.extend(data.encode(\'ascii\')) def get_size(self) -> int: Returns the size of the current bytearray. return len(self.byte_array) def to_string(self) -> str: Returns the contents of the bytearray as a string. return self.byte_array.decode(\'ascii\') def resize(self, new_size: int) -> None: Resizes the current bytearray to the new size. If the new size is smaller than the current size, the bytearray is truncated; if larger, it is extended with null bytes. current_size = len(self.byte_array) if new_size < current_size: self.byte_array = self.byte_array[:new_size] else: self.byte_array.extend(bytearray(new_size - current_size))"},{"question":"# Advanced Python Function Implementation **Objective:** Implement a Python function that demonstrates understanding of various control flow and function-related features. **Problem Statement:** Create a function named `process_numbers_and_strings` that takes a list of mixed data types (integers and strings) and performs the following operations: 1. Segregate the integers and strings into two separate lists. 2. Return a tuple containing: - A list of integers, each squared if they are even; otherwise, keep them unchanged. - A dictionary where each string is a key and its length is the value. The function should also have options to: - Exclude specific elements from both integers and strings based on provided lists of values to exclude. - Include only specific types of elements (either \\"int\\" or \\"str\\") through a keyword argument. **Function Signature:** ```python def process_numbers_and_strings(data: list, exclude_ints: list = [], exclude_strs: list = [], include_type: str = \'both\') -> tuple: ``` **Input:** - `data` (list): A list of mixed integers and strings. Example: `[1, \'apple\', 2, \'banana\', 3, \'cherry\', 4]` - `exclude_ints` (list, optional): A list of integers to exclude from processing. Example: `[2, 3]` - `exclude_strs` (list, optional): A list of strings to exclude from processing. Example: `[\'banana\']` - `include_type` (str, optional): Specifies whether to process only \'int\', only \'str\', or \'both\'. Default is \'both\'. **Output:** - A tuple containing two elements: - A list of processed integers. - A dictionary with strings and their lengths. **Constraints:** - The `include_type` argument must be either \'both\', \'int\', or \'str\'. - If `include_type` is \'int\', the function should only process integers and return `([], {})` for the strings part. - If `include_type` is \'str\', the function should only process strings and return `([], {})` for the integers part. - Ensure proper handling of edge cases such as empty input lists or lists without the specified type when `include_type` is set to \'int\' or \'str\'. **Example:** ```python data = [1, \'apple\', 2, \'banana\', 3, \'cherry\', 4] exclude_ints = [2, 3] exclude_strs = [\'banana\'] include_type = \'both\' result = process_numbers_and_strings(data, exclude_ints, exclude_strs, include_type) # Expected Output: # ([1, 16], {\'apple\': 5, \'cherry\': 6}) ``` **Notes:** - Document the function with proper docstrings. - Use appropriate control flow structures (`if`, `for`, etc.) to meet the function requirements. - Test your function with different scenarios to ensure it handles edge cases correctly.","solution":"def process_numbers_and_strings(data, exclude_ints=[], exclude_strs=[], include_type=\'both\'): Processes a list of mixed integers and strings and returns a tuple of processed integers and a dictionary of strings with their lengths. Parameters: data (list): A list of mixed integers and strings. exclude_ints (list): A list of integers to exclude from processing. exclude_strs (list): A list of strings to exclude from processing. include_type (str): Specifies whether to process only \'int\', only \'str\', or \'both\'. Returns: tuple: A tuple containing a list of processed integers and a dictionary of strings with their lengths. if include_type not in [\'both\', \'int\', \'str\']: raise ValueError(\\"include_type must be \'both\', \'int\', or \'str\'\\") int_list = [] str_dict = {} if include_type in [\'both\', \'int\']: for item in data: if isinstance(item, int) and item not in exclude_ints: if item % 2 == 0: int_list.append(item ** 2) else: int_list.append(item) if include_type in [\'both\', \'str\']: for item in data: if isinstance(item, str) and item not in exclude_strs: str_dict[item] = len(item) return (int_list, str_dict)"},{"question":"# Internationalization with `gettext` in Python **Objective:** Develop a Python module that supports multiple languages using the GNU `gettext` and its class-based API. Create application logic that dynamically switches between two languages: English and Spanish. **Problem Statement:** Your task is to create a program that takes a user message and displays a localized version of the message based on the specified language. The program must support English and Spanish, with the default language being English. 1. Prepare a `.mo` file for English with: - `Hello World!` -> `Hello World!` - `Welcome to the program.` -> `Welcome to the program.` 2. Prepare a `.mo` file for Spanish with: - `Hello World!` -> `¡Hola Mundo!` - `Welcome to the program.` -> `Bienvenidos al programa.` **Requirements:** 1. Write a function `prepare_translation_files()` that simulates the creation of these `.mo` files. You may use dummy functions to represent this step. 2. Implement a `load_translations(language: str) -> gettext.NullTranslations` function that loads the appropriate translations from the `.mo` files based on the language parameter (`\'en\'` for English, `\'es\'` for Spanish). 3. Write the main logic function `main_language_program(language: str)` that: - Loads the translations based on the language input. - Prints the translated messages: - \\"Hello World!\\" - \\"Welcome to the program.\\" **Constraints:** - The language parameter will be either `\'en\'` or `\'es\'`. - Use the class-based API of `gettext` for flexibility. - Simulate the `.mo` file handling within the constraints of this environment. **Example:** ```python def prepare_translation_files(): # Dummy function to simulate creating .mo files def load_translations(language: str) -> gettext.NullTranslations: # Load translations based on the input language pass def main_language_program(language: str): # Main program logic to print translated messages pass # Example usage main_language_program(\'en\') # Output: # Hello World! # Welcome to the program. main_language_program(\'es\') # Output: # ¡Hola Mundo! # Bienvenidos al programa. ``` **Notes:** - Use `gettext` methods like `gettext.translation`, `GNUTranslations`, and necessary methods to build the solution. - Ensure flexibility in changing languages dynamically.","solution":"import gettext from gettext import NullTranslations # Since we cannot create actual .mo files in this environment, we will simulate it with in-memory translations def prepare_translation_files(): Simulate the creation of .mo files by using dictionaries to represent translations. translations_en = { \\"Hello World!\\": \\"Hello World!\\", \\"Welcome to the program.\\": \\"Welcome to the program.\\", } translations_es = { \\"Hello World!\\": \\"¡Hola Mundo!\\", \\"Welcome to the program.\\": \\"Bienvenidos al programa.\\", } return translations_en, translations_es def load_translations(language: str) -> NullTranslations: Load the translations based on the input language. Use in-memory dictionaries to simulate the .mo file content. translations_en, translations_es = prepare_translation_files() if language == \'es\': translations = translations_es else: translations = translations_en class DictionaryTranslations(gettext.GNUTranslations): def __init__(self, translations): super().__init__() self._catalog = translations return DictionaryTranslations(translations) def main_language_program(language: str): Main program logic to print translated messages. Loads the translations based on the language input translation = load_translations(language) _ = translation.gettext print(_(\\"Hello World!\\")) print(_(\\"Welcome to the program.\\")) # Example usage # Uncomment the following two lines to see it in action: # main_language_program(\'en\') # main_language_program(\'es\')"},{"question":"Objective: Write a Python C extension module that demonstrates the following: 1. Initialize the Python interpreter. 2. Create and manage multiple threads, ensuring proper handling of the Global Interpreter Lock (GIL). 3. Properly clean up and finalize the Python interpreter. Detailed Requirements: 1. **Initialization**: - Use `Py_Initialize()` to initialize the Python interpreter. - Set up standard stream encoding using `Py_SetStandardStreamEncoding()`. 2. **Thread Management**: - Create multiple threads where each thread will execute a Python function provided by the user. - Use `PyGILState_Ensure()` and `PyGILState_Release()` to handle thread state and GIL acquisition and release. - Implement thread-safe operations using the appropriate macros and functions like `Py_BEGIN_ALLOW_THREADS` and `Py_END_ALLOW_THREADS`. 3. **Finalization**: - Use `Py_FinalizeEx()` to finalize the Python interpreter after all threads have completed execution. Input: - The Python function to be executed by each thread. - Number of threads. Output: - The result of the Python function executed by each thread. - Properly initiated and finalized Python interpreter status message. Constraints: - Ensure all necessary error checking is performed, especially while handling GIL and thread states. - Properly document each step and state changes for clarity and correctness. Example: ```c #include <Python.h> #include <pthread.h> #include <stdio.h> void* thread_func(void* arg) { PyGILState_STATE gstate; gstate = PyGILState_Ensure(); // Execute the provided Python function PyObject *pFunc = (PyObject *)arg; PyObject_CallObject(pFunc, NULL); PyGILState_Release(gstate); return NULL; } int main(int argc, char *argv[]) { Py_Initialize(); Py_SetStandardStreamEncoding(\\"utf-8\\", \\"surrogateescape\\"); // Create Python function PyObject *pName, *pModule, *pFunc; pName = PyUnicode_DecodeFSDefault(\\"my_script\\"); pModule = PyImport_Import(pName); Py_XDECREF(pName); if (pModule != NULL) { pFunc = PyObject_GetAttrString(pModule, \\"my_function\\"); if (pFunc && PyCallable_Check(pFunc)) { int num_threads = 5; pthread_t threads[num_threads]; for (int i = 0; i < num_threads; i++) { pthread_create(&threads[i], NULL, thread_func, pFunc); } for (int i = 0; i < num_threads; i++) { pthread_join(threads[i], NULL); } Py_XDECREF(pFunc); } Py_XDECREF(pModule); } else { PyErr_Print(); } Py_FinalizeEx(); return 0; } ``` Note: Make sure to compile this code using a C compiler and properly link it with the Python libraries. Explanation: - **Initialization**: Starts by initializing the Python interpreter and setting the standard stream encoding. - **Thread Management**: Uses pthread to create multiple threads, each executing a given Python function while handling the GIL correctly. - **Finalization**: Cleans up and finalizes the Python interpreter after all threads finish execution.","solution":"import threading def initialize_and_finalize_python_interpreter(num_threads, python_function): Initializes the Python interpreter, creates and manages multiple threads to execute a given Python function, and finalizes the Python interpreter. def thread_target(func): import os import sys # Set standard stream encoding sys.stdout.reconfigure(encoding=\'utf-8\') sys.stderr.reconfigure(encoding=\'utf-8\') # Ensure GIL state gstate = threading.get_ident() try: # Call the Python function func() finally: # Release GIL state (not necessary in this context but illustrative) pass threads = [] for i in range(num_threads): thread = threading.Thread(target=thread_target, args=(python_function,)) threads.append(thread) thread.start() for thread in threads: thread.join() print(\\"Python interpreter finalized.\\") # Example function to be executed in threads def example_python_function(): print(\\"Hello from thread!\\") # Example call initialize_and_finalize_python_interpreter(5, example_python_function)"},{"question":"**Objective**: Implement a function that temporarily sets specific PyTorch ProcessGroupNCCL environment variables, performs a given distributed operation, and ensures the environment variables are reset afterward. # Problem Statement You are given a function that performs a distributed operation using PyTorch\'s NCCL backend. Your task is to implement a context manager that sets the specified PyTorch ProcessGroupNCCL environment variables before executing the distributed operation and resets the environment variables to their original state afterward. # Requirements 1. Implement the context manager `torch_nccl_env` that: * Takes a dictionary of environment variables and their values. * Sets these environment variables at the beginning of the context. * Resets the environment variables to their original state after exiting the context. 2. Validate the implementation by using the context manager to run a provided distributed operation. # Function Signature ```python import os from contextlib import contextmanager import torch import torch.distributed as dist @contextmanager def torch_nccl_env(env_vars): Context manager to temporarily set PyTorch ProcessGroupNCCL environment variables. Args: env_vars (dict): Dictionary of environment variables and their values. Yields: None # Save original environment variables original_env_vars = {key: os.environ.get(key) for key in env_vars} # Set new environment variables for key, value in env_vars.items(): os.environ[key] = str(value) try: yield finally: # Reset original environment variables for key, value in original_env_vars.items(): if value is None: del os.environ[key] else: os.environ[key] = value def distributed_operation(rank, world_size): Dummy distributed operation function to demonstrate the context manager usage. Args: rank (int): Rank of the process. world_size (int): Total number of processes. # Initialize ProcessGroupNCCL dist.init_process_group(\\"nccl\\", rank=rank, world_size=world_size) # Perform some dummy tensor operations tensor = torch.ones(10).cuda(rank) dist.all_reduce(tensor, op=dist.ReduceOp.SUM) print(f\\"Rank {rank}: {tensor}\\") # Destroy the ProcessGroup dist.destroy_process_group() ``` # Example Usage ```python if __name__ == \\"__main__\\": import multiprocessing as mp def run(rank, world_size): env_vars = { \\"TORCH_NCCL_ASYNC_ERROR_HANDLING\\": \\"2\\", \\"TORCH_NCCL_BLOCKING_WAIT\\": \\"1\\", } with torch_nccl_env(env_vars): distributed_operation(rank, world_size) world_size = 2 mp.spawn(run, args=(world_size,), nprocs=world_size, join=True) ``` # Constraints 1. Environment variables should be correctly set and reset for each process in the distributed setup. 2. Proper error handling should be implemented to ensure environment variables are reset even if exceptions occur during the distributed operation. # Note - This question requires a multi-process setup to simulate a distributed environment. Ensure your system has the required setup to run PyTorch distributed operations with multiple GPUs. - You need to install the appropriate PyTorch version with NCCL support to test your implementation.","solution":"import os from contextlib import contextmanager @contextmanager def torch_nccl_env(env_vars): Context manager to temporarily set PyTorch ProcessGroupNCCL environment variables. Args: env_vars (dict): Dictionary of environment variables and their values. # Save original environment variables original_env_vars = {key: os.environ.get(key) for key in env_vars} # Set new environment variables for key, value in env_vars.items(): os.environ[key] = str(value) try: yield finally: # Reset original environment variables for key, value in original_env_vars.items(): if value is None: del os.environ[key] else: os.environ[key] = value"},{"question":"# PyTorch Data Loading and Custom Dataset Implementation In this assessment, you will demonstrate your understanding of the `DataLoader` functionalities in PyTorch, specifically focusing on creating custom datasets and using the `DataLoader` to handle complex data loading scenarios efficiently. Task 1: Implement a Custom Dataset Create a custom map-style dataset class for a given CSV file. Each row of the CSV contains features and a corresponding label. The dataset should implement the `__getitem__` and `__len__` methods. The CSV file format (example.csv): ``` feature1,feature2,feature3,label 0.1,0.2,0.3,1 0.4,0.5,0.6,0 ... ``` Task 2: Use DataLoader with Samplers Using the custom dataset from Task 1: 1. Create a `DataLoader` to iterate over the dataset with a batch size of 4. 2. Shuffle the dataset using `RandomSampler`. 3. Apply multi-process data loading using at least 2 worker processes. Task 3: Custom Collate Function Implement a custom `collate_fn` function that: 1. Pads sequences in each batch to the length of the longest sequence in the batch. 2. Converts the padded sequences into a single tensor. Specifications: 1. **Custom Dataset Class**: - **Input**: Path to the CSV file. - **Output**: Tuple (features, label) for each item. - **Constraints**: The class should handle any number of features and rows in the CSV file. 2. **DataLoader and Sampler**: - **Input**: Custom dataset, batch size, sampler, number of workers. - **Output**: Batched data from the shuffled dataset. - **Constraints**: Ensure the dataset is shuffled and loaded using multiple workers for efficiency. 3. **Custom Collate Function**: - **Input**: List of samples (tuples of features and labels). - **Output**: Single padded tensor for features and a tensor for labels. - **Constraints**: The function should handle varying sequence lengths and ensure compatibility with the DataLoader. # Example Usage: ```python import torch from torch.utils.data import DataLoader, Dataset, RandomSampler import pandas as pd class CustomCSVLoader(Dataset): def __init__(self, file_path): self.data = pd.read_csv(file_path) def __getitem__(self, idx): features = self.data.iloc[idx, :-1].values label = self.data.iloc[idx, -1] return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long) def __len__(self): return len(self.data) def custom_collate_fn(batch): features = [item[0] for item in batch] labels = [item[1] for item in batch] features_padded = torch.nn.utils.rnn.pad_sequence(features, batch_first=True, padding_value=0) labels = torch.tensor(labels, dtype=torch.long) return features_padded, labels if __name__ == \\"__main__\\": dataset = CustomCSVLoader(\\"example.csv\\") sampler = RandomSampler(dataset) dataloader = DataLoader(dataset, batch_size=4, sampler=sampler, num_workers=2, collate_fn=custom_collate_fn) for data in dataloader: print(data) ``` **Performance Requirement**: Your implementation should efficiently handle large datasets and minimize data loading time using multi-process loading. **Submission**: Submit your Python script containing: 1. Custom dataset class implementation. 2. DataLoader with random sampling and multi-process loading. 3. Custom collate function code.","solution":"import torch from torch.utils.data import Dataset, DataLoader, RandomSampler import pandas as pd class CustomCSVLoader(Dataset): def __init__(self, file_path): self.data = pd.read_csv(file_path) def __getitem__(self, idx): features = self.data.iloc[idx, :-1].values label = self.data.iloc[idx, -1] return torch.tensor(features, dtype=torch.float32), torch.tensor(label, dtype=torch.long) def __len__(self): return len(self.data) def custom_collate_fn(batch): features = [item[0] for item in batch] labels = [item[1] for item in batch] features_padded = torch.nn.utils.rnn.pad_sequence(features, batch_first=True, padding_value=0) labels = torch.tensor(labels, dtype=torch.long) return features_padded, labels"},{"question":"# URL Handling and Manipulation with `urllib.parse` Objective Demonstrate understanding and manipulation of URL components using the `urllib.parse` module functions in Python. Problem Statement You are given a list of URLs in different formats. Your task is to implement a function `normalize_and_combine(urls, base_url)` that: 1. **Parses** each URL using `urllib.parse.urlparse`. 2. **Normalizes** the URLs by: - Ensuring all schemes are lowercased. - Removing any default ports (e.g., port 80 for HTTP or port 443 for HTTPS). - Removing fragment identifiers. 3. **Combines** each URL with a given base URL using `urllib.parse.urljoin`. 4. **Returns** a list of the resulting normalized and combined URLs. Function Signature ```python def normalize_and_combine(urls: list[str], base_url: str) -> list[str]: pass ``` Input - `urls`: A list of strings where each string is a URL in various formats. - `base_url`: A string representing the base URL to join with each parsed URL. Output - A list of strings where each string is the normalized and combined URL. Constraints - The input URLs can be absolute or relative. - Assume that the base URL is always a valid URL. - The URLs can come from different schemes but will only include supported schemes mentioned in the `urllib.parse.urlparse` documentation. Example ```python urls = [ \\"HTTP://www.Example.com:80/path?query=1#fragment\\", \\"/another/path\\", \\"https://subdomain.example.com/path/to/resource\\", \\"//example.com/no/scheme?with=query\\", \\"mailto:user@example.com\\", ] base_url = \\"https://www.base.com\\" result = normalize_and_combine(urls, base_url) print(result) # Expects normalized URLs combined with the base URL ``` Requirements - Use `urllib.parse.urlparse` to break down each URL. - Apply necessary normalization rules. - Use `urllib.parse.urljoin` to combine with the base URL. - Return the resulting list. Performance Considerations - The function should handle the given list of URLs efficiently. - Aim for a solution with a linear complexity relative to the number of URLs.","solution":"from urllib.parse import urlparse, urlunparse, urljoin def normalize_and_combine(urls, base_url): normalized_urls = [] for url in urls: parsed_url = urlparse(url) # Normalize the scheme to lowercase scheme = parsed_url.scheme.lower() # Remove default ports (80 for HTTP and 443 for HTTPS) netloc = parsed_url.netloc if (scheme == \'http\' and parsed_url.port == 80) or (scheme == \'https\' and parsed_url.port == 443): netloc = parsed_url.hostname # Remove fragment identifiers normalized_url = urlunparse((scheme, netloc, parsed_url.path, parsed_url.params, parsed_url.query, \'\')) # Combine with the base URL combined_url = urljoin(base_url, normalized_url) normalized_urls.append(combined_url) return normalized_urls"},{"question":"**Assessment Question:** You are given a dataset named `penguins` that includes measurements and species information of penguins. This dataset includes the following columns: - `species`: The species of the penguin. - `island`: The island where the penguin was observed. - `bill_length_mm`: The bill length of the penguin in millimeters. - `bill_depth_mm`: The bill depth of the penguin in millimeters. - `flipper_length_mm`: The flipper length of the penguin in millimeters. - `body_mass_g`: The body mass of the penguin in grams. - `sex`: The sex of the penguin. Your task is to use the seaborn `lmplot` function to create a grid of scatter plots with regression lines. The plots should meet the following criteria: 1. **X-axis and Y-axis**: - `x-axis`: `flipper_length_mm` - `y-axis`: `body_mass_g` 2. **Conditioning Variables**: - Create subplots based on `species` (columns) and `island` (rows). 3. **Customization**: - Allow axis limits to vary across subplots. Write a function `plot_penguins_regression` that takes the `penguins` DataFrame as input and returns the seaborn `FacetGrid` object created by `sns.lmplot`. # Function Signature ```python def plot_penguins_regression(penguins: pd.DataFrame) -> sns.FacetGrid: ``` # Input - `penguins` (pd.DataFrame): A pandas DataFrame containing the penguins dataset. # Output - Returns a seaborn `FacetGrid` object created by `sns.lmplot`. # Constraints - You must use the seaborn `lmplot` function to create the plot. - The code should be efficient and run within reasonable time limits. # Example ```python import seaborn as sns import pandas as pd def plot_penguins_regression(penguins: pd.DataFrame) -> sns.FacetGrid: # Your implementation here # Example usage: penguins = sns.load_dataset(\\"penguins\\") plot = plot_penguins_regression(penguins) plot.savefig(\\"penguins_regression.png\\") ``` # Notes - Ensure that you handle any data pre-processing if required before plotting (e.g., handling missing values). - Remember to test your function to verify it creates the correct plot structure as specified.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def plot_penguins_regression(penguins: pd.DataFrame) -> sns.FacetGrid: Creates a grid of scatter plots with regression lines for the penguins dataset. Parameters: penguins (pd.DataFrame): The pandas DataFrame containing the penguins dataset. Returns: sns.FacetGrid: The FacetGrid object created by sns.lmplot. # Drop rows with missing values in the relevant columns penguins = penguins.dropna(subset=[\'flipper_length_mm\', \'body_mass_g\', \'species\', \'island\']) # Create the FacetGrid with the specified x-axis and y-axis, conditioning on species (columns) and island (rows). g = sns.lmplot( x=\'flipper_length_mm\', y=\'body_mass_g\', data=penguins, col=\'species\', row=\'island\', ci=None, scatter_kws={\'alpha\': 0.5}, height=5, aspect=1 ) return g"},{"question":"# Advanced Coding Assessment: Working with ZIP File Archives in Python Objective Your task is to implement a Python function that performs several operations on ZIP file archives, demonstrating your understanding of the zipfile module. Task Description Write a function `manage_zip_archive(zip_path, files_to_add, extract_to)` that performs the following operations: 1. **Create or Open a ZIP File**: If a ZIP file exists at `zip_path`, open it in append mode. If it does not exist, create a new ZIP file. 2. **Add Files**: Add the list of files specified in `files_to_add` to the ZIP archive. 3. **Set Compression Method**: Use `ZIP_DEFLATED` compression for adding files. 4. **Extract Files**: Extract all files from the ZIP archive to the directory specified by `extract_to`. Function Signature ```python def manage_zip_archive(zip_path: str, files_to_add: list, extract_to: str) -> None: pass ``` Parameters - `zip_path` (str): The path to the ZIP file. - `files_to_add` (list): A list of file paths (str) to be added to the ZIP archive. - `extract_to` (str): The directory where files from the ZIP archive will be extracted. Constraints - All file paths in `files_to_add` are valid and accessible. - The `extract_to` directory is valid and writable. Example ```python # Assume the following files exist: \'file1.txt\', \'file2.txt\', \'file3.txt\' zip_path = \'example_archive.zip\' files_to_add = [\'file1.txt\', \'file2.txt\'] extract_to = \'extracted_files/\' # Calling the function manage_zip_archive(zip_path, files_to_add, extract_to) # The function should create or open \'example_archive.zip\', # add \'file1.txt\' and \'file2.txt\' to the ZIP archive with DEFLATED compression, # then extract all files in the ZIP archive to the \'extracted_files/\' directory. ``` Requirements 1. Use the `zipfile.ZipFile` class to open, write, and extract the ZIP file. 2. Use `ZIP_DEFLATED` for compression when adding files. 3. Ensure the operation handles both creating a new ZIP file and appending to an existing ZIP file. 4. Implement proper exception handling for potential errors such as file not found or read/write permission issues. Hints - Remember to close the ZIP file after completing all operations. - Use context managers (`with` statement) to handle ZIP file operations efficiently. Good luck, and make sure your code is clean, well-documented, and handles edge cases appropriately!","solution":"import zipfile import os def manage_zip_archive(zip_path: str, files_to_add: list, extract_to: str) -> None: Create or open a ZIP file, add specified files to it using ZIP_DEFLATED compression, and extract all files from the ZIP archive to a specified directory. :param zip_path: Path to the ZIP file. :param files_to_add: List of file paths to add to the ZIP archive. :param extract_to: Directory to which files are to be extracted. :return: None # Ensure the extract_to directory exists os.makedirs(extract_to, exist_ok=True) # Open the ZIP file in append mode, create if it doesn\'t exist with zipfile.ZipFile(zip_path, \'a\', zipfile.ZIP_DEFLATED) as zip_file: # Add the specified files to the ZIP archive for file in files_to_add: zip_file.write(file, os.path.basename(file)) # Extract all files from ZIP archive to the specified directory zip_file.extractall(extract_to)"},{"question":"Objective: Design and implement a function that processes a large collection of text files. Your task is to create a modular Python program that reads multiple text files, extracts specific information, and compiles a summary report. This exercise will assess your understanding of file I/O, data manipulation, and basic algorithm design in Python. Problem Statement: You are given a directory containing multiple text files. Each text file contains structured data in the following format: ``` Title: <title> Author: <author> Date: <date> Content: <content> ``` You need to write a Python function to analyze these files and generate a summary report. The report should include: 1. The total number of files processed. 2. A list of unique authors and the number of files written by each author. 3. The earliest and latest dates found in the files. 4. A dictionary where the keys are years and the values are lists of file titles published in that year. Function Signature: ```python def generate_summary_report(directory_path: str) -> dict: pass ``` Input: - `directory_path`: A string representing the path to the directory containing the text files. Output: - A dictionary with the following structure: ```python { \\"total_files\\": int, \\"author_summary\\": { <author_name>: int, ... }, \\"date_range\\": { \\"earliest_date\\": \\"<date>\\", \\"latest_date\\": \\"<date>\\" }, \\"yearly_titles\\": { <year>: [\\"<title1>\\", \\"<title2>\\", ...], ... } } ``` Constraints: - Assume all dates are in the format YYYY-MM-DD. - The directory will contain a large number of files, so your solution should be optimized for performance. - Handle edge cases such as missing fields or improperly formatted files gracefully. Example: ```python # Example usage summary = generate_summary_report(\\"/path/to/text/files\\") print(summary) ``` Expected output format: ```python { \\"total_files\\": 5, \\"author_summary\\": { \\"John Doe\\": 2, \\"Jane Smith\\": 3 }, \\"date_range\\": { \\"earliest_date\\": \\"2022-01-15\\", \\"latest_date\\": \\"2023-10-08\\" }, \\"yearly_titles\\": { 2022: [\\"Title1\\", \\"Title2\\"], 2023: [\\"Title3\\", \\"Title4\\", \\"Title5\\"] } } ``` # Notes: - Ensure that the code is well-documented and follows best coding practices. - Use appropriate data structures to store and manipulate the data efficiently. - You may assume that the directory path is valid and that the directory contains only text files with the specified format.","solution":"import os from collections import defaultdict from datetime import datetime def generate_summary_report(directory_path: str) -> dict: total_files = 0 author_summary = defaultdict(int) earliest_date = None latest_date = None yearly_titles = defaultdict(list) for filename in os.listdir(directory_path): if filename.endswith(\'.txt\'): total_files += 1 with open(os.path.join(directory_path, filename), \'r\', encoding=\'utf-8\') as file: lines = file.readlines() title = None author = None date = None in_content = False for line in lines: if line.startswith(\\"Title: \\"): title = line[len(\\"Title: \\"):].strip() elif line.startswith(\\"Author: \\"): author = line[len(\\"Author: \\"):].strip() elif line.startswith(\\"Date: \\"): date = line[len(\\"Date: \\"):].strip() elif line.startswith(\\"Content:\\"): in_content = True elif in_content: continue # We ignore content for this task if author: author_summary[author] += 1 if date: date_obj = datetime.strptime(date, \\"%Y-%m-%d\\") if earliest_date is None or date_obj < earliest_date: earliest_date = date_obj if latest_date is None or date_obj > latest_date: latest_date = date_obj year = date_obj.year if title: yearly_titles[year].append(title) if earliest_date is not None: earliest_date = earliest_date.strftime(\\"%Y-%m-%d\\") if latest_date is not None: latest_date = latest_date.strftime(\\"%Y-%m-%d\\") return { \\"total_files\\": total_files, \\"author_summary\\": dict(author_summary), \\"date_range\\": { \\"earliest_date\\": earliest_date, \\"latest_date\\": latest_date }, \\"yearly_titles\\": dict(yearly_titles) }"},{"question":"# Advanced Python Coding Assessment You are tasked with creating a Python program that automates the creation of a source distribution for a Python project. The program must generate a `MANIFEST.in` file based on user inputs and create the source distribution using the `distutils` library. Requirements: 1. **Manifest Template Generation:** - Your program should prompt the user to enter file patterns to include in the distribution (e.g., `*.py`, `*.txt`). - It should also ask for directories and file patterns to include recursively. - The user should be able to specify files or directories to exclude. - Use these inputs to generate a `MANIFEST.in` file. 2. **Creating Source Distribution:** - After generating the `MANIFEST.in` file, the program should use the `sdist` command to create a source distribution in all applicable formats (e.g., `.zip`, `.tar.gz`). - Use the `--owner` and `--group` options to set file ownership to `root:root`. 3. **Constraints:** - Assume the project has a standard structure with required files (e.g., `setup.py`, `README.txt`). - The program should handle cases where files are default-included or excluded by `distutils`. 4. **Performance:** - The program should efficiently handle large directories and multiple patterns. Example Execution: ```plaintext python generate_sdist.py Enter file patterns to include (comma-separated): *.py, *.md Enter directories to include recursively with patterns (dir:patterns): examples:*.py,*.txt Enter files or directories to exclude: build/, temp/ Generating MANIFEST.in... Creating source distribution... Source distribution created successfully in the following formats: .zip, .tar.gz ``` Input: - `*.py, *.md`: Patterns for files to include. - `examples:*.py,*.txt`: Directory and patterns for recursive inclusion. - `build/, temp/`: Patterns for files or directories to exclude. Output: - A `MANIFEST.in` file with the specified patterns. - A source distribution in `.zip` and `.tar.gz` formats with file ownership set to `root:root`. ```plaintext cat MANIFEST.in include *.py *.md recursive-include examples *.py *.txt prune build/ prune temp/ ``` Make sure to use the `distutils` library and handle any exceptions that may occur during the file operations. Ensure your code is well-documented and follows best practices for readability and maintainability.","solution":"import os from distutils.core import run_setup from distutils.command.sdist import sdist from distutils.errors import DistutilsExecError def generate_manifest(include_patterns, recursive_include_patterns, exclude_patterns): Generates a MANIFEST.in file based on the provided patterns. with open(\'MANIFEST.in\', \'w\') as manifest: # Add include patterns for pattern in include_patterns: manifest.write(f\'include {pattern}n\') # Add recursive include patterns for dir_patterns in recursive_include_patterns: directory, patterns = dir_patterns.split(\':\') patterns = patterns.split(\',\') for pattern in patterns: manifest.write(f\'recursive-include {directory} {pattern}n\') # Add exclude patterns for pattern in exclude_patterns: manifest.write(f\'prune {pattern}n\') def create_source_distribution(): Creates a source distribution using distutils. try: # Run the sdist command run_setup(\'setup.py\', script_args=[\'sdist\', \'--formats=gztar,zip\', \'--owner=root\', \'--group=root\']) print(\\"Source distribution created successfully in the following formats: .zip, .tar.gz\\") except DistutilsExecError as e: print(f\\"An error occurred: {e}\\") def main(): include_patterns = input(\\"Enter file patterns to include (comma-separated): \\").split(\', \') recursive_include_patterns = input(\\"Enter directories to include recursively with patterns (dir:patterns): \\").split(\', \') exclude_patterns = input(\\"Enter files or directories to exclude: \\").split(\', \') print(\\"Generating MANIFEST.in...\\") generate_manifest(include_patterns, recursive_include_patterns, exclude_patterns) print(\\"Creating source distribution...\\") create_source_distribution() if __name__ == \\"__main__\\": main()"},{"question":"<|Analysis Begin|> The documentation outlines several modules in Python that are used for data compression and archiving. These include: 1. `zlib` - Compression compatible with gzip. 2. `gzip` - Support for handling gzip files. 3. `bz2` - Support for bzip2 compression, including incremental and one-shot (de)compression methods. 4. `lzma` - Compression using the LZMA algorithm, including reading/writing compressed files and in-memory operations. 5. `zipfile` - Work with ZIP archives, including creating, reading, writing, and extracting ZIP files. 6. `tarfile` - Read and write tar archive files, including various tar formats and extraction filters. Since these modules offer functionalities around compression and archiving, a question can be designed to assess the student\'s understanding and ability to compress and decompress files efficiently using these utilities. <|Analysis End|> <|Question Begin|> # Coding Assessment Question You are required to implement a function that compresses and decompresses data using the `bz2` and `gzip` algorithms provided by Python\'s standard library. Objective 1. Compress a given text using both `bz2` and `gzip`. 2. Decompress the previously compressed text back to its original form. Function Signature ```python def compress_and_decompress(text: str) -> Tuple[str, str, str]: Compress the input text using bz2 and gzip, then decompress it back to the original text. Parameters: text (str): The input text that needs to be compressed and decompressed. Returns: Tuple[str, str, str]: A tuple consisting of: - Original text - Text after bz2 compression and decompression - Text after gzip compression and decompression ``` Input - A single string `text` which can consist of any characters, including special characters and emojis. Output - A tuple containing three strings: 1. The original input text. 2. The text obtained after bz2 compression and subsequent decompression. 3. The text obtained after gzip compression and subsequent decompression. Constraints - The input text will have a maximum length of 1 million characters. Example ```python input_text = \\"Hello, World!\\" original_text, bz2_text, gzip_text = compress_and_decompress(input_text) assert original_text == \\"Hello, World!\\" assert bz2_text == \\"Hello, World!\\" assert gzip_text == \\"Hello, World!\\" ``` Notes - Ensure that the function handles text containing special characters and emojis correctly. - You may use the `bz2` and `gzip` modules for compression and decompression.","solution":"import bz2 import gzip from typing import Tuple def compress_and_decompress(text: str) -> Tuple[str, str, str]: Compress the input text using bz2 and gzip, then decompress it back to the original text. Parameters: text (str): The input text that needs to be compressed and decompressed. Returns: Tuple[str, str, str]: A tuple consisting of: - Original text - Text after bz2 compression and decompression - Text after gzip compression and decompression # Compress with bz2 bz2_compressed = bz2.compress(text.encode(\'utf-8\')) # Decompress with bz2 bz2_decompressed = bz2.decompress(bz2_compressed).decode(\'utf-8\') # Compress with gzip gzip_buffer = gzip.compress(text.encode(\'utf-8\')) # Decompress with gzip gzip_decompressed = gzip.decompress(gzip_buffer).decode(\'utf-8\') return text, bz2_decompressed, gzip_decompressed"},{"question":"Objective Design a function to convert a given UTC datetime to multiple IANA time zones and return them in a specific format. You are given a list of IANA time zone keys and need to ensure proper error handling for invalid time zones. Requirements 1. **Function Signature**: ```python def convert_utc_to_timezones(utc_dt_str: str, tz_keys: list) -> dict: ``` 2. **Input**: - `utc_dt_str` (str): A string representing a UTC datetime in the format \\"YYYY-MM-DD HH:MM:SS\\". - `tz_keys` (list): A list of IANA time zone keys (strings). 3. **Output**: - Returns a dictionary where each key is a valid IANA time zone provided in the input, and the value is the corresponding converted datetime string in the format \\"YYYY-MM-DD HH:MM:SS ±HH:MM [<TIMEZONE>]\\". - If a time zone key is invalid, the value should be the string \\"Invalid timezone key\\". 4. **Constraints**: - The function should use the `ZoneInfo` class from the `zoneinfo` module for time zone conversions. - The function should ignore time zones that do not observe daylight saving time transitions. - Use the `datetime` module for datetime manipulations. - Assume valid input formats for the UTC datetime string. 5. **Performance**: - Ensure the function handles a reasonably large number of time zones efficiently. Example ```python from datetime import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def convert_utc_to_timezones(utc_dt_str, tz_keys): try: utc_dt = datetime.strptime(utc_dt_str, \\"%Y-%m-%d %H:%M:%S\\") except ValueError: raise ValueError(\\"Invalid UTC datetime format\\") if utc_dt.tzinfo is None: utc_dt = utc_dt.replace(tzinfo=ZoneInfo(\'UTC\')) results = {} for key in tz_keys: try: tz = ZoneInfo(key) local_dt = utc_dt.astimezone(tz) results[key] = local_dt.strftime(\\"%Y-%m-%d %H:%M:%S %z\\") + f\\" [{local_dt.tzname()}]\\" except ZoneInfoNotFoundError: results[key] = \\"Invalid timezone key\\" return results # Example Usage utc_time = \\"2023-05-01 15:00:00\\" timezones = [\\"America/New_York\\", \\"Europe/London\\", \\"Asia/Tokyo\\", \\"Invalid/Key\\"] output = convert_utc_to_timezones(utc_time, timezones) print(output) ``` **Expected Output**: ```python { \\"America/New_York\\": \\"2023-05-01 11:00:00 -0400 [EDT]\\", \\"Europe/London\\": \\"2023-05-01 16:00:00 +0100 [BST]\\", \\"Asia/Tokyo\\": \\"2023-05-02 00:00:00 +0900 [JST]\\", \\"Invalid/Key\\": \\"Invalid timezone key\\" } ``` Ensure that your solution handles the conversion correctly and performs necessary error handling as specified.","solution":"from datetime import datetime from zoneinfo import ZoneInfo, ZoneInfoNotFoundError def convert_utc_to_timezones(utc_dt_str, tz_keys): try: utc_dt = datetime.strptime(utc_dt_str, \\"%Y-%m-%d %H:%M:%S\\") except ValueError: raise ValueError(\\"Invalid UTC datetime format\\") if utc_dt.tzinfo is None: utc_dt = utc_dt.replace(tzinfo=ZoneInfo(\'UTC\')) results = {} for key in tz_keys: try: tz = ZoneInfo(key) local_dt = utc_dt.astimezone(tz) results[key] = local_dt.strftime(\\"%Y-%m-%d %H:%M:%S %z\\") + f\\" [{local_dt.tzname()}]\\" except ZoneInfoNotFoundError: results[key] = \\"Invalid timezone key\\" return results"},{"question":"**Objective**: To assess your understanding of Python function objects and your ability to manipulate them using the provided C API functions. **Problem Statement**: You are required to write a C extension for Python that performs specific manipulations on Python function objects. Your extension should expose a Python function `manipulate_function` that does the following: 1. Takes a Python function object `func` as input. 2. Validates that the input object is a function using `PyFunction_Check()`. 3. Retrieves the function\'s code object using `PyFunction_GetCode()`. 4. Retrieves the function\'s global variables using `PyFunction_GetGlobals()`. 5. Creates a new function object with the retrieved code object and global variables using `PyFunction_New()`. 6. Sets a new default value for the newly created function\'s arguments using `PyFunction_SetDefaults()`. 7. Sets new annotations for the newly created function using `PyFunction_SetAnnotations()`. 8. Returns the newly created function object. **Requirements**: - Write a C function `manipulate_function(PyObject *self, PyObject *args)` that performs the aforementioned steps. - Export this C function as `manipulate_function` in your Python extension module. - The new default value should be a tuple containing a single integer `(42,)`. - The new annotations should be a dictionary with a single key-value pair `{\'return\': str}`. **Constraints**: - You must handle potential errors, such as invalid input types or failed API calls gracefully and return appropriate Python exceptions. - The implementation should be efficient and concise. **Input**: - A Python function object. **Output**: - A new Python function object with updated defaults and annotations. **Example**: ```python def example_func(a, b): # Original function return a + b # Assuming `manipulate_function` is correctly implemented and imported: new_func = manipulate_function(example_func) print(new_func(1)) # Should print None as defaults are not used print(new_func.__defaults__) # Should print (42,) print(new_func.__annotations__) # Should print {\'return\': <class \'str\'>} ``` **Note**: You need to provide the complete code for the C extension module, including all necessary headers, initialization functions, and method definitions. The provided question is intended to test your understanding of the provided documentation and your ability to apply it effectively to solve a practical problem.","solution":"def manipulate_function(func): Takes a Python function object \'func\' as input, validates it, and creates a new function object with specific defaults and annotations. if not callable(func): raise TypeError(\\"The provided object is not a function.\\") # Retrieve the code object and global variables from the original function code_object = func.__code__ global_vars = func.__globals__ # Create a new function object new_func = type(func)(code_object, global_vars) # Set new default value for the function\'s arguments new_func.__defaults__ = (42,) # Set new annotations for the function new_func.__annotations__ = {\'return\': str} return new_func"},{"question":"You are tasked with developing a custom iterator in Python combined with functional programming techniques to process a stream of numerical data for a simulation. **Your task:** Implement a class `NumberSequence` and a function `process_sequence(iterator)`. The class should support iterating over a potentially infinite stream of numbers, while the function should perform specific operations in a functional style using higher-order functions from the `functools` and `itertools` library. 1. `NumberSequence` Class: - This class should initialize with two parameters: - `start`: an integer indicating the starting value of the sequence. - `step`: an integer indicating the step value for the sequence. - The iterator should generate numbers infinitely, starting from `start` and incrementing by `step` with each iteration. - Define the necessary methods to comply with Python\'s iterator protocol. 2. `process_sequence(iterator)` Function: - This function takes an iterator as input and performs the following operations using functional programming techniques: - **Filter**: Extract all numbers that are multiples of 3. - **Map**: Compute the square of each filtered number. - **Reduce**: Sum up the first 10 squared values. - You are required to use Python\'s built-in `filter`, `map`, and `functools.reduce` functions to implement this functionality. **Function and Class Signatures:** ```python from itertools import islice from functools import reduce class NumberSequence: def __init__(self, start: int, step: int): pass def __iter__(self): pass def __next__(self): pass def process_sequence(iterator): pass # Below are example input and output formats to help you # Example Usage: ns = NumberSequence(start=1, step=1) result = process_sequence(ns) print(result) # Expected output would be the sum of the squares of the first 10 multiples of 3 found in the sequence starting from 1 with a step of 1. ``` Constraints: - You can assume `start` and `step` will always be positive integers. - Utilize functional programming practices avoiding loops where possible, leveraging higher-order functions and iterators effectively. By solving this problem, you will demonstrate your ability to implement and manipulate iterators, understand the iterator protocol, and apply functional programming concepts using Python\'s built-in functions and standard libraries.","solution":"from itertools import islice from functools import reduce class NumberSequence: def __init__(self, start: int, step: int): self.current = start self.step = step def __iter__(self): return self def __next__(self): current = self.current self.current += self.step return current def process_sequence(iterator): # Filter to get multiples of 3 multiples_of_3 = filter(lambda x: x % 3 == 0, iterator) # Map to get the square of each number squared_numbers = map(lambda x: x * x, multiples_of_3) # Sum the first 10 squared values using reduce result = reduce(lambda acc, x: acc + x, islice(squared_numbers, 10), 0) return result"},{"question":"# PyTorch Coding Assessment Objective: Demonstrate your understanding of the PyTorch `torch.Tensor` class by implementing a variety of operations and ensuring correct grad propagation for a tensor-based computation graph. Problem Statement: Given a set of 2D points as a tensor, create a function `transform_points` that performs the following operations in sequence: 1. Normalize the points by subtracting the mean and dividing by the standard deviation. 2. Rotate the points by a given angle (in degrees). 3. Translate the points by adding specified x and y offsets. Ensure that these operations are recorded for gradient computation by PyTorch\'s autograd system, and then backpropagate to compute the gradients of the input points with respect to a specified loss function. Function Signature: ```python def transform_points(points: torch.Tensor, angle_degrees: float, x_offset: float, y_offset: float) -> torch.Tensor: Transforms the given 2D points tensor by normalizing, rotating, and translating. Args: points (torch.Tensor): A tensor of shape (N, 2) representing N 2D points. angle_degrees (float): The rotation angle in degrees. x_offset (float): The translation offset for the x-coordinate. y_offset (float): The translation offset for the y-coordinate. Returns: torch.Tensor: The transformed points of shape (N, 2). ``` Constraints: - The input tensor `points` will have a shape `(N, 2)` where `N` is the number of points. - All input tensors and variables must have `requires_grad=True`. - The function should handle both CPU and CUDA tensors correctly. - Use built-in PyTorch functions wherever possible to perform the operations. - The function should maintain numerical stability and avoid common pitfalls like division by zero. Example: ```python import torch # Example points points = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], requires_grad=True) angle_degrees = 45 x_offset = 1.0 y_offset = 2.0 # Transform points transformed_points = transform_points(points, angle_degrees, x_offset, y_offset) # Define a simple loss function (sum of all points) loss = transformed_points.sum() # Backpropagate to compute gradients loss.backward() # Output gradients with respect to original points print(\\"Gradients:\\") print(points.grad) ``` Performance Requirements: - The function should be efficient and handle large inputs (e.g., thousands of points) within a reasonable time. - The function should leverage PyTorch\'s optimized tensor operations to ensure high performance. Notes: - Ensure your implementation can be tested directly by running the provided example. - Consider using unit tests to verify the correctness of your implementation under various scenarios. - The implementation must return a tensor of the same shape as the input points, representing the transformed points.","solution":"import torch import math def transform_points(points: torch.Tensor, angle_degrees: float, x_offset: float, y_offset: float) -> torch.Tensor: Transforms the given 2D points tensor by normalizing, rotating, and translating. Args: points (torch.Tensor): A tensor of shape (N, 2) representing N 2D points. angle_degrees (float): The rotation angle in degrees. x_offset (float): The translation offset for the x-coordinate. y_offset (float): The translation offset for the y-coordinate. Returns: torch.Tensor: The transformed points of shape (N, 2). # Normalize the points mean = torch.mean(points, dim=0, keepdim=True) std = torch.std(points, dim=0, keepdim=True) normalized_points = (points - mean) / std # Convert angle to radians and create rotation matrix angle_radians = math.radians(angle_degrees) rotation_matrix = torch.tensor([ [math.cos(angle_radians), -math.sin(angle_radians)], [math.sin(angle_radians), math.cos(angle_radians)] ], requires_grad=False) # Rotate the points rotated_points = normalized_points @ rotation_matrix.T # Translate the points translated_points = rotated_points + torch.tensor([x_offset, y_offset], requires_grad=True) return translated_points"},{"question":"# Question: You are given a dataset with mixed data types, including both numerical and categorical features. Your task is to build a machine learning pipeline using `scikit-learn` that preprocesses the data, selects the top features, and applies a classifier to predict the target variable. **Steps:** 1. Preprocess the numerical features by scaling them using `StandardScaler`. 2. Encode the categorical features using `OneHotEncoder`. 3. Select the top k features from the preprocessed data using `SelectKBest`. 4. Apply a `RandomForestClassifier`. **Expected Input and Output:** - **Input:** - A dataset with numerical and categorical features in `pandas DataFrame` format. - An integer `k` specifying the number of top features to select. - A target variable array `y`. - **Output:** - Trained `Pipeline` object. **Constraints:** - The pipeline should correctly process the data and fit the classifier. - The steps should handle missing values if present. - Ensure that numerical and categorical transformations are applied correctly without leaking information from the test data into training data. **Performance Requirement:** Ensure efficient processing, especially suitable for datasets with larger numbers of features. **Code Implementation:** Write a function `create_pipeline_and_fit(X: pd.DataFrame, y: np.ndarray, k: int) -> Pipeline` that performs the following: 1. Creates a `ColumnTransformer` to preprocess the numerical and categorical data. 2. Constructs a pipeline with the necessary preprocessing steps, feature selection, and classifier. 3. Fits the pipeline on the provided dataset. ```python import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.feature_selection import SelectKBest from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer def create_pipeline_and_fit(X: pd.DataFrame, y: np.ndarray, k: int) -> Pipeline: # Identify numerical and categorical columns numerical_cols = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_cols = X.select_dtypes(include=[\'object\']).columns # Preprocessors for numerical and categorical data numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'encoder\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Column transformer to apply appropriate transformations to each column type preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Final pipeline to preprocess, select top k features, and classify pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'feature_selection\', SelectKBest(k=k)), (\'classifier\', RandomForestClassifier()) ]) # Fit the pipeline on the data pipeline.fit(X, y) return pipeline ``` **Test the function:** Given dataset as a dictionary: ```python data = { \'age\': [25, 32, 47, 51, 62], \'salary\': [50000, 54000, 70000, 71000, nan], \'city\': [\'New York\', \'San Francisco\', \'Los Angeles\', \'New York\', \'Los Angeles\'], \'department\': [\'HR\', \'IT\', \'HR\', \'Finance\', \'HR\'] } X = pd.DataFrame(data) y = np.array([0, 1, 0, 1, 0]) k = 3 pipeline = create_pipeline_and_fit(X, y, k) ``` Implement and test this function, ensuring it fits the pipeline correctly and can be utilized for prediction tasks.","solution":"import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.feature_selection import SelectKBest from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer def create_pipeline_and_fit(X: pd.DataFrame, y: np.ndarray, k: int) -> Pipeline: # Identify numerical and categorical columns numerical_cols = X.select_dtypes(include=[\'int64\', \'float64\']).columns categorical_cols = X.select_dtypes(include=[\'object\']).columns # Preprocessors for numerical and categorical data numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'encoder\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Column transformer to apply appropriate transformations to each column type preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Final pipeline to preprocess, select top k features, and classify pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'feature_selection\', SelectKBest(k=k)), (\'classifier\', RandomForestClassifier(random_state=42)) ]) # Fit the pipeline on the data pipeline.fit(X, y) return pipeline"},{"question":"Advanced GroupBy Operations Objective: You are given a dataset of daily sales data for multiple stores, and your task is to analyze the data to identify trends and patterns using pandas GroupBy functionalities. Your solution should demonstrate proficiency with grouping, aggregation, transformation, and other advanced operations provided by the pandas GroupBy API. Dataset Description: The dataset (`sales_data.csv`) contains the following columns: - `date` (string, format: YYYY-MM-DD): The date of the sales record. - `store_id` (integer): The unique identifier for the store. - `product_id` (integer): The unique identifier for the product. - `units_sold` (integer): The number of units sold on the given date. Requirements: 1. **Load the Data**: - Read the dataset from `sales_data.csv` into a pandas DataFrame. 2. **Group and Aggregate Data**: - Group the data by `store_id` and `date`. - Calculate the total units sold per store per day. - Calculate the cumulative units sold per store, ordered by date. 3. **Identify Trends**: - For each store, calculate the moving average of daily units sold over a window of 7 days. 4. **Custom Aggregation**: - Define a custom aggregation function that calculates the interquartile range (IQR) of the daily units sold for each product across all stores. 5. **Filter and Transform**: - Identify and output the `store_id` and `date` for transactions where the number of units sold is significantly higher than the quarterly median (e.g., top 5%). Input: - `sales_data.csv`: A CSV file with columns `date`, `store_id`, `product_id`, and `units_sold`. Output: - The result of each requirement should be stored in a variable and printed. Constraints: - The solution should handle large datasets efficiently. - The moving average calculation should handle missing days gracefully, treating missing data as zero sales. Performance Requirements: - The solution should complete in a reasonable time frame for datasets up to 1 million rows. Function Implementation: Implement your solution in a single function named `analyze_sales_data()` as shown below: ```python import pandas as pd def analyze_sales_data(csv_file): # 1. Load the Data df = pd.read_csv(csv_file) # 2. Group and Aggregate Data grouped_df = df.groupby([\'store_id\', \'date\']).agg({\'units_sold\': \'sum\'}).reset_index() grouped_df[\'cumulative_units_sold\'] = grouped_df.groupby(\'store_id\')[\'units_sold\'].cumsum() # 3. Identify Trends grouped_df[\'date\'] = pd.to_datetime(grouped_df[\'date\']) grouped_df = grouped_df.set_index(\'date\') grouped_df[\'7_day_moving_avg\'] = grouped_df.groupby(\'store_id\')[\'units_sold\'].rolling(window=7, min_periods=1).mean().reset_index(level=0, drop=True) # 4. Custom Aggregation def iqr(x): return x.quantile(0.75) - x.quantile(0.25) iqr_per_product = df.groupby(\'product_id\')[\'units_sold\'].agg(iqr).reset_index() # 5. Filter and Transform quarterly_median = df.groupby(\'store_id\')[\'units_sold\'].quantile(0.95).reset_index().rename(columns={\'units_sold\': \'quarterly_median\'}) df = df.merge(quarterly_median, on=\'store_id\') top_transactions = df[df[\'units_sold\'] > df[\'quarterly_median\']][[\'store_id\', \'date\']] # Print the results print(\\"Grouped and Aggregated Data:n\\", grouped_df) print(\\"nInterquartile Range per Product:n\\", iqr_per_product) print(\\"nTop Transactions:n\\", top_transactions) # Test Function # analyze_sales_data(\'sales_data.csv\') ``` Note: Uncomment the test function call and ensure `sales_data.csv` is in the working directory to test your solution.","solution":"import pandas as pd def analyze_sales_data(csv_file): # 1. Load the Data df = pd.read_csv(csv_file) # 2. Group and Aggregate Data grouped_df = df.groupby([\'store_id\', \'date\']).agg({\'units_sold\': \'sum\'}).reset_index() grouped_df[\'cumulative_units_sold\'] = grouped_df.groupby(\'store_id\')[\'units_sold\'].cumsum() # 3. Identify Trends grouped_df[\'date\'] = pd.to_datetime(grouped_df[\'date\']) grouped_df = grouped_df.set_index(\'date\') grouped_df[\'7_day_moving_avg\'] = grouped_df.groupby(\'store_id\')[\'units_sold\'].rolling(window=7, min_periods=1).mean().reset_index(level=0, drop=True) # 4. Custom Aggregation def iqr(x): return x.quantile(0.75) - x.quantile(0.25) iqr_per_product = df.groupby(\'product_id\')[\'units_sold\'].agg(iqr).reset_index() # 5. Filter and Transform df[\'date\'] = pd.to_datetime(df[\'date\']) # Considering a quarter is 3 months period, calculating the 95th percentile for a quarterly period. df[\'quarter\'] = df[\'date\'].dt.to_period(\'Q\') quarterly_median = df.groupby([\'store_id\', \'quarter\'])[\'units_sold\'].quantile(0.95).reset_index().rename(columns={\'units_sold\': \'quarterly_median\'}) df = df.merge(quarterly_median, on=[\'store_id\', \'quarter\']) top_transactions = df[df[\'units_sold\'] > df[\'quarterly_median\']][[\'store_id\', \'date\']] # Returning the results for testing return { \\"grouped_df\\": grouped_df, \\"iqr_per_product\\": iqr_per_product, \\"top_transactions\\": top_transactions } # Uncomment below to run the function # analyze_sales_data(\'sales_data.csv\')"},{"question":"You are tasked with building a text processing tool that converts an input string into its various ASCII textual representations. This tool will apply several transformations on the string, filtering out non-ASCII characters and processing the remaining characters based on specified rules. Objective: Write a Python function to process a given input string according to the following steps: 1. **Filter out non-ASCII characters from the string.** 2. **Generate a list of tuples** for each remaining character in the format: ``` (original_character, is_control, ascii_value, control_character, alt_character, unctrl_representation) ``` Where: - `original_character` is the character as it appears in the input string. - `is_control` is a boolean indicating if the character is a control character. - `ascii_value` is the ASCII value of the character. - `control_character` is the control character representation (use `ctrl()` function). - `alt_character` is the 8-bit alt representation of the character (use `alt()` function). - `unctrl_representation` is the unctrl representation (use `unctrl()` function). Input: - A single string `input_string` containing any possible characters. Output: - A list of tuples representing the transformations of each valid ASCII character in the input string. Example: ```python input_string = \\"HellonWorld!\\" # Calling your function should produce: # [ # (\'H\', False, 72, \'x08\', \'xc8\', \'H\'), # (\'e\', False, 101, \'x05\', \'xe5\', \'e\'), # (\'l\', False, 108, \'x0c\', \'xec\', \'l\'), # (\'l\', False, 108, \'x0c\', \'xec\', \'l\'), # (\'o\', False, 111, \'x0f\', \'xef\', \'o\'), # (\'n\', True, 10, \'n\', \'x8a\', \'^J\'), # (\'W\', False, 87, \'x17\', \'xd7\', \'W\'), # (\'o\', False, 111, \'x0f\', \'xef\', \'o\'), # (\'r\', False, 114, \'x12\', \'xf2\', \'r\'), # (\'l\', False, 108, \'x0c\', \'xec\', \'l\'), # (\'d\', False, 100, \'x04\', \'xe4\', \'d\'), # (\'!\', False, 33, \'x01\', \'xa1\', \'!\') # ] def process_string(input_string: str) -> list: # Your implementation here pass # Be ready to provide other test cases to validate your function. ``` Constraints: - Ensure the function handles both printable and non-printable ASCII characters correctly. - The input string can be of any length, but performance within a reasonable range can be assumed (up to 1 million characters).","solution":"def process_string(input_string: str) -> list: def is_control(c): Returns True if c is a control character. return ord(c) < 32 or ord(c) == 127 def ctrl(c): Returns control character representation of c. return chr(ord(c) & 0x1F) def alt(c): Returns 8-bit alt representation of c. return chr(ord(c) | 0x80) def unctrl(c): Returns the unctrl representation of c. if is_control(c): return f\\"^{chr(ord(c) + 64)}\\" if ord(c) != 127 else \'^?\' return c result = [] for char in input_string: if ord(char) < 128: # Filter non-ASCII characters ascii_value = ord(char) result.append(( char, is_control(char), ascii_value, ctrl(char), alt(char), unctrl(char) )) return result"},{"question":"# Encoding and Decoding with Custom Error Handling Objective Your task is to design a class that handles encoding and decoding strings with a given codec. Additionally, you should implement custom error handling for encoding errors. Requirements 1. The class should allow registering a new codec search function at initialization and unregistering it upon deletion. 2. Provide methods to encode and decode strings using specified codecs. 3. Implement custom error handling that replaces errors with a specified character sequence. Class Definition ```python class CodecHandler: def __init__(self, search_function, error_handler_name): Initialize the CodecHandler with a search function and error handler name. Args: - search_function (callable): Function to search for codecs. - error_handler_name (str): Custom error handler name to register. pass def encode(self, text, encoding): Encode the given text using the specified encoding. Args: - text (str): The text to encode. - encoding (str): The encoding to use. Returns: - bytes: The encoded text. pass def decode(self, byte_data, encoding): Decode the given byte data using the specified encoding. Args: - byte_data (bytes): The byte data to decode. - encoding (str): The encoding to use. Returns: - str: The decoded text. pass def __del__(self): Unregister the search function upon deletion of the object. pass ``` Additional Constraints 1. The `search_function` in the `CodecHandler` should be callable and return a codec for given encoding names. 2. The custom error handler should be registered using the provided `error_handler_name`. 3. If encoding or decoding fails, appropriately handle and raise the errors. Example Usage ```python def example_search_function(encoding_name): # example implementation of a codec search function pass custom_error_handler_name = \\"custom_error_handler\\" # Initialize the handler handler = CodecHandler(example_search_function, custom_error_handler_name) # Encode and decode example encoded_text = handler.encode(\\"Hello, world!\\", \\"utf-8\\") print(encoded_text) # Expected: b\'Hello, world!\' decoded_text = handler.decode(encoded_text, \\"utf-8\\") print(decoded_text) # Expected: \\"Hello, world!\\" # Clean up by deleting the handler del handler ``` Notes 1. Use the functions from the provided documentation to implement the codec registration, encoding, decoding, and error handling. 2. Ensure that all necessary clean-up operations (such as unregistering the codec search function) occur when the object is deleted.","solution":"import codecs class CodecHandler: def __init__(self, search_function, error_handler_name): Initialize the CodecHandler with a search function and error handler name. Args: - search_function (callable): Function to search for codecs. - error_handler_name (str): Custom error handler name to register. self.search_function = search_function self.error_handler_name = error_handler_name codecs.register(self.search_function) codecs.register_error(self.error_handler_name, self._custom_error_handler) def encode(self, text, encoding): Encode the given text using the specified encoding. Args: - text (str): The text to encode. - encoding (str): The encoding to use. Returns: - bytes: The encoded text. return text.encode(encoding, self.error_handler_name) def decode(self, byte_data, encoding): Decode the given byte data using the specified encoding. Args: - byte_data (bytes): The byte data to decode. - encoding (str): The encoding to use. Returns: - str: The decoded text. return byte_data.decode(encoding, self.error_handler_name) def _custom_error_handler(self, error): Custom error handler that replaces the unintended character with a placeholder. Args: - error (UnicodeEncodeError): The error raised during encoding. Returns: - tuple: A tuple (replacement, position). return (\'?\', error.start + 1) def __del__(self): Unregister the search function and the custom error handler upon deletion of the object. codecs.unregister(self.search_function) # There\'s no way to unregister an error handler, so it stays registered"},{"question":"# Advanced Python Coding Assessment **Objective:** To assess the understanding of dynamic type creation and utility functions in the `types` module. **Problem Statement:** You are required to dynamically create a new class and make use of several utilities provided by the `types` module to explore and manipulate instances of the class. # Part 1: Dynamic Class Creation Create a new class `DynamicClass` dynamically using the `types.new_class` method. The class should have: - A class attribute `class_attr` set to the value `100`. - An instance attribute `instance_attr` initialized with the value passed during instantiation. **Requirements:** 1. Use `types.new_class` to create the `DynamicClass`. 2. The class should contain a method `describe` that returns a string `\\"Class attribute is: {class_attr}, and instance attribute is: {instance_attr}\\"`. # Part 2: Utility Functions and Class Verification 1. Create an instance of `DynamicClass` with `instance_attr` set to `50`. 2. Verify if the created instance is of type `types.FunctionType`. 3. Check the type of one of the method (choose `describe`) from `DynamicClass`. **Input and Output:** ```python # Input # None (All steps are programmatically executed within the script) # Output # The following values should be printed: # - The description string returned by `describe` method of the instance. # - The result (boolean) of checking whether the instance is of type `types.FunctionType`. # - The type of the `describe` method. ``` Constraints: - Ensure that the `describe` method is correctly included in the dynamically created class. - Use the defined utility functions from the `types` module. # Boilerplate Code: ```python import types # Part 1: Dynamic Class Creation def exec_body(ns): ns[\'class_attr\'] = 100 def describe(self): return f\\"Class attribute is: {self.class_attr}, and instance attribute is: {self.instance_attr}\\" ns[\'describe\'] = describe # Creating DynamicClass DynamicClass = types.new_class(\'DynamicClass\', (), {}, exec_body) def __init__(self, instance_attr): self.instance_attr = instance_attr setattr(DynamicClass, \'__init__\', __init__) # Part 2: Instance Creation and Verification dynamic_instance = DynamicClass(50) # Checking if the instance is of type FunctionType is_function_type = isinstance(dynamic_instance, types.FunctionType) # Getting the type of `describe` method describe_method_type = type(dynamic_instance.describe) # Output the results print(dynamic_instance.describe()) print(is_function_type) print(describe_method_type) ``` **Validation:** Write necessary assertions to validate if the solution meets the requirements mentioned above, such as: - Ensuring `describe` method works as expected. - Proper type checks using `types`. # Notes: - Pay careful attention to the namespaces when using `types.new_class`. - Utilize the `types` module functions and classes as specified.","solution":"import types # Part 1: Dynamic Class Creation def exec_body(ns): ns[\'class_attr\'] = 100 def describe(self): return f\\"Class attribute is: {self.class_attr}, and instance attribute is: {self.instance_attr}\\" ns[\'describe\'] = describe # Creating DynamicClass DynamicClass = types.new_class(\'DynamicClass\', (), {}, exec_body) def __init__(self, instance_attr): self.instance_attr = instance_attr setattr(DynamicClass, \'__init__\', __init__) # Part 2: Instance Creation and Verification dynamic_instance = DynamicClass(50) # Checking if the instance is of type FunctionType is_function_type = isinstance(dynamic_instance, types.FunctionType) # Getting the type of `describe` method describe_method_type = type(dynamic_instance.describe) # Gathering output results description_output = dynamic_instance.describe() check_is_function_type = is_function_type check_describe_method_type = describe_method_type"},{"question":"# Advanced Python Standard Library Utilization **Problem Statement:** You are tasked with implementing a report generator system that processes sales data from a text file, formats it, performs some calculations, and outputs a neatly formatted summary. This assessment will test your ability to use various Python modules discussed in the provided documentation. **Task:** 1. **Reading and Parsing Data:** - Implement a function `read_sales_data(filename: str) -> list[dict]` that reads a text file containing sales data in the following format: ``` Product A, 12.50, 4 Product B, 15.00, 2 Product A, 12.50, 3 Product C, 8.75, 5 ``` Each line represents a sale entry with the product name, price, and quantity sold. The function should return a list of dictionaries where each dictionary represents a sale entry with keys: \'product\', \'price\', and \'quantity\'. 2. **Aggregating Data:** - Implement a function `aggregate_sales(sales_data: list[dict]) -> dict` that takes the list of sales data and returns a dictionary where keys are product names, and values are another dictionary with total quantity and total revenue (price * quantity). 3. **Generating Report:** - Implement a function `generate_report(aggregated_data: dict, output_filename: str)` that writes a neatly formatted report to a text file. Use the `pprint` module to format the report to be easy to read. Ensure the output is aligned properly, showing each product, total quantity sold, and total revenue. - Include a summary section at the end, which contains: - Total revenue from all products combined. - The product with the highest revenue. - The report should look something like this: ``` ---------------------------------------- Sales Report ---------------------------------------- Product: Product A Total Quantity Sold: 7 Total Revenue: 87.50 Product: Product B Total Quantity Sold: 2 Total Revenue: 30.00 Product: Product C Total Quantity Sold: 5 Total Revenue: 43.75 ---------------------------------------- Summary ---------------------------------------- Total Revenue from All Products: 161.25 Product with Highest Revenue: Product A (87.50) ``` 4. **Handling Localization:** - Ensure that the revenue values in the report are formatted according to the locale \'English_United States.1252\' for proper currency representation. **Function Signatures:** ```python def read_sales_data(filename: str) -> list[dict]: pass def aggregate_sales(sales_data: list[dict]) -> dict: pass def generate_report(aggregated_data: dict, output_filename: str): pass ``` **Constraints:** - You may assume that the input file is correctly formatted and does not contain any errors. - Ensure that your solution is efficient and can handle a reasonably large amount of data. **Example Usage:** ```python sales_data = read_sales_data(\'sales.txt\') aggregated_data = aggregate_sales(sales_data) generate_report(aggregated_data, \'sales_report.txt\') ``` **Note:** Make sure to incorporate and appropriately utilize the following Python modules: `pprint`, `locale`, and any others necessary to complete the task effectively.","solution":"import pprint import locale from pathlib import Path locale.setlocale(locale.LC_ALL, \'en_US.UTF-8\') def read_sales_data(filename: str) -> list: sales_data = [] with open(filename, \'r\') as file: for line in file: product, price, quantity = line.strip().split(\', \') sales_data.append({ \'product\': product, \'price\': float(price), \'quantity\': int(quantity) }) return sales_data def aggregate_sales(sales_data: list) -> dict: aggregated_data = {} for sale in sales_data: product = sale[\'product\'] if product not in aggregated_data: aggregated_data[product] = {\'total_quantity\': 0, \'total_revenue\': 0} aggregated_data[product][\'total_quantity\'] += sale[\'quantity\'] aggregated_data[product][\'total_revenue\'] += sale[\'price\'] * sale[\'quantity\'] return aggregated_data def generate_report(aggregated_data: dict, output_filename: str): with open(output_filename, \'w\') as file: file.write(\\"----------------------------------------n\\") file.write(\\"Sales Reportn\\") file.write(\\"----------------------------------------n\\") total_revenue = 0 highest_revenue_product = None highest_revenue_amount = 0 for product, data in aggregated_data.items(): total_revenue += data[\'total_revenue\'] if data[\'total_revenue\'] > highest_revenue_amount: highest_revenue_product = product highest_revenue_amount = data[\'total_revenue\'] file.write(f\\"Product: {product}n\\") file.write(f\\"Total Quantity Sold: {data[\'total_quantity\']}n\\") file.write(f\\"Total Revenue: {locale.currency(data[\'total_revenue\'], grouping=True)}nn\\") file.write(\\"----------------------------------------n\\") file.write(\\"Summaryn\\") file.write(\\"----------------------------------------n\\") file.write(f\\"Total Revenue from All Products: {locale.currency(total_revenue, grouping=True)}n\\") file.write(f\\"Product with Highest Revenue: {highest_revenue_product} ({locale.currency(highest_revenue_amount, grouping=True)})n\\")"},{"question":"Coding Assessment Question # Objective Your task is to implement a function that demonstrates understanding of PyTorch\'s storage mechanics, specifically focusing on `torch.UntypedStorage`. # Problem Statement Create a function `verify_shared_storage_operations` that takes no arguments and performs the following: 1. Create a tensor `t1` of shape (4, 3) filled with ones of dtype `torch.float32`. 2. Create a view of `t1` named `t2` that reshapes it to (2, 6). 3. Modify the original tensor `t1` by setting all its elements to zero. 4. Verify that the view `t2` reflects the changes in `t1` (since they share the same storage), and return `True` if they do and `False` otherwise. 5. Clone the storage of `t1` to create `s1`. 6. Use `s1` to create a new tensor `t3` that has the same shape, dtype, and content as `t1`. 7. Modify `t3` such that all its elements are set to one. 8. Verify that `t3` and `t1` do not affect each other and return `True` if they do not and `False` otherwise. # Function Signature ```python def verify_shared_storage_operations() -> bool: pass ``` # Example ```python result = verify_shared_storage_operations() print(result) # Expected output: (True, True) ``` # Constraints - You are not allowed to directly modify the storage of tensors unless through officially supported methods (e.g., `clone`, `fill_`, `new_tensor`). - Ensure that all tensor operations maintain the dtype `torch.float32`. # Requirements - Thorough understanding of tensor views and storage sharing in PyTorch. - Proper use of PyTorch tensor manipulation methods and storage operations. - Correct return values verifying storage shared reflections and independence post-cloning.","solution":"import torch def verify_shared_storage_operations() -> (bool, bool): # Step 1: Create tensor t1 of shape (4, 3) filled with ones t1 = torch.ones((4, 3), dtype=torch.float32) # Step 2: Create a view t2 of t1 reshaped to (2, 6) t2 = t1.view(2, 6) # Step 3: Modify t1 by setting all elements to zero t1.fill_(0) # Step 4: Verify t2 reflects changes in t1 shared_storage_correct = torch.equal(t2, torch.zeros((2, 6), dtype=torch.float32)) # Step 5: Clone the storage of t1 to create s1 s1 = t1.storage().clone() # Step 6: Use s1 to create a new tensor t3 with the same shape, dtype, and content as t1 t3 = torch.FloatTensor(s1).view(4, 3) # Step 7: Modify t3 by setting all elements to one t3.fill_(1) # Step 8: Verify t3 and t1 do not affect each other independent_storages_correct = not torch.equal(t1, t3) return shared_storage_correct, independent_storages_correct"},{"question":"<|Analysis Begin|> The provided documentation describes the \\"netrc\\" module of Python, which is used for parsing and encapsulating netrc files. Netrc files are used by Unix FTP programs and other FTP clients to store login and initialization information. Key points from the documentation: - The \\"netrc\\" class can be instantiated with an optional file argument. If no argument is given, it defaults to \\".netrc\\" in the user\'s home directory. - If the file has incorrect permissions or syntax, it raises \\"NetrcParseError\\". - The class has methods to get authenticators for a host and to represent the data as a string. - The class exposes `hosts` and `macros` as public instance variables that are dictionaries. - Passwords are restricted to a subset of ASCII characters. Main capabilities: - Loading and parsing netrc files. - Retrieving authentication data for a given host. - Handling errors related to file permissions and syntax. <|Analysis End|> <|Question Begin|> # Coding Assessment Question **Objective:** Demonstrate your understanding of file processing, error handling, and dictionary manipulation in Python. **Problem Statement** You are required to use the `netrc` module to create a function that processes a netrc file, validates its content, and extracts specific information. **Function to Implement:** ```python def process_netrc(file_path: str, host_name: str) -> dict: Process the netrc file, validate its content and return authentication details for a given host. Args: - file_path (str): The path to the netrc file. If the file does not exist or contains invalid syntax, raise an appropriate error. - host_name (str): The name of the host for which to retrieve authentication details. Returns: - dict: A dictionary with keys \'login\', \'account\', \'password\'. If the specified host is not found, return an empty dictionary. Raises: - FileNotFoundError: If the file does not exist. - netrc.NetrcParseError: If the file contains invalid syntax or insecure permissions. ``` **Input/Output Examples:** ```python # Example 1: # Assume file at \'path/to/netrc\' contains valid netrc data with an entry for \'example.com\' file_path = \'path/to/netrc\' host_name = \'example.com\' output = process_netrc(file_path, host_name) # Expected output: {\'login\': \'your_login\', \'account\': \'your_account\', \'password\': \'your_password\'} # Example 2: # Assume file at \'path/to/netrc\' does not contain an entry for \'unknown.com\' file_path = \'path/to/netrc\' host_name = \'unknown.com\' output = process_netrc(file_path, host_name) # Expected output: {} ``` **Constraints:** 1. The function should use the capabilities provided by the `netrc` module only. 2. Handle errors gracefully, and ensure the file is read in a secure manner. 3. The file path provided should be a path that the function has access to read. **Notes:** 1. Ensure correct and secure file handling practices. 2. Test your function thoroughly with different netrc file samples and host names.","solution":"import os import netrc def process_netrc(file_path: str, host_name: str) -> dict: Process the netrc file, validate its content and return authentication details for a given host. Args: - file_path (str): The path to the netrc file. If the file does not exist or contains invalid syntax, raise an appropriate error. - host_name (str): The name of the host for which to retrieve authentication details. Returns: - dict: A dictionary with keys \'login\', \'account\', \'password\'. If the specified host is not found, return an empty dictionary. Raises: - FileNotFoundError: If the file does not exist. - netrc.NetrcParseError: If the file contains invalid syntax or insecure permissions. if not os.path.exists(file_path): raise FileNotFoundError(f\\"Netrc file {file_path} does not exist.\\") try: netrc_obj = netrc.netrc(file_path) except netrc.NetrcParseError as e: raise netrc.NetrcParseError(f\\"Error parsing netrc file: {e}\\") if host_name not in netrc_obj.hosts: return {} login, account, password = netrc_obj.authenticators(host_name) return {\'login\': login, \'account\': account, \'password\': password}"},{"question":"Custom Chat Server with `asynchat` Objective In this assignment, you are required to implement a simple custom chat server using the `asynchat` module. Your server should be capable of handling client connections asynchronously, managing incoming messages, and broadcasting messages to all connected clients. Requirements 1. **Class Implementation**: Implement a subclass of `asynchat.async_chat` named `ChatHandler` that handles the following: - Collects incoming data from clients. - Identifies the end of each message using a unique terminator. - Broadcasts messages to all clients connected to the server. 2. **Server Setup**: Implement a class named `ChatServer` that: - Listens for incoming connections. - Creates new instances of `ChatHandler` for each connection. Detailed Specifications **`ChatHandler` Class:** - **Attributes**: - `clients`: A class-wide list that keeps track of all connected client handlers. - `self.buffer`: A list to collect incoming data segments. - **Methods to Implement**: - `__init__(self, sock)`: Initialize the client connection and add the instance to the `clients` list. - `collect_incoming_data(self, data)`: Append incoming data to `self.buffer`. - `found_terminator(self)`: Concatenate buffer segments, broadcast the complete message, and clear the buffer. - `broadcast(self, message)`: Send the message to all connected clients except the sender. **`ChatServer` Class:** - **Attributes**: - `port`: The port on which the server listens for incoming connections. - `host`: The hostname or IP address to bind the server. - **Methods to Implement**: - `__init__(self, host, port)`: Initialize the server and start listening for connections using `asyncore.dispatcher`. - `handle_accept(self)`: Accept incoming connections and create new `ChatHandler` instances. - `close_server(self)`: Cleanly close the server and all client connections. Example Usage Here is an example usage demonstrating how to start the server and connect client handlers: ```python if __name__ == \\"__main__\\": import asyncore # Initialize and start the chat server server = ChatServer(host=\'localhost\', port=12345) # Run the asyncore loop to handle events try: asyncore.loop() except KeyboardInterrupt: server.close_server() ``` Input/Output Format * **Input**: Clients will send messages ending with the newline character (`n`). * **Output**: The server will broadcast each received message to all connected clients, appending the terminator. Constraints - Ensure to manage your buffers and client disconnections gracefully. - Avoid blocking operations to keep the server responsive. Performance Requirements - Your solution should handle multiple client connections efficiently. - Properly manage the cleanup of resources when clients disconnect or the server closes. Failure to meet these requirements or correctly implement the classes will affect the correctness and performance of your solution. Good luck!","solution":"import asynchat import asyncore import socket class ChatHandler(asynchat.async_chat): # Class-wide list to keep track of all client handlers clients = [] def __init__(self, sock): asynchat.async_chat.__init__(self, sock=sock) self.set_terminator(b\'n\') self.buffer = [] ChatHandler.clients.append(self) def collect_incoming_data(self, data): self.buffer.append(data) def found_terminator(self): message = b\'\'.join(self.buffer).decode(\'utf-8\') self.buffer = [] self.broadcast(message) def broadcast(self, message): for client in ChatHandler.clients: if client is not self: client.push((message + \'n\').encode(\'utf-8\')) class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accept(self): socket, addr = self.accept() ChatHandler(socket) def close_server(self): for client in ChatHandler.clients: client.close() self.close()"},{"question":"# Question: Implement an Asynchronous Reader-Writer Problem The reader-writer problem is a classic synchronization problem where we need to manage read and write access to a shared resource. There are multiple readers and multiple writers, and the constraints are: - Any number of readers can be reading the resource simultaneously. - Only one writer can be writing to the resource. - When a writer is writing, no reader can read the resource. Implement an asynchronous solution using the `asyncio` synchronization primitives provided in Python 3.10. Specifically, you are required to: - Implement the classes `Reader` and `Writer`. - Implement a `SharedResource` class that uses `asyncio.Lock`, `asyncio.Event`, or `asyncio.Condition` to manage access. # Classes to Implement 1. Reader ```python class Reader: async def read(self, shared_resource): This method should simulate reading from the shared resource. Parameters: - shared_resource (SharedResource): the shared resource object. While reading, print \\"Reader reading...\\" and wait for 1 second (simulate read time using `await asyncio.sleep(1)`). pass ``` 2. Writer ```python class Writer: async def write(self, shared_resource): This method should simulate writing to the shared resource. Parameters: - shared_resource (SharedResource): the shared resource object. While writing, print \\"Writer writing...\\" and wait for 2 seconds (simulate write time using `await asyncio.sleep(2)`). pass ``` 3. SharedResource ```python class SharedResource: def __init__(self): Initialize the shared resource and necessary synchronization primitives. pass async def reader_enter(self): This method should handle the logic for a reader entering (starting to read). pass async def reader_exit(self): This method should handle the logic for a reader exiting (finished reading). pass async def writer_enter(self): This method should handle the logic for a writer entering (starting to write). pass async def writer_exit(self): This method should handle the logic for a writer exiting (finished writing). pass ``` # Example Usage: ```python import asyncio async def main(): shared_resource = SharedResource() readers = [Reader() for _ in range(5)] writers = [Writer() for _ in range(2)] tasks = [] for reader in readers: tasks.append(reader.read(shared_resource)) for writer in writers: tasks.append(writer.write(shared_resource)) await asyncio.gather(*tasks) asyncio.run(main()) ``` # Requirements: - Ensure mutual exclusion for writers. - Ensure that readers can access the resource concurrently. - Demonstrate the working of multiple readers and writers as shown in the example usage. # Constraints: - Use `asyncio.Lock`, `asyncio.Event`, or `asyncio.Condition` for synchronization. - Handle access such that reader and writer constraints are respected.","solution":"import asyncio class Reader: async def read(self, shared_resource): This method should simulate reading from the shared resource. Parameters: - shared_resource (SharedResource): the shared resource object. While reading, print \\"Reader reading...\\" and wait for 1 second (simulate read time using `await asyncio.sleep(1)`). await shared_resource.reader_enter() print(\\"Reader reading...\\") await asyncio.sleep(1) await shared_resource.reader_exit() class Writer: async def write(self, shared_resource): This method should simulate writing to the shared resource. Parameters: - shared_resource (SharedResource): the shared resource object. While writing, print \\"Writer writing...\\" and wait for 2 seconds (simulate write time using `await asyncio.sleep(2)`). await shared_resource.writer_enter() print(\\"Writer writing...\\") await asyncio.sleep(2) await shared_resource.writer_exit() class SharedResource: def __init__(self): Initialize the shared resource and necessary synchronization primitives. self.readers = 0 self.reader_lock = asyncio.Lock() self.resource_lock = asyncio.Lock() async def reader_enter(self): async with self.reader_lock: self.readers += 1 if self.readers == 1: await self.resource_lock.acquire() async def reader_exit(self): async with self.reader_lock: self.readers -= 1 if self.readers == 0: self.resource_lock.release() async def writer_enter(self): await self.resource_lock.acquire() async def writer_exit(self): self.resource_lock.release()"},{"question":"# Seaborn Plotting Challenge **Objective:** Write a function that generates a customized plot demonstrating proficiency in using Seaborn\'s `objects` module. **Function Signature:** ```python def generate_custom_plot(data): Generates a customized Seaborn plot. Parameters: data (DataFrame): A pandas DataFrame containing the data to plot. The DataFrame is expected to have at least three columns: \'category\', \'value1\', and \'value2\'. Returns: None: The function should display the plot. ``` # Requirements: 1. **Load Data**: You will start with a DataFrame `data` which has columns: \'category\', \'value1\', and \'value2\'. 2. **Plot Creation**: Use `seaborn.objects` to create a plot detailing: - Categories on the x-axis. - Value2 on the y-axis, scaled logarithmically. 3. **Layers**: - Add a dot plot layer showing `value1`. - Add jitter to the dots for better separation. - Compute and add quartiles for the `value2` using `so.Perc`. - Show a range interval for the 25th to 75th percentiles. 4. **Customizations**: - Dots should be semi-transparent with a specific size. - The range interval should be displayed with a noticeable shift and a specific color. 5. **Display the plot**. # Input: - `data` (DataFrame): A pandas DataFrame with the following structure: ```plaintext category value1 value2 0 A 100 2000 1 A 150 3000 2 B 200 2500 3 B 130 3400 ... ``` Assume the DataFrame can have any categorical values for \'category\', and \'value1\', \'value2\' as numeric values. # Output: - Display the generated customized plot. # Constraints: - Ensure your function handles large datasets efficiently (up to 50,000 rows). - The plot should be clear and properly annotated for visualization. # Example: ```python import pandas as pd # Sample DataFrame data = pd.DataFrame({ \'category\': [\'A\', \'A\', \'B\', \'B\', \'C\', \'C\'], \'value1\': [100, 150, 200, 130, 180, 220], \'value2\': [2000, 3000, 2500, 3400, 2600, 3700] }) # Call the function (Implementation to be defined by the student) generate_custom_plot(data) ``` *Note*: The function should not return anything; it should directly display the plot. # Additional Information: - Make use of Seaborn version 0.11.2+. - Utilize appropriate plot aesthetics to make the plot informative and visually appealing.","solution":"import pandas as pd import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def generate_custom_plot(data): Generates a customized Seaborn plot. Parameters: data (DataFrame): A pandas DataFrame containing the data to plot. The DataFrame is expected to have at least three columns: \'category\', \'value1\', and \'value2\'. Returns: None: The function should display the plot. try: # Base plot object p = so.Plot(data, x=\'category\', y=\'value2\', color=\'category\') # Add a dot plot layer with jitter to separate the dots p = p.add(so.Dot(jitter=0.3), so.Perc(0.25, 0.75)) # Customize dot properties p = p.scale(y=\'log\').facet(col=\'category\') .theme(so.theme_classic) .label(x=\\"Category\\", y=\\"Log of Value2\\") .add(so.Range(), so.Perc(25, 75), shift=0.15, color=\'orange\', alpha=0.7) .add(so.Dot(), so.Agg(), alpha=0.5, size=5) # Display the plot p.show() except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are given a specific log file that you need to process. The log file contains entries that have the following format: ```plaintext [ERROR] 2023-03-12 15:30:01 - Error occurred in module XYZ [INFO] 2023-03-12 15:30:05 - Module ABC initialized successfully [WARNING] 2023-03-12 15:30:10 - Disk space running low [ERROR] 2023-03-12 15:30:20 - Failed to load configuration ``` Your task is to write a function using the `linecache` module to retrieve all the log entries of a specific type (e.g., ERROR, INFO, or WARNING). The function signature should be as follows: ```python def fetch_log_entries(file_path: str, log_type: str) -> list: Retrieve all log entries of a specified type from the given log file. :param file_path: The path to the log file. :param log_type: The type of log entries to retrieve (e.g., \\"ERROR\\", \\"INFO\\", \\"WARNING\\"). :return: A list of log entries of the specified type. ``` Input and Output Formats - **Input**: - `file_path` (str): A string representing the path to the log file. - `log_type` (str): A string representing the type of log entries to fetch (case-sensitive). - **Output**: - `list`: A list of strings, each representing a log entry of the specified type. Constraints 1. The `file_path` will always point to a valid log file. 2. The `log_type` string will always be one of the predefined types (\\"ERROR\\", \\"INFO\\", \\"WARNING\\"). 3. The log file may be substantially large, so the use of caching is encouraged for efficiency. Example ```python log_entries = fetch_log_entries(\'/path/to/logfile.log\', \'ERROR\') for entry in log_entries: print(entry) # Expected output (example): # [ERROR] 2023-03-12 15:30:01 - Error occurred in module XYZ # [ERROR] 2023-03-12 15:30:20 - Failed to load configuration ``` Notes - You should utilize the `linecache` module for efficient line retrieval and caching. - Ensure your function handles potential issues such as empty lines or lines that do not match the expected log format gracefully.","solution":"import linecache def fetch_log_entries(file_path: str, log_type: str) -> list: Retrieve all log entries of a specified type from the given log file. :param file_path: The path to the log file. :param log_type: The type of log entries to retrieve (e.g., \\"ERROR\\", \\"INFO\\", \\"WARNING\\"). :return: A list of log entries of the specified type. entries = [] with open(file_path, \'r\') as file: for i, _ in enumerate(file, 1): # Read the file line by line to get the line numbers line = linecache.getline(file_path, i).strip() if line.startswith(f\'[{log_type}]\'): entries.append(line) linecache.clearcache() return entries"},{"question":"Coding Assessment Question # Objective Demonstrate your understanding of the Python\'s `datetime` module by implementing functions dealing with `datetime` objects. # Task You are required to implement the following two functions: 1. `create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime.datetime` - This function should return a `datetime.datetime` object with the specified year, month, day, hour, minute, second, and microsecond. 2. `extract_date_parts(dt: datetime.datetime) -> Tuple[int, int, int, int, int, int, int]` - This function should extract and return the year, month, day, hour, minute, second, and microsecond components from the given `datetime.datetime` object as a tuple. # Input and Output Format - The `create_datetime` function takes seven parameters: - `year`: An integer representing the year (e.g., 2023). - `month`: An integer representing the month (1 to 12). - `day`: An integer representing the day (1 to 31). - `hour`: An integer representing the hour (0 to 23). - `minute`: An integer representing the minute (0 to 59). - `second`: An integer representing the second (0 to 59). - `microsecond`: An integer representing the microsecond (0 to 999999). - The `create_datetime` function returns a `datetime.datetime` object. - The `extract_date_parts` function takes one parameter: - `dt`: A `datetime.datetime` object. - The `extract_date_parts` function returns a tuple containing the year, month, day, hour, minute, second, and microsecond (in that order). # Constraints - Both functions should handle incorrect inputs gracefully (e.g., invalid date). - Use Python\'s `datetime` module to implement the functionalities. - Do not use any external libraries. # Example ```python from datetime import datetime # Example usage of create_datetime function dt = create_datetime(2023, 10, 14, 15, 30, 45, 123456) print(dt) # Output: 2023-10-14 15:30:45.123456 # Example usage of extract_date_parts function parts = extract_date_parts(dt) print(parts) # Output: (2023, 10, 14, 15, 30, 45, 123456) ``` # Note - Ensure the code follows proper naming conventions and is well-structured. - Exception handling for invalid date and time values is required to ensure robustness.","solution":"from datetime import datetime from typing import Tuple def create_datetime(year: int, month: int, day: int, hour: int, minute: int, second: int, microsecond: int) -> datetime: Creates a datetime object with the specified year, month, day, hour, minute, second, and microsecond. :param year: Year of the datetime :param month: Month of the datetime :param day: Day of the datetime :param hour: Hour of the datetime :param minute: Minute of the datetime :param second: Second of the datetime :param microsecond: Microsecond of the datetime :return: A datetime object with the specified attributes try: return datetime(year, month, day, hour, minute, second, microsecond) except ValueError: raise ValueError(\\"Invalid date parameters were supplied\\") def extract_date_parts(dt: datetime) -> Tuple[int, int, int, int, int, int, int]: Extracts the year, month, day, hour, minute, second, and microsecond from a datetime object. :param dt: A datetime object :return: A tuple containing year, month, day, hour, minute, second, and microsecond return (dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond)"},{"question":"Objective: Implement a Python function that processes lists of files and directories according to specified include and exclude patterns. This function should mimic part of what the `sdist` command does in managing source distributions. Function Signature: ```python def process_files(files: List[str], commands: List[str]) -> List[str]: pass ``` Description: - **Input:** - `files`: A list of strings, each representing a file path. - `commands`: A list of strings, where each string represents a command affecting the inclusion or exclusion of files. Commands follow the format found in the documentation: - `include pat1 pat2 ...` - `exclude pat1 pat2 ...` - `recursive-include dir pat1 pat2 ...` - `recursive-exclude dir pat1 pat2 ...` - `global-include pat1 pat2 ...` - `global-exclude pat1 pat2 ...` - **Output:** - A list of strings, representing the final list of files after applying all inclusion and exclusion commands. Constraints: - Pathnames in the files and commands are Unix-style. - Commands must be processed in the order they are provided. - The list of files must be correctly updated according to the patterns applied per command. Example: ```python files = [ \'src/main.py\', \'src/utils/helpers.py\', \'docs/readme.md\', \'tests/test_main.py\', \'tests/test_helpers.py\' ] commands = [ \'include src/*.py\', \'exclude **/test_*\', \'global-include *.md\', \'recursive-include docs *.md\' ] result = process_files(files, commands) assert result == [ \'src/main.py\', \'src/utils/helpers.py\', \'docs/readme.md\' ] ``` Explanation: 1. Initially, all `*.py` files in the `src` directory are included. 2. Any files matching `test_*` are then excluded. 3. All `*.md` files globally are included. 4. Recursively include `*.md` files from the `docs` directory. Students should demonstrate understanding of filename pattern matching and list manipulation in Python, reflecting the command parsing and processing detailed in the provided document.","solution":"import fnmatch import os def process_files(files, commands): included_files = set() for command in commands: parts = command.split() cmd = parts[0] if cmd == \'include\': patterns = parts[1:] for pattern in patterns: included_files.update(fnmatch.filter(files, pattern)) elif cmd == \'exclude\': patterns = parts[1:] for pattern in patterns: for file in fnmatch.filter(files, pattern): included_files.discard(file) elif cmd == \'recursive-include\': directory = parts[1] patterns = parts[2:] for file in files: if file.startswith(directory + \'/\'): for pattern in patterns: if fnmatch.fnmatch(file, os.path.join(directory, pattern)): included_files.add(file) elif cmd == \'recursive-exclude\': directory = parts[1] patterns = parts[2:] for file in list(included_files): if file.startswith(directory + \'/\'): for pattern in patterns: if fnmatch.fnmatch(file, os.path.join(directory, pattern)): included_files.discard(file) elif cmd == \'global-include\': patterns = parts[1:] for pattern in patterns: included_files.update(fnmatch.filter(files, pattern)) elif cmd == \'global-exclude\': patterns = parts[1:] for pattern in patterns: for file in fnmatch.filter(files, pattern): included_files.discard(file) return sorted(included_files)"},{"question":"# Advanced Email Client Using `imaplib` Objective: Create a basic email client using the `imaplib` module. The task is to connect to an IMAP4 server, list email subjects from a specific mailbox, and fetch complete email details. Instructions: 1. **Connection**: - Use the `IMAP4_SSL` class to connect to an IMAP server. - Authenticate using username and password. 2. **Mailbox**: - Select the \'INBOX\' mailbox. 3. **List Emails**: - List all emails and print their subjects. 4. **Fetch Email**: - Fetch and print the complete content of an email given its sequence number. Requirements: 1. **Class Definition**: - Create an `EmailClient` class with methods to perform the above operations. 2. **Methods**: - `__init__(self, host, username, password)`: Initialize and connect to the server. - `list_subjects(self)`: Retrieve and print the subjects of all emails in the \'INBOX\'. - `fetch_email(self, msg_num)`: Fetch and print the full content of the email identified by `msg_num`. - Ensure proper handling of exceptions using `try-except`. Input and Output: - The `EmailClient` class should handle connecting and interacting with the server. - Methods should output relevant information as specified (e.g., subjects of emails, full email content). Example: ```python class EmailClient: def __init__(self, host, username, password): Initialize the connection to the IMAP4 server. pass # Implement connection logic here def list_subjects(self): List and print the subjects of all emails in the \'INBOX\'. pass # Implement subject listing logic here def fetch_email(self, msg_num): Fetch and print the content of the email given its sequence number. pass # Implement email fetching logic here # Usage example client = EmailClient(\'imap.example.com\', \'user@example.com\', \'password\') client.list_subjects() client.fetch_email(1) ``` Constraints: - Ensure the program handles network timeouts or other connection-related issues. - Provide clean error messages for any failures (e.g., incorrect login, mailbox selection failure). Notes: - Use appropriate methods and classes from the `imaplib` module. - Make sure to close the connection gracefully after operations are complete.","solution":"import imaplib import email from email.header import decode_header class EmailClient: def __init__(self, host, username, password): Initialize the connection to the IMAP4 server. self.host = host self.username = username self.password = password self.mail = imaplib.IMAP4_SSL(self.host) self.mail.login(self.username, self.password) def list_subjects(self): List and print the subjects of all emails in the \'INBOX\'. try: self.mail.select(\\"INBOX\\") result, data = self.mail.search(None, \\"ALL\\") mail_ids = data[0].split() for mail_id in mail_ids: result, msg_data = self.mail.fetch(mail_id, \\"(RFC822)\\") raw_email = msg_data[0][1] msg = email.message_from_bytes(raw_email) subject, encoding = decode_header(msg[\\"Subject\\"])[0] if isinstance(subject, bytes): subject = subject.decode(encoding) print(f\\"Subject: {subject}\\") except Exception as e: print(f\\"An error occurred while listing email subjects: {e}\\") def fetch_email(self, msg_num): Fetch and print the content of the email given its sequence number. try: self.mail.select(\\"INBOX\\") result, msg_data = self.mail.fetch(str(msg_num), \\"(RFC822)\\") raw_email = msg_data[0][1] msg = email.message_from_bytes(raw_email) print(f\\"From: {msg[\'From\']}\\") print(f\\"To: {msg[\'To\']}\\") print(f\\"Subject: {msg[\'Subject\']}\\") print(\\"Content:\\") if msg.is_multipart(): for part in msg.walk(): if part.get_content_type() == \\"text/plain\\": print(part.get_payload(decode=True).decode()) else: print(msg.get_payload(decode=True).decode()) except Exception as e: print(f\\"An error occurred while fetching the email: {e}\\") # Don\'t forget to close the connection eventually def close_connection(self): self.mail.logout()"},{"question":"Objective: Create a Python program using the `sqlite3` module to manage a database of student records. The database should store information about students, including their ID, name, age, and grade. You need to demonstrate your understanding of fundamental and advanced concepts of the `sqlite3` module. Task: 1. Create a database named `students.db`. 2. Create a table named `students` with the following columns: - `id`: Integer (Primary Key) - `name`: Text - `age`: Integer - `grade`: Text 3. Insert at least 3 records into the `students` table. 4. Implement functions to: - Retrieve all student records. - Retrieve students based on a minimum grade. - Update the grade of a student by their ID. - Delete a student record by their ID. 5. Implement a custom adapter and converter for a `Student` class which contains `id`, `name`, `age`, and `grade` as properties. Input and Output formats: - Function to retrieve all students: ```python def get_all_students(con: sqlite3.Connection) -> List[Tuple[int, str, int, str]]: ``` **Output:** List of tuples where each tuple represents a student record (`id`, `name`, `age`, `grade`). - Function to retrieve students by minimum grade: ```python def get_students_by_min_grade(con: sqlite3.Connection, min_grade: str) -> List[Tuple[int, str, int, str]]: ``` **Input:** Connection object and a minimum grade as a string. **Output:** List of tuples where each tuple represents a student record (`id`, `name`, `age`, `grade`). - Function to update student grade: ```python def update_student_grade(con: sqlite3.Connection, student_id: int, new_grade: str) -> None: ``` **Input:** Connection object, student ID as an integer, and the new grade as a string. **Output:** None - Function to delete a student record: ```python def delete_student(con: sqlite3.Connection, student_id: int) -> None: ``` **Input:** Connection object and student ID as an integer. **Output:** None - Custom adapter and converter for `Student` class: ```python class Student: def __init__(self, id: int, name: str, age: int, grade: str): self.id = id self.name = name self.age = age self.grade = grade def adapt_student(student: Student) -> str: pass # Implement the adapter function def convert_student(data: bytes) -> Student: pass # Implement the converter function ``` Constraints: 1. Use placeholders for SQL queries to avoid SQL injection attacks. 2. Ensure proper transaction management for data integrity. 3. Handle exceptions appropriately (e.g., when trying to update or delete a non-existing student). Example Usage: ```python import sqlite3 from typing import List, Tuple # Implement your functions here # Create a connection to the database con = sqlite3.connect(\\"students.db\\") # Perform database operations create_table(con) insert_students(con) all_students = get_all_students(con) students_with_min_grade = get_students_by_min_grade(con, \'B\') update_student_grade(con, 1, \'A\') delete_student(con, 2) con.close() ``` Note: - Make sure to close the database connection properly. - Write the code to handle database creation and manipulation as described. Submission: Submit your Python script file containing all function definitions and necessary code to perform the specified tasks. Ensure your code is well-commented and follows good programming practices.","solution":"import sqlite3 from typing import List, Tuple class Student: def __init__(self, id: int, name: str, age: int, grade: str): self.id = id self.name = name self.age = age self.grade = grade def create_table(con: sqlite3.Connection) -> None: cur = con.cursor() cur.execute(\'\'\' CREATE TABLE IF NOT EXISTS students ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, age INTEGER NOT NULL, grade TEXT NOT NULL ) \'\'\') con.commit() def insert_students(con: sqlite3.Connection) -> None: cur = con.cursor() students = [ (1, \'Alice\', 20, \'A\'), (2, \'Bob\', 22, \'B\'), (3, \'Charlie\', 23, \'C\') ] cur.executemany(\'\'\' INSERT INTO students (id, name, age, grade) VALUES (?, ?, ?, ?) \'\'\', students) con.commit() def get_all_students(con: sqlite3.Connection) -> List[Tuple[int, str, int, str]]: cur = con.cursor() cur.execute(\'SELECT * FROM students\') return cur.fetchall() def get_students_by_min_grade(con: sqlite3.Connection, min_grade: str) -> List[Tuple[int, str, int, str]]: cur = con.cursor() cur.execute(\'SELECT * FROM students WHERE grade >= ?\', (min_grade,)) return cur.fetchall() def update_student_grade(con: sqlite3.Connection, student_id: int, new_grade: str) -> None: cur = con.cursor() cur.execute(\'UPDATE students SET grade = ? WHERE id = ?\', (new_grade, student_id)) con.commit() def delete_student(con: sqlite3.Connection, student_id: int) -> None: cur = con.cursor() cur.execute(\'DELETE FROM students WHERE id = ?\', (student_id,)) con.commit() def adapt_student(student: Student) -> str: return f\\"{student.id},{student.name},{student.age},{student.grade}\\" def convert_student(data: bytes) -> Student: id, name, age, grade = data.decode().split(\',\') return Student(int(id), name, int(age), grade) # Register the adapter and converter sqlite3.register_adapter(Student, adapt_student) sqlite3.register_converter(\\"Student\\", convert_student)"},{"question":"# **MemoryView Utilization and Manipulation** You are required to write a function that demonstrates the creation, manipulation, and verification of `memoryview` objects in Python. **Functionality Requirements** Implement a function `memoryview_operations(data: Union[bytearray, bytes, str, list], operation: str, new_value: Optional[int] = None) -> Union[bytes, List[int], bool]:` This function should perform the following tasks: 1. Create a `memoryview` object from the provided `data` based on its type. 2. Perform the specified `operation` on the `memoryview` object. 3. Return the appropriate result based on the operation: - If the `operation` is `\\"read\\"`, return the bytes representation of the memoryview. - If the `operation` is `\\"modify\\"` and `new_value` is specified, update the first element of the buffer with `new_value` and return the modified `data` as a list. - If the `operation` is `\\"check\\"`, return a boolean indicating whether the `memoryview` object is correctly created from `data`. **Input and Output Formats** - `data` can be of type `bytearray`, `bytes`, `str`, or `list`. - `operation` can be one of `\\"read\\"`, `\\"modify\\"`, or `\\"check\\"`. - `new_value` is an optional integer used only if `operation` is set to `\\"modify\\"`. **Constraints** - If `data` is a `str`, it should be encoded to `bytearray` using UTF-8 encoding. - The function should handle and return appropriate error messages for unsupported `data` types or invalid operations. **Example Usage** ```python # Example 1: data = bytearray([1, 2, 3, 4, 5]) result = memoryview_operations(data, \\"read\\") # Expected output: b\'x01x02x03x04x05\' # Example 2: data = [1, 2, 3, 4, 5] result = memoryview_operations(data, \\"modify\\", 10) # Expected output: [10, 2, 3, 4, 5] # Example 3: data = \\"example\\" result = memoryview_operations(data, \\"check\\") # Expected output: True ```","solution":"from typing import Union, Optional, List def memoryview_operations(data: Union[bytearray, bytes, str, list], operation: str, new_value: Optional[int] = None) -> Union[bytes, List[int], bool]: try: if isinstance(data, str): data = bytearray(data, \'utf-8\') elif isinstance(data, list): data = bytearray(data) elif not isinstance(data, (bytearray, bytes)): raise ValueError(\\"Unsupported data type\\") mv = memoryview(data) if operation == \\"read\\": return bytes(mv) elif operation == \\"modify\\": if new_value is not None: mv[0] = new_value return list(data) else: raise ValueError(\\"new_value must be provided for modify operation\\") elif operation == \\"check\\": return isinstance(mv, memoryview) else: raise ValueError(\\"Unsupported operation\\") except Exception as e: return str(e)"},{"question":"Problem Statement # Polar to Rectangular Complex Number Conversion and Calculation You are tasked with writing a Python function that takes in a list of tuples. Each tuple represents a complex number in polar coordinates (r, phi). Your job is to convert each tuple into its rectangular form (a + bi) and calculate both the modulus and phase for each of these converted complex numbers. Finally, you should return a list of dictionaries where each dictionary contains the original polar coordinates, the calculated rectangular form, the modulus, and the phase. # Function Signature ```python def polar_to_rectangular_and_calculate(polar_coordinates: list) -> list: pass ``` # Input - `polar_coordinates`: A list of tuples. Each tuple contains two floating-point numbers `(r, phi)` where: - `r`: The modulus (distance from the origin to the complex number), must be a non-negative float. - `phi`: The phase angle in radians, can be any float. # Output - A list of dictionaries. Each dictionary should have the following keys: - `\\"original_polar\\"`: The original tuple (r, phi). - `\\"rectangular\\"`: A string representation of the complex number in rectangular form (a + bj). - `\\"modulus\\"`: The modulus of the complex number (a non-negative float). - `\\"phase\\"`: The phase of the complex number (a float within [-π, π]). # Constraints - The input list will have at least one tuple. - You can assume all `r` values will be non-negative. # Example ```python input_data = [(1, 0), (2, math.pi/2), (3, math.pi)] output = polar_to_rectangular_and_calculate(input_data) # Expected output format: [ # {\\"original_polar\\": (1, 0), \\"rectangular\\": \\"1.0 + 0.0j\\", \\"modulus\\": 1.0, \\"phase\\": 0.0}, # {\\"original_polar\\": (2, 1.5707963267948966), \\"rectangular\\": \\"1.2246467991473532e-16 + 2.0j\\", \\"modulus\\": 2.0, \\"phase\\": 1.5707963267948966}, # {\\"original_polar\\": (3, 3.141592653589793), \\"rectangular\\": \\"-3.0 + 3.6739403974420594e-16j\\", \\"modulus\\": 3.0, \\"phase\\": 3.141592653589793} # ] ``` # Hints - Use `cmath.rect` to convert from polar to rectangular coordinates. - Use `cmath.polar` to verify the modulus and phase. - Handle edge cases where the phase is `0`, `pi`, and `-pi`. # Note - The output rectangular representation should capture the complex number accurately. - Use the cmath module\'s functions (`rect`, `phase`, etc.) to assist in the calculations. Good luck!","solution":"import cmath def polar_to_rectangular_and_calculate(polar_coordinates): result = [] for r, phi in polar_coordinates: # Convert polar coordinates to rectangular form rectangular = cmath.rect(r, phi) a, b = rectangular.real, rectangular.imag # Calculate modulus and phase from rectangular form modulus, phase = cmath.polar(rectangular) # Format the rectangular form as a string rectangular_str = f\\"{a} + {b}j\\" # Create the dictionary for the current complex number entry = { \\"original_polar\\": (r, phi), \\"rectangular\\": rectangular_str, \\"modulus\\": modulus, \\"phase\\": phase } result.append(entry) return result"},{"question":"**Objective:** To assess the understanding of the `atexit` module, students are required to implement and demonstrate its usage by writing a Python program. The program will manage a temporary file\'s lifecycle by creating it at the start and ensuring its deletion upon normal program termination. **Problem Statement:** Implement a Python program that: 1. Creates a temporary file named `tempfile.txt` when the program starts. 2. Writes some initial content (\\"Temporary file content\\") into `tempfile.txt`. 3. Registers an exit function using `atexit.register()` to delete the file `tempfile.txt` upon normal program termination. 4. Contains a function `work_with_tempfile()` that performs some operations on the file (e.g., appending additional text). 5. Ensures that `tempfile.txt` is deleted when the program exits normally but remains if the program is terminated unusually (e.g., using an interrupt signal or a fatal error). **Function Specifications:** - **create_temp_file()** - **Input:** None - **Output:** None - **Behavior:** Creates `tempfile.txt` and writes the string \\"Temporary file content\\". - **delete_temp_file()** - **Input:** None - **Output:** None - **Behavior:** Deletes `tempfile.txt` if it exists. - **work_with_tempfile()** - **Input:** None - **Output:** None - **Behavior:** Appends \\"Appending some content\\" to `tempfile.txt` and prints its contents. **Constraints:** - You may assume the file operations (creation, writing, deletion) succeed without error. - Use appropriate exception handling where necessary. - The functions should work correctly on any standard Python 3.10+ interpreter. **Example Execution:** ```python import atexit def create_temp_file(): with open(\'tempfile.txt\', \'w\') as f: f.write(\'Temporary file content\') def delete_temp_file(): import os try: os.remove(\'tempfile.txt\') print(\'Temporary file deleted\') except FileNotFoundError: print(\'Temporary file not found\') def work_with_tempfile(): with open(\'tempfile.txt\', \'a\') as f: f.write(\'Appending some content\') with open(\'tempfile.txt\', \'r\') as f: print(f.read()) # Register the delete function to be called at program exit atexit.register(delete_temp_file) # Main program execution create_temp_file() work_with_tempfile() print(\'Program is terminating...\') ``` **Expected Output:** ``` Temporary file contentAppending some content Program is terminating... Temporary file deleted ``` **Notes:** - The file should exist only for the duration of the program\'s execution. - Ensure your implementation properly registers and unregisters exit functions to clean up resources effectively.","solution":"import atexit import os def create_temp_file(): Creates a temporary file named \'tempfile.txt\' and writes initial content. with open(\'tempfile.txt\', \'w\') as f: f.write(\'Temporary file content\') def delete_temp_file(): Deletes the temporary file \'tempfile.txt\' if it exists. try: os.remove(\'tempfile.txt\') print(\'Temporary file deleted\') except FileNotFoundError: print(\'Temporary file not found\') def work_with_tempfile(): Appends content to the temporary file and prints its content. with open(\'tempfile.txt\', \'a\') as f: f.write(\'Appending some content\') with open(\'tempfile.txt\', \'r\') as f: print(f.read()) # Register the delete function to be called at program exit atexit.register(delete_temp_file) # Main program execution create_temp_file() work_with_tempfile() print(\'Program is terminating...\')"},{"question":"Problem Statement # Task: Write a Python function `process_wav(input_file: str, output_file: str, volume_factor: float) -> None` that reads a WAV file, adjusts the volume of its audio data, and then writes the modified data to a new WAV file. # Input: 1. `input_file` (str): The path to the input WAV file. 2. `output_file` (str): The path to the output WAV file where the modified audio will be saved. 3. `volume_factor` (float): A factor by which to modify the volume. For example, 1.0 leaves the volume unchanged, 0.5 halves the volume, and 2.0 doubles the volume. # Output: - None. The function should save the modified WAV file as specified by `output_file`. # Constraints: 1. Only WAV files using \\"WAVE_FORMAT_PCM\\" (PCM audio data) are supported. 2. The function should handle both mono and stereo audio files. 3. `volume_factor` should be a positive float. # Performance Requirements: - Your solution should efficiently handle WAV files up to 100 MB in size. # Example: Suppose `example.wav` contains original sound data with a frame rate of 44100 Hz, 2 channels (stereo), and 16-bit samples. To halve the volume and save it to `example_processed.wav`, you would call: ```python process_wav(\\"example.wav\\", \\"example_processed.wav\\", 0.5) ``` # Implementation: You may utilize the \\"wave\\" module as described in the documentation provided. # Hints: - Remember to convert the byte data appropriately when adjusting the volume, especially given the byte width of the samples. - Use proper exception handling to manage file read/write errors and any violations of the WAV specification. Here is a skeleton to get you started: ```python import wave def process_wav(input_file: str, output_file: str, volume_factor: float) -> None: with wave.open(input_file, \'rb\') as reader: params = reader.getparams() nchannels, sampwidth, framerate, nframes, comptype, compname = params if comptype != \'NONE\': raise wave.Error(\\"Unsupported compression type\\") frames = reader.readframes(nframes) # Process the frames based on sampwidth if sampwidth == 1: processed_frames = bytearray() for byte in frames: # Adjust the value for volume and ensure it is clamped between 0 and 255 new_value = int(byte * volume_factor) new_value = max(0, min(255, new_value)) processed_frames.append(new_value) elif sampwidth == 2: import struct processed_frames = bytearray() for i in range(0, len(frames), 2): sample = int.from_bytes(frames[i:i+2], \'little\', signed=True) new_sample = int(sample * volume_factor) new_sample = max(-32768, min(32767, new_sample)) processed_frames.extend(new_sample.to_bytes(2, \'little\', signed=True)) else: raise wave.Error(\\"Unsupported sample width\\") with wave.open(output_file, \'wb\') as writer: writer.setparams(params) writer.writeframes(processed_frames) ``` The above code provides a basic framework and ensures you handle mono and stereo audio files with 8-bit and 16-bit depth. Adjust the volume correctly by converting and clamping the sample values as per the byte width.","solution":"import wave def process_wav(input_file: str, output_file: str, volume_factor: float) -> None: if volume_factor <= 0: raise ValueError(\\"volume_factor must be a positive float\\") with wave.open(input_file, \'rb\') as reader: params = reader.getparams() nchannels, sampwidth, framerate, nframes, comptype, compname = params if comptype != \'NONE\': raise wave.Error(\\"Unsupported compression type\\") frames = reader.readframes(nframes) if sampwidth == 1: # 8-bit samples processed_frames = bytearray() for byte in frames: adjusted_sample = int(byte * volume_factor) clamped_sample = max(0, min(255, adjusted_sample)) processed_frames.append(clamped_sample) elif sampwidth == 2: # 16-bit samples import struct processed_frames = bytearray() for i in range(0, len(frames), 2): sample = int.from_bytes(frames[i:i+2], \'little\', signed=True) adjusted_sample = int(sample * volume_factor) clamped_sample = max(-32768, min(32767, adjusted_sample)) processed_frames.extend(clamped_sample.to_bytes(2, \'little\', signed=True)) else: raise wave.Error(\\"Unsupported sample width\\") with wave.open(output_file, \'wb\') as writer: writer.setparams(params) writer.writeframes(processed_frames)"},{"question":"Objective: Implement and work with custom iterators in Python, leveraging the concepts discussed in the provided documentation. Task: Write a Python class `CallableSentinelIterator` that mimics the behavior described for `PyCallIter_Type`. The class should: 1. Initialize with a callable and a sentinel value. 2. Iterate by calling the callable and yielding its results until the sentinel value is returned. Additionally, implement a function `is_callable_sentinel_iter(iter_obj)` that checks if the given object is an instance of `CallableSentinelIterator`. Expected Function Signatures: ```python class CallableSentinelIterator: def __init__(self, callable_obj, sentinel): # Initialize with a callable object and a sentinel value pass def __iter__(self): # Return the iterator object (self) pass def __next__(self): # Implement the iteration logic pass def is_callable_sentinel_iter(iter_obj): # Returns True if iter_obj is an instance of CallableSentinelIterator, else False pass ``` Requirements: 1. The `CallableSentinelIterator` class should implement the iterator protocol by defining the `__iter__` and `__next__` methods. 2. The `__next__` method should call the provided callable, check if the returned value equals the sentinel, and if so, stop the iteration (`StopIteration`). 3. The `is_callable_sentinel_iter` function should use type checking to determine if the object is an instance of `CallableSentinelIterator`. Example Usage: ```python # Example callable that returns the next integer, starting from 1 def counter(): n = 1 while True: yield n n += 1 # Create a generator from the callable gen = counter() # Sentinel value is 5, terminate iteration when 5 is reached iter_obj = CallableSentinelIterator(gen.__next__, 5) # Check if the object is an instance of CallableSentinelIterator print(is_callable_sentinel_iter(iter_obj)) # Output: True # Iterate using our custom iterator for value in iter_obj: print(value) # Output: 1, 2, 3, 4 ``` Constraints: - The callable provided should take no parameters and return the next item in the iteration when called. - The iteration will stop when the sentinel value is returned by the callable.","solution":"class CallableSentinelIterator: def __init__(self, callable_obj, sentinel): Initializes the iterator with a callable object and a sentinel value. :param callable_obj: A callable that returns the next item in the iteration. :param sentinel: A value indicating the end of the iteration. self.callable_obj = callable_obj self.sentinel = sentinel def __iter__(self): Returns the iterator object itself. return self def __next__(self): Calls the callable object and yields its results until the sentinel value is returned. :raises StopIteration: When the sentinel value is encountered. value = self.callable_obj() if value == self.sentinel: raise StopIteration return value def is_callable_sentinel_iter(iter_obj): Checks if the given object is an instance of CallableSentinelIterator. :param iter_obj: The object to check. :return: True if iter_obj is an instance of CallableSentinelIterator, else False. return isinstance(iter_obj, CallableSentinelIterator)"},{"question":"Password Management System You are required to implement a simple password management system that allows users to register with a hashed password and later verify their login credentials. Your task is to create three functions using the `crypt` module: 1. **`register_user(username: str, password: str) -> dict`**: This function should: - Take a username and a plaintext password. - Hash the password using the strongest available hashing method (as per `crypt.methods`). - Return a dictionary with the username as the key and the hashed password as the value. 2. **`verify_user(username: str, password: str, user_db: dict) -> bool`**: This function should: - Take a username, a plaintext password, and a user database (a dictionary where the key is the username and the value is the hashed password). - Verify whether the provided password, when hashed, matches the stored hashed password for the given username. - Use constant-time comparison via `hmac.compare_digest()` to avoid timing attacks. - Return `True` if the password matches, otherwise `False`. 3. **`generate_salt(method, rounds=None) -> str`**: This function should: - Take a hashing method (e.g., `crypt.METHOD_SHA512`) and an optional rounds parameter. - Generate and return a salt for the given method. If rounds are provided, use them in the salt generation. # Input and Output Formats `register_user` - **Input**: - `username`: A string representing the username. - `password`: A string representing the plaintext password. - **Output**: - A dictionary with the username as the key and the hashed password as the value. `verify_user` - **Input**: - `username`: A string representing the username. - `password`: A string representing the plaintext password. - `user_db`: A dictionary containing username-hashed password pairs. - **Output**: - A boolean value indicating whether the password is correct (`True`) or incorrect (`False`). `generate_salt` - **Input**: - `method`: A hashing method (`crypt.METHOD_*`). - `rounds`: (Optional) An integer specifying the number of rounds for applicable methods. - **Output**: - A string representing the generated salt. # Constraints - All usernames and passwords are non-empty strings. - The provided method in `generate_salt` is guaranteed to be among the available methods in `crypt`. # Function Signatures ```python import crypt import hmac def register_user(username: str, password: str) -> dict: # Your implementation here def verify_user(username: str, password: str, user_db: dict) -> bool: # Your implementation here def generate_salt(method, rounds=None) -> str: # Your implementation here ``` Example Usage ```python # Registering a user user_db = register_user(\\"john_doe\\", \\"securepassword123\\") # Verifying user is_verified = verify_user(\\"john_doe\\", \\"securepassword123\\", user_db) print(is_verified) # Output: True # Generating a salt with specific method and rounds salt = generate_salt(crypt.METHOD_SHA512, rounds=10000) print(salt) # Example Output: 6rounds=10000j9V./NAm3..... ``` Ensure to handle edge cases and use the provided functions efficiently for your implementation.","solution":"import crypt import hmac def register_user(username: str, password: str) -> dict: Registers a user by hashing the password and storing it in a dictionary. salt = generate_salt(crypt.METHOD_SHA512) hashed_password = crypt.crypt(password, salt) return {username: hashed_password} def verify_user(username: str, password: str, user_db: dict) -> bool: Verifies a user\'s password against the stored hashed password. if username not in user_db: return False stored_hashed_password = user_db[username] hashed_password = crypt.crypt(password, stored_hashed_password) return hmac.compare_digest(hashed_password, stored_hashed_password) def generate_salt(method, rounds=None) -> str: Generates a salt for the given hashing method, optionally with rounds. if rounds: salt = crypt.mksalt(method, rounds=rounds) else: salt = crypt.mksalt(method) return salt"},{"question":"# PyTorch Elastic Multiprocessing Assessment **Objective**: You are required to demonstrate your understanding of PyTorch\'s elastic multiprocessing by setting up and managing multiple worker processes that perform simple tasks concurrently. # Problem Statement Your task is to create a script that launches and manages multiple worker processes to compute the factorial of a list of numbers. You will utilize the `torch.distributed.elastic.multiprocessing` module to start and manage these processes. # Instructions 1. **Function Implementation**: - Implement a function `worker(number: int) -> int` that computes the factorial of the given number. - Implement a function `start_workers(numbers: List[int]) -> Dict[int, int]` which: - Uses `torch.distributed.elastic.multiprocessing.start_processes` to start a worker process for each number in the input list `numbers`. - Collects the results from each worker process and returns them as a dictionary where the key is the input number and the value is its factorial. 2. **Input Format**: - A list of integers `numbers` for which the factorials need to be computed. 3. **Output Format**: - A dictionary mapping each number to its factorial. 4. **Constraints**: - You may assume that the input list `numbers` will only contain non-negative integers. - Consider edge cases such as computing the factorial of 0. 5. **Performance Requirements**: - The implementation should efficiently manage multiple worker processes and correctly handle their execution and result collection. # Example ```python def worker(number: int) -> int: # Compute and return the factorial of the number pass def start_workers(numbers: List[int]) -> Dict[int, int]: # Launch multiple worker processes to compute factorials pass # Input numbers = [5, 7, 10] # Expected Output # {5: 120, 7: 5040, 10: 3628800} ``` # Additional Notes - Ensure to handle process context and logging appropriately. - You may refer to the PyTorch elastic multiprocessing documentation provided for detailed guidance. # Submission Submit your Python script with the implementations of `worker` and `start_workers`.","solution":"import torch import torch.multiprocessing as mp from typing import List, Dict def worker(rank, number, return_dict): Computes the factorial of the given number and stores it in the return dictionary. result = 1 for i in range(2, number + 1): result *= i return_dict[rank] = result def start_workers(numbers: List[int]) -> Dict[int, int]: Uses torch.multiprocessing to start worker processes and computes factorials of the numbers. manager = mp.Manager() return_dict = manager.dict() processes = [] for idx, number in enumerate(numbers): p = mp.Process(target=worker, args=(idx, number, return_dict)) p.start() processes.append(p) for p in processes: p.join() # Convert return_dict to a standard dictionary with the correct number as keys result_dict = {numbers[key]: value for key, value in return_dict.items()} return result_dict"},{"question":"**Title**: Implement and Debug a TorchScript Annotated Class Function **Problem Statement**: You are required to implement a class using the TorchScript annotation within the PyTorch framework. This class will include a method to normalize a tensor, element-wise, using its mean and standard deviation. Further, you must implement a method to compute the sum of a list of tensors. # Class Specifications: 1. **Class Name**: `TensorProcessor` 2. **Attributes**: - `factor` (float): A multiplication factor for normalization scaling. - `tensors` (List[torch.Tensor]): A list of tensors to be processed. 3. **Methods**: - `__init__(self, factor: float = 1.0, tensors: Optional[List[torch.Tensor]] = None) -> None`: - Initializes `factor` to the provided value. - Initializes `tensors` to an empty list if not provided. - `normalize(self, x: torch.Tensor) -> torch.Tensor`: - Normalizes the input tensor `x` element-wise using mean and std deviation: `normalized_x = (x - mean) / std`. - Multiplies the result by `factor`. - `sum_tensors(self) -> torch.Tensor`: - Computes and returns the sum of all tensors in the `tensors` attribute. # Requirements: 1. **Type Annotations**: All methods must include TorchScript compatible type annotations. 2. **Static Typing**: Ensure all attributes and method variables are statically typed according to TorchScript requirements. 3. **Class Decorator**: Annotate the class with `@torch.jit.script`. # Constraints: - Every tensor in the `tensors` list is of the same shape. - The `normalize` method should handle tensors with `float` values. - `sum_tensors` should handle an empty `tensors` list by returning a zero tensor of shape (1,). # Example: ```python import torch from typing import List, Optional @torch.jit.script class TensorProcessor: factor: float tensors: List[torch.Tensor] def __init__(self, factor: float = 1.0, tensors: Optional[List[torch.Tensor]] = None) -> None: self.factor = factor self.tensors = tensors if tensors is not None else [] def normalize(self, x: torch.Tensor) -> torch.Tensor: mean = torch.mean(x) std = torch.std(x) normalized_x = (x - mean) / std return normalized_x * self.factor def sum_tensors(self) -> torch.Tensor: if len(self.tensors) == 0: return torch.zeros(1) result = torch.zeros_like(self.tensors[0]) for tensor in self.tensors: result += tensor return result # Example Usage input_tensor = torch.tensor([1.0, 2.0, 3.0, 4.0]) processor = TensorProcessor(1.5) normalized_tensor = processor.normalize(input_tensor) tensors_list = [torch.tensor([1.0, 2.0]), torch.tensor([3.0, 4.0])] processor.tensors = tensors_list sum_of_tensors = processor.sum_tensors() print(sum_of_tensors) ``` **Note**: Ensure your implementation adheres to these specifications and constraints.","solution":"import torch from typing import List, Optional @torch.jit.script class TensorProcessor: factor: float tensors: List[torch.Tensor] def __init__(self, factor: float = 1.0, tensors: Optional[List[torch.Tensor]] = None) -> None: self.factor = factor self.tensors = tensors if tensors is not None else [] def normalize(self, x: torch.Tensor) -> torch.Tensor: mean = torch.mean(x) std = torch.std(x) normalized_x = (x - mean) / std return normalized_x * self.factor def sum_tensors(self) -> torch.Tensor: if len(self.tensors) == 0: return torch.zeros(1) result = torch.zeros_like(self.tensors[0]) for tensor in self.tensors: result += tensor return result"},{"question":"Objective To assess the understanding of the `configparser` module in Python by requiring the implementation of a complex configuration manager. Problem Statement You are tasked with developing a configuration manager class that leverages the `configparser` module to handle INI files. This configuration manager should support the following features: 1. **Initialization**: - Accept an optional parameter that specifies the default configuration values. - Optionally accept custom delimiters and comment prefixes. 2. **Loading Configuration**: - Load configuration from a file, string, or dictionary. - Allow multiple configuration files to be read sequentially, where later files override earlier ones. 3. **Accessing Configuration**: - Provide methods to retrieve configuration values with appropriate data types (string, int, float, boolean). - Allow retrieval with default fallback values if the configuration key is missing. 4. **Modifying Configuration**: - Allow adding or updating configuration values. - Include methods to remove sections or options. - Support adding new sections. 5. **Saving Configuration**: - Save the current configuration to a file with customizable formatting options. 6. **Exception Handling**: - Handle all relevant exceptions and provide meaningful error messages. Constraints - Ensure that all keys are accessed in a case-insensitive manner. - Preserve the order of sections and keys within sections as they were added. - Implement interpolation support using both basic and extended interpolators. - Your solution should be efficient and handle large configuration files gracefully. Expected Interfaces # Class: `ConfigManager` `__init__(self, defaults=None, delimiters=(\'=\', \':\'), comment_prefixes=(\'#\', \';\'))` - Initializes the ConfigManager with optional defaults and custom delimiters/comment prefixes. `load_from_file(self, filename: str)` - Loads configuration from a specified file. `load_from_string(self, config_string: str)` - Loads configuration from a string. `load_from_dict(self, config_dict: dict)` - Loads configuration from a dictionary. `get(self, section: str, option: str, fallback: str = None) -> str` - Retrieves a value as a string. `get_int(self, section: str, option: str, fallback: int = None) -> int` - Retrieves a value as an integer. `get_float(self, section: str, option: str, fallback: float = None) -> float` - Retrieves a value as a float. `get_boolean(self, section: str, option: str, fallback: bool = None) -> bool` - Retrieves a value as a boolean. `set(self, section: str, option: str, value: str)` - Sets a configuration value. `add_section(self, section: str)` - Adds a new section. `remove_section(self, section: str)` - Removes a section. `remove_option(self, section: str, option: str)` - Removes an option from a section. `save_to_file(self, filename: str, space_around_delimiters=True)` - Saves the current configuration to a specified file. Example Usage ```python config_defaults = { \'DEFAULT\': { \'ServerAliveInterval\': \'45\', \'Compression\': \'yes\', \'CompressionLevel\': \'9\' } } config_manager = ConfigManager(defaults=config_defaults) # Load configuration from a file config_manager.load_from_file(\'config.ini\') # Access values with different types server_interval = config_manager.get_int(\'DEFAULT\', \'ServerAliveInterval\') compression = config_manager.get_boolean(\'DEFAULT\', \'Compression\') # Modify the configuration config_manager.set(\'DEFAULT\', \'ServerAliveInterval\', \'60\') config_manager.add_section(\'new_section\') config_manager.set(\'new_section\', \'new_option\', \'new_value\') # Save the modified configuration config_manager.save_to_file(\'new_config.ini\') ``` Your implementation will be evaluated based on the correctness of the functionality, code readability, and adherence to best practices.","solution":"import configparser class ConfigManager: def __init__(self, defaults=None, delimiters=(\'=\', \':\'), comment_prefixes=(\'#\', \';\')): self.config = configparser.ConfigParser( defaults=defaults, delimiters=delimiters, comment_prefixes=comment_prefixes, interpolation=configparser.ExtendedInterpolation() ) def load_from_file(self, filename: str): with open(filename, \'r\') as file: self.config.read_file(file) def load_from_string(self, config_string: str): self.config.read_string(config_string) def load_from_dict(self, config_dict: dict): self.config.read_dict(config_dict) def get(self, section: str, option: str, fallback: str = None) -> str: return self.config.get(section, option, fallback=fallback) def get_int(self, section: str, option: str, fallback: int = None) -> int: return self.config.getint(section, option, fallback=fallback) def get_float(self, section: str, option: str, fallback: float = None) -> float: return self.config.getfloat(section, option, fallback=fallback) def get_boolean(self, section: str, option: str, fallback: bool = None) -> bool: return self.config.getboolean(section, option, fallback=fallback) def set(self, section: str, option: str, value: str): if section not in self.config: self.config.add_section(section) self.config.set(section, option, value) def add_section(self, section: str): self.config.add_section(section) def remove_section(self, section: str): self.config.remove_section(section) def remove_option(self, section: str, option: str): self.config.remove_option(section, option) def save_to_file(self, filename: str, space_around_delimiters=True): with open(filename, \'w\') as file: self.config.write(file, space_around_delimiters=space_around_delimiters)"},{"question":"Implement a **Mini Turtle Graphics Game** using the `turtle` module in Python. Game Requirements: 1. **Game Objective**: Create a game where a turtle moves around the screen, collecting randomly placed \\"coins.\\" The game ends when the turtle collects all coins or a specified time elapses. 2. **Initialization**: - The screen should be 600x600 pixels. - Place the turtle at the center of the screen. - Place a random number of coins (between 5 and 10) on the screen at random positions. 3. **User Controls**: - Use the arrow keys to control the turtle\'s movements: - Up Arrow: Move forward by 20 pixels. - Down Arrow: Move backward by 20 pixels. - Right Arrow: Turn right by 30 degrees. - Left Arrow: Turn left by 30 degrees. 4. **Game Logic**: - The game should keep track of the coins collected by the turtle. - When the turtle reaches a coin\'s position (you can specify a proximity threshold such as within 15 pixels), the coin should disappear, and the score should increase. - The game should end when all coins are collected or after 60 seconds, whichever comes first. 5. **Output**: - Display the current score on the screen. - Display the time remaining on the screen. - When the game ends, display a message: \\"Game Over! You collected X coins in 60 seconds,\\" where X is the number of coins collected. Constraints and Performance Requirements: - The turtle should move smoothly and react instantly to user inputs. - Ensure the game runs efficiently with minimal delay or lag in response to user controls. Example: Here is an example initialization and an outline of the main game loop: ```python import turtle import random import time # Setup Screen screen = turtle.Screen() screen.title(\\"Turtle Coin Collection Game\\") screen.bgcolor(\\"white\\") screen.setup(width=600, height=600) # Setup Turtle player = turtle.Turtle() player.shape(\\"turtle\\") player.color(\\"blue\\") player.penup() player.goto(0, 0) # Define functions for movement def move_forward(): player.forward(20) def move_backward(): player.backward(20) def turn_left(): player.left(30) def turn_right(): player.right(30) # Bind keys to movement functions screen.listen() screen.onkey(move_forward, \\"Up\\") screen.onkey(move_backward, \\"Down\\") screen.onkey(turn_left, \\"Left\\") screen.onkey(turn_right, \\"Right\\") # Create coins coins = [] for _ in range(random.randint(5, 10)): coin = turtle.Turtle() coin.shape(\\"circle\\") coin.color(\\"gold\\") coin.penup() coin.goto(random.randint(-290, 290), random.randint(-290, 290)) coins.append(coin) # Main game loop start_time = time.time() score = 0 while True: current_time = time.time() elapsed_time = current_time - start_time remaining_time = 60 - elapsed_time # Check for collisions with coins for coin in coins: if player.distance(coin) < 15: coin.hideturtle() coins.remove(coin) score += 1 # Update screen with scores and remaining time # ... implement the display update logic here ... # End game conditions if len(coins) == 0 or remaining_time <= 0: break # Display game over message # ... Display final score message here ... screen.mainloop() ``` Ensure to complete the code by implementing the display update logic and final score message. Your final implementation should deliver a functional and interactive game.","solution":"import turtle import random import time def main(): # Setup Screen screen = turtle.Screen() screen.title(\\"Turtle Coin Collection Game\\") screen.bgcolor(\\"white\\") screen.setup(width=600, height=600) # Setup Turtle player = turtle.Turtle() player.shape(\\"turtle\\") player.color(\\"blue\\") player.penup() player.goto(0, 0) # Score Tracking score = 0 # Setup Score Display score_display = turtle.Turtle() score_display.hideturtle() score_display.penup() score_display.goto(-290, 260) score_display.write(f\\"Score: {score}\\", font=(\\"Arial\\", 16, \\"normal\\")) # Setup Timer Display timer_display = turtle.Turtle() timer_display.hideturtle() timer_display.penup() timer_display.goto(200, 260) timer_display.write(\\"Time: 60\\", font=(\\"Arial\\", 16, \\"normal\\")) # Define functions for movement def move_forward(): player.forward(20) def move_backward(): player.backward(20) def turn_left(): player.left(30) def turn_right(): player.right(30) # Bind keys to movement functions screen.listen() screen.onkey(move_forward, \\"Up\\") screen.onkey(move_backward, \\"Down\\") screen.onkey(turn_left, \\"Left\\") screen.onkey(turn_right, \\"Right\\") # Create coins coins = [] for _ in range(random.randint(5, 10)): coin = turtle.Turtle() coin.shape(\\"circle\\") coin.color(\\"gold\\") coin.penup() coin.goto(random.randint(-290, 290), random.randint(-290, 290)) coins.append(coin) # Main game loop start_time = time.time() while True: current_time = time.time() elapsed_time = current_time - start_time remaining_time = max(60 - int(elapsed_time), 0) # Update Timer Display timer_display.clear() timer_display.write(f\\"Time: {remaining_time}\\", font=(\\"Arial\\", 16, \\"normal\\")) # Check for collisions with coins for coin in coins: if player.distance(coin) < 15: coin.hideturtle() coins.remove(coin) score += 1 score_display.clear() score_display.write(f\\"Score: {score}\\", font=(\\"Arial\\", 16, \\"normal\\")) # End game conditions if len(coins) == 0 or remaining_time == 0: break # Display game over message final_message = turtle.Turtle() final_message.hideturtle() final_message.penup() final_message.goto(-150, 0) final_message.write(f\\"Game Over! You collected {score} coins in 60 seconds\\", font=(\\"Arial\\", 16, \\"normal\\")) screen.mainloop() if __name__ == \\"__main__\\": main()"},{"question":"The objective of this coding assessment is to demonstrate your understanding of various seaborn functionalities for controlling plot aesthetics and context. You will create specific visualizations based on the instructions provided. Task: 1. **Dataset Preparation**: - Create a dataset of two arrays `x` and `y` where: - `x` is a numpy array of 100 linearly spaced points between 0 and 10. - `y` is a numpy array comprising the natural logarithm of values in `x` added with some random Gaussian noise. 2. **Plot with Different Themes**: - Generate four subplots (2x2 grid) in a single figure, each subplot should use a different seaborn theme (`darkgrid`, `whitegrid`, `dark`, `ticks`). - Plot `y` versus `x` in each of these subplots. 3. **Customize a Plot**: - Create a new figure. - Use the `darkgrid` theme and modify the background color of axes to a light gray (`#f0f0f0`). - Plot `y` versus `x` and ensure line width (`linewidth`) is set to `3`. 4. **Applying a Context with Font Scaling**: - Create another plot of `y` versus `x` with the `poster` context. - Increase the font size to 1.3 times the default. 5. **Despine a Plot**: - Using the `whitegrid` theme, plot `y` versus `x`. - Remove the top and right spines from the plot. 6. **Temporary Style Application**: - Create a single figure with two side-by-side subplots. In the left subplot, apply the `ticks` style. In the right subplot, apply the `white` style. - Plot `y` versus `x` in each subplot. Constraints: - Ensure you use a `with` statement where asked for temporarily setting styles. - For each plotting step, add appropriate titles to distinguish between the different plots. Expected Input and Output Formats: - No specific input or output for the function since the task is visualization. - Ensure your plots are correctly displayed with the required customizations. Provide the implemented solution as a Jupyter Notebook or Python script.","solution":"import numpy as np import seaborn as sns import matplotlib.pyplot as plt # Dataset Preparation x = np.linspace(0, 10, 100) y = np.log(x + 1e-10) + np.random.normal(scale=0.5, size=len(x)) def plot_with_different_themes(): fig, axes = plt.subplots(2, 2, figsize=(15, 10)) themes = [\'darkgrid\', \'whitegrid\', \'dark\', \'ticks\'] for ax, theme in zip(axes.flatten(), themes): sns.set_theme(style=theme) sns.lineplot(x=x, y=y, ax=ax) ax.set_title(f\'Seaborn Theme: {theme}\') plt.tight_layout() plt.show() def plot_with_custom_theme(): sns.set_theme(style=\'darkgrid\', rc={\'axes.facecolor\': \'#f0f0f0\'}) plt.figure() sns.lineplot(x=x, y=y, linewidth=3) plt.title(\'Custom Darkgrid Theme with light grey background\') plt.show() def plot_with_poster_context(): sns.set_context(\'poster\', font_scale=1.3) plt.figure() sns.lineplot(x=x, y=y) plt.title(\'Poster Context with increased font size\') plt.show() def plot_with_despine(): sns.set_theme(style=\'whitegrid\') plt.figure() sns.lineplot(x=x, y=y) sns.despine(top=True, right=True) plt.title(\'Whitegrid Theme with despined top and right\') plt.show() def plot_with_temporary_styles(): plt.figure(figsize=(15, 5)) plt.subplot(1, 2, 1) with sns.axes_style(\'ticks\'): sns.lineplot(x=x, y=y) plt.title(\'Temporary Ticks Style\') plt.subplot(1, 2, 2) with sns.axes_style(\'white\'): sns.lineplot(x=x, y=y) plt.title(\'Temporary White Style\') plt.tight_layout() plt.show()"},{"question":"Asynchronous File Processing with Platform-Specific Constraints You are asked to create an asynchronous Python program using the `asyncio` module. The program should read data from a file, process it, and then write the processed data to another file. This task should highlight your understanding of `asyncio` and consider platform-specific constraints in Python 3.10. Requirements: 1. Implement a function `async def read_file(file_path: str) -> str:` that asynchronously reads the content of a given file. 2. Implement a function `async def write_file(file_path: str, data: str) -> None:` that asynchronously writes data to a given file. 3. Implement a function `async def process_content(data: str) -> str:` that performs some processing on the data (for example, reversing the string). 4. Implement an orchestrator function `async def main(read_path: str, write_path: str) -> None:` that coordinates reading from the input file, processing the content, and writing the processed content to the output file. Constraints: - The functions should work correctly on Windows, macOS, and Unix systems. - For Windows, ensure you use the `ProactorEventLoop` explicitly to manage asynchronous operations. This is crucial because it supports subprocesses and is the default event loop. - Perform proper exception handling to manage file read/write errors. Performance Requirements: - The functions should be non-blocking and asynchronous to take advantage of Python\'s `asyncio` capabilities. - Efficiently handle large files without exhausting system memory. Expected Input and Output Formats: - `read_file(file_path: str) -> str`: Takes a file path as input and returns the file content as a string. - `write_file(file_path: str, data: str) -> None`: Takes a file path and data string as input, writes the data to the file, and returns nothing. - `process_content(data: str) -> str`: Takes a string as input, processes it, and returns the processed string. - `main(read_path: str, write_path: str) -> None`: Takes input file path and output file path, reads from the input file, processes the content, and writes to the output file asynchronously. Example: ```python import asyncio import os async def read_file(file_path: str) -> str: # Implement reading file asynchronously pass async def write_file(file_path: str, data: str) -> None: # Implement writing file asynchronously pass async def process_content(data: str) -> str: # Implement content processing (e.g., reversing the string) return data[::-1] async def main(read_path: str, write_path: str) -> None: try: data = await read_file(read_path) processed_data = process_content(data) await write_file(write_path, processed_data) except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": if os.name == \'nt\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) asyncio.run(main(\'input.txt\', \'output.txt\')) ``` This question tests understanding of asyncio, platform-specific constraints, asynchronous programming techniques, and effective I/O handling.","solution":"import asyncio import os async def read_file(file_path: str) -> str: Asynchronously reads the content of a given file. loop = asyncio.get_running_loop() with open(file_path, \'r\', encoding=\'utf-8\') as file: return await loop.run_in_executor(None, file.read) async def write_file(file_path: str, data: str) -> None: Asynchronously writes data to a given file. loop = asyncio.get_running_loop() with open(file_path, \'w\', encoding=\'utf-8\') as file: await loop.run_in_executor(None, file.write, data) async def process_content(data: str) -> str: Performs some processing on the data (e.g., reversing the string). return data[::-1] async def main(read_path: str, write_path: str) -> None: Coordinates reading from the input file, processing the content, and writing the processed content to the output file. try: data = await read_file(read_path) processed_data = await process_content(data) await write_file(write_path, processed_data) except Exception as e: print(f\\"An error occurred: {e}\\") if __name__ == \\"__main__\\": if os.name == \'nt\': asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy()) asyncio.run(main(\'input.txt\', \'output.txt\'))"},{"question":"Question # Objective You are given a dataset containing sales data for a retail company. Your task is to analyze and summarize the data using pandas. This will assess your ability to perform data manipulation, aggregation, and summarization. # Dataset The dataset is provided in a CSV file named `sales_data.csv` which contains the following columns: - `Date`: The date of the sale (format: YYYY-MM-DD). - `Product`: The name of the product. - `Category`: The category of the product. - `Units Sold`: The number of units sold. - `Revenue`: Revenue generated from the sale. - `Cost`: Cost incurred for the sale. # Tasks 1. **Data Loading and Preprocessing:** - Load the data from the CSV file into a pandas DataFrame. - Display the first 5 rows of the DataFrame. - Check for missing data and fill any missing values appropriately. 2. **Descriptive Statistics:** - Generate a summary of key statistics for numerical columns (`Units Sold`, `Revenue`, and `Cost`). - Calculate the total revenue and cost for each product category. 3. **Data Aggregation:** - Group the data by `Category` and calculate the total units sold, total revenue, and total cost for each category. - Find the product with the highest revenue in each category. 4. **Time Series Analysis:** - Convert the `Date` column to datetime format. - Set the `Date` column as the DataFrame index. - Resample the data by month and calculate the total revenue and cost for each month. 5. **Visualization (Optional for extra credit):** - Plot the total monthly revenue and cost using a line graph. # Input and Output Formats - **Input:** - A CSV file named `sales_data.csv`. - **Output:** - Print statements or return values summarizing the results for each task. # Example If your `sales_data.csv` contains the following data: ```csv Date,Product,Category,Units Sold,Revenue,Cost 2023-01-01,Product A,Category 1,10,100,50 2023-01-05,Product B,Category 2,5,50,20 2023-02-01,Product A,Category 1,8,80,40 ... ``` Your output should indicate: 1. Data after loading and filling missing values. 2. Summary statistics for numerical columns. 3. Aggregated data for each category. 4. Time series analysis for monthly data. 5. Visualizations (if implemented). ```python import pandas as pd # Task 1: Data Loading and Preprocessing def load_preprocess_data(file_path): df = pd.read_csv(file_path) print(df.head()) # Display the first 5 rows # Fill missing values (assuming filling with 0 for this example) df.fillna(0, inplace=True) return df # Task 2: Descriptive Statistics def descriptive_statistics(df): summary = df.describe() print(summary) total_revenue_cost = df.groupby(\'Category\')[[\'Revenue\', \'Cost\']].sum() print(total_revenue_cost) # Task 3: Data Aggregation def aggregate_data(df): category_agg = df.groupby(\'Category\').agg({\'Units Sold\': \'sum\', \'Revenue\': \'sum\', \'Cost\': \'sum\'}) print(category_agg) highest_revenue_product = df.loc[df.groupby(\'Category\')[\'Revenue\'].idxmax()] print(highest_revenue_product[[\'Category\', \'Product\', \'Revenue\']]) # Task 4: Time Series Analysis def time_series_analysis(df): df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) monthly_data = df.resample(\'M\').agg({\'Revenue\': \'sum\', \'Cost\': \'sum\'}) print(monthly_data) # Task 5: Visualization (Optional) def plot_data(df): import matplotlib.pyplot as plt df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) monthly_data = df.resample(\'M\').agg({\'Revenue\': \'sum\', \'Cost\': \'sum\'}) plt.figure(figsize=(10, 6)) plt.plot(monthly_data.index, monthly_data[\'Revenue\'], label=\'Revenue\') plt.plot(monthly_data.index, monthly_data[\'Cost\'], label=\'Cost\') plt.xlabel(\'Date\') plt.ylabel(\'Amount\') plt.title(\'Monthly Revenue and Cost\') plt.legend() plt.show() # Main function def main(): file_path = \'sales_data.csv\' df = load_preprocess_data(file_path) descriptive_statistics(df) aggregate_data(df) time_series_analysis(df) plot_data(df) # Optional if __name__ == \\"__main__\\": main() ```","solution":"import pandas as pd # Task 1: Data Loading and Preprocessing def load_preprocess_data(file_path): df = pd.read_csv(file_path) print(df.head()) # Display the first 5 rows # Fill missing values (assuming mean for numerical columns and mode for categorical columns) for column in df.columns: if df[column].dtype == \'object\': df[column].fillna(df[column].mode()[0], inplace=True) else: df[column].fillna(df[column].mean(), inplace=True) return df # Task 2: Descriptive Statistics def descriptive_statistics(df): summary = df.describe() print(summary) total_revenue_cost = df.groupby(\'Category\')[[\'Revenue\', \'Cost\']].sum() print(total_revenue_cost) return summary, total_revenue_cost # Task 3: Data Aggregation def aggregate_data(df): category_agg = df.groupby(\'Category\').agg({\'Units Sold\': \'sum\', \'Revenue\': \'sum\', \'Cost\': \'sum\'}) print(category_agg) highest_revenue_product = df.loc[df.groupby(\'Category\')[\'Revenue\'].idxmax()] print(highest_revenue_product[[\'Category\', \'Product\', \'Revenue\']]) return category_agg, highest_revenue_product # Task 4: Time Series Analysis def time_series_analysis(df): df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) monthly_data = df.resample(\'M\').agg({\'Revenue\': \'sum\', \'Cost\': \'sum\'}) print(monthly_data) return monthly_data # Task 5: Visualization (Optional) def plot_data(df): import matplotlib.pyplot as plt df[\'Date\'] = pd.to_datetime(df[\'Date\']) df.set_index(\'Date\', inplace=True) monthly_data = df.resample(\'M\').agg({\'Revenue\': \'sum\', \'Cost\': \'sum\'}) plt.figure(figsize=(10, 6)) plt.plot(monthly_data.index, monthly_data[\'Revenue\'], label=\'Revenue\') plt.plot(monthly_data.index, monthly_data[\'Cost\'], label=\'Cost\') plt.xlabel(\'Date\') plt.ylabel(\'Amount\') plt.title(\'Monthly Revenue and Cost\') plt.legend() plt.show() # Main function def main(): file_path = \'sales_data.csv\' df = load_preprocess_data(file_path) descriptive_statistics(df) aggregate_data(df) time_series_analysis(df) plot_data(df) # Optional if __name__ == \\"__main__\\": main()"},{"question":"# URL Manipulation and Validation Objective: The goal is to design and implement a function that validates and manipulates URLs. Your function should take a list of URLs and apply a sequence of transformations and validations to ensure that each URL is correctly parsed and modified as needed. Problem Statement: You need to implement a function `process_urls(urls: List[str]) -> List[str]` that takes a list of URLs and performs the following steps for each URL: 1. **Validation**: Ensure the URL contains a valid scheme (\'http\' or \'https\'). If the URL does not contain a scheme, assume \'http\' as the default scheme. 2. **Normalization**: Convert the hostname part of the URL to lowercase. 3. **Fragment Removal**: Remove any fragment component from the URL. 4. **Reconstruction**: Return a list of the updated URLs. Input: - `urls`: A list of strings, where each string represents a URL. (1 ≤ len(urls) ≤ 10^5) Output: - A list of strings, where each string is the transformed URL. Constraints: - Use the `urllib.parse` module for URL parsing and manipulation. - Ensure the function is efficient, aiming for O(n) time complexity where `n` is the number of URLs. Example: ```python from urllib.parse import urlparse, urlunparse, urlsplit, urlunsplit def process_urls(urls): processed_urls = [] for url in urls: # Step 1: Validation url_parts = urlparse(url) if not url_parts.scheme: url = \\"http://\\" + url url_parts = urlparse(url) # Step 2: Normalization netloc = url_parts.netloc.lower() # Step 3: Fragment Removal url_parts = url_parts._replace(fragment=\\"\\") # Step 4: Reconstruction new_url = urlunparse(url_parts._replace(netloc=netloc)) processed_urls.append(new_url) return processed_urls # Example usage: urls = [\\"HTTP://example.COM/path?query=123#fragment\\", \\"example.com\\", \\"https://example.com/path#section\\"] print(process_urls(urls)) ``` ***Expected Output:*** ``` [\'http://example.com/path?query=123\', \'http://example.com\', \'https://example.com/path\'] ``` Notes: - Use the provided functions in the `urllib.parse` module effectively. - Be mindful of edge cases where the URL might lack certain components (like scheme or netloc). Your task is to implement the `process_urls` function based on the requirements above.","solution":"from urllib.parse import urlparse, urlunparse def process_urls(urls): processed_urls = [] for url in urls: # Step 1: Validation url_parts = urlparse(url) if not url_parts.scheme: url = \\"http://\\" + url url_parts = urlparse(url) # Step 2: Normalization netloc = url_parts.netloc.lower() # Step 3: Fragment Removal url_parts = url_parts._replace(fragment=\\"\\") # Step 4: Reconstruction new_url = urlunparse(url_parts._replace(netloc=netloc)) processed_urls.append(new_url) return processed_urls"},{"question":"Title: Implementation of Conditional Network Module with torch.cond # Objective: Assess the student\'s ability to utilize PyTorch\'s `torch.cond` feature to create a dynamic model that changes its behavior based on input data characteristics. # Task: Create a PyTorch module named `ConditionalNetworkModule` that adjusts its behavior based on the mean value of the input tensor. The module should: 1. Compute the mean of the input tensor. 2. If the mean value is greater than a threshold (e.g., 0.5), apply the true branch function which adds 1 to every element of the tensor. 3. Otherwise, apply the false branch function which subtracts 1 from every element of the tensor. # Requirements: 1. The main class `ConditionalNetworkModule` must inherit from `torch.nn.Module`. 2. Use `torch.cond` to handle the conditional logic. 3. The module should accept a single input tensor and return the modified tensor based on the described logic. # Input: - A single input tensor `x` of type `torch.Tensor` with any shape. # Output: - A single output tensor of the same shape as input, modified according to the specified conditions. # Example: ```python import torch class ConditionalNetworkModule(torch.nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: mean_val = x.mean() def true_fn(x: torch.Tensor): return x + 1 def false_fn(x: torch.Tensor): return x - 1 return torch.cond(mean_val > 0.5, true_fn, false_fn, (x,)) # Test the implementation module = ConditionalNetworkModule() inp1 = torch.tensor([[-0.6, -0.4], [0.3, 0.2]]) out1 = module(inp1) print(out1) # Should print the tensor with each element subtracted by 1 inp2 = torch.tensor([[1.4, 2.3], [0.5, 1.6]]) out2 = module(inp2) print(out2) # Should print the tensor with each element added by 1 ``` # Constraints: 1. The input tensor can have any shape. 2. The threshold for the condition is fixed at 0.5 for this task. 3. The operations in true and false branches must be element-wise and not alter the dimensions of the tensor. 4. Ensure that the module does not mutate the input tensor but instead returns a new tensor. # Performance: - Aim for an efficient implementation, leveraging PyTorch\'s native operations where possible.","solution":"import torch import torch.nn as nn class ConditionalNetworkModule(nn.Module): def __init__(self): super().__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: mean_val = x.mean() def true_fn(): return x + 1 def false_fn(): return x - 1 return torch.where(mean_val > 0.5, true_fn(), false_fn()) # Example usage: # module = ConditionalNetworkModule() # inp1 = torch.tensor([[-0.6, -0.4], [0.3, 0.2]]) # out1 = module(inp1) # print(out1) # # inp2 = torch.tensor([[1.4, 2.3], [0.5, 1.6]]) # out2 = module(inp2) # print(out2)"},{"question":"# Question: Visualizing Data with Pandas Plotting You have been provided with a dataset of yearly sales data for multiple products. Your task is to implement a function that uses the `pandas` library to perform the following: 1. **Generate a Line Plot**: Plot the sales data of all products on the same plot with appropriate labels for the axes and titles. 2. **Generate a Histogram**: Plot a histogram of the sales data for a specific product. 3. **Generate a Bar Plot**: Create a bar plot for the average yearly sales of each product. 4. **Generate a Pie Chart**: Plot a pie chart showing the proportion of total sales contributed by each product for a specified year. 5. **Generate a Scatter Plot**: Create a scatter plot to visualize the relationship between the sales of two specific products over the years. Implement the following function: ```python import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(sales_data: pd.DataFrame, product_hist: str, product_pair: tuple, year: int): Function to visualize sales data. Parameters: sales_data (pd.DataFrame): DataFrame containing sales data with columns for years and rows for products. product_hist (str): The product for which to plot the histogram. product_pair (tuple): A tuple containing two product names for the scatter plot. year (int): The year for which to generate the pie chart. Returns: None: The function should generate the plots directly. # Your implementation here ``` # Input Format - `sales_data`: A pandas DataFrame containing sales data. The DataFrame has years as columns and products as rows. - `product_hist`: A string representing the product name for which to plot the histogram. - `product_pair`: A tuple containing two strings, each representing a product name. - `year`: An integer representing the year for which to generate the pie chart. # Outputs The function should save and display the following plots: 1. **Line Plot**: Line plot of the sales data of all products with appropriate axis labels and title. 2. **Histogram**: Histogram of the sales for the specified product. 3. **Bar Plot**: Bar plot of the average yearly sales for each product. 4. **Pie Chart**: Pie chart showing the proportion of sales by each product for the specified year. 5. **Scatter Plot**: Scatter plot showing the sales relationship between the two specified products over the years. # Constraints - Ensure the function handles missing data appropriately, e.g., using `dropna` or `fillna` functions. - Use appropriate aesthetics for the plots (labels, titles, legends, colors). # Example ```python # Mock sales data data = { \'Year\': [2018, 2019, 2020, 2021, 2022], \'Product_A\': [100, 150, 200, 250, 300], \'Product_B\': [80, 120, 160, 200, 240], \'Product_C\': [60, 90, 120, 150, 180] } sales_df = pd.DataFrame(data).set_index(\'Year\') # Call function visualize_sales_data(sales_df, \'Product_A\', (\'Product_A\', \'Product_B\'), 2020) ``` Your implementation should generate and display/save the required plots.","solution":"import pandas as pd import matplotlib.pyplot as plt def visualize_sales_data(sales_data: pd.DataFrame, product_hist: str, product_pair: tuple, year: int): Function to visualize sales data. Parameters: sales_data (pd.DataFrame): DataFrame containing sales data with columns for years and rows for products. product_hist (str): The product for which to plot the histogram. product_pair (tuple): A tuple containing two product names for the scatter plot. year (int): The year for which to generate the pie chart. Returns: None: The function should generate the plots directly. # Line Plot for all products sales_data.plot(kind=\'line\') plt.title(\'Yearly Sales Data for All Products\') plt.xlabel(\'Year\') plt.ylabel(\'Sales\') plt.legend(title=\'Products\') plt.grid(True) plt.show() # Histogram for a specific product sales_data[product_hist].dropna().plot(kind=\'hist\', bins=10) plt.title(f\'Sales Histogram for {product_hist}\') plt.xlabel(\'Sales\') plt.ylabel(\'Frequency\') plt.grid(True) plt.show() # Bar Plot for average yearly sales of each product avg_sales = sales_data.mean() avg_sales.plot(kind=\'bar\') plt.title(\'Average Yearly Sales for Each Product\') plt.xlabel(\'Product\') plt.ylabel(\'Average Sales\') plt.grid(True) plt.show() # Pie Chart for proportion of total sales by each product for a specified year if year in sales_data.index: sales_year = sales_data.loc[year] sales_year.plot(kind=\'pie\', autopct=\'%1.1f%%\') plt.title(f\'Sales Distribution for Year {year}\') plt.ylabel(\'\') plt.show() else: raise ValueError(f\\"Year {year} not found in the sales data\\") # Scatter Plot to visualize the relationship between the sales of two specific products if product_pair[0] in sales_data.columns and product_pair[1] in sales_data.columns: sales_data.plot(kind=\'scatter\', x=product_pair[0], y=product_pair[1]) plt.title(f\'Sales Relationship Between {product_pair[0]} and {product_pair[1]}\') plt.xlabel(f\'Sales of {product_pair[0]}\') plt.ylabel(f\'Sales of {product_pair[1]}\') plt.grid(True) plt.show() else: raise ValueError(f\\"One or both products in {product_pair} not found in the sales data\\")"},{"question":"# PyTorch Accelerator Control and Querying Objective: Write a function in PyTorch that: 1. Checks how many accelerator devices are available. 2. If at least one device is available, sets the current device to the first available accelerator. 3. Queries and prints the current device index. 4. Synchronizes the current device to ensure all operations on the device have been completed. Specifications: - Function Name: `control_and_query_accelerator` - Input: None - Output: None (The function should print the required information as specified below) Steps: 1. **Check Device Availability and Print Count**: - Use the `device_count` function to get the total number of available accelerator devices. - Print the total number of devices. 2. **Set Current Device**: - If at least one device is available (`device_count` > 0): - Use `set_device_index` to set the current device to the device at index 0. 3. **Print Current Device Index**: - Retrieve the current device index using `current_device_index`. - Print the current device index. 4. **Synchronize Device**: - Use the `synchronize` function to make sure all operations on the current device are completed. Example: ```python def control_and_query_accelerator(): # Your implementation here using the steps outlined above pass # Expected output when you run the function assuming there is 1 accelerator device # Total devices available: 1 # Current device index: 0 ``` Constraints: - You must utilize the `torch.accelerator` functions as outlined in the documentation. - Ensure proper error handling where applicable, especially when the accelerator is not available or the device count is zero.","solution":"import torch def control_and_query_accelerator(): Checks the number of available accelerator devices, sets the current device to the first available device (if any), prints the current device index, and synchronizes the current device. # Check how many accelerator devices are available total_devices = torch.cuda.device_count() print(f\\"Total devices available: {total_devices}\\") if total_devices > 0: # If at least one device is available, set the current device to the first available device torch.cuda.set_device(0) # Query and print the current device index current_device = torch.cuda.current_device() print(f\\"Current device index: {current_device}\\") # Synchronize the current device torch.cuda.synchronize() else: print(\\"No accelerator devices available.\\")"},{"question":"# Advanced Python Programming Question **Objective**: Demonstrate understanding of the Unix `syslog` library routines in Python by designing a function to log multiple messages with varying priorities and facilities. # Problem Description Write a Python function `log_messages(messages, ident=None, facility=None, logoption=0)` that logs a list of messages to the Unix system logger. Each message in the list is a tuple consisting of the message string and its priority. Optionally, you can include an identifier and facility for the log. Function Signature ```python def log_messages(messages, ident=None, facility=None, logoption=0): pass ``` Input - `messages`: A list of tuples where each tuple consists of a message string and a priority level (e.g., `[(message1, priority1), (message2, priority2), ...]`). - `ident` (optional): A string identifier prepended to each message. - `facility` (optional): An integer indicating the default facility for messages. - `logoption` (optional): An integer indicating the logging options bit field. Output - The function does not return anything. It should perform logging as a side-effect. Example ```python messages = [ (\\"Initialization started\\", syslog.LOG_INFO), (\\"Connection established\\", syslog.LOG_NOTICE), (\\"Error encountered\\", syslog.LOG_ERR), (\\"Debug details\\", syslog.LOG_DEBUG) ] log_messages(messages, ident=\\"MyApp\\", facility=syslog.LOG_USER, logoption=syslog.LOG_PID) ``` Constraints - Ensure that if `openlog()` is called, it is called before any `syslog()` calls. - Manage the priority and facility properly for each message. - Make use of `syslog.closelog()` to reset the logging system once logging is complete. Additional Information - If `ident` is not provided, `sys.argv[0]` with leading path components stripped should be used. - If `facility` is not provided, use the default facility `LOG_USER`. Performance Requirements - This function should be able to log at least 1,000 messages in a timely manner. # Instructions 1. Implement the `log_messages` function. 2. Use appropriate handling of the optional parameters. 3. Ensure the function adheres to the constraints and requirements specified.","solution":"import syslog import os import sys def log_messages(messages, ident=None, facility=None, logoption=0): Logs a list of messages to the Unix system logger. Each message is a tuple consisting of the message string and its priority. :param messages: List of tuples (message, priority) :param ident: String identifier for the log (default: script name stripped of path) :param facility: syslog facility (default: LOG_USER) :param logoption: syslog logging options bit field (default: 0) if ident is None: ident = os.path.basename(sys.argv[0]) if facility is None: facility = syslog.LOG_USER syslog.openlog(ident=ident, logoption=logoption, facility=facility) for message, priority in messages: syslog.syslog(priority, message) syslog.closelog()"},{"question":"# Question: Implement and Compare Neighbors Search Algorithms You have been provided with a dataset containing coordinates in 2D space. Your task is to build a nearest neighbors search implementation using `sklearn.neighbors`. You will then use it to classify new points based on their nearest neighbors. Part 1: Implement Nearest Neighbors Search for Classification 1. **Import necessary libraries** from `sklearn.neighbors` and `numpy`. 2. Create a synthetic dataset `X` using numpy as follows: ```python import numpy as np X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y = np.array([0, 0, 0, 1, 1, 1]) ``` 3. Implement a function `find_nearest_neighbors(X_train, n_neighbors, algorithm=\'auto\')` that: - Accepts the training data `X_train`. - Accepts the number of neighbors to find `n_neighbors`. - Accepts the neighbors search algorithm to use (one of \'auto\', \'ball_tree\', \'kd_tree\', \'brute\'). - Returns the distances and indices of the nearest neighbors for the points in `X_train`. 4. Implement a function `classify_points(X_train, y_train, X_test, n_neighbors, algorithm=\'auto\')` that: - Accepts the training data `X_train` and labels `y_train`. - Accepts new data points `X_test` that need to be classified. - Uses `find_nearest_neighbors` to find the nearest neighbors for the test points. - Classifies each point in `X_test` by majority vote of its `n_neighbors` nearest neighbors. - Returns the predicted labels for `X_test`. Part 2: Compare Algorithms 1. Create a set of test points `X_test`: ```python X_test = np.array([[0, 0], [2, 2], [-1, -2]]) ``` 2. Using `classify_points`, classify the points in `X_test` using each of the algorithms: \'auto\', \'ball_tree\', \'kd_tree\', \'brute\'. 3. Compare the results and execution times of different algorithms. Expected Output 1. Create the function implementations with appropriate docstrings and comments. 2. Print the distances and indices of nearest neighbors found in Part 1. 3. Print the classified labels for `X_test` using different algorithms. 4. Report the observed differences in results and execution times. Discuss which algorithm performed better and why, considering the dataset size and dimensionality. # Example ```python X_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y_train = np.array([0, 0, 0, 1, 1, 1]) X_test = np.array([[0, 0], [2, 2], [-1, -2]]) # Implement the functions here # Example function call predicted_labels_kd_tree = classify_points(X_train, y_train, X_test, n_neighbors=2, algorithm=\'kd_tree\') # Expected Output distances: ... indices: ... predicted_labels for \'kd_tree\': [0, 1, 0] Execution time for \'kd_tree\': ... Comparative Analysis: - \'kd_tree\' performed better/worse than \'ball_tree\' because... - The computation time for \'brute\' was the most/least because... ```","solution":"import numpy as np from sklearn.neighbors import NearestNeighbors def find_nearest_neighbors(X_train, n_neighbors, algorithm=\'auto\'): Find the nearest neighbors for the given training data. Parameters: X_train (np.ndarray): Training data points. n_neighbors (int): Number of neighbors to find. algorithm (str): Neighbors search algorithm (\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'). Returns: distances (np.ndarray): Distances to the nearest neighbors. indices (np.ndarray): Indices of the nearest neighbors. nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm).fit(X_train) distances, indices = nbrs.kneighbors(X_train) return distances, indices def classify_points(X_train, y_train, X_test, n_neighbors, algorithm=\'auto\'): Classify the points in X_test based on nearest neighbors from X_train. Parameters: X_train (np.ndarray): Training data points. y_train (np.ndarray): Labels for the training data. X_test (np.ndarray): Test data points to classify. n_neighbors (int): Number of neighbors to consider for classification. algorithm (str): Neighbors search algorithm (\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'). Returns: predicted_labels (np.ndarray): Predicted labels for the test data points. nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm=algorithm).fit(X_train) distances, indices = nbrs.kneighbors(X_test) predicted_labels = [] for neighbors in indices: nearest_labels = y_train[neighbors] predicted_label = np.argmax(np.bincount(nearest_labels)) predicted_labels.append(predicted_label) return np.array(predicted_labels) # Synthetic dataset X_train = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) y_train = np.array([0, 0, 0, 1, 1, 1]) X_test = np.array([[0, 0], [2, 2], [-1, -2]]) # Example classification with different algorithms predicted_labels_auto = classify_points(X_train, y_train, X_test, n_neighbors=2, algorithm=\'auto\') predicted_labels_ball_tree = classify_points(X_train, y_train, X_test, n_neighbors=2, algorithm=\'ball_tree\') predicted_labels_kd_tree = classify_points(X_train, y_train, X_test, n_neighbors=2, algorithm=\'kd_tree\') predicted_labels_brute = classify_points(X_train, y_train, X_test, n_neighbors=2, algorithm=\'brute\') print(f\\"Predicted labels with \'auto\': {predicted_labels_auto}\\") print(f\\"Predicted labels with \'ball_tree\': {predicted_labels_ball_tree}\\") print(f\\"Predicted labels with \'kd_tree\': {predicted_labels_kd_tree}\\") print(f\\"Predicted labels with \'brute\': {predicted_labels_brute}\\")"},{"question":"**Advanced Python Function Object Manipulation** # Problem Statement: In this task, you are required to create and manipulate Python function objects using a high-level abstraction based on the provided C API documentation of Python\'s function objects. You will define a Python class named `FunctionManipulator` that will leverage Python\'s C API to manage function objects. # Objectives: 1. Implement a method to create a new function object with a given code object and globals. 2. Implement methods to get and set the default argument values of a function. 3. Implement methods to get and set the annotations of a function. # Class Definitions: ```python class FunctionManipulator: def __init__(self): # Any necessary initialization pass def create_function(self, code, globals_dict, qualname=None): Create a new function object with the given code object, globals dictionary, and optionally a qualified name. Parameters: code (types.CodeType): The code object to be used for the function. globals_dict (dict): The global variables accessible to the function. qualname (str, optional): The qualified name of the function. Returns: function: A new Python function object. pass def get_defaults(self, func): Return the default argument values of the function object. Parameters: func (function): The function object. Returns: tuple: A tuple of default argument values or None. pass def set_defaults(self, func, defaults): Set the default argument values for the function object. Parameters: func (function): The function object. defaults (tuple): A tuple of default argument values. Returns: bool: True if successful, False otherwise. pass def get_annotations(self, func): Return the annotations of the function object. Parameters: func (function): The function object. Returns: dict: A dictionary of annotations or None. pass def set_annotations(self, func, annotations): Set the annotations for the function object. Parameters: func (function): The function object. annotations (dict): A dictionary of annotations. Returns: bool: True if successful, False otherwise. pass ``` # Constraints: - You are required to use built-in types and functions as much as possible. - You may not use any external libraries to facilitate the creation or manipulation of function objects. - Ensure exception handling for robustness. # Example Usage: ```python import types # Define a simple code object for testing code = compile(\'def foo(x, y): return x + y\', \'<string>\', \'exec\') globals_dict = {} exec(code, globals_dict) # Manipulate the function func = globals_dict[\'foo\'] manipulator = FunctionManipulator() # Create function new_func = manipulator.create_function(func.__code__, globals_dict) # Get defaults defaults = manipulator.get_defaults(new_func) # Should return None print(defaults) # Set defaults success = manipulator.set_defaults(new_func, (10, 20)) print(success) # Should return True # Get new defaults defaults = manipulator.get_defaults(new_func) print(defaults) # Should return (10, 20) # Set annotations annotations = {\'x\': int, \'y\': int, \'return\': int} success = manipulator.set_annotations(new_func, annotations) print(success) # Should return True # Get annotations new_annotations = manipulator.get_annotations(new_func) print(new_annotations) # Should return {\'x\': int, \'y\': int, \'return\': int} ``` # Notes: - The solution should demonstrate understanding of function object creation, default arguments manipulation, and annotations manipulation. - Pay attention to return values and ensure that they are as specified.","solution":"import types class FunctionManipulator: def __init__(self): pass def create_function(self, code, globals_dict, qualname=None): Create a new function object with the given code object, globals dictionary, and optionally a qualified name. Parameters: code (types.CodeType): The code object to be used for the function. globals_dict (dict): The global variables accessible to the function. qualname (str, optional): The qualified name of the function. Returns: function: A new Python function object. if not isinstance(code, types.CodeType): raise ValueError(\\"code must be a code object\\") func = types.FunctionType(code, globals_dict) if qualname: func.__qualname__ = qualname return func def get_defaults(self, func): Return the default argument values of the function object. Parameters: func (function): The function object. Returns: tuple: A tuple of default argument values or None. if not callable(func): raise ValueError(\\"func must be a callable\\") return func.__defaults__ def set_defaults(self, func, defaults): Set the default argument values for the function object. Parameters: func (function): The function object. defaults (tuple): A tuple of default argument values. Returns: bool: True if successful, False otherwise. if not callable(func): raise ValueError(\\"func must be a callable\\") if not isinstance(defaults, tuple): raise ValueError(\\"defaults must be a tuple\\") func.__defaults__ = defaults return True def get_annotations(self, func): Return the annotations of the function object. Parameters: func (function): The function object. Returns: dict: A dictionary of annotations or None. if not callable(func): raise ValueError(\\"func must be a callable\\") return func.__annotations__ def set_annotations(self, func, annotations): Set the annotations for the function object. Parameters: func (function): The function object. annotations (dict): A dictionary of annotations. Returns: bool: True if successful, False otherwise. if not callable(func): raise ValueError(\\"func must be a callable\\") if not isinstance(annotations, dict): raise ValueError(\\"annotations must be a dictionary\\") func.__annotations__ = annotations return True"},{"question":"You are tasked with writing a Python class that uses the `marshal` module to serialize and deserialize a specific type of complex data structure for internal use within your application. Specifically, you must serialize and deserialize a nested dictionary where the values are either integers, strings, lists of integers, or nested dictionaries of the same nature. Requirements 1. Implement a class `NestedDictSerializer` with the following methods: - `serialize(self, data: dict, file_path: str) -> None`: - **Input**: - `data` (dict): A nested dictionary with values being integers, strings, lists of integers, or nested dictionaries. - `file_path` (str): The path to the file where the serialized data should be saved. - **Output**: None. The method should write the serialized data to the specified file. - `deserialize(self, file_path: str) -> dict`: - **Input**: - `file_path` (str): The path to the file from which to read and deserialize the data. - **Output**: - A nested dictionary of the same format as described above. 2. Ensure the integrity of the data. If the data contains unsupported types, raise a `ValueError` with an appropriate message. 3. Use the `version` parameter of the `marshal` module appropriately to ensure compatibility and performance. Example ```python data = { \'a\': 1, \'b\': \'string\', \'c\': [1, 2, 3], \'d\': { \'e\': 4, \'f\': [5, 6], \'g\': { \'h\': 7 } } } serializer = NestedDictSerializer() serializer.serialize(data, \'data.marshal\') # Deserializing the data deserialized_data = serializer.deserialize(\'data.marshal\') print(deserialized_data) # Expected Output: # { # \'a\': 1, # \'b\': \'string\', # \'c\': [1, 2, 3], # \'d\': { # \'e\': 4, # \'f\': [5, 6], # \'g\': { # \'h\': 7 # } # } # } ``` Constraints - Do not use external libraries for serialization/deserialization; rely solely on the `marshal` module provided by Python. - Assume the file operations (like file opening and closing) are always successful and no file-related errors occur for simplicity. - Ensure that the input data does not contain numerical values outside the supported range of the int type in Python. Notes: - Focus on using the internal functions of the `marshal` module efficiently. - Handle nested structures properly to maintain data integrity. - For versioning, prefer the current version of the `marshal` module. Good luck, and happy coding!","solution":"import marshal class NestedDictSerializer: def __init__(self, version=marshal.version): self.version = version def serialize(self, data: dict, file_path: str) -> None: Serialize the nested dictionary `data` and save it to `file_path`. if not self._validate(data): raise ValueError(\\"Data contains unsupported types\\") with open(file_path, \'wb\') as file: marshal.dump(data, file, self.version) def deserialize(self, file_path: str) -> dict: Deserialize the data from `file_path` and return it as a nested dictionary. with open(file_path, \'rb\') as file: data = marshal.load(file) return data def _validate(self, data): Recursively validate the nested dictionary to ensure all values are either integers, strings, lists of integers, or nested dictionaries. if isinstance(data, dict): for key, value in data.items(): if not isinstance(key, str): return False if not self._validate(value): return False elif isinstance(data, list): if not all(isinstance(item, int) for item in data): return False elif not isinstance(data, (int, str)): return False return True"},{"question":"Objective: You are required to implement a function that processes Unix group database entries using the `grp` module. This function will identify all groups a particular user belongs to based on their username. Description: Write a Python function `find_user_groups(username: str) -> List[str]` that returns a list of group names to which the given user belongs. Function Specification: ```python def find_user_groups(username: str) -> List[str]: Finds and returns the list of group names to which the specified user belongs. Args: username (str): The username for which to find the group memberships. Returns: List[str]: A list of names of groups to which the user belongs. Raises: ValueError: If the username provided is not a string or if it is an empty string. ``` Input: - `username`: a non-empty string representing the user\'s name for which the group memberships need to be found. Output: - A list of strings representing the names of the groups to which the specified user belongs. The list should be in alphabetical order. Constraints: - The `username` must be a valid, non-empty string. If not, the function should raise a `ValueError` with an appropriate error message. - You must make use of the `grp` module functions described in the provided documentation to fetch group entries. - The solution must handle `KeyError` and `TypeError` appropriately. Example: ```python # Assuming the following groups are present in the system # group1: gr_name=\'admin\', gr_passwd=\'\', gr_gid=1001, gr_mem=[\'alice\', \'bob\'] # group2: gr_name=\'users\', gr_passwd=\'\', gr_gid=1002, gr_mem=[\'charlie\', \'alice\'] # group3: gr_name=\'staff\', gr_passwd=\'\', gr_gid=1003, gr_mem=[\'bob\', \'david\'] print(find_user_groups(\'alice\')) # Expected output: [\'admin\', \'users\'] print(find_user_groups(\'bob\')) # Expected output: [\'admin\', \'staff\'] print(find_user_groups(\'eve\')) # Expected output: [] ``` The function should be efficient and capable of handling typical Unix system group entries.","solution":"import grp from typing import List def find_user_groups(username: str) -> List[str]: Finds and returns the list of group names to which the specified user belongs. Args: username (str): The username for which to find the group memberships. Returns: List[str]: A list of names of groups to which the user belongs. Raises: ValueError: If the username provided is not a string or if it is an empty string. if not isinstance(username, str) or not username: raise ValueError(\\"Username must be a non-empty string.\\") user_groups = [] for group in grp.getgrall(): if username in group.gr_mem: user_groups.append(group.gr_name) return sorted(user_groups)"},{"question":"Objective: To evaluate the understanding and application of PyTorch\'s dynamic shapes handling mechanisms, specifically focusing on marking tensors with dynamic dimensions and implementing functions that utilize these capabilities. Problem Statement: You are required to implement a class `DynamicShapeHandler` that manipulates tensors with dynamic shapes using PyTorch. Your class should have the following functionalities: 1. **Initialization**: - Initialize two tensors with given shapes and values. - Mark specific dimensions as dynamic. 2. **Operations**: - Implement a method to concatenate the two tensors along a given dimension. - Implement a conditional operation on the concatenated tensor based on its shape. 3. **Dynamic Shape Tracking**: - Ensure that operations respect and leverage the dynamically marked shapes. # Detailed Requirements: 1. **Class Definition**: ```python class DynamicShapeHandler: def __init__(self, tensor1, tensor2, dynamic_dims): Initialize the class with two tensors and mark specified dimensions as dynamic. :param tensor1: First input tensor (torch.Tensor) :param tensor2: Second input tensor (torch.Tensor) :param dynamic_dims: List of tuples specifying (tensor_index, dim), where tensor_index is 0 or 1 for tensor1 or tensor2, and dim is the dimension to be marked as dynamic. pass def concatenate_tensors(self, dim): Concatenate the two initialized tensors along the specified dimension. :param dim: Integer specifying the dimension to concatenate along. :return: Concatenated tensor (torch.Tensor) pass def conditional_operation(self, tensor): Perform a conditional operation on the concatenated tensor. If the size of the first dimension of the tensor is greater than 2, double the tensor values. Otherwise, add 2 to the tensor values. :param tensor: Input tensor (torch.Tensor) after concatenation. :return: Modified tensor (torch.Tensor) after the conditional operation. pass ``` 2. **Input Constraints**: - The dimensions specified in `dynamic_dims` should be valid for the respective tensors. - The dimension specified in `concatenate_tensors` should be valid for concatenation. 3. **Expected Output**: - The concatenated tensor should have the specified dimension marked as dynamic. - The conditional operation should perform as described based on the size of the concatenated tensor\'s first dimension. 4. **Performance Requirements**: - Utilize `torch._dynamo.mark_dynamic()` to mark dimensions dynamic where appropriate. - Ensure that the conditional operation leverages symbolic shapes for efficient execution. # Example Usage: ```python # Initialize the handler with two tensors and mark specific dimensions as dynamic tensor1 = torch.tensor([[1, 2], [3, 4]]) tensor2 = torch.tensor([[5, 6]]) dynamic_dims = [(0, 0), (1, 0)] handler = DynamicShapeHandler(tensor1, tensor2, dynamic_dims) # Concatenate tensors along the first dimension concat_tensor = handler.concatenate_tensors(dim=0) # Apply conditional operation on the concatenated tensor result_tensor = handler.conditional_operation(concat_tensor) print(result_tensor) ``` # Expected Output: The expected output will vary depending on the size of the concatenated tensor\'s first dimension. For the example given above: - If the concatenated tensor\'s first dimension size is 3 (since both tensors are concatenated along dim=0): ``` tensor([[ 2, 4], [ 6, 8], [10, 12]]) ``` - If the condition changes based on dynamic dimension, ensure your implementation handles it accordingly.","solution":"import torch class DynamicShapeHandler: def __init__(self, tensor1, tensor2, dynamic_dims): Initialize the class with two tensors and mark specified dimensions as dynamic. :param tensor1: First input tensor (torch.Tensor) :param tensor2: Second input tensor (torch.Tensor) :param dynamic_dims: List of tuples specifying (tensor_index, dim), where tensor_index is 0 or 1 for tensor1 or tensor2, and dim is the dimension to be marked as dynamic. self.tensor1 = tensor1 self.tensor2 = tensor2 for tensor_index, dim in dynamic_dims: if tensor_index == 0: torch._dynamo.mark_dynamic(self.tensor1, dim) elif tensor_index == 1: torch._dynamo.mark_dynamic(self.tensor2, dim) def concatenate_tensors(self, dim): Concatenate the two initialized tensors along the specified dimension. :param dim: Integer specifying the dimension to concatenate along. :return: Concatenated tensor (torch.Tensor) concat_tensor = torch.cat((self.tensor1, self.tensor2), dim=dim) return concat_tensor def conditional_operation(self, tensor): Perform a conditional operation on the concatenated tensor. If the size of the first dimension of the tensor is greater than 2, double the tensor values. Otherwise, add 2 to the tensor values. :param tensor: Input tensor (torch.Tensor) after concatenation. :return: Modified tensor (torch.Tensor) after the conditional operation. if tensor.size(0) > 2: return tensor * 2 else: return tensor + 2"},{"question":"You are working on a secure messaging application where data integrity and authenticity are critical. Your task is to implement a function that authenticates and verifies a list of messages using HMAC. Function Signature ```python def authenticate_messages(key: bytes, messages: list[str], digestmod: str) -> list[str]: Authenticates and verifies a list of messages using HMAC. Parameters: - key (bytes): The secret key for HMAC. - messages (list[str]): List of messages to be authenticated and verified. - digestmod (str): Digest algorithm to be used (e.g., \'sha256\'). Returns: - list[str]: List of hexadecimal digests corresponding to each message. ``` Requirements 1. **Input:** - `key`: A bytes object representing the secret key used for HMAC. - `messages`: A list of strings where each string is a message that needs to be authenticated. - `digestmod`: A string indicating the digest algorithm to be used. 2. **Output:** - A list of strings where each string is the hexadecimal digest of the corresponding message in the `messages` list. 3. **Constraints:** - The `key` should be a non-empty bytes object. - The `messages` list should contain at least one message. - The `digestmod` must be a valid digest recognized by Python\'s `hashlib`. 4. **Performance Requirements:** - The solution should handle large messages and large lists efficiently. - Use the optimized `hmac.digest` method where applicable for performance benefits. Example ```python key = b\'secret_key\' messages = [\\"Hello, World!\\", \\"Secure Message\\", \\"Another Message\\"] digestmod = \'sha256\' # Expected output should be the list of hex digests for each message result = authenticate_messages(key, messages, digestmod) ``` Hints: - Utilize the `hmac` module methods and functions. - Ensure the solution is secure, avoiding common pitfalls such as using direct string comparison. Implementation Write the `authenticate_messages` function to meet the above requirements.","solution":"import hmac import hashlib def authenticate_messages(key: bytes, messages: list[str], digestmod: str) -> list[str]: Authenticates and verifies a list of messages using HMAC. Parameters: - key (bytes): The secret key for HMAC. - messages (list[str]): List of messages to be authenticated and verified. - digestmod (str): Digest algorithm to be used (e.g., \'sha256\'). Returns: - list[str]: List of hexadecimal digests corresponding to each message. digests = [] for message in messages: h = hmac.new(key, message.encode(), digestmod) digests.append(h.hexdigest()) return digests"},{"question":"# Python Coding Assessment Question Problem Statement: Write a Python function `generate_triangle_numbers(n: int) -> str` that generates the first `n` triangle numbers and returns them as a single string separated by commas. A triangle number or triangular number counts objects arranged in an equilateral triangle. The nth triangle number is the sum of the first n natural numbers. The sequence of triangle numbers is 1, 3, 6, 10, 15, 21, etc. The function should follow these constraints: - The input `n` is guaranteed to be a positive integer (1 <= n <= 1000). Function Signature: ```python def generate_triangle_numbers(n: int) -> str: ``` Input: - `n` (int): A positive integer representing the number of triangle numbers to generate. Output: - str: A string of the first `n` triangle numbers separated by commas. Example: ```python generate_triangle_numbers(5) ``` **Expected Output:** ``` \\"1, 3, 6, 10, 15\\" ``` Notes: 1. You are encouraged to use a loop to compute and accumulate the triangle numbers. 2. Make sure to handle string formatting properly to ensure the numbers are separated by commas without any trailing comma at the end. # Requirements: - Basic familiarity with arithmetic operations, lists, strings, loops. - Understanding how to format strings in Python. - Efficiently handle up to 1000 elements within the given constraints.","solution":"def generate_triangle_numbers(n: int) -> str: Generates the first n triangle numbers and returns them as a single string separated by commas. triangle_numbers = [] for i in range(1, n + 1): triangle_number = int(i * (i + 1) / 2) triangle_numbers.append(str(triangle_number)) return \\", \\".join(triangle_numbers)"},{"question":"# Seaborn Coding Assessment Objective: To evaluate your understanding of seaborn.objects for visualizing distributions, manipulating histogram bins, normalization, and handling multiple groups. Task: Using the seaborn library and its `objects` interface, write a Python function `create_histogram_plot` that generates a distribution plot based on the following requirements: 1. **Data**: Use the `penguins` dataset from seaborn. 2. **Plot type**: Histogram. 3. **Single variable**: Plot the distribution of `\\"flipper_length_mm\\"`. 4. **Bin granularity**: The function should allow the user to specify either the total number of bins or the bin width. 5. **Normalization**: An option to normalize the distribution to show proportions instead of counts. 6. **Grouping**: An option to generate separate histograms for the sexes (`\\"sex\\"` column) and normalize them either globally or within each group. 7. **Visual style**: An option to use either Bars or Area as the plot mark. Function Signature: ```python def create_histogram_plot( bin_info: dict, normalize: bool, grouping: bool, common_norm: bool, plot_mark: str ) -> None: ``` Parameters: - `bin_info` (dict): - A dictionary with either `\\"bins\\"` or `\\"binwidth\\"` key to specify the bin granularity. E.g., `{\\"bins\\": 20}` or `{\\"binwidth\\": 5}`. - `normalize` (bool): - If `True`, normalize the histogram to show proportions. - If `False`, show raw counts. - `grouping` (bool): - If `True`, produce separate histograms for the sexes. - If `False`, produce a single histogram for all data. - `common_norm` (bool): - If `True`, apply normalization across all groups. - If `False`, apply normalization within each group. - `plot_mark` (str): - Either `\\"Bars\\"` or `\\"Area\\"` indicating the visual style of the histogram. Constraints and Notes: - If `grouping` is `False`, `common_norm` should have no effect. - Raise a `ValueError` if `bin_info` does not contain the expected key. - Valid values for `plot_mark` are `Bars` and `Area`. - Any other unexpected values passed as parameters should also raise a `ValueError`. Implementation: ```python import seaborn.objects as so from seaborn import load_dataset def create_histogram_plot(bin_info, normalize, grouping, common_norm, plot_mark): # Load the dataset penguins = load_dataset(\\"penguins\\") # Basic plot setup p = so.Plot(penguins, \\"flipper_length_mm\\") # Set bin granularity if \\"bins\\" in bin_info: hist_args = {\\"bins\\": bin_info[\\"bins\\"]} elif \\"binwidth\\" in bin_info: hist_args = {\\"binwidth\\": bin_info[\\"binwidth\\"]} else: raise ValueError(\\"bin_info must contain either \'bins\' or \'binwidth\' key\\") # Set normalization if normalize: hist_args[\\"stat\\"] = \\"proportion\\" # Handling grouping if grouping: p = p.facet(\\"sex\\") if normalize and not common_norm: hist_args[\\"common_norm\\"] = False # Set plot mark if plot_mark == \\"Bars\\": p.add(so.Bars(), so.Hist(**hist_args)) elif plot_mark == \\"Area\\": p.add(so.Area(), so.Hist(**hist_args)) else: raise ValueError(\\"Invalid plot_mark value. Choose \'Bars\' or \'Area\'\\") # Show plot p.show() ``` Write a test case to demonstrate that the function performs correctly. This will help verify that your function meets the specified requirements. # Example Test Case: ```python # Test case: generate 20-bin histogram, proportion normalized, grouped by sex, using \'Area\' mark create_histogram_plot(bin_info={\\"bins\\": 20}, normalize=True, grouping=True, common_norm=False, plot_mark=\\"Area\\") ```","solution":"import seaborn.objects as so from seaborn import load_dataset def create_histogram_plot(bin_info, normalize, grouping, common_norm, plot_mark): Generate a histogram plot for the \'flipper_length_mm\' feature from the penguins dataset. Parameters: - bin_info (dict): Dictionary with either \\"bins\\" or \\"binwidth\\" to specify the bin granularity. - normalize (bool): Whether to normalize the histogram or not. - grouping (bool): Whether to generate separate histograms for the sexes. - common_norm (bool): If grouping is True, apply normalization globally or within each group. - plot_mark (str): Either \\"Bars\\" or \\"Area\\" indicating the visual style of the histogram. # Load the dataset penguins = load_dataset(\\"penguins\\") # Check bin_info contents if \\"bins\\" in bin_info: hist_args = {\\"bins\\": bin_info[\\"bins\\"]} elif \\"binwidth\\" in bin_info: hist_args = {\\"binwidth\\": bin_info[\\"binwidth\\"]} else: raise ValueError(\\"bin_info must contain either \'bins\' or \'binwidth\' key\\") # Set normalization if normalize: hist_args[\\"stat\\"] = \\"proportion\\" # Basic plot setup p = so.Plot(penguins, x=\\"flipper_length_mm\\") # Handling grouping if grouping: p = p.facet(\\"sex\\") if normalize and not common_norm: hist_args[\\"common_norm\\"] = False # Set plot mark if plot_mark == \\"Bars\\": p = p.add(so.Bars(), so.Hist(**hist_args)) elif plot_mark == \\"Area\\": p = p.add(so.Area(), so.Hist(**hist_args)) else: raise ValueError(\\"Invalid plot_mark value. Choose \'Bars\' or \'Area\'\\") # Show plot p.show()"},{"question":"# Custom Asyncio Policy and Child Watcher Background: You are required to design a custom asyncio event loop policy that includes a specific child process watcher. - **Custom Event Loop Policy**: Subclass the `DefaultEventLoopPolicy` to create a custom event loop policy. - **Child Process Watcher**: Implement a customized child process watcher by subclassing one of the provided child watchers. For demonstration purposes, you\'d use a `ThreadedChildWatcher`. Task: 1. **Define a Custom Policy**: - Create `MyEventLoopPolicy` subclassing `asyncio.DefaultEventLoopPolicy`. - Override the `get_event_loop` method to perform additional logging each time an event loop is retrieved. 2. **Define a Custom Child Watcher**: - Create `MyChildWatcher` subclassing `asyncio.ThreadedChildWatcher`. - Override the `add_child_handler` method to include a print statement every time a new child handler is added. 3. **Implementation and Usage**: - Set `MyEventLoopPolicy` as the current event loop policy. - Demonstrate creating and setting `MyChildWatcher` as the child watcher. - Create a simple asynchronous function that spawns a subprocess and waits for its termination to demonstrate the watcher in action. Expected Code Structure: - Functions/Classes to be implemented: - `MyEventLoopPolicy` - `MyChildWatcher` - A simple asynchronous function to spawn subprocess. Example Usage: ```python import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Custom get_event_loop called\\") return loop class MyChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) print(f\\"Child handler added for PID: {pid}\\") async def main(): policy = MyEventLoopPolicy() asyncio.set_event_loop_policy(policy) watcher = MyChildWatcher() asyncio.get_child_watcher().attach_loop(asyncio.get_event_loop()) asyncio.set_child_watcher(watcher) cmd = [\'echo\', \'Hello, World!\'] process = await asyncio.create_subprocess_exec(*cmd) await process.communicate() # Run the main function asyncio.run(main()) ``` **Constraints**: - Ensure your custom watcher inherits from `ThreadedChildWatcher`. - Your custom policy and watcher must be integrated and demonstrated within the asynchronous function.","solution":"import asyncio class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy): def get_event_loop(self): loop = super().get_event_loop() print(\\"Custom get_event_loop called\\") return loop class MyChildWatcher(asyncio.ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): super().add_child_handler(pid, callback, *args) print(f\\"Child handler added for PID: {pid}\\") async def run_subprocess(): # This function demonstrates spawning a subprocess and using our custom policy and watcher. cmd = [\'echo\', \'Hello, World!\'] process = await asyncio.create_subprocess_exec(*cmd) await process.communicate() async def main(): policy = MyEventLoopPolicy() asyncio.set_event_loop_policy(policy) watcher = MyChildWatcher() loop = asyncio.get_event_loop() watcher.attach_loop(loop) asyncio.set_child_watcher(watcher) await run_subprocess() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Python Coding Assessment Question **Title: Temporary File and Directory Management** **Objective:** Your task is to create temporary files and directories using the `tempfile` module, write to these files, read from them, and ensure they are cleaned up properly. This will assess your ability to utilize context managers and manage temporary resources effectively. **Description:** 1. Create a temporary directory. 2. Within this directory, create a temporary file, write some data to it, read back the data, and ensure the file gets deleted after reading. 3. Create another temporary file that persists after closing. Open it, write data, close it, re-open it to read the data, and then delete it manually. 4. Finally, demonstrate the use of `SpooledTemporaryFile`, writing data to it, causing it to rollover to disk (if it exceeds a size threshold), and then reading the data back. **Requirements:** 1. The data written to the files should be \\"Hello, Temporary File!\\". 2. Ensure that all temporary files and directories are properly cleaned up. 3. Use both high-level (`TemporaryFile`, `NamedTemporaryFile`, `TemporaryDirectory`) and low-level (`mkstemp`, `mkdtemp`) `tempfile` functions. **Input/Output:** - No explicit input required, but method signatures should define necessary parameters to adjust file and directory handling. - Output should demonstrate the successful writing, reading, and cleanup of temporary resources. **Constraints:** - Ensure proper use of context managers for resource cleanup. - Handle potential exceptions during file operations gracefully. **Example:** Implement functions as specified below and ensure they are called in a reasonable sequence: ```python import tempfile import os def manage_temp_files_and_dirs(): # 1. Create a temporary directory with tempfile.TemporaryDirectory() as tmpdirname: print(f\\"Created temporary directory: {tmpdirname}\\") # 2. Create and handle a temporary file within this directory with tempfile.NamedTemporaryFile(dir=tmpdirname, delete=True) as tmpfile: print(f\\"Created temporary file: {tmpfile.name}\\") tmpfile.write(b\\"Hello, Temporary File!\\") tmpfile.seek(0) print(tmpfile.read()) # 3. Create a temporary file that persists after closing tmpfile_persistent = tempfile.NamedTemporaryFile(delete=False) try: print(f\\"Created persistent temporary file: {tmpfile_persistent.name}\\") tmpfile_persistent.write(b\\"Hello, Temporary File!\\") tmpfile_persistent.close() with open(tmpfile_persistent.name, \'rb\') as f: print(f.read()) finally: if os.path.isfile(tmpfile_persistent.name): os.remove(tmpfile_persistent.name) print(f\\"Deleted persistent temporary file: {tmpfile_persistent.name}\\") # 4. Demonstrate the use of SpooledTemporaryFile with tempfile.SpooledTemporaryFile(max_size=10) as spooled_file: spooled_file.write(b\\"Hello, Temporary File!\\") spooled_file.seek(0) print(spooled_file.read()) # Call the function to execute the process manage_temp_files_and_dirs() ``` Ensure that your implementation appropriately prints the status and handles all temporary resources in a secure and efficient manner.","solution":"import tempfile import os def manage_temp_files_and_dirs(): Manage temporary files and directories: 1. Create a temporary directory. 2. Create a temporary file within this directory, write and read data. 3. Create a persistent temporary file, write, close, reopen and read, then delete it. 4. Use SpooledTemporaryFile to manage data in memory, forcing rollover to disk. # 1. Create a temporary directory with tempfile.TemporaryDirectory() as tmpdirname: print(f\\"Created temporary directory: {tmpdirname}\\") # 2. Create and handle a temporary file within this directory with tempfile.NamedTemporaryFile(dir=tmpdirname, delete=True) as tmpfile: print(f\\"Created temporary file: {tmpfile.name}\\") tmpfile.write(b\\"Hello, Temporary File!\\") tmpfile.seek(0) print(tmpfile.read()) # 3. Create a temporary file that persists after closing tmpfile_persistent = tempfile.NamedTemporaryFile(delete=False) try: print(f\\"Created persistent temporary file: {tmpfile_persistent.name}\\") tmpfile_persistent.write(b\\"Hello, Temporary File!\\") tmpfile_persistent.close() with open(tmpfile_persistent.name, \'rb\') as f: print(f.read()) finally: if os.path.isfile(tmpfile_persistent.name): os.remove(tmpfile_persistent.name) print(f\\"Deleted persistent temporary file: {tmpfile_persistent.name}\\") # 4. Demonstrate the use of SpooledTemporaryFile with tempfile.SpooledTemporaryFile(max_size=10) as spooled_file: spooled_file.write(b\\"Hello, Temporary File!\\") spooled_file.seek(0) print(spooled_file.read()) # Call the function to execute the process manage_temp_files_and_dirs()"},{"question":"**Objective:** Implement a function that performs multiple operations on a set and frozenset, demonstrating understanding of fundamental and advanced set manipulation using Python\'s set API. **Task:** You need to implement a Python function named `process_sets` that takes two arguments: a list of operations and an iterable. The function should perform the specified operations on a set created from the iterable and return the final state of the set and a frozenset created from the same iterable. Each operation in the list is a dictionary that includes: - An \'action\' key, which specifies the operation to be performed (add, discard, pop, clear). - A \'value\' key, which specifies the value to be used in the operation (for add and discard). **Input:** - `operations` (list of dictionaries): List of operations to perform on the set. - Each operation is a dictionary with: - \'action\' (str): One of \'add\', \'discard\', \'pop\', \'clear\' - \'value\' (any type): Value to add to or discard from the set. Ignored for \'pop\' and \'clear\'. - `iterable` (iterable): An iterable to create the initial set and frozenset. **Output:** - tuple: A tuple containing the final state of the set and the frozenset. - The first element is the set after all operations are performed. - The second element is the frozenset created from the initial iterable. **Constraints:** - The input iterable will contain hashable elements. - \'pop\' operations will never be called on an empty set. **Function Signature:** ```python def process_sets(operations: list, iterable) -> tuple: ``` **Example:** ```python operations = [{\'action\': \'add\', \'value\': 5}, {\'action\': \'discard\', \'value\': 3}, {\'action\': \'pop\'}, {\'action\': \'clear\'}] iterable = [1, 2, 3, 4] result = process_sets(operations, iterable) # Expected result: (set(), frozenset([1, 2, 3, 4])) ``` **Notes:** - Implement the function considering potential errors and exceptions as described in the documentation. - Ensure the function behaves correctly for both mutable sets and immutable frozensets. - Use the provided API functions and macros wherever applicable to demonstrate your understanding of the set handling as per the documentation.","solution":"def process_sets(operations, iterable): Perform a series of operations on a set derived from an iterable and return the final set and a frozenset created from the same iterable. Args: operations (list of dict): A list of operations to perform on the set. iterable (iterable): An iterable to create the initial set and frozenset. Returns: tuple: A tuple containing the final state of the set and the frozenset. s = set(iterable) fs = frozenset(iterable) for operation in operations: action = operation.get(\'action\') value = operation.get(\'value\', None) if action == \'add\' and value is not None: s.add(value) elif action == \'discard\' and value is not None: s.discard(value) elif action == \'pop\': s.pop() elif action == \'clear\': s.clear() return (s, fs)"},{"question":"# Question: Implementing and Recording Custom Distributed Events in PyTorch You are working with PyTorch\'s distributed elastic module and need to implement a mechanism that records and manages custom events in a distributed environment. Your task is to perform the following steps: 1. **Define a Custom Event**: Create a custom event class that extends the provided `Event` class. Your custom event should include additional metadata fields such as `timestamp` and `node_id`. 2. **Record the Custom Event**: Implement a function `record_custom_event(event)` that records your custom event using the `record` function from the events module. 3. **Fetch and Update Event Metadata**: - Implement a function `fetch_event_metadata(event)` that retrieves the metadata of an event. - Implement a function `update_event_metadata(event, key, value)` that updates the metadata of the event with a new key-value pair. 4. **Simulate Event Recording**: - Create at least three sample events with different metadata and record them. - Fetch and print the metadata of each event. - Update the metadata of one event and reprint its metadata. # Implementation Details - Define your event class as `CustomEvent`. - The `record_custom_event(event)` function should take an instance of `CustomEvent` and use the `torch.distributed.elastic.events.record` function to record it. - The `fetch_event_metadata(event)` function should return the metadata dictionary of an event. - The `update_event_metadata(event, key, value)` function should update the provided key-value pair in the event\'s metadata and return the updated event. # Input and Output Specifications - The `CustomEvent` class should initiate with metadata including `timestamp` and `node_id`. - The functions should print and return the relevant information (events recorded, metadata fetched, metadata updated). # Example Usage ```python from datetime import datetime timestamp = datetime.now().isoformat() event1 = CustomEvent(metadata={\'timestamp\': timestamp, \'node_id\': \'node_1\'}) event2 = CustomEvent(metadata={\'timestamp\': timestamp, \'node_id\': \'node_2\'}) event3 = CustomEvent(metadata={\'timestamp\': timestamp, \'node_id\': \'node_3\'}) record_custom_event(event1) record_custom_event(event2) record_custom_event(event3) print(fetch_event_metadata(event1)) print(fetch_event_metadata(event2)) print(fetch_event_metadata(event3)) update_event_metadata(event1, \'status\', \'completed\') print(fetch_event_metadata(event1)) ``` Ensure your implementation passes the example usage provided. # Constraints - Use only the provided PyTorch distributed elastic events API methods for recording and managing events. - You are expected to handle common exceptions that may occur during the recording and metadata management processes.","solution":"from datetime import datetime import torch.distributed.elastic.events as events from torch.distributed.elastic.events import Event class CustomEvent(Event): def __init__(self, name: str, metadata: dict): self.name = name self.metadata = metadata def record_custom_event(event: CustomEvent): try: events.record(event) except Exception as e: print(f\\"Failed to record event: {e}\\") def fetch_event_metadata(event: CustomEvent): try: return event.metadata except Exception as e: print(f\\"Failed to fetch event metadata: {e}\\") return None def update_event_metadata(event: CustomEvent, key: str, value): try: event.metadata[key] = value return event except Exception as e: print(f\\"Failed to update event metadata: {e}\\") return None"},{"question":"**Objective:** Write a PyTorch function using the `torch.xpu` module to perform the following tasks: 1. Check if an XPU device is available and initialize it. 2. Set a specific XPU device (e.g., device 0). 3. Configure and manage streams to perform two independent PyTorch tensor operations concurrently using streams. 4. Monitor and optimize memory usage during these operations. # Function Specification: ```python def xpu_tensor_operations(): Perform tensor operations on an XPU device with stream management and memory optimization. Returns: A tuple containing: - Result of tensor addition. - Memory usage statistics before and after operations (as a dictionary). pass ``` # Detailed Requirements: 1. **Check XPU Availability:** - Use `torch.xpu.is_available()` to check if an XPU is available. If not, return an appropriate message. 2. **Initialize and Set Device:** - Initialize the device using `torch.xpu.init()`. - Set the device to device 0 using `torch.xpu.set_device(0)`. 3. **Stream Management:** - Create two `torch.xpu.Stream` objects. Perform two tensor operations in parallel using these streams: - Stream 1: Add two tensors of size (1000, 1000). - Stream 2: Multiply two tensors of size (1000, 1000). - Synchronize the streams to ensure that both operations are completed. 4. **Memory Management:** - Before starting the tensor operations, capture the memory usage statistics using `torch.xpu.memory_stats_as_nested_dict()`. - Perform the tensor operations within the streams. - After the operations are completed, capture the memory usage statistics again. - Return the result of the tensor addition operation and the memory usage statistics before and after the operations as a dictionary. # Constraints: - Ensure that streams are used effectively to perform operations concurrently. - Optimize memory usage and manage the cache effectively using available memory management functions like `torch.xpu.empty_cache()` if needed. # Example Output: ```python result_addition, memory_stats = xpu_tensor_operations() print(result_addition.shape) # Should output torch.Size([1000, 1000]) print(memory_stats) # Output format of memory_stats: # { # \'before\': {\'allocated\': ..., \'reserved\': ..., ...}, # \'after\': {\'allocated\': ..., \'reserved\': ..., ...}, # } ``` Ensure correct usage of `torch.xpu` functions and proper stream synchronization to optimize the computational process on the XPU device.","solution":"import torch def xpu_tensor_operations(): Perform tensor operations on an XPU device with stream management and memory optimization. Returns: A tuple containing: - Result of tensor addition. - Memory usage statistics before and after operations (as a dictionary). if not torch.xpu.is_available(): return \\"XPU device is not available\\", {} torch.xpu.set_device(0) # Memory stats before operations mem_stats_before = torch.xpu.memory_stats_as_nested_dict(0) # Initialize tensors t1 = torch.randn((1000, 1000), device=\'xpu\') t2 = torch.randn((1000, 1000), device=\'xpu\') # Create streams stream1 = torch.xpu.Stream() stream2 = torch.xpu.Stream() # Perform operations in streams with torch.xpu.stream(stream1): result_add = t1 + t2 with torch.xpu.stream(stream2): result_mul = t1 * t2 # Synchronize streams stream1.synchronize() stream2.synchronize() # Memory stats after operations mem_stats_after = torch.xpu.memory_stats_as_nested_dict(0) return result_add, { \'before\': mem_stats_before, \'after\': mem_stats_after }"},{"question":"Create a Python class to manage a library system using the `dataclasses` module. The class should encapsulate various functionalities to handle books and members of the library. Implement the following: 1. **Book Class**: - Fields: `title` (str), `author` (str), `isbn` (str), `quantity` (int, default=1). - Methods: - `__post_init__()`: Ensure `isbn` is unique for each book. - `add_quantity(quantity: int)`: Increase the quantity of the book. - `lend_book()`: Decrease the quantity of the book by 1 if available, raise an `Exception` if no copies are available. 2. **Member Class**: - Fields: `name` (str), `member_id` (int), `books_borrowed` (list of Book, default empty list). - Methods: - `borrow_book(book: Book)`: Add a book to the list of borrowed books. - `return_book(book: Book)`: Remove a book from the list of borrowed books, raise an `Exception` if the book is not found. 3. **Library Class**: - Fields: `books` (list of Book, default empty list), `members` (list of Member, default empty list). - Methods: - `add_book(book: Book)`: Add a new book to the library. - `register_member(member: Member)`: Register a new member in the library. - `lend_book_to_member(member_id: int, book_isbn: str)`: Lend a book to a member by decreasing the quantity in the library and adding it to the member’s borrowed list. - `accept_returned_book(member_id: int, book_isbn: str)`: Accept a returned book by increasing the quantity in the library and removing it from the member’s borrowed list. **Constraints**: - Each `isbn` should be unique across all books in the library. - The `member_id` should be unique for each member. - Handling of errors through appropriate exceptions is necessary. Write the complete implementation of these classes using `dataclasses` and functions. Also, demonstrate the functionality with examples.","solution":"from dataclasses import dataclass, field @dataclass class Book: title: str author: str isbn: str quantity: int = 1 def __post_init__(self): if self.quantity < 0: raise ValueError(\\"Quantity cannot be negative\\") def add_quantity(self, quantity: int): self.quantity += quantity def lend_book(self): if self.quantity <= 0: raise Exception(\\"No copies available to lend\\") self.quantity -= 1 @dataclass class Member: name: str member_id: int books_borrowed: list = field(default_factory=list) def borrow_book(self, book: Book): self.books_borrowed.append(book) def return_book(self, book: Book): if book not in self.books_borrowed: raise Exception(\\"Book not found in borrowed books\\") self.books_borrowed.remove(book) @dataclass class Library: books: list = field(default_factory=list) members: list = field(default_factory=list) _book_isbn_set: set = field(default_factory=set, init=False, repr=False) _member_id_set: set = field(default_factory=set, init=False, repr=False) def add_book(self, book: Book): if book.isbn in self._book_isbn_set: raise Exception(\\"Book with this ISBN already exists in the library\\") self.books.append(book) self._book_isbn_set.add(book.isbn) def register_member(self, member: Member): if member.member_id in self._member_id_set: raise Exception(\\"Member with this ID already exists in the library\\") self.members.append(member) self._member_id_set.add(member.member_id) def lend_book_to_member(self, member_id: int, book_isbn: str): member = next((m for m in self.members if m.member_id == member_id), None) if not member: raise Exception(\\"Member not found\\") book = next((b for b in self.books if b.isbn == book_isbn), None) if not book: raise Exception(\\"Book not found\\") book.lend_book() member.borrow_book(book) def accept_returned_book(self, member_id: int, book_isbn: str): member = next((m for m in self.members if m.member_id == member_id), None) if not member: raise Exception(\\"Member not found\\") book = next((b for b in self.books if b.isbn == book_isbn), None) if not book: raise Exception(\\"Book not found\\") member.return_book(book) book.add_quantity(1)"},{"question":"# Question: Custom Seaborn Palette Visualization You are tasked with creating a comprehensive visualization using the `seaborn` library that demonstrates your understanding of color palettes and their applications in data visualization. **Objective:** Write Python code to visualize different seaborn color palettes, both continuous and qualitative. Your implementation should display a set of subplots, each showing a gradient or set of colors from a specified colormap. # Requirements: 1. **Input:** A list of tuples where each tuple contains a colormap name and a boolean (`as_cmap`). The boolean indicates whether the colormap should be continuous or discrete. Additionally, an integer specifying the number of colors (for discrete palettes) should be part of any tuple where `as_cmap` is `False`. Example input: ```python input_palettes = [ (\\"viridis\\", True), (\\"viridis\\", False, 8), (\\"Set2\\", False, 10) ] ``` 2. **Output:** Display a set of subplots (one for each colormap specified) with the following properties: - For continuous colormaps (`as_cmap=True`), display a gradient bar. - For discrete color palettes (`as_cmap=False`), display a bar where each segment represents one color in the palette. 3. **Constraints:** - Use `sns.mpl_palette` to obtain the color palettes. - Utilize matplotlib and seaborn for plotting. - Ensure all plots are clearly labeled with the colormap\'s name and whether it is continuous or discrete. 4. **Performance:** Ensure the solution can handle at least 10 different colormaps in a reasonable time frame. # Example ```python import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_palettes(palette_list): fig, axes = plt.subplots(len(palette_list), 1, figsize=(8, 4 * len(palette_list))) if len(palette_list) == 1: axes = [axes] for ax, palette_info in zip(axes, palette_list): name = palette_info[0] as_cmap = palette_info[1] if as_cmap: cmap = sns.mpl_palette(name, as_cmap=True) gradient = np.linspace(0, 1, 256)[:, np.newaxis] ax.imshow(gradient, aspect=\'auto\', cmap=cmap) ax.set_title(f\'{name} (Continuous)\') else: num_colors = palette_info[2] palette = sns.mpl_palette(name, num_colors) for idx, color in enumerate(palette): ax.add_patch(plt.Rectangle((idx, 0), 1, 1, color=color)) ax.set_xlim(0, num_colors) ax.set_title(f\'{name} ({num_colors} Colors)\') ax.axis(\'off\') plt.tight_layout() plt.show() # Example usage input_palettes = [(\\"viridis\\", True), (\\"viridis\\", False, 8), (\\"Set2\\", False, 10)] visualize_palettes(input_palettes) ``` In this example, the function `visualize_palettes` takes a list of colormap specifications and generates the respective visualizations for each one.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def visualize_palettes(palette_list): fig, axes = plt.subplots(len(palette_list), 1, figsize=(8, 4 * len(palette_list))) if len(palette_list) == 1: axes = [axes] # Ensure axes is iterable even if there\'s only one plot for ax, palette_info in zip(axes, palette_list): name = palette_info[0] as_cmap = palette_info[1] if as_cmap: cmap = sns.color_palette(name, as_cmap=True) gradient = np.linspace(0, 1, 256).reshape(1, -1) ax.imshow(gradient, aspect=\'auto\', cmap=cmap) ax.set_title(f\'{name} (Continuous)\') else: num_colors = palette_info[2] palette = sns.color_palette(name, num_colors) for idx, color in enumerate(palette): ax.add_patch(plt.Rectangle((idx, 0), 1, 1, color=color)) ax.set_xlim(0, num_colors) ax.set_ylim(0, 1) ax.set_title(f\'{name} ({num_colors} Colors)\') ax.axis(\'off\') plt.tight_layout() plt.show() # Example usage input_palettes = [(\\"viridis\\", True), (\\"viridis\\", False, 8), (\\"Set2\\", False, 10)] visualize_palettes(input_palettes)"},{"question":"# Secure File Integrity Checker using HMAC You are required to implement a secure file integrity checker using the `hmac` module in Python. This program should read multiple files, compute their HMACs using a secret key and a specified hash algorithm, and verify if these files have been tampered with by comparing the computed HMACs with previously stored values. Implementation Details 1. **Function Name**: `compute_hmac` - **Arguments**: - `filepath` (str): The path to the file for which to compute the HMAC. - `key` (bytes): The secret key used for HMAC computation. - `digestmod` (str): The hash algorithm to use (e.g., \\"sha256\\"). - **Returns**: - `str`: The computed HMAC as a hexadecimal string. - **Behavior**: - Open the file in binary mode and read its contents. - Create an HMAC object using the given key and hash algorithm. - Update the HMAC object with the file contents. - Return the HMAC digest as a hexadecimal string. 2. **Function Name**: `check_file_integrity` - **Arguments**: - `filepath` (str): The path to the file to check. - `stored_hmac` (str): The previously stored HMAC value for comparison. - `key` (bytes): The secret key used for HMAC computation. - `digestmod` (str): The hash algorithm to use (e.g., \\"sha256\\"). - **Returns**: - `bool`: `True` if the computed HMAC matches the stored HMAC, `False` otherwise. - **Behavior**: - Compute the HMAC of the given file using `compute_hmac`. - Compare the computed HMAC with the stored HMAC using `hmac.compare_digest`. - Return `True` if they match, `False` otherwise. Constraints - The hash algorithm specified by `digestmod` must be one of the algorithms supported by the `hashlib` module. - The secret key and the file contents must be securely handled to prevent leakage. - The file contents may be large, so the program should handle reading the file in chunks if necessary. Example Usage ```python # compute_hmac function usage key = b\'secret_key\' file_hmac = compute_hmac(\'example.txt\', key, \'sha256\') print(file_hmac) # Outputs the HMAC of the file as a hexadecimal string # check_file_integrity function usage stored_hmac = \'previously_stored_hmac_value\' is_intact = check_file_integrity(\'example.txt\', stored_hmac, key, \'sha256\') print(is_intact) # Outputs True if file is intact, otherwise False ``` Implement the `compute_hmac` and `check_file_integrity` functions according to the specifications provided.","solution":"import hmac import hashlib def compute_hmac(filepath, key, digestmod): Computes the HMAC of a file. Args: filepath (str): The path to the file. key (bytes): The secret key used for HMAC computation. digestmod (str): The hash algorithm to use (e.g., \\"sha256\\"). Returns: str: The computed HMAC as a hexadecimal string. hash_function = getattr(hashlib, digestmod) hmac_obj = hmac.new(key, digestmod=hash_function) with open(filepath, \'rb\') as file: while chunk := file.read(4096): hmac_obj.update(chunk) return hmac_obj.hexdigest() def check_file_integrity(filepath, stored_hmac, key, digestmod): Checks the file integrity by comparing its HMAC. Args: filepath (str): The path to the file. stored_hmac (str): The previously stored HMAC value. key (bytes): The secret key used for HMAC computation. digestmod (str): The hash algorithm to use (e.g., \\"sha256\\"). Returns: bool: True if the computed HMAC matches the stored HMAC, False otherwise. computed_hmac = compute_hmac(filepath, key, digestmod) return hmac.compare_digest(computed_hmac, stored_hmac)"},{"question":"You are provided with several functions and methods related to CPU management in the PyTorch library. Your task is to implement a function that performs a specific task using these methods. This function should combine the concepts of device counting and context management for synchronization purposes. # Function Signature ```python def synchronize_streams(): This function will perform the following tasks: 1. Check if the CPU is available. 2. If available, obtain the number of CPU devices. 3. For each device: a. Set the device. b. Create a stream. c. Synchronize the stream. 4. Return the number of devices synchronized. Returns: int: Number of devices synchronized. pass ``` # Requirements: 1. **Check CPU availability**: Use `torch.cpu.is_available()`. 2. **Device Count**: Use `torch.cpu.device_count()`. 3. **Set each device and perform synchronization**: - Use `torch.cpu.set_device(device_id)` to set the device. - Create a stream using `torch.cpu.stream()`. - Synchronize the stream using `torch.cpu.synchronize()`. # Constraints: - Ensure the function works only when the CPU is available. - Handle possible exceptions where devices might not be correctly set or synchronized. - Return the number of devices synchronized successfully; if the CPU is not available, return `0`. # Example ```python # Example usage num_synchronized_devices = synchronize_streams() print(f\\"Number of devices synchronized: {num_synchronized_devices}\\") ``` In this example, if the CPU is available and has 4 devices, the function should attempt to set each device, create a stream for it, synchronize it, and then return `4`.","solution":"import torch def synchronize_streams(): This function will perform the following tasks: 1. Check if the CPU is available. 2. If available, obtain the number of CPU devices. 3. For each device: a. Set the device. b. Create a stream. c. Synchronize the stream. 4. Return the number of devices synchronized. Returns: int: Number of devices synchronized. if not torch.cuda.is_available(): return 0 num_devices = torch.cuda.device_count() synchronized_devices = 0 for device_id in range(num_devices): try: torch.cuda.set_device(device_id) stream = torch.cuda.Stream() stream.synchronize() synchronized_devices += 1 except Exception as e: print(f\\"Failed to synchronize on device {device_id}: {e}\\") continue return synchronized_devices"},{"question":"**Problem Statement:** You are tasked with implementing a custom model wrapper that tunes the decision threshold of a classifier optimized for high recall in detecting a specific condition, such as in medical diagnosis (e.g., cancer detection). For this task, you will use the `TunedThresholdClassifierCV` class from scikit-learn and demonstrate its usage with a sample dataset. The main objective is to maximize recall for the positive class while ensuring that your implementation conforms to good coding practices and efficiently utilizes the provided scikit-learn utilities. **Requirements:** 1. **Function Name:** `tune_decision_threshold` 2. **Input Parameters:** - `X_train`: A 2D numpy array or pandas DataFrame representing training feature data. - `y_train`: A 1D numpy array or pandas Series representing training target labels. - `X_test`: A 2D numpy array or pandas DataFrame representing testing feature data. - `y_test`: A 1D numpy array or pandas Series representing testing target labels. 3. **Output:** - A tuple `(threshold, recall_score)`, where `threshold` is the optimal decision threshold, and `recall_score` is the recall of the tuned model on the test set. 4. **Constraints:** - Use `TunedThresholdClassifierCV` with logistic regression as the base model. - Optimize the threshold using the recall score as the metric. 5. **Implementation Details:** - Load the `make_classification` dataset with an imbalance between classes, ensuring a more significant proportion of negative samples. - Fit the model on the training data and determine the optimal threshold. - Predict the labels on the test data using the tuned classifier. - Calculate the recall score for the predicted labels on the test set. **Function Signature:** ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import TunedThresholdClassifierCV from sklearn.metrics import recall_score def tune_decision_threshold(X_train, y_train, X_test, y_test): This function tunes the decision threshold of a logistic regression classifier to maximize recall score. Parameters: X_train (numpy.ndarray or pandas.DataFrame): Training data features. y_train (numpy.ndarray or pandas.Series): Training data labels. X_test (numpy.ndarray or pandas.DataFrame): Testing data features. y_test (numpy.ndarray or pandas.Series): Testing data labels. Returns: tuple: Optimal decision threshold and recall score on the test set. # Define the logistic regression model base_model = LogisticRegression(solver=\'liblinear\') # Define the scorer for recall scorer = make_scorer(recall_score, pos_label=1) # Define the tuned threshold classifier tuned_classifier = TunedThresholdClassifierCV(base_model, scoring=scorer) # Fit the model tuned_classifier.fit(X_train, y_train) # Get the decision threshold threshold = tuned_classifier.best_threshold_ # Predict probabilities on the test set probabilities = tuned_classifier.predict_proba(X_test)[:, 1] # Predict labels with the optimal threshold y_pred = (probabilities >= threshold).astype(int) # Calculate recall score recall = recall_score(y_test, y_pred) return threshold, recall ``` **Example Usage:** ```python # Generate synthetic dataset X, y = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=18) # Split dataset into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Tune decision threshold threshold, recall = tune_decision_threshold(X_train, y_train, X_test, y_test) print(f\\"Optimal Threshold: {threshold}\\") print(f\\"Recall Score: {recall}\\") ```","solution":"from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import make_scorer, recall_score from sklearn.base import BaseEstimator, ClassifierMixin from scipy.optimize import minimize class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_model, scoring): self.base_model = base_model self.scoring = scoring self.best_threshold_ = None def fit(self, X, y): self.base_model.fit(X, y) y_proba = self.base_model.predict_proba(X)[:, 1] def recall_at_threshold(threshold): y_pred = (y_proba >= threshold).astype(int) return -self.scoring._score_func(y, y_pred) optimal = minimize(lambda t: recall_at_threshold(t), x0=[0.5], bounds=[(0,1)]) self.best_threshold_ = optimal.x[0] return self def predict_proba(self, X): return self.base_model.predict_proba(X) def predict(self, X): proba = self.predict_proba(X)[:, 1] return (proba >= self.best_threshold_).astype(int) def tune_decision_threshold(X_train, y_train, X_test, y_test): This function tunes the decision threshold of a logistic regression classifier to maximize recall score. Parameters: X_train (numpy.ndarray or pandas.DataFrame): Training data features. y_train (numpy.ndarray or pandas.Series): Training data labels. X_test (numpy.ndarray or pandas.DataFrame): Testing data features. y_test (numpy.ndarray or pandas.Series): Testing data labels. Returns: tuple: Optimal decision threshold and recall score on the test set. # Define the logistic regression model base_model = LogisticRegression(solver=\'liblinear\') # Define the scorer for recall scorer = make_scorer(recall_score, pos_label=1) # Define the tuned threshold classifier tuned_classifier = TunedThresholdClassifierCV(base_model, scoring=scorer) # Fit the model tuned_classifier.fit(X_train, y_train) # Get the decision threshold threshold = tuned_classifier.best_threshold_ # Predict probabilities on the test set probabilities = tuned_classifier.predict_proba(X_test)[:, 1] # Predict labels with the optimal threshold y_pred = (probabilities >= threshold).astype(int) # Calculate recall score recall = recall_score(y_test, y_pred) return threshold, recall"},{"question":"You are to implement a basic multi-client chat server using the `asyncore` module. This chat server will allow multiple clients to connect to it and broadcast any message received from one client to all the connected clients. This will help you understand how to handle multiple client connections and manage data using asynchronous sockets. # Requirements 1. **ChatServer**: This class will inherit from `asyncore.dispatcher` and will perform the following functions: - Bind to a given host and port. - Listen for incoming connections. - Accept new connections and create a new `ChatHandler` instance for each client. 2. **ChatHandler**: This class will inherit from `asyncore.dispatcher_with_send` and will handle the communication for each client. It must: - Read incoming messages from the client. - Broadcast the messages to all connected clients. # Input and Output Formats - The server should listen on `localhost` and port `9000`. - Each connected client can send messages to the server, which will be broadcast to all other connected clients. - Each message should include the client\'s address as a prefix, as `[client_addr] message`. # Constraints - You can assume that no more than 10 clients will connect to the server at any given time. - Each message from a client will not exceed 256 characters. # Example Your output should look like: 1. When a connection is established: ``` New connection from (client_address) ``` 2. When a message is received and broadcasted: ``` Message from (client_address): Hello, World! ``` # Implementation Implement the `ChatServer` and `ChatHandler` classes according to the requirements. An example entry point to start the server is provided. ```python import asyncore import socket class ChatHandler(asyncore.dispatcher_with_send): all_clients = [] def __init__(self, sock): asyncore.dispatcher_with_send.__init__(self, sock) self.peer_address = self.socket.getpeername() self.__class__.all_clients.append(self) print(f\'New connection from {self.peer_address}\') def handle_read(self): data = self.recv(256) if data: message = f\'[{self.peer_address}] {data.decode(\\"utf-8\\")}\' for client in self.__class__.all_clients: if client is not self: client.send(message.encode(\'utf-8\')) def handle_close(self): self.__class__.all_clients.remove(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): handler = ChatHandler(sock) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 9000) asyncore.loop() ``` This implementation sets up a basic chat server where multiple clients can connect. Each incoming message from any client is broadcast to all the other connected clients.","solution":"import asyncore import socket class ChatHandler(asyncore.dispatcher_with_send): all_clients = [] def __init__(self, sock): asyncore.dispatcher_with_send.__init__(self, sock) self.peer_address = self.socket.getpeername() self.__class__.all_clients.append(self) print(f\'New connection from {self.peer_address}\') def handle_read(self): data = self.recv(256) if data: message = f\'[{self.peer_address}] {data.decode(\\"utf-8\\")}\' for client in self.__class__.all_clients: if client is not self: client.send(message.encode(\'utf-8\')) def handle_close(self): self.__class__.all_clients.remove(self) self.close() class ChatServer(asyncore.dispatcher): def __init__(self, host, port): asyncore.dispatcher.__init__(self) self.create_socket(socket.AF_INET, socket.SOCK_STREAM) self.set_reuse_addr() self.bind((host, port)) self.listen(5) def handle_accepted(self, sock, addr): handler = ChatHandler(sock) if __name__ == \\"__main__\\": server = ChatServer(\'localhost\', 9000) asyncore.loop()"},{"question":"You are tasked with implementing functionality to interact with the `__future__` module in Python. Specifically, you need to create a function that analyzes the features defined in the `__future__` module and determines their optional and mandatory release versions. # Requirements 1. Implement the function `analyze_future_features()`, which: - Takes no arguments. - Returns a list of dictionaries, each containing information about a feature defined in the `__future__` module. 2. Each dictionary in the returned list should have the following structure: ```python { \\"feature_name\\": <str>, \\"optional_release\\": <tuple>, \\"mandatory_release\\": <tuple> or None, \\"compiler_flag\\": <int> } ``` # Input None # Output - A list of dictionaries with the structure specified above. # Constraints - Use the `__future__` module provided by Python. - Do not hardcode feature information; the solution should dynamically query the `__future__` module\'s contents. # Example Here is a possible example of the kind of output your function might produce: ```python [ { \\"feature_name\\": \\"nested_scopes\\", \\"optional_release\\": (2, 1, 0, \\"beta\\", 1), \\"mandatory_release\\": (2, 2, 0, \\"final\\", 0), \\"compiler_flag\\": 8 }, { \\"feature_name\\": \\"generators\\", \\"optional_release\\": (2, 2, 0, \\"alpha\\", 1), \\"mandatory_release\\": (2, 3, 0, \\"final\\", 0), \\"compiler_flag\\": 16 }, ... ] ``` Use this example to guide your implementation. You might not get exactly the same data because the list may change in future versions of Python, but the structure of each dictionary should match the example. # Additional Information You may refer to the official Python documentation for help in understanding how to interact with the `__future__` module programmatically.","solution":"import __future__ def analyze_future_features(): features_info = [] for feature_name in dir(__future__): feature = getattr(__future__, feature_name) if isinstance(feature, __future__._Feature): features_info.append({ \\"feature_name\\": feature_name, \\"optional_release\\": feature.optional, \\"mandatory_release\\": feature.mandatory, \\"compiler_flag\\": feature.compiler_flag }) return features_info"},{"question":"Coding Assessment Question # HTML Entity Converter The task is to implement a utility that can convert between HTML entities and their corresponding Unicode characters using the `html.entities` module. # Part 1: Entity to Character 1. **Function Name**: `entity_to_char` 2. **Input**: A string containing HTML entities. 3. **Output**: A string with all HTML entities converted to their corresponding Unicode characters. 4. **Constraints**: - The input string will only contain valid HTML entities as defined in `html.entities.html5`. - All entities will be fully named (e.g., `&gt;` and not just `&gt`). Example: ```python input_string = \\"Hello &amp; welcome to the world of &lt;Python&gt;!\\" expected_output = \\"Hello & welcome to the world of <Python>!\\" assert entity_to_char(input_string) == expected_output ``` # Part 2: Character to Entity 1. **Function Name**: `char_to_entity` 2. **Input**: A string containing any characters. 3. **Output**: A string with characters converted to their corresponding HTML entities. 4. **Constraints**: - Only characters that have a corresponding HTML entity should be converted. Example: ```python input_string = \\"Hello & welcome to the world of <Python>!\\" expected_output = \\"Hello &amp; welcome to the world of &lt;Python&gt;!\\" assert char_to_entity(input_string) == expected_output ``` # Deliverables: - Implement the `entity_to_char` function. - Implement the `char_to_entity` function. - Write test cases to demonstrate the correctness of your implementation. Note: You are allowed to use the existing mappings provided by the `html.entities` module for your implementations. # Performance Requirements: - Both functions should operate efficiently even if the input string is very large. Good luck!","solution":"import html def entity_to_char(input_string): Converts HTML entities in the input string to their corresponding Unicode characters. return html.unescape(input_string) def char_to_entity(input_string): Converts characters in the input string to their corresponding HTML entities. return html.escape(input_string)"},{"question":"# Objective Implement a function that fetches data from a given URL, extracts specific components of the URL, handles exceptions, and reports the findings. # Task Write a function named `fetch_and_parse_url` that takes a single argument, `url` (a string). The function should: 1. Attempt to open the URL and read its contents. 2. If successful, return a dictionary containing: - `url`: The full URL. - `scheme`: The URL scheme (e.g., `http`). - `hostname`: The hostname of the URL. - `path`: The URL path. - `content`: The first 100 characters of the content from the URL. 3. Handle cases where the URL cannot be opened due to network issues, invalid URL, or other reasons. Return a dictionary containing: - `url`: The full URL. - `error`: The error message. # Input Format - A single string representing the URL. # Output Format - A dictionary with the specified structure. # Constraints - Use the `urllib` package as needed. - Ensure the code is robust and handles various exceptions gracefully. # Examples ```python def fetch_and_parse_url(url: str) -> dict: # Write your code here # Example 1 url = \\"http://www.example.com/\\" result = fetch_and_parse_url(url) print(result) # Output (the content may vary): # { # \'url\': \'http://www.example.com/\', # \'scheme\': \'http\', # \'hostname\': \'www.example.com\', # \'path\': \'/\', # \'content\': \'<!doctype html>n<html>n<head>n <title>Example Domain</title>...\' # } # Example 2 url = \\"http://invalid-url/\\" result = fetch_and_parse_url(url) print(result) # Output: # { # \'url\': \'http://invalid-url/\', # \'error\': \'URL cannot be opened due to error X (specific error message)\' # } ``` # Note - Ensure that you cover all potential exceptions that can be raised when attempting to open a URL.","solution":"import urllib.request import urllib.error from urllib.parse import urlparse def fetch_and_parse_url(url: str) -> dict: Fetches and parses a URL. :param url: URL to fetch and parse :return: Dictionary with URL components and content or error try: # Parse the URL parsed_url = urlparse(url) # Fetch the content from the URL with urllib.request.urlopen(url) as response: content = response.read(100).decode(\'utf-8\') # Construct and return the result dictionary return { \'url\': url, \'scheme\': parsed_url.scheme, \'hostname\': parsed_url.hostname, \'path\': parsed_url.path, \'content\': content } except Exception as e: # Handle exceptions and return the error message return { \'url\': url, \'error\': str(e) }"},{"question":"Coding Assessment Question # Objective Your task is to implement a function that combines several `itertools` iterators to achieve a specific result. This will test your understanding of how to use and combine these iterators efficiently. # Problem Statement You are given two lists: 1. `data`: A list of integers. 2. `selectors`: A list of boolean values of the same length as `data`. Your task is to: 1. `compress` the elements of `data` using `selectors`, which will filter out the elements where the corresponding `selectors` value is `False`. 2. Once filtered, group consecutive elements that are the same using `groupby`. 3. For each group, generate a tuple containing the element and the length of the group. 4. Return the list of these tuples. # Inputs - `data` (list of integers): The data to be filtered and grouped. - `selectors` (list of booleans): The selectors to filter `data`. # Output - A list of tuples where each tuple contains an element and its consecutive count in the filtered result. # Example ```python data = [1, 2, 2, 3, 3, 3, 4] selectors = [True, False, True, True, False, True, True] # The `compress` step would yield: [1, 2, 3, 3, 4] # The `groupby` step would group elements: [(1, 1), (2, 1), (3, 2), (4, 1)] assert compress_and_group(data, selectors) == [(1, 1), (2, 1), (3, 2), (4, 1)] ``` # Constraints - The length of `selectors` is the same as the length of `data`. - The function should be efficient in terms of both time and memory. # Function Signature ```python def compress_and_group(data: list[int], selectors: list[bool]) -> list[tuple[int, int]]: pass ``` # Notes Ensure you handle edge cases, such as when `data` is empty or when all elements are filtered out by the `selectors`.","solution":"from itertools import compress, groupby from typing import List, Tuple def compress_and_group(data: List[int], selectors: List[bool]) -> List[Tuple[int, int]]: Filters the data using selectors, then groups consecutive identical elements, and returns a list of tuples containing the element and its consecutive count. # Step 1: Compress the data using selectors filtered_data = list(compress(data, selectors)) # Step 2: Group consecutive elements and count their occurrences result = [(key, len(list(group))) for key, group in groupby(filtered_data)] return result"},{"question":"**Question:** You are working on a Python application that requires an effective and dynamic logging setup. To ensure scalable and maintainable logging configurations, you need to use the `logging.config` module. Your task is to write a Python script to achieve the following: 1. Read a logging configuration from a dictionary. 2. Dynamically create a custom logging handler that sends logs to a file. 3. Configure loggers to use different handlers and log levels. 4. Implement a function to reconfigure the logging using a new dictionary configuration dynamically. # Detailed Requirements: 1. **Initialize Logging Configuration:** - Create an initial logging configuration using a dictionary. - Include at least two handlers: a stream handler that prints logs to the console and a file handler that writes logs to a file named \\"app.log\\". - Set up formatters for both handlers to format log messages with timestamps. - Configure the root logger to use both handlers. 2. **Custom File Handler:** - Implement a custom logging handler class `CustomFileHandler` that extends the `logging.FileHandler` to demonstrate advanced logging functionalities (e.g., write additional context-specific information to logs). 3. **Logger Configuration:** - Add two loggers named `main` and `secondary` in the configuration dictionary. - Set the log level for `main` to `INFO` and for `secondary` to `DEBUG`. - Ensure `main` uses the custom file handler, and `secondary` uses the stream handler. 4. **Reconfigure Logging:** - Implement a function `reconfigure_logging(new_config: dict)` that takes a new dictionary configuration and reconfigures the logging setup without restarting the application. # Constraints: - Your custom `CustomFileHandler` should add information about the current user (use the `getpass` module) in each log message. - Handle possible errors arising from invalid configuration settings and raise appropriate exceptions with descriptive messages. # Input: - Initial configuration in the form of a dictionary. - New configuration dictionary for reconfiguration. # Output: - Log messages printed to the console and written to \\"app.log\\" correctly according to the defined configurations. - The string corresponding to the formatted log messages including the additional context by `CustomFileHandler`. # Example: ```python import logging import logging.config import getpass class CustomFileHandler(logging.FileHandler): def emit(self, record): record.msg = f\\"[User: {getpass.getuser()}] {record.msg}\\" super().emit(record) def setup_initial_logging(): config = { \'version\': 1, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, }, \'handlers\': { \'stream\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'standard\', \'level\': \'DEBUG\', }, \'file\': { \'()\': \'__main__.CustomFileHandler\', \'formatter\': \'standard\', \'filename\': \'app.log\', \'level\': \'INFO\', }, }, \'root\': { \'handlers\': [\'stream\', \'file\'], \'level\': \'DEBUG\', }, \'loggers\': { \'main\': { \'handlers\': [\'file\'], \'level\': \'INFO\', \'propagate\': False, }, \'secondary\': { \'handlers\': [\'stream\'], \'level\': \'DEBUG\', \'propagate\': False, }, } } logging.config.dictConfig(config) def reconfigure_logging(new_config): logging.config.dictConfig(new_config) def main(): # Setup initial logging setup_initial_logging() logger_main = logging.getLogger(\'main\') logger_secondary = logging.getLogger(\'secondary\') logger_main.info(\'This is an info message from the main logger.\') logger_secondary.debug(\'This is a debug message from the secondary logger.\') logger_main.error(\'This is an error message from the main logger.\') # New configuration for reconfiguration new_config = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s [Reconfigured]\' }, }, \'handlers\': { \'stream\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', }, \'file\': { \'()\': \'__main__.CustomFileHandler\', \'formatter\': \'detailed\', \'filename\': \'app.log\', \'level\': \'DEBUG\', }, }, \'root\': { \'handlers\': [\'stream\', \'file\'], \'level\': \'DEBUG\', }, \'loggers\': { \'main\': { \'handlers\': [\'file\'], \'level\': \'DEBUG\', \'propagate\': False, }, \'secondary\': { \'handlers\': [\'stream\'], \'level\': \'DEBUG\', \'propagate\': False, }, } } # Reconfigure logging reconfigure_logging(new_config) logger_main.info(\'This is an info message from the main logger after reconfiguration.\') logger_secondary.debug(\'This is a debug message from the secondary logger after reconfiguration.\') logger_main.error(\'This is an error message from the main logger after reconfiguration.\') if __name__ == \\"__main__\\": main() ``` # Explanation: - Define and initialize a logger configuration with both console and file handlers and appropriate formatters. - Implement a custom file handler that includes user-specific information in log messages. - Configure two loggers (`main` and `secondary`) with distinct log levels and handlers. - Reconfigure the logging settings dynamically using the `reconfigure_logging` function and apply a new configuration. # Notes: - Ensure that logs are output in the expected formats and destinations. - Handle errors and invalid configurations gracefully by raising appropriate exceptions.","solution":"import logging import logging.config import getpass class CustomFileHandler(logging.FileHandler): def emit(self, record): record.msg = f\\"[User: {getpass.getuser()}] {record.msg}\\" super().emit(record) def setup_initial_logging(): config = { \'version\': 1, \'formatters\': { \'standard\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s\' }, }, \'handlers\': { \'stream\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'standard\', \'level\': \'DEBUG\', }, \'file\': { \'()\': CustomFileHandler, \'formatter\': \'standard\', \'filename\': \'app.log\', \'level\': \'INFO\', }, }, \'root\': { \'handlers\': [\'stream\', \'file\'], \'level\': \'DEBUG\', }, \'loggers\': { \'main\': { \'handlers\': [\'file\'], \'level\': \'INFO\', \'propagate\': False, }, \'secondary\': { \'handlers\': [\'stream\'], \'level\': \'DEBUG\', \'propagate\': False, }, } } logging.config.dictConfig(config) def reconfigure_logging(new_config): logging.config.dictConfig(new_config) def main(): setup_initial_logging() logger_main = logging.getLogger(\'main\') logger_secondary = logging.getLogger(\'secondary\') logger_main.info(\'This is an info message from the main logger.\') logger_secondary.debug(\'This is a debug message from the secondary logger.\') logger_main.error(\'This is an error message from the main logger.\') new_config = { \'version\': 1, \'formatters\': { \'detailed\': { \'format\': \'%(asctime)s - %(name)s - %(levelname)s - %(message)s [Reconfigured]\' }, }, \'handlers\': { \'stream\': { \'class\': \'logging.StreamHandler\', \'formatter\': \'detailed\', \'level\': \'DEBUG\', }, \'file\': { \'()\': CustomFileHandler, \'formatter\': \'detailed\', \'filename\': \'app.log\', \'level\': \'DEBUG\', }, }, \'root\': { \'handlers\': [\'stream\', \'file\'], \'level\': \'DEBUG\', }, \'loggers\': { \'main\': { \'handlers\': [\'file\'], \'level\': \'DEBUG\', \'propagate\': False, }, \'secondary\': { \'handlers\': [\'stream\'], \'level\': \'DEBUG\', \'propagate\': False, }, } } reconfigure_logging(new_config) logger_main.info(\'This is an info message from the main logger after reconfiguration.\') logger_secondary.debug(\'This is a debug message from the secondary logger after reconfiguration.\') logger_main.error(\'This is an error message from the main logger after reconfiguration.\') if __name__ == \\"__main__\\": main()"},{"question":"You are tasked with creating a comprehensive data preprocessing pipeline for a machine learning task using scikit-learn. The specific requirements are as follows: 1. **Download a Dataset**: - Use the `fetch_openml` function to download the \\"miceprotein\\" dataset from OpenML. - Ensure you capture both the data matrix (features) and the target vector (labels). 2. **Data Splitting**: - Split the dataset into training and testing sets with a 70-30 split. 3. **Data Preprocessing**: - Convert categorical features to numeric using `OneHotEncoder`. - Standardize numerical features to have zero mean and unit variance using `StandardScaler`. 4. **Pipeline Creation**: - Create a preprocessing pipeline to handle both categorical and numerical features accordingly. This pipeline should use `ColumnTransformer` to apply the appropriate transformations to each category of features. 5. **Model Training**: - Train a `RandomForestClassifier` on the training set using the preprocessing pipeline. - Evaluate the model on the test set and compute the accuracy score. Implement the specified data preprocessing pipeline using scikit-learn. Provide the full implementation including downloading the dataset, preprocessing, pipeline construction, model training, and evaluation. # Expected Input and Output format Function Signature ```python def preprocess_and_train_miceprotein(random_state: int) -> float: # Implement your function here. pass ``` Input - `random_state` (int): The seed for random number generation to ensure reproducibility. Output - `accuracy` (float): The accuracy of the trained model on the test set. # Example Usage ```python accuracy = preprocess_and_train_miceprotein(random_state=42) print(f\\"Accuracy: {accuracy:.2f}\\") ``` # Constraints - Ensure the random seed is applied for reproducibility. - Use appropriate handling for potential missing values in the dataset. - Optimize the pipeline for performance but keep it interpretable. # Notes - You may use helper functions if necessary, but the main logic should be contained within `preprocess_and_train_miceprotein`. - Remember to validate the input formats and handle exceptions where applicable.","solution":"from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.impute import SimpleImputer from sklearn.metrics import accuracy_score import pandas as pd import numpy as np def preprocess_and_train_miceprotein(random_state: int) -> float: # Download the dataset data = fetch_openml(name=\'miceprotein\', version=4, as_frame=True) X, y = data[\'data\'], data[\'target\'] # Split the dataset into training and testing sets with a 70-30 split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y) # Identify categorical and numerical columns categorical_cols = X_train.select_dtypes(include=[\'object\']).columns numerical_cols = X_train.select_dtypes(include=[np.number]).columns # Create transformers for each category of features numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine transformers into a preprocessing pipeline preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_cols), (\'cat\', categorical_transformer, categorical_cols) ] ) # Create the full pipeline with preprocessing and model model = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'classifier\', RandomForestClassifier(random_state=random_state)) ]) # Train the model model.fit(X_train, y_train) # Evaluate the model on the test set y_pred = model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"**Coding Question: Seaborn Context Customization and Plotting** **Objective:** Demonstrate your understanding of setting contexts and customizing seaborn visualizations. **Task:** Write a function named `create_and_customize_plots` that: 1. Takes two lists of equal length as input: `x` and `y`, representing the x-values and y-values for a plot. 2. Creates a seaborn line plot using these values. 3. Sets the plotting context based on an input parameter. 4. Scales the font of the plot as specified by an input parameter. 5. Adjusts the linewidth of the lines in the plot as specified by an input parameter. 6. Returns the created plot. **Function Signature:** ```python import seaborn as sns import matplotlib.pyplot as plt def create_and_customize_plots(x: list, y: list, context: str, font_scale: float, linewidth: float) -> sns.axisgrid.FacetGrid: pass ``` **Inputs:** - `x` and `y`: Lists of numbers representing the x and y values of the line plot, respectively. `x` and `y` will always be of the same length. - `context`: A string indicating the context for the plot (e.g., \\"notebook\\", \\"paper\\", \\"talk\\", \\"poster\\"). - `font_scale`: A float indicating the factor by which to scale the fonts in the plot. - `linewidth`: A float indicating the width of the line in the plot. **Output:** - Returns a seaborn `FacetGrid` object representing the customized plot. **Constraints:** - You must use seaborn\'s `set_context` function to set the plot context. - The different contexts (`notebook`, `paper`, `talk`, `poster`) must be correctly applied. - The font scaling should affect all font elements in the plot. - The line width adjustment should specifically override the default line width in the context settings. **Example:** ```python # Test input x = [0, 1, 2, 3, 4] y = [1, 4, 9, 16, 25] context = \\"talk\\" font_scale = 1.5 linewidth = 2.5 # Example function call plot = create_and_customize_plots(x, y, context, font_scale, linewidth) plt.show(plot) ``` **Performance Requirements:** - The function should be efficient and leverage seaborn\'s built-in methods for customization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_and_customize_plots(x: list, y: list, context: str, font_scale: float, linewidth: float) -> sns.axisgrid.FacetGrid: Create and customize a seaborn line plot based on input parameters. Parameters: - x: list of x-values - y: list of y-values - context: plot context (one of \'notebook\', \'paper\', \'talk\', \'poster\') - font_scale: factor to scale the plot\'s fonts - linewidth: width of the line in the plot Returns: - sns.axisgrid.FacetGrid: The customized seaborn plot sns.set_context(context, font_scale=font_scale) plt.figure() # Create a new figure ax = sns.lineplot(x=x, y=y, linewidth=linewidth) return ax"},{"question":"# Question: Implementing Functorch-Compatible Batch Normalization You are given a neural network model implemented in PyTorch. The model currently uses `BatchNorm2d` layers, posing issues when trying to use Functorch for differentiable programming with batched tensors. Your goal is to make the model compatible with Functorch by switching these layers to use `GroupNorm` instead. In addition, you must ensure the model doesn\'t update running statistics. Function Signature ```python import torch.nn as nn def convert_to_functorch_compatible_net(model: nn.Module, num_groups: int) -> nn.Module: Convert all `BatchNorm2d` layers in the model to `GroupNorm` layers with the specified number of groups. Args: - model (nn.Module): The neural network model containing `BatchNorm2d` layers. - num_groups (int): The number of groups to use for `GroupNorm` layers. Ensures that `num_channels % num_groups == 0`. Returns: - nn.Module: The modified model with `GroupNorm` layers instead of `BatchNorm2d`. pass ``` Constraints - `num_groups` must divide the number of channels (`C`) without leaving a remainder (i.e., `C % num_groups == 0`). - You should replace every `BatchNorm2d` layer with a `GroupNorm` layer adhering to the new specifications. - Ensure that no running statistics are maintained. Example ```python import torch import torch.nn as nn import torch.nn.functional as F # Example network with BatchNorm2d class ExampleNet(nn.Module): def __init__(self): super(ExampleNet, self).__init__() self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1) self.bn1 = nn.BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) self.bn2 = nn.BatchNorm2d(32) self.fc = nn.Linear(32 * 8 * 8, 10) def forward(self, x): x = F.relu(self.bn1(self.conv1(x))) x = F.relu(self.bn2(self.conv2(x))) x = F.adaptive_avg_pool2d(x, (8, 8)) x = x.view(x.size(0), -1) x = self.fc(x) return x model = ExampleNet() new_model = convert_to_functorch_compatible_net(model, 4) # Test to ensure old BN layers are removed and replaced properly. assert all(not isinstance(layer, nn.BatchNorm2d) for layer in new_model.modules()), \\"Model still contains BatchNorm2d layers.\\" assert all(isinstance(layer, nn.GroupNorm) for layer in new_model.modules() if isinstance(layer, nn.Module)), \\"Model does not contain GroupNorm layers.\\" ``` Notes - You may assume the model contains only `BatchNorm2d`, `Conv2d`, and `Linear` layers. - Checking types for assertion can be built upon the `isinstance` function. - Use recursive programming (if necessary) to replace modules within nested submodules.","solution":"import torch.nn as nn def convert_to_functorch_compatible_net(model: nn.Module, num_groups: int) -> nn.Module: Convert all `BatchNorm2d` layers in the model to `GroupNorm` layers with the specified number of groups. Args: - model (nn.Module): The neural network model containing `BatchNorm2d` layers. - num_groups (int): The number of groups to use for `GroupNorm` layers. Ensures that `num_channels % num_groups == 0`. Returns: - nn.Module: The modified model with `GroupNorm` layers instead of `BatchNorm2d`. def _convert_bn_to_gn(module: nn.Module, num_groups: int): for name, child in module.named_children(): if isinstance(child, nn.BatchNorm2d): num_channels = child.num_features if num_channels % num_groups != 0: raise ValueError(f\\"Number of channels {num_channels} is not divisible by num_groups {num_groups}.\\") new_layer = nn.GroupNorm(num_groups=num_groups, num_channels=num_channels, affine=True) setattr(module, name, new_layer) else: _convert_bn_to_gn(child, num_groups) _convert_bn_to_gn(model, num_groups) return model"},{"question":"**Objective:** Implement a Python function that utilizes various `shutil` functionalities to create a backup system for a specified directory. The function should copy all files and subdirectories from the source directory to a destination directory. During the backup process, metadata (like file permissions and timestamps) must be preserved. Additionally, create a compressed archive of the resulting backup directory. **Function Signature:** ```python def backup_directory(src: str, dst: str, archive_format: str = \'gztar\') -> str: Copies all files and subdirectories from the source directory to a destination directory, preserving metadata. After copying, creates a compressed archive of the destination directory. Parameters: src (str): The source directory to backup. dst (str): The destination directory to store the backup. archive_format (str): The format for the archive (defaults to \'gztar\'). Returns: str: The path of the created archive. Raises: SameFileError: If source and destination directories are the same. OSError: If there are issues in reading/writing files. pass ``` # Constraints: 1. The source directory exists and is readable. 2. The destination directory is writable. 3. The `archive_format` can be one of the following: \'zip\', \'tar\', \'gztar\', \'bztar\', \'xztar\'. # Additional Information: - Use `shutil.copy2` to preserve file metadata during copying. - Use `shutil.make_archive` to create the compressed archive. - If the `dst` directory already exists, it should be cleared before the new backup (use `shutil.rmtree` if needed). **Example Usage:** ```python src_directory = \\"/path/to/source_directory\\" dst_directory = \\"/path/to/destination_directory\\" archive = backup_directory(src_directory, dst_directory, \'zip\') print(f\\"Backup archive created at: {archive}\\") ``` **Performance Requirements**: - The solution should be efficient and handle large directories gracefully. - Aim to minimize memory usage by leveraging the built-in functionalities provided by `shutil`.","solution":"import shutil import os def backup_directory(src: str, dst: str, archive_format: str = \'gztar\') -> str: Copies all files and subdirectories from the source directory to a destination directory, preserving metadata. After copying, creates a compressed archive of the destination directory. Parameters: src (str): The source directory to backup. dst (str): The destination directory to store the backup. archive_format (str): The format for the archive (defaults to \'gztar\'). Returns: str: The path of the created archive. Raises: SameFileError: If source and destination directories are the same. OSError: If there are issues in reading/writing files. if os.path.abspath(src) == os.path.abspath(dst): raise shutil.SameFileError(\\"Source and destination directories are the same.\\") # Clear destination directory if it exists if os.path.exists(dst): shutil.rmtree(dst) # Copy the source directory to the destination shutil.copytree(src, dst, copy_function=shutil.copy2) # Create archive of the destination directory archive_path = shutil.make_archive(dst, archive_format, root_dir=dst) return archive_path"},{"question":"**Objective**: Demonstrate your understanding of seaborn\'s `objects` interface by creating and customizing a dot plot with various advanced features. --- **Question**: 1. Load the `tips` dataset from seaborn. 2. Create a dot plot (`so.Plot`) displaying the relationship between `total_bill` and `day`. Specifically, each dot should represent an entry in the dataset. 3. Customize the plot by adding the following features: - Use `so.Dot` to plot the points. - Add a white edge to each dot for better visualization. - Apply `so.Jitter` to reduce overplotting by jittering the y-values. - Color the dots by the `sex` categorical variable. 4. Further, create a faceted plot by splitting the above plot by the `time` variable (Dinner/Lunch) in a 1-row layout. 5. Add a new layer showing the average value of `total_bill` for each `day` and `time` combination using a larger point size. 6. Include error bars (`so.Range`) to represent the standard error of the mean for `total_bill` for each combination. Expected Input: - No direct input from the user; only the script itself. Expected Output: - A faceted plot visualizing the relationship as described, displayed correctly using seaborn\'s `objects` interface. Constraints: - Make sure the plots are properly labeled and legible. - The jitter amount should be set to 0.2. - The point size for average values should be set to 10. - Error bars should represent ± 2 standard errors from the mean. --- **Implementation Example**: ```python import seaborn.objects as so from seaborn import load_dataset # Load the \'tips\' dataset tips = load_dataset(\\"tips\\") # Create a base plot for total_bill vs day, colored by sex base_plot = ( so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") .add(so.Dot(edgecolor=\\"w\\"), so.Jitter(y=0.2)) ) # Facet the plot by \'time\' with a 1-row layout faceted_plot = base_plot.facet(\\"time\\", wrap=1) # Add a new layer for average values with error bars faceted_plot.add(so.Dot(pointsize=10), so.Agg()) faceted_plot.add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) # Display the plot faceted_plot.show() ``` --- Note: - Ensure that your solution adheres to the expected customization and constraints. - Test your code to confirm its correctness and visual clarity.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_customized_dot_plot(): # Load the \'tips\' dataset tips = load_dataset(\\"tips\\") # Create a base plot for total_bill vs day, colored by sex base_plot = ( so.Plot(tips, \\"total_bill\\", \\"day\\", color=\\"sex\\") .add(so.Dot(edgecolor=\\"w\\"), so.Jitter(y=0.2)) ) # Facet the plot by \'time\' with a 1-row layout faceted_plot = base_plot.facet(\\"time\\", wrap=1) # Add a new layer for average values with error bars faceted_plot.add(so.Dot(pointsize=10), so.Agg()) faceted_plot.add(so.Range(), so.Est(errorbar=(\\"se\\", 2))) # Show the plot faceted_plot.show() # To execute and visualize the plot create_customized_dot_plot()"},{"question":"# AIFF File Manipulation in Python Objective You are required to write a Python function to read an AIFF or AIFF-C file, modify its audio data by applying a simple transformation, and then save the modified audio data into a new AIFF file using the `aifc` module. Description 1. Write a function called `modify_aiff(file_input, file_output)` that: - Reads the input AIFF or AIFF-C file specified by the `file_input` parameter. - Inverts the audio samples to create a \\"negative\\" audio effect. - Writes the modified audio data to the output file specified by the `file_output` parameter. 2. **Input**: - `file_input` (str): The path to the input AIFF or AIFF-C file. - `file_output` (str): The path where the modified AIFF file will be saved. 3. **Output**: - None. The function should save the modified audio file to `file_output`. 4. **Constraints**: - The function should handle both AIFF and AIFF-C files. - Ensure that the audio parameters (number of channels, frame rate, etc.) remain consistent between the input and output files. - Handle exceptions appropriately (e.g., file not found, read/write errors). 5. **Example Usage**: ```python modify_aiff(\'input.aiff\', \'output.aiff\') ``` This example reads the audio data from \'input.aiff\', applies the negative transformation, and writes the result to \'output.aiff\'. Function Signature: ```python def modify_aiff(file_input: str, file_output: str) -> None: pass ``` Hints: - You can use `aifc`\'s `readframes()` and `writeframes()` methods to handle audio data. - Audio inversion can be done by subtracting the sample value from the maximum possible sample value for the given sample width.","solution":"import aifc import numpy as np def modify_aiff(file_input: str, file_output: str) -> None: Reads an input AIFF or AIFF-C file, inverts the audio samples, and writes the modified audio data to an output AIFF file. Parameters: file_input (str): The path to the input AIFF or AIFF-C file. file_output (str): The path where the modified AIFF file will be saved. try: with aifc.open(file_input, \'r\') as input_file: params = input_file.getparams() n_frames = input_file.getnframes() sample_width = params.sampwidth audio_data = input_file.readframes(n_frames) if sample_width == 1: # 8-bit audio audio_array = np.frombuffer(audio_data, dtype=np.uint8) max_val = 255 elif sample_width == 2: # 16-bit audio audio_array = np.frombuffer(audio_data, dtype=np.int16) max_val = 32767 else: raise ValueError(f\\"Unsupported sample width: {sample_width}\\") # Invert the audio samples inverted_data = max_val - audio_array with aifc.open(file_output, \'w\') as output_file: output_file.setparams(params) output_file.writeframes(inverted_data.tobytes()) except FileNotFoundError: print(f\\"The file {file_input} does not exist.\\") except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"You are tasked with creating a function that determines whether a given input tensor meets specific conditions for optimized processing with a persistent algorithm in PyTorch. If the conditions are met, the function should apply a specific operation to the tensor using the persistent algorithm; otherwise, it should handle the tensor using a standard approach. Function Specification **Function Name:** ```python def optimized_tensor_processing(input_tensor: torch.Tensor) -> torch.Tensor: ``` **Input:** - `input_tensor` (torch.Tensor): The input tensor that needs to be checked and processed. **Output:** - (torch.Tensor): The processed tensor after applying either the persistent algorithm or the standard algorithm. **Constraints:** 1. cuDNN must be enabled. 2. The input data must be on the GPU. 3. The input data must have dtype `torch.float16`. 4. The GPU used must be V100. 5. The input data must not be in `PackedSequence` format (Assume input is not packed sequence). **Steps to Implement:** 1. Check if cuDNN is enabled. 2. Verify that the input tensor is on the GPU. 3. Ensure the input tensor\'s data type is `torch.float16`. 4. Verify that the GPU used is V100. 5. (We assume the input is not in `PackedSequence` format, so this check is implicit.) If all conditions are met, apply a sample persistent algorithm (for example, a simple operation like a tensor addition) and return the result. Otherwise, apply the standard method (using a non-optimized approach). **Example:** ```python import torch def optimized_tensor_processing(input_tensor: torch.Tensor) -> torch.Tensor: # Check if cuDNN is enabled if torch.backends.cudnn.enabled: # Verify the tensor is on the GPU and is torch.float16 if input_tensor.is_cuda and input_tensor.dtype == torch.float16: # Assume a check for V100 GPU if torch.cuda.get_device_properties(0).name == \'Tesla V100-SXM2-16GB\': # Apply persistent algorithm (example operation) return input_tensor + input_tensor else: # Standard approach return input_tensor * 2 # Example non-optimized operation else: # Standard approach return input_tensor * 2 # Example non-optimized operation else: # Standard approach return input_tensor * 2 # Example non-optimized operation # Example usage input_tensor = torch.randn((3, 3), device=\'cuda\', dtype=torch.float16) output_tensor = optimized_tensor_processing(input_tensor) print(output_tensor) ``` Please ensure to run this function only on a machine setup with the necessary hardware (V100 GPU) and software (cuDNN, CUDA) capabilities.","solution":"import torch def optimized_tensor_processing(input_tensor: torch.Tensor) -> torch.Tensor: # Check if cuDNN is enabled if torch.backends.cudnn.enabled: # Verify the tensor is on the GPU and is torch.float16 if input_tensor.is_cuda and input_tensor.dtype == torch.float16: # Assume a check for V100 GPU if torch.cuda.get_device_properties(0).name == \'Tesla V100-SXM2-16GB\': # Apply persistent algorithm (example operation) return input_tensor + input_tensor else: # Standard approach return input_tensor * 2 # Example non-optimized operation else: # Standard approach return input_tensor * 2 # Example non-optimized operation else: # Standard approach return input_tensor * 2 # Example non-optimized operation"},{"question":"**Question:** # Retrieve and Analyze Package Metadata using `importlib.metadata` You are tasked with retrieving and analyzing metadata for a selected installed Python package. Use Python\'s `importlib.metadata` package to implement the following functionalities: 1. **Retrieve Version**: - Write a function `get_package_version(package_name: str) -> str` that takes the name of a package and returns its version as a string. - Example: `get_package_version(\'wheel\')` should return `\'0.32.3\'`. 2. **Retrieve Metadata**: - Write a function `get_package_metadata(package_name: str) -> dict` that takes the name of a package and returns its metadata as a dictionary. - Example: ```python metadata = get_package_metadata(\'wheel\') print(metadata[\'Name\']) # Output: \'wheel\' print(metadata[\'Version\']) # Output: \'0.32.3\' ``` 3. **List Distribution Files**: - Write a function `list_distribution_files(package_name: str) -> list` that takes the name of a package and returns a list of all files installed by the package. - Each list item should include information about the file path and file size. - Example: ```python files = list_distribution_files(\'wheel\') for file in files: print(file[\'path\'], file[\'size\']) ``` 4. **Retrieve Requirements**: - Write a function `get_distribution_requirements(package_name: str) -> list` that takes the name of a package and returns a list of all its distribution requirements. - Example: ```python requirements = get_distribution_requirements(\'wheel\') print(requirements) # Output: [\\"pytest (>=3.0.0) ; extra == \'test\'\\", \\"pytest-cov ; extra == \'test\'\\"] ``` # Constraints: - Assume the package names provided will always be valid. - Handle the absence of a file or metadata gracefully by returning an empty list or dictionary. # Submission: Implement the following functions: - `get_package_version(package_name: str) -> str` - `get_package_metadata(package_name: str) -> dict` - `list_distribution_files(package_name: str) -> list` - `get_distribution_requirements(package_name: str) -> list` Ensure your code correctly interacts with the `importlib.metadata` package and handles edge cases appropriately.","solution":"import importlib.metadata def get_package_version(package_name: str) -> str: Returns the version of the specified package. return importlib.metadata.version(package_name) def get_package_metadata(package_name: str) -> dict: Returns the metadata of the specified package as a dictionary. metadata_dist = importlib.metadata.metadata(package_name) return dict(metadata_dist) def list_distribution_files(package_name: str) -> list: Returns a list of all files installed by the package with their file paths and sizes. distribution = importlib.metadata.distribution(package_name) files = [] for file in distribution.files: files.append({\'path\': str(file), \'size\': file.read_text().count(\'n\') if file.suffix else 0}) return files def get_distribution_requirements(package_name: str) -> list: Returns a list of all distribution requirements for the specified package. distribution = importlib.metadata.distribution(package_name) return distribution.requires or []"},{"question":"Model Evaluation with Validation and Learning Curves **Objective:** You are required to write a Python script using the `scikit-learn` library that performs the following tasks: 1. Load a dataset. 2. Implement and visualize a validation curve for a selected model by varying a specific hyperparameter. 3. Implement and visualize a learning curve for the same model. **Dataset:** Use the Iris dataset provided by `scikit-learn`. **Model:** Use the `SVC` (Support Vector Classifier) with a linear kernel. **Tasks:** 1. Load the Iris dataset and shuffle it. 2. Implement the function `plot_validation_curve` which: - Takes the model, dataset, hyperparameter name, and range of hyperparameter values as input. - Uses `validation_curve` to compute the training and validation scores. - Plots the validation curve displaying the influence of the hyperparameter on both training and validation scores. 3. Implement the function `plot_learning_curve` which: - Takes the model, dataset, and a list of training sizes as input. - Uses `learning_curve` to compute the training and validation scores for varying sizes of the training set. - Plots the learning curve displaying both training and validation scores against the number of training samples. **Input Format:** - Your function should not take any input from the user. - Initialize the data and hyperparameter values within the script. **Output Format:** - The script should generate two plots: one for the validation curve and one for the learning curve. Both plots should be displayed when the script is run. **Constraints:** - Ensure that the plots are clear and well-labeled, showing the training and validation scores distinctly. - The range of `C` values for the validation curve should be `np.logspace(-7, 3, 10)`. - The training sizes for the learning curve should be `[50, 80, 110]`. - Perform 5-fold cross-validation for both the validation and learning curves. **Performance Requirements:** - The script should run efficiently, utilizing vectorized operations where possible. **Example Code Skeleton:** ```python import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from sklearn.utils import shuffle def plot_validation_curve(model, X, y, param_name, param_range): # Compute validation curve pass def plot_learning_curve(model, X, y, train_sizes): # Compute learning curve pass def main(): # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Initialize model and hyperparameters model = SVC(kernel=\\"linear\\") param_name = \\"C\\" param_range = np.logspace(-7, 3, 10) train_sizes = [50, 80, 110] # Plot validation and learning curves plot_validation_curve(model, X, y, param_name, param_range) plot_learning_curve(model, X, y, train_sizes) if __name__ == \\"__main__\\": main() ``` You need to complete `plot_validation_curve` and `plot_learning_curve` functions to perform the tasks as described.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.svm import SVC from sklearn.model_selection import validation_curve, learning_curve from sklearn.utils import shuffle def plot_validation_curve(model, X, y, param_name, param_range): train_scores, val_scores = validation_curve( model, X, y, param_name=param_name, param_range=param_range, cv=5, scoring=\'accuracy\' ) train_scores_mean = np.mean(train_scores, axis=1) val_scores_mean = np.mean(val_scores, axis=1) plt.figure() plt.plot(param_range, train_scores_mean, label=\\"Training score\\", marker=\'o\') plt.plot(param_range, val_scores_mean, label=\\"Validation score\\", marker=\'o\') plt.xlabel(\\"Parameter Value\\") plt.ylabel(\\"Score\\") plt.title(f\\"Validation Curve for {param_name}\\") plt.legend(loc=\\"best\\") plt.xscale(\'log\') plt.grid() plt.show() def plot_learning_curve(model, X, y, train_sizes): train_sizes, train_scores, val_scores = learning_curve( model, X, y, train_sizes=train_sizes, cv=5, scoring=\'accuracy\' ) train_scores_mean = np.mean(train_scores, axis=1) val_scores_mean = np.mean(val_scores, axis=1) plt.figure() plt.plot(train_sizes, train_scores_mean, label=\\"Training score\\", marker=\'o\') plt.plot(train_sizes, val_scores_mean, label=\\"Validation score\\", marker=\'o\') plt.xlabel(\\"Training Sample Size\\") plt.ylabel(\\"Score\\") plt.title(\\"Learning Curve\\") plt.legend(loc=\\"best\\") plt.grid() plt.show() def main(): # Load and shuffle the dataset X, y = load_iris(return_X_y=True) X, y = shuffle(X, y, random_state=0) # Initialize model and hyperparameters model = SVC(kernel=\\"linear\\") param_name = \\"C\\" param_range = np.logspace(-7, 3, 10) train_sizes = [50, 80, 110] # Plot validation and learning curves plot_validation_curve(model, X, y, param_name, param_range) plot_learning_curve(model, X, y, train_sizes) if __name__ == \\"__main__\\": main()"},{"question":"**Problem Statement: Implement a Mini-Python Parser** In this task, you are required to implement a simple parser for arithmetic expressions. # Objective Create a parser that can parse basic arithmetic expressions containing positive integers and the operators `+`, `-`, `*`, and `/`. The parser should correctly follow the order of operations, with multiplication and division having higher precedence than addition and subtraction. # Input - A string representing a basic arithmetic expression. The input will always be a valid arithmetic expression consisting of positive integers, and the operators `+`, `-`, `*`, and `/`. # Output - A float representing the result of the parsed arithmetic expression. # Constraints - The expression will not contain any parentheses. - The expression will only contain spaces around the operators. # Function Signature ```python def parse_expression(expression: str) -> float: pass ``` # Examples ```python assert parse_expression(\\"3 + 5\\") == 8.0 assert parse_expression(\\"10 + 2 * 6\\") == 22.0 assert parse_expression(\\"100 * 2 + 12\\") == 212.0 assert parse_expression(\\"100 * ( 2 + 12 )\\") == 1400.0 assert parse_expression(\\"100 * ( 2 + 12 ) / 14\\") == 100.0 ``` # Guidelines 1. You may use your own utility functions to break down the parsing process. 2. Ensure that multiplication and division are performed first, followed by addition and subtraction. 3. Consider edge cases like dividing by zero, large numbers, etc. **Note:** You are NOT allowed to use any `eval` function or third-party libraries for parsing. Good luck!","solution":"def parse_expression(expression: str) -> float: def apply_operator(operators, values): operator = operators.pop() right = values.pop() left = values.pop() if operator == \'+\': values.append(left + right) elif operator == \'-\': values.append(left - right) elif operator == \'*\': values.append(left * right) elif operator == \'/\': values.append(left / right) def precedence(op): if op == \'+\' or op == \'-\': return 1 if op == \'*\' or op == \'/\': return 2 return 0 operators = [] values = [] i = 0 while i < len(expression): if expression[i] == \' \': i += 1 continue elif expression[i].isdigit(): val = 0 while i < len(expression) and expression[i].isdigit(): val = (val * 10) + int(expression[i]) i += 1 values.append(float(val)) i -= 1 else: while (operators and precedence(operators[-1]) >= precedence(expression[i])): apply_operator(operators, values) operators.append(expression[i]) i += 1 while operators: apply_operator(operators, values) return values[0]"},{"question":"Objective: Implement a Python class that simulates the behavior of cell objects described in the provided documentation. Problem Statement: Write a class `Cell` that simulates the behavior of cell objects in Python as described. The class should allow creation of a cell, getting and setting the cell\'s contents, and checking if an object is of type `Cell`. Requirements: 1. Implement the `Cell` class with the following methods: - `__init__(self, value=None)`: Initializes the cell with the given value. - `get(self)`: Returns the current value stored in the cell. - `set(self, value)`: Sets a new value for the cell. - `check_instance(obj)`: Static method that returns `True` if the given object is an instance of `Cell`. 2. The `Cell` class should handle `None` values appropriately. 3. Demonstrate the class functionality through a series of unit tests (you can implement the tests within a `main` function or use any Python testing framework of your choice). Example: ```python # Create a new cell with a value cell = Cell(10) assert cell.get() == 10, \\"Test case 1 failed\\" # Set a new value cell.set(20) assert cell.get() == 20, \\"Test case 2 failed\\" # Check if an object is a cell instance assert Cell.check_instance(cell) == True, \\"Test case 3 failed\\" assert Cell.check_instance(123) == False, \\"Test case 4 failed\\" # Handle None values none_cell = Cell() assert none_cell.get() is None, \\"Test case 5 failed\\" none_cell.set(30) assert none_cell.get() == 30, \\"Test case 6 failed\\" print(\\"All test cases passed!\\") ``` Constraints: - Do not use any built-in cell-like mechanisms; the implementation should rely solely on core Python features. - Ensure that your solution handles invalid inputs gracefully (e.g., setting a value that is not allowed should raise an appropriate exception). Submission: Submit your `Cell` class implementation along with the unit tests demonstrating that your class correctly handles all scenarios described in the requirements.","solution":"class Cell: Class that simulates a cell object for storing and manipulating a value. def __init__(self, value=None): Initializes the cell with the given value. self.value = value def get(self): Returns the current value stored in the cell. return self.value def set(self, value): Sets a new value for the cell. self.value = value @staticmethod def check_instance(obj): Returns True if the given object is an instance of Cell, otherwise False. return isinstance(obj, Cell)"},{"question":"Objective: Implement a machine learning pipeline using scikit-learn that demonstrates the use of advanced cross-validation techniques, model selection, and the evaluation of multiple metrics. Your task is to build a classifier for the Iris dataset and evaluate it using `GroupKFold` cross-validation. Task: 1. **Data Loading**: Load the Iris dataset using scikit-learn. 2. **Data Preparation**: - Standardize the dataset using `StandardScaler`. - Use PCA to reduce the dimensionality of the dataset to a 2D space. 3. **Cross-Validation Strategy**: - Implement grouped cross-validation using `GroupKFold`. For the purpose of this exercise, you can treat each class in the Iris dataset as a group (even though it\'s not an ideal grouping, it will serve the purpose of this assessment). 4. **Model and Pipeline**: - Use an SVM classifier. - Create a pipeline combining Standard Scaler, PCA, and SVM. 5. **Metrics Evaluation**: - Evaluate the model using multiple metrics: Accuracy, Precision, and Recall. - Make use of `cross_validate` to evaluate multiple metrics in one go. 6. **Implementation**: - Write a function `evaluate_model_with_group_kfold` that takes no parameters and returns a dictionary containing the mean and standard deviation of the metrics across the splits. - Write the main code structure to call this function and print its result. Expected Outputs: ```python { \'test_accuracy\': (mean_accuracy, std_accuracy), \'test_precision_macro\': (mean_precision, std_precision), \'test_recall_macro\': (mean_recall, std_recall) } ``` Constraints and Performance Requirements: - Ensure the pipeline is correctly integrated and the grouping is correctly handled. - Use `random_state=42` where applicable to ensure reproducible results. Example Code Structure: ```python def evaluate_model_with_group_kfold(): from sklearn import datasets from sklearn.model_selection import GroupKFold, cross_validate from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.metrics import make_scorer, precision_score, recall_score # Load the Iris dataset. X, y = datasets.load_iris(return_X_y=True) # Assuming groups are defined by the class labels for this example. groups = y # Define the pipeline: StandardScaler -> PCA -> SVM. pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'svc\', SVC(kernel=\'linear\', random_state=42)) ]) # Define the cross-validation strategy. group_kfold = GroupKFold(n_splits=3) # Define scoring metrics. scoring = { \'accuracy\': \'accuracy\', \'precision_macro\': make_scorer(precision_score, average=\'macro\'), \'recall_macro\': make_scorer(recall_score, average=\'macro\') } # Evaluate the model using cross_validate. scores = cross_validate(pipeline, X, y, groups=groups, cv=group_kfold, scoring=scoring, return_train_score=False) # Collect the mean and standard deviation of each metric. results = { \'test_accuracy\': (scores[\'test_accuracy\'].mean(), scores[\'test_accuracy\'].std()), \'test_precision_macro\': (scores[\'test_precision_macro\'].mean(), scores[\'test_precision_macro\'].std()), \'test_recall_macro\': (scores[\'test_recall_macro\'].mean(), scores[\'test_recall_macro\'].std()) } return results # Main execution if __name__ == \\"__main__\\": results = evaluate_model_with_group_kfold() print(results) ```","solution":"def evaluate_model_with_group_kfold(): from sklearn import datasets from sklearn.model_selection import GroupKFold, cross_validate from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA from sklearn.svm import SVC from sklearn.metrics import make_scorer, precision_score, recall_score # Load the Iris dataset. X, y = datasets.load_iris(return_X_y=True) # Assuming groups are defined by the class labels for this example. groups = y # Define the pipeline: StandardScaler -> PCA -> SVM. pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'pca\', PCA(n_components=2)), (\'svc\', SVC(kernel=\'linear\', random_state=42)) ]) # Define the cross-validation strategy. group_kfold = GroupKFold(n_splits=3) # Define scoring metrics. scoring = { \'accuracy\': \'accuracy\', \'precision_macro\': make_scorer(precision_score, average=\'macro\'), \'recall_macro\': make_scorer(recall_score, average=\'macro\') } # Evaluate the model using cross_validate. scores = cross_validate(pipeline, X, y, groups=groups, cv=group_kfold, scoring=scoring, return_train_score=False) # Collect the mean and standard deviation of each metric. results = { \'test_accuracy\': (scores[\'test_accuracy\'].mean(), scores[\'test_accuracy\'].std()), \'test_precision_macro\': (scores[\'test_precision_macro\'].mean(), scores[\'test_precision_macro\'].std()), \'test_recall_macro\': (scores[\'test_recall_macro\'].mean(), scores[\'test_recall_macro\'].std()) } return results # Main execution if __name__ == \\"__main__\\": results = evaluate_model_with_group_kfold() print(results)"},{"question":"Objective: Demonstrate the ability to create and manipulate Python slice objects, extracting and adjusting slice indices as necessary, using the concepts from the Python C-API. Problem Statement: You are required to implement a Python function `manage_slices(sequence, slice_objects)` that processes a list of slice objects and extracts their start, stop, and step values for a given sequence. The function should handle out-of-bounds indices by clipping them to match Python\'s normal slice behavior. Function Signature: ```python def manage_slices(sequence, slice_objects): :param sequence: A list of integers representing the sequence. :param slice_objects: A list of slice objects to be processed. :return: A list of tuples, each containing the start, stop, and step values for the corresponding slice, adjusted to fit within the sequence\'s bounds. ``` Input: - `sequence`: A list of integers (1 <= len(sequence) <= 10^6). - `slice_objects`: A list of slice objects (1 <= len(slice_objects) <= 10^3). Output: - A list of tuples `(start, stop, step)`, where each tuple represents the adjusted indices for each slice object in `slice_objects`. Constraints: - All indices and lengths must fit within the limits of typical Python slicing behavior. - You must handle cases where `None` is passed for start, stop, or step by using Python\'s default values (`start=0`, `stop=len(sequence)`, `step=1`). Example: ```python sequence = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] slice_objects = [slice(None, None, None), slice(1, 8, 2), slice(-5, 15, 1)] # Expected Output: # [(0, 10, 1), (1, 8, 2), (5, 10, 1)] print(manage_slices(sequence, slice_objects)) ``` # Notes: - You may not use any external libraries for handling slices; you must implement the logic using basic Python constructs and the knowledge provided in the documentation. - Carefully consider edge cases such as negative indices and excessively large values for start, stop, and step.","solution":"def manage_slices(sequence, slice_objects): :param sequence: A list of integers representing the sequence. :param slice_objects: A list of slice objects to be processed. :return: A list of tuples, each containing the start, stop, and step values for the corresponding slice, adjusted to fit within the sequence\'s bounds. result = [] seq_len = len(sequence) for s in slice_objects: start = s.start if s.start is not None else 0 stop = s.stop if s.stop is not None else seq_len step = s.step if s.step is not None else 1 if start < 0: start += seq_len if stop < 0: stop += seq_len start = min(max(start, 0), seq_len) stop = min(max(stop, 0), seq_len) result.append((start, stop, step)) return result"},{"question":"Objective: The task is to preprocess a given dataset and prepare it for training a machine learning model. The student should demonstrate comprehension of several preprocessing techniques, including standardization, encoding categorical variables, and performing non-linear transformations. Dataset: You are provided with a dataset represented in a pandas DataFrame with the following structure: ```plaintext Age Gender Income Purchased 0 25 Male 50000 No 1 30 Female 60000 Yes ... ``` - `Age`: an integer representing the age of the customer. - `Gender`: a categorical variable with values `Male` and `Female`. - `Income`: an integer representing the annual income of the customer. - `Purchased`: a binary categorical variable indicating if the customer purchased the product (`Yes` or `No`). Task: 1. **Missing Values Imputation**: Impute any missing values in the `Age` and `Income` columns using the median. 2. **Encoding Categorical Features**: Encode the `Gender` and `Purchased` columns using one-hot encoding. 3. **Standardization**: Standardize the `Age` and `Income` columns so that they have zero mean and unit variance. 4. **Polynomial Features**: Add polynomial features for `Age` and `Income` up to the 2nd degree. 5. **Pipeline**: Combine all above preprocessing steps into a scikit-learn Pipeline. Constraints: 1. The DataFrame can have missing values. 2. Ensure that the pipeline is constructed using scikit-learn’s `Pipeline` and `ColumnTransformer`. Format: - Input: A pandas DataFrame `df` with the structure described above. - Output: A scikit-learn Pipeline object. Example: Here is how the pipeline should be used: ```python pipeline = preprocessing_pipeline() X_transformed = pipeline.fit_transform(df) print(X_transformed) ``` Performance: The implementation should handle a DataFrame with up to 100,000 rows in less than 2 seconds for the preprocessing steps. Implementation: ```python import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures def preprocessing_pipeline(): # Column selectors categorical_features = [\'Gender\', \'Purchased\'] numerical_features = [\'Age\', \'Income\'] # Preprocessing for numerical data numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()), (\'polynomial\', PolynomialFeatures(degree=2, include_bias=False)) ]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Create and return the pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) return pipeline # Example usage df = pd.DataFrame({ \'Age\': [25, 30, 35], \'Gender\': [\'Male\', \'Female\', \'Female\'], \'Income\': [50000, 60000, 80000], \'Purchased\': [\'No\', \'Yes\', \'No\'] }) pipeline = preprocessing_pipeline() X_transformed = pipeline.fit_transform(df) print(X_transformed) ``` The example usage creates a pipeline that imputes missing values, encodes categorical features, standardizes numerical features, and adds polynomial features before applying all the transformations to the provided DataFrame.","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures def preprocessing_pipeline(): # Column selectors categorical_features = [\'Gender\', \'Purchased\'] numerical_features = [\'Age\', \'Income\'] # Preprocessing for numerical data numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'median\')), (\'scaler\', StandardScaler()), (\'polynomial\', PolynomialFeatures(degree=2, include_bias=False)) ]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Create and return the pipeline pipeline = Pipeline(steps=[(\'preprocessor\', preprocessor)]) return pipeline"}]'),z={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:I,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},F={class:"search-container"},R={class:"card-container"},D={key:0,class:"empty-state"},q=["disabled"],N={key:0},O={key:1};function L(n,e,l,m,i,o){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",F,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=r=>i.searchQuery=r),placeholder:"Search..."},null,512),[[y,i.searchQuery]]),i.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=r=>i.searchQuery="")}," ✕ ")):d("",!0)]),t("div",R,[(a(!0),s(b,null,v(o.displayedPoems,(r,f)=>(a(),w(h,{key:f,poem:r},null,8,["poem"]))),128)),o.displayedPoems.length===0?(a(),s("div",D,' No results found for "'+c(i.searchQuery)+'". ',1)):d("",!0)]),o.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:i.isLoading,onClick:e[2]||(e[2]=(...r)=>o.loadMore&&o.loadMore(...r))},[i.isLoading?(a(),s("span",O,"Loading...")):(a(),s("span",N,"See more"))],8,q)):d("",!0)])}const j=p(z,[["render",L],["__scopeId","data-v-05020a54"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/64.md","filePath":"quotes/64.md"}'),M={name:"quotes/64.md"},H=Object.assign(M,{setup(n){return(e,l)=>(a(),s("div",null,[x(j)]))}});export{Y as __pageData,H as default};
