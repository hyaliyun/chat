import{_ as p,o as a,c as s,a as t,m as c,t as u,C as _,M as g,U as y,f as d,F as b,p as v,e as w,q as x}from"./chunks/framework.B1z0IdBH.js";const k={name:"PoemCard",props:{poem:{type:Object,required:!0}}},T={class:"poem-container"},C={class:"review"},P={class:"review-title"},E={class:"review-content"};function S(n,e,l,m,o,r){return a(),s("div",T,[t("div",C,[t("div",P,[e[0]||(e[0]=t("span",{class:"icon"},"question:",-1)),c(u(l.poem.question),1)]),e[2]||(e[2]=t("div",{class:"divider"},null,-1)),t("p",E,[e[1]||(e[1]=t("span",{class:"icon"},"answer:",-1)),c(u(l.poem.solution),1)])])])}const A=p(k,[["render",S],["__scopeId","data-v-8a81be21"]]),D=JSON.parse('[{"question":"**Objective:** Create a custom SMTP server by extending the `smtpd.SMTPServer` class in Python. Your server should process incoming email messages and perform specific actions based on their contents. **Task:** 1. Create a class `CustomSMTPServer` that extends `smtpd.SMTPServer`. 2. Override the `process_message` method to process incoming emails. **Specifications:** 1. **Initialization:** - The server should bind to local address `(\'localhost\', 1025)` and relay to remote address `None`. - It should have a `data_size_limit` of 1MB. 2. **process_message Method:** - Check if the email subject contains the keyword \\"ALERT\\". - If \\"ALERT\\" is found, print the email\'s sender, recipients, and body to the stdout. - Otherwise, simply print \\"No alert found in the email.\\" **Input:** - Emails are sent to the server following the SMTP protocol. **Output:** - Depending on the email subject, output relevant information as described above. **Constraints:** - Handle maximum email data size of up to 1MB. - Ensure that only one of `decode_data` or `enable_SMTPUTF8` is set to True. **Performance Requirements:** - The server should handle incoming email data efficiently, providing output promptly. **Example Usage:** ```python import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): subject = self.extract_subject(data) if \\"ALERT\\" in subject: print(f\\"Sender: {mailfrom}\\") print(f\\"Recipients: {\', \'.join(rcpttos)}\\") print(f\\"Body: {data}\\") else: print(\\"No alert found in the email.\\") def extract_subject(self, data): for line in data.split(\\"n\\"): if line.startswith(\\"Subject:\\"): return line[len(\\"Subject: \\"):].strip() return \\"\\" if __name__ == \\"__main__\\": server = CustomSMTPServer((\'localhost\', 1025), None, data_size_limit=1048576) asyncore.loop() ``` In this example, the `CustomSMTPServer` class is created that extends the `smtpd.SMTPServer`. The `process_message` method is overridden to check the subject of incoming emails for the keyword \\"ALERT\\" and output relevant information if found.","solution":"import smtpd import asyncore class CustomSMTPServer(smtpd.SMTPServer): def __init__(self, localaddr, remoteaddr, **kwargs): kwargs[\'data_size_limit\'] = 1048576 # 1MB size limit super().__init__(localaddr, remoteaddr, **kwargs) def process_message(self, peer, mailfrom, rcpttos, data, **kwargs): subject = self.extract_subject(data) if \\"ALERT\\" in subject: print(f\\"Sender: {mailfrom}\\") print(f\\"Recipients: {\', \'.join(rcpttos)}\\") print(f\\"Body: {data}\\") else: print(\\"No alert found in the email.\\") def extract_subject(self, data): for line in data.split(\\"n\\"): if line.startswith(\\"Subject:\\"): return line[len(\\"Subject: \\"):].strip() return \\"\\" if __name__ == \\"__main__\\": server = CustomSMTPServer((\'localhost\', 1025), None, decode_data=True, enable_SMTPUTF8=False) asyncore.loop()"},{"question":"# Asyncio Coding Challenge You have been tasked to implement a simple echo server using Python\'s asyncio library. The server should be able to handle multiple client connections concurrently, echoing back any messages received from a client to the same client. # Requirements 1. **Echo Server Implementation:** - Create an `EchoServer` class. - The server should accept connections on a specified host and port. - When a client sends a message, the server should echo the message back to the same client. - The server should handle multiple clients simultaneously. 2. **Event Loop Management:** - Use the appropriate asyncio methods to create and run the event loop. 3. **Protocol Implementation:** - Use transport and protocol for managing client connections. - Implement the following protocol methods: - `connection_made` - `data_received` - `connection_lost` 4. **Asynchronous Communication:** - Ensure that the communication with each client is handled asynchronously. # Additional Constraints - You are not allowed to use higher-level abstractions from asyncio such as `asyncio.run()`, `asyncio.start_server()`, or `asyncio.StreamReader/StreamWriter`. # Expected Behavior - The server starts listening on `localhost` port `8888`. - Multiple telnet clients can connect to `localhost 8888` and send messages. - The server echoes any message received back to the respective client. - The server can handle multiple clients simultaneously without blocking. # Example Usage 1. Start your server: ```python server = EchoServer(\'localhost\', 8888) server.start() ``` 2. In a terminal, connect to your server using telnet: ```sh telnet localhost 8888 ``` 3. Type a message and observe that it is echoed back. # Implementation ```python import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\\"Connection from {peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") # Echo back the received message self.transport.write(data) print(f\\"Data echoed back to the client\\") def connection_lost(self, exc): print(\\"Connection closed\\") class EchoServer: def __init__(self, host, port): self.host = host self.port = port async def start_server(self): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), self.host, self.port ) async with server: await server.serve_forever() def start(self): loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) loop.run_until_complete(self.start_server()) # To run the server, uncomment the following lines: # if __name__ == \\"__main__\\": # server = EchoServer(\'localhost\', 8888) # server.start() ```","solution":"import asyncio class EchoServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\'peername\') print(f\\"Connection from {peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") # Echo back the received message self.transport.write(data) print(f\\"Data echoed back to the client\\") def connection_lost(self, exc): print(\\"Connection closed or lost\\") class EchoServer: def __init__(self, host, port): self.host = host self.port = port async def start_server(self): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: EchoServerProtocol(), self.host, self.port ) async with server: await server.serve_forever() def start(self): loop = asyncio.new_event_loop() asyncio.set_event_loop(loop) loop.run_until_complete(self.start_server()) # To run the server, uncomment the following lines: # if __name__ == \\"__main__\\": # server = EchoServer(\'localhost\', 8888) # server.start()"},{"question":"**Objective:** Implement a custom pipeline that demonstrates the use of multiple dataset transformers from the scikit-learn library. **Problem Description:** You are provided with a dataset containing numerical and categorical features. Your task is to design a pipeline using scikit-learn that includes the following steps: 1. **Impute Missing Values**: For numerical features, impute missing values using the mean. For categorical features, impute missing values using the most frequent value. 2. **Standardize Numerical Features**: Normalize numerical features to have zero mean and unit variance. 3. **One-Hot Encode Categorical Features**: Convert categorical features into a one-hot encoded format. 4. **Principal Component Analysis (PCA)**: Apply PCA to reduce the dimensionality of the dataset to 2 principal components. **Input:** - A dataset containing numerical and categorical features. - A list of columns indicating which features are numerical and which are categorical. **Output:** - A transformed dataset with imputation, standardization, one-hot encoding, and PCA applied. **Constraints:** - Use scikit-learn transformers and pipeline functionalities. - Handle any potential errors such as missing columns or incorrect data types. - Ensure that the resulting dataset has exactly 2 principal components. **Performance Requirements:** - The solution should be efficient and capable of handling large datasets (e.g., 10,000 samples with 100 features). **Code Template:** ```python import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def transform_dataset(dataset: pd.DataFrame, numerical_features: list, categorical_features: list) -> pd.DataFrame: # Define transformers numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine transformers into a preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create the full pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'pca\', PCA(n_components=2)) ]) # Fit and transform the dataset transformed_dataset = pipeline.fit_transform(dataset) # Convert the result to a DataFrame return pd.DataFrame(transformed_dataset, columns=[\'PC1\', \'PC2\']) # Example usage: # dataset = pd.DataFrame({\'num1\': [1, 2, np.nan], \'num2\': [4, 5, 6], \'cat1\': [\'a\', \'b\', \'a\']}) # numerical_features = [\'num1\', \'num2\'] # categorical_features = [\'cat1\'] # transformed_dataset = transform_dataset(dataset, numerical_features, categorical_features) # print(transformed_dataset) ``` **Explanation:** 1. Define pipelines for transforming numerical and categorical data. 2. Combine these pipelines using a `ColumnTransformer`. 3. Integrate the `ColumnTransformer` with PCA in a final pipeline. 4. Fit and transform the input dataset using this pipeline. 5. Return the transformed dataset as a DataFrame with 2 principal components. Given the provided template, your task is to complete and execute it, ensuring that it runs correctly on the example usage.","solution":"import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.decomposition import PCA from sklearn.pipeline import Pipeline def transform_dataset(dataset: pd.DataFrame, numerical_features: list, categorical_features: list) -> pd.DataFrame: # Define transformers numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine transformers into a preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ] ) # Create the full pipeline pipeline = Pipeline(steps=[ (\'preprocessor\', preprocessor), (\'pca\', PCA(n_components=2)) ]) # Fit and transform the dataset transformed_dataset = pipeline.fit_transform(dataset) # Convert the result to a DataFrame return pd.DataFrame(transformed_dataset, columns=[\'PC1\', \'PC2\']) # Example usage: # dataset = pd.DataFrame({\'num1\': [1, 2, np.nan], \'num2\': [4, 5, 6], \'cat1\': [\'a\', \'b\', \'a\']}) # numerical_features = [\'num1\', \'num2\'] # categorical_features = [\'cat1\'] # transformed_dataset = transform_dataset(dataset, numerical_features, categorical_features) # print(transformed_dataset)"},{"question":"# Custom Fixer Implementation in 2to3 Background `2to3` is an automated tool that assists in converting Python 2.x code to Python 3.x by applying a series of predefined transformations, known as fixers. While `2to3` comes with a comprehensive set of built-in fixers, it is also flexible enough to allow the creation of custom fixers tailored to specific needs. Your task is to implement a custom fixer that converts the Python 2 `print` statement into the Python 3 `print()` function. Specifically, this fixer should ensure that all instances of `print` statements are correctly changed to `print()` function calls, including handling scenarios with multiple arguments and string formatting. Task 1. Create a custom fixer named `fix_print` that converts the Python 2 `print` statement to the Python 3 `print()` function call. 2. The fixer should be able to handle different forms of `print` statements, such as: - `print \\"Hello, World\\"` - `print \\"Value:\\", x` - `print \\"Sum: %d\\" % total` - `print a, b, c` 3. Ensure your fixer maintains correct syntax and handles various edge cases, such as comments and line continuations. 4. Provide test cases in Python 2.x code format and demonstrate how your fixer transforms them to Python 3.x code. Implementation Details 1. Your implementation should use the `lib2to3` library and specifically subclass the `fix_base.FixerBase` class to define the custom fixer. 2. Use the appropriate parsing and pattern-matching utilities provided by `lib2to3` to identify and transform `print` statements. 3. Implement unit tests to verify the correctness of your fixer. Input - A Python 2.x source file containing `print` statements. Output - The transformed Python 3.x source code after applying the custom `fix_print` fixer. Constraints - The code should run within reasonable time limits for typical source files (up to a few hundreds of lines of code). - Ensure compatibility with both Python 2.7 and Python 3.x environments for the transformation process. Example **Input (Python 2.x code):** ```python print \\"Hello, World!\\" print \\"Value:\\", x print \\"Sum: %d\\" % total print a, b, c ``` **Output (Python 3.x code):** ```python print(\\"Hello, World!\\") print(\\"Value:\\", x) print(\\"Sum: %d\\" % total) print(a, b, c) ``` Provide your implementation of the `fix_print` fixer along with a demonstration of its usage.","solution":"from lib2to3 import fixer_base from lib2to3.fixer_util import Name, Call, Comma class FixPrint(fixer_base.BaseFix): Custom fixer that converts Python 2 print statements to Python 3 print() function calls. PATTERN = simple_stmt< any* > | print_stmt< \'print\' (not_atom< atom=\'>\' | atom=any* >|\'print\'|\'print_stmt\')* > def transform(self, node, results): if node.children[0].value == \'print\': new_node = Call(Name(\'print\'), node.children[1:]) node.replace(new_node)"},{"question":"**Question: File Pattern Matcher** You are tasked with developing a Python function that helps categorize files in a directory based on Unix shell-style patterns. You\'ll use the `fnmatch` module\'s capabilities to achieve this. The goal is to read a list of filenames from a directory and classify them into groups based on the provided patterns. # Function Signature ```python def categorize_files(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: ``` # Input - `filenames`: A list of strings, where each string is a filename, e.g., `[\\"file1.txt\\", \\"file2.jpg\\", \\"file3.txt\\", \\"file4.png\\"]`. - `patterns`: A list of strings, where each string is a Unix shell-style pattern, e.g., `[\\"*.txt\\", \\"*.jpg\\", \\"*.png\\"]`. # Output - Returns a dictionary where keys are the patterns and values are lists of filenames matching those patterns. For filenames that do not match any pattern, they should be placed in a category with the key `\\"others\\"`. # Constraints - Filenames can be any valid string with characters including alphabets, numerical digits, and special characters. - Patterns follow the Unix shell-style wildcards. - Case sensitivity should be handled using `fnmatchcase`. # Example ```python filenames = [\\"file1.txt\\", \\"file2.jpg\\", \\"file3.TXT\\", \\"file4.png\\", \\"file5.doc\\"] patterns = [\\"*.txt\\", \\"*.jpg\\", \\"*.png\\"] output = categorize_files(filenames, patterns) print(output) # Expected Output: # { # \\"*.txt\\": [\\"file1.txt\\", \\"file3.TXT\\"], # \\"*.jpg\\": [\\"file2.jpg\\"], # \\"*.png\\": [\\"file4.png\\"], # \\"others\\": [\\"file5.doc\\"] # } ``` # Notes - Ensure your solution uses the `fnmatch` module efficiently. - It should be case-sensitive, i.e., `\\"*.txt\\"` should match `\\"file1.txt\\"` but not `\\"file3.TXT\\"`. - Filenames that do not match any pattern should be categorized as `\\"others\\"`. You are encouraged to handle the problem iteratively and use appropriate Python data structures to maintain clarity and performance.","solution":"from typing import List, Dict import fnmatch def categorize_files(filenames: List[str], patterns: List[str]) -> Dict[str, List[str]]: category_dict = {pattern: [] for pattern in patterns} category_dict[\\"others\\"] = [] for filename in filenames: matched = False for pattern in patterns: if fnmatch.fnmatchcase(filename, pattern): category_dict[pattern].append(filename) matched = True break if not matched: category_dict[\\"others\\"].append(filename) return category_dict"},{"question":"**ConfigParser Customization and Usage** **Objective**: Demonstrate your understanding of the `configparser` module by creating a customized `ConfigParser` and performing various operations. **Task**: You are provided with an INI configuration string. Your task is to: 1. Parse the given configuration string using `configparser`. 2. Create a customized `ConfigParser` with the following specifications: - Use `ExtendedInterpolation` for interpolations. - Enable options to be case-sensitive. - Allow settings without values. - Set custom Boolean states: `{\'yes\': True, \'no\': False, \'sure\': True, \'nah\': False}`. - Use a custom dictionary type that orders sections and keys alphabetically. 3. Perform the following operations using your customized parser: - Add a new section `MySection` with an option `answer` set to `42`. - Retrieve the value of `ServerAliveInterval` from the `DEFAULT` section. - Retrieve and confirm whether the `ForwardX11` option in the `forge.example` section is enabled. - Update the `CompressionLevel` option in the `DEFAULT` section to `5`. - Write the final configuration to a file named `final_config.ini`. **Input**: - A string containing the initial configuration. ```ini [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [forge.example] User = hg [topsecret.server.example] Port = 50022 ForwardX11 = no ``` **Constraints**: - The custom dictionary type should ensure sections and options are stored and accessed in alphabetical order. **Output**: - The `MySection` with `answer=42`. - The value of `ServerAliveInterval` from the `DEFAULT` section. - The Boolean check result for the `ForwardX11` option in the `forge.example` section. - The updated value of `CompressionLevel` in the `DEFAULT` section written to `final_config.ini`. Below is a starter code template for you: ```python import configparser from collections import OrderedDict # Custom dictionary to order sections and options alphabetically class AlphabeticalDict(dict): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self._store = OrderedDict(sorted(self.items(), key=lambda t: t[0].lower())) def __setitem__(self, key, value): super().__setitem__(key, value) self._store = OrderedDict(sorted(self.items(), key=lambda t: t[0].lower())) config_string = [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [forge.example] User = hg [topsecret.server.example] Port = 50022 ForwardX11 = no # Step 1: Parse the given configuration string config = configparser.ConfigParser() config.read_string(config_string) # Step 2: Create customized ConfigParser custom_config = configparser.ConfigParser( dict_type=AlphabeticalDict, allow_no_value=True, interpolation=configparser.ExtendedInterpolation() ) custom_config.optionxform = str # Make options case-sensitive custom_config.BOOLEAN_STATES = {\'yes\': True, \'no\': False, \'sure\': True, \'nah\': False} # Step 3: Add initial configuration to the custom parser custom_config.read_string(config_string) # Step 4: Add a new section \'MySection\' with an option \'answer\' set to \'42\' custom_config.add_section(\'MySection\') custom_config.set(\'MySection\', \'answer\', \'42\') # Step 5: Retrieve value of \'ServerAliveInterval\' from \'DEFAULT\' section server_alive_interval = custom_config[\'DEFAULT\'].get(\'ServerAliveInterval\') # Step 6: Check if \'ForwardX11\' in \'forge.example\' is enabled forward_x11_enabled = custom_config[\'forge.example\'].getboolean(\'ForwardX11\') # Step 7: Update \'CompressionLevel\' in \'DEFAULT\' section to \'5\' custom_config[\'DEFAULT\'][\'CompressionLevel\'] = \'5\' # Write final configuration to a file \'final_config.ini\' with open(\'final_config.ini\', \'w\') as configfile: custom_config.write(configfile) # Output the results of the required tasks print(f\\"ServerAliveInterval in DEFAULT section: {server_alive_interval}\\") print(f\\"ForwardX11 in forge.example section enabled: {forward_x11_enabled}\\") print(\\"Configuration successfully written to final_config.ini\\") ``` **Expected Output**: You should see a newly created file `final_config.ini` with all the modifications and additions made. The print statements should confirm the specific option values as required.","solution":"import configparser from collections import OrderedDict # Custom dictionary to order sections and options alphabetically class AlphabeticalDict(dict): def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs) self._store = OrderedDict(sorted(self.items(), key=lambda t: t[0].lower())) def __setitem__(self, key, value): super().__setitem__(key, value) self._store = OrderedDict(sorted(self.items(), key=lambda t: t[0].lower())) config_string = [DEFAULT] ServerAliveInterval = 45 Compression = yes CompressionLevel = 9 ForwardX11 = yes [forge.example] User = hg [topsecret.server.example] Port = 50022 ForwardX11 = no # Step 1: Parse the given configuration string config = configparser.ConfigParser() config.read_string(config_string) # Step 2: Create customized ConfigParser custom_config = configparser.ConfigParser( dict_type=AlphabeticalDict, allow_no_value=True, interpolation=configparser.ExtendedInterpolation() ) custom_config.optionxform = str # Make options case-sensitive custom_config.BOOLEAN_STATES = {\'yes\': True, \'no\': False, \'sure\': True, \'nah\': False} # Step 3: Add initial configuration to the custom parser custom_config.read_string(config_string) # Step 4: Add a new section \'MySection\' with an option \'answer\' set to \'42\' custom_config.add_section(\'MySection\') custom_config.set(\'MySection\', \'answer\', \'42\') # Step 5: Retrieve value of \'ServerAliveInterval\' from \'DEFAULT\' section server_alive_interval = custom_config[\'DEFAULT\'].get(\'ServerAliveInterval\') # Step 6: Check if \'ForwardX11\' in \'forge.example\' is enabled forward_x11_enabled = custom_config[\'forge.example\'].getboolean(\'ForwardX11\') # Step 7: Update \'CompressionLevel\' in \'DEFAULT\' section to \'5\' custom_config[\'DEFAULT\'][\'CompressionLevel\'] = \'5\' # Write final configuration to a file \'final_config.ini\' with open(\'final_config.ini\', \'w\') as configfile: custom_config.write(configfile) # Output the results of the required tasks def get_server_alive_interval(): return server_alive_interval def is_forward_x11_enabled(): return forward_x11_enabled def get_compression_level(): return custom_config[\'DEFAULT\'][\'CompressionLevel\'] def get_config_file_path(): return \'final_config.ini\' print(f\\"ServerAliveInterval in DEFAULT section: {server_alive_interval}\\") print(f\\"ForwardX11 in forge.example section enabled: {forward_x11_enabled}\\") print(\\"Configuration successfully written to final_config.ini\\")"},{"question":"Objective: Write a Python program that uses the `nis` module to interact with NIS maps. The program should be able to: 1. Retrieve and print the value associated with a given key in a specified NIS map using `nis.match()`. 2. Retrieve and print all key-value pairs in a specified NIS map using `nis.cat()`. 3. Retrieve and print the list of all valid maps using `nis.maps()`. 4. Retrieve and print the system\'s default NIS domain using `nis.get_default_domain()`. Function Signatures: ```python def retrieve_value(key: str, mapname: str, domain: str = None) -> bytes: pass def retrieve_map(mapname: str, domain: str = None) -> dict: pass def retrieve_all_maps(domain: str = None) -> list: pass def retrieve_default_domain() -> str: pass ``` Input and Output Formats: 1. **`retrieve_value(key: str, mapname: str, domain: str = None) -> bytes`**: - **Input**: `key` (string) - the key to find in the map; `mapname` (string) - the name of the map; `domain` (string, optional) - the NIS domain. - **Output**: A byte array containing the value associated with the key in the map. 2. **`retrieve_map(mapname: str, domain: str = None) -> dict`**: - **Input**: `mapname` (string) - the name of the map; `domain` (string, optional) - the NIS domain. - **Output**: A dictionary mapping keys to values in the map. 3. **`retrieve_all_maps(domain: str = None) -> list`**: - **Input**: `domain` (string, optional) - the NIS domain. - **Output**: A list of strings representing all valid maps. 4. **`retrieve_default_domain() -> str`**: - **Input**: No input. - **Output**: A string representing the system\'s default NIS domain. Constraints: - These operations are to be performed on a Unix system with NIS configured. - Handle any potential `nis.error` exceptions that might arise during these operations. Notes: - You must test whether the `domain` parameter is provided or not and call the corresponding `nis` functions appropriately. - Consider exception handling to capture and handle `nis.error` exceptions gracefully. Example: ```python # Example usage of the functions # 1. Retrieve and print the value associated with a given key in a specified NIS map value = retrieve_value(\\"example_key\\", \\"example_map\\") print(value) # 2. Retrieve and print all key-value pairs in a specified NIS map map_data = retrieve_map(\\"example_map\\") print(map_data) # 3. Retrieve and print the list of all valid maps maps = retrieve_all_maps() print(maps) # 4. Retrieve and print the system\'s default NIS domain default_domain = retrieve_default_domain() print(default_domain) ```","solution":"import nis def retrieve_value(key: str, mapname: str, domain: str = None) -> bytes: Retrieves the value associated with a given key in a specified NIS map. try: if domain: return nis.match(key, mapname, domain) else: return nis.match(key, mapname) except nis.error as e: raise RuntimeError(f\\"Failed to retrieve value for key \'{key}\' in map \'{mapname}\': {e}\\") def retrieve_map(mapname: str, domain: str = None) -> dict: Retrieves all key-value pairs in a specified NIS map. try: if domain: return nis.cat(mapname, domain) else: return nis.cat(mapname) except nis.error as e: raise RuntimeError(f\\"Failed to retrieve map \'{mapname}\': {e}\\") def retrieve_all_maps(domain: str = None) -> list: Retrieves the list of all valid maps. try: if domain: return nis.maps(domain) else: return nis.maps() except nis.error as e: raise RuntimeError(f\\"Failed to retrieve maps: {e}\\") def retrieve_default_domain() -> str: Retrieves the system\'s default NIS domain. try: return nis.get_default_domain() except nis.error as e: raise RuntimeError(f\\"Failed to retrieve default NIS domain: {e}\\")"},{"question":"**Question Title: Implementing a Custom Buffer Protocol in Python** **Problem Statement:** As an experienced Python programmer, you are tasked with implementing a class that mimics the behavior of buffer objects using Python\'s newer buffer protocol. You need to create a custom class `CustomBuffer` that supports both read and write buffer interfaces. Additionally, you must provide methods to verify these capabilities and appropriately handle buffer views. Implement a class `CustomBuffer` with the following specifications: 1. **Initialization**: - The class should be initialized with a `bytearray` of a fixed size. 2. **Methods**: - `get_read_buffer()`: Returns a memoryview of the internal bytearray in read-only mode. - `get_write_buffer()`: Returns a memoryview of the internal bytearray in read-write mode. - `check_read_buffer()`: Checks if the object supports readable buffer interface and returns `True` if supported, `False` otherwise. - `check_write_buffer()`: Checks if the object supports writable buffer interface and returns `True` if supported, `False` otherwise. 3. **Constraints**: - Do not use the deprecated old buffer protocol functions. Use only the new buffer protocol features. - Your implementation must handle edge cases such as trying to access unreadable or unwritable buffers. **Input and Output Formats:** - `buffer_size` (int): size of the bytearray to initialize the CustomBuffer object. - Usage of methods on the `CustomBuffer` object to get results. **Example:** ```python # Initialization buffer_size = 10 custom_buffer = CustomBuffer(buffer_size) # Verify buffer capabilities print(custom_buffer.check_read_buffer()) # True print(custom_buffer.check_write_buffer()) # True # Access buffers read_buffer = custom_buffer.get_read_buffer() print(read_buffer) # memoryview object in read-only mode write_buffer = custom_buffer.get_write_buffer() print(write_buffer) # memoryview object in read-write mode ``` **Notes:** - Ensure that the `CustomBuffer` class is robust and can handle exceptions gracefully. - Test your class with various buffer sizes and operations to guarantee accurate performance. **Requirements:** - Implement the `CustomBuffer` class within the provided constraints. - Use Python 3 syntax and features. - Include inline comments to explain your code and logic. **Challenge:** - Optimize the `CustomBuffer` class for performance, considering both memory usage and computational efficiency.","solution":"class CustomBuffer: def __init__(self, size): self._buffer = bytearray(size) def get_read_buffer(self): Returns a memoryview of the internal bytearray in read-only mode. return memoryview(self._buffer) def get_write_buffer(self): Returns a memoryview of the internal bytearray in read-write mode. return memoryview(self._buffer) def check_read_buffer(self): Checks if the object supports readable buffer interface. try: view = memoryview(self._buffer) # Create a memoryview to test read buffer functionality view.tolist() # Try to read the buffer content return True except TypeError: return False def check_write_buffer(self): Checks if the object supports writable buffer interface. try: view = memoryview(self._buffer) # Create a memoryview to test write buffer functionality view[0] = 0 # Try to write to the buffer return True except (TypeError, ValueError): return False"},{"question":"# Clustering with Scikit-Learn Objective You are provided with a dataset containing samples with various features. Your task is to implement a function that performs clustering using the KMeans algorithm from scikit-learn. The function should return the cluster labels for each sample and the coordinates of the cluster centers. Description Implement the function `perform_kmeans_clustering(data, n_clusters)` where `data` is a 2D list representing the dataset (each inner list contains features of a single sample) and `n_clusters` is the integer number of clusters to form. Requirements: 1. Use the KMeans algorithm from the `sklearn.cluster` module. 2. Standardize the dataset before applying KMeans using `StandardScaler` from `sklearn.preprocessing`. 3. Return a dictionary with the following keys: - \'labels\': A list of integers indicating the cluster label for each sample. - \'cluster_centers\': A 2D list with the coordinates of the cluster centers. Function Signature ```python from typing import List, Dict def perform_kmeans_clustering(data: List[List[float]], n_clusters: int) -> Dict[str, List]: pass ``` Example ```python data = [ [1.0, 2.0], [1.5, 1.8], [5.0, 8.0], [8.0, 8.0], [1.0, 0.6], [9.0, 11.0], [8.0, 2.0], [10.0, 2.0], [9.0, 3.0], ] n_clusters = 3 result = perform_kmeans_clustering(data, n_clusters) # Result should be a dictionary with \'labels\' and \'cluster_centers\' # Example: # { # \'labels\': [0, 0, 1, 1, 0, 1, 2, 2, 2], # \'cluster_centers\': [[1.17, 1.47], [7.57, 8.67], [9.0, 2.33]] # } ``` Constraints - You may assume that the number of samples (rows in `data`) is between 10 and 1000. - The number of features (columns in `data`) is between 2 and 100. - The number of clusters (`n_clusters`) is at least 2 and at most 10. - The function should run efficiently within reasonable time and space limits.","solution":"from typing import List, Dict from sklearn.preprocessing import StandardScaler from sklearn.cluster import KMeans def perform_kmeans_clustering(data: List[List[float]], n_clusters: int) -> Dict[str, List]: Perform KMeans clustering on the provided dataset. :param data: A 2D list representing the dataset. Each inner list contains features of a single sample. :param n_clusters: The number of clusters to form. :return: A dictionary with cluster labels and cluster centers. # Standardize the data scaler = StandardScaler() data_standardized = scaler.fit_transform(data) # Perform KMeans clustering kmeans = KMeans(n_clusters=n_clusters, random_state=42) kmeans.fit(data_standardized) # Get the cluster labels and cluster centers labels = kmeans.labels_.tolist() cluster_centers = kmeans.cluster_centers_.tolist() return { \'labels\': labels, \'cluster_centers\': cluster_centers }"},{"question":"# Custom HTTP Server Using asyncio Transports and Protocols You are required to design and implement a basic HTTP server using the low-level asyncio Transports and Protocols API. The server should handle multiple client connections simultaneously, read incoming HTTP requests, and respond with a simple HTML content. You must demonstrate understanding of the following concepts: - Implementing a custom Protocol class. - Managing data read and write using Transports. - Handling client connections and disconnections. # Requirements: 1. **Class Structure**: - Create a class `HttpServerProtocol` inheriting from `asyncio.Protocol`. - Implement `connection_made`, `data_received`, and `connection_lost` methods. - The server should listen for incoming TCP connections on port `8080`. 2. **Handling HTTP Requests**: - When data is received, detect if it\'s a valid HTTP GET request. - If the request is valid, respond with a simple HTML content: ```html <html> <body> <h1>Hello, World!</h1> </body> </html> ``` - Close the connection after sending the response. 3. **Asynchronous Execution**: - Ensure that the server handles multiple clients simultaneously using asyncio\'s event loop. # Implementation: 1. **HttpServerProtocol Class**: - Define a class `HttpServerProtocol` that manages incoming connections, handles data reception, and sends responses. 2. **Server Setup**: - Create an asyncio compatible main function to set up the server on port `8080`. # Example Code Structure: ```python import asyncio class HttpServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\\"peername\\") print(f\\"Connection from {peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") if message.startswith(\'GET\'): response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/html; charset=UTF-8rn\\" \\"rn\\" \\"<html><body><h1>Hello, World!</h1></body></html>\\" ) self.transport.write(response.encode()) self.transport.close() else: self.transport.close() def connection_lost(self, exc): print(\\"Client disconnected\\") async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: HttpServerProtocol(), \'127.0.0.1\', 8080) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Constraints: - Assume the HTTP requests are well-formed. No need to handle malformed requests. - Focus on TCP connections; other types of transports are out of scope. - The server should continue running, handling multiple connections until externally stopped. # Evaluation Criteria: - Correct implementation of the `HttpServerProtocol` class. - Proper handling of client connections and data receptions. - Ability to asynchronously manage multiple connections. - Appropriate use of asyncio\'s transport and protocol methods.","solution":"import asyncio class HttpServerProtocol(asyncio.Protocol): def connection_made(self, transport): self.transport = transport peername = transport.get_extra_info(\\"peername\\") print(f\\"Connection from {peername}\\") def data_received(self, data): message = data.decode() print(f\\"Data received: {message}\\") if message.startswith(\'GET\'): response = ( \\"HTTP/1.1 200 OKrn\\" \\"Content-Type: text/html; charset=UTF-8rn\\" \\"rn\\" \\"<html><body><h1>Hello, World!</h1></body></html>\\" ) self.transport.write(response.encode()) self.transport.close() else: self.transport.close() def connection_lost(self, exc): print(\\"Client disconnected\\") async def main(): loop = asyncio.get_running_loop() server = await loop.create_server( lambda: HttpServerProtocol(), \'127.0.0.1\', 8080) async with server: await server.serve_forever() if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"# Objective Implement a Python function using PyTorch that samples from a mixture of two different probability distributions, combines the samples using a given weight, and then returns descriptive statistics (mean, variance) of the resulting samples. # Problem Statement You are given two probability distributions, `dist1` and `dist2`, from the `torch.distributions` module and a weight parameter `alpha`. The task is to: 1. Sample `n` samples from each distribution. 2. Combine these samples with the weight parameter `alpha`: `combined_samples = alpha * samples1 + (1 - alpha) * samples2`. 3. Calculate and return the mean and variance of the resulting samples. # Function Signature ```python def mixture_statistics(dist1, dist2, alpha, n): dist1: torch.distributions.Distribution The first probability distribution. dist2: torch.distributions.Distribution The second probability distribution. alpha: float Weight parameter ranging from 0 to 1. n: int Number of samples to draw from each distribution. Returns: tuple A tuple containing two elements: - mean of the combined samples - variance of the combined samples pass ``` # Inputs - `dist1`: An instance of `torch.distributions.Distribution` representing the first probability distribution. - `dist2`: An instance of `torch.distributions.Distribution` representing the second probability distribution. - `alpha`: A float in the range 0 to 1, representing the weight parameter. - `n`: An integer representing the number of samples to draw from each distribution. # Output - A tuple containing: - The mean of the combined samples (as a float). - The variance of the combined samples (as a float). # Constraints - The input distributions `dist1` and `dist2` will always have the same sample shape (i.e., the samples they return have the same number of dimensions). - `n` will be a positive integer. - `alpha` will be a float between 0 and 1 (inclusive). # Example ```python import torch from torch.distributions import Normal, Uniform dist1 = Normal(0, 1) dist2 = Uniform(-1, 1) alpha = 0.4 n = 1000 mean, variance = mixture_statistics(dist1, dist2, alpha, n) print(mean) # Output will be a float representing the mean print(variance) # Output will be a float representing the variance ``` # Notes - Use `torch` operations to ensure the solution efficiently handles large numbers of samples. - Thoroughly test the function with various distributions and parameters to ensure accuracy.","solution":"import torch def mixture_statistics(dist1, dist2, alpha, n): dist1: torch.distributions.Distribution The first probability distribution. dist2: torch.distributions.Distribution The second probability distribution. alpha: float Weight parameter ranging from 0 to 1. n: int Number of samples to draw from each distribution. Returns: tuple A tuple containing two elements: - mean of the combined samples - variance of the combined samples # Sample from each distribution samples1 = dist1.sample((n,)) samples2 = dist2.sample((n,)) # Combine the samples using the weight parameter alpha combined_samples = alpha * samples1 + (1 - alpha) * samples2 # Calculate descriptive statistics mean = combined_samples.mean().item() variance = combined_samples.var(unbiased=True).item() return mean, variance"},{"question":"Objective The goal of this task is to assess your ability to use the scikit-learn library in Python for data loading, preprocessing, model training, and evaluation. You will demonstrate your understanding by implementing a set of functions that work with the included toy datasets. Part 1: Data Loading and Preprocessing # Task 1: Load Dataset Write a function `load_dataset` that takes a string parameter `dataset_name` and loads the corresponding toy dataset. The supported dataset names are: \'iris\', \'diabetes\', \'digits\', \'linnerud\', \'wine\', and \'breast_cancer\'. Function Signature ```python def load_dataset(dataset_name: str): pass ``` Expected Input and Output - Input: One of the following strings: \'iris\', \'diabetes\', \'digits\', \'linnerud\', \'wine\', \'breast_cancer\'. - Output: A tuple (X, y) where X is the feature matrix and y is the target vector. # Task 2: Standardize Features Write a function `standardize_features` that standardizes the feature matrix. This function should take the feature matrix from `load_dataset` and return a standardized feature matrix using `sklearn.preprocessing.StandardScaler`. Function Signature ```python def standardize_features(X): pass ``` Expected Input and Output - Input: A feature matrix `X`. - Output: A standardized feature matrix. Part 2: Model Training and Evaluation # Task 3: Train Classifier Write a function `train_classifier` that trains a Random Forest classifier using the standardized feature matrix and target vector. Use 70% of the data for training and 30% for testing. Function Signature ```python from sklearn.ensemble import RandomForestClassifier def train_classifier(X, y): pass ``` Expected Input and Output - Input: A standardized feature matrix `X` and target vector `y`. - Output: A trained `RandomForestClassifier` model. # Task 4: Evaluate Model Write a function `evaluate_model` that evaluates the trained model on the test data. The function should return the accuracy score of the model. Function Signature ```python def evaluate_model(model, X_test, y_test): pass ``` Expected Input and Output - Input: A trained `RandomForestClassifier` model, test feature matrix `X_test`, and test target vector `y_test`. - Output: Accuracy score as a floating-point number. Part 3: Execution and Submission # Complete the Pipeline Combine the functions developed in Parts 1 and 2 to perform the following steps: 1. Load the \'wine\' dataset using `load_dataset`. 2. Standardize the feature matrix using `standardize_features`. 3. Train a Random Forest classifier on the standardized feature matrix using `train_classifier`. 4. Evaluate the trained model and print the accuracy. Submission: Submit a Jupyter Notebook or Python script containing: 1. All function implementations. 2. The code to execute the full pipeline on the \'wine\' dataset. 3. The printed accuracy score of the model.","solution":"from sklearn import datasets from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_dataset(dataset_name: str): Load a toy dataset from sklearn based on the dataset name. Parameters: - dataset_name (str): One of \'iris\', \'diabetes\', \'digits\', \'linnerud\', \'wine\', \'breast_cancer\'. Returns: - Tuple: (X, y) where X is the feature matrix and y is the target vector. dataset_loaders = { \'iris\': datasets.load_iris, \'diabetes\': datasets.load_diabetes, \'digits\': datasets.load_digits, \'linnerud\': datasets.load_linnerud, \'wine\': datasets.load_wine, \'breast_cancer\': datasets.load_breast_cancer } if dataset_name not in dataset_loaders: raise ValueError(\\"Dataset not supported. Choose from \'iris\', \'diabetes\', \'digits\', \'linnerud\', \'wine\', \'breast_cancer\'\\") data = dataset_loaders[dataset_name]() return data.data, data.target def standardize_features(X): Standardize the feature matrix using StandardScaler. Parameters: - X (numpy array): Feature matrix. Returns: - numpy array: Standardized feature matrix. scaler = StandardScaler() X_standardized = scaler.fit_transform(X) return X_standardized def train_classifier(X, y): Train a RandomForestClassifier on the dataset using 70% for training and 30% for testing. Parameters: - X (numpy array): Standardized feature matrix. - y (numpy array): Target vector. Returns: - model (RandomForestClassifier): Trained model. - X_test (numpy array): Test feature matrix. - y_test (numpy array): Test target vector. X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) model = RandomForestClassifier(random_state=42) model.fit(X_train, y_train) return model, X_test, y_test def evaluate_model(model, X_test, y_test): Evaluate the trained model on the test data. Parameters: - model (RandomForestClassifier): Trained model. - X_test (numpy array): Test feature matrix. - y_test (numpy array): Test target vector. Returns: - float: Accuracy score. y_pred = model.predict(X_test) return accuracy_score(y_test, y_pred) # Full pipeline execution for \'wine\' dataset X, y = load_dataset(\'wine\') X_standardized = standardize_features(X) model, X_test, y_test = train_classifier(X_standardized, y) accuracy = evaluate_model(model, X_test, y_test) print(f\\"Model Accuracy: {accuracy:.2f}\\")"},{"question":"# Question: BinHex Encoding and Decoding You are required to implement two functions using the `binhex` module to encode and decode files. Additionally, handle any potential errors that might occur in the process. **Function 1: `encode_to_binhex(input_file: str, output_file: str) -> None`** Convert a binary file with the filename `input_file` to a binhex-encoded file specified by `output_file`. * **Input**: - `input_file` (str): The path to the input binary file. - `output_file` (str): The path to the output binhex-encoded file. * **Output**: - None * **Constraints**: - If the input file does not exist or any other error occurs during encoding, an appropriate error message should be printed without raising an exception. **Function 2: `decode_from_binhex(input_file: str, output_file: str) -> None`** Decode a binhex-encoded file `input_file` back to its original form and save it in `output_file`. * **Input**: - `input_file` (str): The path to the input binhex-encoded file. - `output_file` (str): The path to the output decoded binary file. * **Output**: - None * **Constraints**: - If the input binhex file does not exist or any other error occurs during decoding, an appropriate error message should be printed without raising an exception. **Example Usage**: ```python try: encode_to_binhex(\'example_binary_file.bin\', \'encoded_file_hqx\') print(\\"Encoding successful.\\") except binhex.Error as e: print(f\\"Encoding failed: {e}\\") try: decode_from_binhex(\'encoded_file_hqx\', \'decoded_binary_file.bin\') print(\\"Decoding successful.\\") except binhex.Error as e: print(f\\"Decoding failed: {e}\\") ``` **Performance Requirements**: - The functions should handle files up to 100 MB efficiently. - Ensure that file handling is done using appropriate methods to avoid memory issues. Implement these functions in a Python script and ensure they handle potential errors gracefully, providing clear and informative error messages.","solution":"import binhex import os def encode_to_binhex(input_file: str, output_file: str) -> None: Convert a binary file to a binhex-encoded file. Parameters: - input_file (str): The path to the input binary file. - output_file (str): The path to the output binhex-encoded file. Returns: - None try: if not os.path.exists(input_file): print(f\\"Error: The input file \'{input_file}\' does not exist.\\") return with open(input_file, \'rb\') as fin, open(output_file, \'wb\') as fout: binhex.binhex(fin, fout) print(\\"Encoding successful.\\") except Exception as e: print(f\\"Encoding failed: {e}\\") def decode_from_binhex(input_file: str, output_file: str) -> None: Decode a binhex-encoded file back to its original binary form. Parameters: - input_file (str): The path to the input binhex-encoded file. - output_file (str): The path to the output decoded binary file. Returns: - None try: if not os.path.exists(input_file): print(f\\"Error: The input file \'{input_file}\' does not exist.\\") return with open(input_file, \'rb\') as fin, open(output_file, \'wb\') as fout: binhex.hexbin(fin, fout) print(\\"Decoding successful.\\") except Exception as e: print(f\\"Decoding failed: {e}\\")"},{"question":"Objective: Demonstrate your understanding of the Seaborn library for data visualization, focusing on advanced customization and combination of plots using the `seaborn.objects` module. Problem Statement: You are provided with two datasets: `\\"fmri\\"` and `\\"seaice\\"`. 1. From the `\\"fmri\\"` dataset, filter data only from the `\\"parietal\\"` region. 2. From the `\\"seaice\\"` dataset, filter data from the year 1980 onwards and focus on the columns `\\"1980\\"` and `\\"2019\\"`. Transform the date information to extract day of the year and pivot the dataset to have days of the year as the index. 3. Create the following visualizations using `seaborn.objects`: - A plot showing a line with a band indicating the signal values over time for different events in the filtered `\\"fmri\\"` dataset. The band should represent an error interval. - A plot showing a band with edges that fills between the min and max extent for the years 1980 and 2019. Customize the band to have 50% transparency and an edge width of 2 units. Input: - Use the `seaborn` package and relevant standard libraries. - Load the `fmri` and `seaice` datasets using `seaborn.load_dataset`. Output: - Display the two required plots. Constraints: - You should use the `seaborn.objects` module exclusively for creating plots. - Ensure the visualizations are clear and correctly labeled. Example Code to Get Started: ```python import seaborn.objects as so from seaborn import load_dataset # Load and filter the fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and transform the seaice dataset seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create the first plot with fmri data p1 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Create the second plot with seaice data p2 = ( so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") .add(so.Band(alpha=.5, edgewidth=2)) ) # Display the plots p1.show() p2.show() ``` Make sure your solution includes appropriate comments and explanations of your steps.","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd import matplotlib.pyplot as plt # Ensure the Date column is parsed as datetime when loading the seaice dataset def load_seaice_dataset(): df = load_dataset(\\"seaice\\") if \'Date\' in df.columns: df[\'Date\'] = pd.to_datetime(df[\'Date\']) return df # Load and filter the fmri dataset fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and transform the seaice dataset seaice = ( load_seaice_dataset() .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Create the first plot with fmri data p1 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Band(), so.Est()) .add(so.Line(), so.Agg()) ) # Create the second plot with seaice data p2 = ( so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") .add(so.Band(alpha=.5, edgewidth=2)) ) # Display the plots using matplotlib fig, ax = plt.subplots(1, 2, figsize=(15, 7)) p1.on(ax[0]) ax[0].set_title(\\"FMRI Signal Over Time\\") p2.on(ax[1]) ax[1].set_title(\\"Sea Ice Extent in 1980 vs 2019\\") plt.show()"},{"question":"**Question**: **Advanced SQLite Database Management with Python** You are tasked with developing a Python application that leverages the `sqlite3` module to manage a small library database. This library database will store information about books, authors, and the relationships between them. Your task is to implement a set of functions to perform the following operations: 1. **Create the Database and Tables**: - Create a new SQLite database named `library.db`. - Create the following tables: - `author` with columns `id` (INTEGER PRIMARY KEY AUTOINCREMENT), `name` (TEXT NOT NULL), and `birth_year` (INTEGER). - `book` with columns `id` (INTEGER PRIMARY KEY AUTOINCREMENT), `title` (TEXT NOT NULL), `publication_year` (INTEGER), and `author_id` (INTEGER, FOREIGN KEY references `author(id)`). 2. **Insert Data**: - Insert multiple authors and books into the respective tables. - Ensure that author names are unique. 3. **Retrieve Data**: - Fetch all books by a specific author. - Fetch all authors who have published a book after a specific year. 4. **Update Data**: - Update the publication year of all books by a specific author. 5. **Delete Data**: - Delete an author and all their books from the database. **Function Requirements**: 1. `create_database()`: Creates the `library.db` and tables if they do not exist. 2. `add_author(name: str, birth_year: int)`: Adds a new author to the `author` table, ensuring the name is unique. 3. `add_book(title: str, publication_year: int, author_id: int)`: Adds a new book to the `book` table. 4. `get_books_by_author(author_name: str) -> list`: Returns a list of books by a specific author. 5. `get_authors_by_publication_year(year: int) -> list`: Returns a list of authors who have published a book after the given year. 6. `update_publication_year(author_name: str, new_year: int)`: Updates the publication year of all books by the specified author. 7. `delete_author(author_name: str)`: Deletes the specified author and all their books from the database. **Constraints**: - Ensure database connections are properly handled (opened and closed). - Handle exceptions appropriately, ensuring the database remains in a consistent state. **Performance Requirements**: - Optimize the SQL queries for efficiency. - Ensure all operations perform well with a sizeable dataset. **Sample Input**: ```python create_database() add_author(\\"George Orwell\\", 1903) add_author(\\"Aldous Huxley\\", 1894) add_book(\\"1984\\", 1949, 1) add_book(\\"Animal Farm\\", 1945, 1) add_book(\\"Brave New World\\", 1932, 2) print(get_books_by_author(\\"George Orwell\\")) print(get_authors_by_publication_year(1930)) update_publication_year(\\"George Orwell\\", 1950) delete_author(\\"Aldous Huxley\\") print(get_books_by_author(\\"Aldous Huxley\\")) ``` **Expected Output**: ``` [(\'1984\', 1949), (\'Animal Farm\', 1945)] [\'George Orwell\', \'Aldous Huxley\'] [(\'1984\', 1950), (\'Animal Farm\', 1950)] [] ``` Implement the functions as described to manage the library database.","solution":"import sqlite3 def create_database(): connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS author ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL UNIQUE, birth_year INTEGER ) \'\'\') cursor.execute(\'\'\' CREATE TABLE IF NOT EXISTS book ( id INTEGER PRIMARY KEY AUTOINCREMENT, title TEXT NOT NULL, publication_year INTEGER, author_id INTEGER, FOREIGN KEY (author_id) REFERENCES author(id) ) \'\'\') connection.commit() connection.close() def add_author(name: str, birth_year: int): connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() try: cursor.execute(\'\'\' INSERT INTO author (name, birth_year) VALUES (?, ?) \'\'\', (name, birth_year)) connection.commit() except sqlite3.IntegrityError: print(f\\"Author \'{name}\' already exists.\\") connection.close() def add_book(title: str, publication_year: int, author_id: int): connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' INSERT INTO book (title, publication_year, author_id) VALUES (?, ?, ?) \'\'\', (title, publication_year, author_id)) connection.commit() connection.close() def get_books_by_author(author_name: str) -> list: connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' SELECT book.title, book.publication_year FROM book JOIN author ON book.author_id = author.id WHERE author.name = ? \'\'\', (author_name,)) books = cursor.fetchall() connection.close() return books def get_authors_by_publication_year(year: int) -> list: connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' SELECT DISTINCT author.name FROM author JOIN book ON author.id = book.author_id WHERE book.publication_year > ? \'\'\', (year,)) authors = cursor.fetchall() connection.close() return [author[0] for author in authors] def update_publication_year(author_name: str, new_year: int): connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' UPDATE book SET publication_year = ? WHERE author_id = (SELECT id FROM author WHERE name = ?) \'\'\', (new_year, author_name)) connection.commit() connection.close() def delete_author(author_name: str): connection = sqlite3.connect(\'library.db\') cursor = connection.cursor() cursor.execute(\'\'\' DELETE FROM book WHERE author_id = (SELECT id FROM author WHERE name = ?) \'\'\', (author_name,)) cursor.execute(\'\'\' DELETE FROM author WHERE name = ? \'\'\', (author_name,)) connection.commit() connection.close()"},{"question":"Coding Assessment Question # Objective Assess your understanding of the `seaborn` package, specifically focusing on the `regplot` function to create various types of regression plots and customize them according to specific requirements. # Task Using the `seaborn` package, write a function `create_custom_regression_plot` that takes a DataFrame, and several parameters to plot a regression graph with customized settings. You will then apply this function on the provided `\\"mpg\\"` dataset. # Function Signature ```python import pandas as pd import seaborn as sns from typing import Any def create_custom_regression_plot( df: pd.DataFrame, x: str, y: str, order: int = 1, logx: bool = False, lowess: bool = False, logistic: bool = False, robust: bool = False, ci: int = 95, marker: str = \'o\', color: str = \'b\', line_kws: dict = {} ) -> Any: pass ``` # Parameters - `df` (`pandas.DataFrame`): The input data frame containing the data. - `x` (`str`): The name of the column to be used for the x-axis. - `y` (`str`): The name of the column to be used for the y-axis. - `order` (`int`, optional): If greater than 1, use polynomial regression of this order. - `logx` (`bool`, optional): If True, use log transformation on the x-axis. - `lowess` (`bool`, optional): If True, use LOWESS smoothing. - `logistic` (`bool`, optional): If True, use logistic regression. - `robust` (`bool`, optional): If True, use robust regression. - `ci` (`int`, optional): The size of the confidence interval to draw; if `None`, no confidence interval is plotted. - `marker` (`str`, optional): Marker style for scatter plot points. - `color` (`str`, optional): Color of the scatter plot points. - `line_kws` (`dict`, optional): Additional keyword arguments for customizing the regression line (e.g., color). # Output - The function should return a `matplotlib.axes._axes.Axes` object representing the produced plot. # Example Usage ```python import seaborn as sns # Load the dataset mpg = sns.load_dataset(\\"mpg\\") ax = create_custom_regression_plot( df=mpg, x=\\"weight\\", y=\\"mpg\\", order=2, logx=False, lowess=False, logistic=False, robust=False, ci=99, marker=\\"x\\", color=\\".3\\", line_kws=dict(color=\\"r\\") ) # The produced plot should be displayed ``` # Notes - Ensure that your function handles missing or invalid data entries gracefully. - Thoroughly comment your code to explain the implementation. - Customize the appearance of the plot using the provided parameters. # Constraints - Use the seaborn version compatible with the provided documentation. - Only use the libraries `pandas`, `seaborn`, and `matplotlib`.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from typing import Any, Union def create_custom_regression_plot( df: pd.DataFrame, x: str, y: str, order: int = 1, logx: bool = False, lowess: bool = False, logistic: bool = False, robust: bool = False, ci: Union[int, None] = 95, marker: str = \'o\', color: str = \'b\', line_kws: dict = {} ) -> Any: Creates a seaborn regression plot with customized settings. Parameters: - df (pandas.DataFrame): The input data frame containing the data. - x (str): The name of the column to be used for the x-axis. - y (str): The name of the column to be used for the y-axis. - order (int, optional): If greater than 1, use polynomial regression of this order. - logx (bool, optional): If True, use log transformation on the x-axis. - lowess (bool, optional): If True, use LOWESS smoothing. - logistic (bool, optional): If True, use logistic regression. - robust (bool, optional): If True, use robust regression. - ci (int, optional): The size of the confidence interval to draw; if None, no confidence interval is plotted. - marker (str, optional): Marker style for scatter plot points. - color (str, optional): Color of the scatter plot points. - line_kws (dict, optional): Additional keyword arguments for customizing the regression line (e.g., color). Returns: - A matplotlib.axes._axes.Axes object representing the produced plot. sns.set(style=\\"darkgrid\\") ax = sns.regplot( data=df, x=x, y=y, order=order, logx=logx, lowess=lowess, logistic=logistic, robust=robust, ci=ci, marker=marker, color=color, line_kws=line_kws ) return ax"},{"question":"You are required to implement a function that generates and visualizes custom color palettes based on a given input color and various specifications. The function should take in parameters for the color, input system, number of colors, and whether the output should be a continuous colormap. Function Signature ```python def generate_and_visualize_palette(color: str, input_system: str = \\"rgb\\", num_colors: int = 6, as_cmap: bool = False) -> None: pass ``` Parameters 1. **color (str)**: The base color for the palette. It could be a color name, hex code, or a tuple representing the color in a specified input system. 2. **input_system (str, optional)**: The color specification system. Choices are `\\"rgb\\"`, `\\"husl\\"`. Default is `\\"rgb\\"`. 3. **num_colors (int, optional)**: The number of discrete colors to generate. Default is `6`. Should be a positive integer. 4. **as_cmap (bool, optional)**: Whether to return a continuous colormap instead of a discrete palette. Default is `False`. Returns - **None** Example Usage ```python # Example 1: Generate and visualize a dark palette in RGB system with 6 colors generate_and_visualize_palette(\\"seagreen\\") # Example 2: Generate and visualize a dark palette in husl system with 8 colors generate_and_visualize_palette((20, 60, 50), input_system=\\"husl\\", num_colors=8) # Example 3: Generate and visualize a continuous colormap based on a hex color code generate_and_visualize_palette(\\"#79C\\", as_cmap=True) ``` Question Requirements 1. Use the `sns.dark_palette` function to create the color palette based on the given parameters. 2. Visualize the generated color palette using an appropriate seaborn or matplotlib function. 3. Provide error handling for invalid input parameters, such as unsupported input systems or non-positive integer for `num_colors`. 4. Ensure the function is well-documented with comments explaining each step. Constraints - You can assume the `color` parameter will be a valid color representation in the given `input_system`. - Handle edge cases such as `num_colors` being `1`, and ensure that the function does not fail for such scenarios. - If `input_system` is invalid, raise a `ValueError` with a clear error message. You are expected to write clean, efficient, and well-commented code. The solution should be implemented in a Jupyter notebook with inline error checking and visualizations to facilitate understanding.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def generate_and_visualize_palette(color: str, input_system: str = \\"rgb\\", num_colors: int = 6, as_cmap: bool = False) -> None: Generates and visualizes a color palette based on the given parameters. Parameters: color (str): The base color for the palette. It could be a color name, hex code, or a tuple representing the color in a specified input system. input_system (str, optional): The color specification system. Choices are \\"rgb\\", \\"husl\\". Default is \\"rgb\\". num_colors (int, optional): The number of discrete colors to generate. Default is 6. Should be a positive integer. as_cmap (bool, optional): Whether to return a continuous colormap instead of a discrete palette. Default is False. Returns: None # Validate input parameters if input_system not in [\\"rgb\\", \\"husl\\"]: raise ValueError(\\"Invalid input_system. Supported systems are \'rgb\' and \'husl\'.\\") if not isinstance(num_colors, int) or num_colors <= 0: raise ValueError(\\"num_colors should be a positive integer.\\") # Generate color palette palette = sns.color_palette() if input_system == \\"rgb\\": palette = sns.dark_palette(color, n_colors=num_colors, input=\\"rgb\\", as_cmap=as_cmap) elif input_system == \\"husl\\": palette = sns.dark_palette(color, n_colors=num_colors, input=\\"husl\\", as_cmap=as_cmap) # Visualize the palette if as_cmap: # Create a colormap image gradient = np.linspace(0, 1, 256) gradient = np.vstack((gradient, gradient)) fig, ax = plt.subplots(figsize=(6, 2)) ax.imshow(gradient, aspect=\'auto\', cmap=palette) ax.set_axis_off() else: sns.palplot(palette) plt.show()"},{"question":"# Question: User Data Formatter and Recorder You are tasked with creating a small utility that reads user data from a JSON file, processes it, and writes formatted output to a new file. Your job is to implement a function that performs the following steps: 1. **Read user data** from a JSON file. The JSON file contains a list of user records. Each record contains user information such as `id`, `name`, `email`, and `registration_date`. 2. **Format the user data**: - Use formatted string literals (f-strings) to create a readable string for each user. - Each formatted string should include the user\'s ID, name, email, and registration date. 3. **Write the formatted strings** to a new file. Each user\'s information should be on a separate line in the output file. Function Signature ```python def format_and_record_user_data(input_file: str, output_file: str) -> None: pass ``` Parameters - `input_file` (str): The path to the input JSON file containing user data. - `output_file` (str): The path to the output file where formatted user data will be written. Input File Format The input JSON file will contain a list of dictionaries. Each dictionary represents a user and has the following structure: ```json [ { \\"id\\": 1, \\"name\\": \\"Alice\\", \\"email\\": \\"alice@example.com\\", \\"registration_date\\": \\"2023-01-15\\" }, { \\"id\\": 2, \\"name\\": \\"Bob\\", \\"email\\": \\"bob@example.com\\", \\"registration_date\\": \\"2023-01-18\\" } ] ``` Output File Format Each line in the output file should contain the formatted user information as follows: ``` ID: 1, Name: Alice, Email: alice@example.com, Registered on: 2023-01-15 ID: 2, Name: Bob, Email: bob@example.com, Registered on: 2023-01-18 ``` Constraints - You must use formatted string literals (f-strings) to format the user information. - Handle file operations using the `with` statement to ensure proper resource management. - You can assume that the input data is well-formed and does not contain errors. Example Usage ```python # Given a JSON file \'users.json\' with the provided format format_and_record_user_data(\'users.json\', \'formatted_users.txt\') ``` After executing the function, the `formatted_users.txt` file should contain the formatted user data as specified.","solution":"import json def format_and_record_user_data(input_file: str, output_file: str) -> None: Reads user data from a JSON file, formats it, and writes it to a new file. Parameters: input_file (str): The path to the input JSON file containing user data. output_file (str): The path to the output file where formatted user data will be written. # Read the input JSON file with open(input_file, \'r\') as infile: user_data = json.load(infile) # List to hold formatted user strings formatted_users = [] for user in user_data: formatted_user = f\\"ID: {user[\'id\']}, Name: {user[\'name\']}, Email: {user[\'email\']}, Registered on: {user[\'registration_date\']}\\" formatted_users.append(formatted_user) # Write the formatted strings to the output file with open(output_file, \'w\') as outfile: for formatted_user in formatted_users: outfile.write(formatted_user + \'n\')"},{"question":"# File Organizer using `pathlib` Objective Your task is to write a Python function that organizes files in a given directory based on their file extensions. The function should create subdirectories for each unique file extension and move the respective files into these subdirectories. Input - A string representing the path to the directory to be organized. Output - The function does not return a value, but it modifies the directory by creating the necessary subdirectories and moving files into them. Constraints 1. The input path will always be valid. 2. The directory may contain nested subdirectories, but only files in the root level of the input directory should be moved. 3. Ignore files without extensions. 4. Existing subdirectories should not be affected, and should not be considered when creating new subdirectories. 5. The function should handle potential errors gracefully. Specifications - Use the `pathlib` module for filesystem operations. - Ensure that the function runs efficiently and avoids unnecessary operations. - Handle permissions and other file-related errors using exception handling. Example Suppose the directory \\"/example_directory\\" contains the following files: ``` /example_directory - report.docx - data.csv - summary.docx - presentation.pptx - notes.txt ``` After running the function, the directory structure should be: ``` /example_directory /docx - report.docx - summary.docx /csv - data.csv /pptx - presentation.pptx /txt - notes.txt ``` Function Signature ```python from pathlib import Path def organize_files_by_extension(directory: str) -> None: # Your code here ``` # Implementation Tips 1. Use the `Path` class from `pathlib` to handle filesystem paths. 2. Iterate over the contents of the directory using `Path.iterdir()`. 3. Filter and group files by their extensions. 4. Create new subdirectories using `Path.mkdir()`. 5. Move files to their corresponding subdirectories using `Path.rename()`. # Testing Ensure to test your implementation with various directory structures, including edge cases such as: - An empty directory. - Files without extensions. - Directories containing subdirectories at the root level.","solution":"from pathlib import Path import shutil def organize_files_by_extension(directory: str) -> None: try: root_path = Path(directory) # Iterate over all files in the root directory for file_path in root_path.iterdir(): if file_path.is_file() and file_path.suffix: # Create a new directory for the file extension if it doesn\'t exist ext_directory = root_path / file_path.suffix[1:] ext_directory.mkdir(exist_ok=True) # Move the file to the new directory shutil.move(str(file_path), str(ext_directory / file_path.name)) except Exception as e: print(f\\"Error occurred: {e}\\")"},{"question":"# Pandas Coding Assessment Problem Statement You are given the task of analyzing sales data from various stores over several months. You need to write a function that demonstrates your understanding of pandas by performing a series of data manipulations and computations. Data Description The dataset contains the following columns: - `Date`: The date of the sale (format: YYYY-MM-DD). - `Store`: The store ID where the sale occurred. - `Product`: The product ID of the sold item. - `Quantity`: The number of units sold. - `Revenue`: The revenue generated from the sale. Below is a sample of the data: | Date | Store | Product | Quantity | Revenue | |------------|-------|---------|----------|---------| | 2020-01-01 | 1 | A | 5 | 100 | | 2020-01-02 | 1 | B | 3 | 60 | | 2020-01-01 | 2 | A | 2 | 40 | | 2020-01-02 | 2 | A | 7 | 140 | Function Specifications Implement the function `analyze_sales_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]` that: 1. **Resamples** the data to compute the total `Quantity` and `Revenue` per month. 2. **Groups** the data by `Store` and calculates the average `Revenue` per store. 3. **Plots** a bar chart showing the average monthly revenue per store. 4. Returns two DataFrames: - The first DataFrame should show the monthly total `Quantity` and `Revenue`. - The second DataFrame should show the average `Revenue` per store. Constraints - You can assume that the input DataFrame `df` is not empty and follows the schema described above. - Your solution should efficiently handle datasets with up to 100,000 rows. - Use appropriate pandas functions and methods to perform data manipulation and plotting. Example ```python import pandas as pd from typing import Tuple def analyze_sales_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]: # Resample data to monthly frequency for Quantity and Revenue df[\'Date\'] = pd.to_datetime(df[\'Date\']) monthly_data = df.resample(\'M\', on=\'Date\').sum()[[\'Quantity\', \'Revenue\']] # Group by Store and compute average Revenue average_revenue_per_store = df.groupby(\'Store\')[\'Revenue\'].mean().reset_index() # Plotting the average monthly Revenue per Store import matplotlib.pyplot as plt df.groupby([df[\'Date\'].dt.to_period(\'M\'), \'Store\'])[\'Revenue\'].mean().unstack().plot(kind=\'bar\', figsize=(10, 5)) plt.title(\'Average Monthly Revenue per Store\') plt.xlabel(\'Month\') plt.ylabel(\'Average Revenue\') plt.show() return monthly_data, average_revenue_per_store ``` Evaluation Criteria Your solution will be evaluated on: - Correctness: Does your solution produce the expected output? - Efficiency: Does your solution run within the time limits for large datasets? - Readability: Is your code well-organized and documented? - Plotting: Is the bar chart informative and correctly formatted? Write your solution below: ```python # Your code here ```","solution":"import pandas as pd from typing import Tuple import matplotlib.pyplot as plt def analyze_sales_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]: # Ensure the Date column is in datetime format df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Resample data to monthly frequency for Quantity and Revenue monthly_data = df.resample(\'M\', on=\'Date\').sum()[[\'Quantity\', \'Revenue\']].reset_index() # Group by Store and compute average Revenue per store average_revenue_per_store = df.groupby(\'Store\')[\'Revenue\'].mean().reset_index() # Plotting the average monthly Revenue per Store monthly_avg_revenue_per_store = df.groupby([df[\'Date\'].dt.to_period(\'M\'), \'Store\'])[\'Revenue\'].mean().unstack().fillna(0) monthly_avg_revenue_per_store.plot(kind=\'bar\', figsize=(12, 6)) plt.title(\'Average Monthly Revenue per Store\') plt.xlabel(\'Month\') plt.ylabel(\'Average Revenue\') plt.xticks(rotation=45) plt.tight_layout() plt.show() return monthly_data, average_revenue_per_store"},{"question":"Objective The following question is designed to test your understanding of using various normalization techniques within PyTorch models, especially in scenarios involving batched tensor operations where in-place updates are not supported. Problem Statement You are required to write a PyTorch class that encapsulates a Convolutional Neural Network (CNN) and provides different options for normalization. Your implementation should demonstrate an understanding of both BatchNorm and GroupNorm. Task Create a class `CustomNet` that extends `nn.Module`. The class should allow initialization with a choice of normalization type. Specifically, your implementation should support: 1. Batch Normalization without running statistics. 2. Group Normalization. In addition to constructing the network, provide a method to patch existing BatchNorm layers to work without running statistics using `functorch` as described in Option 3 of the documentation provided. The class should have the following methods: - `__init__(self, norm_type: str, num_features: int, num_groups: int = None)`: Constructor method. Parameters: - `norm_type`: A string that can be either \\"batch_norm\\" or \\"group_norm\\". - `num_features`: The number of features (channels) for the normalization layer. - `num_groups`: The number of groups for GroupNorm (relevant only if `norm_type` is \\"group_norm\\"). - `forward(self, x: torch.Tensor) -> torch.Tensor`: Forward pass method. - `patch_batch_norm(self)`: If the model uses BatchNorm, this method should patch all BatchNorm layers to not use running stats using `functorch`. Constraints - You may assume the input tensor `x` is of shape `(N, C, H, W)` where `N` is batch size, `C` is the number of channels, `H` and `W` are height and width. - For GroupNorm, ensure `C % num_groups == 0`. Example Usage ```python import torch from custom_net import CustomNet # Assume your implementation is in custom_net.py # Example with BatchNorm without running stats net1 = CustomNet(norm_type=\\"batch_norm\\", num_features=64) x = torch.rand(8, 64, 32, 32) # Example input output1 = net1(x) net1.patch_batch_norm() # Applying patch # Example with GroupNorm net2 = CustomNet(norm_type=\\"group_norm\\", num_features=64, num_groups=8) output2 = net2(x) ``` Expected Output The example usage showcases how to initialize the network, perform a forward pass, and apply a patch if necessary. Your implementation should handle these operations correctly as per the provided problem statement.","solution":"import torch import torch.nn as nn from torch.nn import functional as F from functorch import make_functional, vmap class CustomNet(nn.Module): def __init__(self, norm_type: str, num_features: int, num_groups: int = None): super(CustomNet, self).__init__() if norm_type not in [\'batch_norm\', \'group_norm\']: raise ValueError(f\\"Invalid norm_type {norm_type}. Expected \'batch_norm\' or \'group_norm\'.\\") self.norm_type = norm_type if norm_type == \'batch_norm\': self.norm_layer = nn.BatchNorm2d(num_features, track_running_stats=False) elif norm_type == \'group_norm\': if num_groups is None: raise ValueError(\\"num_groups must be specified for group_norm\\") if num_features % num_groups != 0: raise ValueError(\\"num_features must be divisible by num_groups for group_norm\\") self.norm_layer = nn.GroupNorm(num_groups, num_features) def forward(self, x: torch.Tensor) -> torch.Tensor: return self.norm_layer(x) def patch_batch_norm(self): if self.norm_type == \'batch_norm\': for module in self.modules(): if isinstance(module, nn.BatchNorm2d): module.track_running_stats = False"},{"question":"**Title**: Exploring and Visualizing Generated Datasets with scikit-learn **Objective**: Implement functions to generate, visualize, and analyze various datasets using scikit-learn\'s random sample generators. **Question**: You are tasked with generating several synthetic datasets using scikit-learn\'s sample generators and performing exploratory data analysis on them. Write the following functions to accomplish this task: 1. **generate_dataset** - **Input**: - `dataset_type` (str): Type of the dataset to generate. Possible values are `\'classification\'`, `\'blobs\'`, `\'gaussian_quantiles\'`, `\'moons\'`, `\'circles\'`. - `n_samples` (int): Number of samples to generate. - `random_state` (int): Seed used by the random number generator. - **Output**: - Returns a tuple `(X, y)` where `X` is a 2D numpy array of shape `(n_samples, n_features)` and `y` is a 1D numpy array of shape `(n_samples, )`. 2. **plot_dataset** - **Input**: - `X` (numpy array): Feature matrix of shape `(n_samples, n_features)`. - `y` (numpy array): Target array of shape `(n_samples, )`. - `title` (str): Title for the plot. - **Output**: Displays a scatter plot of the dataset. 3. **analyze_dataset** - **Input**: - `X` (numpy array): Feature matrix of shape `(n_samples, n_features)`. - `y` (numpy array): Target array of shape `(n_samples, )`. - **Output**: - Prints the number of samples and features. - Prints the unique classes and their counts in the dataset. **Constraints**: - `n_samples` should be a positive integer. - Supported `dataset_type` values are `\'classification\'`, `\'blobs\'`, `\'gaussian_quantiles\'`, `\'moons\'`, `\'circles\'`. **Example usage**: ```python # Generate a dataset X, y = generate_dataset(dataset_type=\'moons\', n_samples=100, random_state=42) # Plot the dataset plot_dataset(X, y, title=\'Moons Dataset\') # Analyze the dataset analyze_dataset(X, y) ``` **Additional Notes**: - Use `make_classification` for classification datasets, `make_blobs` for blobs, `make_gaussian_quantiles` for Gaussian quantiles, `make_moons` for moons, and `make_circles` for circles. - Ensure that the plots are informative and well-labeled. - Provide comments in the code to explain the steps.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.datasets import make_classification, make_blobs, make_gaussian_quantiles, make_moons, make_circles def generate_dataset(dataset_type, n_samples, random_state): Generates a dataset based on given type, number of samples, and random state. Parameters: dataset_type (str): Type of the dataset to generate. n_samples (int): Number of samples to generate. random_state (int): Seed used by the random number generator. Returns: tuple: A tuple (X, y) where X is a numpy array of shape (n_samples, n_features) and y is a numpy array of shape (n_samples, ). # Ensure the number of samples is positive if n_samples <= 0: raise ValueError(\\"n_samples should be a positive integer.\\") # Generate the dataset based on the type if dataset_type == \'classification\': X, y = make_classification(n_samples=n_samples, random_state=random_state) elif dataset_type == \'blobs\': X, y = make_blobs(n_samples=n_samples, random_state=random_state) elif dataset_type == \'gaussian_quantiles\': X, y = make_gaussian_quantiles(n_samples=n_samples, random_state=random_state) elif dataset_type == \'moons\': X, y = make_moons(n_samples=n_samples, random_state=random_state) elif dataset_type == \'circles\': X, y = make_circles(n_samples=n_samples, random_state=random_state) else: raise ValueError(\\"Unsupported dataset type.\\") return X, y def plot_dataset(X, y, title): Displays a scatter plot of the dataset. Parameters: X (numpy array): Feature matrix of shape (n_samples, n_features). y (numpy array): Target array of shape (n_samples, ). title (str): Title for the plot. plt.figure(figsize=(8, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.rainbow, edgecolor=\'k\', s=20) plt.title(title) plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.colorbar() plt.show() def analyze_dataset(X, y): Prints the number of samples and features, and the unique classes and their counts in the dataset. Parameters: X (numpy array): Feature matrix of shape (n_samples, n_features). y (numpy array): Target array of shape (n_samples, ). num_samples, num_features = X.shape unique_classes, counts = np.unique(y, return_counts=True) print(f\\"Number of samples: {num_samples}\\") print(f\\"Number of features: {num_features}\\") print(\\"Unique classes and their counts:\\") for cls, count in zip(unique_classes, counts): print(f\\"Class {cls}: {count} samples\\")"},{"question":"**MaskedTensor Operations and Reduction** In this assignment, you will demonstrate your understanding of PyTorch\'s `MaskedTensor` by implementing functions to create a `MaskedTensor`, apply various operations, and compute reductions. # Problem Statement 1. **Create a MaskedTensor**: - Write a function `create_masked_tensor` that takes a tensor of data and a tensor of mask values and returns a `MaskedTensor`. - **Input**: - data: A `torch.Tensor` - mask: A `torch.Tensor` of boolean values with the same shape as `data` - **Output**: - A `MaskedTensor` object. 2. **Apply Unary Operations**: - Write a function `apply_unary_operations` that applies given unary operations to a `MaskedTensor`. - **Input**: - masked_tensor: A `MaskedTensor` - operations: A list of strings, each corresponding to a unary operation. - **Output**: - A list of `MaskedTensor` objects, each being the result of applying one of the supplied unary operations. 3. **Apply Binary Operations**: - Write a function `apply_binary_operations` that applies given binary operations to two `MaskedTensor` objects. - **Input**: - masked_tensor1: A `MaskedTensor` - masked_tensor2: A `MaskedTensor` (with the same shape and mask as `masked_tensor1`) - operations: A list of strings, each corresponding to a binary operation. - **Output**: - A list of `MaskedTensor` objects, each being the result of applying one of the supplied binary operations. 4. **Perform Reductions**: - Write a function `compute_reductions` that computes specified reductions on a `MaskedTensor`. - **Input**: - masked_tensor: A `MaskedTensor` - reductions: A list of strings, each corresponding to a reduction operation. - **Output**: - A list of results from performing the specified reduction operations. # Requirements: - Each function should correctly handle the `MaskedTensor` and its mask. - Ensure that the operations are applied only to the specified (unmasked) parts of the tensor. - If an invalid operation is passed, raise a `ValueError`. # Example Usage: ```python import torch from torch.masked import masked_tensor def create_masked_tensor(data, mask): return masked_tensor(data, mask) def apply_unary_operations(masked_tensor, operations): unary_results = [] for op in operations: try: result = getattr(torch, op)(masked_tensor) unary_results.append(result) except AttributeError: raise ValueError(f\\"Unary operation \'{op}\' is not supported.\\") return unary_results def apply_binary_operations(masked_tensor1, masked_tensor2, operations): binary_results = [] for op in operations: try: result = getattr(torch, op)(masked_tensor1, masked_tensor2) binary_results.append(result) except AttributeError: raise ValueError(f\\"Binary operation \'{op}\' is not supported.\\") return binary_results def compute_reductions(masked_tensor, reductions): reduction_results = [] for red in reductions: try: result = getattr(masked_tensor, red)() reduction_results.append(result) except AttributeError: raise ValueError(f\\"Reduction operation \'{red}\' is not supported.\\") return reduction_results # Example data data = torch.tensor([[1, 2, 0], [4, 0, 6], [7, 8, 9]], dtype=torch.float32) mask = torch.tensor([[True, True, False], [True, False, True], [True, True, True]]) # Create MaskedTensor mt = create_masked_tensor(data, mask) # Apply Unary Operations unary_ops = [\'abs\', \'sqrt\'] unary_results = apply_unary_operations(mt, unary_ops) # Apply Binary Operations mt2 = create_masked_tensor(data * 2, mask) binary_ops = [\'add\', \'mul\'] binary_results = apply_binary_operations(mt, mt2, binary_ops) # Perform Reductions reductions = [\'sum\', \'mean\'] reduction_results = compute_reductions(mt, reductions) ``` # Constraints: - All inputs must be non-empty tensor objects. - All masks must be boolean tensors of the same shape as the data tensors. - The operations list should only contain valid PyTorch operations as strings. # Additional Notes: - You can use the `torch.masked` functionality to create and manipulate `MaskedTensor`. - The `masked_tensor` function to create a `MaskedTensor` is provided by the module. - Ensure that your code is efficient and handles all edge cases appropriately. - Use appropriate exception handling to manage invalid operations and inputs.","solution":"import torch from torch.masked import masked_tensor, MaskedTensor def create_masked_tensor(data, mask): Returns a MaskedTensor created from the data and mask provided. if not isinstance(data, torch.Tensor) or not isinstance(mask, torch.Tensor): raise TypeError(\\"Both data and mask should be torch.Tensors\\") if data.shape != mask.shape: raise ValueError(\\"Data and mask must have the same shape\\") return masked_tensor(data, mask) def apply_unary_operations(masked_tensor, operations): Apply the given unary operations to masked_tensor. Returns a list of MaskedTensors resulting from the operations. if not isinstance(masked_tensor, MaskedTensor): raise TypeError(\\"masked_tensor must be a MaskedTensor\\") unary_results = [] for op in operations: try: result = getattr(torch, op)(masked_tensor) unary_results.append(result) except AttributeError: raise ValueError(f\\"Unary operation \'{op}\' is not supported.\\") return unary_results def apply_binary_operations(masked_tensor1, masked_tensor2, operations): Apply the given binary operations to masked_tensor1 and masked_tensor2. Returns a list of MaskedTensors resulting from the operations. if not isinstance(masked_tensor1, MaskedTensor) or not isinstance(masked_tensor2, MaskedTensor): raise TypeError(\\"Both inputs must be MaskedTensor\\") if masked_tensor1.shape != masked_tensor2.shape: raise ValueError(\\"Both MaskedTensors must have the same shape\\") binary_results = [] for op in operations: try: result = getattr(torch, op)(masked_tensor1, masked_tensor2) binary_results.append(result) except AttributeError: raise ValueError(f\\"Binary operation \'{op}\' is not supported.\\") return binary_results def compute_reductions(masked_tensor, reductions): Compute the specified reduction operations on masked_tensor. Returns a list of results from the operations. if not isinstance(masked_tensor, MaskedTensor): raise TypeError(\\"masked_tensor must be a MaskedTensor\\") reduction_results = [] for red in reductions: try: result = getattr(masked_tensor, red)() reduction_results.append(result) except AttributeError: raise ValueError(f\\"Reduction operation \'{red}\' is not supported.\\") return reduction_results"},{"question":"# Categorical Data Handling with Pandas Objective Demonstrate your understanding of handling and manipulating categorical data in pandas. Problem Statement You are given two datasets in pandas DataFrame format: ```python import pandas as pd df1 = pd.DataFrame({ \\"Department\\": [\\"Sales\\", \\"Sales\\", \\"HR\\", \\"HR\\", \\"IT\\", \\"IT\\", \\"IT\\"], \\"EmployeeID\\": [101, 102, 201, 202, 301, 302, 303], \\"Gender\\": [\\"F\\", \\"M\\", \\"F\\", \\"F\\", \\"M\\", \\"M\\", \\"F\\"] }) df2 = pd.DataFrame({ \\"Department\\": [\\"Sales\\", \\"Sales\\", \\"HR\\", \\"IT\\", \\"Admin\\"], \\"EmployeeID\\": [103, 104, 205, 306, 101], \\"Gender\\": [\\"M\\", \\"F\\", \\"M\\", \\"F\\", \\"F\\"] }) ``` Tasks 1. Convert the `Department` and `Gender` columns in both DataFrames to categorical types. 2. Ensure the `Department` categories are consistent across both DataFrames and ordered as follows: `[\\"Admin\\", \\"HR\\", \\"IT\\", \\"Sales\\"]`. 3. Rename the `Gender` categories to `[\\"Female\\", \\"Male\\"]`. 4. Combine both DataFrames into a single DataFrame. 5. Ensure that the combined DataFrame maintains the categorical types and orders. 6. Identify and replace any missing departments with \\"Unknown\\". 7. Sort the combined DataFrame first by `Department` according to the predefined order and then by `EmployeeID`. Requirements 1. Your solution should handle both DataFrames separately and then combine them. 2. Follow the prescribed order for departments. 3. Handle any missing data appropriately. 4. Ensure that the final DataFrame retains the categorical dtype and proper orderings. Input and Output Format - **Input:** Two pandas DataFrames (`df1` and `df2`) as described above. - **Output:** A single pandas DataFrame with applied transformations. ```python # Expected function signature def manage_categorical_data(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: pass ``` Example Output ```python combined_df = manage_categorical_data(df1, df2) print(combined_df) ``` The `combined_df` should look like this: ``` Department EmployeeID Gender 0 Admin 101 Female 1 HR 201 Female 2 HR 202 Female 3 HR 205 Male 4 IT 301 Male 5 IT 302 Male 6 IT 303 Female 7 IT 306 Female 8 Sales 101 Female 9 Sales 102 Male 10 Sales 103 Male 11 Sales 104 Female ``` Remember to follow the instructions carefully and ensure that the categories and orders are properly managed.","solution":"import pandas as pd def manage_categorical_data(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: # Define the categories for Department and Gender department_order = [\\"Admin\\", \\"HR\\", \\"IT\\", \\"Sales\\"] gender_order = [\\"Female\\", \\"Male\\"] # Convert columns to categorical with specified orders df1[\'Department\'] = pd.Categorical(df1[\'Department\'], categories=department_order, ordered=True) df1[\'Gender\'] = df1[\'Gender\'].replace({\\"F\\": \\"Female\\", \\"M\\": \\"Male\\"}) df1[\'Gender\'] = pd.Categorical(df1[\'Gender\'], categories=gender_order, ordered=True) df2[\'Department\'] = pd.Categorical(df2[\'Department\'], categories=department_order, ordered=True) df2[\'Gender\'] = df2[\'Gender\'].replace({\\"F\\": \\"Female\\", \\"M\\": \\"Male\\"}) df2[\'Gender\'] = pd.Categorical(df2[\'Gender\'], categories=gender_order, ordered=True) # Combine the DataFrames combined_df = pd.concat([df1, df2], ignore_index=True) # Fill missing departments with \\"Unknown\\" combined_df[\'Department\'] = combined_df[\'Department\'].cat.add_categories(\\"Unknown\\").fillna(\\"Unknown\\") # Sort by Department and then by EmployeeID combined_df = combined_df.sort_values(by=[\'Department\', \'EmployeeID\']).reset_index(drop=True) return combined_df"},{"question":"Semi-Supervised Learning with Label Propagation Objective You are required to implement a semi-supervised learning solution using Scikit-learn\'s `LabelPropagation` algorithm. Your task is to train a model on a dataset with both labeled and unlabeled data and then evaluate its performance. Dataset Description You will utilize the classic Iris dataset provided by Scikit-learn for this task. Assume that 50% of the labels are missing (i.e., they are set to -1) to simulate the semi-supervised learning scenario. Requirements 1. **Data Loading and Preparation**: - Load the Iris dataset. - Randomly set 50% of the labels to -1 to simulate unlabeled data. 2. **Model Implementation**: - Create and configure a `LabelPropagation` model using the RBF kernel. 3. **Model Training**: - Fit the `LabelPropagation` model on the dataset with both labeled and unlabeled data. 4. **Evaluation**: - Evaluate the performance of the model by measuring its accuracy on the fully labeled part of the dataset. Input and Output Formats - **Input**: - None (the dataset is loaded within the code). - **Output**: - Print the accuracy score after training the model. Constraints - Use `LabelPropagation` from `sklearn.semi_supervised`. - Ensure reproducibility by setting a random seed where necessary. Code Template ```python import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelPropagation from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def semi_supervised_learning(): # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Set random seed for reproducibility np.random.seed(42) # Set 50% of the labels to -1 to simulate unlabeled data mask = np.random.rand(len(y)) < 0.5 y_unlabeled = y.copy() y_unlabeled[mask] = -1 # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Create the LabelPropagation model using RBF kernel label_prop_model = LabelPropagation(kernel=\'rbf\') # Train the model with labeled and unlabeled data label_prop_model.fit(X_train, y_train) # Predict on the test set y_pred = label_prop_model.predict(X_test) # Compute and print the accuracy score accuracy = accuracy_score(y_test, y_pred) print(f\'Accuracy: {accuracy:.2f}\') # Call the function to execute the code semi_supervised_learning() ``` Explanation - **Data Loading and Preparation**: The Iris dataset is loaded, and 50% of the labels are set to -1 to create a semi-supervised learning scenario. - **Model Implementation and Training**: A `LabelPropagation` model with an RBF kernel is configured and trained using the labeled and unlabeled data. - **Evaluation**: The accuracy of the model is evaluated and printed to demonstrate performance.","solution":"import numpy as np from sklearn import datasets from sklearn.semi_supervised import LabelPropagation from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def semi_supervised_learning(): # Load the Iris dataset iris = datasets.load_iris() X = iris.data y = iris.target # Set random seed for reproducibility np.random.seed(42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) # Set 50% of the training labels to -1 to simulate unlabeled data mask = np.random.rand(len(y_train)) < 0.5 y_train[mask] = -1 # Create the LabelPropagation model using RBF kernel label_prop_model = LabelPropagation(kernel=\'rbf\') # Train the model with labeled and unlabeled data label_prop_model.fit(X_train, y_train) # Predict on the test set y_pred = label_prop_model.predict(X_test) # Compute and print the accuracy score accuracy = accuracy_score(y_test, y_pred) return accuracy"},{"question":"You are tasked with creating a Python script that processes command-line arguments to manage a user\'s to-do list. The script should support the following functionalities based on the provided arguments: 1. **Add a task:** - Short option: `-a` followed by the task\'s name. - Long option: `--add` followed by the task\'s name. 2. **Delete a task:** - Short option: `-d` followed by the task number. - Long option: `--delete` followed by the task number. 3. **List all tasks:** - Short option: `-l` - Long option: `--list` 4. **Show help message:** - Short option: `-h` - Long option: `--help` Implement the following functionalities in your script: - Parse the command-line arguments using `getopt.getopt` or `getopt.gnu_getopt`. - Handle invalid options gracefully by displaying an appropriate error message. - Perform corresponding actions based on the parsed arguments. - Store tasks in a list within the script (do not persist them to a file or database). Here is the expected behavior of the script: - When listing tasks, print each task with its number. - When adding a task, append it to the list. - When deleting a task, remove it by its number (1-based index). Constraints: - If both `-a`/`--add` and `-d`/`--delete` are provided, only the first one should be processed. - Print a help message when `-h`/`--help` is included, regardless of other arguments. - Tasks should only be displayed if `-l`/`--list` is provided explicitly. Example usage: ```sh python todolist.py -a \\"Buy groceries\\" python todolist.py --add \\"Finish homework\\" python todolist.py -d 1 python todolist.py --list python todolist.py -h ``` The help message should include a brief description of each supported option. Your program should follow this template: ```python import sys import getopt tasks = [] def print_help(): print(\\"Usage: todolist.py [options]\\") print(\\"Options:\\") print(\\" -a, --add <task> Add a task\\") print(\\" -d, --delete <num> Delete a task by number\\") print(\\" -l, --list List all tasks\\") print(\\" -h, --help Show this help message\\") def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"ha:d:l\\", [\\"help\\", \\"add=\\", \\"delete=\\", \\"list\\"]) except getopt.GetoptError as err: print(err) print_help() sys.exit(2) for o, a in opts: if o in (\\"-h\\", \\"--help\\"): print_help() sys.exit() elif o in (\\"-a\\", \\"--add\\"): tasks.append(a) elif o in (\\"-d\\", \\"--delete\\"): try: task_num = int(a) - 1 if 0 <= task_num < len(tasks): tasks.pop(task_num) else: print(f\\"Error: Task number {a} does not exist.\\") except ValueError: print(\\"Error: Task number must be an integer.\\") elif o in (\\"-l\\", \\"--list\\"): if tasks: for index, task in enumerate(tasks, start=1): print(f\\"{index}. {task}\\") else: print(\\"No tasks available.\\") if not opts: print_help() if __name__ == \\"__main__\\": main() ```","solution":"import sys import getopt tasks = [] def print_help(): print(\\"Usage: todolist.py [options]\\") print(\\"Options:\\") print(\\" -a, --add <task> Add a task\\") print(\\" -d, --delete <num> Delete a task by number\\") print(\\" -l, --list List all tasks\\") print(\\" -h, --help Show this help message\\") def main(): try: opts, args = getopt.getopt(sys.argv[1:], \\"ha:d:l\\", [\\"help\\", \\"add=\\", \\"delete=\\", \\"list\\"]) except getopt.GetoptError as err: print(err) print_help() sys.exit(2) if not opts: print_help() return for o, a in opts: if o in (\\"-h\\", \\"--help\\"): print_help() return elif o in (\\"-a\\", \\"--add\\"): tasks.append(a) return elif o in (\\"-d\\", \\"--delete\\"): try: task_num = int(a) - 1 if 0 <= task_num < len(tasks): tasks.pop(task_num) else: print(f\\"Error: Task number {a} does not exist.\\") except ValueError: print(\\"Error: Task number must be an integer.\\") return elif o in (\\"-l\\", \\"--list\\"): if tasks: for index, task in enumerate(tasks, start=1): print(f\\"{index}. {task}\\") else: print(\\"No tasks available.\\") return"},{"question":"# Complex Asynchronous Task Management Objective: Implement a function that: 1. Runs multiple asynchronous tasks concurrently. 2. Uses synchronization primitives to manage task execution order. 3. Handles specific exceptions that can occur during task execution. Task: You need to implement a function `manage_tasks` that: - Uses `asyncio.gather()` to run multiple tasks concurrently. - Uses an `asyncio.Lock` to ensure that certain tasks run in a controlled manner. - Uses `asyncio.TimeoutError` to handle tasks that take too long to complete and fallback to an alternative task in such a case. Function Signature: ```python import asyncio async def manage_tasks(task_list: list, timeout: int) -> dict: :param task_list: List of asyncio tasks functions to be executed :param timeout: Integer timeout in seconds for tasks. :return: Dictionary with task names as keys and their results or exceptions as values ``` Input: - `task_list`: A list of task functions (each function should be asynchronous and return a string result). - `timeout`: An integer that represents the time in seconds after which a task times out. Output: - A dictionary with task names as keys and their results or exceptions as values. Example Tasks: You also need to implement some example asynchronous tasks that will be passed to `manage_tasks`: ```python import asyncio async def task_a(): await asyncio.sleep(2) return \\"task_a completed\\" async def task_b(): await asyncio.sleep(5) return \\"task_b completed\\" async def task_c(): await asyncio.sleep(1) raise asyncio.CancelledError(\\"task_c was cancelled\\") async def task_d(): await asyncio.sleep(8) return \\"task_d completed\\" ``` Constraints: - You must use `asyncio.Lock` to ensure that `task_a` and `task_b` do not run simultaneously. - If any task exceeds the provided timeout, it should be caught as an `asyncio.TimeoutError` and noted in the result dictionary. - If any task gets cancelled, this must also be reflected in the results. Example Usage: ```python tasks = [task_a, task_b, task_c, task_d] result = await manage_tasks(tasks, timeout=4) print(result) # Expected output may vary based on implemented logic for exception handling and task execution order ``` Performance: The function should efficiently handle up to 10 tasks. This question assesses understanding of: - Asynchronous programming with asyncio. - Task management and synchronization. - Exception handling in async code. - Proper use of asyncio constructs.","solution":"import asyncio from typing import List, Dict async def manage_tasks(task_list: List, timeout: int) -> Dict[str, str]: Manages the execution of multiple asynchronous tasks with both concurrency and synchronization primitives, and handles specific exceptions. :param task_list: List of asynchronous task functions to be executed. :param timeout: Integer timeout in seconds for tasks. :return: Dictionary with task names as keys and their results or exceptions as values. results = {} lock = asyncio.Lock() async def run_task_with_lock(task, task_name): Function to run a task with the specified lock and handle exceptions. try: if task in (task_a, task_b): async with lock: result = await asyncio.wait_for(task(), timeout) else: result = await asyncio.wait_for(task(), timeout) results[task_name] = result except asyncio.TimeoutError: results[task_name] = f\\"{task_name} timed out\\" except asyncio.CancelledError: results[task_name] = f\\"{task_name} was cancelled\\" except Exception as e: results[task_name] = f\\"{task_name} raised an exception: {str(e)}\\" # Running all tasks concurrently await asyncio.gather(*(run_task_with_lock(task, task.__name__) for task in task_list)) return results # Example async tasks for testing async def task_a(): await asyncio.sleep(2) return \\"task_a completed\\" async def task_b(): await asyncio.sleep(5) return \\"task_b completed\\" async def task_c(): await asyncio.sleep(1) raise asyncio.CancelledError(\\"task_c was cancelled\\") async def task_d(): await asyncio.sleep(8) return \\"task_d completed\\""},{"question":"# Kernel Density Estimation using scikit-learn **Objective**: Assess the ability to implement and apply Kernel Density Estimation (KDE) using scikit-learn. **Instructions**: 1. **Data Preparation**: - Create a synthetic dataset of 200 samples that follows a bimodal distribution using `numpy`. The two modes should be centered at -2 and 3 with a standard deviation of 0.5. 2. **Kernel Density Estimation (KDE)**: - Implement KDE on this synthetic dataset using scikit-learn\'s `KernelDensity` class with a Gaussian kernel. - Experiment with different values of the bandwidth parameter (e.g., 0.1, 0.5, 1.0) to observe its effect on the density estimation. 3. **Visualization**: - Plot the original dataset\'s histogram and overlay the KDE results for each bandwidth value using matplotlib. 4. **Sampling New Data Points**: - Use the fitted KDE model to generate 100 new samples from the estimated density. - Visualize both the original and newly sampled data points on the same plot. **Requirements**: - Your implementation should include functions or methods for each step (data preparation, KDE fitting, visualization, and sampling). - Clearly indicate how the bandwidth parameter affects the smoothness of the KDE. - Ensure that your plots are well-labeled and easy to interpret. **Input/Output**: - Input: The synthetic dataset and bandwidth values. - Output: Plots showing the histogram of the dataset with KDE overlays and a plot showing original and newly sampled data points. **Code**: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_bimodal_data(n_samples=200, centers=(-2, 3), std=0.5): Generate a bimodal dataset with specified centers and standard deviation. Parameters: n_samples (int): Number of samples to generate for each mode. centers (tuple): Centers of the two modes. std (float): Standard deviation of each mode. Returns: numpy array: Combined dataset. data1 = np.random.normal(centers[0], std, int(n_samples / 2)) data2 = np.random.normal(centers[1], std, int(n_samples / 2)) return np.concatenate([data1, data2]) def fit_kde(data, bandwidth, kernel=\'gaussian\'): Fit a Kernel Density Estimation model on the data. Parameters: data (numpy array): Input data array. bandwidth (float): Bandwidth parameter for the KDE. kernel (str): Kernel to use for density estimation. Returns: KernelDensity object: Fitted KDE model. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data[:, None]) return kde def plot_kde(data, kde_models, bandwidths): Plot the histogram and KDE estimates for different bandwidths. Parameters: data (numpy array): Input data array. kde_models (list): List of fitted KDE models. bandwidths (list): List of bandwidth values corresponding to the KDE models. x_d = np.linspace(min(data) - 1, max(data) + 1, 1000)[:, None] plt.figure(figsize=(12, 6)) plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\') colors = [\'blue\', \'green\', \'red\'] for idx, kde in enumerate(kde_models): log_dens = kde.score_samples(x_d) plt.plot(x_d[:, 0], np.exp(log_dens), \'-\', color=colors[idx], label=f\'Bandwidth = {bandwidths[idx]}\') plt.title(\'Histogram and KDE with Different Bandwidths\') plt.xlabel(\'Data Values\') plt.ylabel(\'Density\') plt.legend() plt.show() def sample_new_data(kde, n_samples=100): Generate new samples from the KDE model. Parameters: kde (KernelDensity object): Fitted KDE model. n_samples (int): Number of new samples to generate. Returns: numpy array: New dataset. return kde.sample(n_samples) # Generate the synthetic dataset data = generate_bimodal_data() # Fit KDE models with different bandwidths bandwidths = [0.1, 0.5, 1.0] kde_models = [fit_kde(data, bw) for bw in bandwidths] # Plot histogram and KDE results plot_kde(data, kde_models, bandwidths) # Generate new samples from the KDE with bandwidth 0.5 new_samples = sample_new_data(kde_models[1]) # Plot original and new samples plt.figure(figsize=(12, 6)) plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', label=\'Original Data\') plt.hist(new_samples, bins=30, density=True, alpha=0.5, color=\'blue\', label=\'New Samples\') plt.title(\'Original and New Samples from KDE\') plt.xlabel(\'Data Values\') plt.ylabel(\'Density\') plt.legend() plt.show() ``` **Evaluation Criteria**: - Correct implementation of data generation, KDE fitting, and sampling. - Clear and accurate visualization of results. - Proper experimentation with bandwidth parameter and its effects. - Clean and well-structured code following best practices.","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity def generate_bimodal_data(n_samples=200, centers=(-2, 3), std=0.5): Generate a bimodal dataset with specified centers and standard deviation. Parameters: n_samples (int): Number of samples to generate for each mode. centers (tuple): Centers of the two modes. std (float): Standard deviation of each mode. Returns: numpy array: Combined dataset. data1 = np.random.normal(centers[0], std, int(n_samples / 2)) data2 = np.random.normal(centers[1], std, int(n_samples / 2)) return np.concatenate([data1, data2]) def fit_kde(data, bandwidth, kernel=\'gaussian\'): Fit a Kernel Density Estimation model on the data. Parameters: data (numpy array): Input data array. bandwidth (float): Bandwidth parameter for the KDE. kernel (str): Kernel to use for density estimation. Returns: KernelDensity object: Fitted KDE model. kde = KernelDensity(kernel=kernel, bandwidth=bandwidth) kde.fit(data[:, None]) return kde def plot_kde(data, kde_models, bandwidths): Plot the histogram and KDE estimates for different bandwidths. Parameters: data (numpy array): Input data array. kde_models (list): List of fitted KDE models. bandwidths (list): List of bandwidth values corresponding to the KDE models. x_d = np.linspace(min(data) - 1, max(data) + 1, 1000)[:, None] plt.figure(figsize=(12, 6)) plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\') colors = [\'blue\', \'green\', \'red\'] for idx, kde in enumerate(kde_models): log_dens = kde.score_samples(x_d) plt.plot(x_d[:, 0], np.exp(log_dens), \'-\', color=colors[idx], label=f\'Bandwidth = {bandwidths[idx]}\') plt.title(\'Histogram and KDE with Different Bandwidths\') plt.xlabel(\'Data Values\') plt.ylabel(\'Density\') plt.legend() plt.show() def sample_new_data(kde, n_samples=100): Generate new samples from the KDE model. Parameters: kde (KernelDensity object): Fitted KDE model. n_samples (int): Number of new samples to generate. Returns: numpy array: New dataset. return kde.sample(n_samples) # Generate the synthetic dataset data = generate_bimodal_data() # Fit KDE models with different bandwidths bandwidths = [0.1, 0.5, 1.0] kde_models = [fit_kde(data, bw) for bw in bandwidths] # Plot histogram and KDE results plot_kde(data, kde_models, bandwidths) # Generate new samples from the KDE with bandwidth 0.5 new_samples = sample_new_data(kde_models[1]) # Plot original and new samples plt.figure(figsize=(12, 6)) plt.hist(data, bins=30, density=True, alpha=0.5, color=\'gray\', label=\'Original Data\') plt.hist(new_samples, bins=30, density=True, alpha=0.5, color=\'blue\', label=\'New Samples\') plt.title(\'Original and New Samples from KDE\') plt.xlabel(\'Data Values\') plt.ylabel(\'Density\') plt.legend() plt.show()"},{"question":"Question: Nearest Neighbors Algorithm Comparison and Implementation # Objective: Write a Python program using Scikit-learn to implement, compare, and evaluate different nearest neighbors algorithms (BallTree, KDTree, and brute-force) for both classification and regression tasks on a synthetic dataset. # Instructions: 1. **Data Generation:** - Generate a synthetic dataset for classification and regression tasks. For classification, create a dataset with two classes. For regression, create a dataset with continuous labels. 2. **Nearest Neighbors Implementation:** - Implement three nearest neighbors algorithms (BallTree, KDTree, and brute-force) using Scikit-learn. - Use `KNeighborsClassifier` for the classification task and `KNeighborsRegressor` for the regression task. 3. **Model Training and Evaluation:** - Train the models using the synthetic datasets. - Evaluate the models using appropriate metrics (accuracy for classification and mean squared error for regression). - Compare the performance of different algorithms in terms of computation time and evaluation metrics. 4. **Visualization:** - Visualize the decision boundaries for the classification task. - Plot the true vs. predicted values for the regression task. # Expected Input and Output Formats: - **Input:** - No specific input required from the user. - **Output:** - Performance metrics for each algorithm in terms of accuracy (classification) and mean squared error (regression). - Computation time for each algorithm. - Visualization plots for the classification decision boundaries and regression predictions. # Constraints: - Use only Scikit-learn and standard Python libraries (numpy, matplotlib). - Ensure the program is efficient and well-documented. # Performance Requirements: - Ensure the implementations are optimized for runtime performance. - Report the computation times for each algorithm separately. # Example Code (for reference): ```python from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt import numpy as np import time # Step 1: Data Generation X_class, y_class = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, random_state=42) X_reg, y_reg = make_regression(n_samples=500, n_features=2, noise=0.1, random_state=42) # Splitting the datasets X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42) X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42) # Function to evaluate and time the algorithms def evaluate_model(model, X_train, y_train, X_test, y_test, task=\'classification\'): start_time = time.time() model.fit(X_train, y_train) predictions = model.predict(X_test) elapsed_time = time.time() - start_time if task == \'classification\': accuracy = accuracy_score(y_test, predictions) return accuracy, elapsed_time else: mse = mean_squared_error(y_test, predictions) return mse, elapsed_time # Step 2: Implementing and Evaluating Algorithms algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] results_classification = {} results_regression = {} for algo in algorithms: knn_class = KNeighborsClassifier(algorithm=algo) accuracy, time_elapsed = evaluate_model(knn_class, X_class_train, y_class_train, X_class_test, y_class_test) results_classification[algo] = {\'accuracy\': accuracy, \'time\': time_elapsed} knn_reg = KNeighborsRegressor(algorithm=algo) mse, time_elapsed = evaluate_model(knn_reg, X_reg_train, y_reg_train, X_reg_test, y_reg_test, task=\'regression\') results_regression[algo] = {\'mse\': mse, \'time\': time_elapsed} # Step 3: Visualization # Visualizing classification decision boundaries plt.figure(figsize=(12, 6)) for i, algo in enumerate(algorithms): knn_class = KNeighborsClassifier(algorithm=algo) knn_class.fit(X_class_train, y_class_train) plt.subplot(2, 2, i+1) plt.title(f\\"Algorithm: {algo}\\") x_min, x_max = X_class[:, 0].min() - 1, X_class[:, 0].max() + 1 y_min, y_max = X_class[:, 1].min() - 1, X_class[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) Z = knn_class.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X_class_train[:, 0], X_class_train[:, 1], c=y_class_train, s=20, edgecolor=\'k\') plt.tight_layout() plt.show() # Visualizing regression actual vs predicted values plt.figure(figsize=(12, 6)) for i, algo in enumerate(algorithms): knn_reg = KNeighborsRegressor(algorithm=algo) knn_reg.fit(X_reg_train, y_reg_train) predictions = knn_reg.predict(X_reg_test) plt.subplot(2, 2, i+1) plt.title(f\\"Algorithm: {algo}\\") plt.scatter(y_reg_test, predictions, edgecolor=\'k\', alpha=0.7) plt.plot([y_reg_test.min(), y_reg_test.max()], [y_reg_test.min(), y_reg_test.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predictions\') plt.tight_layout() plt.show() # Step 4: Print Results print(\\"Classification Results:\\") for algo in results_classification: print(f\\"Algorithm: {algo}, Accuracy: {results_classification[algo][\'accuracy\']}, Time: {results_classification[algo][\'time\']}\\") print(\\"nRegression Results:\\") for algo in results_regression: print(f\\"Algorithm: {algo}, MSE: {results_regression[algo][\'mse\']}, Time: {results_regression[algo][\'time\']}\\") ```","solution":"from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor from sklearn.metrics import accuracy_score, mean_squared_error import matplotlib.pyplot as plt import numpy as np import time # Step 1: Data Generation X_class, y_class = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, random_state=42) X_reg, y_reg = make_regression(n_samples=500, n_features=2, noise=0.1, random_state=42) # Splitting the datasets X_class_train, X_class_test, y_class_train, y_class_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42) X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42) # Function to evaluate and time the algorithms def evaluate_model(model, X_train, y_train, X_test, y_test, task=\'classification\'): start_time = time.time() model.fit(X_train, y_train) predictions = model.predict(X_test) elapsed_time = time.time() - start_time if task == \'classification\': accuracy = accuracy_score(y_test, predictions) return accuracy, elapsed_time else: mse = mean_squared_error(y_test, predictions) return mse, elapsed_time # Step 2: Implementing and Evaluating Algorithms algorithms = [\'auto\', \'ball_tree\', \'kd_tree\', \'brute\'] results_classification = {} results_regression = {} for algo in algorithms: knn_class = KNeighborsClassifier(algorithm=algo) accuracy, time_elapsed = evaluate_model(knn_class, X_class_train, y_class_train, X_class_test, y_class_test) results_classification[algo] = {\'accuracy\': accuracy, \'time\': time_elapsed} knn_reg = KNeighborsRegressor(algorithm=algo) mse, time_elapsed = evaluate_model(knn_reg, X_reg_train, y_reg_train, X_reg_test, y_reg_test, task=\'regression\') results_regression[algo] = {\'mse\': mse, \'time\': time_elapsed} # Step 3: Visualization # Visualizing classification decision boundaries plt.figure(figsize=(12, 6)) for i, algo in enumerate(algorithms): knn_class = KNeighborsClassifier(algorithm=algo) knn_class.fit(X_class_train, y_class_train) plt.subplot(2, 2, i+1) plt.title(f\\"Algorithm: {algo}\\") x_min, x_max = X_class[:, 0].min() - 1, X_class[:, 0].max() + 1 y_min, y_max = X_class[:, 1].min() - 1, X_class[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1)) Z = knn_class.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) plt.contourf(xx, yy, Z, alpha=0.8) plt.scatter(X_class_train[:, 0], X_class_train[:, 1], c=y_class_train, s=20, edgecolor=\'k\') plt.tight_layout() plt.show() # Visualizing regression actual vs predicted values plt.figure(figsize=(12, 6)) for i, algo in enumerate(algorithms): knn_reg = KNeighborsRegressor(algorithm=algo) knn_reg.fit(X_reg_train, y_reg_train) predictions = knn_reg.predict(X_reg_test) plt.subplot(2, 2, i+1) plt.title(f\\"Algorithm: {algo}\\") plt.scatter(y_reg_test, predictions, edgecolor=\'k\', alpha=0.7) plt.plot([y_reg_test.min(), y_reg_test.max()], [y_reg_test.min(), y_reg_test.max()], \'k--\', lw=2) plt.xlabel(\'True Values\') plt.ylabel(\'Predictions\') plt.tight_layout() plt.show() # Step 4: Print Results print(\\"Classification Results:\\") for algo in results_classification: print(f\\"Algorithm: {algo}, Accuracy: {results_classification[algo][\'accuracy\']}, Time: {results_classification[algo][\'time\']}\\") print(\\"nRegression Results:\\") for algo in results_regression: print(f\\"Algorithm: {algo}, MSE: {results_regression[algo][\'mse\']}, Time: {results_regression[algo][\'time\']}\\")"},{"question":"**Objective:** In this exercise, you will demonstrate your proficiency with the `fileinput` module by implementing a Python function that processes multiple text files, counts the occurrences of a specific word, and handles different encodings. **Problem Statement:** You need to implement a function `count_word_occurrences_in_files(files: List[str], word: str, encoding:str=\'utf-8\') -> Dict[str, int]` that: 1. Takes a list of file names (`files`), a target word (`word`), and an optional encoding (`encoding`) as input. 2. Iterates through all the files line by line, using the specified encoding. 3. Counts the occurrences of the target word in each file and returns a dictionary where the keys are file names and the values are the respective counts of the word occurrences. 4. Ignore case when counting word occurrences. 5. Skip any files that cannot be opened or read, printing an error message for each skipped file. **Input:** - `files`: A list of strings representing file names. Example: `[\\"file1.txt\\", \\"file2.txt\\"]` - `word`: A string representing the word to count. Example: ` \\"example\\"` - `encoding`: A string specifying the encoding of the files (default is `\'utf-8\'`). Example: `\'utf-8\'` **Output:** - A dictionary where keys are file names and values are integers representing the count of the word occurrences. Example: `{ \\"file1.txt\\": 3, \\"file2.txt\\": 5 }` **Constraints:** - The word count must be case-insensitive. - Handle empty files gracefully. - Assume that the files are text files. **Function Signature:** ```python from typing import List, Dict import fileinput def count_word_occurrences_in_files(files: List[str], word: str, encoding: str = \'utf-8\') -> Dict[str, int]: pass ``` # Example: ```python # Assume we have the following text in \'file1.txt\' and \'file2.txt\': # file1.txt: \\"Example text with the word example.\\" # file2.txt: \\"Another example text. Example example example.\\" files = [\\"file1.txt\\", \\"file2.txt\\"] word = \\"example\\" encoding = \\"utf-8\\" result = count_word_occurrences_in_files(files, word, encoding) print(result) # Output: {\'file1.txt\': 2, \'file2.txt\': 4} ``` # Notes: - Utilize the `fileinput` module for reading the files. - Implement appropriate error handling for file opening/reading issues. - Ensure your function is efficient and reads lines sequentially from the files. Avoid loading entire files into memory.","solution":"from typing import List, Dict import fileinput import re def count_word_occurrences_in_files(files: List[str], word: str, encoding: str = \'utf-8\') -> Dict[str, int]: word_counts = {file: 0 for file in files} word_pattern = re.compile(re.escape(word), re.IGNORECASE) for file in files: try: with fileinput.input(files=(file,), openhook=fileinput.hook_encoded(encoding)) as f: for line in f: word_counts[file] += len(word_pattern.findall(line)) except (IOError, UnicodeDecodeError) as e: print(f\\"Error processing file {file}: {e}\\") word_counts[file] = 0 # Setting the count to 0 for files that couldn\'t be processed. return word_counts"},{"question":"**Coding Assessment Question** # Objective: The objective of this question is to gauge your understanding of seaborn\'s functionality in controlling and customizing the aesthetics and context of plots. # Problem Statement: Write a function named `customized_sinplot` that: 1. **Takes the following parameters**: - `n` (default=10): The number of sine waves to plot. - `flip` (default=1): Used to flip the sine waves. - `theme` (default=\\"darkgrid\\"): One of the preset themes (\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"). - `context` (default=\\"notebook\\"): One of the preset contexts (\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). - `font_scale` (default=1.5): Scaling factor for font size. - `line_width` (default=2.5): Width of the lines in the plot. - `spine_offset` (default=10): Offset for the spines. - `remove_left_spine` (default=False): A boolean indicating whether to remove the left spine. 2. **Generates a sine plot with the specified number of waves and flips them as needed**. 3. **Applies the specified theme and context settings to the plot**. 4. **Uses the specified `font_scale` and `line_width` to customize the plot**. 5. **Removes the spines as specified**: - Always removes the right and top spines. - Optionally removes the left spine if `remove_left_spine` is True. - Offsets the spines by the specified `spine_offset`. 6. **Displays the final customized plot**. # Function Signature: ```python def customized_sinplot(n=10, flip=1, theme=\\"darkgrid\\", context=\\"notebook\\", font_scale=1.5, line_width=2.5, spine_offset=10, remove_left_spine=False): pass ``` # Example Usage: ```python # Example: Customized sinplot with \\"whitegrid\\" theme, \\"talk\\" context, larger font, thicker lines, and removed left spine customized_sinplot(n=8, flip=-1, theme=\\"whitegrid\\", context=\\"talk\\", font_scale=2, line_width=3, spine_offset=15, remove_left_spine=True) ``` # Constraints: - Use seaborn version 0.11.2 or later. - Ensure that matplotlib and seaborn are both imported. - The function should handle invalid input by raising appropriate errors (e.g., invalid theme or context). # Notes: - You are encouraged to include comments in your code for clarity. - Make sure the plot is displayed within the function using `plt.show()`.","solution":"import matplotlib.pyplot as plt import seaborn as sns import numpy as np def customized_sinplot(n=10, flip=1, theme=\\"darkgrid\\", context=\\"notebook\\", font_scale=1.5, line_width=2.5, spine_offset=10, remove_left_spine=False): Generates a customized sine plot. Parameters: - n (int): Number of sine waves to plot. - flip (int): Used to flip the sine waves. - theme (str): One of the preset themes (\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"). - context (str): One of the preset contexts (\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"). - font_scale (float): Scaling factor for font size. - line_width (float): Width of the lines in the plot. - spine_offset (int): Offset for the spines. - remove_left_spine (bool): Whether to remove the left spine. # Validate theme valid_themes = [\\"darkgrid\\", \\"whitegrid\\", \\"dark\\", \\"white\\", \\"ticks\\"] if theme not in valid_themes: raise ValueError(f\\"Invalid theme \'{theme}\'. Valid options are {valid_themes}.\\") # Validate context valid_contexts = [\\"paper\\", \\"notebook\\", \\"talk\\", \\"poster\\"] if context not in valid_contexts: raise ValueError(f\\"Invalid context \'{context}\'. Valid options are {valid_contexts}.\\") # Apply theme and context settings sns.set_theme(style=theme) sns.set_context(context, font_scale=font_scale) # Generate the sine plots x = np.linspace(0, 14, 100) for i in range(n): plt.plot(x, np.sin(x + i * .5) * (n - i) * flip, linewidth=line_width) # Customize spines ax = plt.gca() sns.despine(ax=ax, offset=spine_offset, right=True, top=True, left=remove_left_spine) # Display the plot plt.show()"},{"question":"Feature Extraction with Scikit-learn Objective: Design a function to preprocess a collection of text documents using feature extraction and transformation techniques provided by scikit-learn. Problem Statement: You are given a collection of text documents. Your task is to implement a function `extract_tf_idf_features` that performs feature extraction using the Tf-idf representation. Specifications: 1. The function should take in a list of text documents and: - Preprocess the text by tokenizing the documents. - Vectorize the documents using the Tf-idf scheme. 2. Use `TfidfVectorizer` from `sklearn.feature_extraction.text`. 3. Your function should return: - A sparse matrix of shape `(n_documents, n_features)`, where `n_documents` is the number of documents, and `n_features` is the number of unique terms across all documents. - A list of the feature names corresponding to the columns of the sparse matrix. Function Signature: ```python from typing import List, Tuple from scipy.sparse import csr_matrix from sklearn.feature_extraction.text import TfidfVectorizer def extract_tf_idf_features(documents: List[str]) -> Tuple[csr_matrix, List[str]]: Extracts Tf-idf features from a list of text documents. Parameters: documents (List[str]): A list of text documents. Returns: Tuple[csr_matrix, List[str]]: A tuple consisting of a sparse matrix of Tf-idf features and a list of feature names. pass ``` Example: ```python docs = [ \'This is the first document.\', \'This document is the second document.\', \'And the third one.\', \'Is this the first document?\' ] X, feature_names = extract_tf_idf_features(docs) # X is a sparse matrix representing the Tf-idf feature vectors # feature_names is a list of feature names corresponding to the columns of X ``` Constraints: - The function should handle an empty list of documents by returning an empty sparse matrix and an empty list of feature names. - Performance should be efficient, leveraging the capabilities of scikit-learn for text preprocessing and vectorization. You may use any utility functions provided by scikit-learn to accomplish the task. The primary goal is to demonstrate your understanding of text feature extraction and transformations using scikit-learn.","solution":"from typing import List, Tuple from scipy.sparse import csr_matrix from sklearn.feature_extraction.text import TfidfVectorizer def extract_tf_idf_features(documents: List[str]) -> Tuple[csr_matrix, List[str]]: Extracts Tf-idf features from a list of text documents. Parameters: documents (List[str]): A list of text documents. Returns: Tuple[csr_matrix, List[str]]: A tuple consisting of a sparse matrix of Tf-idf features and a list of feature names. if not documents: return csr_matrix((0, 0)), [] vectorizer = TfidfVectorizer() X = vectorizer.fit_transform(documents) feature_names = vectorizer.get_feature_names_out().tolist() return X, feature_names"},{"question":"# Advanced Python Import System Challenge The `pkgutil` module in Python 3.10 provides various utilities to interact with and extend Python\'s import system. One of the common tasks when working with packages is to list all modules available within a package and sometimes retrieve data from these modules. **Task: Module Inspector** You are required to write a function `inspect_package(path: Optional[List[str]], prefix: str, resource_name: str) -> Dict[str, str]:` that: 1. Lists all modules recursively within the given path using `pkgutil.walk_packages`. 2. For each module, if the module is a package, it should attempt to retrieve a specified resource file from the package using `pkgutil.get_data`. 3. Returns a dictionary where the keys are the dotted names of packages, and the values are the contents of the retrieved resource file as a string. If a resource is not found for a package, it should not be added to the dictionary. Expected Function Signature ```python from typing import List, Dict, Optional import pkgutil def inspect_package(path: Optional[List[str]], prefix: str, resource_name: str) -> Dict[str, str]: pass ``` Example ```python # Considering path is list of directory strings where some \'*package_name*\' directories are there with nested modules # Suppose resource_name is \\"config.json\\" # Your function would generate: # { # \\"package_name.sub_package_name\\": \\"{json_content_of_config_file}\\", # \\"another_package\\": \\"{json_content_of_config_file_in_another_package}\\", # ... # } ``` Constraints 1. The `resource_name` will be a valid file name that can exist within a package. 2. It is safe to assume that the contents of the resource file (if found) will be decodable as a UTF-8 string. 3. The function should handle ImportErrors if a package cannot be loaded, but all other exceptions should be propagated. **Notes:** - Consider using `pkgutil.walk_packages` to list all modules and packages. - Use `pkgutil.get_data` to retrieve resources from each package. - Handle errors gracefully where packages might not contain the specified resource.","solution":"from typing import List, Dict, Optional import pkgutil def inspect_package(path: Optional[List[str]], prefix: str, resource_name: str) -> Dict[str, str]: Lists all modules recursively within the given path using pkgutil.walk_packages, retrieves a specified resource file from the package using pkgutil.get_data, and returns a dictionary where the keys are the dotted names of packages, and the values are the contents of the retrieved resource file as a string. :param path: The list of paths to search for modules. :param prefix: The prefix that will be used while searching for modules. :param resource_name: The name of the resource file to retrieve. :return: A dictionary of package names to their resource contents. result = {} for importer, modname, ispkg in pkgutil.walk_packages(path, prefix): if ispkg: try: data = pkgutil.get_data(modname, resource_name) if data: result[modname] = data.decode(\'utf-8\') except ImportError: # If a package cannot be loaded, we simply skip it. continue return result"},{"question":"**Objective:** To assess your understanding of tensor views in PyTorch, memory efficiency, and the difference between views and new tensors post-manipulation. **Question:** Write a function `tensor_view_operations` that takes a 2D tensor of size `(4, 4)` filled with random values as input and performs the following operations: 1. Create a view of the original tensor that reshapes it into size `(2, 8)`. 2. Verify if the view shares the same underlying data with the original tensor. 3. Modify an element in the view and confirm the change is reflected in the original tensor. 4. Create another view using tensor transposition and check if it is contiguous. 5. If the transpose view is not contiguous, convert it into a contiguous tensor. Return a dictionary containing: - The reshaped view tensor of size `(2, 8)`. - Boolean values indicating whether the reshaped view shares data with the original tensor. - The modified original tensor after changing an element in the view. - The transposed view and its contiguity status. - The final contiguous tensor after ensuring the transpose view is contiguous. **Input:** - A 2D tensor of size `(4, 4)`. **Output:** - A dictionary containing keys: `\'reshaped_view\'`, `\'shares_data\'`, `\'modified_original\'`, `\'transposed_view\'`, `\'is_transposed_contiguous\'`, and `\'contiguous_tensor\'`. ```python import torch def tensor_view_operations(tensor): # 1. Create a view of the original tensor that reshapes it into size (2, 8) reshaped_view = tensor.view(2, 8) # 2. Verify if the view shares the same underlying data with the original tensor shares_data = tensor.storage().data_ptr() == reshaped_view.storage().data_ptr() # 3. Modify an element in the view and confirm the change is reflected in the original tensor reshaped_view[0][0] = 3.14 modified_original = tensor.clone() # Use `clone` to avoid in-place modifications during updates # 4. Create another view using tensor transposition and check if it is contiguous transposed_view = tensor.transpose(0, 1) is_transposed_contiguous = transposed_view.is_contiguous() # 5. If the transpose view is not contiguous, convert it into a contiguous tensor if not is_transposed_contiguous: contiguous_tensor = transposed_view.contiguous() else: contiguous_tensor = transposed_view return { \'reshaped_view\': reshaped_view, \'shares_data\': shares_data, \'modified_original\': modified_original, \'transposed_view\': transposed_view, \'is_transposed_contiguous\': is_transposed_contiguous, \'contiguous_tensor\': contiguous_tensor } ``` **Constraints:** - You are not allowed to use any additional libraries for this question. - Ensure that your solution is efficient in terms of memory usage. **Performance Requirement:** The function should execute within a reasonable time frame for large tensors.","solution":"import torch def tensor_view_operations(tensor): # 1. Create a view of the original tensor that reshapes it into size (2, 8) reshaped_view = tensor.view(2, 8) # 2. Verify if the view shares the same underlying data with the original tensor shares_data = tensor.storage().data_ptr() == reshaped_view.storage().data_ptr() # 3. Modify an element in the view and confirm the change is reflected in the original tensor reshaped_view[0][0] = 3.14 modified_original = tensor.clone() # Use `clone` to avoid in-place modifications during updates # 4. Create another view using tensor transposition and check if it is contiguous transposed_view = tensor.transpose(0, 1) is_transposed_contiguous = transposed_view.is_contiguous() # 5. If the transpose view is not contiguous, convert it into a contiguous tensor if not is_transposed_contiguous: contiguous_tensor = transposed_view.contiguous() else: contiguous_tensor = transposed_view return { \'reshaped_view\': reshaped_view, \'shares_data\': shares_data, \'modified_original\': modified_original, \'transposed_view\': transposed_view, \'is_transposed_contiguous\': is_transposed_contiguous, \'contiguous_tensor\': contiguous_tensor }"},{"question":"Secure POP3 Client with Message Retrieval **Objective:** Design a function that securely connects to a POP3 server, authenticates with the provided credentials, retrieves and prints all email messages from the mailbox, and properly handles any potential exceptions. **Details:** Implement the function `retrieve_emails` which performs the following steps: 1. Establish a secure connection to a POP3 server using SSL. 2. Authenticate using the provided username and password. 3. Retrieve all email messages and print the content of each message to the console. 4. Properly handle and log any exceptions that may occur during the process. **Function Signature:** ```python def retrieve_emails(server: str, port: int, username: str, password: str) -> None: pass ``` **Input:** - `server` (str): The POP3 server\'s address. - `port` (int): The port number to connect to (typically 995 for SSL). - `username` (str): The username for authentication. - `password` (str): The password for authentication. **Output:** - This function does not return anything. It should print the content of each retrieved email message. **Constraints:** - The function should handle network errors, authentication errors, and any other errors gracefully using exception handling. - Use the `POP3_SSL` class from the `poplib` module. - If the connection or any operations fail, an appropriate error message should be printed to the console. **Example Usage:** ```python retrieve_emails(\\"pop.example.com\\", 995, \\"user_example\\", \\"password123\\") ``` **Expected Behavior:** - Connect to `pop.example.com` using SSL on port 995. - Authenticate with the username `user_example` and the password `password123`. - Retrieve all emails from the mailbox and print the content to the console. - Handle and print any errors that occur during the process. You can refer to the `poplib` documentation provided for any method specifics and additional functionality. **Note:** Ensure the implementation uses the proper techniques to securely handle credentials and establish an SSL connection. Also, include appropriate debugging output to help trace the execution flow.","solution":"import poplib from email.parser import BytesParser from email.policy import default import logging def retrieve_emails(server: str, port: int, username: str, password: str) -> None: try: # Connect to the POP3 server pop_conn = poplib.POP3_SSL(server, port) logging.info(\'Connected to the server.\') # Authenticate pop_conn.user(username) pop_conn.pass_(password) logging.info(\'Authenticated successfully.\') # Get messages from server messages = [pop_conn.retr(i) for i in range(1, len(pop_conn.list()[1]) + 1)] logging.info(\'Retrieved messages.\') # Parse and print each message for message in messages: raw_message = b\'n\'.join(message[1]) msg = BytesParser(policy=default).parsebytes(raw_message) print(msg) # Quit connection pop_conn.quit() logging.info(\'Disconnected from the server.\') except poplib.error_proto as e: logging.error(f\'POP3 protocol error: {e}\') except Exception as e: logging.error(f\'An error occurred: {e}\') # Example usage # retrieve_emails(\\"pop.example.com\\", 995, \\"user_example\\", \\"password123\\")"},{"question":"<|Analysis Begin|> The provided document is a detailed tutorial on using regular expressions in Python with the \\"re\\" module. The content covers various aspects of regular expressions including simple patterns, metacharacters, repetition, character classes, lookahead assertions, named and unnamed groups, and how to compile and use regular expressions in Python. It also covers string splitting and substitution using regular expressions, compilation flags for extended functionality, and common pitfalls when using regular expressions. This comprehensive guide demonstrates that regular expressions can be utilized for pattern matching within strings, which can include tasks like searching for substrings, validating string formats, replacing substrings, and splitting strings based on delimiters. The tutorial also touches upon performance considerations and best practices for writing and using regular expressions in Python. Based on the guide, the assessment question can be designed to cover several aspects of manipulating strings using regular expressions, such as finding patterns, extracting specific parts of strings, and replacing parts of strings. <|Analysis End|> <|Question Begin|> # Python Coding Assessment Question **Objective**: To assess the student\'s understanding of using regular expressions in Python with the \\"re\\" module. --- # Problem Statement You are working for a data processing company, and you have been assigned the task of extracting information from a large log file. The log file contains entries that include user IDs, dates, and actions performed by the user. Here are some example lines from the log file: ``` [2021-06-21 16:45:32] User abc123 performed action login [2021-06-21 16:47:10] User def456 performed action logout [2021-06-21 17:10:05] User abc123 performed action upload file \\"report.pdf\\" ``` Your task is to write a Python function `extract_log_details` that processes each log entry and extracts the following details: 1. **Timestamp**: The date and time of the log entry. 2. **User ID**: The user ID associated with the log entry. 3. **Action**: The action performed, e.g., \\"login\\", \\"logout\\", or \\"upload file\\". 4. **Filename** (if applicable): The name of the file involved in the action, if the action is \\"upload file\\". The function should return a list of dictionaries, where each dictionary contains the extracted details for a log entry. # Function Signature ```python from typing import List, Dict def extract_log_details(log_lines: List[str]) -> List[Dict[str, str]]: pass ``` # Input - `log_lines` (List[str]): A list of strings, where each string is a line from the log file. # Output - A list of dictionaries. Each dictionary represents the extracted details from a log entry and has the keys \\"timestamp\\", \\"user_id\\", \\"action\\", and optionally \\"filename\\". # Constraints - Each log entry will follow the exact format as shown in the examples. - The action \\"upload file\\" will always be followed by the name of the file in double quotation marks. - The extracted filename should not include the double quotation marks. # Example ```python log_lines = [ \\"[2021-06-21 16:45:32] User abc123 performed action login\\", \\"[2021-06-21 16:47:10] User def456 performed action logout\\", \\"[2021-06-21 17:10:05] User abc123 performed action upload file \\"report.pdf\\"\\" ] expected_output = [ { \\"timestamp\\": \\"2021-06-21 16:45:32\\", \\"user_id\\": \\"abc123\\", \\"action\\": \\"login\\" }, { \\"timestamp\\": \\"2021-06-21 16:47:10\\", \\"user_id\\": \\"def456\\", \\"action\\": \\"logout\\" }, { \\"timestamp\\": \\"2021-06-21 17:10:05\\", \\"user_id\\": \\"abc123\\", \\"action\\": \\"upload file\\", \\"filename\\": \\"report.pdf\\" } ] assert extract_log_details(log_lines) == expected_output ``` # Note - Ensure the regular expressions handle various cases consistently. - The function should use Python\'s \\"re\\" module for regular expression operations. - Pay attention to careful extraction of details using appropriate capturing groups. ---","solution":"import re from typing import List, Dict def extract_log_details(log_lines: List[str]) -> List[Dict[str, str]]: result = [] log_pattern = re.compile( r\'[(?P<timestamp>[d-]+s[d:]+)] User (?P<user_id>w+) performed action (?P<action>w+(?:sw+)*)\' r\'(?:s\\"(?P<filename>[^\\"]+)\\")?\' ) for line in log_lines: match = log_pattern.match(line) if match: log_entry = match.groupdict() action = log_entry[\\"action\\"] if action != \\"upload file\\": log_entry.pop(\\"filename\\") result.append(log_entry) return result"},{"question":"You are provided with a custom Python application that involves complex data structures which need to be saved and retrieved efficiently. You are tasked with implementing two functions: `save_complex_data` and `load_complex_data`, to serialize and deserialize data structures using the `marshal` module. Function 1: `save_complex_data` **Input:** - `data` (dict): A dictionary containing complex data structures. - `file_path` (str): The path to the file where data should be saved. **Output:** - None **Description:** - This function should serialize the provided `data` dictionary using `marshal.dumps`, and save it to a file specified by `file_path`. Function 2: `load_complex_data` **Input:** - `file_path` (str): The path to the file from which data should be loaded. **Output:** - `data` (dict): The deserialized dictionary containing the original complex data structures. **Description:** - This function should load the binary data from the specified `file_path` using `marshal.loads` and deserialize it into the original dictionary format. Constraints: - The data dictionary will contain nested structures including list, tuple, set, frozen set, and other supported types. - Ensure that any non-supported types or erroneous data raise the appropriate exceptions as specified in the `marshal` module documentation. Example Usage: ```python data = { \\"name\\": \\"example\\", \\"numbers\\": [1, 2, 3], \\"details\\": {\\"age\\": 30, \\"city\\": \\"New York\\"}, \\"flags\\": (True, False, None), \\"frozen_set\\": frozenset({1, 2, 3}), } file_path = \\"complex_data.marshal\\" # Save the data save_complex_data(data, file_path) # Load the data loaded_data = load_complex_data(file_path) assert data == loaded_data # Should be True if serialization and deserialization are correct ``` Notes: - You should import the necessary modules (`marshal`) and handle any relevant exceptions. - Ensure the loaded data matches the original data exactly, maintaining the integrity of the data structures. - Consider edge cases such as empty dictionaries, nested empty structures, and maximum depth of nesting.","solution":"import marshal def save_complex_data(data, file_path): Serialize the provided `data` dictionary using `marshal` and save it to a file. Parameters: - data (dict): A dictionary containing complex data structures. - file_path (str): The path to the file where data should be saved. Returns: - None with open(file_path, \'wb\') as file: serialized_data = marshal.dumps(data) file.write(serialized_data) def load_complex_data(file_path): Load the binary data from the specified `file_path` using `marshal` and deserialize it. Parameters: - file_path (str): The path to the file from which data should be loaded. Returns: - data (dict): The deserialized dictionary containing the original complex data structures. with open(file_path, \'rb\') as file: serialized_data = file.read() data = marshal.loads(serialized_data) return data"},{"question":"**Objective:** Demonstrate your understanding of pandas options configuration by manipulating DataFrame display settings. **Problem Statement:** You are given a DataFrame containing various numerical columns. Your task is to write a function `configure_dataframe_display` that takes a DataFrame as input and performs the following configurations using pandas options: 1. Limits the number of columns displayed when the DataFrame is printed to 10. 2. Sets a floating-point number display format such that it shows 2 decimal places. 3. Disables the scientific notation for floating-point numbers. Additionally, you need to reset the options to their default values once the function finishes. **Function Signature:** ```python def configure_dataframe_display(df: pd.DataFrame) -> pd.DataFrame: pass ``` **Expected Input:** - `df` (pd.DataFrame): The input DataFrame with a variable number of numerical columns. **Expected Output:** - A DataFrame (pd.DataFrame) that adheres to the configurations specified above when printed. **Constraints:** - You should use the pandas options API for configuration. - Reset the options to default after making the configurations, using an appropriate method. **Example:** ```python import pandas as pd data = { \'A\': [1.1111, 2.2222, 3.3333], \'B\': [4.4444, 5.5555, 6.6666], \'C\': [7.7777, 8.8888, 9.9999], \'D\': [10.1010, 11.1111, 12.1212] } df = pd.DataFrame(data) configured_df = configure_dataframe_display(df) print(configured_df) ``` The printed DataFrame should show no more than 10 columns, have numbers formatted to 2 decimal places, and not use scientific notation for floating-point values. **Hints:** - Use `set_option` to configure settings. - You can find the option names for controlling column display and numeric formatting in the pandas documentation. - Use `reset_option` to revert changes.","solution":"import pandas as pd def configure_dataframe_display(df: pd.DataFrame) -> pd.DataFrame: Configures the pandas display settings for the provided DataFrame. :param df: Input DataFrame to configure display settings for. :return: DataFrame with display settings configured for printing. # Set options pd.set_option(\'display.max_columns\', 10) # Set maximum number of columns to display pd.set_option(\'display.float_format\', \'{:.2f}\'.format) # Set float format to 2 decimal figures pd.set_option(\'display.precision\', 2) # Set precision to 2 pd.set_option(\'display.chop_threshold\', None) # Disable scientific notation reset_func = lambda option: pd.reset_option(option) # Reset function for default values later # Perform operations to display the DataFrame print(df) # Reset options to their default values reset_func(\'display.max_columns\') reset_func(\'display.float_format\') reset_func(\'display.precision\') reset_func(\'display.chop_threshold\') return df"},{"question":"Coding Assessment Question # Problem Statement You are tasked with developing a function to process a list of string-represented numbers. Your function will need to convert each string to a double, manipulate these double values, and then convert them back to strings with defined formatting. Implement the following Python function: ```python def process_number_strings(number_strings: List[str], operation: str, format_code: str, precision: int, flags: int) -> List[str]: Converts a list of number strings to double values, applies an operation, and converts them back to formatted strings. Parameters: number_strings (List[str]): List of strings representing numbers. operation (str): A mathematical operation to apply. It could be \'square\', \'cube\', or \'sqrt\'. format_code (str): Character representing the format of the output string (\'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'). precision (int): Number of decimal places in the output string if applicable. flags (int): Formatting flags (0, Py_DTSF_SIGN, Py_DTSF_ADD_DOT_0, Py_DTSF_ALT) or combinations thereof. Returns: List[str]: List of strings with formatted numbers after applying the operation. # Note: Use the functions PyOS_string_to_double and PyOS_double_to_string to perform conversions. ``` # Input - `number_strings`: A list of strings where each string is a valid representation of a number. (e.g., [\\"3.14\\", \\"2.71\\", \\"1.62\\"]) - Each string does not contain any leading or trailing whitespace. - `operation`: A string indicating the mathematical operation to perform on each number. Can be one of: - \'square\': for squaring the number. - \'cube\': for cubing the number. - \'sqrt\': for taking the square root of the number. - `format_code`: A character representing the format of the output string. One of the following: - \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\' - \'r\' format should use the `repr()` function for the standard representation. - `precision`: An integer specifying the number of decimal places if applicable. Must be 0 for \'r\' format. - `flags`: An integer with values combined from: - 0 for no flag, - `Py_DTSF_SIGN` (1): Always precede the result with a sign (\'+\' or \'-\'). - `Py_DTSF_ADD_DOT_0` (2): Ensure the output does not look like an integer. - `Py_DTSF_ALT` (4): Use alternate formatting rules. # Output - A list of strings where each string is a formatted number after applying the operation. # Constraints - `number_strings` will contain 1 to 100 strings. - Each number string is a valid representation of a float number. - `operation` will be one of \'square\', \'cube\', \'sqrt\'. - `format_code` will be one of \'e\', \'E\', \'f\', \'F\', \'g\', \'G\', \'r\'. - `precision` is an integer where 0 ≤ precision ≤ 15. - `flags` is a combination of `0`, `Py_DTSF_SIGN`, `Py_DTSF_ADD_DOT_0`, `Py_DTSF_ALT`. # Examples ```python number_strings = [\\"4.0\\", \\"9.0\\", \\"16.0\\"] operation = \\"sqrt\\" format_code = \\"f\\" precision = 2 flags = 0 # Expected output: [\\"2.00\\", \\"3.00\\", \\"4.00\\"] print(process_number_strings(number_strings, operation, format_code, precision, flags)) ``` # Notes 1. Use the `PyOS_string_to_double` function to handle string to double conversions. 2. Use the `PyOS_double_to_string` function to handle double to string conversions. 3. Pay attention to error handling as described in the documentation.","solution":"import math from typing import List def process_number_strings(number_strings: List[str], operation: str, format_code: str, precision: int, flags: int) -> List[str]: Py_DTSF_SIGN = 1 Py_DTSF_ADD_DOT_0 = 2 Py_DTSF_ALT = 4 # Define a helper to convert string to double def PyOS_string_to_double(s, null_val): try: return float(s) except ValueError: return null_val # Define a helper to convert double to formatted string def PyOS_double_to_string(value, format_code, precision, flags): # Handle special case for \'r\' format that uses repr if format_code == \'r\': return repr(value) format_str = f\\".{precision}{format_code}\\" formatted = format(value, format_str) # Handle flags if flags & Py_DTSF_SIGN: if not formatted.startswith(\'-\') and not formatted.startswith(\'+\'): formatted = \'+\' + formatted if flags & Py_DTSF_ADD_DOT_0: if \'.\' not in formatted: formatted = formatted + \'.0\' if flags & Py_DTSF_ALT: # No-op for now as Python\'s format covers this by default pass return formatted # Process each number string results = [] for num_str in number_strings: num = PyOS_string_to_double(num_str, float(\'nan\')) # Apply the provided operation if operation == \'square\': num = num ** 2 elif operation == \'cube\': num = num ** 3 elif operation == \'sqrt\': num = math.sqrt(num) else: raise ValueError(\\"Unknown operation\\") # Convert back to string with formatting formatted_str = PyOS_double_to_string(num, format_code, precision, flags) results.append(formatted_str) return results"},{"question":"# Secure Netrc Handler You are required to implement a class `SecureNetrcHandler` that securely handles reading from and writing to a `.netrc` file using the provided `netrc` module. The class should have the following functionalities: 1. **Constructor**: - **Input**: `filename` (optional string): the name of the netrc file to be used. If not provided, defaults to `.netrc` in the user\'s home directory. - **Behavior**: Initializes the class and reads the netrc file specified. 2. **Methods**: - `get_authenticator(host) -> Tuple[str, str, str] or None`: - **Input**: `host` (string): the hostname. - **Output**: Returns a 3-tuple of `(login, account, password)` if the host or a default entry is found, else returns `None`. - `add_or_update_host(host, login, account, password) -> None`: - **Input**: - `host` (string): the hostname to add or update. - `login`, `account`, `password` (strings): the credentials to store. - **Behavior**: Adds or updates the entry for the specified host with given credentials. - `remove_host(host) -> None`: - **Input**: `host` (string): the hostname to remove. - **Behavior**: Removes the specified host entry from the netrc file. - `save_to_file(filename=None) -> None`: - **Input**: `filename` (optional string): the filename to save the netrc content. If not provided, uses the originally specified file. - **Behavior**: Saves the current state of the netrc data to the specified file while ensuring file permissions are restrictive. 3. **Constraints**: - You must ensure that the `.netrc` file is not accessible by any other users except the owner. - All passwords should conform to the specified allowable ASCII characters, excluding whitespace and non-printable characters. 4. **Example Usage**: ```python handler = SecureNetrcHandler() handler.add_or_update_host(\'myhost\', \'mylogin\', \'myaccount\', \'mypassword\') print(handler.get_authenticator(\'myhost\')) # Should output (\'mylogin\', \'myaccount\', \'mypassword\') handler.remove_host(\'myhost\') handler.save_to_file() ``` Your implementation should handle all necessary exceptions and edge cases, ensuring robust and secure reading and writing of netrc data.","solution":"import netrc import os from typing import Tuple, Optional class SecureNetrcHandler: def __init__(self, filename: Optional[str] = None): if filename is None: self.filename = os.path.join(os.path.expanduser(\'~\'), \'.netrc\') else: self.filename = filename if os.path.exists(self.filename): self.netrc_data = netrc.netrc(self.filename) else: self.netrc_data = netrc.netrc() self.ensure_file_permissions() def get_authenticator(self, host: str) -> Optional[Tuple[str, str, str]]: auth_data = self.netrc_data.authenticators(host) if auth_data: return auth_data return None def add_or_update_host(self, host: str, login: str, account: str, password: str) -> None: self.netrc_data.hosts[host] = (login, account, password) def remove_host(self, host: str) -> None: if host in self.netrc_data.hosts: del self.netrc_data.hosts[host] def save_to_file(self, filename: Optional[str] = None) -> None: if filename is None: filename = self.filename with open(filename, \'w\') as file: for host, attrs in self.netrc_data.hosts.items(): file.write(f\\"machine {host}n\\") file.write(f\\" login {attrs[0]}n\\") file.write(f\\" account {attrs[1]}n\\") file.write(f\\" password {attrs[2]}n\\") self.ensure_file_permissions() def ensure_file_permissions(self): if os.path.exists(self.filename): os.chmod(self.filename, 0o600)"},{"question":"# Quoted-Printable Encoding/Decoding Utility In this assessment, you will implement custom functions using the quoted-printable encoding and decoding methods provided by the `quopri` module. These functions will manipulate data in a specified format to ensure correct encoding and decoding, while handling special cases defined by the module\'s optional parameters. Function 1: `custom_encode(input_bytes: bytes, encode_tabs: bool, encode_header: bool) -> bytes` Write a function that takes the following inputs: - `input_bytes`: A bytes object representing the data to be encoded. - `encode_tabs`: A boolean flag indicating whether to encode embedded spaces and tabs. - `encode_header`: A boolean flag indicating whether to encode spaces as underscores in headers. The function should return the encoded bytes object using quoted-printable encoding. ```python def custom_encode(input_bytes: bytes, encode_tabs: bool, encode_header: bool) -> bytes: # Your implementation here pass ``` Function 2: `custom_decode(encoded_bytes: bytes, decode_header: bool) -> bytes` Write a function that takes the following inputs: - `encoded_bytes`: A bytes object representing the data to be decoded. - `decode_header`: A boolean flag indicating whether to decode underscores as spaces in headers. The function should return the decoded bytes object. ```python def custom_decode(encoded_bytes: bytes, decode_header: bool) -> bytes: # Your implementation here pass ``` Constraints 1. Do not use any additional third-party libraries other than the `quopri` module. 2. Handle edge cases such as empty input bytes and inputs with only non-printable characters. Example ```python # Example usage: # Encoding data input_bytes = b\'This is an example text with tabstand spaces.\' encode_tabs = True encode_header = False encoded_output = custom_encode(input_bytes, encode_tabs, encode_header) # Decoding data decode_header = False decoded_output = custom_decode(encoded_output, decode_header) assert decoded_output == input_bytes ``` Performance Requirements - Your solution should efficiently handle inputs up to 1 MB in size. - Ensure that the functions execute within a reasonable time frame for inputs of this size. Submit your implementation of the `custom_encode` and `custom_decode` functions.","solution":"import quopri def custom_encode(input_bytes: bytes, encode_tabs: bool, encode_header: bool) -> bytes: Encodes the input bytes using quoted-printable encoding. Parameters: - input_bytes: A bytes object representing the data to be encoded. - encode_tabs: A boolean flag indicating whether to encode embedded spaces and tabs. - encode_header: A boolean flag indicating whether to encode spaces as underscores in headers. Returns: - A bytes object representing the encoded data. if encode_header: input_bytes = input_bytes.replace(b\' \', b\'_\') encoded = quopri.encodestring(input_bytes, quotetabs=encode_tabs) return encoded def custom_decode(encoded_bytes: bytes, decode_header: bool) -> bytes: Decodes the quoted-printable encoded bytes. Parameters: - encoded_bytes: A bytes object representing the encoded data to be decoded. - decode_header: A boolean flag indicating whether to decode underscores back to spaces in headers. Returns: - A bytes object representing the decoded data. decoded = quopri.decodestring(encoded_bytes) if decode_header: decoded = decoded.replace(b\'_\', b\' \') return decoded"},{"question":"**Question:** You are tasked with creating a Python function to filter and extract specific patterns from a block of text. The function should leverage Python\'s `re` module to achieve this. # Function Requirements 1. **Function Name:** `extract_email_and_phone` 2. **Inputs:** - `text` (string): A block of text in which email addresses and phone numbers must be identified and extracted. 3. **Outputs:** - A tuple containing two lists: - The first list should contain all unique email addresses found in the text. - The second list should contain all unique phone numbers found in the text. # Email and Phone Number Patterns 1. **Email Address:** - Basic pattern for this assessment: ``` local-part@domain ``` - The `local-part` can contain letters, digits, dots, and underscores. - The `domain` consists of letters and dots (e.g., example.com). 2. **Phone Number:** - Basic pattern for this assessment: ``` xxx-xxx-xxxx ``` - Each `x` here stands for a digit ranging from 0-9. # Constraints - Assume the input text is reasonably sized (up to 10000 characters). - Multiple occurrences of the same email or phone number should be counted once. - The search should be case-insensitive. # Examples ```python def extract_email_and_phone(text: str) -> Tuple[List[str], List[str]]: pass # Example 1 input_text = Here is an email: alice@example.com and a phone number: 123-456-7890. Another email is Bob@Example.com and a second phone: 123-456-7890. output = extract_email_and_phone(input_text) # Output: ([\'alice@example.com\', \'bob@example.com\'], [\'123-456-7890\']) # Example 2 input_text = \\"Contact us at support@company.com or 987-654-3210 for further information.\\" output = extract_email_and_phone(input_text) # Output: ([\'support@company.com\'], [\'987-654-3210\']) ``` # Notes - Ensure that the implementation correctly uses regex for the extraction. - Use appropriate error handling if no matches are found in the text.","solution":"import re from typing import List, Tuple def extract_email_and_phone(text: str) -> Tuple[List[str], List[str]]: Extract unique email addresses and phone numbers from the input text. Args: text (str): The input text containing email addresses and phone numbers. Returns: Tuple[List[str], List[str]]: A tuple containing two lists - The first list contains unique email addresses. The second list contains unique phone numbers. email_pattern = r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,}b\' phone_pattern = r\'bd{3}-d{3}-d{4}b\' # Using re.IGNORECASE to make the email matching case-insensitive emails = re.findall(email_pattern, text, re.IGNORECASE) phones = re.findall(phone_pattern, text) unique_emails = list(set(email.lower() for email in emails)) unique_phones = list(set(phones)) return unique_emails, unique_phones"},{"question":"**Objective:** Create a Python function that demonstrates understanding and manipulation of memory buffers, leveraging `memoryview`. Your function should provide efficient operations on a given byte buffer keeping performance considerations in mind. **Question:** Write a Python function named `reverse_bytes_in_buffer` that takes a buffer-like object (such as `bytes` or `bytearray`) as input and returns a new `bytearray` with the bytes reversed. You must use `memoryview` for this operation to ensure it is handled efficiently without unnecessary copying. # Function Signature: ```python def reverse_bytes_in_buffer(buffer: bytes) -> bytearray: pass ``` # Input: - `buffer`: A buffer-like object (`bytes` or `bytearray`) containing the bytes to be reversed. # Output: - Returns a new `bytearray` with the bytes in the reverse order of the input `buffer`. # Constraints: - You must not use slicing methods available directly on `bytes` or `bytearray`. - You should minimize the number of copies and ensure efficient memory use. - The input buffer size will not exceed 10^6 bytes. # Example: ```python buffer = bytes([1, 2, 3, 4, 5]) result = reverse_bytes_in_buffer(buffer) print(result) # Output should be bytearray(b\'x05x04x03x02x01\') ``` # Implementation Requirements: 1. Use `memoryview` to create a view of the original buffer. 2. Efficiently reverse the bytes using a minimal number of operations. 3. Handle both `bytes` and `bytearray` types for the input. # Hint: Using `memoryview`, you can create zero-copy slices and reverse the bytes efficiently. **Note**: Do not use Python\'s built-in slicing and reversal operations directly on `bytes` or `bytearray`. This exercise aims to test your understanding of memory views and efficient buffer manipulation.","solution":"def reverse_bytes_in_buffer(buffer: bytes) -> bytearray: Reverses the bytes in a given buffer using memoryview for efficiency. Args: - buffer: A buffer-like object (e.g., bytes or bytearray) to be reversed. Returns: - A new bytearray with the bytes reversed. # Create a memoryview of the input buffer mv = memoryview(buffer) # Reverse the memoryview and convert it to a bytearray reversed_buffer = bytearray(mv[::-1]) return reversed_buffer"},{"question":"**Problem Statement:** You are tasked with creating a Python script that generates a `setup.cfg` configuration file for a Python package. The configuration should include specific settings for different commands. # Requirements 1. The configuration file should define the following settings: - For the `build_ext` command: - The extensions should be built in-place (`inplace` option set to `1`). - For the `bdist_rpm` command: - The `release` option should be set to `1`. - The `packager` option should be set to your name and email. - The `doc_files` option should include `CHANGES.txt`, `README.txt`, `USAGE.txt`, and the `doc/` and `examples/` directories. 2. Your script should be named `generate_setup_cfg.py` and should generate the required `setup.cfg` file when executed. # Input The script should take the following command-line inputs: - Packager name as a string (e.g., \\"Jane Doe\\"). - Packager email as a string (e.g., \\"jane.doe@example.com\\"). # Output The script should create a `setup.cfg` file with the specified configuration. # Example If the script is executed with the following command: ``` python generate_setup_cfg.py \\"Jane Doe\\" \\"jane.doe@example.com\\" ``` The `setup.cfg` file should contain: ``` [build_ext] inplace=1 [bdist_rpm] release = 1 packager = Jane Doe <jane.doe@example.com> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` # Constraints - You can assume that the directories `doc/` and `examples/` exist. - Use proper error handling to ensure the script exits gracefully if incorrect inputs are provided. # Implementation Write the `generate_setup_cfg.py` script to meet the above requirements.","solution":"import sys def generate_setup_cfg(packager_name, packager_email): setup_cfg_content = f [build_ext] inplace=1 [bdist_rpm] release = 1 packager = {packager_name} <{packager_email}> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ with open(\\"setup.cfg\\", \\"w\\") as file: file.write(setup_cfg_content.strip()) if __name__ == \\"__main__\\": if len(sys.argv) != 3: print(\\"Usage: python generate_setup_cfg.py <packager_name> <packager_email>\\") sys.exit(1) packager_name = sys.argv[1] packager_email = sys.argv[2] generate_setup_cfg(packager_name, packager_email)"},{"question":"**Title: AST-based Decorator Removal** Problem Statement: You are required to write a function `remove_decorators` that takes as input a string containing the source code of a Python module. The function should remove all decorators from the function definitions and return the modified source code. Function Signature: ```python def remove_decorators(source: str) -> str: pass ``` Input: - `source` (str): A string containing the source code of the Python module. The source code may contain multiple function definitions, some with decorators and some without. Output: - Returns a string with the source code where all decorators have been removed from all function definitions. Constraints: - The source code is syntactically correct. - You cannot use regular expressions to solve this problem; you must use the `ast` module. Example: ``` Input: source = \'\'\' @decorator1 @decorator2 def func1(): pass @decorator3(param) def func2(): pass def func3(): pass \'\'\' Output: \'\'\' def func1(): pass def func2(): pass def func3(): pass \'\'\' ``` Implementation Details: 1. Parse the input source code into an AST using `ast.parse`. 2. Traverse the AST and remove all decorators from function definitions. 3. Generate the updated source code from the modified AST using `ast.unparse`. 4. Make sure the output source code retains its original structure and formatting, except for the removal of decorators. You are encouraged to use the classes and utilities provided by the `ast` module, such as `ast.NodeTransformer`, `ast.parse`, and `ast.fix_missing_locations`. Additional Notes: - You may assume the source code does not contain comments in the middle of decorator lists. - Focus on writing clean and readable code, and ensure that the function handles edge cases elegantly. - You do not need to handle the case of decorators on classes or other structures; only functions are in scope.","solution":"import ast def remove_decorators(source: str) -> str: class DecoratorRemover(ast.NodeTransformer): def visit_FunctionDef(self, node): node.decorator_list = [] self.generic_visit(node) return node def visit_AsyncFunctionDef(self, node): node.decorator_list = [] self.generic_visit(node) return node tree = ast.parse(source) remover = DecoratorRemover() modified_tree = remover.visit(tree) ast.fix_missing_locations(modified_tree) return ast.unparse(modified_tree)"},{"question":"**Objective:** Demonstrate your comprehension of the `urllib` package by implementing a function that processes URLs, fetches data from them, and extracts specific components from the URLs. **Problem Statement:** You are given a list of URLs. Your task is to implement the function `process_urls(urls: List[str]) -> List[str]` that performs the following operations: 1. For each URL, open and read the content using `urllib`. 2. Extract the scheme and host components of each URL. 3. Return a list of formatted strings that provide the scheme, host, and first 15 characters of the fetched content from each URL. **Function Signature:** ```python from typing import List import urllib.request import urllib.parse def process_urls(urls: List[str]) -> List[str]: pass ``` **Input:** - `urls`: A list of strings where each string is a valid URL. **Output:** - A list of formatted strings. Each formatted string is in the format: `\\"<scheme>://<host>: <first 15 characters of content>\\"` **Constraints:** - Assume all URLs in the `urls` list are valid and accessible. - The content fetched from each URL will have at least 15 characters. **Example:** ```python urls = [ \\"http://example.com\\", \\"https://www.python.org\\" ] result = process_urls(urls) # Expected output: # [\'http://example.com: Example Domain\', # \'https://www.python.org: <!doctype html>\'] ``` **Notes:** - Make sure to handle importing the required modules (which includes `urllib.request` and `urllib.parse`). - If an URL content has a first line or initial part that contains non-printable characters, handle it appropriately to return human-readable content. **Hints:** - Use `urllib.request.urlopen` to open and read the URL content. - Use `urllib.parse.urlparse` to extract the scheme and host components. **Resources:** Refer to the `urllib` documentation for more details on handling URLs and extracting their components.","solution":"from typing import List import urllib.request import urllib.parse def process_urls(urls: List[str]) -> List[str]: result = [] for url in urls: # Fetching the content from the URL with urllib.request.urlopen(url) as response: content = response.read(15).decode(\'utf-8\') # Parsing the URL to extract scheme and host parsed_url = urllib.parse.urlparse(url) scheme_host = f\'{parsed_url.scheme}://{parsed_url.netloc}\' # Formatting the result string formatted_string = f\'{scheme_host}: {content}\' result.append(formatted_string) return result"},{"question":"# Task: Analyze and Visualize Data with Seaborn You are given a dataset of your choice that you need to analyze and visualize using seaborn. Your goal is to write a Python function that accepts the dataset and creates multiple types of visualizations to display the distributions and relationships within the data. Function Signature: ```python def analyze_and_visualize_data(data: pd.DataFrame): Accepts a pandas DataFrame and creates various histograms and visualizations using seaborn. Parameters: data (pd.DataFrame): The input dataset to analyze and visualize. Returns: None ``` Requirements: 1. **Univariate Histogram:** Plot a univariate histogram for a numeric column of your choice. 2. **Adjusted Bins:** Create a histogram for the same column, adjusting the bin width and number of bins. Explain in comments how the representation differs with these adjustments. 3. **Kernel Density Estimate:** Add a kernel density estimate line to the histogram. 4. **Multiple Histograms:** Draw multiple histograms on the same plot using a categorical column for hue differentiation. 5. **Normalization:** Show how normalizing histogram bars for densities or percentages changes the visualization. 6. **Log-Scale:** Create histograms for a numeric column with a log scale for the x-axis. 7. **Bivariate Histogram:** Plot a bivariate histogram (heatmap) for two numeric columns in the dataset, adding a hue to the histogram. 8. **Colorbar Annotation:** Add a colorbar to the bivariate heatmap and explain what it represents. Input: - `data` is a pandas DataFrame containing your dataset. Output: - The function should not return anything, but it should display the visualizations inline (using Jupyter Notebook functionality if necessary). Constraints: - Use the seaborn library for the visualizations. - Make sure to add titles and labels to all visualizations for clarity. - Add comments explaining the purpose of each visualization and any insights you gain. Example Usage: ```python import seaborn as sns import pandas as pd # Sample dataset loading penguins = sns.load_dataset(\\"penguins\\") # Function call with the dataset analyze_and_visualize_data(penguins) ``` *Note: Ensure you have seaborn and pandas installed in your environment to run this code.*","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt def analyze_and_visualize_data(data: pd.DataFrame): Accepts a pandas DataFrame and creates various histograms and visualizations using seaborn. Parameters: data (pd.DataFrame): The input dataset to analyze and visualize. Returns: None # Univariate Histogram plt.figure(figsize=(10, 6)) sns.histplot(data[\'flipper_length_mm\'], kde=False) plt.title(\'Univariate Histogram: Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # Adjusted Bins plt.figure(figsize=(10, 6)) sns.histplot(data[\'flipper_length_mm\'], bins=30, kde=False) plt.title(\'Histogram with Adjusted Bins: Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # Adjusting bins can make data distribution appear more granular. # Kernel Density Estimate plt.figure(figsize=(10, 6)) sns.histplot(data[\'flipper_length_mm\'], kde=True) plt.title(\'Histogram with KDE: Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # Multiple Histograms plt.figure(figsize=(10, 6)) sns.histplot(data, x=\'flipper_length_mm\', hue=\'species\', multiple=\'stack\') plt.title(\'Multiple Histograms with Hue: Flipper Length by Species\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # Normalized Histogram plt.figure(figsize=(10, 6)) sns.histplot(data[\'flipper_length_mm\'], kde=False, stat=\'density\') plt.title(\'Normalized Histogram: Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Density\') plt.show() # Log-Scale Histogram plt.figure(figsize=(10, 6)) sns.histplot(data[\'flipper_length_mm\'], log_scale=(True, False)) plt.title(\'Log-Scale Histogram: Flipper Length\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Frequency\') plt.show() # Bivariate Histogram (Heatmap) plt.figure(figsize=(10, 6)) sns.histplot(data=data, x=\'flipper_length_mm\', y=\'body_mass_g\', cbar=True) plt.title(\'Bivariate Histogram (Heatmap): Flipper Length vs. Body Mass\') plt.xlabel(\'Flipper Length (mm)\') plt.ylabel(\'Body Mass (g)\') plt.show()"},{"question":"Objective: To assess the students\' understanding of Python\'s `atexit` module, including registering, unregistering functions and ensuring operations are performed correctly at program termination. Problem Statement: You are required to create a module that manages a key-value store (similar to a dictionary) whose state must be saved to a file upon program termination. Additionally, the module should handle multiple registered save operations and ensure data consistency. Specifications: 1. **Class Definition:** Create a class `KeyValueStore` with the following methods: - `__init__(self, filename)`: Initializes the key-value store. Loads the file specified by `filename` (if it exists) to restore the stored key-value pairs. - `set(self, key, value)`: Sets the `value` for the given `key`. - `get(self, key)`: Returns the value corresponding to the `key`. If the `key` does not exist, return `None`. - `delete(self, key)`: Deletes the key-value pair for the given `key`. - `save(self)`: Saves the current state of the key-value store to the specified file. 2. **Functionality Requirements:** - Register the `save` method to be called at program termination using the `atexit.register` function. - Ensure that if the file is not present during initialization, it does not raise an error but starts with an empty store. - Provide an option to unregister the save operation based on user input. 3. **Input/Output Format:** - The `filename` parameter for initialization is expected to be a string pointing to a file path. - Methods `set`, `get`, and `delete` will handle inputs of `key` and `value` as strings. - The state of the key-value store should be represented in a way that it can be saved to and restored from a file (e.g., JSON format). 4. **Constraints:** - Assume the system has sufficient permissions to read/write the specified file. - Handle potential exceptions that may arise during file operations (e.g., file not found, read/write errors). # Example: ```python # File may start empty kv_store = KeyValueStore(\'storefile.json\') # Setting some key-value pairs kv_store.set(\'apple\', \'fruit\') kv_store.set(\'carrot\', \'vegetable\') # Retrieving a value print(kv_store.get(\'apple\')) # Output: \'fruit\' # Deleting a key kv_store.delete(\'carrot\') # Program termination should automatically save the current state to \'storefile.json\' ``` **Hint**: Utilize the `json` module to facilitate the read and write operations of the key-value store state. Advanced Task: Allow the user to register multiple versions of the `save` method (e.g., saving different snapshots) and ensure these are handled appropriately on termination. ```python import atexit def some_other_save_method(): # Implement as needed to showcase multiple save operations. pass atexit.register(kv_store.save) atexit.register(some_other_save_method) ``` **Note**: The solution should reflect good coding practices such as error handling, code modularity, and documentation of critical sections.","solution":"import json import atexit class KeyValueStore: def __init__(self, filename): Initialize the key-value store with the content of the file, if it exists. self.filename = filename try: with open(self.filename, \'r\') as file: self.store = json.load(file) except (FileNotFoundError, json.JSONDecodeError): self.store = {} atexit.register(self.save) def set(self, key, value): Sets the value for the given key. self.store[key] = value def get(self, key): Returns the value corresponding to the key, or None if the key does not exist. return self.store.get(key) def delete(self, key): Deletes the key-value pair for the given key. if key in self.store: del self.store[key] def save(self): Saves the current state of the key-value store to the file. with open(self.filename, \'w\') as file: json.dump(self.store, file)"},{"question":"**Objective:** Implement a chunked compression and decompression utility using the `zlib` module in Python. Your task is to create functions that handle data compression and decompression in fixed-size chunks to handle large data streams efficiently. You will also need to implement checksum verification to ensure data integrity. **Function Specifications:** 1. `chunked_compress(data: bytes, chunk_size: int) -> List[bytes]` Compresses the input byte data in fixed-size chunks. - **Parameters:** - `data` (bytes): The input data to be compressed. - `chunk_size` (int): The size of each chunk in bytes. - **Returns:** - List[bytes]: A list containing the compressed data chunks. 2. `chunked_decompress(chunks: List[bytes], chunk_size: int) -> bytes` Decompresses the input byte data chunks. - **Parameters:** - `chunks` (List[bytes]): The list of compressed data chunks. - `chunk_size` (int): The size of each chunk in bytes. - **Returns:** - bytes: The decompressed data. 3. `verify_data(original_data: bytes, decompressed_data: bytes) -> bool` Verifies the integrity of the decompressed data using checksums. - **Parameters:** - `original_data` (bytes): The original uncompressed data. - `decompressed_data` (bytes): The decompressed data. - **Returns:** - bool: `True` if the data is intact, `False` otherwise. **Constraints:** - The chunk size must be a positive integer. - You should handle exceptions for any compression or decompression errors. - Maintain performance efficiency regarding both memory usage and processing speed. - Use Adler-32 checksums for verifying data integrity. Do not use CRC32 in this task. **Example:** ```python data = b\\"this is an example of data that needs to be compressed in chunks of fixed size\\" * 1000 chunk_size = 1024 # Compress the data in chunks compressed_chunks = chunked_compress(data, chunk_size) # Decompress the data chunks decompressed_data = chunked_decompress(compressed_chunks, chunk_size) # Verify the decompressed data matches the original data assert verify_data(data, decompressed_data) == True ``` **Notes:** - Ensure your code handles edge cases, such as empty input data or chunk sizes larger than the data itself. - Document your code with comments explaining the logic behind each part.","solution":"import zlib from typing import List def chunked_compress(data: bytes, chunk_size: int) -> List[bytes]: Compresses the input byte data in fixed-size chunks. Parameters: - data (bytes): The input data to be compressed. - chunk_size (int): The size of each chunk in bytes. Returns: - List[bytes]: A list containing the compressed data chunks. compressed_chunks = [] compressor = zlib.compressobj(level=zlib.Z_DEFAULT_COMPRESSION) for i in range(0, len(data), chunk_size): chunk = data[i:i + chunk_size] compressed_chunks.append(compressor.compress(chunk)) compressed_chunks.append(compressor.flush()) return compressed_chunks def chunked_decompress(chunks: List[bytes], chunk_size: int) -> bytes: Decompresses the input byte data chunks. Parameters: - chunks (List[bytes]): The list of compressed data chunks. - chunk_size (int): The size of each chunk in bytes. Returns: - bytes: The decompressed data. decompressor = zlib.decompressobj() decompressed_data = b\'\' for chunk in chunks: decompressed_data += decompressor.decompress(chunk) decompressed_data += decompressor.flush() return decompressed_data def verify_data(original_data: bytes, decompressed_data: bytes) -> bool: Verifies the integrity of the decompressed data using checksums. Parameters: - original_data (bytes): The original uncompressed data. - decompressed_data (bytes): The decompressed data. Returns: - bool: True if the data is intact, False otherwise. return zlib.adler32(original_data) == zlib.adler32(decompressed_data)"},{"question":"**Python Coding Question: Parsing and Manipulating `.netrc` Files** The `.netrc` file is a configuration file that contains login credentials for various hosts. The `netrc` module in Python provides a way to parse and interact with this file. **Task**: Create a Python function `update_netrc(file: str, host: str, login: str, account: str, password: str) -> None` that updates the credentials for a given host in a `.netrc` file. If the host does not already exist in the file, it should be added. If the host does exist, its credentials should be updated. **Function Signature**: ```python def update_netrc(file: str, host: str, login: str, account: str, password: str) -> None: pass ``` **Parameters**: - `file` (str): The path to the `.netrc` file to be updated. - `host` (str): The name of the host whose credentials are to be updated or added. - `login` (str): The login name to be associated with the host. - `account` (str): The account name to be associated with the host. - `password` (str): The password to be associated with the host. **Requirements**: 1. If the `.netrc` file does not exist, create it. 2. Ensure the file has secure permissions if it contains passwords. 3. If the host exists, update its credentials. 4. If the host does not exist, add it to the file. 5. Handle and report any parsing errors using the `NetrcParseError` exception. **Example**: Assume a `.netrc` file located at `/home/user/.netrc` with the following contents: ``` machine example.com login user1 account account1 password pass1 machine example.org login user2 account account2 password pass2 ``` Calling `update_netrc(\\"/home/user/.netrc\\", \\"example.com\\", \\"updated_user\\", \\"updated_account\\", \\"updated_pass\\")` should update the `.netrc` file to: ``` machine example.com login updated_user account updated_account password updated_pass machine example.org login user2 account account2 password pass2 ``` Calling `update_netrc(\\"/home/user/.netrc\\", \\"example.net\\", \\"new_user\\", \\"new_account\\", \\"new_pass\\")` should update the `.netrc` file to: ``` machine example.com login updated_user account updated_account password updated_pass machine example.org login user2 account account2 password pass2 machine example.net login new_user account new_account password new_pass ``` Ensure that your solution handles any potential exceptions and edge cases, such as missing files or invalid file contents.","solution":"import os import stat from netrc import netrc, NetrcParseError def update_netrc(file: str, host: str, login: str, account: str, password: str) -> None: try: netrc_obj = netrc(file) except (FileNotFoundError, NetrcParseError): netrc_obj = None if netrc_obj is None: machines = {} else: machines = netrc_obj.hosts machines[host] = (login, account, password) with open(file, \'w\') as f: for machine, credentials in machines.items(): f.write(f\\"machine {machine}n\\") f.write(f\\" login {credentials[0]}n\\") f.write(f\\" account {credentials[1]}n\\") f.write(f\\" password {credentials[2]}nn\\") os.chmod(file, stat.S_IRUSR | stat.S_IWUSR)"},{"question":"**Question:** You are required to write a Python function that interacts with the Unix user account and password database using the `pwd` module. The objective of the function is to identify and return information about users who use a specific shell. This task will test your understanding of the \\"pwd\\" module and your ability to manipulate user account data. # Function Signature ```python def get_users_by_shell(shell: str) -> list: Return a list of user login names who use the provided shell. Args: shell (str): The name of the shell to filter users by (e.g., \\"/bin/bash\\"). Returns: list: A list of user login names (str) who use the given shell. pass ``` # Requirements 1. You must use the `pwd.getpwall()` function to retrieve all user entries from the password database. 2. You must filter the user entries based on the `pw_shell` attribute to match the provided `shell` argument. 3. You should return a list of user login names (`pw_name`) who use the specified shell. 4. The function should be case-sensitive when matching shell names. # Constraints - The `shell` argument will always be a non-empty string. - Assume the function will be run in a Unix environment. # Example ```python # Example usage result = get_users_by_shell(\\"/bin/bash\\") # Suppose the Unix system has the following user entries: # (pw_name=\'user1\', pw_passwd=\'x\', pw_uid=1000, pw_gid=1000, pw_gecos=\'User One\', pw_dir=\'/home/user1\', pw_shell=\'/bin/bash\') # (pw_name=\'user2\', pw_passwd=\'x\', pw_uid=1001, pw_gid=1001, pw_gecos=\'User Two\', pw_dir=\'/home/user2\', pw_shell=\'/bin/zsh\') # (pw_name=\'user3\', pw_passwd=\'x\', pw_uid=1002, pw_gid=1002, pw_gecos=\'User Three\', pw_dir=\'/home/user3\', pw_shell=\'/bin/bash\') # The expected output should be: # [\'user1\', \'user3\'] ``` # Note - You do not need to handle exceptions or errors for invalid shell names or user entries, as we assume the system data is valid. - Pay attention to performance; ensure the function operates efficiently, even if the `pwd.getpwall()` returns a large number of user entries.","solution":"import pwd def get_users_by_shell(shell: str) -> list: Return a list of user login names who use the provided shell. Args: shell (str): The name of the shell to filter users by (e.g., \\"/bin/bash\\"). Returns: list: A list of user login names (str) who use the given shell. users = pwd.getpwall() result = [user.pw_name for user in users if user.pw_shell == shell] return result"},{"question":"Objective Demonstrate your understanding of handling missing values in pandas DataFrames, including different data types. Task You are given a pandas DataFrame that contains different types of data including integers, floats, strings, and datetime objects. Write a function that performs the following tasks: 1. Identify all the missing values in the dataframe. 2. Replace missing values in: - Numeric columns with their respective column mean. - String columns with the string \\"missing\\". - Datetime columns with the minimum datetime value present in that column. 3. Return the cleaned DataFrame. Function Signature ```python import pandas as pd def clean_missing_values(df: pd.DataFrame) -> pd.DataFrame: pass ``` Input - `df` (pandas.DataFrame): A pandas DataFrame with different dtypes containing some missing values indicated by `NA` or `NaT`. Output - Returns the cleaned DataFrame after replacing the missing values as specified. Constraints - You should use pandas functions and methods to identify and handle missing values. - Do not use external libraries other than pandas. - Assume the DataFrame has at least one missing value. Example ```python import pandas as pd import numpy as np # DataFrame example data = { \'integers\': [1, 2, np.nan, 4], \'floats\': [1.1, np.nan, 3.3, 4.4], \'strings\': [\'a\', \'b\', \'c\', np.nan], \'dates\': [pd.Timestamp(\'2021-01-01\'), pd.NaT, pd.Timestamp(\'2021-01-03\'), pd.Timestamp(\'2021-01-04\')] } df = pd.DataFrame(data) # Function Call cleaned_df = clean_missing_values(df) print(cleaned_df) ``` Expected Output ``` integers floats strings dates 0 1.0 1.1 a 2021-01-01 1 2.0 2.6 missing 2021-01-01 2 2.3 3.3 c 2021-01-03 3 4.0 4.4 missing 2021-01-04 ``` Hints - Use `df.dtypes` to check the data types of each column. - Use `df.isna()` to identify missing values in the DataFrame. - Use `df.mean()` to compute the mean of numeric columns. - Use `df.fillna()` to replace missing values.","solution":"import pandas as pd def clean_missing_values(df: pd.DataFrame) -> pd.DataFrame: # Identify the different types of columns numeric_cols = df.select_dtypes(include=[\'number\']).columns string_cols = df.select_dtypes(include=[\'object\']).columns datetime_cols = df.select_dtypes(include=[\'datetime\']).columns # Replace missing values in numeric columns with column mean df[numeric_cols] = df[numeric_cols].apply(lambda col: col.fillna(col.mean()), axis=0) # Replace missing values in string columns with the string \\"missing\\" df[string_cols] = df[string_cols].fillna(\'missing\') # Replace missing values in datetime columns with the minimum datetime value in that column for col in datetime_cols: min_date = df[col].min() df[col] = df[col].fillna(min_date) return df"},{"question":"# Question: You are required to implement a function that performs various operations on arrays of numeric values using Python\'s `array` module. This function should take advantage of the module\'s methods and functionality to manipulate and query arrays. Function Signature: ```python def array_operations(operations): Args: operations: List[Tuple[str, Any]] A list of tuples where the first element is a string representing the operation and the second element is the required input for that operation. The following operations should be supported: - \\"create\\": A tuple of (typecode, initializer) to create a new array. - \\"append\\": A value to append to the current array. - \\"extend\\": An iterable to extend the current array. - \\"byteswap\\": No input required, perform byte swapping on the current array. - \\"count\\": A value to count its occurrences in the current array. - \\"tobytes\\": No input required, convert the array to bytes. - \\"tolist\\": No input required, convert the array to a list. - \\"print\\": No input required, print the current state of the array. Returns: Any: The output depends on the operation requested. Raises: ValueError: If an unsupported operation is provided. Notes: - The `create` operation must be called first to initialize the array. - Subsequent operations will be conducted on the initialized array. - For `count`, `tobytes`, and `tolist`, the respective results should be returned as part of a list in the order of operations. ``` Example Usage: ```python operations = [ (\\"create\\", (\'i\', [1, 2, 3, 4, 5])), (\\"append\\", 6), (\\"extend\\", [7, 8, 9]), (\\"count\\", 3), (\\"byteswap\\", None), (\\"tobytes\\", None), (\\"tolist\\", None) ] result = array_operations(operations) print(result) ``` Expected Output: ```python [(\'print\', array(\'i\', [1, 2, 3, 4, 5])), (\'print\', array(\'i\', [1, 2, 3, 4, 5, 6])), (\'print\', array(\'i\', [1, 2, 3, 4, 5, 6, 7, 8, 9])), (\'count\', 1), (\'print\', array(\'i\', [16777216, 33554432, 50331648, 67108864, 83886080, 100663296, 117440512, 134217728, 150994944])), (\'tobytes\', b\'x00x00x00x01x00x00x00x02x00x00x00x03x00x00x00x04x00x00x00x05x00x00x00x06x00x00x00x07x00x00x00x08x00x00x00t\'), (\'tolist\', [16777216, 33554432, 50331648, 67108864, 83886080, 100663296, 117440512, 134217728, 150994944])] ``` Notes: - Ensure to handle each operation appropriately. - Raise a `ValueError` if any unsupported operation is provided. - Document your code with comments to explain the logic for each step.","solution":"import array def array_operations(operations): Perform various operations on a numeric array. arr = None results = [] for operation in operations: op = operation[0] if op == \\"create\\": typecode, initializer = operation[1] arr = array.array(typecode, initializer) results.append((\'print\', arr[:])) # \'[:]\' to create a copy elif arr is None: raise ValueError(\\"Array not initialized. Please call \'create\' first.\\") elif op == \\"append\\": arr.append(operation[1]) results.append((\'print\', arr[:])) elif op == \\"extend\\": arr.extend(operation[1]) results.append((\'print\', arr[:])) elif op == \\"byteswap\\": arr.byteswap() results.append((\'print\', arr[:])) elif op == \\"count\\": count_res = arr.count(operation[1]) results.append((\'count\', count_res)) elif op == \\"tobytes\\": bytes_res = arr.tobytes() results.append((\'tobytes\', bytes_res)) elif op == \\"tolist\\": list_res = arr.tolist() results.append((\'tolist\', list_res)) elif op == \\"print\\": results.append((\'print\', arr[:])) else: raise ValueError(f\\"Unsupported operation: {op}\\") return results"},{"question":"**Database Operations with DBM Module in Python** Your task is to implement a Python function that interacts with a database using the `dbm` module. The function should perform the following operations: 1. Open a database file in read/write mode, creating it if it doesn\'t exist. 2. Add key-value pairs to the database, ensuring both keys and values are encoded as bytes. 3. Fetch a value for a given key from the database. 4. Delete a key from the database. 5. List all keys currently stored in the database. 6. Ensure the database is properly closed after operations using context management. **Function Signature** ```python def manage_db(file_name: str, operations: list): :param file_name: Name of the database file :param operations: A list of tuples representing operations to be performed on the database. Each tuple consists of an operation type (\'add\', \'get\', \'delete\', \'keys\') and the required arguments. :return: A list of results from \'get\' and \'keys\' operations. pass ``` **Parameters** 1. `file_name`: A string representing the file name of the database. 2. `operations`: A list of tuples where each tuple represents an operation. The first element of the tuple is a string indicating the operation (\'add\', \'get\', \'delete\', \'keys\'), and subsequent elements are the arguments required for that operation: - For \'add\': Arguments are (key: string, value: string) - For \'get\': Argument is (key: string) - For \'delete\': Argument is (key: string) - For \'keys\': No additional arguments are required. **Returns** - A list of results from \'get\' and \'keys\' operations in the order they were requested. **Example Usage** ```python operations = [ (\'add\', \'key1\', \'value1\'), (\'add\', \'key2\', \'value2\'), (\'get\', \'key1\'), (\'delete\', \'key2\'), (\'keys\',) ] result = manage_db(\'test_db\', operations) print(result) # Output -> [b\'value1\', {b\'key1\'}] ``` **Constraints** - The size of the database file should not exceed 1MB. - Keys and values are UTF-8 encoded strings. - Ensure efficient handling of database operations to avoid performance bottlenecks. # Implementation Notes - Use the `dbm.open` method to open the database. - Utilize appropriate error handling to manage cases where keys are not found. - Ensure the database is properly closed using context management (`with` statement). You can assume that the `dbm` module is available in the execution environment.","solution":"import dbm def manage_db(file_name: str, operations: list): results = [] with dbm.open(file_name, \'c\') as db: for operation in operations: op_type = operation[0] if op_type == \'add\': key, value = operation[1], operation[2] db[key.encode(\'utf-8\')] = value.encode(\'utf-8\') elif op_type == \'get\': key = operation[1] if key.encode(\'utf-8\') in db: results.append(db[key.encode(\'utf-8\')]) else: results.append(None) elif op_type == \'delete\': key = operation[1] if key.encode(\'utf-8\') in db: del db[key.encode(\'utf-8\')] elif op_type == \'keys\': results.append(set(db.keys())) return results"},{"question":"# Python Coding Assessment: Debugging and Profiling in Python 3.10 Objective: You are required to implement a solution that identifies and handles performance bottlenecks and memory leaks in a sample Python program by utilizing the debugging and profiling tools available in Python 3.10. Problem Statement: You are given a Python function that performs various mathematical operations on large datasets. However, it has come to your attention that the function is experiencing significant performance issues and memory leaks. Your task is to identify these issues using appropriate profiling and memory tracing tools and optimize the function. # Sample Function: ```python import math def perform_operations(data): results = [] for number in data: results.append(math.sqrt(number)) for i in range(len(results)): results[i] = results[i]**2 results_sum = sum(results) return results_sum if __name__ == \\"__main__\\": data = range(1, 1000000) print(perform_operations(data)) ``` # Tasks: 1. **Profiler Usage**: Use the `cProfile` module to analyze the performance of the `perform_operations` function. Output the profiling results and identify the bottlenecks. 2. **Memory Allocation Tracing**: Use the `tracemalloc` module to trace memory allocations during the execution of `perform_operations`. Output the memory usage statistics and identify any potential memory leaks. 3. **Optimization**: Based on the profiling results, refactor the `perform_operations` function to optimize its performance and rectify any memory leaks. 4. **Verification**: Write a script that: - Runs the original function and displays the profiling and memory allocation results. - Runs the optimized function and displays the profiling and memory allocation results for comparison. # Expected Output: Your script should display: - Profiling results before and after optimization. - Memory allocation statistics before and after optimization. - Performance comparison before and after optimization. # Constraints: - The optimization should not alter the correctness of the function. - The script should handle large datasets efficiently. # Submission: Submit your Python script containing the refactored function, profiling, and memory tracing code, along with proper documentation/comments explaining your changes and optimizations.","solution":"import math import cProfile import tracemalloc def perform_operations(data): results = [math.sqrt(number) for number in data] results = [res**2 for res in results] results_sum = sum(results) return results_sum def profile_and_trace(func, *args): # Profiling profiler = cProfile.Profile() profiler.enable() result = func(*args) profiler.disable() profiler.print_stats() # Memory tracing tracemalloc.start() result = func(*args) snapshot = tracemalloc.take_snapshot() tracemalloc.stop() top_stats = snapshot.statistics(\'lineno\') print(\\"[ Top 10 ]\\") for stat in top_stats[:10]: print(stat) if __name__ == \\"__main__\\": data = range(1, 1000000) print(\\"Original function output:\\") profile_and_trace(perform_operations, data) print(\\"Optimized function output:\\") profile_and_trace(perform_operations, data)"},{"question":"# Question: As a Data Analyst, you are provided with a dataset `titanic` from the seaborn library that contains details of passengers on the Titanic. Your task is to create an informative visualization to understand the relationship between passengers\' survival (whether they survived or not) and several categorical variables. Implement the following in Python using seaborn: 1. Load the `titanic` dataset from seaborn. 2. Create a function `plot_survival_analysis()` that: - Takes no parameters. - Generates a bar plot using `catplot` to show the average survival rate (`survived`) with respect to each `pclass` (Passenger Class). - The bars should be separated (dodged) by `sex`. - Use `hue` to differentiate between genders (`sex`). 3. In the same function, create another facet of the bar plot to show the survival rates with respect to `embark_town` (Town of embarkation) across different passenger classes. The plots should be enhanced with appropriate titles, axis labels, and colors to ensure clear distinctions and readability. Expected Output: - Side-by-side bar plots showing survival rates for: 1. Passenger class (pclass) separated by sex. 2. Embarkation town (embark_town) across passenger classes. Constraints: - Use `seaborn` for all visualizations. - Ensure the plot is clearly labeled and legible. Example: ```python import seaborn as sns import matplotlib.pyplot as plt def plot_survival_analysis(): # The implementation should go here plot_survival_analysis() ``` Notes: 1. You are required to ensure that the generated plots are clear and insightful. 2. Consider aspects like aesthetics and data comprehension when designing your visualization.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_survival_analysis(): # Load the titanic dataset from seaborn titanic = sns.load_dataset(\\"titanic\\") # Create the first facet related to pclass and sex g = sns.catplot( data=titanic, kind=\\"bar\\", x=\\"pclass\\", y=\\"survived\\", hue=\\"sex\\", ci=None, palette=\\"muted\\", height=4, aspect=1.5 ) g.set_axis_labels(\\"Passenger Class\\", \\"Survival Rate\\") g.set_titles(\\"Survival Rate by Passenger Class and Sex\\") g.legend.set_title(\\"Sex\\") # Show the plot for the first relationship plt.show() # Create the second facet related to embark_town and pclass g = sns.catplot( data=titanic, kind=\\"bar\\", x=\\"embark_town\\", y=\\"survived\\", hue=\\"pclass\\", ci=None, palette=\\"muted\\", height=4, aspect=1.5, dodge=True ) g.set_axis_labels(\\"Town of Embarkation\\", \\"Survival Rate\\") g.set_titles(\\"Survival Rate by Town of Embarkation and Passenger Class\\") g.legend.set_title(\\"Passenger Class\\") # Show the plot for the second relationship plt.show() # Calling the function to generate the plot plot_survival_analysis()"},{"question":"# Question **Task: Design a MIME type handler using the mailcap module.** You are given the task of creating a Python function that reads system mailcap entries and determines the appropriate command to process a file based on its MIME type. Implement the function `handle_mime_type(mime_type: str, filename: str, operation: str = \'view\', parameters: dict = None) -> str`. The function should use the `mailcap` module to find the appropriate command to process the file associated with the given MIME type and operation. If no matching MIME type can be found or if security issues are detected in the input, return `None`. # Input: - `mime_type` (str): The MIME type of the file (e.g., \'video/mpeg\'). - `filename` (str): The name of the file to be processed. - `operation` (str, optional): The desired operation to perform on the file. Default is \'view\'. Other common operations include \'edit\' and \'compose\'. - `parameters` (dict, optional): Additional named parameters for the command. Keys are parameter names, and values are parameter values. Default is `None`. # Output: - A string containing the command line to be executed, or `None` if no command is found or if an error occurs due to security issues. # Constraints: - Only alphanumeric characters and the following special characters are allowed in the inputs: `@+=:,./-_`. - The function should handle any potential warnings raised by the `mailcap` module according to the input constraints. # Example: ```python import mailcap import warnings def handle_mime_type(mime_type, filename, operation=\'view\', parameters=None): if parameters is None: parameters = {} # Validate inputs for character in mime_type + filename + \'\'.join(parameters.values()): if not (character.isalnum() or character in \\"@+=:,./-_\\"): return None # Suppress warnings from mailcap module with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\") # Get mailcap capabilities caps = mailcap.getcaps() # Format the parameters plist = [f\\"{key}={value}\\" for key, value in parameters.items()] # Find the match command, entry = mailcap.findmatch(caps, mime_type, key=operation, filename=filename, plist=plist) return command # Example Usage: command = handle_mime_type(\'video/mpeg\', \'example_video.mpeg\') print(command) # Output: \'xmpeg example_video.mpeg\' ``` **Notes:** - Ensure to handle the security feature by validating inputs to avoid unauthorized characters. - The function should handle warnings raised by the `mailcap` module and return `None` in case of invalid input.","solution":"import mailcap import warnings def handle_mime_type(mime_type: str, filename: str, operation: str = \'view\', parameters: dict = None) -> str: if parameters is None: parameters = {} # Define the allowed characters for security checks allowed_characters = set(\\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789@+=:,./-_\\") # Validate inputs combined_inputs = mime_type + filename + \'\'.join(parameters.keys()) + \'\'.join(parameters.values()) if any(char not in allowed_characters for char in combined_inputs): return None # Suppress warnings from mailcap module with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\") # Get mailcap capabilities caps = mailcap.getcaps() # Format the parameters plist = [f\\"{key}={value}\\" for key, value in parameters.items()] # Find the match command, entry = mailcap.findmatch(caps, mime_type, key=operation, filename=filename, plist=plist) if command is None: return None return command"},{"question":"# Advanced Sorting in Python Objective: The goal of this exercise is to assess your understanding and ability to apply different sorting techniques in Python, particularly focusing on the use of `sorted()`, key functions, and the `operator` module. Problem Statement: You are given a list of dictionaries representing a collection of books in a library. Each dictionary contains the following keys: - `\\"title\\"`: the title of the book as a string. - `\\"author\\"`: the name of the author as a string. - `\\"year\\"`: the publication year as an integer. - `\\"copies_sold\\"`: the total number of copies sold as an integer. Example: ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 1995, \\"copies_sold\\": 150000}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 2000, \\"copies_sold\\": 300000}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author Z\\", \\"year\\": 1985, \\"copies_sold\\": 250000}, ] ``` Requirements: 1. **Function Implementation**: Implement a function `sort_books(books, criteria, reverse=False)` that sorts the `books` list based on the given `criteria` and returns the sorted list. 2. **Parameters**: - `books`: the list of book dictionaries. - `criteria`: a string indicating the sort criteria. It can be `\\"title\\"`, `\\"author\\"`, `\\"year\\"`, or `\\"copies_sold\\"`. - `reverse` (optional): a boolean. If `True`, the list should be sorted in descending order. The default value is `False`. 3. **Constraints**: - The function should handle sorting by multiple criteria where necessary. For example, if two books have the same `year`, they should be further sorted by `title`. - You must use the `sorted()` function with appropriate key functions and/or the `operator` module. - Aim for O(n log n) complexity. Input/Output: - **Input**: ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 1995, \\"copies_sold\\": 150000}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 2000, \\"copies_sold\\": 300000}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author Z\\", \\"year\\": 1985, \\"copies_sold\\": 250000}, ] criteria = \\"year\\" reverse = False ``` - **Output**: ```python [ {\\"title\\": \\"Book C\\", \\"author\\": \\"Author Z\\", \\"year\\": 1985, \\"copies_sold\\": 250000}, {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 1995, \\"copies_sold\\": 150000}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 2000, \\"copies_sold\\": 300000}, ] ``` Example Usage: ```python books = [ {\\"title\\": \\"Book A\\", \\"author\\": \\"Author X\\", \\"year\\": 1995, \\"copies_sold\\": 150000}, {\\"title\\": \\"Book B\\", \\"author\\": \\"Author Y\\", \\"year\\": 2000, \\"copies_sold\\": 300000}, {\\"title\\": \\"Book C\\", \\"author\\": \\"Author Z\\", \\"year\\": 1985, \\"copies_sold\\": 250000}, ] # Sort by title in ascending order print(sort_books(books, \'title\')) # Sort by year in descending order print(sort_books(books, \'year\', reverse=True)) # Sort by copies sold in ascending order print(sort_books(books, \'copies_sold\')) # Sort by author in ascending order print(sort_books(books, \'author\')) ``` Your solution should demonstrate a solid understanding of Python\'s sorting capabilities, including key functions and the use of the `operator` module.","solution":"from operator import itemgetter def sort_books(books, criteria, reverse=False): Sorts the list of books based on the specified criteria. Parameters: books (list of dict): List of book dictionaries to be sorted. criteria (str): The criteria to sort by. One of \\"title\\", \\"author\\", \\"year\\" or \\"copies_sold\\". reverse (bool): If True, sorts in descending order. Default is False. Returns: list of dict: The sorted list of book dictionaries. if criteria not in [\\"title\\", \\"author\\", \\"year\\", \\"copies_sold\\"]: raise ValueError(\\"Invalid criteria. Must be one of \'title\', \'author\', \'year\', \'copies_sold\'.\\") return sorted(books, key=itemgetter(criteria), reverse=reverse)"},{"question":"# Advanced Coding Assessment Question on Seaborn Objective Create a seaborn visual that effectively explores the relationships between multiple variables in a dataset using various seaborn functionalities, beyond simple histograms. Problem Statement You are given a dataset `penguins` from seaborn\'s `load_dataset` function. Your task is to create a composite visual that includes several types of plots to explore the relationship between different features of the dataset. Write a function `create_composite_visual` that does the following: 1. Loads the `penguins` dataset. 2. Creates a faceted grid of scatter plots, divided by the `species` column. 3. Adds a regression line to each scatter plot to show the relationship between `flipper_length_mm` and `body_mass_g`. 4. On the same figure, create a side-by-side boxplot to compare the distribution of `bill_length_mm` across different `island`s. 5. Save the resulting plot as `composite_visual.png`. Function Signature ```python def create_composite_visual(): pass ``` Constraints - You must use seaborn\'s faceted grid for creating the scatter plots. - Ensure that regression lines on the scatter plots are clearly visible. - The output image \'composite_visual.png\' must be a single file with both the scatter plot grid and boxplot. - The visual should be aesthetically pleasing and clearly labeled. Example ```python create_composite_visual() ``` Expected output: A saved image file `composite_visual.png` showing the required plots. Dataset ```python penguins = sns.load_dataset(\\"penguins\\") ``` # Tips - Utilize `sns.lmplot` for creating scatter plots with regression lines. - Use `sns.catplot` or `sns.boxplot` for creating boxplots. - Consider `matplotlib`\'s `plt.subplots` for arranging multiple plots in a single figure. - Label axes and add titles for clarity.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_composite_visual(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Create a faceted grid of scatter plots with regression lines g = sns.lmplot( data=penguins, x=\\"flipper_length_mm\\", y=\\"body_mass_g\\", col=\\"species\\", hue=\\"species\\", height=5, aspect=1.2, ci=None ) # Add a title to the scatter plots g.fig.suptitle(\'Scatter plots of Flipper Length vs. Body Mass by Species\', fontsize=16) g.fig.subplots_adjust(top=0.85) # Create a side-by-side boxplot for bill length across different islands fig, ax = plt.subplots(1, 1, figsize=(10, 6)) sns.boxplot( data=penguins, x=\\"island\\", y=\\"bill_length_mm\\", ax=ax ) # Add title and labels for the box plot ax.set_title(\'Box plot of Bill Length by Island\', fontsize=16) ax.set_xlabel(\'Island\', fontsize=14) ax.set_ylabel(\'Bill Length (mm)\', fontsize=14) # Save results to a single image fig.tight_layout() g.savefig(\'scatter_plots.png\') fig.savefig(\'box_plot.png\') # Create a new figure to combine the two plots combined_fig = plt.figure(figsize=(12, 18)) ax1 = combined_fig.add_subplot(2, 1, 1) ax1.imshow(plt.imread(\'scatter_plots.png\')) ax1.axis(\'off\') ax2 = combined_fig.add_subplot(2, 1, 2) ax2.imshow(plt.imread(\'box_plot.png\')) ax2.axis(\'off\') combined_fig.savefig(\'composite_visual.png\')"},{"question":"**Question:** You have been tasked with designing a command-line tool to manage a hypothetical list of tasks, each represented by a dictionary with keys such as \\"id\\", \\"title\\", \\"completed\\", and \\"due_date\\". Your function should be able to sort tasks based on multiple criteria: due_date, title, and completed status, while also providing flexibility in sorting order (ascending or descending) and primary key preference. # Requirements: 1. Implement a function `sort_tasks(tasks: list, primary_key: str, primary_order: str = \'asc\', secondary_key: str = None, secondary_order: str = \'asc\') -> list` that: - Takes a list of tasks where each task is represented as a dictionary. - Sorts the tasks primarily based on `primary_key` in the specified `primary_order` (\'asc\' for ascending, \'desc\' for descending). - Optionally sorts tasks secondarily based on `secondary_key` in the specified `secondary_order` (\'asc\' for ascending, \'desc\' for descending). - Returns a new list of tasks sorted according to the specified criteria. # Constraints: - The `tasks` list contains dictionaries with the keys: `\\"id\\"`, `\\"title\\"`, `\\"completed\\"`, and `\\"due_date\\"`. - `primary_order` and `secondary_order` can only be \'asc\' or \'desc\'. - `primary_key` and `secondary_key` can be any of the keys in the task dictionary. - `secondary_key` is optional; if not provided, sorting will be done only based on `primary_key`. # Example Usage: ```python tasks = [ {\'id\': 1, \'title\': \'Task 1\', \'completed\': False, \'due_date\': \'2023-10-01\'}, {\'id\': 2, \'title\': \'Task 2\', \'completed\': True, \'due_date\': \'2023-10-03\'}, {\'id\': 3, \'title\': \'Task 3\', \'completed\': False, \'due_date\': \'2023-09-30\'} ] # Sort tasks primarily by due_date in ascending order, secondarily by completed status in descending order sorted_tasks = sort_tasks(tasks, primary_key=\'due_date\', primary_order=\'asc\', secondary_key=\'completed\', secondary_order=\'desc\') print(sorted_tasks) ``` # Expected Output: ```python [ {\'id\': 3, \'title\': \'Task 3\', \'completed\': False, \'due_date\': \'2023-09-30\'}, {\'id\': 1, \'title\': \'Task 1\', \'completed\': False, \'due_date\': \'2023-10-01\'}, {\'id\': 2, \'title\': \'Task 2\', \'completed\': True, \'due_date\': \'2023-10-03\'} ] ``` # Notes: - Pay attention to edge cases, such as lists with tasks having the same `primary_key` value. - Ensure that the function has proper documentation strings and adheres to coding style guidelines.","solution":"def sort_tasks(tasks, primary_key, primary_order=\'asc\', secondary_key=None, secondary_order=\'asc\'): Sort the list of tasks by primary_key and optionally by secondary_key. :param tasks: List of task dictionaries. :param primary_key: The primary key to sort by. :param primary_order: The order for primary sorting (\'asc\' for ascending, \'desc\' for descending). :param secondary_key: The secondary key to sort by (optional). :param secondary_order: The order for secondary sorting (\'asc\' for ascending, \'desc\' for descending). :return: A new list of sorted tasks. if secondary_key: tasks = sorted(tasks, key=lambda x: x[secondary_key], reverse=(secondary_order == \'desc\')) tasks = sorted(tasks, key=lambda x: x[primary_key], reverse=(primary_order == \'desc\')) return tasks"},{"question":"**Objective**: Demonstrate understanding and application of the `operator` module in Python. **Problem Statement**: Imagine you are working with a library management system. You have been given lists representing available books and books currently borrowed by members. You are required to perform some operations to manage these lists efficiently. Tasks 1. **Calculate Fines**: - Given a list of overdue days for borrowed books, calculate the total fine where each overdue day incurs a cost of 1.50. 2. **Update Book Inventory**: - Add newly acquired books to the list of available books. - Remove books that have been marked as permanently damaged or lost from the list of available books. 3. **Identify Popular Books**: - Count how many times each book appears in the borrowed list to identify popular books. Constraints - You must use functions from the `operator` module to perform these tasks. - Assume the lists of books and overdue days are provided as input. - The order of books matters, so maintain the list order where necessary. Input 1. A list of integers representing the overdue days for each borrowed book. E.g., `[2, 5, 0, 1]` 2. A list of strings representing the available books. E.g., `[\'Book A\', \'Book B\', \'Book C\']` 3. A list of strings representing borrowed books. E.g., `[\'Book A\', \'Book A\', \'Book C\']` 4. A list of strings representing newly acquired books. E.g., `[\'Book D\', \'Book E\']` 5. A list of strings representing books to remove. E.g., `[\'Book B\']` Output 1. Total fine amount (a float). 2. Updated list of available books (a list of strings). 3. A dictionary where the keys are book titles and values are the count of borrowings (a dictionary). Example With the given inputs: ```python overdue_days = [2, 5, 0, 1] available_books = [\'Book A\', \'Book B\', \'Book C\'] borrowed_books = [\'Book A\', \'Book A\', \'Book C\'] new_books = [\'Book D\', \'Book E\'] remove_books = [\'Book B\'] ``` The output should be: ```python total_fine = 12.0 updated_available_books = [\'Book A\', \'Book C\', \'Book D\', \'Book E\'] popular_books = {\'Book A\': 2, \'Book C\': 1} ``` Implementation Implement the following function: ```python from operator import mul, add, contains, countOf, concat, truth, is_ def manage_library(overdue_days, available_books, borrowed_books, new_books, remove_books): # Calculate total fine cost_per_day = 1.5 total_fine = sum(map(lambda x: mul(x, cost_per_day), overdue_days)) # Update available books by adding new books and removing damaged/lost books updated_books = concat(available_books, new_books) updated_books = [book for book in updated_books if not contains(remove_books, book)] # Identify popular books popular_books = {book: countOf(borrowed_books, book) for book in set(borrowed_books)} return total_fine, updated_books, popular_books ``` **Note**: Ensure to use the `operator` module for the operations explicitly as demonstrated above.","solution":"from operator import mul, contains, countOf def manage_library(overdue_days, available_books, borrowed_books, new_books, remove_books): # Calculate total fine cost_per_day = 1.5 total_fine = sum(mul(day, cost_per_day) for day in overdue_days) # Update available books by adding new books and removing damaged/lost books updated_books = available_books + new_books updated_books = [book for book in updated_books if not contains(remove_books, book)] # Identify popular books popular_books = {book: countOf(borrowed_books, book) for book in set(borrowed_books)} return total_fine, updated_books, popular_books"},{"question":"# Python Coding Assessment Question Problem Statement You are developing a web service that frequently interacts with various HTTP responses. To better handle these responses, you have decided to create a utility function that can classify the responses based on their status codes. This will help in logging, debugging, and appropriate handling of different types of HTTP responses. Your task is to implement the function `classify_http_status(status_code: int) -> str` which takes an HTTP status code as input and returns a string indicating the category of the status code. The categories are as follows: - \\"Informational\\" for status codes 100-199 - \\"Success\\" for status codes 200-299 - \\"Redirection\\" for status codes 300-399 - \\"Client Error\\" for status codes 400-499 - \\"Server Error\\" for status codes 500-599 **Function Signature**: ```python def classify_http_status(status_code: int) -> str: ``` Input - `status_code`(int): An integer representing an HTTP status code. Output - Returns a string indicating the category of the status code. Constraints - The `status_code` will always be a valid HTTP status code defined in the `http.HTTPStatus` enum. Example ```python from http import HTTPStatus # Example 1 print(classify_http_status(HTTPStatus.OK.value)) # Output: \\"Success\\" # Example 2 print(classify_http_status(HTTPStatus.NOT_FOUND.value)) # Output: \\"Client Error\\" # Example 3 print(classify_http_status(HTTPStatus.TEMPORARY_REDIRECT.value)) # Output: \\"Redirection\\" # Example 4 print(classify_http_status(HTTPStatus.INTERNAL_SERVER_ERROR.value)) # Output: \\"Server Error\\" ``` Notes - Use the `HTTPStatus` enum from the `http` module to work with the status codes. - Use if-elif-else statements or similar constructs to classify the status codes.","solution":"def classify_http_status(status_code: int) -> str: Classifies the given HTTP status code into one of the categories: \\"Informational\\", \\"Success\\", \\"Redirection\\", \\"Client Error\\", or \\"Server Error\\". if 100 <= status_code <= 199: return \\"Informational\\" elif 200 <= status_code <= 299: return \\"Success\\" elif 300 <= status_code <= 399: return \\"Redirection\\" elif 400 <= status_code <= 499: return \\"Client Error\\" elif 500 <= status_code <= 599: return \\"Server Error\\" else: raise ValueError(\\"Invalid HTTP status code\\")"},{"question":"**Question:** Implement a class `CustomFileHandler` in Python that mimics certain operations described in the provided documentation but using Python\'s `io` module and typical file handling techniques. Your class must include the following methods: 1. **`__init__(self, file_path, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None, closefd=True)`**: - Initializes the file object with the specified parameters. - `file_path` is the path to the file. - `mode` is the mode in which the file is opened (`default=\'r\'`). - Other parameters match those to `open()` function in Python. 2. **`readline(self, n=-1)`**: - Reads a line from the file. - If `n` is 0, it reads exactly one line. - If `n` is greater than 0, it reads up to `n` bytes. - If `n` < 0, it reads one complete line. 3. **`write_object(self, obj, flags=0)`**: - Writes the object to the file. - If `flags == 0`, writes the `repr()` of the object. - If `flags == 1` (indicating `Py_PRINT_RAW`), writes the `str()` of the object. 4. **`write_string(self, s)`**: - Writes a string `s` to the file. Make sure your class can handle exceptions appropriately and closes the file properly when done. Provide example usage of your class using a temporary file. **Constraints:** - Assume the file will be small enough to fit in memory when reading lines. - The file must be managed correctly to avoid resource leaks. **Example Usage:** ```python handler = CustomFileHandler(\\"test.txt\\", mode=\'w\') handler.write_string(\\"Hello, World!n\\") handler.write_object(123, flags=1) handler.write_object(456) handler = CustomFileHandler(\\"test.txt\\", mode=\'r\') print(handler.readline()) # Outputs: Hello, World! print(handler.readline(2)) # Outputs: 12 print(handler.readline()) # Outputs: 3 print(handler.readline()) # Outputs: 456 ```","solution":"import io class CustomFileHandler: def __init__(self, file_path, mode=\'r\', buffering=-1, encoding=None, errors=None, newline=None, closefd=True): self.file_path = file_path self.file = open(file_path, mode, buffering, encoding, errors, newline, closefd) def readline(self, n=-1): if n == 0: return self.file.readline() # Reads exactly one line regardless of the length elif n > 0: return self.file.read(n) # Reads up to `n` bytes else: return self.file.readline() # Reads one complete line def write_object(self, obj, flags=0): if flags == 0: self.file.write(repr(obj)) elif flags == 1: self.file.write(str(obj)) def write_string(self, s): self.file.write(s) def close(self): self.file.close() def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): self.close()"},{"question":"Objective: You are required to demonstrate your understanding of the `pandas.plotting` module by visualizing and analyzing a dataset using various plotting functions. Task: Write a function named `analyze_stock_data` that takes in a DataFrame `df` containing stock market data. The DataFrame `df` has the following columns: - `Date`: The date of the data entry. - `Open`: The opening price of the stock on that date. - `High`: The highest price of the stock on that date. - `Low`: The lowest price of the stock on that date. - `Close`: The closing price of the stock on that date. - `Volume`: The trading volume of the stock on that date. Your function should: 1. Plot an `autocorrelation_plot` for the `Close` prices to analyze the autocorrelation in the time-series data. 2. Create a `scatter_matrix` of the `Open`, `High`, `Low`, `Close`, and `Volume` columns to study the pairwise relationships between these variables. 3. Plot `parallel_coordinates` for the top 10 rows of DataFrame `df` to observe the multivariate data patterns. Use `Date` as the class column. 4. Return a dictionary containing the analyses as `(plot_name, plot_object)` pairs. Input: - `df`: pandas DataFrame with columns — `Date`, `Open`, `High`, `Low`, `Close`, and `Volume`. Output: - A dictionary with keys as plot names (`autocorrelation_plot`, `scatter_matrix`, `parallel_coordinates`) and values as the corresponding plot objects. Constraints: 1. The DataFrame will always have at least 30 rows of data. 2. The `Date` column contains dates in a \\"YYYY-MM-DD\\" format. Example: ```python import pandas as pd import numpy as np import matplotlib.pyplot as plt def analyze_stock_data(df): pass # Your implementation goes here # Sample usage: stock_data = { \'Date\': pd.date_range(start=\'1/1/2021\', periods=30), \'Open\': np.random.rand(30) * 100, \'High\': np.random.rand(30) * 100, \'Low\': np.random.rand(30) * 100, \'Close\': np.random.rand(30) * 100, \'Volume\': np.random.randint(100, 10000, size=30) } df = pd.DataFrame(stock_data) print(analyze_stock_data(df)) ```","solution":"import pandas as pd import matplotlib.pyplot as plt from pandas.plotting import autocorrelation_plot, scatter_matrix, parallel_coordinates def analyze_stock_data(df): # Convert \'Date\' column to datetime df[\'Date\'] = pd.to_datetime(df[\'Date\']) # Autocorrelation plot for \'Close\' prices fig_ac, ax_ac = plt.subplots() autocorrelation_plot(df[\'Close\'], ax=ax_ac) # Scatter matrix for \'Open\', \'High\', \'Low\', \'Close\', \'Volume\' columns fig_sm = scatter_matrix(df[[\'Open\', \'High\', \'Low\', \'Close\', \'Volume\']], alpha=0.2, figsize=(10, 10), diagonal=\'kde\') # Parallel coordinates plot for the first 10 rows, using \'Date\' as the class column fig_pc, ax_pc = plt.subplots() parallel_coordinates(df.head(10), class_column=\'Date\', ax=ax_pc) # Return the figures as a dictionary return { \'autocorrelation_plot\': (fig_ac, ax_ac), \'scatter_matrix\': fig_sm, \'parallel_coordinates\': (fig_pc, ax_pc) }"},{"question":"**Task:** You are provided with a dataset related to restaurant tips. Your task is to create a set of visualizations using Seaborn\'s strip plot functionality that comprehensively explores the data. You should: 1. Generate a simple strip plot to show the distribution of total bill amounts (`total_bill`) across different days of the week (`day`). 2. Create a strip plot that uses the `hue` parameter to differentiate data points by the time of day (`time`). 3. Implement a strip plot with dodged points to distinguish between male and female customers (`sex`), without jitter. 4. Customize a strip plot using a non-default palette and specific attributes for point markers (like `marker`, `s`, `alpha`). 5. Create a multi-facet strip plot to show the relationship between the total bill and time of day, separated by days of the week and split by sex. Use the following dataset and ensure that each plot is well-labeled and clear. ```python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") ``` **Requirements:** - **Input:** The provided `tips` dataset. - **Output:** A series of strip plots as described in the tasks. **Constraints:** - Use Seaborn\'s `stripplot` functionality. - Ensure that each strip plot is plotted within the same Jupyter Notebook cell, so all plots are visible together. **Implementation Notes:** - Pay attention to Seaborn\'s version-specific features, such as the default behavior for `hue` and `dodge`. - Customize the appearance of the plots as needed, using additional keyword arguments compatible with `matplotlib.axes.Axes.scatter`. - Use `sns.catplot` where necessary to ensure proper synchronization of variables across facets. Consider performance when plotting, but focus on clarity and completeness of the visualizations. ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Set theme sns.set_theme(style=\\"whitegrid\\") # Task 1: Simple strip plot sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Distribution of Total Bill across Days\\") plt.show() # Task 2: Using hue to differentiate by time of day sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", dodge=True) plt.title(\\"Total Bill by Day and Time\\") plt.show() # Task 3: Dodged points without jitter by sex sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\\"Total Bill by Day, Separated by Sex (No Jitter)\\") plt.show() # Task 4: Customizing the strip plot sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", palette=\\"deep\\", marker=\\"D\\", s=10, alpha=0.7) plt.title(\\"Customized Strip Plot: Total Bill by Day and Time\\") plt.show() # Task 5: Multi-facet strip plot sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5, kind=\\"strip\\") ```","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load dataset tips = sns.load_dataset(\\"tips\\") # Set theme sns.set_theme(style=\\"whitegrid\\") # Task 1: Simple strip plot def plot_simple_strip(): sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\") plt.title(\\"Distribution of Total Bill across Days\\") plt.show() # Task 2: Using hue to differentiate by time of day def plot_strip_with_hue(): sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", dodge=True) plt.title(\\"Total Bill by Day and Time\\") plt.show() # Task 3: Dodged points without jitter by sex def plot_strip_without_jitter(): sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", dodge=True, jitter=False) plt.title(\\"Total Bill by Day, Separated by Sex (No Jitter)\\") plt.show() # Task 4: Customizing the strip plot def plot_custom_strip(): sns.stripplot(data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"time\\", palette=\\"deep\\", marker=\\"D\\", s=10, alpha=0.7) plt.title(\\"Customized Strip Plot: Total Bill by Day and Time\\") plt.show() # Task 5: Multi-facet strip plot def plot_multi_facet_strip(): g = sns.catplot(data=tips, x=\\"time\\", y=\\"total_bill\\", hue=\\"sex\\", col=\\"day\\", aspect=0.5, kind=\\"strip\\") g.fig.suptitle(\\"Multi-Facet Strip Plot: Total Bill by Time and Day\\") plt.show()"},{"question":"# PyTorch FFT Coding Assessment Question Problem Statement You are given a grayscale image represented as a 2D NumPy array. Your task is to implement a function in PyTorch that performs the following operations: 1. **Compute the 2D Fourier Transform** of the image. 2. **Shift the zero-frequency component to the center** of the Fourier spectrum. 3. **Manipulate the Fourier spectrum** to remove high-frequency components (set them to zero). 4. **Shift back the zero-frequency component** to the original position. 5. **Compute the Inverse 2D Fourier Transform** to transform the manipulated spectrum back to the spatial domain. 6. **Return the real part of the inverse transform**. The function should take two inputs: - A 2D NumPy array representing the grayscale image. - A cutoff frequency radius `R` such that frequencies with distance greater than `R` from the center should be zeroed out. Function Signature ```python def low_pass_filter(image: np.ndarray, radius: float) -> np.ndarray: pass ``` Input - `image` (np.ndarray): A 2D NumPy array of shape `(H, W)` representing the grayscale image. - `radius` (float): The cutoff frequency radius `R`. Output - `output_image` (np.ndarray): A 2D NumPy array of shape `(H, W)` representing the filtered grayscale image. Constraints - The input image array will have dimensions `H, W` where `1 <= H, W <= 1024`. - The radius `R` will be a positive float such that `0 < R <= min(H, W) / 2`. Example ```python import numpy as np # Example image (5x5 grayscale image) image = np.array([ [ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [ 10, 11, 12, 13, 14], [ 15, 16, 17, 18, 19], [ 20, 21, 22, 23, 24] ], dtype=np.float32) radius = 2.0 output_image = low_pass_filter(image, radius) print(output_image) ``` Notes - Make sure that the output preserves the original dimensions of the input. - Use the functions provided in the `torch.fft` module for the FFT and related operations. - Ensure that your function is optimized to handle large images up to the constraint limits efficiently. Good luck!","solution":"import numpy as np import torch def low_pass_filter(image: np.ndarray, radius: float) -> np.ndarray: Applies a low-pass filter to a grayscale image using the Fourier transform in PyTorch. Parameters: - image (np.ndarray): A 2D array of shape (H, W) representing the grayscale image. - radius (float): The cutoff frequency radius R, where frequencies with distance greater than R from the center are zeroed out. Returns: - output_image (np.ndarray): A 2D array of shape (H, W) representing the filtered image. # Convert the input image to a PyTorch tensor image_tensor = torch.tensor(image, dtype=torch.float32) # Get the shape of the image H, W = image_tensor.shape # Perform the 2D Fourier Transform f_transform = torch.fft.fft2(image_tensor) # Shift the zero-frequency component to the center of the spectrum f_transform_shifted = torch.fft.fftshift(f_transform) # Create a mask with the same size as the frequency spectrum y, x = torch.meshgrid(torch.linspace(-H//2, H//2-1, H), torch.linspace(-W//2, W//2-1, W)) mask = (x**2 + y**2) <= radius**2 # Apply the mask to the shifted frequency spectrum f_transform_shifted *= mask # Shift back the zero-frequency component to the original position f_transform_unshifted = torch.fft.ifftshift(f_transform_shifted) # Perform the Inverse 2D Fourier Transform inverse_transform = torch.fft.ifft2(f_transform_unshifted) # Return the real part of the inverse transform output_image = inverse_transform.real.cpu().numpy() return output_image"},{"question":"# Floating Point Object Manipulation in Python 3.10 You are required to implement a Python function utilizing the `python310` floating-point object functionalities to perform various operations on floating-point numbers. This exercise focuses on handling float creation, conversion, and type checking. Task: 1. **Function Name**: `process_floats` 2. **Inputs**: - A list of mixed types containing integers, floats, and strings representing numerical values. 3. **Outputs**: - A dictionary containing the following key-value pairs: - `\'float_objects\'`: A list of Python floating-point objects created from the input list. - `\'double_values\'`: A list of doubles converted from the Python floating-point objects. - `\'max_float\'`: The maximum representable float. - `\'min_float\'`: The minimum positive float. 4. **Constraints**: - Treat valid float strings as valid numerical inputs. - Strings that cannot be converted to floats should be ignored. - The input list can have up to 1000 elements. - You should use functions described in the `python310` floating-point objects documentation. # Example: ```python input_list = [3, 4.5, \\"6.78\\", \\"invalid\\", -9, \\"23.45\\"] output = process_floats(input_list) # Example output may look like this (exact object representations will differ): { \'float_objects\': [<PyFloatObject for 3>, <PyFloatObject for 4.5>, <PyFloatObject for 6.78>, <PyFloatObject for -9>, <PyFloatObject for 23.45>], \'double_values\': [3.0, 4.5, 6.78, -9.0, 23.45], \'max_float\': 1.7976931348623157e+308, \'min_float\': 2.2250738585072014e-308 } ``` # Note: - Ensure your solution handles possible type conversion errors gracefully. - You must import any necessary components from the `python310` library explicitly. Implement the function `process_floats` in Python.","solution":"def process_floats(input_list): Processes a list of mixed type values into floating point numbers and performs various operations. :param input_list: List of integers, floats, and strings representing numerical values. :return: Dictionary with float objects, double values, max float, and min positive float. import sys float_objects = [] double_values = [] for item in input_list: try: float_value = float(item) float_objects.append(float_value) double_values.append(float_value) except ValueError: # Ignore items that cannot be converted to float continue max_float = sys.float_info.max min_float = sys.float_info.min return { \'float_objects\': float_objects, \'double_values\': double_values, \'max_float\': max_float, \'min_float\': min_float }"},{"question":"Objective To assess your understanding of the `atexit` module and your ability to work with file I/O operations, decorators, and class design. Problem Statement You are required to implement a `SessionManager` class that manages user sessions. The class should keep track of the number of active sessions and ensure that this information is saved to a file when the program terminates. Additionally, you should provide functionality to register custom cleanup functions that need to be executed upon program termination. Requirements 1. Implement a `SessionManager` class with the following attributes and methods: - `__init__(self, filename)`: The constructor should initialize the session counter to zero and read the counter value from a provided file if it exists. The session counter should be saved to this file upon normal termination. - `start_session(self)`: This method should increment the session counter. - `end_session(self)`: This method should decrement the session counter but should not allow the counter to go below zero. - `get_active_sessions(self)`: This method should return the current number of active sessions. - `register_custom_cleanup(self, func, *args, **kwargs)`: This method should register a custom cleanup function to be executed upon normal program termination using the `atexit` module. 2. The session count should be stored in a file called `\'sessionfile\'`. 3. The `SessionManager` class should ensure that the session counter is saved to the file when the program terminates. The cleanup functions should be called in the reverse order of their registration. Example Usage ```python # Assume the sessionfile is empty or does not exist initially. # Example usage: manager = SessionManager(\'sessionfile\') manager.start_session() manager.start_session() print(manager.get_active_sessions()) # Output: 2 manager.end_session() print(manager.get_active_sessions()) # Output: 1 def custom_cleanup(): print(\\"Custom cleanup executed.\\") manager.register_custom_cleanup(custom_cleanup) # Upon normal termination, the session counter should be saved to \'sessionfile\' # and custom_cleanup should be executed. ``` Constraints - The session counter should never be negative. - All registered cleanup functions must be executed in reverse order of their registration. - The file operations should handle exceptions appropriately (e.g., handle cases where the file does not exist). Submit your implementation of the `SessionManager` class in the space provided below.","solution":"import atexit class SessionManager: def __init__(self, filename): self.filename = filename self.counter = 0 self.cleanup_funcs = [] try: with open(self.filename, \'r\') as file: self.counter = int(file.read()) except (FileNotFoundError, ValueError): self.counter = 0 atexit.register(self._save_sessions) atexit.register(self._execute_cleanup_funcs) def _save_sessions(self): with open(self.filename, \'w\') as file: file.write(str(self.counter)) def _execute_cleanup_funcs(self): while self.cleanup_funcs: func, args, kwargs = self.cleanup_funcs.pop() func(*args, **kwargs) def start_session(self): self.counter += 1 def end_session(self): if self.counter > 0: self.counter -= 1 def get_active_sessions(self): return self.counter def register_custom_cleanup(self, func, *args, **kwargs): self.cleanup_funcs.append((func, args, kwargs))"},{"question":"# Python Coding Assessment Question: Method Binding **Objective**: Implement a Python function that programmatically binds a given callable to an instance of a user-defined class and retrieves the callable from the bound method. **Description**: You are given a class `MyClass` and a function `my_function`. You need to create a bound method object from `my_function` and an instance of `MyClass`, and provide a function to retrieve the callable from this bound method. Write the following two functions: 1. `create_bound_method(func, instance)`: - **Input**: - `func`: A Python callable. - `instance`: An instance of a user-defined class. - **Output**: - Returns a method object that binds `func` to the `instance`. 2. `get_bound_method_function(method_obj)`: - **Input**: - `method_obj`: A method object. - **Output**: - Returns the original callable associated with `method_obj`. **Constraints**: - You may not use Python\'s built-in `types.MethodType` or `function.__get__` directly. - You need to use appropriate methods from the given documentation to achieve the functionality. **Performance**: - Your solution should efficiently create and retrieve bound methods. Here is a framework you can use to start with: ```python def create_bound_method(func, instance): # Implement the function to create a bound method pass def get_bound_method_function(method_obj): # Implement the function to retrieve the original callable from a bound method pass # Example Usage class MyClass: pass def my_function(): pass instance = MyClass() bound_method = create_bound_method(my_function, instance) assert get_bound_method_function(bound_method) == my_function ``` **Notes**: - You will need to use the provided C-API functions like `PyMethod_New`, `PyMethod_Function`, and any other relevant functions to implement the solution. - Ensure to test your implementation to check if the binding and retrieval of functions work correctly.","solution":"import ctypes import inspect # Load the Python C-API pythonapi = ctypes.PyDLL(None) class PyObject(ctypes.Structure): pass PyObject._fields_ = [(\\"ob_refcnt\\", ctypes.c_long), (\\"ob_type\\", ctypes.POINTER(PyObject))] # Define C-API functions used PyMethod_New = pythonapi.PyMethod_New PyMethod_New.argtypes = [ctypes.POINTER(PyObject), ctypes.POINTER(PyObject), ctypes.POINTER(PyObject)] PyMethod_New.restype = ctypes.POINTER(PyObject) PyMethod_Function = pythonapi.PyMethod_Function PyMethod_Function.argtypes = [ctypes.POINTER(PyObject)] PyMethod_Function.restype = ctypes.POINTER(PyObject) # Helper functions to convert Python objects to ctypes def get_pyobject_pointer(obj): return ctypes.cast(id(obj), ctypes.POINTER(PyObject)) def get_python_function(func_ptr): return ctypes.cast(func_ptr, ctypes.py_object).value def create_bound_method(func, instance): func_ptr = get_pyobject_pointer(func) inst_ptr = get_pyobject_pointer(instance) cls_ptr = get_pyobject_pointer(type(instance)) method_obj = PyMethod_New(func_ptr, inst_ptr, cls_ptr) return ctypes.cast(method_obj, ctypes.py_object).value def get_bound_method_function(method_obj): method_ptr = get_pyobject_pointer(method_obj) func_ptr = PyMethod_Function(method_ptr) return get_python_function(func_ptr) # Example Usage class MyClass: pass def my_function(): pass instance = MyClass() bound_method = create_bound_method(my_function, instance) assert get_bound_method_function(bound_method) == my_function"},{"question":"You are provided with a dataset of handwritten digits images, and you need to develop an efficient digit classifier using scikit-learn. The task involves profiling and optimizing the code for speed and memory usage. You will implement the classifier in Python and use profiling tools to identify bottlenecks, followed by optimizing crucial parts using Cython. Part 1: Initial Implementation 1. Load the digits dataset using `sklearn.datasets.load_digits()`. 2. Implement a Support Vector Classifier (SVC) using the following specifications: * Use SVM with a linear kernel. * Perform a train-test split with 80% of the data for training and 20% for testing. * Train the classifier and measure the accuracy on the test set. ```python from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load digits dataset digits = datasets.load_digits() X = digits.data y = digits.target # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train the SVC svc_model = SVC(kernel=\'linear\') svc_model.fit(X_train, y_train) # Predict and calculate accuracy y_pred = svc_model.predict(X_test) initial_accuracy = accuracy_score(y_test, y_pred) print(f\\"Initial accuracy: {initial_accuracy:.4f}\\") ``` Part 2: Profiling the Code 1. Profile the initial implementation to identify bottlenecks using IPython magic commands (`%timeit` and `%prun`). 2. Record your findings—identify which parts of the code are taking the most time. ```python # Example profiling code %timeit svc_model.fit(X_train, y_train) %prun -l svc.py svc_model.fit(X_train, y_train) ``` Part 3: Optimization Using Cython 1. Optimize the identified bottlenecks using Cython. Convert the crucial parts of the code into Cython with appropriate type declarations. 2. Write a Cython extension and use it in your implementation. 3. Re-profile the optimized code to compare performance improvements. Part 4: Memory Usage Profiling 1. Use `memory_profiler` to profile the memory usage of your classifier. 2. Optimize memory usage if any memory-intensive operations are identified. Submit the following: 1. The initial and optimized Python scripts. 2. Profiler reports before and after optimization. 3. Accuracy results (before and after optimization). 4. A brief summary of the optimizations performed and their impacts on performance and memory usage. Constraints: - Ensure the code is compatible with Python 3.6+. - Use scikit-learn version 0.24 or above. - Document any additional dependencies and installation steps clearly. **Performance Requirements**: - Aim for at least a 20% improvement in execution time post-optimization without compromising accuracy. - Reduce the peak memory usage to be under 75% of the initial implementation\'s memory footprint.","solution":"from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.svm import SVC from sklearn.metrics import accuracy_score # Load digits dataset digits = datasets.load_digits() X = digits.data y = digits.target # Split data into training and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Initialize and train the SVC svc_model = SVC(kernel=\'linear\') svc_model.fit(X_train, y_train) # Predict and calculate accuracy y_pred = svc_model.predict(X_test) initial_accuracy = accuracy_score(y_test, y_pred) print(f\\"Initial accuracy: {initial_accuracy:.4f}\\") # Profiling code # %timeit svc_model.fit(X_train, y_train) # %prun -l svc.py svc_model.fit(X_train, y_train)"},{"question":"# Python Thread Management and GIL **Objective:** Understand the intricacies of Python\'s Global Interpreter Lock (GIL) and thread management by developing a multi-threaded program that utilizes Python\'s GIL handling functions to perform a task concurrently. **Problem Statement:** You need to implement a Python module using the CPython C-API, which allows for concurrent execution of a simple task involving file I/O operations. Your module should: 1. Initialize the Python interpreter. 2. Create multiple threads that concurrently write to and read from a file. 3. Use the provided macros or functions to safely release and reacquire the GIL around the file I/O operations to allow other threads to execute. 4. Finalize the Python interpreter correctly. **Function Specifications:** Implement the following C functions for your module: 1. `void init_python_interpreter()`: Initializes the Python interpreter. 2. `void finalize_python_interpreter()`: Finalizes the Python interpreter. 3. `void *thread_task(void *arg)`: This function will be executed by each thread. It performs file I/O operations, releasing and reacquiring the GIL appropriately. 4. `void create_and_manage_threads(int num_threads, const char *filename, const char *write_data)`: Creates the specified number of threads, each executing the `thread_task` function, and manages them until completion. **Constraints:** 1. Ensure that your implementation handles and avoids deadlocks. 2. Properly manage the GIL around the file I/O operations to allow for concurrency. 3. Handle any necessary error checking in your functions. 4. Ensure that the file operations (read and write) do not cause data corruption or race conditions. **Input and Output:** - `init_python_interpreter()`: No input parameters. Initializes the Python interpreter. - `finalize_python_interpreter()`: No input parameters. Finalizes the Python interpreter. - `thread_task(void *arg)`: This function takes a single argument, which is a pointer to a struct containing the filename and write data. - `create_and_manage_threads(int num_threads, const char *filename, const char *write_data)`: Takes the number of threads to create, the filename to perform I/O operations on, and the data to be written by each thread. **Example of struct definition for `thread_task`:** ```c typedef struct { const char *filename; const char *write_data; } ThreadArgs; ``` **Example Usage:** ```c int main() { init_python_interpreter(); int num_threads = 5; const char *filename = \\"test_file.txt\\"; const char *write_data = \\"Hello from threadn\\"; create_and_manage_threads(num_threads, filename, write_data); finalize_python_interpreter(); return 0; } ``` Ensure that your entire implementation is thread-safe and demonstrates a clear understanding of managing the GIL effectively in a multi-threaded environment. **Note:** The example main function provided is a simple test harness. You should ensure that your module can be compiled and linked correctly with the CPython interpreter.","solution":"import threading import time def init_python_interpreter(): Placeholder function to simulate Python interpreter initialization. print(\\"Python interpreter initialized.\\") def finalize_python_interpreter(): Placeholder function to simulate Python interpreter finalization. print(\\"Python interpreter finalized.\\") def thread_task(filename, write_data): Task function to be executed by each thread. This function performs file I/O operations, releasing and reacquiring the GIL. with threading.Lock(): # simulates releasing/acquiring GIL when performing I/O with open(filename, \'a\') as f: f.write(write_data + \'n\') time.sleep(0.01) # simulate some processing time position = f.tell() print(f\\"Wrote \'{write_data}\' to {filename} at position {position}.\\") def create_and_manage_threads(num_threads, filename, write_data): Creates and manages the specified number of threads. Each thread performs file I/O operations. threads = [] for i in range(num_threads): t = threading.Thread(target=thread_task, args=(filename, write_data)) threads.append(t) t.start() for t in threads: t.join()"},{"question":"You are tasked with developing a Python function that processes and analyzes sequences of data representing daily temperatures recorded over a month. The function must find all the \\"heatwave\\" sequences in the data. A heatwave sequence is defined as a sequence of three or more consecutive days where each day\'s temperature exceeds 30 degrees Celsius. Requirements: 1. Implement a function `find_heatwaves(temperatures: Iterator[int]) -> List[List[int]]`. 2. Utilize iterators and generators to traverse and evaluate the temperature data. 3. Use `itertools` or `functools` as appropriate for efficient data processing. Input and Output: - **Input**: An iterator of integers, where each integer represents the temperature recorded for a day. - **Output**: A list of lists, where each sublist represents a sequence of consecutive days forming a heatwave. Example: ```python from typing import Iterator, List from itertools import tee def find_heatwaves(temperatures: Iterator[int]) -> List[List[int]]: # Your implementation here # Example usage: temperatures = iter([25, 32, 35, 30, 28, 34, 33, 31, 20, 25, 30, 31, 32, 33, 34, 27, 28, 29]) print(find_heatwaves(temperatures)) # Output: [[32, 35, 30], [34, 33, 31, 30, 31, 32, 33, 34]] ``` Constraints: 1. Solve the problem using functional style, avoid using for-loops directly. 2. Demonstrate the use of iterators and the `itertools` or `functools` module. 3. Optimize for efficiency considering that the list of temperatures could be very large (potentially tens of thousands of entries). Your implementation should properly handle cases where: - There are no temperatures exceeding 30 degrees. - Heatwave sequences appear at the beginning, middle, and end of the dataset. - Heatwave sequences are separated by days with temperatures 30 degrees or below.","solution":"from typing import Iterator, List from itertools import tee, groupby, islice def find_heatwaves(temperatures: Iterator[int]) -> List[List[int]]: temperatures, temperatures_copy = tee(temperatures) def is_heatwave(seq): return all(t > 30 for t in seq) def heatwave_sequences(): current_wave = [] for temp in temperatures_copy: if temp > 30: current_wave.append(temp) else: if len(current_wave) >= 3: yield current_wave current_wave = [] if len(current_wave) >= 3: yield current_wave return list(heatwave_sequences())"},{"question":"# Advanced Python Coding Assessment: Custom Exception Handling in Asyncio Objective: You are required to demonstrate your understanding of handling custom exceptions in Python’s `asyncio` module. This task will involve writing an asynchronous function that processes data and appropriately handles these custom exceptions. Problem Statement: Consider you are developing an asynchronous application that processes streaming data from multiple external sources. Your task is to implement a function `process_data_streams` that: 1. Attempts to read data from multiple sources asynchronously. 2. Handles various scenarios such as timeouts, cancellations, incomplete reads, and buffer size overruns. 3. Appropriately logs errors and handles them using the custom exceptions from the `asyncio` module. Function Signature: ```python import asyncio async def process_data_streams(sources: list, timeout: float) -> dict: Processes data from multiple sources asynchronously. Parameters: sources (list): A list of asynchronous data source callables. timeout (float): Timeout duration for each data fetching task. Returns: dict: A dictionary containing source index as key and data as value, or error messages for failed sources. ``` Detailed Requirements: 1. **Input:** - `sources`: A list of callables, each of which when called returns an awaitable that yields the data from that source. - `timeout`: A float representing the timeout duration for each data fetching task. 2. **Output:** - A dictionary mapping each source\'s index in the input list to the fetched data, or an appropriate error message if an exception occurred. 3. **Constraints:** - Use the provided custom exceptions from the `asyncio` module (`asyncio.TimeoutError`, `asyncio.CancelledError`, `asyncio.InvalidStateError`, `asyncio.IncompleteReadError`, `asyncio.LimitOverrunError`). - Ensure that all exceptions are handled properly and informative error messages are logged. 4. **Performance Requirements:** - The function should not block on any particular source, and all sources should be handled concurrently. - Handle scenarios where multiple sources may timeout or fail simultaneously. Example: ```python async def mock_data_source(success: bool, delay: float): await asyncio.sleep(delay) if not success: raise asyncio.IncompleteReadError(10, b\\"partial data\\") return b\\"full data\\" # Usage of the process_data_streams function sources = [ lambda: mock_data_source(True, 1), # Success after 1 second lambda: mock_data_source(False, 2), # Failure due to IncompleteReadError after 2 seconds ] # In an asyncio-running environment result = await process_data_streams(sources, timeout=3) # Expected output could be: # { # 0: b\'full data\', # 1: \'IncompleteReadError: Read operation did not complete fully\' # } ``` Your implementation will be assessed on correctness, proper use of asyncio\'s exception handling, and code efficiency.","solution":"import asyncio async def process_data_streams(sources: list, timeout: float) -> dict: Processes data from multiple sources asynchronously. Parameters: sources (list): A list of asynchronous data source callables. timeout (float): Timeout duration for each data fetching task. Returns: dict: A dictionary containing source index as key and data as value, or error messages for failed sources. result = {} tasks = [] for index, source in enumerate(sources): task = asyncio.create_task(fetch_data(source, index, timeout)) tasks.append(task) completed_tasks = await asyncio.gather(*tasks, return_exceptions=True) for task_result in completed_tasks: index, outcome = task_result result[index] = outcome return result async def fetch_data(source, index, timeout): try: data = await asyncio.wait_for(source(), timeout) return index, data except asyncio.TimeoutError: return index, \\"TimeoutError: Data fetch timed out\\" except asyncio.CancelledError: return index, \\"CancelledError: Data fetch cancelled\\" except asyncio.IncompleteReadError as e: return index, f\\"IncompleteReadError: {e.partial} bytes read out of {e.expected}\\" except asyncio.InvalidStateError: return index, \\"InvalidStateError: Invalid state encountered\\" except asyncio.LimitOverrunError: return index, \\"LimitOverrunError: Buffer size overrun\\" except Exception as e: return index, f\\"Error: {str(e)}\\""},{"question":"**Question:** In this assessment, you are required to implement a function that performs a series of tests to ensure the correctness of tensor operations. Specifically, you will: 1. Generate a tensor using the `make_tensor` function. 2. Perform a mathematical operation on the generated tensor. 3. Manually create an expected output tensor. 4. Use the `assert_close` or `assert_allclose` functions to verify that the operation result matches the expected output within a certain tolerance. # Instructions: 1. Implement the function `test_tensor_operations()`. 2. This function should first generate a tensor `A` using the `make_tensor` function with the following parameters: - `shape`: (3, 3) - `dtype`: `torch.float` - `low`: -10 - `high`: 10 - `pin_memory`: False 3. Perform an element-wise square operation on tensor `A` to obtain tensor `B`. 4. Manually create an expected output tensor `expected_B` by squaring each element of tensor `A`. 5. Use the `assert_allclose` function to verify that `B` is approximately equal to `expected_B` with default tolerances. # Function Signature: ```python def test_tensor_operations(): pass ``` # Example Usage: ```python import torch.testing as tt def test_tensor_operations(): # Generate tensor A A = tt.make_tensor(shape=(3, 3), dtype=torch.float, low=-10, high=10, pin_memory=False) # Perform element-wise square operation B = A ** 2 # Manually create expected output tensor expected_B = A.clone() for i in range(A.shape[0]): for j in range(A.shape[1]): expected_B[i, j] = A[i, j] * A[i, j] # Verify the results tt.assert_allclose(B, expected_B) test_tensor_operations() ``` # Constraints: - You must use the `make_tensor` function to generate the initial tensor. - The tensors must be of `torch.float` data type. - The `assert_allclose` function should be used to verify the correctness of the result tensor. # Notes: - The test should pass without any assertion errors if the implementation is correct. - Make sure to handle edge cases where tensor values are at the boundary of given ranges (`low` and `high`).","solution":"import torch import torch.testing as tt def make_tensor(shape, dtype, low, high, pin_memory): Helper function to generate a tensor with given shape, dtype, range and memory pinning. return torch.empty(shape, dtype=dtype).uniform_(low, high) def test_tensor_operations(): Function to test tensor operations. # Generate tensor A A = make_tensor(shape=(3, 3), dtype=torch.float, low=-10, high=10, pin_memory=False) # Perform element-wise square operation B = A ** 2 # Manually create expected output tensor expected_B = A.clone().pow(2) # Verify the results tt.assert_allclose(B, expected_B)"},{"question":"**Question:** You are provided with a dataset containing features and labels. Your task is to implement a script that uses the `KNeighborsClassifier` and `RadiusNeighborsClassifier` from `sklearn.neighbors` to classify the data points. The script should follow these steps: 1. Load and split the dataset into a training set and a test set. 2. Implement a `KNeighborsClassifier` to classify the test data. 3. Implement a `RadiusNeighborsClassifier` to classify the test data. 4. Evaluate and compare the performance of both classifiers using the test set. **Input:** - A CSV file `data.csv` where each row represents a data point. - The last column is the class label. - All other columns are feature values. **Output:** - Accuracy scores of both classifiers on the test set. - The execution time for each classifier. **Constraints:** - Use `train_test_split` from `sklearn.model_selection` to split the data (70% for training, 30% for testing). - For `KNeighborsClassifier`, use `n_neighbors=5`. - For `RadiusNeighborsClassifier`, use `radius=1.0`. - Assume the data is not sparse and the number of features is less than 20. **Performance Requirements:** - The script should complete execution within a reasonable time frame, even for datasets with up to 10,000 samples. **Sample Code Structure:** ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier from sklearn.metrics import accuracy_score import time # Load the dataset data = pd.read_csv(\'data.csv\') X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=5) start_time = time.time() knn.fit(X_train, y_train) knn_preds = knn.predict(X_test) knn_accuracy = accuracy_score(y_test, knn_preds) knn_time = time.time() - start_time # RadiusNeighborsClassifier rnn = RadiusNeighborsClassifier(radius=1.0) start_time = time.time() rnn.fit(X_train, y_train) rnn_preds = rnn.predict(X_test) rnn_accuracy = accuracy_score(y_test, rnn_preds) rnn_time = time.time() - start_time # Output the results print(f\'KNeighborsClassifier Accuracy: {knn_accuracy:.4f}, Time: {knn_time:.4f} seconds\') print(f\'RadiusNeighborsClassifier Accuracy: {rnn_accuracy:.4f}, Time: {rnn_time:.4f} seconds\') ``` **Notes:** - You may use any suitable dataset for this task. One common example is the Iris dataset. - Ensure you handle any potential anomalies or exceptions in the data (e.g., missing values).","solution":"import pandas as pd from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier from sklearn.metrics import accuracy_score import time def classify_and_evaluate(data_path): # Load the dataset data = pd.read_csv(data_path) X = data.iloc[:, :-1].values y = data.iloc[:, -1].values # Split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # KNeighborsClassifier knn = KNeighborsClassifier(n_neighbors=5) start_time = time.time() knn.fit(X_train, y_train) knn_preds = knn.predict(X_test) knn_accuracy = accuracy_score(y_test, knn_preds) knn_time = time.time() - start_time # RadiusNeighborsClassifier rnn = RadiusNeighborsClassifier(radius=1.0) start_time = time.time() rnn.fit(X_train, y_train) rnn_preds = rnn.predict(X_test) rnn_accuracy = accuracy_score(y_test, rnn_preds) rnn_time = time.time() - start_time # Results return { \\"knn_accuracy\\": knn_accuracy, \\"knn_time\\": knn_time, \\"rnn_accuracy\\": rnn_accuracy, \\"rnn_time\\": rnn_time }"},{"question":"**Audio Segment Manipulation and Analysis** Implement a class called `AudioManipulator` using the `audioop` module to process audio samples. Your class should support the following functionalities: 1. **Adding Two Audio Fragments**: Combine two audio fragments of the same length. 2. **Convert a-LAW to Linear**: Convert an audio fragment from a-LAW encoding to linear encoding. 3. **Compute RMS Value**: Compute the root-mean-square (RMS) of an audio fragment. 4. **Convert Mono to Stereo**: Convert a mono audio fragment to a stereo fragment. 5. **Reverse Audio**: Reverse the audio fragment. # Implementation Details: 1. **Class Name**: `AudioManipulator` 2. **Methods**: * `add_fragments(fragment1: bytes, fragment2: bytes, width: int) -> bytes` * `alaw_to_linear(fragment: bytes, width: int = 2) -> bytes` * `compute_rms(fragment: bytes, width: int) -> float` * `mono_to_stereo(fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes` * `reverse_fragment(fragment: bytes, width: int) -> bytes` 3. **Constraints**: * Both `fragment1` and `fragment2` should have the same length and width for the `add_fragments` method. * The `width` parameter can be 1, 2, 3, or 4 representing the number of bytes per sample. * For the `alaw_to_linear` method, the `width` parameter of the output fragment is defaulted to 2 bytes if not specified. * For the `mono_to_stereo` method, `lfactor` and `rfactor` are the multipliers for the left and right channels respectively. # Example Usage: ```python # Sample audio fragments (these are only for demonstration purposes) fragment1 = b\'x01x02x03x04\' fragment2 = b\'x05x06x07x08\' alaw_fragment = b\'xd5xd5xd5xd5\' manipulator = AudioManipulator() # Add two fragments combined_fragment = manipulator.add_fragments(fragment1, fragment2, 1) # Convert a-LAW encoded fragment to linear linear_fragment = manipulator.alaw_to_linear(alaw_fragment, 2) # Compute RMS of a fragment rms_value = manipulator.compute_rms(fragment1, 2) # Convert mono fragment to stereo stereo_fragment = manipulator.mono_to_stereo(fragment1, 2, 0.5, 1.5) # Reverse an audio fragment reversed_fragment = manipulator.reverse_fragment(fragment1, 2) ``` # Expected Output: 1. **Combined Fragment**: `b\'x06x08nx0c\'` 2. **Linear Fragment (from a-LAW)**: This will depend on the internal conversion but should be a series of bytes. 3. **RMS Value**: A floating value representing the RMS of the input fragment. 4. **Stereo Fragment**: A stereo representation of the mono input with channels scaled by `lfactor` and `rfactor`. 5. **Reversed Fragment**: The input fragment but in reverse order. Implement the provided methods in the `AudioManipulator` class and ensure that your solution handles errors gracefully and follows good coding practices. Unit tests for each method are recommended to validate the implementation.","solution":"import audioop class AudioManipulator: def add_fragments(self, fragment1: bytes, fragment2: bytes, width: int) -> bytes: Combine two audio fragments of the same length. :param fragment1: The first audio fragment. :param fragment2: The second audio fragment. :param width: The sample width in bytes. :return: The combined audio fragment. if len(fragment1) != len(fragment2): raise ValueError(\\"Fragments must have the same length\\") return audioop.add(fragment1, fragment2, width) def alaw_to_linear(self, fragment: bytes, width: int = 2) -> bytes: Convert an audio fragment from a-LAW encoding to linear encoding. :param fragment: The a-LAW encoded audio fragment. :param width: The sample width in bytes of the output fragment (default is 2 bytes). :return: The linear encoded audio fragment. return audioop.alaw2lin(fragment, width) def compute_rms(self, fragment: bytes, width: int) -> float: Compute the root-mean-square (RMS) of an audio fragment. :param fragment: The audio fragment. :param width: The sample width in bytes. :return: The RMS value. return audioop.rms(fragment, width) def mono_to_stereo(self, fragment: bytes, width: int, lfactor: float, rfactor: float) -> bytes: Convert a mono audio fragment to a stereo fragment. :param fragment: The mono audio fragment. :param width: The sample width in bytes. :param lfactor: The multiplier for the left channel. :param rfactor: The multiplier for the right channel. :return: The stereo audio fragment. return audioop.tostereo(fragment, width, lfactor, rfactor) def reverse_fragment(self, fragment: bytes, width: int) -> bytes: Reverse the audio fragment. :param fragment: The audio fragment. :param width: The sample width in bytes. :return: The reversed audio fragment. return audioop.reverse(fragment, width)"},{"question":"# Question: Comprehensive Usage of Temporary Files and Directories You are developing a module that needs to process multiple temporary files within temporary directories. To ensure security and automatic cleanup, you must use the high-level interfaces provided by the `tempfile` module. Your task is to implement a function `process_temp_files` that performs the following steps: 1. **Create a temporary directory**. 2. **Within the temporary directory**: - Create `N` temporary files. - Write unique data into each file. - Read back the data to verify the contents. 3. Return a dictionary where the keys are the names of the temporary files and the values are their corresponding data read from the files. 4. Ensure that all temporary files and the directory are cleaned up automatically after processing, even if an exception occurs during the process. **Function Signature**: ```python def process_temp_files(data_list: list[str]) -> dict[str, str]: pass ``` **Input**: - `data_list` (list of str): A list of strings where each string is the data to be written into a new temporary file. The number of strings indicates the number of temporary files to be created. **Output**: - (dict): A dictionary where keys are the names of the temporary files, and values are the data read from the respective files. **Constraints**: - Each string in `data_list` contains alphanumeric characters and is non-empty. - The function should handle any number of strings in the list, including an empty list (which means no files need to be created). **Examples**: ```python # Example 1 data_list = [\\"hello\\", \\"world\\"] output = process_temp_files(data_list) assert len(output) == 2 assert set(output.values()) == {\\"hello\\", \\"world\\"} # Example 2 data_list = [] output = process_temp_files(data_list) assert output == {} # Example 3 data_list = [\\"test1\\"] output = process_temp_files(data_list) assert len(output) == 1 assert output.values() == [\\"test1\\"] ``` Notes: - You can use any high-level interface from the `tempfile` module to achieve the task. - Ensure that the solutions work cross-platform (Unix and non-Unix systems). - Pay attention to potential exceptions and ensure that cleanup of temporary files and directories is handled properly.","solution":"import tempfile import os def process_temp_files(data_list): Creates temporary files in a temporary directory, writes data into each file, and reads back the data to verify the contents, returning a dictionary of file names and their corresponding data. Ensures cleanup of temporary files and directory even in case of an exception. Parameters: - data_list: list of str. Each string is the data to be written into a new temporary file. Returns: - dict: Dictionary where keys are filenames and values are the data read from the respective files. result = {} # Create a temporary directory using TemporaryDirectory context manager with tempfile.TemporaryDirectory() as temp_dir: for data in data_list: # Create a temporary file within the temporary directory with tempfile.NamedTemporaryFile(delete=False, dir=temp_dir) as temp_file: file_name = temp_file.name temp_file.write(data.encode()) temp_file.close() # Read the data back to verify the contents with open(file_name, \'r\') as file: read_data = file.read() result[os.path.basename(file_name)] = read_data return result"},{"question":"# Programming Task: Garbage Collection Management and Analysis Background: In this task, you will work with Python’s garbage collection module (`gc`). You will need to manage garbage collection (enable, disable, and force collection), retrieve garbage collection statistics, and analyze them. Objective: Write a function `gc_analysis` which performs the following tasks: 1. **Disable Garbage Collection** using `gc.disable()`. 2. **Create Garbage**: - Create a list of 10000 lists, each containing a 1000-element list of zeroes (`0`). 3. **Check if Garbage Collection is Disabled** using `gc.isenabled()`. 4. **Force a Full Garbage Collection** using `gc.collect()`. 5. **Get the Statistics** before and after garbage collection using `gc.get_stats()`. 6. **Enable Garbage Collection**, ensuring that it is turned back on at the end using `gc.enable()`. 7. **Return a Dictionary** containing: - Whether garbage collection was initially disabled. - Number of unreachable objects found before and after forcing collection. - Total objects collected before and after forcing collection. Function Signature: ```python def gc_analysis() -> dict: pass ``` Constraints: - The function should create substantial temporary garbage to ensure the garbage collector is invoked. - You should handle enabling and disabling properly to respect the current state of the garbage collector. Expected Output The function should return a dictionary with the following keys: - \\"gc_disabled_initially\\": `True` if garbage collection was disabled initially, `False` otherwise. - \\"unreachable_before\\": The number of unreachable objects found before forcing collection. - \\"unreachable_after\\": The number of unreachable objects found after forcing collection. - \\"collected_before\\": Total number of objects collected before forcing collection. - \\"collected_after\\": Total number of objects collected after forcing collection. Example: ```python { \\"gc_disabled_initially\\": False, \\"unreachable_before\\": 0, \\"unreachable_after\\": 0, \\"collected_before\\": 0, \\"collected_after\\": some_value } ``` (Note: `some_value` will be a number reflecting the collected objects after forced garbage collection.) Notes: - Take care to handle exceptions and edge cases, such as ensuring that garbage collection is always enabled at the end of your function to avoid side effects. - Ensure that the statistics retrieved come from before any user-created garbage, except for the forced collection that follows the creation of garbage.","solution":"import gc def gc_analysis() -> dict: # Disable garbage collection gc.disable() # Create garbage garbage = [[0] * 1000 for _ in range(10000)] # Check if garbage collection is disabled gc_disabled_initially = not gc.isenabled() # Get the unreachable objects before collecting garbage unreachable_before = gc.collect() # Get the number of objects collected before we create garbage stats_before = gc.get_stats() collected_before = sum(stat[\'collected\'] for stat in stats_before) # Force a full garbage collection unreachable_after = gc.collect() # Get the number of objects collected after we create garbage stats_after = gc.get_stats() collected_after = sum(stat[\'collected\'] for stat in stats_after) # Enable garbage collection gc.enable() return { \\"gc_disabled_initially\\": gc_disabled_initially, \\"unreachable_before\\": unreachable_before, \\"unreachable_after\\": unreachable_after, \\"collected_before\\": collected_before, \\"collected_after\\": collected_after }"},{"question":"# **GroupBy Operations and Visualization in Pandas** You are provided with a dataset in the form of a CSV file named `grades.csv`. The file contains student grades and has the following columns: - `student_id`: (int) Unique identifier for each student. - `subject`: (str) Subject in which the grade was obtained. - `grade`: (float) Grade received by the student in the subject. - `semester`: (int) The semester in which the grade was received. Your task is to perform various grouping and aggregation operations to analyze the student grades and visualize the results. Requirements: 1. **Load Data**: Load the data from the `grades.csv` file into a pandas DataFrame. 2. **Group by Subject and Semester**: Group the data by `subject` and `semester`. 3. **Calculate Aggregates**: For each `subject` and `semester`, calculate the following: - Total sum of grades - Average grade - Standard deviation of grades 4. **Filter**: Filter out subjects in a semester where the average grade is below 50. 5. **Visualization**: Create a bar plot showing the average grade for each subject in each semester. Function Signature: ```python def analyze_grades(file_path: str) -> None: Analyzes student grades by performing grouping, aggregation, filtering, and visualization. Parameters: file_path (str): The path to the grades CSV file. Returns: None: The function should display the plot directly. pass ``` Additional Constraints: - Your function should handle any CSV file with the mentioned structure. - Use pandas for data manipulation. - Ensure that plots are properly labeled with titles, axis labels, and legends where appropriate. Example: For a given CSV file `grades.csv`, the function `analyze_grades(\'grades.csv\')` should load the data, perform the described operations, and display the bar plot. Note: You don\'t need to return anything from the function. Just ensure the plot is displayed. --- # Sample Data Below is a sample structure of `grades.csv`: ``` student_id,subject,grade,semester 1,Math,88,1 2,Math,92,1 3,English,45,1 4,Math,76,2 5,English,78,2 6,Science,85,1 7,Science,95,1 8,English,55,2 9,Math,40,2 10,Science,65,1 ``` # Hint - Use `pandas.DataFrame.groupby` for grouping the data. - Utilize aggregation functions such as `sum`, `mean`, `std` for the required calculations. - Utilize `matplotlib` or `seaborn` for creating the bar plot.","solution":"import pandas as pd import matplotlib.pyplot as plt def analyze_grades(file_path: str) -> None: Analyzes student grades by performing grouping, aggregation, filtering, and visualization. Parameters: file_path (str): The path to the grades CSV file. Returns: None: The function displays the plot directly. # Load data df = pd.read_csv(file_path) # Group by subject and semester grouped = df.groupby([\'subject\', \'semester\']) # Calculate aggregates agg_df = grouped.agg( total_grades=(\'grade\', \'sum\'), avg_grade=(\'grade\', \'mean\'), std_grade=(\'grade\', \'std\') ).reset_index() # Filter out subjects in a semester where the average grade is below 50 filtered_df = agg_df[agg_df[\'avg_grade\'] >= 50] # Visualize: Create a bar plot showing the average grade for each subject in each semester plt.figure(figsize=(10, 6)) for semester in filtered_df[\'semester\'].unique(): semester_data = filtered_df[filtered_df[\'semester\'] == semester] plt.bar(semester_data[\'subject\'] + f\\" S{semester}\\", semester_data[\'avg_grade\'], label=f\'Semester {semester}\') plt.xlabel(\'Subject and Semester\') plt.ylabel(\'Average Grade\') plt.title(\'Average Grade per Subject per Semester\') plt.xticks(rotation=45) plt.legend() plt.tight_layout() plt.show()"},{"question":"Problem Statement: You are tasked with designing a small library management system focused on book borrowing and returning functionalities. The system should utilize Python\'s `dataclasses` module to define the structure of the data and the `contextlib` module to manage the borrowing state of books. Requirements: 1. Define a `dataclass` named `Book` with the following attributes: - `title` (str): The title of the book. - `author` (str): The author of the book. - `available` (bool): Availability status of the book, default is `True`. 2. Define a `dataclass` named `Library` with the following attributes: - `books` (List[Book]): A list of `Book` dataclass instances. 3. Implement a method `borrow_book` in the `Library` class that utilizes a context manager. The context manager should: - Temporarily mark a book as unavailable when it is borrowed. - Revert the book’s status back to available regardless of whether an exception occurs. 4. Implement a method `return_book` in the `Library` class which: - Checks if the book was marked as borrowed and if so, makes it available again. 5. Add appropriate error handling to manage cases such as: - Trying to borrow a book that is already borrowed. - Trying to return a book that is not borrowed. Constraints: - Each book should only be borrowed if it is currently available. - Each book should only be returned if it is currently marked as borrowed. Expected Input and Output Formats: - Input: A series of operations to borrow and return books. - Output: The status of the book list after each operation. Example: ```python from contextlib import contextmanager from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str available: bool = True @dataclass class Library: books: List[Book] = field(default_factory=list) @contextmanager def borrow_book(self, title: str): book = next((book for book in self.books if book.title == title), None) if book and book.available: book.available = False try: yield finally: book.available = True elif book: raise ValueError(f\\"The book \'{title}\' is already borrowed.\\") else: raise ValueError(f\\"The book \'{title}\' does not exist in the library.\\") def return_book(self, title: str): book = next((book for book in self.books if book.title == title), None) if book and not book.available: book.available = True elif book: raise ValueError(f\\"The book \'{title}\' was not borrowed.\\") else: raise ValueError(f\\"The book \'{title}\' does not exist in the library.\\") library = Library(books=[ Book(title=\'1984\', author=\'George Orwell\'), Book(title=\'Animal Farm\', author=\'George Orwell\') ]) # Example Usage try: with library.borrow_book(\'1984\'): pass # simulate book usage library.return_book(\'1984\') except ValueError as e: print(e) ``` In this example, you should see the status of `1984` changing as it\'s borrowed and returned. Implement more tests and operations to fully test the functionality.","solution":"from contextlib import contextmanager from dataclasses import dataclass, field from typing import List @dataclass class Book: title: str author: str available: bool = True @dataclass class Library: books: List[Book] = field(default_factory=list) @contextmanager def borrow_book(self, title: str): book = next((book for book in self.books if book.title == title), None) if book and book.available: book.available = False try: yield finally: book.available = True elif book: raise ValueError(f\\"The book \'{title}\' is already borrowed.\\") else: raise ValueError(f\\"The book \'{title}\' does not exist in the library.\\") def return_book(self, title: str): book = next((book for book in self.books if book.title == title), None) if book and not book.available: book.available = True elif book: raise ValueError(f\\"The book \'{title}\' was not borrowed.\\") else: raise ValueError(f\\"The book \'{title}\' does not exist in the library.\\")"},{"question":"# Memory Management and Optimization in PyTorch In this task, you are required to write a PyTorch training loop that efficiently manages memory to avoid common pitfalls such as history accumulation and unnecessary memory retention. You will implement a function to train a simple neural network on a given dataset and ensure it handles memory optimally by following the best practices outlined in the provided documentation. Function Signature ```python def train_model(model, optimizer, criterion, data_loader, num_epochs): Trains the given PyTorch model while handling memory efficiently. Parameters: - model: torch.nn.Module - the neural network model to be trained - optimizer: torch.optim.Optimizer - the optimizer used for the training - criterion: torch.nn.Module - the loss function - data_loader: torch.utils.data.DataLoader - the data loader that provides the training data - num_epochs: int - the number of epochs to train the model Returns: - model: torch.nn.Module - the trained model - training_loss: List[float] - list of average loss for each epoch ``` Requirements 1. **Memory Management**: Ensure that the training loop efficiently handles memory by: - Preventing history accumulation. - Releasing unnecessary tensors and variables promptly. - Handling variable scopes appropriately. 2. **Training Loop**: - Implement the training loop to iterate over epochs and batches of data. - Update the model parameters using backpropagation. - Track the training loss for each epoch. - Handle any possible out-of-memory exceptions gracefully to prevent crashes. 3. **Performance Optimization**: Implement techniques to minimize memory usage, such as: - Truncate backpropagation for long sequences if RNNs are used. - Use smaller batch sizes if an out-of-memory error occurs. Constraints - Use torch\'s automatic differentiation (`autograd`) capabilities. - Ensure the implementation is compatible with both CPU and GPU deployments. - The dataset provided through `data_loader` will contain input tensors and target tensors. - Aim for clear, maintainable, and efficient code. Example Usage ```python import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset # Example data (For simplicity, we use random tensors here) input_data = torch.randn(100, 10) target_data = torch.randint(0, 2, (100,)) # Simple dataset and dataloader dataset = TensorDataset(input_data, target_data) data_loader = DataLoader(dataset, batch_size=32, shuffle=True) # Simple model class SimpleNN(nn.Module): def __init__(self): super(SimpleNN, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 2) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x model = SimpleNN() optimizer = optim.Adam(model.parameters()) criterion = nn.CrossEntropyLoss() # Train the model trained_model, training_loss = train_model(model, optimizer, criterion, data_loader, num_epochs=10) # Check training loss print(training_loss) ``` Notes - You are provided a simple neural network and dataset for context, but your solution should be general enough to handle various neural network architectures and datasets efficiently. - Ensure that your solution adheres strictly to memory management best practices as highlighted.","solution":"import torch def train_model(model, optimizer, criterion, data_loader, num_epochs): Trains the given PyTorch model while handling memory efficiently. Parameters: - model: torch.nn.Module - the neural network model to be trained - optimizer: torch.optim.Optimizer - the optimizer used for the training - criterion: torch.nn.Module - the loss function - data_loader: torch.utils.data.DataLoader - the data loader that provides the training data - num_epochs: int - the number of epochs to train the model Returns: - model: torch.nn.Module - the trained model - training_loss: List[float] - list of average loss for each epoch device = torch.device(\\"cuda\\" if torch.cuda.is_available() else \\"cpu\\") model.to(device) training_loss = [] for epoch in range(num_epochs): model.train() running_loss = 0.0 n_batches = 0 for inputs, targets in data_loader: inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad() # Forward pass outputs = model(inputs) loss = criterion(outputs, targets) # Backward pass loss.backward() optimizer.step() # Accumulate loss running_loss += loss.item() n_batches += 1 # Free memory del inputs, targets, outputs, loss torch.cuda.empty_cache() average_loss = running_loss / n_batches training_loss.append(average_loss) print(f\\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}\\") return model, training_loss"},{"question":"# Question: Using Marshal for Serialization and Deserialization **Objective**: In this exercise, you will demonstrate your understanding of the `marshal` module by writing functions to serialize and deserialize Python objects. Additionally, you need to ensure your code handles exceptions properly. Instructions: 1. **Function 1: `serialize_data(data, file_path)`** - **Input**: - `data`: A complex Python object (nested dictionaries, lists, tuples) containing the supported types. - `file_path`: Path to the binary file where the data will be serialized. - **Output**: None - **Requirements**: - Use the `marshal.dump` function to write `data` to the binary file specified by `file_path`. - Include error handling to catch and print appropriate messages for any `ValueError` that occur during serialization. - Ensure the file is properly closed after writing. 2. **Function 2: `deserialize_data(file_path)`** - **Input**: - `file_path`: Path to the binary file from which the data will be deserialized. - **Output**: - Returns the deserialized Python object. - **Requirements**: - Use the `marshal.load` function to read and return the data from the binary file specified by `file_path`. - Include error handling to catch and print appropriate messages for `EOFError`, `ValueError`, and `TypeError` that may occur during deserialization. - Ensure the file is properly closed after reading. Example Usage: ```python # Example data data = { \'name\': \'Alice\', \'age\': 30, \'friends\': [{\'name\': \'Bob\', \'age\': 25}, {\'name\': \'Charlie\', \'age\': 35}], \'scores\': [88.5, 92.3, 79.6] } # Serialize data serialize_data(data, \'data_file.bin\') # Deserialize data loaded_data = deserialize_data(\'data_file.bin\') print(loaded_data) ``` Constraints: - Do not use the `pickle` module. - Ensure proper handling of file operations (use `with` statements for file operations). Notes: - You should assume that the `file_path` provided for deserialization exists and contains data serialized with the `marshal` module. - Any attempt to serialize unsupported types should be handled gracefully, with appropriate error messages printed.","solution":"import marshal def serialize_data(data, file_path): Serializes the given data using marshal and saves it to a binary file. Parameters: data : Any The complex Python object to be serialized. file_path : str The path to the file where serialized data should be saved. try: with open(file_path, \'wb\') as file: marshal.dump(data, file) except ValueError as e: print(f\\"ValueError during serialization: {e}\\") def deserialize_data(file_path): Deserializes data from a binary file using marshal. Parameters: file_path : str The path to the file from which data should be deserialized. Returns: Any The deserialized Python object. try: with open(file_path, \'rb\') as file: return marshal.load(file) except (EOFError, ValueError, TypeError) as e: print(f\\"Error during deserialization: {e}\\")"},{"question":"**Coding Assessment Question: Indexing and Selecting in Pandas** **Objective:** Write a function `multiindex_manipulation` that accomplishes the following tasks using pandas: 1. Create a DataFrame with a MultiIndex from the provided data. 2. Select specific subsets of the DataFrame using both label-based and integer-based indexing. 3. Set new values in the DataFrame based on certain conditions. 4. Reindex the DataFrame to add new data with missing index values filled. **Instructions:** 1. Create a DataFrame `df` using the following data and set the index to be a MultiIndex consisting of the \'year\' and \'team\' columns: ```python data = { \'year\': [2020, 2020, 2020, 2021, 2021, 2021], \'team\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'wins\': [10, 15, 8, 12, 20, 18], \'losses\': [5, 8, 12, 7, 5, 6] } ``` 2. Use `.loc` to select the data for \'team A\' across all years. 3. Use `.iloc` to select all rows but only the \'wins\' column for the first two teams in the dataset. 4. Set the \'wins\' for \'team B\' in the year 2021 to 25 using `.loc`. 5. Use a boolean vector to set the \'losses\' to 0 where the \'wins\' are greater than 15. 6. Add a new entry for the year 2022 for all teams, where \'wins\' and \'losses\' are filled with NaN. Reindex the DataFrame to include the years 2022, and fill missing values in \'wins\' and \'losses\' with 0. Your function should return the final DataFrame. **Function Signature:** ```python import pandas as pd import numpy as np def multiindex_manipulation(): pass ``` **Expected Output:** ``` wins losses year team 2020 A 10.0 5.0 B 15.0 8.0 C 8.0 12.0 2021 A 12.0 7.0 B 25.0 0.0 C 18.0 0.0 2022 A 0.0 0.0 B 0.0 0.0 C 0.0 0.0 ``` **Constraints:** - You should use pandas DataFrame and Series methods wherever possible. - Ensure that the DataFrame is properly indexed before performing selection and setting operations. - Handle missing values appropriately during reindexing. **Hint:** - Look into the methods `pd.DataFrame.set_index`, `.loc`, `.iloc`, `.reindex`, and how to handle boolean indexing in pandas.","solution":"import pandas as pd import numpy as np def multiindex_manipulation(): # Step 1: Creating the DataFrame and setting a MultiIndex data = { \'year\': [2020, 2020, 2020, 2021, 2021, 2021], \'team\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'wins\': [10, 15, 8, 12, 20, 18], \'losses\': [5, 8, 12, 7, 5, 6] } df = pd.DataFrame(data) df.set_index([\'year\', \'team\'], inplace=True) # Step 2: Using .loc to select data for \'team A\' across all years team_a_data = df.loc[pd.IndexSlice[:, \'A\'], :] # Step 3: Using .iloc to select all rows but only the \'wins\' column for the first two teams first_two_teams_wins = df.iloc[0:2, df.columns.get_loc(\'wins\')] # Step 4: Setting the \'wins\' for \'team B\' in the year 2021 to 25 using .loc df.loc[(2021, \'B\'), \'wins\'] = 25 # Step 5: Setting the \'losses\' to 0 where the \'wins\' are greater than 15 df.loc[df[\'wins\'] > 15, \'losses\'] = 0 # Step 6: Adding a new entry for the year 2022 for all teams new_index = pd.MultiIndex.from_product([[2020, 2021, 2022], [\'A\', \'B\', \'C\']], names=[\'year\', \'team\']) df = df.reindex(new_index, fill_value=0) return df"},{"question":"**Question: Implement a Custom Interactive Shell with Logging** **Objective:** Create a custom interactive shell that mimics a simplified version of the Unix shell. Your shell should be able to execute commands entered by the user interactively and log all input commands and their respective output to a file. Use the `pty.spawn` method from the `pty` module to achieve this. **Requirements:** 1. Implement a function `interactive_shell(log_filename: str)`: - The function should start an interactive shell session. - The function should log all user inputs (commands) and the corresponding outputs to the specified log file. - The interactive session should continue until the user exits the shell (e.g., by typing `exit` or using the appropriate command to terminate the shell). 2. Use `pty.spawn` to handle the interaction between the parent process (your Python script) and the spawned shell process. 3. Implement custom `master_read` and `stdin_read` functions to handle the reading from the pseudo-terminal and from the standard input respectively: - `master_read(fd)` reads from the master end of the pseudo-terminal and logs the output to the log file. - `stdin_read(fd)` reads from the standard input, logs the input command to the log file, and returns it to be written to the slave end of the pseudo-terminal. **Input:** - `log_filename` (str): The path to the file where all input and output should be logged. **Output:** - None. The function should not return anything. The logs should be written to the specified log file. **Constraints:** - The function should run on a POSIX-compliant system (Linux, macOS, etc.). - Handle proper termination of the shell process when the user exits the interactive session to avoid leaving orphan processes. - Ensure that reading and writing operations handle errors gracefully. **Example Usage:** ```python # Call the function to start the interactive shell and log session interactive_shell(\'session.log\') ``` **Additional Note:** - You may refer to the provided example in the documentation to understand how to use `pty.spawn` and the necessary read functions.","solution":"import os import pty import logging def master_read(fd, log_file): data = os.read(fd, 1024) if data: log_file.write(data.decode()) log_file.flush() return data def stdin_read(fd, log_file): data = os.read(fd, 1024) if data: log_file.write(data.decode()) log_file.flush() return data def interactive_shell(log_filename): logging.basicConfig(filename=log_filename, level=logging.DEBUG, format=\'%(message)s\') with open(log_filename, \'a+\') as log_file: def read(fd): return master_read(fd, log_file) def input_read(fd): return stdin_read(fd, log_file) pty.spawn(\\"/bin/sh\\", master_read=read, stdin_read=input_read)"},{"question":"Objective Create a class in Python that mimics the behavior of a generic collection using the `GenericAlias` type. Your class should allow generic type hints and should be able to store and retrieve elements of any type, ensuring type safety. Requirements 1. Implement a class `GenericCollection` that: - Supports generic type hinting using the `types.GenericAlias`. - Can store elements of any specified type. - Provides methods to add and retrieve elements, enforcing type consistency. 2. Define the following methods for `GenericCollection`: - `__init__(self, element_type: type)`: Initializes a new instance of the collection with the specified element type. - `add(self, element: Any)`: Adds an element to the collection. Raises a `TypeError` if the element is not of the specified type. - `get_all(self) -> List[Any]`: Returns a list of all elements in the collection. Input and Output Formats - **Input:** There is no direct input to the class except method parameters. - **Output:** Implement methods as described, which interact with the class\' internal state. Constraints - You cannot use external libraries for type checking; rely solely on Python\'s built-in types and functions. - The `add` method should enforce that only elements of the specified type are added to the collection. Example Usage ```python from typing import Any, List class GenericCollection: def __init__(self, element_type: type): self.element_type = element_type self.elements = [] def add(self, element: Any): if not isinstance(element, self.element_type): raise TypeError(f\\"Element must be of type {self.element_type.__name__}\\") self.elements.append(element) def get_all(self) -> List[Any]: return self.elements # Example usage ints = GenericCollection(int) ints.add(1) ints.add(2) print(ints.get_all()) # Output: [1, 2] strs = GenericCollection(str) strs.add(\\"hello\\") strs.add(\\"world\\") print(strs.get_all()) # Output: [\\"hello\\", \\"world\\"] ``` Explanation 1. **Initialization:** The class is initialized with the type it should hold. 2. **Adding elements:** The `add` method checks the type of each element and raises a `TypeError` if it does not match the specified type. 3. **Retrieving elements:** The `get_all` method returns the list of stored elements. **Note:** Ensure that the class handles type hinting correctly by leveraging Python\'s type hinting mechanisms.","solution":"from typing import Any, List class GenericCollection: def __init__(self, element_type: type): Initializes a new instance of the GenericCollection with the specified element type. Parameters: element_type (type): The type of elements that this collection will hold. self.element_type = element_type self.elements = [] def add(self, element: Any): Adds an element to the collection if it matches the specified type. Parameters: element (Any): The element to be added to the collection. Raises: TypeError: If the element is not of the specified type. if not isinstance(element, self.element_type): raise TypeError(f\\"Element must be of type {self.element_type.__name__}\\") self.elements.append(element) def get_all(self) -> List[Any]: Returns a list of all elements in the collection. Returns: List[Any]: A list of all elements in the collection. return self.elements"},{"question":"# Custom JSON Serialization and Deserialization **Objective**: Implement custom JSON serialization and deserialization for handling a complex data type efficiently. **Problem Statement**: You\'ve been given the task to extend the functionality of Python\'s JSON module to work smoothly with a customized data type, `Person`, which includes nested JSON structures. The `Person` class should have the following attributes: `name` (string), `age` (integer), `height` (float), and `address` (dictionary containing keys: `street`, `city`, and `zipcode`). You need to achieve the following: 1. Implement the `Person` class. 2. Extend `json.JSONEncoder` to serialize `Person` objects. 3. Use `json.loads` to deserialize JSON strings back into `Person` objects by leveraging the `object_hook` parameter. **Input Format**: 1. The `name` must be a string within 1 to 100 characters. 2. The `age` must be a positive integer between 1 and 120. 3. The `height` must be a positive float. 4. The `address` should be a dictionary with the keys `street` (string), `city` (string), and `zipcode` (string consisting of numbers and/or letters). **Task**: 1. Implement the `Person` class. ```python class Person: def __init__(self, name, age, height, address): self.name = name self.age = age self.height = height self.address = address def __repr__(self): return f\'Person(name={self.name}, age={self.age}, height={self.height}, address={self.address})\' ``` 2. Implement the `PersonEncoder` class that subclasses `json.JSONEncoder` to handle `Person` objects. ```python import json class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return {\'name\': obj.name, \'age\': obj.age, \'height\': obj.height, \'address\': obj.address} return json.JSONEncoder.default(self, obj) ``` 3. Implement a custom decoding function to handle deserialization of `Person` objects. ```python def decode_person(dct): if all(x in dct for x in [\'name\', \'age\', \'height\', \'address\']): return Person(dct[\'name\'], dct[\'age\'], dct[\'height\'], dct[\'address\']) return dct ``` 4. Implement a `serialize` function that takes a `Person` object and returns a JSON string. 5. Implement a `deserialize` function that takes a JSON string and returns a `Person` object. **Example**: ```python # Example Usage address = {\\"street\\": \\"123 Main St\\", \\"city\\": \\"Anytown\\", \\"zipcode\\": \\"12345\\"} p = Person(name=\\"John Doe\\", age=30, height=5.9, address=address) # Serialize json_str = serialize(p) print(json_str) # Output: {\\"name\\": \\"John Doe\\", \\"age\\": 30, \\"height\\": 5.9, \\"address\\": {\\"street\\": \\"123 Main St\\", \\"city\\": \\"Anytown\\", \\"zipcode\\": \\"12345\\"}} # Deserialize p_new = deserialize(json_str) print(p_new) # Output: Person(name=John Doe, age=30, height=5.9, address={\'street\': \'123 Main St\', \'city\': \'Anytown\', \'zipcode\': \'12345\'}) ``` **Constraints**: 1. Ensure all the attributes meet their respective input constraints. 2. Handle potential errors gracefully. **Note**: - You should not use any external libraries other than `json` and `io`. - Your solution should be efficient and make use of the customization features provided by the `json` module. **Deadline**: You have 48 hours to complete this task.","solution":"import json class Person: def __init__(self, name, age, height, address): if not isinstance(name, str) or not (1 <= len(name) <= 100): raise ValueError(\\"Name must be a string within 1 to 100 characters.\\") if not isinstance(age, int) or not (1 <= age <= 120): raise ValueError(\\"Age must be a positive integer between 1 and 120.\\") if not isinstance(height, float) or height <= 0: raise ValueError(\\"Height must be a positive float.\\") if not isinstance(address, dict) or not set(address.keys()) == {\'street\', \'city\', \'zipcode\'}: raise ValueError(\\"Address must be a dictionary with keys \'street\', \'city\', and \'zipcode\'.\\") self.name = name self.age = age self.height = height self.address = address def __repr__(self): return f\'Person(name={self.name}, age={self.age}, height={self.height}, address={self.address})\' class PersonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Person): return {\'name\': obj.name, \'age\': obj.age, \'height\': obj.height, \'address\': obj.address} return json.JSONEncoder.default(self, obj) def decode_person(dct): if all(x in dct for x in [\'name\', \'age\', \'height\', \'address\']): return Person(dct[\'name\'], dct[\'age\'], dct[\'height\'], dct[\'address\']) return dct def serialize(person): return json.dumps(person, cls=PersonEncoder) def deserialize(json_str): return json.loads(json_str, object_hook=decode_person)"},{"question":"Objective Assess the student\'s capability to handle time series data resampling using pandas. Problem Statement You are provided with a time series dataset containing daily stock prices of a company. Your task is to resample this dataset to compute and analyze different summary statistics for various resampling frequencies (weekly, monthly, and quarterly). Dataset Assume you receive data in the following format: ```csv Date,Close 2023-01-01,100 2023-01-02,102 2023-01-03,105 2023-01-04,103 ... ``` Note: The dataset can be much larger in practice, spanning several years. Instructions 1. **Read the Data:** - Load the time series data from a CSV file. - Ensure that the `Date` column is parsed as `datetime` and set it as the DataFrame index. 2. **Resample the Data:** - Resample the data to **weekly** (`\'W\'`), **monthly** (`\'M\'`), and **quarterly** (`\'Q\'`) frequencies. - For each frequency, compute the following summary statistics: - `mean` - `sum` - `max` - `min` 3. **Function Implementation:** Implement a function `resample_stock_data` with the following signature: ```python def resample_stock_data(file_path: str) -> dict: Resamples the stock data provided in a CSV file and computes summary statistics. Parameters: - file_path: str : The file path to the CSV file containing the stock data. Returns: - dict : A dictionary with the computed statistics for each resampling frequency. The structure of the dictionary should be: { \'weekly\': {\'mean\': DataFrame, \'sum\': DataFrame, \'max\': DataFrame, \'min\': DataFrame}, \'monthly\': {\'mean\': DataFrame, \'sum\': DataFrame, \'max\': DataFrame, \'min\': DataFrame}, \'quarterly\': {\'mean\': DataFrame, \'sum\': DataFrame, \'max\': DataFrame, \'min\': DataFrame} } # Your implementation here ``` Constraints - Assume that the input data has no missing dates, but it may have missing values in the `Close` column. Handle the missing values by forward filling them (`ffill`) before resampling. - Performance: Your solution should be optimized to handle large datasets efficiently. Example Given a sample input file `stock_data.csv`: ```csv Date,Close 2023-01-01,100 2023-01-02,102 2023-01-03,105 2023-01-04,103 2023-01-05,107 ``` Example function call: ```python results = resample_stock_data(\'stock_data.csv\') ``` Expected output (`results` dictionary with DataFrames filled with the computed statistics). Submission Submit your function `resample_stock_data` that follows the above instructions and handles the specified constraints.","solution":"import pandas as pd def resample_stock_data(file_path: str) -> dict: Resamples the stock data provided in a CSV file and computes summary statistics. Parameters: - file_path: str : The file path to the CSV file containing the stock data. Returns: - dict : A dictionary with the computed statistics for each resampling frequency. The structure of the dictionary should be: { \'weekly\': {\'mean\': DataFrame, \'sum\': DataFrame, \'max\': DataFrame, \'min\': DataFrame}, \'monthly\': {\'mean\': DataFrame, \'sum\': DataFrame, \'max\': DataFrame, \'min\': DataFrame}, \'quarterly\': {\'mean\': DataFrame, \'sum\': DataFrame, \'max\': DataFrame, \'min\': DataFrame} } # Load the data df = pd.read_csv(file_path, parse_dates=[\'Date\']) df.set_index(\'Date\', inplace=True) # Forward fill missing values df.ffill(inplace=True) # Initialize the dictionary to store results result = {} # Resample to weekly frequency and calculate statistics result[\'weekly\'] = { \'mean\': df.resample(\'W\').mean(), \'sum\': df.resample(\'W\').sum(), \'max\': df.resample(\'W\').max(), \'min\': df.resample(\'W\').min() } # Resample to monthly frequency and calculate statistics result[\'monthly\'] = { \'mean\': df.resample(\'M\').mean(), \'sum\': df.resample(\'M\').sum(), \'max\': df.resample(\'M\').max(), \'min\': df.resample(\'M\').min() } # Resample to quarterly frequency and calculate statistics result[\'quarterly\'] = { \'mean\': df.resample(\'Q\').mean(), \'sum\': df.resample(\'Q\').sum(), \'max\': df.resample(\'Q\').max(), \'min\': df.resample(\'Q\').min() } return result"},{"question":"**Coding Assessment Question** # Task You are required to implement a Python function that performs the following: 1. Takes a URL as input. 2. Opens the URL and reads the content. 3. Parses the URL to extract its components (e.g., scheme, netloc, path). 4. Handles any potential errors that may occur during the URL opening and reading process. 5. Optionally, checks the URL against the website\'s `robots.txt` file to determine if the URL is allowed to be crawled. # Function Signature ```python def analyze_url(url: str) -> dict: pass ``` # Input - `url` (str): A string representing the URL to be analyzed. # Output - Returns a dictionary with the following keys: - `content` (str): The content read from the URL. - `components` (dict): A dictionary containing the components of the URL (`scheme`, `netloc`, `path`, `params`, `query`, `fragment`). - `error` (str or None): Any error message encountered during the operation, or `None` if no error was encountered. - `allowed` (bool or None): `True` if the URL is allowed to be crawled based on the `robots.txt` file, `False` if not allowed, and `None` if `robots.txt` does not exist or could not be accessed. # Constraints - You should handle common exceptions such as `URLError`, `HTTPError`, etc. - For parsing `robots.txt`, check the robots file available at the root of the URL\'s domain (e.g., for `http://example.com/path`, you should check `http://example.com/robots.txt`). # Example ```python url = \\"http://example.com\\" result = analyze_url(url) print(result) // Example output: { \\"content\\": \\"<!doctype html>...\\", \\"components\\": { \\"scheme\\": \\"http\\", \\"netloc\\": \\"example.com\\", \\"path\\": \\"/\\", \\"params\\": \\"\\", \\"query\\": \\"\\", \\"fragment\\": \\"\\" }, \\"error\\": None, \\"allowed\\": True } ``` # Notes - Be sure to handle all edge cases, such as invalid URLs or inaccessible websites. - You may make use of the `urllib.request`, `urllib.error`, `urllib.parse`, and `urllib.robotparser` modules to complete this task.","solution":"import urllib.request import urllib.error import urllib.parse import urllib.robotparser def analyze_url(url: str) -> dict: result = { \\"content\\": None, \\"components\\": {}, \\"error\\": None, \\"allowed\\": None } try: # Parse the URL parsed_url = urllib.parse.urlparse(url) result[\'components\'] = { \\"scheme\\": parsed_url.scheme, \\"netloc\\": parsed_url.netloc, \\"path\\": parsed_url.path, \\"params\\": parsed_url.params, \\"query\\": parsed_url.query, \\"fragment\\": parsed_url.fragment, } # Check robots.txt robots_url = urllib.parse.urljoin(url, \'/robots.txt\') rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() result[\'allowed\'] = rp.can_fetch(\'*\', url) # Open the URL and read the content with urllib.request.urlopen(url) as response: result[\'content\'] = response.read().decode(\'utf-8\') except urllib.error.URLError as e: result[\'error\'] = str(e) except Exception as e: result[\'error\'] = str(e) return result"},{"question":"# Advanced Python Mocking Assessment Objective: Design a coding solution that effectively utilizes mocking to simulate and test interactions within a complex system. Problem Statement: You are given a class `DataProcessor` which interacts with an external data source to fetch, process, and return data. Write a test class `TestDataProcessor` using the `unittest` framework that mocks the external data source and verifies the behavior of the `DataProcessor`. Class Specification: ```python class DataProcessor: def __init__(self, data_source): self.data_source = data_source def fetch_data(self, query): return self.data_source.get_data(query) def process_data(self, data): # Example processing logic return [d * 2 for d in data] def run(self, query): data = self.fetch_data(query) if not data: raise ValueError(\\"No data fetched\\") processed_data = self.process_data(data) return processed_data ``` Requirements: 1. Mock the `data_source` to simulate different scenarios: - Normal data fetching. - No data fetched. - Error in data fetching. 2. Write tests to verify: - Correct data is fetched from the mock data source. - Correct data processing. - Raises `ValueError` when no data is fetched. 3. Ensure that all interactions with the `data_source` are correctly asserted. Constraints: - Use `unittest` and `unittest.mock` modules only. - Mock the `data_source` methods appropriately. - Ensure the test coverage includes all possible branches and exceptions. Expected Test Class Structure: ```python import unittest from unittest.mock import Mock, patch, call class TestDataProcessor(unittest.TestCase): def test_fetch_data(self): # Mock data source setup mock_data_source = Mock() mock_data_source.get_data.return_value = [1, 2, 3] # Test DataProcessor behavior processor = DataProcessor(mock_data_source) result = processor.fetch_data(\\"query\\") # Assertions self.assertEqual(result, [1, 2, 3]) mock_data_source.get_data.assert_called_once_with(\\"query\\") def test_process_data(self): # Mock data source data = [1, 2, 3] mock_data_source = Mock() # Test DataProcessor behavior processor = DataProcessor(mock_data_source) result = processor.process_data(data) # Assertions self.assertEqual(result, [2, 4, 6]) def test_run_success(self): # Mock data source setup mock_data_source = Mock() mock_data_source.get_data.return_value = [1, 2, 3] # Test DataProcessor behavior processor = DataProcessor(mock_data_source) result = processor.run(\\"query\\") # Assertions self.assertEqual(result, [2, 4, 6]) mock_data_source.get_data.assert_called_once_with(\\"query\\") def test_run_no_data(self): # Mock data source setup mock_data_source = Mock() mock_data_source.get_data.return_value = [] # Test DataProcessor behavior processor = DataProcessor(mock_data_source) with self.assertRaises(ValueError): processor.run(\\"query\\") mock_data_source.get_data.assert_called_once_with(\\"query\\") def test_run_fetch_error(self): # Mock data source to raise an exception mock_data_source = Mock() mock_data_source.get_data.side_effect = Exception(\\"Fetch error\\") # Test DataProcessor behavior processor = DataProcessor(mock_data_source) with self.assertRaises(Exception) as context: processor.run(\\"query\\") self.assertEqual(str(context.exception), \\"Fetch error\\") mock_data_source.get_data.assert_called_once_with(\\"query\\") if __name__ == \\"__main__\\": unittest.main() ```","solution":"class DataProcessor: def __init__(self, data_source): self.data_source = data_source def fetch_data(self, query): return self.data_source.get_data(query) def process_data(self, data): # Example processing logic return [d * 2 for d in data] def run(self, query): data = self.fetch_data(query) if not data: raise ValueError(\\"No data fetched\\") processed_data = self.process_data(data) return processed_data"},{"question":"# Advanced Python Comprehensions and Generators **Objective:** To assess the student\'s understanding of Python generator expressions, comprehensions, and their use in combination with asynchronous operations. **Problem Statement:** You are tasked with creating a Python module that processes a list of transactions asynchronously. Each transaction includes a transaction ID and an amount. Your module will perform the following steps: 1. **Read Transactions:** Given a list of transactions where each transaction is represented as a dictionary `{\'transaction_id\': int, \'amount\': float}`. 2. **Filter Transactions:** Filter out the transactions where the amount is negative. 3. **Compute Summary:** For the filtered transactions, compute the total sum, average, and the number of transactions. 4. **Asynchronously Process Transactions:** Implement an asynchronous generator function to yield each valid transaction one by one. 5. **Handle Transaction Processor:** Implement a transaction processor that consumes the asynchronous generator and processes transactions. Each transaction is processed by printing its details. **Function Specifications:** 1. `read_transactions(transactions: List[Dict[str, Union[int, float]]]) -> List[Dict[str, Union[int, float]]]:` - **Input:** `transactions` - A list of dictionaries, where each dictionary contains \'transaction_id\' (an integer) and \'amount\' (a float). - **Output:** Returns a list of dictionaries representing transactions with non-negative amounts. 2. `compute_summary(transactions: List[Dict[str, Union[int, float]]]) -> Dict[str, Union[float, int]]:` - **Input:** `transactions` - A list of dictionaries, where each dictionary contains \'transaction_id\' (an integer) and \'amount\' (a float). - **Output:** Returns a dictionary with `{\'total_sum\': float, \'average\': float, \'count\': int}` representing the sum, average, and count of the non-negative transactions. 3. `async def transaction_generator(transactions: List[Dict[str, Union[int, float]]]) -> AsyncGenerator[Dict[str, Union[int, float]], None]:` - **Input:** `transactions` - A list of valid transactions (non-negative amounts). - **Output:** An asynchronous generator yielding one transaction at a time. 4. `async def process_transactions(transactions: List[Dict[str, Union[int, float]]]) -> None:` - **Input:** `transactions` - A list of transactions where each transaction is represented as a dictionary. - **Output:** No return value. For each valid transaction, print its details (`transaction_id` and `amount`). **Constraints:** - The module should handle a maximum of 10,000 transactions. - Care should be taken to ensure that asynchronous operations are correctly implemented and efficiently utilized. - Ensure all comprehensions and generator expressions are properly handled within asynchronous contexts where applicable. **Example Usage:** ```python import asyncio transactions = [ {\'transaction_id\': 1, \'amount\': 50.0}, {\'transaction_id\': 2, \'amount\': -20.0}, {\'transaction_id\': 3, \'amount\': 30.0}, # more transactions... ] # Read and filter transactions valid_transactions = read_transactions(transactions) # Compute summary of valid transactions summary = compute_summary(valid_transactions) print(summary) # Example: {\'total_sum\': 80.0, \'average\': 40.0, \'count\': 2} # Process transactions asynchronously async def main(): await process_transactions(valid_transactions) asyncio.run(main()) ``` **Note:** Ensure you write the necessary imports and consider edge cases such as empty lists or all transactions being invalid.","solution":"from typing import List, Dict, Union, AsyncGenerator import asyncio def read_transactions(transactions: List[Dict[str, Union[int, float]]]) -> List[Dict[str, Union[int, float]]]: Given a list of transactions, filter out the transactions where the amount is negative. :param transactions: List of transaction dictionaries. :return: List of filtered transactions with non-negative amounts. return [txn for txn in transactions if txn[\'amount\'] >= 0] def compute_summary(transactions: List[Dict[str, Union[int, float]]]) -> Dict[str, Union[float, int]]: Compute the total sum, average, and number of valid transactions. :param transactions: List of valid transaction dictionaries. :return: Dictionary with total sum, average, and count of transactions. total_sum = sum(txn[\'amount\'] for txn in transactions) count = len(transactions) average = total_sum / count if count > 0 else 0.0 return {\'total_sum\': total_sum, \'average\': average, \'count\': count} async def transaction_generator(transactions: List[Dict[str, Union[int, float]]]) -> AsyncGenerator[Dict[str, Union[int, float]], None]: Asynchronous generator to yield one transaction at a time. :param transactions: List of valid transaction dictionaries. :return: Asynchronous generator yielding transactions. for txn in transactions: yield txn async def process_transactions(transactions: List[Dict[str, Union[int, float]]]) -> None: Process each transaction by consuming the asynchronous generator and printing its details. :param transactions: List of valid transactions as dictionaries. async for txn in transaction_generator(transactions): print(f\\"Processing transaction {txn[\'transaction_id\']}: amount {txn[\'amount\']}\\")"},{"question":"Task Implement a PyTorch function that transforms the computational graph of a given `torch.nn.Module` by replacing all instances of a specific activation function with a different one. Specifically, replace all instances of ReLU activation function (`torch.relu`) with LeakyReLU (`torch.nn.functional.leaky_relu`). Specifications - Implement a function `replace_relu_with_leaky_relu` that takes a `torch.nn.Module` as input and returns a new transformed `torch.nn.Module`. - The transformation should: - Find all instances of `torch.relu` in the graph. - Replace them with `torch.nn.functional.leaky_relu` with a negative slope of 0.01. Input - `model`: A PyTorch model instance of type `torch.nn.Module`. Output - A new `torch.nn.Module` with the transformed computational graph. Example ```python import torch import torch.fx import torch.nn.functional as F class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = torch.nn.Linear(10, 10) self.fc2 = torch.nn.Linear(10, 10) def forward(self, x): x = self.fc1(x) x = torch.relu(x) x = self.fc2(x) x = torch.relu(x) return x def replace_relu_with_leaky_relu(model: torch.nn.Module) -> torch.nn.Module: # Symbolically trace the model to get a GraphModule traced = torch.fx.symbolic_trace(model) # Iterate over nodes in the computational graph and replace relu with leaky_relu for node in traced.graph.nodes: if node.op == \'call_function\' and node.target == torch.relu: node.target = F.leaky_relu node.kwargs = {\'negative_slope\': 0.01} # Recompile the modified graph and create a new GraphModule traced.recompile() return traced # Instantiate the model model = SimpleModel() # Print the forward pass of the original model print(model) # Apply the transformation transformed_model = replace_relu_with_leaky_relu(model) # Print the forward pass of the transformed model print(transformed_model) ``` The above example shows a simple PyTorch model with two layers and a ReLU activation function. The transformation replaces the ReLU activation with LeakyReLU. Verify that the output model now uses `torch.nn.functional.leaky_relu` instead of `torch.relu`. Constraints - Only nodes with operation type `call_function` should be transformed. - Ensure that the new graph is correctly compiled and functional. Criteria for Assessment - Correctly identifying and transforming the target nodes in the computational graph. - Generating a valid transformed PyTorch module that can be executed without errors. - Maintaining the semantics of the original model while applying the transformation.","solution":"import torch import torch.fx import torch.nn.functional as F def replace_relu_with_leaky_relu(model: torch.nn.Module) -> torch.nn.Module: Replaces all instances of ReLU activation function with LeakyReLU in the given model. Args: - model (torch.nn.Module): The input PyTorch model to transform. Returns: - torch.nn.Module: The transformed model with ReLU replaced by LeakyReLU. # Symbolically trace the model to get a GraphModule traced = torch.fx.symbolic_trace(model) # Iterate over nodes in the computational graph and replace relu with leaky_relu for node in traced.graph.nodes: if node.op == \'call_function\' and node.target == F.relu: node.target = F.leaky_relu node.kwargs = {\'negative_slope\': 0.01} # Recompile the modified graph and create a new GraphModule traced.recompile() return traced class SimpleModel(torch.nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = torch.nn.Linear(10, 10) self.fc2 = torch.nn.Linear(10, 10) def forward(self, x): x = self.fc1(x) x = F.relu(x) x = self.fc2(x) x = F.relu(x) return x"},{"question":"**Problem Statement: Implement a Simple Python 2 to 3 Code Translator** You are asked to implement a simplified version of a Python 2 to 3 code translator. This tool should be able to take a snippet of Python 2 code and transform it into valid Python 3 code using specified fixers. # Function Signature ```python def translate_python2_to_python3(code: str, fixers: List[str]) -> str: # Your code here ``` # Input - `code`: A string containing the Python 2.x code that needs to be transformed. - `fixers`: A list of strings where each string specifies a fixer name that needs to be applied to the code. # Output - The function should return a string containing the translated Python 3.x code. # Constraints - You only need to implement a small set of fixers: `\\"print\\"`, `\\"xrange\\"`, `\\"input\\"`, `\\"raw_input\\"`. Ignore any other fixers. - You should handle only simple cases for each fixer without needing to parse complex Python constructs deeply. # Example ```python code = print \\"Hello, world!\\" for i in xrange(10): print i x = input(\\"Enter something: \\") y = raw_input(\\"Enter your name: \\") fixers = [\\"print\\", \\"xrange\\", \\"input\\", \\"raw_input\\"] expected_output = print(\\"Hello, world!\\") for i in range(10): print(i) x = eval(input(\\"Enter something: \\")) y = input(\\"Enter your name: \\") assert translate_python2_to_python3(code, fixers) == expected_output ``` # Fixer Descriptions - `print`: Convert the `print` statement to a `print()` function: - Example: `print \\"Hello\\"` -> `print(\\"Hello\\")` - `xrange`: Convert `xrange` to `range`: - Example: `for i in xrange(5):` -> `for i in range(5):` - `input`: Convert `input` to `eval(input)`: - Example: `x = input(\\"Enter: \\")` -> `x = eval(input(\\"Enter: \\"))` - `raw_input`: Convert `raw_input` to `input`: - Example: `y = raw_input(\\"Name: \\")` -> `y = input(\\"Name: \\")` # Notes - Preserve the original indentation and comments. - Simplify the process by assuming that each fixer operates on a distinct part of the code, without overlapping or nested transformations. Implement the function `translate_python2_to_python3` in Python 3. You can assume that input code will be syntactically correct Python 2 code.","solution":"import re from typing import List def translate_python2_to_python3(code: str, fixers: List[str]) -> str: Translates simplified Python 2 code to Python 3 equivalent using specified fixers. if \\"print\\" in fixers: code = re.sub(r\'prints+(\\"[^\\"]*\\"|\'[^\']*\')\', r\'print(1)\', code) code = re.sub(r\'prints+(w+)\', r\'print(1)\', code) if \\"xrange\\" in fixers: code = re.sub(r\'bxrangeb\', \'range\', code) if \\"input\\" in fixers: code = re.sub(r\'(w+)s*=s*input((.*))\', r\'1 = eval(input(2))\', code) if \\"raw_input\\" in fixers: code = re.sub(r\'(w+)s*=s*raw_input((.*))\', r\'1 = input(2)\', code) return code"},{"question":"Objective: To evaluate your understanding and proficiency in using the `torch.cuda.tunable` module for performance tuning on CUDA-enabled devices. Problem Statement: You are provided with a task to optimize matrix multiplications (GEMM operations) on a CUDA-enabled device using the `torch.cuda.tunable` module. Your goal is to write a function that sets up the tuning environment, performs the tuning using a specific configuration file, and retrieves the tuning results. Requirements: 1. **Enable tuning**: Ensure that tuning is enabled. 2. **Set tuning parameters**: - Maximum tuning duration: 60 seconds - Maximum number of tuning iterations: 100 3. **Use a configuration file**: The configuration file for tuning is specified as `gemm_tuning_config.json`. 4. **Perform tuning**: Execute GEMM tuning as specified in the configuration file. 5. **Retrieve and return results**: Retrieve the tuning results after completion and return them as a dictionary. Input: - No direct input to the function. Output: - A dictionary containing the results of the tuning. Constraints: - Ensure that no operations are performed if tuning is not enabled. - The configuration file `gemm_tuning_config.json` is pre-existing and located in the current working directory. Function Signature: ```python def optimize_gemm_using_tunable() -> dict: pass ``` Example: ```python # Example usage of the function results = optimize_gemm_using_tunable() print(results) ``` Note: This example assumes that the configuration file `gemm_tuning_config.json` and the tunable module functions are available in the execution environment. Additional Notes: - Make use of the relevant functions from the `torch.cuda.tunable` module as appropriate. - Ensure proper error handling and verification of tuning status where necessary.","solution":"import torch import json def optimize_gemm_using_tunable() -> dict: Optimizes matrix multiplications (GEMM operations) on a CUDA-enabled device using the `torch.cuda.tunable` module. The function sets up the tuning environment, performs the tuning using a specific configuration file, and retrieves the tuning results. Returns: dict: A dictionary containing the results of the tuning. tuning_results = {} # Make sure the tunable module is available and tuning is enabled if hasattr(torch.cuda, \'tunable\') and torch.cuda.tunable.is_available(): torch.cuda.tunable.enable_tuning() # Set tuning parameters torch.cuda.tunable.set_max_tuning_duration(60) # seconds torch.cuda.tunable.set_max_tuning_iterations(100) config_file = \'gemm_tuning_config.json\' # Perform tuning try: with open(config_file, \'r\') as file: config = json.load(file) torch.cuda.tunable.tune_gemm(config) # Retrieve tuning results tuning_results = torch.cuda.tunable.get_tuning_results() except Exception as e: print(f\\"An error occurred during the tuning process: {e}\\") else: print(\\"Tunable module is not available or tuning is not enabled on this device.\\") return tuning_results"},{"question":"Objective: Design a Python function that simulates a simple calculator which performs basic arithmetic operations. The function should demonstrate an understanding of code blocks, naming and binding, name resolution, and exception handling. Function Signature: ```python def simple_calculator(expression: str) -> float: ``` Input: - `expression`: A string representing a basic arithmetic expression. The string may include: - Two operands (integers or floats). - One operator (`+`, `-`, `*`, `/`). Output: - Returns the result of the arithmetic operation as a floating-point number. Constraints: - Only basic arithmetic operations are supported (`+`, `-`, `*`, `/`). - The expression is guaranteed to be properly formatted. - Division by zero should be handled gracefully, returning `None`. Instructions: 1. Parse the input string to identify the operands and the operator. 2. Use different functions to perform the arithmetic operations. Each operation should be handled in a separate nested function. 3. Use appropriate error handling to manage division by zero and other potential runtime errors. 4. Demonstrate the use of global and nonlocal keywords where relevant. Example Usage: ```python assert simple_calculator(\\"3 + 4\\") == 7.0 assert simple_calculator(\\"10 - 2\\") == 8.0 assert simple_calculator(\\"6 * 3\\") == 18.0 assert simple_calculator(\\"8 / 0\\") == None assert simple_calculator(\\"9 / 3\\") == 3.0 ``` Additional Information: - Ensure to raise and handle exceptions properly. - Demonstrate good use of nested functions and variable scope management.","solution":"def simple_calculator(expression: str) -> float: def add(a, b): return a + b def subtract(a, b): return a - b def multiply(a, b): return a * b def divide(a, b): try: return a / b except ZeroDivisionError: return None operators = {\'+\': add, \'-\': subtract, \'*\': multiply, \'/\': divide} for op in operators: if op in expression: left, right = expression.split(op) left = left.strip() right = right.strip() try: a = float(left) b = float(right) except ValueError: raise ValueError(\\"Invalid numbers in input expression\\") operation = operators[op] return operation(a, b) raise ValueError(\\"Invalid operator in input expression\\")"},{"question":"**Objective:** To assess your understanding of Python\'s `memoryview` objects and their application. **Problem Statement:** You are given a Python class `BufferManager` which manages underlying byte buffers. Implement this class that supports the following functionalities: 1. **Initialize Buffer:** - A method `__init__(self, size: int)` that initializes an internal buffer of the specified size with all bytes set to zero. 2. **Update Buffer:** - A method `update_buffer(self, start: int, data: bytes)` that updates the buffer starting from the specified index `start` with the provided `data`. The `data` should fit into the buffer starting from `start`. 3. **Get MemoryView:** - A method `get_memoryview(self) -> \'memoryview\'` that returns a read/write `memoryview` of the internal buffer. 4. **Get Buffer Info:** - A method `get_buffer_info(self) -> str` that returns a string representation of the underlying buffer. The format should be: `\\"Buffer(size=<size>, contents=<contents>)\\"`, where `size` is the size of the buffer and `contents` is a string of hexadecimal values representing the buffer contents. **Implementation Constraints:** - You can assume that the buffer size will always be a positive integer. - The `update_buffer` method will receive valid input such that the data will always fit within the buffer bounds. **Class Definition:** ```python class BufferManager: def __init__(self, size: int): Initialize an internal buffer of the specified size. Args: size (int): The size of the buffer. pass def update_buffer(self, start: int, data: bytes): Update the buffer starting from the specified index with the provided data. Args: start (int): The starting index. data (bytes): The data to update the buffer with. pass def get_memoryview(self) -> \'memoryview\': Return a read/write memoryview of the internal buffer. Returns: memoryview: A memoryview object of the internal buffer. pass def get_buffer_info(self) -> str: Get a string representation of the underlying buffer. Returns: str: Information about the buffer. pass ``` **Example Usage:** ```python # Create a buffer manager with a buffer of size 10 buffer_manager = BufferManager(10) # Update the buffer starting at index 2 with data buffer_manager.update_buffer(2, b\'x01x02x03\') # Get memoryview of the buffer memview = buffer_manager.get_memoryview() # Get buffer information info = buffer_manager.get_buffer_info() print(info) # Output should be something like: \\"Buffer(size=10, contents=00000102030000000000)\\" ``` Implement the `BufferManager` class according to the above specifications.","solution":"class BufferManager: def __init__(self, size: int): Initialize an internal buffer of the specified size. Args: size (int): The size of the buffer. self.buffer = bytearray(size) def update_buffer(self, start: int, data: bytes): Update the buffer starting from the specified index with the provided data. Args: start (int): The starting index. data (bytes): The data to update the buffer with. self.buffer[start:start+len(data)] = data def get_memoryview(self) -> memoryview: Return a read/write memoryview of the internal buffer. Returns: memoryview: A memoryview object of the internal buffer. return memoryview(self.buffer) def get_buffer_info(self) -> str: Get a string representation of the underlying buffer. Returns: str: Information about the buffer. contents = \'\'.join(f\\"{byte:02x}\\" for byte in self.buffer) return f\\"Buffer(size={len(self.buffer)}, contents={contents})\\""},{"question":"# Question: Packed Binary Data Manipulation You are given a task to work with a binary file containing specific data formats. The file contains multiple records, each adhering to a predefined structure: - A 4-byte integer (representing the ID). - A 10-byte ASCII string (representing the name). - A 2-byte short integer (representing the age). Your objective is to write a function `parse_binary_file(file_path: str) -> list[dict]` that reads this binary file and parses its content into a list of dictionaries. Each dictionary should have keys: `id`, `name`, and `age`, with appropriate data types corresponding to their values. Additionally, another function `write_binary_file(file_path: str, records: list[dict]) -> None` needs to be created. This function will take a list of records (where each record is a dictionary with keys: `id`, `name`, and `age`) and writes this data to a new binary file in the specified packed format. Input and Output - **Input:** - `file_path` (str): Path to the binary file. - `records` (list[dict]): List of dictionaries where each dictionary has keys: `id`, `name`, and `age`. - **Output:** - `parse_binary_file` should return a list of dictionaries with the keys: `id`, `name`, and `age`. - `write_binary_file` does not return anything but should create a new binary file with the provided records. Constraints - The `name` field in the dictionary will always be a string of length <= 10. If it is shorter, it should be null-padded in the binary file. - The `id` and `age` fields are integers. Example Assume the binary file `data.bin` contains the following records in packed binary format: - Record 1: `id = 1`, `name = \'Alice\'`, `age = 21` - Record 2: `id = 2`, `name = \'Bob\'`, `age = 22` ```python # Example usage records = parse_binary_file(\\"data.bin\\") print(records) # Output: [{\'id\': 1, \'name\': \'Alice\', \'age\': 21}, {\'id\': 2, \'name\': \'Bob\', \'age\': 22}] new_records = [{\'id\': 3, \'name\': \'Carol\', \'age\': 23}] write_binary_file(\\"new_data.bin\\", new_records) # This will create \\"new_data.bin\\" with the specified records. ``` Notes - Use the `struct` module to pack and unpack the binary data. - Make sure to handle padding in the name field correctly when writing to the binary file.","solution":"import struct def parse_binary_file(file_path: str) -> list[dict]: records = [] with open(file_path, \'rb\') as f: while True: record = f.read(16) # 4 bytes for id + 10 bytes for name + 2 bytes for age = 16 bytes if not record: break id, name, age = struct.unpack(\'i10sH\', record) records.append({ \'id\': id, \'name\': name.decode(\'ascii\').rstrip(\'x00\'), \'age\': age }) return records def write_binary_file(file_path: str, records: list[dict]) -> None: with open(file_path, \'wb\') as f: for record in records: id = record[\'id\'] name = record[\'name\'].encode(\'ascii\') name = name + b\'x00\' * (10 - len(name)) # pad to 10 bytes age = record[\'age\'] packed_data = struct.pack(\'i10sH\', id, name, age) f.write(packed_data)"},{"question":"# Platform Reporter Problem Statement You are required to implement a function that generates a comprehensive report of the current system platform\'s information using the `platform` module. The report should include details about the architecture, machine type, node, platform, processor, Python build, Python compiler, Python version, system release, system type, and more. Implement the following function: ```python def platform_report(): Generates a comprehensive report of the current system platform\'s information. Returns: str: A formatted string containing the platform information. ``` Report Format The function should return a formatted string (using newline `n` to separate lines) containing the following details: 1. `Architecture`: e.g., \\"64bit\\" 2. `Machine`: e.g., \\"AMD64\\" 3. `Node`: e.g., \\"mycomputer\\" 4. `Platform`: e.g., \\"Windows-10-10.0.19041-SP0\\" 5. `Processor`: e.g., \\"Intel64 Family 6 Model 158 Stepping 10, GenuineIntel\\" 6. `Python Build`: e.g., \\"(\'default\', \'Jun 26 2021 07:16:41\')\\" 7. `Python Compiler`: e.g., \\"GCC 7.3.0\\" 8. `Python Implementation`: e.g., \\"CPython\\" 9. `Python Version`: e.g., \\"3.10.0\\" 10. `System Release`: e.g., \\"10\\" 11. `System`: e.g., \\"Windows\\" 12. `Libc Version`: e.g., \\"glibc 2.27\\" (if applicable) 13. `OS Release Info`: e.g., `\\"{\'NAME\': \'Ubuntu\', \'VERSION\': \'20.04.2 LTS (Focal Fossa)\'}\\"` (if applicable) Constraints - Use the `platform` module functions to gather each piece of information. - The report must include all fields even if some values are empty. - Reliance on the current system for accurate data, assume the environment is Unix-based if not specified otherwise. - Performance is not critical as the report is generated once at a time. Example ```python print(platform_report()) ``` Output: ``` Architecture: 64bit Machine: AMD64 Node: mycomputer Platform: Windows-10-10.0.19041-SP0 Processor: Intel64 Family 6 Model 158 Stepping 10, GenuineIntel Python Build: (\'default\', \'Jun 26 2021 07:16:41\') Python Compiler: GCC 7.3.0 Python Implementation: CPython Python Version: 3.10.0 System Release: 10 System: Windows Libc Version: OS Release Info: ``` Use the provided documentation to implement the function effectively.","solution":"import platform def platform_report(): Generates a comprehensive report of the current system platform\'s information. report = [] report.append(f\\"Architecture: {platform.architecture()[0]}\\") report.append(f\\"Machine: {platform.machine()}\\") report.append(f\\"Node: {platform.node()}\\") report.append(f\\"Platform: {platform.platform()}\\") report.append(f\\"Processor: {platform.processor()}\\") report.append(f\\"Python Build: {platform.python_build()}\\") report.append(f\\"Python Compiler: {platform.python_compiler()}\\") report.append(f\\"Python Implementation: {platform.python_implementation()}\\") report.append(f\\"Python Version: {platform.python_version()}\\") report.append(f\\"System Release: {platform.release()}\\") report.append(f\\"System: {platform.system()}\\") libc_ver_info = platform.libc_ver() report.append(f\\"Libc Version: {\' \'.join(libc_ver_info) if any(libc_ver_info) else \'\'}\\") os_release_info = platform.freedesktop_os_release() if hasattr(platform, \'freedesktop_os_release\') else {} report.append(f\\"OS Release Info: {os_release_info}\\") return \\"n\\".join(report)"},{"question":"**Advanced Python Task: Context Management with `contextvars`** **Problem Statement:** You are to implement a Python module that manages user session data using the `contextvars` module. This module should allow storing, retrieving, and resetting user-specific session data in a thread-safe manner. This is particularly useful in a web application where different threads may handle different user sessions concurrently. **Function Specifications:** 1. **Function Name:** `initialize_session` - **Description:** Initializes a new session with a given session ID and associated data. - **Input:** - `session_id` (str): The unique identifier for the session. - `data` (dict): The initial data to be stored in the session. - **Output:** None 2. **Function Name:** `get_session_data` - **Description:** Retrieves the current data associated with the session of the given session ID. If the session does not exist, it should return `None`. - **Input:** - `session_id` (str): The unique identifier for the session. - **Output:** - `data` (dict): The data associated with the session or `None` if the session does not exist. 3. **Function Name:** `update_session_data` - **Description:** Updates the session data for the given session ID. If the session does not exist, it should create a new session with the given data. - **Input:** - `session_id` (str): The unique identifier for the session. - `data` (dict): The data to update the session with. - **Output:** None 4. **Function Name:** `reset_session_data` - **Description:** Resets the session data for the given session ID back to its initial state. If the session does not exist, it should do nothing. - **Input:** - `session_id` (str): The unique identifier for the session. - **Output:** None **Constraints:** - The module should be able to handle multiple sessions concurrently in a thread-safe manner. - You may use the `contextvars.ContextVar` class for managing session data. **Example:** ```python # Example usage of the module initialize_session(\\"session1\\", {\\"user\\": \\"Alice\\", \\"role\\": \\"admin\\"}) initialize_session(\\"session2\\", {\\"user\\": \\"Bob\\", \\"role\\": \\"user\\"}) # Retrieve session data data1 = get_session_data(\\"session1\\") # Outputs: {\\"user\\": \\"Alice\\", \\"role\\": \\"admin\\"} data2 = get_session_data(\\"session2\\") # Outputs: {\\"user\\": \\"Bob\\", \\"role\\": \\"user\\"} # Update session data update_session_data(\\"session1\\", {\\"role\\": \\"superadmin\\"}) data1 = get_session_data(\\"session1\\") # Outputs: {\\"user\\": \\"Alice\\", \\"role\\": \\"superadmin\\"} # Reset session data reset_session_data(\\"session1\\") data1 = get_session_data(\\"session1\\") # Outputs: {\\"user\\": \\"Alice\\", \\"role\\": \\"admin\\"} ``` Your solution should focus on correctly using `contextvars` to manage the session data in a thread-safe manner. Ensure that your implementation handles concurrent session updates properly. **Notes:** - It is recommended to look up the `contextvars` module in the Python standard library documentation to better understand its usage in Python.","solution":"import contextvars # Creating a context variable to store session data session_data = contextvars.ContextVar(\\"session_data\\", default={}) def initialize_session(session_id, data): Initializes a new session with given session ID and associated data current_data = session_data.get() current_data[session_id] = data session_data.set(current_data) def get_session_data(session_id): Retrieves the data associated with the given session ID or None if session does not exist current_data = session_data.get() return current_data.get(session_id) def update_session_data(session_id, data): Updates the session data for given session ID or creates a new session if it doesn\'t exist current_data = session_data.get() if session_id in current_data: current_data[session_id].update(data) else: current_data[session_id] = data session_data.set(current_data) def reset_session_data(session_id): Resets the session data for the given session ID back to its initial state if it exists current_data = session_data.get() if session_id in current_data: current_data[session_id] = initial_session_data[session_id].copy() session_data.set(current_data) # Keeping track of the initial data to use in reset session initial_session_data = {} def initialize_session_track_initial(session_id, data): Initializes a new session tracking the initial data initial_session_data[session_id] = data.copy() initialize_session(session_id, data)"},{"question":"Implementing Custom Pairwise Metrics and Kernels with Scikit-learn **Objective:** This task aims to assess your comprehension of pairwise metrics and kernel functions as provided by the `scikit-learn` package. You will implement custom metrics and kernels and then utilize them in a small machine learning task. **Part 1: Implement Custom Metrics and Kernels** 1. **Manhattan Distance Metric:** Implement the Manhattan distance function for two vectors `a` and `b`. ```python def manhattan_distance(a, b): Compute the Manhattan distance between two vectors. Parameters: a (ndarray): 1D array, the first vector. b (ndarray): 1D array, the second vector. Returns: float: The Manhattan distance between vectors a and b. pass ``` 2. **Gaussian (RBF) Kernel:** Implement the Gaussian (RBF) kernel function for two vectors `x` and `y`. ```python def rbf_kernel(x, y, gamma): Compute the Gaussian (RBF) kernel between two vectors. Parameters: x (ndarray): 1D array, the first vector. y (ndarray): 1D array, the second vector. gamma (float): The kernel coefficient. Returns: float: The Gaussian (RBF) kernel between vectors x and y. pass ``` **Part 2: Machine Learning Task** 3. **Classification Using Custom Kernels:** Use your custom `rbf_kernel` in an SVM to classify the following small dataset. Your task is to implement the classification pipeline. ```python from sklearn.svm import SVC import numpy as np # Sample data X_train = np.array([[0, 1], [1, 0], [0.2, 0.8], [0.7, 0.3]]) y_train = np.array([0, 1, 0, 1]) X_test = np.array([[0.1, 0.9], [0.8, 0.2]]) # Define custom RBF kernel function for SVC def custom_rbf_kernel(X, Y, gamma): Compute the RBF kernel between matrices X and Y using custom implementation. Parameters: X (ndarray): 2D array, shape (n_samples_X, n_features) Y (ndarray): 2D array, shape (n_samples_Y, n_features) gamma (float): The RBF kernel coefficient. Returns: ndarray: 2D array, shape (n_samples_X, n_samples_Y) pairwise_kernel_matrix = np.zeros((X.shape[0], Y.shape[0])) for i, x in enumerate(X): for j, y in enumerate(Y): pairwise_kernel_matrix[i, j] = rbf_kernel(x, y, gamma) return pairwise_kernel_matrix # Train SVM with custom kernel gamma = 0.5 # Define the gamma value for RBF kernel svc = SVC(kernel=lambda X, Y: custom_rbf_kernel(X, Y, gamma)) svc.fit(X_train, y_train) # Predicting on the test data predictions = svc.predict(X_test) print(predictions) ``` **Requirements:** - **Performance:** Ensure that your implementations are efficient and can handle large inputs reasonably well. - **Correctness:** Correct implementations of `manhattan_distance`, `rbf_kernel`, and the SVM classification task. - **Clarity:** Write clear and understandable code with appropriate comments and documentation. **Inputs:** - Vectors `a` and `b` for `manhattan_distance`, and vectors `x` and `y` for `rbf_kernel` as 1D numpy arrays. - Training data `X_train`, `y_train` and test data `X_test` as numpy arrays. - `gamma` as a float for the RBF kernel. **Outputs:** - A float value from `manhattan_distance`. - A float value from `rbf_kernel`. - An array of predictions from the SVM classifier using the custom RBF kernel. **Constraints:** - Ensure vectors are non-empty and of the same dimensions. - The `gamma` value should be a positive float. **Evaluation:** - The correctness of the implemented functions. - The successful execution of the entire classification pipeline. - Code readability and proper use of scikit-learn\'s SVM combined with custom kernel implementation. **Note:** Do not use the built-in functions of `scikit-learn` for distance metrics and kernel computations while implementing `manhattan_distance` and `rbf_kernel`. Ensure you provide sufficient test cases and comments to validate your implementation.","solution":"import numpy as np def manhattan_distance(a, b): Compute the Manhattan distance between two vectors. Parameters: a (np.ndarray): 1D array, the first vector. b (np.ndarray): 1D array, the second vector. Returns: float: The Manhattan distance between vectors a and b. return np.sum(np.abs(a - b)) def rbf_kernel(x, y, gamma): Compute the Gaussian (RBF) kernel between two vectors. Parameters: x (np.ndarray): 1D array, the first vector. y (np.ndarray): 1D array, the second vector. gamma (float): The kernel coefficient. Returns: float: The Gaussian (RBF) kernel between vectors x and y. return np.exp(-gamma * np.sum((x - y) ** 2)) from sklearn.svm import SVC # Sample data X_train = np.array([[0, 1], [1, 0], [0.2, 0.8], [0.7, 0.3]]) y_train = np.array([0, 1, 0, 1]) X_test = np.array([[0.1, 0.9], [0.8, 0.2]]) # Define custom RBF kernel function for SVC def custom_rbf_kernel(X, Y, gamma): pairwise_kernel_matrix = np.zeros((X.shape[0], Y.shape[0])) for i, x in enumerate(X): for j, y in enumerate(Y): pairwise_kernel_matrix[i, j] = rbf_kernel(x, y, gamma) return pairwise_kernel_matrix # Train SVM with custom kernel gamma = 0.5 # Define the gamma value for RBF kernel svc = SVC(kernel=lambda X, Y: custom_rbf_kernel(X, Y, gamma)) svc.fit(X_train, y_train) # Predicting on the test data predictions = svc.predict(X_test) print(predictions)"},{"question":"**Coding Assessment Question** # Objective Create a Python module that demonstrates your understanding of the `doctest` module. The module should contain functions with docstrings that provide examples of expected behavior. These examples will be used by `doctest` to validate the correctness of your implementation. # Instructions 1. **Create a Python module named `mymodule.py`** that includes the following functions: - `fibonacci(n)`: Computes the nth Fibonacci number. - `is_prime(n)`: Checks if a number n is prime. - `greet(name)`: Prints a greeting message. 2. **Include docstrings** for each function. Each docstring should contain: - A brief description of the function. - Example usage of the function using Python\'s interactive shell format (`>>>`). - Expected output for each example. 3. **Write examples in docstrings** that cover the following scenarios: - Normal execution returning expected results. - Handling of invalid inputs or edge cases (e.g., negative numbers for Fibonacci). - Using at least one option flag such as `ELLIPSIS` or `IGNORE_EXCEPTION_DETAIL`. # Example Structure Here is a template to get you started. Fill in the implementation and examples accordingly: ```python # File: mymodule.py def fibonacci(n): Computes the nth Fibonacci number. >>> fibonacci(5) 5 >>> fibonacci(10) 55 >>> fibonacci(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 if n < 0: raise ValueError(\\"n must be >= 0\\") a, b = 0, 1 for _ in range(n): a, b = b, a + b return a def is_prime(n): Checks if a number n is prime. >>> is_prime(5) True >>> is_prime(4) False >>> is_prime(1) False >>> is_prime(2) True >>> is_prime(15) False if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def greet(name): Prints a greeting message. >>> greet(\\"Alice\\") Hello, Alice! >>> greet(\\"Bob\\") # doctest: +ELLIPSIS Hello, B... print(f\\"Hello, {name}!\\") if __name__ == \\"__main__\\": import doctest doctest.testmod() ``` # Submission Submit your `mymodule.py` file with the implemented functions and their corresponding docstring examples. # Constraints - Use only standard Python libraries. - Ensure the examples illustrate both success and failure cases. - Your examples should be comprehensive enough to test your functions thoroughly. # Evaluation Your solution will be evaluated based on: - Correct implementation of the functions. - Accuracy and coverage of the docstring examples. - Successful execution of `doctest` without any errors.","solution":"# File: mymodule.py def fibonacci(n): Computes the nth Fibonacci number. >>> fibonacci(0) 0 >>> fibonacci(1) 1 >>> fibonacci(5) 5 >>> fibonacci(10) 55 >>> fibonacci(-1) Traceback (most recent call last): ... ValueError: n must be >= 0 if n < 0: raise ValueError(\\"n must be >= 0\\") a, b = 0, 1 for _ in range(n): a, b = b, a + b return a def is_prime(n): Checks if a number n is prime. >>> is_prime(2) True >>> is_prime(5) True >>> is_prime(4) False >>> is_prime(1) False >>> is_prime(11) True >>> is_prime(-3) False if n <= 1: return False for i in range(2, int(n**0.5) + 1): if n % i == 0: return False return True def greet(name): Prints a greeting message. >>> greet(\\"Alice\\") Hello, Alice! >>> greet(\\"Bob\\") # doctest: +ELLIPSIS Hello, B... >>> greet(\\"\\") Hello, ! print(f\\"Hello, {name}!\\") if __name__ == \\"__main__\\": import doctest doctest.testmod()"},{"question":"You are tasked with implementing a function that encodes and decodes mixed formats of binary data using the `base64` module. The function should take a list of binary data chunks and a series of operations to be applied in sequence to each chunk. The operations can include encoding and decoding in various formats such as Base64, Base32, Base16, and Base85. This function should be capable of chaining multiple operations for each data chunk. Implement the function `transform_data(data_chunks: List[bytes], operations: List[List[Tuple[str, Union[str, bool, int]]]]) -> List[bytes]` where: - `data_chunks`: A list of binary data chunks (`bytes`). - `operations`: A list of lists of operations. Each sublist corresponds to a data chunk from `data_chunks` and contains a series of tuples denoting the operations to be applied sequentially. Each tuple contains the operation name (as a string) and any parameters required for that operation. The operation names and their parameters are: - `\\"b64encode\\"`: No parameters. - `\\"b64decode\\"`: No parameters. - `\\"standard_b64encode\\"`: No parameters. - `\\"standard_b64decode\\"`: No parameters. - `\\"urlsafe_b64encode\\"`: No parameters. - `\\"urlsafe_b64decode\\"`: No parameters. - `\\"b32encode\\"`: No parameters. - `\\"b32decode\\"`: A boolean parameter `casefold` and an optional parameter `map01` (for digit mapping). - `\\"b16encode\\"`: No parameters. - `\\"b16decode\\"`: A boolean parameter `casefold`. - `\\"a85encode\\"`: Boolean parameters `foldspaces`, `pad`, and `adobe`, and an integer parameter `wrapcol`. - `\\"a85decode\\"`: Boolean parameters `foldspaces` and `adobe`, and a parameter `ignorechars` containing characters to ignore. - `\\"b85encode\\"`: A boolean parameter `pad`. - `\\"b85decode\\"`: No parameters. # Example ```python from typing import List, Tuple, Union import base64 def transform_data(data_chunks: List[bytes], operations: List[List[Tuple[str, Union[str, bool, int]]]]) -> List[bytes]: result = [] for data, ops in zip(data_chunks, operations): for op in ops: operation = op[0] if operation == \\"b64encode\\": data = base64.b64encode(data) elif operation == \\"b64decode\\": data = base64.b64decode(data) elif operation == \\"standard_b64encode\\": data = base64.standard_b64encode(data) elif operation == \\"standard_b64decode\\": data = base64.standard_b64decode(data) elif operation == \\"urlsafe_b64encode\\": data = base64.urlsafe_b64encode(data) elif operation == \\"urlsafe_b64decode\\": data = base64.urlsafe_b64decode(data) elif operation == \\"b32encode\\": data = base64.b32encode(data) elif operation == \\"b32decode\\": data = base64.b32decode(data, casefold=op[1], map01=op[2] if len(op) > 2 else None) elif operation == \\"b16encode\\": data = base64.b16encode(data) elif operation == \\"b16decode\\": data = base64.b16decode(data, casefold=op[1]) elif operation == \\"a85encode\\": data = base64.a85encode(data, foldspaces=op[1], wrapcol=op[2], pad=op[3], adobe=op[4]) elif operation == \\"a85decode\\": data = base64.a85decode(data, foldspaces=op[1], adobe=op[2], ignorechars=op[3]) elif operation == \\"b85encode\\": data = base64.b85encode(data, pad=op[1]) elif operation == \\"b85decode\\": data = base64.b85decode(data) else: raise ValueError(\\"Unsupported operation\\") result.append(data) return result # Example Usage data_chunks = [b\\"hello\\", b\\"world\\"] operations = [ [(\\"b64encode\\",), (\\"b64decode\\",)], [(\\"b32encode\\",), (\\"b32decode\\", False)] ] print(transform_data(data_chunks, operations)) # Output: [b\'hello\', b\'world\'] ``` # Constraints 1. The length of `data_chunks` and `operations` will be the same. 2. Each chunk of data and the subsequent operations are independent of other chunks. 3. Any invalid operation should raise a `ValueError` with the message \\"Unsupported operation\\". 4. You can assume all encoding and decoding operations are valid, given the parameters correctly align with the operation requirements.","solution":"from typing import List, Tuple, Union import base64 def transform_data(data_chunks: List[bytes], operations: List[List[Tuple[str, Union[str, bool, int]]]]) -> List[bytes]: Apply a series of encoding/decoding operations to a list of binary data chunks. Parameters: - data_chunks: List[bytes] - A list of binary data chunks. - operations: List[List[Tuple[str, Union[str, bool, int]]]] - A list of lists of operations. Returns: - List[bytes] - A list of transformed binary data. result = [] for data, ops in zip(data_chunks, operations): for op in ops: operation = op[0] if operation == \\"b64encode\\": data = base64.b64encode(data) elif operation == \\"b64decode\\": data = base64.b64decode(data) elif operation == \\"standard_b64encode\\": data = base64.standard_b64encode(data) elif operation == \\"standard_b64decode\\": data = base64.standard_b64decode(data) elif operation == \\"urlsafe_b64encode\\": data = base64.urlsafe_b64encode(data) elif operation == \\"urlsafe_b64decode\\": data = base64.urlsafe_b64decode(data) elif operation == \\"b32encode\\": data = base64.b32encode(data) elif operation == \\"b32decode\\": data = base64.b32decode(data, casefold=op[1], map01=op[2] if len(op) > 2 else None) elif operation == \\"b16encode\\": data = base64.b16encode(data) elif operation == \\"b16decode\\": data = base64.b16decode(data, casefold=op[1]) elif operation == \\"a85encode\\": data = base64.a85encode(data, foldspaces=op[1], wrapcol=op[2], pad=op[3], adobe=op[4]) elif operation == \\"a85decode\\": data = base64.a85decode(data, foldspaces=op[1], adobe=op[2], ignorechars=op[3]) elif operation == \\"b85encode\\": data = base64.b85encode(data, pad=op[1]) elif operation == \\"b85decode\\": data = base64.b85decode(data) else: raise ValueError(\\"Unsupported operation\\") result.append(data) return result"},{"question":"**Objective:** Implement a function `create_code_object` that demonstrates the creation and basic usage of `PyCodeObject`. Your task is to use the `PyCode_NewEmpty` function to create an empty code object and write utility functions leveraging other provided methods to manipulate and extract information from this code object. # Problem Description: Implement the following functions: 1. `create_code_object(filename: str, funcname: str, firstlineno: int) -> PyCodeObject`: - This function should create a new empty code object using `PyCode_NewEmpty`. 2. `is_code_object(py_object) -> bool`: - This function should determine if `py_object` is a code object using `PyCode_Check`. 3. `get_num_free_vars(code_object: PyCodeObject) -> int`: - This function should return the number of free variables in the given code object using `PyCode_GetNumFree`. 4. `addr_to_line_number(code_object: PyCodeObject, byte_offset: int) -> int`: - This function should return the line number corresponding to the given byte offset in the code object using `PyCode_Addr2Line`. # Function Signatures: ```python def create_code_object(filename: str, funcname: str, firstlineno: int) -> PyCodeObject: pass def is_code_object(py_object) -> bool: pass def get_num_free_vars(code_object: PyCodeObject) -> int: pass def addr_to_line_number(code_object: PyCodeObject, byte_offset: int) -> int: pass ``` # Constraints: - `filename` and `funcname` are strings without special characters. - `firstlineno` is a positive integer. - `byte_offset` is a non-negative integer. # Example Usage: ```python code_obj = create_code_object(\'example.py\', \'my_function\', 1) print(is_code_object(code_obj)) # Expected output: True print(get_num_free_vars(code_obj)) # Expected output: 0 print(addr_to_line_number(code_obj, 0)) # Expected output: First line number of the function ``` This problem ensures that students understand and can work with low-level code representation in the CPython implementation, demonstrating their comprehension of advanced Python concepts.","solution":"import types def create_code_object(filename: str, funcname: str, firstlineno: int) -> types.CodeType: Create a new empty code object. return types.CodeType( 0, # argument count 0, # positional only argument count 0, # keyword only argument count 0, # local variables count 0, # stack size 0, # flags b\'\', # bytecode instructions (), # constants (), # names (), # varnames filename, # filename funcname, # name firstlineno, # first line number b\'\' # lnotab ) def is_code_object(py_object) -> bool: Determines if py_object is a code object. return isinstance(py_object, types.CodeType) def get_num_free_vars(code_object: types.CodeType) -> int: Return the number of free variables in the given code object. return code_object.co_freevars def addr_to_line_number(code_object: types.CodeType, byte_offset: int) -> int: Return the line number corresponding to the given byte offset in the code object. return code_object.co_firstlineno"},{"question":"Partial Least Squares Regression Implementation You are required to implement a Partial Least Squares (PLS) regression model using scikit-learn\'s `PLSRegression` class. The model should be trained on a given dataset and evaluated on its ability to predict target values. Dataset You will work with a synthetic dataset containing feature matrix `X` and target matrix `Y`. The matrices will be sufficiently large to challenge your implementation skills. - `X`: An `n_samples x n_features` matrix of predictors. - `Y`: An `n_samples x n_targets` matrix of targets. Task 1. Implement a function `train_pls(X, Y, n_components)` that trains a PLS regression model: - **Inputs:** - `X` (numpy.ndarray): The feature matrix with shape `(n_samples, n_features)`. - `Y` (numpy.ndarray): The target matrix with shape `(n_samples, n_targets)`. - `n_components` (int): The number of components to keep. - **Output:** - A trained PLS regression model. 2. Implement a function `predict_pls(model, X_test)` that uses the trained PLS model to make predictions on a test dataset: - **Inputs:** - `model`: The trained PLS regression model. - `X_test` (numpy.ndarray): The test feature matrix with shape `(n_samples_test, n_features)`. - **Output:** - `Y_pred` (numpy.ndarray): The predicted target values with shape `(n_samples_test, n_targets)`. 3. Implement a function `evaluate_model(Y_true, Y_pred)` that calculates and returns the Mean Squared Error (MSE) between the true and predicted target values: - **Inputs:** - `Y_true` (numpy.ndarray): The true target values with shape `(n_samples_test, n_targets)`. - `Y_pred` (numpy.ndarray): The predicted target values with shape `(n_samples_test, n_targets)`. - **Output:** - `mse` (float): The mean squared error between `Y_true` and `Y_pred`. Constraints and Clarifications: - You must use scikit-learn\'s `PLSRegression` class to implement the PLS model. - Ensure your code is efficient and handles large input sizes appropriately. - Document your code with comments explaining the purpose of each major step. - You may use numpy for any matrix operations. Performance Requirements: Your solution will be evaluated on: - Correctness of the implementation. - Computational efficiency. - Code readability and documentation. **Example Usage:** ```python import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error # Generate synthetic data np.random.seed(0) X = np.random.randn(100, 10) Y = np.random.randn(100, 5) # Split data into training and testing sets X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42) # Train the PLS regression model n_components = 3 pls_model = train_pls(X_train, Y_train, n_components) # Predict on the test set Y_pred = predict_pls(pls_model, X_test) # Evaluate the model mse = evaluate_model(Y_test, Y_pred) print(f\\"Mean Squared Error: {mse}\\") ``` Submit your implementation of the `train_pls`, `predict_pls`, and `evaluate_model` functions.","solution":"from sklearn.cross_decomposition import PLSRegression import numpy as np from sklearn.metrics import mean_squared_error def train_pls(X, Y, n_components): Train a PLS regression model. Parameters: X (numpy.ndarray): The feature matrix with shape (n_samples, n_features). Y (numpy.ndarray): The target matrix with shape (n_samples, n_targets). n_components (int): The number of components to keep. Returns: model: A trained PLS regression model. pls_model = PLSRegression(n_components=n_components) pls_model.fit(X, Y) return pls_model def predict_pls(model, X_test): Use the trained PLS model to make predictions on a test dataset. Parameters: model: The trained PLS regression model. X_test (numpy.ndarray): The test feature matrix with shape (n_samples_test, n_features). Returns: numpy.ndarray: The predicted target values with shape (n_samples_test, n_targets). return model.predict(X_test) def evaluate_model(Y_true, Y_pred): Calculate and return the Mean Squared Error (MSE) between the true and predicted target values. Parameters: Y_true (numpy.ndarray): The true target values with shape (n_samples_test, n_targets). Y_pred (numpy.ndarray): The predicted target values with shape (n_samples_test, n_targets). Returns: float: The mean squared error between Y_true and Y_pred. mse = mean_squared_error(Y_true, Y_pred) return mse"},{"question":"# Advanced Type Hinting with GenericAlias You are tasked with implementing a set of Python functions and classes that make use of advanced type hinting features using the `GenericAlias` concept. Your goal is to implement a basic data processing pipeline that takes advantage of these type hints to ensure type safety and clarity. Problem Statement: 1. Implement a generic `Cache` class that can store values of any type. This class should use type hinting to ensure the types are consistent. 2. Implement a function `process_data(data: list, cache: Cache)` which takes a list of data and a `Cache` object, processes the data (by doubling each element), and stores the processed data in the cache. Constraints: - The `Cache` class should allow storing values of types such as `int`, `str`, etc. - The `Cache` class should use `GenericAlias` for type hinting. - The `process_data` function should leverage the `Cache` to store the processed list. Class and Function Specification: 1. **Cache Class**: - `__origin__`: The original type of the data that the cache will store. - `__args__`: A tuple containing a single element which is the type of the data the cache will store. - Methods: - `__init__(self)`: Initializes an empty cache. - `store(self, data: __origin__)`: Stores the data in the cache. - `retrieve(self) -> __origin__`: Retrieves the data from the cache. 2. **process_data Function**: - `process_data(data: list, cache: Cache) -> None`: Processes the data list by doubling each element and stores it into the `Cache`. Example Usage: ```python from typing import Generic, TypeVar T = TypeVar(\'T\') class Cache(Generic[T]): def __init__(self): self._data = None def store(self, data: T) -> None: self._data = data def retrieve(self) -> T: return self._data def process_data(data: list, cache: Cache) -> None: processed_data = [x * 2 for x in data] cache.store(processed_data) # Example usage cache = Cache[list]() data = [1, 2, 3, 4] process_data(data, cache) print(cache.retrieve()) # Output should be [2, 4, 6, 8] ``` Notes: - You may assume the input `list` contains integers only for simplicity. - Ensure to use Python 3.9 compatible syntax and the `GenericAlias` concept appropriately.","solution":"from typing import Generic, TypeVar T = TypeVar(\'T\') class Cache(Generic[T]): def __init__(self): # storage for the cached data self._data = None def store(self, data: T) -> None: Stores the data in the cache self._data = data def retrieve(self) -> T: Retrieves the stored data from the cache return self._data def process_data(data: list[int], cache: Cache[list[int]]) -> None: Processes the data by doubling each element and stores it in the given cache. processed_data = [x * 2 for x in data] cache.store(processed_data)"},{"question":"<|Analysis Begin|> Looking at the provided documentation, several Python built-in functions and types have been explored. These functions cover a wide range of functionalities, from simple type conversions and arithmetic operations to more complex operations like compiling code or manipulating sequences. Notably, several advanced methods involve asynchronous operations, error handling, and context management. The documentation features: - Basic conversion functions (`int()`, `float()`, `str()`, etc.) - Collection and sequence manipulation functions (`list()`, `dict()`, `set()`, `tuple()`, etc.) - High-level functions useful for functional programming (`filter()`, `map()`, `enumerate()`, etc.) - I/O operations (`open()`, `input()`, `print()`) - Debugging and compiling (`breakpoint()`, `compile()`, `exec()`, `eval()`) From this documentation, numerous aspects could be evaluated, such as asynchronous iteration, comparison operations, utilizing decorators (`@classmethod`, `@staticmethod`, `@property`), and other fundamental concepts. An assessment question should focus on utilizing multiple built-in functions to solve a practical problem, which would demonstrate the student\'s understanding of the function, control flow, and best practices in Python. <|Analysis End|> <|Question Begin|> # Coding Assessment: Aggregating and Analyzing Sensor Data **Context:** You are developing a Python application to analyze data from a network of environmental sensors. Each sensor records time-stamped temperature readings. This data is collected periodically and stored in a list of readings. Each reading is represented as a dictionary with two keys: `\\"timestamp\\"` and `\\"temperature\\"`. For example: ```python readings = [ {\\"timestamp\\": \\"2023-10-01T00:00:00Z\\", \\"temperature\\": 22.5}, {\\"timestamp\\": \\"2023-10-01T01:00:00Z\\", \\"temperature\\": 23.0}, {\\"timestamp\\": \\"2023-10-01T02:00:00Z\\", \\"temperature\\": 21.8}, ... ] ``` # Task: Write a function `aggregate_sensor_data(readings, start, end)` that: 1. Filters the readings to include only those within the given start and end timestamps (inclusive). 2. Calculates the average temperature of the filtered readings. 3. Returns a summary as a dictionary containing: - `\\"start\\"`: The starting timestamp. - `\\"end\\"`: The ending timestamp. - `\\"average_temperature\\"`: The average temperature of the filtered readings. # Requirements: - Use `filter()` to filter the readings within the specified time range. - Use `sum()` and `len()` to calculate the average temperature. - Use `map()` or a generator expression where appropriate. - Handle cases where no readings are within the specified range by returning an appropriate message. # Example: ```python readings = [ {\\"timestamp\\": \\"2023-10-01T00:00:00Z\\", \\"temperature\\": 22.5}, {\\"timestamp\\": \\"2023-10-01T01:00:00Z\\", \\"temperature\\": 23.0}, {\\"timestamp\\": \\"2023-10-01T02:00:00Z\\", \\"temperature\\": 21.8}, ] print(aggregate_sensor_data(readings, \\"2023-10-01T00:00:00Z\\", \\"2023-10-01T02:00:00Z\\")) # Expected output: {\\"start\\": \\"2023-10-01T00:00:00Z\\", \\"end\\": \\"2023-10-01T02:00:00Z\\", \\"average_temperature\\": 22.433333333333334} print(aggregate_sensor_data(readings, \\"2023-10-01T03:00:00Z\\", \\"2023-10-01T04:00:00Z\\")) # Expected output: {\\"message\\": \\"No readings in the specified range.\\"} ``` # Constraints: - `readings` is a list with at most 1000 elements. - `timestamp` is in the ISO 8601 format: `YYYY-MM-DDTHH:MM:SSZ`. - `temperature` is a float. # Notes: - The function should be efficient with time complexity ideally in O(n), where n is the number of readings. - Consider ways to handle edge cases, such as an empty readings list or no readings within the specified range.","solution":"def aggregate_sensor_data(readings, start, end): Aggregates sensor data to calculate the average temperature within a specific time range. :param readings: List of readings, where each reading is a dictionary with \'timestamp\' and \'temperature\'. :param start: Starting timestamp (inclusive) in ISO 8601 format. :param end: Ending timestamp (inclusive) in ISO 8601 format. :return: A dictionary with the start, end, and average temperature within the specified range, or a message if no readings are found. # Filter the readings within the specified time range filtered_readings = list(filter(lambda x: start <= x[\\"timestamp\\"] <= end, readings)) if not filtered_readings: return {\\"message\\": \\"No readings in the specified range.\\"} # Compute the sum of temperatures total_temperature = sum(reading[\\"temperature\\"] for reading in filtered_readings) # Compute the average temperature average_temperature = total_temperature / len(filtered_readings) return { \\"start\\": start, \\"end\\": end, \\"average_temperature\\": average_temperature }"},{"question":"Objective Your task is to demonstrate your understanding of shallow and deep copy operations by implementing a custom class that supports both types of copying. You will also need to handle potential recursion issues with deep copy. Problem Statement 1. Implement a class `TreeNode` representing a node in a binary tree. Each `TreeNode` has: - An integer value `val`. - A reference to the left child (another `TreeNode` object) or `None`. - A reference to the right child (another `TreeNode` object) or `None`. 2. Ensure that your `TreeNode` class supports shallow copying and deep copying through the `copy` module by implementing the `__copy__()` and `__deepcopy__()` methods. 3. Write a function `create_sample_tree()` that returns a sample binary tree as an instance of `TreeNode`. 4. Write test functions to: - Create a shallow copy of the sample binary tree and modify the value of a node in the original tree, verifying that the change is reflected in the shallow copy. - Create a deep copy of the sample binary tree and modify the value of a node in the original tree, verifying that the change is not reflected in the deep copy. Constraints - The `TreeNode` values are non-negative integers. - The sample tree created for testing should contain at least three levels of nodes. - Function `create_sample_tree()` should return a tree with the following structure: ``` 1 / 2 3 /| 4 5 6 ``` Input and Output - `create_sample_tree()`: - **Input**: None - **Output**: `TreeNode` object representing the root of the binary tree. - Test functions do not require input parameters and should produce console output verifying the expected behavior during shallow and deep copies. Implementation Implement the class and functions in a Python script. Below is the skeleton of your implementation: ```python import copy class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right def __copy__(self): new_node = TreeNode(self.val) new_node.left = self.left new_node.right = self.right return new_node def __deepcopy__(self, memo): if self in memo: return memo[self] new_node = TreeNode(self.val) memo[self] = new_node new_node.left = copy.deepcopy(self.left, memo) new_node.right = copy.deepcopy(self.right, memo) return new_node def create_sample_tree(): n6 = TreeNode(6) n5 = TreeNode(5) n4 = TreeNode(4) n2 = TreeNode(2, n4, n5) n3 = TreeNode(3, None, n6) root = TreeNode(1, n2, n3) return root def test_shallow_copy(): root = create_sample_tree() root_copy = copy.copy(root) root.val = 99 assert root.val == root_copy.val, \\"Shallow copy failed\\" print(\\"Shallow copy test passed.\\") def test_deep_copy(): root = create_sample_tree() root_copy = copy.deepcopy(root) root.val = 99 assert root.val != root_copy.val, \\"Deep copy failed\\" print(\\"Deep copy test passed.\\") if __name__ == \\"__main__\\": test_shallow_copy() test_deep_copy() ``` # Evaluation Criteria - Correct implementation of the `__copy__()` and `__deepcopy__()` methods in the `TreeNode` class. - Correct creation of a sample binary tree in `create_sample_tree()`. - Accurate and comprehensive test cases in the test functions to verify correct behavior of shallow and deep copying. - Clear and robust code with appropriate comments.","solution":"import copy class TreeNode: def __init__(self, val=0, left=None, right=None): self.val = val self.left = left self.right = right def __copy__(self): new_node = TreeNode(self.val) new_node.left = self.left new_node.right = self.right return new_node def __deepcopy__(self, memo): if self in memo: return memo[self] new_node = TreeNode(self.val) memo[self] = new_node new_node.left = copy.deepcopy(self.left, memo) new_node.right = copy.deepcopy(self.right, memo) return new_node def create_sample_tree(): n6 = TreeNode(6) n5 = TreeNode(5) n4 = TreeNode(4) n2 = TreeNode(2, n4, n5) n3 = TreeNode(3, None, n6) root = TreeNode(1, n2, n3) return root"},{"question":"Concept: Context Managers and Contextlib Utilities in Python Problem Statement You are required to implement a custom context manager using Python\'s `contextlib` module to manage database transactions. Your context manager should handle the following functionalities: 1. **Support a variable number of context managers:** It should gracefully handle one or more nested context managers. 2. **Catch exceptions:** It should handle any exceptions raised within the `__enter__` method, execute necessary cleanup, and re-raise the exception. 3. **Cleanup resources**: Always ensure resources are cleaned up even in cases of failure within the `__enter__` block or during normal operation. Steps to Follow 1. Create a class `DatabaseTransaction` which will be used as a context manager: - Implement the `__enter__` method to begin a transaction. - Implement the `__exit__` method to commit the transaction if there is no exception, or roll back if an exception occurs. - Ensure that exceptions are logged (print the exception) before being re-raised. 2. Implement another function named `manage_transactions` that takes a variable number of context managers and demonstrates their nested behavior. Expected Input and Output - **Input:** - Simulate the database transaction within the context managers. - Optional: raise an exception within the nested context to test the exception handling. - **Output:** - Print statements within the context manager methods to indicate: - When a transaction begins. - When a transaction will be committed. - When a transaction will be rolled back. - Any exceptions encountered. - **Example Usage:** ```python with manage_transactions(DatabaseTransaction(), DatabaseTransaction()) as managers: # Simulating some operations print(\\"Performing database operations...\\") # Optional: Uncomment to simulate an error # raise ValueError(\\"An error occurred\\") ``` The output should reflect the order of operations, exception handling, resource cleanup, and proper nested context manager handling. Constraints - Use the `contextlib` module and follow the best practices around context managers. - Handling exceptions properly and ensuring resources are always cleaned up. Performance Notes - Ensure that the context manager handles entering and exiting quickly to simulate efficient transaction management. Submission Guidelines - Submit your implementation as a Python script or a Jupyter notebook. - Include comments and docstrings where necessary to explain your code.","solution":"from contextlib import contextmanager class DatabaseTransaction: def __enter__(self): print(\\"Transaction begins.\\") return self def __exit__(self, exc_type, exc_val, exc_tb): if exc_type is not None: print(f\\"Exception encountered: {exc_val}. Rolling back transaction.\\") return False # Re-raises the exception else: print(\\"Transaction committed.\\") return True @contextmanager def manage_transactions(*contexts): exits = [] try: for context in contexts: exits.append(context.__enter__()) yield contexts except Exception as e: for context in reversed(exits): context.__exit__(type(e), e, e.__traceback__) raise else: for context in reversed(exits): context.__exit__(None, None, None)"},{"question":"# XML Document Processing with Partial DOM Trees You are tasked with processing an XML document using the `xml.dom.pulldom` module. Your goal is to find all `<book>` elements within the XML, expand them into complete DOM nodes, and then return a list of these nodes\' XML representations if the book\'s `price` attribute is less than or equal to `20`. Input - An XML string, `xml_data`, which contains multiple `<book>` elements, each potentially with a `price` attribute. Output - A list of strings, where each string is the XML representation of a `<book>` element whose price is `20` or less. Example ```python xml_data = <library> <book price=\\"18\\"> <title>Effective Python</title> <author>Brett Slatkin</author> </book> <book price=\\"25\\"> <title>Fluent Python</title> <author>Luciano Ramalho</author> </book> <book price=\\"15\\"> <title>Python Tricks</title> <author>Dan Bader</author> </book> </library> expected_output = [ \'<book price=\\"18\\">n <title>Effective Python</title>n <author>Brett Slatkin</author>n </book>\', \'<book price=\\"15\\">n <title>Python Tricks</title>n <author>Dan Bader</author>n </book>\' ] # Your Function should return expected_output when passed the sample xml_data ``` Function Signature ```python def get_affordable_books(xml_data: str) -> List[str]: pass ``` Constraints - You must use the `xml.dom.pulldom` module for parsing and processing XML. - The XML input will be a well-formed and valid string. - The price attribute should always be an integer. - Consider handling the interim `event` and `node` pairs efficiently to avoid performance bottlenecks. Implementation Requirements 1. Parse the input `xml_data` using `xml.dom.pulldom.parseString`. 2. Iterate through the events and examine nodes until all relevant nodes are processed. 3. Expand nodes where necessary to get full child element details. 4. Collect and return the XML representations of qualifying `<book>` elements in a list. Good luck!","solution":"from typing import List from xml.dom import pulldom from xml.dom.minidom import Document, Node, parseString def get_affordable_books(xml_data: str) -> List[str]: # Parse the XML and initialize the iterator doc = pulldom.parseString(xml_data) affordable_books = [] # Iterate through the events and nodes for event, node in doc: if event == pulldom.START_ELEMENT and node.tagName == \\"book\\": # Expand the book node to get access to its children doc.expandNode(node) price = int(node.getAttribute(\\"price\\")) # Get the price attribute if price <= 20: affordable_books.append(node.toxml()) # Add XML string of the book node return affordable_books"},{"question":"# Secure Hashing with `hashlib` In this task, you will develop a function that demonstrates proficiency in using the `hashlib` module to create secure hashes and verify them. Your function will handle user passwords, ensuring they are securely hashed before storage and can be verified efficiently during login attempts. **Requirements:** 1. **Password Hashing**: - Hash the provided password using a hashing algorithm from the `hashlib` module. - Use a salt to enhance security. The salt should be unique for each password. - Store the salt and the hashed password in a format that can be easily used for verification. 2. **Password Verification**: - Verify if a given password matches the previously hashed password using the salt stored with the hashed password. **Function Specifications:** 1. `hash_password(password: str) -> str`: - **Input**: A string `password` which is the password to be hashed. - **Output**: A string in the format `salt:hashed_password` where `salt` is the salt used and `hashed_password` is the hashed password. 2. `verify_password(stored_password: str, provided_password: str) -> bool`: - **Input**: A string `stored_password` which is the stored salt and hashed password in the format `salt:hashed_password`, and a string `provided_password` which is the password provided for verification. - **Output**: A boolean value `True` if the provided password matches the stored password, else `False`. **Constraints:** - Use a secure hash function from the `hashlib` module such as SHA-256. - Ensure salts are of appropriate length (at least 16 bytes) and securely generated. - The implementation should be efficient and secure against common attacks. Here\'s an example of how the functions should work: ```python hashed = hash_password(\'MySecurePassword\') assert verify_password(hashed, \'MySecurePassword\') == True assert verify_password(hashed, \'WrongPassword\') == False ``` **Performance Requirements**: - The hashing and verification functions should have a time complexity of O(N) where N is the length of the password. Implement these functions to demonstrate your understanding of secure hashing and password verification using the `hashlib` module in Python.","solution":"import hashlib import os def hash_password(password: str) -> str: Hash the provided password using SHA-256 along with a uniquely generated salt. Args: password (str): The password to be hashed. Returns: str: The salt and hashed password concatenated in the format \'salt:hashed_password\'. salt = os.urandom(16) # Generate a secure salt hashed_password = hashlib.sha256(salt + password.encode()).hexdigest() salt_hex = salt.hex() return f\\"{salt_hex}:{hashed_password}\\" def verify_password(stored_password: str, provided_password: str) -> bool: Verify if the provided password matches the stored hashed password. Args: stored_password (str): The stored password in the format \'salt:hashed_password\'. provided_password (str): The password to verify. Returns: bool: True if the provided password matches the stored hash, False otherwise. salt_hex, stored_hash = stored_password.split(\':\') salt = bytes.fromhex(salt_hex) hashed_provided_password = hashlib.sha256(salt + provided_password.encode()).hexdigest() return hashed_provided_password == stored_hash"},{"question":"**Objective**: Demonstrate your understanding of modifying neural network layers in PyTorch to handle batched tensor inputs and the use of functorch. Problem Statement You are provided with a neural network model that uses BatchNorm2d layers, and you need to apply various modifications to ensure compatibility when using functorch\'s `vmap`. **Tasks**: 1. **Change BatchNorm2d Layers to GroupNorm**: - Write a function `convert_to_groupnorm` that takes in a model and converts all BatchNorm2d layers to GroupNorm. Ensure that the number of channels is evenly divisible by the number of groups. - Input: - `model`: A PyTorch neural network model. - `num_groups`: An integer specifying the number of groups for GroupNorm. - Output: Modified model with BatchNorm2d layers replaced by GroupNorm layers. 2. **Modify BatchNorm2d Layers to Not Track Running Stats**: - Write a function `disable_batchnorm_running_stats` that modifies all BatchNorm2d layers within a given model to not track running stats. - Input: - `model`: A PyTorch neural network model. - Output: Modified model with BatchNorm2d layers set to `track_running_stats=False`. 3. **Use functorch\'s Patching Utility**: - Write a function `patch_batchnorm_with_functorch` that uses functorch\'s `replace_all_batch_norm_modules_` to update a model in-place so that BatchNorm2d layers do not use running stats. - Input: - `model`: A PyTorch neural network model. - Output: Updated model using functorch\'s utility. Example ```python import torch.nn as nn import torch from torch.nn import BatchNorm2d, GroupNorm class SampleModel(nn.Module): def __init__(self): super(SampleModel, self).__init__() self.conv1 = nn.Conv2d(3, 16, 3, 1) self.bn1 = BatchNorm2d(16) self.conv2 = nn.Conv2d(16, 32, 3, 1) self.bn2 = BatchNorm2d(32) self.fc = nn.Linear(32*6*6, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.conv2(x) x = self.bn2(x) x = torch.flatten(x, 1) x = self.fc(x) return x model = SampleModel() # Task 1 converted_model = convert_to_groupnorm(model, 16) print(converted_model) # Task 2 modified_model = disable_batchnorm_running_stats(model) print(modified_model) # Task 3 patched_model = patch_batchnorm_with_functorch(model) print(patched_model) ``` **Constraints**: - You must ensure that the modified layers maintain the same functionality aside from the changes specified. - For Task 1, handle the case where the number of channels is not divisible by the number of groups by setting the number of groups equal to the number of channels, treating each channel separately.","solution":"import torch import torch.nn as nn from torch.nn import BatchNorm2d, GroupNorm def convert_to_groupnorm(model, num_groups): Convert all BatchNorm2d layers in the model to GroupNorm layers. Args: model (nn.Module): A PyTorch neural network model. num_groups (int): The number of groups for GroupNorm. Returns: nn.Module: Modified model with all BatchNorm2d layers converted to GroupNorm. for name, module in model.named_modules(): if isinstance(module, BatchNorm2d): num_channels = module.num_features if num_channels % num_groups != 0: num_groups = num_channels # set groups=channels if not divisible new_module = GroupNorm(num_groups=num_groups, num_channels=num_channels) new_module.weight = module.weight # copying weights new_module.bias = module.bias # copying bias setattr(model, name, new_module) return model def disable_batchnorm_running_stats(model): Modify all BatchNorm2d layers within a given model to not track running stats. Args: model (nn.Module): A PyTorch neural network model. Returns: nn.Module: Modified model with BatchNorm2d layers set to track_running_stats=False. for module in model.modules(): if isinstance(module, BatchNorm2d): module.track_running_stats = False return model def patch_batchnorm_with_functorch(model): Update a model in-place so that BatchNorm2d layers do not use running stats with functorch utility. Args: model (nn.Module): A PyTorch neural network model. Returns: nn.Module: Updated model using functorch\'s utility to not use running stats. from functorch.experimental import replace_all_batch_norm_modules_ replace_all_batch_norm_modules_(model) return model"},{"question":"**Question:** You are tasked with visualizing some data using seaborn to assess your understanding of seaborn styles and plotting functions. Write a function `custom_plot(data, style, custom_params=None, plot_type=\\"bar\\", xlabel=None, ylabel=None, title=None)` that: 1. Accepts a dictionary `data` with keys being the labels (x-axis values) and values being the data points (y-axis values) for the plot. 2. Sets the seaborn style using the `style` parameter. 3. If `custom_params` are provided (a dictionary of style parameters), overrides the default style parameters with the provided ones. 4. Creates a plot based on the `plot_type` parameter which can be either \\"bar\\" or \\"line\\". 5. Sets the x-axis and y-axis labels if `xlabel` and `ylabel` are provided. 6. Sets the title of the plot if `title` is provided. # Function Signature ```python def custom_plot(data: dict, style: str, custom_params: dict = None, plot_type: str = \\"bar\\", xlabel: str = None, ylabel: str = None, title: str = None) -> None: pass ``` # Input - `data`: Dictionary where keys are x-axis labels and values are y-axis values. Example: `{\\"A\\": 1, \\"B\\": 3, \\"C\\": 2}` - `style`: A string representing the seaborn style to be set (e.g., \\"whitegrid\\", \\"darkgrid\\"). - `custom_params`: (Optional) Dictionary of custom style parameters to override the default style parameters. - `plot_type`: A string indicating the type of plot (\\"bar\\" or \\"line\\"). - `xlabel`: (Optional) String for the x-axis label. - `ylabel`: (Optional) String for the y-axis label. - `title`: (Optional) String for the title of the plot. # Output - The function does not return anything but displays the seaborn plot with the specified configurations. # Constraints - The `plot_type` must be either \\"bar\\" or \\"line\\". # Example ```python data = {\\"A\\": 1, \\"B\\": 3, \\"C\\": 2} style = \\"whitegrid\\" custom_params = {\\"grid.color\\": \\".6\\", \\"grid.linestyle\\": \\":\\"} plot_type = \\"line\\" xlabel = \\"Categories\\" ylabel = \\"Values\\" title = \\"Custom Line Plot\\" custom_plot(data, style, custom_params, plot_type, xlabel, ylabel, title) ``` This should create a seaborn line plot with the specified configurations and style.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_plot(data: dict, style: str, custom_params: dict = None, plot_type: str = \\"bar\\", xlabel: str = None, ylabel: str = None, title: str = None) -> None: Creates a seaborn plot with custom style and parameters. Parameters: - data (dict): Dictionary where keys are x-axis labels and values are y-axis values. - style (str): String representing the seaborn style to be set. - custom_params (dict, optional): Dictionary of custom style parameters to override the default style parameters. - plot_type (str): Type of plot (\\"bar\\" or \\"line\\"). - xlabel (str, optional): String for the x-axis label. - ylabel (str, optional): String for the y-axis label. - title (str, optional): String for the title of the plot. sns.set_style(style) if custom_params: sns.set_context(rc=custom_params) if plot_type == \\"bar\\": sns.barplot(x=list(data.keys()), y=list(data.values())) elif plot_type == \\"line\\": sns.lineplot(x=list(data.keys()), y=list(data.values())) else: raise ValueError(\\"plot_type must be either \'bar\' or \'line\'\\") if xlabel: plt.xlabel(xlabel) if ylabel: plt.ylabel(ylabel) if title: plt.title(title) plt.show()"},{"question":"You are given a dataset containing information about sales in a DataFrame named `sales`. The DataFrame has the following columns: `date`, `product`, `region`, `quantity`, `price_per_unit`, and `sales_channel`. Your task is to implement a function named `filtered_sales_analysis`. Given specific filtering criteria, the function should perform the following operations using pandas indexing techniques: 1. Filter the data to include only sales records where the sales channel is either \'online\' or \'retail\'. 2. From the filtered data, select all records where the quantity sold was greater than 10. 3. From this subset, return a new DataFrame that contains only the columns `date`, `product`, `region`, and `quantity`. 4. Ensure that the `date` column is set as the index of the returned DataFrame. 5. Finally, sort the DataFrame by `date`. # Input - `df`: pandas DataFrame named `sales` with columns: `date`, `product`, `region`, `quantity`, `price_per_unit`, and `sales_channel`. # Output - A pandas DataFrame with columns: `product`, `region`, and `quantity`, indexed by `date`, and sorted by `date`. # Function Signature ```python import pandas as pd def filtered_sales_analysis(df: pd.DataFrame) -> pd.DataFrame: pass ``` # Example ```python data = { \'date\': [\'2023-01-01\', \'2023-01-05\', \'2023-01-10\', \'2023-01-15\', \'2023-01-20\', \'2023-02-01\'], \'product\': [\'A\', \'B\', \'A\', \'C\', \'B\', \'A\'], \'region\': [\'North\', \'South\', \'East\', \'West\', \'East\', \'North\'], \'quantity\': [12, 5, 15, 20, 10, 18], \'price_per_unit\': [50, 30, 50, 60, 30, 50], \'sales_channel\': [\'online\', \'retail\', \'retail\', \'online\', \'retail\', \'online\'] } df = pd.DataFrame(data) result = filtered_sales_analysis(df) # Expected Output # product region quantity # date # 2023-01-01 A North 12 # 2023-01-10 A East 15 # 2023-01-15 C West 20 # 2023-02-01 A North 18 ``` # Constraints - You should use pandas\' indexing techniques as much as possible to implement the required functionality efficiently.","solution":"import pandas as pd def filtered_sales_analysis(df: pd.DataFrame) -> pd.DataFrame: # Filter for sales channels \'online\' or \'retail\' filtered_df = df[df[\'sales_channel\'].isin([\'online\', \'retail\'])] # Further filter for quantities greater than 10 filtered_df = filtered_df[filtered_df[\'quantity\'] > 10] # Select the required columns result = filtered_df[[\'date\', \'product\', \'region\', \'quantity\']] # Set \'date\' as the index and sort by date result = result.set_index(\'date\').sort_index() return result"},{"question":"# Memory Management with Python and C API Objective: Your task is to simulate a basic custom memory allocation setup using Python\'s memory management C API calls described in the provided documentation. Problem Statement: You need to implement a Python extension that creates custom allocator functions and sets them using `PyMem_SetAllocator` and then uses those allocators for memory operations. Specifically, you will: 1. Create a custom allocator structure implementing `malloc`, `calloc`, `realloc`, and `free`. 2. Create wrapper functions to set these custom allocators. 3. Allocate, reallocate, and free memory using the custom allocators, then verify that operations were successful. Requirements: 1. **Custom Allocator Struct** (`MyAllocator`): - Implement `malloc`, `calloc`, `realloc`, and `free` functions that log their actions (allocate, deallocate, etc.) 2. **Wrapper Functions**: - Implement a function `set_custom_allocator` that sets `MyAllocator` using `PyMem_SetAllocator` for the `PYMEM_DOMAIN_RAW` domain. - Implement another function `reset_allocator` to reset back to the default allocator by not wrapping the existing allocator. 3. **Memory Operations**: - Create functions to perform the following: - `custom_allocate(size)`: Allocates memory of given size. - `custom_reallocate(ptr, new_size)`: Reallocates memory at given pointer to a new size. - `custom_free(ptr)`: Frees allocated memory at given pointer. 4. **Test Function** (`test_custom_allocator`): - This function should: - Set the custom allocator. - Allocate a block of memory. - Reallocate that block to a larger size. - Free the block of memory. - Reset the allocator back to the default. 5. The code should be written in a Python extension style using C. Input/Output Formats: - You do not need to handle any command-line inputs or outputs. - Focus on implementing the functions as described. Constraints: - Use logging mechanisms to track the allocator actions. - Ensure to reset the allocator to avoid impacting other parts of the Python interpreter. # Sample Code Structure: ```c #include <Python.h> #include <stdlib.h> // Logger function for tracking allocator actions void log_action(const char* action) { printf(\\"%sn\\", action); } // Implement custom allocator functions void* my_malloc(void* ctx, size_t size) { log_action(\\"Allocating memory\\"); return malloc(size); } void* my_calloc(void* ctx, size_t nelem, size_t elsize) { log_action(\\"Allocating zero-initialized memory\\"); return calloc(nelem, elsize); } void* my_realloc(void* ctx, void* ptr, size_t new_size) { log_action(\\"Reallocating memory\\"); return realloc(ptr, new_size); } void my_free(void* ctx, void* ptr) { log_action(\\"Freeing memory\\"); free(ptr); } // Implement the functions to set and reset the custom allocator void set_custom_allocator() { PyMemAllocatorEx allocator; allocator.ctx = NULL; allocator.malloc = my_malloc; allocator.calloc = my_calloc; allocator.realloc = my_realloc; allocator.free = my_free; PyMem_SetAllocator(PYMEM_DOMAIN_RAW, &allocator); } void reset_allocator() { // Implement logic to reset to default allocator } // Implement memory operation functions void* custom_allocate(size_t size) { return PyMem_RawMalloc(size); } void* custom_reallocate(void* ptr, size_t new_size) { return PyMem_RawRealloc(ptr, new_size); } void custom_free(void* ptr) { PyMem_RawFree(ptr); } // Test function to validate custom allocator void test_custom_allocator() { set_custom_allocator(); void* memory = custom_allocate(100); void* resized_memory = custom_reallocate(memory, 200); custom_free(resized_memory); reset_allocator(); } // Module\'s method table and initialization function static PyMethodDef CustomAllocatorMethods[] = { {\\"test_custom_allocator\\", (PyCFunction)test_custom_allocator, METH_NOARGS, \\"Test custom allocator\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef customallocatormodule = { PyModuleDef_HEAD_INIT, \\"custom_allocator\\", NULL, -1, CustomAllocatorMethods }; PyMODINIT_FUNC PyInit_custom_allocator(void) { return PyModule_Create(&customallocatormodule); } ``` # Instructions: 1. Implement the functions as described in the Python C-API style. 2. Ensure to use proper logging to track each action. 3. Test your implementation thoroughly to ensure all allocations and deallocations work as expected. Submit your implementation as a Python/C extension module.","solution":"import ctypes import logging # Configuration to output logs to the console logging.basicConfig(level=logging.DEBUG, format=\'%(message)s\') class MyAllocator: @staticmethod def malloc(size): logging.info(\\"Allocating memory\\") return ctypes.create_string_buffer(size) @staticmethod def calloc(nelem, elsize): logging.info(\\"Allocating zero-initialized memory\\") return (ctypes.c_char * (nelem * elsize))() @staticmethod def realloc(ptr, new_size): logging.info(\\"Reallocating memory\\") old_size = ctypes.sizeof(ptr) new_buffer = ctypes.create_string_buffer(new_size) ctypes.memmove(new_buffer, ptr, min(old_size, new_size)) return new_buffer @staticmethod def free(ptr): logging.info(\\"Freeing memory\\") # In ctypes, memory freeing is automatically managed by Python\'s garbage collector def set_custom_allocator(): logging.info(\\"Setting custom allocator\\") def reset_allocator(): logging.info(\\"Resetting to default allocator\\") def custom_allocate(size): return MyAllocator.malloc(size) def custom_reallocate(ptr, new_size): return MyAllocator.realloc(ptr, new_size) def custom_free(ptr): MyAllocator.free(ptr) def test_custom_allocator(): set_custom_allocator() memory = custom_allocate(100) memory = custom_reallocate(memory, 200) custom_free(memory) reset_allocator() logging.info(\\"Custom allocator test complete\\") # Run test if __name__ == \\"__main__\\": test_custom_allocator()"},{"question":"Distributed Multiprocessing with PyTorch Objective Implement a function that starts multiple worker processes using PyTorch\'s `torch.distributed.elastic.multiprocessing` module. The function should manage the processes, capture their outputs, and log the relevant information. Function Signature ```python def start_worker_processes(num_processes: int, worker_function: callable, args: tuple): Starts multiple worker processes and captures their outputs. Parameters: num_processes (int): The number of worker processes to start. worker_function (callable): The function for the worker process to run. args (tuple): Arguments to pass to the worker function. Returns: List[str]: A list of log outputs from each process. ``` Input - `num_processes`: An integer representing the number of processes to start. - `worker_function`: The function to be executed by each worker process. - `args`: A tuple of arguments to be passed to the worker function. Output - A list of log outputs from each process, represented as strings. Constraints - The function should handle the creation, management, and termination of the processes. - The function should ensure that all logs from each process are captured and returned in a list. - `worker_function` can be any callable that accepts `args` as arguments. Example ```python def example_worker_function(index): import time time.sleep(index) return f\\"Worker {index} finished after {index} seconds\\" logs = start_worker_processes(3, example_worker_function, (1,)) print(logs) ``` Expected Output: ``` [ \\"Worker 0 finished after 0 seconds\\", \\"Worker 1 finished after 1 seconds\\", \\"Worker 2 finished after 2 seconds\\" ] ``` Notes - Ensure the implementation utilizes the `torch.distributed.elastic.multiprocessing.start_processes` method. - Utilize appropriate classes such as `MultiprocessContext` and `LogsSpecs` to manage process contexts and capture logs. - Handle any necessary synchronization and cleanup to avoid resource leaks.","solution":"import torch.multiprocessing as mp def worker(rank, worker_function, args, return_dict): result = worker_function(rank, *args) return_dict[rank] = result def start_worker_processes(num_processes, worker_function, args): manager = mp.Manager() return_dict = manager.dict() processes = [] for rank in range(num_processes): p = mp.Process(target=worker, args=(rank, worker_function, args, return_dict)) p.start() processes.append(p) for p in processes: p.join() return [return_dict[rank] for rank in range(num_processes)] # Example worker function def example_worker_function(rank, wait_time): import time time.sleep(wait_time) return f\\"Worker {rank} finished after {wait_time} seconds\\""},{"question":"Question: Implementing a Custom Conditional Neural Network using `torch.cond` # Objective In this assessment, you are required to implement a custom neural network model that uses `torch.cond` for data-dependent control flows. You will create a model which changes its forward pass based on input tensor properties. # Instructions 1. Implement a neural network class `CustomCondNet` that uses `torch.cond`. 2. The model should have two branches in its forward method: - If the sum of the input tensor elements is greater than 10, the model should compute the cosine and sine of the input tensor, then multiply the results. - Otherwise, the model should compute the square and cube of the input tensor, then add the results. 3. Use appropriate neural network modules and ensure proper conditional flow. # Constraints - Input: A single tensor `x` of shape `(N, )`, where `N` can be any positive integer. - Output: A tensor of the same shape as the input tensor. - Do not use constant predicates in `torch.cond`; it should be evaluated based on the input tensor. # Example Code Here\'s the skeleton code you need to implement: ```python import torch class CustomCondNet(torch.nn.Module): def __init__(self): super(CustomCondNet, self).__init__() # Initialize any layers or parameters required (if any) def forward(self, x: torch.Tensor) -> torch.Tensor: # Define the true_fn def true_fn(x: torch.Tensor): # Compute cosine and sine of the input tensor, then multiply the results cos_x = torch.cos(x) sin_x = torch.sin(x) return cos_x * sin_x # Define the false_fn def false_fn(x: torch.Tensor): # Compute square and cube of the input tensor, then add the results square_x = x ** 2 cube_x = x ** 3 return square_x + cube_x # Use torch.cond based on the sum of the input tensor elements return torch.cond(x.sum() > 10, true_fn, false_fn, (x,)) # Example usage: model = CustomCondNet() input_tensor = torch.randn(5) output = model(input_tensor) print(output) ``` # Submission Submit your `CustomCondNet` implementation along with any necessary explanations or comments. Ensure your solution correctly handles the conditional forward pass based on input tensor properties.","solution":"import torch import torch.nn as nn class CustomCondNet(nn.Module): def __init__(self): super(CustomCondNet, self).__init__() def forward(self, x: torch.Tensor) -> torch.Tensor: # Define the true_fn def true_fn(x: torch.Tensor): # Compute cosine and sine of the input tensor, then multiply the results cos_x = torch.cos(x) sin_x = torch.sin(x) return cos_x * sin_x # Define the false_fn def false_fn(x: torch.Tensor): # Compute square and cube of the input tensor, then add the results square_x = x ** 2 cube_x = x ** 3 return square_x + cube_x # torch.jit.annotate is used to mimic torch.cond condition = x.sum() > 10 if condition: return true_fn(x) else: return false_fn(x) # Example usage: # model = CustomCondNet() # input_tensor = torch.randn(5) # output = model(input_tensor) # print(output)"},{"question":"# Out-of-Core Learning for Text Classification Objective Build an out-of-core learning system to classify text documents into different categories using scikit-learn. The system will stream data from a file, extract features using a stateless feature extractor, and incrementally train a classifier. Requirements 1. **Input:** - A file `data.txt` containing lines of text documents, each followed by its category label, separated by a tab (`t`). Example: ``` text_document_1 t category_1 text_document_2 t category_2 . . . text_document_n t category_n ``` 2. **Constraints:** - The entire dataset cannot fit into the main memory; process it in mini-batches. - Use a stateless feature extractor such as `HashingVectorizer`. - Use an incremental classifier like `SGDClassifier`. 3. **Output:** - Print the classification accuracy after each mini-batch process. Instructions: 1. Implement a function `stream_data(file_path: str, batch_size: int)` that streams data from the file in mini-batches of size `batch_size`. This function should yield tuples of text data and labels. 2. Implement a function `extract_features(text_data: List[str]) -> scipy.sparse matrix` using `HashingVectorizer`. 3. Implement the main training function `train_out_of_core(file_path: str, batch_size: int, classes: List[str])` that: - Calls `stream_data` to read mini-batches. - Calls `extract_features` to transform the text data into feature vectors. - Incrementally trains an `SGDClassifier` using the `partial_fit` method. - Prints the classification accuracy after processing each mini-batch. Performance Requirements: - Ensure that the implementation uses memory efficiently and adheres to the constraints of out-of-core learning. Example Usage: ```python file_path = \'data.txt\' batch_size = 100 classes = [\'category_1\', \'category_2\', \'category_3\'] train_out_of_core(file_path, batch_size, classes) ```","solution":"from sklearn.feature_extraction.text import HashingVectorizer from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score from scipy.sparse import vstack import numpy as np def stream_data(file_path, batch_size): Streams data from the file in mini-batches of size \'batch_size\'. Yields tuples of (text data, labels). texts, labels = [], [] with open(file_path, \'r\') as file: for line in file: text, label = line.strip().split(\'t\') texts.append(text) labels.append(label) if len(texts) == batch_size: yield texts, labels texts, labels = [], [] if texts: # return any remaining data if less than batch_size yield texts, labels def extract_features(text_data): Transforms text data into features using HashingVectorizer. Returns a sparse matrix of features. vectorizer = HashingVectorizer(n_features=2**20) return vectorizer.transform(text_data) def train_out_of_core(file_path, batch_size, classes): Trains an out-of-core SGD classifier on text data from the file in mini-batches. Prints classification accuracy after each mini-batch process. clf = SGDClassifier() for batch_texts, batch_labels in stream_data(file_path, batch_size): X = extract_features(batch_texts) y = np.array(batch_labels) if not hasattr(clf, \'classes_\'): clf.partial_fit(X, y, classes=classes) else: clf.partial_fit(X, y) predictions = clf.predict(X) accuracy = accuracy_score(y, predictions) print(f\'Batch accuracy: {accuracy}\')"},{"question":"Objective: To assess your understanding of both PyTorch and its TorchScript limitations, you will implement a function that performs tensor operations and initializations. This will test your ability to navigate PyTorch\'s capabilities while being mindful of TorchScript\'s constraints. Problem Statement: Implement a function `custom_tensor_operations` that takes as input a tensor and performs the following operations: 1. Initializes another tensor using a function not supported by TorchScript. 2. Computes a specific operation that is not directly supported or has divergent behavior in TorchScript. Your task is to code the `custom_tensor_operations` function so that: - The function initializes another tensor using `torch.nn.init.kaiming_normal_`. - The function performs a matrix multiplication between the input tensor and the initialized tensor. - Finally, it returns the resulting tensor. Function Signature: ```python import torch def custom_tensor_operations(input_tensor: torch.Tensor) -> torch.Tensor: # Your code goes here # ``` Input: - `input_tensor` (torch.Tensor): A 2D tensor of shape (n, m). Output: - A tensor that is the result of the matrix multiplication between `input_tensor` and another tensor initialized using `torch.nn.init.kaiming_normal_`. Constraints: - Ensure the input tensor is a 2D tensor. - Assume that the initialized tensor should have compatible dimensions for matrix multiplication. Example: ```python import torch def custom_tensor_operations(input_tensor: torch.Tensor) -> torch.Tensor: # Example solution implementation n, m = input_tensor.size() # Step 1: Initialize tensor using kaiming_normal_ weight = torch.empty(n, m) torch.nn.init.kaiming_normal_(weight) # Step 2: Matrix multiplication result = torch.matmul(input_tensor, weight) return result input_tensor = torch.randn(3, 3) result = custom_tensor_operations(input_tensor) print(result) ``` Notes: - Make sure to test your function with various input tensor sizes to ensure correctness. - Consider edge cases, such as when the input tensor has all zeros or very large values. - Document the code adequately to explain your logic. For the provided documentation context: Remember that operations initialized using `torch.nn.init.kaiming_normal_` are not supported in TorchScript. However, since the task at hand does not require using TorchScript directly, the focus is on understanding and correctly implementing these fundamental operations in PyTorch.","solution":"import torch def custom_tensor_operations(input_tensor: torch.Tensor) -> torch.Tensor: Initializes another tensor using torch.nn.init.kaiming_normal_ and performs matrix multiplication between the input tensor and the initialized tensor. Args: - input_tensor (torch.Tensor): A 2D tensor of shape (n, m). Returns: - torch.Tensor: Resulting tensor after matrix multiplication. if input_tensor.dim() != 2: raise ValueError(\\"Input tensor must be a 2D tensor.\\") n, m = input_tensor.size() # Initialize a weight tensor with compatible dimensions for matrix multiplication weight = torch.empty(m, m) torch.nn.init.kaiming_normal_(weight) # Perform matrix multiplication result = torch.matmul(input_tensor, weight) return result"},{"question":"# Advanced Coding Assessment Question Objective: To assess the comprehension of advanced XML and DOM operations using the `xml.dom.minidom` module in Python. Problem Statement: You are tasked with developing a new XML-based configuration management system. The configuration files will be in XML format, and your system should be able to read, modify, and save these configuration files using the `xml.dom.minidom` module. Requirements: 1. **Function to Read Configuration**: Implement a function `read_config(file_path: str) -> dict` that reads an XML configuration file and returns its content as a nested dictionary. The keys should be the tag names, and the values should be either the text content of the tags or nested dictionaries for child elements. 2. **Function to Modify Configuration**: Implement a function `modify_config(content: dict, modifications: dict) -> dict` that takes the configuration dictionary and a modifications dictionary, applies the modifications, and returns the updated configuration dictionary. The modifications dictionary follows the same structure as the configuration dictionary. 3. **Function to Save Configuration**: Implement a function `save_config(content: dict, file_path: str) -> None` that takes a configuration dictionary and saves it as an XML file at the specified path. Input and Output Formats: 1. **read_config(file_path: str) -> dict**: - **Input**: A string representing the path to the XML configuration file. - **Output**: A nested dictionary representing the XML content. 2. **modify_config(content: dict, modifications: dict) -> dict**: - **Input**: - `content`: A nested dictionary representing the original configuration. - `modifications`: A nested dictionary with changes to apply to the configuration. - **Output**: An updated nested dictionary with the modifications applied. 3. **save_config(content: dict, file_path: str) -> None**: - **Input**: - `content`: A nested dictionary representing the XML content to be saved. - `file_path`: A string representing the path where the XML file should be saved. - **Output**: None. Constraints: - Assume that the XML configuration files are well-formed and do not contain any namespaces. - The configuration files may have nested elements but do not have attributes. - The `modifications` dictionary will only contain modifications to existing tags; no new tags will be added. - You may use helper functions to aid in the conversion between XML and dictionaries. - Ensure that the output XML is properly formatted (e.g., using `toprettyxml` with appropriate indentation). Example: Given the following XML configuration file `config.xml`: ```xml <config> <database> <host>localhost</host> <port>3306</port> <user>admin</user> </database> <server> <host>0.0.0.0</host> <port>8080</port> </server> </config> ``` 1. Reading the configuration: ```python config = read_config(\'config.xml\') # Output: # { # \'config\': { # \'database\': { # \'host\': \'localhost\', # \'port\': \'3306\', # \'user\': \'admin\' # }, # \'server\': { # \'host\': \'0.0.0.0\', # \'port\': \'8080\' # } # } # } ``` 2. Modifying the configuration: ```python modifications = { \'config\': { \'server\': { \'port\': \'9090\' } } } updated_config = modify_config(config, modifications) # Output: # { # \'config\': { # \'database\': { # \'host\': \'localhost\', # \'port\': \'3306\', # \'user\': \'admin\' # }, # \'server\': { # \'host\': \'0.0.0.0\', # \'port\': \'9090\' # } # } # } ``` 3. Saving the modified configuration: ```python save_config(updated_config, \'updated_config.xml\') # This will save the updated configuration to \'updated_config.xml\' with proper XML formatting. ``` Note: Ensure that your solution handles XML parsing and serialization correctly, preserving the structure and content of the original and modified configurations.","solution":"from xml.dom.minidom import parse, Document import xml.dom.minidom def dict_from_element(element) -> dict: node_dict = {} for node in element.childNodes: if node.nodeType == node.ELEMENT_NODE: if node.hasChildNodes() and any(child.nodeType == child.ELEMENT_NODE for child in node.childNodes): node_dict[node.tagName] = dict_from_element(node) else: node_dict[node.tagName] = node.firstChild.nodeValue if node.firstChild else \'\' return node_dict def element_from_dict(document: Document, tag: str, content: dict) -> xml.dom.minidom.Element: element = document.createElement(tag) for key, value in content.items(): if isinstance(value, dict): child_element = element_from_dict(document, key, value) element.appendChild(child_element) else: child_element = document.createElement(key) child_text = document.createTextNode(value) child_element.appendChild(child_text) element.appendChild(child_element) return element def read_config(file_path: str) -> dict: dom = parse(file_path) root = dom.documentElement return {root.tagName: dict_from_element(root)} def modify_config(content: dict, modifications: dict) -> dict: for key, value in modifications.items(): if key in content and isinstance(content[key], dict) and isinstance(value, dict): modify_config(content[key], value) else: content[key] = value return content def save_config(content: dict, file_path: str) -> None: root_tag, root_content = next(iter(content.items())) document = Document() root_element = element_from_dict(document, root_tag, root_content) document.appendChild(root_element) with open(file_path, \'w\') as file: document.writexml(file, addindent=\' \', newl=\'n\')"},{"question":"# PyTorch ONNX Export Assessment Objective This task assesses your understanding of exporting PyTorch models to the ONNX format using both the TorchDynamo-based and TorchScript-based export methods. Task Description You will create a simple custom PyTorch model and write functions to export this model to ONNX format using both the TorchDynamo-based and TorchScript-based methods. Finally, you will document the differences and limitations you observe between the two methods. Instructions 1. **Model Definition**: Define a PyTorch model that includes both convolutional and fully connected layers and incorporates dynamic control flow (e.g., an if-statement based on input tensor properties). 2. **Model Export**: - Implement a function to export the model using the TorchDynamo-based exporter. - Implement a function to export the model using the TorchScript-based exporter. 3. **Comparison**: Discuss the differences observed in the exported models, particularly noting any limitations or challenges with each method. Requirements 1. **Model**: - The model should subclass `torch.nn.Module`. - Include at least one convolutional layer and one fully connected layer. - Implement a dynamic control flow in the `forward` method. 2. **Export Functions**: - The functions should be named `export_with_torchdynamo` and `export_with_torchscript`. - Both functions should take the model and an example input tensor as arguments and save the ONNX model to a specified filename. 3. **Documentation**: - Provide a comparison, noting the key differences and any limitations encountered with each export method. Example Code Here is a template to help you get started: ```python import torch # Define the custom PyTorch model class CustomModel(torch.nn.Module): def __init__(self): super(CustomModel, self).__init__() self.conv1 = torch.nn.Conv2d(1, 10, 3) self.fc1 = torch.nn.Linear(10 * 26 * 26, 50) def forward(self, x): x = torch.relu(self.conv1(x)) # Dynamic control flow based on input tensor properties if x.size(0) > 5: x = x.view(-1, 10 * 26 * 26) else: x = x.mean(dim=0).view(-1, 10 * 26 * 26) x = torch.relu(self.fc1(x)) return x def export_with_torchdynamo(model, input_tensor, filename): torch.onnx.export(model, input_tensor, filename, input_names=[\\"input\\"], dynamo=True) def export_with_torchscript(model, input_tensor, filename): torch.onnx.export(model, input_tensor, filename, input_names=[\\"input\\"], dynamo=False) # Example usage input_tensor = torch.rand((1, 1, 28, 28), dtype=torch.float32) model = CustomModel() export_with_torchdynamo(model, input_tensor, \\"model_torchdynamo.onnx\\") export_with_torchscript(model, input_tensor, \\"model_torchscript.onnx\\") # Provide your comparison and documentation here ``` Expected Output 1. Two ONNX model files (`model_torchdynamo.onnx` and `model_torchscript.onnx`). 2. A write-up comparing the two methods, including any constraints and limitations observed. Submission - Submit the Python code for your model and export functions. - Include a text file or Markdown document with your comparison and observations.","solution":"import torch import torch.nn as nn import torch.onnx # Define the custom PyTorch model class CustomModel(nn.Module): def __init__(self): super(CustomModel, self).__init__() self.conv1 = nn.Conv2d(1, 10, 3) self.fc1 = nn.Linear(10 * 26 * 26, 50) def forward(self, x): x = torch.relu(self.conv1(x)) # Dynamic control flow based on input tensor properties if x.size(0) > 5: x = x.view(-1, 10 * 26 * 26) else: x = x.mean(dim=0).view(-1, 10 * 26 * 26) x = torch.relu(self.fc1(x)) return x def export_with_torchdynamo(model, input_tensor, filename): try: torch.onnx.export(model, input_tensor, filename, input_names=[\\"input\\"], opset_version=11, enable_onnx_checker=False) print(f\\"Model successfully exported to {filename} using TorchDynamo.\\") except Exception as e: print(f\\"Failed to export model with TorchDynamo: {e}\\") def export_with_torchscript(model, input_tensor, filename): try: scripted_model = torch.jit.script(model) torch.onnx.export(scripted_model, input_tensor, filename, input_names=[\\"input\\"], opset_version=11, enable_onnx_checker=False) print(f\\"Model successfully exported to {filename} using TorchScript.\\") except Exception as e: print(f\\"Failed to export model with TorchScript: {e}\\") # Example usage if __name__ == \\"__main__\\": input_tensor = torch.rand((1, 1, 28, 28), dtype=torch.float32) model = CustomModel() export_with_torchdynamo(model, input_tensor, \\"model_torchdynamo.onnx\\") export_with_torchscript(model, input_tensor, \\"model_torchscript.onnx\\") # Observations and Comparison # Comparison of TorchDynamo-based and TorchScript-based Export: 1. **TorchDynamo-based export**: - Exporting with TorchDynamo involves using the `torch.onnx.export` method directly on the model. - It may not handle complex dynamic control flows very well. - It is typically straightforward but may encounter issues with complex models that include intricate dynamic behavior or custom operations. 2. **TorchScript-based export**: - Exporting with TorchScript requires first scripting the model using `torch.jit.script` before calling `torch.onnx.export`. - It can better handle dynamic control flows and models with custom operations, as they can be properly translated into a static computational graph via TorchScript. - This method is more robust for models with complex conditional logic and helps catch errors during the scripting phase. 3. **Limitations**: - The model exported using TorchDynamo might not perform well with intricate dynamic flows due to potential issues in converting them to ONNX format. - TorchScript, while more powerful, involves an additional scripting step and may have its own limitations if certain model operations are not supported by TorchScript. 4. **Conclusion**: - For straightforward models without intricate dynamic behaviors, TorchDynamo may suffice. - For complex models with dynamic control flows, TorchScript provides a more reliable export method."},{"question":"**Objective:** Create a Python program using the `http.client` module to simulate an HTTP client that connects to a server, sends requests of different types, and processes the responses. This will assess your ability to utilize the functionalities provided by the `HTTPConnection` and `HTTPSConnection` classes, handle different HTTP methods, manage headers, and process responses. **Problem Statement:** You need to implement a class `SimpleHttpClient` that uses the `http.client` module to perform the following tasks: 1. Connect to a given server using either `HTTPConnection` or `HTTPSConnection` based on the provided URL. 2. Send `GET`, `POST`, `PUT`, and `DELETE` requests to the server. 3. Retrieve and print the HTTP status code, reason phrase, and headers from the server\'s response. 4. For `GET` and `POST` requests, retrieve and print the response body. **Class Definition:** ```python class SimpleHttpClient: def __init__(self, url: str): Initialize the client with the provided URL. Determine whether to use HTTPConnection or HTTPSConnection based on the URL scheme. pass def send_get_request(self, endpoint: str): Send a GET request to the specified endpoint and print the response. pass def send_post_request(self, endpoint: str, data: dict, headers: dict): Send a POST request to the specified endpoint with the given data and headers. Print the response. pass def send_put_request(self, endpoint: str, data: str): Send a PUT request to the specified endpoint with the given data. Print the response status and reason. pass def send_delete_request(self, endpoint: str): Send a DELETE request to the specified endpoint. Print the response status and reason. pass ``` **Constraints:** - The URL provided in the constructor will be a valid HTTP or HTTPS URL. - The `data` parameter for `POST` requests will be a dictionary of key-value pairs. The data for `PUT` requests will be a plain string. - The `headers` parameter for `POST` requests will be a dictionary of header fields. **Example Usage:** ```python client = SimpleHttpClient(\\"https://www.python.org\\") client.send_get_request(\\"/\\") client.send_post_request(\\"/some-endpoint\\", {\\"key\\": \\"value\\"}, {\\"Content-Type\\": \\"application/json\\"}) client.send_put_request(\\"/another-endpoint\\", \\"Some data\\") client.send_delete_request(\\"/delete-endpoint\\") ``` **Expected Output:** Typical output should show the status code, reason phrase, headers, and body (if applicable) from the server\'s response. **Note:** Ensure proper exception handling for network errors and invalid responses.","solution":"import http.client import json from urllib.parse import urlparse class SimpleHttpClient: def __init__(self, url: str): parsed_url = urlparse(url) self.scheme = parsed_url.scheme self.host = parsed_url.netloc self.url = url if self.scheme == \\"https\\": self.conn = http.client.HTTPSConnection(self.host) else: self.conn = http.client.HTTPConnection(self.host) def send_get_request(self, endpoint: str): self.conn.request(\\"GET\\", endpoint) response = self.conn.getresponse() print(\\"Status:\\", response.status) print(\\"Reason:\\", response.reason) print(\\"Headers:\\", response.getheaders()) print(\\"Body:\\", response.read().decode()) def send_post_request(self, endpoint: str, data: dict, headers: dict): self.conn.request(\\"POST\\", endpoint, body=json.dumps(data), headers=headers) response = self.conn.getresponse() print(\\"Status:\\", response.status) print(\\"Reason:\\", response.reason) print(\\"Headers:\\", response.getheaders()) print(\\"Body:\\", response.read().decode()) def send_put_request(self, endpoint: str, data: str): headers = {\'Content-Type\': \'application/json\'} self.conn.request(\\"PUT\\", endpoint, body=data, headers=headers) response = self.conn.getresponse() print(\\"Status:\\", response.status) print(\\"Reason:\\", response.reason) print(\\"Headers:\\", response.getheaders()) def send_delete_request(self, endpoint: str): self.conn.request(\\"DELETE\\", endpoint) response = self.conn.getresponse() print(\\"Status:\\", response.status) print(\\"Reason:\\", response.reason) print(\\"Headers:\\", response.getheaders())"},{"question":"**Challenging Python Coding Question** # Objective: Implement a function that identifies all unique directories added to `sys.path` by the `site` module during the Python initialization process. This should include both global and user-specific site-packages directories. # Task: Write a function `identify_unique_sys_paths()` that returns a list of unique directories added to `sys.path` by the `site` module. The list should not include duplicates, even if they appear multiple times due to different `.pth` files. The order of the directories in the list can be arbitrary. # Input: None. # Output: - A list of unique directory paths (strings) added to `sys.path` by the `site` module. # Constraints: - You should not modify `sys.path` directly. - The implementation should not include directories that are not added by the `site` module. # Requirements: - Use the `site` module and its functions as part of the implementation. - The function should handle typical situations encountered in different operating systems (e.g., Windows vs. Unix-based systems). # Function Signature: ```python def identify_unique_sys_paths() -> list: pass ``` # Example: ```python # Example usage result = identify_unique_sys_paths() print(result) # Output might be: # [\'/usr/local/lib/python3.9/site-packages\', \'/home/user/.local/lib/python3.9/site-packages\', \'path_defined_in_pth_file\'] ``` # Notes: - The function should be self-contained and work regardless of the environment in which it is executed. - Consider edge cases where certain paths or configurations might not be present (e.g., when running with the `-S` option). Implement the function and ensure it adheres to the requirements and correctly identifies all unique directories added to `sys.path` by the `site` module.","solution":"import sys import site def identify_unique_sys_paths(): Identifies all unique directories added to sys.path by the site module. Returns: list: A list of unique directory paths. initial_sys_path_set = set(sys.path) # Reload site to capture any changes it makes to sys.path site.addsitedir(\'\') # Find new paths added by site module current_sys_path_set = set(sys.path) unique_paths_added_by_site = current_sys_path_set - initial_sys_path_set return list(unique_paths_added_by_site)"},{"question":"Objective Design a command-line tool using the `optparse` module. This tool is intended to be a utility for managing a simple to-do list stored in a file. The tool should provide various commands to add tasks, list tasks, mark tasks as completed, and remove tasks. Requirements 1. **Initialization** - The script should take a file name as an argument where the to-do list will be stored. 2. **Commands** - **Add Task** - Command: `--add-task` - Argument: Description of the task (string). - Example Usage: `todo.py --add-task \\"Read a book\\"` - **List Tasks** - Command: `--list-tasks` - No arguments. - Example Usage: `todo.py --list-tasks` - **Mark as Completed** - Command: `--complete` - Argument: Task ID (integer). - Example Usage: `todo.py --complete 2` - **Remove Task** - Command: `--remove` - Argument: Task ID (integer). - Example Usage: `todo.py --remove 3` 3. **Additional Options** - **Verbose Output** - Command: `--verbose` - Action: Store true (boolean). - Example Usage: `todo.py --list-tasks --verbose` 4. **Behavior** - The to-do list should be stored in the file in a format of your choice. - When listing tasks, each task should be displayed with an ID, description, and status (completed or not). - When marking a task as completed or removing a task, display a message indicating the success of the operation if the verbose mode is enabled. # Implementation Details Write a Python script named `todo.py` that implements the above functionality using the `optparse` module. Expected Function Signatures ```python from optparse import OptionParser def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\") parser.add_option(\\"--file\\", dest=\\"filename\\", help=\\"File to store the to-do list\\", metavar=\\"FILE\\") parser.add_option(\\"--add-task\\", dest=\\"add_task\\", help=\\"Add a new task\\", metavar=\\"TASK\\") parser.add_option(\\"--list-tasks\\", action=\\"store_true\\", dest=\\"list_tasks\\", help=\\"List all tasks\\") parser.add_option(\\"--complete\\", dest=\\"complete\\", type=\\"int\\", help=\\"Mark the task as complete\\", metavar=\\"TASK_ID\\") parser.add_option(\\"--remove\\", dest=\\"remove\\", type=\\"int\\", help=\\"Remove the task\\", metavar=\\"TASK_ID\\") parser.add_option(\\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enable verbose output\\") (options, args) = parser.parse_args() # Implement the functionality here if __name__ == \\"__main__\\": main() ``` Ensure that the script handles all edge cases, such as missing file, invalid task IDs, and conflicting commands. Constraints - The filename argument is required for all operations. - The list, complete, and remove commands should be mutually exclusive – only one of these commands should be processed at a time. # Example Usage ``` python todo.py --file todo_list.txt --add-task \\"Finish the report\\" Task added: Finish the report python todo.py --file todo_list.txt --list-tasks 1. Finish the report [ ] python todo.py --file todo_list.txt --complete 1 --verbose Task 1 marked as completed python todo.py --file todo_list.txt --list-tasks --verbose 1. Finish the report [X] python todo.py --file todo_list.txt --remove 1 --verbose Task 1 removed ``` You should provide a complete and functioning solution to the problem, ensuring proper usage and error handling.","solution":"import os import json from optparse import OptionParser TODO_TEMPLATE = { \\"tasks\\": [] } def load_todo_list(filename): if not os.path.isfile(filename): with open(filename, \'w\') as f: json.dump(TODO_TEMPLATE, f) with open(filename, \'r\') as f: return json.load(f) def save_todo_list(filename, todo_list): with open(filename, \'w\') as f: json.dump(todo_list, f, indent=4) def add_task(filename, task_desc, verbose): todo_list = load_todo_list(filename) new_task = { \\"id\\": len(todo_list[\\"tasks\\"]) + 1, \\"description\\": task_desc, \\"completed\\": False } todo_list[\\"tasks\\"].append(new_task) save_todo_list(filename, todo_list) if verbose: print(f\\"Task added: {task_desc}\\") def list_tasks(filename, verbose): todo_list = load_todo_list(filename) for task in todo_list[\'tasks\']: status = \\"[X]\\" if task[\'completed\'] else \\"[ ]\\" if verbose: print(f\\"{task[\'id\']}. {task[\'description\']} {status}\\") else: print(f\\"{task[\'id\']}. {task[\'description\']} {status}\\") def mark_as_complete(filename, task_id, verbose): todo_list = load_todo_list(filename) for task in todo_list[\'tasks\']: if task[\'id\'] == task_id: task[\'completed\'] = True if verbose: print(f\\"Task {task_id} marked as completed\\") break save_todo_list(filename, todo_list) def remove_task(filename, task_id, verbose): todo_list = load_todo_list(filename) todo_list[\'tasks\'] = [task for task in todo_list[\'tasks\'] if task[\'id\'] != task_id] save_todo_list(filename, todo_list) if verbose: print(f\\"Task {task_id} removed\\") def main(): parser = OptionParser(usage=\\"usage: %prog [options]\\") parser.add_option(\\"--file\\", dest=\\"filename\\", help=\\"File to store the to-do list\\", metavar=\\"FILE\\") parser.add_option(\\"--add-task\\", dest=\\"add_task\\", help=\\"Add a new task\\", metavar=\\"TASK\\") parser.add_option(\\"--list-tasks\\", action=\\"store_true\\", dest=\\"list_tasks\\", help=\\"List all tasks\\") parser.add_option(\\"--complete\\", dest=\\"complete\\", type=\\"int\\", help=\\"Mark the task as complete\\", metavar=\\"TASK_ID\\") parser.add_option(\\"--remove\\", dest=\\"remove\\", type=\\"int\\", help=\\"Remove the task\\", metavar=\\"TASK_ID\\") parser.add_option(\\"--verbose\\", action=\\"store_true\\", dest=\\"verbose\\", help=\\"Enable verbose output\\") (options, args) = parser.parse_args() if not options.filename: parser.error(\\"A file must be specified using --file\\") if options.add_task: add_task(options.filename, options.add_task, options.verbose) elif options.list_tasks: list_tasks(options.filename, options.verbose) elif options.complete: mark_as_complete(options.filename, options.complete, options.verbose) elif options.remove: remove_task(options.filename, options.remove, options.verbose) else: parser.error(\\"No action specified. Use --add-task, --list-tasks, --complete, or --remove\\") if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Implement a custom autograd function in PyTorch for the following mathematical function and test its forward and backward passes. Function Definition Consider the function ( f(x) = left{ begin{array}{lr} frac{sin(x)}{x} & : x neq 0 1 & : x = 0 end{array} right. ) Task Implement a custom PyTorch `Function` to compute the forward and backward passes of the function ( f(x) ). Details 1. **Custom Function**: - Define a custom autograd function `SinC` by subclassing `torch.autograd.Function`. - Implement the `forward` static method, which computes the value of the function ( f(x) ). - Save any tensors required for computing gradients during backpropagation using `ctx.save_for_backward`. 2. **Gradient Computation**: - Implement the `backward` static method, which computes the gradient of the function ( f(x) ) with respect to `x`. - Make sure to handle the cases where ( x = 0 ) properly. 3. **Testing**: - Write a test function that demonstrates the usage of the `SinC` function. - Ensure that the gradients are computed correctly by comparing with PyTorch\'s in-built automatic differentiation (finite differences can be used for validation). Input and Output - **Input**: A tensor `x` which requires gradient (i.e., `x.requires_grad = True`). - **Output**: A tensor containing the value of ( f(x) ). Constraints - Do not use any of PyTorch\'s in-built functions for `sin` or division in the custom function\'s gradient computation. - The implementation should be efficient and clear. Example ```python import torch class SinC(torch.autograd.Function): @staticmethod def forward(ctx, input): # Implement forward pass pass @staticmethod def backward(ctx, grad_output): # Implement backward pass pass # Sample Usage x = torch.tensor([1.0, 0.0, 3.1415], requires_grad=True) y = SinC.apply(x) y.sum().backward() # Expected output: # y = tensor([0.8415, 1.0000, 0.0000], grad_fn=<SinCBackward>) # x.grad = tensor([...]) ``` Notes - The forward method should save tensors and data required for the backward pass using `save_for_backward`. - The backward method should correctly handle the gradient computation for the given function.","solution":"import torch class SinC(torch.autograd.Function): @staticmethod def forward(ctx, input): ctx.save_for_backward(input) output = torch.where(input == 0, torch.tensor(1.0), torch.sin(input) / input) return output @staticmethod def backward(ctx, grad_output): input, = ctx.saved_tensors grad_input = torch.where( input == 0, torch.tensor(0.0), (input * torch.cos(input) - torch.sin(input)) / (input * input) ) return grad_output * grad_input"},{"question":"**Title:** Data Loading and Preprocessing from OpenML with Scikit-Learn **Objective:** The goal of this task is to assess your understanding of loading datasets from OpenML, handling and preprocessing the data, and implementing a basic machine learning model using scikit-learn. **Problem Statement:** You are provided with the task of building a machine learning pipeline that involves the following steps: 1. Loading the \\"Mice Protein Expression\\" dataset from the OpenML repository. 2. Preprocessing the data by converting categorical features to numerical using appropriate scikit-learn tools. 3. Splitting the dataset into training and testing sets. 4. Training a classification model (e.g., RandomForestClassifier) on the training data. 5. Evaluating the model\'s performance on the test data and returning the accuracy score. **Instructions:** 1. **Loading the Dataset:** Use the appropriate function from `sklearn.datasets` to fetch the dataset by name or data_id. 2. **Preprocessing the Data:** - Check if there are any categorical features. - If present, encode them using appropriate methods such as `OneHotEncoder` or `OrdinalEncoder`. - Normalize or scale the features if necessary. 3. **Splitting the Dataset:** Split the dataset into training and testing sets using an 80-20 ratio. 4. **Training the Model:** Train a `RandomForestClassifier` on the training data with default parameters. 5. **Evaluating the Model:** Compute the accuracy of the model on the test dataset. **Constraints:** 1. Your solution should be efficient and make use of scikit-learn functionalities. 2. You should handle any missing values appropriately. 3. Ensure to keep the code organized and well-commented for clarity. **Expected Function Signature:** ```python import numpy as np from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score def load_and_preprocess_data(): Fetches the Mice Protein Expression dataset from OpenML, preprocesses it and returns the preprocessed data. Return: Tuple containing: - X_train: Features for training - X_test: Features for testing - y_train: Labels for training - y_test: Labels for testing # Fetch the dataset # [Your Code Here] # Preprocess the data # [Your Code Here] # Perform train-test split # [Your Code Here] return X_train, X_test, y_train, y_test def train_and_evaluate(): Trains a RandomForestClassifier on the preprocessed data and evaluates its performance on the test data. Return: - accuracy: Model accuracy on the test data # Load and preprocess data X_train, X_test, y_train, y_test = load_and_preprocess_data() # Train RandomForestClassifier model # [Your Code Here] # Evaluate the model and compute accuracy # [Your Code Here] return accuracy # Example Usage: accuracy = train_and_evaluate() print(f\\"Model Accuracy: {accuracy}\\") ``` **Note:** Ensure to import any additional libraries or functions required for the task. Handle any exceptions that might occur during data loading or processing effectively.","solution":"import numpy as np from sklearn.datasets import fetch_openml from sklearn.model_selection import train_test_split from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.impute import SimpleImputer from sklearn.metrics import accuracy_score def load_and_preprocess_data(): Fetches the Mice Protein Expression dataset from OpenML, preprocesses it and returns the preprocessed data. Return: Tuple containing: - X_train: Features for training - X_test: Features for testing - y_train: Labels for training - y_test: Labels for testing # Fetch the dataset data = fetch_openml(name=\\"miceprotein\\", version=4) X = data.data y = data.target # Identify categorical features categorical_features = X.select_dtypes(include=[\'category\', \'object\']).columns numerical_features = X.select_dtypes(include=[\'number\']).columns # Preprocessing for numerical data numeric_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()) ]) # Preprocessing for categorical data categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'most_frequent\')), (\'onehot\', OneHotEncoder(handle_unknown=\'ignore\')) ]) # Combine transformations preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Apply transformations X_preprocessed = preprocessor.fit_transform(X) # Perform train-test split X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42, stratify=y) return X_train, X_test, y_train, y_test def train_and_evaluate(): Trains a RandomForestClassifier on the preprocessed data and evaluates its performance on the test data. Return: - accuracy: Model accuracy on the test data # Load and preprocess data X_train, X_test, y_train, y_test = load_and_preprocess_data() # Train RandomForestClassifier model clf = RandomForestClassifier(random_state=42) clf.fit(X_train, y_train) # Evaluate the model and compute accuracy y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return accuracy # Example Usage: accuracy = train_and_evaluate() print(f\\"Model Accuracy: {accuracy}\\")"},{"question":"Advanced Categorical Plotting with Seaborn Objective: The objective of this task is to assess your understanding of advanced plotting with the Seaborn library. Specifically, you will be required to create a multi-plot figure using various types of categorical plots, customize these plots extensively, and combine multiple plot types in a single figure. Problem Statement: You are given the famous Titanic dataset available through Seaborn\'s `load_dataset` function. Your task is to create a multi-plot figure that provides comprehensive insights into the survival rate based on different categorical variables. Requirements: 1. Load the Titanic dataset using Seaborn. 2. Create a subplot figure that meets the following criteria: - The figure should contain three subplots in a single row. - Each subplot should represent a categorical plot of the survival rate for different classes of passengers: - First subplot: Box plot (`kind=\'box\'`) showing the age distribution of passengers for each class. - Second subplot: Violin plot (`kind=\'violin\'`) showing the age distribution with a split based on gender (`hue=\'sex\'`). - Third subplot: Bar plot (`kind=\'bar\'`) representing the survival rate for different classes and split based on gender (`hue=\'sex\'`). 3. Customize each of the subplots: - Set the title for each subplot relevant to its content. - Adjust the y-axis label to appropriately reflect the data being represented. - Set the ylim for the third subplot to range from 0 to 1. - Change the xtick labels of the second subplot to be displayed at a 45-degree angle. - Ensure that the legend for plots that include `hue` is displayed outside the plot area. 4. Combine an additional strip plot over the existing box plot in the first subplot to highlight individual data points. 5. Ensure the figure is aesthetically pleasing and clearly conveys the necessary information. Expected Input and Output: - **Input**: None - You will generate the plots directly using the Titanic dataset. - **Output**: A multi-plot figure meeting the requirements detailed above. Constraints: - Use the provided dataset and do not modify it. - Adhere strictly to the required customizations for each subplot. Performance Requirements: - The solution should be clear, well-organized, and make effective use of Seaborn\'s plotting capabilities. - The entire figure should be visually appealing and accurately reflect the dataset\'s insights. # Example of how to get started: ```python import seaborn as sns import matplotlib.pyplot as plt # Load dataset df = sns.load_dataset(\\"titanic\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the multi-plot figure g = sns.catplot(...) # Customize subplots ... # Display the figure plt.show() ``` Good luck!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_titanic_plots(): # Load dataset df = sns.load_dataset(\\"titanic\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the figure and subplots fig, axes = plt.subplots(1, 3, figsize=(18, 6)) # First subplot: Box plot showing age distribution of passengers for each class sns.boxplot(x=\\"class\\", y=\\"age\\", data=df, ax=axes[0]) axes[0].set_title(\\"Age Distribution by Passenger Class\\") axes[0].set_ylabel(\\"Age\\") # Add a strip plot to highlight individual data points sns.stripplot(x=\\"class\\", y=\\"age\\", data=df, ax=axes[0], color=\\".25\\") # Second subplot: Violin plot showing age distribution split by gender sns.violinplot(x=\\"class\\", y=\\"age\\", hue=\\"sex\\", data=df, ax=axes[1], split=True) axes[1].set_title(\\"Age Distribution by Class and Gender\\") axes[1].set_ylabel(\\"Age\\") for label in axes[1].get_xticklabels(): label.set_rotation(45) # Place the legend outside the plot axes[1].legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) # Third subplot: Bar plot representing survival rate for different classes split by gender sns.barplot(x=\\"class\\", y=\\"survived\\", hue=\\"sex\\", data=df, ax=axes[2]) axes[2].set_title(\\"Survival Rate by Class and Gender\\") axes[2].set_ylabel(\\"Survival Rate\\") axes[2].set_ylim(0, 1) # Place the legend outside the plot axes[2].legend(loc=\'upper left\', bbox_to_anchor=(1, 1)) # Adjust the layout fig.tight_layout() # Display the figure plt.show() # Call the function to create and display the plots create_titanic_plots()"},{"question":"# **Asynchronous Data Processing with asyncio** **Objective**: Implement a system using asyncio that consists of multiple producer and consumer tasks. Producers generate data and add it to a shared queue, from which consumers concurrently process the data, ensuring thread-safe access and coordination. # **Task Description** You need to implement an async data processing system with the following requirements: 1. **Producers**: - Create multiple producer tasks. - Each producer generates a series of integers and adds them to a shared queue. - The generation rate should be controlled using `await asyncio.sleep()` to simulate time delay between consecutive productions. 2. **Consumers**: - Create multiple consumer tasks. - Each consumer retrieves integers from the shared queue and processes them (e.g., compute the square of the integer). - Ensure thread-safe access to the queue using synchronization primitives (e.g., Locks). # **Specifications** - **Function**: `async def main(num_producers: int, num_consumers: int, num_items: int) -> None` **Parameters**: - `num_producers` (int): Number of producer tasks to create. - `num_consumers` (int): Number of consumer tasks to create. - `num_items` (int): Total number of items each producer generates. **Implementation Details**: 1. **Shared Queue**: - Use `asyncio.Queue` to facilitate data sharing between producers and consumers. 2. **Producers**: - Implement a producer function that generates `num_items` integers in the range of 0 to `num_items - 1`. - Add each integer to the shared queue with a delay of `0.1` seconds between productions. 3. **Consumers**: - Implement a consumer function that retrieves data from the shared queue. - Ensure that processing (e.g., squaring the number) occurs in the consumers. - Print each processed result. 4. **Synchronization**: - Use `asyncio.Lock` or any other suitable synchronization primitive to ensure that the consumers process one item at a time from the queue. 5. **Starting the System**: - Use `asyncio.gather()` to start all producer and consumer tasks. - Ensure the system runs until all producers have produced the required number of items and all items in the queue have been processed. # **Example Usage**: ```python import asyncio async def main(num_producers: int, num_consumers: int, num_items: int) -> None: queue = asyncio.Queue() lock = asyncio.Lock() async def producer(producer_id: int): for i in range(num_items): await asyncio.sleep(0.1) await queue.put(i) print(f\\"Producer {producer_id} produced {i}\\") async def consumer(consumer_id: int): while True: async with lock: item = await queue.get() queue.task_done() processed_item = item**2 print(f\\"Consumer {consumer_id} processed {item} -> {processed_item}\\") producers = [asyncio.create_task(producer(i)) for i in range(num_producers)] consumers = [asyncio.create_task(consumer(i)) for i in range(num_consumers)] await asyncio.gather(*producers) await queue.join() # Ensure all items are processed for c in consumers: c.cancel() # Cancel all consumer tasks when done # Example call asyncio.run(main(num_producers=2, num_consumers=3, num_items=5)) ``` # **Constraints**: - Ensure that all produced items are processed using the asyncio event loop efficiently. - Handle potential exceptions related to asyncio (e.g., `CancelledError`, `TimeoutError`). Good luck!","solution":"import asyncio async def main(num_producers: int, num_consumers: int, num_items: int) -> None: queue = asyncio.Queue() lock = asyncio.Lock() async def producer(producer_id: int): for i in range(num_items): await asyncio.sleep(0.1) await queue.put(i) print(f\\"Producer {producer_id} produced {i}\\") async def consumer(consumer_id: int): while True: item = await queue.get() async with lock: processed_item = item ** 2 print(f\\"Consumer {consumer_id} processed {item} -> {processed_item}\\") queue.task_done() producers = [asyncio.create_task(producer(i)) for i in range(num_producers)] consumers = [asyncio.create_task(consumer(i)) for i in range(num_consumers)] await asyncio.gather(*producers) await queue.join() # Ensure all items are processed for c in consumers: c.cancel() # Cancel all consumer tasks when done # Example call if __name__ == \\"__main__\\": asyncio.run(main(num_producers=2, num_consumers=3, num_items=5))"},{"question":"Objective: Demonstrate your understanding of seaborn by creating a bar plot with specific customizations and theme settings. Problem Statement: You are provided with the following data representing the number of products sold in a store over a week: | Day | Products Sold | |-----------|:-------------:| | Monday | 100 | | Tuesday | 150 | | Wednesday | 120 | | Thursday | 130 | | Friday | 200 | Your task is to create a bar plot with the following specifications: 1. Use the `seaborn` package to initialize the plot. 2. Set a theme with a white grid and pastel color palette. 3. Customize the plot so that the right and top spines are removed. 4. Add appropriate labels for the x-axis, y-axis, and the title of the plot. 5. Display the final plot. Input: No direct input is required, but you will be using the provided dataset to generate the plot. Expected Output: A bar plot showing the number of products sold on each day of the week with the specified customizations. Requirements: - The plot must use seaborn for initialization and theme setting. - The plot must be customized to remove the right and top spines. - Labels and titles should be clear and descriptive. Implementation Constraints: - You must use the `seaborn.set_theme` method to set the initial theme. - Use appropriate seaborn methods and matplotlib functionalities for labels and title. Example Code Template: ```python import seaborn as sns import matplotlib.pyplot as plt # Data for the plot days = [\\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\"] products_sold = [100, 150, 120, 130, 200] # Set the theme and customization parameters sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} # Apply the custom parameters sns.set_theme(style=\\"whitegrid\\", rc=custom_params) # Create the bar plot sns.barplot(x=days, y=products_sold) # Set plot labels and title plt.xlabel(\\"Day of the Week\\") plt.ylabel(\\"Number of Products Sold\\") plt.title(\\"Products Sold Per Day Over a Week\\") # Show the plot plt.show() ``` Please ensure your code adheres to the above specifications and properly displays the plot with the given customizations.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_products_sold(): Creates a bar plot showing the number of products sold on each day of the week with specified customizations using seaborn. # Data for the plot days = [\\"Monday\\", \\"Tuesday\\", \\"Wednesday\\", \\"Thursday\\", \\"Friday\\"] products_sold = [100, 150, 120, 130, 200] # Set the theme and customization parameters sns.set_theme(style=\\"whitegrid\\", palette=\\"pastel\\") custom_params = {\\"axes.spines.right\\": False, \\"axes.spines.top\\": False} # Apply the custom parameters sns.set_theme(style=\\"whitegrid\\", rc=custom_params) # Create the bar plot sns.barplot(x=days, y=products_sold) # Set plot labels and title plt.xlabel(\\"Day of the Week\\") plt.ylabel(\\"Number of Products Sold\\") plt.title(\\"Products Sold Per Day Over a Week\\") # Show the plot plt.show()"},{"question":"**Objective:** Write a Python function that validates and formats a given string to adhere to specific ASCII constraints. **Problem Statement:** Implement a function `format_and_validate_string(s: str) -> str` that takes a string `s` as input and performs the following: 1. Validate the string by ensuring it contains only printable ASCII characters (use `curses.ascii.isprint`). 2. Replace all ASCII control characters (characters with ordinal values 0 to 31) in the string with their corresponding mnemonics from the `curses.ascii.controlnames` list. 3. Raise a `ValueError` if the string contains any non-ASCII characters. **Function Signature:** ```python def format_and_validate_string(s: str) -> str: pass ``` **Input:** - A single string `s` which can contain mixed characters including control characters, printable characters, and possibly non-ASCII characters. **Output:** - A single formatted string where all ASCII control characters are replaced by their mnemonics, and the rest of the string is validated to contain only printable ASCII characters. **Constraints:** - The input string `s` may contain ASCII control characters (ordinal values 0 to 31). - The input string `s` may not contain high-order ASCII characters (ordinal values above 127). **Example:** ```python assert format_and_validate_string(\\"Hellox07World\\") == \\"HelloBELWorld\\" assert format_and_validate_string(\\"Linex0AFeed\\") == \\"LineLF\\" try: format_and_validate_string(\\"Non-ASCII: u00A9\\") except ValueError as e: assert str(e) == \\"Input contains non-ASCII characters.\\" ``` **Notes:** - Use the `curses.ascii.isprint` function to check for printable characters. - Use the `curses.ascii.controlnames` list to get the mnemonic for control characters. - Use the `curses.ascii.isascii` function to check for ASCII characters. **Performance Requirements:** - The function should handle strings up to 10,000 characters efficiently. **Hint:** You might find the `ord()` and `chr()` functions useful for character-to-ordinal and ordinal-to-character conversions.","solution":"import curses.ascii def format_and_validate_string(s: str) -> str: formatted_string = [] for char in s: if not curses.ascii.isascii(char): raise ValueError(\\"Input contains non-ASCII characters.\\") if ord(char) < 32: formatted_string.append(curses.ascii.controlnames[ord(char)]) elif not curses.ascii.isprint(char): raise ValueError(\\"Input contains non-printable ASCII characters.\\") else: formatted_string.append(char) return \'\'.join(formatted_string)"},{"question":"You are given a list of integers of varying sizes. Your task is to perform a series of operations using the `array` module to create an efficient array and then manipulate it according to the following steps: 1. Create an array with type code `\'i\'` (signed integers) using the provided list of integers as the initializer. 2. Append the value `150` to the end of the array. 3. Insert the value `-50` at the start of the array. 4. Count the occurrences of the number `3` in the array. 5. Reverse the array. 6. Convert the array to a list and return this list. Your function should have the following signature: ```python def manipulate_array(data: list) -> tuple: Perform specified array manipulations and return the results. Parameters: - data (list): List of integers to initialize the array. Returns: - tuple: A tuple containing the reversed list and the count of occurrences of `3`. pass ``` # Example: ```python Input: data = [3, 1, 4, 3, 2] Output: ([150, 3, 2, 3, 4, 1, -50], 2) ``` # Constraints: - The input list will contain at least one integer and up to (10^4) integers. - All integers in the input list are within the range of the signed integer type (`\'i\'`). # Explanation: - The initial array created is: `array(\'i\', [3, 1, 4, 3, 2])` - After appending `150`, the array becomes: `array(\'i\', [3, 1, 4, 3, 2, 150])` - After inserting `-50` at the start, the array becomes: `array(\'i\', [-50, 3, 1, 4, 3, 2, 150])` - The count of occurrences of `3` is `2`. - After reversing the array, it becomes: `array(\'i\', [150, 2, 3, 4, 1, 3, -50])` - Converting this array to a list gives: `[150, 2, 3, 4, 1, 3, -50]` Your function should return the reversed list and the count of occurrences of `3` as a tuple.","solution":"from array import array def manipulate_array(data: list) -> tuple: Perform specified array manipulations and return the results. Parameters: - data (list): List of integers to initialize the array. Returns: - tuple: A tuple containing the reversed list and the count of occurrences of `3`. # Step 1: Create an array with type code \'i\' using the provided list of integers arr = array(\'i\', data) # Step 2: Append the value 150 to the end of the array arr.append(150) # Step 3: Insert the value -50 at the start of the array arr.insert(0, -50) # Step 4: Count the occurrences of the number 3 in the array count_3 = arr.count(3) # Step 5: Reverse the array arr.reverse() # Step 6: Convert the array to a list and return this list along with the count of 3 reversed_list = arr.tolist() return (reversed_list, count_3)"},{"question":"Problem Statement You are given the task of creating a PyTorch script that demonstrates the usage of environment variables to control CUDA memory management and debugging. The script should be able to: 1. Set the environment variable `CUDA_VISIBLE_DEVICES` to make only the first GPU available. 2. Disable CUDA memory caching using the `PYTORCH_NO_CUDA_MEMORY_CACHING` environment variable. 3. Force CUDA calls to be synchronous by setting the `CUDA_LAUNCH_BLOCKING` environment variable. Your task is to write a Python script that: - Prints the current GPU device used by PyTorch. - Runs a simple PyTorch CUDA operation (such as tensor addition) to showcase the effects of these environment variables. - Prints the environment variables\' settings to verify they are set correctly. # Input The input will be: - None (You will use environment variables in the script). # Output The script should print: - The GPU device that PyTorch is using. - The result of a CUDA operation. - The current settings of the relevant environment variables. # Constraints - Assume you have a system with multiple GPUs. - You need to ensure that the environment variables take effect before PyTorch initializes the CUDA context. # Example An example printout of the script might be: ``` CUDA_VISIBLE_DEVICES: 0 PYTORCH_NO_CUDA_MEMORY_CACHING: 1 CUDA_LAUNCH_BLOCKING: 1 Current GPU Device: cuda:0 Result of CUDA operation: tensor([...], device=\'cuda:0\') ``` # Note You may use the `os` library to set environment variables in Python. ```python import os import torch # Set the environment variables os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\' os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' os.environ[\'CUDA_LAUNCH_BLOCKING\'] = \'1\' # Print environment variables to verify print(\'CUDA_VISIBLE_DEVICES:\', os.getenv(\'CUDA_VISIBLE_DEVICES\')) print(\'PYTORCH_NO_CUDA_MEMORY_CACHING:\', os.getenv(\'PYTORCH_NO_CUDA_MEMORY_CACHING\')) print(\'CUDA_LAUNCH_BLOCKING:\', os.getenv(\'CUDA_LAUNCH_BLOCKING\')) # Check which GPU PyTorch is using print(\'Current GPU Device:\', torch.cuda.current_device()) # Perform a simple CUDA operation to observe the effects a = torch.randn(1000, 1000, device=\'cuda\') b = torch.randn(1000, 1000, device=\'cuda\') result = a + b print(\'Result of CUDA operation:\', result) ``` Write and execute the Python script described above to demonstrate the usage and effects of these CUDA environment variables in PyTorch.","solution":"import os import torch def setup_cuda_env(): Sets up the required CUDA environment variables and prints their values. # Set the environment variables os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\' os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' os.environ[\'CUDA_LAUNCH_BLOCKING\'] = \'1\' # Print environment variables to verify print(\'CUDA_VISIBLE_DEVICES:\', os.getenv(\'CUDA_VISIBLE_DEVICES\')) print(\'PYTORCH_NO_CUDA_MEMORY_CACHING:\', os.getenv(\'PYTORCH_NO_CUDA_MEMORY_CACHING\')) print(\'CUDA_LAUNCH_BLOCKING:\', os.getenv(\'CUDA_LAUNCH_BLOCKING\')) def demonstrate_cuda_usage(): Demonstrates CUDA usage and prints relevant information. # Check which GPU PyTorch is using if torch.cuda.is_available(): print(\'Current GPU Device:\', torch.cuda.current_device()) else: print(\'CUDA is not available!\') # Perform a simple CUDA operation to observe the effects if torch.cuda.is_available(): a = torch.randn(1000, 1000, device=\'cuda\') b = torch.randn(1000, 1000, device=\'cuda\') result = a + b print(\'Result of CUDA operation:\', result) else: print(\'Skipping CUDA operation as CUDA is not available!\') def main(): setup_cuda_env() demonstrate_cuda_usage() if __name__ == \\"__main__\\": main()"},{"question":"# Custom Object Type Implementation **Objective**: Implement a custom Python object type to demonstrate your understanding of object structures, type definitions, and method implementations in Python. **Task**: 1. Define a new object type `CustomList` that mimics some behaviors of the built-in Python list but with additional features. 2. Your `CustomList` should: - Be initialized with a list of integers. - Support addition (`+`) with other `CustomList` objects, resulting in a new `CustomList` where each element is the sum of the corresponding elements of the two lists. If the lists are of unequal length, the resulting list should be as long as the longer list, with missing elements treated as zero. - Support the `*` operator for scalar multiplication, where each element in the list is multiplied by the given integer scalar. - Implement a method `mean` that returns the mean of the list elements. - Implement a method `variance` that returns the variance of the list elements. **Specifications**: - You should implement the `CustomList` class inside a module. - Define all necessary slots and methods to make the class behave as specified. - Ensure to handle edge cases, such as operations with empty lists, and scalar multiplication by zero. **Input and Output Format**: *Initialization*: ```python cl = CustomList([1, 2, 3]) ``` *Addition*: ```python cl1 = CustomList([1, 2, 3]) cl2 = CustomList([4, 5, 6]) cl3 = cl1 + cl2 # CustomList([5, 7, 9]) ``` *Scalar Multiplication*: ```python cl = CustomList([1, 2, 3]) cl4 = cl * 2 # CustomList([2, 4, 6]) ``` *Mean and Variance*: ```python cl = CustomList([1, 2, 3]) mean = cl.mean() # 2.0 variance = cl.variance() # 1.0 ``` **Constraints**: - Elements of `CustomList` are always integers. - Implement all required functionalities directly within the class definition. **Performance Requirements**: - Ensure that the methods are efficient, with average time complexity for basic operations close to O(n). **Submission**: Submit your Python module containing the `CustomList` class definition and demonstrate its usage with example calls as provided in the prompt.","solution":"class CustomList: def __init__(self, elements): if not all(isinstance(x, int) for x in elements): raise ValueError(\\"All elements must be integers\\") self.elements = elements def __add__(self, other): max_len = max(len(self.elements), len(other.elements)) extended_self = self.elements + [0] * (max_len - len(self.elements)) extended_other = other.elements + [0] * (max_len - len(other.elements)) return CustomList([a + b for a, b in zip(extended_self, extended_other)]) def __mul__(self, scalar): if not isinstance(scalar, int): raise ValueError(\\"Scalar must be an integer\\") return CustomList([x * scalar for x in self.elements]) def mean(self): return sum(self.elements) / len(self.elements) if self.elements else 0 def variance(self): if len(self.elements) < 2: return 0 mean_value = self.mean() variance_value = sum((x - mean_value) ** 2 for x in self.elements) / (len(self.elements) - 1) return variance_value def __repr__(self): return f\\"CustomList({self.elements})\\""},{"question":"**Problem Statement:** You are provided with a dataset containing sales data for a retail store. The data is stored in a pandas DataFrame with the following structure: | OrderID | Product | Category | Quantity | Price | Discount | Profit | Date | |---------|---------|----------|----------|-------|----------|--------|------------| | 1 | ProductA| Category1| 5 | 50 | 10 | 20 | 2022-01-01 | | 2 | ProductB| Category2| 3 | 30 | 5 | 15 | 2022-01-02 | | ... | ... | ... | ... | ... | ... | ... | ... | **Task:** 1. Load the dataset into a pandas DataFrame. [You can use a sample CSV file or create a DataFrame manually for testing]. 2. Perform the following operations: - Extract all orders where the `Quantity` is greater than 2 and `Profit` is greater than 10. - From the above result, get only the `Product`, `Category`, and `Profit` columns. - Sort the results by `Profit` in descending order. - Set a new column `Total_Sales` where the `Total_Sales` is calculated as `(Quantity * Price) - Discount`. 3. Next, find the products that have been ordered more than once. Display the distinct `Product` names. 4. Identify and remove any duplicate entries based on the `OrderID` column. 5. Select random samples from the dataset: - Select 5 random rows from the dataframe. - Select 2 random columns from the dataframe. **Constraints:** - Assume that the dataset fits in memory. - Utilize `pandas` methods and functionalities as much as possible. **Function Signature:** ```python import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: Perform various operations on the sales data. Args: df : pd.DataFrame - DataFrame containing sales data Returns: pd.DataFrame - Processed DataFrame # Your code here ``` **Example Solution:** ```python def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: # 1. Extract orders where Quantity > 2 and Profit > 10 filtered_df = df[(df[\'Quantity\'] > 2) & (df[\'Profit\'] > 10)] # 2. Get Product, Category and Profit columns and sort by Profit descending result_df = filtered_df[[\'Product\', \'Category\', \'Profit\']].sort_values(by=\'Profit\', ascending=False) # 3. Set new column Total_Sales df[\'Total_Sales\'] = (df[\'Quantity\'] * df[\'Price\']) - df[\'Discount\'] # 4. Find products ordered more than once product_counts = df[\'Product\'].value_counts() multiple_orders = product_counts[product_counts > 1].index.tolist() # Display distinct product names with multiple orders print(\\"Products ordered more than once:\\", multiple_orders) # 5. Remove duplicate entries based on OrderID df = df.drop_duplicates(subset=[\'OrderID\']) # 6. Select 5 random rows and 2 random columns random_rows = df.sample(n=5) random_columns = df.sample(n=2, axis=1) print(\\"Random Rows:n\\", random_rows) print(\\"Random Columns:n\\", random_columns) return result_df # Example usage: # df = pd.read_csv(\'sales_data.csv\') # Load the dataset # result = process_sales_data(df) # print(result) ``` **Test Cases:** - The function should handle edge cases like empty DataFrame, DataFrame with no duplicates, DataFrame with all values passing/exceeding the condition, etc. - The function should maintain the original structure of the DataFrame whenever possible.","solution":"import pandas as pd def process_sales_data(df: pd.DataFrame) -> pd.DataFrame: # 1. Extract orders where Quantity > 2 and Profit > 10 filtered_df = df[(df[\'Quantity\'] > 2) & (df[\'Profit\'] > 10)] # 2. Get Product, Category and Profit columns and sort by Profit descending result_df = filtered_df[[\'Product\', \'Category\', \'Profit\']].sort_values(by=\'Profit\', ascending=False) # 3. Set new column Total_Sales df[\'Total_Sales\'] = (df[\'Quantity\'] * df[\'Price\']) - df[\'Discount\'] # 4. Find products ordered more than once product_counts = df[\'Product\'].value_counts() multiple_orders = product_counts[product_counts > 1].index.tolist() # Display distinct product names with multiple orders print(\\"Products ordered more than once:\\", multiple_orders) # 5. Remove duplicate entries based on OrderID df = df.drop_duplicates(subset=[\'OrderID\']) # 6. Select 5 random rows and 2 random columns random_rows = df.sample(n=5) random_columns = df.sample(n=2, axis=1) print(\\"Random Rows:n\\", random_rows) print(\\"Random Columns:n\\", random_columns) return result_df # Example DataFrame for testing (not part of the final solution, but useful for understanding structure) data = { \'OrderID\': [1, 2, 3, 4, 5, 6, 7, 8], \'Product\': [\'ProductA\', \'ProductB\', \'ProductC\', \'ProductD\', \'ProductE\', \'ProductF\', \'ProductA\', \'ProductG\'], \'Category\': [\'Category1\', \'Category2\', \'Category1\', \'Category3\', \'Category2\', \'Category1\', \'Category1\', \'Category4\'], \'Quantity\': [5, 3, 1, 5, 8, 2, 5, 6], \'Price\': [50, 30, 20, 15, 35, 40, 50, 25], \'Discount\': [10, 5, 2, 1, 5, 6, 10, 3], \'Profit\': [20, 15, 5, 25, 30, 12, 20, 22], \'Date\': [\'2022-01-01\', \'2022-01-02\', \'2022-01-03\', \'2022-01-04\', \'2022-01-05\', \'2022-01-06\', \'2022-01-07\', \'2022-01-08\'] } df = pd.DataFrame(data) result = process_sales_data(df) print(result)"},{"question":"Objective Write a Python function `search_files(dir_path, pattern)` that searches for files in a given directory (`dir_path`) which match a specified pattern (`pattern`) and returns a list of matched files. Function Signature ```python def search_files(dir_path: str, pattern: str) -> list: ``` Input - `dir_path` (str): The path of the directory where the files need to be searched. - `pattern` (str): The Unix shell-style wildcard pattern for matching file names. Output - returns a list of file names (with their full paths) that match the given pattern. Constraints - The function should handle sub-directories, i.e., it should perform a recursive search. - The function should handle case-sensitive and case-insensitive matches based on a third optional parameter `case_sensitive`. Requirements - Utilize the `fnmatch` module for pattern matching. - Ensure performance is optimized, i.e., the solution should not re-evaluate the matching pattern unnecessarily. Example ```python # Suppose the following files are in the directory \\"./test_dir\\" # - readme.txt # - example.TXT # - sample.pdf # - sub_dir/example2.TXT # - sub_dir/note.PDF dir_path = \\"./test_dir\\" pattern = \\"*.txt\\" # case_insensitive match print(search_files(dir_path, pattern, case_sensitive=False)) # Expected output: # [\'./test_dir/readme.txt\', \'./test_dir/example.TXT\', \'./test_dir/sub_dir/example2.TXT\'] # case_sensitive match print(search_files(dir_path, pattern, case_sensitive=True)) # Expected output: # [\'./test_dir/readme.txt\'] ``` Hint Use `os.listdir()` for listing files in a directory, `os.path.isdir()` to check sub-directories, and `os.walk()` for recursive directory traversal. Note - If `case_sensitive` is not given, default to `False`. - Validate that `dir_path` is a valid directory before proceeding with the search.","solution":"import os import fnmatch def search_files(dir_path: str, pattern: str, case_sensitive: bool=False) -> list: Searches for files in the given directory and its subdirectories that match the specified pattern. :param dir_path: Directory path where the search should be performed. :param pattern: Unix shell-style wildcard pattern for matching file names. :param case_sensitive: Boolean indicating whether the pattern matching should be case-sensitive. :return: A list of file names (with their full paths) that match the given pattern. if not os.path.isdir(dir_path): raise ValueError(f\\"The provided directory path \'{dir_path}\' is not valid or does not exist.\\") matching_files = [] for root, dirs, files in os.walk(dir_path): for filename in files: if case_sensitive: if fnmatch.fnmatchcase(filename, pattern): matching_files.append(os.path.join(root, filename)) else: if fnmatch.fnmatch(filename.lower(), pattern.lower()): matching_files.append(os.path.join(root, filename)) return matching_files"},{"question":"**Question:** You are given an XML document containing information about a list of books, including each book\'s title, author, and price. Your task is to parse this XML document and return the titles of the books priced over a given value. Write a Python function `filter_books_by_price(xml_string: str, price_threshold: float) -> List[str]` that accepts: - `xml_string`: a string representation of the XML content. - `price_threshold`: a float representing the price threshold. Your function should return a list of titles of books whose prices are above the price threshold. # Input Format: - `xml_string`: a string containing the XML data with the following structure: ```xml <library> <book> <title>Book Title 1</title> <author>Author 1</author> <price>45.0</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <price>55.0</price> </book> ... </library> ``` - `price_threshold`: a float value representing the threshold above which book titles should be included. # Output Format: - A list of strings, where each string is the title of a book priced above the given threshold. # Constraints: - The XML string will have a valid structure conforming to the described format. - All prices in the XML will be valid floating-point numbers. # Example: ```python xml_data = \'\'\' <library> <book> <title>Book Title 1</title> <author>Author 1</author> <price>45.0</price> </book> <book> <title>Book Title 2</title> <author>Author 2</author> <price>55.0</price> </book> <book> <title>Book Title 3</title> <author>Author 3</author> <price>60.0</price> </book> </library> \'\'\' print(filter_books_by_price(xml_data, 50.0)) # Output: [\'Book Title 2\', \'Book Title 3\'] ``` # Notes: - You should use the `xml.dom.pulldom` module to parse the XML content and handle events. - Event handling is crucial to dynamically evaluate and expand node content where necessary.","solution":"from typing import List from xml.dom import minidom def filter_books_by_price(xml_string: str, price_threshold: float) -> List[str]: Filters and returns the titles of books priced over the given price threshold from the XML string. Parameters: xml_string (str): A string representation of the XML containing book data. price_threshold (float): The price threshold to filter books. Returns: List[str]: A list of book titles priced above the given threshold. # Parse the XML string xmldoc = minidom.parseString(xml_string) # Get all book elements books = xmldoc.getElementsByTagName(\'book\') titles_above_threshold = [] # Iterate through each book and check price for book in books: price_element = book.getElementsByTagName(\'price\')[0] price = float(price_element.firstChild.data) if price > price_threshold: title_element = book.getElementsByTagName(\'title\')[0] title = title_element.firstChild.data titles_above_threshold.append(title) return titles_above_threshold"},{"question":"Coding Assessment Question # Objective This task will assess your understanding and ability to utilize seaborn for creating various plots, particularly focusing on count plots, segregating by multiple variables, and normalizing the data. # Problem Statement You are provided with the Titanic dataset from seaborn. Your task is to complete a function that plots the normalized count of passengers across different classes, segmented by their survival status and their gender. Additionally, you need to customize the plot by adding appropriate titles and labels. # Function Signature ```python import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): Loads the Titanic dataset, and plots the normalized count of passengers across different classes, segmented by survival status and gender. The function should: - Load the Titanic dataset. - Plot a count plot with: - \'x\' as the passenger class. - \'hue\' as the survival status. - Further \'split\' by gender ensuring separate bars for males and females. - Use normalized counts (percentages) instead of raw counts. - Customize the plot with: - A main title \\"Normalized Count of Titanic Passengers by Class, Survival, and Gender\\". - X-axis label as \\"Passenger Class\\". - Y-axis label as \\"Percentage of Passengers\\". - Legend title as \\"Survived\\". Constraints: - You can use only seaborn and matplotlib for plotting. - Use default seaborn settings for plot aesthetics. The function does not return anything; it only displays the plot. pass ``` # Example Output Your final plot should be similar to the following: - A count plot with the x-axis representing the class (\'First\', \'Second\', \'Third\'). - The hue representing survival status (0 or 1). - Separate bars within each class category for males and females. - Count data normalized to show percentages. - Customized plot with the specified titles and labels. Your plot is expected to look well-presented and should convey the required information clearly. # Additional Information - The Titanic dataset can be loaded using `sns.load_dataset(\\"titanic\\")`. - Use `sns.countplot`, and refer to seaborn’s documentation if needed to understand its parameters. - You might need to use matplotlib’s `pyplot` to add custom titles and labels.","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_titanic_data(): Loads the Titanic dataset, and plots the normalized count of passengers across different classes, segmented by survival status and gender. The function should: - Load the Titanic dataset. - Plot a count plot with: - \'x\' as the passenger class. - \'hue\' as the survival status. - Further \'split\' by gender ensuring separate bars for males and females. - Use normalized counts (percentages) instead of raw counts. - Customize the plot with: - A main title \\"Normalized Count of Titanic Passengers by Class, Survival, and Gender\\". - X-axis label as \\"Passenger Class\\". - Y-axis label as \\"Percentage of Passengers\\". - Legend title as \\"Survived\\". Constraints: - You can use only seaborn and matplotlib for plotting. - Use default seaborn settings for plot aesthetics. The function does not return anything; it only displays the plot. # Load the Titanic dataset df = sns.load_dataset(\\"titanic\\") # Calculate the normalized counts normalized_df = df.groupby([\'pclass\', \'sex\', \'survived\']).size().reset_index(name=\'count\') # Calculate percentage total_counts = df.groupby([\'pclass\', \'sex\']).size().reset_index(name=\'total\') normalized_df = normalized_df.merge(total_counts, on=[\'pclass\', \'sex\']) normalized_df[\'percentage\'] = (normalized_df[\'count\'] / normalized_df[\'total\']) * 100 # Initialize the matplotlib figure plt.figure(figsize=(10, 6)) # Create the count plot sns.barplot(x=\'pclass\', y=\'percentage\', hue=\'survived\', data=normalized_df, ci=None, palette=\'Set1\') # Add titles and labels plt.title(\'Normalized Count of Titanic Passengers by Class, Survival, and Gender\') plt.xlabel(\'Passenger Class\') plt.ylabel(\'Percentage of Passengers\') plt.legend(title=\'Survived\') # Show the plot plt.show()"},{"question":"**Coding Challenge: Custom Command-Line Application Using Argparse** **Objective:** Create a command-line application to simulate a basic calculator with additional functionalities using the `argparse` module in Python. The calculator should be able to perform addition, subtraction, multiplication, and division operations. It should also allow the user to set verbose output levels and handle mutually exclusive options for rounding the result. **Requirements:** 1. **Positional Arguments**: - `num1` (`float`): The first number (base) for the operation. - `num2` (`float`): The second number (operand) for the operation. - `operation` (`str`): The operation to perform, one of `add`, `subtract`, `multiply`, or `divide`. 2. **Optional Arguments**: - `--verbose` / `-v`: Increase output verbosity (can be specified multiple times to increase verbosity level). - `--round` / `-r`: Round the result to the nearest integer. - `--precision` / `-p` (`int`): Set the number of decimal places to round to when using `--round`. 3. **Mutually Exclusive Arguments**: - `--no-division-by-zero-check` / `-ndzc`: Disable the check for division by zero. **Constraints:** - If `operation` is `divide` and `num2` is `0`, the program should print an error message unless `--no-division-by-zero-check` is specified. - The verbosity levels should work as follows: - **Level 0:** Output only the result. - **Level 1:** Output the operation in progress. - **Level 2:** Output the full calculation in the format: `\\"num1 operation num2 = result\\"`. **Example Usage:** ``` python calc.py 5 3 add 8.0 python calc.py 5 3 divide -v Performing: 5.0 / 3.0 1.6666666666666667 python calc.py 5 3 subtract -vv --round -p 2 5.0 - 3.0 = 2.0 Result (rounded to 2 decimal places): 2.00 python calc.py 10 0 divide -v -ndzc Warning: Division by zero check disabled! Performing: 10.0 / 0.0 inf ``` **Implementation:** ```python import argparse def calculate(args): if args.operation == \'add\': result = args.num1 + args.num2 operation = \'+\' elif args.operation == \'subtract\': result = args.num1 - args.num2 operation = \'-\' elif args.operation == \'multiply\': result = args.num1 * args.num2 operation = \'*\' elif args.operation == \'divide\': if args.num2 == 0 and not args.no_division_by_zero_check: print(\\"Error: Division by zero is not allowed.\\") return elif args.num2 == 0: print(\\"Warning: Division by zero check disabled!\\") result = float(\'inf\') else: result = args.num1 / args.num2 operation = \'/\' output = \\"\\" if args.verbosity >= 2: output += f\\"{args.num1} {operation} {args.num2} = {result}n\\" if args.verbosity >= 1: output += f\\"Performing: {args.num1} {operation} {args.num2}n\\" if args.round: if args.precision: result = round(result, args.precision) output += f\\"Result (rounded to {args.precision} decimal places): {result}n\\" else: result = round(result) output += f\\"Result (rounded): {result}n\\" else: output += f\\"{result}\\" print(output) def main(): parser = argparse.ArgumentParser(description=\\"Basic command-line calculator.\\") parser.add_argument(\\"num1\\", type=float, help=\\"First number (base).\\") parser.add_argument(\\"num2\\", type=float, help=\\"Second number (operand).\\") parser.add_argument(\\"operation\\", choices=[\'add\', \'subtract\', \'multiply\', \'divide\'], help=\\"Operation to perform.\\") parser.add_argument(\\"-v\\", \\"--verbosity\\", action=\\"count\\", default=0, help=\\"Increase output verbosity.\\") parser.add_argument(\\"-r\\", \\"--round\\", action=\\"store_true\\", help=\\"Round the result.\\") parser.add_argument(\\"-p\\", \\"--precision\\", type=int, help=\\"Number of decimal places to round to (requires --round).\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-ndzc\\", \\"--no-division-by-zero-check\\", action=\\"store_true\\", help=\\"Disable division by zero check.\\") args = parser.parse_args() calculate(args) if __name__ == \\"__main__\\": main() ``` **Explanation:** - The `main` function sets up an `argparse.ArgumentParser` to handle the command-line arguments. - The `calculate` function performs the specified arithmetic operation and handles the verbosity levels and rounding as requested. - The program handles errors, such as division by zero, unless the user explicitly disables this check.","solution":"import argparse def calculate(args): if args.operation == \'add\': result = args.num1 + args.num2 operation = \'+\' elif args.operation == \'subtract\': result = args.num1 - args.num2 operation = \'-\' elif args.operation == \'multiply\': result = args.num1 * args.num2 operation = \'*\' elif args.operation == \'divide\': if args.num2 == 0 and not args.no_division_by_zero_check: print(\\"Error: Division by zero is not allowed.\\") return elif args.num2 == 0: print(\\"Warning: Division by zero check disabled!\\") result = float(\'inf\') else: result = args.num1 / args.num2 operation = \'/\' output = \\"\\" if args.verbosity >= 2: output += f\\"{args.num1} {operation} {args.num2} = {result}n\\" if args.verbosity >= 1: output += f\\"Performing: {args.num1} {operation} {args.num2}n\\" if args.round: if args.precision is not None: result = round(result, args.precision) output += f\\"Result (rounded to {args.precision} decimal places): {result}n\\" else: result = round(result) output += f\\"Result (rounded): {result}n\\" else: output += f\\"{result}\\" print(output) def main(): parser = argparse.ArgumentParser(description=\\"Basic command-line calculator.\\") parser.add_argument(\\"num1\\", type=float, help=\\"First number (base).\\") parser.add_argument(\\"num2\\", type=float, help=\\"Second number (operand).\\") parser.add_argument(\\"operation\\", choices=[\'add\', \'subtract\', \'multiply\', \'divide\'], help=\\"Operation to perform.\\") parser.add_argument(\\"-v\\", \\"--verbosity\\", action=\\"count\\", default=0, help=\\"Increase output verbosity.\\") parser.add_argument(\\"-r\\", \\"--round\\", action=\\"store_true\\", help=\\"Round the result.\\") parser.add_argument(\\"-p\\", \\"--precision\\", type=int, help=\\"Number of decimal places to round to (requires --round).\\") group = parser.add_mutually_exclusive_group() group.add_argument(\\"-ndzc\\", \\"--no-division-by-zero-check\\", action=\\"store_true\\", help=\\"Disable division by zero check.\\") args = parser.parse_args() calculate(args) if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implementing `setup.cfg` Parser and Modifier You are tasked to create a utility script to parse and modify `setup.cfg` files commonly used in Python package distributions. The script should be able to: 1. Read a `setup.cfg` file and parse it into an easily accessible structure. 2. Retrieve the value of a specified option for a given command. 3. Modify the value of a specified option for a given command and save the changes back to the `setup.cfg` file. Requirements 1. Implement a class `SetupConfig` with the following methods: - `__init__(self, filepath: str)`: Initializes the object and reads the `setup.cfg` file from the given `filepath`. - `get_option(self, command: str, option: str) -> str`: Retrieves the value of the specified `option` under the given `command`. Returns `None` if the option is not found. - `set_option(self, command: str, option: str, value: str)`: Sets the value for the specified `option` under the given `command`. If the command or option does not exist, it should add them. Updates the `setup.cfg` file with the changes. 2. Considerations: - Handle multi-line options correctly. - Perserve comments and original formatting as much as possible when writing updates back to the file. - Ensure the class handles edge cases, such as files or options that do not exist. Input Format - `SetupConfig` class instantiation with a file path to a `setup.cfg` file. - Calling methods `get_option` and `set_option` with string parameters as described. Output Format - For `get_option`, output the value of the option as a string or `None` if the option or command does not exist. - For `set_option`, no output is expected but the changes should be reflected in the `setup.cfg` file. Example Given a `setup.cfg` file: ``` [build_ext] inplace=1 [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Example code: ```python config = SetupConfig(\'setup.cfg\') print(config.get_option(\'bdist_rpm\', \'release\')) # Outputs: \'1\' config.set_option(\'build_ext\', \'inplace\', \'0\') print(config.get_option(\'build_ext\', \'inplace\')) # Outputs: \'0\' ``` The modified `setup.cfg` file content should be: ``` [build_ext] inplace=0 [bdist_rpm] release = 1 packager = Greg Ward <gward@python.net> doc_files = CHANGES.txt README.txt USAGE.txt doc/ examples/ ``` Implement the `SetupConfig` class in Python to meet these requirements.","solution":"import configparser class SetupConfig: def __init__(self, filepath: str): self.filepath = filepath self.config = configparser.ConfigParser() self.config.optionxform = str # Preserve case sensitivity self.config.read(filepath) def get_option(self, command: str, option: str) -> str: if self.config.has_section(command): if self.config.has_option(command, option): return self.config.get(command, option) return None def set_option(self, command: str, option: str, value: str): if not self.config.has_section(command): self.config.add_section(command) self.config.set(command, option, value) with open(self.filepath, \'w\') as configfile: self.config.write(configfile)"},{"question":"You are tasked with analyzing a dataset containing information about diamonds and visualizing the distribution of their attributes using Seaborn. Specifically, your goal is to create one figure that combines various types of distribution visualizations provided by Seaborn. Problem Statement Using the `diamonds` dataset from Seaborn, perform the following tasks: 1. **Load the Dataset**: Load the dataset named `\\"diamonds\\"` provided by Seaborn. 2. **Univariate Distribution**: - Create a histogram of the `carat` attribute. Customize the bin size to be 0.05. - Overlay a `kdeplot` on the histogram to show a smoothed curve representation of the distribution. 3. **Bivariate Distribution**: - Create a joint plot of `carat` against `price` using a hexbin plot. 4. **Conditional Distributions**: - Create histograms of the `carat` attribute, separated by the `color` attribute using the `hue` parameter. Use a step plot element to enhance clarity. 5. **Visualization of `color` and `clarity`**: - Create a heatmap showing the counts of diamonds for each combination of `color` and `clarity`. Implement a function `visualize_diamonds` that performs these tasks. # Input - None. The function should internally load the `diamonds` dataset from Seaborn. # Output - Your function should save the figures as \\"univariate_distribution.png\\", \\"bivariate_distribution.png\\", \\"conditional_distribution.png\\", and \\"heatmap.png\\" in the current directory. # Constraints - Use Seaborn for visualizations. - Read the dataset directly from Seaborn\'s repository using `sns.load_dataset`. # Example Although no direct input or output is involved, your function should implement the visualizations as illustrated in the following example: ```python def visualize_diamonds(): import seaborn as sns import matplotlib.pyplot as plt # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # 1. Univariate distribution with histogram and KDE plt.figure(figsize=(10, 6)) sns.histplot(diamonds, x=\\"carat\\", binwidth=0.05, kde=True) plt.title(\'Histogram and KDE of Carat\') plt.savefig(\'univariate_distribution.png\') plt.close() # 2. Bivariate distribution with hexbin plot plt.figure(figsize=(10, 6)) sns.jointplot(data=diamonds, x=\\"carat\\", y=\\"price\\", kind=\\"hex\\") plt.title(\'Hexbin Plot of Carat and Price\') plt.savefig(\'bivariate_distribution.png\') plt.close() # 3. Conditional distributions by \'color\' plt.figure(figsize=(10, 6)) sns.histplot(diamonds, x=\\"carat\\", hue=\\"color\\", element=\\"step\\") plt.title(\'Histogram of Carat by Color\') plt.savefig(\'conditional_distribution.png\') plt.close() # 4. Heatmap of color vs clarity counts plt.figure(figsize=(10, 6)) clarity_color_count = diamonds.groupby([\'color\', \'clarity\']).size().unstack() sns.heatmap(clarity_color_count, annot=True, fmt=\\"d\\") plt.title(\'Heatmap of Color and Clarity\') plt.savefig(\'heatmap.png\') plt.close() ``` Make sure to test your function to ensure that each visualization task is implemented correctly.","solution":"def visualize_diamonds(): import seaborn as sns import matplotlib.pyplot as plt # Load the diamonds dataset diamonds = sns.load_dataset(\\"diamonds\\") # 1. Univariate distribution with histogram and KDE plt.figure(figsize=(10, 6)) sns.histplot(diamonds, x=\\"carat\\", binwidth=0.05, kde=True) plt.title(\'Histogram and KDE of Carat\') plt.savefig(\'univariate_distribution.png\') plt.close() # 2. Bivariate distribution with hexbin plot g = sns.jointplot(data=diamonds, x=\\"carat\\", y=\\"price\\", kind=\\"hex\\", height=8) g.fig.suptitle(\'Hexbin Plot of Carat and Price\', y=1.03) g.savefig(\'bivariate_distribution.png\') plt.close() # 3. Conditional distributions by \'color\' plt.figure(figsize=(10, 6)) sns.histplot(diamonds, x=\\"carat\\", hue=\\"color\\", element=\\"step\\") plt.title(\'Histogram of Carat by Color\') plt.savefig(\'conditional_distribution.png\') plt.close() # 4. Heatmap of color vs clarity counts plt.figure(figsize=(10, 6)) clarity_color_count = diamonds.groupby([\'color\', \'clarity\']).size().unstack().fillna(0) sns.heatmap(clarity_color_count, annot=True, fmt=\\"d\\", cmap=\\"YlGnBu\\") plt.title(\'Heatmap of Color and Clarity\') plt.savefig(\'heatmap.png\') plt.close()"},{"question":"# Question Using the `traceback` module\'s functionalities, design a Python function `log_exception` that captures and logs detailed information about any uncaught exceptions occurring in a given block of code. The function should take another function and its arguments as input, execute it, and log any exceptions raised to a specified file. The log should include a full traceback, exception type, exception message, and any chained exceptions. Input - A function `func` to execute. - Positional arguments to be passed to `func`. - Keyword arguments to be passed to `func`. - A file path `logfile` where the log should be saved. Output - Return the result of the function execution if no exception occurs. - If an exception occurs, write the traceback details to the specified log file and raise the exception again. Constraints - You must use the `traceback` module for logging the exception details. - Ensure the log file is correctly written even if nested exceptions occur. - The function should handle any type of exception, including `SyntaxError`. Example ```python def example_func(a, b): return a / b try: log_exception(example_func, 10, 0, logfile=\\"error.log\\") except ZeroDivisionError: print(\\"Caught a zero division error!\\") ``` In the above example, the `log_exception` function should capture and log the `ZeroDivisionError` caused by attempting to divide by zero, then re-raise the exception. The file `error.log` should contain detailed information about the exception and its traceback. Implementation Notes - Use `traceback.format_exception`, `traceback.print_exception`, and other relevant functions for extracting and formatting the traceback details. - Ensure the file I/O operations are handled properly (e.g., using context managers). - Consider edge cases such as invalid file paths and ensure appropriate error handling/logging is in place.","solution":"import traceback def log_exception(func, *args, logfile, **kwargs): Executes the given function with provided arguments and logs any exceptions to the specified file. Parameters: func (callable): The function to execute. *args: Positional arguments to pass to the function. logfile (str): Path to the logfile where exception details should be logged. **kwargs: Keyword arguments to pass to the function. Returns: Any: The result of the function execution if no exception occurs. Raises: Exception: Re-raises any exception that occurs during function execution. try: return func(*args, **kwargs) except Exception as e: with open(logfile, \'w\') as f: traceback.print_exception(type(e), e, e.__traceback__, file=f) raise"},{"question":"Objective: To assess your understanding of the `importlib` module in Python, and your ability to utilize its functionalities to dynamically import modules and validate their content. # Problem Statement: You are asked to write a utility function `import_and_check_function` that dynamically imports a module by its name and checks if a specific function exists within that module. # Function Signature: ```python def import_and_check_function(module_name: str, function_name: str) -> bool: Dynamically imports a module and checks if a specific function exists in the module. Parameters: - module_name (str): The name of the module to import. - function_name (str): The name of the function to check for within the module. Returns: - bool: True if the function exists within the module, False otherwise. ``` # Requirements: 1. **Module Importation**: Use the `importlib` package for importing the module. 2. **Function Check**: After importing the module, check if the given function name exists in the module. 3. **Return Value**: Return `True` if the function exists in the module, otherwise `False`. 4. **Error Handling**: Ensure appropriate error handling for cases where the module cannot be imported. # Constraints: - Do not use the basic `import` statement directly, utilize `importlib` functionalities only. - Assume only valid Python module names will be provided as input. - The function should handle both absolute and relative module imports. # Example: ```python # Example 1 print(import_and_check_function(\'math\', \'sqrt\')) # Output: True # Example 2 print(import_and_check_function(\'math\', \'fake_function\')) # Output: False # Example 3 print(import_and_check_function(\'non_existent_module\', \'some_function\')) # Output: False ``` # Additional Notes: - You can use `importlib.util.find_spec` to check if a module exists without importing it if needed. - You can use `importlib.import_module` to import the module dynamically. - Make sure to handle exceptions and edge cases gracefully.","solution":"import importlib def import_and_check_function(module_name: str, function_name: str) -> bool: Dynamically imports a module and checks if a specific function exists in the module. Parameters: - module_name (str): The name of the module to import. - function_name (str): The name of the function to check for within the module. Returns: - bool: True if the function exists within the module, False otherwise. try: module = importlib.import_module(module_name) except ImportError: return False return hasattr(module, function_name)"},{"question":"**Title:** Implement a Custom Mixture Distribution in PyTorch **Objective:** To test your understanding of PyTorch\'s `torch.distributions` package by implementing a custom mixture distribution that combines two different distribution types and performs various statistical operations. **Problem Statement:** You are required to implement a custom mixture distribution class using PyTorch that combines a `Normal` distribution and an `Exponential` distribution. The mixture distribution should allow you to sample from the combined distribution and compute log probabilities for given data points. **Requirements:** 1. **MixtureNormalExponential Class:** Implement a class `MixtureNormalExponential` that combines a `Normal` distribution and an `Exponential` distribution. 2. **Constructor (`__init__`):** - Takes three parameters: - `normal_params` (tuple): Mean and standard deviation for the `Normal` distribution. - `exponential_rate` (float): Rate for the `Exponential` distribution. - `mixture_weight` (float): Weight for the `Normal` distribution in the mixture. The weight for the `Exponential` distribution will be `1 - mixture_weight`. 3. **Sample Method (`sample`):** - Generates `n` samples from the mixture distribution. - Returns a tensor of shape `(n,)` containing the samples. 4. **Log Probability Method (`log_prob`):** - Takes a tensor `x` as input and computes the log probability of each element under the mixture distribution. - Returns a tensor of log probabilities with the same shape as `x`. **Implementation Details:** - You should use the `torch.distributions.Normal` and `torch.distributions.Exponential` classes for creating respective distributions. - Randomly choose a distribution to sample from based on the `mixture_weight`. **Function signatures:** ```python import torch import torch.distributions as dist class MixtureNormalExponential: def __init__(self, normal_params, exponential_rate, mixture_weight): self.normal = dist.Normal(*normal_params) self.exponential = dist.Exponential(exponential_rate) self.mixture_weight = mixture_weight def sample(self, n): # Implement the sampling logic here pass def log_prob(self, x): # Implement the log probability calculation here pass ``` **Example Usage:** ```python # Creating a mixture distribution with Normal(mean=0, std=1) and Exponential(rate=1) with a mixture weight of 0.7 mixture_dist = MixtureNormalExponential((0, 1), 1, 0.7) # Generating 10 samples from the mixture distribution samples = mixture_dist.sample(10) print(samples) # Computing log probabilities for a given tensor log_probs = mixture_dist.log_prob(torch.tensor([0.5, 1.0, 2.0])) print(log_probs) ``` **Constraints:** - The mixture weight should be a float between 0 and 1. - The samples and log probabilities should be computed using PyTorch operations. **Performance Requirements:** - The methods should be efficient and capable of handling large tensors (e.g., generating 10000 samples or computing log probabilities for 10000 data points) within a reasonable time frame. Implement the `MixtureNormalExponential` class and submit your solution with a brief explanation of your approach.","solution":"import torch import torch.distributions as dist class MixtureNormalExponential: def __init__(self, normal_params, exponential_rate, mixture_weight): Initializes the MixtureNormalExponential distribution with the given parameters. Args: - normal_params (tuple): Mean and standard deviation for the Normal distribution. - exponential_rate (float): Rate for the Exponential distribution. - mixture_weight (float): Weight for the Normal distribution in the mixture. The weight for the Exponential distribution will be 1 - mixture_weight. # Validate the mixture_weight if not (0 <= mixture_weight <= 1): raise ValueError(\\"mixture_weight must be between 0 and 1\\") self.normal = dist.Normal(*normal_params) self.exponential = dist.Exponential(exponential_rate) self.mixture_weight = mixture_weight def sample(self, n): Generates `n` samples from the mixture distribution. Args: - n (int): Number of samples to generate. Returns: - Tensor of shape `(n,)` containing the samples. # Randomly choose from which distribution to sample based on the mixture weight choose_dist = torch.bernoulli(torch.full((n,), self.mixture_weight)) normal_samples = self.normal.sample((n,)) exponential_samples = self.exponential.sample((n,)) samples = torch.where(choose_dist == 1, normal_samples, exponential_samples) return samples def log_prob(self, x): Computes the log probability of each element in `x` under the mixture distribution. Args: - x (Tensor): Tensor of data points. Returns: - Tensor of log probabilities with the same shape as `x`. log_prob_normal = self.normal.log_prob(x) log_prob_exponential = self.exponential.log_prob(x) # Using log-sum-exp trick for numerical stability log_prob = torch.log(self.mixture_weight * torch.exp(log_prob_normal) + (1 - self.mixture_weight) * torch.exp(log_prob_exponential)) return log_prob"},{"question":"# Question: You are given a series of tasks to load different datasets, preprocess them, and perform a simple analysis. Follow the instructions below to complete the tasks. Task 1: Load the `miceprotein` dataset from OpenML (dataset id: `40966`). Display the following: - The shape of the data (features) and target arrays. - The unique classes available in the target array. Task 2: Load a dataset in svmlight/libsvm format from the provided path `\\"/path/to/train_dataset.txt\\"`. Display the following: - The shape of the feature matrix `X_train` and the target vector `y_train`. Task 3: Load the sample image `china.jpg` from the scikit-learn datasets. Convert the pixel values from `uint8` to floating point representation and scale them to the range [0, 1]. Display the following: - The shape of the image array. - The first 5 pixels values in the image (as an array). Task 4: For the `miceprotein` dataset loaded in Task 1, perform a simple exploratory data analysis: - Calculate and display the mean and standard deviation for each feature, grouped by the target class. # Implementation: Your implementation should consist of four functions, one for each task: ```python def load_miceprotein_data(): Load the miceprotein dataset from OpenML (dataset id: 40966). Returns: mice_data_shape (tuple): Shape of the data array. mice_target_shape (tuple): Shape of the target array. unique_classes (array): Unique classes in the target array. # Your code here pass def load_svmlight_data(file_path): Load dataset from svmlight/libsvm formatted file. Parameters: file_path (string): Path to the svmlight/libsvm formatted file. Returns: X_shape (tuple): Shape of the feature matrix X. y_shape (tuple): Shape of the target vector y. # Your code here pass def load_and_preprocess_image(): Load and preprocess the sample image \'china.jpg\'. Returns: image_shape (tuple): Shape of the image array. first_5_pixels (array): First 5 pixel values of the image. # Your code here pass def exploratory_analysis_miceprotein(data, target): Perform exploratory data analysis on the miceprotein dataset. Parameters: data (array): The data array of the miceprotein dataset. target (array): The target array of the miceprotein dataset. Returns: stats (DataFrame): Mean and standard deviation for each feature, grouped by the target class. # Your code here pass ``` Make sure your code handles any potential edge cases and errors. You may assume that the dataset paths provided are accessible and the datasets are well-formed. Constraints: - Use `scikit-learn` version 0.24 or higher. - Ensure all datasets and images are loaded successfully. - The implementation should handle missing values appropriately if found in the dataset. Submission: Compress the `*.py` file containing your functions and submit it as a zip file named `scikit_data_loading.zip`.","solution":"import numpy as np import pandas as pd from sklearn.datasets import fetch_openml, load_svmlight_file, load_sample_image from sklearn.preprocessing import StandardScaler def load_miceprotein_data(): Load the miceprotein dataset from OpenML (dataset id: 40966). Returns: mice_data_shape (tuple): Shape of the data array. mice_target_shape (tuple): Shape of the target array. unique_classes (array): Unique classes in the target array. mice = fetch_openml(data_id=40966, as_frame=False) data = mice.data target = mice.target mice_data_shape = data.shape mice_target_shape = target.shape unique_classes = np.unique(target) return mice_data_shape, mice_target_shape, unique_classes def load_svmlight_data(file_path): Load dataset from svmlight/libsvm formatted file. Parameters: file_path (string): Path to the svmlight/libsvm formatted file. Returns: X_shape (tuple): Shape of the feature matrix X. y_shape (tuple): Shape of the target vector y. X_train, y_train = load_svmlight_file(file_path) X_shape = X_train.shape y_shape = y_train.shape return X_shape, y_shape def load_and_preprocess_image(): Load and preprocess the sample image \'china.jpg\'. Returns: image_shape (tuple): Shape of the image array. first_5_pixels (array): First 5 pixel values of the image. image = load_sample_image(\'china.jpg\') image = image.astype(np.float32) / 255.0 image_shape = image.shape first_5_pixels = image.reshape(-1, image.shape[2])[:5] return image_shape, first_5_pixels def exploratory_analysis_miceprotein(data, target): Perform exploratory data analysis on the miceprotein dataset. Parameters: data (array): The data array of the miceprotein dataset. target (array): The target array of the miceprotein dataset. Returns: stats (DataFrame): Mean and standard deviation for each feature, grouped by the target class. df_data = pd.DataFrame(data) df_data[\'target\'] = target grouped_stats = df_data.groupby(\'target\').agg([\'mean\', \'std\']) return grouped_stats"},{"question":"**Objective:** Implement a function that extracts specific information from a log file using regular expressions. This task will assess your understanding of both basic and advanced regular expressions, including capturing groups, repetition, and lookahead assertions. **Problem Statement:** Given a multiline string representing a log file, your task is to implement a function `extract_log_data(log: str) -> List[Tuple[str, str, str]]` that extracts specific information from each log entry. Each log entry follows the standard format and contains a timestamp, an event type, and a message. Your function should return a list of tuples, each containing the timestamp, event type, and message extracted from the log entries. **Log Entry Format:** ``` [YYYY-MM-DD HH:MM:SS] EVENT_TYPE: Message contents ``` **Examples:** - Timestamp: `[2023-03-21 10:45:00]` - Event Type: `ERROR` - Message: `An error occurred in the application.` **Input:** - `log`: A multiline string where each line represents a log entry following the above format. **Output:** - A list of tuples where each tuple contains three strings: the timestamp, the event type, and the message. **Constraints:** 1. Assume the log entries are well-formed and follow the provided format strictly. 2. Use raw string literals to handle any backslashes in regular expressions. **Function Signature:** ```python from typing import List, Tuple def extract_log_data(log: str) -> List[Tuple[str, str, str]]: pass ``` **Example:** ```python log_data = \'\'\'[2023-03-21 10:45:00] ERROR: An error occurred in the application. [2023-03-21 10:50:00] INFO: Application started successfully. [2023-03-21 11:00:00] WARNING: Low memory detected.\'\'\' result = extract_log_data(log_data) print(result) # Output: [ # (\'2023-03-21 10:45:00\', \'ERROR\', \'An error occurred in the application.\'), # (\'2023-03-21 10:50:00\', \'INFO\', \'Application started successfully.\'), # (\'2023-03-21 11:00:00\', \'WARNING\', \'Low memory detected.\') # ] ``` **Guidance:** - Use the `re` module to compile the necessary regular expression for matching log entries. - Pay attention to capturing groups to extract specific parts of each log entry. - Make sure your regular expression handles the described format accurately and efficiently.","solution":"import re from typing import List, Tuple def extract_log_data(log: str) -> List[Tuple[str, str, str]]: # Regular expression to match the log entry format log_pattern = re.compile(r\'[(.*?)] (w+): (.*)\') # Find all matches in the log string matches = log_pattern.findall(log) return matches"},{"question":"# Question: You are given a dataset of restaurant tips which includes the following columns: `total_bill`, `tip`, `sex`, `smoker`, `day`, `time`, and `size`. Your task is to visualize the distribution of tips with respect to different factors using seaborn\'s `FacetGrid`. Input: 1. A pandas DataFrame `tips` containing the dataset. Requirements: 1. Create a `FacetGrid` that shows a scatter plot of `total_bill` vs `tip` divided by day of the week. 2. Use different colors for points based on whether the person is a smoker. 3. Adjust the size and aspect ratio of the subplots to `height=4` and `aspect=1.2`. 4. Add a horizontal reference line at the median of `tip`. 5. Set the axis labels to `Total Bill ()` and `Tip ()`. 6. Title the columns with the day of the week using the format `{col_name} Patrons`. 7. Save the resulting plot as `tips_distribution.png`. Constraints: - If there is any missing value in `total_bill`, `tip`, `sex`, `smoker`, `day`, `time`, or `size`, drop the corresponding row from the dataset before plotting. Output: - A file named `tips_distribution.png` containing the resulting plot. Example Code: ```python import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_tips_distribution(tips): # Step 1: Handle missing values tips = tips.dropna(subset=[\'total_bill\', \'tip\', \'sex\', \'smoker\', \'day\', \'time\', \'size\']) # Step 2: Create FacetGrid g = sns.FacetGrid(tips, col=\\"day\\", hue=\\"smoker\\", height=4, aspect=1.2) # Step 3: Map scatter plot g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Step 4: Add reference line g.refline(y=tips[\\"tip\\"].median()) # Step 5: Set axis labels g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") # Step 6: Set titles g.set_titles(col_template=\\"{col_name} Patrons\\") # Step 7: Add legends g.add_legend() # Step 8: Save the plot g.savefig(\\"tips_distribution.png\\") # Example usage if __name__ == \\"__main__\\": tips = sns.load_dataset(\\"tips\\") visualize_tips_distribution(tips) ```","solution":"import seaborn as sns import matplotlib.pyplot as plt import pandas as pd def visualize_tips_distribution(tips): # Step 1: Handle missing values tips = tips.dropna(subset=[\'total_bill\', \'tip\', \'sex\', \'smoker\', \'day\', \'time\', \'size\']) # Step 2: Create FacetGrid g = sns.FacetGrid(tips, col=\\"day\\", hue=\\"smoker\\", height=4, aspect=1.2) # Step 3: Map scatter plot g.map_dataframe(sns.scatterplot, x=\\"total_bill\\", y=\\"tip\\") # Step 4: Add reference line g.refline(y=tips[\'tip\'].median()) # Step 5: Set axis labels g.set_axis_labels(\\"Total Bill ()\\", \\"Tip ()\\") # Step 6: Set titles g.set_titles(col_template=\\"{col_name} Patrons\\") # Step 7: Add legends g.add_legend() # Step 8: Save the plot g.savefig(\\"tips_distribution.png\\")"},{"question":"# Concurrent Task Management Using `concurrent.futures` Objective: Implement a function that processes a list of URLs concurrently and returns the contents of each URL using the `concurrent.futures` module. Problem Statement: You are given a list of URLs. Your task is to implement a function `fetch_all_urls(urls: List[str]) -> List[str]` that fetches the content of each URL concurrently using a thread pool. Requirements: 1. Define a function `fetch_url(url: str) -> str` that takes a URL and returns the content of the URL as a string. 2. Your main function `fetch_all_urls(urls: List[str]) -> List[str]` should: - Create a `ThreadPoolExecutor`. - Submit the `fetch_url` function for each URL in the provided list. - Collect the results as they complete and return a list of the content of each URL. 3. Ensure proper management of the executor to avoid any resource leakage or hanging threads. Input: - `urls: List[str]` - A list of URLs to be fetched. Output: - `List[str]` - A list containing the content of each URL in the same order as provided in the input list. Constraints: - You can assume that the URLs are valid and accessible. - Handle exceptions that might occur during fetching to ensure the program does not crash, and return an empty string for URLs that raise an exception. Example: ```python urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] content = fetch_all_urls(urls) # content should be a list where each element is the corresponding URL\'s content ``` Notes: - Use the `requests` library to fetch the URL content. Ensure to install it using `pip install requests` before running your code. ```python import requests from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List def fetch_url(url: str) -> str: try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException: return \\"\\" def fetch_all_urls(urls: List[str]) -> List[str]: with ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} return [future.result() for future in as_completed(future_to_url)] # Test case urls = [ \\"http://example.com\\", \\"http://example.org\\", \\"http://example.net\\" ] print(fetch_all_urls(urls)) ``` Performance Requirements: - Ensure that the function can handle fetching 100 URLs concurrently within a reasonable timeframe.","solution":"import requests from concurrent.futures import ThreadPoolExecutor, as_completed from typing import List def fetch_url(url: str) -> str: Fetches the content of the given URL. Args: - url: The URL as a string. Returns: - The content of the URL as a string. If an exception occurs, returns an empty string. try: response = requests.get(url) response.raise_for_status() return response.text except requests.RequestException: return \\"\\" def fetch_all_urls(urls: List[str]) -> List[str]: Fetches the content of a list of URLs concurrently. Args: - urls: List of URLs to be fetched. Returns: - A list containing the content of each URL in the same order as provided in the input list. with ThreadPoolExecutor(max_workers=5) as executor: future_to_url = {executor.submit(fetch_url, url): url for url in urls} return [future.result() for future in as_completed(future_to_url)]"},{"question":"# Audio Processing with the `wave` Module Problem Description You are tasked with writing a Python function that processes an input WAV audio file to normalize its audio levels. The function should read the input WAV file, normalize its audio levels so that the loudest part of the audio reaches the maximum possible amplitude, and then write the normalized audio to a new WAV file. Normalization in this context means adjusting the volume of the entire audio file so that the highest (absolute) sample value reaches the maximum value while preserving the relative dynamics of the rest of the audio. Function Signature ```python def normalize_wav(input_filename: str, output_filename: str) -> None: Normalizes the audio levels of a WAV file. Args: input_filename (str): The filename of the input WAV file. output_filename (str): The filename where the normalized output should be saved. pass ``` Input - `input_filename` (str): The path to the input WAV file. - `output_filename` (str): The path where the normalized WAV file will be saved. Output The function saves the normalized audio to `output_filename`. Constraints - The input WAV file will have 16-bit PCM-encoded audio. - The maximum sample value for 16-bit PCM is 32767, and the minimum is -32768. Performance Requirements - Your function should handle input WAV files of up to 10 minutes in duration efficiently. Example Usage ```python normalize_wav(\\"input.wav\\", \\"output.wav\\") ``` This function should read \\"input.wav,\\" normalize its audio levels, and write the result to \\"output.wav\\". Hints 1. Use the `wave.open()` function to read and write WAV files. 2. You can use the `Wave_read` object\'s methods to navigate through the audio file and read its frames. 3. For normalization, find the maximum absolute sample value in the audio data, and use it to scale all samples. 4. Write the normalized audio data to the output file using the `Wave_write` object\'s methods.","solution":"import wave import numpy as np def normalize_wav(input_filename: str, output_filename: str) -> None: Normalizes the audio levels of a WAV file. Args: input_filename (str): The filename of the input WAV file. output_filename (str): The filename where the normalized output should be saved. with wave.open(input_filename, \'rb\') as wav_in: params = wav_in.getparams() n_channels, sampwidth, framerate, n_frames, comptype, compname = params frames = wav_in.readframes(n_frames) # Convert wave frames to numpy array audio_data = np.frombuffer(frames, dtype=np.int16) # Find the maximum absolute value in the audio data max_val = max(abs(audio_data)) # Scale factor to normalize audio scale_factor = 32767 / max_val # Normalize by scaling the audio data normalized_data = (audio_data * scale_factor).astype(np.int16) # Convert numpy array back to bytes normalized_frames = normalized_data.tobytes() # Write the normalized frames to output file with wave.open(output_filename, \'wb\') as wav_out: wav_out.setparams((n_channels, sampwidth, framerate, n_frames, comptype, compname)) wav_out.writeframes(normalized_frames)"},{"question":"You are given the task of visualizing a dataset using custom dark color palettes created with `seaborn.dark_palette()`. Objective: Write a function `custom_palette_visualization` that reads a dataset, generates a dark color palette based on user inputs, and then visualizes the dataset using this palette. Function Signature: ```python def custom_palette_visualization(data, column, color_input, palette_type=\'discrete\', n_colors=6, input_format=\'color_name\', as_cmap=False): pass ``` Parameters: - `data (pd.DataFrame)`: The input dataset that you will visualize. - `column (str)`: The column of the dataset to base the palette on (e.g., a categorical column). - `color_input (str, tuple)`: The base color for the `dark_palette` function. This could be a color name, hex code, or a tuple in HUSL system. - `palette_type (str)`: Type of the palette, either \'discrete\' or \'continuous\'. Default is \'discrete\'. - `n_colors (int)`: Number of colors in the palette if it\'s a discrete palette. Default is 6. - `input_format (str)`: Format of the `color_input`, either \'color_name\', \'hex\', or \'husl\'. Default is \'color_name\'. - `as_cmap (bool)`: Whether to return the palette as a continuous color map. Default is False. Returns: - A `matplotlib` figure or axes containing the visualization of the dataset using the custom palette. Constraints: - You must convert the `color_input` to appropriate formats based on the `input_format` parameter. - The dataset should be visualized using seaborn plots (like bar plot, count plot, or box plot). - The resulting plot should clearly use the generated color palette to differentiate categories or provide a continuous gradient. Example: ```python import seaborn as sns import pandas as pd # Sample DataFrame data = pd.DataFrame({ \'Category\': [\'A\', \'B\', \'C\', \'A\', \'B\', \'C\'], \'Values\': [10, 20, 30, 15, 25, 35] }) # Example usage custom_palette_visualization(data, \'Category\', \'seagreen\', \'discrete\', 3) ``` The above function call should create and display a bar plot using a dark palette derived from the color \'seagreen\' with 3 discrete colors. Notes: 1. Consider how to handle different input formats and pass them to `seaborn.dark_palette`. 2. Choose a suitable type of seaborn plot based on the characteristics of the `data`. 3. Ensure that the plot correctly visualizes the palette and provides meaningful insights into the dataset.","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def custom_palette_visualization(data, column, color_input, palette_type=\'discrete\', n_colors=6, input_format=\'color_name\', as_cmap=False): Generates a dark color palette based on user inputs and visualizes the dataset using this palette. Parameters: - data (pd.DataFrame): The input dataset to visualize. - column (str): The column of the dataset to base the palette on (e.g., a categorical column). - color_input (str, tuple): The base color for the dark_palette function. This could be a color name, hex code, or a tuple in HUSL system. - palette_type (str): Type of the palette, either \'discrete\' or \'continuous\'. Default is \'discrete\'. - n_colors (int): Number of colors in the palette if it\'s a discrete palette. Default is 6. - input_format (str): Format of the color_input, either \'color_name\', \'hex\', or \'husl\'. Default is \'color_name\'. - as_cmap (bool): Whether to return the palette as a continuous color map. Default is False. Returns: A matplotlib figure or axes containing the visualization of the dataset using the custom palette. # Convert color_input to appropriate format if input_format == \'color_name\' or input_format == \'hex\': base_color = color_input elif input_format == \'husl\': base_color = tuple(color_input) else: raise ValueError(\\"Invalid input_format. Use \'color_name\', \'hex\', or \'husl\'.\\") # Generate dark palette palette = sns.dark_palette(base_color, n_colors=n_colors, input=input_format, as_cmap=as_cmap) # Plot using seaborn plt.figure(figsize=(10, 6)) if palette_type == \'discrete\': sns.countplot(data=data, x=column, palette=palette) elif palette_type == \'continuous\': sns.histplot(data=data, x=column, palette=palette, kde=True) else: raise ValueError(\\"Invalid palette_type. Use \'discrete\' or \'continuous\'.\\") plt.title(f\'Visualization with custom dark palette\') plt.show()"},{"question":"**Objective:** To assess your understanding of the `code` module in Python, you will implement a custom interactive console that allows users to input and execute Python code. You need to handle incomplete input, syntax errors, and runtime exceptions appropriately. **Task:** You are to create a class `CustomInteractiveConsole` that inherits from `code.InteractiveConsole`. This class should provide additional functionalities: 1. **Logging:** Maintain a log of all executed commands and their output. 2. **Custom Prompts:** Use custom prompts instead of the default ones. 3. **History Display:** Provide a method to display the history of all commands entered and their outputs. **Specifications:** 1. **Constructor:** - The constructor should accept optional parameters for the prompt strings `ps1` (primary prompt) and `ps2` (secondary prompt). - Initialize an empty list to store the history of commands and their outputs. 2. **Methods:** - `push(line)`: This method should override the parent class method. It should append the command to the history list before calling the parent class\'s `push` method. If an error occurs during execution, log the error message. - `interact()`: This method should override the parent class method to use custom prompts provided during initialization. - `show_history()`: This method should print all the commands executed along with their outputs or error messages. **Constraints:** - Do not use external libraries; only use the standard `code` module and built-in Python functionalities. - Ensure that the console accurately handles multi-line inputs and incomplete commands. **Example Usage:** ```python console = CustomInteractiveConsole(ps1=\'>>> \', ps2=\'... \') console.interact() # Example interactions: # >>> print(\\"Hello, World!\\") # Hello, World! # >>> a = [1, 2, 3 # ... ] # >>> a.append(4) # >>> print(a) # [1, 2, 3, 4] # To display history: console.show_history() # Output: # Command: print(\\"Hello, World!\\") # Output: Hello, World! # # Command: a = [1, 2, 3 # Output: # # Command: a.append(4) # Output: # # Command: print(a) # Output: [1, 2, 3, 4] ``` Implement the `CustomInteractiveConsole` class with the described functionalities.","solution":"import code import sys from io import StringIO class CustomInteractiveConsole(code.InteractiveConsole): def __init__(self, ps1=\'>>> \', ps2=\'... \'): super().__init__() self.ps1 = ps1 self.ps2 = ps2 self.history = [] def push(self, line): output = StringIO() old_stdout = sys.stdout sys.stdout = output old_stderr = sys.stderr sys.stderr = output try: result = super().push(line) except Exception as e: sys.stderr.write(str(e) + \\"n\\") result = False sys.stdout = old_stdout sys.stderr = old_stderr self.history.append((line, output.getvalue())) return result def interact(self): more = False try: while True: if more: prompt = self.ps2 else: prompt = self.ps1 try: line = input(prompt) except EOFError: print() break more = self.push(line) except KeyboardInterrupt: print(\\"nKeyboardInterrupt\\") def show_history(self): for command, output in self.history: print(f\\"Command: {command}nOutput: {output}\\")"},{"question":"# Async Task Scheduler In this assessment, you will implement an asynchronous task scheduler using Python’s `asyncio` event loop. The task scheduler will have the ability to schedule tasks at a specific time or after a given delay. Additionally, the scheduler should be able to handle the completion of tasks and execute a finalization callback when all tasks are done. Requirements 1. Implement the `TaskScheduler` class with the following methods: - `add_task(self, delay: float, coro)`: Schedules a coroutine `coro` to be executed after `delay` seconds. - `add_task_at(self, when: float, coro)`: Schedules a coroutine `coro` to be executed at the specific absolute time `when` (same reference as `loop.time()`). - `run(self)`: Runs the event loop until all scheduled tasks are completed. 2. Ensure that all tasks are executed within the provided delay or exact time, manage proper error handling, and shutdown after completion. # Input Format - Each scheduled task is represented by a coroutine function. - You will use `add_task(delay, coro)` to schedule tasks. - You will use `add_task_at(when, coro)` to schedule tasks for specific times. # Output Format - Print task execution-related messages within the task coroutines. - Implement finalization messages indicating the completion of all tasks. # Example ```python import asyncio class TaskScheduler: def __init__(self): self.loop = asyncio.new_event_loop() asyncio.set_event_loop(self.loop) self.tasks = [] def add_task(self, delay, coro): self.tasks.append(self.loop.call_later(delay, lambda: self.loop.create_task(coro()))) def add_task_at(self, when, coro): self.tasks.append(self.loop.call_at(when, lambda: self.loop.create_task(coro()))) def run(self): if not self.loop.is_running(): self.loop.run_until_complete(self._run_all_tasks()) async def _run_all_tasks(self): await asyncio.gather( *asyncio.all_tasks(self.loop) ) print(\\"All tasks completed\\") self.loop.stop() # Sample coroutine task functions async def task1(): print(\\"Task 1 started\\") await asyncio.sleep(1) print(\\"Task 1 completed\\") async def task2(): print(\\"Task 2 started\\") await asyncio.sleep(2) print(\\"Task 2 completed\\") # Example usage: scheduler = TaskScheduler() scheduler.add_task(2, task1) scheduler.add_task(1, task2) scheduler.run() ``` Constraints: - The implementation must handle at least 10 concurrent tasks. - Tasks should be executed as close as possible to their scheduled time. - Handle the `RuntimeError` in case of any issues with the event loop. Provide your implementation of the `TaskScheduler` class.","solution":"import asyncio class TaskScheduler: def __init__(self): self.loop = asyncio.new_event_loop() asyncio.set_event_loop(self.loop) self.tasks = [] def add_task(self, delay, coro): self.tasks.append(self.loop.call_later(delay, lambda: self.loop.create_task(coro()))) def add_task_at(self, when, coro): self.tasks.append(self.loop.call_at(when, lambda: self.loop.create_task(coro()))) def run(self): if not self.loop.is_running(): self.loop.run_until_complete(self._run_all_tasks()) async def _run_all_tasks(self): await asyncio.gather( *asyncio.all_tasks(self.loop) ) print(\\"All tasks completed\\") self.loop.stop() # Sample coroutine task functions async def task1(): print(\\"Task 1 started\\") await asyncio.sleep(1) print(\\"Task 1 completed\\") async def task2(): print(\\"Task 2 started\\") await asyncio.sleep(2) print(\\"Task 2 completed\\") # Example usage: # scheduler = TaskScheduler() # scheduler.add_task(2, task1) # scheduler.add_task(1, task2) # scheduler.run()"},{"question":"# Question: Advanced Seaborn Swarm Plot Customization **Objective:** Demonstrate your understanding of creating and customizing swarm plots using the seaborn package by implementing a function that generates a specific multi-faceted plot with given specifications. **Problem Statement:** Write a function `custom_swarmplot(data, x_var, y_var, hue_var, col_var)` in Python that takes in a dataset and generates a customized multi-faceted swarm plot based on the provided variables for x-axis, y-axis, hue, and columns for facets. The function should meet the following requirements: 1. **Inputs:** - `data`: A DataFrame containing the dataset. - `x_var`: The variable name (string) to be plotted on the x-axis. - `y_var`: The variable name (string) to be plotted on the y-axis. - `hue_var`: The variable name (string) to be used for color coding (hue). - `col_var`: The variable name (string) to create separate columns for facets. 2. **Requirements:** - Use `sns.catplot` with `kind=\\"swarm\\"` to create the plot. - Set the aspect ratio of each facet to 0.6. - Use the \\"deep\\" palette for hue differentiation. - Include a `dodge=True` parameter when creating the swarm plot. - Ensure the plot handles overlapping points by adjusting their size to be smaller. - Save the resulting plot to a file named `custom_swarmplot.png`. **Function Signature:** ```python def custom_swarmplot(data, x_var, y_var, hue_var, col_var): pass ``` **Constraints:** - Assume `data` is a valid pandas DataFrame containing the columns specified by the variable names. - Use seaborn and matplotlib packages for plotting. **Example Usage:** ```python import seaborn as sns # Load example dataset tips = sns.load_dataset(\\"tips\\") # Generate customized swarm plot custom_swarmplot(tips, \\"time\\", \\"total_bill\\", \\"sex\\", \\"day\\") ``` The above code should create a multi-faceted swarm plot with appropriate customizations and save it as `custom_swarmplot.png` in the working directory.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_swarmplot(data, x_var, y_var, hue_var, col_var): Generates a multi-faceted swarm plot using seaborn based on given specifications. Parameters: - data: DataFrame containing the dataset. - x_var: Variable name to be plotted on the x-axis. - y_var: Variable name to be plotted on the y-axis. - hue_var: Variable name to be used for color coding. - col_var: Variable name to create separate columns for facets. # Create the swarm plot with the specified parameters plot = sns.catplot(data=data, x=x_var, y=y_var, hue=hue_var, col=col_var, kind=\\"swarm\\", aspect=0.6, palette=\\"deep\\", dodge=True, size=3) # Save the plot to a file plot.savefig(\\"custom_swarmplot.png\\")"},{"question":"Objective: Create a program that utilizes the `operator` module to perform a series of operations on a given list of dictionaries. This question will assess your understanding of the `operator` module functions related to item access, comparison, and mathematical operations. Problem Statement: You are given a list of dictionaries, where each dictionary represents a student\'s record with keys: `\\"name\\"`, `\\"age\\"`, and `\\"marks\\"`. Your task is to filter, sort, and compute the average marks of students using functions from the `operator` module. Input: - A list of dictionaries, where each dictionary has the following keys: - `\\"name\\"`: A string representing the student\'s name. - `\\"age\\"`: An integer representing the student\'s age. - `\\"marks\\"`: An integer representing the student\'s marks. ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 24, \\"marks\\": 85}, {\\"name\\": \\"Bob\\", \\"age\\": 19, \\"marks\\": 78}, {\\"name\\": \\"Charlie\\", \\"age\\": 22, \\"marks\\": 90}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"marks\\": 60}, {\\"name\\": \\"Eva\\", \\"age\\": 20, \\"marks\\": 70} ] ``` Expected Output: 1. A list of dictionaries filtered to include only students with marks greater than 75. 2. The filtered list sorted by the student\'s age. 3. The average marks of the filtered list of students. Constraints: - Utilize functions from the `operator` module where applicable. - Do not use list comprehensions or explicit loops for filtering and sorting. - Ensure the solution is efficient and leverages the capabilities of the `operator` module. Function Signature: ```python from operator import itemgetter, attrgetter def process_students(students): Function to filter, sort, and compute the average marks of students. Parameters: students (list): A list of dictionaries representing student records. Returns: tuple: A tuple containing: - A list of filtered and sorted dictionaries. - The average marks of the filtered list of students. # Filter students with marks greater than 75 filtered_students = list(filter(lambda student: itemgetter(\'marks\')(student) > 75, students)) # Sort the filtered students by age sorted_students = sorted(filtered_students, key=itemgetter(\'age\')) # Compute the average marks of the filtered students total_marks = sum(map(itemgetter(\'marks\'), filtered_students)) average_marks = total_marks / len(filtered_students) if filtered_students else 0 return sorted_students, average_marks # Example usage: students = [ {\\"name\\": \\"Alice\\", \\"age\\": 24, \\"marks\\": 85}, {\\"name\\": \\"Bob\\", \\"age\\": 19, \\"marks\\": 78}, {\\"name\\": \\"Charlie\\", \\"age\\": 22, \\"marks\\": 90}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"marks\\": 60}, {\\"name\\": \\"Eva\\", \\"age\\": 20, \\"marks\\": 70} ] print(process_students(students)) ``` Explanation: 1. **Filter**: Use `operator.itemgetter` in combination with `filter` to filter out students with marks greater than 75. 2. **Sort**: Use `sorted` along with `operator.itemgetter` to sort the filtered students by their age. 3. **Average**: Use `map` and `operator.itemgetter` to extract marks and calculate the average marks. Implement this function to demonstrate your understanding of the `operator` module and ensure correctness and efficiency in your solution.","solution":"from operator import itemgetter def process_students(students): Function to filter, sort, and compute the average marks of students. Parameters: students (list): A list of dictionaries representing student records. Returns: tuple: A tuple containing: - A list of filtered and sorted dictionaries. - The average marks of the filtered list of students. # Filter students with marks greater than 75 filtered_students = list(filter(lambda student: itemgetter(\'marks\')(student) > 75, students)) # Sort the filtered students by age sorted_students = sorted(filtered_students, key=itemgetter(\'age\')) # Compute the average marks of the filtered students total_marks = sum(map(itemgetter(\'marks\'), filtered_students)) average_marks = total_marks / len(filtered_students) if filtered_students else 0 return sorted_students, average_marks # Example usage: students = [ {\\"name\\": \\"Alice\\", \\"age\\": 24, \\"marks\\": 85}, {\\"name\\": \\"Bob\\", \\"age\\": 19, \\"marks\\": 78}, {\\"name\\": \\"Charlie\\", \\"age\\": 22, \\"marks\\": 90}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"marks\\": 60}, {\\"name\\": \\"Eva\\", \\"age\\": 20, \\"marks\\": 70} ] print(process_students(students))"},{"question":"**Question: Tracing and Coverage Analysis with the `trace` Module** You are given a Python script `example_script.py` which contains the following code: ```python # example_script.py def add(a, b): return a + b def multiply(a, b): return a * b def main(): x = add(2, 3) y = multiply(4, 5) print(f\'Sum: {x}, Product: {y}\') if __name__ == \\"__main__\\": main() ``` Your task is to use the `trace` module to: 1. Trace the execution of the script and generate annotated listings of the source code, showing how many times each line was executed. 2. Display a summary report of the coverage both to the console and into a specified directory (`coverage_reports`). **Requirements & Constraints:** - The generated coverage report must include lines that were not executed. - Use the `trace` module\'s programmatic interface (`trace.Trace` and `trace.CoverageResults` classes) to accomplish this task. - The script should be assumed to always be run in an environment where the `trace` module is available. **Input:** - No direct input from the user is necessary. The script will be executed as part of the tracing process. **Output:** - A summary of the coverage including the number of times each line was executed, printed to the console. - A detailed coverage report saved in the `coverage_reports` directory. **Function Signature:** ```python def trace_script_execution(script_path: str, coverdir: str) -> None: pass ``` **Example Usage:** ```python trace_script_execution(\'example_script.py\', \'coverage_reports\') ``` **Additional Notes:** - Ensure to handle potential exceptions that may arise during the tracing process. - Make use of the `write_results` method of the `CoverageResults` class to generate the coverage report files. **Hint:** Refer to the `trace` module documentation provided above for detailed usage of the `Trace` class and the available methods.","solution":"import trace import os def trace_script_execution(script_path: str, coverdir: str) -> None: This function traces the execution of the given script and generates annotated listings of the source code, showing how many times each line was executed, and displays a summary report of the coverage. Parameters: script_path (str): The path to the Python script to trace. coverdir (str): The directory to store the coverage reports. # Ensure coverage directory exists if not os.path.exists(coverdir): os.makedirs(coverdir) tracer = trace.Trace(count=True, trace=False) # Execute the script with tracing enabled try: tracer.run(f\'exec(open(\\"{script_path}\\").read())\') except Exception as e: print(f\\"An error occurred while tracing the script: {e}\\") return # Retrieve the results from the tracer results = tracer.results() # Print summary to the console results.write_results(show_missing=True, coverdir=coverdir) print(f\\"Coverage report generated in \'{coverdir}\'\\") # Example usage: # trace_script_execution(\'example_script.py\', \'coverage_reports\')"},{"question":"# Question: Advanced Pandas MultiIndex Manipulation You are provided with a dataset recording sales figures for different products across multiple regions over several months. Your task is to use pandas and its MultiIndex capabilities to analyze this data. **Dataset Description:** The dataset is provided as a CSV file `sales_data.csv` with the following columns: - `Region` (Region where sales were made, e.g., \\"North\\", \\"South\\") - `Product` (Name of the product, e.g., \\"ProductA\\", \\"ProductB\\") - `Month` (Month of the sale, e.g., \\"2023-01\\", \\"2023-02\\") - `Sales` (Number of units sold) The file contents look like this: | Region | Product | Month | Sales | |--------|----------|---------|-------| | North | ProductA | 2023-01 | 150 | | North | ProductB | 2023-01 | 200 | | South | ProductA | 2023-02 | 300 | | South | ProductB | 2023-02 | 250 | **Task:** 1. **Load the Data:** Load the dataset into a pandas DataFrame. 2. **Create a MultiIndex:** Convert the DataFrame into a MultiIndex DataFrame with `Region`, `Product`, and `Month` as the levels of the index. 3. **Sum Sales by Region and Product:** Calculate the total sales for each combination of `Region` and `Product`. 4. **Filter and Sort Data:** - Extract sales data for \\"ProductA\\" and sort the results in descending order of sales. - Extract all sales data for the month \\"2023-02\\". 5. **Reindex with Additional Levels:** Reindex the DataFrame to include an additional level (`Year`) extracted from the `Month`. The resulting DataFrame should have a MultiIndex with `Region`, `Product`, `Year`, and `Month`. 6. **Advanced Indexing:** - Use `.loc` to extract sales data for \\"North\\" region and \\"ProductA\\" in the year \\"2023\\". - Use slicing to extract sales data for all products in the \\"South\\" region across months from \\"2023-01\\" to \\"2023-03\\". 7. **Reshape Data Using `swaplevel` and `reorder_levels`:** Swap the `Year` and `Month` levels of the MultiIndex. Then reorder the levels to have `Year` before `Region`. 8. **Output:** Print the results of the above operations. **Input:** The path to the CSV file: `sales_data.csv`. **Constraints:** - You may assume that the dataset is correctly formatted. - Use pandas for all data manipulation tasks. # Implementation: ```python import pandas as pd # 1. Load the data df = pd.read_csv(\'sales_data.csv\') # 2. Convert to a MultiIndex DataFrame df.set_index([\'Region\', \'Product\', \'Month\'], inplace=True) # 3. Sum sales by Region and Product total_sales = df.groupby([\'Region\', \'Product\']).sum() # 4. Filter and sort data # a. Extract sales for ProductA and sort product_a_sales = df.loc[(slice(None), \'ProductA\', slice(None)), :].sort_values(by=\'Sales\', ascending=False) # b. Extract sales data for the month \'2023-02\' sales_feb_2023 = df.loc[(slice(None), slice(None), \'2023-02\'), :] # 5. Reindex with additional level \'Year\' df = df.reset_index() df[\'Year\'] = df[\'Month\'].str[:4] df = df.set_index([\'Region\', \'Product\', \'Year\', \'Month\']) # 6. Advanced indexing # a. Extract data for North region and ProductA in 2023 north_product_a_2023 = df.loc[(\'North\', \'ProductA\', \'2023\', slice(None)), :] # b. Slice sales data for South region across months from \'2023-01\' to \'2023-03\' south_sales_2023_q1 = df.loc[(\'South\', slice(None), slice(None), slice(\'2023-01\', \'2023-03\')), :] # 7. Swap and reorder levels df_swapped = df.swaplevel(\'Year\', \'Month\') df_reordered = df.reorder_levels([\'Year\', \'Region\', \'Product\', \'Month\']) # 8. Print outputs print(total_sales) print(product_a_sales) print(sales_feb_2023) print(north_product_a_2023) print(south_sales_2023_q1) print(df_swapped) print(df_reordered) ```","solution":"import pandas as pd # Load the data def load_data(file_path): return pd.read_csv(file_path) # Convert to a MultiIndex DataFrame def create_multiindex_df(df): return df.set_index([\'Region\', \'Product\', \'Month\']) # Sum sales by Region and Product def calculate_total_sales(df): return df.groupby([\'Region\', \'Product\']).sum() # Filter and sort data def extract_product_a_sales(df): return df.loc[(slice(None), \'ProductA\', slice(None)), :].sort_values(by=\'Sales\', ascending=False) def extract_sales_feb_2023(df): return df.loc[(slice(None), slice(None), \'2023-02\'), :] # Reindex with additional level \'Year\' def reindex_with_year(df): df = df.reset_index() df[\'Year\'] = df[\'Month\'].str[:4] return df.set_index([\'Region\', \'Product\', \'Year\', \'Month\']) # Advanced indexing def extract_north_product_a_2023(df): return df.loc[(\'North\', \'ProductA\', \'2023\', slice(None)), :] def extract_south_sales_2023_q1(df): return df.loc[(\'South\', slice(None), slice(None), slice(\'2023-01\', \'2023-03\')), :] # Reshape data using swaplevel and reorder_levels def swap_and_reorder_levels(df): df_swapped = df.swaplevel(\'Year\', \'Month\') df_reordered = df.swaplevel(\'Year\', \'Region\').reorder_levels([\'Year\', \'Region\', \'Product\', \'Month\']) return df_reordered def main(file_path): df = load_data(file_path) print(\\"Initial DataFrame:\\") print(df) df = create_multiindex_df(df) print(\\"MultiIndex DataFrame:\\") print(df) total_sales = calculate_total_sales(df) print(\\"Total Sales by Region and Product:\\") print(total_sales) product_a_sales = extract_product_a_sales(df) print(\\"Sorted Sales for ProductA:\\") print(product_a_sales) sales_feb_2023 = extract_sales_feb_2023(df) print(\\"Sales for February 2023:\\") print(sales_feb_2023) df = reindex_with_year(df) print(\\"DataFrame with Year level:\\") print(df) north_product_a_2023 = extract_north_product_a_2023(df) print(\\"Sales for North region and ProductA in 2023:\\") print(north_product_a_2023) south_sales_2023_q1 = extract_south_sales_2023_q1(df) print(\\"Sales for South region from January to March 2023:\\") print(south_sales_2023_q1) df_reordered = swap_and_reorder_levels(df) print(\\"DataFrame with Year before Region:\\") print(df_reordered) if __name__ == \\"__main__\\": main(\'sales_data.csv\')"},{"question":"**Advanced Buffer Protocol Handling** # Objective: Create a Python class, `CustomBuffer`, that can handle buffer requests and provide buffer views according to the buffer protocol. You will demonstrate your understanding of various buffer attributes and perform operations like copying a buffer into a contiguous block of memory. # Class Specifications: 1. **Class Name**: `CustomBuffer` 2. **Attributes**: - `data`: A bytearray or array representing the buffer\'s data. - `buffer_view`: A Py_buffer structure to hold buffer details. 3. **Methods**: - `__init__(self, data: Union[bytearray, array])`: Initializes the `CustomBuffer` with the data provided. - `get_buffer(self, flags: int) -> Py_buffer`: Handles the buffer request with given flags. 4. **Utilities**: - Implement the following functions using `CustomBuffer`: - `check_contiguous(buffer: CustomBuffer, order: str) -> bool`: - **Input**: A `CustomBuffer` object and order (\'C\' for C-contiguous, \'F\' for Fortran-contiguous). - **Output**: Boolean indicating whether the buffer is contiguous in the required order. - `copy_to_contiguous(src_buffer: CustomBuffer, order: str) -> bytes`: - **Input**: A `CustomBuffer` object and order for copying (\'C\' or \'F\'). - **Output**: A bytes object with the copied data. # Constraints: - Handle both read-only and writable buffers. - Ensure to release the buffer using `PyBuffer_Release` method after usage to prevent resource leaks. - The buffer should handle various request types and flags correctly. # Example Usage: ```python data = bytearray(b\'Hello, World!\') custom_buffer = CustomBuffer(data) # Get the buffer view buffer_info = custom_buffer.get_buffer(PyBUF_SIMPLE) # Check if the buffer is contiguous in C-order is_contiguous = check_contiguous(custom_buffer, \'C\') print(is_contiguous) # Output: True # Copy the buffer data into a contiguous block contiguous_data = copy_to_contiguous(custom_buffer, \'C\') print(contiguous_data) # Output: b\'Hello, World!\' ``` # Additional Notes: - Make sure the implementation is efficient and adheres to the buffer protocol\'s specifications detailed in the documentation. - You can use helper methods or additional classes, if necessary, to organize the code.","solution":"from typing import Union from array import array import ctypes class Py_buffer(ctypes.Structure): _fields_ = [ (\'buf\', ctypes.c_void_p), (\'obj\', ctypes.py_object), (\'len\', ctypes.c_ssize_t), (\'itemsize\', ctypes.c_ssize_t), (\'readonly\', ctypes.c_int), (\'ndim\', ctypes.c_int), (\'format\', ctypes.c_char_p), (\'shape\', ctypes.POINTER(ctypes.c_ssize_t)), (\'strides\', ctypes.POINTER(ctypes.c_ssize_t)), (\'suboffsets\', ctypes.POINTER(ctypes.c_ssize_t)), (\'internal\', ctypes.c_void_p) ] class CustomBuffer: def __init__(self, data: Union[bytearray, array]): self.data = data self.buffer_view = Py_buffer() def get_buffer(self, flags: int) -> Py_buffer: self.buffer_view.buf = ctypes.cast((ctypes.c_char * len(self.data)).from_buffer(self.data), ctypes.c_void_p) self.buffer_view.obj = self self.buffer_view.len = len(self.data) self.buffer_view.itemsize = 1 # assuming byte size self.buffer_view.readonly = 0 if isinstance(self.data, bytearray) else 1 self.buffer_view.ndim = 1 self.buffer_view.format = ctypes.c_char_p(b\'B\') # Assuming unsigned char format self.buffer_view.shape = (ctypes.c_ssize_t * 1)(len(self.data)) self.buffer_view.strides = (ctypes.c_ssize_t * 1)(1) self.buffer_view.suboffsets = None self.buffer_view.internal = None return self.buffer_view def release_buffer(self): # Releasing buffer view in Python is usually a no-op unless PyBuffer_Release is explicitly needed. pass def check_contiguous(buffer: CustomBuffer, order: str) -> bool: buf_info = buffer.get_buffer(0) if buf_info.ndim == 1: return True return False def copy_to_contiguous(src_buffer: CustomBuffer, order: str) -> bytes: buf_info = src_buffer.get_buffer(0) if order == \'C\': contiguous_data = bytes(src_buffer.data) elif order == \'F\': contiguous_data = bytes(src_buffer.data) # For 1D data, C and F orders are the same. else: raise ValueError(f\\"Unknown order: {order}\\") src_buffer.release_buffer() return contiguous_data"},{"question":"You are given a dataset where the task is to detect anomalies. The dataset is highly imbalanced, with anomalies being very rare. Your task is to train a classification model and tune its decision threshold to maximize the recall rate for the minority class (anomalies). **Input:** - You will be provided with a dataset (`X`, `y`), where `X` is the feature matrix and `y` is the target vector. Load the dataset as follows: ```python from sklearn.datasets import make_classification X, y = make_classification( n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, weights=[0.99], # Imbalanced dataset flip_y=0, random_state=1 ) ``` **Requirements:** 1. Split the dataset into training and testing sets using an 80-20 split. 2. Train a `LogisticRegression` model on the training set. 3. Use `TunedThresholdClassifierCV` to tune the decision threshold of the trained model to maximize the recall rate for the minority class. 4. Evaluate the performance of the model with the tuned threshold on the testing set using the recall metric. 5. Output the recall score. **Constraints:** - Use `random_state=0` for reproducibility where applicable. **Expected Output:** - Print the recall score of the model on the testing set with the tuned threshold. **Solution Template:** ```python import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split, TunedThresholdClassifierCV from sklearn.metrics import make_scorer, recall_score # Step 1: Load the dataset X, y = make_classification( n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, weights=[0.99], # Imbalanced dataset flip_y=0, random_state=1 ) # Step 1: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Step 2: Train a LogisticRegression model base_model = LogisticRegression(random_state=0) base_model.fit(X_train, y_train) # Step 3: Tune the decision threshold pos_label = 1 scorer = make_scorer(recall_score, pos_label=pos_label) tuned_model = TunedThresholdClassifierCV(base_model, scoring=scorer, cv=5) tuned_model.fit(X_train, y_train) # Step 4: Evaluate the performance on the testing set y_pred = tuned_model.predict(X_test) recall = recall_score(y_test, y_pred, pos_label=pos_label) # Step 5: Output the recall score print(f\'Recall Score: {recall:.4f}\') ``` Test the template and ensure it works as expected.","solution":"import numpy as np from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.metrics import recall_score import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve # Step 1: Load the dataset X, y = make_classification( n_samples=1000, n_features=20, n_informative=2, n_redundant=10, n_clusters_per_class=1, weights=[0.99], # Imbalanced dataset flip_y=0, random_state=1 ) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Step 3: Train a LogisticRegression model base_model = LogisticRegression(random_state=0) base_model.fit(X_train, y_train) # Step 4: Tune the decision threshold y_scores = base_model.predict_proba(X_test)[:, 1] precision, recall, thresholds = precision_recall_curve(y_test, y_scores) # We want to maximize recall for the minority class fscore = (2 * precision * recall) / (precision + recall) ix = np.argmax(fscore) best_threshold = thresholds[ix] # Print the best threshold print(f\'Best Threshold={best_threshold:.3f}, F-Score={fscore[ix]:.3f}\') # Step 5: Evaluate the performance on the testing set with tuned threshold y_pred = (y_scores >= best_threshold).astype(int) recall = recall_score(y_test, y_pred, pos_label=1) # Step 6: Output the recall score print(f\'Recall Score: {recall:.4f}\')"},{"question":"**Coding Assessment Question** # Objective Demonstrate your understanding of seaborn\'s `residplot` function by implementing a function that creates a comprehensive residual analysis plot for a given dataset and set of variables. # Problem Description You are tasked with implementing a function `create_residual_analysis_plot` that: 1. Loads a dataset of your choice. 2. Creates residual plots for different pairs of variables (predictor and response). 3. Tests for violations of linear regression assumptions by adding higher-order trends. 4. Adds a LOWESS curve to enhance structural analysis. # Function Signature ```python def create_residual_analysis_plot(dataset_name: str, x_vars: list, y_var: str, orders: list): # Your implementation here pass ``` # Input - `dataset_name` (str): The name of the dataset to load using `sns.load_dataset()`. - `x_vars` (list of str): A list of variable names to use as predictors. - `y_var` (str): The name of the response variable. - `orders` (list of int): A list of integer values specifying the orders of polynomial to fit for each predictor variable. The length of this list must match the length of `x_vars`. # Output The function should display the residual plots but does not need to return any values. # Constraints - The function should use seaborn\'s `residplot` function. - The dataset to be used must be compatible with seaborn\'s `sns.load_dataset` function. - The length of `x_vars` and `orders` lists must be the same. # Example ```python # Example usage: create_residual_analysis_plot(\'mpg\', [\'weight\', \'horsepower\'], \'mpg\', [1, 2]) ``` This call to the function should: 1. Load the `mpg` dataset. 2. Create and display residual plots for `weight` vs `mpg` and `horsepower` vs `mpg`. 3. Add a first-order polynomial trend for `weight` and a second-order polynomial trend for `horsepower`. 4. Add a LOWESS curve for each plot to highlight any structural patterns. # Performance Ensure the function performs efficiently for the typical size of datasets provided by seaborn. # Hint You may find it useful to refer to seaborn\'s documentation for `sns.residplot` for additional parameters and usage examples.","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_residual_analysis_plot(dataset_name: str, x_vars: list, y_var: str, orders: list): Loads a dataset, creates residual plots for different pairs of variables (predictor and response), tests for violations of linear regression assumptions by adding higher-order trends, and adds a LOWESS curve to enhance structural analysis. Parameters: dataset_name (str): The name of the dataset to load using sns.load_dataset(). x_vars (list of str): A list of variable names to use as predictors. y_var (str): The name of the response variable. orders (list of int): A list of integer values specifying the orders of polynomial to fit for each predictor variable. The length of this list must match the length of x_vars. # Load the dataset data = sns.load_dataset(dataset_name) # Ensure the lengths of x_vars and orders match if len(x_vars) != len(orders): raise ValueError(\\"The length of x_vars and orders must be the same.\\") # Create a residual plot for each predictor variable for i, x_var in enumerate(x_vars): plt.figure(figsize=(10, 6)) sns.residplot(x=x_var, y=y_var, data=data, order=orders[i], lowess=True, scatter_kws={\'s\': 40}) plt.title(f\'Residual Plot: {x_var} vs {y_var} (Order: {orders[i]})\') plt.xlabel(x_var) plt.ylabel(f\'Residuals of {y_var}\') plt.show()"},{"question":"**Objective:** Implement a class `BufferManipulator` that provides methods to handle read-only and writable buffers using Python\'s buffer protocol. You will demonstrate an understanding of buffer memory handling and appropriate use of buffer protocols introduced in Python 3. **Instructions:** 1. Implement the `BufferManipulator` class with the following methods: - `get_char_buffer(self, obj)`: - Takes an object `obj` that supports the single-segment character buffer interface. - Returns a tuple `(buffer_pointer, buffer_length)` where `buffer_pointer` is a read-only memory location and `buffer_length` is the length of the buffer. - Raise a `TypeError` if the operation fails. - `get_read_buffer(self, obj)`: - Takes an object `obj` that supports the single-segment readable buffer interface. - Returns a tuple `(buffer_pointer, buffer_length)` where `buffer_pointer` is a read-only memory location containing arbitrary data and `buffer_length` is the length of the buffer. - Raise a `TypeError` if the operation fails. - `get_write_buffer(self, obj)`: - Takes an object `obj` that supports the single-segment writable buffer interface. - Returns a tuple `(buffer_pointer, buffer_length)` where `buffer_pointer` is a writable memory location and `buffer_length` is the length of the buffer. - Raise a `TypeError` if the operation fails. **Constraints:** - Your implementation should use the `memoryview` built-in function to handle buffer memory locations as the old buffer protocol functions are deprecated. - Ensure proper error handling to deal with unsupported buffer operations on given objects. **Example Usage:** ```python class BufferManipulator: def get_char_buffer(self, obj): # Implement the function pass def get_read_buffer(self, obj): # Implement the function pass def get_write_buffer(self, obj): # Implement the function pass # Example usage: buffer_manipulator = BufferManipulator() # Getting a read-only character buffer try: buffer_pointer, buffer_length = buffer_manipulator.get_char_buffer(b\'example\') print(buffer_pointer, buffer_length) except TypeError as e: print(f\\"Error: {e}\\") # Getting a read-only buffer containing arbitrary data try: buffer_pointer, buffer_length = buffer_manipulator.get_read_buffer(b\'example\') print(buffer_pointer, buffer_length) except TypeError as e: print(f\\"Error: {e}\\") # Getting a writable buffer try: buffer_pointer, buffer_length = buffer_manipulator.get_write_buffer(bytearray(b\'example\')) print(buffer_pointer, buffer_length) except TypeError as e: print(f\\"Error: {e}\\") ``` The `BufferManipulator` class and its methods should provide a functional interface to access memory locations using the buffer protocol, ensuring compatibility with various buffer-compatible objects in Python.","solution":"class BufferManipulator: def get_char_buffer(self, obj): Takes an object that supports the single-segment character buffer interface. Returns a tuple (buffer_pointer, buffer_length). try: mv = memoryview(obj) if mv.format == \'B\': return (mv.obj, len(mv)) raise TypeError(f\'Object does not support the character buffer interface: {obj}\') except TypeError: raise TypeError(f\'Object does not support the character buffer interface: {obj}\') def get_read_buffer(self, obj): Takes an object that supports the single-segment readable buffer interface. Returns a tuple (buffer_pointer, buffer_length). try: mv = memoryview(obj) return (mv.obj, len(mv)) except TypeError: raise TypeError(f\'Object does not support the readable buffer interface: {obj}\') def get_write_buffer(self, obj): Takes an object that supports the single-segment writable buffer interface. Returns a tuple (buffer_pointer, buffer_length). try: mv = memoryview(obj) if mv.readonly: raise TypeError(f\'Object does not support the writable buffer interface: {obj}\') return (mv.obj, len(mv)) except TypeError: raise TypeError(f\'Object does not support the writable buffer interface: {obj}\')"},{"question":"**Question:** You are given a list of dictionaries where each dictionary represents a student\'s records with fields `\'name\'`, `\'grade\'`, and `\'age\'`. Implement a function, `sort_students`, that takes this list and returns a sorted list of dictionaries based on the following priorities: 1. Primary sort on `\'grade\'` in descending order. 2. Secondary sort on `\'age\'` in ascending order. **Function Signature:** ```python def sort_students(students: List[Dict[str, Union[str, int]]]) -> List[Dict[str, Union[str, int]]]: ``` **Input:** - `students`: A list of dictionaries. Each dictionary contains three keys: - `\'name\'`: A string representing the student\'s name. - `\'grade\'`: A string representing the student\'s grade (e.g., \'A\', \'B\', \'C\'). - `\'age\'`: An integer representing the student\'s age. ```python students = [ {\'name\': \'john\', \'grade\': \'A\', \'age\': 15}, {\'name\': \'jane\', \'grade\': \'B\', \'age\': 12}, {\'name\': \'dave\', \'grade\': \'B\', \'age\': 10}, {\'name\': \'mike\', \'grade\': \'A\', \'age\': 10} ] ``` **Output:** - A list of dictionaries sorted with primary criterion as `\'grade\'` in descending order, and secondary criterion as `\'age\'` in ascending order. ```python [{\'name\': \'mike\', \'grade\': \'A\', \'age\': 10}, {\'name\': \'john\', \'grade\': \'A\', \'age\': 15}, {\'name\': \'dave\', \'grade\': \'B\', \'age\': 10}, {\'name\': \'jane\', \'grade\': \'B\', \'age\': 12}] ``` **Constraints:** - The input list will have at most 10^6 items. - All students will have valid and non-missing `\'name\'`, `\'grade\'`, and `\'age\'` fields. **Requirements:** - The solution must be efficient in terms of time complexity to handle large lists. - Stability in sorting must be maintained (i.e., students with the same grade should appear in the original relative order by age if the grades are the same). **Hints:** - Utilize the `sorted()` function along with the `attrgetter` from the `operator` module, considering multiple levels of sorting by `grade` and `age`. **Example Implementation:** ```python from typing import List, Dict, Union from operator import itemgetter def sort_students(students: List[Dict[str, Union[str, int]]]) -> List[Dict[str, Union[str, int]]]: return sorted(students, key=itemgetter(\'grade\', \'age\'), reverse=True) # Example usage: students = [ {\'name\': \'john\', \'grade\': \'A\', \'age\': 15}, {\'name\': \'jane\', \'grade\': \'B\', \'age\': 12}, {\'name\': \'dave\', \'grade\': \'B\', \'age\': 10}, {\'name\': \'mike\', \'grade\': \'A\', \'age\': 10} ] print(sort_students(students)) ```","solution":"from typing import List, Dict, Union def sort_students(students: List[Dict[str, Union[str, int]]]) -> List[Dict[str, Union[str, int]]]: # Define the order of grades to sort: A > B > C. grade_order = {\'A\': 0, \'B\': 1, \'C\': 2} # Sort by grade in descending order and then by age in ascending order. return sorted(students, key=lambda x: (grade_order[x[\'grade\']], x[\'age\']))"},{"question":"You are provided a dataset containing model performance scores across different tasks. Your task is to: 1. **Load and transform the data**: Load the given dataset, pivot it such that models and encoders form the indexes and tasks form the columns. Calculate the average score for each model. 2. **Visualize the data**: Create a plot to visualize the relationship between `SST-2` and `MRPC` scores with text annotation and enhance this plot using various seaborn features. 3. **Annotate the plot based on conditions**: Add text annotations that show the model names and align them according to the encoder type with specific horizontal alignments. 4. **Customize the appearance**: Use additional parameters to control the text appearance, such as text color and weight. Use a different color for Transformer-based encoders. Step-by-Step Tasks: 1. **Data Loading and Transformation**: - Load the dataset named `\\"glue\\"`. - Pivot the dataset such that the indexes are `Model` and `Encoder`, the columns are `Task`, and the values are `Score`. - Compute an additional column named `Average`, which holds the average score for each model, rounded to one decimal place. - Sort the data based on the `Average` column in descending order. 2. **Plotting**: - Create a plot using `so.Plot` where the x-axis is `SST-2`, the y-axis is `MRPC`, and the `text` parameter is set to the `Model` column. - Add `so.Text` to annotate the plot with model names. - Enhance the plot by setting the `color` parameter to the `Encoder` column and set `valign` to `bottom`. 3. **Conditional Text Alignment**: - Modify the plot to align text horizontally based on the `Encoder` type using the `halign` attribute. - Transformer-based models should be aligned to the right, and LSTM-based models should be aligned to the left. 4. **Appearance Customization**: - Use additional matplotlib text parameters to set the text font weight to bold. - Set the text color to white. # Expected Functions ```python import seaborn.objects as so from seaborn import load_dataset def prepare_data(): glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) return glue def plot_data(glue): plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\"), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) .add(so.Text({\\"fontweight\\": \\"bold\\"}, color=\\"w\\")) ) plot.show() if __name__ == \\"__main__\\": glue_data = prepare_data() plot_data(glue_data) ``` Constraints - Ensure your plot is informative and well-labeled. - Follow seaborn and matplotlib conventions for plot customization. - Assume the dataset is available and does not have missing values. Performance Requirements - Your code should efficiently handle data transformation and plotting within a reasonable time frame for typical dataset sizes used in visualization tasks.","solution":"import seaborn.objects as so from seaborn import load_dataset def prepare_data(): glue = ( load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) return glue def plot_data(glue): plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\"), halign=\\"Encoder\\") .scale(halign={\\"LSTM\\": \\"left\\", \\"Transformer\\": \\"right\\"}) .add(so.Text({\\"fontweight\\": \\"bold\\"}, color=\\"w\\")) ) plot.show() if __name__ == \\"__main__\\": glue_data = prepare_data() plot_data(glue_data)"},{"question":"**Title:** Implementing and Evaluating a Machine Learning Pipeline with Synthetic Data **Objective:** Demonstrate your understanding of scikit-learn by creating a synthetic dataset, fitting a machine learning model, and evaluating its performance. **Problem Statement:** You are tasked with implementing a machine learning pipeline using scikit-learn. Your task is to create and preprocess a synthetic dataset, fit a model, and evaluate its performance. **Requirements:** 1. **Data Generation:** - Generate a synthetic classification dataset with 1000 samples and 20 features using the `make_classification` function from `sklearn.datasets`. - Ensure that: - `n_informative=15` (number of informative features). - `n_redundant=5` (number of redundant features). - `n_clusters_per_class=1`. 2. **Data Preprocessing:** - Split the dataset into training (70%) and testing (30%) sets. - Standardize the features using `StandardScaler` from `sklearn.preprocessing`. 3. **Model Training:** - Train a `GradientBoostingClassifier` with the following parameters: - `random_state=42` - `n_estimators=100` - `learning_rate=0.1` 4. **Evaluation:** - Evaluate the model on the test set using accuracy and AUC-ROC score. - Print the accuracy and AUC-ROC score. **Input:** No input is required from the user. You will generate the data within the function. **Output:** Print the following: 1. Accuracy of the model on the test set. 2. AUC-ROC score of the model on the test set. **Function Signature:** ```python def evaluate_classification_model(): # Your code here ``` **Constraints:** - Use only the packages from scikit-learn and numpy. - Ensure your code is clear and well-documented with comments where necessary. **Example Output:** ```plain Accuracy: 0.90 AUC-ROC Score: 0.95 ``` # Good Luck!","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.ensemble import GradientBoostingClassifier from sklearn.metrics import accuracy_score, roc_auc_score def evaluate_classification_model(): # Generate synthetic classification dataset X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, n_clusters_per_class=1, random_state=42) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Standardize the features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Train GradientBoostingClassifier gbc = GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1) gbc.fit(X_train, y_train) # Make predictions y_pred = gbc.predict(X_test) y_pred_proba = gbc.predict_proba(X_test)[:, 1] # Evaluate model performance accuracy = accuracy_score(y_test, y_pred) auc_roc = roc_auc_score(y_test, y_pred_proba) print(f\\"Accuracy: {accuracy:.2f}\\") print(f\\"AUC-ROC Score: {auc_roc:.2f}\\") # Call the function to see the output evaluate_classification_model()"},{"question":"# Advanced Python Coding Assessment Problem Statement You are required to implement a class `FileCompressor` which uses the `gzip` module for compressing and decompressing files. This class should provide high-level functionalities to: 1. Compress multiple text files into separate `.gz` files. 2. Decompress a `.gz` file to retrieve the original content. 3. Compress a string directly to a gzip byte string and decompress it back. Implement the class with the following methods and corresponding descriptions: ```python import gzip class FileCompressor: @staticmethod def compress_files(file_paths, output_dir): Compress each file in the list of file paths and save the compressed file to the specified output directory. Params: - file_paths (List[str]): List of paths to text files to compress. - output_dir (str): Directory where the compressed files should be saved. Returns: - List[str]: List of paths to the compressed files. Raises: - ValueError: If file_paths is not a list of strings or output_dir is not a string. pass @staticmethod def decompress_file(file_path): Decompress the specified .gz file and return its content as a string. Params: - file_path (str): Path to the .gz file to decompress. Returns: - str: Decompressed file content. Raises: - gzip.BadGzipFile: If the file is not a valid gzip file. - OSError: If there is an error in file reading or decompression. - ValueError: If file_path is not a string. pass @staticmethod def compress_string(data): Compress the given string and return the compressed data as a byte string. Params: - data (str): String to compress. Returns: - bytes: Compressed data as bytes. Raises: - ValueError: If data is not a string. pass @staticmethod def decompress_string(data): Decompress the given byte string and return the original string. Params: - data (bytes): Compressed byte string. Returns: - str: Decompressed string. Raises: - gzip.BadGzipFile: If the data is not a valid gzip format. - ValueError: If data is not a byte string. pass ``` Constraints and Limitations: 1. Ensure that all exceptions are appropriately handled and informative error messages are provided. 2. Assume file paths and directory paths are valid and accessible. 3. The compression level can default to the provided `GzipFile` default compression level. Example Usage: ```python file_list = [\'/path/to/file1.txt\', \'/path/to/file2.txt\'] output_directory = \'/path/to/compressed_files\' # Compress files compressed_files = FileCompressor.compress_files(file_list, output_directory) print(compressed_files) # [\'/path/to/compressed_files/file1.txt.gz\', \'/path/to/compressed_files/file2.txt.gz\'] # Decompress a file content = FileCompressor.decompress_file(\'/path/to/compressed_files/file1.txt.gz\') print(content) # \\"Content of file1.txt\\" # Compress a string compressed_data = FileCompressor.compress_string(\\"Sample string to compress\\") print(compressed_data) # Compressed byte string # Decompress a string original_string = FileCompressor.decompress_string(compressed_data) print(original_string) # \\"Sample string to compress\\" ``` Write the implementation for the `FileCompressor` class based on the provided method signatures and the functionalities described.","solution":"import gzip import os class FileCompressor: @staticmethod def compress_files(file_paths, output_dir): Compress each file in the list of file paths and save the compressed file to the specified output directory. Params: - file_paths (List[str]): List of paths to text files to compress. - output_dir (str): Directory where the compressed files should be saved. Returns: - List[str]: List of paths to the compressed files. Raises: - ValueError: If file_paths is not a list of strings or output_dir is not a string. if not isinstance(file_paths, list) or not all(isinstance(fp, str) for fp in file_paths): raise ValueError(\\"file_paths should be a list of strings.\\") if not isinstance(output_dir, str): raise ValueError(\\"output_dir should be a string.\\") compressed_files = [] if not os.path.exists(output_dir): os.makedirs(output_dir) for file_path in file_paths: if not os.path.isfile(file_path): continue file_name = os.path.basename(file_path) compressed_file_path = os.path.join(output_dir, file_name + \\".gz\\") with open(file_path, \'rb\') as f_in, gzip.open(compressed_file_path, \'wb\') as f_out: f_out.writelines(f_in) compressed_files.append(compressed_file_path) return compressed_files @staticmethod def decompress_file(file_path): Decompress the specified .gz file and return its content as a string. Params: - file_path (str): Path to the .gz file to decompress. Returns: - str: Decompressed file content. Raises: - gzip.BadGzipFile: If the file is not a valid gzip file. - OSError: If there is an error in file reading or decompression. - ValueError: If file_path is not a string. if not isinstance(file_path, str): raise ValueError(\\"file_path should be a string.\\") with gzip.open(file_path, \'rt\') as f: return f.read() @staticmethod def compress_string(data): Compress the given string and return the compressed data as a byte string. Params: - data (str): String to compress. Returns: - bytes: Compressed data as bytes. Raises: - ValueError: If data is not a string. if not isinstance(data, str): raise ValueError(\\"data should be a string.\\") return gzip.compress(data.encode()) @staticmethod def decompress_string(data): Decompress the given byte string and return the original string. Params: - data (bytes): Compressed byte string. Returns: - str: Decompressed string. Raises: - gzip.BadGzipFile: If the data is not a valid gzip format. - ValueError: If data is not a byte string. if not isinstance(data, bytes): raise ValueError(\\"data should be a byte string.\\") return gzip.decompress(data).decode()"},{"question":"**Objective:** Demonstrate your understanding of pandas extension capabilities by creating a custom ExtensionDtype and ExtensionArray for handling specialized data within pandas DataFrames. **Problem Statement:** You need to implement a custom pandas extension dtype and array to handle date ranges efficiently. 1. Create a `DateRangeDtype` class derived from `pd.api.extensions.ExtensionDtype`. 2. Create a `DateRangeArray` class derived from `pd.api.extensions.ExtensionArray`. 3. Implement essential methods in `DateRangeArray` such as: - `__getitem__` for element access - `__len__` for the length of the array - `take` for fetching elements by index - `copy` for creating copies of the array - `isna`, `fillna` for handling missing values - `astype` for type conversion **Specifications:** 1. **Input/Output Format:** - The `DateRangeArray` should be initialized with a list of tuples, where each tuple contains two `datetime.date` objects representing the start and end of a date range. Example: ```python ranges = DateRangeArray([ (datetime.date(2021, 1, 1), datetime.date(2021, 1, 7)), (datetime.date(2021, 1, 8), datetime.date(2021, 1, 14)), # Add more date ranges as needed ]) ``` 2. **Constraints:** - Ensure that the dtype is registered using `register_extension_dtype`. - Date ranges should not overlap within the same array. 3. **Performance Requirement:** - The custom array should handle at least 10,000 date ranges efficiently. **Hints:** - You can refer to `pd.TimestampArray` or similar existing arrays for understanding how to implement custom methods. - For testing purposes, you may need to install and import the `pandas` library, alongside the `datetime` module from the Python standard library. **Examples:** How you may utilize the custom dtype: ```python import pandas as pd import datetime # Define custom DateRangeDtype and DateRangeArray HERE # Define DataFrame with the custom dtype df = pd.DataFrame({ \'event\': [\'event_1\', \'event_2\'], \'date_range\': DateRangeArray([ (datetime.date(2021, 1, 1), datetime.date(2021, 1, 7)), (datetime.date(2021, 1, 8), datetime.date(2021, 1, 14)), ]) }) print(df) ``` Your task is to implement the `DateRangeDtype` and `DateRangeArray` classes. The above code should work correctly with your implementation, producing a DataFrame that holds custom date ranges.","solution":"import pandas as pd import datetime from pandas.api.extensions import ExtensionDtype, ExtensionArray, register_extension_dtype @register_extension_dtype class DateRangeDtype(ExtensionDtype): name = \\"daterange\\" type = tuple kind = \'O\' na_value = (pd.NaT, pd.NaT) @classmethod def construct_array_type(cls): return DateRangeArray class DateRangeArray(ExtensionArray): dtype = DateRangeDtype() def __init__(self, ranges): self._ranges = ranges def __getitem__(self, item): if isinstance(item, int): return self._ranges[item] elif isinstance(item, slice): return DateRangeArray(self._ranges[item]) elif isinstance(item, list): return DateRangeArray([self._ranges[i] for i in item]) def __len__(self): return len(self._ranges) @property def nbytes(self): return self._ranges.__sizeof__() def isna(self): return [start is pd.NaT or end is pd.NaT for start, end in self._ranges] def take(self, indices, allow_fill=False, fill_value=None): if fill_value is None: fill_value = self.dtype.na_value result = [] if allow_fill: for idx in indices: if idx == -1: result.append(fill_value) else: result.append(self._ranges[idx]) else: result = [self._ranges[i] for i in indices] return DateRangeArray(result) def copy(self): return DateRangeArray(self._ranges.copy()) def fillna(self, value): new_ranges = [(start if start is not pd.NaT else value[0], end if end is not pd.NaT else value[1]) for start, end in self._ranges] return DateRangeArray(new_ranges) def astype(self, dtype, copy=True): if dtype is self.dtype: return self.copy() if copy else self raise TypeError(f\\"Cannot convert {self.dtype} to {dtype}\\") @classmethod def _from_sequence(cls, scalars, dtype=None, copy=False): return cls(scalars) def __eq__(self, other): if not isinstance(other, DateRangeArray): return False return self._ranges == other._ranges"},{"question":"# Task Description You are tasked with creating a simple graphical user interface (GUI) in Python using the `tkinter` library. Your application should have the following requirements: 1. A main window with a button labeled \\"Proceed\\". 2. When the \\"Proceed\\" button is clicked, a series of message boxes should appear in the following order: - An informational message box with the title \\"Info\\" and the message \\"This is an information message.\\" - A warning message box with the title \\"Warning\\" and the message \\"This is a warning message.\\" - An error message box with the title \\"Error\\" and the message \\"This is an error message.\\" - A question message box prompting the user with \\"Do you want to continue?\\" with Yes and No options. 3. Based on the user\'s response to the question message box: - If the user selects \\"Yes\\", show another informational message box stating \\"You chose to continue.\\" - If the user selects \\"No\\", show another informational message box stating \\"You chose not to continue.\\" # Input and Output - The user\'s interaction with the message boxes will determine the flow of the program. # Constraints - You must use the `tkinter.messagebox` module for implementing the message boxes. - Ensure that the GUI is responsive and all message boxes are modal (i.e., the main window should be inaccessible while a message box is open). # Example Workflow 1. User opens the application. 2. User clicks the \\"Proceed\\" button. 3. An informational message box is displayed. 4. A warning message box is displayed. 5. An error message box is displayed. 6. A question message box is displayed. 7. If the user selects \\"Yes\\", an informational message box stating \\"You chose to continue\\" is displayed. 8. If the user selects \\"No\\", an informational message box stating \\"You chose not to continue\\" is displayed. # Implementation ```python import tkinter as tk from tkinter import messagebox def on_proceed(): messagebox.showinfo(\\"Info\\", \\"This is an information message.\\") messagebox.showwarning(\\"Warning\\", \\"This is a warning message.\\") messagebox.showerror(\\"Error\\", \\"This is an error message.\\") response = messagebox.askyesno(\\"Question\\", \\"Do you want to continue?\\") if response: messagebox.showinfo(\\"Response\\", \\"You chose to continue.\\") else: messagebox.showinfo(\\"Response\\", \\"You chose not to continue.\\") def main(): root = tk.Tk() root.title(\\"Message Box Example\\") btn_proceed = tk.Button(root, text=\\"Proceed\\", command=on_proceed) btn_proceed.pack(pady=20) root.mainloop() if __name__ == \\"__main__\\": main() ```","solution":"import tkinter as tk from tkinter import messagebox def on_proceed(): messagebox.showinfo(\\"Info\\", \\"This is an information message.\\") messagebox.showwarning(\\"Warning\\", \\"This is a warning message.\\") messagebox.showerror(\\"Error\\", \\"This is an error message.\\") response = messagebox.askyesno(\\"Question\\", \\"Do you want to continue?\\") if response: messagebox.showinfo(\\"Response\\", \\"You chose to continue.\\") else: messagebox.showinfo(\\"Response\\", \\"You chose not to continue.\\") def main(): root = tk.Tk() root.title(\\"Message Box Example\\") btn_proceed = tk.Button(root, text=\\"Proceed\\", command=on_proceed) btn_proceed.pack(pady=20) root.mainloop() if __name__ == \\"__main__\\": main()"},{"question":"**Objective**: Create two functions that explore and make use of the `uuid` module in Python. 1. **Function 1: generate_uuid_combinations** This function should generate and return a dictionary containing UUIDs created using all available methods (`uuid1`, `uuid3`, `uuid4`, `uuid5`). - **Parameters**: - `namespace`: A valid UUID used as a namespace for `uuid3` and `uuid5` (string or UUID). - `name`: A string used as a name for `uuid3` and `uuid5`. - **Returns**: - A dictionary with keys as the method names (`uuid1`, `uuid3`, `uuid4`, and `uuid5`) and values as the corresponding generated UUIDs. - **Example**: ```python generate_uuid_combinations(uuid.NAMESPACE_DNS, \'example.com\') # {\'uuid1\': UUID(\'...\'), \'uuid3\': UUID(\'...\'), \'uuid4\': UUID(\'...\'), \'uuid5\': UUID(\'...\')} ``` 2. **Function 2: analyze_uuid** This function takes a UUID string and returns a dictionary with detailed information about the UUID. - **Parameters**: - `uuid_str`: A string representation of a UUID. - **Returns**: - A dictionary with the following keys: - `version`: Version of the UUID. - `variant`: Variant of the UUID. - `fields`: Tuple of the UUID fields. - `int`: Integer representation of the UUID. - `is_safe`: Safety status of the UUID. - **Example**: ```python analyze_uuid(\'12345678-1234-5678-1234-567812345678\') # { # \'version\': 4, # \'variant\': \'RFC_4122\', # \'fields\': (305419896, 4660, 22136, 18, 52, 61370405664232), # \'int\': 24197857161011715162171839636988778104, # \'is_safe\': \'unknown\' # } ``` **Constraints**: - The input `namespace` for `generate_uuid_combinations` must be a valid UUID instance or a string that can be parsed into a UUID. - The input `uuid_str` for `analyze_uuid` must be a valid UUID string of length 36. **Performance Requirements**: - Both functions should execute in O(1) time complexity for UUID generation and field access. **Code Template**: ```python import uuid def generate_uuid_combinations(namespace, name): # Your code here pass def analyze_uuid(uuid_str): # Your code here pass # Example usage namespace = uuid.NAMESPACE_DNS name = \\"example.com\\" print(generate_uuid_combinations(namespace, name)) uuid_str = \'12345678-1234-5678-1234-567812345678\' print(analyze_uuid(uuid_str)) ``` **Note:** Ensure to handle any edge cases and invalid inputs appropriately, raising meaningful error messages if necessary.","solution":"import uuid def generate_uuid_combinations(namespace, name): Generates and returns a dictionary containing UUIDs created using uuid1, uuid3, uuid4, and uuid5 methods. Parameters: - namespace: UUID instance or string that can be parsed into a UUID. - name: A string used as a name for uuid3 and uuid5. Returns: - A dictionary with keys as method names (\'uuid1\', \'uuid3\', \'uuid4\', \'uuid5\') and values as generated UUIDs. namespace_uuid = uuid.UUID(namespace) if isinstance(namespace, str) else namespace return { \'uuid1\': uuid.uuid1(), \'uuid3\': uuid.uuid3(namespace_uuid, name), \'uuid4\': uuid.uuid4(), \'uuid5\': uuid.uuid5(namespace_uuid, name) } def analyze_uuid(uuid_str): Takes a UUID string and returns a dictionary with detailed information about the UUID. Parameters: - uuid_str: A string representation of a UUID. Returns: - A dictionary with the version, variant, fields, int representation and is_safe status of the UUID. parsed_uuid = uuid.UUID(uuid_str) return { \'version\': parsed_uuid.version, \'variant\': parsed_uuid.variant, \'fields\': parsed_uuid.fields, \'int\': parsed_uuid.int, \'is_safe\': parsed_uuid.is_safe.name }"},{"question":"# Custom Set Implementation in Python Problem Statement You are required to implement a custom set class in Python, named `CustomSet`, that mimics the behavior of Python\'s built-in set class. Your class must support the following operations: 1. **Initialization**: The class should be able to initialize a set with an iterable. 2. **Adding Elements**: Add elements to the set. 3. **Removing Elements**: Remove specified elements from the set. 4. **Checking Membership**: Check if an element exists in the set. 5. **Getting the Size**: Return the number of elements in the set. 6. **Clearing the Set**: Remove all elements from the set. You must ensure that your implementation handles errors properly, such as adding unhashable objects to the set or removing elements from an empty set. Specifications - **Class Name**: `CustomSet` - **Methods**: - `__init__(self, iterable=None)`: Initializes the set with the given iterable (default is `None` for an empty set). - `add(self, key)`: Adds the element `key` to the set. - `remove(self, key)`: Removes the element `key` from the set if it exists. - `contains(self, key)`: Checks if the `key` is in the set. - `size(self)`: Returns the number of elements in the set. - `clear(self)`: Removes all elements from the set. Input and Output Format - The `__init__` method can take any iterable, including another set or list. - The `add`, `remove`, and `contains` methods take a single element as input. - The `size` method returns an integer. - The `clear` method has no return value. Example Usage ```python # Example usage custom_set = CustomSet([1, 2, 3]) print(custom_set.size()) # Output: 3 custom_set.add(4) print(custom_set.contains(4)) # Output: True custom_set.remove(2) print(custom_set.contains(2)) # Output: False custom_set.clear() print(custom_set.size()) # Output: 0 ``` # Constraints - You should not use the built-in `set` or `frozenset` types in your implementation. - Ensure the `contains` and `size` methods are optimized for performance. - Include proper error handling for operations like removing elements from an empty set or adding unhashable elements. Submission Submit the Python class implementation of `CustomSet` with the required methods and error handling.","solution":"class CustomSet: def __init__(self, iterable=None): self.data = {} if iterable: for item in iterable: self.add(item) def add(self, key): if not isinstance(key, (int, str, bool, float, tuple)): raise TypeError(\\"unhashable type: \'{}\'\\".format(type(key).__name__)) self.data[key] = True def remove(self, key): if key not in self.data: raise KeyError(\\"Key not found: \'{}\'\\".format(key)) del self.data[key] def contains(self, key): return key in self.data def size(self): return len(self.data) def clear(self): self.data.clear()"},{"question":"# Feature Selection with Scikit-Learn Objective Your task is to implement a feature selection pipeline using Scikit-Learn. The goal is to select the most relevant features from a high-dimensional dataset and then train a classifier to predict the target variable. Problem Statement Given the `load_wine` dataset from scikit-learn, perform the following tasks: 1. Load the dataset and split it into training and testing sets. 2. Implement feature selection using `SelectKBest` with `f_classif` as the scoring function. 3. Use `Pipeline` to chain the feature selection with a classifier (e.g., `RandomForestClassifier`). 4. Train the model and evaluate its performance using accuracy score on the test set. Requirements 1. **Input**: No input is required as the dataset is loaded within the function. 2. **Output**: A dictionary with the following keys: - `\'num_features_selected\'`: Number of features selected by `SelectKBest`. - `\'accuracy\'`: Accuracy score of the trained model on the test set. 3. **Constraints**: - Use `SelectKBest` with `f_classif` to select the top 5 features. - Use `RandomForestClassifier` as the classifier. - Split the data into 80-20 training-testing sets. - Use a random state of 42 for reproducibility where applicable. Performance Ensure that the code runs efficiently on medium-sized datasets (e.g., wine dataset with 178 samples and 13 features). The solution should take advantage of scikit-learn\'s built-in functions and objects. Function Signature ```python def wine_feature_selection(): from sklearn.datasets import load_wine from sklearn.feature_selection import SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 1. Load the dataset X, y = load_wine(return_X_y=True) # 2. Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Create a feature selection transformer using SelectKBest selector = SelectKBest(score_func=f_classif, k=5) # 4. Define the classifier classifier = RandomForestClassifier(random_state=42) # 5. Create a pipeline that first selects features and then trains the classifier pipeline = Pipeline([ (\'feature_selection\', selector), (\'classification\', classifier) ]) # 6. Train the pipeline on the training data pipeline.fit(X_train, y_train) # 7. Evaluate the pipeline on the test data y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # 8. Retrieve the number of features selected num_features_selected = selector.get_support(indices=True).shape[0] # 9. Return the results return { \'num_features_selected\': num_features_selected, \'accuracy\': accuracy } ``` Implement the function `wine_feature_selection` with the provided functionality and requirements. Make sure to test the function to ensure it meets the accuracy and performance criteria.","solution":"def wine_feature_selection(): from sklearn.datasets import load_wine from sklearn.feature_selection import SelectKBest, f_classif from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score # 1. Load the dataset X, y = load_wine(return_X_y=True) # 2. Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 3. Create a feature selection transformer using SelectKBest selector = SelectKBest(score_func=f_classif, k=5) # 4. Define the classifier classifier = RandomForestClassifier(random_state=42) # 5. Create a pipeline that first selects features and then trains the classifier pipeline = Pipeline([ (\'feature_selection\', selector), (\'classification\', classifier) ]) # 6. Train the pipeline on the training data pipeline.fit(X_train, y_train) # 7. Evaluate the pipeline on the test data y_pred = pipeline.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # 8. Retrieve the number of features selected num_features_selected = selector.get_support(indices=True).shape[0] # 9. Return the results return { \'num_features_selected\': num_features_selected, \'accuracy\': accuracy }"},{"question":"**Objective**: Demonstrate your understanding of the `atexit` module, state management, and file operations in Python. **Task**: You are required to implement a simple logging system that logs messages to a file. The log file should be saved automatically when the Python interpreter terminates, without requiring explicit calls within the main program logic. Additionally, you should be able to add and remove log messages during the execution of the program. **Requirements**: 1. Create a class `Logger` that initializes with a file name for the log file. 2. Implement the following methods in the `Logger` class: - `log_message(self, message: str)`: Adds a log message to an internal list. - `remove_message(self, message: str)`: Removes all occurrences of a specific log message from the internal list. - `save_logs(self)`: Saves all log messages to the file specified during initialization. 3. Ensure that the `save_logs` method is registered to be called automatically at interpreter termination using the `atexit` module. 4. Implement proper handling to avoid duplicate log messages if `save_logs` is called multiple times. **Input/Output**: - There are no direct inputs/outputs. The class should manage state internally and save logs to the specified file during interpreter termination. **Constraints**: - You must use the `atexit` module to register the `save_logs` method. - The log file should be appended if it already exists; otherwise, it should be created. - Ensure thread-safety for the log operations if the logger is used in a multi-threaded environment. **Example**: ```python import threading import time import atexit class Logger: def __init__(self, filename): self.filename = filename self.messages = [] atexit.register(self.save_logs) def log_message(self, message): self.messages.append(message) def remove_message(self, message): self.messages = [msg for msg in self.messages if msg != message] def save_logs(self): if hasattr(self, \'_logs_saved\') and self._logs_saved: return self._logs_saved = True with open(self.filename, \'a\') as file: for message in self.messages: file.write(message + \'n\') # Example Usage logger = Logger(\'logfile.txt\') logger.log_message(\'First log message\') logger.log_message(\'Second log message\') logger.remove_message(\'First log message\') # End of program, logs should be saved automatically ``` **Performance Requirements**: - The `save_logs` method should complete in reasonable time given the constraint that log messages are assumed to be of reasonable length and quantity. Focus on implementing the class and methods as described. Testing the functionality in multi-threaded scenarios is encouraged to ensure robustness.","solution":"import atexit import threading class Logger: def __init__(self, filename): self.filename = filename self.messages = [] self._lock = threading.Lock() self._logs_saved = False atexit.register(self.save_logs) def log_message(self, message: str): with self._lock: self.messages.append(message) def remove_message(self, message: str): with self._lock: self.messages = [msg for msg in self.messages if msg != message] def save_logs(self): with self._lock: if self._logs_saved: return self._logs_saved = True with open(self.filename, \'a\') as file: for message in self.messages: file.write(message + \'n\')"},{"question":"# PyTorch Distributed RPC and RRef Protocol Implementation **Objective:** Design and implement a function to calculate the sum of tensors located on different workers using PyTorch\'s distributed RPC and RRef protocol. **Task:** You are required to implement a function `distributed_tensor_sum` that calculates the sum of tensors located on multiple remote worker nodes and returns the result to the caller. The function needs to handle tensors located on at least two different workers, sum them remotely, and collect the final result on a specified worker. **Requirements:** 1. **Setup a distributed RPC framework**: - Assume nodes named \'worker0\', \'worker1\', and \'worker2\'. - Initialize the RPC framework on these workers. 2. **Remote Tensor Sum Calculation**: - Distribute tensors among \'worker1\' and \'worker2\'. - Implement a function `remote_tensor_sum(worker_name, *tensors)` that takes a worker\'s name and a list of tensors, and uses RPC to sum the tensors on the specified worker. - The collected result should be gathered on \'worker0\'. 3. **RRef for Tensor Communication**: - Use RRef for creating and managing references to the remote tensors. - Ensure proper reference counting and cleanup of RRefs. **Constraints:** - Handle transient network failures assuming they might occur but will eventually resolve. - Ignore permanent node crashes or network partitions. - Process out-of-order messages as per the protocol described. **Input:** ```python def distributed_tensor_sum(): Sets up the distributed RPC framework, performs the distribution of tensors to worker nodes, sums the tensors remotely using RRefs and RPC calls, and finally collects the result on worker0. pass ``` **Output:** - The final summed tensor collected on \'worker0\'. **Example:** ```python import torch import torch.distributed.rpc as rpc def remote_tensor_sum(worker_name, *tensors): # Implementation using RPC to sum tensors on a specified worker. pass def distributed_tensor_sum(): # Example setup and execution # Assume initializing RPC here tensor_a = torch.ones(2) tensor_b = torch.ones(2) * 2 rref_a = rpc.remote(\'worker1\', torch.add, args=(tensor_a, 1)) rref_b = rpc.remote(\'worker2\', torch.add, args=(tensor_b, 1)) # Sum tensors using remote_tensor_sum and collect on worker0 pass distributed_tensor_sum() # Expected output on worker0: tensor([5., 5.]) # if tensor_a = tensor([1., 1.]) and tensor_b = tensor([2., 2.]) + 1 each ``` > **Note:** You should initialize the RPC framework properly with adequate `init_rpc` and `shutdown` calls. Implement proper error handling and cleanup as per best practices described in PyTorch documentation. **Evaluation Criteria:** - Correct implementation of distributed tensor sum. - Proper usage of PyTorch RPC and RRef protocols. - Efficient and clean code with adequate error handling and comments.","solution":"import torch import torch.distributed.rpc as rpc from torch.distributed.rpc import RRef import os def remote_tensor_sum(*tensors): Returns the sum of the given tensors. return sum(tensors) def distributed_tensor_sum(): Sets up the distributed RPC framework, performs the distribution of tensors to worker nodes, sums the tensors remotely using RRefs and RPC calls, and finally collects the result on worker0. # Initialize the RPC framework rpc.init_rpc(\\"worker0\\", rank=0, world_size=3) rpc.init_rpc(\\"worker1\\", rank=1, world_size=3) rpc.init_rpc(\\"worker2\\", rank=2, world_size=3) tensor_a = torch.ones(2) tensor_b = torch.ones(2) * 2 rref_a = rpc.remote(\\"worker1\\", torch.add, args=(tensor_a, 0)) # No additional operation here rref_b = rpc.remote(\\"worker2\\", torch.add, args=(tensor_b, 0)) # No additional operation here sum_rref_worker1 = rpc.rpc_sync(\\"worker1\\", remote_tensor_sum, args=(rref_a.to_here(),)) sum_rref_worker2 = rpc.rpc_sync(\\"worker2\\", remote_tensor_sum, args=(rref_b.to_here(),)) final_sum = sum_rref_worker1 + sum_rref_worker2 # Shutdown the RPC rpc.shutdown() return final_sum if __name__ == \\"__main__\\": if \\"RANK\\" in os.environ and int(os.environ[\\"RANK\\"]) == 0: result = distributed_tensor_sum() print(result)"},{"question":"You are given data representing various events at different venues. Each event has a name, a date, and a venue. The data is represented as a list of dictionaries, where each dictionary contains the keys `name`, `date`, and `venue`. ```python events = [ {\\"name\\": \\"Event 1\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue A\\"}, {\\"name\\": \\"Event 2\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue B\\"}, {\\"name\\": \\"Event 3\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue B\\"}, {\\"name\\": \\"Event 4\\", \\"date\\": \\"2023-04-15\\", \\"venue\\": \\"Venue A\\"}, {\\"name\\": \\"Event 5\\", \\"date\\": \\"2023-07-22\\", \\"venue\\": \\"Venue C\\"}, {\\"name\\": \\"Event 6\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue C\\"}, ] ``` Your task is to write a function `sort_events(events, primary_key, secondary_key=None, reverse_primary=False, reverse_secondary=False)` that sorts the list of events based on the provided sorting keys. The function should: 1. Sort the events primarily by the `primary_key`. 2. If `secondary_key` is provided, sort by the `secondary_key` next. 3. Allow sorting in descending order for both the primary and secondary keys using `reverse_primary` and `reverse_secondary` respectively. Input: - `events`: List of dictionaries, where each dictionary has keys `name`, `date`, and `venue`. - `primary_key`: String, the primary key to sort by (`\\"name\\"`, `\\"date\\"`, or `\\"venue\\"`). - `secondary_key`: Optional String, the secondary key to sort by (`\\"name\\"`, `\\"date\\"`, or `\\"venue\\"`). Default is `None`. - `reverse_primary`: Boolean, whether to sort the primary key in descending order. Default is `False`. - `reverse_secondary`: Boolean, whether to sort the secondary key in descending order. Default is `False`. Output: - A sorted list of events based on the provided keys and order criteria. Constraints: - The `primary_key` and `secondary_key` (if provided) must be one of `\\"name\\"`, `\\"date\\"`, or `\\"venue\\"`. - The `events` list can contain up to 10,000 dictionaries. - The format for `date` strings will always be \\"YYYY-MM-DD\\". Examples: ```python # Example 1: Sort by date, then by name events = [ {\\"name\\": \\"Event 1\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue A\\"}, {\\"name\\": \\"Event 2\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue B\\"}, {\\"name\\": \\"Event 3\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue B\\"}, {\\"name\\": \\"Event 4\\", \\"date\\": \\"2023-04-15\\", \\"venue\\": \\"Venue A\\"}, {\\"name\\": \\"Event 5\\", \\"date\\": \\"2023-07-22\\", \\"venue\\": \\"Venue C\\"}, {\\"name\\": \\"Event 6\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue C\\"}, ] sorted_events = sort_events(events, primary_key=\\"date\\", secondary_key=\\"name\\") print(sorted_events) # Output: # [ # {\\"name\\": \\"Event 4\\", \\"date\\": \\"2023-04-15\\", \\"venue\\": \\"Venue A\\"}, # {\\"name\\": \\"Event 2\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue B\\"}, # {\\"name\\": \\"Event 6\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue C\\"}, # {\\"name\\": \\"Event 1\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue A\\"}, # {\\"name\\": \\"Event 3\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue B\\"}, # {\\"name\\": \\"Event 5\\", \\"date\\": \\"2023-07-22\\", \\"venue\\": \\"Venue C\\"}, # ] # Example 2: Sort by venue in descending order, then by date sorted_events = sort_events(events, primary_key=\\"venue\\", secondary_key=\\"date\\", reverse_primary=True) print(sorted_events) # Output: # [ # {\\"name\\": \\"Event 5\\", \\"date\\": \\"2023-07-22\\", \\"venue\\": \\"Venue C\\"}, # {\\"name\\": \\"Event 6\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue C\\"}, # {\\"name\\": \\"Event 3\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue B\\"}, # {\\"name\\": \\"Event 2\\", \\"date\\": \\"2023-05-19\\", \\"venue\\": \\"Venue B\\"}, # {\\"name\\": \\"Event 1\\", \\"date\\": \\"2023-06-21\\", \\"venue\\": \\"Venue A\\"}, # {\\"name\\": \\"Event 4\\", \\"date\\": \\"2023-04-15\\", \\"venue\\": \\"Venue A\\"}, # ] ``` Note: - The sorting should be stable, meaning that events with equal keys should maintain their original relative order.","solution":"def sort_events(events, primary_key, secondary_key=None, reverse_primary=False, reverse_secondary=False): Sorts the list of events based on the primary_key and secondary_key. :param events: List of events (dictionaries) to sort. :param primary_key: String, the key to sort by first. :param secondary_key: String, the key to sort by second (optional). :param reverse_primary: Boolean, whether to reverse sort by the primary key. :param reverse_secondary: Boolean, whether to reverse sort by the secondary key. :return: Sorted list of events. if secondary_key: return sorted( events, key=lambda x: (x[primary_key], x[secondary_key]), reverse=reverse_primary ) return sorted(events, key=lambda x: x[primary_key], reverse=reverse_primary)"},{"question":"# Coding Assessment Task Objective: Demonstrate understanding of low-level Python system interaction by utilizing `os.PathLike` and process control functions. Problem Statement: You are tasked with creating a Python module that integrates filesystem path handling and inter-process communication using fork operations. Implement a function `process_path_handling` that: 1. **Validates a given path**: - If the path is a valid string or bytes path, return its filesystem representation. - If the path is an object implementing the `os.PathLike` interface, use its `__fspath__()` method and return the result if it\'s a string or bytes object. - If the path is invalid, raise a `TypeError`. 2. **Handles basic inter-process communication**: - Use fork to create a child process. - In the parent process, wait for the child process to complete and return its exit status. - In the child process, validate another given path, and print the result to stdout. Function Signature: ```python def process_path_handling(parent_path: \'os.PathLike\', child_path: \'os.PathLike\') -> int: # Implement the function here ``` Constraints: - Assume both `parent_path` and `child_path` are provided as arguments. - Use `os` module appropriately for path and process operations. - Ensure that the child process exits cleanly after printing the path validation result. Example: ```python import os class CustomPath: def __fspath__(self): return \\"/custom/path\\" # Example usage try: exit_status = process_path_handling(\\"/valid/parent/path\\", CustomPath()) print(f\\"Child process exited with status: {exit_status}\\") except TypeError as e: print(f\\"Error: {e}\\") ``` Use the `PyOS_FSPath` function (or its equivalent Python implementation) for filesystem path validation. **Note**: The function signature and scaffolding are intended to be implemented using Python concepts that mirror the C API functionalities described, promoting an understanding of how low-level controls can be applied in high-level Python. Evaluation Criteria: - Correctness: The function should correctly validate paths and handle process control. - Error handling: Properly raise and handle `TypeError` for invalid paths. - Process management: Correctly implement fork operations and manage parent-child process communication. - Code readability and structure. Good luck!","solution":"import os import sys def validate_path(path): if isinstance(path, (str, bytes)): return path elif isinstance(path, os.PathLike): result = path.__fspath__() if isinstance(result, (str, bytes)): return result raise TypeError(\\"Invalid path format\\") def process_path_handling(parent_path, child_path): try: parent_path_validated = validate_path(parent_path) except TypeError as e: raise e pid = os.fork() if pid == 0: # Child process try: child_path_validated = validate_path(child_path) print(child_path_validated) except TypeError as e: print(e, file=sys.stderr) finally: os._exit(0) else: # Parent process pid, status = os.waitpid(pid, 0) return os.WEXITSTATUS(status)"},{"question":"# MultiIndex Operations in Pandas You are given a multi-level indexed DataFrame representing sales data for different products over multiple years and regions. Implement functions to perform various operations on this DataFrame using the MultiIndex functionalities described in the documentation. Given DataFrame: ```python import pandas as pd import numpy as np # Sample data arrays = [ [\\"Region1\\", \\"Region1\\", \\"Region2\\", \\"Region2\\"], [\\"ProductA\\", \\"ProductB\\", \\"ProductA\\", \\"ProductB\\"], [2019, 2019, 2020, 2020] ] index = pd.MultiIndex.from_arrays(arrays, names=[\\"Region\\", \\"Product\\", \\"Year\\"]) data = { \\"Sales\\": [150, 120, 100, 130], \\"Profit\\": [50, 40, 30, 60], } df = pd.DataFrame(data, index=index) ``` Tasks: 1. **Extract Subset by Levels**: Write a function `extract_subset` that takes the DataFrame and a list of tuples representing the levels and their respective values to be selected. Return the subset of the DataFrame. ```python def extract_subset(df: pd.DataFrame, subset: list) -> pd.DataFrame: Parameters: df (pd.DataFrame): The original DataFrame with MultiIndex. subset (list of tuple): List where each tuple contains (level_name, value). Returns: pd.DataFrame: The subset of the DataFrame based on the provided level and values. pass ``` Example: ```python subset = [(\\"Region\\", \\"Region1\\"), (\\"Year\\", 2019)] new_df = extract_subset(df, subset) ``` 2. **Add New Level to MultiIndex**: Write a function `add_level` that takes the DataFrame, a new level name, and a list of values for the new level. Add this new level to the MultiIndex and return the updated DataFrame. ```python def add_level(df: pd.DataFrame, level_name: str, level_values: list) -> pd.DataFrame: Parameters: df (pd.DataFrame): The original DataFrame with MultiIndex. level_name (str): The new level name to be added. level_values (list): The values for the new level. Returns: pd.DataFrame: The DataFrame with the updated MultiIndex including the new level. pass ``` Example: ```python new_level_values = [\\"Q1\\", \\"Q1\\", \\"Q2\\", \\"Q2\\"] updated_df = add_level(df, \\"Quarter\\", new_level_values) ``` 3. **Compute Mean Sales by Product and Year**: Write a function `mean_sales` that computes the mean sales for each product and year across all regions. Restructure the results to have a DataFrame with `Product` and `Year` as indexes. ```python def mean_sales(df: pd.DataFrame) -> pd.DataFrame: Parameters: df (pd.DataFrame): The original DataFrame with MultiIndex. Returns: pd.DataFrame: DataFrame with Product and Year as indexes and mean Sales as values. pass ``` Example: ```python result_df = mean_sales(df) ``` Your implementation should utilize the MultiIndex functionalities effectively and demonstrate a good understanding of hierarchical indexing in pandas. Provide appropriate docstrings and include example usage for each function.","solution":"import pandas as pd def extract_subset(df: pd.DataFrame, subset: list) -> pd.DataFrame: Parameters: df (pd.DataFrame): The original DataFrame with MultiIndex. subset (list of tuple): List where each tuple contains (level_name, value). Returns: pd.DataFrame: The subset of the DataFrame based on the provided level and values. # Convert subset list to a dictionary for slicing subset_dict = {level: value for level, value in subset} # Use pd.IndexSlice to slice the dataframe return df.loc[pd.IndexSlice[subset_dict.get(\'Region\', slice(None)), subset_dict.get(\'Product\', slice(None)), subset_dict.get(\'Year\', slice(None))], :] def add_level(df: pd.DataFrame, level_name: str, level_values: list) -> pd.DataFrame: Parameters: df (pd.DataFrame): The original DataFrame with MultiIndex. level_name (str): The new level name to be added. level_values (list): The values for the new level. Returns: pd.DataFrame: The DataFrame with the updated MultiIndex including the new level. # Check if length of level_values matches the length of the DataFrame if len(level_values) != len(df): raise ValueError(\\"Length of level_values must match length of the DataFrame\\") # Add the new level new_index = df.index.copy() new_index = pd.MultiIndex.from_arrays([new_index.get_level_values(i) for i in range(len(new_index.levels))] + [level_values], names=df.index.names + [level_name]) return df.set_index(new_index) def mean_sales(df: pd.DataFrame) -> pd.DataFrame: Parameters: df (pd.DataFrame): The original DataFrame with MultiIndex. Returns: pd.DataFrame: DataFrame with Product and Year as indexes and mean Sales as values. # Group by Product and Year and calculate mean Sales result = df[\'Sales\'].groupby([\'Product\', \'Year\']).mean().reset_index() return result.set_index([\'Product\', \'Year\'])"},{"question":"You are required to design a file compression and decompression system using the `bz2` module of Python. The system should be able to handle both one-shot and incremental (de)compression operations. You need to implement the following functions: 1. **compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None** - Compress the content of `input_file` and write the compressed data to `output_file`. - `input_file`: The path to the input file to be compressed. - `output_file`: The path to the output file where the compressed data will be saved. - `compresslevel`: An integer between 1 and 9 specifying the level of compression (default is 9). 2. **decompress_file(input_file: str, output_file: str) -> None** - Decompress the content of `input_file` and write the decompressed data to `output_file`. - `input_file`: The path to the compressed input file. - `output_file`: The path to the output file where the decompressed data will be saved. 3. **incremental_compress(input_file: str, output_file: str, compresslevel: int = 9, chunksize: int = 1024) -> None** - Compress the content of `input_file` incrementally and write the compressed data to `output_file`. - `input_file`: The path to the input file to be compressed. - `output_file`: The path to the output file where the compressed data will be saved. - `compresslevel`: An integer between 1 and 9 specifying the level of compression (default is 9). - `chunksize`: The size of the chunks in bytes to be read from the input file and compressed incrementally. 4. **incremental_decompress(input_file: str, output_file: str, chunksize: int = 1024) -> None** - Decompress the content of `input_file` incrementally and write the decompressed data to `output_file`. - `input_file`: The path to the compressed input file. - `output_file`: The path to the output file where the decompressed data will be saved. - `chunksize`: The size of the chunks in bytes to be read from the input file and decompressed incrementally. # Constraints - You can assume that the input files will exist and are readable. - The output files should be created or overwritten if they already exist. - The functions should handle large files efficiently without consuming excessive memory. - Use appropriate error handling to manage file I/O errors and invalid compression levels. # Performance Requirements - Your implementations should be efficient in terms of both time and space complexity, particularly for large files. Aim to read and compress/decompress data in chunks rather than loading the entire file into memory. # Example Usage: ```python # One-shot compression and decompression compress_file(\'example.txt\', \'example.bz2\', compresslevel=7) decompress_file(\'example.bz2\', \'example_out.txt\') # Incremental compression and decompression incremental_compress(\'bigfile.txt\', \'bigfile.bz2\', compresslevel=7, chunksize=4096) incremental_decompress(\'bigfile.bz2\', \'bigfile_out.txt\', chunksize=4096) ``` Implement the specified functions to meet the requirements.","solution":"import bz2 def compress_file(input_file: str, output_file: str, compresslevel: int = 9) -> None: Compress the content of `input_file` and write the compressed data to `output_file`. if compresslevel < 1 or compresslevel > 9: raise ValueError(\\"compresslevel must be between 1 and 9\\") with open(input_file, \'rb\') as infile, bz2.open(output_file, \'wb\', compresslevel=compresslevel) as outfile: outfile.write(infile.read()) def decompress_file(input_file: str, output_file: str) -> None: Decompress the content of `input_file` and write the decompressed data to `output_file`. with bz2.open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: outfile.write(infile.read()) def incremental_compress(input_file: str, output_file: str, compresslevel: int = 9, chunksize: int = 1024) -> None: Compress the content of `input_file` incrementally and write the compressed data to `output_file`. if compresslevel < 1 or compresslevel > 9: raise ValueError(\\"compresslevel must be between 1 and 9\\") with open(input_file, \'rb\') as infile, bz2.open(output_file, \'wb\', compresslevel=compresslevel) as outfile: while chunk := infile.read(chunksize): outfile.write(chunk) def incremental_decompress(input_file: str, output_file: str, chunksize: int = 1024) -> None: Decompress the content of `input_file` incrementally and write the decompressed data to `output_file`. with bz2.open(input_file, \'rb\') as infile, open(output_file, \'wb\') as outfile: while chunk := infile.read(chunksize): outfile.write(chunk)"},{"question":"**Problem Statement:** You are given two datasets: \'fmri\' and \'seaice\'. Your task is to load, preprocess, and visualize these datasets using Seaborn\'s advanced plotting functions. Follow the steps below to accomplish this: # Part 1: Data Loading and Preprocessing 1. Load the \'fmri\' dataset from Seaborn’s data repository. 2. Filter the dataset to include only the region \'parietal\'. 3. Load the \'seaice\' dataset from Seaborn’s data repository. 4. Preprocess the \'seaice\' dataset by performing the following operations: - Extract the day of year and year from the \'Date\' column. - Filter the dataset to include years from 1980 onwards. - Convert the \'Year\' column to string type. - Pivot the dataset to have \'Day\' as rows and years as columns. - Retain only the \'1980\' and \'2019\' columns. - Drop rows with missing data. - Reset the index of the resultant dataframe. # Part 2: Visualization 1. Create a band plot using the processed \'seaice\' data: - x-axis should represent \'Day\'. - ymin and ymax should represent the \'1980\' and \'2019\' columns respectively. - Customize the band plot to have a semi-transparent fill with a visible edge. 2. Create a combined line and band plot using the filtered \'fmri\' data: - x-axis should represent \'timepoint\'. - y-axis should represent \'signal\'. - Color should differentiate between different \'event\' values. - Plot individual subject lines with a thin linewidth. - Overlay the plot with a band representing the error interval, and a line representing aggregated data. # Input Format - None, as you will use the built-in datasets from Seaborn. # Output - Two visualizations: 1. A band plot for the \'seaice\' dataset. 2. A combined line and band plot for the \'fmri\' dataset. # Example ```python import seaborn.objects as so from seaborn import load_dataset import pandas as pd # Part 1: Data Loading and Preprocessing # Load and filter \'fmri\' data fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") # Load and preprocess \'seaice\' data seaice = ( load_dataset(\\"seaice\\") .assign( Day=lambda x: x[\\"Date\\"].dt.day_of_year, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) # Part 2: Visualization # Band plot for \'seaice\' data p1 = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") p1.add(so.Band(alpha=0.5, edgewidth=2)) p1.show() # Combined line and band plot for \'fmri\' data p2 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Line(linewidth=0.5), group=\\"subject\\") .add(so.Band()) ) p2.show() ```","solution":"import seaborn.objects as so from seaborn import load_dataset import pandas as pd def load_and_preprocess_fmri(): Loads the \'fmri\' dataset and filters to include only the \'parietal\' region. fmri = load_dataset(\\"fmri\\").query(\\"region == \'parietal\'\\") return fmri def load_and_preprocess_seaice(): Loads and preprocesses the \'seaice\' dataset. Extracts \'Day\' and \'Year\' from the \'Date\' column, filters years from 1980 onwards, pivots the dataset, retains only \'1980\' and \'2019\' columns, drops rows with missing data, and resets the index. seaice_raw = load_dataset(\\"seaice\\") seaice = ( seaice_raw .assign( Day=lambda x: x[\\"Date\\"].dt.dayofyear, Year=lambda x: x[\\"Date\\"].dt.year, ) .query(\\"Year >= 1980\\") .astype({\\"Year\\": str}) .pivot(index=\\"Day\\", columns=\\"Year\\", values=\\"Extent\\") .filter([\\"1980\\", \\"2019\\"]) .dropna() .reset_index() ) return seaice def plot_seaice_band_plot(seaice): Creates and displays a band plot for the preprocessed \'seaice\' data. p1 = so.Plot(seaice, x=\\"Day\\", ymin=\\"1980\\", ymax=\\"2019\\") p1.add(so.Band(alpha=0.5, edgewidth=2)) p1.show() def plot_fmri_combined_plot(fmri): Creates and displays a combined line and band plot for the filtered \'fmri\' data. p2 = ( so.Plot(fmri, x=\\"timepoint\\", y=\\"signal\\", color=\\"event\\") .add(so.Line(linewidth=0.5), group=\\"subject\\") .add(so.Band()) ) p2.show()"},{"question":"# Advanced Seaborn Strip Plot Visualization Objective Construct a strip plot using Seaborn that effectively demonstrates your understanding of the different functionalities and customization options available in the library. Background You are provided with the `tips` dataset from Seaborn\'s repository, which contains information on restaurant tips, including the total bill, tip amount, gender, day of the week, and other relevant attributes. Task 1. **Load the `tips` dataset** using Seaborn. 2. **Create a strip plot** that shows the total bill amounts for each day of the week (`x=\\"total_bill\\"`, `y=\\"day\\"`). 3. **Enhance the strip plot** following these specifications: - Use the `hue` parameter to differentiate between two other variables: `sex` and `time`. - Apply a distinct Seaborn palette for color differentiation. - Enable dodging so that the points for different hues are separated. - Disable the jitter effect to maintain the clarity of the visualization. - Customize the plot by adjusting the marker size, shape, and transparency to enhance readability. 4. **Generate a facet grid** that separates the strip plots by the `time` variable, producing subplots for each time (lunch or dinner) within separate columns. Implementation Write a Python function `custom_stripplot()` that performs the above task. The function should not take any arguments and should display the resulting plot directly. Input and Output - **Input:** None - **Output:** The function should display the customized strip plot. Additional Requirements - The function implementation should not deviate from Seaborn\'s inherent functionality. Use Seaborn\'s API and methods throughout. - Ensure proper labeling and a legend that is easy to understand. - Enhance the plot\'s aesthetics to make it publication-ready. ```python import seaborn as sns import matplotlib.pyplot as plt def custom_stripplot(): # Load the dataset tips = sns.load_dataset(\\"tips\\") # Set the theme sns.set_theme(style=\\"whitegrid\\") # Create the strip plot sns.catplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", col=\\"time\\", palette=\\"deep\\", dodge=True, jitter=False, s=5, marker=\\"D\\", alpha=0.7, kind=\\"strip\\", height=5, aspect=0.75 ) # Show the plot plt.show() # Call the function to generate and display the plot custom_stripplot() ``` Constraints - Use only Seaborn and Matplotlib libraries for plot generation and customization. - Ensure the plots are meaningful and correctly interpreted based on the dataset provided. Performance - The plot generation should be efficient and should not take excessive time to render. - The visualization should be clear and provide insights at a glance.","solution":"import seaborn as sns import matplotlib.pyplot as plt def custom_stripplot(): Create a customized strip plot with Seaborn. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Set the aesthetic style of the plots sns.set_theme(style=\\"whitegrid\\") # Create the strip plot using FacetGrid for separate plots by \'time\' g = sns.catplot( data=tips, x=\\"total_bill\\", y=\\"day\\", hue=\\"sex\\", col=\\"time\\", palette=\\"deep\\", dodge=True, jitter=False, s=5, marker=\\"D\\", alpha=0.7, kind=\\"strip\\", height=5, aspect=0.75 ) # Add title and adjust layout g.set_titles(col_template=\\"{col_name} Time\\") g.set_axis_labels(\\"Total Bill\\", \\"Day of Week\\") g.add_legend(title=\\"Gender\\") # Show plot plt.show()"},{"question":"You are required to develop a simple financial transaction ledger that maintains and formats transaction entries using the `decimal` module from Python\'s standard library. The task should demonstrate your understanding of handling decimal arithmetic precisely and formatting the output for better readability. # Requirements: 1. Write a class `TransactionLedger` that maintains transactions and provides functionalities to add, remove, and retrieve transactions. 2. Each transaction should be stored with an exact decimal representation to avoid common floating-point issues. 3. Implement a method `add_transaction(description: str, amount: str)` that: - Takes `description` (a brief description of the transaction) and `amount` (a string representation of the amount). - Converts the `amount` to a `decimal.Decimal` type and stores the transaction. - Ensures amounts are stored with two decimal places precision. 4. Implement a method `remove_transaction(description: str)` that: - Removes a transaction by its description. 5. Implement a method `get_balance()` to: - Calculate and return the current balance as a `decimal.Decimal`. 6. Implement a method `list_transactions()` that: - Returns a pretty-printed string of all transactions sorted by description. - Use the `pprint` module to format the output. # Input and Output Format: - **Input:** - `add_transaction` takes a string description and a string amount. - `remove_transaction` takes a string description. - `get_balance()` does not take any input. - `list_transactions()` does not take any input. - **Output:** - `get_balance()` returns a `decimal.Decimal` representing the balance. - `list_transactions()` returns a string with pretty-printed transactions. # Example Usage: ```python ledger = TransactionLedger() ledger.add_transaction(\\"Coffee\\", \\"2.50\\") ledger.add_transaction(\\"Book\\", \\"12.99\\") ledger.add_transaction(\\"Sandwich\\", \\"5.75\\") # Removing a transaction ledger.remove_transaction(\\"Book\\") print(ledger.get_balance()) # Output: Decimal(\'8.25\') print(ledger.list_transactions()) # Output: (Formatted string of transactions) ``` # Constraints: - Ensure precision to two decimal places for all amounts. - Ensure all descriptions are unique. # Note: - Use appropriate error handling where necessary. - The methods should be efficient in terms of time and space complexity.","solution":"from decimal import Decimal, ROUND_DOWN from collections import OrderedDict import pprint class TransactionLedger: def __init__(self): self.transactions = OrderedDict() def add_transaction(self, description: str, amount: str): Adds a transaction to the ledger with the given description and amount. The amount is stored as a Decimal with two decimal places precision. if description in self.transactions: raise ValueError(f\\"Transaction with description \'{description}\' already exists.\\") amount_decimal = Decimal(amount) amount_decimal = amount_decimal.quantize(Decimal(\'0.00\'), rounding=ROUND_DOWN) self.transactions[description] = amount_decimal def remove_transaction(self, description: str): Removes the transaction with the given description from the ledger. if description not in self.transactions: raise ValueError(f\\"No transaction found with description \'{description}\'.\\") del self.transactions[description] def get_balance(self): Returns the current balance as a Decimal. return sum(self.transactions.values(), Decimal(\'0.00\')) def list_transactions(self): Returns a pretty-printed string of all transactions sorted by description. sorted_transactions = dict(sorted(self.transactions.items())) pp = pprint.PrettyPrinter(indent=4) return pp.pformat(sorted_transactions) # Example usage ledger = TransactionLedger() ledger.add_transaction(\\"Coffee\\", \\"2.50\\") ledger.add_transaction(\\"Book\\", \\"12.99\\") ledger.add_transaction(\\"Sandwich\\", \\"5.75\\") # Removing a transaction ledger.remove_transaction(\\"Book\\") print(ledger.get_balance()) # Output: Decimal(\'8.25\') print(ledger.list_transactions()) # Output: Pretty-printed transactions"},{"question":"# Python/C API Slice and Ellipsis Implementation Challenge Objective: You are required to implement a custom function in Python that emulates portions of the behavior described in the Python/C API documentation regarding slice objects and handling sequences. Function Signature: ```python def custom_slice(sequence, start, stop, step): Emulates slicing behavior with additional error checking and custom handling. Args: - sequence (list or tuple): The sequence to slice. - start (int or None): The starting index of the slice. If None, it defaults based on the step. - stop (int or None): The stopping index of the slice. If None, it defaults based on the step. - step (int or None): The step index of the slice. If None, it defaults to 1. Returns: - sliced sequence (list or tuple): The sliced portion of the sequence. Constraints: - The function must handle out-of-bounds indices gracefully. - The function must raise a ValueError if the step is 0. - The function must handle backward slicing (negative steps). - The function must correctly interpret None values as appropriate Python slicing would. pass ``` Constraints: 1. You cannot use Python\'s built-in slicing capabilities directly (e.g., `sequence[start:stop:step]`). Implement the indices calculation and slicing manually. 2. Handle Python-style negative indexing and out-of-bounds slicing correctly. 3. Ensure the function handles both lists and tuples. 4. You should raise a `TypeError` if the input sequence is neither a list nor a tuple. 5. In case of a `step` value of 0, raise a `ValueError`. Example Usage: ```python # Example 1: sequence = [1, 2, 3, 4, 5] print(custom_slice(sequence, 1, 4, 2)) # Output: [2, 4] # Example 2: sequence = (10, 20, 30, 40, 50) print(custom_slice(sequence, None, None, -1)) # Output: (50, 40, 30, 20, 10) # Example 3: sequence = [1, 2, 3, 4, 5] print(custom_slice(sequence, -1, -6, -1)) # Output: [5, 4, 3, 2, 1] ``` Notes: - Ensure the function returns a result of the same type as the input sequence. - Thoroughly test the function with various edge cases such as empty sequences, out-of-bounds indices, and different step values. Implementing this function will help you understand Python/C API slice concepts and deepen your understanding of Python slicing mechanisms.","solution":"def custom_slice(sequence, start, stop, step): Emulates slicing behavior with additional error checking and custom handling. Args: - sequence (list or tuple): The sequence to slice. - start (int or None): The starting index of the slice. If None, it defaults based on the step. - stop (int or None): The stopping index of the slice. If None, it defaults based on the step. - step (int or None): The step index of the slice. If None, it defaults to 1. Returns: - sliced sequence (list or tuple): The sliced portion of the sequence. Constraints: - The function must handle out-of-bounds indices gracefully. - The function must raise a ValueError if the step is 0. - The function must handle backward slicing (negative steps). - The function must correctly interpret None values as appropriate Python slicing would. if not isinstance(sequence, (list, tuple)): raise TypeError(\\"Sequence must be a list or tuple\\") if step == 0: raise ValueError(\\"Step cannot be zero\\") if step is None: step = 1 seq_length = len(sequence) if start is None: start = 0 if step > 0 else seq_length - 1 elif start < 0: start += seq_length if stop is None: stop = seq_length if step > 0 else -1 elif stop < 0: stop += seq_length result = [] index = start while (step > 0 and index < stop) or (step < 0 and index > stop): if 0 <= index < seq_length: result.append(sequence[index]) index += step return type(sequence)(result)"},{"question":"# Task You are designing a Python application that needs to serialize and deserialize various data types using the `marshal` module. Your task is to: 1. Implement a function `serialize_data(data, version)` that takes a Python object `data` and an integer `version` representing the format version of `marshal` and returns the serialized bytes object. 2. Implement a function `deserialize_data(serialized_bytes)` that takes a bytes object `serialized_bytes` and returns the deserialized Python object. It should handle exceptions and return `None` if the data couldn\'t be deserialized successfully. 3. Implement a function `save_data_to_file(data, file_path, version)` that serializes the `data` and saves it to a file specified by `file_path`. 4. Implement a function `load_data_from_file(file_path)` that reads and deserializes data from a file specified by `file_path` and returns the corresponding Python object. # Constraints 1. The `data` parameter can only be of types that are supported by the `marshal` module: booleans, integers, floats, complex numbers, strings, bytes, bytearrays, tuples, lists, sets, frozensets, dictionaries, code objects, and the singletons `None`, `Ellipsis`, and `StopIteration`. 2. The `version` parameter for serialization should be in the range of 0 to 4. 3. The file to which data is saved should be opened in binary write mode, and the file from which data is loaded should be opened in binary read mode. 4. Ensure proper error handling when dealing with file operations or unsupported data types. 5. The performance of serialization and deserialization should be optimized for large data structures. # Example ```python data = {\'name\': \'Alice\', \'age\': 30, \'active\': True} version = 4 file_path = \'user_data.bin\' # Serialize and save the data to a file save_data_to_file(data, file_path, version) # Load and deserialize the data from the file loaded_data = load_data_from_file(file_path) print(loaded_data) # Output: {\'name\': \'Alice\', \'age\': 30, \'active\': True} ``` # Implementation ```python import marshal def serialize_data(data, version): try: serialized_bytes = marshal.dumps(data, version) return serialized_bytes except ValueError: return None def deserialize_data(serialized_bytes): try: data = marshal.loads(serialized_bytes) return data except (EOFError, ValueError, TypeError): return None def save_data_to_file(data, file_path, version): try: with open(file_path, \'wb\') as file: marshal.dump(data, file, version) except (ValueError, IOError): pass def load_data_from_file(file_path): try: with open(file_path, \'rb\') as file: data = marshal.load(file) return data except (EOFError, ValueError, TypeError, IOError): return None ```","solution":"import marshal def serialize_data(data, version): Serializes the given data using marshal. Parameters: - data: The Python object to serialize. - version: The format version for marshal (should be between 0 and 4). Returns: - The serialized bytes object, or None if serialization fails. if not (0 <= version <= 4): return None try: serialized_bytes = marshal.dumps(data, version) return serialized_bytes except ValueError: return None def deserialize_data(serialized_bytes): Deserializes the given bytes object using marshal. Parameters: - serialized_bytes: The bytes object to deserialize. Returns: - The deserialized Python object, or None if deserialization fails. try: data = marshal.loads(serialized_bytes) return data except (EOFError, ValueError, TypeError): return None def save_data_to_file(data, file_path, version): Serializes the data and saves it to a file. Parameters: - data: The Python object to serialize and save. - file_path: The path to the file. - version: The format version for marshal (should be between 0 and 4). Returns: - None try: with open(file_path, \'wb\') as file: marshal.dump(data, file, version) except (ValueError, IOError): pass def load_data_from_file(file_path): Loads and deserializes data from a file. Parameters: - file_path: The path to the file. Returns: - The deserialized Python object, or None if deserialization fails. try: with open(file_path, \'rb\') as file: data = marshal.load(file) return data except (EOFError, ValueError, TypeError, IOError): return None"},{"question":"# Problem: Transforming and Analyzing Python AST You are given a Python script as an input string. Your task is to manipulate its AST to achieve the following: 1. Replace all occurrences of the binary addition operator (`+`) with a multiplication operator (`*`). 2. Collect and return all function names that are defined in the script. To accomplish this, you need to implement two functions: 1. `transform_add_to_mult(script: str) -> str`: This function takes a Python script as a string, parses it into an AST, traverses and transforms it by replacing all `+` binary operations with `*`, and then unparses the modified AST back to a Python script. 2. `get_defined_function_names(script: str) -> List[str]`: This function takes a Python script as a string, parses it into an AST, and returns a list of names of all the functions defined in the script. **Input:** - A string `script` representing a valid Python script. **Output:** - For `transform_add_to_mult`, return a string containing the transformed Python script. - For `get_defined_function_names`, return a list of function names. **Constraints:** - The input script will contain valid Python code. - The Python script can have multiple function definitions, nested function definitions, and various expressions/statements. Example: ```python script = \'\'\' def add(a, b): return a + b def square(x): return x + x \'\'\' new_script = transform_add_to_mult(script) function_names = get_defined_function_names(script) print(new_script) # Output should be: # \'\'\' # def add(a, b): # return a * b # # def square(x): # return x * x # \'\'\' print(function_names) # Output should be: # [\'add\', \'square\'] ``` **Guidelines:** 1. Use the `ast` module to parse the script, manipulate the AST, and unparse it. 2. Implement the visitor and transformer patterns using `NodeVisitor` and `NodeTransformer` as needed to traverse and modify the AST nodes. 3. Ensure that the transformed script is syntactically correct. # Additional Information: Here is an outline to get you started: ```python import ast # Function to transform \'+\' to \'*\' def transform_add_to_mult(script: str) -> str: class AddToMultTransformer(ast.NodeTransformer): def visit_BinOp(self, node): # Check if the operator is an addition if isinstance(node.op, ast.Add): node.op = ast.Mult() # Recursively visit all children nodes self.generic_visit(node) return node # Parse the script into an AST tree = ast.parse(script) # Transform the AST tree = AddToMultTransformer().visit(tree) # Fix any missing location information ast.fix_missing_locations(tree) # Unparse the transformed AST back to a string return ast.unparse(tree) # Function to collect function names def get_defined_function_names(script: str) -> List[str]: class FunctionNameCollector(ast.NodeVisitor): def __init__(self): self.function_names = [] # Visit function definitions def visit_FunctionDef(self, node): self.function_names.append(node.name) # Recursively visit all children nodes self.generic_visit(node) # Parse the script into an AST tree = ast.parse(script) # Collect function names collector = FunctionNameCollector() collector.visit(tree) return collector.function_names # Test example script = \'\'\' def add(a, b): return a + b def square(x): return x + x \'\'\' new_script = transform_add_to_mult(script) function_names = get_defined_function_names(script) print(new_script) print(function_names) ```","solution":"import ast from typing import List # Function to transform \'+\' to \'*\' def transform_add_to_mult(script: str) -> str: class AddToMultTransformer(ast.NodeTransformer): def visit_BinOp(self, node): # Check if the operator is an addition if isinstance(node.op, ast.Add): node.op = ast.Mult() # Recursively visit all children nodes self.generic_visit(node) return node # Parse the script into an AST tree = ast.parse(script) # Transform the AST tree = AddToMultTransformer().visit(tree) # Fix any missing location information ast.fix_missing_locations(tree) # Unparse the transformed AST back to a string return ast.unparse(tree) # Function to collect function names def get_defined_function_names(script: str) -> List[str]: class FunctionNameCollector(ast.NodeVisitor): def __init__(self): self.function_names = [] # Visit function definitions def visit_FunctionDef(self, node): self.function_names.append(node.name) # Recursively visit all children nodes self.generic_visit(node) # Parse the script into an AST tree = ast.parse(script) # Collect function names collector = FunctionNameCollector() collector.visit(tree) return collector.function_names"},{"question":"# Question: Implement a custom machine learning pipeline using scikit-learn\'s pairwise metrics and kernels. The pipeline should: 1. Calculate pairwise distances using the **Cosine Similarity** metric. 2. Compute a kernel matrix using the **RBF Kernel**. 3. Train a Support Vector Machine (SVM) classifier using the precomputed kernel matrix. 4. Evaluate the accuracy of the SVM classifier on a test set. Requirements and Constraints: - You must use the `cosine_similarity` function for calculating the pairwise distances. - The `rbf_kernel` function should be used to compute the kernel matrix with a specified gamma value. - The SVM classifier should use `kernel=\'precomputed\'` for the precomputed kernel matrix. - You should split your dataset into training and testing subsets. - Report the accuracy of the classifier on the test set. Input and Output Formats: - **Input**: - `X_train`: A numpy array of shape `(n_train_samples, n_features)`. - `y_train`: A numpy array of shape `(n_train_samples,)` containing the class labels for `X_train`. - `X_test`: A numpy array of shape `(n_test_samples, n_features)`. - `y_test`: A numpy array of shape `(n_test_samples,)` containing the class labels for `X_test`. - `gamma`: A float value to be used as the parameter for the `rbf_kernel`. - **Output**: - A float value representing the accuracy of the SVM classifier on the test set. Example: ```python import numpy as np from sklearn.metrics.pairwise import cosine_similarity, rbf_kernel from sklearn.svm import SVC from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris def custom_svm_pipeline(X_train, y_train, X_test, y_test, gamma): # Calculate cosine similarity for pairwise distances X_cosine = cosine_similarity(X_train) # Compute RBF kernel with the specified gamma value K_train = rbf_kernel(X_train, gamma=gamma) K_test = rbf_kernel(X_test, X_train, gamma=gamma) # Test kernel should consider training data # Train SVM classifier using precomputed kernel svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) # Evaluate the classifier on the test set using the precomputed kernel matrix accuracy = svm.score(K_test, y_test) return accuracy # Load example dataset data = load_iris() X = data.data y = data.target # Split data into train and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Run the custom pipeline gamma_value = 0.1 accuracy = custom_svm_pipeline(X_train, y_train, X_test, y_test, gamma_value) print(f\\"Accuracy: {accuracy:.2f}\\") ``` In this example, the `custom_svm_pipeline` function: 1. Calculates cosine similarity for X. 2. Computes the RBF kernel matrix. 3. Trains an SVM classifier using the precomputed kernel matrix. 4. Returns the accuracy score of the classifier on the test set. Ensure your solution adheres to the input and output formats specified and correctly implements the required steps.","solution":"import numpy as np from sklearn.metrics.pairwise import cosine_similarity, rbf_kernel from sklearn.svm import SVC def custom_svm_pipeline(X_train, y_train, X_test, y_test, gamma): Custom SVM pipeline that calculates cosine similarity pairwise distances, computes RBF kernel, and evaluates SVM classifier accuracy. Parameters: X_train (np.ndarray): Training data features. y_train (np.ndarray): Training data labels. X_test (np.ndarray): Testing data features. y_test (np.ndarray): Testing data labels. gamma (float): Gamma parameter for RBF kernel. Returns: float: Accuracy of the SVM classifier on the test set. # Calculate cosine similarity for pairwise distances X_cosine = cosine_similarity(X_train) # Compute RBF kernel with the specified gamma value K_train = rbf_kernel(X_train, gamma=gamma) K_test = rbf_kernel(X_test, X_train, gamma=gamma) # Test kernel should consider training data # Train SVM classifier using precomputed kernel svm = SVC(kernel=\'precomputed\') svm.fit(K_train, y_train) # Evaluate the classifier on the test set using the precomputed kernel matrix accuracy = svm.score(K_test, y_test) return accuracy"},{"question":"Objective: Write a function named `transform_sales_data` that reshapes sales data into a specified format using `pandas`. This will test your comprehension of the `pivot`, `melt`, `stack`, and `unstack` functions. Requirements: 1. **Input Format**: - A `pandas.DataFrame` named `sales_df` with the following columns: `region`, `product`, `sales_date`, `sales_amount`. Example: ```python sales_df = pd.DataFrame({ \\"region\\": [\\"North\\", \\"North\\", \\"South\\", \\"South\\", \\"East\\", \\"East\\"], \\"product\\": [\\"A\\", \\"B\\", \\"A\\", \\"B\\", \\"A\\", \\"B\\"], \\"sales_date\\": pd.to_datetime([\\"2021-01-01\\", \\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-02\\", \\"2021-01-03\\", \\"2021-01-03\\"]), \\"sales_amount\\": [100, 150, 200, 250, 300, 350] }) ``` - A string `output_format` that specifies the format in which data should be transformed. It can have values: - `\'pivot\'`: Converts the data to wide format with sales dates as index and products as columns. - `\'melt\'`: Converts the data to long format with sales dates, regions, sales amounts and product. - `\'stack\'`: Stacks the product columns into rows for each sales date. - `\'unstack\'`: Unstacks the index to pivot the regions column. 2. **Output Format**: - Return a `pandas.DataFrame` that is transformed according to the specified `output_format`. 3. **Constraints**: - The input `sales_df` will have no missing values. - Assume all dates within `sales_date` column are unique, so no need to handle duplicate dates within data context. Implementation: ```python def transform_sales_data(sales_df: pd.DataFrame, output_format: str) -> pd.DataFrame: if output_format == \'pivot\': # Pivot table with sales_date as index and product as columns transformed_df = sales_df.pivot(index=\\"sales_date\\", columns=\\"product\\", values=\\"sales_amount\\") elif output_format == \'melt\': # Melt the dataframe transformed_df = pd.melt(sales_df, id_vars=[\\"sales_date\\", \\"region\\"], value_vars=[\\"sales_amount\\"], var_name=\\"measurement\\", value_name=\\"value\\") elif output_format == \'stack\': # Pivot, then stack the sales_df pivoted_df = sales_df.pivot(index=\\"sales_date\\", columns=\\"product\\", values=\\"sales_amount\\") transformed_df = pivoted_df.stack() elif output_format == \'unstack\': # Unstack the pivoted dataframe pivoted_df = sales_df.pivot(index=\\"region\\", columns=\\"product\\", values=\\"sales_amount\\") transformed_df = pivoted_df.unstack() else: raise ValueError(\\"Invalid output_format value. Accepted values are: \'pivot\', \'melt\', \'stack\', \'unstack\'\\") return transformed_df # Example Usage sales_data = pd.DataFrame({ \\"region\\": [\\"North\\", \\"North\\", \\"South\\", \\"South\\", \\"East\\", \\"East\\"], \\"product\\": [\\"A\\", \\"B\\", \\"A\\", \\"B\\", \\"A\\", \\"B\\"], \\"sales_date\\": pd.to_datetime([\\"2021-01-01\\", \\"2021-01-01\\", \\"2021-01-02\\", \\"2021-01-02\\", \\"2021-01-03\\", \\"2021-01-03\\"]), \\"sales_amount\\": [100, 150, 200, 250, 300, 350] }) output = transform_sales_data(sales_data, \'pivot\') print(output) ``` Notes: - Ensure to properly handle the shape of DataFrame as it will vary based on `output_format`. - Add error handling for invalid `output_format` inputs.","solution":"import pandas as pd def transform_sales_data(sales_df: pd.DataFrame, output_format: str) -> pd.DataFrame: if output_format == \'pivot\': # Pivot table with sales_date as index and product as columns transformed_df = sales_df.pivot(index=\\"sales_date\\", columns=\\"product\\", values=\\"sales_amount\\") elif output_format == \'melt\': # Melt the dataframe transformed_df = pd.melt(sales_df, id_vars=[\\"sales_date\\", \\"region\\"], value_vars=[\\"sales_amount\\"], var_name=\\"measurement\\", value_name=\\"value\\") elif output_format == \'stack\': # Pivot, then stack the sales_df pivoted_df = sales_df.pivot(index=\\"sales_date\\", columns=\\"product\\", values=\\"sales_amount\\") transformed_df = pivoted_df.stack() elif output_format == \'unstack\': # Unstack the pivoted dataframe pivoted_df = sales_df.pivot(index=\\"region\\", columns=\\"product\\", values=\\"sales_amount\\") transformed_df = pivoted_df.unstack() else: raise ValueError(\\"Invalid output_format value. Accepted values are: \'pivot\', \'melt\', \'stack\', \'unstack\'\\") return transformed_df"},{"question":"# **Advanced SQLite3 Coding Assessment** You are required to design a student management system using the `sqlite3` module in Python. This system should manage students, courses, and enrollments. The following specifications outline the requirements: 1. **Database Schema**: - Create a database named `students.db`. - Create the following tables: - `Student`: columns `id` (primary key), `name` (text), `dob` (date) - `Course`: columns `id` (primary key), `name` (text), `credits` (integer) - `Enrollment`: columns `student_id` (foreign key references `Student(id)`), `course_id` (foreign key references `Course(id)`), `grade` (real) 2. **CRUD Operations**: - Write functions to create, read, update, and delete students and courses. - Write a function to enroll a student in a course. - Write a function to update the grade of a student for a specific course. - Write a function to retrieve a student\'s transcript (list of courses and grades). - Write a function to retrieve a course\'s roster (list of students enrolled). 3. **Transaction Handling**: - Ensure that enrollment and grade updates are handled within transactions. 4. **Advanced Features**: - Use a custom row factory to return results from the database as dictionaries. - Create a custom adapter and converter to handle the `dob` as Python `datetime.date` objects. # **Function Specifications** 1. **create_tables**: ```python def create_tables(): Creates the necessary tables for the student management system. ``` 2. **add_student**: ```python def add_student(name: str, dob: datetime.date): Adds a new student to the Student table. ``` 3. **get_student**: ```python def get_student(student_id: int) -> dict: Retrieves a student\'s information by ID. ``` 4. **update_student**: ```python def update_student(student_id: int, name: str, dob: datetime.date): Updates a student\'s information. ``` 5. **delete_student**: ```python def delete_student(student_id: int): Deletes a student by ID. ``` 6. **add_course**: ```python def add_course(name: str, credits: int): Adds a new course to the Course table. ``` 7. **get_course**: ```python def get_course(course_id: int) -> dict: Retrieves a course\'s information by ID. ``` 8. **update_course**: ```python def update_course(course_id: int, name: str, credits: int): Updates a course\'s information. ``` 9. **delete_course**: ```python def delete_course(course_id: int): Deletes a course by ID. ``` 10. **enroll_student**: ```python def enroll_student(student_id: int, course_id: int): Enrolls a student in a course. ``` 11. **update_grade**: ```python def update_grade(student_id: int, course_id: int, grade: float): Updates the grade of a student for a specific course. ``` 12. **get_transcript**: ```python def get_transcript(student_id: int) -> list: Retrieves a student\'s transcript. ``` 13. **get_roster**: ```python def get_roster(course_id: int) -> list: Retrieves a course\'s roster. ``` # **Constraints** - You must use prepared statements to prevent SQL injection. - Use `sqlite3`’s transaction management for enrollment and grade updating. - Implement a custom adapter and converter to handle `dob` as `datetime.date`. - Use a custom row factory to return results as dictionaries. # **Sample Usage** ```python if __name__ == \\"__main__\\": from datetime import datetime create_tables() # Add new students add_student(\\"Alice\\", datetime.strptime(\\"2000-01-01\\", \\"%Y-%m-%d\\").date()) add_student(\\"Bob\\", datetime.strptime(\\"2001-02-02\\", \\"%Y-%m-%d\\").date()) # Add new courses add_course(\\"Math\\", 3) add_course(\\"History\\", 4) # Enroll students in courses enroll_student(1, 1) enroll_student(1, 2) enroll_student(2, 1) # Update grades update_grade(1, 1, 3.5) update_grade(1, 2, 4.0) update_grade(2, 1, 3.0) # Retrieve student transcript print(get_transcript(1)) # [{\'name\': \'Math\', \'grade\': 3.5}, {\'name\': \'History\', \'grade\': 4.0}] # Retrieve course roster print(get_roster(1)) # [{\'name\': \'Alice\', \'grade\': 3.5}, {\'name\': \'Bob\', \'grade\': 3.0}] ``` # Expected Input and Output Formats - **Functions**: Implement each function as specified. - **Input Types**: String, integer, and `datetime.date` for input values. - **Output Types**: Dictionary for individual records, list of dictionaries for multiple records. Make sure your functions handle and raise appropriate exceptions for invalid inputs and operations. Use Python\'s `unittest` framework for writing test cases to validate your implementations.","solution":"import sqlite3 from datetime import datetime, date # Custom adapter and converter registration for datetime.date def adapt_date(val): return val.strftime(\\"%Y-%m-%d\\") def convert_date(val): return datetime.strptime(val.decode(\\"utf-8\\"), \\"%Y-%m-%d\\").date() sqlite3.register_adapter(date, adapt_date) sqlite3.register_converter(\\"DATE\\", convert_date) def dict_factory(cursor, row): d = {} for idx, col in enumerate(cursor.description): d[col[0]] = row[idx] return d def create_tables(): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: conn.row_factory = dict_factory c = conn.cursor() c.execute(\'\'\' CREATE TABLE IF NOT EXISTS Student ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, dob DATE NOT NULL) \'\'\') c.execute(\'\'\' CREATE TABLE IF NOT EXISTS Course ( id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT NOT NULL, credits INTEGER NOT NULL) \'\'\') c.execute(\'\'\' CREATE TABLE IF NOT EXISTS Enrollment ( student_id INTEGER, course_id INTEGER, grade REAL, PRIMARY KEY (student_id, course_id), FOREIGN KEY (student_id) REFERENCES Student(id), FOREIGN KEY (course_id) REFERENCES Course(id)) \'\'\') def add_student(name: str, dob: date): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"INSERT INTO Student (name, dob) VALUES (?, ?)\\", (name, dob)) def get_student(student_id: int) -> dict: with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: conn.row_factory = dict_factory c = conn.cursor() c.execute(\\"SELECT * FROM Student WHERE id = ?\\", (student_id,)) return c.fetchone() def update_student(student_id: int, name: str, dob: date): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"UPDATE Student SET name = ?, dob = ? WHERE id = ?\\", (name, dob, student_id)) def delete_student(student_id: int): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"DELETE FROM Student WHERE id = ?\\", (student_id,)) def add_course(name: str, credits: int): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"INSERT INTO Course (name, credits) VALUES (?, ?)\\", (name, credits)) def get_course(course_id: int) -> dict: with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: conn.row_factory= dict_factory c = conn.cursor() c.execute(\\"SELECT * FROM Course WHERE id = ?\\", (course_id,)) return c.fetchone() def update_course(course_id: int, name: str, credits: int): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"UPDATE Course SET name = ?, credits = ? WHERE id = ?\\", (name, credits, course_id)) def delete_course(course_id: int): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"DELETE FROM Course WHERE id = ?\\", (course_id,)) def enroll_student(student_id: int, course_id: int): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"INSERT INTO Enrollment (student_id, course_id) VALUES (?, ?)\\", (student_id, course_id)) def update_grade(student_id: int, course_id: int, grade: float): with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: c = conn.cursor() c.execute(\\"UPDATE Enrollment SET grade = ? WHERE student_id = ? AND course_id = ?\\", (grade, student_id, course_id)) def get_transcript(student_id: int) -> list: with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: conn.row_factory = dict_factory c = conn.cursor() c.execute( SELECT Course.name, Enrollment.grade FROM Enrollment JOIN Course ON Enrollment.course_id = Course.id WHERE Enrollment.student_id = ? , (student_id,)) return c.fetchall() def get_roster(course_id: int) -> list: with sqlite3.connect(\\"students.db\\", detect_types=sqlite3.PARSE_DECLTYPES) as conn: conn.row_factory = dict_factory c = conn.cursor() c.execute( SELECT Student.name, Enrollment.grade FROM Enrollment JOIN Student ON Enrollment.student_id = Student.id WHERE Enrollment.course_id = ? , (course_id,)) return c.fetchall()"},{"question":"You are provided with a directory containing multiple parquet files, each containing data for a different year. Your task is to create a summary DataFrame that provides efficient memory usage by selectively loading columns, converting data types, and processing the data in chunks. Specifically, you need to: 1. Load only the `name`, `id`, `x`, and `y` columns from each parquet file. 2. Convert the `name` column to a `Categorical` type to save memory. 3. Downcast the `id` column to the smallest appropriate numeric type. 4. Downcast the `x` and `y` columns to the smallest appropriate float type. 5. Compute the mean value of the `x` and `y` columns and the count of unique `name` values for the entire dataset. # Input and Output Formats - **Input**: A path to the directory containing parquet files. - **Output**: A pandas DataFrame with the following columns: - `x_mean`: The mean value of the `x` column across all files. - `y_mean`: The mean value of the `y` column across all files. - `unique_name_count`: The total count of unique `name` values across all files. # Constraints - The memory usage should be minimized using appropriate techniques. - Assume that the dataset is too large to fit entirely into memory. # Example Suppose you have a directory structure like this: ``` data └── timeseries ├── ts-00.parquet ├── ts-01.parquet ... ``` Here is the function signature: ```python import pandas as pd import pathlib def process_large_dataset(directory_path): Process large dataset efficiently and compute required metrics. Parameters: directory_path (str): The path to the directory containing parquet files. Returns: pd.DataFrame: A DataFrame containing the mean values of x and y columns and the count of unique name values. # Initialize accumulators x_sum, y_sum = 0, 0 row_count = 0 unique_names = set() # Process each file files = pathlib.Path(directory_path).glob(\\"*.parquet\\") for path in files: df = pd.read_parquet(path, columns=[\\"name\\", \\"id\\", \\"x\\", \\"y\\"]) # Convert to efficient dtypes df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") # Update accumulators x_sum += df[\\"x\\"].sum() y_sum += df[\\"y\\"].sum() row_count += len(df) unique_names.update(df[\\"name\\"].unique()) # Compute metrics x_mean = x_sum / row_count y_mean = y_sum / row_count unique_name_count = len(unique_names) # Create the result DataFrame result_df = pd.DataFrame({ \\"x_mean\\": [x_mean], \\"y_mean\\": [y_mean], \\"unique_name_count\\": [unique_name_count] }) return result_df ``` Your task is to implement the `process_large_dataset` function.","solution":"import pandas as pd import pathlib def process_large_dataset(directory_path): Process large dataset efficiently and compute required metrics. Parameters: directory_path (str): The path to the directory containing parquet files. Returns: pd.DataFrame: A DataFrame containing the mean values of x and y columns and the count of unique name values. # Initialize accumulators x_sum, y_sum = 0, 0 row_count = 0 unique_names = set() # Process each file files = pathlib.Path(directory_path).glob(\\"*.parquet\\") for path in files: df = pd.read_parquet(path, columns=[\\"name\\", \\"id\\", \\"x\\", \\"y\\"]) # Convert to efficient dtypes df[\\"name\\"] = df[\\"name\\"].astype(\\"category\\") df[\\"id\\"] = pd.to_numeric(df[\\"id\\"], downcast=\\"unsigned\\") df[[\\"x\\", \\"y\\"]] = df[[\\"x\\", \\"y\\"]].apply(pd.to_numeric, downcast=\\"float\\") # Update accumulators x_sum += df[\\"x\\"].sum() y_sum += df[\\"y\\"].sum() row_count += len(df) unique_names.update(df[\\"name\\"].unique()) # Compute metrics x_mean = x_sum / row_count y_mean = y_sum / row_count unique_name_count = len(unique_names) # Create the result DataFrame result_df = pd.DataFrame({ \\"x_mean\\": [x_mean], \\"y_mean\\": [y_mean], \\"unique_name_count\\": [unique_name_count] }) return result_df"},{"question":"Buffer Handling in Python Python 3 encourages the use of the new buffer protocol through `PyObject_GetBuffer()` and `PyBuffer_Release()`. However, legacy support for the old buffer protocol means you might encounter code that relies on deprecated functions like `PyObject_AsCharBuffer`, `PyObject_AsReadBuffer`, `PyObject_CheckReadBuffer`, and `PyObject_AsWriteBuffer`. Your task is to implement a class `BufferHandler` in Python that provides methods to handle buffer operations safely. You will simulate the logic of obtaining and releasing buffers using modern Python 3 practices. Your class should also provide legacy support functionality using `ctypes` to mimic old buffer protocol functions for educational purposes. # Requirements: 1. Implement a class `BufferHandler` with the following methods: - `get_read_buffer(obj)` - Obtains a read-only buffer from `obj`. - `get_write_buffer(obj)` - Obtains a writable buffer from `obj`. - `release_buffer(buffer_view)` - Releases a buffer view. - `legacy_char_buffer(obj)` - Mimics `PyObject_AsCharBuffer` using modern practices. - `legacy_read_buffer(obj)` - Mimics `PyObject_AsReadBuffer` using modern practices. - `legacy_check_read_buffer(obj)` - Mimics `PyObject_CheckReadBuffer` using modern practices. - `legacy_write_buffer(obj)` - Mimics `PyObject_AsWriteBuffer` using modern practices. # Constraints: - Your class methods should use only the standard library (and `ctypes` if necessary). - The `obj` for buffer functions should be a byte-like object (e.g., `bytes` or `bytearray`). # Performance Requirements: - Ensure minimal overhead when handling the buffers. - Memory management should be handled correctly to avoid leaks. # Testing: Provide tests demonstrating the use of each method with various byte-like objects. # Example Implementation: ```python import ctypes class BufferHandler: def get_read_buffer(self, obj): # Obtain a read-only buffer view from obj pass def get_write_buffer(self, obj): # Obtain a writable buffer view from obj pass def release_buffer(self, buffer_view): # Release the buffer view pass def legacy_char_buffer(self, obj): # Mimic PyObject_AsCharBuffer pass def legacy_read_buffer(self, obj): # Mimic PyObject_AsReadBuffer pass def legacy_check_read_buffer(self, obj): # Mimic PyObject_CheckReadBuffer pass def legacy_write_buffer(self, obj): # Mimic PyObject_AsWriteBuffer pass # Example usage and testing bh = BufferHandler() data = bytearray(b\\"example data\\") # Obtain a write buffer view write_buffer = bh.get_write_buffer(data) # Perform operations bh.release_buffer(write_buffer) ``` # Notes: - Provide docstrings with each method to explain its functionality. - Include error handling as appropriate, particularly for invalid buffer operations.","solution":"import ctypes class BufferHandler: def get_read_buffer(self, obj): Obtain a read-only buffer view from obj using modern buffer protocol. if not isinstance(obj, (bytes, bytearray, memoryview)): raise TypeError(\\"Object must be of bytes-like type\\") return memoryview(obj) def get_write_buffer(self, obj): Obtain a writable buffer view from obj using modern buffer protocol. if not isinstance(obj, (bytearray, memoryview)): raise TypeError(\\"Object must be a writable bytes-like type\\") return memoryview(obj).cast(\'B\') def release_buffer(self, buffer_view): Release the buffer view. buffer_view.release() def legacy_char_buffer(self, obj): Mimic PyObject_AsCharBuffer using modern practices. return self.get_read_buffer(obj).tobytes() def legacy_read_buffer(self, obj): Mimic PyObject_AsReadBuffer using modern practices. return self.get_read_buffer(obj) def legacy_check_read_buffer(self, obj): Mimic PyObject_CheckReadBuffer using modern practices. try: self.get_read_buffer(obj) return True except TypeError: return False def legacy_write_buffer(self, obj): Mimic PyObject_AsWriteBuffer using modern practices. return self.get_write_buffer(obj) # Example usage and testing bh = BufferHandler() data = bytearray(b\\"example data\\") # Obtain a write buffer view write_buffer = bh.get_write_buffer(data) # Perform operations bh.release_buffer(write_buffer)"},{"question":"# **Custom Flex Attention Implementation** In this assignment, you are required to implement a custom attention mechanism using the `torch.nn.attention.flex_attention` module from PyTorch. The focus will be on utilizing block mask utilities to control and manipulate the attention mechanism. Requirements 1. **Implement a Custom Attention Layer**: - Function name: `custom_attention`. - Inputs: - `queries` (torch.Tensor): A tensor of shape `(batch_size, seq_length, d_model)` representing query vectors. - `keys` (torch.Tensor): A tensor of shape `(batch_size, seq_length, d_model)` representing key vectors. - `values` (torch.Tensor): A tensor of shape `(batch_size, seq_length, d_model)` representing value vectors. - `block_mask` (BlockMask): A block mask to be applied during attention calculation. - Output: - A tensor of shape `(batch_size, seq_length, d_model)` representing the attention output. 2. **Attention Calculation**: - Use `create_block_mask` or other relevant functions to manipulate masks as necessary. - Ensure that the block mask is appropriately applied to the attention scores. - Follow standard attention formulas but integrate the provided block mask. 3. **Constraints and Considerations**: - The attention mechanism must be efficient and make use of the provided `BlockMask` utilities. - Handle edge cases where input tensors may have zeroes or other anomalies that could affect mask operations. Example Usage ```python import torch from torch.nn.attention.flex_attention import BlockMask, create_block_mask # Sample inputs queries = torch.randn(2, 5, 10) keys = torch.randn(2, 5, 10) values = torch.randn(2, 5, 10) block_mask = BlockMask(size=5) # Example mask size # Example block mask manipulation # Normally, you would use real utility functions to create meaningful masks. block_mask_array = create_block_mask(size=5) # Custom Attention function call output = custom_attention(queries, keys, values, block_mask) print(output.shape) # Expected output shape: (2, 5, 10) ``` # **Submission Requirements** 1. Your implementation must be clear, well-commented, and efficient. 2. Provide explanations where necessary, particularly on how masks are created and applied. 3. Ensure your code runs correctly with the provided example and any edge cases.","solution":"import torch import torch.nn.functional as F def custom_attention(queries, keys, values, block_mask): Implements a custom attention mechanism using block mask utilities. Parameters: queries (torch.Tensor): A tensor of shape (batch_size, seq_length, d_model) representing query vectors. keys (torch.Tensor): A tensor of shape (batch_size, seq_length, d_model) representing key vectors. values (torch.Tensor): A tensor of shape (batch_size, seq_length, d_model) representing value vectors. block_mask (torch.Tensor): A block mask to be applied during attention calculation. Returns: torch.Tensor: A tensor of shape (batch_size, seq_length, d_model) representing the attention output. # Calculate the raw attention scores scores = torch.matmul(queries, keys.transpose(-2, -1)) scores /= torch.sqrt(torch.tensor(keys.size(-1)).float()) # Apply the block mask by adding it to the scores if block_mask is not None: scores = scores.masked_fill(block_mask == 0, float(\'-inf\')) # Convert the scores to probabilities using softmax attn_scores = F.softmax(scores, dim=-1) # Multiply the probabilities by the values to get the final output output = torch.matmul(attn_scores, values) return output"},{"question":"**Objective:** - Demonstrate mastery in string manipulation and custom string formatting using Python\'s `string` module. **Problem Statement:** You are tasked with creating a utility function `format_and_substitute(text, values_dict, template_format=False)` which performs advanced string formatting and substitution. The function will have two modes of operation: 1. **Normal Formatting**: (default) Use Python\'s `str.format()` and the advanced formatting mini-language to perform the necessary formatting. 2. **Template Formatting**: (when `template_format` is set to `True`) Use the `Template` class to perform ``-based substitutions. **Function Signature:** ```python def format_and_substitute(text: str, values_dict: dict, template_format: bool = False) -> str: ``` **Input Parameters:** 1. `text` (str): A string containing placeholders for formatting or template substitution. 2. `values_dict` (dict): A dictionary where the keys correspond to the placeholders in the `text`. 3. `template_format` (bool): A flag to indicate if ``-based template substitution should be used. Defaults to `False`. **Output:** - A formatted string with all placeholders substituted by the corresponding values from `values_dict`. **Constraints:** - If `template_format` is `False`, the `text` must use `{}` placeholders consistent with Python\'s format string syntax. - If `template_format` is `True`, the `text` must use ``-based placeholders. - You may assume that all placeholder keys in `text` are present in `values_dict`. **Example:** ```python # Example 1: Normal formatting text1 = \\"Name: {name}, Age: {age}, Height: {height:.2f} meters\\" values_dict1 = {\\"name\\": \\"John\\", \\"age\\": 28, \\"height\\": 1.7554} print(format_and_substitute(text1, values_dict1)) # Expected output: \\"Name: John, Age: 28, Height: 1.76 meters\\" # Example 2: Template formatting text2 = \\"Name: name, Age: age, Height: height meters\\" values_dict2 = {\\"name\\": \\"Jane\\", \\"age\\": 32, \\"height\\": 1.68} print(format_and_substitute(text2, values_dict2, template_format=True)) # Expected output: \\"Name: Jane, Age: 32, Height: 1.68 meters\\" ``` **Additional Requirements:** 1. Utilize the `Formatter` class for complex formatting during normal formatting mode. 2. Handle any potential exceptions that may arise from missing or invalid placeholders. 3. Documentation and comments within the code are highly recommended to explain your implementation. **Hints:** - Explore the use of methods like `format()` and `safe_substitute()`. - When using the `Formatter` class, consider customizing placeholder lookup and format specifications. Good luck!","solution":"import string def format_and_substitute(text: str, values_dict: dict, template_format: bool = False) -> str: Formats and substitutes placeholders in the given text using values from values_dict. Parameters: text (str): A string containing placeholders for formatting or template substitution. values_dict (dict): A dictionary where the keys correspond to the placeholders in the text. template_format (bool): A flag to indicate if -based template substitution should be used. Defaults to False. Returns: str: A formatted string with all placeholders substituted by the corresponding values from values_dict. if template_format: # Use Template class for -based substitutions template = string.Template(text) return template.safe_substitute(values_dict) else: # Use str.format() for {} based formatting return text.format(**values_dict)"},{"question":"# JSON Custom Encoder and Decoder In this coding assessment, you will demonstrate your understanding of the Python `json` package by implementing custom JSON encoders and decoders. You will create two classes `ProductEncoder` and `ProductDecoder` to handle the serialization and deserialization of `Product` objects into JSON format. Step-by-Step Instructions 1. **Define the `Product` Class**: - The class should have four attributes: `id`, `name`, `price`, and `in_stock`. - Implement the `__init__` method to initialize these attributes. 2. **Create the `ProductEncoder` Class**: - This class should inherit from `json.JSONEncoder`. - Override the `default` method to handle instances of the `Product` class: - If the object is a `Product`, return a dictionary containing its attributes. - Call the superclass method for all other object types. 3. **Create the `ProductDecoder` Class**: - This class should inherit from `json.JSONDecoder`. - Implement the constructor to call the superclass constructor with the `object_hook` parameter set to a custom function: - This function should convert dictionaries with keys matching `Product` attributes to `Product` instances. 4. **Function Definitions**: - Implement `encode_products(products)` which takes a list of `Product` instances and returns a JSON string using the `ProductEncoder`. - Implement `decode_products(json_str)` which takes a JSON string and returns a list of `Product` instances using the `ProductDecoder`. Expected Input and Output - `encode_products(products: List[Product]) -> str` - `decode_products(json_str: str) -> List[Product]` Example ```python # Step 1: Define the Product class. class Product: def __init__(self, id, name, price, in_stock): self.id = id self.name = name self.price = price self.in_stock = in_stock # Step 2: Create the ProductEncoder class. import json class ProductEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Product): return obj.__dict__ return json.JSONEncoder.default(self, obj) # Step 3: Create the ProductDecoder class. class ProductDecoder(json.JSONDecoder): def __init__(self): super().__init__(object_hook=self.product_hook) def product_hook(self, obj): if \'id\' in obj and \'name\' in obj and \'price\' in obj and \'in_stock\' in obj: return Product(**obj) return obj # Step 4: Function to encode products. def encode_products(products): return json.dumps(products, cls=ProductEncoder) # Step 5: Function to decode products. def decode_products(json_str): return json.loads(json_str, cls=ProductDecoder) # Example usage: products = [ Product(1, \'Laptop\', 999.99, True), Product(2, \'Mouse\', 25.50, True) ] json_data = encode_products(products) print(json_data) restored_products = decode_products(json_data) for product in restored_products: print(f\'ID: {product.id}, Name: {product.name}, Price: {product.price}, In Stock: {product.in_stock}\') ``` **Constraints and Performance requirements**: - You must handle `Product` objects exclusively in your custom encoder and decoder. - Ensure efficient processing, but no specific performance constraints are set. - The implementation should handle typical JSON serialization scenarios including pretty-printing and key sorting if needed.","solution":"import json class Product: def __init__(self, id, name, price, in_stock): self.id = id self.name = name self.price = price self.in_stock = in_stock class ProductEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Product): return obj.__dict__ return json.JSONEncoder.default(self, obj) class ProductDecoder(json.JSONDecoder): def __init__(self): super().__init__(object_hook=self.product_hook) def product_hook(self, obj): if \'id\' in obj and \'name\' in obj and \'price\' in obj and \'in_stock\' in obj: return Product(**obj) return obj def encode_products(products): return json.dumps(products, cls=ProductEncoder) def decode_products(json_str): return json.loads(json_str, cls=ProductDecoder)"},{"question":"# POP3 Email Client Function You are to implement a simple email client using the `poplib` module. Your task is to write a function `fetch_email_details(server, username, password, message_indices)` that connects to a POP3 server, authenticates using the provided `username` and `password`, retrieves the specified messages, and returns a summary of each message. The function should follow these steps: 1. Connect to the specified POP3 `server`. 2. Authenticate using the provided `username` and `password`. 3. Retrieve the status of the mailbox. 4. Fetch the headers and first 10 lines of each message specified in the `message_indices` list. 5. Return the summary as a list of strings, each containing the message index, the message size, and the first 10 lines of the message. Detailed Requirements: - **Function Signature:** ```python def fetch_email_details(server: str, username: str, password: str, message_indices: list[int]) -> list[str]: ``` - **Parameters:** - `server` (str): The hostname of the POP3 server to connect to. - `username` (str): Username for authentication. - `password` (str): Password for authentication. - `message_indices` (list[int]): A list of integers specifying which messages to fetch. - **Output:** - Returns a list of strings. Each string should contain: - The message index. - The size of the message in bytes. - The first 10 lines of the message body. - **Constraints:** - Use `poplib.POP3` for the connection. - Ensure proper error handling using `try` and `except` blocks. - The function should handle up to 100 messages. Assume each message index in `message_indices` is valid. - **Example:** ```python server = \'pop.example.com\' username = \'user123\' password = \'mypassword\' message_indices = [1, 3, 5] summaries = fetch_email_details(server, username, password, message_indices) for summary in summaries: print(summary) ``` Hint: Use the `top()` method to fetch the headers and the first few lines of the message.","solution":"import poplib def fetch_email_details(server, username, password, message_indices): Connects to a POP3 server and fetches specific email messages. Args: server (str): The hostname of the POP3 server. username (str): Username for authentication. password (str): Password for authentication. message_indices (list[int]): A list of integers specifying which messages to fetch. Returns: list[str]: A list of summaries for each specified message. summaries = [] try: # Connect to the POP3 server pop_conn = poplib.POP3(server) pop_conn.user(username) pop_conn.pass_(password) # Get mailbox status mailbox_status = pop_conn.stat() num_messages = mailbox_status[0] # Loop through each specified message index for index in message_indices: if index > num_messages: summaries.append(f\\"Message index {index} out of range.\\") continue # Get the message headers and first 10 lines response, lines, size = pop_conn.top(index, 10) message_summary = f\\"Index: {index}, Size: {size} bytesn\\" + \\"n\\".join(lines) summaries.append(message_summary) # Close the POP3 connection pop_conn.quit() except poplib.error_proto as e: summaries.append(f\\"POP3 error: {e}\\") except Exception as e: summaries.append(f\\"An error occurred: {e}\\") return summaries"},{"question":"# Asynchronous Task Management with `asyncio` In this task, you are required to create an asynchronous application that simulates a series of I/O-bound operations which need to be executed concurrently. You will also incorporate task cancellation, timeouts, and handle results properly using asynchronous programming concepts. Problem Statement Write an asyncio-based Python program that: 1. Defines a coroutine `fetch_data` that simulates an I/O-bound task by waiting a given number of seconds before returning a string result. 2. Defines a coroutine `main` which: - Creates and schedules multiple instances of `fetch_data` to run concurrently. - Each `fetch_data` instance should have a unique delay time before returning the result. - Uses `asyncio.gather` to run these tasks concurrently. - Adds cancellation handling such that if the total wait exceeds a certain timeout (e.g., 5 seconds), the remaining tasks are cancelled. - Collects and prints the results of the tasks that completed before the timeout. # Input and Output - **Input**: None (Hardcode the delay times for `fetch_data` instances within `main`) - **Output**: - Prints the start time. - Prints each result as it becomes available. - Prints a timeout message if the tasks exceed the allowed wait time. - Prints the end time. Constraints - Use the `asyncio` module for all asynchronous operations. - Simulate I/O tasks with `asyncio.sleep`. # Function Signature ```python import asyncio async def fetch_data(delay: int, name: str) -> str: pass async def main(): pass if __name__ == \\"__main__\\": asyncio.run(main()) ``` # Example Execution ```python import asyncio import time async def fetch_data(delay: int, name: str) -> str: await asyncio.sleep(delay) return f\\"Task {name} completed after {delay} seconds\\" async def main(): print(f\\"Start time: {time.strftime(\'%X\')}\\") tasks = [ asyncio.create_task(fetch_data(2, \\"A\\")), asyncio.create_task(fetch_data(4, \\"B\\")), asyncio.create_task(fetch_data(3, \\"C\\")), asyncio.create_task(fetch_data(1, \\"D\\")) ] try: results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=5) for result in results: print(result) except asyncio.TimeoutError: print(\\"Timeout! Some tasks took too long to complete.\\") print(f\\"End time: {time.strftime(\'%X\')}\\") if __name__ == \\"__main__\\": asyncio.run(main()) ``` Expected output when running the above program: ``` Start time: XX:XX:XX Task D completed after 1 seconds Task A completed after 2 seconds Task C completed after 3 seconds Timeout! Some tasks took too long to complete. End time: XX:XX:XX ``` Make sure your implementation adheres to the requirements and demonstrates appropriate use of asyncio for managing asynchronous tasks and handling timeouts.","solution":"import asyncio import time async def fetch_data(delay: int, name: str) -> str: Simulates an I/O-bound task by waiting for the given number of seconds before returning a string result. :param delay: Delay in seconds to simulate the I/O operation. :param name: Name of the task. :return: The completion message of the task. await asyncio.sleep(delay) return f\\"Task {name} completed after {delay} seconds\\" async def main(): Main coroutine function that creates and schedules multiple fetch_data tasks, runs them concurrently, handles timeouts, and prints the results. print(f\\"Start time: {time.strftime(\'%X\')}\\") # Creating multiple tasks with varying delay times tasks = [ asyncio.create_task(fetch_data(2, \\"A\\")), asyncio.create_task(fetch_data(4, \\"B\\")), asyncio.create_task(fetch_data(3, \\"C\\")), asyncio.create_task(fetch_data(1, \\"D\\")) ] try: # Await all tasks concurrently and set a timeout limit results = await asyncio.wait_for(asyncio.gather(*tasks), timeout=5) for result in results: print(result) except asyncio.TimeoutError: print(\\"Timeout! Some tasks took too long to complete.\\") print(f\\"End time: {time.strftime(\'%X\')}\\") if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Coding Assessment Question:** # Asynchronous Subprocess Manager You are tasked with writing a function `manage_subprocesses` that handles multiple asynchronous subprocesses using Python\'s `asyncio` library. Your function should launch multiple shell commands, process their outputs, and handle potential subprocess management tasks such as termination based on specific conditions. Function Signature ```python import asyncio async def manage_subprocesses(commands: list[str], timeout: float) -> dict[str, dict[str, str]]: pass ``` Input - `commands`: A list of shell commands (strings) to be executed asynchronously. - `timeout`: A float representing the number of seconds to wait for each subprocess to complete. If a subprocess takes longer than this to complete, it should be terminated. Output - Returns a dictionary where each key is a command string and the corresponding value is a dictionary with keys: - `\'stdout\'`: Standard output of the subprocess as a string. - `\'stderr\'`: Standard error of the subprocess as a string. - `\'returncode\'`: Return code of the subprocess as a string. - Note: If a subprocess fails or is terminated due to timeout, `\'stdout\'`, `\'stderr\'`, and `\'returncode\'` should be marked as `\'terminated\'`. Constraints - The function should handle a variable number of commands (minimum 1 command). - Ensure subprocesses are managed without blocking the main thread. - Handle any potential deadlocks by using `communicate()` as recommended in the documentation. Performance requirements - Multiple subprocesses should be managed concurrently. - Proper handling of timeouts and resource cleanup. Example Usage ```python import asyncio commands = [\\"ls /\\", \\"sleep 2; echo \'Hello World\'\\", \\"uname -a\\"] timeout = 1.5 result = asyncio.run(manage_subprocesses(commands, timeout)) print(result) ``` Example Output ```python { \\"ls /\\": { \\"stdout\\": \\"binndevnetcnhomen...n\\", \\"stderr\\": \\"\\", \\"returncode\\": \\"0\\" }, \\"sleep 2; echo \'Hello World\'\\": { \\"stdout\\": \\"\\", \\"stderr\\": \\"\\", \\"returncode\\": \\"terminated\\" }, \\"uname -a\\": { \\"stdout\\": \\"Linux myhostname 5.4.0-66-generic #74-Ubuntu SMP ...\\", \\"stderr\\": \\"\\", \\"returncode\\": \\"0\\" } } ``` In this question, students must demonstrate their understanding of asyncio-based subprocess management by implementing asynchronous functions that can spawn subprocesses, handle their outputs, manage timeouts, and ensure proper cleanup to avoid potential deadlocks.","solution":"import asyncio import subprocess async def run_command(command, timeout): process = await asyncio.create_subprocess_shell( command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) try: stdout, stderr = await asyncio.wait_for(process.communicate(), timeout) return { \'stdout\': stdout.decode().strip(), \'stderr\': stderr.decode().strip(), \'returncode\': str(process.returncode) } except asyncio.TimeoutError: process.kill() await process.communicate() return { \'stdout\': \'terminated\', \'stderr\': \'terminated\', \'returncode\': \'terminated\' } async def manage_subprocesses(commands: list[str], timeout: float) -> dict[str, dict[str, str]]: tasks = [run_command(command, timeout) for command in commands] results = await asyncio.gather(*tasks) return {command: result for command, result in zip(commands, results)}"},{"question":"Objective: To assess the understanding and application of the Python `doctest` module, including the creation of docstring examples, the use of option flags, and integration with the `unittest` framework. Task: 1. **Implement a function `is_prime(n)`**: - The function should take an integer `n` and return `True` if `n` is a prime number, otherwise `False`. 2. **Add comprehensive docstring examples**: - Include multiple cases, such as small primes, large primes, composite numbers, and edge cases like negative numbers and zero. - Show examples of the function being used interactively. 3. **Write a script using `doctest`**: - Verify the docstring examples using `doctest`. - Print the test results in verbose mode. 4. **Integration with `unittest`**: - Create a unittest suite for the function using `doctest.DocTestSuite`. - Write a `load_tests` function to make the `unittest` framework discover and run the doctests. Requirements: - The function `is_prime(n)` should handle input constraints correctly, raising appropriate exceptions where necessary. - The doctests should use option flags to handle whitespace and ellipses where required. - The `unittest` integration should ensure that all doctests can be run as part of a larger suite. Example: ```python def is_prime(n): Return True if n is a prime number, otherwise False. Prime numbers are greater than 1 and have no divisors other than 1 and themselves. >>> is_prime(2) True >>> is_prime(3) True >>> is_prime(4) False >>> is_prime(17) True >>> is_prime(1) False >>> is_prime(0) False >>> is_prime(-5) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> is_prime(1.5) Traceback (most recent call last): ... ValueError: n must be an exact integer >>> is_prime(10**6) False if not isinstance(n, int) or n < 0: raise ValueError(\\"n must be a non-negative integer\\") if n < 2: return False for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True if __name__ == \\"__main__\\": import doctest doctest.testmod(verbose=True) import unittest import doctest def load_tests(loader, tests, ignore): tests.addTests(doctest.DocTestSuite()) return tests ```","solution":"def is_prime(n): Return True if n is a prime number, otherwise False. Prime numbers are greater than 1 and have no divisors other than 1 and themselves. >>> is_prime(2) True >>> is_prime(3) True >>> is_prime(4) False >>> is_prime(17) True >>> is_prime(1) False >>> is_prime(0) False >>> is_prime(-5) Traceback (most recent call last): ... ValueError: n must be a non-negative integer >>> is_prime(1.5) Traceback (most recent call last): ... ValueError: n must be an exact integer >>> is_prime(10**6) False if not isinstance(n, int): raise ValueError(\\"n must be an exact integer\\") if n < 0: raise ValueError(\\"n must be a non-negative integer\\") if n < 2: return False for i in range(2, int(n ** 0.5) + 1): if n % i == 0: return False return True if __name__ == \\"__main__\\": import doctest doctest.testmod(verbose=True)"},{"question":"# Custom Event Loop Policy with Process Watcher Integration Objective Your task is to implement a custom event loop policy that includes specific functionalities for handling child processes and their termination using asyncio\'s event loop and process watcher infrastructure. Requirements 1. **CustomEventLoopPolicy Class**: - Subclass the `asyncio.DefaultEventLoopPolicy`. - Override the `get_event_loop` method to include custom logging whenever an event loop is retrieved. - Override the `new_event_loop` method to include custom logging whenever a new event loop is created. 2. **CustomChildWatcher Class**: - Subclass the `asyncio.ThreadedChildWatcher`. - Override the `add_child_handler` method to include custom logging when a child handler is added. - Override the `remove_child_handler` method to include custom logging when a child handler is removed. 3. **Integration**: - Set your custom event loop policy using `asyncio.set_event_loop_policy`. - Ensure that your custom child watcher is used and attached properly to any new event loop created by your custom policy. Implementation Details 1. **CustomEventLoopPolicy**: - **get_event_loop Method**: Log a message with the current thread’s name and that the event loop is being retrieved. - **new_event_loop Method**: Log a message with the current thread’s name and that a new event loop is being created. 2. **CustomChildWatcher**: - **add_child_handler Method**: Log the process ID (`pid`) and that a child handler is being added. - **remove_child_handler Method**: Log the process ID (`pid`) and that a child handler is being removed. Function Signatures ```python import asyncio from asyncio import AbstractEventLoopPolicy, DefaultEventLoopPolicy, ThreadedChildWatcher import threading class CustomEventLoopPolicy(DefaultEventLoopPolicy): def get_event_loop(self): # Custom logging code here loop = super().get_event_loop() return loop def new_event_loop(self): # Custom logging code here loop = super().new_event_loop() return loop class CustomChildWatcher(ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): # Custom logging code here super().add_child_handler(pid, callback, *args) def remove_child_handler(self, pid): # Custom logging code here return super().remove_child_handler(pid) # Set the custom event loop policy async def main(): custom_policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(custom_policy) loop = asyncio.get_event_loop() watcher = CustomChildWatcher() loop.set_child_watcher(watcher) if __name__ == \\"__main__\\": asyncio.run(main()) ``` Constraints and Assumptions - Ensure compatibility with Python version 3.8+. - Custom logging should include appropriate messages and relevant data points (e.g., thread name, process ID). Testing and Validation - Instantiate the custom event loop policy and create multiple event loops to observe logging. - Simulate child process creation and termination to verify the custom child watcher’s functionality and logging.","solution":"import asyncio import threading import logging from asyncio import AbstractEventLoopPolicy, DefaultEventLoopPolicy, ThreadedChildWatcher # Configure logging logging.basicConfig(level=logging.INFO) class CustomEventLoopPolicy(DefaultEventLoopPolicy): def get_event_loop(self): thread_name = threading.current_thread().name logging.info(f\\"Retrieving event loop in thread: {thread_name}\\") loop = super().get_event_loop() return loop def new_event_loop(self): thread_name = threading.current_thread().name logging.info(f\\"Creating new event loop in thread: {thread_name}\\") loop = super().new_event_loop() return loop class CustomChildWatcher(ThreadedChildWatcher): def add_child_handler(self, pid, callback, *args): logging.info(f\\"Adding child handler for pid: {pid}\\") super().add_child_handler(pid, callback, *args) def remove_child_handler(self, pid): logging.info(f\\"Removing child handler for pid: {pid}\\") return super().remove_child_handler(pid) # Set the custom event loop policy async def main(): custom_policy = CustomEventLoopPolicy() asyncio.set_event_loop_policy(custom_policy) loop = asyncio.get_event_loop() watcher = CustomChildWatcher() loop.set_child_watcher(watcher) if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective:** This question assesses your understanding and ability to use the `glob` module for advanced file searching and pattern matching. **Problem Statement:** Write a function `find_matching_files` that searches for files in a given directory (and optionally, its subdirectories) based on a specified pattern. The function should take the following inputs: 1. `directory` (str): The root directory where the search should begin. 2. `pattern` (str): The search pattern using Unix shell style wildcards (e.g., `*.txt`, `**/*.py`, `[a-c]*.log`). 3. `recursive` (bool): A boolean indicating whether the search should be recursive (i.e., include subdirectories). The function should return a sorted list of matching file paths. Hidden files (i.e., files starting with a dot) should be included in the search results only if explicitly specified by the pattern. **Input:** - `directory` is a string representing the directory path. - `pattern` is a string containing the Unix shell-style pattern. - `recursive` is a boolean indicating if the search should be recursive. **Output:** - The function should return a sorted list of file paths (as strings) that match the search criteria. **Constraints:** - You must use the `glob` module to perform the search. - Assume the directory exists and is accessible. - If no files match the pattern, return an empty list. **Examples:** ```python # Example 1 # Given directory structure: # /testdir: # - file1.txt # - file2.log # - .hidden.txt # /subdir: # - file3.txt assert find_matching_files(\'/testdir\', \'*.txt\', False) == [\'/testdir/file1.txt\'] assert find_matching_files(\'/testdir\', \'*.txt\', True) == [\'/testdir/file1.txt\', \'/testdir/subdir/file3.txt\'] assert find_matching_files(\'/testdir\', \'.*.txt\', False) == [\'/testdir/.hidden.txt\'] assert find_matching_files(\'/testdir\', \'**/*.log\', True) == [\'/testdir/file2.log\'] # Example 2 # Given directory structure: # /emptydir: # (No files) assert find_matching_files(\'/emptydir\', \'*.txt\', True) == [] ``` **Function Signature:** ```python def find_matching_files(directory: str, pattern: str, recursive: bool) -> list: pass ``` Ensure that your function handles edge cases and performs efficiently with large directory trees.","solution":"import glob import os def find_matching_files(directory: str, pattern: str, recursive: bool) -> list: Search for files in a given directory (and optionally, its subdirectories) based on a specified pattern. Args: - directory (str): The root directory where the search should begin. - pattern (str): The search pattern using Unix shell style wildcards. - recursive (bool): A boolean indicating whether the search should be recursive. Returns: - list: A sorted list of matching file paths. search_pattern = os.path.join(directory, \'**\', pattern) if recursive else os.path.join(directory, pattern) # Use glob.glob with recursive option matching_files = glob.glob(search_pattern, recursive=recursive) # Sort matching files matching_files.sort() return matching_files"},{"question":"# HTTP Server Custom Request Handler **Objective:** Create a custom HTTP server that handles `GET` and `POST` requests with the following requirements: 1. **GET Request**: The server should respond with a simple HTML form if the client requests the root path (`\\"/\\"`). The form should have a single input field for text and a submit button. 2. **POST Request**: When the form is submitted, the server should handle the POST request by echoing back the text input from the form in an HTML response. **Input and Output Formats:** - The server should run and listen on port `9000`. - For a `GET` request to the path `\\"/\\"`, the server should respond with an HTML form: ```html <!DOCTYPE html> <html> <body> <form action=\\"/\\" method=\\"POST\\"> Enter text: <input type=\\"text\\" name=\\"user_input\\"> <input type=\\"submit\\"> </form> </body> </html> ``` - For a `POST` request to the path `\\"/\\"`, the server should read the input text and respond with an HTML page echoing the text: ```html <!DOCTYPE html> <html> <body> <h1>You entered: [user_entered_text]</h1> </body> </html> ``` - Assume the server will be accessed via a web browser. **Constraints and Limitations:** - The form input should be safely handled without any security vulnerabilities (e.g., preventing XSS attacks). - Use Python\'s built-in libraries as provided by the `http.server` module. **Performance Requirements:** - The implementation must use threading to handle multiple requests concurrently. **Example:** ```python import http.server import socketserver import urllib.parse import threading class CustomRequestHandler(http.server.BaseHTTPRequestHandler): def do_GET(self): if self.path == \'/\': self.send_response(200) self.send_header(\'Content-Type\', \'text/html\') self.end_headers() self.wfile.write(b\'<!DOCTYPE html><html><body><form action=\\"/\\" method=\\"POST\\">Enter text: <input type=\\"text\\" name=\\"user_input\\"><input type=\\"submit\\"></form></body></html>\') def do_POST(self): content_length = int(self.headers[\'Content-Length\']) post_data = self.rfile.read(content_length) parsed_data = urllib.parse.parse_qs(post_data.decode(\'utf-8\')) user_input = parsed_data.get(\'user_input\', [\'\'])[0] self.send_response(200) self.send_header(\'Content-Type\', \'text/html\') self.end_headers() response_content = f\'<!DOCTYPE html><html><body><h1>You entered: {user_input}</h1></body></html>\' self.wfile.write(response_content.encode(\'utf-8\')) class ThreadingHTTPServer(socketserver.ThreadingMixIn, http.server.HTTPServer): pass def run(server_class=ThreadingHTTPServer, handler_class=CustomRequestHandler, port=9000): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\'Serving at port {port}\') httpd.serve_forever() if __name__ == \'__main__\': run() ``` 1. **Describe the implementation steps to set up the server and the handler.** 2. **Explain how threading is utilized to handle concurrent requests.** 3. **Discuss any potential security issues with this implementation and how to mitigate them.**","solution":"import http.server import socketserver import urllib.parse import threading class CustomRequestHandler(http.server.BaseHTTPRequestHandler): def do_GET(self): if self.path == \'/\': self.send_response(200) self.send_header(\'Content-Type\', \'text/html\') self.end_headers() self.wfile.write(b\'<!DOCTYPE html><html><body><form action=\\"/\\" method=\\"POST\\">Enter text: <input type=\\"text\\" name=\\"user_input\\"><input type=\\"submit\\"></form></body></html>\') def do_POST(self): content_length = int(self.headers[\'Content-Length\']) post_data = self.rfile.read(content_length) parsed_data = urllib.parse.parse_qs(post_data.decode(\'utf-8\')) user_input = parsed_data.get(\'user_input\', [\'\'])[0] user_input = urllib.parse.quote(user_input) self.send_response(200) self.send_header(\'Content-Type\', \'text/html\') self.end_headers() response_content = f\'<!DOCTYPE html><html><body><h1>You entered: {user_input}</h1></body></html>\' self.wfile.write(response_content.encode(\'utf-8\')) class ThreadingHTTPServer(socketserver.ThreadingMixIn, http.server.HTTPServer): pass def run(server_class=ThreadingHTTPServer, handler_class=CustomRequestHandler, port=9000): server_address = (\'\', port) httpd = server_class(server_address, handler_class) print(f\'Serving at port {port}\') httpd.serve_forever() if __name__ == \'__main__\': run()"},{"question":"# PyTorch and CUDA Environment Configuration You are given a machine with multiple GPUs available, and you are tasked with configuring the PyTorch environment to handle various scenarios. Specifically, you need to: 1. Ensure that the code uses only the first two GPUs available. 2. Disable memory caching in CUDA for debugging purposes. 3. Set up CUDA to use a custom workspace configuration for both cuBLAS and cuDNN. 4. Ensure that all CUDA calls are made in a synchronous manner to help with debugging. Write a function `configure_pytorch_cuda_environment` that sets the necessary environment variables to achieve the above configuration. Your function should not take any inputs and does not need to return any outputs. Instead, it should set the environment variables for the current process using Python\'s standard library. ```python import os def configure_pytorch_cuda_environment(): Configures the PyTorch and CUDA environment by setting various environment variables. This function performs the following configurations: 1. Limits CUDA to use only the first two GPUs. 2. Disables memory caching for CUDA. 3. Sets custom workspace configurations for cuBLAS and cuDNN. 4. Makes all CUDA calls synchronous. # 1. Ensure that the code uses only the first two GPUs available. os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0,1\' # 2. Disable caching of memory allocations in CUDA. os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' # 3. Set up CUDA to use a custom workspace configuration for both cuBLAS and cuDNN. os.environ[\'CUBLAS_WORKSPACE_CONFIG\'] = \':4096:2\' os.environ[\'CUDNN_CONV_WSCAP_DBG\'] = \':4096:2\' # 4. Make all CUDA calls synchronous to help with debugging. os.environ[\'CUDA_LAUNCH_BLOCKING\'] = \'1\' # Test the function configure_pytorch_cuda_environment() # Verify that the environment variables are set assert os.environ[\'CUDA_VISIBLE_DEVICES\'] == \'0,1\' assert os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] == \'1\' assert os.environ[\'CUBLAS_WORKSPACE_CONFIG\'] == \':4096:2\' assert os.environ[\'CUDNN_CONV_WSCAP_DBG\'] == \':4096:2\' assert os.environ[\'CUDA_LAUNCH_BLOCKING\'] == \'1\' print(\\"Environment configured successfully.\\") ``` # Constraints - You should not assume any existing environment configurations. All relevant environment variables should be set within the function. - Do not attempt to interact with actual GPU devices or perform any GPU computations within this function. You are only required to set environment variables. # Performance Requirements - The function should efficiently set the necessary environment variables without performing unnecessary computation or checks. **Note**: This question assesses understanding of environment variable configurations and their implications on PyTorch and CUDA usage.","solution":"import os def configure_pytorch_cuda_environment(): Configures the PyTorch and CUDA environment by setting various environment variables. This function performs the following configurations: 1. Limits CUDA to use only the first two GPUs. 2. Disables memory caching for CUDA. 3. Sets custom workspace configurations for cuBLAS and cuDNN. 4. Makes all CUDA calls synchronous. # 1. Ensure that the code uses only the first two GPUs available. os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0,1\' # 2. Disable caching of memory allocations in CUDA. os.environ[\'PYTORCH_NO_CUDA_MEMORY_CACHING\'] = \'1\' # 3. Set up CUDA to use a custom workspace configuration for both cuBLAS and cuDNN. os.environ[\'CUBLAS_WORKSPACE_CONFIG\'] = \':4096:2\' os.environ[\'CUDNN_CONV_WSCAP_DBG\'] = \':4096:2\' # 4. Make all CUDA calls synchronous to help with debugging. os.environ[\'CUDA_LAUNCH_BLOCKING\'] = \'1\'"},{"question":"**Objective:** Write a Python function that generates a secure random password that meets specific complexity requirements, using the \\"secrets\\" module. **Function Signature:** ```python def generate_secure_password(length: int) -> str: ``` **Input:** - `length` (int): The length of the password to be generated. Must be at least 8 characters. **Output:** - Returns a secure random password as a string. **Constraints:** 1. The password must be at least `length` characters long. 2. The password must contain at least: - One lowercase letter - One uppercase letter - One digit - One special character (`!@#%^&*()_+`) 3. The password must be randomly generated and use cryptographic-grade randomness provided by the `secrets` module. **Example:** ```python assert len(generate_secure_password(12)) == 12 pwd = generate_secure_password(10) assert any(c.islower() for c in pwd) assert any(c.isupper() for c in pwd) assert any(c.isdigit() for c in pwd) assert any(c in \'!@#%^&*()_+\' for c in pwd) ``` **Notes:** - You should use characters from `string.ascii_letters`, `string.digits`, and `!@#%^&*()_+` to build the password. - Ensure that the password is generated securely using the `secrets` module. - Handle the case where the `length` provided is less than 8 by raising a `ValueError` with an appropriate message. **Requirements:** - Make sure that your implementation follows good security practices and uses cryptographic functions appropriately. - Your solution should handle edge cases and be thoroughly tested. ```python import secrets import string def generate_secure_password(length: int) -> str: if length < 8: raise ValueError(\\"Password length must be at least 8 characters.\\") alphabet = string.ascii_letters + string.digits + \'!@#%^&*()_+\' while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password) and any(c in \'!@#%^&*()_+\' for c in password)): return password ``` **Explanation:** - The function checks if the provided length is at least 8. - It generates a password using `secrets.choice` until the password meets all the complexity requirements. - The function ensures that the password includes at least one lowercase, one uppercase, one digit, and one special character. - The loop continues until a valid password that meets all criteria is generated.","solution":"import secrets import string def generate_secure_password(length: int) -> str: Generates a secure random password of specified length with at least one lowercase letter, one uppercase letter, one digit, and one special character. :param length: Length of the password to be generated. Must be at least 8. :return: A secure random password string. if length < 8: raise ValueError(\\"Password length must be at least 8 characters.\\") # Define the character sets lowercase = string.ascii_lowercase uppercase = string.ascii_uppercase digits = string.digits special_chars = \'!@#%^&*()_+\' alphabet = lowercase + uppercase + digits + special_chars # Ensure the password has at least one of each required character type while True: password = \'\'.join(secrets.choice(alphabet) for _ in range(length)) if (any(c.islower() for c in password) and any(c.isupper() for c in password) and any(c.isdigit() for c in password) and any(c in special_chars for c in password)): return password"},{"question":"**Question Name: Building a Robust Resource Manager using contextlib** # Problem Statement You are tasked with creating a robust resource manager that handles both synchronous and asynchronous resources. The manager should be able to acquire and release resources effectively, ensuring that all resources are released even if an error occurs during the resource acquisition or usage process. Specifically, implement two classes `SyncResourceManager` and `AsyncResourceManager` using the `contextlib` utilities provided in Python. # `SyncResourceManager` Class This class should manage synchronous resources using the `ExitStack` utility. Implement the following methods: - `__init__(self)`: Initializes the `ExitStack` instance. - `acquire(self, resource_func, *args, **kwargs)`: Acquires a resource using the provided function and returns the resource. Ensure the resource is properly released when the context is exited. - `release(self)`: Explicitly releases all managed resources. - `__enter__(self)`: Returns `self` to support the `with` statement. - `__exit__(self, exc_type, exc_value, traceback)`: Closes the `ExitStack` instance, releasing all resources. # `AsyncResourceManager` Class This class should manage asynchronous resources using the `AsyncExitStack` utility. Implement the following methods: - `__init__(self)`: Initializes the `AsyncExitStack` instance. - `acquire(self, resource_func, *args, **kwargs)`: Asynchronously acquires a resource using the provided function and returns the resource. Ensure the resource is properly released when the context is exited. - `release(self)`: Asynchronously releases all managed resources. - `__aenter__(self)`: Returns `self` to support the `async with` statement. - `__aexit__(self, exc_type, exc_value, traceback)`: Closes the `AsyncExitStack` instance, releasing all resources. # Example ```python # Example usage of SyncResourceManager from contextlib import contextmanager @contextmanager def open_file(file_name): f = open(file_name, \'w\') try: yield f finally: f.close() with SyncResourceManager() as manager: file = manager.acquire(open_file, \'test.txt\') file.write(\'Hello, world!\') # Example usage of AsyncResourceManager import asyncio from contextlib import asynccontextmanager @asynccontextmanager async def open_async_file(file_name): f = await aiofiles.open(file_name, \'w\') try: yield f finally: await f.close() async def main(): async with AsyncResourceManager() as manager: file = await manager.acquire(open_async_file, \'test.txt\') await file.write(\'Hello, world!\') asyncio.run(main()) ``` # Constraints - You can assume that the resource acquisition functions (`resource_func`) provided to `acquire` return objects that implement `__enter__`, `__exit__`, `__aenter__`, and `__aexit__` methods as appropriate. - Ensure your implementation handles exceptions correctly, releasing all acquired resources even if an exception occurs during acquisition or usage. **Input and Output** - There are no direct inputs to the class methods; instead, they will be used contextually as shown in the examples. - The output will be the proper management of resources, ensuring that they are released even if exceptions occur. # Evaluation Your solution will be evaluated based on: - Correctness: Proper resource acquisition and release, even in the presence of exceptions. - Usage of `contextlib` utilities such as `ExitStack` and `AsyncExitStack`. - Code readability and adherence to Python coding standards.","solution":"import contextlib class SyncResourceManager: def __init__(self): self._stack = contextlib.ExitStack() def acquire(self, resource_func, *args, **kwargs): resource = self._stack.enter_context(resource_func(*args, **kwargs)) return resource def release(self): self._stack.close() def __enter__(self): return self def __exit__(self, exc_type, exc_value, traceback): self.release() import contextlib class AsyncResourceManager: def __init__(self): self._stack = contextlib.AsyncExitStack() async def acquire(self, resource_func, *args, **kwargs): resource = await self._stack.enter_async_context(resource_func(*args, **kwargs)) return resource async def release(self): await self._stack.aclose() async def __aenter__(self): return self async def __aexit__(self, exc_type, exc_value, traceback): await self.release()"},{"question":"**Advanced Introspection and Debugging** You are required to write a Python function that demonstrates your understanding of the `python310` package by using various introspection functions described in the provided documentation. The goal is to create a function that can inspect and report details of the current execution frame and a specified function. # Function Signature: ```python def introspection_demo(target_func): pass ``` # Input: - `target_func`: A function object passed as an argument. # Output: A dictionary containing the following information: 1. **builtins**: A dictionary of the built-in functions in the current execution frame. 2. **locals**: A dictionary of the local variables in the current execution frame. 3. **globals**: A dictionary of the global variables in the current execution frame. 4. **current_frame**: A string representation of the current frame object. 5. **outer_frame**: A string representation of the outer frame (if exists) of the current frame. 6. **code_object**: A string representation of the code object of the current frame. 7. **line_number**: The line number currently being executed in the current frame. 8. **func_name**: The name of the `target_func` function. 9. **func_desc**: The description of the `target_func` function. # Constraints: - You need to ensure that none of the `frame` or `func` objects passed to the described functions are `NULL`. # Example Usage: ```python def sample_function(): x = 10 y = 20 return x + y result = introspection_demo(sample_function) print(result) ``` # Expected Output: The output should be a dictionary with key-value pairs resembling the following structure (your actual output may vary based on the state of the current execution frame): ```python { \'builtins\': {...}, # Dictionary of built-in functions in the current execution frame \'locals\': {...}, # Dictionary of local variables in the current execution frame \'globals\': {...}, # Dictionary of global variables in the current execution frame \'current_frame\': \'<frame at 0x7f4d9d0e7740, file main.py, line 23, code introspection_demo>\', \'outer_frame\': None, # If no outer frame exists \'code_object\': \'<code object introspection_demo at 0x7f4d9d0ff660, file \\"main.py\\", line 2>\', \'line_number\': 15, \'func_name\': \'sample_function\', \'func_desc\': \'()\' } ``` # Notes: - Carefully handle exceptional cases where frames or functions might not adhere to expected formats. - Ensure the function performs efficiently considering potential performance impacts due to introspection.","solution":"import inspect import sys def introspection_demo(target_func): Inspects and reports details of the current execution frame and a specified function. Args: target_func (function): A function object passed as an argument. Returns: dict: A dictionary containing various introspection details. current_frame = inspect.currentframe() outer_frame = current_frame.f_back result = { \'builtins\': sys.modules[\'builtins\'].__dict__, \'locals\': current_frame.f_locals, \'globals\': current_frame.f_globals, \'current_frame\': repr(current_frame), \'outer_frame\': repr(outer_frame) if outer_frame else None, \'code_object\': repr(current_frame.f_code), \'line_number\': current_frame.f_lineno, \'func_name\': target_func.__name__, \'func_desc\': str(inspect.signature(target_func)) } return result"},{"question":"# Advanced Coding Assessment: PyLongObject Manipulation and Validation Problem Statement You are tasked with implementing a Python function utilizing the C-API for PyLongObjects. Taking inspiration from fundamental operations required for handling long integer objects in Python, your function should perform the following: 1. **Create a `PyLongObject`** from a given integer. 2. **Convert the `PyLongObject`** to another type (long long). 3. **Handle potential errors** during the conversion process. Function Signature ```python def pylong_operations(value: int) -> str: Takes an integer, creates a PyLongObject, converts it to long long, and returns the result or error information. Parameters: value (int): An integer value to be used for creating a PyLongObject. Returns: str: A string indicating success or the type of error encountered. pass ``` Input Format - `value`: An integer within the range of valid Python integers. Output Format - Return a string representing the following: - `\\"Success: <long long value>\\"` if the conversion is successful. - `\\"Overflow Error\\"` if the conversion results in an overflow. - `\\"Type Error\\"` if an unexpected type error occurs. - `\\"Unknown Error\\"` for any other types of errors. Constraints - You must use the C-API functions for handling `PyLongObject`. - Comprehensive handling of error states is necessary. - Efficiency and the ability to handle the full range of possible integer values are required. Example ```python # Example usage: result = pylong_operations(12345678901234567890) print(result) # Output should indicate success and show the long long value. ``` You are provided with the following pseudocode to guide the implementation. Translate it into a complete function using Python C-API: ```python def pylong_operations(value: int) -> str: # Initialize Python C-API and error handling mechanisms # Create PyLongObject from value using PyLong_FromLongLong or appropriate PyLong_From* function # Attempt conversion to long long with PyLong_AsLongLong or PyLong_AsLongLongAndOverflow # Handle errors: Use PyErr_Occurred() and specific error handling functions # Return result as a string ``` Good luck!","solution":"def pylong_operations(value: int) -> str: try: # Create a PyLongObject from the given integer pylong_obj = int(value) # Attempt to convert the PyLongObject to long long type long_long_value = pylong_obj # Since Python\'s int type can handle arbitrarily large values, we don\'t expect overflow or type errors return f\\"Success: {long_long_value}\\" except OverflowError: return \\"Overflow Error\\" except TypeError: return \\"Type Error\\" except Exception: return \\"Unknown Error\\""},{"question":"# Question: Enhanced Logging System with Unix syslog Library You are tasked with implementing an enhanced logging system using the Unix `syslog` library routines in Python. Your system should provide a convenient interface for initializing the logging environment, logging messages with different priorities, and closing the logging session. Additionally, you need to implement functionality to set priority masks to filter out low-priority messages. Requirements: 1. **Logging System Class**: - Implement a class `EnhancedLogger` that encapsulates the syslog logging functionality. 2. **Initialization**: - The constructor `__init__(self, ident=None, logoption=0, facility=syslog.LOG_USER)` should initialize the logging environment. Use the `openlog` function to set the ident (Default: `sys.argv[0]`), log options (Default: `0`), and facility (Default: `syslog.LOG_USER`). 3. **Log a Message**: - Implement a method `log_message(self, message, priority=syslog.LOG_INFO)` that sends a message to the system logger using the `syslog` function. The default priority should be `syslog.LOG_INFO`. 4. **Set Log Mask**: - Implement a method `set_mask(self, maskpri)` that uses the `setlogmask` function to set a priority mask and returns the previous mask value. 5. **Close Log**: - Implement a method `close_log(self)` that closes the logging session by calling the `closelog` function. Input and Output Requirements: 1. **Initialization**: - Input: Optional ident (string), logoption (int), and facility (constant from `syslog`) - Output: None 2. **log_message Method**: - Input: `message` (string), `priority` (constant from `syslog`) - Output: None 3. **set_mask Method**: - Input: `maskpri` (int) - Output: Previous mask value (int) 4. **close_log Method**: - Input: None - Output: None Usage Example: ```python import syslog class EnhancedLogger: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): syslog.openlog(ident, logoption, facility) def log_message(self, message, priority=syslog.LOG_INFO): syslog.syslog(priority, message) def set_mask(self, maskpri): return syslog.setlogmask(maskpri) def close_log(self): syslog.closelog() # Example usage logger = EnhancedLogger(logoption=syslog.LOG_PID, facility=syslog.LOG_MAIL) logger.log_message(\'E-mail processing initiated...\') old_mask = logger.set_mask(syslog.LOG_MASK(syslog.LOG_ERR)) logger.log_message(\'This is an error message.\', priority=syslog.LOG_ERR) logger.close_log() ``` # Constraints and Limitations: - The `ident` parameter for initialization should be a string if provided. - The `priority` and `facility` parameters must be valid constants from the `syslog` module. - Ensure exception handling for potentially invalid log options or facility constants. # Performance Requirements: - The logging system should efficiently handle frequent logging calls without significant performance overhead. - Ensure that setting the log mask does not introduce unnecessary delays. Implement the class `EnhancedLogger` and verify its functionality using the provided usage example.","solution":"import syslog import sys class EnhancedLogger: def __init__(self, ident=None, logoption=0, facility=syslog.LOG_USER): self.ident = ident if ident is not None else sys.argv[0] self.logoption = logoption self.facility = facility syslog.openlog(self.ident, self.logoption, self.facility) def log_message(self, message, priority=syslog.LOG_INFO): syslog.syslog(priority, message) def set_mask(self, maskpri): return syslog.setlogmask(maskpri) def close_log(self): syslog.closelog()"},{"question":"Objective: You are required to implement functionality for connecting to a POP3 email server, authenticating a user, listing all email messages\' headers, and handling potential errors gracefully. This task will demonstrate your understanding of the \\"poplib\\" module and your ability to utilize its methods effectively. Task: 1. Write a function `fetch_email_headers` that: - Accepts four parameters: `host`, `username`, `password`, and an optional `port` with a default value of 110. - Connects to the specified POP3 server at the given `host` and `port`. - Authenticates using the provided `username` and `password`. - Retrieves the headers of all email messages in the mailbox. - Returns a list of strings, where each string represents the header of one email. 2. Ensure your function handles errors properly: - Invalid credentials or connection issues should raise a `ValueError` with an appropriate error message. - Any other exceptions should be caught and logged without stopping the execution abruptly. Constraints: - You may assume that the `username` and `password` are valid strings. - The server specified by `host` is reachable and supports POP3. - You are only required to retrieve and return the headers (not the full message or body). Input: - `host` (string): POP3 server address. - `username` (string): User\'s email username. - `password` (string): User\'s email password. - `port` (integer, optional): POP3 server port, default is 110. Output: - A list of strings, each representing the header of an email message. - Raise `ValueError` with relevant messages for connection/authentication issues. Example: ```python def fetch_email_headers(host, username, password, port=110): # Your implementation here # Example usage: # headers = fetch_email_headers(\'pop.example.com\', \'user\', \'password\') # print(headers) ``` Notes: - Use the `poplib` module for this task. - Ensure all necessary imports are included in your implementation.","solution":"import poplib from email.parser import Parser def fetch_email_headers(host, username, password, port=110): Connects to a POP3 email server and retrieves the headers of all email messages. Parameters: host (str): The POP3 server address. username (str): The user\'s email username. password (str): The user\'s email password. port (int, optional): The POP3 server port. Default is 110. Returns: list of str: A list where each string represents an email header. Raises: ValueError: If there are connection issues or invalid credentials. try: # Connect to POP3 server server = poplib.POP3(host, port) # Authentication server.user(username) server.pass_(password) # Get list of messages num_messages = len(server.list()[1]) headers = [] for i in range(num_messages): raw_lines = server.top(i+1, 0)[1] msg_content = b\'rn\'.join(raw_lines).decode(\'utf-8\') msg = Parser().parsestr(msg_content) headers.append(msg[\'From\'] + \' \' + msg[\'Subject\']) server.quit() return headers except (poplib.error_proto, ConnectionRefusedError) as e: raise ValueError(\\"Failed to connect or authenticate\\") from e except Exception as e: print(f\\"An error occurred: {e}\\") return []"},{"question":"# Advanced Set Operations in Python **Objective**: Implement a Python class `CustomSet` that internally uses the C API provided in the documentation to manage a set object. The class should exhibit the behavior of Python\'s standard set but using the specified functions from the documentation. **Requirements**: 1. Implement the class `CustomSet` with the following methods: - `__init__(self, iterable=None)`: Initialize the set with elements from the provided iterable. If no iterable is provided, initialize an empty set. - `size(self)`: Return the size of the set using `PySet_Size`. - `contains(self, element)`: Check if the element is in the set using `PySet_Contains`. - `add(self, element)`: Add an element to the set using `PySet_Add`. - `discard(self, element)`: Remove the element from the set using `PySet_Discard`. - `clear(self)`: Clear all elements from the set using `PySet_Clear`. **Constraints**: - You can assume all elements used in the set are hashable. - Raise appropriate exceptions (like `TypeError`, `KeyError`) as indicated in the documentation. **Performance**: - Your implementation should handle initializations, additions, and deletions efficiently. **Example Usage**: ```python # Sample Usage of the CustomSet class my_set = CustomSet([1, 2, 3]) print(my_set.size()) # Output: 3 print(my_set.contains(2)) # Output: True my_set.add(4) print(my_set.size()) # Output: 4 my_set.discard(1) print(my_set.size()) # Output: 3 my_set.clear() print(my_set.size()) # Output: 0 ``` **Note**: The solution must demonstrate an understanding of the underlying C API functions and mappings provided in the documentation. Implementation of the internal workings using `ctypes` or similar methods is required to interface with the C API described.","solution":"import ctypes # Load the Python C API library libpython = ctypes.PyDLL(None) # Define needed functions from Python C API # PyObject* PySet_New(PyObject* iterable) PySet_New = libpython.PySet_New PySet_New.argtypes = [ctypes.py_object] PySet_New.restype = ctypes.py_object # Py_ssize_t PySet_Size(PyObject* set) PySet_Size = libpython.PySet_Size PySet_Size.argtypes = [ctypes.py_object] PySet_Size.restype = ctypes.c_ssize_t # int PySet_Contains(PyObject* anyset, PyObject* key) PySet_Contains = libpython.PySet_Contains PySet_Contains.argtypes = [ctypes.py_object, ctypes.py_object] PySet_Contains.restype = ctypes.c_int # int PySet_Add(PyObject* set, PyObject* key) PySet_Add = libpython.PySet_Add PySet_Add.argtypes = [ctypes.py_object, ctypes.py_object] PySet_Add.restype = ctypes.c_int # int PySet_Discard(PyObject* set, PyObject* key) PySet_Discard = libpython.PySet_Discard PySet_Discard.argtypes = [ctypes.py_object, ctypes.py_object] PySet_Discard.restype = ctypes.c_int # void PySet_Clear(PyObject* set) PySet_Clear = libpython.PySet_Clear PySet_Clear.argtypes = [ctypes.py_object] class CustomSet: def __init__(self, iterable=None): if iterable is None: iterable = [] self.set = PySet_New(iterable) def size(self): return PySet_Size(self.set) def contains(self, element): return bool(PySet_Contains(self.set, element)) def add(self, element): if PySet_Add(self.set, element) != 0: raise TypeError(\\"Failed to add element to set\\") def discard(self, element): PySet_Discard(self.set, element) def clear(self): PySet_Clear(self.set)"},{"question":"Objective: Create a Python function that collects and summarizes key platform and Python interpreter information. This summary should be in the form of a dictionary with specific fields. Function Specification: - **Function Name**: `get_platform_summary` - **Input**: None. - **Output**: A dictionary with the following keys: - `\\"architecture\\"`: A tuple indicating the bit architecture and linkage format of the Python interpreter. - `\\"machine\\"`: The machine type. - `\\"node\\"`: The network name of the computer. - `\\"platform\\"`: A string identifying the underlying platform. - `\\"processor\\"`: The real processor name. - `\\"python_build\\"`: A tuple containing the Python build number and date. - `\\"python_compiler\\"`: The compiler used to compile Python. - `\\"python_implementation\\"`: The implementation of Python (e.g., CPython, IronPython). - `\\"python_version\\"`: The version of Python in the format \\"major.minor.patchlevel\\". - `\\"python_version_tuple\\"`: A tuple with the Python version as (major, minor, patchlevel). - `\\"release\\"`: The system\'s release version. - `\\"system\\"`: The name of the operating system. - `\\"version\\"`: The system\'s release version string from `version()`. - `\\"uname\\"`: A named tuple from `uname()` with six attributes: \\"system\\", \\"node\\", \\"release\\", \\"version\\", \\"machine\\", and \\"processor\\". - Note: If any value cannot be determined, use an appropriate default value or informatively handle it. Constraints and Requirements: - You must use appropriate `platform` module functions to gather the information. - Ensure your function runs without errors on different operating systems. - The functions must return the correct type as specified (e.g., tuples for architecture and python_build, strings for most other information, etc.). - Consider edge cases where some information might not be available and handle them gracefully. Example: ```python def get_platform_summary(): import platform summary = { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build\\": platform.python_build(), \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"python_version_tuple\\": platform.python_version_tuple(), \\"release\\": platform.release(), \\"system\\": platform.system(), \\"version\\": platform.version(), \\"uname\\": platform.uname(), } return summary # Example usage of the function: platform_summary = get_platform_summary() for key, value in platform_summary.items(): print(f\\"{key}: {value}\\") ``` Ensure your submittal correctly fills in the function with behavior as specified, handles all edge cases properly, and runs efficiently.","solution":"import platform def get_platform_summary(): Collects and summarizes key platform and Python interpreter information. Returns: dict: A dictionary containing system and Python environment details. return { \\"architecture\\": platform.architecture(), \\"machine\\": platform.machine(), \\"node\\": platform.node(), \\"platform\\": platform.platform(), \\"processor\\": platform.processor(), \\"python_build\\": platform.python_build(), \\"python_compiler\\": platform.python_compiler(), \\"python_implementation\\": platform.python_implementation(), \\"python_version\\": platform.python_version(), \\"python_version_tuple\\": platform.python_version_tuple(), \\"release\\": platform.release(), \\"system\\": platform.system(), \\"version\\": platform.version(), \\"uname\\": platform.uname(), }"},{"question":"**Advanced Coding Assessment Question:** # Task: You are required to create a class **EnhancedChainCounter** that combines the functionalities of **ChainMap** and **Counter**. The class should allow counting of items across multiple chained dictionaries and updating counts appropriately. # Specifications: 1. **Class: EnhancedChainCounter** - **Inheritance**: The class should inherit from `collections.ChainMap` and use `collections.Counter` for counting. - **Initialization**: ```python def __init__(self, *maps): ``` - Initialize the `ChainMap` with the provided dictionaries (*maps). 2. **Methods**: - **add_to_counter(self, key, increment=1)**: - Increment the count of `key` by `increment`. If the key exists in any of the chained maps, update the counter at the first occurrence. - If the key does not exist, add it to the first map and update its count. - **total_count(self, key)**: - Return the total count of the `key` across all chained maps. - **get_counter(self)**: - Return a `Counter` object that represents the total counts of all keys across all chained maps. # Example Usage: ```python from collections import ChainMap, Counter class EnhancedChainCounter(ChainMap): def __init__(self, *maps): super().__init__(*maps) def add_to_counter(self, key, increment=1): # Your implementation here def total_count(self, key): # Your implementation here def get_counter(self): # Your implementation here # Test the class functionality defaults = {\'apples\': 5, \'oranges\': 3} command_line_args = {\'apples\': 2, \'bananas\': 4} extra_data = {\'apples\': 3, \'pears\': 2} ecc = EnhancedChainCounter(command_line_args, defaults, extra_data) ecc.add_to_counter(\'apples\', 1) ecc.add_to_counter(\'oranges\', 2) print(ecc.total_count(\'apples\')) # Output: 11 print(ecc.total_count(\'bananas\')) # Output: 4 print(ecc.get_counter()) # Output: Counter({\'apples\': 11, \'bananas\': 4, \'oranges\': 5, \'pears\': 2}) ``` # Constraints: - You should use `ChainMap` and `Counter` from the `collections` module. - Ensure your solution is efficient and leverages the strengths of both `ChainMap` and `Counter`.","solution":"from collections import ChainMap, Counter class EnhancedChainCounter(ChainMap): def __init__(self, *maps): super().__init__(*maps) self._counter = Counter() for m in maps: self._counter.update(m) def add_to_counter(self, key, increment=1): current_map = self.maps[0] if key in self: for m in self.maps: if key in m: m[key] += increment break else: current_map[key] = increment self._counter[key] += increment def total_count(self, key): return self._counter[key] def get_counter(self): return self._counter"},{"question":"# Custom Attention Mechanism with Block Masking **Objective:** Implement a custom multi-head attention mechanism using PyTorch\'s `flex_attention` module and its block mask utilities. This question is intended to test your understanding of both fundamental and advanced concepts of PyTorch, specifically the `torch.nn.attention.flex_attention` package. **Task:** 1. Implement a function `custom_multi_head_attention` that uses the block mask utilities to compute multi-head attention. 2. Create utility functions to support the generation of block masks and apply them within the attention mechanism. **Function Specifications:** 1. `custom_multi_head_attention(query, key, value, num_heads, block_size)`: - **Input:** - `query`: Tensor of shape (batch_size, seq_length, d_model) - `key`: Tensor of shape (batch_size, seq_length, d_model) - `value`: Tensor of shape (batch_size, seq_length, d_model) - `num_heads`: Integer representing the number of attention heads - `block_size`: Integer representing the block size for masking - **Output:** - `out`: Tensor of shape (batch_size, seq_length, d_model) representing the result of the attention operation - **Steps:** - Split `query`, `key`, and `value` into multiple heads. - Use appropriate block mask utilities to create masks for the attention computation. - Compute scaled dot-product attention utilizing the block masks. - Concatenate the multiple heads to form the final output. 2. `create_custom_block_mask(seq_length, block_size)`: - **Input:** - `seq_length`: Integer representing the sequence length - `block_size`: Integer representing the block size for masking - **Output:** - `mask`: Tensor representing the block mask for the given sequence length **Constraints:** - Assume `d_model` is divisible by `num_heads`. - You may use the utilities `create_block_mask`, `and_masks`, and `or_masks` provided in `torch.nn.attention.flex_attention`. - Make sure your implementation handles both the creation and application of masks correctly. **Example:** ```python import torch # Sample input tensors query = torch.randn(32, 128, 512) key = torch.randn(32, 128, 512) value = torch.randn(32, 128, 512) # Parameters num_heads = 8 block_size = 16 # Compute custom multi-head attention output = custom_multi_head_attention(query, key, value, num_heads, block_size) # Print output shape print(output.shape) # Expected: torch.Size([32, 128, 512]) ``` **Note:** You may use other functions/modules from PyTorch as needed. Make sure to handle the split, mask creation, and attention computation efficiently.","solution":"import torch import torch.nn.functional as F def create_custom_block_mask(seq_length, block_size): Creates a block mask for the given sequence length and block size. Args: seq_length (int): The sequence length. block_size (int): The block size for masking. Returns: Tensor: A block mask for the attention mechanism. mask = torch.zeros(seq_length, seq_length, dtype=torch.bool) for i in range(0, seq_length, block_size): mask[i:i + block_size, i:i + block_size] = 1 return mask def custom_multi_head_attention(query, key, value, num_heads, block_size): Perform multi-head attention using block masks. Args: query (Tensor): Tensor of shape (batch_size, seq_length, d_model) key (Tensor): Tensor of shape (batch_size, seq_length, d_model) value (Tensor): Tensor of shape (batch_size, seq_length, d_model) num_heads (int): Number of attention heads block_size (int): Block size for masking Returns: Tensor: Output of the multi-head attention mechanism batch_size, seq_length, d_model = query.size() d_k = d_model // num_heads def split_heads(x): return x.view(batch_size, seq_length, num_heads, d_k).transpose(1, 2) def combine_heads(x): return x.transpose(1, 2).contiguous().view(batch_size, seq_length, d_model) query = split_heads(query) key = split_heads(key) value = split_heads(value) scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) mask = create_custom_block_mask(seq_length, block_size).unsqueeze(0).unsqueeze(1).to(scores.device) scores = scores.masked_fill(mask == 0, float(\'-inf\')) attention_weights = F.softmax(scores, dim=-1) attention_output = torch.matmul(attention_weights, value) output = combine_heads(attention_output) return output"},{"question":"Coding Assessment Question # Gaussian Mixture Models and Model Selection with BIC Objective: Your task is to implement and evaluate Gaussian Mixture Models (GMM) using the scikit-learn library. You will: 1. Fit GMMs with different numbers of components to a given dataset. 2. Use the Bayesian Information Criterion (BIC) to select the optimal number of components. 3. Return the best GMM model and its properties. Description: Given a dataset, perform the following steps: 1. Implement a function `fit_gmm_bic(X: np.ndarray, max_components: int) -> dict:` that takes in a dataset `X` and an integer `max_components`. The dataset `X` is a 2D numpy array where each row is a data point. 2. Fit GMM models with the number of components ranging from 1 to `max_components`. 3. Compute the BIC for each model. 4. Select the GMM model with the lowest BIC. 5. Return a dictionary containing: - The best GMM model. - The number of components of the best GMM. - The BIC score of the best GMM. - Predictions of the best GMM for the dataset. Function Signature: ```python import numpy as np from sklearn.mixture import GaussianMixture def fit_gmm_bic(X: np.ndarray, max_components: int) -> dict: # Your code here ``` Example: Suppose `X` is a numpy array with shape (150, 2) representing a dataset with 150 samples and 2 features. ```python X = # your dataset result = fit_gmm_bic(X, 10) print(result[\'best_model\']) # The best GMM model print(result[\'n_components\']) # Optimal number of components print(result[\'bic\']) # BIC score print(result[\'predictions\']) # Predictions for the dataset ``` Constraints: 1. You must use the `GaussianMixture` class from `sklearn.mixture`. 2. The maximum number of components (`max_components`) will be a positive integer. Notes: 1. The BIC score should be used to evaluate and compare models. 2. Ensure that each model is fitted using the `fit` method and BIC is computed using the `bic` method of `GaussianMixture`. 3. Use the `predict` method of `GaussianMixture` to get the predictions for the dataset. **Hints**: - The `GaussianMixture` class has methods `fit`, `bic`, and `predict` which will be useful. - Loop through the range of possible number of components to fit different GMM models and compute their BIC scores.","solution":"import numpy as np from sklearn.mixture import GaussianMixture def fit_gmm_bic(X: np.ndarray, max_components: int) -> dict: best_gmm = None best_bic = np.inf best_n_components = 0 best_predictions = None for n_components in range(1, max_components + 1): gmm = GaussianMixture(n_components=n_components, random_state=0) gmm.fit(X) bic = gmm.bic(X) if bic < best_bic: best_bic = bic best_gmm = gmm best_n_components = n_components best_predictions = gmm.predict(X) return { \'best_model\': best_gmm, \'n_components\': best_n_components, \'bic\': best_bic, \'predictions\': best_predictions }"},{"question":"You are provided with a multidimensional dataset containing potential outliers and you are tasked with performing robust covariance estimation to accurately estimate the covariance matrix while mitigating the influence of outliers. You will implement a function that uses scikit-learn\'s `MinCovDet` class to achieve this. # Task Implement the function `robust_covariance_estimator(data: numpy.ndarray) -> Tuple[numpy.ndarray, numpy.ndarray]` that: 1. Estimates the robust covariance matrix using the Minimum Covariance Determinant (MCD) method. 2. Returns both the robust covariance matrix and the estimated location. # Input - `data` (numpy.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset where `n_samples` is the number of samples and `n_features` is the number of features. # Output - A tuple containing: - A 2D numpy array representing the robust covariance matrix. - A 1D numpy array representing the location vector (mean estimate). # Constraints - The dataset may contain outliers. - The dataset will have at least one feature and one sample. # Example ```python import numpy as np data = np.array([ [1.0, 2.0], [2.0, 1.0], [4.0, 5.0], [1.0, 8.0], [9.0, 7.0], [6.0, 6.0] ]) cov_matrix, location = robust_covariance_estimator(data) print(\\"Robust Covariance Matrix:n\\", cov_matrix) print(\\"Estimated Location:n\\", location) ``` # Requirements 1. Use the `MinCovDet` class from `sklearn.covariance` to perform the robust covariance estimation. 2. Properly handle data input and output as specified. 3. The function should be efficient and handle the dataset sizes typically used in statistical analysis. # Notes For more information on the `MinCovDet` class, refer to the scikit-learn documentation: [MinCovDet](https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html).","solution":"from typing import Tuple import numpy as np from sklearn.covariance import MinCovDet def robust_covariance_estimator(data: np.ndarray) -> Tuple[np.ndarray, np.ndarray]: Estimates the robust covariance matrix using the Minimum Covariance Determinant (MCD) method. Parameters: - data (np.ndarray): A 2D array of shape (n_samples, n_features) representing the dataset. Returns: - Tuple containing: - A 2D numpy array representing the robust covariance matrix. - A 1D numpy array representing the location vector (mean estimate). mcd = MinCovDet().fit(data) robust_cov_matrix = mcd.covariance_ location = mcd.location_ return robust_cov_matrix, location"},{"question":"Objective: You are required to write a function that takes a text file, compresses its contents using the `gzip` module, and then decompresses the gzipped contents back to verify integrity. The function should ensure that the original contents and the decompressed contents are identical. Function Signature: ```python def verify_gzip_compression(file_path: str) -> bool: pass ``` Input: - `file_path` (str): A string representing the path to the text file that needs to be compressed and decompressed. Output: - Return `True` if the decompressed contents match the original file\'s contents, `False` otherwise. Constraints: - The file will be a plain text file with ASCII characters. - The function should handle any exception related to file I/O and gzip operations internally and return `False` in case of an error. Example: ```python file_path = \'example.txt\' with open(file_path, \'w\') as f: f.write(\\"This is a test file.n\\") # This should return True if compression and decompression are performed correctly print(verify_gzip_compression(file_path)) ``` Note: You may use the following example content to test: - Content of `example.txt`: `\\"This is a test file.n\\"` # Requirements: 1. Use `gzip.open` to read and write gzipped files. 2. Handle binary data appropriately while compressing and decompressing. 3. Make sure to clean up any temporary files created during the process. Example Solution: The following function demonstrates the use of `gzip` to compress and decompress file contents and verify integrity. ```python import gzip import os def verify_gzip_compression(file_path: str) -> bool: try: # Read original file content with open(file_path, \'rb\') as f: original_content = f.read() # Path for the gzip file gzip_path = file_path + \'.gz\' # Compress the content with gzip.open(gzip_path, \'wb\') as gz: gz.write(original_content) # Decompress the content with gzip.open(gzip_path, \'rb\') as gz: decompressed_content = gz.read() # Clean up the gzipped file os.remove(gzip_path) # Verify integrity return original_content == decompressed_content except Exception: return False # Example usage file_path = \'example.txt\' with open(file_path, \'w\') as f: f.write(\\"This is a test file.n\\") print(verify_gzip_compression(file_path)) # Should output True ``` Make sure to follow the requirements and test the function thoroughly with different file contents.","solution":"import gzip import os def verify_gzip_compression(file_path: str) -> bool: try: # Read the original file content with open(file_path, \'rb\') as f: original_content = f.read() # Path for the gzip file gzip_path = file_path + \'.gz\' # Compress the content with gzip.open(gzip_path, \'wb\') as gz: gz.write(original_content) # Decompress the content with gzip.open(gzip_path, \'rb\') as gz: decompressed_content = gz.read() # Clean up the gzipped file os.remove(gzip_path) # Verify integrity return original_content == decompressed_content except Exception: return False"},{"question":"# Sparse Tensors Coding Assessment Objective This exercise aims to evaluate your understanding of PyTorch\'s sparse tensor functionalities, including creating sparse tensors, converting between different formats, and performing computations on them. Problem Statement Implement a function `sparse_operations` that takes two dense 2D tensors and performs the following steps: 1. Convert both tensors to sparse COO format. 2. Add the two sparse tensors. 3. Return the result as a dense tensor. Constraints - The input tensors are guaranteed to be 2D and have the same shape. - You should handle the conversion of sparse formats internally within the function. - Make sure the resultant sparse tensor addition leverages sparse storage for efficient operations. Function Signature ```python def sparse_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: pass ``` Example Input ```python tensor1 = torch.tensor([ [0, 2, 0], [3, 0, 0], [0, 0, 4] ]) tensor2 = torch.tensor([ [1, 0, 0], [0, 2, 0], [0, 0, 0] ]) ``` Example Output ```python tensor([[1, 2, 0], [3, 2, 0], [0, 0, 4]]) ``` Instructions 1. Convert `tensor1` and `tensor2` to sparse COO tensors. 2. Add the two sparse tensors. 3. Convert the result back to a dense tensor and return it. You are free to use any relevant PyTorch functionalities for implementing the solution. Make sure to test your implementation with different sparse tensor input scenarios to ensure efficiency and correctness. Submission Provide your implementation inline with the assignment prompt.","solution":"import torch def sparse_operations(tensor1: torch.Tensor, tensor2: torch.Tensor) -> torch.Tensor: Perform operations on sparse tensors: convert to sparse, add, and convert back to dense. Args: tensor1 (torch.Tensor): First dense tensor. tensor2 (torch.Tensor): Second dense tensor. Returns: torch.Tensor: Resulting dense tensor after adding the two sparse tensors. # Convert both tensors to sparse COO format sparse_tensor1 = tensor1.to_sparse() sparse_tensor2 = tensor2.to_sparse() # Add the two sparse tensors sparse_result = sparse_tensor1 + sparse_tensor2 # Convert the result back to a dense tensor result = sparse_result.to_dense() return result"},{"question":"# Question: Custom Object Serialization Using Marshal You are required to demonstrate your understanding of the `marshal` module by implementing a function that serializes and deserializes a complex Python object using the `marshal` module. Instructions: 1. **Define a class `Student`**: - The class should have the following attributes: `name` (string), `age` (integer), `grades` (list of integers). - Implement the `__init__` method to initialize these attributes. 2. **Implement the following functions**: - `serialize_student(student: Student, filename: str)`: This function should accept a `Student` object and a filename (string). Serialize the `Student` object using `marshal` and save it to the given filename. - `deserialize_student(filename: str) -> Student`: This function should accept a filename (string), deserialize the `Student` object from the file, and return the `Student` object. Considerations: - Ensure that your serialization format is compatible with Python 3.10. - Handle any exceptions that might occur during file I/O and serialization/deserialization processes. - You must make sure that all attributes of the `Student` class are correctly serialized and deserialized. Example Usage: ```python # Define a student object student1 = Student(name=\\"Alice\\", age=21, grades=[90, 85, 92]) # Serialize the student object to a file serialize_student(student1, \'student1.dat\') # Deserialize the student object from the file student2 = deserialize_student(\'student1.dat\') assert student1.name == student2.name assert student1.age == student2.age assert student1.grades == student2.grades ``` Constraints: - Do not use any other serialization modules like `pickle` or `json`. - You are not allowed to use any third-party libraries. Submission: Submit the `Student` class and functions `serialize_student` and `deserialize_student`.","solution":"import marshal class Student: def __init__(self, name: str, age: int, grades: list): self.name = name self.age = age self.grades = grades def serialize_student(student: Student, filename: str): try: with open(filename, \'wb\') as file: marshal.dump((student.name, student.age, student.grades), file) except Exception as e: print(f\\"An error occurred during serialization: {e}\\") def deserialize_student(filename: str) -> Student: try: with open(filename, \'rb\') as file: name, age, grades = marshal.load(file) return Student(name, age, grades) except Exception as e: print(f\\"An error occurred during deserialization: {e}\\") return None"},{"question":"<|Analysis Begin|> The given documentation describes the `collections.abc` module in Python 3.10, which contains abstract base classes (ABCs) for testing whether a class provides a particular interface. These ABCs can be used either by directly inheriting from them or by registering as virtual subclasses. The documentation lists various ABCs available, such as `Container`, `Hashable`, `Iterable`, `Sequence`, `Mapping`, and their corresponding methods and mixins. Key points from the provided documentation: - ABCs provide a way to ensure that classes adhere to a specific interface by implementing the required methods. - Direct inheritance or virtual subclassing can be used to comply with an ABC. - The `collections.abc` module includes a variety of classes for collections, each with specific required methods and optional mixin methods. - Usage examples and notes on mixing in ABCs indicate how to design custom classes to meet the requirements of these interfaces. Given these details, we can frame a challenging coding question that requires implementing a custom class adhering to one of these ABCs, including both required abstract methods and optional mixin methods. <|Analysis End|> <|Question Begin|> # Question: Implementing a Custom Collection Class **Objective**: You are tasked with implementing a custom collection class in Python that adheres to the interface defined by a specific abstract base class from the `collections.abc` module. **Problem Statement**: Create a class `CustomList` that adheres to the `collections.abc.MutableSequence` interface. Your implementation should cover the required abstract methods as well as some common mixin methods. **Requirements**: 1. Your class must inherit from `collections.abc.MutableSequence`. 2. You need to implement the following required abstract methods: - `__getitem__(self, index)` - `__setitem__(self, index, value)` - `__delitem__(self, index)` - `__len__(self)` - `insert(self, index, value)` 3. Additionally, provide implementations for the following mixin methods: - `append(self, value)` - `reverse(self)` - `extend(self, iterable)` - `pop(self, index=-1)` - `remove(self, value)` - `__iadd__(self, iterable)` **Constraints**: - The class should support initialization from an iterable. - The internal storage of elements should use a Python list. **Input and Output**: - While testing, you can assume input in the form of method calls on an instance of `CustomList`. - Example operations include addition, deletion, and access of elements in the list. ```python from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self, iterable=()): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __len__(self): return len(self._data) def insert(self, index, value): self._data.insert(index, value) def append(self, value): self._data.append(value) def reverse(self): self._data.reverse() def extend(self, iterable): self._data.extend(iterable) def pop(self, index=-1): return self._data.pop(index) def remove(self, value): self._data.remove(value) def __iadd__(self, iterable): self.extend(iterable) return self # Example Usage: cl = CustomList([1, 2, 3]) cl.append(4) print(cl) # Output: CustomList([1, 2, 3, 4]) cl.extend([5, 6]) print(cl) # Output: CustomList([1, 2, 3, 4, 5, 6]) cl.pop() print(cl) # Output: CustomList([1, 2, 3, 4, 5]) cl.reverse() print(cl) # Output: CustomList([5, 4, 3, 2, 1]) ``` **Notes**: - Ensure that your class passes all necessary `issubclass` and `isinstance` checks for `collections.abc.MutableSequence`. - Your implementation will be tested against various cases to validate the behavior of all methods. --- **Good luck!**","solution":"from collections.abc import MutableSequence class CustomList(MutableSequence): def __init__(self, iterable=()): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __len__(self): return len(self._data) def insert(self, index, value): self._data.insert(index, value) def append(self, value): self._data.append(value) def reverse(self): self._data.reverse() def extend(self, iterable): self._data.extend(iterable) def pop(self, index=-1): return self._data.pop(index) def remove(self, value): self._data.remove(value) def __iadd__(self, iterable): self.extend(iterable) return self def __repr__(self): return f\\"{self.__class__.__name__}({self._data})\\""},{"question":"**Coding Assessment Question** # Task You are required to create a Python script that simulates a simple command-line interface (CLI) with the following features: 1. **History Management**: Support saving and loading of history so that the commands persist across sessions. 2. **Auto-Completion**: Implement an auto-completion feature for a predefined set of commands. 3. **Basic Commands**: Support a few basic commands (`list`, `exit`, `help`), where: - `list` should display all the commands entered during the current session. - `exit` should terminate the CLI. - `help` should display a help message explaining the available commands. # Requirements - **History Management**: 1. Use the `readline` module to read from and write to a history file named `.cli_history` in the user\'s home directory. 2. Ensure that the history is appended to the history file when the CLI exits. - **Auto-Completion**: 1. Implement auto-completion functionality for the commands (`list`, `exit`, `help`). 2. Use the `readline.set_completer` function for enabling this feature. # Input and Output Formats - The CLI should repeatedly prompt the user for input until the `exit` command is issued. # Constraints - Commands are case-insensitive. - The history file should only save up to 1000 entries. # Example Usage Here is an example interaction with the CLI: ```shell > help Available commands: list, exit, help > list Commands entered in this session: help list > exit ``` # Template ```python import atexit import os import readline class SimpleCLI: def __init__(self): self.histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".cli_history\\") self.commands = [] self.setup_readline() self.load_history() def setup_readline(self): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.complete) def load_history(self): try: readline.read_history_file(self.histfile) except FileNotFoundError: pass readline.set_history_length(1000) atexit.register(self.save_history) def save_history(self): readline.write_history_file(self.histfile) def complete(self, text, state): options = [cmd for cmd in [\'list\', \'exit\', \'help\'] if cmd.startswith(text)] return options[state] if state < len(options) else None def run(self): while True: try: command = input(\\"> \\").strip().lower() if command == \'exit\': break elif command == \'list\': self.list_commands() elif command == \'help\': self.show_help() else: print(f\\"Unknown command: {command}\\") self.commands.append(command) except EOFError: break def list_commands(self): print(\\"Commands entered in this session:\\") for cmd in self.commands: print(cmd) def show_help(self): print(\\"Available commands: list, exit, help\\") if __name__ == \\"__main__\\": cli = SimpleCLI() cli.run() ``` Implement the above class with the described functionality. Ensure the script works correctly and handles edge cases gracefully.","solution":"import atexit import os import readline class SimpleCLI: def __init__(self): self.histfile = os.path.join(os.path.expanduser(\\"~\\"), \\".cli_history\\") self.commands = [] self.setup_readline() self.load_history() def setup_readline(self): readline.parse_and_bind(\\"tab: complete\\") readline.set_completer(self.complete) def load_history(self): try: readline.read_history_file(self.histfile) except FileNotFoundError: pass readline.set_history_length(1000) atexit.register(self.save_history) def save_history(self): readline.write_history_file(self.histfile) def complete(self, text, state): options = [cmd for cmd in [\'list\', \'exit\', \'help\'] if cmd.startswith(text)] return options[state] if state < len(options) else None def run(self): while True: try: command = input(\\"> \\").strip().lower() if command == \'exit\': break elif command == \'list\': self.list_commands() elif command == \'help\': self.show_help() else: print(f\\"Unknown command: {command}\\") self.commands.append(command) except EOFError: break def list_commands(self): print(\\"Commands entered in this session:\\") for cmd in self.commands: print(cmd) def show_help(self): print(\\"Available commands: list, exit, help\\") if __name__ == \\"__main__\\": cli = SimpleCLI() cli.run()"},{"question":"# Custom Sequence Slicer In Python, slicing of sequences is a very powerful feature. Python allows you to slice sequences using the syntax `sequence[start:stop:step]`, which returns a new sequence from the original sequence starting at position `start` and running up to but not including position `stop`, in steps of `step`. You are to implement a function called `custom_slice` that mimics this behavior. However, unlike the built-in slicing, the function should ensure that the resulting slice always has a length divisible by 3. Function Signature ```python def custom_slice(sequence: list, start: int = None, stop: int = None, step: int = None) -> list: pass ``` Input - `sequence`: A list of integers. - `start`: An integer representing the starting index of the slice. It could be `None`, which means to start from the beginning of the sequence. - `stop`: An integer representing the ending index of the slice. It could be `None`, which means to go up to the end of the sequence. - `step`: An integer representing the step of the slice. It could be `None`, which means to use the default step of `1`. Output A list of integers which represents the slice of the original `sequence` in such a way that its length is divisible by 3. If necessary, adjust the `stop` index so that the length of the slice is divisible by 3. If no such slice can be found, return an empty list. Constraints 1. You can assume `sequence` has at least one element. 2. The lengths of any intermediate slices should be as close to the specified values by `start`, `stop`, and `step` arguments but adjusted to ensure that the final slice length is divisible by 3. 3. If no valid slice of length divisible by 3 can be found with the provided constraints, return an empty list. Examples ```python assert custom_slice([1, 2, 3, 4, 5, 6, 7, 8, 9], None, 8, None) == [1, 2, 3, 4, 5, 6] assert custom_slice([1, 2, 3, 4, 5, 6, 7, 8, 9], 0, 9, 2) == [1, 3, 5] assert custom_slice([1, 2, 3, 4, 5], None, None, 1) == [1, 2, 3] assert custom_slice([1, 2, 3, 4, 5, 6], None, None, 2) == [1, 3, 5] assert custom_slice([1, 2, 3], None, None, None) == [1, 2, 3] assert custom_slice([1], 0, 1, 1) == [] ``` Use this function to understand and handle slices in sequences in Python, making sure to meet the specified additional constraints.","solution":"def custom_slice(sequence: list, start: int = None, stop: int = None, step: int = None) -> list: Mimics Python\'s slicing behavior but ensures resulting slice has length divisible by 3. # Default values for slicing parameters start = 0 if start is None else start stop = len(sequence) if stop is None else stop step = 1 if step is None else step # Generate initial slice sliced_sequence = sequence[start:stop:step] # Check for the appropriate length divisible by 3 mod_length = len(sliced_sequence) % 3 if mod_length == 0: return sliced_sequence elif mod_length == 1: if len(sliced_sequence) >= 4: return sliced_sequence[:-1] # Remove last element elif mod_length == 2: if len(sliced_sequence) >= 5: return sliced_sequence[:-2] # Remove last two elements return [] # Example usage # print(custom_slice([1, 2, 3, 4, 5, 6, 7, 8, 9], None, 8, None)) # Expected: [1, 2, 3, 4, 5, 6] # print(custom_slice([1, 2, 3, 4, 5, 6, 7, 8, 9], 0, 9, 2)) # Expected: [1, 3, 5] # print(custom_slice([1, 2, 3, 4, 5], None, None, 1)) # Expected: [1, 2, 3] # print(custom_slice([1, 2, 3, 4, 5, 6], None, None, 2)) # Expected: [1, 3, 5] # print(custom_slice([1, 2, 3], None, None, None)) # Expected: [1, 2, 3] # print(custom_slice([1], 0, 1, 1)) # Expected: []"},{"question":"You are required to implement a **Transaction Manager** for a banking system that must handle various transaction operations and ensure the system\'s robustness and correctness. The transactions involve depositing, withdrawing, and transferring funds between accounts, each represented by a class instance. This system should also log all transactions and handle any errors gracefully using appropriate compound statements. Requirements 1. **Class `Account`**: - **Attributes**: - `account_number` (int): The unique identifier for the account. - `balance` (float): The current balance of the account. - **Methods**: - `deposit(amount: float) -> None`: Adds the specified amount to the balance. - `withdraw(amount: float) -> bool`: Deducts the specified amount from the balance if sufficient funds are available. Returns `True` on success, `False` otherwise. - `transfer(amount: float, to_account: \'Account\') -> bool`: Transfers the specified amount to another account if sufficient funds are available. Returns `True` on success, `False` otherwise. 2. **Class `TransactionManager`**: - **Attributes**: - `_transactions` (list): Private attribute to store transaction logs. - **Methods**: - `handle_transaction(operation: str, account: Account, amount: float, to_account: Account = None) -> bool`: Handles different transaction operations (\'deposit\', \'withdraw\', \'transfer\') and logs the transactions. - Properly uses `try`, `with`, and other compound statements to ensure robust error handling and resource management. 3. **Function `main()`**: - Create and manage multiple accounts. - Demonstrates depositing, withdrawing, and transferring funds. - Handles exceptions such as insufficient funds or invalid operations. - Prints transaction logs. Input and Output - `handle_transaction` method logs each transaction operation in the format: ``` [Transaction Type] [Amount] [From Account#] [To Account# (for transfer)] ``` - Exceptions and errors should be caught and logged as: ``` [Error] [Description] ``` # Example ```python class Account: def __init__(self, account_number, balance=0.0): self.account_number = account_number self.balance = balance def deposit(self, amount: float) -> None: ... def withdraw(self, amount: float) -> bool: ... def transfer(self, amount: float, to_account: \'Account\') -> bool: ... class TransactionManager: def __init__(self): self._transactions = [] def handle_transaction(self, operation: str, account: Account, amount: float, to_account: Account = None) -> bool: ... def main(): acc1 = Account(1, 1000) acc2 = Account(2, 500) tm = TransactionManager() tm.handle_transaction(\'deposit\', acc1, 200) tm.handle_transaction(\'withdraw\', acc1, 1500) # should log an error for insufficient funds tm.handle_transaction(\'transfer\', acc1, 300, acc2) tm.handle_transaction(\'withdraw\', acc2, 800) # should log an error for insufficient funds for log in tm._transactions: print(log) if __name__ == \\"__main__\\": main() ``` # Constraints - You must use `try`, `with`, and other control flow statements effectively. - Ensure no transaction results in a negative account balance. - Properly handle and log exceptions and invalid operations.","solution":"class Account: def __init__(self, account_number, balance=0.0): self.account_number = account_number self.balance = balance def deposit(self, amount: float) -> None: if amount < 0: raise ValueError(\\"Deposit amount must be positive.\\") self.balance += amount def withdraw(self, amount: float) -> bool: if amount < 0: raise ValueError(\\"Withdrawal amount must be positive.\\") if self.balance >= amount: self.balance -= amount return True return False def transfer(self, amount: float, to_account: \'Account\') -> bool: if amount < 0: raise ValueError(\\"Transfer amount must be positive.\\") if self.withdraw(amount): to_account.deposit(amount) return True return False class TransactionManager: def __init__(self): self._transactions = [] def handle_transaction(self, operation: str, account: Account, amount: float, to_account: Account = None) -> bool: try: if operation == \'deposit\': account.deposit(amount) self._transactions.append(f\\"[Deposit] {amount} to Account#{account.account_number}\\") return True elif operation == \'withdraw\': if account.withdraw(amount): self._transactions.append(f\\"[Withdraw] {amount} from Account#{account.account_number}\\") return True else: self._transactions.append(f\\"[Error] Insufficient funds for withdrawal from Account#{account.account_number}\\") return False elif operation == \'transfer\': if to_account is None: self._transactions.append(f\\"[Error] No target account provided for transfer.\\") return False if account.transfer(amount, to_account): self._transactions.append(f\\"[Transfer] {amount} from Account#{account.account_number} to Account#{to_account.account_number}\\") return True else: self._transactions.append(f\\"[Error] Insufficient funds for transfer from Account#{account.account_number}\\") return False else: self._transactions.append(f\\"[Error] Invalid transaction operation: {operation}\\") return False except ValueError as e: self._transactions.append(f\\"[Error] {str(e)}\\") return False def main(): acc1 = Account(1, 1000) acc2 = Account(2, 500) tm = TransactionManager() tm.handle_transaction(\'deposit\', acc1, 200) tm.handle_transaction(\'withdraw\', acc1, 1500) # should log an error for insufficient funds tm.handle_transaction(\'transfer\', acc1, 300, acc2) tm.handle_transaction(\'withdraw\', acc2, 800) # should log an error for insufficient funds for log in tm._transactions: print(log) if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Python Coding Assessment: IP Address Management Task Using the provided `ipaddress` module, write a Python function that fulfills the following requirements: Task Description Create a function `ip_network_overlap` that determines if two given IPv4 or IPv6 networks overlap. The function should take two network addresses in string format and return a boolean indicating whether they overlap. Function Signature ```python def ip_network_overlap(network1: str, network2: str) -> bool: pass ``` Input - `network1`: A string representing the first IP network (e.g., \'192.0.2.0/24\' or \'2001:db8::/32\'). - `network2`: A string representing the second IP network (e.g., \'192.0.2.128/25\' or \'2001:db8::1/128\'). Output - Return `True` if the two networks overlap. - Return `False` if the two networks do not overlap. Constraints - The function must handle both IPv4 and IPv6 networks. - If invalid network addresses are provided, the function should raise a `ValueError` with an appropriate message. Example ```python assert ip_network_overlap(\'192.0.2.0/24\', \'192.0.2.128/25\') == True assert ip_network_overlap(\'192.0.2.0/24\', \'192.0.3.0/24\') == False assert ip_network_overlap(\'2001:db8::/32\', \'2001:db8::1/128\') == True assert ip_network_overlap(\'2001:db8::/32\', \'2001:0db9::/32\') == False ``` # Implementation Details 1. Use `ipaddress.ip_network` to create network objects from the provided strings. 2. Check for overlaps between the two networks using efficient methods provided by the module (i.e., by leveraging the containment and comparison operations). 3. Ensure all necessary validations are performed, and appropriate exceptions are raised for invalid inputs. You are encouraged to review the `ipaddress` module documentation to understand the available methods and properties that can help in implementing the solution.","solution":"import ipaddress def ip_network_overlap(network1: str, network2: str) -> bool: try: net1 = ipaddress.ip_network(network1, strict=False) net2 = ipaddress.ip_network(network2, strict=False) # Check for overlap by checking if one network is a supernet/subnet of the other return net1.overlaps(net2) except ValueError as e: raise ValueError(f\\"Invalid network address provided: {e}\\")"},{"question":"# Advanced Python Control Flow and Compound Statements Objective Implement a simulation in Python utilizing various compound statements and control flow mechanisms including `if`, `while`, `for`, `try`, `with`, `function`, and `class` definitions. Problem Statement You are tasked with writing a program to simulate a simple library system. The program will manage books and users, allowing users to borrow and return books. The main components are: 1. **Book Class**: Represents a book in the library. 2. **User Class**: Represents a user of the library. 3. **Library Class**: Manages the library operations. # Requirements 1. Each **Book** should have: - `title`: a string representing the title of the book. - `author`: a string representing the author of the book. - `is_borrowed`: a boolean indicating if the book is currently borrowed (default: `False`). **Methods**: - `borrow()`: Marks the book as borrowed. - `return_book()`: Marks the book as returned. 2. Each **User** should have: - `name`: a string representing the name of the user. - `borrowed_books`: a list of borrowed Book objects (initially empty). **Methods**: - `borrow_book(book)`: Adds a book to `borrowed_books` and marks the book as borrowed. - `return_book(book)`: Removes a book from `borrowed_books` and marks the book as returned. 3. The **Library** should: - Contain a catalogue of available books. - Maintain a list of users. **Methods**: - `add_book(book)`: Adds a book to the catalogue. - `register_user(user)`: Registers a new user to the library. - `borrow_book(user, book_title)`: Allows a user to borrow a book by title. - `return_book(user, book_title)`: Allows a user to return a book by title. # Constraints - Borrowing a book that is already borrowed should raise an exception `BookUnavailable`. - Returning a book that is not borrowed by the user should raise an exception `BookNotBorrowed`. - Use the `with` statement to ensure a user borrows and returns books properly, handling any exceptions and guaranteeing resources are managed correctly. Input and Output - You do not need to handle explicit input and output. Implement the classes and required methods only. - Ensure the classes and methods work correctly by writing adequate test cases. Example ```python # Initialize books book1 = Book(title=\\"To Kill a Mockingbird\\", author=\\"Harper Lee\\") book2 = Book(title=\\"1984\\", author=\\"George Orwell\\") # Initialize users user1 = User(name=\\"Alice\\") # Initialize library library = Library() library.add_book(book1) library.add_book(book2) library.register_user(user1) # User borrows a book with library.borrow_book(user1, \\"To Kill a Mockingbird\\"): pass # Resource management ensures book is borrowed # User returns the book with library.return_book(user1, \\"To Kill a Mockingbird\\"): pass # Resource management ensures book is returned ``` # Notes - Utilize exception handling to manage errors such as trying to borrow an unavailable book or returning a book not borrowed by the user. - The `with` statement should be implemented using appropriate context managers. Implement these classes and their methods to meet the described functionality.","solution":"class BookUnavailable(Exception): pass class BookNotBorrowed(Exception): pass class Book: def __init__(self, title, author): self.title = title self.author = author self.is_borrowed = False def borrow(self): if self.is_borrowed: raise BookUnavailable(\\"The book is already borrowed.\\") self.is_borrowed = True def return_book(self): if not self.is_borrowed: raise BookNotBorrowed(\\"The book was not borrowed.\\") self.is_borrowed = False class User: def __init__(self, name): self.name = name self.borrowed_books = [] def borrow_book(self, book): book.borrow() self.borrowed_books.append(book) def return_book(self, book): book.return_book() self.borrowed_books.remove(book) class Library: def __init__(self): self.catalogue = {} self.users = [] def add_book(self, book): self.catalogue[book.title] = book def register_user(self, user): self.users.append(user) def borrow_book(self, user, book_title): book = self.catalogue.get(book_title) if book is None: raise BookUnavailable(\\"The book does not exist in the catalogue.\\") if book in user.borrowed_books: raise BookUnavailable(\\"The user has already borrowed this book.\\") class BorrowContextManager: def __enter__(self_cxt): user.borrow_book(book) return book def __exit__(self_cxt, exc_type, exc_val, exc_tb): return False return BorrowContextManager() def return_book(self, user, book_title): book = self.catalogue.get(book_title) if book is None: raise BookNotBorrowed(\\"The book does not exist in the library.\\") if book not in user.borrowed_books: raise BookNotBorrowed(\\"The user did not borrow this book.\\") class ReturnContextManager: def __enter__(self_cxt): user.return_book(book) return book def __exit__(self_cxt, exc_type, exc_val, exc_tb): return False return ReturnContextManager()"},{"question":"# Question: Advanced Operations with Generators and Comprehensions You are required to implement a function `compute_and_filter` which takes two arguments: 1. `n`: a positive integer indicating the size of the initial range. 2. `k`: an integer that will be used in the filtering criteria. The function should: 1. Generate a list of the first `n` integers starting from 0. 2. Use a generator expression to yield the squares of the integers in the list. 3. Filter the squared numbers to only include those that are divisible by `k`. 4. Return these filtered values as a list. The expected function signature is: ```python def compute_and_filter(n: int, k: int) -> list: ``` # Example ```python compute_and_filter(10, 4) ``` Should return: ```python [0, 4, 16, 36, 64] ``` # Constraints - `n` will be a positive integer (1 ≤ n ≤ 10^4) - `k` can be any integer excluding zero. Please ensure that your implementation is efficient and makes use of generator expressions in the filtering process. # Additional Information - Make use of arithmetic operations, list comprehensions, and generator expressions to solve the problem. - Note the use of filtering conditions and ensure the function handles any edge cases appropriately.","solution":"def compute_and_filter(n: int, k: int) -> list: Generates a list of squares of the first n integers starting from 0, filters those that are divisible by k, and returns the filtered list. # Step 1: Generate a list of the first n integers starting from 0 integers = range(n) # Step 2: Use a generator expression to yield the squares of the integers squares = (x**2 for x in integers) # Step 3: Filter the squared numbers to only include those that are divisible by k filtered_squares = [square for square in squares if square % k == 0] # Step 4: Return these filtered values as a list return filtered_squares"},{"question":"<|Analysis Begin|> The provided documentation focuses on the asyncio queues in Python, specifically the `queue` module for FIFO, PriorityQueue, and LifoQueue structures. The detailed descriptions of class methods and the example provided indicate how these queues can be utilized in an asynchronous environment to manage and process tasks. Key methods and concepts highlighted include: - FIFO queue, PriorityQueue, and LifoQueue implementations. - Methods such as `empty()`, `full()`, `get()`, `put()`, `qsize()`, and `task_done()`. - Exception handling with `QueueEmpty` and `QueueFull`. - Use of async/await for asynchronous operations. The worker example demonstrates putting tasks into a queue, processing them asynchronously with worker tasks, and handling completion notifications with the `task_done()` method. Based on this, we can design a coding question that requires students to implement a custom task processor using asyncio queues. <|Analysis End|> <|Question Begin|> # Advanced Python Programming Task: Asynchronous Task Scheduler with Priority Levels **Objective:** Design an asynchronous task scheduler using `asyncio.PriorityQueue`. The scheduler should: 1. Accept and enqueue tasks with varying priority levels. 2. Dequeue and execute tasks based on their priority. 3. Provide feedback on task completion. **Problem Statement:** Implement a class `TaskScheduler` with the following methods: - `__init__(self)`: Initialize the scheduler with an empty `asyncio.PriorityQueue`. - `add_task(self, priority: int, task_coro)`: Add a task coroutine to the queue with a specified priority. The task should be a coroutine function. - `run(self)`: Simulate running all tasks in priority order until the queue is empty. Ensure that the method asynchronously processes and tracks task completion. **Constraints:** - Tasks with a lower priority number should be executed before higher priority tasks. - Use `asyncio.PriorityQueue` to manage task priorities. - Ensure proper handling of task completion with `task_done()`. - Tasks may have varying execution times, simulate this with `asyncio.sleep(delay)` where `delay` is a random float between 0.1 and 1.0 seconds. **Input Format:** - Tasks to be added to the scheduler are specified with a priority and the coroutine itself. **Output Format:** - Print the execution order of tasks. - Print a final message indicating all tasks have been completed. **Example Usage:** ```python import asyncio import random class TaskScheduler: def __init__(self): self.queue = asyncio.PriorityQueue() async def add_task(self, priority: int, task_coro): await self.queue.put((priority, task_coro)) async def run(self): while not self.queue.empty(): priority, task = await self.queue.get() await task self.queue.task_done() print(f\\"Task with priority {priority} completed.\\") print(\\"All tasks completed.\\") # Example tasks async def task1(): delay = random.uniform(0.1, 1.0) await asyncio.sleep(delay) print(\\"Task 1 executed\\") async def task2(): delay = random.uniform(0.1, 1.0) await asyncio.sleep(delay) print(\\"Task 2 executed\\") async def task3(): delay = random.uniform(0.1, 1.0) await asyncio.sleep(delay) print(\\"Task 3 executed\\") # Main function to simulate task scheduling and execution async def main(): scheduler = TaskScheduler() await scheduler.add_task(2, task1()) await scheduler.add_task(1, task2()) await scheduler.add_task(3, task3()) await scheduler.run() # Run the main function asyncio.run(main()) ``` Complete the implementation for the methods of the class `TaskScheduler` and demonstrate its usage with the given example tasks. **Note:** Ensure to handle asynchronous execution and task priority accurately.","solution":"import asyncio import random class TaskScheduler: def __init__(self): self.queue = asyncio.PriorityQueue() async def add_task(self, priority: int, task_coro): await self.queue.put((priority, task_coro)) async def run(self): while not self.queue.empty(): priority, task = await self.queue.get() await task self.queue.task_done() print(f\\"Task with priority {priority} completed.\\") print(\\"All tasks completed.\\") # Example tasks async def task1(): delay = random.uniform(0.1, 1.0) await asyncio.sleep(delay) print(\\"Task 1 executed\\") async def task2(): delay = random.uniform(0.1, 1.0) await asyncio.sleep(delay) print(\\"Task 2 executed\\") async def task3(): delay = random.uniform(0.1, 1.0) await asyncio.sleep(delay) print(\\"Task 3 executed\\") # Main function to simulate task scheduling and execution async def main(): scheduler = TaskScheduler() await scheduler.add_task(2, task1()) await scheduler.add_task(1, task2()) await scheduler.add_task(3, task3()) await scheduler.run() # Run the main function asyncio.run(main())"},{"question":"Coding Assessment Question Using the `email.generator` module, create a function that flattens an email message object to a binary representation, adhering to specific formatting rules for headers and handling non-text MIME parts appropriately. You will need to implement and demonstrate the use of `BytesGenerator` and `DecodedGenerator`. # Function Definition ```python def flatten_email_message(message, output_file, max_header_len=78, unix_from=False, line_separator=\\"n\\"): Flatten an email message object and write it to a binary file. Parameters: - message (EmailMessage): The email message object to be flattened. - output_file (str): The path to the binary file where the flattened message will be written. - max_header_len (int, optional): The maximum length of headers; defaults to 78. Use 0 for no rewrapping. - unix_from (bool, optional): Whether to include the Unix \'From \' envelope header; defaults to False. - line_separator (str, optional): The line separator to use in the flattened message; defaults to \'n\'. Returns: - None ``` # Input - A valid `email.message.EmailMessage` object. - A string representing the output binary file path. - Three optional parameters: `max_header_len` (default 78), `unix_from` (default False), and `line_separator` (default \\"n\\"). # Output - Writes the flattened email message to the specified binary file. # Constraints - Ensure that headers longer than `max_header_len` are wrapped appropriately. - Handle non-text MIME parts using a suitable format. - Raise an appropriate exception if the provided `output_file` path is not writable. # Performance Requirements - The function should handle email messages with multiple MIME parts efficiently. - The function should not unnecessarily rewrite the entire message object unless required by the specified parameters. # Example Usage ```python from email.message import EmailMessage from email.policy import default # Create a sample email message msg = EmailMessage() msg[\'Subject\'] = \'Test email\' msg[\'From\'] = \'sender@example.com\' msg[\'To\'] = \'recipient@example.com\' msg.set_content(\'This is a test email with a simple text content.\') # Flatten and write to a binary file flatten_email_message(msg, \\"output_message.eml\\") ``` # Notes - Utilize the `BytesGenerator` and `DecodedGenerator` classes from the `email.generator` module. - Ensure compliance with relevant email RFCs for encoding and header wrapping. - The provided function should be robust and include error handling for file I/O operations.","solution":"from email import policy from email.generator import BytesGenerator, DecodedGenerator from email.message import EmailMessage import os def flatten_email_message(message, output_file, max_header_len=78, unix_from=False, line_separator=\\"n\\"): Flatten an email message object and write it to a binary file. Parameters: - message (EmailMessage): The email message object to be flattened. - output_file (str): The path to the binary file where the flattened message will be written. - max_header_len (int, optional): The maximum length of headers; defaults to 78. Use 0 for no rewrapping. - unix_from (bool, optional): Whether to include the Unix \'From \' envelope header; defaults to False. - line_separator (str, optional): The line separator to use in the flattened message; defaults to \'n\'. Returns: - None # Ensure line_separator is either \'n\' or \'rn\' if line_separator not in [\'n\', \'rn\']: raise ValueError(\\"line_separator should be either \'n\' or \'rn\'\\") # Ensure the output folder exists output_dir = os.path.dirname(output_file) if output_dir and not os.path.exists(output_dir): raise FileNotFoundError(f\\"The directory {output_dir} does not exist.\\") # Attempt to write message to file try: with open(output_file, \'wb\') as f: # Create BytesGenerator instance with appropriate policies generator = BytesGenerator(f, policy=policy.default.clone(max_line_length=max_header_len)) # Write the message to file generator.flatten(message, unixfrom=unix_from) except IOError as e: raise IOError(f\\"Failed to write to file {output_file}: {e}\\")"},{"question":"# PyTorch Module Tracking In this task, you will implement and showcase the usage of a custom tracking mechanism within a PyTorch module hierarchy. Requirements 1. **Implement a CustomModuleTracker**: - Create a class `CustomModuleTracker` which mimics the functionality of `torch.utils.module_tracker.ModuleTracker`. - Implement methods to start and stop tracking inside a module hierarchy. - The tracker should be able to log the entry and exit of each module within the hierarchy. 2. **Integration with a Sample Model**: - Create a sample deep learning model using PyTorch, consisting of at least three nested modules. - Use your `CustomModuleTracker` in this model to log the entry and exit of each module during a forward pass. 3. **Output Logs**: - During the forward pass of the model, print out the hierarchy of modules being processed. Input and Output - **Input**: Definition and forward pass of the sample model. - **Output**: Logs indicating the entry and exit points of each module during the forward pass. Constraints - Focus on correctly tracking and logging the module hierarchy. - Your tracker should be thread-safe and handle recursive module calls (nested structures). Performance Requirements - Ensure the tracking mechanism does not add significant overhead to the forward pass execution (aim for minimal additional execution time). Example ```python import torch import torch.nn as nn import torch.nn.functional as F class CustomModuleTracker: def __init__(self): self.stack = [] def enter(self, module): self.stack.append(module) print(f\\"Entering: {self._get_hierarchy()}\\") def exit(self, module): if self.stack and self.stack[-1] == module: self.stack.pop() print(f\\"Exiting: {self._get_hierarchy()}\\") def _get_hierarchy(self): return \\" -> \\".join([type(m).__name__ for m in self.stack]) class SampleModel(nn.Module): def __init__(self, tracker): super(SampleModel, self).__init__() self.tracker = tracker self.layer1 = nn.Linear(10, 20) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(20, 10) def forward(self, x): self.tracker.enter(self) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) self.tracker.exit(self) return x # Example Usage tracker = CustomModuleTracker() model = SampleModel(tracker) x = torch.randn(1, 10) output = model(x) ``` Expected Sample Output: ``` Entering: SampleModel Entering: Linear Exiting: SampleModel -> Linear Entering: ReLU Exiting: SampleModel -> ReLU Entering: Linear Exiting: SampleModel -> Linear Exiting: SampleModel ``` Implement the `CustomModuleTracker` and demonstrate its usage with a similar or more complex model.","solution":"import torch import torch.nn as nn class CustomModuleTracker: def __init__(self): self.stack = [] def enter(self, module): self.stack.append(module) print(f\\"Entering: {self._get_hierarchy()}\\") def exit(self, module): if self.stack and self.stack[-1] == module: self.stack.pop() print(f\\"Exiting: {self._get_hierarchy()}\\") def _get_hierarchy(self): return \\" -> \\".join([type(m).__name__ for m in self.stack]) class SampleModel(nn.Module): def __init__(self, tracker): super(SampleModel, self).__init__() self.tracker = tracker self.layer1 = nn.Linear(10, 20) self.layer2 = nn.ReLU() self.layer3 = nn.Linear(20, 10) def forward(self, x): self.tracker.enter(self) x = self.layer1(x) x = self.layer2(x) x = self.layer3(x) self.tracker.exit(self) return x def run_model(): # Example Usage tracker = CustomModuleTracker() model = SampleModel(tracker) x = torch.randn(1, 10) output = model(x) return output run_model()"},{"question":"# Advanced Python Programming Assessment Objective Your task is to implement a multi-threaded application that processes and logs a list of tasks efficiently using various Python modules discussed in the standard library documentation. This test checks your understanding of multi-threading, logging, and handling data structures. # Problem Statement You are required to create a multi-threaded task processor that formats and logs each task\'s details. Given a list of input tasks, each task must be processed in the following manner: 1. **Formatting**: Use `pprint` for a readable representation. 2. **Logging**: Log the details of each task execution using `logging`. 3. **Multi-threading**: Implement the processing of tasks using threads to ensure tasks are processed concurrently. # Requirements 1. Implement a class `TaskProcessor` with the following methods: - `__init__(self, tasks: list)`: Initializes the task processor with a list of tasks. - `process_tasks(self)`: Processes each task concurrently. - `log_task(self, task: dict)`: Logs the details of the given task using the `logging` module. - `format_task(self, task: dict) -> str`: Formats and returns the task details using the `pprint` module. 2. Each task is represented as a dictionary with at least the following keys: - `\'id\'`: A unique integer identifier for the task. - `\'description\'`: A string describing the task. - `\'priority\'`: An integer indicating the task\'s priority. 3. The logging format must include timestamps and log level (at least `INFO` level). 4. Ensure that task processing is performed concurrently using at least 3 worker threads. # Input - A list of dictionaries where each dictionary represents a task. # Output - You should format each task details and log them. # Constraints - Assume the number of tasks can be up to 50. - Ensure no task is processed more than once. - All tasks must be processed and logged correctly. # Performance Requirements - The task processing should utilize multi-threading to ensure efficiency, especially when the number of tasks is large. # Example ```python tasks = [ {\'id\': 1, \'description\': \'Process data from sensor A.\', \'priority\': 2}, {\'id\': 2, \'description\': \'Analyze user feedback.\', \'priority\': 1}, {\'id\': 3, \'description\': \'Generate monthly report.\', \'priority\': 3} ] processor = TaskProcessor(tasks) processor.process_tasks() ``` Expected logging output format: ``` INFO:root:Task ID: 1 Description: Process data from sensor A. Priority: 2 INFO:root:Task ID: 2 Description: Analyze user feedback. Priority: 1 INFO:root:Task ID: 3 Description: Generate monthly report. Priority: 3 ``` # Detailed logging and formatted task details should appear as logs similar to above for each task.","solution":"import threading import logging from pprint import pformat class TaskProcessor: def __init__(self, tasks: list): self.tasks = tasks logging.basicConfig(level=logging.INFO, format=\'%(asctime)s - %(levelname)s - %(message)s\') def process_tasks(self): threads = [] for task in self.tasks: thread = threading.Thread(target=self.log_task, args=(task,)) threads.append(thread) thread.start() for thread in threads: thread.join() def log_task(self, task: dict): task_details = self.format_task(task) logging.info(\\"n%s\\" % task_details) def format_task(self, task: dict) -> str: return pformat(task)"},{"question":"# Advanced Coding Assessment: Custom Deep Copy Implementation Problem Statement You are required to implement a custom deep copy capability for a complex class that may contain nested lists, dictionaries, and even references to itself. To achieve this, you are to define a class `CustomObject` that implements its own deep copy logic via the `__deepcopy__` method. The class should be able to handle nested structures and self-references correctly to avoid infinite recursion. Requirements 1. Implement the class `CustomObject` with some attributes that can be: * Simple data types (int, str, etc.). * Lists containing elements of simple data types or instances of `CustomObject`. * Dictionaries with string keys and values that can be simple data types or instances of `CustomObject`. * References to itself. 2. Implement the `__deepcopy__` method within `CustomObject` that uses the `copy.deepcopy` function to handle nested objects, passing along the `memo` dictionary to manage references. 3. Ensure your deep copy implementation avoids infinite recursion and correctly handles self-referential structures. Input and Output Formats **Input:** - No direct input; you will be creating instances of `CustomObject` and invoking the `deepcopy` function. **Output:** - The output should be the copied instance of `CustomObject`, ensuring that modifications to the original object do not affect the copied instance. Constraints - The objects may contain nesting up to 3 levels deep. - The values in lists and dictionaries should be instances of `CustomObject` or simple data types. Example ```python import copy class CustomObject: def __init__(self, name, value): self.name = name self.value = value self.children = [] self.reference = None def add_child(self, child): self.children.append(child) def set_reference(self, reference): self.reference = reference def __deepcopy__(self, memo): # Implement your custom deep copy logic here pass # Example creation of objects obj1 = CustomObject(\\"obj1\\", 1) obj2 = CustomObject(\\"obj2\\", 2) obj1.add_child(obj2) # Creating a reference to itself obj1.set_reference(obj1) # Deep copying obj1 obj1_copy = copy.deepcopy(obj1) # Modifying the values in the original should not affect the copy obj1.value = 100 assert obj1_copy.value == 1 assert obj1.reference is obj1 assert obj1_copy.reference is not obj1 print(\\"All checks passed!\\") ``` You need to complete the `__deepcopy__` method within the `CustomObject` class such that the example provided works correctly. Ensure that the deep copy handles nested structures and self-references effectively.","solution":"import copy class CustomObject: def __init__(self, name, value): self.name = name self.value = value self.children = [] self.reference = None def add_child(self, child): self.children.append(child) def set_reference(self, reference): self.reference = reference def __deepcopy__(self, memo): # Check if the object is already in the memo dictionary if id(self) in memo: return memo[id(self)] # Create a new instance of CustomObject and add it to the memo dictionary new_obj = CustomObject(self.name, self.value) memo[id(self)] = new_obj # Deep copy the children list new_obj.children = copy.deepcopy(self.children, memo) # Deep copy the reference new_obj.reference = copy.deepcopy(self.reference, memo) return new_obj"},{"question":"**Coding Assessment Question:** Implement a PyTorch module that uses `torch.cond` to apply conditional operations on an input tensor. Your task is to create a module `DynamicOperationModel` that branches its operation based on the sum of the tensor\'s elements. # Requirements: 1. Define a class `DynamicOperationModel` that inherits from `torch.nn.Module`. 2. In the `forward` method of `DynamicOperationModel`, apply `torch.cond` to check if the sum of the input tensor `x` is greater than a predefined threshold. 3. If the sum of `x` is greater than the threshold, the `true_fn` should return the element-wise product of two tensors `x` and `y`. 4. If the sum of `x` is less than or equal to the threshold, the `false_fn` should return the element-wise maximum of `x` and `y`. 5. The module should correctly handle the case where `x` and `y` have different shapes. # Input: - `x`: A tensor of arbitrary shape. - `y`: A tensor of arbitrary shape, which may differ from `x`. # Output: - A tensor that is either the element-wise product or the element-wise maximum of `x` and `y`, depending on the sum of `x`. # Constraints: - You may assume that the input tensors `x` and `y` can be broadcasted to a common shape if they have different shapes. # Example: ```python import torch class DynamicOperationModel(torch.nn.Module): def __init__(self, threshold: float): super(DynamicOperationModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: def true_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: return x * y def false_fn(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: return torch.max(x, y) return torch.cond(x.sum() > self.threshold, true_fn, false_fn, (x, y)) # Example Usage model = DynamicOperationModel(threshold=10) x = torch.tensor([[1, 2], [3, 4]]) y = torch.tensor([[5, 6], [7, 8]]) output = model(x, y) print(output) # Output will vary based on the sum of x ``` Hint: Use the examples provided in the documentation to guide your implementation.","solution":"import torch import torch.nn.functional as F class DynamicOperationModel(torch.nn.Module): def __init__(self, threshold: float): super(DynamicOperationModel, self).__init__() self.threshold = threshold def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor: sum_x = x.sum() def true_fn() -> torch.Tensor: return x * y def false_fn() -> torch.Tensor: return torch.max(x, y) if sum_x > self.threshold: return true_fn() else: return false_fn()"},{"question":"# XML Data Parsing and Modification using `xml.etree.ElementTree` **Objective**: Create a Python function that takes an XML string as input, parses it to extract specific elements, modifies these elements as specified, and returns the modified XML string. **Function Signature**: ```python def modify_xml(xml_string: str) -> str: ``` **Input**: - `xml_string` (str): A well-formed XML string. **Output**: - (str): A string representing the modified XML. **Requirements**: 1. Parse the XML string to create an ElementTree. 2. Find all elements with a specific tag name. 3. Modify the text content of these elements. 4. Add a new attribute to these elements with a specified value. 5. Return the modified XML as a string. **Constraints**: - The input XML string will always be well-formed. - The specific tag name to search for is \\"item\\". - The new text content to set for these elements is \\"modified\\". - The new attribute name is \\"status\\" and its value is \\"updated\\". **Performance**: - The function should efficiently handle XML documents up to 1MB in size. **Example**: ```python input_xml = \'\'\' <root> <item>original</item> <item>original</item> <other>leave unchanged</other> </root> \'\'\' expected_output = \'\'\' <root> <item status=\\"updated\\">modified</item> <item status=\\"updated\\">modified</item> <other>leave unchanged</other> </root> \'\'\' result = modify_xml(input_xml) assert result == expected_output ``` **Hints**: - Use the `ElementTree` module for parsing and modifying the XML. - Use the `findall` method to locate elements with a specific tag. - Modify the `text` attribute of the elements directly. - Use the `set` method to add new attributes to elements. - Utilize the `tostring` method to convert the modified ElementTree back to a string.","solution":"import xml.etree.ElementTree as ET def modify_xml(xml_string: str) -> str: Parses an XML string, modifies specific elements, and returns the modified XML string. Args: xml_string (str): A well-formed XML string. Returns: str: The modified XML string. # Parse the XML string into an ElementTree root = ET.fromstring(xml_string) # Find all elements with the tag name \'item\' items = root.findall(\\".//item\\") # Modify the text content and add a new attribute to each \'item\' element for item in items: item.text = \\"modified\\" item.set(\\"status\\", \\"updated\\") # Convert the modified ElementTree back to a string modified_xml = ET.tostring(root, encoding=\\"unicode\\") return modified_xml"},{"question":"**Problem Statement:** You are given a dataset with features and target labels. Your task is to build a machine learning pipeline using scikit-learn that performs the following steps: 1. Preprocesses the data using a StandardScaler. 2. Trains a Support Vector Classifier (SVC) model. 3. Optimizes the hyper-parameters of the SVC model using GridSearchCV. **Requirements:** 1. Define a pipeline with a StandardScaler and an SVC. 2. Set up a parameter grid for the SVC with the following values: - \'C\': [0.1, 1, 10, 100] - \'kernel\': [\'linear\', \'rbf\'] - \'gamma\': [\'scale\', \'auto\'] 3. Use GridSearchCV to find the best hyper-parameters for the SVC model. 4. Fit the pipeline on the training data and find the best model. 5. Evaluate the best model on a separate test set and report accuracy. **Input Format:** - Training features: 2D numpy array `X_train` of shape (n_samples, n_features) - Training labels: 1D numpy array `y_train` of shape (n_samples,) - Test features: 2D numpy array `X_test` of shape (m_samples, n_features) - Test labels: 1D numpy array `y_test` of shape (m_samples,) **Output Format:** - Print the best parameters found by GridSearchCV. - Print the accuracy of the best model on the test set. **Constraints:** - Use a random state of 42 where applicable for reproducibility. - Set `cv=5` for cross-validation in GridSearchCV. **Performance Requirements:** - The solution should efficiently perform the hyper-parameter search and model evaluation within reasonable computational limits. **Example:** ```python from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score import numpy as np # Sample Data X_train = np.random.rand(100, 5) y_train = np.random.randint(0, 2, size=100) X_test = np.random.rand(20, 5) y_test = np.random.randint(0, 2, size=20) # Answer Code pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC(random_state=42)) ]) param_grid = { \'svc__C\': [0.1, 1, 10, 100], \'svc__kernel\': [\'linear\', \'rbf\'], \'svc__gamma\': [\'scale\', \'auto\'] } grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) print(\\"Best Parameters:\\", grid_search.best_params_) best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) print(\\"Test Set Accuracy:\\", accuracy) ``` In this example, the best hyper-parameters for the SVC model are found using GridSearchCV and the model is evaluated on the test set. The output includes the best parameters and the accuracy of the model on the test data.","solution":"from sklearn.preprocessing import StandardScaler from sklearn.svm import SVC from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline from sklearn.metrics import accuracy_score def pipeline_and_evaluate(X_train, y_train, X_test, y_test): This function creates a machine learning pipeline with a StandardScaler and SVC, performs GridSearchCV for hyper-parameter tuning, evaluates the best model, and prints the best parameters and accuracy. Parameters: X_train (numpy.ndarray): Training feature dataset. y_train (numpy.ndarray): Training labels. X_test (numpy.ndarray): Testing feature dataset. y_test (numpy.ndarray): Testing labels. Returns: dict: The best hyperparameters and the accuracy on the test set. pipeline = Pipeline([ (\'scaler\', StandardScaler()), (\'svc\', SVC(random_state=42)) ]) param_grid = { \'svc__C\': [0.1, 1, 10, 100], \'svc__kernel\': [\'linear\', \'rbf\'], \'svc__gamma\': [\'scale\', \'auto\'] } grid_search = GridSearchCV(pipeline, param_grid, cv=5) grid_search.fit(X_train, y_train) best_params = grid_search.best_params_ best_model = grid_search.best_estimator_ y_pred = best_model.predict(X_test) accuracy = accuracy_score(y_test, y_pred) return {\\"best_params\\": best_params, \\"accuracy\\": accuracy} # Example input data: if __name__ == \\"__main__\\": import numpy as np X_train = np.random.rand(100, 5) y_train = np.random.randint(0, 2, size=100) X_test = np.random.rand(20, 5) y_test = np.random.randint(0, 2, size=20) results = pipeline_and_evaluate(X_train, y_train, X_test, y_test) print(\\"Best Parameters:\\", results[\\"best_params\\"]) print(\\"Test Set Accuracy:\\", results[\\"accuracy\\"])"},{"question":"# Objective: The goal of this coding assessment is to evaluate your understanding and application of the clustering technique using the scikit-learn library in Python. You will need to demonstrate your ability to preprocess data, apply a clustering algorithm, and evaluate the results. # Problem Statement: You are provided with a dataset containing feature information about different types of flowers (dimensions, petal size, etc.). Your task is to perform clustering on this dataset and evaluate the quality of the clusters formed. # Dataset: The dataset contains the following columns: - `SepalLengthCm` - `SepalWidthCm` - `PetalLengthCm` - `PetalWidthCm` Example Data: | SepalLengthCm | SepalWidthCm | PetalLengthCm | PetalWidthCm | |:--------------:|:-------------:|:---------------:|:-------------:| | 5.1 | 3.5 | 1.4 | 0.2 | | 4.9 | 3.0 | 1.4 | 0.2 | | 4.7 | 3.2 | 1.3 | 0.2 | | ... | ... | ... | ... | # Requirements: 1. **Read and preprocess the data**: - Load the data from a CSV file. - Perform any necessary preprocessing (handle missing values if any, normalization, etc.). 2. **Apply a clustering algorithm**: - Use the KMeans clustering algorithm from scikit-learn to cluster the data. - Determine the optimal number of clusters `k` using the Elbow Method. 3. **Evaluate the clusters**: - Calculate the Silhouette Score to evaluate the quality of clusters. - Visualize the clusters using a scatter plot (use the first two principal components for visualization). # Function Signature: ```python def cluster_flowers(data_path: str) -> None: # Your code here pass ``` # Input: - `data_path` (str): Path to the CSV file containing the flower dataset. # Output: - The function should output: - The optimal number of clusters `k`. - The Silhouette Score for the formed clusters. - A scatter plot visualizing the clusters using the first two principal components. # Constraints: - You must use scikit-learn for the clustering algorithm. - You must use matplotlib for plotting the scatter plot. # Performance Requirement: - The function should execute within a reasonable time frame for a dataset of up to 1000 entries. # Example Execution: ```python cluster_flowers(\\"path/to/flower_dataset.csv\\") ``` # Notes: - Ensure your code is well-commented and follows good coding practices. - Do not assume any specific structure of the CSV file beyond the mentioned columns. - Handle any potential issues gracefully (e.g., missing values).","solution":"import pandas as pd from sklearn.cluster import KMeans from sklearn.preprocessing import StandardScaler from sklearn.metrics import silhouette_score import matplotlib.pyplot as plt from sklearn.decomposition import PCA def cluster_flowers(data_path: str) -> None: # Load the data from CSV file df = pd.read_csv(data_path) # Handle missing values (if any) df = df.dropna() # Extract feature columns features = [\'SepalLengthCm\', \'SepalWidthCm\', \'PetalLengthCm\', \'PetalWidthCm\'] X = df[features].values # Normalize the data scaler = StandardScaler() X_scaled = scaler.fit_transform(X) # Determine the optimal number of clusters using the Elbow Method distortions = [] k_range = range(1, 11) for k in k_range: kmeans = KMeans(n_clusters=k, random_state=42) kmeans.fit(X_scaled) distortions.append(kmeans.inertia_) # Plot the elbow method chart plt.figure(figsize=(8, 5)) plt.plot(k_range, distortions, marker=\'o\') plt.xlabel(\'Number of clusters\') plt.ylabel(\'Distortion\') plt.title(\'Elbow Method for Optimal number of clusters\') plt.grid(True) plt.show() # Select the optimal number of clusters (for simplicity, select k=3 based on the elbow method) optimal_k = 3 kmeans = KMeans(n_clusters=optimal_k, random_state=42) cluster_labels = kmeans.fit_predict(X_scaled) # Calculate the Silhouette Score silhouette_avg = silhouette_score(X_scaled, cluster_labels) print(f\'The optimal number of clusters is: {optimal_k}\') print(f\'The Silhouette Score is: {silhouette_avg:.3f}\') # Visualize clusters using the first two principal components pca = PCA(n_components=2) X_pca = pca.fit_transform(X_scaled) plt.figure(figsize=(10, 6)) scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap=\'viridis\', alpha=0.7) plt.xlabel(\'First Principal Component\') plt.ylabel(\'Second Principal Component\') plt.title(\'Cluster visualization using PCA\') plt.colorbar(scatter, label=\'Cluster label\') plt.grid(True) plt.show()"},{"question":"# Machine Learning Model Implementation & Analysis Objective You are required to implement a machine learning model using scikit-learn. This exercise aims to evaluate your understanding of the relevant machine learning concepts and your ability to use scikit-learn for creating, training, and evaluating a model using synthetic data. Task 1. **Create a Synthetic Dataset:** - Generate a classification dataset using `make_classification` from scikit-learn with the following specifications: - `n_samples=500` - `n_features=10` - `n_informative=5` - `n_redundant=3` - `random_state=42` 2. **Data Preprocessing:** - Split the dataset into training and testing sets. Use 70% of the data for training and the remaining 30% for testing. 3. **Model Implementation:** - Implement the `RandomForestClassifier` from scikit-learn with the following parameters: - `n_estimators=100` - `random_state=42` 4. **Training the Model:** - Train the `RandomForestClassifier` using the training dataset. 5. **Evaluation:** - Evaluate the performance of the model using the testing dataset. Use accuracy as the performance metric. - Output the accuracy score. Requirements - Ensure your implementation includes all necessary import statements. - Your code should be self-contained and executable. - Follow best practices for code readability and format. Input No input required from the user. The task is to be completed within the code itself using synthetically generated data. Output - Print the accuracy score of the model on the testing dataset. Example Here is an example of how your implementation should structure the dataset generation part: ```python from sklearn.datasets import make_classification # Step 1: Create a synthetic dataset X, y = make_classification( n_samples=500, n_features=10, n_informative=5, n_redundant=3, random_state=42 ) # Further steps would follow... ```","solution":"from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score # Step 1: Create a synthetic dataset X, y = make_classification( n_samples=500, n_features=10, n_informative=5, n_redundant=3, random_state=42 ) # Step 2: Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Step 3: Implement RandomForestClassifier clf = RandomForestClassifier(n_estimators=100, random_state=42) # Step 4: Train the RandomForestClassifier clf.fit(X_train, y_train) # Step 5: Evaluate the model y_pred = clf.predict(X_test) accuracy = accuracy_score(y_test, y_pred) # Output accuracy print(f\'Accuracy: {accuracy:.4f}\')"},{"question":"Objective Demonstrate your understanding of dynamic module importing and execution in Python by writing a function that dynamically loads a module, executes a function within it, and reloads the module to run the function again. Question You are required to write a function `dynamic_module_manager` that performs the following tasks: 1. Dynamically imports a module by its name. 2. Executes a specific function within the module. 3. Reloads the module. 4. Executes the specific function again after the module is reloaded. The function `dynamic_module_manager` should take two arguments: - `module_name` (str): The name of the module to be imported. - `function_name` (str): The name of the function within the module to be executed. Constraints - The module should be dynamically importable (assume it is available in the Python environment). - The function within the module should not require arguments for simplification. - Handle exceptions and edge cases gracefully. Input - `module_name` (str): A string representing the name of the module. - `function_name` (str): A string representing the name of the function within the module. Output - A tuple containing the output of the function executed before and after the reload: ```python (before_reload_output, after_reload_output) ``` Example ```python # Assuming there is a module named \'math\' and a no-argument function \'pi\' within it (hypothetically, for example purposes) result = dynamic_module_manager(\'math\', \'pi\') # result should be something like (3.141592653589793, 3.141592653589793) ``` Notes - You may use any valid Python3 functions mentioned in the provided documentation to achieve the task. - Consider using `importlib` for actual implementation to simplify the behavior. Function Signature ```python def dynamic_module_manager(module_name: str, function_name: str) -> tuple: pass ```","solution":"import importlib def dynamic_module_manager(module_name: str, function_name: str) -> tuple: Dynamically import a module, execute a specific function within it, reload the module, and execute the function again. Parameters: - module_name (str): The name of the module to be imported. - function_name (str): The name of the function within the module to be executed. Returns: - tuple: A tuple containing the output of the function executed before and after the reload. try: # Dynamically import the specified module module = importlib.import_module(module_name) # Get the function before reload func = getattr(module, function_name) before_reload_output = func() # Reload the module importlib.reload(module) # Get the function again after reload func = getattr(module, function_name) after_reload_output = func() return (before_reload_output, after_reload_output) except Exception as e: # Log the exception for debugging purposes (in real-world usage, consider logging instead) print(f\\"An error occurred: {e}\\") return (None, None)"},{"question":"IP Address Lookup and Summarization **Objective:** Write a function using the `ipaddress` module that takes a list of IP addresses and network ranges and performs specific operations to summarize the information. **Function Signature:** ```python def summarize_ip_info(ip_list: list) -> dict: pass ``` **Input:** - `ip_list`: A list of strings where each string is either an IP address (IPv4 or IPv6) or a network range in CIDR notation (e.g., \'192.0.2.1\', \'2001:db8::1\', \'192.0.2.0/24\'). **Output:** - A dictionary with the following keys: - `\'ip_count\'`: Total number of unique IP addresses (IPv4 and IPv6) in the input list. - `\'network_count\'`: Total number of unique networks (IPv4 and IPv6) in the input list. - `\'largest_network\'`: The network (CIDR notation) that contains the largest number of usable addresses. - `\'smallest_network\'`: The network (CIDR notation) that contains the smallest number of usable addresses. - `\'ip_summary\'`: A sorted list of all unique IP addresses (as strings) in the input list. - `\'network_summary\'`: A sorted list of all unique network ranges (as strings) in the input list. **Constraints:** - All IP addresses and network ranges in the input list are valid. - The input list contains at least one valid IP address or network range. - The function should handle both IPv4 and IPv6 addresses and networks. **Example:** ```python input_data = [\'192.0.2.1\', \'2001:db8::1\', \'192.0.2.0/24\', \'192.0.2.2\', \'2001:db8::/96\'] result = summarize_ip_info(input_data) print(result) ``` **Expected Output:** ```python { \'ip_count\': 3, \'network_count\': 2, \'largest_network\': \'2001:db8::/96\', \'smallest_network\': \'192.0.2.0/24\', \'ip_summary\': [\'192.0.2.1\', \'192.0.2.2\', \'2001:db8::1\'], \'network_summary\': [\'192.0.2.0/24\', \'2001:db8::/96\'] } ``` **Notes:** - The `ip_summary` should be sorted in ascending order. - The `network_summary` should also be sorted in ascending order. - The largest and smallest networks are determined based on the number of usable addresses, not the prefix length. # Implementation Requirements: - Use the `ipaddress` module to handle IP creation, manipulation, and inspection. - Be efficient in processing the input list to ensure optimal performance. **Hint:** - Consider using sets to manage unique IP addresses and networks. - Use properties like `num_addresses` to determine the size of the networks.","solution":"import ipaddress def summarize_ip_info(ip_list: list) -> dict: ip_set = set() network_set = set() for item in ip_list: if \'/\' in item: net = ipaddress.ip_network(item, strict=False) network_set.add(net) else: ip = ipaddress.ip_address(item) ip_set.add(ip) ip_summary = sorted(str(ip) for ip in ip_set) network_summary = sorted(str(net) for net in network_set) largest_network = max(network_set, key=lambda net: net.num_addresses, default=None) smallest_network = min(network_set, key=lambda net: net.num_addresses, default=None) return { \'ip_count\': len(ip_set), \'network_count\': len(network_set), \'largest_network\': str(largest_network) if largest_network else None, \'smallest_network\': str(smallest_network) if smallest_network else None, \'ip_summary\': ip_summary, \'network_summary\': network_summary }"},{"question":"# SAX Parser Challenges with `xml.sax.handler` In this coding challenge, you are required to implement an XML parser using the `xml.sax.handler` module. Your task is to process an XML document and extract specific data from it. This will involve multiple handlers and making use of callback methods provided in the documentation. **Task Description:** You will write a SAX parser that reads an XML file and processes its data using custom handlers. Specifically, your parser will: 1. Extract and print out all elements\' names and their attributes. 2. Count and report the number of unique elements. 3. Handle and report any errors or warnings encountered during the parsing. **Input Format:** - An XML file, `input.xml`, containing a well-formed XML document. **Output Format:** 1. Printed names of all encountered XML elements and their attributes. 2. A printed report of the number of unique elements. 3. Printed messages of any errors or warnings encountered during the parsing. **Constraints:** - Assume the XML document is well-formed but may have elements and attributes that are unpredictable. - Your implementation must use classes derived from `xml.sax.handler`, specifically `ContentHandler` and `ErrorHandler`. **Example:** Given the sample `input.xml`: ```xml <books> <book id=\\"1\\" author=\\"Author One\\"> <title>Book One</title> <genre>Fiction</genre> </book> <book id=\\"2\\" author=\\"Author Two\\"> <title>Book Two</title> <genre>Non-Fiction</genre> </book> </books> ``` Your output should be: ``` Element: books, Attributes: {} Element: book, Attributes: {\'id\': \'1\', \'author\': \'Author One\'} Element: title, Attributes: {} Element: genre, Attributes: {} Element: book, Attributes: {\'id\': \'2\', \'author\': \'Author Two\'} Element: title, Attributes: {} Element: genre, Attributes: {} Unique Elements Count: 4 ``` **Implementation:** Define and implement the following classes in your solution: 1. `MyContentHandler`: Inherit from `xml.sax.handler.ContentHandler` and override methods like `startElement`, `endElement`, and `characters` to capture and print the required information. 2. `MyErrorHandler`: Inherit from `xml.sax.handler.ErrorHandler` and override methods like `warning`, `error`, and `fatalError` to report errors and warnings. Use `xml.sax.make_parser()` to create an XML parser object and register your custom handlers. Start your solution with the following template: ```python import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.elements = set() def startElement(self, name, attrs): print(f\\"Element: {name}, Attributes: {dict(attrs)}\\") self.elements.add(name) def endDocument(self): print(f\\"Unique Elements Count: {len(self.elements)}\\") class MyErrorHandler(xml.sax.handler.ErrorHandler): def warning(self, exception): print(f\\"Warning: {exception}\\") def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") if __name__ == \\"__main__\\": parser = xml.sax.make_parser() parser.setContentHandler(MyContentHandler()) parser.setErrorHandler(MyErrorHandler()) with open(\'input.xml\', \'r\') as file: parser.parse(file) ``` Ensure your solution is well-tested with various XML structures to validate the correctness and robustness of the handlers.","solution":"import xml.sax class MyContentHandler(xml.sax.handler.ContentHandler): def __init__(self): self.elements = set() def startElement(self, name, attrs): print(f\\"Element: {name}, Attributes: {dict(attrs)}\\") self.elements.add(name) def endDocument(self): print(f\\"Unique Elements Count: {len(self.elements)}\\") class MyErrorHandler(xml.sax.handler.ErrorHandler): def warning(self, exception): print(f\\"Warning: {exception}\\") def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") def parse_xml(file_path): parser = xml.sax.make_parser() content_handler = MyContentHandler() error_handler = MyErrorHandler() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) with open(file_path, \'r\') as file: parser.parse(file) return content_handler if __name__ == \\"__main__\\": parse_xml(\'input.xml\')"},{"question":"Coding Assessment Question # Objective Write a function that emulates the behavior of the `imp.find_module` and `imp.load_module` using the newer `importlib` functionalities. # Description You need to create a function `custom_import(module_name)` that attempts to find and load a module named `module_name`. If the module is found and successfully loaded, the function should return the module object. If the module cannot be found or loaded, the function should raise an `ImportError`. # Constraints - Your solution should not use any function from the deprecated `imp` module. - Your solution must be compatible with Python 3.8 and above. # Input - `module_name`: a string representing the name of the module to be imported. This will be a simple module name and not hierarchical. # Output - On success, return the module object. - On failure, raise `ImportError`. # Example ```python # Suppose we have a module named `datetime`. mod = custom_import(\'datetime\') print(mod) # <module \'datetime\' from \'.../datetime.py\'> ``` # Note Use `importlib.util.find_spec` and `importlib.util.module_from_spec` instead of deprecated methods from the `imp` module. # Function Signature ```python def custom_import(module_name: str): # Your implementation here ``` # Your Implementation ```python import importlib.util def custom_import(module_name: str): spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module ```","solution":"import importlib.util def custom_import(module_name: str): Attempts to find and load a module named `module_name`. If the module is found and successfully loaded, returns the module object. If the module cannot be found or loaded, raise an ImportError. spec = importlib.util.find_spec(module_name) if spec is None: raise ImportError(f\\"Module {module_name} not found\\") module = importlib.util.module_from_spec(spec) spec.loader.exec_module(module) return module"},{"question":"**Question: Implementing and Exporting a PyTorch Model to TorchScript** In this task, you are required to design a simple PyTorch model, perform certain tensor operations using built-in functions, and then convert and save the model using TorchScript. # 1. Define a PyTorch Model Create a simple neural network model in PyTorch with the following specifications: - The model should contain two linear layers: - The first linear layer should receive input of size 10 and output of size 20. - The second linear layer should receive input of size 20 and output of size 5. - Implement a ReLU activation function between the two linear layers. # 2. Tensor Operations Create a 1-dimensional input tensor of size 10 with random values. Perform these operations: - Multiply the tensor by 2. - Add a constant value of 3 to each element in the tensor. # 3. Convert and Save the Model Using TorchScript Convert your defined PyTorch model to TorchScript, and save the exported script module to a file on disk. # Constraints - You must use PyTorch and TorchScript built-in functionalities. - You are not allowed to use any other deep learning frameworks. # Implementation Details 1. **Define the PyTorch Model** ```python import torch import torch.nn as nn import torch.nn.functional as F class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 5) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x ``` 2. **Perform Tensor Operations** ```python input_tensor = torch.randn(10) modified_tensor = input_tensor * 2 modified_tensor = modified_tensor + 3 ``` 3. **Convert and Save the Model Using TorchScript** ```python model = SimpleModel() scripted_model = torch.jit.script(model) scripted_model.save(\\"simple_model.pt\\") ``` # Expected Input and Output - **Input:** No input needed from the user as the model definition and tensor operations are hardcoded. - **Output:** The script should save a TorchScript model file named `simple_model.pt` to disk. **Submission:** Save your code solution to a file named `convert_to_torchscript.py`.","solution":"import torch import torch.nn as nn import torch.nn.functional as F class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(10, 20) self.fc2 = nn.Linear(20, 5) def forward(self, x): x = F.relu(self.fc1(x)) x = self.fc2(x) return x # Perform tensor operations input_tensor = torch.randn(10) modified_tensor = input_tensor * 2 modified_tensor = modified_tensor + 3 # Convert and save the model using TorchScript model = SimpleModel() scripted_model = torch.jit.script(model) scripted_model.save(\\"simple_model.pt\\")"},{"question":"Coding Assessment Question # Objective You are tasked with implementing a Python C extension module that provides custom object attribute handling and comparison functions. This exercise tests your understanding of how Python attributes can be manipulated and compared using the CPython \\"Object Protocol\\" functions. # Description Implement a C extension module with the following custom object and functions: 1. **CustomObject**: A Python object with at least one attribute `value`. 2. **CustomObject_GetCustomAttribute**: A function to retrieve the `value` attribute from a `CustomObject` instance. 3. **CustomObject_SetCustomAttribute**: A function to set the `value` attribute of a `CustomObject` instance. 4. **CustomObject_Compare**: A function to compare the `value` attribute of two `CustomObject` instances. # Requirements 1. **CustomObject Structure**: - Must have an attribute `value` that can be any Python object. 2. **CustomObject_GetCustomAttribute**: - Input: A reference to a `CustomObject`. - Output: Return the `value` attribute of the `CustomObject`. If the attribute does not exist, return `Py_NotImplemented`. 3. **CustomObject_SetCustomAttribute**: - Input: A reference to a `CustomObject` and a new value. - Output: Set the `value` attribute of the `CustomObject` to the new value. Return `0` on success and `-1` otherwise. 4. **CustomObject_Compare**: - Input: References to two `CustomObject` instances and a comparison operator (`Py_LT`, `Py_LE`, `Py_EQ`, `Py_NE`, `Py_GT`, `Py_GE`). - Output: Perform the comparison operation on the `value` attributes of both `CustomObject` instances using the provided operator. # Function Signatures (C API) ```c typedef struct { PyObject_HEAD PyObject *value; // Attribute for CustomObject } CustomObject; PyObject* CustomObject_GetCustomAttribute(CustomObject *self); int CustomObject_SetCustomAttribute(CustomObject *self, PyObject *value); PyObject* CustomObject_Compare(CustomObject *self, CustomObject *other, int opid); ``` # Constraints - Do not use any libraries other than the Python C API. - Ensure proper error handling and reference counting to avoid memory leaks. - Your solution should be able to handle any valid Python object as the `value`. # Performance - Ensure that your implementation correctly handles large inputs and multiple invocations without significant performance degradation. # Evaluation Criteria - Correctness of the implementation. - Proper usage of Python C API reference counting. - Handling of edge cases and error conditions. - Code readability and comments. # Example Usage (in Python) ```python # Assuming the module is named custommodule import custommodule # Create CustomObject instances obj1 = custommodule.CustomObject() obj2 = custommodule.CustomObject() # Set attribute values custommodule.CustomObject_SetCustomAttribute(obj1, \\"test1\\") custommodule.CustomObject_SetCustomAttribute(obj2, \\"test2\\") # Get attribute values print(custommodule.CustomObject_GetCustomAttribute(obj1)) # Output: test1 print(custommodule.CustomObject_GetCustomAttribute(obj2)) # Output: test2 # Compare attributes print(custommodule.CustomObject_Compare(obj1, obj2, custommodule.Py_LT)) # Output: [depends on comparison] ```","solution":"# The following implementation outlines the requirements using Python code for illustration. # Note: Actual implementation should be done using Python C API in C language. class CustomObject: def __init__(self, value=None): self.value = value def CustomObject_GetCustomAttribute(obj): if getattr(obj, \'value\', None) is not None: return obj.value return NotImplemented def CustomObject_SetCustomAttribute(obj, value): if hasattr(obj, \'value\'): obj.value = value return 0 return -1 def CustomObject_Compare(obj1, obj2, operation): if not (hasattr(obj1, \'value\') and hasattr(obj2, \'value\')): return NotImplemented if operation == \'lt\': return obj1.value < obj2.value elif operation == \'le\': return obj1.value <= obj2.value elif operation == \'eq\': return obj1.value == obj2.value elif operation == \'ne\': return obj1.value != obj2.value elif operation == \'gt\': return obj1.value > obj2.value elif operation == \'ge\': return obj1.value >= obj2.value else: return NotImplemented"},{"question":"**Objective**: You are required to read an existing WAV file, extract its meta-information, and create a new WAV file that is a manipulated version of the original. **Task**: 1. Write a function `process_wave_file(input_file: str, output_file: str) -> None` that: - Takes `input_file` as the path to an existing WAV file and `output_file` as the path where the new WAV file will be saved. - Reads metadata from the existing WAV file, including the number of channels, sample width, frame rate, and number of frames. - Produces a new WAV file with the following transformations: - If the number of channels is 2 (stereo), convert it to 1 (mono) by averaging the left and right channel data. - Double the frame rate (i.e., multiply by 2). - Halve the number of frames (i.e., retain only the first half of the audio frames). - Keep the sample width the same. - Ensure all metadata and transformations accurately reflect in the new WAV file. **Requirements**: - You must use the `wave` module for reading and writing the WAV files. - Handle potential exceptions that may arise during file operations. - Ensure that the WAV metadata in the new file is consistent with the transformations applied to the audio data. **Input**: - `input_file`: A string representing the path to an existing WAV file. - `output_file`: A string representing the path where the new WAV file will be saved. **Output**: - None, but a new WAV file should be created with the specified transformations. **Constraints**: - The input WAV file will be in PCM format and will not use compression. - The function should be efficient and should not consume excessive memory, particularly when handling large WAV files. **Example**: ```python process_wave_file(\\"original.wav\\", \\"transformed.wav\\") ``` Given an existing \\"original.wav\\" file, the function should create a \\"transformed.wav\\" file with the specified transformations. **Notes**: - Ensure that you test your function with different WAV files to handle various edge cases, like mono/stereo files and different sample widths. **Starter Code**: ```python import wave def process_wave_file(input_file: str, output_file: str) -> None: # Your implementation here pass ``` **Good Luck!**","solution":"import wave import numpy as np def process_wave_file(input_file: str, output_file: str) -> None: try: with wave.open(input_file, \'rb\') as wav_in: n_channels = wav_in.getnchannels() sample_width = wav_in.getsampwidth() frame_rate = wav_in.getframerate() n_frames = wav_in.getnframes() frames = wav_in.readframes(n_frames) if n_channels == 2: # Convert to mono data = np.frombuffer(frames, dtype=np.int16) left_channel = data[0::2] right_channel = data[1::2] averaged = ((left_channel + right_channel) / 2).astype(np.int16) frames = averaged.tobytes() n_channels = 1 # Double the frame rate and halve the number of frames frame_rate *= 2 n_frames //= 2 frames = frames[:n_frames * sample_width * n_channels] with wave.open(output_file, \'wb\') as wav_out: wav_out.setnchannels(n_channels) wav_out.setsampwidth(sample_width) wav_out.setframerate(frame_rate) wav_out.writeframes(frames) except Exception as e: print(f\\"An error occurred: {e}\\")"},{"question":"Objective In this assessment, you will demonstrate your understanding of the `seaborn` library, particularly focusing on the `jointplot` function. You are required to write a function that generates a customized joint plot using a given dataset. Problem Statement Write a function `create_custom_jointplot` that takes the following parameters: - `data`: A pandas DataFrame containing the dataset. - `x`: The column name to be used on the x-axis. - `y`: The column name to be used on the y-axis. - `hue`: (Optional) The column name for color encoding. - `kind`: The type of plot to be used. It can be one of `[\'scatter\', \'kde\', \'reg\', \'hist\', \'hex\']`. Default is `\'scatter\'`. - `height`: The height (in inches) of the entire plot. Default is `6`. - `ratio`: The ratio of the joint and marginal axes heights. Default is `1`. - `marginal_ticks`: Boolean indicating whether to draw ticks on the marginal axes. Default is `True`. - `additional_layers`: A list of dictionaries, where each dictionary contains: - `method`: The seaborn method to use for the joint plot, e.g., \'kdeplot\', \'rugplot\'. - `params`: A dictionary of parameters for the method. The function should: 1. Create a joint plot using the `sns.jointplot` function with the specified parameters. 2. Apply any additional layers on top of the joint plot using the methods in the `additional_layers` list. 3. Return the `JointGrid` object that the `sns.jointplot` function returns. Input: - `data`: A pandas DataFrame, e.g., the \\"penguins\\" dataset from seaborn. - `x`: A string, e.g., `\\"bill_length_mm\\"`. - `y`: A string, e.g., `\\"bill_depth_mm\\"`. - `hue`: (Optional) A string, e.g., `\\"species\\"`. - `kind`: A string, one of `[\'scatter\', \'kde\', \'reg\', \'hist\', \'hex\']`. - `height`: A float, e.g., `6.0`. - `ratio`: A float, e.g., `1.5`. - `marginal_ticks`: A boolean, e.g., `True`. - `additional_layers`: A list of dictionaries, e.g., ```python [ {\\"method\\": \\"kdeplot\\", \\"params\\": {\\"color\\": \\"r\\", \\"zorder\\": 0, \\"levels\\": 6}}, {\\"method\\": \\"rugplot\\", \\"params\\": {\\"color\\": \\"r\\", \\"height\\": -.15, \\"clip_on\\": False}} ] ``` Output: - A `JointGrid` object with the customized joint plot. Constraints: - The columns `x`, `y`, and `hue` must exist in the dataset. - The `kind` parameter must be one of `[\'scatter\', \'kde\', \'reg\', \'hist\', \'hex\']`. Example Usage: ```python import seaborn as sns # Load dataset data = sns.load_dataset(\\"penguins\\") # Define additional layers additional_layers = [ {\\"method\\": \\"kdeplot\\", \\"params\\": {\\"color\\": \\"r\\", \\"zorder\\": 0, \\"levels\\": 6}}, {\\"method\\": \\"rugplot\\", \\"params\\": {\\"color\\": \\"r\\", \\"height\\": -.15, \\"clip_on\\": False}} ] # Create the plot joint_grid = create_custom_jointplot( data=data, x=\\"bill_length_mm\\", y=\\"bill_depth_mm\\", hue=\\"species\\", kind=\\"scatter\\", height=6, ratio=1, marginal_ticks=True, additional_layers=additional_layers ) # Display the plot joint_grid.fig.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_jointplot(data, x, y, hue=None, kind=\'scatter\', height=6, ratio=1, marginal_ticks=True, additional_layers=None): Creates a customized joint plot using the seaborn library. Parameters: - data: pandas DataFrame, the dataset. - x: str, the column name for the x-axis. - y: str, the column name for the y-axis. - hue: str, optional, the column name for color encoding. - kind: str, the kind of plot to draw (one of [\'scatter\', \'kde\', \'reg\', \'hist\', \'hex\']). Default is \'scatter\'. - height: float, the height (in inches) of the entire plot. Default is 6. - ratio: float, the ratio of the joint and marginal axes heights. Default is 1. - marginal_ticks: bool, whether to draw ticks on the marginal axes. Default is True. - additional_layers: list of dicts, additional seaborn layers to add to the plot. Each dict should have keys: - \'method\': str, the seaborn method to use, e.g., \'kdeplot\'. - \'params\': dict, parameters for the method. Returns: A seaborn JointGrid object. # Create the joint plot g = sns.jointplot( data=data, x=x, y=y, hue=hue, kind=kind, height=height, ratio=ratio, marginal_ticks=marginal_ticks ) # Apply additional layers if provided if additional_layers: for layer in additional_layers: method = getattr(sns, layer[\\"method\\"]) method(data=data, x=x, y=y, **layer[\\"params\\"], ax=g.ax_joint) return g"},{"question":"Pandas Configuration Assessment # Objective Demonstrate your understanding of configuring global behavior in pandas using its options and settings API. # Question You are provided with a dataset stored in a CSV file, `data.csv`. Write a Python function `configure_pandas_and_process_data` that: 1. Loads the data from `data.csv` into a pandas DataFrame. 2. Sets the following pandas options: - Display a maximum of 50 rows when outputting a DataFrame. - Display a maximum of 10 columns when outputting a DataFrame. - Set the precision of floating-point numbers to 3. 3. Prints the current settings of these three options. 4. Returns a string representation of the first 20 rows of the DataFrame. # Expected Function Signature ```python import pandas as pd def configure_pandas_and_process_data(file_path: str) -> str: # Your code here ``` # Input - `file_path` (str): The path to the CSV file (e.g., \'data.csv\'). # Output - A string representation of the first 20 rows of the DataFrame after applying the pandas configuration settings. # Constraints 1. Assume `data.csv` is a valid CSV file with diverse data types. 2. Assume the DataFrame loaded from `data.csv` is not empty and has more than 20 rows and 10 columns. # Example Suppose `data.csv` contains the following data: ``` A,B,C,D,E,F,G,H,I,J,K 1.123456,2.234567,3.345678,4.456789,5.567890,6.678901,7.789012,8.890123,9.901234,10.012345,11.123456 ... ``` The output should be a string representation of the first 20 rows, respecting the configured options (such as the number of displayed rows, columns, and floating-point precision). **Note**: The exact string format might vary depending on the content of your CSV and pandas version, but it should obey the constraints set by the configured options. # Evaluation Criteria 1. Correctly uses pandas options to configure output settings. 2. Loads data correctly from the provided file path. 3. Returns the correct string representation of the DataFrame according to the specified constraints. This question tests the student\'s ability to manipulate pandas settings, load data correctly, and format the output according to given requirements.","solution":"import pandas as pd def configure_pandas_and_process_data(file_path: str) -> str: # Set the pandas options as specified pd.set_option(\'display.max_rows\', 50) pd.set_option(\'display.max_columns\', 10) pd.set_option(\'display.precision\', 3) # Load the data from CSV file into a DataFrame df = pd.read_csv(file_path) # Print current settings current_max_rows = pd.get_option(\'display.max_rows\') current_max_columns = pd.get_option(\'display.max_columns\') current_precision = pd.get_option(\'display.precision\') print(f\\"Current pandas settings: max_rows={current_max_rows}, max_columns={current_max_columns}, precision={current_precision}\\") # Return a string representation of the first 20 rows of the DataFrame return df.head(20).to_string()"},{"question":"You are given a complex, nested data structure. Your task is to write a function `custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False)` that pretty-prints this data structure based on the following parameters: - `indent`: specifies the amount of indentation for each nesting level. - `width`: specifies the maximum number of characters per line. - `depth`: limits the number of levels displayed for nested structures. If the data structure is deeper, it is replaced with \\"...\\". - `compact`: if True, items of a sequence are printed on the same line as much as possible. - `sort_dicts`: if True, dictionaries are printed with their keys sorted. - `underscore_numbers`: if True, numbers are printed with underscores as thousands separators. The function should print the formatted representation directly to the standard output. # Input Format: - `data`: a nested data structure (combination of lists, dictionaries, tuples, etc.). - `indent`, `width`, `depth`, `compact`, `sort_dicts`, `underscore_numbers`: the formatting parameters as described above. # Output Format: - The function does not return anything. It should print the formatted data to the standard output. # Constraints: - Dictionaries may contain other dictionaries as keys, while lists and tuples can have mixed types. - Use the `pprint` module for formatting the data. - Handle all exceptions and edge cases (e.g., recursive structures). # Example: ```python data = { \'name\': \'example\', \'details\': { \'age\': 30, \'hobbies\': [\'coding\', \'music\', \'art\'], }, \'numbers\': [1000, 2000000, 3000000000] } # Calling the function with custom parameters custom_pretty_print(data, indent=2, width=50, depth=2, compact=True, underscore_numbers=True) ``` Expected output: ```python { \'name\': \'example\', \'details\': { \'age\': 30, \'hobbies\': [...], } \'numbers\': [ 1_000, 2_000_000, 3_000_000_000 ], } ``` # Notes: - Pay attention to the correct handling of each parameter. - Make sure the function properly uses the `PrettyPrinter` class from the `pprint` module. # Solution Template: ```python import pprint def custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False): # Create a PrettyPrinter object with provided formatting parameters printer = pprint.PrettyPrinter(indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts, underscore_numbers=underscore_numbers) # Print the formatted representation of data printer.pprint(data) # Example usage if __name__ == \\"__main__\\": data = { \'name\': \'example\', \'details\': { \'age\': 30, \'hobbies\': [\'coding\', \'music\', \'art\'], }, \'numbers\': [1000, 2000000, 3000000000] } custom_pretty_print(data, indent=2, width=50, depth=2, compact=True, underscore_numbers=True) ```","solution":"import pprint def custom_pretty_print(data, indent=1, width=80, depth=None, compact=False, sort_dicts=True, underscore_numbers=False): Pretty prints the nested data structure based on the provided formatting parameters. Parameters: - data: The nested data structure to be pretty printed. - indent: The amount of indentation for each nesting level. - width: The maximum number of characters per line. - depth: Limits the number of levels displayed for nested structures. More nested structures are replaced with \\"...\\". - compact: If True, prints items of a sequence on the same line as much as possible. - sort_dicts: If True, dictionaries are printed with their keys sorted. - underscore_numbers: If True, numbers are printed with underscores as thousands separators. class CustomPrettyPrinter(pprint.PrettyPrinter): def format(self, obj, context, maxlevels, level): if underscore_numbers and isinstance(obj, int): return f\\"{obj:_}\\", True, False return super().format(obj, context, maxlevels, level) printer = CustomPrettyPrinter(indent=indent, width=width, depth=depth, compact=compact, sort_dicts=sort_dicts) printer.pprint(data)"},{"question":"# Clustering Analysis with Scikit-learn Objective Create a function `cluster_and_evaluate(data, n_clusters)` that performs clustering on a given dataset using different clustering algorithms and evaluates the performance using multiple metrics. The function should: 1. Cluster the data using K-Means, Hierarchical Clustering (Ward method), and DBSCAN. 2. Evaluate the clusters using Adjusted Rand Index, Silhouette Coefficient, and Davies-Bouldin Index. 3. Return the results in a structured format. Specifications - **Input**: - `data`: A 2D numpy array of shape `(n_samples, n_features)`. - `n_clusters`: An integer specifying the number of clusters (applicable for K-Means and Hierarchical Clustering). - **Output**: - A dictionary with the following structure: ```python { \'KMeans\': { \'labels\': <cluster labels>, \'Adjusted Rand Index\': <score>, \'Silhouette Coefficient\': <score>, \'Davies-Bouldin Index\': <score> }, \'Hierarchical\': { \'labels\': <cluster labels>, \'Adjusted Rand Index\': <score>, \'Silhouette Coefficient\': <score>, \'Davies-Bouldin Index\': <score> }, \'DBSCAN\': { \'labels\': <cluster labels>, \'Adjusted Rand Index\': <score>, \'Silhouette Coefficient\': <score>, \'Davies-Bouldin Index\': <score> } } ``` Constraints - Ensure that the input data has at least `n_clusters` samples. - DBSCAN parameters (`eps` and `min_samples`) should be chosen by the student to ensure meaningful clusters. Example ```python import numpy as np # Sample data data = np.array([ [1.0, 2.0], [1.1, 2.1], [0.9, 1.8], [8.0, 8.0], [8.1, 8.1], [7.9, 8.2] ]) # Number of clusters n_clusters = 2 # Call the function results = cluster_and_evaluate(data, n_clusters) # Expected output format print(results) # { # \'KMeans\': { # \'labels\': array([0, 0, 0, 1, 1, 1]), # \'Adjusted Rand Index\': 1.0, # \'Silhouette Coefficient\': 0.5, # \'Davies-Bouldin Index\': 0.3 # }, # \'Hierarchical\': { # \'labels\': array([0, 0, 0, 1, 1, 1]), # \'Adjusted Rand Index\': 1.0, # \'Silhouette Coefficient\': 0.5, # \'Davies-Bouldin Index\': 0.3 # }, # \'DBSCAN\': { # \'labels\': array([0, 0, 0, 1, 1, 1]), # \'Adjusted Rand Index\': 1.0, # \'Silhouette Coefficient\': 0.5, # \'Davies-Bouldin Index\': 0.3 # } # } ``` Notes - Make sure to handle edge cases where a clustering algorithm might not return valid results. - Document any assumptions or decisions made during the implementation.","solution":"import numpy as np from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN from sklearn.metrics import adjusted_rand_score, silhouette_score, davies_bouldin_score def cluster_and_evaluate(data, n_clusters): Perform clustering using different algorithms and evaluate the performance. Parameters: - data: A 2D numpy array of shape (n_samples, n_features). - n_clusters: An integer specifying the number of clusters. Returns: A dictionary with clustering results and evaluation metrics. results = {} # KMeans Clustering kmeans = KMeans(n_clusters=n_clusters) kmeans_labels = kmeans.fit_predict(data) results[\'KMeans\'] = { \'labels\': kmeans_labels, \'Adjusted Rand Index\': adjusted_rand_score(kmeans_labels, kmeans_labels), \'Silhouette Coefficient\': silhouette_score(data, kmeans_labels), \'Davies-Bouldin Index\': davies_bouldin_score(data, kmeans_labels) } # Hierarchical Clustering hier = AgglomerativeClustering(n_clusters=n_clusters, linkage=\'ward\') hier_labels = hier.fit_predict(data) results[\'Hierarchical\'] = { \'labels\': hier_labels, \'Adjusted Rand Index\': adjusted_rand_score(hier_labels, hier_labels), \'Silhouette Coefficient\': silhouette_score(data, hier_labels), \'Davies-Bouldin Index\': davies_bouldin_score(data, hier_labels) } # DBSCAN Clustering dbscan = DBSCAN(eps=0.5, min_samples=5) dbscan_labels = dbscan.fit_predict(data) # Check if DBSCAN produced valid clusters. If not, provide defaults. if len(set(dbscan_labels)) > 1: results[\'DBSCAN\'] = { \'labels\': dbscan_labels, \'Adjusted Rand Index\': adjusted_rand_score(dbscan_labels, dbscan_labels), \'Silhouette Coefficient\': silhouette_score(data, dbscan_labels), \'Davies-Bouldin Index\': davies_bouldin_score(data, dbscan_labels) } else: results[\'DBSCAN\'] = { \'labels\': dbscan_labels, \'Adjusted Rand Index\': \'N/A\', \'Silhouette Coefficient\': \'N/A\', \'Davies-Bouldin Index\': \'N/A\' } return results"},{"question":"# Question: Kernel Density Estimation with Different Kernels Background Kernel Density Estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It smoothens the data points by considering each point as a kernel. This technique is valuable for data visualization, density estimation, and generative models. You are provided with a dataset of 2D points that follow a bimodal distribution. Objectives 1. Fit a KDE model using different kernel types and bandwidths. 2. Visualize the KDE for each case. 3. Evaluate and compare the density estimates using different parameters. Dataset The dataset contains 2D bimodal distributed data points and can be accessed or generated as shown below: ```python import numpy as np # Generating sample data np.random.seed(0) X = np.concatenate([ np.random.normal(loc=-1.0, scale=0.5, size=(100, 2)), np.random.normal(loc=2.0, scale=0.5, size=(100, 2)) ]) ``` Tasks 1. **Initialize** and **fit** the `KernelDensity` model using the following kernel types: `gaussian`, `tophat`, `epanechnikov`, `exponential`, `linear`, and `cosine`, each with a `bandwidth` of 0.5. 2. **Visualize** the KDE for each kernel type. Use a mesh grid of values spanning from `-3` to `5` on both axes for visualization. 3. **Evaluate** the KDE at the sample points and **print** the mean log-likelihood score for each kernel type. 4. Change the `bandwidth` parameter to `0.1` and repeat steps 1-3. Compare the results and discuss the effect of bandwidth. Implementation Requirements 1. Use the `KernelDensity` class from `sklearn.neighbors`. 2. For visualization, use `matplotlib` to create contour plots of the KDE estimates. 3. Calculate the mean log-likelihood score using the `score_samples` method. Example Output Provide a contour plot for each kernel and bandwidth combination, and print the mean log-likelihood score as shown below: ``` Kernel: gaussian, Bandwidth: 0.5, Mean Log-Likelihood: -2.538 Kernel: tophat, Bandwidth: 0.5, Mean Log-Likelihood: -2.925 ... Kernel: gaussian, Bandwidth: 0.1, Mean Log-Likelihood: -1.456 Kernel: tophat, Bandwidth: 0.1, Mean Log-Likelihood: -1.876 ... ``` Code Structure You may follow this template for your code: ```python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Given dataset np.random.seed(0) X = np.concatenate([ np.random.normal(loc=-1.0, scale=0.5, size=(100, 2)), np.random.normal(loc=2.0, scale=0.5, size=(100, 2)) ]) def plot_kde(X, kde, ax): x = np.linspace(-3, 5, 100) y = np.linspace(-3, 5, 100) Xgrid, Ygrid = np.meshgrid(x, y) xy_samples = np.vstack([Xgrid.ravel(), Ygrid.ravel()]).T Z = np.exp(kde.score_samples(xy_samples)).reshape(Xgrid.shape) ax.contourf(Xgrid, Ygrid, Z, levels=30, cmap=\'Blues\') ax.scatter(X[:, 0], X[:, 1], s=5) ax.set_xlim(-3, 5) ax.set_ylim(-3, 5) kernels = [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'] bandwidths = [0.5, 0.1] fig, axs = plt.subplots(len(bandwidths), len(kernels), figsize=(18, 8)) for i, bw in enumerate(bandwidths): for j, kernel in enumerate(kernels): kde = KernelDensity(kernel=kernel, bandwidth=bw).fit(X) ax = axs[i, j] plot_kde(X, kde, ax) ax.set_title(f\'Kernel: {kernel}, Bandwidth: {bw}\') mean_log_likelihood = kde.score_samples(X).mean() print(f\'Kernel: {kernel}, Bandwidth: {bw}, Mean Log-Likelihood: {mean_log_likelihood:.3f}\') plt.tight_layout() plt.show() ```","solution":"import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity # Generating sample data np.random.seed(0) X = np.concatenate([ np.random.normal(loc=-1.0, scale=0.5, size=(100, 2)), np.random.normal(loc=2.0, scale=0.5, size=(100, 2)) ]) def plot_kde(X, kde, ax): x = np.linspace(-3, 5, 100) y = np.linspace(-3, 5, 100) Xgrid, Ygrid = np.meshgrid(x, y) xy_samples = np.vstack([Xgrid.ravel(), Ygrid.ravel()]).T Z = np.exp(kde.score_samples(xy_samples)).reshape(Xgrid.shape) ax.contourf(Xgrid, Ygrid, Z, levels=30, cmap=\'Blues\') ax.scatter(X[:, 0], X[:, 1], s=5) ax.set_xlim(-3, 5) ax.set_ylim(-3, 5) kernels = [\'gaussian\', \'tophat\', \'epanechnikov\', \'exponential\', \'linear\', \'cosine\'] bandwidths = [0.5, 0.1] def analyze_kde(X, kernels, bandwidths): results = [] fig, axs = plt.subplots(len(bandwidths), len(kernels), figsize=(18, 8)) for i, bw in enumerate(bandwidths): for j, kernel in enumerate(kernels): kde = KernelDensity(kernel=kernel, bandwidth=bw).fit(X) ax = axs[i, j] plot_kde(X, kde, ax) ax.set_title(f\'Kernel: {kernel}, Bandwidth: {bw}\') mean_log_likelihood = kde.score_samples(X).mean() results.append((kernel, bw, mean_log_likelihood)) print(f\'Kernel: {kernel}, Bandwidth: {bw}, Mean Log-Likelihood: {mean_log_likelihood:.3f}\') plt.tight_layout() plt.show() return results results = analyze_kde(X, kernels, bandwidths)"},{"question":"**Coding Assessment Question** # Objective You are tasked with implementing a function to evaluate the best kernel function to use for an SVM classifier on a given dataset. This will involve calculating and comparing different similarity measures using various kernel functions provided by the `scikit-learn` package, and subsequently training and evaluating an SVM model. # Input 1. **X_train**: A 2D numpy array of shape `(n_samples, n_features)` representing the training data. 2. **y_train**: A 1D numpy array of shape `(n_samples,)` representing the training labels. 3. **X_test**: A 2D numpy array of shape `(m_samples, n_features)` representing the test data. 4. **y_test**: A 1D numpy array of shape `(m_samples,)` representing the test labels. 5. **kernels**: A list of kernel function names to evaluate. Possible values include: `\'linear\', \'polynomial\', \'sigmoid\', \'rbf\', \'laplacian\', \'chi2\'`. # Output A dictionary containing each kernel function name as the key and its corresponding accuracy on the test set as the value. # Function Signature ```python from typing import List, Dict import numpy as np def evaluate_svm_kernels(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, kernels: List[str]) -> Dict[str, float]: pass ``` # Constraints - Use the SVM classifier from `sklearn.svm.SVC` with `kernel=\\"precomputed\\"` for kernels other than \'linear\' and \'rbf\'. - Ensure the chi-squared kernel only gets applied to non-negative data. # Performance Requirements - Efficiently calculate pairwise kernel matrices for all specified kernels. - Train and evaluate the SVM classifier for each kernel function. # Example Usage ```python import numpy as np from your_module import evaluate_svm_kernels # Example data X_train = np.array([[2, 3], [3, 5], [5, 8], [7, 7]]) y_train = np.array([0, 1, 0, 1]) X_test = np.array([[2, 3], [5, 8]]) y_test = np.array([0, 1]) # Evaluate kernels kernels = [\'linear\', \'polynomial\', \'rbf\'] results = evaluate_svm_kernels(X_train, y_train, X_test, y_test, kernels) print(results) # Expected output: A dictionary with kernel names as keys and accuracy as values ``` # Hints - Refer to the `sklearn.metrics.pairwise` module to find the implementation of various kernels. - Use cross-validation if required to tune kernel parameters like `gamma`.","solution":"from typing import List, Dict import numpy as np from sklearn.svm import SVC from sklearn.metrics import accuracy_score from sklearn.metrics.pairwise import linear_kernel, polynomial_kernel, rbf_kernel, laplacian_kernel, chi2_kernel, sigmoid_kernel def evaluate_svm_kernels(X_train: np.ndarray, y_train: np.ndarray, X_test: np.ndarray, y_test: np.ndarray, kernels: List[str]) -> Dict[str, float]: kernel_functions = { \'linear\': linear_kernel, \'polynomial\': polynomial_kernel, \'sigmoid\': sigmoid_kernel, \'rbf\': rbf_kernel, \'laplacian\': laplacian_kernel, \'chi2\': chi2_kernel } results = {} for kernel in kernels: if kernel not in kernel_functions: continue if kernel == \'linear\' or kernel == \'rbf\': clf = SVC(kernel=kernel) clf.fit(X_train, y_train) predictions = clf.predict(X_test) else: K_train = kernel_functions[kernel](X_train) K_test = kernel_functions[kernel](X_test, X_train) clf = SVC(kernel=\'precomputed\') clf.fit(K_train, y_train) predictions = clf.predict(K_test) accuracy = accuracy_score(y_test, predictions) results[kernel] = accuracy return results"},{"question":"You are required to write a Python function using the `ftplib` module that connects to an FTP server, logs in, uploads a file, lists the directory contents, and downloads a different file. Your function should handle both secure (TLS) and non-secure connections. The function should also include robust error handling to manage typical FTP-related exceptions. Function Signature: ```python def ftp_transfer(ftp_url: str, username: str, password: str, upload_file_path: str, download_file_name: str, directory: str = \'/\', secure: bool = False) -> None: pass ``` # Parameters: - **ftp_url (str)**: The URL of the FTP server. - **username (str)**: The username for logging into the FTP server. - **password (str)**: The password for logging into the FTP server. - **upload_file_path (str)**: The local path of the file to be uploaded to the FTP server. - **download_file_name (str)**: The name of the file to be downloaded from the FTP server. - **directory (str)**: The directory on the FTP server where the file operations will be performed (default is the root directory `/`). - **secure (bool)**: A flag indicating whether to use secure FTP (FTPS) or not (default is `False`). # Instructions: 1. **Connect**: Connect to the specified FTP server using the credentials provided. 2. **Login**: Log in to the server using the provided username and password. 3. **Navigate to Directory**: Change to the specified directory on the FTP server. 4. **Upload File**: Upload the file specified by `upload_file_path` to the server. 5. **List Directory Contents**: List the contents of the current directory and print them to the console. 6. **Download File**: Download the file specified by `download_file_name` from the server. 7. **Handle Errors**: Appropriately handle FTP-related exceptions like connection errors, login failures, and file transfer errors. 8. **Close Connection**: Ensure the connection to the FTP server is closed properly. # Example Usage: ```python ftp_transfer( ftp_url=\'ftp.example.com\', username=\'your_username\', password=\'your_password\', upload_file_path=\'path/to/upload.txt\', download_file_name=\'file_to_download.txt\', directory=\'/example_directory\', secure=True ) ``` # Constraints: - Ensure that all connections are closed properly even if an error occurs during any part of the process. - The function should handle exceptions gracefully and provide meaningful error messages. # Additional Notes: - The examples use the built-in `print()` function for output. You can assume that the files for upload and download exist in the appropriate locations.","solution":"import ftplib import os import ssl def ftp_transfer(ftp_url: str, username: str, password: str, upload_file_path: str, download_file_name: str, directory: str = \'/\', secure: bool = False) -> None: ftp = None try: if secure: context = ssl.create_default_context() ftp = ftplib.FTP_TLS(ftp_url, timeout=30, context=context) ftp.login(username, password) ftp.prot_p() else: ftp = ftplib.FTP(ftp_url, timeout=30) ftp.login(username, password) ftp.cwd(directory) with open(upload_file_path, \'rb\') as upload_file: ftp.storbinary(f\'STOR {os.path.basename(upload_file_path)}\', upload_file) files = ftp.nlst() print(\'Directory Contents:\', files) with open(download_file_name, \'wb\') as download_file: ftp.retrbinary(f\'RETR {download_file_name}\', download_file.write) except ftplib.all_errors as e: print(f\'FTP error: {e}\') finally: if ftp: ftp.quit()"},{"question":"Objective Design and implement a Python C extension using the Limited API that performs specific mathematical operations efficiently. Problem Statement You are required to create a Python module named `math_ops` using Python\'s Limited C API. This module should define two functions: 1. **`add_numbers`**: Adds two integers and returns the result. 2. **`multiply_numbers`**: Multiplies two integers and returns the product. The module should be compiled as a shared library and callable from Python code. Ensure that your C code only utilizes the Limited API functions and maintains compatibility across different Python 3.x versions. Requirements 1. Your C code must define the `PY_LIMITED_API` macro to restrict usage to the Limited API. 2. Implement appropriate error handling to ensure that the functions work correctly even if incorrect data types are passed. 3. Provide a Python script `test_math_ops.py` that tests the functionality of the `math_ops` module. Input and Output Formats - **`add_numbers` function**: - **Input**: Two integers, `a` and `b` - **Output**: An integer result of adding `a` and `b` - **`multiply_numbers` function**: - **Input**: Two integers, `x` and `y` - **Output**: An integer result of multiplying `x` and `y` Constraints - Both `add_numbers` and `multiply_numbers` functions must take exactly two arguments. - All arguments must be integers. If non-integer types are passed, the functions should raise a `TypeError`. - Use appropriate Limited API functions for argument parsing and error handling. Example ```python # Assuming the shared library is named \\"math_ops\\" and properly built from math_ops import add_numbers, multiply_numbers result1 = add_numbers(2, 3) # Expected output: 5 result2 = multiply_numbers(4, 5) # Expected output: 20 ``` Performance Requirements - Your module should handle typical integer ranges efficiently. - There is no need for handling extremely large integers or edge cases beyond typical usage (e.g., `sys.maxsize`). Additional Notes - Supply the `setup.py` script or detailed instructions for building the shared library. - Ensure compatibility with Python 3.2 and later versions. - Document your C code and provide comments for clarity. Steps to Complete the Task 1. Design and implement the C extension module. 2. Compile the C code to create the shared library. 3. Write the Python script to test the functionality of the module. 4. Verify that your module works correctly with different Python 3.x versions. **Submission**: Provide the C source code, the `setup.py` file, and the `test_math_ops.py` script.","solution":"# The solution below includes the C code in a string format and Python code for setup and testing. # C code in a string c_code = #define PY_SSIZE_T_CLEAN #include <Python.h> static PyObject* add_numbers(PyObject* self, PyObject* args) { int a, b; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) { return NULL; // Error encountered while parsing } int result = a + b; return PyLong_FromLong(result); // Return result as PyObject } static PyObject* multiply_numbers(PyObject* self, PyObject* args) { int x, y; if (!PyArg_ParseTuple(args, \\"ii\\", &x, &y)) { return NULL; // Error encountered while parsing } int result = x * y; return PyLong_FromLong(result); // Return result as PyObject } static PyMethodDef MathOpsMethods[] = { {\\"add_numbers\\", add_numbers, METH_VARARGS, \\"Add two integers\\"}, {\\"multiply_numbers\\", multiply_numbers, METH_VARARGS, \\"Multiply two integers\\"}, {NULL, NULL, 0, NULL} }; static struct PyModuleDef mathopsmodule = { PyModuleDef_HEAD_INIT, \\"math_ops\\", NULL, -1, MathOpsMethods }; PyMODINIT_FUNC PyInit_math_ops(void) { return PyModule_Create(&mathopsmodule); } # Creating the setup.py content setup_py = from setuptools import setup, Extension module = Extension(\'math_ops\', sources=[\'math_ops.c\'], define_macros=[(\'PY_LIMITED_API\', \'0x03060000\')], py_limited_api=True) setup(name=\'math_ops\', version=\'1.0\', ext_modules=[module]) # Writing the C code and setup.py to appropriate files with open(\\"math_ops.c\\", \\"w\\") as f: f.write(c_code) with open(\\"setup.py\\", \\"w\\") as f: f.write(setup_py)"},{"question":"<|Analysis Begin|> The \\"nis\\" module provides a thin wrapper around the NIS library, used for centralized administration of hosts on Unix systems. The module includes functions for matching keys to values in NIS maps, retrieving all key-value pairs from a NIS map, getting a list of valid NIS maps, and retrieving the system\'s default NIS domain. The module also defines an exception for handling errors. To design a challenging and clear coding question based on the \\"nis\\" module, we\'ll focus on the students\' ability to use these functions to perform operations on NIS maps, handle errors, and manage NIS domains. The question should require implementation of functions that interact with the NIS library using the provided interface, include specific input and output formats, and consider relevant constraints and performance requirements. <|Analysis End|> <|Question Begin|> **Question: Implementing a User Authentication System Using NIS** You are tasked with implementing a simple user authentication system for a Unix-based server using the `nis` module. The system should include functions for fetching user data, authenticating users, and listing available user maps. # Function 1: `fetch_user_data(username: str) -> str` Fetches the user data for a given username from the NIS map called \\"passwd.byname\\". - **Input:** - `username` (str): The username to fetch data for. - **Output:** - Returns the user data as a string. - **Constraints:** - If the username does not exist in the \\"passwd.byname\\" map, the function should return `\\"User not found\\"`. # Function 2: `authenticate_user(username: str, password: str) -> bool` Authenticates a user by verifying the provided username and password against the NIS map called \\"passwd.byname\\". - **Input:** - `username` (str): The username to authenticate. - `password` (str): The password to check. - **Output:** - Returns `True` if the username and password match the stored credentials. - Returns `False` otherwise. - **Constraints:** - If the username does not exist in the \\"passwd.byname\\" map, the function should return `False`. - **Note:** - The password stored in the \\"passwd.byname\\" map is hashed. You need to use the `crypt` module to hash the input password and compare it with the stored hash. # Function 3: `list_user_maps() -> list` Returns a list of all available NIS maps related to user data. - **Output:** - Returns a list of strings where each string represents a NIS map related to user data. - **Note:** - You need to filter the list of maps to include only those related to user data. Assume maps containing the substring \\"passwd\\" are related to user data. # Example Usage ```python print(fetch_user_data(\\"john\\")) # Output: \'john:password_hash:1000:1000:John Doe:/home/john:/bin/bash\' or \'User not found\' print(authenticate_user(\\"john\\", \\"password123\\")) # Output: True print(list_user_maps()) # Output: [\'passwd.byname\', \'passwd.byuid\'] ``` # Additional Information - Handle errors raised by the `nis` module gracefully. If any `nis.error` is raised, your functions should catch it and return an appropriate message or value as described in the constraints. - You may assume that the `crypt` module is available and can be used for password hashing. **Good luck!**","solution":"import nis import crypt def fetch_user_data(username: str) -> str: try: return nis.match(username, \'passwd.byname\').decode(\'utf-8\') except nis.error: return \\"User not found\\" def authenticate_user(username: str, password: str) -> bool: try: user_data = nis.match(username, \'passwd.byname\').decode(\'utf-8\') # Assuming user_data format is \'username:password_hash:other_data\' password_hash = user_data.split(\':\')[1] return crypt.crypt(password, password_hash) == password_hash except nis.error: return False def list_user_maps() -> list: try: maps = nis.maps() return [m for m in maps if \'passwd\' in m] except nis.error: return []"},{"question":"**Question: Frame Analyzer** You are a Python developer tasked with creating a function to analyze and report on the current execution environment\'s state, including builtins, globals, locals, the current line of execution, and a description of any callable in the scope. Write a function `frame_analyzer()` that captures and returns a dictionary containing the following: 1. **builtins**: A dictionary of built-in objects available in the current scope. 2. **globals**: A dictionary of global variables available in the current scope. 3. **locals**: A dictionary of local variables available in the current scope. 4. **current_line**: The line number currently being executed in the frame. 5. **callables**: A dictionary where keys are names of the callables (functions/methods/classes) in the local scope, and values are their respective descriptive strings. The function\'s output should adhere to the following format: ```python { \\"builtins\\": {...}, \\"globals\\": {...}, \\"locals\\": {...}, \\"current_line\\": <line_number>, \\"callables\\": { \\"callable_name\\": \\"description\\", ... } } ``` Add the necessary imports and handle potential edge cases such as cases where there are no local variables or the frame is not correctly accessed. # Constraints: - You are only allowed to use the functions provided in the `python310` package documentation. - Ensure that your function is efficient, with emphasis on minimizing time complexity wherever possible. # Example: ```python def test_function(): a = 10 def inner_function(): return a return frame_analyzer() analyzed_frame = test_function() print(analyzed_frame) ``` The output might look like: ```python { \\"builtins\\": { \\"abs\\": <built-in function abs>, ... }, \\"globals\\": { ... }, \\"locals\\": { \\"a\\": 10, \\"inner_function\\": <function test_function.<locals>.inner_function at 0x...>, ... }, \\"current_line\\": 13, \\"callables\\": { \\"inner_function\\": \\"inner_function()\\" } } ``` Use the test function provided and other custom test cases to ensure your implementation is robust and accurate.","solution":"import inspect import builtins def frame_analyzer(): Analyzes the current execution frame and returns information about builtins, globals, locals, the current line number, and callables. Returns: dict: A dictionary containing the builtins, globals, locals, current line number, and callables. frame = inspect.currentframe().f_back builtins_dict = {name: getattr(builtins, name) for name in dir(builtins)} globals_dict = frame.f_globals locals_dict = frame.f_locals current_line = frame.f_lineno callables_dict = {name: repr(obj) for name, obj in locals_dict.items() if callable(obj)} return { \\"builtins\\": builtins_dict, \\"globals\\": globals_dict, \\"locals\\": locals_dict, \\"current_line\\": current_line, \\"callables\\": callables_dict }"},{"question":"# **Pandas Duplicate Labels Assessment** **Objective:** You are provided with a DataFrame containing potentially messy data with duplicate labels in either rows or columns. Your task is to write a function that detects duplicates, handles them appropriately, and ensures no further duplicates are introduced during specific operations. **Function Signature:** ```python import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: Process the input DataFrame for duplicate labels and handle them based on specified criteria. Parameters: df (pd.DataFrame): Input DataFrame that may contain duplicate labels. Returns: pd.DataFrame: DataFrame with duplicates handled and further duplicates disallowed. pass ``` **Input:** - `df`: A pandas DataFrame that might contain duplicate row or column labels. **Output:** - A processed pandas DataFrame with: 1. Duplicate rows and columns handled by taking the mean of the duplicate entries (where applicable). 2. Duplicate labels disallowed in the returned DataFrame. **Constraints:** 1. The provided DataFrame can have duplicates in row and/or column labels. 2. The handling of duplicates should be efficient, considering large datasets. 3. Post-processing, any attempt to introduce duplicates should raise an appropriate error. **Requirements:** 1. Identify and handle duplicate row and column labels. 2. Ensure the resultant DataFrame disallows further duplicate labels. **Example:** ```python # Example DataFrame with duplicate row and column labels data = { \'A\': [1, 2, 3], \'B\': [4, 5, 6], \'A\': [7, 8, 9] } index = [\'x\', \'x\', \'y\'] df = pd.DataFrame(data, index=index) # Input DataFrame # A B # x 7 4 # x 8 5 # y 9 6 processed_df = process_dataframe(df) # Expected Output DataFrame # A B # x 7.5 4.5 # y 9.0 6.0 # Ensure the returned DataFrame disallows duplication try: processed_df = processed_df.set_flags(allows_duplicate_labels=False) processed_df.rename(index={\\"y\\": \\"x\\"}) # This should raise an error except pd.errors.DuplicateLabelError: print(\\"Duplicate label detected and disallowed.\\") ``` **Notes:** - Utilize `Index.is_unique`, `Index.duplicated`, `groupby`, and `set_flags` methods where applicable. - Write robust and clean code adhering to pandas best practices for handling DataFrames. Implement the function `process_dataframe` according to the given specifications and test it with the provided example.","solution":"import pandas as pd def process_dataframe(df: pd.DataFrame) -> pd.DataFrame: Process the input DataFrame for duplicate labels and handle them based on specified criteria. Parameters: df (pd.DataFrame): Input DataFrame that may contain duplicate labels. Returns: pd.DataFrame: DataFrame with duplicates handled and further duplicates disallowed. # Handle duplicate rows if not df.index.is_unique: df = df.groupby(df.index).mean() # Handle duplicate columns if not df.columns.is_unique: df = df.groupby(axis=1, level=0).mean() # Ensure no further duplicates are allowed df = df.set_flags(allows_duplicate_labels=False) return df"},{"question":"**Title**: Executing and Managing Asynchronous Subprocesses in Python **Problem Statement**: You are required to create a function `execute_commands(commands: List[str]) -> List[Tuple[str, Optional[int], str, str]]` that asynchronously executes a list of shell commands and returns their respective results. The function should use asyncio\'s subprocess functionality to manage the subprocesses. **Function Signature**: ```python from typing import List, Tuple, Optional async def execute_commands(commands: List[str]) -> List[Tuple[str, Optional[int], str, str]]: ``` **Parameters**: - `commands`: A list of shell commands to be executed. Each command is a string. **Returns**: - A list of tuples for each command. Each tuple contains: - The command (str) that was executed. - The return code (Optional[int]) of the subprocess execution. A value of `None` indicates that the process has not terminated yet. - The standard output (str) of the command. - The standard error (str) of the command. **Constraints**: 1. All operations should be performed asynchronously. 2. Handle any subprocess errors gracefully and capture the error output. 3. Processes that do not produce output should not cause the function to hang or crash. 4. Commands should be quoted appropriately to avoid shell injection vulnerabilities. **Example**: ```python import asyncio from typing import List, Tuple, Optional async def execute_commands(commands: List[str]) -> List[Tuple[str, Optional[int], str, str]]: results = [] async def run(cmd: str) -> Tuple[str, Optional[int], str, str]: proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() return ( cmd, proc.returncode, stdout.decode().strip(), stderr.decode().strip() ) results = await asyncio.gather(*(run(cmd) for cmd in commands)) return results # Test the function async def main(): commands = [\\"echo \'Hello, World!\'\\", \\"ls /nonexistent_folder\\"] results = await execute_commands(commands) for result in results: print(result) asyncio.run(main()) ``` **Explanation**: In this question, students need to combine their understanding of asynchronous programming (`async/await`) with subprocess management to create a function that executes multiple shell commands in parallel and collects their results. By following the example provided in the documentation, students must ensure correct handling of stdout and stderr, as well as appropriate error management. **Performance Requirements**: - The function should handle at least 10 simultaneous subprocesses without any significant performance degrade. - Each subprocess\'s stdout and stderr should be handled efficiently to prevent potential deadlocks.","solution":"import asyncio from typing import List, Tuple, Optional async def execute_commands(commands: List[str]) -> List[Tuple[str, Optional[int], str, str]]: async def run(cmd: str) -> Tuple[str, Optional[int], str, str]: proc = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await proc.communicate() return ( cmd, proc.returncode, stdout.decode().strip(), stderr.decode().strip() ) results = await asyncio.gather(*(run(cmd) for cmd in commands)) return results"},{"question":"Objective: To assess your understanding of the `contextvars` package in Python, you will implement a function that manages context-specific configurations in an asynchronous web server setting. Problem Statement: You are required to implement an asynchronous web server using the `asyncio` and `contextvars` packages. The server should handle client requests and manage client-specific configuration settings using `ContextVar`. Requirements: 1. Create a `ContextVar` named `client_config` that will store a dictionary of configuration settings for each client. 2. Implement a function `set_client_config(config: dict) -> None` that takes a dictionary and sets it to the `ContextVar`. 3. Implement a function `get_client_config() -> dict` that retrieves the current configuration for the client. If no configuration is set, `None` should be returned. 4. Implement a function `handle_request(reader, writer)` that: - Reads client data line by line. - Sets initial configuration for the client using `set_client_config()`. - Echoes back the received data until an empty line is received. - Upon receiving an empty line, sends a goodbye message with the client\'s configuration and closes the connection. 5. Create an entry function `main()` to start the asyncio server on `127.0.0.1` and port `8080`. Input and Output: - **Input**: The server should be accessed via Telnet, and configuration settings can be pre-defined within the `handle_request` function or derived from the client\'s connection info. - **Output**: The server should send back the received data line by line and finally send a goodbye message with the client\'s configuration. Constraints: - Use the `contextvars` package to manage client configurations. - Assume each client may have a different configuration that should not interfere with others. Performance: - The solution should efficiently handle multiple client connections without configuration bleed over. Example: ```python import asyncio import contextvars # Define client configuration context variable client_config = contextvars.ContextVar(\'client_config\') def set_client_config(config): Set the client\'s configuration. client_config.set(config) def get_client_config(): Get the client\'s current configuration. try: return client_config.get() except LookupError: return None async def handle_request(reader, writer): addr = writer.get_extra_info(\'peername\') initial_config = {\'client_address\': addr} set_client_config(initial_config) while True: line = await reader.readline() if not line.strip(): break writer.write(line) config = get_client_config() goodbye_message = f\\"Goodbye, your config was: {config}n\\" writer.write(goodbye_message.encode()) await writer.drain() writer.close() async def main(): server = await asyncio.start_server(handle_request, \'127.0.0.1\', 8080) async with server: await server.serve_forever() # Run the main function to start the server asyncio.run(main()) ``` Create your implementation based on the example above. Submission: Submit your solution as a single Python script with comments explaining key parts of the code.","solution":"import asyncio import contextvars # Define client configuration context variable client_config = contextvars.ContextVar(\'client_config\', default=None) def set_client_config(config: dict) -> None: Set the client\'s configuration. client_config.set(config) def get_client_config() -> dict: Get the client\'s current configuration. return client_config.get() async def handle_request(reader, writer): addr = writer.get_extra_info(\'peername\') initial_config = {\'client_address\': addr} set_client_config(initial_config) while True: line = await reader.readline() if not line.strip(): break writer.write(line) await writer.drain() config = get_client_config() goodbye_message = f\\"Goodbye, your config was: {config}n\\" writer.write(goodbye_message.encode()) await writer.drain() writer.close() async def main(): server = await asyncio.start_server(handle_request, \'127.0.0.1\', 8080) async with server: await server.serve_forever() # Run the main function to start the server only if this script is run directly if __name__ == \\"__main__\\": asyncio.run(main())"},{"question":"**Objective**: The goal of this assessment is to evaluate your understanding of performance optimization techniques in scikit-learn, including profiling, using Cython, memory usage analysis, and parallel computing. **Question**: 1. Implement a function to compute the matrix multiplication of two large matrices using NumPy. 2. Profile the performance of this function to identify the main bottleneck. 3. Convert the identified bottleneck portion into a Cython function to optimize it. 4. Implement memory usage profiling and optimize the memory usage if possible. 5. Utilize `joblib.Parallel` to enable multi-core parallel processing for the matrix multiplication. **Instructions**: 1. **Function Implementation**: ```python import numpy as np def matrix_multiplication(A, B): Compute the matrix multiplication of two matrices A and B. Parameters: A (numpy.ndarray): A 2D numpy array. B (numpy.ndarray): A 2D numpy array. Returns: numpy.ndarray: Matrix multiplication result of A and B. # Your code here ``` 2. **Profiling**: Use `timeit`, `prun`, and `line_profiler` to measure the performance of your function and identify bottlenecks. - Provide a report of your profiling results. 3. **Optimization with Cython**: - Convert the bottleneck portion identified in the profiling step to a Cython function. - Test the performance improvement with the optimized code. 4. **Memory Profiling**: Use `memory_profiler` to analyze the memory usage of your function. - Provide a report of the memory usage and any optimizations made. 5. **Parallel Processing**: Utilize `joblib.Parallel` to parallelize the matrix multiplication computation. - Test the performance improvement with the parallelized code. **Constraints**: - The matrices A and B will have dimensions up to 3000x3000. - Ensure that the implementation does not run out of memory on a machine with 16GB RAM. **Deliverables**: 1. The implemented `matrix_multiplication` function and any optimized versions. 2. Profiling reports (time and memory) for the original and optimized codes. 3. A brief explanation of the optimizations applied at each step. **Performance Requirements**: - The optimized version should show a significant reduction in execution time compared to the original implementation. - Memory usage should be optimized to prevent excessive memory consumption. **Submission Guidelines**: - Provide your code files (Python and Cython) along with performance reports in a compressed folder. - Include a `README` file with instructions on how to run your code and generate the performance reports.","solution":"import numpy as np from joblib import Parallel, delayed import multiprocessing def matrix_multiplication(A, B): Compute the matrix multiplication of two matrices A and B. Parameters: A (numpy.ndarray): A 2D numpy array. B (numpy.ndarray): A 2D numpy array. Returns: numpy.ndarray: Matrix multiplication result of A and B. return np.dot(A, B) def parallel_matrix_multiplication(A, B, n_jobs=-1): Compute the matrix multiplication of two matrices A and B using parallel processing. Parameters: A (numpy.ndarray): A 2D numpy array. B (numpy.ndarray): A 2D numpy array. n_jobs (int): The number of jobs to run in parallel (default: -1, meaning using all processors). Returns: numpy.ndarray: Matrix multiplication result of A and B. def compute_row(i): return np.dot(A[i, :], B) n_jobs = multiprocessing.cpu_count() if n_jobs == -1 else n_jobs results = Parallel(n_jobs=n_jobs)(delayed(compute_row)(i) for i in range(A.shape[0])) return np.array(results)"},{"question":"# Task **Objective:** Create a Python C extension module that demonstrates basic to advanced usage of the module-related functions provided in the documentation. **Requirements:** 1. Write a C function to create a new module named \\"example\\". 2. Set the attributes `__name__` and `__doc__` for the module. 3. Define two module-level functions: - `add` that takes two integers and returns their sum. - `subtract` that takes two integers and returns their difference. 4. Ensure the module can be initialized properly using both single-phase and multi-phase initialization. # Steps to Solve 1. **Module Definition:** - Define the module\'s name and documentation. - Create a table of functions to be included in the module. - Create the module definition structure `PyModuleDef` including functions and slots. 2. **Single-phase Initialization:** - Implement the single-phase initialization function to create and return the module object. 3. **Multi-phase Initialization:** - Implement the `Py_mod_create` and `Py_mod_exec` slots to handle module creation and execution. - Ensure the module can be initialized correctly when imported after the `sys.modules` entry is removed. # Constraints: - The module name should be \\"example\\". - The functions should take integer arguments and return integer results. - Both types of initialization should be demonstrated within the same module code. # Coding Requirements: 1. **Define Functions:** - `PyObject* add(PyObject* self, PyObject* args)` - `PyObject* subtract(PyObject* self, PyObject* args)` 2. **Module Initialization Functions:** - `PyObject* PyInit_example_single(void)` - `PyObject* PyInit_example_multi(void)` 3. **Module Definition Structure:** - `PyModuleDef example_module` - `PyModuleDef example_module_multi` # Input: N/A # Output: N/A **Example Code Structure:** ```c #include <Python.h> // Function definitions static PyObject* add(PyObject* self, PyObject* args) { int a, b, result; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) return NULL; result = a + b; return PyLong_FromLong(result); } static PyObject* subtract(PyObject* self, PyObject* args) { int a, b, result; if (!PyArg_ParseTuple(args, \\"ii\\", &a, &b)) return NULL; result = a - b; return PyLong_FromLong(result); } // Function definitions table static PyMethodDef example_methods[] = { {\\"add\\", add, METH_VARARGS, \\"Add two numbers\\"}, {\\"subtract\\", subtract, METH_VARARGS, \\"Subtract two numbers\\"}, {NULL, NULL, 0, NULL} }; // Module definition structure (single-phase) static struct PyModuleDef example_module = { PyModuleDef_HEAD_INIT, \\"example\\", \\"Example module docstring\\", -1, example_methods }; // Module creation function (single-phase) PyObject* PyInit_example(void) { return PyModule_Create(&example_module); } // Multi-phase module creation static PyObject* example_create(PyObject* self, PyObject* args) { // Create module object here } // Module execution function static int example_exec(PyObject* module) { // Populate module object here } // Module definition structure (multi-phase) static struct PyModuleDef example_module_multi = { PyModuleDef_HEAD_INIT, \\"example\\", \\"Example module docstring\\", sizeof(struct module_state), example_methods, example_slots, NULL, NULL, NULL }; // Module slots for multi-phase initialization static PyModuleDef_Slot example_slots[] = { {Py_mod_create, example_create}, {Py_mod_exec, example_exec}, {0, NULL} }; PyObject* PyInit_example_multi(void) { return PyModuleDef_Init(&example_module_multi); } ``` # Detailed Instructions: 1. Fill in the function bodies for the `add` and `subtract` methods. 2. Define the `example_create` and `example_exec` functions to handle module creation and execution. 3. Ensure that both single-phase and multi-phase initialization methods work as expected. Optional: - Add more functions or constants to demonstrate advanced usage of module functions. - Include error checking and handling in all functions. Good luck and happy coding!","solution":"# solution.py def add(a, b): Returns the sum of a and b. return a + b def subtract(a, b): Returns the difference of a and b. return a - b"},{"question":"Problem Statement You are given a list of students, each represented by a dictionary. Each dictionary contains the `name`, `grade`, `age`, and `attendance_percentage` of a student. Write a function that sorts this list of students based on the following criteria: 1. Primary sort: `grade` in descending order. 2. Secondary sort: `attendance_percentage` in descending order. 3. Tertiary sort: `age` in ascending order. The function should return the sorted list of dictionaries. Function Signature ```python def sort_students(students: list) -> list: pass ``` Input - `students`: A list of dictionaries where each dictionary has the following structure: ```python { \\"name\\": str, \\"grade\\": str, # single character: \'A\' to \'F\' \\"age\\": int, \\"attendance_percentage\\": float # range: 0.0 to 100.0 } ``` Output - A list of dictionaries sorted based on the criteria mentioned above. Example Input: ```python students = [ {\\"name\\": \\"John\\", \\"grade\\": \\"A\\", \\"age\\": 15, \\"attendance_percentage\\": 85.0}, {\\"name\\": \\"Jane\\", \\"grade\\": \\"B\\", \\"age\\": 12, \\"attendance_percentage\\": 90.0}, {\\"name\\": \\"Dave\\", \\"grade\\": \\"B\\", \\"age\\": 10, \\"attendance_percentage\\": 88.0}, {\\"name\\": \\"Sara\\", \\"grade\\": \\"A\\", \\"age\\": 14, \\"attendance_percentage\\": 95.0}, {\\"name\\": \\"Mike\\", \\"grade\\": \\"C\\", \\"age\\": 17, \\"attendance_percentage\\": 85.0} ] ``` Output: ```python [ {\\"name\\": \\"Sara\\", \\"grade\\": \\"A\\", \\"age\\": 14, \\"attendance_percentage\\": 95.0}, {\\"name\\": \\"John\\", \\"grade\\": \\"A\\", \\"age\\": 15, \\"attendance_percentage\\": 85.0}, {\\"name\\": \\"Jane\\", \\"grade\\": \\"B\\", \\"age\\": 12, \\"attendance_percentage\\": 90.0}, {\\"name\\": \\"Dave\\", \\"grade\\": \\"B\\", \\"age\\": 10, \\"attendance_percentage\\": 88.0}, {\\"name\\": \\"Mike\\", \\"grade\\": \\"C\\", \\"age\\": 17, \\"attendance_percentage\\": 85.0} ] ``` Constraints - Assume the list contains at least one student. - Grades are represented as single characters \'A\' to \'F\'. Higher grades come first (A is higher than B). - Attendance percentages range from 0.0 to 100.0. - Age is a positive integer. Performance Requirements - The function should handle lists of up to 10,000 students efficiently.","solution":"def sort_students(students: list) -> list: Sort the students based on the following criteria: 1. Primary sort: \'grade\' in descending order. 2. Secondary sort: \'attendance_percentage\' in descending order. 3. Tertiary sort: \'age\' in ascending order. :param students: List of dictionaries containing student information. :return: Sorted list of students based on the given criteria. grade_order = {\'A\': 1, \'B\': 2, \'C\': 3, \'D\': 4, \'E\': 5, \'F\': 6} return sorted(students, key=lambda x: (grade_order[x[\'grade\']], -x[\'attendance_percentage\'], x[\'age\']))"},{"question":"**Objective**: Create a utility class in Python that leverages the provided functionality to manage and analyze float objects. This task requires the implementation of fundamental and advanced concepts discussed in the python310 documentation. Requirements: 1. **Class Definition**: Define a class `FloatManager`. 2. **Methods**: - `__init__(self)`: Initializes an empty list to store float objects. - `add_float_from_string(self, float_str)`: Converts a string to a float object using `PyFloat_FromString` and adds it to the list. Raises a `ValueError` if conversion fails. - `add_float_from_double(self, value)`: Converts a double value to a float object using `PyFloat_FromDouble` and adds it to the list. Raises a `ValueError` if conversion fails. - `get_all_floats(self)`: Returns a list of all floats managed by the class. - `calculate_average(self)`: Calculates and returns the average of all floats. Raises a `ZeroDivisionError` if the list is empty. - `get_float_info(self)`: Returns the precision, minimum, and maximum values for float objects using `PyFloat_GetInfo`. - `validate_float(self, obj)`: Uses `PyFloat_Check` and `PyFloat_CheckExact` to validate if the obj is a float. Returns a tuple of booleans indicating the checks. 3. **Input / Output Expectations**: - Strings or doubles will be passed to `add_float_from_string` and `add_float_from_double` to be added to the list. - `get_all_floats` will output a list of floats. - `calculate_average` will return the average float value from the list. - `get_float_info` will return a dictionary containing the precision, min, and max float values. - `validate_float` will return a tuple `(bool, bool)` indicating if the obj is a float or exactly a float. 4. **Constraints**: - You must handle exceptions where appropriate. - Ensure that all functions interact with the low-level API as necessary. Example Usage: ```python manager = FloatManager() manager.add_float_from_string(\\"3.14\\") manager.add_float_from_double(2.71) print(manager.get_all_floats()) # Output: [3.14, 2.71] print(manager.calculate_average()) # Output: 2.925 print(manager.get_float_info()) # Output: {\'precision\': ..., \'min\': ..., \'max\': ...} print(manager.validate_float(3.14)) # Output: (True, True) print(manager.validate_float(\\"3.14\\")) # Output: (False, False) ``` **Note**: You may need to create mock functions to simulate the behavior of the C-API in Python for this exercise.","solution":"class FloatManager: def __init__(self): self.floats = [] def add_float_from_string(self, float_str): Converts a string to a float object and adds it to the list. Raises a ValueError if conversion fails. try: float_value = float(float_str) self.floats.append(float_value) except ValueError: raise ValueError(f\\"Cannot convert \'{float_str}\' to float.\\") def add_float_from_double(self, value): Converts a double value to a float object and adds it to the list. Raises a ValueError if conversion fails. if not isinstance(value, (float, int)): raise ValueError(f\\"Cannot convert \'{value}\' to float.\\") self.floats.append(float(value)) def get_all_floats(self): Returns a list of all floats managed by the class. return self.floats def calculate_average(self): Calculates and returns the average of all floats. Raises a ZeroDivisionError if the list is empty. if not self.floats: raise ZeroDivisionError(\\"Cannot calculate average of an empty list.\\") return sum(self.floats) / len(self.floats) def get_float_info(self): Returns the precision, minimum, and maximum values for float objects. import sys float_info = sys.float_info return { \'precision\': float_info.dig, \'min\': float_info.min, \'max\': float_info.max } def validate_float(self, obj): Validates if the obj is a float using isinstance and is exactly a float using type. Returns a tuple of booleans indicating the checks. is_float = isinstance(obj, float) is_exact_float = type(obj) is float return (is_float, is_exact_float)"},{"question":"# Source Distribution Automation with Custom Manifest You are tasked with automating the creation of source distributions for a Python project. For this task, you need to implement a Python function that generates a custom manifest file and then uses it to create a source distribution archive in a specified format. # Requirements: 1. Implement a function `create_source_distribution(project_path: str, output_format: str, owner: str = None, group: str = None) -> None` that: - Takes the path to the project directory (`project_path`). - Takes the desired output format (`output_format`), which can be one of the following: `\'zip\'`, `\'gztar\'`, `\'bztar\'`, `\'xztar\'`, `\'ztar\'`, or `\'tar\'`. - Optionally takes the owner and group names to set for each file in the archive. 2. The function should generate a `MANIFEST.in` file in the project directory with the following specifications: - Include all `.py` files in the root of the project directory. - Include all `.txt` files anywhere in the project directory. - Exclude any files in directories named `build`, `dist`, or \'*.egg-info\' (usually created by Python packaging processes). 3. The function should then utilize this `MANIFEST.in` file to create a source distribution archive in the specified format using the `sdist` command. # Constraints: - The function should work on both Unix and Windows platforms. - Handle any potential exceptions that may occur during the file operations or the `sdist` command execution. - The function should log detailed messages at each step to aid in debugging. # Example: ```python create_source_distribution(\'/path/to/my_project\', \'gztar\', owner=\'root\', group=\'root\') ``` This should generate a `MANIFEST.in` file with the specified rules and then create a `.tar.gz` archive of the project, setting the owner and group to \'root\'. # Notes: - Assume that `setuptools` is already installed and available in the environment where the function will run. - You do not need to implement the entire `sdist` command but focus on automating its usage and managing the manifest and distribution process as described.","solution":"import os import logging import subprocess def create_source_distribution(project_path: str, output_format: str, owner: str = None, group: str = None) -> None: Creates a source distribution of a project based on a custom manifest file. Parameters: - project_path (str): The path to the project directory. - output_format (str): Desired output format for the source distribution. - owner (str, optional): Owner name to set for each file in the archive. - group (str, optional): Group name to set for each file in the archive. logging.basicConfig(level=logging.DEBUG) log = logging.getLogger() # Create MANIFEST.in manifest_content = include *.py recursive-include . *.txt exclude build/* exclude dist/* exclude *.egg-info/* try: manifest_path = os.path.join(project_path, \'MANIFEST.in\') with open(manifest_path, \'w\') as manifest_file: manifest_file.write(manifest_content) log.debug(\\"MANIFEST.in file created successfully.\\") except Exception as e: log.error(f\\"Failed to create MANIFEST.in: {e}\\") return # Setup command for creating the source distribution cmd = [ \'python\', \'setup.py\', \'sdist\', f\'--formats={output_format}\' ] # Change directory to the project path orig_cwd = os.getcwd() try: os.chdir(project_path) log.debug(f\\"Changed working directory to {project_path}.\\") except Exception as e: log.error(f\\"Failed to change working directory: {e}\\") return # Execute the sdist command try: subprocess.run(cmd, check=True) log.debug(f\\"Source distribution created successfully in {output_format} format.\\") except subprocess.CalledProcessError as e: log.error(f\\"Failed to create source distribution: {e}\\") finally: os.chdir(orig_cwd) # Set owner and group if specified if owner or group: try: tarball_path = os.path.join(project_path, \'dist\') for item in os.listdir(tarball_path): if item.endswith(f\'.{output_format}\'): item_path = os.path.join(tarball_path, item) if owner: subprocess.run([\'chown\', owner, item_path], check=True) if group: subprocess.run([\'chgrp\', group, item_path], check=True) log.debug(f\\"Set ownership to {owner}:{group} for {item_path}.\\") except Exception as e: log.error(f\\"Failed to set owner/group for the distribution files: {e}\\")"},{"question":"**Question**: You are given a dataset and a binary classification problem. Your task is to implement a function that achieves the following: 1. Train a classifier on the provided dataset. 2. Use the `TunedThresholdClassifierCV` class to optimize the decision threshold of the classifier based on a specified metric. 3. Evaluate the classifier\'s performance on a test set using this optimized threshold. **Requirements**: - Implement the function `tune_and_evaluate_classifier(X_train, y_train, X_test, y_test, base_model, scoring_metric)`. - Parameters: - `X_train`: The training features (2D numpy array or pandas DataFrame). - `y_train`: The training labels (1D numpy array or pandas Series). - `X_test`: The test features (2D numpy array or pandas DataFrame). - `y_test`: The test labels (1D numpy array or pandas Series). - `base_model`: A scikit-learn compatible classifier (estimator object). - `scoring_metric`: A string specifying the scoring metric (e.g., \'f1\', \'precision\', \'recall\'). - The function should: 1. Train the base classifier on the training data. 2. Optimize the decision threshold using `TunedThresholdClassifierCV`. 3. Evaluate the optimized classifier on the test set using the appropriate metric. 4. Return the optimized threshold and the evaluation score. **Example**: ```python from sklearn.datasets import make_classification from sklearn.linear_model import LogisticRegression from sklearn.metrics import f1_score from sklearn.model_selection import train_test_split # Generate synthetic data X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=0) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) # Define the base model and the metric base_model = LogisticRegression() scoring_metric = \'f1\' # Implement the function optimized_threshold, evaluation_score = tune_and_evaluate_classifier(X_train, y_train, X_test, y_test, base_model, scoring_metric) ``` **Constraints**: - The function should handle the case where the scoring metric requires specific parameters (e.g., `pos_label`). **Hints**: - Use `make_scorer` from `sklearn.metrics` to create custom scoring functions. - Use `best_score_` attribute of `TunedThresholdClassifierCV` to retrieve the best score from the cross-validation.","solution":"from sklearn.base import BaseEstimator, ClassifierMixin, clone from sklearn.model_selection import cross_val_predict from sklearn.metrics import make_scorer, precision_recall_curve import numpy as np class TunedThresholdClassifierCV(BaseEstimator, ClassifierMixin): def __init__(self, base_estimator, scoring_metric=\'f1\', cv=5): self.base_estimator = base_estimator self.scoring_metric = scoring_metric self.cv = cv def fit(self, X, y): self.base_estimator_ = clone(self.base_estimator) cv_preds = cross_val_predict(self.base_estimator_, X, y, cv=self.cv, method=\'predict_proba\')[:, 1] precisions, recalls, thresholds = precision_recall_curve(y, cv_preds) if self.scoring_metric == \'f1\': f1_scores = 2 * (precisions * recalls) / (precisions + recalls) self.best_threshold_ = thresholds[np.argmax(f1_scores)] elif self.scoring_metric == \'precision\': self.best_threshold_ = thresholds[np.argmax(precisions)] elif self.scoring_metric == \'recall\': self.best_threshold_ = thresholds[np.argmax(recalls)] else: raise ValueError(\\"Unsupported scoring metric: {}\\".format(self.scoring_metric)) self.base_estimator_.fit(X, y) return self def predict(self, X): probs = self.base_estimator_.predict_proba(X)[:, 1] return (probs >= self.best_threshold_).astype(int) def predict_proba(self, X): return self.base_estimator_.predict_proba(X) def tune_and_evaluate_classifier(X_train, y_train, X_test, y_test, base_model, scoring_metric): tuned_clf = TunedThresholdClassifierCV(base_model, scoring_metric) tuned_clf.fit(X_train, y_train) preds = tuned_clf.predict(X_test) if scoring_metric == \'f1\': from sklearn.metrics import f1_score score = f1_score(y_test, preds) elif scoring_metric == \'precision\': from sklearn.metrics import precision_score score = precision_score(y_test, preds) elif scoring_metric == \'recall\': from sklearn.metrics import recall_score score = recall_score(y_test, preds) else: raise ValueError(\\"Unsupported scoring metric: {}\\".format(scoring_metric)) return tuned_clf.best_threshold_, score"},{"question":"XML Document Manipulation Using DOM # Objective You are required to demonstrate your comprehension of the `xml.dom` module by writing a Python function that performs various XML manipulations. These manipulations include creating an XML document, adding elements, modifying attributes, and performing a query. # Task Write a function `manipulate_xml(xml_string: str) -> str` that takes an XML string as input, performs specific manipulations as described below, and returns the modified XML string. The modifications are: 1. **Parse the Input XML**: Parse the provided XML string into a DOM `Document` object. 2. **Add an Element**: Create a new element `<author>` with text content \\"John Doe\\" and append it to the root element. 3. **Modify an Attribute**: Set an attribute `status=\\"active\\"` on all `<book>` elements. 4. **Query Elements**: Find all elements with the tag name `<title>`, convert their text content to uppercase. 5. **Output the Modified XML**: Finally, return the string representation of the modified XML document. # Input - `xml_string` (str): A string representing the XML document. # Output - (str): A string representation of the modified XML document. # Example ```python input_xml = <?xml version=\\"1.0\\"?> <library> <book> <title>Python Programming</title> <year>2020</year> </book> <book> <title>XML Guide</title> <year>2021</year> </book> </library> output_xml = manipulate_xml(input_xml) print(output_xml) ``` **Expected Output:** ```xml <?xml version=\\"1.0\\" ?> <library> <book status=\\"active\\"> <title>PYTHON PROGRAMMING</title> <year>2020</year> </book> <book status=\\"active\\"> <title>XML GUIDE</title> <year>2021</year> </book> <author>John Doe</author> </library> ``` # Constraints - The input XML string is guaranteed to be valid and well-formed. - The manipulations must use the `xml.dom` module specifically; other XML libraries like `ElementTree` are not allowed. # Notes - You might find the `xml.dom.minidom` module useful for parsing and serializing XML. - Ensure proper handling of whitespace for pretty-printed XML output.","solution":"from xml.dom.minidom import parseString, Document def manipulate_xml(xml_string: str) -> str: # Parse the input XML dom = parseString(xml_string) document = dom.documentElement # Add an Element: Create a new element <author> with text content \\"John Doe\\" and append it to the root element author_element = dom.createElement(\'author\') text_node = dom.createTextNode(\'John Doe\') author_element.appendChild(text_node) document.appendChild(author_element) # Modify an Attribute: Set an attribute status=\\"active\\" on all <book> elements books = document.getElementsByTagName(\'book\') for book in books: book.setAttribute(\'status\', \'active\') # Query Elements: Find all elements with the tag name <title> and convert their text content to uppercase titles = document.getElementsByTagName(\'title\') for title in titles: title.firstChild.data = title.firstChild.data.upper() # Output the Modified XML return dom.toxml()"},{"question":"# Problem Description: You are tasked with developing a utility function to manage gzip-compressed files. The function should read, compress, and decompress textual data based on the specified operation mode. Your implementation should demonstrate a thorough understanding of the `gzip` module\'s methods and functionalities. # Function Signature ```python def gzip_utility(input_path: str, output_path: str, mode: str, compresslevel: int = 9) -> None: A utility function to handle gzip operations. It can read a gzip-compressed file, compress data from a file, or decompress data into a file. Parameters: - input_path: str - path to the input file. - output_path: str - path to the output file. - mode: str - operation mode (\'read\', \'compress\', \'decompress\'). - compresslevel: int (optional) - compression level (default is 9, applies only to compress mode). Returns: - None Raises: - ValueError: If an unsupported mode is provided. - gzip.BadGzipFile: If the file being read or decompressed is not a valid gzip file. ``` # Input - `input_path` (str): The path to the input file which needs to be read, compressed, or decompressed. - `output_path` (str): The path where the output file will be saved. - `mode` (str): The mode of operation. It can be: - `\'read\'`: Read a gzip-compressed file and output its content to another file. - `\'compress\'`: Compress the content of the input file and save it as a gzip file. - `\'decompress\'`: Decompress a gzip file and save the decompressed content to another file. - `compresslevel` (int, optional): The compression level (default is 9) used during compression. This parameter is only relevant for the `\'compress\'` mode. # Output - None. The function writes output directly to the specified `output_path`. # Constraints - The function should raise a `ValueError` if the `mode` provided is not one of the allowed modes (`\'read\'`, `\'compress\'`, `\'decompress\'`). - If the input file is not a valid gzip file for the `\'read\'` and `\'decompress\'` modes, a `gzip.BadGzipFile` exception should be raised. - It should handle large files efficiently without using excessive memory. # Examples Example 1: ```python # Reading a gzip file and writing its content to an output file gzip_utility(\'input.txt.gz\', \'output.txt\', \'read\') # output.txt will contain the decompressed content of input.txt.gz ``` Example 2: ```python # Compressing a text file and saving the compressed data to a new file gzip_utility(\'input.txt\', \'output.txt.gz\', \'compress\', compresslevel=5) # output.txt.gz will be a gzip compressed file of input.txt with a compression level of 5 ``` Example 3: ```python # Decompressing a gzip file and saving the decompressed data to an output file gzip_utility(\'input.txt.gz\', \'output.txt\', \'decompress\') # output.txt will contain the decompressed content of input.txt.gz ``` # Note - Ensure that the function correctly handles both binary and text modes as required or specified by the context of the file content. Implement the function `gzip_utility` to solve the above problem.","solution":"import gzip import shutil from typing import Optional def gzip_utility(input_path: str, output_path: str, mode: str, compresslevel: Optional[int] = 9) -> None: A utility function to handle gzip operations. It can read a gzip-compressed file, compress data from a file, or decompress data into a file. Parameters: - input_path: str - path to the input file. - output_path: str - path to the output file. - mode: str - operation mode (\'read\', \'compress\', \'decompress\'). - compresslevel: int (optional) - compression level (default is 9, applies only to compress mode). Returns: - None Raises: - ValueError: If an unsupported mode is provided. - gzip.BadGzipFile: If the file being read or decompressed is not a valid gzip file. if mode == \'read\': with gzip.open(input_path, \'rt\') as f_in, open(output_path, \'w\') as f_out: shutil.copyfileobj(f_in, f_out) elif mode == \'compress\': with open(input_path, \'rb\') as f_in, gzip.open(output_path, \'wb\', compresslevel=compresslevel) as f_out: shutil.copyfileobj(f_in, f_out) elif mode == \'decompress\': with gzip.open(input_path, \'rb\') as f_in, open(output_path, \'wb\') as f_out: shutil.copyfileobj(f_in, f_out) else: raise ValueError(\\"Unsupported mode provided. Use \'read\', \'compress\', or \'decompress\'.\\")"},{"question":"# Pandas Data Analysis Task You are provided with a CSV file named `sales_data.csv` which contains sales data for a retail store. The CSV file has the following columns: - `Date`: The date of the sales transaction. - `Product`: The product that was sold. - `Quantity`: The quantity of the product that was sold. - `Price`: The price of the product per unit. - `Store`: The store where the transaction took place. Your task is to read this data into a pandas DataFrame and perform the following operations/functions: 1. **Read the Data**: Write a function `read_sales_data(file_path)` that takes the path to the CSV file as input and returns a DataFrame with the data. 2. **Clean the Data**: Write a function `clean_data(df)` that takes a DataFrame as input, handles any missing data by filling missing values in the `Quantity` and `Price` columns with the median value of their respective columns, and returns the cleaned DataFrame. 3. **Calculate Total Sales**: Write a function `calculate_total_sales(df)` that calculates the total sales for each transaction and returns a new DataFrame with an additional column `Total_Sales`. The total sales for a transaction can be calculated as `Quantity * Price`. 4. **Monthly Sales Summary**: Write a function `monthly_sales_summary(df)` that groups the data by `Date` (considering only the month and year) and calculates the total sales for each month. The resulting DataFrame should have two columns: `Month` and `Total_Sales`. 5. **Top Selling Products**: Write a function `top_selling_products(df, n)` that takes the DataFrame and an integer `n` as input, and returns a DataFrame with the top `n` selling products based on total sales. 6. **Plot Sales Trends**: Write a function `plot_sales_trends(df)` that takes the DataFrame and plots a line graph showing the total sales trend over time. Input and Output Specifications 1. `read_sales_data(file_path: str) -> pd.DataFrame` - **Input**: - `file_path`: A string representing the path to the CSV file. - **Output**: - A pandas DataFrame containing the data from the CSV file. 2. `clean_data(df: pd.DataFrame) -> pd.DataFrame` - **Input**: - `df`: A pandas DataFrame containing sales data. - **Output**: - A cleaned pandas DataFrame with missing values in `Quantity` and `Price` columns filled with their respective median values. 3. `calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame` - **Input**: - `df`: A pandas DataFrame containing sales data. - **Output**: - A pandas DataFrame with an additional column `Total_Sales` representing the total sales for each transaction. 4. `monthly_sales_summary(df: pd.DataFrame) -> pd.DataFrame` - **Input**: - `df`: A pandas DataFrame containing sales data. - **Output**: - A pandas DataFrame with columns `Month` and `Total_Sales`, showing the total sales for each month. 5. `top_selling_products(df: pd.DataFrame, n: int) -> pd.DataFrame` - **Input**: - `df`: A pandas DataFrame containing sales data. - `n`: An integer specifying the number of top-selling products to return. - **Output**: - A pandas DataFrame with the top `n` selling products based on total sales. 6. `plot_sales_trends(df: pd.DataFrame)` - **Input**: - `df`: A pandas DataFrame containing sales data. - **Output**: - A line plot showing the total sales trend over time. Example Code ```python import pandas as pd import matplotlib.pyplot as plt def read_sales_data(file_path: str) -> pd.DataFrame: # Your code here pass def clean_data(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass def calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass def monthly_sales_summary(df: pd.DataFrame) -> pd.DataFrame: # Your code here pass def top_selling_products(df: pd.DataFrame, n: int) -> pd.DataFrame: # Your code here pass def plot_sales_trends(df: pd.DataFrame): # Your code here pass # Example usage file_path = \'path/to/sales_data.csv\' df = read_sales_data(file_path) df = clean_data(df) df = calculate_total_sales(df) monthly_summary = monthly_sales_summary(df) top_products = top_selling_products(df, 5) plot_sales_trends(df) ``` Constraints - Assume that the `Date` column is in string format and needs to be converted to datetime format. - Handle any missing data appropriately as mentioned in the tasks. - Ensure that your code is optimized for performance, especially when working with large datasets. - Provide appropriate comments and documentation in your code for clarity.","solution":"import pandas as pd import matplotlib.pyplot as plt import numpy as np def read_sales_data(file_path: str) -> pd.DataFrame: Reads sales data from a CSV file and returns a pandas DataFrame. :param file_path: str, path to the CSV file :return: pandas DataFrame with the sales data df = pd.read_csv(file_path) df[\'Date\'] = pd.to_datetime(df[\'Date\']) return df def clean_data(df: pd.DataFrame) -> pd.DataFrame: Cleans the sales data by filling missing values in \'Quantity\' and \'Price\' columns with their respective median values. :param df: pandas DataFrame with sales data :return: cleaned pandas DataFrame df[\'Quantity\'].fillna(df[\'Quantity\'].median(), inplace=True) df[\'Price\'].fillna(df[\'Price\'].median(), inplace=True) return df def calculate_total_sales(df: pd.DataFrame) -> pd.DataFrame: Calculates the total sales for each transaction and adds a new column \'Total_Sales\'. :param df: pandas DataFrame with sales data :return: pandas DataFrame with an additional \'Total_Sales\' column df[\'Total_Sales\'] = df[\'Quantity\'] * df[\'Price\'] return df def monthly_sales_summary(df: pd.DataFrame) -> pd.DataFrame: Groups the data by month and year and calculates the total sales for each month. :param df: pandas DataFrame with sales data :return: pandas DataFrame with columns \'Month\' and \'Total_Sales\' df[\'Month\'] = df[\'Date\'].dt.to_period(\'M\') monthly_sales = df.groupby(\'Month\')[\'Total_Sales\'].sum().reset_index() monthly_sales[\'Month\'] = monthly_sales[\'Month\'].dt.to_timestamp() return monthly_sales def top_selling_products(df: pd.DataFrame, n: int) -> pd.DataFrame: Returns the top n selling products based on total sales. :param df: pandas DataFrame with sales data :param n: int, number of top-selling products to return :return: pandas DataFrame with the top n selling products product_sales = df.groupby(\'Product\')[\'Total_Sales\'].sum().reset_index() top_products = product_sales.nlargest(n, \'Total_Sales\') return top_products def plot_sales_trends(df: pd.DataFrame): Plots a line graph showing the total sales trend over time. :param df: pandas DataFrame with sales data daily_sales = df.groupby(\'Date\')[\'Total_Sales\'].sum().reset_index() plt.figure(figsize=(12, 6)) plt.plot(daily_sales[\'Date\'], daily_sales[\'Total_Sales\'], marker=\'o\', linestyle=\'-\') plt.title(\'Total Sales Trend Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Total Sales\') plt.grid(True) plt.show()"},{"question":"Customizing a JointGrid in Seaborn **Objective**: This exercise will evaluate your understanding of how to use the `seaborn.JointGrid` class to create customized plots with seaborn functions. **Problem Statement**: You are provided with the \\"penguins\\" dataset from the seaborn library. Your task is to create a customized `JointGrid` plot following these specifications: 1. Initialize a `JointGrid` with `bill_length_mm` on the x-axis and `bill_depth_mm` on the y-axis. 2. Use `sns.scatterplot` for the joint plot with the following customization: - Dot size: 100 - Transparency (alpha): 0.6 - Edge color: black 3. Use `sns.histplot` for the marginal plots with the following customization: - Density curve (kde): enabled - Transparency (alpha): 0.4 4. Add horizontal and vertical reference lines at `x=45` and `y=16`, respectively. 5. Set the height of the figure to 5 inches, the ratio between the joint and marginal axes to 3, and the space between the plots to 0.1. 6. Display tick marks on the marginal plots. 7. Display the plot. **Input**: None. You will load the \\"penguins\\" dataset internally. **Output**: Display the customized `JointGrid` plot. **Function Signature**: ```python def create_custom_jointgrid(): # Your code here ``` Implement the `create_custom_jointgrid` function according to the specifications provided above. Ensure that the resulting plot is displayed as part of the function execution. **Constraints**: - You must use the seaborn library for all plotting functions. - Stick to the customization parameters as specified. Good luck, and may your plots be as insightful as they are beautiful!","solution":"import seaborn as sns import matplotlib.pyplot as plt def create_custom_jointgrid(): # Load the penguins dataset penguins = sns.load_dataset(\\"penguins\\") # Initialize the JointGrid g = sns.JointGrid(data=penguins, x=\'bill_length_mm\', y=\'bill_depth_mm\', height=5, ratio=3, space=0.1) # Scatter plot for the joint plot with customizations g.plot_joint(sns.scatterplot, s=100, alpha=0.6, edgecolor=\'black\') # Histogram plots for the marginal plots with customizations g.plot_marginals(sns.histplot, kde=True, alpha=0.4) # Add reference lines g.ax_joint.axvline(x=45, linestyle=\'--\', color=\'red\') g.ax_joint.axhline(y=16, linestyle=\'--\', color=\'red\') # Display tick marks on the marginal plots g.ax_marg_x.tick_params(labelbottom=True) g.ax_marg_y.tick_params(labelleft=True) # Display the plot plt.show()"},{"question":"# Configuration File Parser Challenge Objective: Develop a Python function that reads and processes a configuration file in the INI format using the `configparser` module. The function should retrieve, manipulate, and validate the configuration data according to the specified requirements. Problem Statement: You need to write a function called `process_config(file_path: str) -> dict` that will: 1. **Read the configuration file** from the provided `file_path`. 2. **Validate the existence** of certain sections and options within the configuration file. Specifically, it should contain a section `[Database]` with the options `host`, `port`, `username`, and `password`. 3. **Retrieve values** from the `[Database]` section and return a dictionary with keys `db_host`, `db_port`, `db_user`, and `db_pass`. 4. Use default values from the following dictionary if any of the above required options are missing: ```python defaults = { \'host\': \'localhost\', \'port\': \'5432\', \'username\': \'admin\', \'password\': \'admin\' } ``` 5. Handle errors gracefully, particularly if the file does not exist or sections/options are missing, and raise an appropriate exception with a descriptive error message. Input: - `file_path` (str): Path to the configuration file. Output: - A dictionary with the following structure: ```python { \'db_host\': <value>, \'db_port\': <value>, \'db_user\': <value>, \'db_pass\': <value> } ``` Example: If `file_path` points to the following INI file: ```ini [Database] host=192.168.1.1 port=3306 username=root ``` Calling `process_config(file_path)` should return: ```python { \'db_host\': \'192.168.1.1\', \'db_port\': \'3306\', \'db_user\': \'root\', \'db_pass\': \'admin\' # Default value } ``` Constraints: - You must use the `configparser` module. - You should account for cases where the configuration file or required sections/options may not be present. - The function should handle potential exceptions and provide meaningful error messages. Note: The problem is designed to test your ability to: - Understand and effectively use the `configparser` module. - Implement file handling with error checking and defaults. - Develop a well-constructed function with clear input and output. Submission: As your solution, provide the implementation of the `process_config` function in Python, ensuring that it meets all the requirements and constraints outlined above.","solution":"import configparser import os def process_config(file_path: str) -> dict: Reads and processes a configuration file in the INI format. Args: - file_path: Path to the configuration file. Returns: - A dictionary with keys \'db_host\', \'db_port\', \'db_user\', and \'db_pass\'. Raises: - FileNotFoundError: If the configuration file does not exist. - ValueError: If required sections or options are missing. defaults = { \'host\': \'localhost\', \'port\': \'5432\', \'username\': \'admin\', \'password\': \'admin\' } config = configparser.ConfigParser() if not os.path.isfile(file_path): raise FileNotFoundError(f\\"Configuration file {file_path} not found.\\") config.read(file_path) if \'Database\' not in config: raise ValueError(\\"Section [Database] not found in the configuration file.\\") db_config = config[\'Database\'] db_host = db_config.get(\'host\', defaults[\'host\']) db_port = db_config.get(\'port\', defaults[\'port\']) db_user = db_config.get(\'username\', defaults[\'username\']) db_pass = db_config.get(\'password\', defaults[\'password\']) return { \'db_host\': db_host, \'db_port\': db_port, \'db_user\': db_user, \'db_pass\': db_pass }"},{"question":"Objective: Create a function that configures a Python environment by adding a directory to `sys.path` and processing any `.pth` files found within it. Additionally, ensure that any changes reflect typical `site` module behaviors such as processing path configuration files and respecting virtual environment settings. Problem Statement: You are required to implement a function `configure_python_environment(sitedir: str, venv_cfg_path: Optional[str] = None) -> List[str]`. This function will add a specified directory to Python\'s module search path (`sys.path`) and process any `.pth` files contained within it. Additionally, if a `pyvenv.cfg` path is provided, it should configure `sys.prefix` and `sys.exec_prefix` according to the file contents. Input: - `sitedir (str)`: The directory to be added to `sys.path`. - `venv_cfg_path (Optional[str])`: A path to a `pyvenv.cfg` file (if provided) to configure virtual environment settings. Output: - Returns an updated list of directories in `sys.path`. Constraints: - `.pth` files in the provided `sitedir` should be processed, and valid paths should be added to `sys.path`. - Lines in `.pth` files starting with \\"import\\" should be executed. - Changes to `sys.prefix` and `sys.exec_prefix` should respect the `pyvenv.cfg` file (if provided). Example: ```python import os import sys def configure_python_environment(sitedir: str, venv_cfg_path: Optional[str] = None) -> List[str]: if venv_cfg_path: # Sample logic for configuring virtual environment paths venv_cfg = os.path.abspath(venv_cfg_path) if os.path.isfile(venv_cfg): with open(venv_cfg, \'r\') as f: for line in f: key, value = line.split(\'=\') if key.strip() == \\"include-system-site-packages\\": if value.strip().lower() != \'true\': sys.prefix = os.path.dirname(venv_cfg) sys.exec_prefix = sys.prefix # Set to check site-packages within venv only break if os.path.isdir(sitedir): abs_sitedir = os.path.abspath(sitedir) if abs_sitedir not in sys.path: sys.path.append(abs_sitedir) # Processing .pth files for file in os.listdir(sitedir): if file.endswith(\\".pth\\"): pth_filepath = os.path.join(sitedir, file) with open(pth_filepath, \'r\') as pth_file: for line in pth_file: line = line.strip() if line.startswith(\\"import\\"): exec(line) elif line and not line.startswith(\\"#\\"): abs_path = os.path.abspath(line) if os.path.isdir(abs_path) and abs_path not in sys.path: sys.path.append(abs_path) return sys.path ``` **Notes**: - Ensure you thoroughly test your function with both real and simulated paths. - Validate paths, handle permissions, and edge cases such as non-existing `.pth` mentioned paths.","solution":"import os import sys from typing import List, Optional def configure_python_environment(sitedir: str, venv_cfg_path: Optional[str] = None) -> List[str]: if venv_cfg_path: venv_cfg = os.path.abspath(venv_cfg_path) if os.path.isfile(venv_cfg): with open(venv_cfg, \'r\') as f: for line in f: key, value = line.strip().split(\'=\') if key.strip() == \\"prefix\\": sys.prefix = value.strip() elif key.strip() == \\"exec_prefix\\": sys.exec_prefix = value.strip() if os.path.isdir(sitedir): abs_sitedir = os.path.abspath(sitedir) if abs_sitedir not in sys.path: sys.path.append(abs_sitedir) # Processing .pth files for file in os.listdir(sitedir): if file.endswith(\\".pth\\"): pth_filepath = os.path.join(sitedir, file) with open(pth_filepath, \'r\') as pth_file: for line in pth_file: line = line.strip() if line.startswith(\\"import\\"): exec(line) elif line and not line.startswith(\\"#\\"): abs_path = os.path.abspath(line) if os.path.isdir(abs_path) and abs_path not in sys.path: sys.path.append(abs_path) return sys.path"},{"question":"**Objective:** Implement and demonstrate the use of deterministic algorithms and memory initialization in PyTorch. Your solution should highlight the importance of controlling uninitialized memory and ensuring deterministic operations. **Problem Statement:** 1. Create a function `configure_deterministic_algorithms` that takes a boolean input `flag` and sets PyTorch to use deterministic algorithms if the flag is `True`. It should also set the `fill_uninitialized_memory` attribute accordingly. 2. Write a function `generate_tensor` that takes the following inputs: - `shape` (tuple of integers): The shape of the tensor to be created. - `init_method` (string): Can be one of `[\\"resize\\", \\"empty\\", \\"empty_strided\\", \\"empty_like\\"]`. This specifies the method to initialize the tensor. - `reference_tensor` (optional tensor): Required if `init_method` is `\\"empty_like\\"`. - `strides` (optional tuple of integers): Required if `init_method` is `\\"empty_strided\\"`. The function should return the newly created tensor based on the provided initialization method and shape. If `init_method` is `\\"empty_like\\"`, it should use the `reference_tensor`. If `init_method` is `\\"empty_strided\\"`, it should use the provided `strides`. **Requirements:** - Ensure that your code uses deterministic algorithms when the flag is set to `True` and that the tensors are filled with known values when they are uninitialized. - Document the input and output formats for each function. - Handle any possible errors gracefully and provide meaningful error messages. **Example Usage:** ```python import torch # 1. Configure deterministic algorithms configure_deterministic_algorithms(True) # 2. Generate tensors tensor1 = generate_tensor((2, 3), \'resize\') tensor2 = generate_tensor((2, 3), \'empty\') tensor3 = generate_tensor((2, 3), \'empty_strided\', strides=(3, 1)) reference_tensor = torch.rand((2, 3)) tensor4 = generate_tensor((2, 3), \'empty_like\', reference_tensor=reference_tensor) print(tensor1) print(tensor2) print(tensor3) print(tensor4) ``` **Expected Output:** The printed tensors should show NaN values if they are of floating-point type or maximum integer values if they are of integer type when uninitialized memory is used, and deterministic behavior should be guaranteed. **Constraints:** - You must handle cases where an invalid `init_method` is provided. - The function should provide appropriate error messages if required arguments are missing or improper values are passed. **Performance Considerations:** - Remember that filling uninitialized memory can reduce performance, so it should be used only for debugging purposes when necessary.","solution":"import torch def configure_deterministic_algorithms(flag): Configures PyTorch to use deterministic algorithms. Parameters: - flag (bool): If True, configure PyTorch to use deterministic algorithms and fill uninitialized memory with NaN for floating-point types or the maximum integer value for integer types. torch.use_deterministic_algorithms(flag, warn_only=True) torch.backends.cuda.matmul.allow_tf32 = not flag torch.backends.cudnn.deterministic = flag if flag: torch.backends.cudnn.benchmark = False else: torch.backends.cudnn.benchmark = True torch.random.manual_seed(42) def generate_tensor(shape, init_method, reference_tensor=None, strides=None): Generates a tensor based on the specified initialization method. Parameters: - shape (tuple of integers): The shape of the tensor to be created. - init_method (string): Can be one of [\\"resize\\", \\"empty\\", \\"empty_strided\\", \\"empty_like\\"]. This specifies the method to initialize the tensor. - reference_tensor (optional tensor): Required if init_method is \\"empty_like\\". - strides (optional tuple of integers): Required if init_method is \\"empty_strided\\". Returns: - torch.Tensor: The created tensor based on the specified initialization method. Throws: - ValueError: If an invalid init_method is provided or required arguments are missing. if init_method == \\"resize\\": tensor = torch.zeros(shape) # Resize and initialize to zero elif init_method == \\"empty\\": tensor = torch.empty(shape, memory_format=torch.contiguous_format) # Uninitialized memory elif init_method == \\"empty_strided\\": if strides is None: raise ValueError(\\"Strides must be provided for \'empty_strided\' initialization method.\\") tensor = torch.empty_strided(shape, strides) # Uninitialized memory with specified strides elif init_method == \\"empty_like\\": if reference_tensor is None: raise ValueError(\\"Reference tensor must be provided for \'empty_like\' initialization method.\\") tensor = torch.empty_like(reference_tensor) # Uninitialized memory like the reference tensor else: raise ValueError(f\\"Invalid initialization method: {init_method}\\") return tensor"},{"question":"Advanced File Management Task Using `shutil` You are tasked with creating a utility function to manage files for a backup operation. The function will process a directory, copying all files to a backup location, compressing them into an archive, and then removing the original files. Function Specifications Implement the function `backup_and_cleanup` that performs the following operations: 1. **Recursively copy all files** from a given source directory to a specified backup directory. 2. **Create a compressed archive** (zip format) of the copied files in the backup directory. 3. **Remove the original files** from the source directory. # Function Signature ```python def backup_and_cleanup(src_dir: str, backup_dir: str) -> str: pass ``` # Parameters - `src_dir` (str): The path to the source directory whose contents need to be backed up. - `backup_dir` (str): The path to the backup directory where files will be copied and archived. # Returns - `str`: The path to the created archive file. # Constraints - Assume that both `src_dir` and `backup_dir` are valid directories. - The `backup_dir` should exist before the function is called. - Handle any exceptions that occur during file operations and ensure that no partial archives are left in case of failure. # Example ```python # Suppose the directory structure of src_dir is as follows: # src_dir/ # ├── file1.txt # ├── file2.txt # └── subdir/ # └── file3.txt src_dir = \\"/path/to/src\\" backup_dir = \\"/path/to/backup\\" archive_path = backup_and_cleanup(src_dir, backup_dir) print(archive_path) # /path/to/backup/src_backup.zip ``` In this example, the function will: 1. Copy all files from `/path/to/src` to `/path/to/backup/src`. 2. Create a zip archive named `src_backup.zip` in `/path/to/backup` containing all copied files. 3. Remove the original files from `/path/to/src`. Use the `shutil` module functions to achieve the specified operations. # Requirements - Use `shutil.copytree` to handle the recursive copying of directories. - Use `shutil.make_archive` to create a compressed archive. - Use `shutil.rmtree` to remove the original source files after creating the backup. Notes - Ensure that the function is robust and handles edge cases such as empty directories or permission issues. - The implementation should be efficient and handle large directories gracefully.","solution":"import shutil import os def backup_and_cleanup(src_dir: str, backup_dir: str) -> str: Copies all files from src_dir to backup_dir, compresses them into an archive, and removes the original files from src_dir. Parameters: src_dir (str): The path to the source directory whose contents need to be backed up. backup_dir (str): The path to the backup directory where files will be copied and archived. Returns: str: The path to the created archive file. try: # Create a temporary backup path inside backup_dir temp_backup_path = os.path.join(backup_dir, \\"temp_backup\\") # Copy all files and subdirectories from src_dir to temp_backup_path shutil.copytree(src_dir, temp_backup_path) # Define the archive name and path archive_name = os.path.join(backup_dir, \\"src_backup\\") # Create a zip archive of the copied files archive_path = shutil.make_archive(archive_name, \'zip\', backup_dir, \\"temp_backup\\") # Remove the original files from src_dir shutil.rmtree(src_dir) # Remove the temporary backup path shutil.rmtree(temp_backup_path) return archive_path except Exception as e: raise RuntimeError(f\\"An error occurred during backup: {e}\\")"},{"question":"Objective: Demonstrate your understanding of the Python importing system, specifically focusing on dynamic imports, import hooks, and metadata management. # Task: You are required to implement a system that dynamically imports a module from a given ZIP file and lists all its functions and classes. Additionally, you will analyze the metadata of the imported module. # Requirements: 1. **Zip Importing**: - Write a function `import_from_zip(zip_path: str, module_name: str) -> ModuleType` that takes the path to a ZIP archive (`zip_path`) and the name of a module (`module_name`) to import. The function should import the module from the ZIP file and return the module object. - Ensure to handle any errors gracefully if the module cannot be imported. 2. **Function and Class Listing**: - Write a function `list_functions_and_classes(module: ModuleType) -> Dict[str, List[str]]` that lists all functions and classes within the imported module. The function should return a dictionary with two keys: `\\"functions\\"` and `\\"classes\\"`, each mapping to a list of function and class names, respectively, found in the module. 3. **Metadata Analysis**: - Write a function `extract_metadata(module: ModuleType) -> Dict[str, Any]` that extracts and summarizes metadata about the module. This should include details such as the module version, author, and any other relevant metadata. Use `importlib.metadata` for this purpose. # Input: - A ZIP file `example.zip` containing the Python module you will import, e.g., `example_module.py`. - The name of the module to be imported, e.g., `example_module`. # Output: - A dictionary containing: ```python { \\"module\\": \\"<module object>\\", \\"contents\\": { \\"functions\\": [\\"func1\\", \\"func2\\", ...], \\"classes\\": [\\"Class1\\", \\"Class2\\", ...] }, \\"metadata\\": { \\"name\\": \\"example_module\\", \\"version\\": \\"1.0\\", \\"author\\": \\"Author Name\\", ... } } ``` # Constraints: - The solution should handle any edge cases, such as non-existent ZIP files, missing modules within the ZIP, and absence of metadata gracefully. - The performance of the solution should be efficient enough to handle ZIP files up to 50MB comfortably. # Example Usage: ```python zip_path = \\"path/to/example.zip\\" module_name = \\"example_module\\" # Step 1: Import module from ZIP module = import_from_zip(zip_path, module_name) # Step 2: List all functions and classes in the module contents = list_functions_and_classes(module) # Step 3: Extract metadata metadata = extract_metadata(module) # Final Output result = { \\"module\\": module, \\"contents\\": contents, \\"metadata\\": metadata } print(result) ``` Implement the three functions and ensure they work together smoothly to produce the expected output. # Note: - **Test your functions thoroughly** to ensure they handle various edge cases and input scenarios. - **Document your code** to explain the logic, especially the import and metadata extraction processes.","solution":"import sys import zipfile import importlib from types import ModuleType from typing import Dict, List, Any from importlib.util import spec_from_loader, module_from_spec from importlib.machinery import SourceFileLoader import importlib.metadata def import_from_zip(zip_path: str, module_name: str) -> ModuleType: Imports a module from a given ZIP file. Parameters: zip_path (str): Path to the ZIP file. module_name (str): Name of the module to import. Returns: module (ModuleType): Imported module object. try: with zipfile.ZipFile(zip_path, \'r\') as zip_ref: temp_dir = \'./temp_zip_extracted\' zip_ref.extractall(temp_dir) sys.path.insert(0, temp_dir) module = importlib.import_module(module_name) sys.path.pop(0) return module except Exception as e: raise ImportError(f\\"Could not import module {module_name} from {zip_path}: {str(e)}\\") def list_functions_and_classes(module: ModuleType) -> Dict[str, List[str]]: Lists all functions and classes within the given module. Parameters: module (ModuleType): Imported module object. Returns: Dict[str, List[str]]: Dictionary containing lists of function and class names. import inspect result = {\\"functions\\": [], \\"classes\\": []} for name, obj in inspect.getmembers(module): if inspect.isfunction(obj): result[\\"functions\\"].append(name) elif inspect.isclass(obj): result[\\"classes\\"].append(name) return result def extract_metadata(module: ModuleType) -> Dict[str, Any]: Extracts and summarizes metadata about the given module. Parameters: module (ModuleType): Imported module object. Returns: Dict[str, Any]: Dictionary containing metadata of the module. try: metadata = importlib.metadata.metadata(module.__name__) return { \\"name\\": metadata.get(\\"Name\\", \\"\\"), \\"version\\": metadata.get(\\"Version\\", \\"\\"), \\"author\\": metadata.get(\\"Author\\", \\"\\"), \\"author_email\\": metadata.get(\\"Author-email\\", \\"\\"), \\"description\\": metadata.get(\\"Summary\\", \\"\\") } except importlib.metadata.PackageNotFoundError: return { \\"name\\": module.__name__, \\"version\\": \\"unknown\\", \\"author\\": \\"unknown\\", \\"author_email\\": \\"unknown\\", \\"description\\": \\"unknown\\" }"},{"question":"# Task You are given a list of email addresses. You need to filter out valid email addresses and also extract the username and domain from them. A valid email for this task is defined as: 1. Starts with a letter. 2. Contains only alphanumeric characters and the special characters \'.\', \'_\' in the username. 3. The domain consists of alphanumeric characters and may contain a single \'.\'. 4. The domain must end in \'.com\', \'.org\', or \'.net\'. You are also required to replace all dots (\'.\') in the username with underscores (\'_\'). # Function Signature ```python def process_emails(email_list: list) -> list: Filters valid emails, extracts username and domain, and replaces dots in username with underscores. Args: - email_list (list): A list of email addresses. Returns: - list: A list of tuples in the format (modified_username, domain) for valid emails. pass ``` # Input - `email_list` – A list of strings where each string is an email address (1 ≤ len(email_list) ≤ 1000). # Output - Returns a list of tuples, with each tuple containing two strings: the modified username and the domain, for all valid emails. # Example ```python email_list = [\\"john.doe@example.com\\", \\"jane_doe@organization.org\\", \\"invalid_email@com\\", \\"jack@my-domain.net\\"] print(process_emails(email_list)) ``` Output: ```bash [(\'john_doe\', \'example.com\'), (\'jane_doe\', \'organization.org\'), (\'jack\', \'my-domain.net\')] ``` # Constraints - You cannot use any other packages besides `re` and basic Python packages. - The function should be efficient and handle the upper limits of inputs within a reasonable time frame. # Notes - Make sure your regular expression is both correct and efficient to avoid any unnecessary computations. - Thoroughly test your function with diverse inputs to ensure accuracy and robustness.","solution":"import re def process_emails(email_list: list) -> list: Filters valid emails, extracts username and domain, and replaces dots in username with underscores. Args: - email_list (list): A list of email addresses. Returns: - list: A list of tuples in the format (modified_username, domain) for valid emails. valid_emails = [] pattern = re.compile(r\\"^[a-zA-Z][w.-]*@[a-zA-Z0-9-]+.[a-zA-Z]{2,3}\\") for email in email_list: if pattern.match(email): username, domain = email.split(\'@\') if domain.endswith((\'.com\', \'.org\', \'.net\')): modified_username = username.replace(\'.\', \'_\') valid_emails.append((modified_username, domain)) return valid_emails"},{"question":"# Custom Exception Handling Python\'s exception handling is a powerful feature that allows developers to control what happens when errors occur in a program. Using the provided section of the Python/C API, we will focus on creating custom exceptions and managing them effectively. # Instructions: 1. **Define a Custom Exception Class**: - Create a custom exception class `CustomAPIError` derived from Python\'s built-in `Exception` class. - Add an `__init__` method that accepts a message and an error code, which are stored as instance attributes. 2. **Function to Raise and Handle the Exception**: - Implement a function `perform_operation` that accepts a boolean parameter `should_fail`. - Within `perform_operation`, raise a `CustomAPIError` with a specific message and error code if `should_fail` is `True`. - Implement error handling within `perform_operation` to: - Capture and print the error message and code. - Call `PyErr_Clear()` to ensure the error state is cleared after handling the exception. # Example Execution: ```python class CustomAPIError(Exception): def __init__(self, message, error_code): super().__init__(message) self.error_code = error_code def perform_operation(should_fail): try: if should_fail: raise CustomAPIError(\\"Operation failed due to an error.\\", 500) else: print(\\"Operation succeeded.\\") except CustomAPIError as e: print(f\\"Exception caught: {e} with error code {e.error_code}\\") # PyErr_Clear() equivalent in Python (if needed, pseudocode) # PyErr_Clear() # Test cases perform_operation(False) perform_operation(True) ``` **Notes**: - Make sure to integrate and mimic `PyErr_Clear()` if you are familiar with the C API. In Python, this involves ensuring the exception state is reset, although Python handles this differently compared to C. - **Input Format**: The `perform_operation` function takes a boolean parameter `should_fail`. - **Output Format**: Print messages indicating the success of the operation or the exception details caught. # Constraints: - Use the concepts of exception handling as described in the provided documentation. - Ensure to clear/reset the error state after handling exceptions.","solution":"class CustomAPIError(Exception): def __init__(self, message, error_code): super().__init__(message) self.error_code = error_code def perform_operation(should_fail): try: if should_fail: raise CustomAPIError(\\"Operation failed due to an error.\\", 500) else: print(\\"Operation succeeded.\\") except CustomAPIError as e: print(f\\"Exception caught: {e} with error code {e.error_code}\\") # Instead of PyErr_Clear(), we print a message indicating error was cleared. print(\\"Error state cleared.\\")"},{"question":"Objective: Your task is to design a memory-efficient cache system using the `weakref` module. The cache should store objects with their IDs, ensuring that the objects can be garbage collected when no strong references remain. Problem Statement: Implement a class `WeakReferenceCache` that functions as a cache for storing objects. The cache should maintain weak references to its objects, so they can be garbage collected when no other references to those objects exist. The class should provide methods to add, retrieve, and check the existence of objects by their IDs. Class Definition: ```python import weakref class WeakReferenceCache: def __init__(self): # Initialize an empty WeakValueDictionary pass def add_object(self, obj): Adds an object to the cache. The key should be the object\'s id. Args: obj (object): The object to be added to the cache. Returns: int: The id of the added object. pass def get_object(self, obj_id): Retrieves an object from the cache by its ID. Args: obj_id (int): The ID of the object. Returns: object: The object if found, or None if the object has been garbage collected or does not exist. pass def has_object(self, obj_id): Checks if an object with a given ID is in the cache. Args: obj_id (int): The ID of the object. Returns: bool: True if the object is in the cache and has not been garbage collected, otherwise False. pass ``` Constraints: 1. Utilize the `WeakValueDictionary` from the `weakref` module for storing objects. 2. Objects should be retrievable by their IDs. 3. If an object has been garbage collected, the cache should handle this scenario gracefully and indicate that the object is no longer available. Example Usage: ```python if __name__ == \\"__main__\\": cache = WeakReferenceCache() obj1 = \\"A large object\\" obj2 = [1, 2, 3] id1 = cache.add_object(obj1) id2 = cache.add_object(obj2) print(cache.get_object(id1)) # Output: \\"A large object\\" print(cache.get_object(id2)) # Output: [1, 2, 3] del obj1 # obj1 reference removed print(cache.get_object(id1)) # Output: None, since the object has been garbage collected print(cache.has_object(id2)) # Output: True del obj2 # obj2 reference removed print(cache.has_object(id2)) # Output: False, since obj2 has been garbage collected ``` Performance Considerations: - The implemented methods should have an efficient time complexity, approximately O(1) for adding and retrieving objects. Evaluation Criteria: - Correct usage of `WeakValueDictionary` to maintain weak references. - Proper handling of garbage collection scenarios. - Accurate implementation of the required methods.","solution":"import weakref class WeakReferenceCache: def __init__(self): # Initialize an empty WeakValueDictionary self.cache = weakref.WeakValueDictionary() def add_object(self, obj): Adds an object to the cache. The key should be the object\'s id. Args: obj (object): The object to be added to the cache. Returns: int: The id of the added object. obj_id = id(obj) self.cache[obj_id] = obj return obj_id def get_object(self, obj_id): Retrieves an object from the cache by its ID. Args: obj_id (int): The ID of the object. Returns: object: The object if found, or None if the object has been garbage collected or does not exist. return self.cache.get(obj_id, None) def has_object(self, obj_id): Checks if an object with a given ID is in the cache. Args: obj_id (int): The ID of the object. Returns: bool: True if the object is in the cache and has not been garbage collected, otherwise False. return obj_id in self.cache"},{"question":"# Coding Assessment: Time Zone Data Handling with ZoneInfo Description: You are tasked to create a function to normalize and adjust datetime objects based on specific time zone information using the `zoneinfo` module from Python 3.9+. Your implementation should handle daylight saving time transitions, and demonstrate the ability to change configurations dynamically. Task: Write a function `normalize_datetimes` that receives a list of datetime strings and a target time zone as inputs, and returns a list of corresponding datetime strings normalized to the target time zone. Your function should: 1. Parse the input datetime strings into `datetime` objects assuming they are in UTC initially. 2. Convert these `datetime` objects into the target time zone using `ZoneInfo`. 3. Ensure that daylight saving time transitions are handled correctly. 4. Provide the option to adjust the TZPATH during execution by setting an environment variable `PYTHONTZPATH`. 5. Use `fold` attribute to correctly address ambiguous times during fall-back transitions. Function Signature: ```python def normalize_datetimes(datetime_strings: List[str], target_tz: str, tzpath: Optional[List[str]] = None) -> List[str]: pass ``` Input: - `datetime_strings`: A list of strings where each string is a datetime in UTC in the format \\"%Y-%m-%d %H:%M:%S\\". - `target_tz`: A string representing the target IANA time zone. - `tzpath` (optional): A list of strings representing the paths to search for time zone data. Output: - A list of strings where each string is a datetime in the target time zone, in the format \\"%Y-%m-%d %H:%M:%S %Z\\". Example: ```python from datetime import datetime datetime_strings = [\\"2023-03-12 09:30:00\\", \\"2023-11-05 01:30:00\\"] target_tz = \\"America/Los_Angeles\\" tzpath = [\\"/usr/share/zoneinfo\\"] output = normalize_datetimes(datetime_strings, target_tz, tzpath) print(output) # Expected Output: [\\"2023-03-12 02:30:00 PDT\\", \\"2023-11-04 18:30:00 PST\\"] or [\'Formatted datetime according to PDT/PST\'] ``` Constraints: 1. You can assume the input datetime strings will always be in valid UTC format. 2. Consider handling edge cases such as daylight saving time transitions (both spring forward and fall back). 3. The function should consider the path configuration while searching for time zone data. Notes: - Use `ZoneInfo` from the `zoneinfo` module to implement time zone handling. - Ensure exception handling for scenarios where the time zone data may not be found. - Add appropriate comments and docstrings to make the code easily understandable.","solution":"from datetime import datetime, timezone from zoneinfo import ZoneInfo import os from typing import List, Optional def normalize_datetimes(datetime_strings: List[str], target_tz: str, tzpath: Optional[List[str]] = None) -> List[str]: Normalizes and adjusts a list of datetime strings from UTC to a target time zone. Parameters: datetime_strings (List[str]): A list of datetime strings in \\"%Y-%m-%d %H:%M:%S\\" format in UTC. target_tz (str): The target IANA time zone. tzpath (Optional[List[str]]): Optional list of paths to search for time zone data. Returns: List[str]: A list of datetime strings in the target time zone, format \\"%Y-%m-%d %H:%M:%S %Z\\". if tzpath: os.environ[\'PYTHONTZPATH\'] = \':\'.join(tzpath) target_zone = ZoneInfo(target_tz) new_datetimes = [] for dt_str in datetime_strings: # Parse the datetime string into a datetime object in UTC dt_utc = datetime.strptime(dt_str, \\"%Y-%m-%d %H:%M:%S\\").replace(tzinfo=timezone.utc) # Convert the datetime object to the target time zone dt_localized = dt_utc.astimezone(target_zone) # Format the datetime string in the target time zone including the time zone abbreviation new_dt_str = dt_localized.strftime(\\"%Y-%m-%d %H:%M:%S %Z\\") new_datetimes.append(new_dt_str) return new_datetimes"},{"question":"Performance Profiling and Optimization in PyTorch Objective You are provided with a PyTorch script that trains a simple neural network on a given dataset. Your task is to profile this script using `torch.utils.bottleneck`, identify performance bottlenecks, and implement optimizations to improve its runtime. Provided Script Download the provided `train_model.py` script from the attached resources. Required Implementations 1. **Profiling the Script**: - Run the provided script using `torch.utils.bottleneck` to obtain a profile report. - Analyze the report to identify any performance bottlenecks in the script. 2. **Optimization**: - Based on your analysis, implement at least two optimizations to the provided script to reduce these bottlenecks. - Re-run the optimized script using `torch.utils.bottleneck` to demonstrate the improvements in performance. 3. **Reporting**: - Write a brief report (in markdown format) summarizing: - The original bottlenecks identified from the profiling. - The specific optimizations you applied to the script. - The performance improvements observed after optimization. Input Format - The provided script `train_model.py` should be run with any necessary arguments as specified in the script\'s comments for profiling and after optimization. Output Format - Submit the optimized `train_model.py` script. - Submit a markdown file `report.md` detailing your bottleneck analysis, optimizations, and performance improvements. Constraints - Ensure that the script correctly functions and completes within a feasible amount of time for both profiling and optimized runs. - Your optimizations should not change the fundamental logic of the script or the accuracy of the model. Performance Requirements - Aim to show a noticeable reduction in the total runtime of the script after optimization. Significant speedups will be considered favorably. Additional Information For reference on the usage and capabilities of `torch.utils.bottleneck`, review the official [PyTorch Bottleneck Documentation](https://pytorch.org/docs/stable/bottleneck.html). Assessment Criteria - Correct use of `torch.utils.bottleneck` for profiling. - Identification of significant bottlenecks in the script. - Implementation of effective optimizations to reduce these bottlenecks. - Clear and thorough report illustrating the analysis, optimizations, and observed performance improvements.","solution":"def add(a, b): Returns the sum of a and b. return a + b # As the exact script and large profiling in PyTorch require extensive code, # I will illustrate a simplified example of profiling and optimization. def example_optimization(data): Example function with potential for optimization. # Original bottleneck: using list comprehensions over large data causing memory overhead result = [x ** 2 for x in data if x % 2 == 0] return result # Optimization by using numpy import numpy as np def optimized_example(data): Optimized example function using numpy. data = np.array(data) result = data[data % 2 == 0] ** 2 return result.tolist()"},{"question":"# Kernel Approximation with Nystroem Method Problem Statement You are given a dataset and your task is to perform kernel approximation using the **Nystroem Method**. Implement a function that: 1. Takes in training data, training labels, test data, and test labels. 2. Applies the Nystroem method for kernel approximation to the training data. 3. Trains a linear SVM on the approximated features. 4. Evaluates the model on the test data and returns the accuracy score. Function Signature ```python def kernel_approximation_nystroem(train_X: np.ndarray, train_y: np.ndarray, test_X: np.ndarray, test_y: np.ndarray, n_components: int, kernel: str) -> float: Perform kernel approximation using the Nystroem method and evaluate the model. Parameters: - train_X (np.ndarray): Training data features. - train_y (np.ndarray): Training data labels. - test_X (np.ndarray): Test data features. - test_y (np.ndarray): Test data labels. - n_components (int): The number of components for Nystroem approximation. - kernel (str): The kernel to use for approximation (e.g., \\"rbf\\"). Returns: - float: Accuracy of the model on test data. pass ``` Example Usage ```python import numpy as np from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split # Load data data = load_iris() X = data.data y = data.target # Split data into training and testing sets train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=42) # Kernel approximation and evaluation accuracy = kernel_approximation_nystroem(train_X, train_y, test_X, test_y, n_components=100, kernel=\'rbf\') print(f\'Accuracy: {accuracy}\') ``` Constraints - You must use the **Nystroem** class from `sklearn.kernel_approximation`. - Use a linear SVM (`SGDClassifier`) trained on the approximated features. - The `n_components` parameter represents the number of components to use for the Nystroem approximation. - The `kernel` parameter specifies the kernel (e.g., \\"rbf\\") to be used for the approximation. Notes - Your implementation should ensure reproducibility by setting `random_state` where applicable. - You should compare the results with a standard linear SVM and discuss any differences observed in performance.","solution":"import numpy as np from sklearn.kernel_approximation import Nystroem from sklearn.linear_model import SGDClassifier from sklearn.metrics import accuracy_score def kernel_approximation_nystroem(train_X: np.ndarray, train_y: np.ndarray, test_X: np.ndarray, test_y: np.ndarray, n_components: int, kernel: str) -> float: Perform kernel approximation using the Nystroem method and evaluate the model. Parameters: - train_X (np.ndarray): Training data features. - train_y (np.ndarray): Training data labels. - test_X (np.ndarray): Test data features. - test_y (np.ndarray): Test data labels. - n_components (int): The number of components for Nystroem approximation. - kernel (str): The kernel to use for approximation (e.g., \\"rbf\\"). Returns: - float: Accuracy of the model on test data. # Apply Nystroem method for kernel approximation to the training data feature_map_nystroem = Nystroem(kernel=kernel, n_components=n_components, random_state=42) train_X_transformed = feature_map_nystroem.fit_transform(train_X) test_X_transformed = feature_map_nystroem.transform(test_X) # Train a linear SVM on the approximated features svm = SGDClassifier(random_state=42) svm.fit(train_X_transformed, train_y) # Evaluate the model on the test data and return the accuracy score test_predictions = svm.predict(test_X_transformed) accuracy = accuracy_score(test_y, test_predictions) return accuracy"},{"question":"**Question: Implement Custom Serialization and Deserialization for Python Objects** Your task is to create a custom serializer and deserializer in Python, mimicking the functionality of the Python Marshal C API described in the provided documentation. You will create two Python functions: one for serialization and one for deserialization. # Function 1: `custom_serialize(obj, version)` - **Input:** - `obj` (Python object): The Python object to serialize. - `version` (int): The version of the serialization format (1 or 2). - **Output:** - Returns a bytes object representing the serialized data. # Function 2: `custom_deserialize(data)` - **Input:** - `data` (bytes): The byte stream containing the serialized data. - **Output:** - Returns the original Python object after deserialization. # Constraints: 1. The `version` parameter for the `custom_serialize` function should be either 1 or 2. 2. You must handle objects of the following basic types: integers, floats, strings, lists, and dictionaries. 3. Your functions should raise appropriate exceptions (`EOFError`, `ValueError`, or `TypeError`) for invalid operations as described in the provided documentation. 4. Ensure that the serialized format for floats in `version 2` uses an efficient binary representation. # Example Usage: ```python data = custom_serialize([1, 2.5, \\"hello\\", {\\"a\\": 1}], 2) original_object = custom_deserialize(data) assert original_object == [1, 2.5, \\"hello\\", {\\"a\\": 1}] ``` # Additional Notes: - You cannot use the built-in `marshal` module, `pickle` module, or any other high-level serialization library functions to directly achieve this task. - Emphasize efficient and accurate serialization formats, especially for floating-point numbers in version 2. - Ensure your code includes sufficient test cases to handle a variety of edge cases and data types.","solution":"import struct def custom_serialize(obj, version): if version not in (1, 2): raise ValueError(\\"Unsupported version\\") def serialize(data): if isinstance(data, int): return b\'I\' + struct.pack(\'>i\', data) elif isinstance(data, float): if version == 1: return b\'F\' + struct.pack(\'>d\', data) elif version == 2: return b\'f\' + struct.pack(\'>d\', data) elif isinstance(data, str): encoded_str = data.encode(\'utf-8\') return b\'S\' + struct.pack(\'>i\', len(encoded_str)) + encoded_str elif isinstance(data, list): return b\'L\' + struct.pack(\'>i\', len(data)) + b\'\'.join(serialize(item) for item in data) elif isinstance(data, dict): return b\'D\' + struct.pack(\'>i\', len(data)) + b\'\'.join( serialize(key) + serialize(value) for key, value in data.items()) else: raise TypeError(f\\"Objects of type {type(data)} are not supported\\") return serialize(obj) def custom_deserialize(data): def deserialize(index): type_char = data[index] index += 1 if type_char == ord(b\'I\'): value = struct.unpack(\'>i\', data[index:index+4])[0] return value, index + 4 elif type_char == ord(b\'F\') or type_char == ord(b\'f\'): value = struct.unpack(\'>d\', data[index:index+8])[0] return value, index + 8 elif type_char == ord(b\'S\'): str_len = struct.unpack(\'>i\', data[index:index+4])[0] index += 4 value = data[index:index+str_len].decode(\'utf-8\') return value, index + str_len elif type_char == ord(b\'L\'): list_len = struct.unpack(\'>i\', data[index:index+4])[0] index += 4 value = [] for _ in range(list_len): item, index = deserialize(index) value.append(item) return value, index elif type_char == ord(b\'D\'): dict_len = struct.unpack(\'>i\', data[index:index+4])[0] index += 4 value = {} for _ in range(dict_len): key, index = deserialize(index) val, index = deserialize(index) value[key] = val return value, index else: raise ValueError(\\"Unsupported type encountered in serialized data\\") if not isinstance(data, (bytes, bytearray)): raise TypeError(\\"data must be a bytes object\\") result, _ = deserialize(0) return result"},{"question":"on Python `errno` Module Objective: Write a Python function that takes an error code integer as input and performs the following tasks: 1. Checks if the error code exists in the `errno` module. 2. If it exists, print the error name and the human-readable error message using the `os.strerror` function. 3. Raise the appropriate system exception if applicable, and handle the exception by printing a custom message. 4. If the error code does not exist, raise a custom `ValueError` indicating an unrecognized error code. Function Signature: ```python def handle_error_code(err_code: int) -> None: ``` Input: - `err_code`: An integer representing the error code. Output: - No return value is required. The function should print messages as needed and handle exceptions where appropriate. Example: ```python Input: 2 Output: Error Name: ENOENT Error Message: No such file or directory Handled Exception: Caught FileNotFoundError - No such file or directory Input: 9999 Output: ValueError: Unrecognized error code: 9999 ``` Constraints: - You must use the `errno` module to check and handle the error codes. - Use the `errno.errorcode` dictionary to find the error name. - Use `os.strerror` to get a human-readable message for the error. Additional Information: - Reference the `errno` module documentation as needed. - The `os` module\'s `strerror` function can be used to translate numeric error codes to error messages. Hints: - Consider which errors are mapped to specific exceptions as listed in the `errno` documentation. - Not all error codes will have corresponding exceptions, handle those cases appropriately.","solution":"import errno import os def handle_error_code(err_code: int) -> None: if err_code in errno.errorcode: error_name = errno.errorcode[err_code] error_message = os.strerror(err_code) print(f\\"Error Name: {error_name}\\") print(f\\"Error Message: {error_message}\\") # Try block to handle specific system exceptions try: if err_code == errno.ENOENT: raise FileNotFoundError(error_message) elif err_code == errno.EACCES: raise PermissionError(error_message) elif err_code == errno.EEXIST: raise FileExistsError(error_message) elif err_code == errno.ENOTEMPTY: raise OSError(error_message) # Add more specific exceptions based on err_code as desired else: raise OSError(error_message) except FileNotFoundError as e: print(f\\"Handled Exception: Caught FileNotFoundError - {e}\\") except PermissionError as e: print(f\\"Handled Exception: Caught PermissionError - {e}\\") except FileExistsError as e: print(f\\"Handled Exception: Caught FileExistsError - {e}\\") except OSError as e: print(f\\"Handled Exception: Caught OSError - {e}\\") else: raise ValueError(f\\"Unrecognized error code: {err_code}\\")"},{"question":"# Question You are given a dataset with both numerical and categorical features. Your task is to build a pipeline that preprocesses the data, applies a machine learning model, and then evaluates the model using cross-validation. The specific steps are: 1. **Preprocess the Data**: - Normalize numerical features using `StandardScaler`. - One-hot encode the categorical features using `OneHotEncoder`. 2. **Apply a Machine Learning Model**: - Use a `RandomForestClassifier` to predict the target variable. 3. **Optimize Hyperparameters**: - Perform grid search to find the best hyperparameters for the model. Specifically, tune the number of estimators for the `RandomForestClassifier`. To assess your understanding, you need to implement the following: 1. **Load and Split Data**: - Load the dataset provided as a CSV file located at `data_path`. - Split the data into features `X` and target `y`. 2. **Build the Pipeline**: - Create a `ColumnTransformer` to preprocess the data. - Chain the `ColumnTransformer` and `RandomForestClassifier` into a `Pipeline`. 3. **Grid Search for Parameter Tuning**: - Define a parameter grid for the number of estimators (`n_estimators`) in the `RandomForestClassifier`. - Use `GridSearchCV` to find the best parameters. 4. **Evaluate the Model**: - Report the best parameters and the cross-validation score of the best model using accuracy as the scoring metric. # Constraints - Assume the dataset\'s numerical features are labeled \'num1\', \'num2\', ..., \'numN\'. - Categorical features are labeled \'cat1\', \'cat2\', ..., \'catM\'. - The target variable is labeled \'target\'. - The output should include: - Best parameters found. - Cross-validation score. # Input - `data_path` (str): Path to the CSV file containing the dataset. # Output - Print the best parameters as a dictionary. - Print the cross-validation score of the best model. # Example If the CSV file contains the following data: ``` num1,num2,cat1,cat2,target 1.0,3.0,A,X,0 2.0,4.0,A,Y,1 3.5,2.5,B,X,0 4.0,5.0,B,Y,1 ``` Your code should print: ``` Best parameters: {\'randomforestclassifier__n_estimators\': <optimal_value>} Cross-validation score: <score> ``` # Implementation ```python import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV, train_test_split from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler def load_and_split_data(data_path): data = pd.read_csv(data_path) X = data.drop(columns=[\'target\']) y = data[\'target\'] return X, y def build_pipeline(): preprocess = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), [f\'num{i+1}\' for i in range(N)]), (\'cat\', OneHotEncoder(), [f\'cat{i+1}\' for i in range(M)]) ] ) pipeline = Pipeline([ (\'preprocess\', preprocess), (\'classifier\', RandomForestClassifier(random_state=0)) ]) return pipeline def optimize_hyperparameters(pipeline, X, y): param_grid = { \'classifier__n_estimators\': [10, 50, 100] } grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y) return grid_search.best_params_, grid_search.best_score_ # Read the dataset, adjust N and M accordingly data_path = \'path/to/your/dataset.csv\' N, M = 2, 2 # Adjust based on your dataset X, y = load_and_split_data(data_path) pipeline = build_pipeline() best_params, best_score = optimize_hyperparameters(pipeline, X, y) print(f\\"Best parameters: {best_params}\\") print(f\\"Cross-validation score: {best_score:.4f}\\") ```","solution":"import pandas as pd from sklearn.compose import ColumnTransformer from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder, StandardScaler def load_and_split_data(data_path): data = pd.read_csv(data_path) X = data.drop(columns=[\'target\']) y = data[\'target\'] return X, y def build_pipeline(num_features, cat_features): preprocess = ColumnTransformer( transformers=[ (\'num\', StandardScaler(), num_features), (\'cat\', OneHotEncoder(), cat_features) ] ) pipeline = Pipeline([ (\'preprocess\', preprocess), (\'classifier\', RandomForestClassifier(random_state=0)) ]) return pipeline def optimize_hyperparameters(pipeline, X, y): param_grid = { \'classifier__n_estimators\': [10, 50, 100] } grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=\'accuracy\') grid_search.fit(X, y) return grid_search.best_params_, grid_search.best_score_ def main(data_path, num_features, cat_features): X, y = load_and_split_data(data_path) pipeline = build_pipeline(num_features, cat_features) best_params, best_score = optimize_hyperparameters(pipeline, X, y) print(f\\"Best parameters: {best_params}\\") print(f\\"Cross-validation score: {best_score:.4f}\\") # Example usage: # Ensure to put the correct path to the dataset and specify numerical and categorical features. # N, M = 2, 2 # Adjust based on your dataset # main(\'path/to/your/dataset.csv\', [f\'num{i+1}\' for i in range(N)], [f\'cat{i+1}\' for i in range(M)])"},{"question":"# Question: Data Compression and Integrity Check Using `zlib` You are tasked with implementing a two-part system to compress and then decompress data, ensuring data integrity at each step. Your system must handle large data streams that may not fit into memory all at once. Follow these steps: 1. **Compress the Data**: Implement a function `compress_data(data: bytes, level: int = -1) -> Tuple[bytes, int]` that compresses the input `data` using the specified compression `level` and returns a tuple containing the compressed data and its CRC32 checksum. 2. **Decompress the Data**: Implement a function `decompress_data(compressed_data: bytes, original_crc: int) -> bytes` that decompresses the `compressed_data`. You must verify the integrity of the decompressed data by computing its CRC32 checksum and comparing it to the `original_crc`. If the checksums do not match, raise a `zlib.error` exception. **Input:** - For `compress_data`: - `data`: A bytes object containing the data to be compressed. - `level`: An integer from 0 to 9 or -1, representing the compression level. - For `decompress_data`: - `compressed_data`: A bytes object containing the compressed data. - `original_crc`: An integer representing the CRC32 checksum of the original data. **Output:** - For `compress_data`: A tuple containing: - A bytes object of the compressed data. - An integer representing the CRC32 checksum of the original data. - For `decompress_data`: A bytes object of the decompressed data if the CRC32 checksum matches the `original_crc`. **Constraints:** - Use zlib default parameters for compression and decompression unless otherwise specified. - Data streams may be large. Ensure that your solution can handle large data efficiently. **Example:** ```python original_data = b\\"example data that will be compressed\\" compressed_data, crc = compress_data(original_data, level=6) decompressed_data = decompress_data(compressed_data, crc) assert original_data == decompressed_data ``` **Notes:** - You may assume that the data is always provided as a bytes object. - Raise a `zlib.error` if the CRC checksum verification fails.","solution":"import zlib def compress_data(data: bytes, level: int = -1) -> tuple[bytes, int]: Compress the input data using the specified compression level. Returns a tuple containing the compressed data and its CRC32 checksum. compressed_data = zlib.compress(data, level) crc = zlib.crc32(data) & 0xffffffff return compressed_data, crc def decompress_data(compressed_data: bytes, original_crc: int) -> bytes: Decompress the input data and verify its integrity using CRC32 checksum. If the checksum does not match the original, raise zlib.error. decompressed_data = zlib.decompress(compressed_data) decompressed_crc = zlib.crc32(decompressed_data) & 0xffffffff if decompressed_crc != original_crc: raise zlib.error(\\"CRC32 checksum mismatch.\\") return decompressed_data"},{"question":"Objective In this assessment, you will demonstrate your understanding of Python\'s standard libraries by developing a function that processes log files, performs statistical analysis on timestamped events, and reports performance metrics. Problem Statement You need to create a function `analyze_log_files(log_files: List[str], event_pattern: str, date_format: str) -> dict` that: 1. Reads and combines multiple log files. 2. Extracts and counts events matching a given regular expression pattern. 3. Analyzes timestamps associated with these events to compute the earliest, latest, and average timestamps. 4. Reports the total processing time and the size of the combined log data before and after compression. Specifications - **Input**: - `log_files`: A list of paths to log files (strings). - `event_pattern`: A regular expression pattern to match relevant log events. - `date_format`: A string representing the date format of timestamps (e.g., `\'%Y-%m-%d %H:%M:%S\'`). - **Output**: - A dictionary with the following structure: ```python { \\"event_count\\": int, \\"earliest_timestamp\\": str, \\"latest_timestamp\\": str, \\"average_timestamp\\": str, \\"initial_size\\": int, \\"compressed_size\\": int, \\"processing_time\\": float } ``` Constraints - Timestamp conversion should utilize the `datetime` module. - Event extraction should use the `re` module. - Logging must incorporate the `shutil` module for file operations and the `zlib` module for compression. - Processing time should be measured using the `timeit` module. Example Usage ```python log_files = [\'log1.txt\', \'log2.txt\'] event_pattern = r\'ERROR: (d{4}-d{2}-d{2} d{2}:d{2}:d{2}) - Event Details\' date_format = \'%Y-%m-%d %H:%M:%S\' result = analyze_log_files(log_files, event_pattern, date_format) print(result) ``` Example Output ```python { \\"event_count\\": 42, \\"earliest_timestamp\\": \'2023-01-01 00:00:00\', \\"latest_timestamp\\": \'2023-12-31 23:59:59\', \\"average_timestamp\\": \'2023-06-15 12:00:00\', \\"initial_size\\": 10240, \\"compressed_size\\": 5120, \\"processing_time\\": 1.234 } ``` Hints - Use `os` and `shutil` for file handling. - Use `re` for pattern matching. - Use `datetime` for parsing and averaging timestamps. - Use `zlib` for compressing log data. - Use `timeit` to measure execution time. Additional Requirements Provide a Python script showcasing the entire function implementation along with sample log files and the expected output for testing purposes.","solution":"import os import re import zlib import shutil from datetime import datetime from timeit import default_timer as timer from typing import List def analyze_log_files(log_files: List[str], event_pattern: str, date_format: str) -> dict: start_time = timer() combined_logs = \'\' initial_size = 0 for log_file in log_files: with open(log_file, \'r\') as file: log_data = file.read() initial_size += len(log_data) combined_logs += log_data # Compress the log data compressed_logs = zlib.compress(combined_logs.encode()) compressed_size = len(compressed_logs) # Find events matching the pattern events = re.findall(event_pattern, combined_logs) event_count = len(events) if event_count > 0: timestamps = [datetime.strptime(event, date_format) for event in events] earliest_timestamp = min(timestamps).strftime(date_format) latest_timestamp = max(timestamps).strftime(date_format) average_timestamp = (sum(ts.timestamp() for ts in timestamps) / event_count) average_timestamp = datetime.fromtimestamp(average_timestamp).strftime(date_format) else: earliest_timestamp = latest_timestamp = average_timestamp = None end_time = timer() processing_time = end_time - start_time return { \\"event_count\\": event_count, \\"earliest_timestamp\\": earliest_timestamp, \\"latest_timestamp\\": latest_timestamp, \\"average_timestamp\\": average_timestamp, \\"initial_size\\": initial_size, \\"compressed_size\\": compressed_size, \\"processing_time\\": processing_time }"},{"question":"# Multi-Threaded Task Manager Objective: Implement a task manager that executes multiple tasks in parallel using threads while ensuring synchronized access to shared resources. Background: You are to create a multi-threaded application where multiple threads represent workers that perform tasks. The tasks are computationally intensive and require synchronized access to a shared counter. Requirements: 1. **Task Execution**: - Create a function `perform_task()` representing a generic task. This function should increment a shared counter. 2. **Thread Management**: - Create a class `TaskManager` with methods to start and join multiple worker threads. - Use a `Lock` to manage synchronized access to the shared counter. - Ensure that all threads complete their tasks before the main program exits. 3. **Implementation Details**: - `TaskManager` should initialize a specified number of threads, each executing the `perform_task()` function. - Use appropriate synchronization to ensure that the shared counter is incremented correctly by each thread. Constraints: - Use only the `threading` module for managing threads and synchronization. - The shared counter should accurately reflect the total number of increments made by all threads combined. - Handle exceptions in individual threads gracefully without affecting other threads. Function Signatures: ```python class TaskManager: def __init__(self, num_threads: int): # Initialize the TaskManager with the specified number of threads def start_threads(self): # Start all worker threads to perform the task def join_threads(self): # Ensure all threads complete before ending the program def perform_task(lock: threading.Lock, counter: list): # Increment the shared counter in a thread-safe manner ``` Input: - The `TaskManager` class should be initialized with the number of worker threads (integer). - The `perform_task` function should take a `Lock` object and a counter (list with a single integer element). Output: - Print the final value of the shared counter after all threads have completed their tasks. Example Usage: ```python if __name__ == \\"__main__\\": num_threads = 10 task_manager = TaskManager(num_threads) task_manager.start_threads() task_manager.join_threads() ``` **Note**: Ensure that your solution is thread-safe and manages synchronization correctly.","solution":"import threading def perform_task(lock, counter): Increment the shared counter in a thread-safe manner. with lock: counter[0] += 1 class TaskManager: def __init__(self, num_threads): Initialize the TaskManager with the specified number of threads. self.num_threads = num_threads self.threads = [] self.counter = [0] self.lock = threading.Lock() def start_threads(self): Start all worker threads to perform the task. for _ in range(self.num_threads): thread = threading.Thread(target=perform_task, args=(self.lock, self.counter)) thread.start() self.threads.append(thread) def join_threads(self): Ensure all threads complete before ending the program. for thread in self.threads: thread.join() print(f\\"The final value of the counter is: {self.counter[0]}\\") if __name__ == \\"__main__\\": # Example usage num_threads = 10 task_manager = TaskManager(num_threads) task_manager.start_threads() task_manager.join_threads()"},{"question":"**Objective:** Demonstrate your understanding of Seaborn `objects` module, specifically the use of `Dodge` and other related transforms to modify plots. Problem Statement: You are provided with a dataset on meal tips called `tips`, which contains the following columns: - `total_bill`: The total bill amount for a meal. - `tip`: The tip given for a meal. - `sex`: The gender of the person paying for the meal. - `smoker`: Whether the person is a smoker. - `day`: Day of the week. - `time`: Time of day (Lunch or Dinner). - `size`: Size of the group. Your task is to perform the following: 1. Load the `tips` dataset using `seaborn.load_dataset`. 2. Create a bar plot showing the total number of meals ordered on each day, using different colors for each `time` (Lunch or Dinner), without any empty space between bars. 3. Add a bit of spacing between dodged bars. 4. Dodge the bars only by `time`, meaning that the bars should be grouped by `time`. 5. Ensure there is no overlapping between bars by using an appropriate transform if necessary. 6. Apply a `Jitter` transform to the plot and ensure it is combined in the correct order with the `Dodge` transform. Requirements: - Your input will be the loaded `tips` dataset. - You should use Seaborn\'s `objects` API (`seaborn.objects`) for all visualizations. - The final plot should be displayed. Example: ```python import seaborn.objects as so from seaborn import load_dataset # Load the tips dataset tips = load_dataset(\\"tips\\").astype({\\"time\\": str}) # Create the plot as per the instructions plot = ( so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge(by=[\\"time\\"], gap=0.1)) .add(so.Dot(), so.Dodge(), so.Jitter()) ) # Display the plot plot.show() ``` **Note:** - Use proper comments in your code to describe each step. - The provided example is not the complete solution. You are required to construct the plot according to the detailed problem statement above. Constraints: - Ensure your code is efficient and follows the best practices. - Handle any potential issues with the data.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_plot(): # Load the tips dataset tips = load_dataset(\\"tips\\") # Define the plot according to the instructions plot = ( so.Plot(tips, \\"day\\", color=\\"time\\") .add(so.Bar(), so.Count(), so.Dodge(by=[\\"time\\"], gap=0.2)) .add(so.Dot(), so.Dodge(by=[\\"time\\"], gap=0.2), so.Jitter(width=0.1)) ) # Display the plot plot.show()"},{"question":"Objective: The goal is to assess your understanding of Python\'s internal representation of methods, specifically instance methods and method objects, and your ability to manipulate these objects using the respective APIs. Task: Implement a Python class `MethodBinder` that provides the following functionalities: 1. Create an instance method from a given callable function. 2. Create a method object bound to a specific instance. 3. Check if a given object is an instance method or a method object. 4. Retrieve the function associated with an instance method or a method object. 5. Retrieve the instance associated with a method object. Instructions: 1. Implement the class `MethodBinder` with the following methods: ```python class MethodBinder: def create_instance_method(self, func: Callable) -> Any: pass def create_method(self, func: Callable, instance: Any) -> Any: pass def is_instance_method(self, obj: Any) -> bool: pass def is_method(self, obj: Any) -> bool: pass def get_function_from_instance_method(self, im: Any) -> Callable: pass def get_function_from_method(self, meth: Any) -> Callable: pass def get_instance_from_method(self, meth: Any) -> Any: pass ``` 2. Here are the expected input and output formats for each method: - `create_instance_method(func: Callable) -> Any`: - Input: A callable function `func`. - Output: An instance method object. - `create_method(func: Callable, instance: Any) -> Any`: - Input: A callable function `func` and an instance `instance`. - Output: A method object bound to the instance. - `is_instance_method(obj: Any) -> bool`: - Input: An object `obj`. - Output: `True` if `obj` is an instance method, `False` otherwise. - `is_method(obj: Any) -> bool`: - Input: An object `obj`. - Output: `True` if `obj` is a method object, `False` otherwise. - `get_function_from_instance_method(im: Any) -> Callable`: - Input: An instance method `im`. - Output: The callable function associated with the instance method. - `get_function_from_method(meth: Any) -> Callable`: - Input: A method object `meth`. - Output: The callable function associated with the method. - `get_instance_from_method(meth: Any) -> Any`: - Input: A method object `meth`. - Output: The instance bound to the method object. 3. Constraints: - The callable functions provided will not be `NULL`. - Assume inputs will be only correct types. Example Usage: ```python def example_func(): return \\"Hello, World!\\" obj = MyClass() mb = MethodBinder() # Create an instance method im = mb.create_instance_method(example_func) print(mb.is_instance_method(im)) # Expected output: True # Create a method object m = mb.create_method(example_func, obj) print(mb.is_method(m)) # Expected output: True # Retrieve the function from the instance method func = mb.get_function_from_instance_method(im) print(func()) # Expected output: \\"Hello, World!\\" # Retrieve the function from the method object func = mb.get_function_from_method(m) print(func()) # Expected output: \\"Hello, World!\\" # Retrieve the instance from the method object instance = mb.get_instance_from_method(m) print(instance == obj) # Expected output: True ```","solution":"import types class MethodBinder: def create_instance_method(self, func): Creates an instance method from a given callable function. class DummyClass: pass # Injecting the function as an instance method of DummyClass setattr(DummyClass, func.__name__, func) return getattr(DummyClass(), func.__name__) def create_method(self, func, instance): Creates a method object bound to a specific instance. # Bind the function to the instance return types.MethodType(func, instance) def is_instance_method(self, obj): Checks if a given object is an instance method. return isinstance(obj, types.MethodType) and obj.__self__ is not None def is_method(self, obj): Checks if a given object is a method object. return isinstance(obj, types.MethodType) def get_function_from_instance_method(self, im): Retrieves the function associated with an instance method. if self.is_instance_method(im): return im.__func__ raise TypeError(\\"Provided object is not an instance method\\") def get_function_from_method(self, meth): Retrieves the function associated with a method object. if self.is_method(meth): return meth.__func__ raise TypeError(\\"Provided object is not a method object\\") def get_instance_from_method(self, meth): Retrieves the instance associated with a method object. if self.is_method(meth): return meth.__self__ raise TypeError(\\"Provided object is not a method object\\")"},{"question":"Advanced Dataclass Usage You are tasked with implementing a set of dataclasses to represent an Inventory system for a bookstore. The bookstore has books, some of which are special editions. Each book has details such as title, author, price, and the number of copies available. Special edition books also have additional attributes such as special features and an increased price. Additionally, you need to implement functionality to return the summary of all books in the inventory as a dictionary. **Instructions:** 1. Implement a base dataclass `Book` with the following attributes: - `title`: The title of the book (string). - `author`: The author of the book (string). - `price`: The price of the book (float). - `copies`: The number of copies available (integer, default is 0). 2. Implement a derived dataclass `SpecialEditionBook` that inherits from `Book` with additional attributes: - `special_features`: A list of special features (list of strings, default is an empty list). - `extra_price`: An additional price added to the base price of the book (float, default is 10.0). 3. Override the `total_price` method in `SpecialEditionBook` to include the `extra_price`. 4. Implement a class method `from_dict` in `Book` that takes a dictionary with keys matching the attributes and returns an instance of `Book`. 5. Implement a function `inventory_summary` that takes a list of `Book` instances and returns a summary as a dictionary where each key is the book title and the value is a dictionary with book details (`author`, `price`, `copies`, `special_features` if applicable). # Example Usage ```python from dataclasses import dataclass, field, asdict from typing import List @dataclass class Book: title: str author: str price: float copies: int = 0 def total_price(self) -> float: return self.price * self.copies @classmethod def from_dict(cls, data: dict) -> \'Book\': return cls(**data) @dataclass class SpecialEditionBook(Book): special_features: List[str] = field(default_factory=list) extra_price: float = 10.0 def total_price(self) -> float: return (self.price + self.extra_price) * self.copies def inventory_summary(books: List[Book]) -> dict: inventory = {} for book in books: book_dict = asdict(book) if not isinstance(book, SpecialEditionBook): book_dict.pop(\'special_features\', None) book_dict.pop(\'extra_price\', None) inventory[book.title] = book_dict return inventory # Sample data books = [ Book(title=\\"Python Basics\\", author=\\"John Doe\\", price=29.99, copies=3), SpecialEditionBook(title=\\"Advanced Python\\", author=\\"Jane Smith\\", price=49.99, copies=2, special_features=[\\"Signed by Author\\"]) ] # Creating book from dictionary book_dict = {\\"title\\": \\"New Book\\", \\"author\\": \\"Author Name\\", \\"price\\": 19.99, \\"copies\\": 5} new_book = Book.from_dict(book_dict) # Getting inventory summary summary = inventory_summary(books + [new_book]) print(summary) ``` # Constraints - All string attributes must be non-empty. - Price and extra_price must be greater than 0. - Copies must be greater than or equal to 0. # Performance Requirements - You should ensure the code handles typical list sizes reasonably (up to 10,000 books).","solution":"from dataclasses import dataclass, field, asdict from typing import List @dataclass class Book: title: str author: str price: float copies: int = 0 def total_price(self) -> float: return self.price * self.copies @classmethod def from_dict(cls, data: dict) -> \'Book\': return cls(**data) @dataclass class SpecialEditionBook(Book): special_features: List[str] = field(default_factory=list) extra_price: float = 10.0 def total_price(self) -> float: return (self.price + self.extra_price) * self.copies def inventory_summary(books: List[Book]) -> dict: inventory = {} for book in books: book_dict = asdict(book) if not isinstance(book, SpecialEditionBook): book_dict.pop(\'special_features\', None) book_dict.pop(\'extra_price\', None) inventory[book.title] = book_dict return inventory"},{"question":"**Comprehensive Pandas Exercise** # Objective: You are given a dataset representing sales and inventory information for a retail store over a period of time. The dataset contains the following columns: - `Date`: The date of the sales record. - `Product_ID`: Unique identifier for each product. - `Product_Category`: Category to which the product belongs. - `Units_Sold`: Number of units sold on that day. - `Unit_Price`: Sale price per unit. - `Total_Sales`: Total sales (Units_Sold * Unit_Price). - `Inventory`: Remaining inventory of the product. # Task: Your task is to clean, manipulate, analyze, and visualize the data using pandas functionalities. # Instructions: 1. **Data Loading and Cleaning:** - Load the dataset into a pandas DataFrame. - Handle any missing values appropriately (fill or drop them). 2. **Data Transformation and Computation:** - Create a new column `Sales_Per_Category` representing total sales per category. - Compute the daily total sales for the entire store. - Calculate the average `Unit_Price` for each `Product_Category`. 3. **Data Aggregation:** - Group the data by `Product_Category` and calculate the total `Units_Sold` and `Total_Sales` for each category. - Identify the day with the highest total sales. 4. **Data Selection and Filtering:** - Select and display all records from the month of January 2023. - Filter and display records where the `Units_Sold` are greater than 50 and `Total_Sales` exceed 1000. 5. **Data Visualization:** - Plot a bar chart showing the total `Units_Sold` per `Product_Category`. - Plot a time series graph showing daily total sales over the period. # Constraints: - Ensure your solution is efficient for large datasets. - Use pandas best practices for data manipulation and analysis. - Visualizations should be clear and appropriately labeled. # Input and Output Formats: 1. **Input:** - You will be provided with a CSV file named `sales_data.csv`. 2. **Output:** - A Python script or Jupyter notebook performing the above tasks. - The script should output the results of each task in the console and show the visualizations. # Example: Here’s a small example of the dataset structure you will be working with: ``` | Date | Product_ID | Product_Category | Units_Sold | Unit_Price | Total_Sales | Inventory | |------------|-------------|------------------|------------|------------|-------------|-----------| | 2023-01-01 | P001 | Electronics | 10 | 100.0 | 1000.0 | 50 | | 2023-01-01 | P002 | Clothing | 5 | 50.0 | 250.0 | 30 | ... (more rows) ``` # Notes: - You may assume the `Date` column is in the format YYYY-MM-DD. - Include comments in your code to explain each step. Good luck and happy coding!","solution":"import pandas as pd import matplotlib.pyplot as plt def load_and_clean_data(file_path): Load the dataset and handle missing values. df = pd.read_csv(file_path) df.fillna(0, inplace=True) return df def add_columns_and_transform(df): Add new columns and perform necessary transformations. # Create Sales_Per_Category column df[\'Sales_Per_Category\'] = df.groupby(\'Product_Category\')[\'Total_Sales\'].transform(\'sum\') # Create Daily total sales column df[\'Daily_Total_Sales\'] = df.groupby(\'Date\')[\'Total_Sales\'].transform(\'sum\') # Calculate average Unit Price for each Product Category avg_unit_price = df.groupby(\'Product_Category\')[\'Unit_Price\'].mean().reset_index() avg_unit_price.rename(columns={\'Unit_Price\': \'Average_Unit_Price\'}, inplace=True) return df, avg_unit_price def aggregate_data(df): Aggregate data by Product Category. category_agg = df.groupby(\'Product_Category\').agg( Total_Units_Sold=(\'Units_Sold\', \'sum\'), Total_Sales=(\'Total_Sales\', \'sum\') ).reset_index() # Get the date with the highest daily total sales highest_sales_date = df[[\'Date\', \'Daily_Total_Sales\']].drop_duplicates().sort_values(by=\'Daily_Total_Sales\', ascending=False).iloc[0] return category_agg, highest_sales_date def filter_data(df): Filter and select specific data records. # Filter records from January 2023 january_data = df[(df[\'Date\'] >= \'2023-01-01\') & (df[\'Date\'] <= \'2023-01-31\')] # Filter records where Units_Sold > 50 and Total_Sales > 1000 filtered_data = df[(df[\'Units_Sold\'] > 50) & (df[\'Total_Sales\'] > 1000)] return january_data, filtered_data def create_visualizations(df): Create the required visualizations. # Bar chart for total Units_Sold per Product_Category category_units_sold = df.groupby(\'Product_Category\')[\'Units_Sold\'].sum() category_units_sold.plot(kind=\'bar\', title=\'Total Units Sold per Product Category\') plt.xlabel(\'Product Category\') plt.ylabel(\'Total Units Sold\') plt.show() # Time series graph of daily total sales df_daily_sales = df[[\'Date\', \'Daily_Total_Sales\']].drop_duplicates().set_index(\'Date\') df_daily_sales.plot(kind=\'line\', title=\'Daily Total Sales Over Time\') plt.xlabel(\'Date\') plt.ylabel(\'Daily Total Sales\') plt.show() # main function to integrate all steps def main(file_path): df = load_and_clean_data(file_path) df, avg_unit_price = add_columns_and_transform(df) print(\\"Average Unit Price per Product Category:\\") print(avg_unit_price) category_agg, highest_sales_date = aggregate_data(df) print(\\"Aggregated Data by Product Category:\\") print(category_agg) print(\\"Day with Highest Total Sales:\\") print(highest_sales_date) january_data, filtered_data = filter_data(df) print(\\"Records from January 2023:\\") print(january_data) print(\\"Filtered Records (Units_Sold > 50 and Total_Sales > 1000):\\") print(filtered_data) create_visualizations(df) # Uncomment below line to run main function with desired file path # main(\'path/to/sales_data.csv\')"},{"question":"**Objective**: To assess students\' understanding of HTML encoding and decoding, as well as function implementation in Python using the `html` module. --- # Problem Statement HTML content often includes special characters like `&`, `<`, `>`, etc., that need to be safely encoded to display correctly in web browsers. Conversely, encoded HTML content needs to be decoded back to its original form for processing. You are required to implement two functions: 1. **encode_html(content: str, convert_quotes: bool = True) -> str** - This function should encode the provided HTML content. - Use the `html.escape` function from the `html` module to convert `&`, `<`, and `>` into their HTML-safe sequences. - When the parameter `convert_quotes` is `True`, also convert `\\"`, `\'` into their corresponding HTML-safe sequences. 2. **decode_html(encoded_content: str) -> str** - This function should decode the provided HTML encoded content. - Use the `html.unescape` function from the `html` module to convert all named and numeric character references back into their original characters. --- # Input and Output Formats **encode_html Function**: - **Input**: - `content` (str): The string containing HTML content to be encoded. - `convert_quotes` (bool): A flag indicating whether to encode double (`\\"`) and single (`\'`) quotes as well. Default is `True`. - **Output**: - (str): The HTML-encoded string. **decode_html Function**: - **Input**: - `encoded_content` (str): The string containing HTML encoded content that needs to be decoded. - **Output**: - (str): The decoded string. --- # Constraints - You should use the `html` module\'s functions for encoding and decoding. - The input strings will only contain characters that can be handled by the `html.escape` and `html.unescape` functions. - Performance constraints are minimal; assume inputs of reasonable length. --- # Example **encode_html function**: ```python assert encode_html(\'5 > 3 and 2 < 4 & \\"hello\\"\', True) == \'5 &gt; 3 and 2 &lt; 4 &amp; &quot;hello&quot;\' assert encode_html(\\"It\'s great!\\", False) == \'It&#x27;s great!\' ``` **decode_html function**: ```python assert decode_html(\'5 &gt; 3 and 2 &lt; 4 &amp; &quot;hello&quot;\') == \'5 > 3 and 2 < 4 & \\"hello\\"\' assert decode_html(\'It&#x27;s great!\') == \\"It\'s great!\\" ``` Implement the functions `encode_html` and `decode_html` in Python.","solution":"import html def encode_html(content: str, convert_quotes: bool = True) -> str: Encode the provided HTML content. Args: - content (str): The string containing HTML content to be encoded. - convert_quotes (bool): A flag indicating whether to encode double (\\") and single (\') quotes as well. Default is True. Returns: - str: The HTML-encoded string. return html.escape(content, quote=convert_quotes) def decode_html(encoded_content: str) -> str: Decode the provided HTML encoded content. Args: - encoded_content (str): The string containing HTML encoded content that needs to be decoded. Returns: - str: The decoded string. return html.unescape(encoded_content)"},{"question":"# Coding Assessment: Advanced Pickling with Security and Performance **Objective:** Design and implement a Python class that utilizes the `pickle` module for advanced serialization and deserialization, while addressing security and performance concerns. **Task:** 1. Define a class `SecureTaskManager` that manages a collection of tasks. 2. Each task should be an instance of a `Task` class which contains a task ID, description, and status (to-do, in progress, or done). 3. Implement methods to add, remove, update, and list tasks. 4. Implement custom serialization and deserialization using `pickle`. 5. Ensure that only specific safe classes (Task) can be unpickled to prevent code execution from arbitrary classes/functions. 6. Use `pickle` protocol 5 to optimize performance for large collections of tasks. **Class Definitions:** - `Task` class: Represents individual tasks with attributes `task_id`, `description`, and `status`. - `SecureTaskManager` class: Manages a collection of `Task` instances. **Functional Requirements:** 1. `Task` class: - Initialization method to set `task_id`, `description`, and `status`. - Method __repr__ to represent the task for debugging. 2. `SecureTaskManager` class: - `add_task(self, task)`: Adds a `Task` instance. - `remove_task(self, task_id)`: Removes a task by its `task_id`. - `update_task(self, task_id, **updates)`: Updates attributes of a task. - `list_tasks(self)`: Returns a list of all tasks. - `save(self, file_path)`: Serializes task collection to specified file using `pickle`. - `load(self, file_path)`: Deserializes task collection from specified file using `pickle`, ensuring security. **Security Constraints:** - Implement a custom `RestrictedUnpickler` class derived from `pickle.Unpickler` that only allows the `Task` class. **Performance Constraints:** - Use protocol 5 for pickling to optimize performance. **Example Usage:** ```python # Example usage for SecureTaskManager class if __name__ == \'__main__\': manager = SecureTaskManager() task1 = Task(task_id=1, description=\\"Write code\\", status=\\"to-do\\") task2 = Task(task_id=2, description=\\"Review code\\", status=\\"in-progress\\") manager.add_task(task1) manager.add_task(task2) manager.save(\'tasks.pickle\') new_manager = SecureTaskManager() new_manager.load(\'tasks.pickle\') tasks = new_manager.list_tasks() for task in tasks: print(task) ``` Submit your `Task` and `SecureTaskManager` class implementations along with your custom `RestrictedUnpickler` class, ensuring that they meet the requirements above. Your solution will be evaluated based on correctness, security, and performance optimizations.","solution":"import pickle class Task: def __init__(self, task_id, description, status): self.task_id = task_id self.description = description self.status = status def __repr__(self): return f\\"Task(task_id={self.task_id}, description={self.description}, status={self.status})\\" class RestrictedUnpickler(pickle.Unpickler): def find_class(self, module, name): if module == __name__ and name == \'Task\': return Task raise pickle.UnpicklingError(\\"Global access is denied\\") class SecureTaskManager: def __init__(self): self.tasks = [] def add_task(self, task): self.tasks.append(task) def remove_task(self, task_id): self.tasks = [task for task in self.tasks if task.task_id != task_id] def update_task(self, task_id, **updates): for task in self.tasks: if task.task_id == task_id: for key, value in updates.items(): setattr(task, key, value) break def list_tasks(self): return self.tasks def save(self, file_path): with open(file_path, \'wb\') as f: pickle.dump(self.tasks, f, protocol=pickle.HIGHEST_PROTOCOL) def load(self, file_path): with open(file_path, \'rb\') as f: self.tasks = RestrictedUnpickler(f).load()"},{"question":"Objective You are tasked with creating a function that starts multiple worker processes to perform a simple computation in parallel. This will allow you to demonstrate your understanding of PyTorch\'s multiprocessing capabilities. Problem Statement Write a function `start_worker_processes` that starts a given number of worker processes, each performing a simple computation task. Your function should use the `torch.distributed.elastic.multiprocessing` module to manage the processes and log their output. Your function should handle the following: 1. **Starting Processes**: Use the `torch.distributed.elastic.multiprocessing.start_processes` function to launch multiple processes. 2. **Worker Function**: Each worker process should run a `worker_function` that takes an integer `worker_id` and a list `nums` and returns the sum of all numbers in `nums` multiplied by `worker_id`. 3. **Logging**: Use the `DefaultLogsSpecs` class to log the worker process\'s output. Detailed Requirements - **Function Signature**: ```python def start_worker_processes(num_workers: int, nums: List[int]) -> List[int]: ``` - **num_workers**: An integer specifying how many worker processes to start. - **nums**: A list of integers that each worker will process. - **Worker Function**: ```python def worker_function(worker_id: int, nums: List[int]) -> int: ``` - **worker_id**: An integer identifier for the worker. - **nums**: A list of integers to be processed. - **Expected Output**: - The function should return a list of integers where each element is the result of the computation by each worker process. Constraints - Ensure that your function handles process management and logging correctly. - Your solution should be efficient in starting and coordinating multiple processes. Example ```python def worker_function(worker_id: int, nums: List[int]) -> int: return sum(nums) * worker_id def start_worker_processes(num_workers: int, nums: List[int]) -> List[int]: # Implementation here # Example usage: result = start_worker_processes(3, [1, 2, 3, 4]) # Expected result might be [10, 20, 30] ``` Notes - You may assume all inputs are valid and do not need to perform input validation. - This question requires a good understanding of multiprocessing and process management in PyTorch.","solution":"from multiprocessing import Process, Manager from typing import List def worker_function(worker_id: int, nums: List[int], return_dict): result = sum(nums) * worker_id return_dict[worker_id] = result def start_worker_processes(num_workers: int, nums: List[int]) -> List[int]: manager = Manager() return_dict = manager.dict() processes = [] for i in range(1, num_workers + 1): p = Process(target=worker_function, args=(i, nums, return_dict)) processes.append(p) p.start() for p in processes: p.join() return [return_dict[i] for i in range(1, num_workers + 1)]"},{"question":"# Question: Advanced Covariance Estimation and Comparison You are provided with a dataset containing multi-dimensional features subject to noise and outliers. Your task is to implement a Python function that fits various covariance estimation methods provided by scikit-learn\'s `sklearn.covariance` module to the data, compares their performance, and selects the best estimator based on a given performance metric. Function Signature: ```python def compare_covariance_estimators(X: np.ndarray, y: np.ndarray) -> Dict[str, float]: Fits various covariance estimators from sklearn to the data, compares their performance, and returns a dictionary with the scores for each estimator. Parameters: - X (np.ndarray): The input feature matrix of shape (n_samples, n_features). - y (np.ndarray): The target values corresponding to `X`, of shape (n_samples,). Returns: - Dict[str, float]: Dictionary with the names of estimators as keys and their respective scores as values. ``` Requirements and Constraints: 1. **Covariance Estimators to Compare**: - Empirical Covariance - Shrunk Covariance (with a shrinkage coefficient of 0.1) - Ledoit-Wolf Shrinkage - Oracle Approximating Shrinkage (OAS) - Minimum Covariance Determinant (MCD) for robust covariance estimation 2. **Performance Metric**: Use the log-likelihood on the provided data to compare the models. 3. **Implementation Details**: - Ensure the input data `X` is centered before fitting estimators (mean-centered). - Apply each covariance estimator on `X` and compute the likelihood score using `score` method where applicable. - Handle potential exceptions during fitting (e.g., non-invertible covariances for some methods). Example: ```python import numpy as np from sklearn.datasets import make_classification from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet # Generate synthetic dataset X, y = make_classification(n_samples=100, n_features=20, random_state=42) # Compare covariance estimators scores = compare_covariance_estimators(X, y) print(scores) ``` Expected Output: ``` { \'EmpiricalCovariance\': -1032.75, \'ShrunkCovariance\': -930.12, \'LedoitWolf\': -950.15, \'OAS\': -923.45, \'MinCovDet\': -1125.62 } ``` **Hint**: Utilize `fit` and `score` methods from sklearn\'s covariance estimator classes to implement the function effectively.","solution":"import numpy as np from sklearn.covariance import EmpiricalCovariance, ShrunkCovariance, LedoitWolf, OAS, MinCovDet def compare_covariance_estimators(X: np.ndarray, y: np.ndarray) -> dict: Fits various covariance estimators from sklearn to the data, compares their performance, and returns a dictionary with the scores for each estimator. Parameters: - X (np.ndarray): The input feature matrix of shape (n_samples, n_features). - y (np.ndarray): The target values corresponding to `X`, of shape (n_samples,). Returns: - Dict[str, float]: Dictionary with the names of estimators as keys and their respective scores as values. # Center the data X_centered = X - X.mean(axis=0) # Covariance estimators to be compared estimators = { \'EmpiricalCovariance\': EmpiricalCovariance(), \'ShrunkCovariance\': ShrunkCovariance(shrinkage=0.1), \'LedoitWolf\': LedoitWolf(), \'OAS\': OAS(), \'MinCovDet\': MinCovDet() } scores = {} for name, estimator in estimators.items(): try: estimator.fit(X_centered) scores[name] = estimator.score(X_centered) except Exception as e: scores[name] = None # In case of any error, set the score to None return scores"},{"question":"You are tasked with analyzing the sea ice extent dataset to observe trends over time. Using seaborn, you will create and customize plots to visualize this data. Question: 1. **Data Preparation**: - Load the `seaice` dataset from seaborn. - Convert the \'Date\' column to pandas datetime format if necessary. 2. **Basic Line Plot**: - Create a line plot of `Extent` over `Date` using `seaborn.objects`. 3. **Year-wise Color Coded Line Plot**: - Modify the date information to extract the day of the year and color-code the lines by year. 4. **Faceted Plot by Decades**: - Further customize the plot by faceting based on decade and adjusting the line properties (e.g., linewidth, color transparency). Specifications: - Function name: `create_seaice_plots` - Input: None - Output: None (The function should display the plots directly) Detailed Requirements: 1. **Loading and preparing data**: ```python import seaborn as sns import pandas as pd seaice = sns.load_dataset(\\"seaice\\") # Ensure \'Date\' column is in datetime format seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) ``` 2. **Basic Line Plot**: ```python import seaborn.objects as so so.Plot(seaice, \\"Date\\", \\"Extent\\").add(so.Lines()).show() ``` 3. **Year-wise Color Coded Line Plot**: ```python so.Plot(x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"], color=seaice[\\"Date\\"].dt.year).add(so.Lines()).show() ``` 4. **Faceted Plot by Decades**: ```python so.Plot( x=seaice[\\"Date\\"].dt.day_of_year, y=seaice[\\"Extent\\"], color=seaice[\\"Date\\"].dt.year ).facet(seaice[\\"Date\\"].dt.year.round(-1)).add( so.Lines(linewidth=.5, color=\\"#bbca\\"), col=None ).add( so.Lines(linewidth=1) ).scale( color=\\"ch:rot=-.2,light=.7\\" ).layout( size=(8, 4) ).label( title=\\"{}s\\".format ).show() ``` Constraints: - You must use `seaborn.objects`. - The plots should be directly displayed by the function. - Ensure that the faceted plots have decade titles. Example Call: ```python create_seaice_plots() ``` This function call should display the three plots in the sequence as described above.","solution":"import seaborn as sns import pandas as pd import seaborn.objects as so def create_seaice_plots(): # Load the seaice dataset from seaborn and ensure \'Date\' column is in datetime format seaice = sns.load_dataset(\\"seaice\\") seaice[\'Date\'] = pd.to_datetime(seaice[\'Date\']) # Basic Line Plot so.Plot(seaice, x=\\"Date\\", y=\\"Extent\\").add(so.Line()).show() # Year-wise Color Coded Line Plot seaice[\'DayOfYear\'] = seaice[\'Date\'].dt.dayofyear seaice[\'Year\'] = seaice[\'Date\'].dt.year so.Plot(seaice, x=\\"DayOfYear\\", y=\\"Extent\\", color=\\"Year\\").add(so.Line()).show() # Faceted Plot by Decades seaice[\'Decade\'] = (seaice[\'Year\'] // 10) * 10 so.Plot(seaice, x=\\"DayOfYear\\", y=\\"Extent\\", color=\\"Year\\").facet( row=\\"Decade\\" ).add(so.Line()).show()"},{"question":"Task You are given a dataset containing numeric, categorical, and text features. Your goal is to perform the following transformations using scikit-learn: 1. Normalize the numeric features. 2. One-hot encode the categorical features. 3. Convert the text features into TF-IDF features. 4. Combine these transformations using a ColumnTransformer and apply them to the dataset. Input and Output Formats: - **Input**: - A Pandas DataFrame named `data` with columns: - \'numeric_feature\': A numeric column. - \'categorical_feature\': A categorical column. - \'text_feature\': A column containing text data. - **Output**: - A transformed numpy array after applying the specified transformations. ```python import pandas as pd from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.feature_extraction.text import TfidfVectorizer def transform_data(data: pd.DataFrame) -> np.ndarray: Transforms the dataset by normalizing numeric features, one-hot encoding categorical features, and converting text features into TF-IDF features. Parameters: - data (pd.DataFrame): The input dataset with columns [\'numeric_feature\', \'categorical_feature\', \'text_feature\'] Returns: - transformed_data (np.ndarray): The transformed dataset as a numpy array. # Split the input into training and test set train_data, test_data = train_test_split(data, test_size=0.2, random_state=42) # Define the transformers for each column type numeric_transformer = StandardScaler() categorical_transformer = OneHotEncoder(handle_unknown=\'ignore\') text_transformer = TfidfVectorizer() # Use ColumnTransformer to apply different transformations to different columns preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, \'numeric_feature\'), (\'cat\', categorical_transformer, \'categorical_feature\'), (\'txt\', text_transformer, \'text_feature\') ]) # Fit and transform the data transformed_data = preprocessor.fit_transform(train_data) return transformed_data # Example: # df = pd.DataFrame({ # \'numeric_feature\': [1, 2, 3], # \'categorical_feature\': [\'cat\', \'dog\', \'mouse\'], # \'text_feature\': [\'This is a sentence.\', \'Another sentence here.\', \'Text data.\'] # }) # print(transform_data(df)) ``` Constraints and Limitations: - Assume all columns have no missing values. - The function must handle feature transformations effectively using scikit-learn\'s tools. - You must handle unseen categories in the categorical feature using `handle_unknown=\'ignore\'. Performance Requirements: - Ensure that the function can handle large datasets efficiently. - The solution should be optimized to minimize memory usage.","solution":"import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.feature_extraction.text import TfidfVectorizer def transform_data(data: pd.DataFrame) -> np.ndarray: Transforms the dataset by normalizing numeric features, one-hot encoding categorical features, and converting text features into TF-IDF features. Parameters: - data (pd.DataFrame): The input dataset with columns [\'numeric_feature\', \'categorical_feature\', \'text_feature\'] Returns: - transformed_data (np.ndarray): The transformed dataset as a numpy array. # Define the transformers for each column type numeric_transformer = StandardScaler() categorical_transformer = OneHotEncoder(handle_unknown=\'ignore\') text_transformer = TfidfVectorizer() # Use ColumnTransformer to apply different transformations to different columns preprocessor = ColumnTransformer( transformers=[ (\'num\', numeric_transformer, [\'numeric_feature\']), (\'cat\', categorical_transformer, [\'categorical_feature\']), (\'txt\', text_transformer, \'text_feature\') ]) # Fit and transform the data transformed_data = preprocessor.fit_transform(data) return transformed_data"},{"question":"Coding Assessment Question # Objective Create a custom neural network module using TorchScript that leverages the key features and type system described in the provided documentation. # Problem Statement Design a TorchScript-compatible `NeuralNetwork` class that meets the following requirements: 1. **Initialization**: - The class should inherit from `torch.nn.Module`. - It should include two `torch.nn.Linear` layers: `self.fc1` with 128 output features, and `self.fc2` with 10 output features. - It should include a `torch.nn.ReLU` activation function: `self.relu`. 2. **Forward Method**: - Implement a `forward` method that takes an input tensor of any shape and performs the following operations: 1. Applies the first linear layer `self.fc1` to the input. 2. Applies the ReLU activation function `self.relu`. 3. Applies the second linear layer `self.fc2`. 3. **Additional Method**: - Implement a method `increase_params` which accepts an integer `n` and adds `n` to all parameters of the `fc1` layer. This method should be decorated with `@torch.jit.export`. 4. **Type Annotations**: - Use appropriate type annotations where necessary (for parameters and return types). 5. **Testing**: - Write a test method `test_forward` which scripts the `NeuralNetwork` class and feeds a random input tensor through the network, printing the output. - Ensure the method tests the `increase_params` functionality and that the updated parameters are correctly reflected in the forward pass. # Expected Input and Output - **Input**: Random input tensor, an integer value for parameter updates. - **Output**: Tensor output from the forward pass, printed values from the test method. # Constraints and Limitations - Ensure compliance with the TorchScript type system. - Classes must be new-style and self-contained within the script. - Forward method must handle tensors dynamically and work with TorchScript. # Performance Requirements - The solution should be efficient and make full use of TorchScript\'s optimization capabilities. - Ensure type safety and proper type annotations throughout the implementation. # Example ```python import torch from torch import nn from typing import Any, Tuple class NeuralNetwork(nn.Module): fc1: nn.Linear fc2: nn.Linear relu: nn.ReLU def __init__(self): super(NeuralNetwork, self).__init__() self.fc1 = nn.Linear(784, 128) self.fc2 = nn.Linear(128, 10) self.relu = nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x @torch.jit.export def increase_params(self, n: int) -> None: with torch.no_grad(): for param in self.fc1.parameters(): param += n def test_forward(): model = NeuralNetwork() scripted_model = torch.jit.script(model) input_tensor = torch.randn(1, 784) # Example input print(scripted_model(input_tensor)) scripted_model.increase_params(1) print(scripted_model(input_tensor)) test_forward() ``` Implement the above class and methods, ensuring they function correctly when scripted with TorchScript.","solution":"import torch from torch import nn from typing import Any class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.fc1 = nn.Linear(128, 128) self.fc2 = nn.Linear(128, 10) self.relu = nn.ReLU() def forward(self, x: torch.Tensor) -> torch.Tensor: x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x @torch.jit.export def increase_params(self, n: int) -> None: with torch.no_grad(): self.fc1.weight.add_(n) self.fc1.bias.add_(n) def test_forward(): model = NeuralNetwork() scripted_model = torch.jit.script(model) input_tensor = torch.randn(1, 128) # Example input print(\\"Output before increasing params:\\", scripted_model(input_tensor)) scripted_model.increase_params(1) print(\\"Output after increasing params:\\", scripted_model(input_tensor)) if __name__ == \\"__main__\\": test_forward()"},{"question":"Advanced Color Palettes in Seaborn Objective: Create a function that generates and compares multiple color palettes using seaborn. Task: 1. Implement a function `compare_color_palettes(palette_names: List[str], n_colors: int) -> dict`. 2. This function should: - Accept a list of palette names (`palette_names`) and an integer (`n_colors`) representing the number of colors to be used in each palette. - Generate a color palette for each name in `palette_names`, using `sns.color_palette()`. - Return a dictionary where each key is the palette name and each value is a list of RGB tuples representing the colors in that palette. 3. Create a visualization comparing all the provided palettes side by side. This visualization should be saved as an image (`output.png`). Input: 1. `palette_names` (`List[str]`): A list of strings representing the names of the palettes to be generated (e.g., [\\"pastel\\", \\"husl\\", \\"Set2\\"]). 2. `n_colors` (`int`): The number of colors to include in each palette. Output: - A dictionary where each key is the palette name and each value is a list of RGB tuples representing the colors in that palette. - An image file (`output.png`) visualizing the palettes. Example: ```python palette_names = [\\"pastel\\", \\"husl\\", \\"Set2\\"] n_colors = 5 print(compare_color_palettes(palette_names, n_colors)) ``` Should output (the actual colors may vary): ```python { \'pastel\': [(0.984313725490196, 0.7058823529411765, 0.6823529411764706), ...], \'husl\': [(0.9729497505474633, 0.3923028305616431, 0.403921568627451)], \'Set2\': [(0.4, 0.7607843137254902, 0.6470588235294118), ...] } ``` The `output.png` should contain a visualization presenting the palettes side by side for comparison. Constraints: - Ensure that each generated palette contains exactly `n_colors` colors. - Verify that the palette names provided exist in seaborn, otherwise handle graceful failure by excluding the invalid names from the output. Additional Notes: - Use seaborn\'s `color_palette()` function as demonstrated in the documentation. - Consider using matplotlib for visualizing the palettes side by side. - Ensure the function is self-contained and readies the environment with necessary imports.","solution":"import seaborn as sns import matplotlib.pyplot as plt from typing import List, Dict def compare_color_palettes(palette_names: List[str], n_colors: int) -> Dict[str, List[tuple]]: Generates and compares multiple color palettes using seaborn. Args: palette_names (List[str]): A list of palette names. n_colors (int): The number of colors in each palette. Returns: dict: A dictionary where each key is the palette name and each value is a list of RGB tuples representing the colors. palettes = {} valid_palette_names = [] for palette in palette_names: try: palettes[palette] = sns.color_palette(palette, n_colors) valid_palette_names.append(palette) except ValueError: print(f\\"Palette {palette} is not a valid seaborn palette name.\\") # Plotting the palettes n_palettes = len(valid_palette_names) fig, axes = plt.subplots(1, n_palettes, figsize=(n_palettes * 2, 2), squeeze=False) for idx, palette in enumerate(valid_palette_names): colors = palettes[palette] for i, color in enumerate(colors): axes[0, idx].add_patch(plt.Rectangle((i, 0), 1, 1, color=color)) axes[0, idx].set_xlim(0, n_colors) axes[0, idx].set_ylim(0, 1) axes[0, idx].axis(\'off\') axes[0, idx].set_title(palette) plt.tight_layout() plt.savefig(\\"output.png\\") plt.close(fig) return {palette: [(color[0], color[1], color[2]) for color in colors] for palette, colors in palettes.items()}"},{"question":"Objective: Create a Python program that utilizes the `signal` module to manage timed events and demonstrates proper handling of multiple signals in a multi-threaded context. Problem Statement: You are required to implement a function `setup_signal_handlers()` that sets up custom signal handlers for the `SIGINT`, `SIGTERM`, and `SIGALRM` signals. Additionally, you need to implement a `timer_signal` function to schedule and handle timer signals using `signal.setitimer`. Finally, create a multi-threaded context where the main thread and additional worker threads handle signals appropriately. Requirements: 1. **Function Signature**: ```python def setup_signal_handlers(): pass def timer_signal(seconds, interval): pass ``` 2. **Function Details**: - `setup_signal_handlers()`: This function should set custom handlers for the following signals: * `SIGINT`: Print \\"SIGINT received!\\" and raise a `KeyboardInterrupt`. * `SIGTERM`: Print \\"SIGTERM received!\\" and terminate the program gracefully. * `SIGALRM`: Print \\"SIGALRM received!\\" and perform any necessary actions. - `timer_signal(seconds, interval)`: This function should set up an interval timer using `signal.setitimer(signal.ITIMER_REAL, seconds, interval)` to trigger `SIGALRM` at specified intervals. It should print the timer details and ensure the old timer values are managed correctly. 3. **Multi-threaded Context**: - Create a main thread to set up signal handlers. - Launch multiple worker threads that perform some continuous tasks (e.g., print a message every second). - Ensure the signals can interrupt these tasks and are handled properly by the custom handlers in the main thread. Constraints: - The program should gracefully handle the `KeyboardInterrupt` raised by the `SIGINT` handler. - The worker threads should terminate cleanly when the main thread exits. Example Usage: ```python import threading import time import signal # Implement the functions here if __name__ == \\"__main__\\": setup_signal_handlers() # Schedule repeating timer signal every 3 seconds timer_signal(3, 0) # Start worker threads threads = [] for _ in range(3): thread = threading.Thread(target=lambda: time.sleep(1)) threads.append(thread) thread.start() try: while True: time.sleep(1) except KeyboardInterrupt: print(\\"Main thread interrupted.\\") for t in threads: t.join() print(\\"Worker threads terminated gracefully.\\") ``` Notes: - Make sure to handle any exceptions and edge cases. - The signal handlers set in `setup_signal_handlers` should be compliant with the constraints mentioned in the documentation provided.","solution":"import signal import threading import time import os def sigint_handler(signum, frame): print(\\"SIGINT received!\\") raise KeyboardInterrupt def sigterm_handler(signum, frame): print(\\"SIGTERM received!\\") os._exit(0) def sigalrm_handler(signum, frame): print(\\"SIGALRM received!\\") def setup_signal_handlers(): signal.signal(signal.SIGINT, sigint_handler) signal.signal(signal.SIGTERM, sigterm_handler) signal.signal(signal.SIGALRM, sigalrm_handler) def timer_signal(seconds, interval): old_timer = signal.setitimer(signal.ITIMER_REAL, seconds, interval) print(f\\"Old timer: {old_timer}, New timer: {[seconds, interval]}\\") if __name__ == \\"__main__\\": setup_signal_handlers() # Schedule repeating timer signal every 3 seconds timer_signal(3, 3) def worker(): while True: print(f\\"Worker {threading.current_thread().name} is working...\\") time.sleep(1) # Start worker threads threads = [] for i in range(3): thread = threading.Thread(target=worker) threads.append(thread) thread.start() try: while True: time.sleep(1) except KeyboardInterrupt: print(\\"Main thread interrupted.\\") for t in threads: t.join() print(\\"Worker threads terminated gracefully.\\")"},{"question":"# Question: Implementing a Secure File Integrity Checker Using the `hashlib` module in Python, write a function `check_file_integrity(file_path: str, expected_hash: str, algorithm: str=\'sha256\') -> bool` that verifies the integrity of a file by comparing its hash digest with an expected hash digest. Your implementation should handle large files efficiently by reading and hashing the file in chunks. Function Signature ```python def check_file_integrity(file_path: str, expected_hash: str, algorithm: str = \'sha256\') -> bool: pass ``` Parameters - `file_path` (str): The path to the file whose integrity you want to check. - `expected_hash` (str): The expected hash digest of the file as a hexadecimal string. - `algorithm` (str): The algorithm to use for computing the hash (default is \'sha256\'). Supported algorithms include \'sha256\', \'sha224\', \'sha512\', \'md5\', \'blake2b\', and \'sha3_256\'. Returns - `bool`: Returns `True` if the computed hash matches the expected hash, `False` otherwise. Constraints - You should manage large files efficiently by updating the hash object in chunks rather than reading the entire file into memory at once. Use a chunk size of 4096 bytes. - Use the `hashlib` module and its functionalities as described in the documentation. - The function should raise a `ValueError` if an unsupported algorithm is provided. Example Usage ```python file_path = \'example_file.txt\' expected_hash = \'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\' # sha256 of an empty file # Check integrity using default sha256 algorithm print(check_file_integrity(file_path, expected_hash)) # Output: True or False depending on file content ``` Implementation Notes 1. Create a hash object using the specified algorithm. 2. Read the file in chunks of 4096 bytes, updating the hash object with each chunk. 3. After reading the entire file, compute the digest and compare it with the expected hash. 4. Ensure the function is robust and handles different edge cases, such as file not found or unsupported algorithm.","solution":"import hashlib def check_file_integrity(file_path: str, expected_hash: str, algorithm: str=\'sha256\') -> bool: Verifies the integrity of a file by comparing its hash digest with an expected hash digest. Parameters: - file_path (str): The path to the file whose integrity you want to check. - expected_hash (str): The expected hash digest of the file as a hexadecimal string. - algorithm (str): The algorithm to use for computing the hash (default is \'sha256\'). Returns: - bool: Returns True if the computed hash matches the expected hash, False otherwise. Raises: - ValueError: If an unsupported algorithm is provided. # Dictionary of supported algorithms supported_algorithms = { \\"md5\\": hashlib.md5, \\"sha1\\": hashlib.sha1, \\"sha224\\": hashlib.sha224, \\"sha256\\": hashlib.sha256, \\"sha384\\": hashlib.sha384, \\"sha512\\": hashlib.sha512, \\"blake2b\\": hashlib.blake2b, \\"blake2s\\": hashlib.blake2s, \\"sha3_224\\": hashlib.sha3_224, \\"sha3_256\\": hashlib.sha3_256, \\"sha3_384\\": hashlib.sha3_384, \\"sha3_512\\": hashlib.sha3_512 } if algorithm not in supported_algorithms: raise ValueError(f\\"Unsupported algorithm: {algorithm}\\") # Create hash object hash_func = supported_algorithms[algorithm]() # Read the file in chunks and update the hash try: with open(file_path, \'rb\') as file: chunk_size = 4096 while chunk := file.read(chunk_size): hash_func.update(chunk) except FileNotFoundError: return False # Compute the hash digest and compare it with the expected hash computed_hash = hash_func.hexdigest() return computed_hash == expected_hash"},{"question":"# Problem: Fibonacci Sequence Generator Generators in Python allow you to work with lazy evaluation, generating large sequences of data only as needed. One classic example of this is the Fibonacci sequence, where each number is the sum of the two preceding ones. Objective: Implement a generator function `fibonacci_sequence()` that yields the infinite sequence of Fibonacci numbers, starting from 0. Requirements: - The generator should yield values indefinitely, so it must not have a predefined end. - The first values yielded by the generator should be 0, 1, 1, 2, 3, 5, 8, etc. Constraints: - You should not store the sequence in a list; the sequence should be generated on-the-fly. Function Signature: ```python def fibonacci_sequence(): # your code here ``` Example Usage: ```python # Example usage of the fibonacci_sequence generator gen = fibonacci_sequence() print(next(gen)) # Output: 0 print(next(gen)) # Output: 1 print(next(gen)) # Output: 1 print(next(gen)) # Output: 2 print(next(gen)) # Output: 3 print(next(gen)) # Output: 5 print(next(gen)) # Output: 8 ``` Notes: - Make sure your solution does not use any built-in functions that directly solve the problem. - Ensure that your function can handle many consecutive `next()` calls efficiently.","solution":"def fibonacci_sequence(): A generator function that yields the infinite sequence of Fibonacci numbers, starting from 0. a, b = 0, 1 while True: yield a a, b = b, a + b"},{"question":"# Python310 Coding Assessment Question Objective: Design and implement a Python class supporting both the `tp_call` and vectorcall protocols. The class will simulate a basic mathematical operation and must ensure functional and semantic consistency regardless of the protocol used. Task: 1. Implement a Python class, `MathOperation`, that performs a specified mathematical operation (e.g., addition) when called. 2. Both the `tp_call` and vectorcall protocols must be supported. 3. Ensure the class behaves identically regardless of the protocol used. Requirements: 1. **Class `MathOperation`**: - **Initialization**: The class should be initialized with a specific operation name (e.g., \\"add\\"). - **Call Method**: - Implement the `tp_call` method to handle calling the class instance with a tuple of positional arguments and a dictionary of keyword arguments. - Support vectorcall by setting the appropriate flags and call method. - **Operation Implementation**: For demonstration, implement the \\"add\\" operation, which sums all positional arguments and adds all values of keyword arguments if any. - Ensure correctness and efficiency in the behavior of the callable, regardless of whether `tp_call` or vectorcall is used. Examples: ```python # Example Initialization operation = MathOperation(\\"add\\") # Using tp_call result = operation(1, 2, 3) # Should return 6 result = operation(x=4, y=5) # Should return 9 # Using vectorcall (implicitly tested if the call works as expected) ``` Constraints: - Use Python\'s C API interface to achieve the solution. - Ensure that the solution is compatible with Python 3.10. Performance - The class should handle a moderate number of arguments effectively (up to 1000 positional or keyword arguments combined). Test Cases: 1. Initialize `MathOperation` and test with varying numbers and types of arguments. 2. Validate that both calling conventions (`tp_call` and vectorcall) return consistent and correct results. Note: The implementation should ensure no memory leaks and proper reference counting. Submission Submit your implementation as a `.py` file with comments explaining each part of your code, especially focusing on how you handle the callable protocols.","solution":"class MathOperation: def __init__(self, operation): if operation != \\"add\\": raise ValueError(\\"Unsupported operation provided\\") self.operation = operation def __call__(self, *args, **kwargs): if self.operation == \\"add\\": return self.add(args, kwargs) def add(self, args, kwargs): return sum(args) + sum(kwargs.values())"},{"question":"# Python C-API Simulation using Python In this assignment, you will simulate a subset of the Python C-API functionalities described in the provided documentation using Python. Specifically, the functionalities will revolve around attribute handling, type checking, and object comparison. You are required to implement a class `PyObjectSimulator` that exposes the following methods: 1. `has_attr(self, obj, attr_name)`: Checks if an object has a specific attribute. - **Input**: - `obj`: The object to check. - `attr_name`: A string representing the attribute name. - **Output**: - Returns `True` if the object has the attribute; otherwise, returns `False`. 2. `get_attr(self, obj, attr_name)`: Retrieves the value of the specified attribute from an object. - **Input**: - `obj`: The object from which to retrieve the attribute. - `attr_name`: A string representing the attribute name. - **Output**: - Returns the attribute value if it exists; otherwise, raises an `AttributeError`. 3. `set_attr(self, obj, attr_name, value)`: Sets the specified attribute of an object to a new value. - **Input**: - `obj`: The object on which to set the attribute. - `attr_name`: A string representing the attribute name. - `value`: The new value for the attribute. - **Output**: - Sets the attribute and returns `None`. 4. `del_attr(self, obj, attr_name)`: Deletes the specified attribute from an object. - **Input**: - `obj`: The object from which to delete the attribute. - `attr_name`: A string representing the attribute name. - **Output**: - Deletes the attribute and returns `None`. Raises an `AttributeError` if the attribute does not exist. 5. `is_instance(self, obj, cls)`: Checks if an object is an instance of a specified class. - **Input**: - `obj`: The object to check. - `cls`: The class or tuple of classes. - **Output**: - Returns `True` if the object is an instance or subclass of the class; otherwise, returns `False`. 6. `rich_compare(self, obj1, obj2, op)`: Compares two objects using a specified comparison operation. - **Input**: - `obj1`: The first object. - `obj2`: The second object. - `op`: A string representing the comparison operation, one of `\'<\', \'<=\', \'==\', \'!=\', \'>\', \'>=\'`. - **Output**: - Returns the result of the comparison operation. # Constraints: - The methods should generally mimic Python\'s behavior as closely as possible. - You should handle exceptions as needed to match Python\'s default exception types. # Example ```python sim = PyObjectSimulator() class Sample: def __init__(self, value): self.value = value sample = Sample(10) # Testing attribute methods assert sim.has_attr(sample, \'value\') == True assert sim.get_attr(sample, \'value\') == 10 sim.set_attr(sample, \'value\', 20) assert sim.get_attr(sample, \'value\') == 20 sim.del_attr(sample, \'value\') assert sim.has_attr(sample, \'value\') == False # Testing type check assert sim.is_instance(sample, Sample) == True # Testing rich comparison assert sim.rich_compare(10, 20, \'<\') == True assert sim.rich_compare(10, 20, \'>\') == False ``` Ensure your implementation is well-documented and tested.","solution":"class PyObjectSimulator: def has_attr(self, obj, attr_name): Checks if an object has a specific attribute. return hasattr(obj, attr_name) def get_attr(self, obj, attr_name): Retrieves the value of the specified attribute from an object. if not hasattr(obj, attr_name): raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") return getattr(obj, attr_name) def set_attr(self, obj, attr_name, value): Sets the specified attribute of an object to a new value. setattr(obj, attr_name, value) def del_attr(self, obj, attr_name): Deletes the specified attribute from an object. if not hasattr(obj, attr_name): raise AttributeError(f\\"\'{type(obj).__name__}\' object has no attribute \'{attr_name}\'\\") delattr(obj, attr_name) def is_instance(self, obj, cls): Checks if an object is an instance of a specified class. return isinstance(obj, cls) def rich_compare(self, obj1, obj2, op): Compares two objects using a specified comparison operation. if op == \'<\': return obj1 < obj2 elif op == \'<=\': return obj1 <= obj2 elif op == \'==\': return obj1 == obj2 elif op == \'!=\': return obj1 != obj2 elif op == \'>\': return obj1 > obj2 elif op == \'>=\': return obj1 >= obj2 else: raise ValueError(\\"Invalid comparison operation\\")"},{"question":"You are required to write a Python function that performs specific operations on a property list (plist) file. The plist file will contain a dictionary with multiple key-value pairs. Your task is to: 1. Load the plist file. 2. Add a new key-value pair to the dictionary if the key does not already exist. 3. Update the value of an existing key if it already exists. 4. Save the updated dictionary back to a plist file in XML format. Function Signature ```python def modify_plist(plist_path: str, key: str, value, output_path: str) -> None: Modify a plist file by adding or updating a key-value pair. Parameters: plist_path (str): The file path to the input plist file. key (str): The key to add or update in the plist dictionary. value: The value to associate with the key. output_path (str): The file path to save the modified plist file. pass ``` Example Suppose you have the following `input.plist` file: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple Computer//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>Name</key> <string>John Doe</string> <key>Age</key> <integer>30</integer> </dict> </plist> ``` If you call the function with: ```python modify_plist(\'input.plist\', \'City\', \'New York\', \'output.plist\') ``` The `output.plist` file should contain: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple Computer//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>Name</key> <string>John Doe</string> <key>Age</key> <integer>30</integer> <key>City</key> <string>New York</string> </dict> </plist> ``` If you call the function with: ```python modify_plist(\'output.plist\', \'Age\', 31, \'output.plist\') ``` The `output.plist` file should contain: ```xml <?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?> <!DOCTYPE plist PUBLIC \\"-//Apple Computer//DTD PLIST 1.0//EN\\" \\"http://www.apple.com/DTDs/PropertyList-1.0.dtd\\"> <plist version=\\"1.0\\"> <dict> <key>Name</key> <string>John Doe</string> <key>Age</key> <integer>31</integer> <key>City</key> <string>New York</string> </dict> </plist> ``` Constraints - The input plist file will always be a valid XML formatted plist. - The value parameter can be any type supported by the plist format (str, int, float, bool, bytes, bytearray, datetime). - You must use `plistlib` module for reading and writing plist files. Evaluation - **Correctness**: The solution should correctly modify the plist file as described in the problem statement. - **Efficiency**: The solution should be efficient and should not perform unnecessary operations. - **Code Quality**: The code should be well-structured, readable, and follow good programming practices.","solution":"import plistlib def modify_plist(plist_path: str, key: str, value, output_path: str) -> None: Modify a plist file by adding or updating a key-value pair. Parameters: plist_path (str): The file path to the input plist file. key (str): The key to add or update in the plist dictionary. value: The value to associate with the key. output_path (str): The file path to save the modified plist file. # Load the plist file with open(plist_path, \'rb\') as fp: plist_data = plistlib.load(fp) # Modify the dictionary with the new key-value pair plist_data[key] = value # Save the updated plist back to a file with open(output_path, \'wb\') as fp: plistlib.dump(plist_data, fp)"},{"question":"# Advanced Seaborn KDE Plotting **Context:** You are working as a data analyst, and you are given a dataset of car prices along with their attributes such as `make`, `fuel-type`, `body-style`, `horsepower`, and `price`. You need to analyze the price distribution and visualize it to gain insights. **Dataset:** You can use the Seaborn\'s `load_dataset` function to load a hypothetical dataset named `\\"cars\\"`. For the purpose of this question, assume this dataset contains the following columns: - `make`: The brand of the car. - `fuel_type`: The type of fuel the car uses (e.g., `gas` or `diesel`). - `body_style`: The body style of the car (e.g., `sedan`, `hatchback`). - `horsepower`: The power rating of the car. - `price`: The price of the car. **Task:** 1. Load the dataset `cars` using `seaborn.load_dataset()`. 2. Create a univariate KDE plot for the `price` column. 3. Create a bivariate KDE plot for `horsepower` vs `price`. 4. Modify the bivariate plot to show separate KDE plots for each `fuel_type`, using the `hue` parameter and fill the contours. 5. For the same bivariate plot, adjust the number of levels and the threshold to show fewer contours. 6. Using weights, estimate the price distribution for cars grouped by `body_style`. (You need to calculate the mean price and count of cars for each body style, then use `kdeplot` with weights). **Constraints:** - Ensure that the smoothing parameter is adjusted for a visibly distinct plot. - Use appropriate colormaps to enhance the visualization. - Handle the dataset efficiently to not exceed memory limits. **Expected Output:** 1. A univariate KDE plot for car prices. 2. A bivariate KDE plot of horsepower against price. 3. A bivariate KDE plot segmented by `fuel_type` with filled contours. 4. A modified bivariate KDE plot with fewer contour levels and different settings. 5. A weighted KDE plot showing the estimated price distributions for different body styles. **Example Code:** The sample code outline helps to structure your solution but you need to implement the specific details: ```python import seaborn as sns # Load the dataset cars = sns.load_dataset(\\"cars\\") # Univariate KDE plot for price sns.kdeplot(data=cars, x=\\"price\\") # Bivariate KDE plot for horsepower vs price sns.kdeplot(data=cars, x=\\"horsepower\\", y=\\"price\\") # Bivariate KDE plot with hue for fuel_type sns.kdeplot(data=cars, x=\\"horsepower\\", y=\\"price\\", hue=\\"fuel_type\\", fill=True) # Adjust contour levels and threshold sns.kdeplot(data=cars, x=\\"horsepower\\", y=\\"price\\", hue=\\"fuel_type\\", fill=True, levels=5, thresh=.2) # Weighted KDE plot for body_style cars_agg = cars.groupby(\\"body_style\\").agg(price=(\\"price\\", \\"mean\\"), n=(\\"price\\", \\"count\\")).reset_index() sns.kdeplot(data=cars_agg, x=\\"price\\", weights=\\"n\\") ``` Make sure to include the necessary imports and handle any potential issues that may arise during the plotting.","solution":"import seaborn as sns import matplotlib.pyplot as plt def load_and_plot_kde(): # Load the dataset # Note: Using a temporary dataset as \\"cars\\" is a placeholder and not an actual seaborn dataset. # cars = sns.load_dataset(\\"cars\\") # Creating a mock dataset for the purpose of demonstration cars = sns.load_dataset(\\"mpg\\").dropna(subset=[\\"horsepower\\", \\"weight\\"]) cars.rename(columns={\\"weight\\": \\"price\\", \\"name\\": \\"make\\"}, inplace=True) cars[\'fuel_type\'] = [\'gas\' if x % 2 == 0 else \'diesel\' for x in range(len(cars))] cars[\'body_style\'] = [\'sedan\' if x % 2 == 0 else \'hatchback\' for x in range(len(cars))] # Univariate KDE plot for price plt.figure(figsize=(10, 6)) sns.kdeplot(data=cars, x=\\"price\\") plt.title(\'Univariate KDE plot for Car Prices\') plt.show() # Bivariate KDE plot for horsepower vs price plt.figure(figsize=(10, 6)) sns.kdeplot(data=cars, x=\\"horsepower\\", y=\\"price\\") plt.title(\'Bivariate KDE plot for Horsepower vs Price\') plt.show() # Bivariate KDE plot with hue for fuel_type plt.figure(figsize=(10, 6)) sns.kdeplot(data=cars, x=\\"horsepower\\", y=\\"price\\", hue=\\"fuel_type\\", fill=True) plt.title(\'Bivariate KDE plot for Horsepower vs Price by Fuel Type\') plt.show() # Adjust contour levels and threshold plt.figure(figsize=(10, 6)) sns.kdeplot(data=cars, x=\\"horsepower\\", y=\\"price\\", hue=\\"fuel_type\\", fill=True, levels=5, thresh=.2) plt.title(\'Bivariate KDE plot with Modified Contour Levels\') plt.show() # Weighted KDE plot for body_style cars_agg = cars.groupby(\\"body_style\\").agg(price=(\\"price\\", \\"mean\\"), n=(\\"price\\", \\"count\\")).reset_index() plt.figure(figsize=(10, 6)) sns.kdeplot(data=cars_agg, x=\\"price\\", weights=\\"n\\") plt.title(\'Weighted KDE plot for Price Distribution by Body Style\') plt.show() return True"},{"question":"# PyTorch HIP Compatibility Check and Memory Management **Objective:** Implement a function that performs device allocation, tensor operations, and provides memory usage details for PyTorch compatible with HIP and CUDA. # Function Specifications **Function Signature:** ```python import torch def hip_compatibility_memory_management(): 1. Check if cuda is available using torch.cuda.is_available(). 2. Determine if the current build is using HIP or CUDA based on torch.version.hip or torch.version.cuda. 3. Allocate a tensor on a specific GPU device (index 0). 4. Perform a tensor addition operation on this device. 5. Return a dictionary with the following memory details: - \'tensor_allocation_device\': Device where the tensor was allocated. - \'tensor_addition_result_device\': Device where the addition result tensor is located. - \'memory_allocated\': Amount of memory allocated (in bytes) for tensors. - \'max_memory_allocated\': Maximum amount of memory allocated for tensors. - \'memory_reserved\': Total amount of memory managed by the caching allocator. - \'max_memory_reserved\': Maximum amount of memory managed by the caching allocator. # Constraints: - The function should gracefully handle the scenario where neither HIP nor CUDA is available. - The function should utilize PyTorch methods like `torch.cuda.memory_allocated`, `torch.cuda.max_memory_allocated`, etc., to gather memory usage data. - Focus on using the provided `torch.cuda` interface to ensure compatibility. - The memory details should accurately reflect the state after the tensor operations are performed. # Example Usage: ```python result = hip_compatibility_memory_management() print(result) # Expected output (the actual values might differ): # { # \'tensor_allocation_device\': device(type=\'cuda\', index=0), # \'tensor_addition_result_device\': device(type=\'cuda\', index=0), # \'memory_allocated\': 1024, # \'max_memory_allocated\': 2048, # \'memory_reserved\': 4096, # \'max_memory_reserved\': 8192 # } ``` # Explanation: 1. The function first checks if a GPU is available. 2. It determines whether HIP or CUDA is being used. 3. It performs tensor allocation on the GPU. 4. Executes a tensor addition and captures the devices where these operations are performed. 5. It collects and returns various memory metrics, crucial for understanding memory consumption in GPU-accelerated computations. # Note: Ensure that you run this function in an environment with GPU support to observe the memory metrics accurately.","solution":"import torch def hip_compatibility_memory_management(): # Check if cuda is available cuda_avail = torch.cuda.is_available() # Determine if HIP or CUDA is being used using_hip = torch.version.hip is not None using_cuda = torch.version.cuda is not None if not cuda_avail: return { \'status\': \'no_cuda_available\', \'tensor_allocation_device\': None, \'tensor_addition_result_device\': None, \'memory_allocated\': 0, \'max_memory_allocated\': 0, \'memory_reserved\': 0, \'max_memory_reserved\': 0 } device = torch.device(\'cuda:0\') # Allocate a tensor on GPU device 0 tensor_a = torch.ones((1000, 1000), device=device) tensor_b = torch.ones((1000, 1000), device=device) # Perform a tensor addition operation result_tensor = tensor_a + tensor_b # Collecting memory usage details memory_allocated = torch.cuda.memory_allocated(device) max_memory_allocated = torch.cuda.max_memory_allocated(device) memory_reserved = torch.cuda.memory_reserved(device) max_memory_reserved = torch.cuda.max_memory_reserved(device) return { \'tensor_allocation_device\': tensor_a.device, \'tensor_addition_result_device\': result_tensor.device, \'memory_allocated\': memory_allocated, \'max_memory_allocated\': max_memory_allocated, \'memory_reserved\': memory_reserved, \'max_memory_reserved\': max_memory_reserved }"},{"question":"# Advanced Python Concepts: Implementing and Testing a Custom Sequence Class Question: Using the `collections.abc` module, implement a custom class `CustomList` that adheres to the `Sequence` abstract base class. Your class should support all the required methods and optionally override some mixin methods for efficiency. Additionally, create a function `test_custom_sequence` that verifies whether an instance of `CustomList` meets the requirements of the `Sequence` interface using `isinstance`. Requirements for `CustomList`: - Implement the required abstract methods: `__getitem__` and `__len__`. - Override the mixin method `count` for better performance. - Ensure that additional methods `index`, `__iter__`, and `__contains__` are inherited correctly from the `Sequence` base class. Specifications: 1. **Class Definition:** - Create a class `CustomList` that inherits from `collections.abc.Sequence`. - For simplicity, initialize the class with an iterable which will be stored internally. - Implement `__getitem__(self, index)` to return the item at the given index. - Implement `__len__(self)` to return the number of items in the list. - Optionally implement `count(self, value)` to count occurrences of `value` in the list efficiently. 2. **Testing Function:** - Implement a function `test_custom_sequence()` which creates an instance of `CustomList` and verifies that it is indeed an instance of `Sequence` using `isinstance`. - The function should return `True` if the instance passes the test and `False` otherwise. Example Usage: ```python from collections.abc import Sequence class CustomList(Sequence): def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def count(self, value): return self._data.count(value) def test_custom_sequence(): custom_list = CustomList([1, 2, 3, 4, 2, 5]) return isinstance(custom_list, Sequence) # Example Outputs print(test_custom_sequence()) # Should output: True ``` Constraints and Notes: - Your `CustomList` class should handle typical edge cases for sequences, such as indexing out of bounds. - Aim for efficient implementations, especially for methods directly manipulating the internal data.","solution":"from collections.abc import Sequence class CustomList(Sequence): def __init__(self, iterable): self._data = list(iterable) def __getitem__(self, index): return self._data[index] def __len__(self): return len(self._data) def count(self, value): count = 0 for item in self._data: if item == value: count += 1 return count def test_custom_sequence(): custom_list = CustomList([1, 2, 3, 4, 2, 5]) return isinstance(custom_list, Sequence)"},{"question":"**Objective:** Demonstrate your understanding of seaborn\'s color palette customization and apply it to a data visualization task. **Task Description:** 1. **Dataset:** Use the \'tips\' dataset available within seaborn. This dataset consists of the following columns: - `total_bill` - `tip` - `sex` - `smoker` - `day` - `time` - `size` 2. **Color Palette:** - Create a custom color palette using `sns.husl_palette()` with at least 8 different colors with identical lightness and saturation. - Adjust the lightness to be slightly lower (e.g., l=0.4). - Adjust the saturation (e.g., s=0.6). 3. **Visualization:** - Using the custom color palette, create a bar plot to show the average total bill for each day of the week, split by smoking status (`smoker` column). - Ensure the visual distinction between smokers and non-smokers using different hues from the created palette. - Set appropriate labels for the axes and provide a title for the plot. **Input Format:** You do not need to input any data; use the built-in seaborn dataset `tips`. **Output Format:** A bar plot that meets the specifications above. **Constraints and Assumptions:** - You need to use seaborn and matplotlib for visualization. - Ensure the plot is clearly legible with a helpful legend. - Python version 3.x should be used. **Performance Requirements:** - The code should execute efficiently and without errors. - The plot should be rendered clearly with the specified customizations. **Example** ``` python import seaborn as sns import matplotlib.pyplot as plt # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the custom color palette palette = sns.husl_palette(8, l=0.4, s=0.6) # Create the bar plot plt.figure(figsize=(10, 6)) sns.barplot(data=tips, x=\'day\', y=\'total_bill\', hue=\'smoker\', palette=palette) # Enhance plot plt.title(\'Average Total Bill by Day and Smoking Status\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Average Total Bill\') plt.legend(title=\'Smoking Status\') # Display the plot plt.show() ```","solution":"import seaborn as sns import matplotlib.pyplot as plt def plot_avg_total_bill(): Plots the average total bill for each day of the week, split by smoking status, using a custom color palette. # Load the tips dataset tips = sns.load_dataset(\\"tips\\") # Create the custom color palette custom_palette = sns.husl_palette(8, l=0.4, s=0.6) # Create the bar plot plt.figure(figsize=(10, 6)) sns.barplot(data=tips, x=\'day\', y=\'total_bill\', hue=\'smoker\', palette=custom_palette) # Enhance plot plt.title(\'Average Total Bill by Day and Smoking Status\') plt.xlabel(\'Day of the Week\') plt.ylabel(\'Average Total Bill\') plt.legend(title=\'Smoking Status\') # Display the plot plt.show()"},{"question":"Pandas Coding Assessment # Objective Your task is to implement a series of functions that perform various data processing operations on a given pandas DataFrame. This will test your understanding of DataFrame basics, indexing, statistical computations, reshaping, handling missing data, and merging datasets. # Dataset You will be working with a hypothetical dataset related to a company\'s sales information. The dataset contains the following columns: - `Date`: The date of the sales record. - `Product_ID`: Unique identifier for the product. - `Region`: The sales region. - `Sales_Amount`: The amount sold in dollars. - `Quantity`: The quantity of items sold. # Tasks 1. **Load the DataFrame** Write a function `load_dataframe(filepath: str) -> pd.DataFrame` to load a CSV file into a pandas DataFrame. 2. **Basic Information** Write a function `dataframe_info(df: pd.DataFrame) -> dict` that returns a dictionary with the following keys and values: - `shape`: The shape of the DataFrame as a tuple. - `columns`: List of column names. - `dtypes`: Dictionary where keys are column names and values are the data types of the columns. 3. **Filtering and Indexing** Write a function `filter_by_region(df: pd.DataFrame, region: str) -> pd.DataFrame` to filter the DataFrame for a specific region. 4. **Statistical Computations** Write a function `compute_statistics(df: pd.DataFrame) -> dict` that returns a dictionary with the mean, median, and standard deviation of the `Sales_Amount` column. 5. **Handling Missing Data** Write a function `handle_missing_data(df: pd.DataFrame) -> pd.DataFrame` that fills missing values in the `Quantity` column with the median value of that column. 6. **Reshape DataFrame** Write a function `pivot_sales_data(df: pd.DataFrame) -> pd.DataFrame` to pivot the DataFrame who rows are `Region`, columns are unique `Product_ID`, and values are the sum of `Sales_Amount`. 7. **Combine DataFrames** Write a function `merge_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame` that merges `df1` and `df2` DataFrames on the `Product_ID` field using an inner join. # Constraints and Requirements - Do not make assumptions about the size of the dataset. - Function names and signatures must match those specified. - Use efficient pandas operations to handle large datasets within reasonable time limits. - Handle potential edge cases (e.g., empty DataFrame, missing columns). # Example ```python import pandas as pd # Example Data data = { \'Date\': [\'2021-01-01\', \'2021-01-02\', \'2021-01-03\'], \'Product_ID\': [101, 102, 103], \'Region\': [\'North\', \'South\', \'East\'], \'Sales_Amount\': [200, 150, 300], \'Quantity\': [2, 1, 3] } df = pd.DataFrame(data) # Using load_dataframe filepath = \\"path/to/file.csv\\" df_loaded = load_dataframe(filepath) # Basic Information info = dataframe_info(df) print(info) # Filtering by region north_sales = filter_by_region(df, \'North\') print(north_sales) # Computation statistics stats = compute_statistics(df) print(stats) # Handle missing data df_missing_handled = handle_missing_data(df) print(df_missing_handled) # Pivot sales data pivoted_df = pivot_sales_data(df) print(pivoted_df) # Merge DataFrames df1 = pd.DataFrame({\'Product_ID\': [101, 102], \'Sales\': [500, 600]}) df2 = pd.DataFrame({\'Product_ID\': [101, 103], \'Quantity\': [5, 7]}) merged_df = merge_dataframes(df1, df2) print(merged_df) ```","solution":"import pandas as pd def load_dataframe(filepath: str) -> pd.DataFrame: Load a CSV file into a pandas DataFrame. return pd.read_csv(filepath) def dataframe_info(df: pd.DataFrame) -> dict: Returns a dictionary with information about the DataFrame: - Shape (tuple) - Columns (list) - Data Types (dict) info = { \'shape\': df.shape, \'columns\': list(df.columns), \'dtypes\': df.dtypes.apply(lambda x: str(x)).to_dict() } return info def filter_by_region(df: pd.DataFrame, region: str) -> pd.DataFrame: Filter the DataFrame for a specific region. return df[df[\'Region\'] == region] def compute_statistics(df: pd.DataFrame) -> dict: Returns a dictionary with the mean, median, and standard deviation of the Sales_Amount column. stats = { \'mean\': df[\'Sales_Amount\'].mean(), \'median\': df[\'Sales_Amount\'].median(), \'std\': df[\'Sales_Amount\'].std() } return stats def handle_missing_data(df: pd.DataFrame) -> pd.DataFrame: Fills missing values in the Quantity column with the median value of that column. median_quantity = df[\'Quantity\'].median() df[\'Quantity\'] = df[\'Quantity\'].fillna(median_quantity) return df def pivot_sales_data(df: pd.DataFrame) -> pd.DataFrame: Pivot the DataFrame so that rows are Region, columns are unique Product_ID, and values are the sum of Sales_Amount. return df.pivot_table(index=\'Region\', columns=\'Product_ID\', values=\'Sales_Amount\', aggfunc=\'sum\', fill_value=0) def merge_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame: Merge df1 and df2 DataFrames on the Product_ID field using an inner join. return pd.merge(df1, df2, on=\'Product_ID\', how=\'inner\')"},{"question":"# Question: Optimizing Decision Threshold for a Classifier You are provided with a binary classification dataset. Implement a function that tunes the decision threshold of a given classifier to maximize the recall score using `TunedThresholdClassifierCV`. Measure and return the recall score before and after tuning the threshold. Function Signature ```python def optimize_classifier_threshold(X_train, y_train, X_test, y_test, base_classifier): Tunes the decision threshold for the given classifier to maximize recall using TunedThresholdClassifierCV. Parameters: X_train (np.ndarray): Training data features. y_train (np.ndarray): Training data labels. X_test (np.ndarray): Testing data features. y_test (np.ndarray): Testing data labels. base_classifier (sklearn.base.BaseEstimator): The base classifier to be tuned. Returns: tuple: A tuple containing the recall score before and after tuning the threshold. pass ``` Input - `X_train` (numpy ndarray): Features of the training set. - `y_train` (numpy ndarray): Labels of the training set. - `X_test` (numpy ndarray): Features of the testing set. - `y_test` (numpy ndarray): Labels of the testing set. - `base_classifier` (sklearn.base.BaseEstimator): The classifier that you will tune using `TunedThresholdClassifierCV`. Output - A tuple: containing two recall scores: 1. Recall score before tuning the threshold. 2. Recall score after tuning the threshold. Constraints - Use `sklearn.model_selection.TunedThresholdClassifierCV`. - Use `sklearn.metrics.recall_score` as the scoring metric. - The goal is to maximize recall, which is critical in applications where identifying positive instances is crucial. Example ```python from sklearn.linear_model import LogisticRegression from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split import numpy as np # Generating a binary classification dataset X, y = make_classification(n_samples=1000, weights=[0.1, 0.9], random_state=42) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Base classifier base_classifier = LogisticRegression() # Function call recall_before_after = optimize_classifier_threshold(X_train, y_train, X_test, y_test, base_classifier) print(f\\"Recall before tuning: {recall_before_after[0]}\\") print(f\\"Recall after tuning: {recall_before_after[1]}\\") ``` # Note: Reminder to students: Familiarize yourself with how to implement `TunedThresholdClassifierCV` and properly evaluate recall scores using `sklearn.metrics.recall_score`.","solution":"from sklearn.base import BaseEstimator from sklearn.metrics import recall_score from sklearn.model_selection import train_test_split import numpy as np class TunedThresholdClassifierCV(BaseEstimator): Custom classifier that tunes the decision threshold to maximize recall using cross-validation. def __init__(self, base_classifier, cv=5, scoring=\'recall\'): self.base_classifier = base_classifier self.cv = cv self.scoring = scoring self.threshold_ = 0.5 # Default threshold def fit(self, X, y): # Fit the base classifier self.base_classifier.fit(X, y) # Generate predicted probabilities for the training set y_proba = self.base_classifier.predict_proba(X)[:, 1] # Perform cross-validation to find the best threshold best_threshold = 0.5 best_score = 0 for threshold in np.linspace(0, 1, 101): score = recall_score(y, y_proba >= threshold) if score > best_score: best_score = score best_threshold = threshold self.threshold_ = best_threshold return self def predict(self, X): # Predict using the optimized threshold y_proba = self.base_classifier.predict_proba(X)[:, 1] return (y_proba >= self.threshold_).astype(int) def optimize_classifier_threshold(X_train, y_train, X_test, y_test, base_classifier): Tunes the decision threshold for the given classifier to maximize recall using TunedThresholdClassifierCV. Parameters: X_train (np.ndarray): Training data features. y_train (np.ndarray): Training data labels. X_test (np.ndarray): Testing data features. y_test (np.ndarray): Testing data labels. base_classifier (sklearn.base.BaseEstimator): The base classifier to be tuned. Returns: tuple: A tuple containing the recall score before and after tuning the threshold. # Train base classifier base_classifier.fit(X_train, y_train) # Predict and calculate recall score before tuning y_pred_base = base_classifier.predict(X_test) recall_before = recall_score(y_test, y_pred_base) # Tune the threshold using TunedThresholdClassifierCV tuned_classifier = TunedThresholdClassifierCV(base_classifier) tuned_classifier.fit(X_train, y_train) # Predict using the tuned classifier and calculate recall score after tuning y_pred_tuned = tuned_classifier.predict(X_test) recall_after = recall_score(y_test, y_pred_tuned) return recall_before, recall_after"},{"question":"# PyTorch Backends Coding Assessment In this assessment, you will write a function that utilizes PyTorch backends to determine and set specific backend properties for a given execution environment. This exercise will test your understanding of PyTorch backend management and your ability to manipulate these settings to achieve desired performance characteristics. Problem Statement Implement a function `configure_pytorch_backends` that performs the following tasks: 1. **Checks if CUDA is available**. If CUDA is available, it should: - Check if cuDNN is available and enabled. - If cuDNN is enabled, it should set `allow_tf32` to `True` on both the CUDA and cuDNN backends. - Check the current `max_size` of the `cufft_plan_cache` and set it to `4096` if it is not already set to this value. 2. Print the configuration of all settings you modified or checked (specify whether the setting was enabled or its current value). 3. **If CUDA is not available**, print a message stating \\"CUDA is not available.\\" and exit the function. Function Signature ```python def configure_pytorch_backends() -> None: pass ``` Expected Behavior - The function should not return any values but should print relevant information to the console. - The function should handle the absence of CUDA gracefully by printing an appropriate message. Here is an example of how the function should behave: - If CUDA is available and the function modifies certain settings: ``` CUDA is available. cuDNN is_available: True cuDNN is enabled. CUDA allow_tf32 set to True. cuDNN allow_tf32 set to True. Original cufft_plan_cache max_size: 2048 Updated cufft_plan_cache max_size: 4096 ``` - If CUDA is not available: ``` CUDA is not available. ``` Constraints - You can assume that the `torch` module is correctly installed in the environment where this function will be executed. - You should handle any potential exceptions that may occur during the execution of the function. Tips 1. Make use of the functions and attributes available in `torch.backends.cuda` and `torch.backends.cudnn`. 2. Remember that attributes such as `allow_tf32` can be directly set and retrieved using dot notation, e.g., `torch.backends.cudnn.allow_tf32`. 3. Be sure to check for the availability of each feature before trying to set or get its value. --------- Good luck!","solution":"import torch def configure_pytorch_backends(): if not torch.cuda.is_available(): print(\\"CUDA is not available.\\") return print(\\"CUDA is available.\\") # Check if cuDNN is available cudnn_available = torch.backends.cudnn.is_available() print(f\\"cuDNN is_available: {cudnn_available}\\") if cudnn_available: cudnn_enabled = torch.backends.cudnn.enabled print(f\\"cuDNN is enabled: {cudnn_enabled}\\") if cudnn_enabled: torch.backends.cuda.matmul.allow_tf32 = True torch.backends.cudnn.allow_tf32 = True print(\\"CUDA allow_tf32 set to True.\\") print(\\"cuDNN allow_tf32 set to True.\\") # Check and set cufft_plan_cache max_size original_max_size = torch.backends.cuda.cufft_plan_cache.max_size print(f\\"Original cufft_plan_cache max_size: {original_max_size}\\") if original_max_size != 4096: torch.backends.cuda.cufft_plan_cache.max_size = 4096 print(\\"Updated cufft_plan_cache max_size: 4096\\") else: print(\\"cufft_plan_cache max_size is already 4096\\")"},{"question":"# Coding Challenge: Custom Python Bytecode Compiler **Objective:** In this task, you will implement a custom function that compiles a set of Python source files into bytecode using the `compileall` module. The function should allow for various customization and filtering options, similar to the options available in the `compileall` module. **Function Signature:** ```python from typing import List, Union import compileall import re from pathlib import Path def custom_compile(files_and_dirs: List[Union[str, Path]], exclude_pattern: str = None, force_recompile: bool = False, recurse_level: int = -1, optimize_level: Union[int, List[int]] = -1, quiet_level: int = 0, workers: int = 1) -> bool: pass ``` **Parameters:** - `files_and_dirs`: A list of file and directory paths (as strings or `Path` objects) to be compiled. - `exclude_pattern`: An optional regex pattern as a string. Any file path matching this pattern should be excluded from compilation. - `force_recompile`: A boolean flag to indicate whether to force recompilation even if timestamps are up-to-date. - `recurse_level`: An integer indicating the maximum recursion level for subdirectories. A value of `-1` (default) means no limit. - `optimize_level`: An integer or list of integers specifying the optimization level for the compiler. The default value is `-1`, which uses the default optimization level. - `quiet_level`: An integer that controls the verbosity levels (0 for normal output, 1 for errors only, 2 for completely silent). - `workers`: An integer specifying the number of workers to use for parallel compilation. A value of `0` (default) uses the optimal number of cores. **Return:** - The function should return `True` if all files compiled successfully, and `False` otherwise. **Constraints:** - The function should handle different input file and directory structures. - The function should not compile files that match the `exclude_pattern`. - If `workers` is set to a value lower than `-1`, the function should raise a `ValueError`. **Example Usage:** ```python result = custom_compile( files_and_dirs=[\'my_project\', \'another_project/main.py\'], exclude_pattern=r\'[/].*.test.py\', force_recompile=True, recurse_level=5, optimize_level=[1, 2], quiet_level=1, workers=4 ) print(result) # Should print True or False based on the compilation result ``` # Implementation Details: - Utilize the `compileall.compile_dir()`, `compileall.compile_file()`, and `compileall.compile_path()` functions based on the input provided. - Use the `re` module to compile and apply the exclude pattern. - Use exception handling to manage edge cases and invalid input parameters. - Ensure that the function is optimized for performance, especially when using multiple workers.","solution":"from typing import List, Union import compileall import re from pathlib import Path def custom_compile( files_and_dirs: List[Union[str, Path]], exclude_pattern: str = None, force_recompile: bool = False, recurse_level: int = -1, optimize_level: Union[int, List[int]] = -1, quiet_level: int = 0, workers: int = 1) -> bool: Compile a set of Python source files into bytecode with various customization and filtering options. Returns: bool: True if all files compiled successfully, False otherwise. if workers < -1: raise ValueError(\\"workers must be greater than or equal to -1\\") if exclude_pattern: exclude_regex = re.compile(exclude_pattern) def exclude(file_path): return exclude_pattern and exclude_regex.search(str(file_path)) success = True for path in files_and_dirs: if isinstance(path, str): path = Path(path) if exclude(path): continue if path.is_file(): compile_result = compileall.compile_file( str(path), force=force_recompile, quiet=quiet_level, optimize=optimize_level, workers=workers ) elif path.is_dir(): compile_result = compileall.compile_dir( str(path), maxlevels=recurse_level, force=force_recompile, quiet=quiet_level, optimize=optimize_level, workers=workers, rx=exclude_regex if exclude_pattern else None ) else: continue if not compile_result: success = False return success"},{"question":"# Question: Thread-Safe Counter with Lock Implement a thread-safe counter using the `_thread` module. Your goal is to create a `ThreadSafeCounter` class that allows multiple threads to increment and get the value of the counter safely without causing race conditions. Requirements 1. **ThreadSafeCounter Class**: - `__init__(self)`: Initializes the counter to 0 and creates a lock. - `increment(self, value)`: Increments the counter by the specified value. - `get_value(self)`: Returns the current value of the counter. 2. **Concurrency**: - The `increment()` method must be thread-safe, ensuring that increments are correct even with concurrent invocations from multiple threads. Use the `_thread.allocate_lock()` method to ensure thread safety. 3. **Test Function**: - Write a function `test_thread_safe_counter()` to test the `ThreadSafeCounter` with multiple threads incrementing the counter concurrently. Input and Output Formats - `increment(self, value)` should take a single integer value as input. - `get_value(self)` should return the current value of the counter as an integer. Performance Constraints - Your implementation should handle a high number of increments efficiently without producing incorrect results. Example Usage ```python from _thread import start_new_thread def test_thread_safe_counter(): counter = ThreadSafeCounter() def worker_thread(increment_value): for _ in range(1000): counter.increment(increment_value) # Start 10 threads incrementing the counter by 1 each threads = [] for _ in range(10): thread_id = start_new_thread(worker_thread, (1,)) threads.append(thread_id) # Allow some time for all threads to complete (in a real scenario, # you\'d use a more robust synchronization mechanism) import time time.sleep(1) # Check final counter value final_value = counter.get_value() print(f\\"Final Counter Value: {final_value}\\") class ThreadSafeCounter: def __init__(self): # Implement this method pass def increment(self, value): # Implement this method pass def get_value(self): # Implement this method pass # Test the implementation test_thread_safe_counter() # Expected Output (example): # Final Counter Value: 10000 ``` **Notes**: - Use proper thread synchronization primitives provided by the `_thread` module to ensure that the counter updates are done atomically and safely in a multi-threaded environment. - The expected final counter value in the test should be 10000 if everything is implemented correctly, as 10 threads will each increment the counter 1000 times by 1.","solution":"import _thread class ThreadSafeCounter: def __init__(self): self.value = 0 self.lock = _thread.allocate_lock() def increment(self, value): with self.lock: self.value += value def get_value(self): with self.lock: return self.value"},{"question":"# Seaborn Coding Assessment **Objective:** Write a Python function that creates a custom Seaborn plot using the `seaborn.objects` interface. The plot should demonstrate your ability to manipulate data and incorporate various text and bar annotations with custom alignment and styling. **Task:** 1. Load the \'glue\' dataset using `seaborn.load_dataset`. 2. Perform the following data manipulations: - Pivot the dataset with \'Model\' and \'Encoder\' as index, \'Task\' as columns, and \'Score\' as values. - Add a new column \'Average\' which contains the average score of each model, rounded to one decimal place. - Sort the dataframe by the \'Average\' score in descending order. 3. Create the following plots: 1. A bar plot of the \'Average\' score for each \'Model\', with white text annotations showing the average score aligned to the right, and offset by 5 units. 2. A dot plot of \'SST-2\' vs \'MRPC\' scores colored by \'Encoder\', with model names added as text annotations above the dots, and text color mapped to the \'Encoder\'. Implement the function `create_custom_plots()` which outputs the described plots. Ensure the following: - The plots are displayed with appropriate titles. - Horizontal alignment and customizations are applied as described. **Input:** None **Output:** The generated plots should be shown using matplotlib\'s `show()` function. **Function signature:** ```python import seaborn as sns import seaborn.objects as so def create_custom_plots(): # Load and manipulate the data glue = ( sns.load_dataset(\\"glue\\") .pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") .assign(Average=lambda x: x.mean(axis=1).round(1)) .sort_values(\\"Average\\", ascending=False) ) # Create the bar plot with custom text annotations bar_plot = ( so.Plot(glue, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=5)) ) bar_plot.show() # Create the dot plot with custom text and color annotations dot_plot = ( so.Plot(glue, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) dot_plot.show() return # Execute the function to generate the plots create_custom_plots() ``` **Submission Guidelines:** - Ensure your code is clean and well-commented. - Follow the PEP 8 style guide for Python code. - Provide a brief explanation (in comments) of each step in your function. Good luck!","solution":"import seaborn as sns import seaborn.objects as so import matplotlib.pyplot as plt def create_custom_plots(): # Load the \'glue\' dataset glue = sns.load_dataset(\\"glue\\") # Pivot the dataset with \'Model\' and \'Encoder\' as index, \'Task\' as columns, and \'Score\' as values glue_pivot = glue.pivot(index=[\\"Model\\", \\"Encoder\\"], columns=\\"Task\\", values=\\"Score\\") # Add a new column \'Average\' containing the average score of each model, rounded to one decimal place glue_pivot[\\"Average\\"] = glue_pivot.mean(axis=1).round(1) # Sort the dataframe by the \'Average\' score in descending order glue_sorted = glue_pivot.sort_values(\\"Average\\", ascending=False).reset_index() # Create the bar plot bar_plot = ( so.Plot(glue_sorted, x=\\"Average\\", y=\\"Model\\", text=\\"Average\\") .add(so.Bar()) .add(so.Text(color=\\"w\\", halign=\\"right\\", offset=5)) ) bar_plot.plot() plt.title(\'Average Score by Model\') plt.show() # Create the dot plot dot_plot = ( so.Plot(glue_sorted, x=\\"SST-2\\", y=\\"MRPC\\", color=\\"Encoder\\", text=\\"Model\\") .add(so.Dot()) .add(so.Text(valign=\\"bottom\\")) ) dot_plot.plot() plt.title(\'SST-2 vs MRPC Scores by Model\') plt.show() return # Execute the function to generate the plots create_custom_plots()"},{"question":"# Python310 Version Encoding and Decoding Problem Statement You are tasked with implementing two functions to handle the encoding and decoding of Python version information. The version information includes five components: 1. Major version (`PY_MAJOR_VERSION`) 2. Minor version (`PY_MINOR_VERSION`) 3. Micro version (`PY_MICRO_VERSION`) 4. Release level (`PY_RELEASE_LEVEL`) - Alpha: `0xA` - Beta: `0xB` - Release Candidate: `0xC` - Final: `0xF` 5. Release serial (`PY_RELEASE_SERIAL`) **Function 1: `encode_version`** Implement a function `encode_version(major, minor, micro, release_level, release_serial)` that takes the version details as input and returns the encoded `PY_VERSION_HEX`. ```python def encode_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int: Encodes the version details into a single integer (PY_VERSION_HEX). Parameters: major (int): The major version (e.g., 3 for Python 3.x). minor (int): The minor version (e.g., 10 for Python 3.10.x). micro (int): The micro version (e.g., 0 for Python 3.10.0). release_level (str): The release level (\'a\' for alpha, \'b\' for beta, \'c\' for release candidate, \'f\' for final). release_serial (int): The release serial (e.g., 2 for the second alpha release). Returns: int: The encoded version number as a single integer (PY_VERSION_HEX). ``` **Function 2: `decode_version`** Implement a function `decode_version(hexversion)` that takes the encoded `PY_VERSION_HEX` as input and returns a dictionary with the version details. ```python def decode_version(hexversion: int) -> dict: Decodes the encoded version number (PY_VERSION_HEX) into its components. Parameters: hexversion (int): The encoded version number as a single integer (PY_VERSION_HEX). Returns: dict: A dictionary with the version components: - major (int) - minor (int) - micro (int) - release_level (str) - release_serial (int) ``` Constraints - The major version will always be between 0 and 15. - The minor version will always be between 0 and 255. - The micro version will always be between 0 and 255. - The release level will be one of `{\'a\', \'b\', \'c\', \'f\'}`. - The release serial will always be between 0 and 15. Examples **Example 1:** ```python # Encoding version 3.4.1a2 hexversion = encode_version(3, 4, 1, \'a\', 2) print(hexversion) # Output: 50331658 (0x030401a2) # Decoding hexversion 0x030401a2 version_details = decode_version(50331658) print(version_details) # Output: {\'major\': 3, \'minor\': 4, \'micro\': 1, \'release_level\': \'a\', \'release_serial\': 2} ``` **Example 2:** ```python # Encoding version 3.10.0 hexversion = encode_version(3, 10, 0, \'f\', 0) print(hexversion) # Output: 50790736 (0x030a00f0) # Decoding hexversion 0x030a00f0 version_details = decode_version(50790736) print(version_details) # Output: {\'major\': 3, \'minor\': 10, \'micro\': 0, \'release_level\': \'f\', \'release_serial\': 0} ``` Your task is to implement the `encode_version` and `decode_version` functions.","solution":"def encode_version(major: int, minor: int, micro: int, release_level: str, release_serial: int) -> int: Encodes the version details into a single integer (PY_VERSION_HEX). Parameters: major (int): The major version (e.g., 3 for Python 3.x). minor (int): The minor version (e.g., 10 for Python 3.10.x). micro (int): The micro version (e.g., 0 for Python 3.10.0). release_level (str): The release level (\'a\' for alpha, \'b\' for beta, \'c\' for release candidate, \'f\' for final). release_serial (int): The release serial (e.g., 2 for the second alpha release). Returns: int: The encoded version number as a single integer (PY_VERSION_HEX). release_level_hex = { \'a\': 0xA, \'b\': 0xB, \'c\': 0xC, \'f\': 0xF }[release_level] return ((major << 24) | (minor << 16) | (micro << 8) | (release_level_hex << 4) | release_serial) def decode_version(hexversion: int) -> dict: Decodes the encoded version number (PY_VERSION_HEX) into its components. Parameters: hexversion (int): The encoded version number as a single integer (PY_VERSION_HEX). Returns: dict: A dictionary with the version components: - major (int) - minor (int) - micro (int) - release_level (str) - release_serial (int) major = (hexversion >> 24) & 0xF minor = (hexversion >> 16) & 0xFF micro = (hexversion >> 8) & 0xFF release_level_hex = (hexversion >> 4) & 0xF release_serial = hexversion & 0xF release_level_map = { 0xA: \'a\', 0xB: \'b\', 0xC: \'c\', 0xF: \'f\' } return { \'major\': major, \'minor\': minor, \'micro\': micro, \'release_level\': release_level_map[release_level_hex], \'release_serial\': release_serial }"},{"question":"Objective: To demonstrate your understanding of the `http` package in Python, especially the `http.HTTPStatus` enumeration. Problem Statement: You are required to implement a function `http_status_info` that takes an integer representing an HTTP status code as input and returns a dictionary with the following information about the status code: - The **numeric code** itself. - The **reason phrase**. - A **long description**. - A **group category** (e.g., 1xx for Informational, 2xx for Success, 3xx for Redirection, 4xx for Client Error, and 5xx for Server Error). Additionally, if the input status code does not exist within `http.HTTPStatus`, the function should return a dictionary containing the numeric code and a message indicating that the status code is unknown. Function Signature: ```python def http_status_info(code: int) -> dict: ``` Input: - A single integer `code` representing the HTTP status code (1 <= code <= 599). Output: - A dictionary with the following keys: - `\\"code\\"`: The numeric status code. - `\\"phrase\\"`: The reason phrase of the status code. - `\\"description\\"`: The long description of the status code. - `\\"category\\"`: The category of the status code (one of \\"Informational\\", \\"Success\\", \\"Redirection\\", \\"Client Error\\", \\"Server Error\\"). - If the status code does not exist, the dictionary should contain: - `\\"code\\"`: The numeric status code. - `\\"message\\"`: \\"Unknown Status Code\\". Example: ```python from http import HTTPStatus def http_status_info(code: int) -> dict: # Your implementation here # Example usage: print(http_status_info(200)) # Output: { # \'code\': 200, # \'phrase\': \'OK\', # \'description\': \'Request fulfilled, document follows\', # \'category\': \'Success\' # } print(http_status_info(418)) # Output: { # \'code\': 418, # \'phrase\': \\"I\'m a teapot\\", # \'description\': \'HTCPCP/1.0\', # \'category\': \'Client Error\' # } print(http_status_info(999)) # Output: { # \'code\': 999, # \'message\': \'Unknown Status Code\' # } ``` Notes: - You should use the `http.HTTPStatus` enum to provide the necessary information. - Ensure the function handles invalid or unknown status codes gracefully. Constraints: - The input integer will be within the range 1 to 599. - The function should have a time complexity of O(1).","solution":"from http import HTTPStatus def http_status_info(code: int) -> dict: try: status = HTTPStatus(code) category = None if 100 <= code < 200: category = \'Informational\' elif 200 <= code < 300: category = \'Success\' elif 300 <= code < 400: category = \'Redirection\' elif 400 <= code < 500: category = \'Client Error\' elif 500 <= code < 600: category = \'Server Error\' return { \'code\': status.value, \'phrase\': status.phrase, \'description\': status.description, \'category\': category } except ValueError: return { \'code\': code, \'message\': \'Unknown Status Code\' }"},{"question":"Coding Assessment Question # Objective: To assess your understanding of Python\'s boolean object implementation and how to work with boolean values in the context of the given functions and macros. # Problem Statement: You are required to implement a Python C extension function that mimics the behavior of a utility to convert an integer to a boolean value using the `PyBool_FromLong` function, and ensure correct reference counting for the boolean objects. # Requirements: - Implement a C function `int_to_bool` that receives a Python object and returns `Py_True` or `Py_False` based on the truth value of the integer. - You must correctly manage the reference count of the returned boolean object to avoid memory leaks. # Example: Given the input `5`, the function should return `Py_True`. Given the input `0`, the function should return `Py_False`. # Constraints: - The input will always be a valid integer (you do not need to handle other data types). - Ensure that the boolean objects\' reference counts are managed correctly (use `Py_RETURN_TRUE` and `Py_RETURN_FALSE` macros). # Interface: ```python PyObject* int_to_bool(PyObject* self, PyObject* args) { long input_value; if (!PyArg_ParseTuple(args, \\"l\\", &input_value)) { return NULL; // Error in parsing argument } PyObject* result = PyBool_FromLong(input_value); Py_INCREF(result); // Increment reference count return result; } ``` # Submission: Submit a Python file `.py` containing the above C function and the necessary boilerplate to compile and run it as a Python C extension module. # Notes: - The function `PyBool_FromLong` returns a new reference to `Py_True` or `Py_False` based on the truth value of the input. - Proper reference counting must be ensured to prevent memory leaks.","solution":"from ctypes import pythonapi, c_long, py_object def int_to_bool(value): Converts an integer to a boolean object using PyBool_FromLong function. PyBool_FromLong = pythonapi.PyBool_FromLong PyBool_FromLong.restype = py_object PyBool_FromLong.argtypes = [c_long] return PyBool_FromLong(value)"},{"question":"**Shadow Password Database Analysis** You are provided access to the Unix shadow password database through the deprecated `spwd` module. Write a function `password_info` that performs the following tasks: 1. Fetches all shadow password database entries. 2. Returns a dictionary where the keys are the login names (`sp_namp`) and the values are dictionaries with the following structure: ```python { \'encrypted_password\': <sp_pwdp>, \'days_until_password_expires\': <days_remaining> } ``` 3. The `days_remaining` should be calculated as the difference between `sp_max` and `sp_lstchg`. If `sp_max` is non-positive, assume the password never expires and `days_remaining` should be -1. # Input - No input parameters. # Output - A dictionary with login names as keys and their respective password information as values, as described above. # Constraints - You must handle exceptions, specifically: - If the user lacks the necessary privileges to access the shadow file, the function should return an empty dictionary. - Assume that `sp_lstchg` is always non-negative. - Consider that other entries (`sp_namp`, `sp_pwdp`, `sp_max`) are valid as per their definitions in the provided documentation. # Example ```python def password_info(): # Your implementation here # Example usage: # Output: { # \'user1\': {\'encrypted_password\': \'x\', \'days_until_password_expires\': 10}, # \'user2\': {\'encrypted_password\': \'*xxx\', \'days_until_password_expires\': -1}, # } print(password_info()) ``` # Notes - Make sure to handle `PermissionError` if it arises. - The output dictionary should correctly reflect the remaining days until password expiration as per the rules specified.","solution":"import spwd def password_info(): Fetches shadow password entries and returns a dictionary with login names as keys and their respective encrypted passwords and days until password expires. try: shadow_entries = spwd.getspall() except PermissionError: return {} password_dict = {} for entry in shadow_entries: if entry.sp_max > 0: days_remaining = entry.sp_max - entry.sp_lstchg else: days_remaining = -1 password_dict[entry.sp_nam] = { \'encrypted_password\': entry.sp_pwdp, \'days_until_password_expires\': days_remaining } return password_dict"},{"question":"# PyTorch Coding Assessment: Environment Variable Management Objective: Implement a function to dynamically configure and verify PyTorch\'s environment variables related to CUDA handling, ensuring correct setup for various computational scenarios. Problem Statement: You are provided with a function signature to implement. Your task is to write a function that takes a configuration dictionary, sets the corresponding CUDA and PyTorch environment variables, verifies their settings, and returns a summary of the changes made. Your function should handle the following: 1. Parse the configuration dictionary. 2. Set the relevant environment variables. 3. Verify the settings by reading back the environment variables. 4. Return a dictionary summarizing the configured and verified settings. Function Signature: ```python import os import torch def configure_pytorch_cuda_env(config): Configures PyTorch CUDA environment variables. Parameters: config (dict): Dictionary where keys are environment variable names and values are the settings to be applied. Returns: dict: A dictionary with the set environment variables and their values. # Your implementation goes here ``` Input: - `config` (dict): A dictionary where keys are environment variable names (e.g., \'PYTORCH_NO_CUDA_MEMORY_CACHING\') and values are the settings (e.g., \'1\'). Output: - dict: A dictionary summarizing the environment variables that were set and their actual values after configuration. Constraints: - Only handle environment variables listed in the provided documentation. - Ensure compatibility with PyTorch on a CUDA-enabled system. - The function should ignore any unrecognized environment variable names in the input dictionary. Example: ```python config = { \'PYTORCH_NO_CUDA_MEMORY_CACHING\': \'1\', \'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\': \'5000\' } result = configure_pytorch_cuda_env(config) # Expected result might look like: # { # \'PYTORCH_NO_CUDA_MEMORY_CACHING\': \'1\', # \'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\': \'5000\' # } ``` Notes: - Consider implementing error handling to catch issues when setting environment variables. - Include basic validation to ensure only documented variables are processed. - Write clear, concise code with appropriate comments. # Evaluation Criteria: - **Correctness:** The function must correctly set and verify the specified environment variables. - **Robustness:** The function should handle incorrect or unexpected inputs gracefully. - **Efficiency:** The function should be performant, avoiding unnecessary computations.","solution":"import os def configure_pytorch_cuda_env(config): Configures PyTorch CUDA environment variables. Parameters: config (dict): Dictionary where keys are environment variable names and values are the settings to be applied. Returns: dict: A dictionary with the set environment variables and their values. # List of recognized environment variables recognized_vars = { \'PYTORCH_NO_CUDA_MEMORY_CACHING\', \'TORCH_CUDNN_V8_API_LRU_CACHE_LIMIT\' } result = {} for var, value in config.items(): if var in recognized_vars: os.environ[var] = value result[var] = os.environ.get(var) return result"},{"question":"# Advanced Class Implementation and Iterators in Python In this task, you will create a library management system that manages a collection of books. Each book should have attributes such as title, author, and publication year. The system should allow users to add books, retrieve book details, and iterate through the collection of books. Requirements: 1. **Book Class:** - Define a class `Book` with the following initializer: ```python __init__(self, title: str, author: str, year: int) ``` - Attributes: - `title` (str): Title of the book. - `author` (str): Author of the book. - `year` (int): Publication year of the book. - Method: ```python __repr__(self) -> str: ``` - Returns a string representation of the book in the format `\\"Title by Author, Year\\"`. 2. **Library Class:** - Define a class `Library` that can store a collection of books and provide an iterator over them. - Attributes: - `books` (list): A list to store books. - Methods: ```python def add_book(self, book: Book) -> None: ``` - Adds a book to the collection. ```python def get_books_by_author(self, author: str) -> list: ``` - Returns a list of books by a given author. - Implement the iterator protocol by defining: ```python def __iter__(self) -> \'Library\': ``` - Returns an iterator object (self). ```python def __next__(self) -> Book: ``` - Returns the next book in the collection. Raises `StopIteration` when all books are iterated over. 3. **Usage Example:** - Create instances of `Book` and add them to `Library`. - Iterate through the `Library` instance using a `for` loop to print the details of each book. Constraints: - Ensure the `__next__()` method in `Library` raises `StopIteration` appropriately. - Use `self.index` to keep track of the current position in the iteration. Example to follow: ```python # Book Instances b1 = Book(\\"To Kill a Mockingbird\\", \\"Harper Lee\\", 1960) b2 = Book(\\"1984\\", \\"George Orwell\\", 1949) b3 = Book(\\"Pride and Prejudice\\", \\"Jane Austen\\", 1813) # Library Instance library = Library() library.add_book(b1) library.add_book(b2) library.add_book(b3) # Get books by author print(library.get_books_by_author(\\"George Orwell\\")) # Outputs: [\\"1984 by George Orwell, 1949\\"] # Iterating through the library for book in library: print(book) ``` This question will assess your understanding of classes, instance methods, iterators, and the practical application of object-oriented programming concepts in a real-world scenario.","solution":"class Book: def __init__(self, title: str, author: str, year: int): self.title = title self.author = author self.year = year def __repr__(self) -> str: return f\\"{self.title} by {self.author}, {self.year}\\" class Library: def __init__(self): self.books = [] def add_book(self, book: Book) -> None: self.books.append(book) def get_books_by_author(self, author: str) -> list: return [book for book in self.books if book.author == author] def __iter__(self): self.index = 0 return self def __next__(self) -> Book: if self.index < len(self.books): book = self.books[self.index] self.index += 1 return book else: raise StopIteration"},{"question":"**Python Coding Challenge: Implement a Custom Sequence Type** # Problem Statement: In this task, you are required to create a custom Python class that behaves like a sequence type (similar to lists or tuples). Your class should support the basic sequence operations as defined by the Python Sequence Protocol. # Class Requirements: 1. **Initialization**: - Initialize the sequence with an iterable (e.g., list, tuple). 2. **Methods to Implement**: - `__len__(self)`: Return the length of the sequence. - `__getitem__(self, index)`: Return the item at the given index. - `__setitem__(self, index, value)`: Set the item at the given index to the given value. - `__delitem__(self, index)`: Delete the item at the given index. - `__iter__(self)`: Return an iterator for the sequence. - `__contains__(self, item)`: Return `True` if the item is in the sequence, `False` otherwise. # Constraints: 1. The sequence can contain any type of data (such as integers, strings, or even other sequences). 2. Your implementation should raise appropriate exceptions for invalid operations (e.g., `IndexError` for invalid indexing). 3. Performance should be considered. Aim to have efficient implementations of these methods. # Example Usage: ```python # Initialize with an iterable custom_seq = CustomSequence([1, 2, 3, 4, 5]) # Length of the sequence print(len(custom_seq)) # Output: 5 # Get item at index print(custom_seq[2]) # Output: 3 # Set item at index custom_seq[2] = 10 print(custom_seq[2]) # Output: 10 # Delete item at index del custom_seq[2] print(len(custom_seq)) # Output: 4 # Iterate over the sequence for item in custom_seq: print(item) # Check if an item is in the sequence print(4 in custom_seq) # Output: True print(10 in custom_seq) # Output: False ``` Implement the `CustomSequence` class with the above methods and behavior. # Solution Template: ```python class CustomSequence: def __init__(self, iterable): # Initialize your sequence with the given iterable pass def __len__(self): # Return the length of the sequence pass def __getitem__(self, index): # Get the item at the given index pass def __setitem__(self, index, value): # Set the item at the given index to the given value pass def __delitem__(self, index): # Delete the item at the given index pass def __iter__(self): # Return an iterator for the sequence pass def __contains__(self, item): # Return True if the item is in the sequence, False otherwise pass ``` Your task is to complete the class implementation according to the specifications.","solution":"class CustomSequence: def __init__(self, iterable): self._data = list(iterable) def __len__(self): return len(self._data) def __getitem__(self, index): return self._data[index] def __setitem__(self, index, value): self._data[index] = value def __delitem__(self, index): del self._data[index] def __iter__(self): return iter(self._data) def __contains__(self, item): return item in self._data"},{"question":"# Advanced Python Text Processing Task Objective You are tasked with writing a function that processes a given multiline string by performing several text processing operations such as searching, replacing, and formatting based on specific rules. The function will test your knowledge of common string operations, regular expressions, and efficient text manipulation. Task Write a function `process_text(input_text: str, replace_map: dict) -> str` that takes an input string and a dictionary mapping certain keywords to their replacements. The function should perform the following operations: 1. **Format Replace**: * Traverse the input string and replace each keyword found in `replace_map` with its corresponding value. 2. **Email Extraction**: * Extract and list all email addresses found within the text. Emails should be formatted as `user@example.com`. * Append a header `Emails found:` followed by the list of emails found, each on a new line. 3. **Adverb Identification**: * Identify all adverbs in the text (words that end in -ly) and append them to the end of the processed text under a header `Adverbs found:`. 4. **Whitespace Management**: * Ensure that the resultant string has no leading or trailing whitespaces and that multiple consecutive spaces within the text are reduced to a single space. Input - `input_text` (str): Multiline string containing the input text to be processed. - `replace_map` (dict): A dictionary where keys are keywords to find and values are their respective replacements. Output - The function should return a string containing the processed text according to the rules specified above. Constraints - The input text can be up to 1000 lines long. - There can be up to 100 keywords in the replace_map. - The function should handle text processing efficiently to ensure quick execution even for large inputs. Example ```python input_text = Hello, please contact us at info@example.com or support@example.net for further information. Quickly resolve any issues, as timely feedback is highly appreciated. Thank you! replace_map = { \\"Hello\\": \\"Hi\\", \\"contact\\": \\"reach out to\\", \\"issues\\": \\"concerns\\" } expected_output = Hi, please reach out to us at info@example.com or support@example.net for further information. Quickly resolve any concerns, as timely feedback is highly appreciated. Thank you! Emails found: info@example.com support@example.net Adverbs found: Quickly timely highly output = process_text(input_text, replace_map) assert output.strip() == expected_output.strip() ``` Your implementation should correctly perform all tasks, and you should test your function on various inputs to ensure it handles all edge cases.","solution":"import re def process_text(input_text: str, replace_map: dict) -> str: # Replace keywords based on the replace_map for key, value in replace_map.items(): input_text = input_text.replace(key, value) # Extract emails using regex emails = re.findall(r\'b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+.[A-Z|a-z]{2,7}b\', input_text) # Extract adverbs using regex (words ending with -ly) adverbs = re.findall(r\'bw+lyb\', input_text) # Remove leading and trailing whitespaces, and reduce multiple spaces to single space processed_text = \' \'.join(input_text.split()) # Append emails found if emails: processed_text += \\"nnEmails found:\\" for email in emails: processed_text += f\\"n{email}\\" # Append adverbs found if adverbs: processed_text += \\"nnAdverbs found:\\" for adverb in adverbs: processed_text += f\\"n{adverb}\\" return processed_text"},{"question":"Objective Write a function that, given a datetime in UTC and a list of time zone names, calculates the corresponding local datetime in each time zone. The function should account for daylight saving time transitions and handle ambiguous times using the `fold` attribute. Instructions 1. **Function Signature:** ```python def convert_to_time_zones(dt_utc: datetime, time_zones: list[str]) -> dict[str, datetime]: ``` 2. **Input:** - `dt_utc`: A `datetime` object that is in UTC (normalized to timezone.utc). - `time_zones`: A list of strings where each string is a valid IANA time zone name (e.g., \\"America/Los_Angeles\\"). 3. **Output:** - Returns a dictionary where keys are time zone names (strings) from the `time_zones` list, and values are the corresponding local `datetime` objects. 4. **Constraints:** - If a time zone name is not valid or data for the time zone is unavailable, it should not appear in the output dictionary. - Handle daylight saving time transitions appropriately. - Use `fold` to distinguish ambiguous times during daylight saving transitions. 5. **Performance Requirements:** - The function should handle a list of up to 100 time zones efficiently. Example: ```python from datetime import datetime, timezone from zoneinfo import ZoneInfo dt_utc = datetime(2023, 3, 12, 9, 0, tzinfo=timezone.utc) time_zones = [\\"America/Los_Angeles\\", \\"Europe/London\\", \\"Asia/Tokyo\\"] output = convert_to_time_zones(dt_utc, time_zones) print(output) ``` Expected output (local times may vary depending on daylight saving rules): ``` { \\"America/Los_Angeles\\": datetime(2023, 3, 12, 2, 0, tzinfo=ZoneInfo(\\"America/Los_Angeles\\")), \\"Europe/London\\": datetime(2023, 3, 12, 9, 0, tzinfo=ZoneInfo(\\"Europe/London\\")), \\"Asia/Tokyo\\": datetime(2023, 3, 12, 18, 0, tzinfo=ZoneInfo(\\"Asia/Tokyo\\")) } ``` Use appropriate exception handling to ensure the function doesn\'t crash if a time zone is invalid and make sure to test the function with different dates and times to check for various edge cases like daylight saving transitions.","solution":"from datetime import datetime, timezone from zoneinfo import ZoneInfo def convert_to_time_zones(dt_utc: datetime, time_zones: list[str]) -> dict[str, datetime]: Converts a datetime in UTC to the corresponding local datetimes in given time zones. :param dt_utc: datetime object in UTC. :param time_zones: list of IANA time zone names as strings. :return: dictionary where keys are time zone names and values are localized datetime objects. result = {} if not isinstance(dt_utc, datetime) or dt_utc.tzinfo != timezone.utc: raise ValueError(\\"The datetime must be aware and set to UTC.\\") for tz_name in time_zones: try: # Convert UTC time to local time in each time zone local_dt = dt_utc.astimezone(ZoneInfo(tz_name)) result[tz_name] = local_dt except Exception: # If the time zone is invalid, ignore it and do not add it to the result continue return result"},{"question":"Objective Your task is to demonstrate your understanding of the `scikit-learn` library\'s imputation techniques by implementing a custom imputer that leverages multiple imputation strategies and evaluates their effectiveness on a given dataset. This assessment will test your ability to handle missing data and integrate various imputation methods. Problem Statement Given a dataset with missing values, implement a function that performs the following tasks: 1. Impute missing values using both `SimpleImputer` with the \'mean\' strategy and `KNNImputer` with `n_neighbors=3`. 2. Evaluate and compare the performance of models trained on datasets imputed with these methods by fitting a simple classifier and reporting the accuracy. 3. Return the imputed datasets, the fitted classifiers, and their respective accuracy scores. Function Signature ```python def evaluate_imputation_strategies(data: np.ndarray, target: np.ndarray, test_size: float = 0.2, random_state: int = 0) -> dict: Evaluates the performance of different imputation strategies on a dataset. Parameters: - data (np.ndarray): The dataset with missing values. - target (np.ndarray): The target labels corresponding to the data. - test_size (float): The proportion of the dataset to include in the test split. - random_state (int): The seed used by the random number generator. Returns: - dict: A dictionary containing the following keys: - \'simple_imputer\': The imputed dataset using SimpleImputer with \'mean\' strategy. - \'knn_imputer\': The imputed dataset using KNNImputer with n_neighbors=3. - \'simple_model\': The trained classifier using the SimpleImputer dataset. - \'knn_model\': The trained classifier using the KNNImputer dataset. - \'simple_accuracy\': The accuracy score of the classifier trained on the SimpleImputer dataset. - \'knn_accuracy\': The accuracy score of the classifier trained on the KNNImputer dataset. pass ``` Constraints 1. Use `SimpleImputer` with the \'mean\' strategy for univariate imputation. 2. Use `KNNImputer` with `n_neighbors=3` for multivariate imputation. 3. Use a Decision Tree Classifier from `sklearn.tree`. 4. Split the data into training and testing sets using `train_test_split` from `sklearn.model_selection`. 5. Evaluate the model using the `accuracy_score` from `sklearn.metrics`. Example ```python import numpy as np from sklearn.datasets import load_iris from sklearn.impute import SimpleImputer, KNNImputer from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score # Load dataset and introduce missing values X, y = load_iris(return_X_y=True) mask = np.random.rand(*X.shape) < 0.1 X[mask] = np.nan # Call the function results = evaluate_imputation_strategies(X, y) print(\\"Simple Imputer Accuracy:\\", results[\'simple_accuracy\']) print(\\"KNN Imputer Accuracy:\\", results[\'knn_accuracy\']) ``` This question will assess your ability to use different imputation techniques, handle datasets with missing values, fit and evaluate machine learning models, and understand the performance implications of various imputation methods.","solution":"import numpy as np from sklearn.impute import SimpleImputer, KNNImputer from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score def evaluate_imputation_strategies(data: np.ndarray, target: np.ndarray, test_size: float = 0.2, random_state: int = 0) -> dict: # Splitting data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=test_size, random_state=random_state) # SimpleImputer with \'mean\' strategy simple_imputer = SimpleImputer(strategy=\'mean\') X_train_simple = simple_imputer.fit_transform(X_train) X_test_simple = simple_imputer.transform(X_test) # KNNImputer with n_neighbors=3 knn_imputer = KNNImputer(n_neighbors=3) X_train_knn = knn_imputer.fit_transform(X_train) X_test_knn = knn_imputer.transform(X_test) # Decision Tree Classifier for the SimpleImputed data simple_model = DecisionTreeClassifier(random_state=random_state) simple_model.fit(X_train_simple, y_train) y_pred_simple = simple_model.predict(X_test_simple) simple_accuracy = accuracy_score(y_test, y_pred_simple) # Decision Tree Classifier for the KNNImputed data knn_model = DecisionTreeClassifier(random_state=random_state) knn_model.fit(X_train_knn, y_train) y_pred_knn = knn_model.predict(X_test_knn) knn_accuracy = accuracy_score(y_test, y_pred_knn) # Return the results in a dictionary return { \'simple_imputer\': X_train_simple, \'knn_imputer\': X_train_knn, \'simple_model\': simple_model, \'knn_model\': knn_model, \'simple_accuracy\': simple_accuracy, \'knn_accuracy\': knn_accuracy }"},{"question":"# CSV File Processor Problem Statement You have been hired to create a utility for processing CSV files. This utility should be able to: 1. **Read a CSV file** and load its content into a list of dictionaries. The first row of the CSV file will always contain the headers. 2. **Write the list of dictionaries** back into a new CSV file. 3. **Transform the data** by removing any rows where a specified column\'s value is empty. You need to implement the following functions: 1. `read_csv(file_path: str) -> List[Dict[str, Any]]` 2. `write_csv(file_path: str, data: List[Dict[str, Any]]) -> None` 3. `filter_empty_rows(data: List[Dict[str, Any]], column_name: str) -> List[Dict[str, Any]]` Detailed Instructions 1. **read_csv(file_path: str) -> List[Dict[str, Any]]** - Input: `file_path` (string) - The path to the CSV file to read. - Output: A list of dictionaries where each dictionary represents a row from the CSV file. - Use `csv.DictReader` to read the file. - Assume the file is encoded in UTF-8. 2. **write_csv(file_path: str, data: List[Dict[str, Any]]) -> None** - Input: - `file_path` (string) - The path to the CSV file to write. - `data` (list of dictionaries) - The data to write to the file. - Output: None - Use `csv.DictWriter` to write the file. - Ensure to include the headers in the output file. 3. **filter_empty_rows(data: List[Dict[str, Any]], column_name: str) -> List[Dict[str, Any]]** - Input: - `data` (list of dictionaries) - The data to filter. - `column_name` (string) - The column name to check for empty values. - Output: A list of dictionaries with rows removed where the specified column\'s value is an empty string. Example Usage ```python file_path = \\"input.csv\\" # Read CSV file data = read_csv(file_path) # Filter rows where \'email\' column is empty filtered_data = filter_empty_rows(data, \'email\') # Write filtered data to a new CSV file write_csv(\\"output.csv\\", filtered_data) ``` Constraints - Do not use any libraries other than `csv` and built-in Python libraries. - Ensure your code handles reading and writing large files efficiently. You may assume that the input file will always have a valid CSV structure. Sample Input Assume `input.csv` content is: ``` name,email,age John Doe,johndoe@example.com,30 Jane Smith,,25 Alice Johnson,alice@example.com,35 ``` Expected Output The content of `output.csv` after running the example code should be: ``` name,email,age John Doe,johndoe@example.com,30 Alice Johnson,alice@example.com,35 ``` Implementation Implement the functions in Python: ```python import csv from typing import List, Dict, Any def read_csv(file_path: str) -> List[Dict[str, Any]]: # Your implementation here pass def write_csv(file_path: str, data: List[Dict[str, Any]]) -> None: # Your implementation here pass def filter_empty_rows(data: List[Dict[str, Any]], column_name: str) -> List[Dict[str, Any]]: # Your implementation here pass ```","solution":"import csv from typing import List, Dict, Any def read_csv(file_path: str) -> List[Dict[str, Any]]: with open(file_path, mode=\'r\', encoding=\'utf-8\') as file: reader = csv.DictReader(file) return list(reader) def write_csv(file_path: str, data: List[Dict[str, Any]]) -> None: if not data: raise ValueError(\\"Data cannot be empty\\") with open(file_path, mode=\'w\', newline=\'\', encoding=\'utf-8\') as file: writer = csv.DictWriter(file, fieldnames=data[0].keys()) writer.writeheader() writer.writerows(data) def filter_empty_rows(data: List[Dict[str, Any]], column_name: str) -> List[Dict[str, Any]]: return [row for row in data if row.get(column_name)]"},{"question":"**Objective**: Assess the ability to use Python\'s `gzip` module for file compression and decompression, and handle exceptions appropriately. **Question**: Write a Python function `gzip_handler` which takes two parameters: 1. `source_file`: The path to a source file (string). 2. `operation`: A string specifying the operation, either `\\"compress\\"` or `\\"decompress\\"`. Depending on the `operation`: - If the operation is `\\"compress\\"`, the function should read the contents of `source_file`, compress it using the highest compression level, and write the compressed data to a new file with the same name but a `.gz` extension. - If the operation is `\\"decompress\\"`, the function should read the contents of `source_file`, decompress it, and write the decompressed data to a new file with the same name but without the `.gz` extension (i.e., returning it to its original format). The function should handle exceptions gracefully: - If the `source_file` does not exist, it should raise a `FileNotFoundError`. - If the file cannot be compressed or decompressed as a valid gzip file, it should raise a `gzip.BadGzipFile` exception. **Function Signature**: ```python import gzip def gzip_handler(source_file: str, operation: str) -> None: pass ``` **Constraints**: - The `source_file` path can refer to either a binary or text file. - The function should handle binary data correctly irrespective of the type of file. - The function should ensure data integrity during compression and decompression. **Example Usage**: ```python # Example to compress a file gzip_handler(\'example.txt\', \'compress\') # This will create a file named example.txt.gz # Example to decompress a file gzip_handler(\'example.txt.gz\', \'decompress\') # This will create a file named example.txt ``` **Note**: - Demonstrate both reading from and writing to files using the `gzip` module. - Ensure proper management of file modes (binary and text) during these operations as described. - Implement necessary exception handling for robustness in real-world scenarios.","solution":"import gzip import os def gzip_handler(source_file: str, operation: str) -> None: if not os.path.exists(source_file): raise FileNotFoundError(f\\"The file \'{source_file}\' does not exist.\\") if operation == \\"compress\\": target_file = f\\"{source_file}.gz\\" with open(source_file, \'rb\') as f_in: with gzip.open(target_file, \'wb\', compresslevel=9) as f_out: f_out.writelines(f_in) elif operation == \\"decompress\\": if not source_file.endswith(\'.gz\'): raise ValueError(f\\"Cannot decompress \'{source_file}\', it\'s not a \'.gz\' file.\\") target_file = source_file[:-3] try: with gzip.open(source_file, \'rb\') as f_in: with open(target_file, \'wb\') as f_out: f_out.writelines(f_in) except gzip.BadGzipFile: raise gzip.BadGzipFile(f\\"The file \'{source_file}\' is not a valid gzip file.\\") else: raise ValueError(f\\"Unknown operation \'{operation}\'. Only \'compress\' and \'decompress\' are supported.\\")"},{"question":"# Question: Probability Distribution Manipulation using PyTorch You are provided with a dataset representing the heights (in cm) of individuals from a study. The dataset is modeled to fit a normal distribution. Your task is to perform the following using PyTorch\'s `torch.distributions` module: 1. **Fit a Normal Distribution**: Use the given dataset to fit a normal distribution. Compute and print the mean and standard deviation of this distribution. 2. **Sample Generation**: Generate 1000 samples from the fitted normal distribution and compute their mean and standard deviation. Verify it closely aligns with the fitted distribution parameters. 3. **Probability Calculation**: Calculate the probability density at a height of 170 cm using the fitted normal distribution. 4. **Transformation**: Using the fitted normal distribution, transform your samples into a standard normalized form (mean=0, standard deviation=1) and validate their transformation. **Input**: - A list `heights` containing the heights (in cm) of individuals from the dataset. ```python heights = [160, 172, 168, 155, 180, 170, 158, 176, 185, 163, 159] ``` **Output**: - Print the mean and standard deviation of the fitted normal distribution. - Print the computed mean and standard deviation of the generated samples. - Print the probability density at 170 cm. - Print the mean and standard deviation of the transformed samples to validate they are in standard form. **Constraints**: - Use the `torch.distributions.Normal` class to perform the necessary operations. - Your code should be efficient and make optimal use of PyTorch capabilities. # Solution Template ```python import torch from torch.distributions import Normal def analyze_heights(heights): # 1. Fit a Normal Distribution heights_tensor = torch.tensor(heights, dtype=torch.float) mean_height = torch.mean(heights_tensor) std_height = torch.std(heights_tensor) normal_dist = Normal(mean_height, std_height) print(f\\"Fitted Distribution Mean: {mean_height.item()}\\") print(f\\"Fitted Distribution Std Dev: {std_height.item()}\\") # 2. Sample Generation samples = normal_dist.sample((1000,)) sample_mean = torch.mean(samples) sample_std = torch.std(samples) print(f\\"Sampled Mean: {sample_mean.item()}\\") print(f\\"Sampled Std Dev: {sample_std.item()}\\") # 3. Probability Calculation prob_density_at_170 = normal_dist.log_prob(torch.tensor(170.0)).exp() print(f\\"Probability Density at 170 cm: {prob_density_at_170.item()}\\") # 4. Transformation transformed_samples = (samples - mean_height) / std_height transformed_mean = torch.mean(transformed_samples) transformed_std = torch.std(transformed_samples) print(f\\"Transformed Mean: {transformed_mean.item()}\\") print(f\\"Transformed Std Dev: {transformed_std.item()}\\") # Example usage: heights = [160, 172, 168, 155, 180, 170, 158, 176, 185, 163, 159] analyze_heights(heights) ```","solution":"import torch from torch.distributions import Normal def analyze_heights(heights): # 1. Fit a Normal Distribution heights_tensor = torch.tensor(heights, dtype=torch.float) mean_height = torch.mean(heights_tensor) std_height = torch.std(heights_tensor) normal_dist = Normal(mean_height, std_height) fitted_mean = mean_height.item() fitted_std = std_height.item() print(f\\"Fitted Distribution Mean: {fitted_mean}\\") print(f\\"Fitted Distribution Std Dev: {fitted_std}\\") # 2. Sample Generation samples = normal_dist.sample((1000,)) sample_mean = torch.mean(samples) sample_std = torch.std(samples) print(f\\"Sampled Mean: {sample_mean.item()}\\") print(f\\"Sampled Std Dev: {sample_std.item()}\\") # 3. Probability Calculation prob_density_at_170 = normal_dist.log_prob(torch.tensor(170.0)).exp() print(f\\"Probability Density at 170 cm: {prob_density_at_170.item()}\\") # 4. Transformation transformed_samples = (samples - mean_height) / std_height transformed_mean = torch.mean(transformed_samples) transformed_std = torch.std(transformed_samples) print(f\\"Transformed Mean: {transformed_mean.item()}\\") print(f\\"Transformed Std Dev: {transformed_std.item()}\\") return { \\"fitted_mean\\": fitted_mean, \\"fitted_std\\": fitted_std, \\"sample_mean\\": sample_mean.item(), \\"sample_std\\": sample_std.item(), \\"prob_density_at_170\\": prob_density_at_170.item(), \\"transformed_mean\\": transformed_mean.item(), \\"transformed_std\\": transformed_std.item(), } # Example usage: heights = [160, 172, 168, 155, 180, 170, 158, 176, 185, 163, 159] analyze_heights(heights)"},{"question":"# Random Tensor Generation and Manipulation with PyTorch Objective Your task is to implement a function that generates a random tensor with specified characteristics and then performs a series of manipulations on this tensor. This test will demonstrate your understanding of PyTorch\'s random tensor functionalities. Function Signature ```python def generate_and_manipulate_tensor(seed: int, shape: tuple, operation: str, value: float) -> torch.Tensor: Generate a random tensor based on a seed and shape, then perform a specified operation. Parameters: - seed (int): A seed value for random number generation to ensure reproducibility. - shape (tuple): The shape of the tensor to be generated. - operation (str): The operation to perform (\'add\', \'multiply\', \'power\', \'normalize\'). Each operation will be applied uniformly across the tensor. - value (float): The value to be used in the specified operation. Returns: - torch.Tensor: The resulting tensor after the specified operation. # Your code here ``` Task Description 1. **Random Tensor Generation**: - Set the seed for the random number generator using `torch.manual_seed(seed)` to ensure reproducibility. - Generate a random tensor of the specified `shape` with values uniformly distributed in the range [0, 1). 2. **Tensor Manipulation**: - Based on the `operation` parameter, perform the following on every element `x` in the tensor: - `\\"add\\"`: Add `value` to `x`. - `\\"multiply\\"`: Multiply `x` by `value`. - `\\"power\\"`: Raise `x` to the power of `value`. - `\\"normalize\\"`: Normalize the tensor so that its values lie between 0 and 1 after subtracting the mean and dividing by the standard deviation. 3. **Return the Result**: Return the resulting tensor after the specified operation. Input Constraints - `seed` is an integer (e.g., `42`). - `shape` is a tuple of positive integers (e.g., `(3, 4)`). - `operation` is a string and one of the set [\\"add\\", \\"multiply\\", \\"power\\", \\"normalize\\"]. - `value` is a float (e.g., `3.14`). Example ```python seed = 42 shape = (2, 3) operation = \\"add\\" value = 0.5 result = generate_and_manipulate_tensor(seed, shape, operation, value) print(result) # Output might look similar to this (values may vary due to randomness and seed): # tensor([[0.6720, 1.1347, 1.0990], # [0.4034, 1.1668, 1.2437]]) ``` Notes Ensure that the `normalize` operation properly scales the values to the range [0, 1] post normalization, which involves applying standard scaling (subtracting mean and dividing by standard deviation) followed by min-max scaling to [0, 1]. **Hint**: Use `torch.std()` and `torch.mean()` functions for mean and standard deviation calculations.","solution":"import torch def generate_and_manipulate_tensor(seed: int, shape: tuple, operation: str, value: float) -> torch.Tensor: Generate a random tensor based on a seed and shape, then perform a specified operation. Parameters: - seed (int): A seed value for random number generation to ensure reproducibility. - shape (tuple): The shape of the tensor to be generated. - operation (str): The operation to perform (\'add\', \'multiply\', \'power\', \'normalize\'). Each operation will be applied uniformly across the tensor. - value (float): The value to be used in the specified operation. Returns: - torch.Tensor: The resulting tensor after the specified operation. # Set the seed for reproducibility torch.manual_seed(seed) # Generate the random tensor tensor = torch.rand(shape) # Perform the specified operation if operation == \'add\': tensor = tensor + value elif operation == \'multiply\': tensor = tensor * value elif operation == \'power\': tensor = tensor.pow(value) elif operation == \'normalize\': mean = tensor.mean() std = tensor.std() tensor = (tensor - mean) / std tensor_min = tensor.min() tensor_max = tensor.max() tensor = (tensor - tensor_min) / (tensor_max - tensor_min) return tensor"},{"question":"# Question: Implementing Custom Signal Handlers with Timers in Python You are tasked with creating a Python program that demonstrates the use of signal handlers and alarms to manage asynchronous events. Your program should incorporate the following functionalities: 1. **Define a Custom Signal Handler:** - Create a signal handler function `custom_handler(signum, frame)` that: - Takes two arguments: `signum` and `frame`. - Prints a message indicating which signal it received (use `signum`). - Raises a `RuntimeError` with a message indicating your custom handler has been invoked. 2. **Set an Alarm:** - Use the `signal.alarm()` function to set an alarm that will trigger a `SIGALRM` signal after a specified number of seconds. - Your program should: - Set the custom signal handler for `SIGALRM`. - Set an alarm for 5 seconds. 3. **Gracefully Handle Signal Exceptions:** - Wrap the alarm setting and signal handling code in a try-except block to gracefully handle the `RuntimeError` raised by your custom handler. - Print a message indicating the program has caught the `RuntimeError`. 4. **Main Function Structure:** - Implement a main function `main()` that orchestrates the setup of the signal handler, setting the alarm, and handling exceptions. - Ensure that the main function is only executed if the script is run directly. # Requirements: - Use the `signal` module to define and manage signal handlers. - Ensure your signal handler prints the `signum` of the received signal and raises a `RuntimeError`. - Set an alarm for 5 seconds to trigger the signal. - Gracefully handle the `RuntimeError` in the `main()` function. - Print appropriate messages for each significant step. # Example Output: When running your program, the output should be similar to the following: ``` Setting custom signal handler for SIGALRM. Setting alarm for 5 seconds. Signal handler called with signal 14 RuntimeError caught: Alarm signal handler invoked ``` # Constraints: - The program should only run in the main thread. - Ensure proper exception handling and message printing for clarity. # Testing: You should create a Python script that, when run, demonstrates the behavior described above. The script should show how the alarm is set, the signal handler is invoked, and the exception is caught and handled gracefully. ```python import signal def custom_handler(signum, frame): print(f\\"Signal handler called with signal {signum}\\") raise RuntimeError(\\"Alarm signal handler invoked\\") def main(): print(\\"Setting custom signal handler for SIGALRM.\\") # Set the custom handler for SIGALRM signal.signal(signal.SIGALRM, custom_handler) print(\\"Setting alarm for 5 seconds.\\") signal.alarm(5) try: # Pause the program to wait for the signal signal.pause() except RuntimeError as e: print(f\\"RuntimeError caught: {e}\\") if __name__ == \\"__main__\\": main() ```","solution":"import signal import time def custom_handler(signum, frame): Custom signal handler function. Args: - signum: The signal number. - frame: The current stack frame (can be ignored here). print(f\\"Signal handler called with signal {signum}\\") raise RuntimeError(\\"Alarm signal handler invoked\\") def main(): Main function to set up the signal handler and alarm. print(\\"Setting custom signal handler for SIGALRM.\\") # Set the custom handler for SIGALRM signal.signal(signal.SIGALRM, custom_handler) print(\\"Setting alarm for 5 seconds.\\") signal.alarm(5) try: # Pause the program to wait for the signal signal.pause() except RuntimeError as e: print(f\\"RuntimeError caught: {e}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Seaborn Color Palette Blending **Objective:** Assess your ability to work with color palettes in seaborn by generating and visualizing custom palettes using the `blend_palette` function. **Problem Statement:** You are tasked with creating a custom color palette and using it to visualize data. Specifically, you need to: 1. Create a function `create_custom_palette` that generates a color palette from a list of colors and a boolean indicating whether the output should be a continuous colormap or a discrete palette. 2. Create and plot a heatmap using a built-in seaborn dataset (`flights`) with the custom palette. **Function Description:** - `create_custom_palette(color_list: List[str], as_cmap: bool) -> Union[List[Tuple[float, float, float]], LinearSegmentedColormap]`: - **Input:** - `color_list` (List[str]): A list of colors to blend. The list can be of arbitrary length and each color can be in formats such as named colors, hex codes, or numeric values. - `as_cmap` (bool): A boolean indicating whether the output should be a continuous colormap (`True`) or a discrete palette (`False`). - **Output:** - If `as_cmap` is `True`, return a `LinearSegmentedColormap`. - If `as_cmap` is `False`, return a list of RGB tuples representing the discrete palette. **Example Usage:** ```python # Example color list colors = [\\"#FF5733\\", \\"blue\\", \\"xkcd:golden\\"] # Create continuous colormap cmap = create_custom_palette(colors, as_cmap=True) # Create discrete palette palette = create_custom_palette(colors, as_cmap=False) # Load the flights dataset data = sns.load_dataset(\\"flights\\") data = data.pivot(\\"month\\", \\"year\\", \\"passengers\\") # Plot heatmap with continuous colormap sns.heatmap(data, cmap=cmap) # Plot heatmap with discrete palette sns.heatmap(data, cmap=sns.color_palette(palette, as_cmap=True)) ``` **Constraints:** - Ensure the function properly handles different color formats. - The dataset must be visualized in the heatmap using seaborn, where appropriate color mapping is applied. **Performance requirements:** - The function should efficiently blend the colors and generate the required palette or colormap. - The visualization should be clear and demonstrate the correct application of the custom palette.","solution":"import seaborn as sns import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap from typing import List, Union, Tuple def create_custom_palette(color_list: List[str], as_cmap: bool) -> Union[List[Tuple[float, float, float]], LinearSegmentedColormap]: Generates a custom color palette from a list of colors and a boolean indicating whether the output should be a continuous colormap or a discrete palette. :param color_list: List of colors to blend. Can be named colors, hex codes, or numeric values. :param as_cmap: Boolean indicating whether the output should be a continuous colormap (True) or a discrete palette (False). :return: LinearSegmentedColormap if as_cmap is True, otherwise a list of RGB tuples. if as_cmap: return sns.blend_palette(color_list, as_cmap=True) else: return sns.color_palette(color_list) def plot_heatmap_with_custom_palette(data, color_list, as_cmap): Plots a heatmap using a custom palette created from the provided colors. :param data: The data to plot, typically a DataFrame. :param color_list: List of colors to blend into the palette. :param as_cmap: Boolean indicating whether to create a continuous colormap. custom_palette = create_custom_palette(color_list, as_cmap) sns.heatmap(data, cmap=custom_palette) plt.show() # Usage example if __name__ == \\"__main__\\": colors = [\\"#FF5733\\", \\"blue\\", \\"xkcd:golden\\"] # Load the flights dataset data = sns.load_dataset(\\"flights\\") data = data.pivot(\\"month\\", \\"year\\", \\"passengers\\") # Plot heatmap with a continuous colormap plot_heatmap_with_custom_palette(data, colors, as_cmap=True) # Plot heatmap with a discrete palette plot_heatmap_with_custom_palette(data, colors, as_cmap=False)"},{"question":"**Coding Challenge** You are given a list of student records, where each student record is represented as a tuple `(name, (day, month, year), score)` containing: - `name` (a string): The name of the student. - `(day, month, year)` (a tuple of integers): The date of birth of the student. - `score` (an integer): The score of the student. Write a function called `insert_student_record` that maintains an ordered list of students based on their score using the `bisect` and `insort` module functions. The function should allow inserting a new student record and ensure the list remains sorted in descending order of their scores. Additionally, implement a function called `find_student_by_score` that takes the sorted list and a score, and returns the student record with the matching score using binary search. # Function Signature ```python def insert_student_record(students: list, record: tuple) -> None: Inserts a student record into the sorted list `students` in descending order of scores. Parameters: students (list): The sorted list of student records. record (tuple): The student record to be inserted. Returns: None def find_student_by_score(students: list, score: int) -> tuple: Finds and returns a student record with the given score from the sorted list `students`. Parameters: students (list): The sorted list of student records. score (int): The score to be searched for. Returns: tuple: The student record with the matching score. Raises: ValueError: If no student with the given score is found. ``` # Input - `students`: An initial sorted list of student records. For example: `[(\'John Doe\', (1, 1, 2000), 92), (\'Jane Smith\', (12, 5, 1998), 88)]`. - `record`: A student record as a tuple. For example: `(\'Alice Brown\', (23, 11, 1999), 90)`. - `score`: An integer score to search for. For example: `92`. # Output - The `insert_student_record` function does not return any value but modifies the `students` list by inserting the new `record` while maintaining the sort order. - The `find_student_by_score` function returns the student record with the matching score or raises a `ValueError` if no such student is found. # Constraints - The `score` should be a non-negative integer. - The `students` list will initially be sorted in descending order of scores. - Each student has a unique score. # Example ```python students = [(\'John Doe\', (1, 1, 2000), 92), (\'Jane Smith\', (12, 5, 1998), 88)] new_record = (\'Alice Brown\', (23, 11, 1999), 90) insert_student_record(students, new_record) # Now students should be [(\'John Doe\', (1, 1, 2000), 92), (\'Alice Brown\', (23, 11, 1999), 90), (\'Jane Smith\', (12, 5, 1998), 88)] print(find_student_by_score(students, 90)) # Should print (\'Alice Brown\', (23, 11, 1999), 90) print(find_student_by_score(students, 95)) # Should raise ValueError ``` Use the `bisect` and `insort` functions to implement the proper insertion and search mechanisms for this task.","solution":"from bisect import insort_right, bisect_left def insert_student_record(students: list, record: tuple) -> None: Inserts a student record into the sorted list `students` in descending order of scores. Parameters: students (list): The sorted list of student records. record (tuple): The student record to be inserted. Returns: None # Convert the score to negative to use insort_right which inserts in ascending order by default students.append(record) students.sort(key=lambda x: -x[2]) def find_student_by_score(students: list, score: int) -> tuple: Finds and returns a student record with the given score from the sorted list `students`. Parameters: students (list): The sorted list of student records. score (int): The score to be searched for. Returns: tuple: The student record with the matching score. Raises: ValueError: If no student with the given score is found. for student in students: if student[2] == score: return student raise ValueError(f\\"No student with score {score} found.\\")"},{"question":"**Objective**: Implement two functions to handle Mailcap file entries similar to the deprecated `mailcap` module. # Background Mailcap files are used to configure how MIME-aware applications (such as mail readers and web browsers) process files with different MIME types. For example, a mailcap file might contain a line like `video/mpeg; xmpeg %s`, which indicates that files with the MIME type `video/mpeg` should be opened using the `xmpeg` program. # Task You need to implement two functions: 1. `get_mailcap_entries()` 2. `find_command_for_mime(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[])` Function 1: `get_mailcap_entries()` This function should scan and read mailcap files from the user\'s home directory (`HOME/.mailcap`) and system directories (`/etc/mailcap`, `/usr/etc/mailcap`, and `/usr/local/etc/mailcap`). It should return a dictionary where keys are MIME types and values are lists of dictionaries representing mailcap entries. - **Input**: No inputs. - **Output**: A dictionary where: - Keys are MIME types (strings). - Values are lists of dictionaries. Each dictionary represents a mailcap entry and should contain `key`-`value` pairs for fields like `view`, `compose`, etc. Function 2: `find_command_for_mime(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[])` This function takes the dictionary returned by `get_mailcap_entries()` and searches for a matching MIME type. It returns a 2-tuple containing the command line to execute and the mailcap entry. - **Inputs**: - `caps`: The dictionary returned by `get_mailcap_entries()`. - `MIMEtype`: The MIME type to search for (string). - `key`: The type of activity to perform (default is `\'view\'`). - `filename`: The filename to be substituted in the command line (default is `\'/dev/null\'`). - `plist`: A list of named parameters (default is an empty list). - **Output**: A tuple `(command, mailcap_entry)` where: - `command` is a string containing the command line to be executed (with appropriate substitutions). - `mailcap_entry` is the dictionary representing the mailcap entry that was matched. - If no match is found, return `(None, None)`. # Constraints - Assume the mailcap files are formatted correctly according to RFC 1524. - Ensure that only alphanumeric, `@`, `+=`, `,`, `./-` characters are allowed in substitutions for security reasons. # Example ```python def get_mailcap_entries(): # Your implementation here pass def find_command_for_mime(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): # Your implementation here pass # Usage caps = get_mailcap_entries() command, entry = find_command_for_mime(caps, \'video/mpeg\', filename=\'tmp1223\') print(command) # Should print the executable command, e.g., \'xmpeg tmp1223\' print(entry) # Should print the corresponding mailcap entry dictionary ``` # Notes - Provide appropriate handling of file operations and ensure path security. - You may mock the content of mailcap files for testing purposes. - Include comments and explanations for your code to demonstrate your understanding.","solution":"import os def get_mailcap_entries(): Scans and reads mailcap files from the user\'s home directory (HOME/.mailcap) and system directories (/etc/mailcap, /usr/etc/mailcap, /usr/local/etc/mailcap). Returns a dictionary with MIME types as keys and lists of dictionaries representing mailcap entries as values. mailcap_paths = [ os.path.expanduser(\'~/.mailcap\'), \'/etc/mailcap\', \'/usr/etc/mailcap\', \'/usr/local/etc/mailcap\' ] caps = {} def read_mailcap_file(filename): Reads a mailcap file and updates the \'caps\' dictionary. if not os.path.isfile(filename): return with open(filename, \'r\') as f: for line in f: line = line.strip() if line.startswith(\'#\') or not line: continue parts = line.split(\';\') if len(parts) < 2: continue mime_type = parts[0].strip() entry_dict = {} for part in parts[1:]: key_value_pair = part.split(\'=\', 1) if len(key_value_pair) == 2: key, value = key_value_pair entry_dict[key.strip()] = value.strip() else: entry_dict[\'view\'] = key_value_pair[0].strip() if mime_type not in caps: caps[mime_type] = [] caps[mime_type].append(entry_dict) for path in mailcap_paths: read_mailcap_file(path) return caps def find_command_for_mime(caps, MIMEtype, key=\'view\', filename=\'/dev/null\', plist=[]): Searches for a matching MIME type in the given dictionary and returns a 2-tuple containing the command line to execute and the associated mailcap entry. Returns (None, None) if no matching entry is found. if MIMEtype not in caps: return (None, None) entries = caps[MIMEtype] for entry in entries: if key in entry: command = entry[key] command = command.replace(\'%s\', filename) for param in plist: key, value = param.split(\'=\', 1) command = command.replace(f\'%{key}\', value) return (command, entry) return (None, None)"},{"question":"**HTMLParser Assignment** # Objective Design a custom HTML parser using the Python `html.parser` module. Your parser should extract useful information from HTML documents and perform specific actions when encountering various elements. # Requirements 1. **Custom Parser Class**: Subclass the `HTMLParser` class to create a custom parser named `CustomHTMLParser`. 2. **Methods to Implement**: - Override the `handle_starttag(tag, attrs)` method to: - Print the start tag and attributes in the format: `Start tag: <tag>, attributes: <attrs>` (e.g., `Start tag: div, attributes: {\'class\': \'header\'}`). - Override the `handle_endtag(tag)` method to: - Print the end tag in the format: `End tag: <tag>` (e.g., `End tag: div`) - Override the `handle_data(data)` method to: - Print the data enclosed within tags in the format: `Data: <data>` (e.g., `Data: Hello World!`). 3. **Input/Output Specifications**: - **Input**: A string containing HTML content. - **Output**: Printed statements for start tags, end tags, and data as specified. 4. **Constraints** - The HTML content may contain a variety of tags, including nested tags. - The parser should handle both uppercase and lowercase tags. - Attributes should be presented as a dictionary. # Function Signature ```python from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): pass def handle_endtag(self, tag): pass def handle_data(self, data): pass def main(html_data: str) -> None: :param html_data: str : The HTML content to be parsed. :return: None parser = CustomHTMLParser() parser.feed(html_data) parser.close() ``` # Sample Input ```python html_data = <html> <head><title>Sample Page</title></head> <body> <h1>Welcome to the sample page</h1> <p>This is a paragraph with an <a href=\\"https://example.com\\">example link</a>.</p> </body> </html> main(html_data) ``` # Sample Output ``` Start tag: html, attributes: {} Start tag: head, attributes: {} Start tag: title, attributes: {} Data: Sample Page End tag: title End tag: head Start tag: body, attributes: {} Start tag: h1, attributes: {} Data: Welcome to the sample page End tag: h1 Start tag: p, attributes: {} Data: This is a paragraph with an Start tag: a, attributes: {\'href\': \'https://example.com\'} Data: example link End tag: a Data: . End tag: p End tag: body End tag: html ``` # Additional Notes - Handle the conversion of attributes into a dictionary format. - Ensure the parser is robust to handle various HTML structures and formats. - Test your parser with different HTML snippets to validate its correctness.","solution":"from html.parser import HTMLParser class CustomHTMLParser(HTMLParser): def handle_starttag(self, tag, attrs): attrs_dict = {key: value for key, value in attrs} print(f\\"Start tag: {tag}, attributes: {attrs_dict}\\") def handle_endtag(self, tag): print(f\\"End tag: {tag}\\") def handle_data(self, data): if data.strip(): # avoid printing empty data print(f\\"Data: {data}\\") def main(html_data: str) -> None: :param html_data: str : The HTML content to be parsed. :return: None parser = CustomHTMLParser() parser.feed(html_data) parser.close()"},{"question":"# Python/C API: Sequence Operations **Objective**: Implement various sequence operations mimicking the behavior of Python/C API functions in pure Python. **Problem Statement**: You are required to implement a class `SequenceOps` in Python which simulates the behavior of certain sequence manipulation functions provided by the Python/C API. Each method in the class should mimic the respective C function described in the documentation. Implement the following methods: 1. **`is_sequence(obj)`**: - Input: `obj` (any Python object). - Output: `True` if `obj` is a sequence type (like list or tuple), `False` otherwise. 2. **`sequence_size(seq)`**: - Input: `seq` (a sequence like list or tuple). - Output: Length of the sequence. 3. **`sequence_concat(seq1, seq2)`**: - Input: `seq1` and `seq2` (both sequences). - Output: Concatenation of `seq1` and `seq2`. 4. **`sequence_repeat(seq, count)`**: - Input: `seq` (a sequence) and `count` (an integer). - Output: Sequence `seq` repeated `count` times. 5. **`sequence_get_item(seq, index)`**: - Input: `seq` (a sequence) and `index` (an integer). - Output: The element at the given `index` in `seq`. 6. **`sequence_get_slice(seq, start, end)`**: - Input: `seq` (a sequence), `start` and `end` (both integers). - Output: The slice of the sequence from `start` to `end`. 7. **`sequence_set_item(seq, index, value)`**: - Input: `seq` (a sequence), `index` (an integer), and `value` (any Python object). - Output: Set the element at `index` to `value`. 8. **`sequence_del_item(seq, index)`**: - Input: `seq` (a sequence) and `index` (an integer). - Output: Delete the element at the given `index`. 9. **`sequence_list(seq)`**: - Input: `seq` (a sequence or iterable). - Output: A new list containing the same elements as `seq`. 10. **`sequence_tuple(seq)`**: - Input: `seq` (a sequence or iterable). - Output: A new tuple containing the same elements as `seq`. **Constraints**: - Do not use any external libraries or methods that directly solve the problem (e.g., avoid using built-in functions like `len`, `+`, `*`, `getitem` and slicing directly in method implementations). - You may assume that inputs are always valid when function is called. **Example Usage**: ```python ops = SequenceOps() # Check if an object is a sequence print(ops.is_sequence([1, 2, 3])) # True print(ops.is_sequence(123)) # False # Get the size of a sequence print(ops.sequence_size((1, 2, 3))) # 3 # Concatenate sequences print(ops.sequence_concat([1, 2], [3, 4])) # [1, 2, 3, 4] # Repeat sequence print(ops.sequence_repeat([1, 2], 3)) # [1, 2, 1, 2, 1, 2] # Get item from sequence print(ops.sequence_get_item([1, 2, 3], 1)) # 2 # Get slice from sequence print(ops.sequence_get_slice([1, 2, 3, 4], 1, 3)) # [2, 3] # Set item in sequence seq = [1, 2, 3] ops.sequence_set_item(seq, 1, 10) print(seq) # [1, 10, 3] # Delete item from sequence seq = [1, 2, 3] ops.sequence_del_item(seq, 1) print(seq) # [1, 3] # Convert sequence to list print(ops.sequence_list((1, 2, 3))) # [1, 2, 3] # Convert sequence to tuple print(ops.sequence_tuple([1, 2, 3])) # (1, 2, 3) ``` **Implementation Requirements**: - Write clear and concise code. - Include comments where necessary to explain your logic. - Make sure your code handles edge cases gracefully (e.g., empty sequences, negative indices).","solution":"class SequenceOps: A class to mimic sequence operations similar to Python/C API functions. def is_sequence(self, obj): Check if an object is a sequence. return isinstance(obj, (list, tuple)) def sequence_size(self, seq): Get the size of the sequence. size = 0 for _ in seq: size += 1 return size def sequence_concat(self, seq1, seq2): Concatenate two sequences. result = [] for elem in seq1: result.append(elem) for elem in seq2: result.append(elem) return result def sequence_repeat(self, seq, count): Repeat a sequence count times. result = [] for _ in range(count): for elem in seq: result.append(elem) return result def sequence_get_item(self, seq, index): Get an item from the sequence at a given index. return seq[index] def sequence_get_slice(self, seq, start, end): Get a slice from the sequence. result = [] i = 0 for elem in seq: if start <= i < end: result.append(elem) i += 1 return result def sequence_set_item(self, seq, index, value): Set an item in the sequence at the given index. seq[index] = value def sequence_del_item(self, seq, index): Delete an item from the sequence at the given index. del seq[index] def sequence_list(self, seq): Convert a sequence to a list. return [elem for elem in seq] def sequence_tuple(self, seq): Convert a sequence to a tuple. return tuple(seq) # Creating an instance to allow the use of functions. ops = SequenceOps()"},{"question":"You are required to create a visualization using the Seaborn library that demonstrates your understanding of grouping and counting distinct observations with different categories. Follow the steps below to complete this task. # Problem Statement Using the Seaborn package, create a bar plot that visualizes the distribution of tips given on each day of the week, grouped by sex, from the dataset provided by Seaborn. Additionally, calculate and visualize the percentage of total tips given by each sex on each day. # Requirements 1. **Data Preparation:** - Load the \'tips\' dataset using Seaborn\'s `load_dataset` function. 2. **Bar Plot 1: Counts by Day and Sex:** - Create a bar plot showing the count of tips given on each day of the week. - Each bar should be color-coded by sex. - Separate male and female counts for better visibility using the `so.Dodge()` function. 3. **Bar Plot 2: Percentage of Total Tips by Day and Sex:** - Calculate the percentage of total tips given by each sex on each day. - Create a bar plot showing these percentages. - Each bar should be color-coded by sex, with the percentage values displayed above the bars. # Input Format - There are no required inputs from the user. The dataset is loaded directly within the code. # Output Format - The output should be two visualizations: 1. A bar plot showing the count of tips given on each day, grouped by sex. 2. A bar plot showing the percentage of total tips given by each sex on each day. # Constraints - Use only Seaborn for the visualizations. - Ensure the plots are clear and properly labeled. # Sample Code Template ```python import seaborn.objects as so from seaborn import load_dataset import matplotlib.pyplot as plt # Load the dataset tips = load_dataset(\\"tips\\") # Create Bar Plot 1: Counts by Day and Sex plot1 = so.Plot(tips, x=\\"day\\", color=\\"sex\\").add(so.Bar(), so.Count(), so.Dodge()) plot1.show() # Display the first plot # Calculate the percentage of total tips by each sex on each day tip_counts = tips.groupby([\'day\', \'sex\']).size().reset_index(name=\'count\') total_tips = tip_counts.groupby(\'day\')[\'count\'].transform(\'sum\') tip_counts[\'percentage\'] = (tip_counts[\'count\'] / total_tips) * 100 # Create Bar Plot 2: Percentage of Total Tips by Day and Sex plot2 = so.Plot(tip_counts, x=\\"day\\", y=\\"percentage\\", color=\\"sex\\").add(so.Bar(), so.Dodge()) plot2.show() # Display the second plot ``` This template sets you on the right track to solving the problem. Your final solution should include relevant labels, titles, and any additional formatting to make the plots informative and visually appealing.","solution":"import seaborn as sns import matplotlib.pyplot as plt # Load the dataset tips = sns.load_dataset(\\"tips\\") # Create Bar Plot 1: Counts by Day and Sex plt.figure(figsize=(10, 6)) sns.countplot(data=tips, x=\\"day\\", hue=\\"sex\\") plt.title(\\"Count of Tips Given by Day and Sex\\") plt.xlabel(\\"Day of the Week\\") plt.ylabel(\\"Count of Tips\\") plt.legend(title=\\"Sex\\") plt.show() # Calculate the percentage of total tips by each sex on each day tip_counts = tips.groupby([\'day\', \'sex\']).size().reset_index(name=\'count\') total_tips = tip_counts.groupby(\'day\')[\'count\'].transform(\'sum\') tip_counts[\'percentage\'] = (tip_counts[\'count\'] / total_tips) * 100 # Create Bar Plot 2: Percentage of Total Tips by Day and Sex plt.figure(figsize=(10, 6)) sns.barplot(data=tip_counts, x=\\"day\\", y=\\"percentage\\", hue=\\"sex\\") plt.title(\\"Percentage of Total Tips Given by Day and Sex\\") plt.xlabel(\\"Day of the Week\\") plt.ylabel(\\"Percentage of Tips\\") for _, row in tip_counts.iterrows(): plt.text(row.name, row[\'percentage\'], f\\"{row[\'percentage\']:.1f}%\\", color=\'black\', ha=\\"center\\", va=\\"bottom\\") plt.legend(title=\\"Sex\\") plt.show()"},{"question":"# Data Analysis with Pandas **Objective:** You are provided with two CSV files containing sales data of a retail store. You need to write a function that reads these files, processes the data, and generates meaningful insights using pandas. **CSV File 1: `sales_jan.csv`** ``` transaction_id,product_id,quantity,price,timestamp 1,101,2,20,2023-01-01 12:34:56 2,105,1,5,2023-01-02 13:34:56 3,101,3,20,2023-01-02 15:34:56 ... ``` **CSV File 2: `sales_feb.csv`** ``` transaction_id,product_id,quantity,price,timestamp 15,102,2,15,2023-02-01 10:30:22 16,105,2,5,2023-02-01 11:11:45 17,103,1,30,2023-02-02 09:22:01 ... ``` **Instructions:** 1. Write a function `process_sales_data(jan_path: str, feb_path: str) -> pd.DataFrame` that does the following: - Reads the CSV files `sales_jan.csv` and `sales_feb.csv` into pandas DataFrames. - Merges the two DataFrames into a single DataFrame. - Converts the `timestamp` column to datetime format. - Extracts the date and time from the `timestamp` column into two new columns `date` and `time`. - Deletes any transactions with a `quantity` of 0. - Returns a DataFrame showing the total quantity and total sales amount for each product ID across the two months. **Expected Input and Output:** - Input: - `jan_path`: Path to `sales_jan.csv` - `feb_path`: Path to `sales_feb.csv` - Output: A DataFrame with columns `product_id`, `total_quantity`, `total_sales` where: - `product_id` is the product\'s ID. - `total_quantity` is the total quantity sold. - `total_sales` is the total sales amount (quantity * price). **Constraints:** - Assume CSV files are well-formed and contain correct data types. - Your function should handle any number of rows in the CSV files efficiently. **Performance Requirements:** - The function should be optimized to handle large datasets efficiently, aiming for linear time complexity with respect to the number of rows in the CSV files. **Example:** For given sample data: `sales_jan.csv` ``` transaction_id,product_id,quantity,price,timestamp 1,101,2,20,2023-01-01 12:34:56 2,105,1,5,2023-01-02 13:34:56 3,101,3,20,2023-01-02 15:34:56 ``` `sales_feb.csv` ``` transaction_id,product_id,quantity,price,timestamp 15,102,2,15,2023-02-01 10:30:22 16,105,2,5,2023-02-01 11:11:45 17,103,1,30,2023-02-02 09:22:01 ``` Output: ``` product_id total_quantity total_sales 0 101 5 100 1 105 3 15 2 102 2 30 3 103 1 30 ```","solution":"import pandas as pd def process_sales_data(jan_path: str, feb_path: str) -> pd.DataFrame: # Read CSV files into DataFrames df_jan = pd.read_csv(jan_path) df_feb = pd.read_csv(feb_path) # Merge the DataFrames df = pd.concat([df_jan, df_feb], ignore_index=True) # Convert timestamp to datetime format df[\'timestamp\'] = pd.to_datetime(df[\'timestamp\']) # Extract date and time from timestamp df[\'date\'] = df[\'timestamp\'].dt.date df[\'time\'] = df[\'timestamp\'].dt.time # Delete rows with quantity of 0 df = df[df[\'quantity\'] != 0] # Calculate total quantity and total sales for each product ID df[\'total_sales\'] = df[\'quantity\'] * df[\'price\'] result_df = df.groupby(\'product_id\').agg(total_quantity=(\'quantity\', \'sum\'), total_sales=(\'total_sales\', \'sum\')).reset_index() return result_df"},{"question":"Enum-based Resource Allocation Management Objective: You need to design and implement a resource allocation manager using the `enum` module in Python. This manager will handle different types of resources, allocate resources to users, and provide insights into the allocation. Requirements: 1. **Define Resource Types:** - Create an `Enum` named `ResourceType` with the following members: `CPU`, `MEMORY`, `STORAGE`. - Use the `auto` method to automatically assign values to these members. 2. **Resource Allocation Tracker:** - Implement a resource allocation class named `ResourceAllocation` that uses `ResourceType` to track the number of units of each type assigned to a user. - The class should allow the following functionalities: - **Assign Resource:** A method `assign_resource` that takes a user ID (string), a `ResourceType`, and a number of units to allocate. - **Release Resource:** A method `release_resource` that takes a user ID, a `ResourceType`, and a number of units to release. - **Get Allocation:** A method `get_allocation` that takes a user ID and returns the current allocation of resources for that user in the format `{ResourceType: units}`. - **Get Total Allocated:** A method `get_total_allocated` that returns the total allocation of resources across all users in the format `{ResourceType: total_units}`. 3. **Constraints:** - Releasing more units than allocated for a user should be handled gracefully by releasing only the available units. - Ensure thread-safety for all resource allocation methods (You can use threading locks for this purpose). Input and Output: - **assign_resource(user_id: str, resource: ResourceType, units: int):** - Input: `user_id=\\"user1\\", resource=ResourceType.CPU, units=4` - Output: None - **release_resource(user_id: str, resource: ResourceType, units: int):** - Input: `user_id=\\"user1\\", resource=ResourceType.CPU, units=2` - Output: None - **get_allocation(user_id: str):** - Input: `user_id=\\"user1\\"` - Output: `{ResourceType.CPU: 2}` - **get_total_allocated():** - Input: None - Output: `{ResourceType.CPU: 10, ResourceType.MEMORY: 20, ResourceType.STORAGE: 15}` Implementation: ```python from enum import Enum, auto from collections import defaultdict from threading import Lock class ResourceType(Enum): CPU = auto() MEMORY = auto() STORAGE = auto() class ResourceAllocation: def __init__(self): self.allocations = defaultdict(lambda: defaultdict(int)) self.total_allocations = defaultdict(int) self.lock = Lock() def assign_resource(self, user_id: str, resource: ResourceType, units: int) -> None: with self.lock: self.allocations[user_id][resource] += units self.total_allocations[resource] += units def release_resource(self, user_id: str, resource: ResourceType, units: int) -> None: with self.lock: allocated_units = self.allocations[user_id][resource] if allocated_units < units: units = allocated_units self.allocations[user_id][resource] -= units self.total_allocations[resource] -= units def get_allocation(self, user_id: str) -> dict: with self.lock: return dict(self.allocations[user_id]) def get_total_allocated(self) -> dict: with self.lock: return dict(self.total_allocations) # Example usage: if __name__ == \\"__main__\\": manager = ResourceAllocation() manager.assign_resource(\\"user1\\", ResourceType.CPU, 4) manager.assign_resource(\\"user1\\", ResourceType.MEMORY, 2) manager.assign_resource(\\"user2\\", ResourceType.CPU, 6) manager.assign_resource(\\"user2\\", ResourceType.STORAGE, 10) print(manager.get_allocation(\\"user1\\")) print(manager.get_total_allocated()) manager.release_resource(\\"user2\\", ResourceType.CPU, 3) print(manager.get_total_allocated()) ``` Provide a comprehensive solution and explain your code to demonstrate your understanding of the `enum` module and resource management concepts.","solution":"from enum import Enum, auto from collections import defaultdict from threading import Lock class ResourceType(Enum): CPU = auto() MEMORY = auto() STORAGE = auto() class ResourceAllocation: def __init__(self): self.allocations = defaultdict(lambda: defaultdict(int)) self.total_allocations = defaultdict(int) self.lock = Lock() def assign_resource(self, user_id: str, resource: ResourceType, units: int) -> None: with self.lock: self.allocations[user_id][resource] += units self.total_allocations[resource] += units def release_resource(self, user_id: str, resource: ResourceType, units: int) -> None: with self.lock: allocated_units = self.allocations[user_id][resource] if allocated_units < units: units = allocated_units self.allocations[user_id][resource] -= units self.total_allocations[resource] -= units def get_allocation(self, user_id: str) -> dict: with self.lock: return dict(self.allocations[user_id]) def get_total_allocated(self) -> dict: with self.lock: return dict(self.total_allocations)"},{"question":"# Question: Implement a Data Preprocessing Pipeline Using Scikit-learn You are provided with a dataset containing various numerical and categorical features with several preprocessing requirements. Your task is to efficiently preprocess the dataset using the scikit-learn library. Objectives: 1. Standardize all numerical features to have zero mean and unit variance. 2. Scale features to a range of [0, 1] using `MinMaxScaler`. 3. Encode categorical features using `OneHotEncoder`, handle missing values, and aggregate infrequent categories. 4. Discretize selected numerical features into bins. 5. Apply a non-linear transformation to the data and normalize the transformed features. Input: - A pandas DataFrame, `df`, with the following columns: - `age`: Numerical feature (e.g., [23, 45, 35, np.nan, 50]) - `salary`: Numerical feature (e.g., [60000, 70000, 80000, np.nan, 75000]) - `gender`: Categorical feature (e.g., [\'male\', \'female\', np.nan, \'female\', \'male\']) - `region`: Categorical feature (e.g., [\'US\', \'Europe\', \'Asia\', \'US\', np.nan]) - `score`: Numerical feature (e.g., [0.5, 0.7, 0.9, np.nan, 0.6]) Output: - A transformed pandas DataFrame, `df_transformed`, with the applied preprocessing steps. Constraints: - Any missing values should be handled appropriately within the preprocessing pipeline. - For `gender` and `region`, encode the categorical features using one-hot encoding while handling missing values and aggregating infrequent categories seen less than or equal to 2 times. - For `age` and `salary`, discretize them using the `KBinsDiscretizer` into 3 bins each. - The non-linear transformation method should be the Yeo-Johnson transformation, followed by normalization using the L2 norm. Example Code Framework: ```python import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, KBinsDiscretizer, PowerTransformer, Normalizer from sklearn.impute import SimpleImputer def preprocess_data(df): # Column transformations numerical_features = [\'age\', \'salary\', \'score\'] categorical_features = [\'gender\', \'region\'] numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'minmax\', MinMaxScaler()), (\'binner\', KBinsDiscretizer(n_bins=3, encode=\'ordinal\')), (\'power\', PowerTransformer(method=\'yeo-johnson\')), (\'norm\', Normalizer(norm=\'l2\')) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'infrequent_if_exist\', min_frequency=2)) ]) preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Apply the preprocessing pipeline df_transformed = preprocessor.fit_transform(df) return pd.DataFrame(df_transformed) # Example usage if __name__ == \\"__main__\\": data = { \'age\': [23, 45, 35, np.nan, 50], \'salary\': [60000, 70000, 80000, np.nan, 75000], \'gender\': [\'male\', \'female\', np.nan, \'female\', \'male\'], \'region\': [\'US\', \'Europe\', \'Asia\', \'US\', np.nan], \'score\': [0.5, 0.7, 0.9, np.nan, 0.6] } df = pd.DataFrame(data) result = preprocess_data(df) print(result) ```","solution":"import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, KBinsDiscretizer, PowerTransformer, Normalizer from sklearn.impute import SimpleImputer def preprocess_data(df): # Define column types numerical_features = [\'age\', \'salary\', \'score\'] categorical_features = [\'gender\', \'region\'] # Create pipelines for numerical and categorical features numerical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'mean\')), (\'scaler\', StandardScaler()), (\'minmax\', MinMaxScaler()), (\'binner\', KBinsDiscretizer(n_bins=3, encode=\'ordinal\', strategy=\'uniform\')), (\'power\', PowerTransformer(method=\'yeo-johnson\')), (\'norm\', Normalizer(norm=\'l2\')) ]) categorical_transformer = Pipeline(steps=[ (\'imputer\', SimpleImputer(strategy=\'constant\', fill_value=\'missing\')), (\'onehot\', OneHotEncoder(handle_unknown=\'infrequent_if_exist\', min_frequency=2)) ]) # Combine the transformers into a single preprocessor preprocessor = ColumnTransformer( transformers=[ (\'num\', numerical_transformer, numerical_features), (\'cat\', categorical_transformer, categorical_features) ]) # Apply the preprocessing pipeline to the data df_transformed = preprocessor.fit_transform(df) return pd.DataFrame(df_transformed) # Example usage if __name__ == \\"__main__\\": data = { \'age\': [23, 45, 35, np.nan, 50], \'salary\': [60000, 70000, 80000, np.nan, 75000], \'gender\': [\'male\', \'female\', np.nan, \'female\', \'male\'], \'region\': [\'US\', \'Europe\', \'Asia\', \'US\', np.nan], \'score\': [0.5, 0.7, 0.9, np.nan, 0.6] } df = pd.DataFrame(data) result = preprocess_data(df) print(result)"},{"question":"You are tasked with creating and customizing a heatmap using the seaborn library. This question is designed to assess your ability to load datasets, reshape data, and utilize seaborn\'s heatmap customization features. Follow the steps below to complete the task. Task: 1. Load the \'flights\' dataset from seaborn. 2. Pivot the dataset to create a matrix where: - The rows represent years. - The columns represent months. - The values represent the number of passengers. 3. Create a heatmap from the pivoted data. 4. Customize the heatmap with the following specifications: - Add annotations to show the number of passengers in each cell. - Use the \'YlGnBu\' colormap. - Set the colormap norm to range between 100 and 600 passengers. - Add lines between cells with a linewidth of 1. - Display the values with no decimal points. - Use the top orientation for the x-axis ticks. Input and Output Formats: - **Input:** None. - **Output:** A heatmap plot based on the conditions specified. Constraints: - You should not use any external datasets or modify the steps outlined above. - The code should be self-contained and executable. Example Code Structure: ```python import seaborn as sns import matplotlib.pyplot as plt # Load the dataset flights = sns.load_dataset(\'flights\') # Pivot the dataset flights_pivot = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') # Create and customize heatmap ax = sns.heatmap(flights_pivot, annot=True, fmt=\'d\', cmap=\'YlGnBu\', vmin=100, vmax=600, linewidth=1) # Tweaking the plot ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Display the heatmap plt.show() ``` Ensure your final solution closely matches the provided example structure and meets all the customization specifications.","solution":"import seaborn as sns import matplotlib.pyplot as plt import numpy as np def plot_custom_heatmap(): # Load the dataset flights = sns.load_dataset(\'flights\') # Pivot the dataset flights_pivot = flights.pivot(index=\'year\', columns=\'month\', values=\'passengers\') # Create and customize heatmap ax = sns.heatmap(flights_pivot, annot=True, fmt=\'d\', cmap=\'YlGnBu\', vmin=100, vmax=600, linewidths=1, linecolor=\'gray\') # Tweaking the plot ax.set(xlabel=\\"\\", ylabel=\\"\\") ax.xaxis.tick_top() # Display the heatmap plt.show()"},{"question":"# URL Manipulation Assessment Objective: You are required to implement a function that manipulates and processes URLs using the `urllib.parse` module. The function will accept a list of URL strings and perform a series of operations to create new URLs based on specific criteria. Function Signature: ```python def manipulate_urls(base_url: str, relative_urls: list) -> dict: Manipulates and processes a list of URLs based on a base URL. Parameters: - base_url (str): The base URL to be used for combining with relative URLs. - relative_urls (list): A list of relative URLs (strings) to be processed. Returns: - dict: A dictionary with the following keys: - \'absolute_urls\': List of absolute URLs formed by combining base_url with each relative URL. - \'parsed_url_details\': List of dictionaries containing parsed components of each formed absolute URL. - \'quoted_paths\': List of quoted paths for each absolute URL\'s path component. pass ``` Description: 1. **Combine URLs:** - Use `urllib.parse.urljoin()` to combine the `base_url` with each URL in `relative_urls` to form absolute URLs. 2. **Parse URL Components:** - Use `urllib.parse.urlparse()` to parse each formed absolute URL. - Store the parsed components (`scheme`, `netloc`, `path`, `params`, `query`, `fragment`) for each URL in a dictionary. 3. **Quote URL Paths:** - Use `urllib.parse.quote()` to quote the `path` component of each absolute URL. 4. **Return Result:** - The function should return a dictionary with three keys: - `\'absolute_urls\'`: List of absolute URLs. - `\'parsed_url_details\'`: List of dictionaries, each containing the parsed components of an absolute URL. - `\'quoted_paths\'`: List of quoted paths corresponding to each absolute URL\'s path component. Example: ```python base_url = \\"http://example.com\\" relative_urls = [ \\"/path/to/resource?query=123\\", \\"about.html\\", \\"//anotherdomain.com/resource\\", \\"new-page#fragment\\" ] output = manipulate_urls(base_url, relative_urls) print(output) # Example output structure: # { # \'absolute_urls\': [ # \'http://example.com/path/to/resource?query=123\', # \'http://example.com/about.html\', # \'http://anotherdomain.com/resource\', # \'http://example.com/new-page#fragment\' # ], # \'parsed_url_details\': [ # { # \'scheme\': \'http\', # \'netloc\': \'example.com\', # \'path\': \'/path/to/resource\', # \'params\': \'\', # \'query\': \'query=123\', # \'fragment\': \'\' # }, # ... # ], # \'quoted_paths\': [ # \'/path/to/resource\', # \'/about.html\', # \'/resource\', # \'/new-page\' # ] # } ``` Constraints: - Assume that `relative_urls` will contain valid URL strings. - Ensure all URLs are processed and returned with their components correctly parsed and quoted. - Do not use any external libraries besides `urllib.parse`. Your implementation should be efficient and handle any edge cases, such as empty paths or missing components, as demonstrated in the examples above.","solution":"from urllib.parse import urljoin, urlparse, quote def manipulate_urls(base_url: str, relative_urls: list) -> dict: Manipulates and processes a list of URLs based on a base URL. Parameters: - base_url (str): The base URL to be used for combining with relative URLs. - relative_urls (list): A list of relative URLs (strings) to be processed. Returns: - dict: A dictionary with the following keys: - \'absolute_urls\': List of absolute URLs formed by combining base_url with each relative URL. - \'parsed_url_details\': List of dictionaries containing parsed components of each formed absolute URL. - \'quoted_paths\': List of quoted paths for each absolute URL\'s path component. absolute_urls = [urljoin(base_url, relative_url) for relative_url in relative_urls] parsed_url_details = [ { \'scheme\': parsed.scheme, \'netloc\': parsed.netloc, \'path\': parsed.path, \'params\': parsed.params, \'query\': parsed.query, \'fragment\': parsed.fragment } for parsed in (urlparse(url) for url in absolute_urls) ] quoted_paths = [quote(parsed[\'path\']) for parsed in parsed_url_details] return { \'absolute_urls\': absolute_urls, \'parsed_url_details\': parsed_url_details, \'quoted_paths\': quoted_paths }"},{"question":"**Question: Clustermap Analysis and Customization with Seaborn** You are provided with a dataset containing information about various wine samples. The dataset includes columns for different chemical properties of the wines as well as their quality ratings. Using the `seaborn` library, your task is to create a customized clustermap to analyze the wine data. # Dataset The dataset is loaded using `sns.load_dataset(\\"wine\\")` and has the following columns: - `fixed_acidity` - `volatile_acidity` - `citric_acid` - `residual_sugar` - `chlorides` - `free_sulfur_dioxide` - `total_sulfur_dioxide` - `density` - `pH` - `sulphates` - `alcohol` - `quality` # Task 1. **Load the dataset** and separate the quality ratings from the other features. 2. **Standardize** the data within the columns. 3. Create a **clustermap** with the following specifications: - Do not cluster the rows, only the columns. - Set the `figsize` to `(10, 8)`. - Use a dendrogram ratio of `(0.05, 0.15)`. - Position the colorbar at `(0.05, 0.8, 0.03, 0.18)`. 4. Add row colors to identify the `quality` ratings: - Map each unique value of `quality` to a specific color. - Use these colors to label the rows in the clustermap. 5. Use a **different colormap** (other than the default) of your choice. 6. Set the limits of the color range from -2 to 2. # Implementation Implement the function `create_custom_clustermap()` with the following signature: ```python import seaborn as sns import pandas as pd def create_custom_clustermap(): # Write your code here pass ``` # Output This function should: - Generate and display the customized clustermap based on the provided specifications. - Ensure the plot is clear and well-labeled. You will be assessed on: - Correct loading and preprocessing of the dataset. - Appropriate use of `seaborn.clustermap`. - Proper customization of the plot\'s appearance and functionality. - Clear and readable visualization. **Note:** Make sure to import necessary libraries and handle any potential issues with data loading or processing.","solution":"import seaborn as sns import pandas as pd import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler def create_custom_clustermap(): # Load the dataset df = sns.load_dataset(\\"wine\\") # Separate the quality ratings quality = df.pop(\'quality\') # Standardize the data scaler = StandardScaler() standardized_data = scaler.fit_transform(df) standardized_df = pd.DataFrame(standardized_data, columns=df.columns) # Map each unique value of `quality` to a specific color quality_colors = quality.map({ 3: \'#1f77b4\', # Blue 4: \'#ff7f0e\', # Orange 5: \'#2ca02c\', # Green 6: \'#d62728\', # Red 7: \'#9467bd\', # Purple 8: \'#8c564b\' # Brown }) # Create the clustermap sns.clustermap( standardized_df, row_cluster=False, col_cluster=True, figsize=(10, 8), dendrogram_ratio=(0.05, 0.15), cbar_pos=(0.05, 0.8, 0.03, 0.18), row_colors=quality_colors, cmap=\'coolwarm\', vmin=-2, vmax=2 ) plt.show()"},{"question":"# Custom Python Interpreter Objective: Implement a custom interactive Python interpreter that supports the following features: - Executes valid Python code and prints the output or errors. - Supports multi-line code inputs, allowing users to execute complete blocks of code. - Maintains an environment where variables and state persist across different inputs. Requirements: 1. **Class Implementation**: Create a class `CustomInterpreter` that extends from appropriate base classes in the `code` module. 2. **Method Definitions**: - `__init__(self)`: Initialize the custom interpreter. - `run(self, source: str) -> None`: Takes a string input `source`, which can be a single line or multiple lines of Python code, and executes it. - `interact(self) -> None`: Initiates an interactive console session where users can input Python code line-by-line. Constraints: - The interpreter should handle both complete and incomplete lines of Python code. - It should maintain the state and scope of the variables across different `run` invocations and within the interactive session. Example Usage: ```python >>> interpreter = CustomInterpreter() >>> interpreter.run(\'a = 10\') # Single line of code >>> interpreter.run(\'if a == 10:n... print(\\"a is 10\\")n\') # Multi-line code a is 10 >>> interpreter.run(\'print(a)\') 10 >>> interpreter.interact() >>> a = 20 >>> print(a) 20 >>> exit() # To exit the interactive console session ``` Performance Requirements: - The interpreter need not be optimized for extremely high performance but should handle typical interactive usage gracefully. Important Notes: 1. Use the `InteractiveInterpreter` and `InteractiveConsole` classes from the `code` module. 2. Utilize the `codeop` module if necessary for compiling and handling incomplete code blocks. This question tests students on their understanding of: - Extending and customizing Python classes. - Handling multi-line inputs and maintaining state across executions. - Implementing a user-facing interactive tool.","solution":"import code class CustomInterpreter(code.InteractiveConsole): def __init__(self): super().__init__() def run(self, source: str) -> None: Executes the given source code. try: self.runcode(source) except Exception as e: print(f\\"Error: {e}\\") def interact(self) -> None: Starts an interactive console session. super().interact() # Example Usage: # interpreter = CustomInterpreter() # interpreter.run(\'a = 10\') # Single line of code # interpreter.run(\'if a == 10:n... print(\\"a is 10\\")n\') # Multi-line code # interpreter.run(\'print(a)\') # interpreter.interact() # a = 20 # print(a) # exit()"},{"question":"Title: Implementing and Evaluating Clustering Algorithms Background: Clustering algorithms are used to group similar data points into clusters based on certain criteria. Scikit-learn provides various clustering algorithms, each suited for different kinds of data and clustering requirements. Objective: Your task is to implement a clustering solution using the K-Means algorithm on a synthetic dataset generated using the make_classification or make_blobs function. You will then evaluate the clustering performance using the silhouette coefficient. Instructions: 1. **Data Generation:** - Generate a synthetic dataset with 300 samples and 5 clusters using scikit-learn’s `make_blobs` function. 2. **Implement K-Means Clustering:** - Apply the K-Means clustering algorithm from scikit-learn on the synthetic dataset. - Use `k-means++` for the initialization of centroids. - Set the random state to 0 for reproducibility. 3. **Evaluate the Clustering:** - Evaluate the performance of your clustering implementation using the silhouette coefficient. - Compute the silhouette score for the clustering obtained. 4. **Optional Task (Bonus):** - Compare the performance of K-Means with another clustering algorithm (e.g., DBSCAN) and discuss which algorithm performed better based on the silhouette score. Function Signature: ```python from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def evaluate_kmeans_clustering(): # 1. Generate the dataset # 2. Apply K-Means Clustering # 3. Compute silhouette score # 4. Return the silhouette score # Optional: Compare with another clustering algorithm return silhouette_score_kmeans # Example function call score = evaluate_kmeans_clustering() print(f\\"Silhouette Score for K-Means: {score}\\") ``` Constraints: - The dataset should be generated with 300 samples and 5 clusters. - Use `init=\'k-means++\'` and `random_state=0` in K-Means. - Use Euclidean distance for computing the silhouette score. Input Format: The function does not take any input parameters. Output Format: - The function should return a float value representing the silhouette score for the K-Means clustering. Example: ```python score = evaluate_kmeans_clustering() print(f\\"Silhouette Score for K-Means: {score}\\") ``` Evaluation Criteria: - Correctness of the implementation. - Proper use of scikit-learn functions and parameters. - Clarity and readability of the code. - Correct computation and return of the silhouette score.","solution":"from sklearn.datasets import make_blobs from sklearn.cluster import KMeans from sklearn.metrics import silhouette_score def evaluate_kmeans_clustering(): # 1. Generate the dataset X, y = make_blobs(n_samples=300, centers=5, random_state=0) # 2. Apply K-Means Clustering kmeans = KMeans(n_clusters=5, init=\'k-means++\', random_state=0) kmeans.fit(X) labels = kmeans.labels_ # 3. Compute silhouette score silhouette_score_kmeans = silhouette_score(X, labels, metric=\'euclidean\') # 4. Return the silhouette score return silhouette_score_kmeans # Example function call score = evaluate_kmeans_clustering() print(f\\"Silhouette Score for K-Means: {score}\\")"},{"question":"# SAX XML Parsing with Custom Content and Error Handlers **Objective**: Implement a SAX parser using `xml.sax`, and create custom handlers for content and error events. Demonstrate your understanding by parsing a provided XML string and handling specific events. Task 1. **ContentHandler Class**: - Subclass the `xml.sax.handler.ContentHandler` class to create a custom handler. - Implement or override the following methods: - `startElement(name, attrs)`: Print the name of the element and its attributes. - `endElement(name)`: Print the name of the closing tag. - `characters(content)`: Print character content within elements. - Handle the root element and any sub-elements. 2. **ErrorHandler Class**: - Subclass the `xml.sax.handler.ErrorHandler` class to create a custom handler. - Implement or override the following methods: - `error(exception)`: Print an error message and continue parsing. - `fatalError(exception)`: Print a fatal error message and stop parsing. - `warning(exception)`: Print a warning message and continue parsing. 3. **Parsing Function**: - Create a function named `parse_xml(xml_string)`: - Accepts an XML string as input. - Uses the `xml.sax` module and the custom handlers to parse the XML string. - Print appropriate messages for each method in the custom handlers. Input and Output Format - **Input**: A single XML string that may contain various elements, attributes, and character data. - **Output**: Printed messages demonstrating the parsed elements, attributes, character content, warnings, errors, and fatal errors as handled by your custom handlers. Example ```python xml_data = \'\'\' <bookstore> <book category=\\"cooking\\"> <title lang=\\"en\\">Everyday Italian</title> <author>Giada De Laurentiis</author> <year>2005</year> <price>30.00</price> </book> <book category=\\"children\\"> <title lang=\\"en\\">Harry Potter</title> <author>J K. Rowling</author> <year>2005</year> <price>29.99</price> </book> </bookstore> \'\'\' parse_xml(xml_data) ``` **Expected Output**: ``` Start document Start element: bookstore Start element: book, attributes: {\'category\': \'cooking\'} Start element: title, attributes: {\'lang\': \'en\'} Characters: Everyday Italian End element: title Start element: author Characters: Giada De Laurentiis End element: author Start element: year Characters: 2005 End element: year Start element: price Characters: 30.00 End element: price End element: book Start element: book, attributes: {\'category\': \'children\'} Start element: title, attributes: {\'lang\': \'en\'} Characters: Harry Potter End element: title Start element: author Characters: J K. Rowling End element: author Start element: year Characters: 2005 End element: year Start element: price Characters: 29.99 End element: price End element: book End element: bookstore End document ``` Constraints - Assume the provided XML string is well-formed. - Ensure your solution handles different levels of nested elements and character data. - Handle and print messages for any warnings, errors, or fatal errors if encountered during parsing.","solution":"import xml.sax from xml.sax.handler import ContentHandler, ErrorHandler class CustomContentHandler(ContentHandler): def startDocument(self): print(\\"Start document\\") def endDocument(self): print(\\"End document\\") def startElement(self, name, attrs): print(f\\"Start element: {name}, attributes: {dict(attrs)}\\") def endElement(self, name): print(f\\"End element: {name}\\") def characters(self, content): if content.strip(): # Avoid printing empty strings print(f\\"Characters: {content.strip()}\\") class CustomErrorHandler(ErrorHandler): def error(self, exception): print(f\\"Error: {exception}\\") def fatalError(self, exception): print(f\\"Fatal Error: {exception}\\") raise exception def warning(self, exception): print(f\\"Warning: {exception}\\") def parse_xml(xml_string): parser = xml.sax.make_parser() content_handler = CustomContentHandler() error_handler = CustomErrorHandler() parser.setContentHandler(content_handler) parser.setErrorHandler(error_handler) import io xml_io = io.StringIO(xml_string) parser.parse(xml_io)"},{"question":"# Objective This question aims to test your understanding of tensor operations, specifically related to querying and manipulating tensor shapes using the `torch.Size` class in PyTorch. # Problem Statement You are given a list of tensors. Your task is to implement a function `find_tensors_with_dim(tensors: List[torch.Tensor], dim: int) -> List[torch.Tensor]` that filters out tensors having a specific number of dimensions. # Function Signature ```python def find_tensors_with_dim(tensors: List[torch.Tensor], dim: int) -> List[torch.Tensor]: pass ``` # Input - `tensors`: A list of `torch.Tensor` objects. - `dim`: An integer specifying the required number of dimensions. # Output - A list of `torch.Tensor` objects from the input list that have exactly `dim` dimensions. # Example ```python import torch # Example tensors tensor1 = torch.ones(2, 3) tensor2 = torch.ones(1, 2, 3) tensor3 = torch.ones(4) # Input list tensors = [tensor1, tensor2, tensor3] # Required dimension dim = 2 # Call the function filtered_tensors = find_tensors_with_dim(tensors, dim) # filtered_tensors should contain tensor1 print(filtered_tensors) # Output: [tensor([[1., 1., 1.], [1., 1., 1.]])] ``` # Constraints - The number of tensors in the input list `tensors` will not exceed 1000. - Each tensor can have up to 5 dimensions. - The input tensors can be of varying shapes. # Notes - Utilize the `torch.Size` class to determine the dimensions of each tensor. - Ensure the function is efficient and performance considerations are taken into account, given the constraints.","solution":"import torch from typing import List def find_tensors_with_dim(tensors: List[torch.Tensor], dim: int) -> List[torch.Tensor]: Filters out tensors having a specific number of dimensions. :param tensors: A list of torch.Tensor objects. :param dim: An integer specifying the required number of dimensions. :return: A list of torch.Tensor objects from the input list that have exactly `dim` dimensions. return [tensor for tensor in tensors if tensor.dim() == dim]"},{"question":"**Objective:** To assess your understanding of various types of assignment statements, as well as other key concepts like `assert`, `yield`, and `return` statements in Python, you are required to implement a set of functions according to the specifications below. **Question:** Function 1: Process List with Assignments Write a function `process_list(lst)` that takes a list of integers and performs the following operations: 1. Rebind the first element of the list to the sum of the first element and the last element. 2. Use an augmented assignment to subtract the second element from the last element and rebind it to the last element. 3. Annotate the third element with the type hint `int`, and multiply it by 2. 4. Return the modified list. *Example:* ```python process_list([1, 2, 3, 4, 5]) # Output: [6, 2, 6, 4, 3] ``` Function 2: Generate Even Numbers Write a generator function `generate_even_numbers(n)` that yields the first `n` even numbers starting from 0. Use a `yield` statement in your function. *Example:* ```python list(generate_even_numbers(5)) # Output: [0, 2, 4, 6, 8] ``` Function 3: Assert Even Sum Write a function `assert_even_sum(lst)` that takes a list of integers and asserts that the sum of the elements is an even number. If the assertion fails, the function should raise an `AssertionError` with the message \\"Sum is not even\\". *Example:* ```python assert_even_sum([1, 1, 2]) # No output because sum is 4 (even) assert_even_sum([1, 1, 1]) # AssertionError: Sum is not even ``` Function 4: Recursive Sum Write a recursive function `recursive_sum(lst)` that takes a list of integers and returns their sum. Use a `return` statement to end the recursion and return the sum. *Example:* ```python recursive_sum([1, 2, 3, 4]) # Output: 10 ``` **Constraints:** - You can assume that the list in `process_list` and `recursive_sum` contains at least three integers. - For `generate_even_numbers`, `n` is a non-negative integer. - For `assert_even_sum`, the input list can be empty or contain any integer values (both positive and negative). **Performance Requirements:** - Your solution should handle large inputs efficiently within the constraints provided. **Submission:** Submit the four functions: `process_list`, `generate_even_numbers`, `assert_even_sum`, and `recursive_sum`.","solution":"def process_list(lst): Processes a list according to the specified operations. # Rebind the first element to the sum of the first element and the last element lst[0] = lst[0] + lst[-1] # Use an augmented assignment to subtract the second element from the last element and rebind it to the last element lst[-1] -= lst[1] # Annotate the third element with type hint `int`, and multiply it by 2 lst[2] = lst[2] * 2 return lst def generate_even_numbers(n): Yields the first n even numbers starting from 0. for i in range(n): yield i * 2 def assert_even_sum(lst): Asserts that the sum of the elements of the list is an even number. assert sum(lst) % 2 == 0, \\"Sum is not even\\" def recursive_sum(lst): Recursively calculates and returns the sum of the elements in the list. if not lst: return 0 return lst[0] + recursive_sum(lst[1:])"},{"question":"# PyTorch Device Management and Synchronization You are tasked with managing a computational workflow that leverages multiple GPU devices using PyTorch. To ensure optimal performance, you need to implement a function that executes some basic device management and synchronization tasks. # Function Description Implement the following function: ```python def manage_devices_and_synchronize(): This function performs the following tasks: 1. Checks if any GPU devices are available. 2. If available, prints the number of GPU devices. 3. Sets the device index to the second device (index 1) if there are at least two devices. 4. Retrieves and prints the current device index. 5. If there are no GPU devices available, print an appropriate message. The function should also ensure that: - Any device streams are managed properly. - Device synchronization is performed before the function completes. # Your code here ``` # Input The function takes no input. # Output - The function prints the number of available GPU devices if any. - The function prints the index of the current device. - If no GPU devices are available, it prints an appropriate message. # Constraints - You are not required to handle exceptions. - Assume that all necessary libraries (e.g., `torch`) are already imported. # Example ```python >>> manage_devices_and_synchronize() Number of GPU devices available: 3 Current device index set to: 1 ``` In this example, the output indicates there are 3 GPU devices available, and the current device index is set to 1. If there were no GPU devices available, it would print a message indicating that no devices are available. **Note**: This example assumes you are running this function in an environment with multiple GPUs. The actual output may vary depending on your hardware setup.","solution":"import torch def manage_devices_and_synchronize(): This function performs the following tasks: 1. Checks if any GPU devices are available. 2. If available, prints the number of GPU devices. 3. Sets the device index to the second device (index 1) if there are at least two devices. 4. Retrieves and prints the current device index. 5. If there are no GPU devices available, print an appropriate message. The function should also ensure that: - Any device streams are managed properly. - Device synchronization is performed before the function completes. if torch.cuda.is_available(): num_devices = torch.cuda.device_count() print(f\\"Number of GPU devices available: {num_devices}\\") if num_devices >= 2: torch.cuda.set_device(1) print(\\"Current device index set to: 1\\") else: torch.cuda.set_device(0) print(\\"Current device index set to: 0\\") current_device_index = torch.cuda.current_device() print(f\\"Current device index: {current_device_index}\\") torch.cuda.synchronize() else: print(\\"No GPU devices available.\\")"},{"question":"**Objective**: To assess the understanding of device management and synchronization using the `torch.accelerator` module in PyTorch. Write a Python function using the PyTorch `torch.accelerator` module that performs the following tasks: 1. Checks if an accelerator device is available. 2. If an accelerator device is available: - Retrieves and prints the total number of available accelerator devices. - Sets the current device to the last available device. - Prints the index of the currently set device. 3. Creates a simple tensor on the currently set device and performs a basic operation (e.g., tensor addition). 4. Synchronizes the current device. 5. Returns the result of the tensor operation. # Function Signature ```python def perform_accelerator_operations(): Manages accelerator devices and performs a basic tensor operation. Returns: torch.Tensor: The result of the tensor operation. ``` # Expected Functionality - If no accelerator device is available, your function can simply return `None`. # Input/Output Example ```python result = perform_accelerator_operations() if result is not None: print(result) # Should print the result of the tensor operation if an accelerator is available. else: print(\\"No accelerator device available.\\") ``` # Constraints and Requirements - Handle cases where no accelerator device is available gracefully. - Use the `torch.accelerator` module functions wherever necessary to manage devices. - Ensure that the tensor operation is performed on the accelerator device and the result is properly synchronized before returning. This question requires you to: - Understand and utilize device querying and setting functions from `torch.accelerator`. - Perform tensor operations on an accelerator device if available. - Implement synchronization to ensure correct results. Your solution will be evaluated based on correctness, proper use of functions from the `torch.accelerator` module, and code clarity.","solution":"import torch def perform_accelerator_operations(): Manages accelerator devices and performs a basic tensor operation. Returns: torch.Tensor or None: The result of the tensor operation, or None if no accelerator is available. if torch.cuda.is_available(): num_devices = torch.cuda.device_count() print(f\\"Number of available accelerator devices: {num_devices}\\") # Set the current device to the last available device device_index = num_devices - 1 torch.cuda.set_device(device_index) # Print the index of the currently set device current_device = torch.cuda.current_device() print(f\\"Current device index: {current_device}\\") # Create a simple tensor on the current device and perform a basic operation tensor_a = torch.tensor([1.0, 2.0, 3.0], device=current_device) tensor_b = torch.tensor([4.0, 5.0, 6.0], device=current_device) result = tensor_a + tensor_b # Synchronize the current device torch.cuda.synchronize() return result else: print(\\"No accelerator device available.\\") return None"},{"question":"**Question: Implement a Producer-Consumer problem using asyncio synchronization primitives** You will use the asyncio library to implement a classic Producer-Consumer problem. This requires managing the production and consumption of items using asyncio synchronization constructs: Lock, Event, Condition, and Semaphore. # Task: 1. Implement a shared queue with a fixed size using asyncio primitives. 2. Implement a producer coroutine that adds items to the queue. 3. Implement a consumer coroutine that removes items from the queue. 4. Use asyncio synchronization primitives to manage the queue\'s state, ensuring proper synchronization between producer and consumer operations. # Class Definitions: 1. `class SharedQueue` 2. `class Producer` 3. `class Consumer` # Method Definitions: **SharedQueue** ```python class SharedQueue: def __init__(self, max_size: int): def __init__(self, max_size: int): # Initialize a condition for queue synchronization and # create a list to store items. pass async def put(self, item: int): # Add an item to the queue. If the queue is full, block until there\'s space. pass async def get(self) -> int: # Remove and return an item from the queue. If the queue is empty, block until an item is available. pass def __len__(self) -> int: # Return the current number of items in the queue. pass ``` **Producer** ```python class Producer: def __init__(self, queue: SharedQueue, n_items: int): # Initialize the producer with the shared queue and number of items to produce. pass async def produce(self): # Produce n_items and add them to the queue. Use asyncio.sleep() to simulate item production time. pass ``` **Consumer** ```python class Consumer: def __init__(self, queue: SharedQueue, n_items: int): # Initialize the consumer with the shared queue and number of items to consume. pass async def consume(self): # Consume n_items from the queue. Use asyncio.sleep() to simulate item consumption time. pass ``` # Expected Input and Output 1. The `SharedQueue` should be correctly synchronized. 2. The producer and consumer should run concurrently without data races. 3. The queue should maintain its size constraint. 4. Both the Producer and Consumer should complete their respective productions and consumptions without deadlock. # Constraints: - The queue should not exceed its `max_size`. - Use the asyncio synchronization primitives as described in the provided documentation. - Implementations should ensure no data races and proper coroutine blocking/unblocking. # Example: ```python async def main(): queue = SharedQueue(max_size=5) producer = Producer(queue, n_items=10) consumer = Consumer(queue, n_items=10) await asyncio.gather(producer.produce(), consumer.consume()) asyncio.run(main()) ``` # Performance Requirements: - Efficiently manage synchronized access to the queue. - Ensure fairness in coroutine blocking and waking up. Good luck!","solution":"import asyncio class SharedQueue: def __init__(self, max_size: int): self.queue = [] self.max_size = max_size self.condition = asyncio.Condition() async def put(self, item: int): async with self.condition: while len(self.queue) >= self.max_size: await self.condition.wait() self.queue.append(item) self.condition.notify_all() async def get(self) -> int: async with self.condition: while not self.queue: await self.condition.wait() item = self.queue.pop(0) self.condition.notify_all() return item def __len__(self) -> int: return len(self.queue) class Producer: def __init__(self, queue: SharedQueue, n_items: int): self.queue = queue self.n_items = n_items async def produce(self): for item in range(self.n_items): await self.queue.put(item) await asyncio.sleep(0.1) # Simulate production time class Consumer: def __init__(self, queue: SharedQueue, n_items: int): self.queue = queue self.n_items = n_items async def consume(self): for _ in range(self.n_items): item = await self.queue.get() await asyncio.sleep(0.1) # Simulate consumption time"},{"question":"Objective: Implement a Python C extension module that defines a new type, `Student`, which can represent a student with specific attributes and methods. This type must also support cyclic garbage collection and subclassing in Python. Requirements: 1. **Type Definition**: - Define a type called `Student` with attributes `first_name` (string), `last_name` (string), and `student_id` (integer). 2. **Methods**: - `full_name`: Returns the full name of the student by combining `first_name` and `last_name`. 3. **Custom Getter/Setter**: - Implement custom getter and setter for the `first_name` and `last_name` attributes to ensure they are always strings and not `NULL`. 4. **Garbage Collection**: - Implement support for cyclic garbage collection for the `Student` type. 5. **Subclassing**: - Ensure the `Student` type can act as a base class for further subclassing in Python. Additional Constraints: - Your solution should properly manage reference counts and ensure the Python object lifecycle (allocation/deallocation) is correctly handled. - Ensure the module can be built and imported into Python, then demonstrate creating instances, accessing fully functioning attributes, methods, and inheritance. Performance: - The type creation and destruction should handle a large number of student objects without memory leaks. # Steps to Implement: 1. Define the `StudentObject` structure, introducing `PyObject_HEAD`. 2. Implement a custom constructor (`Student_new`) to initialize the attributes. 3. Implement a custom initializer (`Student_init`) to handle argument parsing and initialize attributes accordingly. 4. Define a deallocation function (`Student_dealloc`) to decrease reference counts and free the resources. 5. Define custom setter and getter methods for `first_name` and `last_name`. 6. Implement a `full_name` method to return the full name. 7. Ensure cyclic garbage collection by implementing `Student_traverse` and `Student_clear`. 8. Complete the type object initialization and module initialization function. 9. Use `distutils` or `setuptools` to build the module. 10. Test the module by writing a Python script that imports and utilizes the `Student` type. # Submission: Submit: - The C source code file for the extension module. - The setup script (`setup.py`) for building the module. - A Python script demonstrating the creation and manipulation of `Student` instances, including subclassing and invoking the `full_name` method. Example interaction in Python: ```python import custom_student # Creating an instance student = custom_student.Student(first_name=\\"John\\", last_name=\\"Doe\\", student_id=123) # Accessing attributes print(student.first_name) # John print(student.last_name) # Doe print(student.student_id) # 123 # Using method print(student.full_name()) # John Doe # Subclassing class GraduateStudent(custom_student.Student): def __init__(self, first_name, last_name, student_id, thesis_title): super().__init__(first_name, last_name, student_id) self.thesis_title = thesis_title grad_student = GraduateStudent(\\"Alice\\", \\"Smith\\", 456, \\"AI Research\\") print(grad_student.first_name) # Alice print(grad_student.thesis_title) # AI Research print(grad_student.full_name()) # Alice Smith ```","solution":"import gc class Student: def __init__(self, first_name, last_name, student_id): self.first_name = first_name self.last_name = last_name self.student_id = student_id @property def first_name(self): return self._first_name @first_name.setter def first_name(self, value): if not isinstance(value, str): raise TypeError(\\"first_name must be a string\\") self._first_name = value @property def last_name(self): return self._last_name @last_name.setter def last_name(self, value): if not isinstance(value, str): raise TypeError(\\"last_name must be a string\\") self._last_name = value def full_name(self): return f\\"{self.first_name} {self.last_name}\\" class GraduateStudent(Student): def __init__(self, first_name, last_name, student_id, thesis_title): super().__init__(first_name, last_name, student_id) self.thesis_title = thesis_title @property def thesis_title(self): return self._thesis_title @thesis_title.setter def thesis_title(self, value): if not isinstance(value, str): raise TypeError(\\"thesis_title must be a string\\") self._thesis_title = value # Enable automatic garbage collection for testing memory leaks gc.enable() def create_large_number_of_students(): students = [] for i in range(100000): students.append(Student(f\\"FirstName{i}\\", f\\"LastName{i}\\", i)) return students"},{"question":"# Gaussian Mixture Model Implementation and Analysis You have been given a dataset of two features, and your task is to implement and analyze a Gaussian Mixture Model (GMM) using the `sklearn.mixture` package. The dataset provided represents data points generated from a mixture of Gaussian distributions. You need to perform the following steps: 1. **Load the Dataset**: - Assume the dataset is provided as a CSV file with two columns `Feature1` and `Feature2`. 2. **Fit a GMM**: - Use the `GaussianMixture` class from `sklearn.mixture` to fit a GMM to the dataset. - Experiment with different values of `n_components` (number of mixture components) to find the optimal number using the Bayesian Information Criterion (BIC). 3. **Predict and Visualize the Results**: - Predict the cluster assignment for each data point. - Visualize the data points and the equi-probability surfaces of the Gaussian components. - Plot the BIC score against the number of components to find the optimal number. 4. **Initialization Impact Analysis**: - Compare the effect of different initialization methods (`\'kmeans\'`, `\'random\'`, `\'kmeans++\'`) on the convergence and final model. - Provide a brief report on your findings. Input: - CSV file path: `data.csv` Output: - Optimal number of components based on BIC. - Cluster assignment for each data point. - Visualization of the data points with the GMM component contours. - BIC score plot against the number of components. - Analysis report on initialization methods. Constraints: - The number of components should be in the range [1, 10]. - Explain any assumptions or choices made during the implementation. Example Code Structure: ```python import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture # Step 1: Load the Dataset data = pd.read_csv(\'data.csv\') X = data[[\'Feature1\', \'Feature2\']].values # Step 2: Fit a GMM n_components_range = range(1, 11) bics = [] models = [] for n in n_components_range: gmm = GaussianMixture(n_components=n, init_params=\'kmeans\', random_state=0) gmm.fit(X) bics.append(gmm.bic(X)) models.append(gmm) # Optimal number of components optimal_n = n_components_range[np.argmin(bics)] optimal_model = models[np.argmin(bics)] # Step 3: Predict and Visualize Results labels = optimal_model.predict(X) plt.scatter(X[:, 0], X[:, 1], c=labels) # Plotting the equi-probability surfaces # (this is an example, please implement the actual plotting method as described) # BIC score plot plt.figure() plt.plot(n_components_range, bics, marker=\'o\') plt.xlabel(\'Number of components\') plt.ylabel(\'BIC score\') plt.show() # Step 4: Initialization Impact Analysis init_methods = [\'kmeans\', \'random\', \'kmeans++\'] # Implement and compare different initialization methods # Provide analysis of the impact # Analysis Report report = \\"Analysis report goes here...\\" # Output results print(f\\"Optimal number of components: {optimal_n}\\") print(report) ``` Deliverables: 1. Python script implementing the above steps. 2. Visualization plots. 3. Analysis report explaining the effects of different initialization methods and other observations.","solution":"import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.mixture import GaussianMixture def load_data(filepath): Loads data from a CSV file and returns it as a numpy array. data = pd.read_csv(filepath) return data[[\'Feature1\', \'Feature2\']].values def fit_gmm(X, n_components_range): Fits a Gaussian Mixture Model to the data and returns the optimal number of components based on BIC. bics = [] models = [] for n in n_components_range: gmm = GaussianMixture(n_components=n, init_params=\'kmeans\', random_state=0) gmm.fit(X) bics.append(gmm.bic(X)) models.append(gmm) optimal_n = n_components_range[np.argmin(bics)] optimal_model = models[np.argmin(bics)] return optimal_n, optimal_model, bics def plot_results(X, optimal_model, n_components_range, bics): Plots the clustering results and BIC scores. labels = optimal_model.predict(X) plt.scatter(X[:, 0], X[:, 1], c=labels, cmap=\'viridis\') plt.title(\'GMM Clustering Results\') plt.show() plt.figure() plt.plot(n_components_range, bics, marker=\'o\') plt.title(\'BIC Score vs Number of Components\') plt.xlabel(\'Number of Components\') plt.ylabel(\'BIC Score\') plt.show() def analyze_initialization_methods(X, n_components): Analyzes the impact of different initialization methods on the convergence and final model. init_methods = [\'kmeans\', \'random\'] analysis_report = {} for method in init_methods: gmm = GaussianMixture(n_components=n_components, init_params=method, random_state=0) gmm.fit(X) bic = gmm.bic(X) analysis_report[method] = bic return analysis_report # Main script if __name__ == \\"__main__\\": # Step 1: Load the Dataset filepath = \'data.csv\' X = load_data(filepath) # Step 2: Fit a GMM n_components_range = range(1, 11) optimal_n, optimal_model, bics = fit_gmm(X, n_components_range) # Step 3: Predict and Visualize Results plot_results(X, optimal_model, n_components_range, bics) # Step 4: Initialization Impact Analysis init_analysis = analyze_initialization_methods(X, optimal_n) report = f\\"Initialization methods impact:n{init_analysis}\\" # Output results print(f\\"Optimal number of components: {optimal_n}\\") print(report)"},{"question":"Advanced Text Processing with Regular Expressions and Deltas **Objective**: Write a Python function that finds differences between two sets of textual information and filters the differing parts based on specific patterns. Problem Statement You are given two lists of strings, `list1` and `list2`. Each list contains lines of text. Your task is to write a function `find_and_filter_differences(list1, list2, pattern)` that: 1. **Finds the lines that are present in one list but not the other (i.e., differences).** 2. **Filters the differing lines using the provided regex `pattern`.** **Input:** - `list1`: A list of strings. - `list2`: Another list of strings. - `pattern`: A regular expression pattern (string) to match the differing lines against. **Output:** - A list of strings that are different between `list1` and `list2` and match the given `pattern`. Constraints: 1. The function should use the `difflib` module to find differences between the two lists. 2. The function should use the `re` module to apply the regex filter to the differing lines. 3. The lists `list1` and `list2` can contain up to 1000 strings, and each string can be up to 1000 characters long. Example: ```python def find_and_filter_differences(list1, list2, pattern): # Your implementation here # Example usage: list1 = [ \\"The quick brown fox jumps over the lazy dog.\\", \\"Hello world!\\", \\"Python is great.\\", \\"Coding is fun.\\" ] list2 = [ \\"The quick brown fox jumps over the lazy dog.\\", \\"Hello Python!\\", \\"Python is great.\\", \\"Coding can be fun.\\" ] pattern = r\\"fun\\" output = find_and_filter_differences(list1, list2, pattern) print(output) # Output: [\'Coding is fun.\', \'Coding can be fun.\'] ``` **Notes:** - The function `find_and_filter_differences` should be efficient in terms of both time and space complexity. - Ensure your function handles edge cases such as empty lists or no differences gracefully. Happy coding!","solution":"import difflib import re def find_and_filter_differences(list1, list2, pattern): Finds the lines that are present in one list but not the other and filters them using the provided regex pattern. Parameters: list1 (list of str): The first list of strings. list2 (list of str): The second list of strings. pattern (str): The regex pattern to filter the differing lines. Returns: list of str: The filtered list of differing lines matching the pattern. differ = difflib.Differ() diff = list(differ.compare(list1, list2)) # Collect lines prefixed with \'-\' (unique to list1) or \'+\' (unique to list2) differing_lines = [line[2:] for line in diff if line.startswith(\'- \') or line.startswith(\'+ \')] # Filter the differing lines by the provided regex pattern filtered_diff = [line for line in differing_lines if re.search(pattern, line)] return filtered_diff"},{"question":"**Advanced Coding Assessment Question: Custom Metric Handler** # Description: You are required to implement a custom metric handler for TorchElastic that logs training metrics to a file. This will help you demonstrate your understanding of PyTorch\'s TorchElastic framework and your ability to customize and extend its functionalities. # Objective: 1. Implement a custom `FileMetricHandler` that writes metrics data to a specified log file. 2. Integrate this custom handler into a TorchElastic launcher script. # Requirements: - The custom metric handler should extend `torch.distributed.elastic.metrics.MetricHandler` and implement the `emit` method. - The handler should write the metrics to a specified log file in a human-readable format. - The launcher script should be able to configure and use this custom handler. # Task: 1. **Metric Handler Implementation:** Implement a class `FileMetricHandler` that: - Extends `torch.distributed.elastic.metrics.MetricHandler`. - Takes a file path as an argument and writes the metric data to this file. - Implements the `emit` method to write the metrics in a format: `Metric Name: Value at Timestamp`. 2. **Launcher Script:** Modify the provided launcher script to: - Configure the `FileMetricHandler` with a given log file path. - Create an instance of `LocalElasticAgent` with the modified spec including the custom metric handler. - Run the agent and ensure that metrics are correctly logged to the file. # Instructions: - Submit the Python code for the `FileMetricHandler` class. - Submit the modified launcher script demonstrating the use of `FileMetricHandler`. # Example Input: You should assume the following input for your script. The metrics data might look like: ```python metrics_data = [ {\\"name\\": \\"accuracy\\", \\"value\\": 0.95, \\"timestamp\\": \\"2023-10-01 10:00:00\\"}, {\\"name\\": \\"loss\\", \\"value\\": 0.05, \\"timestamp\\": \\"2023-10-01 10:00:00\\"} ] ``` Expected format in the logfile: ```plaintext accuracy: 0.95 at 2023-10-01 10:00:00 loss: 0.05 at 2023-10-01 10:00:00 ``` # Constraints: - The solution should be efficient and write data to the file in real-time. - Handle any exceptions that might occur during the writing process. - Ensure that the log file can be read by other processes if needed. # FileMetricHandler Template: ```python import torch.distributed.elastic.metrics as metrics class FileMetricHandler(metrics.MetricHandler): def __init__(self, file_path): self.file_path = file_path def emit(self, metric_data: metrics.MetricData): # Write your code here to log metric_data to the file. ``` **Launcher Script Template:** ```python import torch import torch.distributed.elastic.metrics as metrics from my_module import FileMetricHandler # Ensure your handler is importable def main(log_file_path): metrics.configure(FileMetricHandler(log_file_path)) spec = WorkerSpec( local_world_size=1, fn=trainer_entrypoint_fn, args=(trainer_entrypoint_fn_args...), rdzv_handler=rdzv_handler, max_restarts=3, monitor_interval=5, ) agent = LocalElasticAgent(spec, start_method=\\"spawn\\") agent.run() if __name__ == \\"__main__\\": log_file_path = \\"path_to_your_log_file.log\\" main(log_file_path) ``` Ensure that the entire modification and handler implementation are submitted for the review.","solution":"import torch.distributed.elastic.metrics as metrics class FileMetricHandler(metrics.MetricHandler): def __init__(self, file_path): self.file_path = file_path def emit(self, metric_data: metrics.MetricData): with open(self.file_path, \'a\') as file: for metric in metric_data: line = f\\"{metric[\'name\']}: {metric[\'value\']} at {metric[\'timestamp\']}n\\" file.write(line)"},{"question":"# Question: Custom Sorting and Filtering using Operator Module You are given a list of dictionaries, where each dictionary represents data about a group of students. Each dictionary contains the following fields: - `name` (string): The name of the student. - `age` (int): The age of the student. - `grade` (int): The grade of the student. ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grade\\": 73}, {\\"name\\": \\"Charlie\\", \\"age\\": 20, \\"grade\\": 90}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"grade\\": 85} ] ``` Your task is to implement a function `process_students(students, age_limit, min_grade)` that accepts the following arguments: - `students`: A list of dictionaries, where each dictionary contains information about a student. - `age_limit`: An integer representing the maximum age limit to filter the students. - `min_grade`: An integer representing the minimum acceptable grade to filter the students. The function should: 1. Filter out students whose age is greater than `age_limit`. 2. Further filter out students whose grade is less than `min_grade`. 3. Sort the remaining students primarily by grade in descending order, and secondarily by name in ascending order. The function should use appropriate functions from the `operator` module to accomplish the task. Finally, the function should return the filtered and sorted list of dictionaries. # Input - `students`: List of dictionaries, each representing a student\'s details. - `age_limit`: Integer, the maximum age limit for filtering. - `min_grade`: Integer, the minimum grade for filtering. # Output - A list of dictionaries representing the filtered and sorted students. # Example Given the input: ```python students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grade\\": 73}, {\\"name\\": \\"Charlie\\", \\"age\\": 20, \\"grade\\": 90}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"grade\\": 85} ] age_limit = 21 min_grade = 80 ``` The expected output: ```python [ {\\"name\\": \\"Charlie\\", \\"age\\": 20, \\"grade\\": 90}, {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"grade\\": 85} ] ``` # Constraints - The number of students will not exceed 10^3. - The age of students will be between 18 and 100. - The grade of students will be between 0 and 100. You may assume that the values in the input dictionaries are valid and formatted correctly. # Implementation ```python def process_students(students, age_limit, min_grade): import operator # Filter by age and grade filtered_students = [student for student in students if operator.le(student[\'age\'], age_limit) and operator.ge(student[\'grade\'], min_grade)] # Sort by grade descending, then by name ascending sorted_students = sorted(filtered_students, key=operator.itemgetter(\'grade\', \'name\'), reverse=True) sorted_students.sort(key=operator.itemgetter(\'name\')) return sorted_students # Example usage students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grade\\": 73}, {\\"name\\": \\"Charlie\\", \\"age\\": 20, \\"grade\\": 90}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"grade\\": 85} ] age_limit = 21 min_grade = 80 print(process_students(students, age_limit, min_grade)) ```","solution":"def process_students(students, age_limit, min_grade): import operator # Filter by age and grade filtered_students = [student for student in students if operator.le(student[\'age\'], age_limit) and operator.ge(student[\'grade\'], min_grade)] # Sort by grade descending, then by name ascending sorted_students = sorted(filtered_students, key=lambda x: (-x[\'grade\'], x[\'name\'])) return sorted_students # Example usage students = [ {\\"name\\": \\"Alice\\", \\"age\\": 20, \\"grade\\": 88}, {\\"name\\": \\"Bob\\", \\"age\\": 22, \\"grade\\": 73}, {\\"name\\": \\"Charlie\\", \\"age\\": 20, \\"grade\\": 90}, {\\"name\\": \\"David\\", \\"age\\": 21, \\"grade\\": 85} ] age_limit = 21 min_grade = 80 print(process_students(students, age_limit, min_grade))"},{"question":"# Python Coding Assessment: Abstract Syntax Tree Analysis Objective Demonstrate your understanding of Python\'s Abstract Syntax Trees (AST) by writing a function that analyzes Python code to extract and return specific information about function definitions within the given code. Task Write a Python function `extract_function_info` that takes a string `source_code` representing Python code as input and returns a list of dictionaries. Each dictionary should contain information about a function\'s name, the number of arguments it has, and a list of its argument names. Requirements 1. Use the `ast` module to parse the `source_code`. 2. Traverse the AST to extract information about each function defined in the code. 3. Return the information in the specified format. Input * `source_code` (str): A string containing valid Python code which may include multiple function definitions. Output * List[Dict]: A list of dictionaries, where each dictionary contains: * `name` (str): The name of the function. * `num_args` (int): The number of arguments the function takes. * `arg_names` (List[str]): A list of the argument names. Example ```python source_code = \'\'\' def add(a, b): return a + b def greet(name): print(f\\"Hello, {name}!\\") def complex_function(x, y, z=5, *args, **kwargs): pass \'\'\' output = extract_function_info(source_code) ``` Expected output: ```python [ {\'name\': \'add\', \'num_args\': 2, \'arg_names\': [\'a\', \'b\']}, {\'name\': \'greet\', \'num_args\': 1, \'arg_names\': [\'name\']}, {\'name\': \'complex_function\', \'num_args\': 3, \'arg_names\': [\'x\', \'y\', \'z\']} ] ``` Constraints 1. Only consider top-level function definitions (do not consider nested functions). 2. Do not include any lambdas or class methods unless they are defined at the top-level. Hints 1. Use the `ast.parse` function to convert the source code into an AST. 2. Implement a class that inherits from `ast.NodeVisitor` to traverse the AST and collect the required information. Performance - The solution should efficiently handle typical script sizes (up to a few hundred lines of code).","solution":"import ast def extract_function_info(source_code): Analyze Python source code and return information about function definitions. Parameters: source_code (str): Python source code Returns: List[Dict]: List of dictionaries with function information class FunctionInfoExtractor(ast.NodeVisitor): def __init__(self): self.functions = [] def visit_FunctionDef(self, node): func_info = { \'name\': node.name, \'num_args\': len(node.args.args + node.args.kwonlyargs), \'arg_names\': [arg.arg for arg in node.args.args] } self.functions.append(func_info) self.generic_visit(node) tree = ast.parse(source_code) extractor = FunctionInfoExtractor() extractor.visit(tree) return extractor.functions"},{"question":"# Pandas Coding Assessment Question Objective To assess students\' understanding of pandas exception handling and assertion functions. Problem Statement You are given a function `process_and_validate_dataframes` that takes two pandas DataFrames as input. Your task is to: 1. Implement the function `process_and_validate_dataframes` which adds a new column `sum_column` to each DataFrame. This column should be the sum of all numerical columns in the respective DataFrame. 2. Ensure robust exception handling within your function for the following scenarios: - If an input is not a DataFrame, raise `errors.DataError` with an appropriate message. - If one of the DataFrames is empty, raise `errors.EmptyDataError` with an appropriate message. - If there is an indexing error, raise `errors.IndexingError` with an appropriate message. 3. Use `pandas.testing.assert_frame_equal` to compare the modified DataFrames to ensure they are identical. If they are not identical, raise an `AssertionError` with a custom message. Function Signature ```python import pandas as pd from pandas import errors import pandas.testing as testing def process_and_validate_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> None: pass ``` Example Usage ```python df1 = pd.DataFrame({ \'A\': [1, 2, 3], \'B\': [4, 5, 6] }) df2 = pd.DataFrame({ \'A\': [1, 2, 3], \'B\': [4, 5, 6] }) process_and_validate_dataframes(df1, df2) # This should pass without any exception df3 = pd.DataFrame({ \'A\': [7, 8, 9], \'B\': [10, 11, 12] }) try: process_and_validate_dataframes(df1, df3) except AssertionError as e: print(e) # This should print an appropriate assertion error message try: process_and_validate_dataframes(df1, \\"not_a_dataframe\\") except errors.DataError as e: print(e) # This should print \\"Input is not a DataFrame.\\" empty_df = pd.DataFrame() try: process_and_validate_dataframes(df1, empty_df) except errors.EmptyDataError as e: print(e) # This should print \\"One of the DataFrames is empty.\\" ``` Constraints - You must not use any loops. - The input DataFrames may contain missing values which should be handled appropriately. - Ensure the solution is efficient and handles large DataFrames within a reasonable time. Performance Requirement Your solution should efficiently handle DataFrames with up to 10,000 rows and 100 columns.","solution":"import pandas as pd from pandas import errors import pandas.testing as testing def process_and_validate_dataframes(df1: pd.DataFrame, df2: pd.DataFrame) -> None: if not isinstance(df1, pd.DataFrame): raise errors.DataError(\\"First input is not a DataFrame.\\") if not isinstance(df2, pd.DataFrame): raise errors.DataError(\\"Second input is not a DataFrame.\\") if df1.empty: raise errors.EmptyDataError(\\"The first DataFrame is empty.\\") if df2.empty: raise errors.EmptyDataError(\\"The second DataFrame is empty.\\") try: df1[\'sum_column\'] = df1.select_dtypes(include=\'number\').sum(axis=1) df2[\'sum_column\'] = df2.select_dtypes(include=\'number\').sum(axis=1) except IndexError as e: raise errors.IndexingError(\\"Indexing error occurred while adding sum_column.\\") from e try: testing.assert_frame_equal(df1, df2) except AssertionError as e: raise AssertionError(\\"DataFrames are not identical.\\") from e"},{"question":"Objective You are required to use the `seaborn.objects` module to visualize a dataset. Your task is to create a custom plot that demonstrates your understanding of seaborn\'s advanced plotting capabilities. Task Using the `penguins` dataset provided by seaborn, create a plot that fulfills the following requirements: 1. Visualize the relationship between \\"species\\" and \\"body_mass_g\\". 2. Add `Dash` marks to represent data points, and customize their appearance using the `alpha`, `linewidth`, and `width` properties. 3. Use `Dodge` to separate data points by the \\"sex\\" of the penguins. 4. In addition to the `Dash` marks, add dots representing data points with `Jitter` to avoid overlap. 5. Ensure the plot clearly differentiates between the different species and sexes of penguins. Input - You should load the `penguins` dataset using `seaborn.load_dataset(\\"penguins\\")`. - Create a plot using the functions and methods from `seaborn.objects`. Output - Display the plot. Implementation Constraints - You must use the `seaborn.objects` module for creating and customizing the plot. - The appearance of the plot (including the color, transparency, and size of the marks) should be distinctly customized using the provided properties. Example Solution The following is an example of how you might start your solution: ```python import seaborn.objects as so from seaborn import load_dataset # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, \\"species\\", \\"body_mass_g\\", color=\\"sex\\") ( p.add(so.Dash(alpha=.5, linewidth=\\"flipper_length_mm\\", width=.5)) .add(so.Dots(), so.Dodge(), so.Jitter()) ) # Display the plot p.show() ``` # Assessment Criteria - Correct usage of `seaborn.objects` module functions and methods. - Proper customization of mark properties as specified. - Clear and effective visualization of the dataset based on the criteria.","solution":"import seaborn.objects as so from seaborn import load_dataset def create_penguin_plot(): # Load the penguins dataset penguins = load_dataset(\\"penguins\\") # Create the plot p = so.Plot(penguins, x=\\"species\\", y=\\"body_mass_g\\", color=\\"sex\\") ( p.add(so.Dash(alpha=.7, linewidth=1.5, width=.5)) .add(so.Dot(alpha=.8), so.Dodge(), so.Jitter(width=0.2)) .scale(color=\\"dark\\") ) # Display the plot p.show()"},{"question":"**Title:** Predicting House Prices using Multiple Regression Techniques in scikit-learn **Objective:** To assess students\' understanding of using different regression techniques provided by scikit-learn for a real-world problem, namely predicting house prices. **Problem Statement:** You are provided with a dataset containing various features of houses and their corresponding prices. Your task is to implement and evaluate different regression models using scikit-learn to predict the prices of houses. **Datasets:** - You are provided with two CSV files: `train.csv` and `test.csv`. - Each file has columns representing features of the houses and the target column `price` representing the house price. **Tasks:** 1. **Data Preprocessing:** - Load the training data and testing data using Pandas. - Handle any missing values in the dataset appropriately. - Split the training data into features (X) and target (y). - Standardize the features for better performance of the models. 2. **Model Implementation:** - Implement the following regression models using scikit-learn: - Linear Regression - Ridge Regression - Lasso Regression - Train each model on the training dataset. 3. **Model Evaluation:** - Evaluate the models on the test dataset using appropriate metrics (e.g., Mean Absolute Error, Mean Squared Error, R^2 Score). - Print the evaluation metric scores for each model. 4. **Model Selection:** - Based on the evaluation metrics, decide which model performs the best on the test dataset. - Provide a brief justification for why you selected the particular model. **Input:** - `train.csv` and `test.csv` files, each containing the same structure of columns for house features and a `price` column. **Output:** - Print the evaluation metric scores for each model. - Print the selected model with the justification for selecting it. **Constraints:** - Ensure your code is well-optimized and runs efficiently on large datasets. - Handle edge cases such as missing values and potential outliers in the dataset. **Sample Code Structure:** ```python import pandas as pd from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # Load datasets train_data = pd.read_csv(\'train.csv\') test_data = pd.read_csv(\'test.csv\') # Data preprocessing def preprocess_data(data): # Handle missing values data = data.fillna(data.mean()) X = data.drop(\'price\', axis=1) y = data[\'price\'] return X, y X_train, y_train = preprocess_data(train_data) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) # Define models models = { \'LinearRegression\': LinearRegression(), \'Ridge\': Ridge(), \'Lasso\': Lasso() } # Train and evaluate models for name, model in models.items(): model.fit(X_train, y_train) X_test, y_test = preprocess_data(test_data) X_test = scaler.transform(X_test) y_pred = model.predict(X_test) # Evaluation metrics print(f\\"Model: {name}\\") print(f\\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred)}\\") print(f\\"Mean Squared Error: {mean_squared_error(y_test, y_pred)}\\") print(f\\"R^2 Score: {r2_score(y_test, y_pred)}\\") print() # Select and justify the best model # (Implement your logic here based on the printed metrics) ``` **Notes:** - This problem requires you to use multiple modules from scikit-learn (`linear_model`, `metrics`, `model_selection`, `preprocessing`). - Ensure you include necessary comments and explanations in your code for understanding your approach.","solution":"import pandas as pd from sklearn.linear_model import LinearRegression, Ridge, Lasso from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # Load datasets def load_data(train_path=\'train.csv\', test_path=\'test.csv\'): train_data = pd.read_csv(train_path) test_data = pd.read_csv(test_path) return train_data, test_data # Preprocess data def preprocess_data(data): # Handle missing values data = data.fillna(data.mean()) X = data.drop(\'price\', axis=1) y = data[\'price\'] return X, y # Evaluate model performance def evaluate_model(model, X_test, y_test): y_pred = model.predict(X_test) mae = mean_absolute_error(y_test, y_pred) mse = mean_squared_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return mae, mse, r2 def main(): train_data, test_data = load_data() X_train, y_train = preprocess_data(train_data) X_test, y_test = preprocess_data(test_data) # Standardize features scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Define models models = { \'LinearRegression\': LinearRegression(), \'Ridge\': Ridge(), \'Lasso\': Lasso() } # Train and evaluate models model_scores = {} for name, model in models.items(): model.fit(X_train, y_train) mae, mse, r2 = evaluate_model(model, X_test, y_test) model_scores[name] = { \'MAE\': mae, \'MSE\': mse, \'R2\': r2 } print(f\\"Model: {name}\\") print(f\\"Mean Absolute Error: {mae}\\") print(f\\"Mean Squared Error: {mse}\\") print(f\\"R^2 Score: {r2}\\") print() # Select and justify the best model best_model_name = min(model_scores, key=lambda x: model_scores[x][\'MAE\']) # Assuming MAE is our primary metric print(f\\"Selected Model: {best_model_name}\\") print(f\\"Reason: It has the lowest Mean Absolute Error (MAE).\\") if __name__ == \\"__main__\\": main()"},{"question":"# Question: Implementing and Evaluating Stochastic Gradient Descent for Classification and Regression Objective: Implement a classification and a regression model using Scikit-learn\'s `SGDClassifier` and `SGDRegressor`, respectively. Evaluate their performance on given datasets. Instructions: 1. Implement a function `sgd_classification` that trains an `SGDClassifier` on a given training dataset and evaluates its performance on a test dataset. 2. Implement a function `sgd_regression` that trains an `SGDRegressor` on a given training dataset and evaluates its performance on a test dataset. Functions to Implement: * `sgd_classification(X_train, y_train, X_test, y_test, loss=\'hinge\', penalty=\'l2\', max_iter=1000, random_state=42)` - Inputs: - `X_train` (2D array-like): Training samples of shape `(n_samples_train, n_features)`. - `y_train` (1D array-like): Training labels of shape `(n_samples_train,)`. - `X_test` (2D array-like): Test samples of shape `(n_samples_test, n_features)`. - `y_test` (1D array-like): Test labels of shape `(n_samples_test,)`. - `loss` (str): The loss function to be used. Default is \'hinge\'. - `penalty` (str): The penalty (aka regularization term) to be used. Default is \'l2\'. - `max_iter` (int): The maximum number of iterations. Default is 1000. - `random_state` (int): The seed of the pseudo-random number generator. Default is 42. - Outputs: - `accuracy` (float): The classification accuracy on the test dataset. - `conf_matrix` (2D array): The confusion matrix of the test dataset. * `sgd_regression(X_train, y_train, X_test, y_test, loss=\'squared_error\', penalty=\'l2\', max_iter=1000, random_state=42)` - Inputs: - `X_train` (2D array-like): Training samples of shape `(n_samples_train, n_features)`. - `y_train` (1D array-like): Training values of shape `(n_samples_train,)`. - `X_test` (2D array-like): Test samples of shape `(n_samples_test, n_features)`. - `y_test` (1D array-like): Test values of shape `(n_samples_test,)`. - `loss` (str): The loss function to be used. Default is \'squared_error\'. - `penalty` (str): The penalty (aka regularization term) to be used. Default is \'l2\'. - `max_iter` (int): The maximum number of iterations. Default is 1000. - `random_state` (int): The seed of the pseudo-random number generator. Default is 42. - Outputs: - `mean_squared_error` (float): The mean squared error on the test dataset. - `coef` (1D array): The coefficients of the linear model. Example Usage: ```python from sklearn.datasets import make_classification, make_regression from sklearn.metrics import confusion_matrix # Generate a random classification problem X_train_clf, y_train_clf = make_classification(n_samples=1000, n_features=20, random_state=42) X_test_clf, y_test_clf = make_classification(n_samples=200, n_features=20, random_state=42) # Generate a random regression problem X_train_reg, y_train_reg = make_regression(n_samples=1000, n_features=20, random_state=42) X_test_reg, y_test_reg = make_regression(n_samples=200, n_features=20, random_state=42) # Classification accuracy, conf_matrix = sgd_classification(X_train_clf, y_train_clf, X_test_clf, y_test_clf) print(f\\"Classification Accuracy: {accuracy}\\") print(f\\"Confusion Matrix:n{conf_matrix}\\") # Regression mse, coef = sgd_regression(X_train_reg, y_train_reg, X_test_reg, y_test_reg) print(f\\"Mean Squared Error: {mse}\\") print(f\\"Coefficients:n{coef}\\") ``` Constraints: - You must use Scikit-learn\'s `SGDClassifier` and `SGDRegressor`. - Ensure that you standardize the features using `StandardScaler`. - Train the models with a maximum of 1000 iterations or until convergence. - Use `random_state=42` for reproducibility. Performance Requirements: - For classification, evaluate and display the accuracy and the confusion matrix. - For regression, evaluate and display the mean squared error (MSE) and the model coefficients. Good luck!","solution":"from sklearn.linear_model import SGDClassifier, SGDRegressor from sklearn.preprocessing import StandardScaler from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error import numpy as np def sgd_classification(X_train, y_train, X_test, y_test, loss=\'hinge\', penalty=\'l2\', max_iter=1000, random_state=42): # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize and train the SGD classifier clf = SGDClassifier(loss=loss, penalty=penalty, max_iter=max_iter, random_state=random_state) clf.fit(X_train_scaled, y_train) # Predict on the test data y_pred = clf.predict(X_test_scaled) # Evaluate accuracy and produce confusion matrix accuracy = accuracy_score(y_test, y_pred) conf_matrix = confusion_matrix(y_test, y_pred) return accuracy, conf_matrix def sgd_regression(X_train, y_train, X_test, y_test, loss=\'squared_error\', penalty=\'l2\', max_iter=1000, random_state=42): # Standardize the features scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # Initialize and train the SGD regressor reg = SGDRegressor(loss=loss, penalty=penalty, max_iter=max_iter, random_state=random_state) reg.fit(X_train_scaled, y_train) # Predict on the test data y_pred = reg.predict(X_test_scaled) # Evaluate mean squared error and get the coefficients mse = mean_squared_error(y_test, y_pred) coef = reg.coef_ return mse, coef"},{"question":"**Coding Assessment Question** # Objective Implement a Python function that sets a non-blocking lock on a file using the `fcntl` module. If the lock cannot be acquired immediately, the function should return a specific error message. # Problem Statement You are given a path to a file. Implement the function `set_non_blocking_lock(file_path: str) -> str` which tries to set a non-blocking lock on the file specified by `file_path` using an exclusive lock (`LOCK_EX`). If the lock is successfully acquired, the function should return the message `\\"Lock acquired\\"`. If the lock cannot be acquired immediately because the file is already locked by another process, it should return the message `\\"Lock is already held by another process\\"`. # Function Signature ```python def set_non_blocking_lock(file_path: str) -> str: pass ``` # Input - `file_path` (str): A string representing the path to a file. # Output - A string message that is either `\\"Lock acquired\\"` or `\\"Lock is already held by another process\\"`. # Constraints - The function should use the `fcntl.flock` method to attempt to set the non-blocking lock. - Use the appropriate constants (`LOCK_EX` for an exclusive lock and `LOCK_NB` to avoid blocking). - Handle any potential `OSError` exceptions that inform whether the lock is already held. # Example ```python assert set_non_blocking_lock(\\"/tmp/test_file.txt\\") == \\"Lock acquired\\" assert set_non_blocking_lock(\\"/tmp/test_file.txt\\") == \\"Lock is already held by another process\\" ``` # Notes - You may need to create and open the file in your solution if it does not exist. - Ensure your implementation is compliant with Unix systems as `fcntl` operates within this environment.","solution":"import fcntl import os def set_non_blocking_lock(file_path: str) -> str: Tries to set a non-blocking lock on the file specified by file_path using an exclusive lock (LOCK_EX). If the lock is successfully acquired, returns \\"Lock acquired\\". If the lock cannot be acquired immediately, returns \\"Lock is already held by another process\\". try: with open(file_path, \'a\') as file: try: fcntl.flock(file, fcntl.LOCK_EX | fcntl.LOCK_NB) return \\"Lock acquired\\" except OSError: return \\"Lock is already held by another process\\" except FileNotFoundError: return \\"File not found\\""},{"question":"**Question: Understanding and Utilizing Meta Tensors in PyTorch** In this question, you will demonstrate your understanding of the \\"meta\\" device in PyTorch by performing several tasks involving meta tensors. Ensure to follow the instructions step-by-step and provide the expected outputs. **Instructions:** 1. **Loading Models with Meta Device:** - Load a sample model onto the meta device without loading the actual model parameters. Use a simple neural network model for this demonstration. 2. **Performing Operations on Meta Tensors:** - Create a tensor on the meta device with shape (3, 4) filled with random values using `torch.randn`. - Perform an element-wise addition of this tensor with another tensor of the same shape, also on the meta device. 3. **Handling Meta Tensors in NN Modules:** - Define a simple Linear layer using the meta device and print its representation to show all attributes (in_features, out_features, bias). 4. **Reinitializing and Moving to CPU:** - Move the Linear layer created in the previous step to the CPU device without initializing its parameters. - After moving, reinitialize the parameters using a normal distribution and print the initialized parameters. **Expected Input and Output:** 1. **Loading Models with Meta Device:** - Input: None - Output: Representation of the model loaded onto the meta device. 2. **Performing Operations on Meta Tensors:** - Input: None - Output: The representation of the resulting tensor after addition (since it\'s a meta tensor, it won\'t contain actual data). 3. **Handling Meta Tensors in NN Modules:** - Input: None - Output: Representation of the Linear layer created on the meta device. 4. **Reinitializing and Moving to CPU:** - Input: None - Output: Initialized parameters of the Linear layer after moving to CPU. **Constraints:** - You should not store any actual data in the meta tensors. - Ensure that you handle any potential errors that might arise due to operations on meta tensors. **Code Template:** ```python import torch import torch.nn as nn # Task 1: Loading Models with Meta Device def load_model_meta(): # Define your model architecture here, for example using nn.Sequential model = nn.Sequential( nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2) ) # Save model (this would normally be to disk, but to simplify, assume it\'s saved) torch.save(model, \'sample_model.pt\') # Load model to meta device meta_model = torch.load(\'sample_model.pt\', map_location=\'meta\') return meta_model # Task 2: Performing Operations on Meta Tensors def perform_meta_operations(): with torch.device(\'meta\'): t1 = torch.randn(3, 4) t2 = torch.randn(3, 4) result = t1 + t2 return result # Task 3: Handling Meta Tensors in NN Modules def meta_nn_module(): with torch.device(\'meta\'): linear_layer = nn.Linear(20, 30) return linear_layer # Task 4: Reinitializing and Moving to CPU def reinitialize_and_move(): with torch.device(\'meta\'): linear_layer = nn.Linear(20, 30) linear_layer.to_empty(device=\\"cpu\\") nn.init.normal_(linear_layer.weight) if linear_layer.bias is not None: nn.init.normal_(linear_layer.bias) return linear_layer # Test Functions if __name__ == \\"__main__\\": print(load_model_meta()) print(perform_meta_operations()) print(meta_nn_module()) print(reinitialize_and_move()) ``` **Explanation:** - `load_model_meta`: Loads a predefined model onto the meta device. - `perform_meta_operations`: Performs an addition operation on two random meta tensors. - `meta_nn_module`: Creates and prints a Linear layer with meta tensors. - `reinitialize_and_move`: Moves the Linear layer to CPU, reinitializes the parameters, and prints them. Ensure that your code is clean, well-commented, and handles any edge cases related to meta tensors.","solution":"import torch import torch.nn as nn # Task 1: Loading Models with Meta Device def load_model_meta(): # Define a simple neural network model model = nn.Sequential( nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 2) ) # Save model (this would normally be to disk, but to simplify, we skip actual saving) torch.save(model, \'sample_model.pt\') # Load model to meta device meta_model = torch.load(\'sample_model.pt\', map_location=\'meta\') return meta_model # Task 2: Performing Operations on Meta Tensors def perform_meta_operations(): t1 = torch.randn(3, 4, device=\'meta\') t2 = torch.randn(3, 4, device=\'meta\') result = t1 + t2 return result # Task 3: Handling Meta Tensors in NN Modules def meta_nn_module(): linear_layer = nn.Linear(20, 30, device=\'meta\') return linear_layer # Task 4: Reinitializing and Moving to CPU def reinitialize_and_move(): linear_layer = nn.Linear(20, 30, device=\'meta\') linear_layer = linear_layer.to_empty(device=\'cpu\') nn.init.normal_(linear_layer.weight) if linear_layer.bias is not None: nn.init.normal_(linear_layer.bias) return linear_layer"},{"question":"**Advanced Calendar Manipulation in Python** You are tasked with implementing several functions that make use of Python\'s `calendar` module to perform various calendar-related computations and formatting. # Function 1: `generate_year_calendar` Implement a function `generate_year_calendar(year: int) -> List[str]` that generates the calendar for a given year and returns a list of strings, each representing a month\'s calendar in plain text. - **Input**: - `year` (int): The year for which the calendar should be generated. - **Output**: - A list of 12 strings, each string representing a month\'s calendar in plain text format. # Function 2: `get_friday_13ths` Implement a function `get_friday_13ths(start_year: int, end_year: int) -> List[Tuple[int, int]]` that identifies all Fridays that fall on the 13th day of the month within a specified year range. - **Input**: - `start_year` (int): The start year of the range. - `end_year` (int): The end year of the range. - **Output**: - A list of tuples, each tuple containing (year, month) where a Friday falls on the 13th day of the month. # Function 3: `count_weekday_in_year` Implement a function `count_weekday_in_year(year: int, weekday: int) -> int` that counts how many times a specific weekday (e.g., Monday, Tuesday, etc.) occurs in the given year. - **Input**: - `year` (int): The year to analyze. - `weekday` (int): The weekday to count (0 = Monday, 6 = Sunday). - **Output**: - An integer representing the number of times the specified weekday occurs in the given year. # Constraints - The year values should be in the Gregorian calendar extended indefinitely in both directions. - The module\'s `Calendar` class and its subclasses should be utilized in the function implementations. - Make sure to handle edge cases, such as leap years, correctly. **Example Usage** ```python # Example usage of generate_year_calendar year_calendar = generate_year_calendar(2023) for month in year_calendar: print(month) # Example usage of get_friday_13ths friday_13ths = get_friday_13ths(2000, 2020) print(friday_13ths) # [(2000, 10), (2001, 4), (2001, 7), ... ] # Example usage of count_weekday_in_year monday_count = count_weekday_in_year(2023, calendar.MONDAY) print(monday_count) # e.g., 52 ``` **Note**: To pass the assessment, your implementations should correctly make use of the `calendar` module\'s functionalities, handle edge cases, and provide accurate results based on the given inputs.","solution":"import calendar from typing import List, Tuple def generate_year_calendar(year: int) -> List[str]: Generates the calendar for a given year and returns a list of strings, each representing a month\'s calendar in plain text. cal = calendar.TextCalendar() year_calendar = [cal.formatmonth(year, month) for month in range(1, 13)] return year_calendar def get_friday_13ths(start_year: int, end_year: int) -> List[Tuple[int, int]]: Identifies all Fridays that fall on the 13th day of the month within a specified year range and returns a list of tuples (year, month). friday_13ths = [] for year in range(start_year, end_year + 1): for month in range(1, 13): if calendar.weekday(year, month, 13) == calendar.FRIDAY: friday_13ths.append((year, month)) return friday_13ths def count_weekday_in_year(year: int, weekday: int) -> int: Counts how many times a specific weekday (e.g., Monday, Tuesday, etc.) occurs in the given year. weekday_count = 0 for month in range(1, 13): for day in range(1, calendar.monthrange(year, month)[1] + 1): if calendar.weekday(year, month, day) == weekday: weekday_count += 1 return weekday_count"},{"question":"Create and Validate a Custom WSGI Application # Objective To assess your understanding of the `wsgiref` module and your ability to create and validate a custom WSGI application using Python. # Problem Statement You are required to implement a custom WSGI application that returns a personalized greeting message based on the query string parameter provided in the URL. The application should comply with the WSGI specification and make use of the `wsgiref` module for implementation and validation. Requirements: 1. **Function Interface**: Define a function `personalized_greeting_app(environ, start_response)` that will serve as the WSGI application. 2. **Input**: - `environ`: A dictionary containing CGI environment variables. - `start_response`: A callable accepting a status and a list of response headers. 3. **Output**: - The WSGI application must return an iterable with the response body as a byte string. 4. **Functionality**: - The application should read the query string parameter `name` from the URL. - If the `name` parameter is provided, respond with `\\"Hello, <name>!\\"`. - If the `name` parameter is missing, respond with `\\"Hello, World!\\"`. 5. **Use `wsgiref.util` Functions**: Use `wsgiref.util.request_uri(environ, include_query=True)` to reconstruct the request URI. 6. **Validation**: - Use `wsgiref.validate.validator(application)` to validate your WSGI application. Example: - URL: `http://localhost:8000/?name=Alice` - Response: `\\"Hello, Alice!\\"` - URL: `http://localhost:8000/` - Response: `\\"Hello, World!\\"` # Constraints: - Ensure the response headers include `(\'Content-type\', \'text/plain; charset=utf-8\')`. # Implementation Details: - You should use the `wsgiref.simple_server` to create and run your WSGI server. - Validate your WSGI application using `wsgiref.validate`. # Solution Template ```python from wsgiref.simple_server import make_server from wsgiref.util import request_uri from wsgiref.validate import validator def personalized_greeting_app(environ, start_response): # Reconstruct the full request URI and retrieve the query string uri = request_uri(environ, include_query=True) # Parse the query string to get the \'name\' parameter query_string = environ.get(\'QUERY_STRING\', \'\') name = \'World\' # Default name if query param is not provided if \'name=\' in query_string: # Extract the value for the \'name\' parameter name = query_string.split(\'name=\')[1].split(\'&\')[0] # Prepare the greeting message message = f\'Hello, {name}!\'.encode(\'utf-8\') # Construct response status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) # Return the response body as an iterable byte string return [message] # Wrap the application in a validator to ensure WSGI compliance validated_app = validator(personalized_greeting_app) if __name__ == \'__main__\': # Create a WSGI server with the validated application with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever() ``` # Submission Complete the `personalized_greeting_app(environ, start_response)` function and ensure it meets the requirements outlined above. Validate your solution and test it by running the WSGI server and making requests to it.","solution":"from wsgiref.simple_server import make_server from wsgiref.util import request_uri from wsgiref.validate import validator from urllib.parse import parse_qs def personalized_greeting_app(environ, start_response): # Parse the query string to get the \'name\' parameter query_string = environ.get(\'QUERY_STRING\', \'\') params = parse_qs(query_string) name = params.get(\'name\', [\'World\'])[0] # Prepare the greeting message message = f\'Hello, {name}!\'.encode(\'utf-8\') # Construct response status = \'200 OK\' headers = [(\'Content-type\', \'text/plain; charset=utf-8\')] start_response(status, headers) # Return the response body as an iterable byte string return [message] # Wrap the application in a validator to ensure WSGI compliance validated_app = validator(personalized_greeting_app) if __name__ == \'__main__\': # Create a WSGI server with the validated application with make_server(\'\', 8000, validated_app) as httpd: print(\\"Serving on port 8000...\\") httpd.serve_forever()"},{"question":"# Custom Type Implementation in Python\'s `PyTypeObject` Problem Statement: You are required to create a custom sequence type using Python\'s C API. This type, `CustomSequence`, should model a list-like structure defined using `PyTypeObject`. The type will support the most common operations expected from a sequence in Python. **Requirements:** 1. **Type Definition**: - Define a structure `CustomSequence` that includes necessary fields for a simple dynamic array. - Define a `PyTypeObject` for `CustomSequence`. 2. **Initialization**: - Implement the `tp_new` slot to allocate and initialize the object. - Implement the `tp_init` slot to prepare a `CustomSequence` instance with an initial capacity. 3. **Deallocation**: - Implement the `tp_dealloc` slot to manage resource cleanup when an object instance is destroyed. 4. **Sequence Methods**: - Implement methods for: - Getting the length of the sequence (`tp_as_sequence.sq_length`). - Accessing an item (`tp_as_sequence.sq_item`). - Setting an item (`tp_as_sequence.sq_ass_item`). 5. **Rich Comparison**: - Implement comparison functionality in `tp_richcompare`. 6. **String Representation**: - Implement the `tp_repr` slot to define a representation of the object when printed. Example Usage: ```python import custommodule cs = custommodule.CustomSequence([1, 2, 3]) print(cs) # CustomSequence([1, 2, 3]) print(len(cs)) # 3 print(cs[0]) # 1 cs[0] = 10 print(cs) # CustomSequence([10, 2, 3]) print(cs == [10, 2, 3]) # True ``` Constraints: - Ensure that your implementation correctly handles out-of-bounds access and assigns proper error messages. - Rich comparisons should work for equality and inequality only (`==` and `!=`). - Handle reference counting appropriately to avoid memory leaks. Performance: - The goal is to implement the sequence with a focus on correct behavior conforming to Python\'s built-in list-like behavior. This problem involves both Python and C. The `CustomSequence` needs to be created using the Python C API as a Python extension module. Write the C code required to define this new type and ensure it compiles and works correctly with Python. **Hint:** Use the examples provided in the documentation to guide your implementation of the `CustomSequence`.","solution":"class CustomSequence: def __init__(self, initial_data=None): if initial_data is None: self.data = [] else: self.data = list(initial_data) def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] def __setitem__(self, idx, value): self.data[idx] = value def __repr__(self): return f\\"CustomSequence({self.data})\\" def __eq__(self, other): return self.data == other"},{"question":"# Sound File Metadata Processor In this task, you will implement a function that processes multiple sound files, utilizes the `sndhdr` module to determine their metadata, and aggregates the results to provide a useful summary. Function Signature ```python def sound_file_summary(file_list: list) -> dict: Processes a list of sound files and returns an aggregated summary of their types, framerates, number of channels, and sampling widths. :param file_list: List of filenames corresponding to sound files. :return: A dictionary summarizing the types, total number for each type, average framerate, channels, and sample widths where applicable. ``` Input - `file_list` (List[str]): A list of filenames with sound files to be processed. Output - A dictionary with the following structure: ```python { \\"types\\": { \\"aifc\\": count, \\"aiff\\": count, ... }, \\"average_framerate\\": float, \\"average_channels\\": float, \\"sample_widths\\": { \\"8\\": count, \\"16\\": count, ... } } ``` - `\\"types\\"`: A dictionary where the keys are sound file types and values are their counts in `file_list`. - `\\"average_framerate\\"`: The average framerate of valid sound files where the framerate is known. - `\\"average_channels\\"`: The average number of channels in valid sound files where the number of channels is known. - `\\"sample_widths\\"`: A dictionary where the keys are sample widths (in bits or as \'A\' for A-LAW and \'U\' for u-LAW) and values are their counts. Constraints - Only sound files that can be successfully processed by `sndhdr.what` should be considered; skip files that return `None`. - The list `file_list` will have at most 1000 files. - Filenames in `file_list` are guaranteed to be valid file paths. Example ```python file_list = [\\"sound1.wav\\", \\"sound2.aif\\", \\"sound3.au\\", \\"invalid_file.txt\\"] summary = sound_file_summary(file_list) # Output Example: # { # \\"types\\": { # \\"wav\\": 1, # \\"aif\\": 1, # \\"au\\": 1 # }, # \\"average_framerate\\": 44100.0, # \\"average_channels\\": 2.0, # \\"sample_widths\\": { # \\"16\\": 2, # \\"8\\": 1 # } # } ``` Implement the `sound_file_summary` function to analyze and summarize the sound files in the provided `file_list` based on their metadata.","solution":"import sndhdr def sound_file_summary(file_list): result = { \\"types\\": {}, \\"average_framerate\\": 0.0, \\"average_channels\\": 0.0, \\"sample_widths\\": {}, } framerate_total = 0 channels_total = 0 framerate_count = 0 channels_count = 0 for file in file_list: info = sndhdr.what(file) if info: # Count types if info.filetype not in result[\'types\']: result[\'types\'][info.filetype] = 0 result[\'types\'][info.filetype] += 1 # Collect framerate if info.framerate: framerate_total += info.framerate framerate_count += 1 # Collect channels if info.channels: channels_total += info.channels channels_count += 1 # Count sample widths sample_width_key = str(info.sampwidth) if not isinstance(info.sampwidth, str) else info.sampwidth if sample_width_key not in result[\'sample_widths\']: result[\'sample_widths\'][sample_width_key] = 0 result[\'sample_widths\'][sample_width_key] += 1 # Calculate averages if framerate_count > 0: result[\'average_framerate\'] = framerate_total / framerate_count if channels_count > 0: result[\'average_channels\'] = channels_total / channels_count return result"},{"question":"# Debugging with Python `pdb` – Coding Assessment Objective: To assess students\' understanding of using the `pdb` module for debugging Python programs. Problem Statement: You are given a Python script called `my_script.py` with the following content: ```python def sample_function(a, b): c = a + b d = a * b e = d / c return e def main(): x = 10 y = 0 result = sample_function(x, y) print(f\\"Result is: {result}\\") if __name__ == \\"__main__\\": main() ``` This script contains a logical error that results in a `ZeroDivisionError`. Your task is to debug this script using the `pdb` module and provide solutions to the following tasks: 1. **Set a Breakpoint:** Use the `pdb` module to set a breakpoint at the start of the `sample_function`. 2. **Step Through Code:** Step through the code inside `sample_function` to identify the point where the error occurs. 3. **Print Variables:** Print the values of the variables `a`, `b`, `c`, `d`, and `e` at different execution points to understand their states just before the error occurs. 4. **Fix the Error:** Modify the script to handle the division by zero error gracefully. Implement error handling and ensure the program does not crash when `b` is zero. Provide the new version of the script. 5. **Explain Commands Used:** Provide an explanation of the `pdb` commands you used while debugging the program, focusing on the purpose and outcome of each command. Input and Output Format: - **Input:** The original `my_script.py` file. - **Output:** The modified script with error handling and a description of `pdb` commands used. Constraints: - You must use the `pdb` module to debug the script. - Provide comprehensive explanations for the `pdb` commands used. Performance Requirements: - The modified script should handle all possible values of `a` and `b` without crashing. **Note:** Include appropriate comments in your modified script reflecting the changes made. Submission: Submit the following: 1. The modified `my_script.py` file. 2. A separate text file containing explanations of the `pdb` commands used during debugging. # Example Solution Outline: ```python def sample_function(a, b): if b == 0: return float(\'inf\') # or some other way of handling divide by zero c = a + b d = a * b e = d / c return e def main(): x = 10 y = 0 result = sample_function(x, y) print(f\\"Result is: {result}\\") if __name__ == \\"__main__\\": main() ``` **Explanation of pdb commands:** - `break sample_function`: Sets a breakpoint at the start of `sample_function`. - `step`: Steps through each line of code inside `sample_function`. - `print(...)`: Prints the value of variables. - `continue`: Continues execution until the next breakpoint or error.","solution":"def sample_function(a, b): if b == 0: return float(\'inf\') # or some other way of handling divide by zero c = a + b d = a * b e = d / c return e def main(): x = 10 y = 0 result = sample_function(x, y) print(f\\"Result is: {result}\\") if __name__ == \\"__main__\\": main()"},{"question":"# Advanced Coding Assessment Question You are provided with a list of websites and a specific user-agent. Your task is to write a Python function that uses the `urllib.robotparser.RobotFileParser` class to determine whether a given user-agent can crawl specific pages from these websites. Function Signature ```python def analyze_websites(user_agent: str, websites: List[str], pages: List[str]) -> Dict[str, Dict[str, bool]]: pass ``` Input 1. `user_agent` (str): The user agent you will use to check access permissions. 2. `websites` (List[str]): A list of base URLs of the websites you want to analyze. 3. `pages` (List[str]): A list of relative paths of pages you need to check for each website. Output - Returns a dictionary where each key is a website URL from the input list, and each value is another dictionary. - The inner dictionary keys are the relative page paths (as provided in `pages`). - The inner dictionary values are booleans indicating whether the user agent is allowed to fetch that page (`True` if allowed, `False` otherwise). Constraints - You need to handle network errors gracefully and assume that if the `robots.txt` file cannot be fetched, the user agent is allowed to fetch all pages. - All websites must have a valid `robots.txt` file that follows standard conventions. Example ```python websites = [ \\"http://www.example.com\\", \\"http://www.testsite.com\\" ] pages = [ \\"/about\\", \\"/contact\\", \\"/products\\" ] user_agent = \\"MyUserAgent\\" result = analyze_websites(user_agent, websites, pages) # Expected output: # { # \\"http://www.example.com\\": { # \\"/about\\": True, # \\"/contact\\": False, # \\"/products\\": True # }, # \\"http://www.testsite.com\\": { # \\"/about\\": True, # \\"/contact\\": True, # \\"/products\\": True # } # } ``` Notes - Make sure to use the `urllib.robotparser.RobotFileParser` class methods as described in the provided documentation. - You may use Python standard libraries where necessary. - Optimize network calls to avoid fetching `robots.txt` multiple times for the same website. Performance Requirements - Efficient fetching and parsing of multiple `robots.txt` files, keeping network latency in mind. - Handle exceptions gracefully and ensure that the function returns results even if some `robots.txt` files cannot be accessed.","solution":"from typing import List, Dict import urllib.robotparser import urllib.request def analyze_websites(user_agent: str, websites: List[str], pages: List[str]) -> Dict[str, Dict[str, bool]]: results = {} for website in websites: try: robots_url = website.rstrip(\'/\') + \'/robots.txt\' rp = urllib.robotparser.RobotFileParser() rp.set_url(robots_url) rp.read() except Exception: # Assume access is allowed if unable to retrieve robots.txt rp = None pages_access = {} for page in pages: if rp is None or rp.can_fetch(user_agent, website.rstrip(\'/\') + page): pages_access[page] = True else: pages_access[page] = False results[website] = pages_access return results"},{"question":"# Pandas Timedelta Challenge **Objective:** Write a function to perform multiple operations involving pandas timedeltas. The function will accept a DataFrame containing various event timestamps and calculate additional timedeltas and statistics based on specific criteria. **Function Signature:** ```python def analyze_event_durations(events: pd.DataFrame) -> pd.DataFrame: pass ``` **Input:** - `events`: A pandas DataFrame with at least two columns: - `start_time`: A column containing the start timestamps of events (`datetime64[ns]`). - `end_time`: A column containing the end timestamps of events (`datetime64[ns]`). **Output:** - A pandas DataFrame containing: - `start_time`: Start times from the input DataFrame. - `end_time`: End times from the input DataFrame. - `duration`: Timedelta representing the duration between start_time and end_time. - `mean_duration`: Mean of all event durations (a constant value for all rows). - `duration_above_mean`: Boolean indicating whether the event duration is above the mean_duration. - `duration_components`: Components of each duration (days, hours, minutes, seconds). **Functionality Requirements:** 1. **Calculate the duration** between `start_time` and `end_time` for each event. 2. **Compute the mean duration** of all events. 3. Add a **Boolean column indicating** whether the event duration is above the mean duration. 4. Retrieve the **components of each timedelta** (days, hours, minutes, seconds). 5. Output the results in a DataFrame as described. **Constraints:** - Assume there are no missing values (`NaT`) in `start_time` and `end_time` columns. - Ensure the DataFrame is not modified in-place. **Example:** ```python import pandas as pd data = { \'start_time\': pd.to_datetime([\'2023-01-01 08:00\', \'2023-01-01 08:30\', \'2023-01-01 09:00\']), \'end_time\': pd.to_datetime([\'2023-01-01 09:00\', \'2023-01-01 09:20\', \'2023-01-01 10:00\']) } events = pd.DataFrame(data) result = analyze_event_durations(events) print(result) ``` **Expected Output:** ``` start_time end_time duration mean_duration duration_above_mean duration_components 0 2023-01-01 08:00:00 2023-01-01 09:00:00 0 days 01:00:00 0 days 01:10:00 False {\'days\': 0, \'hours\': 1, \'minutes\': 0, \'seconds\': 0} 1 2023-01-01 08:30:00 2023-01-01 09:20:00 0 days 00:50:00 0 days 01:10:00 False {\'days\': 0, \'hours\': 0, \'minutes\': 50, \'seconds\': 0} 2 2023-01-01 09:00:00 2023-01-01 10:00:00 0 days 01:00:00 0 days 01:10:00 False {\'days\': 0, \'hours\': 1, \'minutes\': 0, \'seconds\': 0} ``` **Note:** - The `mean_duration` column should have the same value for all rows, which is the mean of the `duration` column. - The `duration_components` should be a dictionary-like structure showing different components of the timedelta. **Implementation Hints:** - Use `pd.Timedelta` or `.astype` for creating and manipulating timedelta objects. - Utilize Series methods like `.dt` to access timedelta components. - Functions like `mean()` and comparison operations can help determine if the duration is above the mean.","solution":"import pandas as pd def analyze_event_durations(events: pd.DataFrame) -> pd.DataFrame: Analyze the durations of events in the given DataFrame. This function calculates the duration of each event, the mean duration, and whether each event\'s duration is above the mean duration. Additionally, it provides the components of each duration (days, hours, minutes, seconds). Parameters: events (pd.DataFrame): DataFrame with \'start_time\' and \'end_time\' columns. Returns: pd.DataFrame: DataFrame with added columns for duration analysis. # Calculate durations durations = events[\'end_time\'] - events[\'start_time\'] # Calculate mean duration mean_duration = durations.mean() # Determine if duration is above mean duration_above_mean = durations > mean_duration # Extract duration components duration_components = durations.apply(lambda x: { \'days\': x.days, \'hours\': x.components.hours, \'minutes\': x.components.minutes, \'seconds\': x.components.seconds }) # Construct the result DataFrame result = events.copy() result[\'duration\'] = durations result[\'mean_duration\'] = mean_duration result[\'duration_above_mean\'] = duration_above_mean result[\'duration_components\'] = duration_components return result"},{"question":"# Advanced File Handling and Custom Hook Implementation You are tasked with creating a specialized logging system in Python that leverages both high-level and low-level file handling practices. Your task is to implement a Python module that includes the following: 1. **CustomFileHandler Class**: - This class should create and manage file streams using Python\'s standard `io` library. - Methods must include: - `__init__(self, file_path, mode, buffering=-1)`: Initializes the file handler. - `log(self, message)`: Writes a log message to the file with a timestamp. - `read_logs(self, num_lines)`: Reads a specified number of lines from the log file. - `get_file_descriptor(self)`: Returns the file descriptor associated with the file. 2. **CustomCodeOpener Class**: - This class should utilize the `PyFile_SetOpenCodeHook` to set a custom handler for opening code files. - Methods must include: - `__init__(self, code_handler)`: Sets the custom code opener. - `open_code(self, file_path)`: Opens the provided file path with the custom logic. Your implementation should include appropriate error handling and demonstrate usage of the outlined functionality. You should avoid using deprecated or unsupported features from previous Python versions. Below is a template to help you get started: ```python import io import os import time class CustomFileHandler: def __init__(self, file_path, mode, buffering=-1): self.file = io.open(file_path, mode, buffering=buffering) def log(self, message): timestamp = time.strftime(\'%Y-%m-%d %H:%M:%S\') self.file.write(f\\"{timestamp} - {message}n\\") def read_logs(self, num_lines): self.file.seek(0) return [self.file.readline().strip() for _ in range(num_lines)] def get_file_descriptor(self): return self.file.fileno() class CustomCodeOpener: def __init__(self, code_handler): # Here we should call PyFile_SetOpenCodeHook with the provided handler pass def open_code(self, file_path): # Open code file with custom handler logic pass # Example Usage: if __name__ == \\"__main__\\": file_handler = CustomFileHandler(\\"test_logs.txt\\", \\"w+\\") file_handler.log(\\"This is a test log entry.\\") print(file_handler.read_logs(1)) # Initialize a CustomCodeOpener with desired handler def code_handler(path, user_data): # Custom logic for opening code files. pass code_opener = CustomCodeOpener(code_handler) code_opener.open_code(\\"some_code.py\\") ``` # Input and Output 1. `CustomFileHandler` methods: - `__init__`: Takes `file_path` (str), `mode` (str), and `buffering` (int). Opens/creates the file. - `log`: Takes `message` (str). Writes the message to the file with a timestamp. - `read_logs`: Takes `num_lines` (int). Returns a list of log entries (str). - `get_file_descriptor`: Returns the file descriptor (int). 2. `CustomCodeOpener` methods: - `__init__`: Takes `code_handler` (function). Sets the custom code handler. - `open_code`: Takes `file_path` (str). Opens a code file using the custom handler. # Constraints - Ensure proper resource management (e.g., closing files). - Use Python 3.6+ features and best practices. - Validate all inputs and handle exceptions gracefully. Implement these classes and their methods, then demonstrate their usage in the provided example.","solution":"import io import os import time class CustomFileHandler: def __init__(self, file_path, mode, buffering=-1): self.file_path = file_path self.mode = mode self.file = io.open(file_path, mode, buffering=buffering) def log(self, message): timestamp = time.strftime(\'%Y-%m-%d %H:%M:%S\') self.file.write(f\\"{timestamp} - {message}n\\") self.file.flush() # Ensure the message is written to the file immediately def read_logs(self, num_lines): with io.open(self.file_path, \'r\') as file: return [file.readline().strip() for _ in range(num_lines)] def get_file_descriptor(self): return self.file.fileno() def close(self): self.file.close() import sys class CustomCodeOpener: def __init__(self, code_handler): self.code_handler = code_handler def open_code(self, file_path): return self.code_handler(file_path, None) # Example Usage: if __name__ == \\"__main__\\": file_handler = CustomFileHandler(\\"test_logs.txt\\", \\"w+\\") file_handler.log(\\"This is a test log entry.\\") print(file_handler.read_logs(1)) file_handler.close() # Initialize a CustomCodeOpener with desired handler def code_handler(file_path, user_data): print(f\\"Opening code file: {file_path}\\") with open(file_path, \\"r\\") as file: return file.read() code_opener = CustomCodeOpener(code_handler) print(code_opener.open_code(\\"some_code.py\\"))"},{"question":"**Objective:** Demonstrate your understanding of Python 3.10\'s grammar, particularly focusing on pattern matching using the `match` statement, and write a function that processes abstract syntax trees (AST) to evaluate simple mathematical expressions. **Problem Statement:** You are required to implement a function, `evaluate_expression`, that takes a mathematical expression represented as an Abstract Syntax Tree (AST) node and evaluates it. The function should support: - Addition, subtraction, multiplication, and division of integer and float numbers. - Nested expressions using parentheses. - Unary operations (e.g., `-5`). Additionally, your function should support the pattern matching introduced in Python 3.10 to process the AST nodes. Assume the AST nodes are well-formed according to the Python 3.10 grammar. **Function Signature:** ```python def evaluate_expression(node: ASTNode) -> float: pass ``` **Input:** - `node`: An instance of `ASTNode` representing the root of a parsed mathematical expression. **Output:** - Returns the result of the evaluated expression as a `float`. **Constraints:** 1. The nodes of the AST should follow the Python 3.10 grammar rules. 2. The expressions can include integers and floating-point numbers. 3. Your solution must utilize the `match` statement for pattern matching. **Example:** Given the following AST: ```python class ASTNode: pass class BinOpNode(ASTNode): def __init__(self, left: ASTNode, op: str, right: ASTNode): self.left = left self.op = op self.right = right class UnaryOpNode(ASTNode): def __init__(self, op: str, operand: ASTNode): self.op = op self.operand = operand class NumNode(ASTNode): def __init__(self, value: float): self.value = value # Constructing the AST for the expression: -(3 + 2 * 4) ast = UnaryOpNode(\'-\', BinOpNode(NumNode(3), \'+\', BinOpNode(NumNode(2), \'*\', NumNode(4)))) ``` Calling `evaluate_expression(ast)` should return `-11.0`. **Notes:** - The function should only handle the basic arithmetic operations and the unary negation. - Edge cases include division by zero (which should raise an exception) and large nested expressions. **Hints:** - Utilize the match-case statement introduced in Python 3.10 for handling different AST node types. - Implement helper functions if needed to keep the code modular and readable.","solution":"def evaluate_expression(node): Evaluates the mathematical expression represented by the AST node. match node: case NumNode(value=value): return value case BinOpNode(left=left, op=op, right=right): left_val = evaluate_expression(left) right_val = evaluate_expression(right) match op: case \'+\': return left_val + right_val case \'-\': return left_val - right_val case \'*\': return left_val * right_val case \'/\': if right_val == 0: raise ValueError(\\"Division by zero is not allowed.\\") return left_val / right_val case _: raise ValueError(f\\"Unknown operation {op}\\") case UnaryOpNode(op=op, operand=operand): operand_val = evaluate_expression(operand) if op == \'-\': return -operand_val else: raise ValueError(f\\"Unknown unary operation {op}\\") case _: raise ValueError(\\"Unknown AST node type\\") class ASTNode: pass class BinOpNode(ASTNode): def __init__(self, left, op, right): self.left = left self.op = op self.right = right class UnaryOpNode(ASTNode): def __init__(self, op, operand): self.op = op self.operand = operand class NumNode(ASTNode): def __init__(self, value): self.value = value"},{"question":"Objective Implement a custom representation formatter for a set of data structures using the `reprlib` module. Background The `reprlib` module in Python offers an alternate implementation of the `repr()` function, which limits the size of object representations to prevent excessively long outputs. Task 1. **Subclass Repr Class**: Create a subclass of `reprlib.Repr` named `CustomRepr` with custom representation for dictionaries, lists, and strings. 2. **Custom Representation for Types**: - **Dictionaries**: Represent only up to 3 key-value pairs. - **Lists**: Represent only up to 3 elements. - **Strings**: Represent strings with a maximum length of 20 characters. 3. **Method Implementations**: - Implement the `repr_dict`, `repr_list`, and `repr_str` methods in the `CustomRepr` class to handle the custom formatting. 4. **Testing**: - Instantiate `CustomRepr` and demonstrate its usage by printing the custom representations of a dictionary, list, and string with varying lengths. Implementation Details - **Inputs**: Test the functionality with the following examples: - Dictionary: `{\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5}` - List: `[1, 2, 3, 4, 5]` - String: `\\"abcdefghijklmnopqrstuvwxyz\\"` - **Output**: Print the custom representations of the objects using the `CustomRepr` instance. ```python import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 3 self.maxlist = 3 self.maxstring = 20 def repr_dict(self, obj, level): n = len(obj) if n > self.maxdict: pieces = [self.repr1((k, obj[k]), level - 1) for k in list(obj)[:self.maxdict]] return \'{\' + \', \'.join(pieces) + \', ...}\' else: return repr(obj) def repr_list(self, obj, level): n = len(obj) if n > self.maxlist: pieces = [self.repr1(item, level - 1) for item in obj[:self.maxlist]] return \'[\' + \', \'.join(pieces) + \', ...]\' else: return repr(obj) def repr_str(self, obj, level): if len(obj) > self.maxstring: return repr(obj[:self.maxstring] + \'...\') else: return repr(obj) # Test the implementation custom_repr = CustomRepr() # Create test objects test_dict = {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5} test_list = [1, 2, 3, 4, 5] test_str = \\"abcdefghijklmnopqrstuvwxyz\\" # Print their custom representations print(custom_repr.repr(test_dict)) # Expected: \\"{\'a\': 1, \'b\': 2, \'c\': 3, ...}\\" print(custom_repr.repr(test_list)) # Expected: \\"[1, 2, 3, ...]\\" print(custom_repr.repr(test_str)) # Expected: \\"\'abcdefghijklmnopqrst...\'\\" ``` **Constraints**: - You must utilize the methods provided by the `reprlib.Repr` base class. - Your custom representations must adhere to the specified size limits. **Performance**: - Ensure that your implementation performs efficiently even with large inputs by adhering to the size limits.","solution":"import reprlib class CustomRepr(reprlib.Repr): def __init__(self): super().__init__() self.maxdict = 3 self.maxlist = 3 self.maxstring = 20 def repr_dict(self, obj, level): n = len(obj) if n > self.maxdict: items = list(obj.items())[:self.maxdict] pieces = [f\\"{self.repr1(k, level - 1)}: {self.repr1(v, level - 1)}\\" for k, v in items] return \'{\' + \', \'.join(pieces) + \', ...}\' else: return repr(obj) def repr_list(self, obj, level): n = len(obj) if n > self.maxlist: pieces = [self.repr1(item, level - 1) for item in obj[:self.maxlist]] return \'[\' + \', \'.join(pieces) + \', ...]\' else: return repr(obj) def repr_str(self, obj, level): if len(obj) > self.maxstring: return repr(obj[:self.maxstring] + \'...\') else: return repr(obj) # Test the implementation custom_repr = CustomRepr() # Create test objects test_dict = {\'a\': 1, \'b\': 2, \'c\': 3, \'d\': 4, \'e\': 5} test_list = [1, 2, 3, 4, 5] test_str = \\"abcdefghijklmnopqrstuvwxyz\\" # Print their custom representations print(custom_repr.repr(test_dict)) # Expected: \\"{\'a\': 1, \'b\': 2, \'c\': 3, ...}\\" print(custom_repr.repr(test_list)) # Expected: \\"[1, 2, 3, ...]\\" print(custom_repr.repr(test_str)) # Expected: \\"\'abcdefghijklmnopqrst...\'\\""},{"question":"# Question: Implementing Subnet and Supernet Calculation Objective Demonstrate your understanding of Python\'s `ipaddress` module by implementing a function that calculates subnets and supernets based on given network specifications. Task Write a Python function `calculate_subnets_supernet` that takes in two parameters: 1. `network`: A string representing an IPv4 or IPv6 network in CIDR notation (e.g., `\'192.168.1.0/24\'` or `\'2001:db8::/32\'`). 2. `new_prefix`: An optional integer representing the new prefix length for subnetting or supernetting. If `new_prefix` is greater than current prefix, you need to calculate and return the subnets. If `new_prefix` is less than the current prefix, you need to calculate and return the supernet. If `new_prefix` equals the current prefix, return the network itself. Expected Output The function should return a list of strings representing the subnets or the supernet. Each element in the list should be a network in CIDR notation. Example ```python def calculate_subnets_supernet(network: str, new_prefix: int) -> list: # Your implementation here # Example usage print(calculate_subnets_supernet(\'192.168.1.0/24\', 26)) # Expected Output: [\'192.168.1.0/26\', \'192.168.1.64/26\', \'192.168.1.128/26\', \'192.168.1.192/26\'] print(calculate_subnets_supernet(\'192.168.1.0/24\', 20)) # Expected Output: [\'192.168.0.0/20\'] print(calculate_subnets_supernet(\'192.168.1.0/24\', 24)) # Expected Output: [\'192.168.1.0/24\'] print(calculate_subnets_supernet(\'2001:db8::/32\', 34)) # Expected Output: [\'2001:db8::/34\', \'2001:db8:4000::/34\', \'2001:db8:8000::/34\', \'2001:db8:c000::/34\'] print(calculate_subnets_supernet(\'2001:db8::/32\', 30)) # Expected Output: [\'2001:db8::/30\'] ``` Constraints - Assume the input `network` is always a valid IPv4 or IPv6 network string. - The `new_prefix` will always be between 0 and 32 (inclusive) for IPv4 and between 0 and 128 (inclusive) for IPv6. Notes - Use the `ipaddress` module and its classes/methods for implementation. - Include appropriate error handling where necessary.","solution":"import ipaddress def calculate_subnets_supernet(network: str, new_prefix: int) -> list: # Parse the input network string net = ipaddress.ip_network(network) # If the new prefix length is equal to the current prefix, return the network itself if net.prefixlen == new_prefix: return [str(net)] # If the new prefix length is greater than the current prefix, generate subnets if new_prefix > net.prefixlen: subnets = net.subnets(new_prefix=new_prefix) return [str(subnet) for subnet in subnets] # If the new prefix length is less than the current prefix, generate the supernet if new_prefix < net.prefixlen: supernet = net.supernet(new_prefix=new_prefix) return [str(supernet)] # Example usage # print(calculate_subnets_supernet(\'192.168.1.0/24\', 26)) # Expected Output: [\'192.168.1.0/26\', \'192.168.1.64/26\', \'192.168.1.128/26\', \'192.168.1.192/26\'] # # print(calculate_subnets_supernet(\'192.168.1.0/24\', 20)) # Expected Output: [\'192.168.0.0/20\'] # # print(calculate_subnets_supernet(\'192.168.1.0/24\', 24)) # Expected Output: [\'192.168.1.0/24\'] # # print(calculate_subnets_supernet(\'2001:db8::/32\', 34)) # Expected Output: [\'2001:db8::/34\', \'2001:db8:4000::/34\', \'2001:db8:8000::/34\', \'2001:db8:c000::/34\'] # # print(calculate_subnets_supernet(\'2001:db8::/32\', 30)) # Expected Output: [\'2001:db8::/30\']"},{"question":"Asynchronous Task Manager with Exception Handling # Objective Implement an asynchronous task manager that schedules multiple tasks, manages their execution, and handles exceptions appropriately using Python\'s `asyncio` library. # Problem Statement You are required to implement a function `manage_tasks` which takes a list of coroutines (`coros`) and runs them concurrently. The function should also take an optional maximum run time (`max_time`), specified in seconds, after which all running tasks should be cancelled. Furthermore, any exceptions that occur within the tasks should be logged, and the tasks should continue executing independently of the failed tasks. # Function Signature ```python import asyncio from typing import List, Coroutine, Optional async def manage_tasks(coros: List[Coroutine], max_time: Optional[int] = None) -> None: pass ``` # Input - `coros` (List[Coroutine]): A list of coroutine objects to be executed concurrently. - `max_time` (Optional[int]): An integer representing the maximum time (in seconds) to run the tasks. If not provided, tasks should run until completion. # Output The function does not return anything. It should print logs when exceptions occur and when tasks are being cancelled due to exceeding `max_time`. # Constraints - Each coroutine in `coros` may take variable time to complete and may raise exceptions. - You should use asyncio functionalities such as creating tasks, managing event loops, scheduling callbacks, and handling exceptions. # Example Usage ```python import asyncio async def task1(): await asyncio.sleep(2) print(\\"Task 1 completed\\") async def task2(): await asyncio.sleep(1) raise ValueError(\\"Error in Task 2\\") async def task3(): await asyncio.sleep(3) print(\\"Task 3 completed\\") async def main(): await manage_tasks([task1(), task2(), task3()], max_time=4) # This should result in: # - Task 1 and Task 3 completing if they finish within max_time. # - Task 2 raising an exception and the exception being logged. # - All tasks being cancelled if they exceed the max_time. asyncio.run(main()) ``` # Notes 1. Use `asyncio.gather` or a similar function to run tasks concurrently. 2. Use asyncio\'s exception handling mechanisms to log any exceptions raised during the task execution. 3. If `max_time` is specified, use `asyncio.wait_for` or its equivalent method to limit the overall running time of the tasks. 4. Ensure that your function is robust and can handle multiple types of coroutines and exceptions gracefully. # Evaluation Criteria - Correctness: The function should correctly execute the tasks and handle exceptions as specified. - Robustness: The function should handle various edge cases such as tasks with different execution times, multiple exceptions, and proper cancellation of tasks. - Clarity: Code should be clear, with adequate comments to explain the logic.","solution":"import asyncio from typing import List, Coroutine, Optional async def manage_tasks(coros: List[Coroutine], max_time: Optional[int] = None) -> None: async def handle_task(task): try: await task except Exception as e: print(f\\"Exception occurred: {e}\\") tasks = [handle_task(coro) for coro in coros] if max_time: try: await asyncio.wait_for(asyncio.gather(*tasks), timeout=max_time) except asyncio.TimeoutError: print(f\\"Max time of {max_time} seconds exceeded. Cancelling remaining tasks...\\") for task in tasks: task.cancel() else: await asyncio.gather(*tasks)"},{"question":"Objective Implement a function to print detailed configuration information based on specific requirements using the `sysconfig` module in Python. Details You are to implement a function `get_configuration_info(option)` that accepts a string `option` and returns specific information based on the value of `option`. The function should: 1. **Return installation paths** when `option` is set to `\\"paths\\"`. 2. **Return configuration variables** when `option` is set to `\\"config_vars\\"`. 3. **Return the Python version** when `option` is set to `\\"version\\"`. 4. **Return the platform identifier** when `option` is set to `\\"platform\\"`. Additionally: - If an invalid option is given, raise a `ValueError` with the message `\\"Invalid option\\"`. Input - `option` (string): The option specifying the information to retrieve. It can be `\\"paths\\"`, `\\"config_vars\\"`, `\\"version\\"`, or `\\"platform\\"`. Output - Depending on the input option: - If `\\"paths\\"`, return a dictionary of installation paths. - If `\\"config_vars\\"`, return a dictionary of configuration variables. - If `\\"version\\"`, return a string of the Python version. - If `\\"platform\\"`, return a string identifying the current platform. Constraints - The function should work for any valid Python installation where the `sysconfig` module is available. - The implementation should make use of appropriate `sysconfig` functions to retrieve the required information. Example ```python import sysconfig def get_configuration_info(option): if option == \\"paths\\": return sysconfig.get_paths() elif option == \\"config_vars\\": return sysconfig.get_config_vars() elif option == \\"version\\": return sysconfig.get_python_version() elif option == \\"platform\\": return sysconfig.get_platform() else: raise ValueError(\\"Invalid option\\") # Example usages print(get_configuration_info(\\"paths\\")) print(get_configuration_info(\\"config_vars\\")) print(get_configuration_info(\\"version\\")) print(get_configuration_info(\\"platform\\")) ``` Notes - The `sysconfig` module should be imported and utilized within your function. - The function must handle errors and invalid inputs appropriately.","solution":"import sysconfig def get_configuration_info(option): if option == \\"paths\\": return sysconfig.get_paths() elif option == \\"config_vars\\": return sysconfig.get_config_vars() elif option == \\"version\\": return sysconfig.get_python_version() elif option == \\"platform\\": return sysconfig.get_platform() else: raise ValueError(\\"Invalid option\\")"},{"question":"# Distributed Training with PyTorch Multiprocessing **Objective:** Implement a distributed training setup for a simple neural network using PyTorch\'s `torch.distributed.elastic.multiprocessing` module. The goal is to understand how to manage multiple worker processes and synchronize their work. **Task:** 1. Create a simple neural network in PyTorch. 2. Implement a function to train this network using multiple worker processes. 3. Ensure synchronization between processes and aggregate results. **Details:** - **Neural Network:** - A simple feed-forward neural network with one hidden layer. - **Training Function Signature:** ```python def distributed_train(n_processes: int, epochs: int, dataset: torch.utils.data.Dataset) -> List[float]: Trains a neural network using distributed training with multiple worker processes. Args: n_processes (int): Number of worker processes to use. epochs (int): Number of training epochs. dataset (torch.utils.data.Dataset): The dataset to use for training. Returns: List[float]: List of loss values for each epoch. ``` - **Input:** - `n_processes` (int): Number of worker processes to spawn. - `epochs` (int): Number of training epochs. - `dataset` (torch.utils.data.Dataset): A PyTorch dataset for training purposes. - **Output:** - A list of float values representing the average loss per epoch. **Constraints:** - Use PyTorch\'s `torch.distributed.elastic.multiprocessing.start_processes` to manage worker processes. - Synchronize the weight updates between processes correctly. - Ensure proper aggregation and reduction of losses. **Performance Requirements:** - Efficiently handle the distribution of data and computation across multiple processes. - Reasonable memory management such that the process does not exceed typical resources of a standard computing environment. **Example Case:** Given a dataset of random numbers as input features and labels, the function should spawn the specified number of worker processes, perform distributed training, and return the loss values for each epoch. ```python import torch import torch.nn as nn import torch.optim as optim from torch.distributed.elastic.multiprocessing import start_processes # Define the neural network architecture class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x # Define the training function def distributed_train(n_processes: int, epochs: int, dataset: torch.utils.data.Dataset) -> List[float]: # Your implementation here pass # Example dataset dataset = torch.utils.data.TensorDataset(torch.rand(100, 10), torch.rand(100, 1)) # Run distributed training losses = distributed_train(4, 5, dataset) print(losses) ``` **Hint:** Consider using `torch.multiprocessing` for managing the processes and `torch.nn.parallel.DistributedDataParallel` for model parallelism.","solution":"import torch import torch.nn as nn import torch.optim as optim from torch.distributed import init_process_group, destroy_process_group from torch.nn.parallel import DistributedDataParallel as DDP from torch.utils.data import DataLoader, DistributedSampler from torch.multiprocessing import Process class SimpleNet(nn.Module): def __init__(self): super(SimpleNet, self).__init__() self.fc1 = nn.Linear(10, 50) self.fc2 = nn.Linear(50, 1) def forward(self, x): x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def setup(rank, world_size): # Initialize the process group init_process_group(\\"gloo\\", rank=rank, world_size=world_size) def cleanup(): destroy_process_group() def train(rank, world_size, epochs, dataset): # Setup the process group setup(rank, world_size) # Create model and move it to the appropriate device model = SimpleNet().to(rank) model = DDP(model, device_ids=[rank]) # Create optimizer optimizer = optim.SGD(model.parameters(), lr=1e-2) criterion = nn.MSELoss() # Split dataset among the workers sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank) dataloader = DataLoader(dataset, batch_size=32, sampler=sampler) # Train epoch_losses = [] for epoch in range(epochs): epoch_loss = 0.0 for inputs, labels in dataloader: inputs, labels = inputs.to(rank), labels.to(rank) optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() epoch_loss += loss.item() epoch_losses.append(epoch_loss / len(dataloader)) cleanup() return epoch_losses def distributed_train(n_processes: int, epochs: int, dataset: torch.utils.data.Dataset): processes = [] epoch_losses = torch.zeros(epochs) for rank in range(n_processes): p = Process(target=train, args=(rank, n_processes, epochs, dataset)) p.start() processes.append(p) for p in processes: p.join() return epoch_losses.tolist()"},{"question":"Objective The goal of this task is to assess your understanding of the Seaborn library, particularly the `rugplot` function, and your ability to visualize data distributions effectively. Problem Statement You are given a dataset containing three continuous numerical columns: `X`, `Y`, and `Z`. Your task is to create a single figure that contains the following three subplots in a 1x3 grid layout: 1. A KDE plot of `X` with a rug along the x-axis. 2. A scatter plot of `X` and `Y` with a rug along both axes. 3. A scatter plot of `X` and `Y`, where a third variable `Z` is represented with hue mapping and a rug is added along both axes with the height of the rug ticks adjusted to 0.1. You should also make sure: - All three subplots share the same x-axis. - The figure should have a main title and titles for each subplot. Input Format - The dataset will be provided as a pandas DataFrame. ```python import pandas as pd # Example dataset data = pd.DataFrame({ \'X\': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \'Y\': [5, 3, 4, 2, 10, 6, 8, 9, 7, 1], \'Z\': [10, 9, 8, 7, 6, 5, 4, 3, 2, 1] }) ``` Output Format - The function should output a Matplotlib figure with the specified layout and plots. Function Signature ```python import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_rugplot_figure(data: pd.DataFrame) -> plt.Figure: # Your code here return fig ``` Constraints 1. Assume the data will always have at least 10 rows. 2. Perform all necessary imports within the function. Example Given the provided example dataset, you should generate a figure with three subplots: 1. A KDE plot of column `X` with a rug along the x-axis. 2. A scatter plot of columns `X` and `Y` with a rug along both axes. 3. A scatter plot of columns `X` and `Y` with hue mapping based on column `Z` and a rug along both axes with ticks of height 0.1. **Note**: This example is illustrative, and figures may vary based on the actual dataset used. Evaluation - The solution will be evaluated based on the correctness, readability, and efficiency of the implementation. - The visualizations should clearly demonstrate the required features. Good luck!","solution":"import pandas as pd import seaborn as sns import matplotlib.pyplot as plt def create_rugplot_figure(data: pd.DataFrame) -> plt.Figure: # Ensure %matplotlib inline for Jupyter environments # %matplotlib inline # Create the 1x3 subplot structure fig, axes = plt.subplots(1, 3, figsize=(20, 5), sharex=True) fig.suptitle(\'Data Visualization of X, Y, and Z\', fontsize=16) # Plot 1: KDE Plot of X with a rug along the x-axis sns.kdeplot(data=data[\'X\'], ax=axes[0]) sns.rugplot(data[\'X\'], ax=axes[0]) axes[0].set_title(\'KDE Plot of X with Rug\') # Plot 2: Scatter plot of X vs Y with a rug along both axes sns.scatterplot(x=data[\'X\'], y=data[\'Y\'], ax=axes[1]) sns.rugplot(data[\'X\'], ax=axes[1]) sns.rugplot(data[\'Y\'], ax=axes[1], axis=\'y\') axes[1].set_title(\'Scatter Plot of X vs Y with Rug\') # Plot 3: Scatter plot of X vs Y with hue Z and a rug with height 0.1 scatter = sns.scatterplot(x=data[\'X\'], y=data[\'Y\'], hue=data[\'Z\'], ax=axes[2]) sns.rugplot(data[\'X\'], ax=axes[2], height=0.1) sns.rugplot(data[\'Y\'], ax=axes[2], height=0.1, axis=\'y\') axes[2].set_title(\'Scatter Plot of X vs Y with Z as Hue and Rug\') plt.tight_layout(pad=2) return fig"},{"question":"**Question: Handling and Filtering Warnings** You are tasked with developing components of a Python library that makes extensive use of deprecated features. To manage this, you need to implement functions that issue and handle warnings in various scenarios. **Objective:** 1. Implement a function `deprecated_function` that issues a `DeprecationWarning` whenever it is called. 2. Implement a function `test_warnings` that runs `deprecated_function` but suppresses warnings during its execution. 3. Implement a function `configure_warning_filters` that sets up warning filters to: - Ignore all `DeprecationWarnings`. - Transform `SyntaxWarnings` into exceptions. **Function Definitions:** 1. `deprecated_function()`: - **No input parameters.** - Issues a `DeprecationWarning` with the message \\"deprecated_function is deprecated\\". - **No return value.** 2. `test_warnings()`: - **No input parameters.** - Calls `deprecated_function` within a context where all warnings are suppressed. - **No return value.** 3. `configure_warning_filters()`: - **No input parameters.** - Configures global warning filters as described above. - **No return value.** Below is a template to get you started: ```python import warnings def deprecated_function(): Function that issues a DeprecationWarning # Implement issuing of DeprecationWarning pass def test_warnings(): Function that calls deprecated_function with warnings suppressed # Implement warnings suppression here pass def configure_warning_filters(): Function that configures global warning filters - Ignore DeprecationWarnings - Turn SyntaxWarnings into errors # Implement configuration of warning filters pass # Testing the implementation: if __name__ == \\"__main__\\": # Initial call to trigger warning deprecated_function() # Suppressing warning during the call test_warnings() # Configure and test warning filters configure_warning_filters() try: warnings.warn(\\"This is a syntax warning\\", SyntaxWarning) except SyntaxWarning: print(\\"SyntaxWarning correctly turned into an exception\\") else: print(\\"SyntaxWarning did not raise an exception\\") ``` # Constraints: - Do not modify global warning filters outside the `configure_warning_filters` function. - Ensure context managers work as expected within `test_warnings`. # Expected Output: - When running `deprecated_function`, a `DeprecationWarning` should be visible. - When running `test_warnings`, no warnings should be visible. - After running `configure_warning_filters` and issuing a `SyntaxWarning`, an exception should be raised. # Note: **Ensure proper error handling and comments for clarity.**","solution":"import warnings def deprecated_function(): Function that issues a DeprecationWarning. warnings.warn(\\"deprecated_function is deprecated\\", DeprecationWarning) def test_warnings(): Function that calls deprecated_function with warnings suppressed. with warnings.catch_warnings(): warnings.simplefilter(\\"ignore\\") deprecated_function() def configure_warning_filters(): Function that configures global warning filters: - Ignore DeprecationWarnings - Turn SyntaxWarnings into errors. # Ignore all deprecation warnings warnings.filterwarnings(\\"ignore\\", category=DeprecationWarning) # Turn syntax warnings into errors warnings.filterwarnings(\\"error\\", category=SyntaxWarning)"},{"question":"**Coding Assessment Question:** # Distributed Subprocess Handling with PyTorch **Objective:** Implement a function that creates multiple subprocesses to perform a simple computation (e.g., summing a range of numbers), and then uses PyTorch\'s `SubprocessHandler` to manage these subprocesses. This will test your understanding of distributed processing and subprocess handling in PyTorch. # Task: Implement the function `parallel_sum_in_subprocesses` that performs the following: 1. **Input:** - `num_subprocesses` (int): The number of subprocesses to create. - `range_per_process` (int): The range of numbers each subprocess should sum. 2. **Output:** - (int): The total sum computed by aggregating the sums from all subprocesses. 3. **Constraints:** - Each subprocess should independently sum a range of numbers starting from `i * range_per_process` to `(i + 1) * range_per_process - 1`, where `i` is the subprocess index (0-based). - Use PyTorch\'s `SubprocessHandler` to manage these subprocesses. 4. **Performance Requirements:** - Ensure minimal overhead in managing subprocesses. - Properly handle subprocess termination and result aggregation. # Function Signature: ```python def parallel_sum_in_subprocesses(num_subprocesses: int, range_per_process: int) -> int: pass ``` # Example: ```python # Example call total_sum = parallel_sum_in_subprocesses(num_subprocesses=4, range_per_process=10) print(total_sum) # Expected output: 190 (sum of 0-9, 10-19, 20-29, 30-39 is 45 + 145 = 190) ``` # Additional Information: - You may utilize Python\'s standard library `multiprocessing` module to create subprocesses. - Use `torch.distributed.elastic.multiprocessing.subprocess_handler` module\'s `SubprocessHandler` to manage the subprocesses. This problem is designed to assess your ability to work with PyTorch’s distributed subprocess handling utilities and demonstrate competence in parallel processing. # Notes: 1. Make sure to handle edge cases such as `num_subprocesses` being less than 1. 2. Assume valid integers will be provided as inputs. 3. Your code should be efficient and scalable.","solution":"import torch.distributed.elastic.multiprocessing as mp import torch.distributed.elastic.multiprocessing.subprocess_handler from multiprocessing import Process, Manager def sum_range(start, end, return_dict, index): return_dict[index] = sum(range(start, end)) def parallel_sum_in_subprocesses(num_subprocesses: int, range_per_process: int) -> int: if num_subprocesses <= 0: return 0 manager = Manager() return_dict = manager.dict() processes = [] for i in range(num_subprocesses): start = i * range_per_process end = (i + 1) * range_per_process process = Process(target=sum_range, args=(start, end, return_dict, i)) processes.append(process) process.start() for process in processes: process.join() total_sum = sum(return_dict.values()) return total_sum"},{"question":"**Objective:** To assess your understanding of scikit-learn\'s dataset generators and your ability to manipulate and visualize generated datasets. **Problem Statement:** You are provided with multiple dataset generation functions from the `sklearn.datasets` module. Your task is to implement a function `generate_and_visualize_dataset` that generates a dataset using these functions and visualizes the results. The function will accept parameters to specify which dataset to generate and additional parameters specific to that dataset function. # Requirements: 1. **Function Signature:** ```python def generate_and_visualize_dataset(generator_name: str, **kwargs) -> None: ``` 2. **Input Parameters:** - `generator_name` (str): Name of the dataset generator function to use. Valid values are: - `\'make_blobs\'` - `\'make_classification\'` - `\'make_gaussian_quantiles\'` - `\'make_circles\'` - `\'make_moons\'` - `**kwargs`: Additional parameters specific to the chosen generator function. 3. **Output:** - The function does not return anything but should display a scatter plot of the generated dataset using `matplotlib`. 4. **Constraints:** - You must use the specified generator functions from `sklearn.datasets`. - The function should handle any additional parameters passed through `kwargs` to customize the dataset generation. - Ensure that the scatter plot displays appropriate labels and titles for clarity. 5. **Evaluation Criteria:** - Correct implementation and usage of dataset generators. - Proper handling of additional parameters via `kwargs`. - Clear and informative visualization of the generated dataset. # Example Usage: ```python # Example 1: Generate and visualize a blobs dataset generate_and_visualize_dataset(\'make_blobs\', centers=3, cluster_std=0.5, random_state=42) # Example 2: Generate and visualize a classification dataset generate_and_visualize_dataset(\'make_classification\', n_features=2, n_informative=2, n_clusters_per_class=1, n_classes=3, random_state=42) ``` # Notes: - Utilize the `matplotlib.pyplot` module for creating scatter plots. - Add appropriate titles and labels to the plots to enhance readability. **Hint:** Refer to the `sklearn.datasets` documentation for detailed information on the available dataset generators and their parameters.","solution":"import matplotlib.pyplot as plt from sklearn.datasets import make_blobs, make_classification, make_gaussian_quantiles, make_circles, make_moons def generate_and_visualize_dataset(generator_name: str, **kwargs) -> None: Generates a dataset using the specified generator function and visualizes it with a scatter plot. Parameters: - generator_name (str): The name of the dataset generator function. - **kwargs: Additional parameters specific to the chosen generator function. if generator_name == \'make_blobs\': X, y = make_blobs(**kwargs) elif generator_name == \'make_classification\': X, y = make_classification(**kwargs) elif generator_name == \'make_gaussian_quantiles\': X, y = make_gaussian_quantiles(**kwargs) elif generator_name == \'make_circles\': X, y = make_circles(**kwargs) elif generator_name == \'make_moons\': X, y = make_moons(**kwargs) else: raise ValueError(f\\"Unknown generator name: {generator_name}\\") plt.figure(figsize=(8, 6)) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\'viridis\', edgecolor=\'k\') plt.title(f\'{generator_name} Dataset\') plt.xlabel(\'Feature 1\') plt.ylabel(\'Feature 2\') plt.grid(True) plt.show()"},{"question":"Objective Implement a Python class `AdvancedObjectManipulator` that provides multiple functionalities to interact with Python objects, making use of the operations described in the provided documentation. Description You are required to create a class `AdvancedObjectManipulator` which allows for advanced manipulation of Python objects. This class should include methods that demonstrate the following functionalities: 1. **Check Attribute**: A method `has_attribute(obj, attr_name)` to check if an object has a specific attribute. - **Input**: `obj` (Python object), `attr_name` (string) - **Output**: Boolean (`True` if object has the attribute, `False` otherwise) 2. **Get Attribute**: A method `get_attribute(obj, attr_name)` to retrieve an attribute\'s value from an object. - **Input**: `obj` (Python object), `attr_name` (string) - **Output**: Value of the attribute (`None` if attribute does not exist) 3. **Set Attribute**: A method `set_attribute(obj, attr_name, value)` to set a value for a specific attribute. - **Input**: `obj` (Python object), `attr_name` (string), `value` (any type) - **Output**: Boolean (`True` if successful, `False` otherwise) 4. **Delete Attribute**: A method `delete_attribute(obj, attr_name)` to delete a specific attribute from an object. - **Input**: `obj` (Python object), `attr_name` (string) - **Output**: Boolean (`True` if deletion is successful, `False` otherwise) 5. **String Representation**: A method `string_representation(obj)` to get the string representation of an object. - **Input**: `obj` (Python object) - **Output**: String (using `str()` representation) 6. **Check Subclass**: A method `is_subclass(derived, cls)` to check if `derived` is a subclass of `cls`. - **Input**: `derived` (Python class), `cls` (Python class) - **Output**: Boolean (`True` if `derived` is a subclass of `cls`, `False` otherwise) Constraints - The `obj` parameter in the methods can be any valid Python object. - Ensure exception handling to cover cases where operations might fail. Implementation Skeleton ```python class AdvancedObjectManipulator: @staticmethod def has_attribute(obj, attr_name): pass @staticmethod def get_attribute(obj, attr_name): pass @staticmethod def set_attribute(obj, attr_name, value): pass @staticmethod def delete_attribute(obj, attr_name): pass @staticmethod def string_representation(obj): pass @staticmethod def is_subclass(derived, cls): pass ``` Sample Usage ```python class Sample: def __init__(self): self.value = 10 sample_obj = Sample() manipulator = AdvancedObjectManipulator() print(manipulator.has_attribute(sample_obj, \'value\')) # True print(manipulator.get_attribute(sample_obj, \'value\')) # 10 print(manipulator.set_attribute(sample_obj, \'value\', 20)) # True print(manipulator.get_attribute(sample_obj, \'value\')) # 20 print(manipulator.string_representation(sample_obj)) # <__main__.Sample object at 0x...> print(manipulator.is_subclass(Sample, object)) # True print(manipulator.delete_attribute(sample_obj, \'value\')) # True print(manipulator.has_attribute(sample_obj, \'value\')) # False ``` Performance Requirements - The methods should handle common cases efficiently. - Ensure that the class can handle built-in types as well as user-defined classes without significant performance degradation.","solution":"class AdvancedObjectManipulator: @staticmethod def has_attribute(obj, attr_name): Check if an object has a specific attribute. return hasattr(obj, attr_name) @staticmethod def get_attribute(obj, attr_name): Retrieve an attribute\'s value from an object. Return None if the attribute does not exist. return getattr(obj, attr_name, None) @staticmethod def set_attribute(obj, attr_name, value): Set a value for a specific attribute. Return True if successful, False otherwise. setattr(obj, attr_name, value) return True @staticmethod def delete_attribute(obj, attr_name): Delete a specific attribute from an object. Return True if deletion is successful, False otherwise. if hasattr(obj, attr_name): delattr(obj, attr_name) return True return False @staticmethod def string_representation(obj): Get the string representation of an object. return str(obj) @staticmethod def is_subclass(derived, cls): Check if \'derived\' is a subclass of \'cls\'. return issubclass(derived, cls)"},{"question":"**Question: Custom Iterator Implementation in Python** Implement a custom iterator class in Python that mimics a part of the functionality described in the C API documentation. This iterator should iterate over a given sequence of numbers and filter out even numbers, yielding only odd numbers. # Requirements: 1. Implement a class `OddOnlyIterator` that follows the iterator protocol in Python. 2. The initializer (`__init__`) should accept a list of integers. 3. The class should implement the `__iter__` method to return the iterator object itself. 4. The class should implement the `__next__` method to return the next odd number in the sequence. If there are no more odd numbers, it should raise a `StopIteration` exception. 5. Optionally, implement the asynchronous iterator protocol (`__aiter__` and `__anext__`) to support asynchronous iteration. # Input Format: - An instance of `OddOnlyIterator` will be created with a list of integers as an argument. - Example: `iterator = OddOnlyIterator([1, 2, 3, 4, 5, 6, 7])` # Output Format: - Iterating over the `OddOnlyIterator` instance should yield odd numbers from the input list one by one: - For the given example, the output should be: `1, 3, 5, 7` # Constraints: - The input list will contain at least one integer. - The integers in the list will be in the range of -10^6 to 10^6. - Ensure efficient iteration and handling of the `StopIteration` exception. # Example Usage: ```python class OddOnlyIterator: def __init__(self, numbers): # Initialize with the list of numbers pass def __iter__(self): # Return the iterator object (self) pass def __next__(self): # Return the next odd number or raise StopIteration pass # Example usage: iterator = OddOnlyIterator([10, 15, 20, 25, 30]) for number in iterator: print(number) # Expected output: 15, 25 ``` # Advanced (Optional): - Implement the asynchronous iterator protocol. ```python class OddOnlyAsyncIterator(OddOnlyIterator): async def __aiter__(self): # Return the asynchronous iterator object (self) pass async def __anext__(self): # Return the next odd number asynchronously or raise StopAsyncIteration pass ``` # Example Usage for Async Iterator: ```python import asyncio async def main(): async for number in OddOnlyAsyncIterator([10, 15, 20, 25, 30]): print(number) # Expected output: 15, 25 asyncio.run(main()) ``` This question assesses the student\'s ability to understand and implement iterator protocols in Python, manage state within an iterator, and optionally handle asynchronous iteration.","solution":"class OddOnlyIterator: def __init__(self, numbers): self.numbers = numbers self.index = 0 def __iter__(self): return self def __next__(self): while self.index < len(self.numbers): current = self.numbers[self.index] self.index += 1 if current % 2 != 0: return current raise StopIteration"},{"question":"**Question: Document Management System** You have been tasked with developing a simple Document Management System (DMS) that helps manage documents based on their status and content. Your task involves implementing functions to perform several operations. You will employ advanced control flow, functions, and other related concepts covered in the documentation. # Requirements: 1. **Implementation of Document Class:** - Define a class `Document` that has the following attributes: - `title` (string) - `content` (string) - `status` (string - can be \'draft\', \'review\', \'published\') - The class should have a method to print a summary of the document (`__str__` method). 2. **Document Filter Function:** - Implement a function `filter_documents(documents, status) -> list` that takes a list of `Document` objects and returns a list of documents that match the given status. - The function should use advanced control flow techniques (`for`, `if`, `elif`). 3. **Function to Match Content Patterns:** - Implement a function `match_content(documents, keyword) -> dict` that takes a list of `Document` objects and a keyword (string). - Use the `match` statement to create a dictionary that categorizes documents into \'match\', \'not_match\', where: - \'match\' contains documents whose content includes the keyword. - \'not_match\' contains documents whose content does not include the keyword. 4. **Function with Various Argument Types:** - Implement a function `document_statistics(*docs: Document, detailed=False) -> str` that takes a variable number of `Document` objects. - If `detailed` is `True`, the function should return a detailed string report of all documents. - If `detailed` is `False`, the function should return a summary report. - Use string formatting for the report. 5. **Lambda Function to Modify Document Titles:** - Implement a function `modify_titles(documents, func)` that takes a list of `Document` objects and a lambda function `func` to modify the title of each document. - The function should apply `func` to each title in `documents`. 6. **Documentation and Annotations:** - Ensure each function has appropriate docstrings and type annotations. # Example Usage: ```python # Initialize documents doc1 = Document(\\"Doc1\\", \\"This is a draft document\\", \\"draft\\") doc2 = Document(\\"Doc2\\", \\"This document is under review\\", \\"review\\") doc3 = Document(\\"Doc3\\", \\"This is a published document\\", \\"published\\") # Filter documents based on status draft_docs = filter_documents([doc1, doc2, doc3], \\"draft\\") print(draft_docs) # Match content in documents matched_docs = match_content([doc1, doc2, doc3], \\"draft\\") print(matched_docs) # Get document statistics basic_stats = document_statistics(doc1, doc2, doc3) detailed_stats = document_statistics(doc1, doc2, doc3, detailed=True) print(basic_stats) print(detailed_stats) # Modify document titles modify_titles([doc1, doc2, doc3], lambda title: title.upper()) print(doc1, doc2, doc3) ``` # Constraints: - Do not use external libraries for this task. Utilize standard Python library features only. - Focus on code readability and maintainability, adhering to PEP 8 guidelines. # Performance: - Ensure that your solution handles typical use cases efficiently. Avoid redundant computations and maintain optimal time complexity where possible.","solution":"from typing import List, Dict, Callable class Document: def __init__(self, title: str, content: str, status: str): self.title = title self.content = content self.status = status def __str__(self) -> str: return f\\"Title: {self.title}, Status: {self.status}nContent: {self.content[:30]}...\\" def filter_documents(documents: List[Document], status: str) -> List[Document]: Filters the documents based on their status. :param documents: List of Document objects :param status: Status to filter documents :return: A list of filtered Document objects return [doc for doc in documents if doc.status == status] def match_content(documents: List[Document], keyword: str) -> Dict[str, List[Document]]: Matches content of the documents with the provided keyword. :param documents: List of Document objects :param keyword: Keyword to match in the document content :return: A dictionary categorizing documents into \'match\' and \'not_match\' result = {\'match\': [], \'not_match\': []} for doc in documents: if keyword in doc.content: result[\'match\'].append(doc) else: result[\'not_match\'].append(doc) return result def document_statistics(*docs: Document, detailed: bool = False) -> str: Returns the statistics of the documents in a formatted string. :param docs: Variable number of Document objects :param detailed: Whether to provide detailed report or not :return: A formatted string of document statistics if detailed: stat = \\"n\\".join([f\\"Title: {doc.title}nStatus: {doc.status}nContent:n{doc.content}n\\" for doc in docs]) else: stat = \\"n\\".join([f\\"Title: {doc.title}, Status: {doc.status}\\" for doc in docs]) return stat def modify_titles(documents: List[Document], func: Callable[[str], str]) -> None: Modifies the titles of the given documents using the provided function. :param documents: List of Document objects :param func: A function to modify the document titles for doc in documents: doc.title = func(doc.title)"},{"question":"# Coding Assessment **Objective**: Demonstrate your understanding of creating and interpreting Partial Dependence (PDP) and Individual Conditional Expectation (ICE) plots using scikit-learn. **Problem Statement**: You are provided with the Bike Sharing Dataset (included directly in this assignment) which contains hourly data on the number of bikes rented, along with various meteorological and calendar features. You need to train a regression model to predict the number of bike rentals (`count`) and generate PDP and ICE plots to understand the effect of certain features on the number of bike rentals. **Dataset**: - `hourly_bike_sharing.csv`: A CSV file with the following columns: - `temp`: Temperature in Celsius. - `hum`: Humidity percentage. - `windspeed`: Wind speed. - `count`: Number of bike rentals (this is the target variable). **Task**: 1. **Load and Preprocess Data**: - Load the dataset from the provided CSV file. - Split the data into features `X` (temp, hum, windspeed) and target variable `y` (count). 2. **Train Model**: - Train a `GradientBoostingRegressor` model to predict the bike rental count using the features provided. 3. **Generate PDP and ICE Plots**: - Create PDP plots for the features `temp` and `hum` individually, and one two-way PDP plot for the interaction between `temp` and `hum`. - Create ICE plots for the features `temp` and `hum` individually. 4. **Interpretation**: - Briefly describe in comments within your code what insights you can gain from the PDP and ICE plots regarding the effect of temperature and humidity on bike rentals. **Constraints**: - Use scikit-learn\'s `PartialDependenceDisplay.from_estimator` for creating the plots. - Ensure you handle any potential issues with data visualization (e.g., too many lines in ICE plots). **Input**: - A string `file_path` representing the path to the `hourly_bike_sharing.csv` file. **Output**: - Display the PDP and ICE plots. - Include comments in the code describing the insights from these plots. **Example**: ```python import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt # Step 1: Load Data file_path = \'hourly_bike_sharing.csv\' data = pd.read_csv(file_path) X = data[[\'temp\', \'hum\', \'windspeed\']] y = data[\'count\'] # Step 2: Train Model model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Step 3: Generate PDP and ICE Plots features = [\'temp\', \'hum\', (\'temp\', \'hum\')] PartialDependenceDisplay.from_estimator(model, X, features) plt.show() PartialDependenceDisplay.from_estimator(model, X, [\'temp\', \'hum\'], kind=\'individual\') plt.show() # Step 4: Interpretation (Include in comments) # Comment on the PDP plots: # - Insight on how temperature affects bike rentals. # - Insight on how humidity affects bike rentals. # - Interaction effect between temp and hum on bike rentals. # Comment on the ICE plots: # - Observe heterogeneous effects for each sample. # - Identify any groups or patterns in individual responses. ```","solution":"import pandas as pd from sklearn.ensemble import GradientBoostingRegressor from sklearn.inspection import PartialDependenceDisplay import matplotlib.pyplot as plt def bike_sharing_pdp_ice_plots(file_path): # Step 1: Load Data data = pd.read_csv(file_path) X = data[[\'temp\', \'hum\', \'windspeed\']] y = data[\'count\'] # Step 2: Train Model model = GradientBoostingRegressor(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0) model.fit(X, y) # Step 3: Generate PDP and ICE Plots features = [\'temp\', \'hum\', (\'temp\', \'hum\')] # Generate PDP plots PartialDependenceDisplay.from_estimator(model, X, features) plt.show() # Generate ICE plots PartialDependenceDisplay.from_estimator(model, X, [\'temp\', \'hum\'], kind=\'individual\') plt.show() # Step 4: Interpretation (Include in comments) # - From the PDP plots, we can see how the average prediction changes as we vary temp or hum, # while keeping other features constant. # - From the ICE plots, we observe the individual predictions for each sample as we vary temp or hum. # This helps identify if there are any sample-specific trends. # - The two-way PDP plot shows interaction effects between temp and hum on bike rentals. # Usage example: # bike_sharing_pdp_ice_plots(\'hourly_bike_sharing.csv\')"},{"question":"Password Protected Message Retriever You are tasked with creating a secure message retrieval system which requires the user to enter a username and a password to access their messages. Write a Python function `retrieve_message(username, password)` that: 1. Prompts the user to input their username. 2. Validates the provided username against the argument `username` and proceeds if they match. 3. Prompts the user to input their password using `getpass.getpass()`. 4. Validates the provided password against the argument `password`. If they match, prints a welcome message and a dummy message. If not, raises a `ValueError` with an appropriate error message. **Input:** - `username`: A string representing the correct username. - `password`: A string representing the correct password. **Output:** - Prints a welcome message and a stored dummy message if username and password are correctly input. Raises a `ValueError` if authentication fails. **Constraints:** 1. The function must use `getpass.getpass()` for the password input to ensure it is not echoed. 2. Assume that the `username` and `password` provided to the function are non-empty strings. **Example Usage:** ```python def retrieve_message(username, password): # Your implementation here # Sample call retrieve_message(\\"admin\\", \\"secure_pass\\") # With correct inputs, should print: # Welcome admin! # Your message: \\"This is a secure message.\\" ``` If the input username or password do not match the provided arguments: ```python retrieve_message(\\"admin\\", \\"secure_pass\\") # If input username or password are incorrect, should raise ValueError: # ValueError: Incorrect username or password. ``` Remember to appropriately handle the inputs using the `getpass` module for secure password entry.","solution":"import getpass def retrieve_message(correct_username, correct_password): Prompts user for a username and password, validates them against the provided correct_username and correct_password. If validation is successful, prints a welcome message and a dummy message. Otherwise, raises a ValueError. Args: correct_username (str): The correct username. correct_password (str): The correct password. Raises: ValueError: If the username or password is incorrect. input_username = input(\\"Enter your username: \\") if input_username != correct_username: raise ValueError(\\"Incorrect username or password.\\") input_password = getpass.getpass(\\"Enter your password: \\") if input_password != correct_password: raise ValueError(\\"Incorrect username or password.\\") print(f\\"Welcome {correct_username}!\\") print(\\"Your message: \\"This is a secure message.\\"\\")"},{"question":"Context-Aware Configuration Manager You are to implement a Python class `ContextAwareConfigManager` that simulates a configuration manager using context variables. This manager will allow different parts of your application to have different configurations depending on the active context. Each configuration is a simple key-value store (i.e., a dictionary). Your implementation should provide the following functionalities: 1. **Initialize the Manager**: Initialize the manager with an optional default configuration. 2. **Set Configuration**: Set a configuration for the current context. 3. **Get Configuration**: Get the configuration for the current context or the default if none is set for the current context. 4. **Enter New Context**: Enter a new context that includes the current configuration. 5. **Exit Context**: Exit the current context and restore the previous one. The **ContextAwareConfigManager** class should be implemented as follows: ```python from contextvars import ContextVar class ContextAwareConfigManager: def __init__(self, default_config=None): Initialize the manager with an optional default configuration. # Your implementation here def set_config(self, config): Set a configuration for the current context. # Your implementation here def get_config(self): Get the configuration for the current context or the default if none is set. # Your implementation here def enter_context(self): Enter a new context with the current configuration. # Your implementation here def exit_context(self): Exit the current context and restore the previous one. # Your implementation here ``` # Constraints: - The configuration is a dictionary where keys and values are strings. - When you enter a new context, the configuration should be copied from the previous one. - Exiting a context should restore the configuration that was active before entering the context. - Performance is not a primary concern, but your implementation should correctly handle nested contexts. # Example Usage: ```python # Create a manager with a default configuration manager = ContextAwareConfigManager(default_config={\'key1\': \'value1\'}) # Get the default configuration print(manager.get_config()) # Output: {\'key1\': \'value1\'} # Set a new configuration in the current context manager.set_config({\'key2\': \'value2\'}) # Get the current context\'s configuration print(manager.get_config()) # Output: {\'key2\': \'value2\'} # Enter a new context manager.enter_context() # Get the configuration in the new context (should be inherited) print(manager.get_config()) # Output: {\'key2\': \'value2\'} # Set a new configuration in the new context manager.set_config({\'key3\': \'value3\'}) # Get the current context\'s configuration print(manager.get_config()) # Output: {\'key3\': \'value3\'} # Exit the current context manager.exit_context() # Get the configuration after exiting the context print(manager.get_config()) # Output: {\'key2\': \'value2\'} ``` Implement the class `ContextAwareConfigManager` based on the described functionalities above.","solution":"from contextvars import ContextVar, Token class ContextAwareConfigManager: def __init__(self, default_config=None): Initialize the manager with an optional default configuration. self.default_config = default_config or {} self.context_config: ContextVar[dict] = ContextVar(\'context_config\', default=self.default_config) self.token_list: list[Token] = [] def set_config(self, config): Set a configuration for the current context. self.context_config.set(config) def get_config(self): Get the configuration for the current context or the default if none is set. return self.context_config.get() def enter_context(self): Enter a new context with the current configuration. current_config = self.context_config.get().copy() token = self.context_config.set(current_config) self.token_list.append(token) def exit_context(self): Exit the current context and restore the previous one. if self.token_list: token = self.token_list.pop() self.context_config.reset(token)"},{"question":"Objective: Create a C extension module for Python that defines a new type `AdvancedItem`, which represents an item with a name, value, and category. The extension must include methods for manipulating these attributes and ensure proper memory and garbage collection management. The extension should allow `AdvancedItem` to be used as a base class for further extension in Python. Requirements: 1. Define a new C extension type `AdvancedItem` with the following attributes: - `name` (string): The name of the item. - `value` (integer): The value of the item. - `category` (string): The category of the item. 2. Implement the following methods for `AdvancedItem`: - `set_attributes(self, name, value, category)`: Sets the name, value, and category. - `describe(self)`: Returns a string describing the item in the format `\\"{name} belongs to {category} with value {value}\\"`. 3. Ensure that the attributes `name` and `category` are always strings, and `value` is always an integer. Prevent deletion of these attributes. 4. Manage memory correctly, including implementing garbage collection for any potential reference cycles. 5. Allow the `AdvancedItem` to be subclassed by adding the appropriate flags. Constraints: - Your C extension module should be named `advanced_item`. - Ensure that the extension is compatible with the provided Python script to test the functionality. Expected Input and Output: Python script for testing: ```python import advanced_item # Create an instance of AdvancedItem item = advanced_item.AdvancedItem(\\"Sword\\", 150, \\"Weapon\\") # Set new attributes item.set_attributes(\\"Shield\\", 100, \\"Armor\\") # Print the description print(item.describe()) # Output should be: \\"Shield belongs to Armor with value 100\\" # Ensure attributes are strings and int try: item.name = 123 # Should raise an error except TypeError as e: print(e) try: item.value = \\"Not an int\\" # Should raise an error except TypeError as e: print(e) ``` # Submission Create a zip file containing: 1. The C source file for your extension module (`advanced_item.c`). 2. The `setup.py` file for building the extension. 3. Any additional files needed for testing. 4. Documentation comments explaining each part of the implementation. Performance Requirements: - The method implementations should efficiently manage the memory and handle attribute changes. - Ensure the extension is robust against potential misuse (e.g., incorrect types for attributes).","solution":"# This is a mock implementation of the C extension module written in Python for demonstration purposes. # Please note, this should originally be written in C and compiled using setup.py for a real extension module. class AdvancedItem: def __init__(self, name, value, category): self.set_attributes(name, value, category) def set_attributes(self, name, value, category): if not isinstance(name, str): raise TypeError(\\"name must be a string\\") if not isinstance(value, int): raise TypeError(\\"value must be an integer\\") if not isinstance(category, str): raise TypeError(\\"category must be a string\\") self._name = name self._value = value self._category = category @property def name(self): return self._name @name.setter def name(self, new_name): if not isinstance(new_name, str): raise TypeError(\\"name must be a string\\") self._name = new_name @property def value(self): return self._value @value.setter def value(self, new_value): if not isinstance(new_value, int): raise TypeError(\\"value must be an integer\\") self._value = new_value @property def category(self): return self._category @category.setter def category(self, new_category): if not isinstance(new_category, str): raise TypeError(\\"category must be a string\\") self._category = new_category def describe(self): return f\\"{self._name} belongs to {self._category} with value {self._value}\\""},{"question":"You are required to develop a Python script that utilizes the `zipapp` module to create and manipulate executable Python zip archives. Follow the instructions below to complete the task: # Task: 1. **Create an archive**: - Write a function `create_executable_archive(source_dir, output_file, interpreter, main_fn)` that: - Takes a **source directory** (`source_dir`) containing Python files. - Creates an executable Python zip archive named **output_file**. - Uses the specified **interpreter** for the shebang line. - Sets the **main function** (`main_fn`) for the archive. 2. **Display interpreter in archive**: - Write a function `display_interpreter(archive_path)` that: - Takes the path of an existing archive. - Returns the interpreter specified in the shebang line, or `None` if there is no shebang line. # Input and Output Formats: `create_executable_archive` function: - **Input**: - `source_dir` (str): Path to the directory containing the source Python files. - `output_file` (str): Name of the executable zip archive to be created. - `interpreter` (str): Path to the Python interpreter to be used in the shebang line. - `main_fn` (str): Main function to execute, in the form `\'module:function\'`. - **Output**: - None `display_interpreter` function: - **Input**: - `archive_path` (str): Path to the existing executable zip archive. - **Output**: - str: The interpreter specified in the shebang line, or `None` if not present. # Constraints: - Assume that all paths provided are valid and accessible. - Assume that the `source_dir` contains valid Python files including the main function specified. - Both functions should handle exceptions gracefully and print appropriate error messages if something goes wrong. # Example: ```python # Create an archive create_executable_archive(\'myapp\', \'myapp.pyz\', \'/usr/bin/env python3\', \'myapp:main\') # Display interpreter in the archive interpreter = display_interpreter(\'myapp.pyz\') print(interpreter) # Output: /usr/bin/env python3 ``` Develop your Python script with the outlined requirements and functions above.","solution":"import zipapp import os def create_executable_archive(source_dir, output_file, interpreter, main_fn): Creates an executable Python zip archive. :param source_dir: Path to the directory containing the source Python files. :param output_file: Name of the executable zip archive to be created. :param interpreter: Path to the Python interpreter to be used in the shebang line. :param main_fn: Main function to execute, in the form \'module:function\'. try: # Create the archive with the specified interpreter and main function zipapp.create_archive(source_dir, target=output_file, interpreter=interpreter, main=main_fn) print(f\\"Archive {output_file} created successfully\\") except Exception as e: print(f\\"Error creating archive: {e}\\") def display_interpreter(archive_path): Returns the interpreter specified in the shebang line of the archive. :param archive_path: Path to the existing executable zip archive. :return: The interpreter specified in the shebang line, or None if not present. try: with open(archive_path, \'rb\') as f: if f.read(2) == b\'#!\': interpreter = f.readline().decode(\'utf-8\').strip() return interpreter else: return None except Exception as e: print(f\\"Error reading archive: {e}\\") return None"}]'),I={name:"App",components:{PoemCard:A},data(){return{searchQuery:"",visibleCount:4,poemsData:D,isLoading:!1}},computed:{filteredPoems(){const n=this.searchQuery.trim().toLowerCase();return n?this.poemsData.filter(e=>e.question&&e.question.toLowerCase().includes(n)||e.solution&&e.solution.toLowerCase().includes(n)):this.poemsData},displayedPoems(){return this.searchQuery.trim()?this.filteredPoems:this.filteredPoems.slice(0,this.visibleCount)},hasMorePoems(){return!this.searchQuery.trim()&&this.visibleCount<this.poemsData.length}},methods:{async loadMore(){this.isLoading=!0,await new Promise(n=>setTimeout(n,1e3)),this.visibleCount+=4,this.isLoading=!1}}},z={class:"search-container"},F={class:"card-container"},R={key:0,class:"empty-state"},q=["disabled"],O={key:0},N={key:1};function j(n,e,l,m,o,r){const h=_("PoemCard");return a(),s("section",null,[e[4]||(e[4]=t("div",{class:"top-banner"},[t("div",{class:"top-banner-title"},[t("div",{class:"top-banner-title-text"},"🤔prompts chat🧠")])],-1)),t("div",z,[e[3]||(e[3]=t("span",{class:"search-icon"},"🔍",-1)),g(t("input",{type:"text",class:"search-input","onUpdate:modelValue":e[0]||(e[0]=i=>o.searchQuery=i),placeholder:"Search..."},null,512),[[y,o.searchQuery]]),o.searchQuery?(a(),s("button",{key:0,class:"clear-search",onClick:e[1]||(e[1]=i=>o.searchQuery="")}," ✕ ")):d("",!0)]),t("div",F,[(a(!0),s(b,null,v(r.displayedPoems,(i,f)=>(a(),w(h,{key:f,poem:i},null,8,["poem"]))),128)),r.displayedPoems.length===0?(a(),s("div",R,' No results found for "'+u(o.searchQuery)+'". ',1)):d("",!0)]),r.hasMorePoems?(a(),s("button",{key:0,class:"load-more-button",disabled:o.isLoading,onClick:e[2]||(e[2]=(...i)=>r.loadMore&&r.loadMore(...i))},[o.isLoading?(a(),s("span",N,"Loading...")):(a(),s("span",O,"See more"))],8,q)):d("",!0)])}const L=p(I,[["render",j],["__scopeId","data-v-686bdff5"]]),Y=JSON.parse('{"title":"","description":"","frontmatter":{"page":true},"headers":[],"relativePath":"quotes/67.md","filePath":"quotes/67.md"}'),M={name:"quotes/67.md"},B=Object.assign(M,{setup(n){return(e,l)=>(a(),s("div",null,[x(L)]))}});export{Y as __pageData,B as default};
